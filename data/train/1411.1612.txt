{
  "article_text": [
    "it is well known that the real neural networks are subject to internal and external random influences ( cf . , e.g. , @xcite , @xcite , @xcite ) .",
    "therefore , stochastic models of biological neural networks in recent years has attracted a lot of attention @xcite , @xcite , @xcite ( a recent survey of stochastic methods in neuroscience can be found in @xcite ) . in this paper",
    "we consider stochastic neural networks on the basis of the master equation @xcite , @xcite , @xcite .",
    "the aim of this work is to explore the interdependence of information - theoretic metrics ( itm ) for stochastic neural networks and the more simple quantities such as the first distribution moments of the activity of individual neurons .",
    "the itms are important for a quantitative description of the cognitive abilities of neural networks and are based on the concept of information entropy .",
    "examples of the itms include conditional entropy , integrated information , effective information , stochastic interaction , etc .",
    "( see , e.g. , @xcite , @xcite , @xcite , @xcite ) .",
    "however , even numerical calculation of such metrics for sufficiently large networks is an extremely challenging and resource - intensive task since their calculation requires knowledge of the full joint probability for the states of all the neurons ( the probability of a given configuration of the network ) .",
    "moreover approximate methods of calculation of these metrics are currently poorly developed .",
    "it is therefore very important to investigate the possibility of using for description of the cognitive properties along with the above mentioned itms more simple characteristic properties for which there exist well - developed methods for the approximate and numerical calculations .",
    "the specific objective of this work is to establish an interdependence between such a hard - to - calculate itm as the integrated information @xcite , or , more precisely , the integrated information averaged over the states of the network ( average integrated information ; aii ) @xcite , @xcite , and the relatively simpler parameters , namely , the average network activity , average correlations between the activity of individual neurons and inter - cluster correlations . for simplicity , we will call them correlation metrics ( cm ) for neural networks .",
    "the advantage of the cms is that they are expressed in terms of finite order moments of the activity of neurons and , therefore , in the case of large networks it is much easier to develop approximate methods for their calculation .",
    "unfortunately , the determination of the exact matching for itms and cms by analytical methods is very difficult ( if solvable at all ) problem .",
    "it is well known that the recovery of the full joint distribution function for the states of all the neurons ( needed for calculation of the itms ) from its moments is a very complicated problem that does not have comprehensive solutions despite many years of research in this area ( see .",
    ", e.g. , @xcite and references therein ) .",
    "but in our case we are facing even more complex variant of this problem , namely , establishing a relationship between quantities constructed from the probability distribution function for neural network configurations , and the averages over this distribution of products of the activity variables of certain groups of neurons ( see more details in section  [ sec : snd ] ) .",
    "therefore , in this paper we try to establish a numerical relationship of these metrics for a certain set of neural networks .",
    "we consider networks of rather small size , with the number of neurons @xmath0 .",
    "even for such a relatively small networks calculation of such itms as the conditional entropy ( ce ) and the average integrated information ( aii ) requires the solution of a large number of equations ( a system of 256 differential equations for 256 initial conditions ) .",
    "this can be done with the help of computer algebra systems ; in particular , in this work we used _ mathematica _ @xcite . as a result",
    "we have established a dependence of the ce and aii on neural correlations for such networks .",
    "we hope that in the future these results can be generalized to certain classes of neural networks of large size .",
    "in addition , the results for networks of small size may be of self - contained interest , for example when using itms for developing neuromorphic robots ( cf . , e.g. , @xcite ) .",
    "the paper is organized as follows .",
    "the section  [ sec : snd ] briefly describes the model of the neural network used in this work . in section  [ sec : itcq ] the itms and cms are specified , and also the corresponding notations are introduced . in section  [ sec : vtkv ] a methodology for calculating the itms and cms of evolving stochastic neural networks are described , and as well the results of the calculations are presented .",
    "section  [ sec : zak ] contains a discussion and conclusion .",
    "as mentioned in the introduction , we will consider the stochastic neural networks in the framework of stochastic neurodynamics ( snd ) @xcite , @xcite , @xcite . in this approach neural networks",
    "are considered as non - equilibrium stochastic markov systems .",
    "the basis of the description is the master equation .",
    "more specifically , in our paper we consider a model with two - state neurons @xcite , @xcite . in this version of the snd , neurons at each node of the network",
    "can be in one of two states , either `` active '' or `` quiescent '' ( passive ) .",
    "it is assumed that the active state corresponds to firing and the quiescent to resting of a neuron .",
    "synaptic connections are determined by the incidence matrix of the network @xmath1 .",
    "an active neuron spontaneously goes into a quiescent state with a decay rate @xmath2 .",
    "the rate of transition into the active state is determined by the activation function @xmath3 .",
    "a configuration of the networks is described by the random variables @xmath4 ( @xmath5 is the number of neurons in the network ) .",
    "more precisely , we assume that @xmath6 can take two values : @xmath7 . for convenience",
    "we introduce the notation @xmath8 . on the configuration space the probabilities @xmath9",
    "are defined .",
    "they are determined from the master equation @xmath10 where @xmath11 is the probability of the system transition from the configuration @xmath12 to the configuration @xmath13 .    hereafter in this paper we consider the linear activation function @xmath14 where @xmath15 is a constant .",
    "notice that in the stochastic case the assumption of linearity does not seem so far away from the real biological neurons , as in the case of deterministic models , where saturation effects are obviously very important .    in the case of a linear activation function",
    "the master equation ( [ snd1 ] ) can be written after the time rescaling @xmath16 in the form @xmath17p({\\vec v}_{i+};t ) -\\bigl[\\lambda v_i+(v_i+1)w_{ij}v_j\\bigr]p({\\vec v}_{i};t)\\ .",
    "\\label{snd2c}\\ ] ] where @xmath18 , @xmath19 are the special configurations matching the @xmath13 , except that @xmath20-th component is @xmath21 .",
    "the addition is understood in the sense of @xmath22 , i.e. by @xmath23 .",
    "further investigation of neural networks in the framework of this approach is based on solving the system of equations ( [ snd2c ] ) .",
    "it is obvious that for large networks an exact solution is not achievable and it is necessary to develop some approximate methods , the latter being a very difficult task @xcite , @xcite , @xcite .",
    "we will use the exact solution of the system ( [ snd2c ] ) for networks of relatively small size .",
    "as the main itm , we consider the average integrated information defined below in this section .",
    "the very integrated information @xmath24 $ ] is defined as follows @xcite : @xmath25{\\,\\stackrel{\\rm def}{\\equiv}\\,}\\varphi[v;\\vec v;{\\mathcal p}^{mip}]\\ ,             \\label{bs2}\\ ] ] where @xmath26{\\,\\stackrel{\\rm def}{\\equiv}\\,}h\\bigl[p_{\\vec v_0|\\vec v_1=\\vec v}\\bigm|\\bigm|\\prod_{k=1}^r p_{\\vec m^k_0|\\vec m^k_1=\\vec m^k}\\bigr]\\ ,             \\label{bs1}\\ ] ] is the effective information and @xmath27 is the partition for which the normalized effective information has a minimal value . in the definitions ( [ bs2 ] ) and ( [ bs1 ] )",
    "we have used the following notations : @xmath28 is the system of binary neurons ; @xmath29 is the vector of random variables corresponding to the configuration of neurons ; @xmath30 is a particular realization of the system configuration ( the set of configurations of neurons ) ; @xmath31 is the configuration of @xmath32-th subsystem when all the system @xmath28 has the configuration @xmath30 ; @xmath33 $ ] is the relative entropy ( kullback - leibler divergence ) ; @xmath34 is the conditional probability distribution for the network at @xmath35 given that at @xmath36 the system is in the configuration @xmath37 .",
    "we denote the solution of equation ( [ snd2c ] ) with the initial condition @xmath38 ( distribution at the initial time @xmath39 ) as follows : @xmath40)$ ] .",
    "let the complete system be divided into two parts : @xmath41 .",
    "accordingly , the vectors of system configurations take the form : @xmath42 .",
    "the probability of the subsystem to be in the configuration @xmath43 has the form @xmath44)=",
    "\\sum_{\\vec l,\\vec l_0}p(\\vec m , \\vec l , t;[p_0(\\vec m_0,\\vec l_0)])p^n_{unif}(\\vec l_0)\\ ,     \\label{uess2b}\\ ] ] where @xmath45 is the uniform distribution for the subsystem @xmath46 .",
    "thus , the probability distribution for a subsystem is obtained by summing over the final configurations of the subsystem and additional averaging over the initial states of the supplementary subsystem by using of the uniform distribution @xcite .",
    "as is usually done , in the formulas of the type ( [ bs1 ] ) we consider only the case of @xmath47 , i.e. bipartitions , because this greatly simplifies calculations .",
    "the effective information averaged over the network configurations has the form @xcite , @xcite : @xmath48=\\sum_{k=1}^2h_c(m_0^k|m_t^k)-h_c(v_0|v_t)\\ . \\label{bs7}\\ ] ] where @xmath49 are the conditional entropies for the corresponding neural ( sub)networks . recall that the conditional entropy @xmath50 determines the amount of information that is needed to describe the possible realizations of the random variable @xmath51 if we know the value of another random variable @xmath52 : @xmath53 here @xmath54 is the joint probability distribution and @xmath55 is the marginal distribution ; @xmath56 and @xmath57 are the sample spaces for @xmath51 and @xmath52 , respectively .",
    "further details can be found , for example , in @xcite .",
    "the average integrated information ( aii ) is defined as follows @xmath58{\\,\\stackrel{\\rm def}{\\equiv}\\,}\\widetilde{\\varphi}[v;t;\\mathcal{b}^{mib}]\\ ,             \\label{bs5}\\ ] ] where @xmath59 is the minimal bipartition : @xmath60}{k(\\mathcal{b})}\\right\\rbrace\\ , \\label{bs8 } \\\\[3 mm ] k(\\mathcal{b})&{\\,\\stackrel{\\rm def}{\\equiv}\\,}&\\min\\bigl[h(m^1_t),h(m^2_t)\\bigl]\\ .",
    "\\label{bs9}\\end{aligned}\\ ] ] here @xmath61 are the subsystem entropies at the time @xmath62 .",
    "further details on the definition and properties of aii can be found in @xcite and @xcite .",
    "there are also other information - theoretic metrics that may play a role similar to the aii , see , e.g. , @xcite , @xcite , @xcite .",
    "let us introduce the following notations for the mean values    * @xmath63 denotes averaging over the stochastic process , i.e. averaging with the distribution function @xmath40)$ ] , which is the solution of the master equation ( [ snd2c ] ) ; * a symbol with a bar denotes a quantity averaged over the system ( for example , @xmath64 is the average activity of neurons in the network ) .    in this paper",
    ", we use the average of the variables of neural activity @xmath6 of order not higher than the second , i.e.,@xmath65 and @xmath66 ( @xmath67 ) .",
    "the first one determines the average activity of the @xmath20-th neuron while the second determines correlation between the activities of the @xmath20-th and @xmath68-th neurons .",
    "from these averages one can form the well - known combinations :    * covariance @xmath69 an important advantage of the covariance is that it vanishes for independent random variables ; * since in general a magnitude of the covariance depends on measurement units , the pearson correlation coefficient ( pcc ) @xmath70 is widely used : @xmath71 where @xmath72 is the standard deviations .",
    "however for the binary variables @xmath6 in our case the dependence of the covariance on measurement units is irrelevant .",
    "the corresponding quantities averaged over the system are defined as follows    * average activity @xmath73 * average second moment for the neuron variables @xmath74 * average covariance @xmath75 * average pcc @xmath76    in addition to the above mentioned conventional correlation metrics we will use the inter - cluster correlation coefficient ( icc ) : @xmath77 \\bar{c}_{s}&{\\,\\stackrel{\\rm def}{\\equiv}\\,}&\\frac{1}{n_s(n_s-1)}\\sum_{i , j\\in s_k}c_{ij } \\label{wc2a}\\\\[3 mm ] \\bar{c}_{s\\hat{s}}&{\\,\\stackrel{\\rm def}{\\equiv}\\,}&\\frac{1}{n_sn_{\\hat{s}}}\\sum_{i\\in s;j\\in \\hat{s}}c_{ij } \\ , \\end{aligned}\\ ] ] where @xmath78 is the supplementary subnetwork for @xmath79 : @xmath80 , @xmath28 is the set of node of the network under consideration , @xmath81 is the number of nodes in the corresponding subnetworks .",
    "we consider a set of six undirected networks ( fig .",
    "[ fig : combined_undirected ] ) and the six directed networks ( fig .",
    "[ fig : combined_directed ] ) with eight neurons , @xmath0 .",
    "each network has the form of a one - dimensional chain ; in most cases the chain is supplemented by a set of shortcuts .    , for which itms and cms are compared ]    ,",
    "for which itms and cms are compared ]      the conditional entropy ( [ it6 ] ) is expressed through the solution of the master equation ( [ snd2c ] ) as follows @xmath82)p_0(\\vec v_0)\\log\\frac{p(\\vec v , t;[p_0(v)])}{p(\\vec v , t;[\\delta_{\\vec v\\vec v_0}])p_0(\\vec v_0)}\\ ,                                   \\label{nsti4}\\ ] ] where @xmath83 is an arbitrary initial distribution , and @xmath84 is such initial distribution that the configuration @xmath85 is realized with the unitary probability : @xmath86 0 & \\mbox{if } & \\vec v\\neq\\vec v_0 \\end{array}\\right . \\label{nsti1}\\ ] ]    as can be seen from the expression ( [ nsti4 ] ) , for the calculation of the conditional entropy and hence the aii it is necessary to find a solution of the system of equations ( [ snd2c ] ) for @xmath87 initial conditions @xmath85 plus solution for the actual initial distribution @xmath83 . as concerns the latter it is worth mentioning that @xmath88)p_0(\\vec v_0)=p(\\vec v , t;[p_0(v)])\\ , \\label{nsti2}\\ ] ] so knowing the solutions for @xmath84 with all @xmath85 , the solution for an arbitrary initial distribution can be calculated by using ( [ nsti2 ] ) . as the initial condition for the master equation we will use the uniform distribution : @xmath89 .",
    "averages of the variables of neuronal activity can be calculated using the moment equation hierarchy using the methods of the stochastic neurodynamics @xcite .",
    "if we needed only these averages , this way would be easier from a computational point of view .",
    "however , since a computation of the itms requires knowledge of the full distribution function of network configurations , it is certainly easier to calculate averages based on the known distribution . given that the neural variables take the values @xmath90 , the expressions for the moments reads @xmath91 in other words , for the averaging variable one has to take the probability of unit value for this variable and sum up over values of other neurons .",
    "in particular , for the uniform distribution one easily obtains @xmath92      it is well known that the value of integrated information determines ability of a systems to the differentiation ( discrimination/``recognition '' of the initial configuration ) and the integration ( higher capacity for differentiation of the entire system compared with the ensemble of its independent subsystems ; in other words , global cohesion of the system ) .",
    "conditional entropy is the basis for the calculation of the aii ( see ( [ bs7 ] ) ) and determines the ability of the system to the differentiation of initial states . roughly speaking , the larger the conditional entropy at a given time",
    ", the more neural network has `` forgotten '' by this time the initial configuration from which the evolution began .",
    "therefore it is very useful to establish an interrelation of this itm with correlation properties of networks .",
    "obviously , the value of the conditional entropy for binary networks is limited to a maximum value of @xmath5 .",
    "therefore it is convenient to present the results on the interrelation for the normalized conditional entropy @xmath93 the dependence of @xmath94 on the averaged activity @xmath95 ( see ( [ skh1 ] ) ) and on the average second moment @xmath96 ( see ( [ skh1a ] ) ) is presented in fig .",
    "[ fig : vm - h_undir_combined ] .",
    "the figures show the values at the time @xmath36 .",
    "notice that in accordance with ( [ ksii1a ] ) , @xmath97 .     on values of the averaged activity @xmath64 ( triangles ) and the second moment @xmath98 ( boxes ) for the undirected networks depicted in the fig .",
    "[ fig : combined_undirected ] : ( a ) @xmath99 ; ( b ) @xmath100 ; ( c ) @xmath101 ; ( d ) @xmath102 , title=\"fig : \" ]   on values of the averaged activity @xmath64 ( triangles ) and the second moment @xmath98 ( boxes ) for the undirected networks depicted in the fig .",
    "[ fig : combined_undirected ] : ( a ) @xmath99 ; ( b ) @xmath100 ; ( c ) @xmath101 ; ( d ) @xmath102 , title=\"fig : \" ]   on values of the averaged activity @xmath64 ( triangles ) and the second moment @xmath98 ( boxes ) for the undirected networks depicted in the fig .",
    "[ fig : combined_undirected ] : ( a ) @xmath99 ; ( b ) @xmath100 ; ( c ) @xmath101 ; ( d ) @xmath102 , title=\"fig : \" ]   on values of the averaged activity @xmath64 ( triangles ) and the second moment @xmath98 ( boxes ) for the undirected networks depicted in the fig .",
    "[ fig : combined_undirected ] : ( a ) @xmath99 ; ( b ) @xmath100 ; ( c ) @xmath101 ; ( d ) @xmath102 , title=\"fig : \" ]     on values of the averaged activity @xmath64 ( triangles ) and the second moment @xmath98 ( boxes ) for the directed networks depicted in the fig .",
    "[ fig : combined_directed ] : ( a ) @xmath99 ; ( b ) @xmath100,title=\"fig : \" ]   on values of the averaged activity @xmath64 ( triangles ) and the second moment @xmath98 ( boxes ) for the directed networks depicted in the fig .",
    "[ fig : combined_directed ] : ( a ) @xmath99 ; ( b ) @xmath100,title=\"fig : \" ]    it is seen that if to fix the value of @xmath103 and compare @xmath94 for different networks of the same type ( directed or undirected ) , the conditional entropy increases monotonically with increasing values of @xmath95 and @xmath96 for a given network .    on the contrary , from the plots in fig .",
    "[ fig : vh_var_ld_undir ] it can be seen that for a given network and the different values of @xmath103 , the average values @xmath95 , @xmath96 on the one side and @xmath94 on the other side are in an inverse relationship .",
    "further discussion of these properties see in sec .",
    "[ sec : zak ] .     on values of the averaged activity @xmath64 for the undirected networks and different values of @xmath103 ;",
    "the letters next to the curves correspond to the designation of the networks in fig .  [",
    "fig : combined_undirected ] ( color on - line : ( a )  red ; ( b )  blue ; ( c )  green ; ( d )  orange ; ( e )  brown ; ( f )  magenta ) ]    the dependence of the conditional entropy of the other correlation metrics are not so obvious or absent .",
    "for example , the average pearson coefficient @xmath104 is almost the same for all undirected graphs in fig .  [",
    "fig : combined_undirected ] , that is , its value is practically independent of the structure of the networks .",
    "this fact was mentioned in @xcite ( but without evidences , arguments or results of numerical calculations ) .",
    "the dependence of the conditional entropy on the average covariance are not stable and varies with values of @xmath103 ; therefore , the latter cm is also inappropriate for establishing interrelations with itms .",
    "the ability to differentiate the initial configurations at time @xmath105 is the larger , the smaller the value of the conditional entropy @xmath106 . in turn ,",
    "@xmath106 , as shown in the preceding section , is proportional to the values of @xmath64 and @xmath98 , the entropy @xmath106 being more sensitive to variations of the average activity than to @xmath98 .",
    "therefore , we can assume that the value of the integrated information is inversely proportional to the @xmath107 .",
    "the integration of information across the system should depend on the interconnection between its subsystems . as a measure of the interconnection of individual subsystems we choose the inter - cluster correlation coefficient ( [ wc1b ] ) .",
    "obviously , if the network consists of two ( or more ) disconnected parts @xmath79 and @xmath78 , then icc is zero ( all @xmath108 are equal to zero ) .",
    "aii for such neural network is also , obviously , must be zero ( since there is a decomposition into subsystems ( disconnected components ) that differentiate the initial state not worse than the whole network ) .",
    "we can therefore assume that the smaller the minimal inter - cluster correlation coefficient @xmath109 , the smaller should be the aii .",
    "these arguments prompt entering the following quantity , which we call the correlation capacity for information integration ( ccii ) @xmath110 where @xmath111 is a phenomenological parameter .",
    "the relationship between ccii and aii for the neural networks depicted in fig .",
    "[ fig : combined_undirected ] and [ fig : combined_directed ] , is shown in fig .",
    "[ fig : aii - k_undirect_delta15 - 15 - 4 - 2 ] and [ fig : aii - k_direct_delta12 ] , respectively .",
    "it is seen that the magnitude of the aii strongly correlates with ccii and the dependence for the selected values of the parameter @xmath111 is close to a linear one .     on the correlation capacity for information integration ( ccii ) @xmath112 for the undirected networks in fig .",
    "[ fig : combined_undirected ] : ( a )  @xmath113 ; ( b )  @xmath114 ; ( c )  @xmath115 ; ( d )  @xmath116,title=\"fig : \" ]   on the correlation capacity for information integration ( ccii ) @xmath112 for the undirected networks in fig .",
    "[ fig : combined_undirected ] : ( a )  @xmath113 ; ( b )  @xmath114 ; ( c )  @xmath115 ; ( d )  @xmath116,title=\"fig : \" ]   on the correlation capacity for information integration ( ccii ) @xmath112 for the undirected networks in fig .",
    "[ fig : combined_undirected ] : ( a )  @xmath113 ; ( b )  @xmath114 ; ( c )  @xmath115 ; ( d )  @xmath116,title=\"fig : \" ]   on the correlation capacity for information integration ( ccii ) @xmath112 for the undirected networks in fig .",
    "[ fig : combined_undirected ] : ( a )  @xmath113 ; ( b )  @xmath114 ; ( c )  @xmath115 ; ( d )  @xmath116,title=\"fig : \" ]     on the correlation capacity for information integration ( ccii ) @xmath112 for the directed networks in fig .",
    "[ fig : combined_directed ] : ( a )  @xmath117 ; ( b )  @xmath118,title=\"fig : \" ]   on the correlation capacity for information integration ( ccii ) @xmath112 for the directed networks in fig .",
    "[ fig : combined_directed ] : ( a )  @xmath117 ; ( b )  @xmath118,title=\"fig : \" ]",
    "in this paper we studied the relationship between such a complex information - theoretic metric as the integrated information ( more precisely , the average integrated information ) , and simpler correlation characteristic properties of neural networks , namely , average activity , the correlation of activities of individual neurons and inter - cluster correlations .",
    "the advantage of the latter metrics is that they are expressed only in terms of the moments of neuron activities of finite order so that it is much easier to develop approximate methods for their calculations in the case of large realistic networks .",
    "as mentioned in the introduction , an establishment of exact interdependence by analytical methods is a very complicated problem .",
    "therefore in this study we tried to establish a quantitative relationship of these metrics for a certain set of neural networks of small size , namely , with eight neurons @xmath0 .",
    "even for such a relatively small network computing such information - theoretic metrics as the conditional entropy and average integrated information ( aii ) requires the solution of a large number of equations which however can be done using computer algebra systems . as a result , the dependence of conditional entropy and aii on neuron correlations has been established for such networks .    as a model of neural networks",
    "the stochastic neurodynamics @xcite , @xcite was chosen .",
    "we believe that since real neural networks are subject to internal and external random influences ( see .",
    ", e.g. , @xcite , @xcite , @xcite ) , stochastic methods are required for their description .",
    "the variant of the stochastic neurodynamics chosen in this work is quite simple and suitable for numerical analysis and , at the same time , retains some important qualitative features of real neural networks .      *",
    "establishing a direct dependence ( fig .",
    "[ fig : vm - h_undir_combined ] and fig .",
    "[ fig : vm - h_dir_combined ] ) between the value of the average activity @xmath107 and the conditional entropy @xmath106 for the same values of the ratio @xmath103 of deactivation and activation constants and for various neural networks of a given type ( undirected or directed ) ; * * a similar direct dependence exists also for conditional entropy and the average second moment of activities @xmath98 , but dependence of the @xmath119 on the average activity @xmath64 is stronger ; * * these dependencies are approximately linear ; * for a fixed network structure the conditional entropy is inversely dependent ( fig .",
    "[ fig : vh_var_ld_undir ] ) on the magnitude of the average activity ( which , in turn , is greater for smaller values of the ratio @xmath103 of deactivation and activation constants ) ; * out of @xmath64 and the minimal inter - cluster correlation coefficient @xmath32 it is possible to construct the quantity @xmath112 , which we called a correlation capacity for information integration , such that with a suitable choice of the free phenomenological parameter @xmath111 the average integrated information approximately linearly depends on @xmath112 ( fig .",
    "[ fig : aii - k_undirect_delta15 - 15 - 4 - 2 ] and fig .",
    "[ fig : aii - k_direct_delta12 ] ) .",
    "it is worth mentioning that there exist hypotheses in the literature that biological neural networks most effectively process information in metastable states , i.e. near phase transition points ( see .",
    ", eg , @xcite , @xcite ) . of course",
    ", in the case of finite - size networks it is impossible to observe a genuine phase transition .",
    "but such a transition was detected in a similar model for a large ( actually infinite ) closed chain of neurons @xcite : steady - state network for small @xmath103 is fully active , while for large @xmath103 it is completely quiescent ( inactive ) . in our notation ,",
    "the change of the steady states occurs at @xmath120 for the infinite chain .",
    "however , even in small networks one can see a trace of such a `` phase transition '' , namely , the change of sign of the time derivative at @xmath121 of the average activity at some value of @xmath103 .",
    "in other words , for large @xmath103 the network activity from the very beginning of the evolution monotonically falls down while for small @xmath103 the activity at first increases and only then begins to fall ( for infinitely large network totally active configuration may become a stationary one @xcite ) . such an alteration in the system behavior can be considered as a trace of the phase transition in the case of the small systems . the range of @xmath103 where the average activity at the initial time is almost constant can be considered as an analogue of the phase transition point .",
    "examples of the average activity evolution for a number of values of @xmath103 are shown in fig .",
    "[ fig : vevolution ] .     for various values of the parameter @xmath103 ( color on - line ) : @xmath99 ( red ) , @xmath100 ( blue ) , @xmath101 ( green ) , @xmath102 ( black curve ) ( the numbers next to the curves correspond to the values of the parameter @xmath103 ) ]    we could assume that the system has the highest aii near the area of `` phase transition '' where the network is not very active and not very relaxed ; cf .",
    "also the arguments in @xcite .",
    "however , our numerical results show that this assumption is not justified : the aii monotonically grows with growing the average activity , see .",
    "[ fig : vaii_samenn.png ] ( recall that the initial value of @xmath122 ) .     for the undirected networks and various values of @xmath103 ; the letters next to the curves",
    "correspond to the designation of the networks in fig .",
    "[ fig : combined_undirected ] ( color on - line : ( a )  red ; ( b )  blue ; ( c )  green ; ( d )  orange ; ( e )  brown ; ( f )  magenta ) ]    notice also that similarly to the average activity ( fig .",
    "[ fig : vevolution ] ) , by means of solving the equation ( [ snd2c ] ) one can compute values of any other quantities not only at @xmath105 but also for an arbitrary value of @xmath62 , that is to determine their total evolution . in particular",
    ", it is rather curious that the mibs , used for calculation of the integrated information , vary in the evolution process .",
    "as far as we know , there is no mentioning of this fact in the literature .",
    "we hope that in the future the results obtained in this work can be generalized to certain classes of large size neural networks .",
    "in particular , in subsequent studies a specific class of two - layer networks whose basic layer consists of regular short - range links ( such as in regular lattices or fractals ) supplemented by the second layer of long range shortcuts will be investigated .",
    "such networks can be considered as a prototype for cognitive neural networks because a similar hierarchical structure was previously observed in experimental studies of structural and functional connections in human brain .",
    "in addition , the results obtained in this paper for the small size networks may be of independent interest , for example when using the information - theoretic metrics for developing neuromorphic robots .",
    "99 _ softky w.r . and",
    "koch c. _ , the highly irregular firing of cortical cells is inconsistent with temporal integration of random epsps , j. neurosci .",
    "* 13 * ( 1993 ) 33450 . _ pakdaman k. , thieullen m. and wainrib g. _ , fluid limit theorems for stochastic hybrid systems with application to neuron models , adv .",
    "* 42 * ( 2010 ) 76194 .",
    "_ faisal a. a. , selen l.p.j . and wolpert d. m. _ , noise in the nervous system , nature rev .",
    "neurosci . ,",
    "* 9 * ( 2008 ) 292",
    ". _ ohira t. and cowan j.d .",
    "_ , master - equation approach to stochastic neurodynamics , phys .",
    "* e48 * ( 1993 ) 2259 . _ buice m. and cowan j. d. _ , field - theoretic approach to fluctuation effects in neural networks .",
    "* e75 * ( 2007 ) 051919 . _",
    "bressloff p. c. _ , spatiotemporal dynamics of continuum neural fields , j. phys . * a45 * ( 2012 ) 033001 .",
    "_ laing c. and lord g. j. _ , stochastic methods in neuroscience , oxford : oxford university press , 2010 . _",
    "cowan j. d. _ , stochastic neurodynamics , in advances in neural information processing systems , edited by _",
    "touretzky d. s. , lippman r. p. , and moody j. e. _ , san mateo : morgan kaufmann publishers , * 3 * ( 1991 ) 62 .",
    "_ balduzzi d. and tononi g. _ , integrated information in discrete dynamical systems : motivation and theoretical framework , plos computational biology , * 4 * ( 2008 ) e1000091 .",
    "_ barrett a.b . and seth a. k. _ , practical measures of integrated information for time - series data , plos computational biology , * 7 * ( 2011 ) e1001052 .",
    "_ nathan a. and barbosa v.c .",
    "_ , network algorithmics and the emergence of information integration in cortical models , physical review * e84 * ( 2011 ) 011904 .",
    "_ ay n. and wennekers t. _ , dynamical properties of strongly interacting markov chains , neural netw .",
    "* 16 * ( 2003 ) 1483 .",
    "_ mnatsakanov r.m . and hakobyan a.s .",
    "_ , recovery of distributions via moments , ims lecture notes ",
    "monograph series optimality : the third erich l. lehmann symposium , * 57 * ( 2009 ) 252 .",
    "wolfram research , inc . ,",
    "mathematica , version 10.0 , champaign , il , 2014 . _ marstaller l. , hintze a. and adami c._,the evolution of representation in simple cognitive networks , neural computation * 25 * ( 2013 ) 129 .",
    "_ ohira t. and cowan j.d .",
    "_ , path integrals for stochastic neurodynamics , in proceedings of world congress on neural networks , san diego , pp.ii-437 , 1994 . _ cover t. m. and thomas j. a. _ elements of information theory , new york : wiley , 1991 .",
    "_ pernice v. , staude b. , cardanobile s. and rotter s. _ , how structure determines correlations in neuronal networks , plos computational biology , * 7 * ( 2011 ) e1002059 .",
    "_ fraiman d. , balenzuela p. , foss j. and chialvo d.r .",
    "_ , ising - like dynamics in large - scale functional brain networks , phys .",
    ", * e79 * ( 2009 ) 061922 . _",
    "werner g. _ metastability , criticality and phase transitions in brain and its models , biosystems * 90 * ( 2007 ) 496 ."
  ],
  "abstract_text": [
    "<S> on the basis of solutions of the master equation for networks with a small number of neurons it is shown that the conditional entropy and integrated information of neural networks depend on their average activity and inter - cluster correlations . </S>"
  ]
}