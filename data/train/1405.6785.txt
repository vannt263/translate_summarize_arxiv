{
  "article_text": [
    "a general intention of subspace signal processing is to partition the vector space of the observed data and isolate the subspace of the signal component(s ) of interest from the disturbance ( noise ) subspace .",
    "subspace signal processing theory and practice rely , conventionally , on the familiar @xmath1-norm based singular - value decomposition ( svd ) of the data matrix .",
    "the svd solution traces its origin to the fundamental problem of @xmath1-norm low - rank matrix approximation @xcite , which is equivalent to the problem of maximum @xmath1-norm data projection with as many projection ( `` principal '' ) components as the desired low - rank value @xcite . among the many strengths of @xmath1-norm principal component analysis ( pca )",
    ", one may point out the simplicity of the solution , scalability ( new principal directions add on to the previous ones ) , and correspondence to maximum - likelihood estimation ( mle ) under the assumption of additively gaussian - noise corrupted data .",
    "practitioners have long observed , however , that @xmath1-norm pca is sensitive to the presence of outlier values in the data matrix , that is , erroneous values that are away from the nominal data , appear only few times in the data matrix , and are not to appear again under normal system operation upon design .",
    "recently , there has been an arguably small but growing interest in pursuing @xmath0-norm based approaches to deal with the problem of outliers in principal - components design [ ] - [ ] .",
    "the growth in interest can also be credited incidentally to the popularity of compressed sensing methods [ ] - [ ] that rely on @xmath0-based calculations in signal reconstruction .",
    "this paper makes a case for @xmath0-subspace signal processing .",
    "interestingly , in contrast to @xmath1 , subspace decomposition under the @xmath0 error minimization criterion and the @xmath0 projection maximization criterion are not the same .",
    "a line of recent research pursues calculation of @xmath0 principal components under error minimization [ ] - [ ] .",
    "the error surface is non - smooth and the problem non - convex resisting attempts to guaranteed optimization even with exponential computational cost .",
    "suboptimal algorithms may be developed by viewing the minimization function as a convex nondifferentiable function with a bounded lipschitz constant @xcite , @xcite .",
    "a different approach is to calculate subspace components by @xmath0 projection maximization [ ] -[]./@xmath1-norm approach has been followed in [ ] , [ ] . ] no algorithm has appeared so far with guaranteed convergence to the criterion - optimal subspace and no upper bounds are known on the expended computational effort .    in this present work ,",
    "given any data matrix @xmath10 of @xmath2 signal samples of dimension @xmath3 , we show that the general problem of finding the maximum @xmath0-projection principal component of @xmath11 is formally np - hard for asymptotically large @xmath2 , @xmath3 .",
    "we prove , however , that the case of engineering interest of fixed given dimension @xmath3 is not np - hard . in particular , for the case where @xmath4 , we present in explicit form an algorithm to find the optimal component with computational cost @xmath5 . for the case where the sample size exceeds the data dimension (",
    "@xmath6 ) which is arguably of higher interest in signal processing applications we present an algorithm that computes the @xmath0-optimal principal component with complexity @xmath12 , @xmath13 .",
    "we generalize the effort to the problem of calculating @xmath9 , @xmath14 , @xmath0 components ( necessarily a joint computational problem ) and present an explicit optimal algorithm for multi - component subspace design of complexity @xmath15 .",
    "we conclude with illustrations of the developed @xmath0 subspaces in problems from the fields of dimensionality reduction , direction - of - arrival estimation , and image reconstruction that demonstrate the inherent outlier resistance of @xmath0 subspace signal processing .",
    "the rest of the paper is organized as follows .",
    "section ii presents the problem statement and establishes notation .",
    "section iii is devoted to the optimal computation of the @xmath0 principal component .",
    "section iv generalizes to optimal @xmath0-subspace calculation ( joint multiple @xmath0 components ) .",
    "experimental illustrations are given in section v and a few concluding remarks are drawn in section vi .",
    "consider @xmath2 real - valued measurements @xmath16 of dimension @xmath3 that form the @xmath17 data matrix @xmath18.\\ ] ] in the common version of the low - rank approximation problem , one seeks to describe ( approximate ) data matrix @xmath19 by a rank-@xmath9 product @xmath20 where @xmath21 , @xmath22 , @xmath23 .",
    "given the observation data matrix @xmath19 , @xmath1-norm matrix approximation minimizes the sum of the element - wise squared error between the original matrix and its rank-@xmath9 surrogate in the form of problem @xmath24 defined below , @xmath25 where @xmath26 is the @xmath1 matrix norm ( that is , frobenius norm ) of a matrix @xmath27 with elements @xmath28 .",
    "problem @xmath29 is our most familiar @xmath9-singular - value - decomposition ( @xmath9-svd ) problem solved with computational complexity @xmath30 @xcite .",
    "@xmath24 corresponds also to the statistical problem of maximum - likelihood estimation ( mle ) of an unknown rank-@xmath9 matrix corrupted by additive element - wise independent gaussian noise  @xcite .",
    "we may expand  ( [ eq : rs ] ) to @xmath31 and inner minimization results to @xmath32 for any fixed @xmath33 , @xmath34 , by the projection theorem @xcite .",
    "hence , we obtain the equivalent problem @xmath35 frequently referred to as left - side @xmath9-svd . since @xmath36 where @xmath37 denotes the trace of a matrix ,",
    "the @xmath1 error minimization problem @xmath38 is also equivalent to the @xmath1 projection ( energy ) maximization problem @xmath39 the optimal @xmath40 ( in @xmath29 , @xmath38 , and @xmath41 ) is known simply as the @xmath9 dominant - singular - value left singular vectors of the original data matrix or @xmath9 dominant - eigenvalue eigenvectors of @xmath42  @xcite ,  @xcite .",
    "note that , if @xmath43 and we possess the solution @xmath44 for @xmath9 singular / eigen vectors in ( [ eq : rs ] ) , ( [ eq : rr ] ) , ( [ eq : r ] ) , then the solution for rank @xmath45 is derived readily by @xmath46 $ ] with @xmath47 this is known as the pca scalability property .",
    "@xmath1 pca , as reviewed above in @xmath29 , @xmath38 , and @xmath41 , has a simple solution , is scalable ( new principal directions add on to the previous ones ) , and corresponds to mle under the assumption of gaussian additively corrupted data .",
    "practitioners , however , have long noticed a drawback . by minimizing the sum of squared errors ,",
    "@xmath1 principal component calculation becomes sensitive to extreme error value occurrences caused by the presence of outlier measurements in the data matrix ( measurements that are numerically distant from the nominal data , appear only few times in the data matrix , and are not to appear under normal system operation upon design ) .",
    "motivated by this observed drawback of @xmath1 subspace signal processing , in this work we study and pursue subspace - decomposition approaches that are based on the @xmath0 norm , @xmath48 we may  translate \" the three equivalent @xmath1 optimization problems ( [ eq : rs ] ) , ( [ eq : rr ] ) , ( [ eq : r ] ) to new problems that utilize the @xmath0 norm as follows , @xmath49 a few comments appear useful at this point : ( i ) @xmath50 corresponds to mle when the additive noise disturbance follows a laplacian distribution @xcite .",
    "( ii ) the optimal metric value in @xmath51 with a single dimension ( @xmath52 ) is the complexity parameter for saddle - point methods when used to provide an approximate solution to the @xmath53/nuclear - norm dantzig selector problem @xcite .",
    "( iii ) under the @xmath0 norm , the three optimization problems @xmath50 , @xmath54 , and @xmath51 are no longer equivalent .",
    "( iv ) under @xmath0 , the pca scalability property does not hold ( due to loss of the projection theorem ) .",
    "( v ) even for reduction to a single dimension ( rank @xmath52 approximation ) , the three problems are difficult to solve .",
    "( vi ) as of today , it is unknown which of the subspaces defined in @xmath55 , @xmath56 , and @xmath57 exhibits stronger resistance against faulty measurements ; indeed , none of these problems had been solved optimally so far for general @xmath58 .    in this present work ,",
    "we focus exclusively on @xmath51 . in section iii ,",
    "we seek to find efficiently the principal maximum @xmath0 projection component of @xmath19 . in section iv",
    ", we investigate the problem of calculating ( jointly necessarily ) multiple ( @xmath59 ) @xmath0 projection components that maximize the @xmath0  energy \" of the data on the projection subspace .",
    "in this section , we concentrate on the calculation of the @xmath0-maximum - projection component of a data matrix @xmath60 ( problem @xmath61 in ( [ eq : rl1 ] ) , @xmath52 ) .",
    "first , we show that the problem is in general np - hard and review briefly suboptimal techniques from the literature .",
    "then , we prove that , if the data dimension @xmath3 is fixed , the principal @xmath0-norm component is in fact computable in polynomial time and present an algorithm that calculates the @xmath0 principal component of @xmath11 with complexity @xmath62 , @xmath63 .",
    "we present a fundamental property of problem @xmath51 , @xmath52 , that will lead us to an efficient solution .",
    "the property is presented in the form of proposition [ prop : quad ] below and interprets @xmath51 as an equivalent quadratic - form maximization problem over the binary field .    for any data matrix @xmath60 ,",
    "the solution to @xmath64 is given by @xmath65 where@xmath66 in addition , @xmath67 .",
    "[ prop : quad ]    _ proof : _ for any @xmath68 , @xmath69 .",
    "therefore , we can rewrite the optimization problem as @xmath70 for any fixed vector @xmath71 , inner maximization in is solved by @xmath72 and @xmath73 combining  ( [ eq : rxb ] ) and  ( [ eq : xb2 ] ) , we obtain @xmath74 that is , @xmath75 where @xmath76 and @xmath77 .    by proposition [ prop : quad ] , to find the principal @xmath0-norm component @xmath78 we solve  ( [ eq : bopt ] ) to obtain @xmath79 and then calculate @xmath80 . the straightforward approach to solve ( [ eq : bopt ] )",
    "is an exhaustive search among all @xmath5 binary vectors of length @xmath2 .",
    "therefore , with computational cost @xmath5 , proposition [ prop : quad ] identifies the @xmath0-optimal principal component of @xmath19 .",
    "as the data record size @xmath2 grows , calculation of the @xmath0 principal component by exhaustive search in ( [ eq : bopt ] ) becomes quickly infeasible .",
    "proposition [ prop : nphard ] below declares that , indeed , in its general form @xmath61 , @xmath52 , is np - hard for jointly asymptotically large @xmath81 .",
    "mccoy and tropp provide an alternative proof in @xcite , that is the earliest known to the authors .    the computation of the @xmath0 principal component of @xmath82 by maximum @xmath0-norm projection ( problem @xmath57 , @xmath52 ) is np - hard in jointly asymptotic @xmath83 .",
    "[ prop : nphard ]    _ proof : _ in  ( [ eq : rxb ] ) , for any fixed @xmath84 , @xmath85 .",
    "hence , @xmath86 by  ( [ eq : rl1 ] ) and  ( [ eq : brl1 ] ) , computation of the @xmath0 principal component of @xmath19 is equivalent to computation of @xmath79 in  ( [ eq : bopt ] ) . consider the special case of  ( [ eq : bopt ] ) where @xmath87 , @xmath88 , @xmath89 ( hence , @xmath90 ) .",
    "then , @xmath91 but @xmath92 is the np - complete equal - partition problem  @xcite .",
    "we conclude that computation of the @xmath0 principal component of @xmath19 is np - hard in jointly asymptotic @xmath83 .",
    "recently there has been a growing documented effort to calculate subspace components by @xmath0 projection maximization  [ ] - [ ] .",
    "the work in  @xcite presented a suboptimal iterative algorithm for the computation of @xmath78 , which , following the formulation and notation of this present paper , initializes the solution to some arbitrary component @xmath93 and executes @xmath94 @xmath95 , until convergence .",
    "the work in  @xcite presented an iterative algorithm for the joint computation of @xmath96 principal @xmath0-norm components .",
    "for the case where @xmath52 , the iteration in  @xcite simplifies to the iteration in  @xcite ( that is , ( [ eq : kwak1 ] ) ,  ( [ eq : kwak2 ] ) above ) .",
    "therefore , for @xmath52 , the algorithms in  @xcite , @xcite are identical and can , in fact , be described by the simple single iteration @xmath97 for the computation of @xmath79 in  ( [ eq : bopt ] ) . equation , however , does not guarantee convergence to the @xmath0-optimal component solution ( convergence to one of the many local maxima may be observed ) . in the following section , we present for the first time in the literature an optimal algorithm to calculate the @xmath98 principal component of a data matrix with complexity polynomial in the sample size @xmath2 when the data dimension @xmath3 is fixed .",
    "proposition [ prop : nphard ] proves np - hardness of the computation of the @xmath0 principal component @xmath78 in @xmath83 ( that is , when @xmath83 are jointly arbitrarily large ) .",
    "however , of engineering interest is the case of fixed data dimension @xmath3 . in the following ,",
    "we show for the first time in the literature that , if @xmath3 is fixed , then computation of @xmath78 is no longer np - hard ( in @xmath2 ) .",
    "we state our result in the form of proposition [ prop : polynomial ] below .    for any fixed data dimension",
    "@xmath3 , computation of the @xmath0 principal component of @xmath82 has complexity @xmath99 , @xmath100 .",
    "[ prop : polynomial ]    by proposition [ prop : nphard ] , computation of the @xmath98 principal component of @xmath11 is equivalent to computation of @xmath101 in ( [ eq : bopt ] ) . to prove proposition [ prop : polynomial ]",
    ", we will prove that @xmath79 can be computed with complexity @xmath62 .",
    "we begin our developments by defining @xmath102 then , @xmath103 also has rank @xmath104 and can be decomposed by @xmath105,\\;\\;\\;{\\bf q}_i^t{\\bf",
    "q}_j=0,\\;i\\neq j , \\label{eq : qq}\\ ] ] where @xmath106 , @xmath107 , @xmath108 , @xmath109 are the @xmath104 eigenvalue - weighted eigenvectors of @xmath103 with nonzero eigenvalue . by  ( [ eq : bopt ] ) ,",
    "@xmath110    for the case @xmath4 , the optimal binary vector @xmath79 can be obtained directly from  ( [ eq : bopt ] ) by an exhaustive search among all @xmath5 binary vectors @xmath111 .",
    "therefore , we can design the @xmath0-optimal principal component @xmath78 with computational cost @xmath112 .",
    "for the case where the sample size exceeds the data dimension ( @xmath113 ) , we find it useful in terms of both theory and practice to present our developments separately for data rank @xmath114 , @xmath115 , and @xmath116 .",
    "+ _ 1 ) case @xmath114 : _ if the data matrix has rank @xmath114 , then @xmath117 and  ( [ eq : qb ] ) becomes @xmath118 by  ( [ eq : rl1 ] ) , the @xmath0-optimal principal component is @xmath119 designed with complexity @xmath120 .",
    "it is of notable practical importance to observe at this point that even when @xmath11 is not of true rank one , presents us with a quality , trivially calculated approximation of the @xmath0 principal component of @xmath11 : calculate the @xmath1 principal component @xmath121 of the @xmath122 matrix @xmath123 , quantize to @xmath124 , and project and normalize to obtain @xmath125 .",
    "+ _ 2 ) case @xmath115 : _ if @xmath115 , then @xmath126 $ ] and  ( [ eq : qb ] ) becomes @xmath127 the binary optimization problem was seen and solved in @xcite by the auxiliary - angle method @xcite , which was also used earlier in [ ] , [ ] . here , we define the @xmath128 complex vector @xmath129 and rewrite  ( [ eq : rank2 ] ) as @xmath130 we introduce the auxiliary angle @xmath131 and note that , for any complex scalar @xmath132 , @xmath133 with equality if and only if @xmath134 .",
    "that is , @xmath135 therefore , the maximization in  ( [ eq : bz ] ) can be rewritten as @xmath136 where , for any given angle @xmath131 , inner maximization is achieved by @xmath137 then , the optimal vector @xmath79 in  ( [ eq : bz ] ) , i.e. , the solution to  ( [ eq : bopt ] ) , is met if we scan the entire interval @xmath138 and collect the locally optimal vector @xmath139 for any point @xmath131 .",
    "interestingly , as we scan the interval @xmath138 , the locally optimal vector @xmath139 does not change unless the sign of @xmath140 changes for some @xmath141 . since the latter happens only at @xmath142 and @xmath143 , we obtain @xmath144 points in total at which @xmath139 changes .",
    "next , we order the @xmath144 points with complexity @xmath145 and create successively @xmath144 binary vectors by changing each time the sign of @xmath146 if the @xmath147th element of @xmath148 is the one that determines a sign change .",
    "it is observed that the @xmath144 binary vectors that we obtain this way are pair - wise opposite ( the vectors that are collected when @xmath149 are opposite to the ones that are collected when @xmath150 ) . since opposite vectors result in the same metric value in  ( [ eq : bopt ] )",
    ", we can restrict our search to @xmath151 and maintain optimality .",
    "therefore , with overall complexity @xmath152 , we obtain a set of @xmath2 binary vectors that contains @xmath79 .",
    "then , we only have to evaluate the @xmath2 vectors against the metric of interest in  ( [ eq : bopt ] ) to obtain @xmath79 .",
    "we conclude that the @xmath0-optimal principal component of a rank-@xmath153 matrix @xmath82 is designed with complexity @xmath152 .",
    "_ 3 ) case @xmath154 : _ if @xmath154 , we design the @xmath0-optimal principal component of @xmath19 with complexity @xmath155 by considering the multiple - auxiliary - angle approach that was presented in  @xcite as a generalization of the work in  @xcite .",
    "consider a unit vector @xmath156 . by the cauchy - schwartz inequality , for any @xmath157 , @xmath158 with equality if and only if @xmath159 is codirectional with @xmath160 .",
    "then , @xmath161 by  ( [ eq : ca ] ) , the optimization problem in  ( [ eq : qb ] ) becomes @xmath162 for every @xmath156 , inner maximization in  ( [ eq : maxmax ] ) is solved by the binary vector @xmath163 which is obtained with complexity @xmath164 .",
    "then , by  ( [ eq : maxmax ] ) , the solution to the original problem in  ( [ eq : qb ] ) is met if we collect all binary vectors @xmath165 returned as @xmath159 scans the unit - radius @xmath104-dimensional hypersphere .",
    "that is , @xmath79 in  ( [ eq : qb ] ) is inth element of vector @xmath166 , @xmath167 , can be set nonnegative without loss of optimality , because , for any given @xmath166 , @xmath168 , the binary vectors @xmath169 and @xmath170 result to the same metric value in .",
    "] @xmath171 two fundamental questions for the computational problem under consideration are what the size ( cardinality ) of set @xmath172 is and how much computational effort is expended to form @xmath172 .",
    "we address first the first question .",
    "we introduce the auxiliary - angle vector @xmath173^t \\in\\phi^{d-1}$ ] , @xmath174 , and parametrize @xmath159 as follows , @xmath175 then , we re - express the candidate set in  ( [ eq : s1 ] ) in the form @xmath176 where , according to  ( [ eq : bc ] ) , @xmath177 we note that , for any point @xmath178 , each element @xmath179 , @xmath180 , depends only on the corresponding row of @xmath181 and is determined by @xmath182 .",
    "hence , the value of the binary element @xmath183 changes only when @xmath184 to gain some insight into the process of introducing the auxiliary - angle vector @xmath185 , we notice that the points @xmath185 that satisfy ( [ eq : vnc ] ) determine a hypersurface ( or @xmath186-manifold ) in the @xmath187-dimensional space that partitions @xmath188 into two regions .",
    "one region corresponds to @xmath189 and the other corresponds to @xmath190 .",
    "a key observation in the algorithm is that , as @xmath191 scans any of the two regions , the decision on @xmath146 does not change .",
    "therefore , the @xmath2 rows of @xmath181 are associated with @xmath2 corresponding hypersurfaces that partition @xmath188 into @xmath192 cells @xmath193 such that @xmath194 , @xmath195 @xmath196 @xmath197 , and each cell @xmath198 corresponds to a distinct vector @xmath199 . as a result ,",
    "the candidate vector set is @xmath200 .    in  @xcite",
    ", it was shown that @xmath201 if pairs of cells that correspond to opposite binary vectors ( hence , equivalent vectors with respect to the metric of interest in  ( [ eq : qb ] ) ) are considered as one .",
    "therefore , the candidate vector set @xmath172 has cardinality @xmath202 .",
    "[ fig : surfaces ] presents a visualization of the algorithm / partition for the case of a data matrix @xmath203 of @xmath204 samples with rank @xmath205 . since @xmath206 , the hypersurfaces ( or @xmath186-manifolds )",
    "are , in fact , curves in the @xmath153-dimensional space that partition @xmath207 into cells . the @xmath208 cells and",
    "associated binary candidate vectors are formed by the eight - row three - column eigenvector matrix @xmath181 of @xmath123 and the scanning angle vector @xmath209^t$ ] .",
    "regarding the cost of calculating @xmath210 , since each cell @xmath211 contains at least one vertex ( that is , intersection of @xmath212 hypersurfaces ) , see for example fig .",
    "[ fig : surfaces ] , it suffices to find all vertices in the partition and determine @xmath71 for all neighboring cells .",
    "consider @xmath212 arbitrary hypersurfaces ; say , for example , @xmath213 , @xmath214 , @xmath108 , @xmath215 .",
    "their intersection satisfies @xmath216 and is computed by solving the equation @xmath217 the solution to  ( [ eq : qc ] ) consists of the spherical coordinates of the unit vector in the null space of the @xmath218 matrix @xmath219 .",
    "is full - rank , then its null space has rank @xmath220 and @xmath221 is uniquely determined ( within a sign ambiguity which is resolved by @xmath222 ) . if , instead , @xmath219 is rank - deficient , then the intersection of the @xmath212 hypersurfaces ( i.e. , the solution of  ( [ eq : qc ] ) ) is a @xmath223-manifold ( with @xmath224 ) in the @xmath187-dimensional space and does not generate a new cell .",
    "hence , linearly dependent combinations of @xmath212 rows of @xmath181 are ignored . ] then , the binary vector @xmath71 that corresponds to a neighboring cell is computed by @xmath225 with complexity @xmath164 . note that  ( [ eq : sgnqc ] ) presents ambiguity regarding the sign of the intersecting @xmath212 hypersurfaces . a straightforward way to resolve the ambiguity of size @xmath226 with complexity @xmath227 . ] is to consider all @xmath228 sign combinations for the corresponding elements @xmath229 and obtain the binary vectors of all @xmath228 neighboring cells .",
    "finally , we repeat the above procedure for any combination of @xmath212 intersecting hypersurfaces among the @xmath2 ones . therefore",
    ", the total number of binary candidates that we obtain ( i.e. , the cardinality of @xmath172 ) is upper bounded by @xmath230 . since complexity @xmath164",
    "is required for each combination of @xmath212 rows of @xmath181 to solve  ( [ eq : sgnqc ] ) , the overall complexity of the construction of @xmath172 is @xmath227 for any given matrix @xmath231 .",
    "our complete , new algorithm for the computation of the @xmath0-optimal principal component of a rank-@xmath104 matrix @xmath82 that has complexity @xmath155 is presented in detail in fig .",
    "[ fig : algo ] .",
    "computation of each element of @xmath232 ( i.e. , column of @xmath233 in the algorithm ) is performed independently of each other .",
    "therefore , the proposed algorithm is fully parallelizable .",
    "the space complexity of the algorithm is @xmath234 , since after every computation of a new binary candidate the best binary candidate needs to be stored .",
    "we note that the required optimal binary vector in ( [ eq : qb ] ) can , alternatively , be computed through the algorithm in @xcite , @xcite with time complexity @xmath235 and space complexity at least @xmath164 based on the reverse search for cell enumeration in arrangements  @xcite or with time complexity @xmath236 but space complexity proportional to @xmath237 based on the incremental algorithm for cell enumeration in arrangements @xcite , @xcite .",
    "another algorithm that can solve ( [ eq : qb ] ) with polynomial complexity is in @xcite .",
    "its time complexity is @xmath238 , while its space complexity is polynomially bounded by the output size ( i.e. , @xmath236 ) . in comparison to the above approaches , the algorithm in fig .",
    "2 is the fastest known with smallest ( linear ) space complexity .",
    "we conclude that the @xmath0-optimal principal component of a rank-@xmath104 data matrix @xmath82 , @xmath239 , is obtained with time complexity @xmath155 and space complexity @xmath164 .",
    "that is , the time complexity is polynomial in the sample size with exponent equal to the rank of the data matrix , which is at most equal to the data dimension @xmath3 .",
    "the space complexity is linear in the sample size .",
    "in this section , we switch our interest to the joint design of @xmath59 principal @xmath0 components of a @xmath17 data matrix @xmath19 .",
    "after we review suboptimal approaches from the recent literature , we generalize the result of the previous section and prove that , if the data dimension @xmath3 is fixed , then the @xmath9 principal @xmath0 components of @xmath19 are computable in polynomial time @xmath240 .      for any @xmath241 matrix @xmath242 , @xmath243 where @xmath244 denotes the nuclear norm ( i.e. , the sum of the singular values ) of @xmath242 .",
    "maximization in  ( [ eq : raa ] ) is achieved by @xmath245 where @xmath246 is the `` compact '' svd of @xmath242 , @xmath247 and @xmath248 are @xmath249 and @xmath250 , respectively , matrices with @xmath251 , @xmath252 is a nonsingular diagonal @xmath253 matrix , and @xmath104 is the rank of @xmath242 .",
    "this is due to the trace version of the cauchy - schwarz inequality  @xcite according to which @xmath254 with equality if @xmath255 which is satisfied by @xmath245 .    to identify the optimal @xmath0 subspace for any number of components @xmath9",
    ", we begin by presenting a property of @xmath51 in the form of proposition  [ prop : nuclear ] below .",
    "proposition [ prop : nuclear ] is a generalization of proposition  [ prop : quad ] and interprets @xmath51 as an equivalent nuclear - norm maximization problem over the binary field .    for any data matrix @xmath60 ,",
    "the solution to @xmath256 is given by @xmath257 where @xmath247 and @xmath248 are the @xmath241 and @xmath258 matrices that consist of the @xmath9 dominant - singular - value left and right , respectively , singular vectors of @xmath259 with @xmath260 in addition , @xmath261 .",
    "[ prop : nuclear ]    _ proof : _ we rewrite the optimization problem in  ( [ eq : rl1 ] ) as @xmath262 that is , @xmath261 where @xmath263 and , by  ( [ eq : raa ] ) and  ( [ eq : sa ] ) , @xmath264 where @xmath246 is the `` compact '' svd of @xmath265 .    by proposition  [ prop :",
    "nuclear ] , to find exactly the optimal @xmath0-norm projection operator @xmath266 we can perform the following steps :    1 .",
    "solve  ( [ eq : bopt ] ) to obtain @xmath267 .",
    "2 .   perform svd on @xmath268 .",
    "3 .   return @xmath269 .",
    "steps @xmath220 - @xmath270 offer for the first time a direct approach for the computation of the @xmath9 jointly - optimal @xmath0 principal components of @xmath19 .",
    "step @xmath220 can be executed by an exhaustive search among all @xmath271 binary matrices of size @xmath258 followed by evaluation in the metric of interest in  ( [ eq : bopt ] ) .",
    "that is , with computational cost @xmath272 we identify the @xmath0-optimal @xmath9 principal components of @xmath19 .      for the case @xmath59 ,  @xcite proposed to design the first @xmath0 principal component @xmath78 by the coupled iteration  ( [ eq : kwak1])-([eq : kwak2 ] ) ( which does not guarantee optimality ) and then project the data onto the subspace that is orthogonal to @xmath78 ; design the @xmath0 principal component of the projected data by the same coupled iteration ; and continue similarly . to avoid the above suboptimal projection - greedy approach ,  @xcite presented an iterative algorithm for the computation of @xmath266 altogether ( that is the joint computation of the @xmath9 principal @xmath0 components ) . in the language of proposition [ prop :",
    "nuclear ] , the algorithm can be described as arbitrary initialization at some @xmath273 followed by updates @xmath274 for @xmath275 , until convergence .",
    "similar to the work in  @xcite , the above iteration does not guarantee convergence to the @xmath0-optimal subspace .      by the proof of proposition  [ prop : nuclear ] , for any given @xmath276",
    "the corresponding metric - maximizing binary matrix is @xmath277 .",
    "hence , @xmath278 by proposition  [ prop : nuclear ] and  ( [ eq : brl1 ] ) , computation of the @xmath9 principal @xmath0 components of @xmath279 is equivalent to computation of @xmath267 in  ( [ eq : bopt ] ) , which indicates np - hardness in @xmath83 ( that is , when @xmath83 are arbitrarily large ) .",
    "as before , in this section we consider the case of engineering interest of fixed data dimension @xmath3 . as in section",
    "[ sec : onecomponent ] , we show that , if @xmath3 is fixed , then computation of the @xmath9 principal @xmath0 components of @xmath19 is no longer np - hard ( in @xmath2 ) .",
    "we state our result in the form of the following proposition .    for any fixed data dimension @xmath3 ,",
    "optimal computation of the @xmath9 principal @xmath0 components of @xmath82 can be carried out with complexity @xmath280 , @xmath100 . [",
    "prop : polynomial_multiple ]    to prove proposition  [ prop : polynomial_multiple ] , it suffices to prove that @xmath267 can be computed with complexity @xmath280 . as in  ( [ eq : d ] ) , ( [ eq : qq ] ) , let @xmath104 denote the rank of @xmath19 and @xmath281 where @xmath282 is the eigen - decomposition matrix of @xmath103 . by  ( [ eq : bopt ] ) ,",
    "@xmath283}=\\operatorname*{arg\\,max}_{{\\bf b}\\in\\{\\pm1\\}^{n\\times k}}\\sum_{k=1}^k\\sqrt{\\lambda_k\\left[{\\bf b}^t{\\bf q}{\\bf q}^t{\\bf b}\\right]}=\\operatorname*{arg\\,max}_{{\\bf b}\\in\\{\\pm1\\}^{n\\times k}}\\left\\|{\\bf q}^t{\\bf b}\\right\\| _ * \\label{eq : qb}\\ ] ] where @xmath284 $ ] denotes the @xmath285th eigenvalue of matrix @xmath242 , @xmath286 .    for the case @xmath4 , the optimal binary matirx @xmath267 can be obtained directly from  ( [ eq : bopt ] ) by an exhaustive search among all @xmath271 binary matrices @xmath287 .",
    "therefore , we can design the @xmath0-optimal @xmath9 principal components with computational cost @xmath288 .    for the ( certainly more interesting ) case where the sample size exceeds the data dimension , @xmath113 , we present for the first time a generalized version of the approach in  @xcite ,  @xcite that introduces an orthonormal scanning matrix to maximize a rank - deficient nuclear norm .",
    "in particular , we observe by  ( [ eq : qb ] ) that we need @xmath289 that solves @xmath290 by interchanging the maximizations in  ( [ eq : bqc ] ) , for any fixed @xmath291 matrix @xmath292 the inner maximization with respect to @xmath293 is solved by @xmath294 , \\label{eq : bc}\\ ] ] which is obtained with complexity linear in @xmath2 .",
    "then , by  ( [ eq : bqc ] ) , the solution to our original problem in  ( [ eq : qb ] ) is met if we collect all possible binary matrices @xmath295 returned as the columns of @xmath292 scan the unit - radius @xmath104-dimensional hypersphere while maintaining orthogonality among them .",
    "that is , @xmath267 in  ( [ eq : qb ] ) is in , @xmath296 , since , for any given @xmath297 , @xmath298 , the binary matrices @xmath299 and @xmath300 result to the same metric value in .",
    "] @xmath301 then , by relaxing orthogonality among the columns of @xmath292 , @xmath302_{k , k}=1,\\\\c_{d , k}\\geq0,\\,k=1,2,\\ldots , k\\end{smallmatrix}}\\hspace{-1cm}{\\bf b}({\\bf c}){=}\\big(\\bigcup_{\\begin{smallmatrix}{\\bf c}\\in{\\mathbbm r}^d,\\,\\left\\|{\\bf c}\\right\\|_2=1,\\\\c_d\\geq0\\end{smallmatrix}}\\hspace{-.5cm}{\\bf b}({\\bf c})\\big)^k{=}{\\mathcal s}_1\\times{\\mathcal s}_1\\times\\ldots\\times{\\mathcal s}_1={\\mathcal s}_1^k , \\label{eq : s1k}\\ ] ] which implies that @xmath303 from  ( [ eq : sko ] ) , we observe that the number of binary matrices that we collect as the columns of @xmath292 scan the unit - radius @xmath104-dimensional hypersphere with or without maintaining orthogonality is polynomial in @xmath2 .",
    "after @xmath292 has finished scanning the hypersphere , all collected binary matrices in @xmath304 are compared to each other against the metric of interest in  ( [ eq : qb ] ) with complexity @xmath164 per matrix .",
    "therefore , the complexity to solve  ( [ eq : bopt ] ) is determined by the complexity to build @xmath304 or at most @xmath305 since @xmath306 by .    since @xmath307",
    ", we already have a direct way to solve  ( [ eq : qb ] ) .",
    "first , we construct @xmath172 with complexity @xmath155 as described in section  [ sec : onecomponent ] .",
    "we note that @xmath172 contains @xmath237 binary vectors .",
    "then , we construct @xmath308 which consists of all selections of @xmath9 elements of @xmath172 allowing repeated elements .",
    "the order of the elements in each selection can be disregarded , since the order of the columns of @xmath233 does not affect the metric in  ( [ eq : qb ] ) .",
    "hence , the total number of selections that we need to consider is the number of possible ways one can choose @xmath9 elements from a set of @xmath309 elements disregarding order and allowing repetitions ( i.e. , the number of size-@xmath9 multisets of all @xmath172 ) , which equals  @xcite @xmath310 since @xmath311 . for each one of the @xmath312 binary matrices , we evaluate the corresponding metric @xmath313 in  ( [ eq : qb ] ) with complexity @xmath164 .",
    "then , we identify the optimal matrix @xmath267 by comparing the calculated metric values .",
    "therefore , the overall complexity to solve  ( [ eq : bopt ] ) is @xmath314 .",
    "the complete algorithm for the computation of the optimal @xmath9-dimensional ( @xmath59 ) @xmath0-principal subspace of a rank-@xmath104 matrix @xmath82 with complexity @xmath315 is given in fig .",
    "[ fig : algo_multi ] . as a simple illustration of the practical computational cost of the presented algorithm , in table i",
    "we show the average cpu time expended by an intel@xmath316 core@xmath317 i5 processor at 3.40 ghz running the algorithm of fig .",
    "[ fig : algo_multi ] in matlab@xmath316 r2012a to calculate the @xmath318 principal components of a @xmath319 rank-@xmath104 data matrix for @xmath320 and @xmath321 ( we consider only the cases @xmath322 ) .",
    "the presented cpu time for each @xmath323 case is the average over @xmath324 data matrix realizations created with independent zero - mean unit - variance gaussian drawn entries .",
    "importantly , per figs . 2 and 3 , both visiting the @xmath325 manifold - intersection points for constructing @xmath210 ( lines 2 - 8 of function _ compute_candidates _ in fig . [",
    "fig : algo ] ) and constructing @xmath326 given @xmath210 ( line 4 of the @xmath0-principal subspace algorithm in fig .",
    "[ fig : algo_multi ] ) are fully parallelizable actions that can be distributed over multiple processing units .",
    "thus , the entire subspace calculation is fully parallelizable and the expended calculation time can be divided down by the number of available processors ( plus necessary inter - processor communication overhead ) .",
    "in this section , we carry out a few experimental studies on @xmath0-subspace signal processing to motivate and illustrate the theoretical developments in the previous sections .",
    "examples are drawn from the research fields of dimensionality reduction , data restoration , direction - of - arrival estimation , and image conditioning / reconstruction .",
    "we generate a nominal data set @xmath203 of @xmath327 two - dimensional ( @xmath328 ) observation points drawn from the gaussian distribution @xmath329 as seen in fig . [",
    "fig : dr1 ] .",
    "we calculate and plot in fig .",
    "[ fig : dr1 ] the @xmath1 ( by standard svd ) and @xmath0 ( by section iii.c , case @xmath115 , complexity about @xmath330 ) principal component of the data matrix @xmath11 . principal component of @xmath331 would have required complexity proportional to @xmath332 ( by ) , which is of course infeasible . ] for reference purposes , we also plot the true nominal data maximum - variance direction , i.e. , the dominant eigenvector of the autocorrelation matrix @xmath333 .",
    "then , we assume that our data matrix is corrupted by four outlier measurements , @xmath334 , shown in the bottom right corner of fig .",
    "[ fig : dr2 ] .",
    "we recalculate the @xmath1 and @xmath0 principal component of the corrupted data matrix @xmath335 $ ] and notice ( fig .",
    "[ fig : dr1 ] versus fig .",
    "[ fig : dr2 ] ) how strongly the @xmath1 component responds to the outliers compared to @xmath0 . to quantify the impact of the outliers , in fig .",
    "[ fig : dr3 ] we generate @xmath336 new independent evaluation data points from @xmath329 and estimate the mean square - fit - error @xmath337 when @xmath338 or @xmath339 .",
    "we find @xmath340 versus @xmath341 .",
    "in contrast , when the principal component is calculated from the clean training set , @xmath342 or @xmath343 , we find estimated mean square - fit - error @xmath344 and @xmath345 , correspondingly .",
    "we conclude that dimensionality reduction by @xmath0 principal components may loose only minimally in mean - square fit compared to @xmath1 when the designs are from clean training sets , but can protect significantly when training is carried out in the presence of erroneous data .",
    "next , we will compare the dimensionality - reduction performance of the proposed @xmath0-principal subspace with that of other subspaces in the literature obtained by means of @xmath0-norm based methods . specifically , alongside the @xmath1 ( svd ) and @xmath0-principal component ( proposed ) , we calculate the @xmath346-principal component @xcite as well as the direction obtained by means of @xmath0-factorization through alternating weighted median calculation @xcite , @xcite .-",
    "pca @xcite and @xmath0-factorization @xcite , @xcite , no optimal solution exists in the literature so far . ]",
    "all directions are calculated from an @xmath347-point corrupted data set @xmath348 with @xmath349 outliers drawn from @xmath350 and @xmath351 nominal points drawn from @xmath352 . in fig .",
    "[ fig : linefitting_mse ] , we plot the mean - squared - fit - error averaged over @xmath353 independent corrupted training data - set experiments as a function of the number of outlying points in the data set @xmath349 .",
    "we notice that , when designed on nominal data , all examined subspaces differ little , if any , from the @xmath1-principal subspace in mean - square fit error . however , when designed on outlier - corrupted data sets , the @xmath0-principal subspace exhibits notable robustness outperforming uniformly and significantly all other subspaces , especially in the @xmath354 - @xmath355 mid - range of corruption . given that @xmath0 and @xmath1 start very near each other in mean - square - fit - error at @xmath356 corruption and meet again only at @xmath357 corruption , one is tempted to say that the @xmath0 subspaces are to be uniformly preferred over @xmath1 if the associated computational cost can be afforded .",
    "as a toy numerical example , consider a hypothetical case where we collect from a sensor system eight samples of five - dimensional data .",
    "due to the nature of the sensed source , the data are to lie in a lower - than - five dimensional space , say a plane .",
    "say , then , the true data are given by the @xmath358-@xmath153 data matrix below @xmath359 assume that due to sensor malfunction or data transfer error or data storage failure , we are presented instead with @xmath360   + where six of the original entries in two of the data points have been altered / overwritten and @xmath361 spans now a four - dimensional subspace of @xmath362 .",
    "our objective is to `` restore '' @xmath361 to @xmath11 taking advantage of our knowledge ( or assumption ) of the rank of the original data . along these lines",
    ", we project @xmath363 onto the span of its @xmath318 @xmath1- or @xmath0-principal components , @xmath364 where @xmath365 $ ] or @xmath366 $ ] .",
    "the resulting @xmath1- and @xmath0-derived representations of @xmath11 are @xmath367 and @xmath368    respectively . in fig .",
    "[ fig : error ] , we plot the element - by - element and per - measurement square - restoration error for the two projections . the relative superiority of @xmath0-subspace data representation is clearly captured and documented .",
    "we consider a uniform linear antenna array of @xmath369 elements that takes @xmath370 snapshots of two incoming signals with angles of arrival @xmath371 and @xmath372 , @xmath373 where @xmath374 are the received - signal amplitudes with array response vectors @xmath375 and @xmath376 , correspondingly , and @xmath377 is additive white complex gaussian noise .",
    "we assume that the signal - to - noise ratio ( snr ) of the two signals is @xmath378 and @xmath379 .",
    "next , we assume that one arbitrarily selected measurement out of the ten observations @xmath380 \\in \\mathbb c^{5 \\times 10}$ ] is corrupted by a jammer operating at angle @xmath381 with amplitude @xmath382 .",
    "we call the resulting corrupted observation set @xmath383 and create the real - valued version @xmath384^t \\in \\mathbb r^{10 \\times 10}$ ] by @xmath385 part concatenation .",
    "we calculate the @xmath318 @xmath1-principal components of @xmath386 , @xmath387 \\in \\mathbb r^{10 \\times 2}$ ] , and the @xmath318 @xmath0-principal components of @xmath386 , @xmath388 \\in \\mathbb r^{10 \\times 2}$ ] . in fig .",
    "[ fig : doa ] , we plot the standard @xmath1 music spectrum @xcite @xmath389 where @xmath390^t$ ] , as well as what we may call `` @xmath0 music spectrum '' with @xmath391 in place of @xmath392 .",
    "it is interesting to observe how @xmath0 music ( in contrast to @xmath1 music ) does not respond to the one - out - of - ten outlying jammer value in the data set and shows only the directions of the two actual nominal signals .",
    "consider the `` clean '' @xmath393 gray - scale image @xmath394 of fig .",
    "[ fig : alexis1 ] .",
    "we assume that @xmath27 is not available and instead we have a data set of @xmath370 corrupted / occluded versions of @xmath27 , say @xmath395 . each corrupted instance @xmath396 , @xmath397 ,",
    "is created by partitioning the original image @xmath27 into sixteen tiles of size @xmath398 and replacing three arbitrarily selected tiles by @xmath398 grayscale - noise patches as seen , for example , in fig .",
    "[ fig : alexis2 ] .",
    "the @xmath399 corrupted instances are vectorized to form the data matrix @xmath400 \\in \\{0 , \\ldots , 255\\}^{6400 \\times 10}.\\end{aligned}\\ ] ] next , we `` condense '' @xmath401 to a rank-@xmath153 representation by both @xmath1- and @xmath0-subspace projection , @xmath402 where @xmath403 consists of the @xmath318 @xmath1 or @xmath0 , accordingly , principal components of @xmath401 . in fig .",
    "[ fig : alexis3 ] we show the projection of the corrupted image of fig .",
    "[ fig : alexis2 ] onto the @xmath1-derived rank-@xmath153 subspace ( maximum-@xmath1-projection reconstruction ) . in fig .",
    "[ fig : alexis4 ] , we show the projection of the same image onto the @xmath0-derived rank-@xmath153 subspace ( maximum-@xmath0-projection reconstruction ) . figs . [ fig : alexis3 ] and ( d ) offer a perceptual ( visual ) interpretation of the difference between @xmath1 and @xmath0-subspace rank reduction .",
    "it is apparent that maximum-@xmath0-projection reconstruction offers a much clearer image representation of @xmath27 than maximum-@xmath1-projection reconstruction .",
    "this is another result that highlights the resistance of @xmath0-principal subspaces against outlying data corruption .",
    "we presented for the first time in the literature optimal ( exact ) algorithms for the calculation of maximum-@xmath0-projection subspaces of data sets with complexity polynomial in the sample size ( and exponent equal to the data dimension ) .",
    "it may be possible in the future to develop an @xmath0 principal - component - analysis ( pca ) line of research that parallels the enormously rewarding @xmath1 pca / feature - extraction developments .",
    "when @xmath0 subspaces are calculated on nominal ",
    "clean \" training data , they differ little arguably from their @xmath1-subspace counterparts in least - squares fit .",
    "when , however , subspaces are calculated from data sets with possible erroneous , out - of - line ,  outlier \" entries , then @xmath0 subspace calculation offers significant robustness / resistance to the presence of inappropriate data values .",
    "+   +      the authors would like to thank the associate editor and the four anonymous reviewers for their comments and suggestions that helped improve this manuscript significantly , both in presentation and content .",
    "q. ke and t. kanade , `` robust @xmath0 norm factorization in the presence of outliers and missing data by alternative convex programming , '' in _ proc .",
    "ieee conf .",
    ". vision pattern recog .",
    "( cvpr ) _ , san diego , ca , june 2005 , pp .",
    "739 - 746 .",
    "a. eriksson and a. van den hengel , `` efficient computation of robust low - rank matrix approximations in the presence of missing data using the @xmath0 norm , '' in _ proc .",
    "ieee conf .",
    "vision pattern recog .",
    "( cvpr ) _ , san francisco , ca , june 2010 , pp .",
    "771 - 778 .",
    "l. yu , m. zhang , and c. ding , `` an efficient algorithm for l1-norm principal component analysis , '' in _ proc .",
    "ieee intern .",
    "acoust . speech and signal proc .",
    "( icassp ) _ , kyoto , japan , mar .",
    "2012 , pp .",
    "1377 - 1380 .",
    "f. nie , h. huang , c. ding , d. luo , and h. wang , `` robust principal component analysis with non - greedy @xmath404-norm maximization , '' in _ proc .",
    "joint conf .",
    "( ijcai ) _ , barcelona , spain , july 2011 , pp .",
    "1433 - 1438 .",
    "h. q. luong , b. goossens , j. aelterman , a. piurica , and w. philips , `` a primal - dual algorithm for joint demosaicking and deconvolution , '' in _ proc .",
    "ieee intern .",
    "image proc .",
    "( icip ) _ , orlando , fl , oct .",
    "2012 , pp .",
    "2801 - 2804 .      c. ding , d. zhou , x. he , and h. zha , `` @xmath346-pca : rotational invariant @xmath0-norm principal component analysis for robust subspace factorization , '' in _ proc .",
    "_ , pittsburgh , pa , 2006 , pp .",
    "281 - 288 .",
    "i. motedayen - aval , a. krishnamoorthy , and a. anastasopoulos , `` optimal joint detection / estimation in fading channels with polynomial complexity , '' _ ieee trans .",
    "inf . theory _ ,",
    "209 - 223 , jan . 2007 .",
    "ferrez , k. fukuda , and t. m. liebling , `` solving the fixed rank convex quadratic maximization in binary variables by a parallel zonotope construction algorithm , '' _",
    "european journal of operational research _ ,",
    "35 - 50 , 2005 .",
    "+ 3 & 0.0172 & 0.0406 & 0.0920 & 0.1966 & 0.3900 & 0.7160 + 4 & - & 0.0624 & 0.3526 & 1.4212 & 4.5178 & 11.8686 + 5 & - & 0.1014 & 0.8471 & 5.4944 & 26.3361 & 99.4600 + 6 & - & - & 1.2308 & 12.2289 & 87.1546 & 471.2275 +"
  ],
  "abstract_text": [
    "<S> we describe ways to define and calculate @xmath0-norm signal subspaces which are less sensitive to outlying data than @xmath1-calculated subspaces . </S>",
    "<S> we start with the computation of the @xmath0 maximum - projection principal component of a data matrix containing @xmath2 signal samples of dimension @xmath3 . </S>",
    "<S> we show that while the general problem is formally np - hard in asymptotically large @xmath2 , @xmath3 , the case of engineering interest of fixed dimension @xmath3 and asymptotically large sample size @xmath2 is not . in particular , for the case where the sample size is less than the fixed dimension ( @xmath4 ) , we present in explicit form an optimal algorithm of computational cost @xmath5 . for the case @xmath6 , we present an optimal algorithm of complexity @xmath7 . </S>",
    "<S> we generalize to multiple @xmath0-max - projection components and present an explicit optimal @xmath0 subspace calculation algorithm of complexity @xmath8 where @xmath9 is the desired number of @xmath0 principal components ( subspace rank ) . </S>",
    "<S> we conclude with illustrations of @xmath0-subspace signal processing in the fields of data dimensionality reduction , direction - of - arrival estimation , and image conditioning / restoration .    </S>",
    "<S> * _ index terms _  * dimensionality reduction , direction - of - arrival estimation , eigen - decomposition , erroneous data , faulty measurements , @xmath0 norm , @xmath1 norm , machine learning , outlier resistance , subspace signal processing . </S>"
  ]
}