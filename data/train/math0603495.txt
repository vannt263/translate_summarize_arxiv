{
  "article_text": [
    "iterative proportional scaling algorithm for contingency tables , first proposed by deming and stephan @xcite , has been well studied and generalized by many authors .",
    "ireland and kullback @xcite proved convergence of ips and fienberg @xcite gave a simpler proof of convergence from geometric consideration .",
    "darroch and ratcliff @xcite made a generalization to ips and its geometrical property was studied by csiszr @xcite .",
    "csiszr @xcite also gave a more general proof of convergence and justified ips in a general framework .",
    "extension of ips to continuous case was studied in kullback @xcite and rschendorf @xcite .",
    "effective algorithms and implementations of ips have been also studied by many authors , including @xcite , @xcite , @xcite , @xcite , @xcite .    in this paper",
    ", we propose another generalization of ips based on decomposable submodels .",
    "decomposable models or graph decompositions have been already considered by jirouek @xcite , jirouek and peuil @xcite and malvestuto @xcite .",
    "however they used decomposable models for efficient implementation of conventional ips in the form of tree - computation . here",
    "we use decomposable submodels for generalizing ips itself . in our algorithm",
    "we adjust a larger set of marginals than the conventional ips .",
    "the set of marginals form the generating class of a decomposable submodel . by adjusting more marginals",
    ", our proposed algorithm achieves a faster convergence to the maximum likelihood estimate than the conventional ips , although at present it seems difficult to theoretically prove that our procedure is always faster .",
    "we prove convergence of our proposed procedure , if we adjust the amount of scaling at each step .",
    "we also analyze in detail the behavior of the proposed algorithms around the maximum likelihood estimate . as shown in section [ sec",
    ": experiments ] our procedure works well in practice without adjusting the amount of scaling at each step .",
    "as suggested by a referee , it is an important topic to combine the idea of the present paper and the tree - computation approach for efficient implementation of ips . although we do not give a general result , in section [ sec",
    ": experiments ] we investigate the combination in the case of cycle models and show effectiveness of the combination by numerical experiments .    the organization of this paper is as follows . in section [ sec : preliminaries ] we summarize notations and basic facts on hierarchical models and decomposable models for multiway contingency tables . in section [ sec : gen - ips ] we propose a generalized ips via decomposable submodels , prove its convergence and clarify its behavior close to the maximum likelihood estimate . in section [ sec : experiments ] we perform some numerical experiments to illustrate the effectiveness of the proposed procedure . some discussions are given in section [ sec : discussions ] .",
    "in this section we summarize notations and preliminary materials on decomposable models and conventional ips .",
    "we follow the notation of lauritzen @xcite .",
    "let @xmath1 denote the set of variables of a multiway contingency table .",
    "for each @xmath2 , @xmath3 denotes the set of levels of @xmath4 .",
    "the set of cells is denoted by @xmath5 .",
    "let @xmath6 denote the frequency of a cell @xmath7 and let @xmath8 denote the total sample size . throughout the paper",
    "we denote the relative frequency ( empirical distribution ) by @xmath9 .",
    "for a cell @xmath10 and a subset of variables @xmath11 , the marginal cell of @xmath10 for @xmath12 is denoted by @xmath13 , the marginal of @xmath14 on @xmath12 is denoted by @xmath15 $ ] , and the marginal relative frequency of @xmath12 is denoted by @xmath16 .",
    "the _ generating class _ of a hierarchical model is the family of the variable sets indexing the maximal interaction terms in the hierarchical model .",
    "we denote a hierarchical model with generating class @xmath17 by @xmath18 , and call the sets in @xmath17 the _ generators _ of @xmath18 .",
    "a hierarchical model is a _",
    "decomposable model _",
    "if there exists an ordering @xmath19 of its generators that satisfies the running intersection property :    ( rip)for each @xmath20 @xmath21 , there exists @xmath22 @xmath23 , such that @xmath24 .",
    "such an ordering is called a _",
    "perfect sequence_. given a perfect sequence @xmath19 of the generators of a decomposable model , let @xmath25 if no @xmath26 is the empty set , then the decomposable model is said to be _",
    "connected_. if this is the case , then each set @xmath26 is called a _ separator _ of the generating class of the decomposable model ; moreover , both the generators of the decomposable model and the separators of its generating class can be graphically viewed as being the ( maximal ) cliques and the minimal vertex separators of a suitable chordal , connected graph , sometimes called the `` adjacency graph '' of the generating class of the decomposable model . in what follows",
    ", we always assume that a decomposable model is connected . in this paper @xmath27",
    "denotes the multiset of separators .",
    "the number of times a separator @xmath28 appears in @xmath29 is called the _ multiplicity _ of @xmath28 .",
    "the mle of the cell probabilities @xmath30 under a hierarchical model @xmath18 is given by the probability distribution denoted by @xmath31 that belongs to @xmath18 and satisfies the marginality constraints @xmath32 = r[c ] , \\qquad \\forall c \\in { \\cal c}.\\ ] ] equivalently , @xmath31 is the extension of the set of probability distributions @xmath33 : c \\in { \\cal c}\\}$ ] that has the maximum entropy .",
    "if @xmath18 is a decomposable model then @xmath34 has the following product - form expression : @xmath35 in the following we call @xmath31 in ( [ eq : mle ] ) the _ maximum - entropy extension _ of the set of probability distributions @xmath33 : c \\in   { \\cal c}\\}$ ] . in algorithm 2 below , we use the maximum - entropy extension of the form ( [ eq : mle ] ) of the set @xmath36 : c \\in   { \\cal c}\\}$ ] even when @xmath37 is not necessarily normalized to be a probability distribution .    for obtaining mle for other graphical or hierarchical models",
    "we need some iterative procedure .",
    "the following conventional ips , cycling through the elements of the generating class , is commonly used for this purpose . in the following let",
    "@xmath38 denote the estimate of the probability of the cell @xmath10 at the @xmath39-th step of iteration and let @xmath40 .",
    "* algorithm 0 * ( conventional ips ) + let @xmath41 .",
    "the updating formula is given as @xmath42}{p^{(t)}[c]},\\ ] ] where @xmath43 ,  @xmath44 .",
    "the kullback - leibler divergence ( kl - divergence ) from a probability distribution @xmath45 to another probability distribution @xmath37 is denoted by @xmath46 the log sum inequality ( chapter 2 of @xcite ) for non - negative numbers @xmath47 and @xmath48 is @xmath49 where @xmath50 if @xmath51 , and @xmath52 .",
    "the equality holds if and only if @xmath53 .",
    "in this section we propose a generalization of conventional ips and study its properties . at each step of our procedure",
    "we update a larger set of marginals , which form a decomposable submodel .",
    "we prove convergence of our proposed procedure , if the amount of scaling is adjusted properly at each step .",
    "we also give a detailed analysis of our procedure when the current estimate is close to mle .",
    "we now describe our proposed procedure .",
    "a model @xmath54 is a submodel of @xmath18 if each generator of @xmath54 is contained in some generator of @xmath18 .",
    "let @xmath55 be a set of decomposable submodels of @xmath18 such that each generator of @xmath18 is contained in the generating class of @xmath56 for some @xmath20 .",
    "in this case we say that @xmath55 _ spans _ @xmath18 .    in our procedure",
    "there is a problem of normalization as discussed below .",
    "therefore we denote the non - normalized estimated cell probability at the @xmath39-th step by @xmath57 and the normalized estimated cell probability by @xmath58 .",
    "* algorithm 1*let @xmath59 .",
    "we cycle through @xmath60 and for the @xmath39-th step we update the non - normalized estimated cell probabilities as follows @xmath61 where @xmath62 is the maximum - entropy extension of the set of probability distributions @xmath33 : c \\in   { \\cal c}_j\\}$ ] and @xmath63 is the maximum - entropy extension of the set of probability distributions @xmath64 : c \\in { \\cal c}_j\\}$ ] , and the normalized cell probabilities as @xmath65    [ ex1 ] consider a 4-way contingency table @xmath66 and the following hierarchical model with generating class @xmath67 ( `` 4-cycle model '' ) : @xmath68 by slight abuse of notation write @xmath69 .",
    "the following @xmath70 and @xmath71 is an example of the family of submodels that spans @xmath72 .",
    "@xmath73 for each submodel , the updating procedure is performed as follows .",
    "@xmath74    if we set @xmath75 , algorithm 1 coincides with the conventional ips .",
    "@xmath76 span @xmath72 .",
    "each @xmath77 is composed of one generator of the model .",
    "hence @xmath56 is a decomposable submodel of @xmath18 .",
    "therefore algorithm 1 is a generalization of conventional ips . in the conventional ips , @xmath78 in ( [ eq : ips ] )",
    "satisfies @xmath79 , which is a likelihood equation in ( [ eq : likelihood - equation ] ) .",
    "but in general @xmath78 in ( [ eq : a1a ] ) does not satisfy ( [ eq : likelihood - equation ] ) .",
    "in other words , from a geometric viewpoint of @xmath0-projection in csiszr ( @xcite,@xcite ) , the updating rule ( [ eq : a1 ] ) is not a projection .",
    "we discuss it again in the next section .",
    "in ( [ eq : a1 ] ) , we update @xmath80 .",
    "it should be noted that we have @xmath81 because the normalizing constant is canceled on the right - hand side of ( [ eq : a1 ] ) .",
    "also it is easy to see that , if algorithm 1 in terms of @xmath82 converges , then the limiting @xmath37 s are automatically normalized .",
    "unfortunately it is difficult to prove convergence of algorithm 1 .",
    "the difficulty lies in the fact that the sum @xmath83 after updating might exceed 1 ( i.e.  @xmath84 ) in algorithm 1 even if @xmath57 is normalized as @xmath85 .",
    "however we recommend it because in practice , it works well and has converged to mle in all of our experiments and converges faster than the conventional ips as shown in section [ sec : experiments ] .    in order to deal with the theoretical difficulty concerning the normalization of @xmath86 we consider adjusting the amount of updating . at this point , we need the following lemma .",
    "[ lem1 ] let @xmath14 and @xmath37 be two probability distributions over @xmath87 , and @xmath54 a decomposable model over a nonempty ( proper or improper ) subset @xmath88 of @xmath89 .",
    "let @xmath90 be the maximum - entropy extension of the set of probability distributions @xmath33 : c \\in { \\cal c}'\\}$ ] and @xmath91 be the maximum - entropy extension of the set of probability distributions @xmath36 : c \\in { \\cal    c}'\\}$ ] .",
    "if @xmath92 $ ] is not an extension of the set of probability distributions @xmath33 : c \\in { \\cal c}'\\}$ ] , then there exists a unique @xmath93 for which the function @xmath94 is a probability distribution .    in view of ( [ eq : mle ] )",
    "we have @xmath95 therefore if @xmath96 for all @xmath10 , then the equality in ( [ eq : ratio1 ] ) holds for all @xmath10 with @xmath97 . therefore under the condition of the lemma there exists at least one cell @xmath7",
    "such that @xmath98 then @xmath99 for this @xmath10 is strictly convex in @xmath100 and diverges to @xmath101 as @xmath102 .",
    "write @xmath103 then @xmath104is also strictly convex in @xmath100 and diverges to @xmath101 as @xmath105 .",
    "write @xmath106 and @xmath107 .",
    "consider the differential of @xmath104 at @xmath108 .",
    "@xmath109 now @xmath110 is the negative of kl - divergence and nonpositive . by the log sum inequality , @xmath111 is also nonpositive for @xmath112 .",
    "equality holds if and only if @xmath113 then , except for such a case , @xmath114 , @xmath115 , @xmath116 , and @xmath104 is strictly convex in @xmath100 .",
    "therefore there exists a unique @xmath117 such that @xmath118 .",
    "we now present the following algorithm and its modification based on lemma [ lem1 ] .",
    "* algorithm 2*let @xmath119 .",
    "we cycle through @xmath120 and for the @xmath39-th step we update the unnormalized estimated cell probabilities as @xmath121 and the normalized cell probabilities as @xmath122 .",
    "note that also in algorithm 2 we do not need to normalize at each step and we can perform normalization any time , because @xmath123 is always proportional to @xmath124 .",
    "* algorithm 3*we cycle through @xmath60 and for the @xmath39-th step we update the estimated cell probabilities as follows @xmath125 where @xmath126 is given in lemma [ lem1 ] with @xmath127 .      in this section ,",
    "we prove the correctness of proposed algorithms .",
    "as before let @xmath128 denote the empirical distribution and let @xmath129 denote the mle .",
    "because we consider hierarchical models , the following equation holds ( @xcite , @xcite ) .",
    "@xmath130 @xmath131 corresponds to the log likelihood .",
    "therefore we can prove the correctness of our algorithms by proving @xmath132 as @xmath133 .",
    "algorithm 3 converges to mle .",
    "[ t1 ]    consider kl - divergence after updating , @xmath134 write @xmath135 and @xmath136 as in the proof of lemma [ lem1 ] . then , @xmath137 is a kl - divergence , and nonnegative . by the log sum inequality , @xmath138 is also nonnegative for @xmath112 .",
    "therefore , @xmath139 holds .",
    "equality holds if and only if @xmath140 , @xmath141 .",
    "we see that @xmath142 always decreases after updating .",
    "the rest of the proof is the same as the classical one ( @xcite ) .",
    "using @xmath143 , algorithm 2 converges to mle .",
    "consider kl - divergence after updating , @xmath144 where @xmath104 is given in ( [ eq : g - alpha ] ) with @xmath127 . because @xmath145 , @xmath146 is nonpositive and @xmath142 always decreases after updating .",
    "the rest of the proof is the same as theorem [ t1 ] .    at this point",
    "we discuss algorithm 3 from a geometric viewpoint of @xmath0-projection in the sense of csiszr ( @xcite , @xcite ) . in our procedure",
    "we adjust a larger set of marginals than the conventional ips and in practice kl - divergence decreases more in our proposed algorithms than the conventional ips for each step .",
    "however it is difficult to guarantee this theoretically .",
    "the difficulty lies in the fact that the updating rule ( [ eq : a3 ] ) is not a projection .",
    "in fact , if we repeat ( [ eq : a3 ] ) twice with the same @xmath147 then the cell probabilities change , whereas in the conventional ips repeating the same updating step twice does not change the cell probabilities after the first update .",
    "we can understand the situation as follows .",
    "starting from the current estimate @xmath124 suppose that we repeat the step ( [ eq : a3 ] ) with the same @xmath147 until the cell probabilities converge to @xmath148 .",
    "then the limit @xmath148 maximizes the likelihood function among @xmath30 of the form @xmath149 the right - hand side of ( [ eq : log - affine ] ) forms a log - affine model through @xmath150 ( section 4.2.3 of @xcite ) .",
    "since updating a single @xmath151 in the conventional ips is a special case of ( [ eq : log - affine ] ) , it follows that @xmath152 where @xmath153 is the updated estimate by the conventional ips for some @xmath154 .",
    "therefore a larger decrease of kl - divergence of our procedure compared to conventional ips is only guaranteed in the sense of ( [ eq : pstar ] ) .",
    "the situation will become more clear when we analyze the behavior of algorithm 3 close to mle in the next section .",
    "in this section , we study the behavior of our algorithms when the current estimate is already close to mle .",
    "we assume that mle is in the interior of the parameter space and @xmath155 for all @xmath7 .",
    "we analyze the behavior of @xmath156 .",
    "we also consider the value of @xmath157 which reduces the kl - divergence most and the value of @xmath158 such that kl - divergence decreases in algorithm 2 for @xmath159 .",
    "we repeatedly use the following expansion , @xmath160 assume that the current estimate @xmath150 is close to mle in the following sense . for sufficiently small @xmath161 and for all",
    "@xmath162 , @xmath163 , @xmath164 , @xmath165 we have @xmath166    the following proposition describes the behavior of @xmath156 in algorithm 3 .",
    "[ prop:3.1 ] assume @xmath150 is close to mle in the sense of ( [ ep ] ) .",
    "then @xmath167 [ alpha0 ]    before giving a proof of this proposition we rewrite the numerator of the right - hand side of ( [ eq : alpha0-close ] ) .",
    "let @xmath135 and @xmath136 .",
    "then @xmath168 therefore the numerator is nonnegative .",
    "also note that the denominator of the right - hand side of ( [ eq : alpha0-close ] ) can be written as @xmath169 we see that the numerator of @xmath156 consists of the diagonal square terms when we expand the square of denominator in the form of ( [ eq : prop31-denom ] ) .",
    "we now give a proof of proposition [ prop:3.1 ] .",
    "consider the following expansion , @xmath170 \\\\ = & \\sum _ { c\\in { { \\cal c}}_j}\\left ( \\frac { r(i_{c_j})}{p^{(t)}(i_c)}-1 \\right)-\\sum _ { s\\in { { \\cal s}}_j}\\left ( \\frac { r(i_s)}{p^{(t)}(i_s)}-1\\right)+o(\\varepsilon ^2)\\displaybreak[0 ] \\\\",
    "= & o(\\varepsilon ) .",
    "\\end{aligned}\\ ] ] then the @xmath171-th derivative of @xmath172 at @xmath173 is @xmath174 the first and the second order derivatives of @xmath172 at @xmath173 are , @xmath175 \\\\ = & \\sum _ { c\\in { { \\cal c}}_j}\\sum _ { i_c } p^{(t)}(i_c)\\left\\ { \\frac { r(i_c)}{p^{(t)}(i_c)}-1-\\frac{1}{2}\\left ( \\frac { r(i_c)}{p^{(t)}(i_c)}-1\\right)^2\\right\\}\\\\ & -\\sum _ { s\\in { { \\cal s}}_j}\\sum _ { i_s }",
    "p^{(t)}(i_s)\\left\\ { \\frac { r(i_s)}{p^{(t)}(i_s)}-1-\\frac{1}{2}\\left ( \\frac { r(i_s)}{p^{(t)}(i_s)}-1\\right)^2 \\right\\}+o(\\varepsilon ^3)\\displaybreak[0 ] \\\\",
    "= & \\sum _ { c\\in { { \\cal c}}_j}\\sum _ { i_c } \\left\\ {   ( r(i_c)-p^{(t)}(i_c))- \\frac{p^{(t)}(i_c)}{2}\\left ( \\frac { r(i_c)}{p^{(t)}(i_c)}-1\\right)^2\\right\\}\\\\ & -\\sum _ { s\\in { { \\cal s}}_j}\\sum _ { i_s } \\left\\ { ( r(i_s)-p^{(t)}(i_s))- \\frac{p^{(t)}(i_s)}{2}\\left ( \\frac { r(i_s)}{p^{(t)}(i_s)}-1\\right)^2 \\right\\}+o(\\varepsilon ^3)\\displaybreak[0 ] \\\\ = & \\frac{1}{2}\\sum _ i p^{(t)}(i)\\left\\ { \\sum _ { s\\in { { \\cal s}}_j } \\left ( \\frac { r(i_s)}{p^{(t)}(i_s)}-1\\right)^2 -\\sum _ { c\\in { { \\cal c}}_j}\\left ( \\frac { r(i_c)}{p^{(t)}(i_c)}-1\\right)^2\\right\\ } + o(\\varepsilon ^3),\\end{aligned}\\ ] ] and @xmath176\\\\ = & \\sum_{i } p^{(t)}(i ) \\left\\ { \\sum _ { c\\in { { \\cal c}}_j}\\left ( \\frac{r(i_c)}{p^{(t)}(i_c)}-1 \\right ) -\\sum _ { s\\in { { \\cal s}}_j}\\left ( \\frac{r(i_s)}{p^{(t)}(i_s)}-1\\right ) \\right\\}^2   + o(\\varepsilon^3).\\end{aligned}\\ ] ] then , we expand @xmath172 at @xmath173 , @xmath177 assuming normalization at each step of the algorithm , we have @xmath114 and substituting @xmath178 for @xmath179 , we obtain @xmath180    consider ( [ eq : prop31-numerator ] ) and ( [ eq : prop31-denom ] )",
    ". if the signs of the terms on the right hand side of ( [ eq : prop31-denom ] ) are `` random '' then we can expect that @xmath156 is close to 1 .",
    "we can imagine that @xmath150 converges to mle from various directions .",
    "then @xmath156 is close to 1 `` on the average '' .",
    "furthermore as shown in the following proposition @xmath156 is the optimum value of the adjustment close to mle .",
    "we believe that this is the reason that algorithm 1 works very well in practice .",
    "assume @xmath150 is close to mle in the sense of ( [ ep ] ) .",
    "then @xmath181 where @xmath182 is the value of @xmath179 which reduces the kl - divergence most .",
    "[ alpha1 ]    define @xmath183 by @xmath184 which corresponds to the decrease of kl - divergence before normalization . consider the derivative of @xmath183 , @xmath185 \\\\ = & g^{(1)}(0)+ \\sum_{i } p^{(t)}(i ) \\left(\\frac{p^{*}(i)}{p^{(t)}(i)}-1 \\right ) \\log \\frac{\\prod _ { c\\in { { \\cal c}}_j}r(i_c)}{\\prod _ { s\\in { { \\cal s}}_j}r(i_s)}\\times \\frac{\\prod _ { s\\in { { \\cal s}}_j}p^{(t)}(i_s)}{\\prod _ { c\\in { { \\cal c}}_j}p^{(t)}(i_c)}\\displaybreak[0 ] \\\\",
    "= & g^{(1)}(0)+ \\sum _ i p^{(t)}(i)\\left(\\frac{p^{*}(i)}{p^{(t)}(i)}-1 \\right)\\left\\{\\sum _ { c\\in { { \\cal c}}_j}\\left ( \\frac { r(i_c)}{p^{(t)}(i_c)}-1 \\right)- \\sum _ { s\\in { { \\cal s}}_j } \\left ( \\frac { r(i_s)}{p^{(t)}(i_s)}-1\\right ) \\right\\}\\\\ & + o(\\varepsilon ^3 ) \\displaybreak[0 ] \\\\ = & g^{(1)}(0)+ \\sum _ i p^{(t)}(i)\\left\\ { \\sum _ { c\\in { { \\cal c}}_j}\\left ( \\frac { r(i_c)}{p^{(t)}(i_c)}-1\\right)^2-\\sum _ { s\\in { { \\cal s}}_j } \\left ( \\frac { r(i_s)}{p^{(t)}(i_s)}-1\\right)^2 \\right\\ } + o(\\varepsilon",
    "^3 ) \\displaybreak[0 ] \\\\",
    "= & -g^{(1)}(0)+o(\\varepsilon ^3).\\end{aligned}\\ ] ] consider the derivative of @xmath186 and equating 0 , we obtain , @xmath187 then @xmath188 and @xmath189 therefore we have @xmath190    finally we show that kl - divergence decreases in the range @xmath191 .",
    "this result indicates that in algorithm 2 , @xmath192 often decreases kl - divergence in practice .",
    "assume @xmath150 is close to mle in the sense of ( [ ep ] ) .",
    "then @xmath193 where @xmath194 is the value of @xmath179 such that @xmath195 in algorithm 2 .",
    "[ alpha2 ]    @xmath196 and @xmath197    we show the behavior of @xmath146 and @xmath198 in figure [ behavior ] .",
    "proposition [ alpha0 ] , proposition [ alpha1 ] and proposition [ alpha2 ] indicate that in many cases we can decrease kl - divergence by using @xmath199 . in the next section",
    "we illustrate this by numerical experiments .",
    "in this section , we compare our algorithm 1 with the conventional ips by numerical experiments .",
    "we consider @xmath201-way cycle model with the generating class @xmath202 for @xmath203 . as a family of decomposable submodels which span the model we use the set of two decomposable submodels obtained by deleting one element of generating class of the hierarchical model .",
    "we show the considered model and its submodels in table [ submodels ] , where @xmath204 is abbreviated as @xmath205 .",
    "for example in the 5-way case we span @xmath206 by @xmath207 and @xmath208 as illustrated in figure [ ex_fgg ] .    before we present the results of the experiments , we consider the space - saving implementation of algorithm 1 .",
    ".the submodels in numerical experiments [ cols=\"^,^,^\",options=\"header \" , ]",
    "for using the proposed algorithms , we have to find a family of decomposable submodels that span a generating class of a hierarchical model .",
    "we recommend spanning the generating class by a small number of large decomposable submodels . here",
    "large decomposable submodels might mean maximal submodels in the sense of model inclusion or submodels with largest degrees of freedom .",
    "in the literature some methods for finding a maximal chordal subgraph of a given graph are studied ( @xcite , @xcite , @xcite ) . in the case of graphical models",
    ", this might give a solution to our problem .",
    "however we have to satisfy the condition that each element of a generating class is contained in at least one decomposable submodel .",
    "therefore we need a method to find a maximal chordal subgraph under the restriction that specific generators are contained .",
    "a referee suggested the following simple algorithm .",
    "suppose that a model @xmath18 with @xmath209 is given .",
    "for each set @xmath210 in @xmath72 + choose an ordering @xmath211 of sets in @xmath17 such that @xmath212 and @xmath213 + for all @xmath214 ; + set @xmath215 ; + for @xmath216 + if @xmath217 has the running intersection property then set @xmath218 .",
    "+ note that testing the running intersection property on a set family takes linear time @xcite .    in this paper we compared various algorithms of ips in terms of the cpu time to convergence .",
    "we showed that proposed algorithm converges faster than conventional ips when the model is large by numerical experiments .",
    "we consider the implementation of the tree - computation of algorithm 1 only in the case of cycle models",
    ". it may be possible to implement the tree - computation of algorithm 1 for general hierarchical model when the decomposable submodels are given .",
    "this topic needs further investigation and is left to our future research",
    ".    * acknowledgment .",
    "* the authors are grateful to hisayuki hara for implementation of the tree - computation in section 4 and to satoshi kuriki for very useful comments .",
    "they thank two referees for very constructive and detailed comments .",
    "jirouek , r. and peuil , s. ( 1995 ) , on the effective implementation of the iterative proportional fitting procedure . ,",
    "vol.19 , pp.177189 .",
    "kullback , s. ( 1968 ) , probability densities with given marginals . ,",
    "vol.39 , pp.12361243 ."
  ],
  "abstract_text": [
    "<S> we propose iterative proportional scaling ( ips ) via decomposable submodels for maximizing likelihood function of a hierarchical model for contingency tables . in ordinary ips </S>",
    "<S> the proportional scaling is performed by cycling through the members of the generating class of a hierarchical model . </S>",
    "<S> we propose to adjust more marginals at each step . </S>",
    "<S> this is accomplished by expressing the generating class as a union of decomposable submodels and cycling through the decomposable models . </S>",
    "<S> we prove convergence of our proposed procedure , if the amount of scaling is adjusted properly at each step . </S>",
    "<S> we also analyze the proposed algorithms around the maximum likelihood estimate ( mle ) in detail . </S>",
    "<S> faster convergence of our proposed procedure is illustrated by numerical examples .    </S>",
    "<S> _ keywords and phrases : _   decomposable model , hierarchical model , @xmath0-projection , iterative proportional fitting , kullback - leibler divergence . </S>"
  ]
}