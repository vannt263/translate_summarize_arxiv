{
  "article_text": [
    "the accessible tflops brought forth by accelerator technologies bolster the booming development in machine learning . in particular , deep neural network ( dnn ) has drastically improved state of art visual systems by training toward large scale datasets such as imagenet .",
    "other applications in natural language processing ( nlp ) @xcite , video motion analysis @xcite , recommender systems @xcite have also seen benefits from the large scale . in this work ,",
    "we focus on accelerating the deep neural network training .",
    "sgd evenly trains a batch once in an epoch . whereas we consider this as a problem by demonstrating that sampling bias , intrinsic images differences , and fixed cycle pseudo random ( fcpr ) sampling result in different learning speeds on batches .",
    "first , sampling bias represents some members of a population are unevenly distributed in a batch . for example , a batch may contain 105 images from subpopulation a and 95 images from subpopulation b. cases such as imperfect data shuffling lead to sampling bias .",
    "second , intrinsic image differences indicate that batches are still different at the pixel level . in this case , two different images from the same category are distinct due to the dissimilarity at pixels .",
    "third , the recurring batch retrieval pattern in fcpr sampling propagates aforementioned differences , resulting in different learning rates on batches . besides we demonstrate the additional training on a fully trained batch has little contribution to the learning .",
    "subsequently the problem of sgd is failling to consider these learning dynamics on batches .    in this paper , we present inconsistent stochastic gradient descent ( isgd ) that rebalances training effort for batches .",
    "the inconsistency is reflected by nonuniform gradient updates on batches .",
    "isgd measures the training status of a batch by the associated loss . at any iteration @xmath0 , isgd traces the losses in iterations @xmath1 $ ] where @xmath2 is the number of distinct batches .",
    "these losses assist in constructing a upper limit , mean + 3@xmath3standard deviation , to identify a under trained batch . if a batch is undertrained , isgd solves a new subproblem with a chasing logic plus a conservative penalty ( avoid dramatic parameters changes from the previous step ) to accelerate the training on this particular batch .",
    "otherwise isgd behaves exactly same as sgd .",
    "isgd converges faster than sgd as it rebalances the training effort .",
    "for example , isgd does more gradient updates than sgd on the slow batches .    to fully exploit isgd",
    ", we also study the effects of isgd batch size on the parallelism , synchronization cost , system saturation and scalability . theoretically increasing batch size slows down the convergence @xcite .",
    "thereby a small batch is favored on the single node training .",
    "whereas the multinode training entails excessive communication and the limited parallelism , all of which offset convergence benefits . in this case , the advantages of increasing batch size are : 1 ) increasing batch batch size enables less iterations to converge ; thereby less communication , ; 2 ) the gradient is more accurate enabling us to choose a large learning rate ; 3 ) it improves system saturation and occupancy ; however a unwieldy batch size is not desired ; because it reduces the total number of gradient updates in a fixed time .",
    "we demonstrate the optimal batch size of isgd is system dependent .    in summary ,",
    "the novelties of this work are :    * we consider learning dynamics on batches ; and demonstrate sampling bias , intrinsic image difference and fcpr sampling differentiate batches in training .",
    "* we present isgd that rebalances training effort on batches with respect to their learning statuses . *",
    "we study the isgd batch size , concluding the optimal batch size is system dependent .",
    "we consider the sgd unbiased treatment on batches as a problem .",
    "specifically sgd evenly trains every batch once in an epoch .",
    "our claim is based on two facts .",
    "first , the additional training on a fully trained batch has little contribution to the learning .",
    "second , batches are subjected to various learning speeds . in this section ,",
    "we demonstrate intrinsic image differences and sampling bias as two major factors that differentiate batches in training .",
    "the fcpr sampling , widely integrated with sgd , propagates the difference resulting in different learning speed on batches .",
    "first , we present the cross entropy loss is a good indicator of learning status of a batch .",
    "second , we substantiate intrinsic image differences and sampling bias are contributing factors to batchwise differences with a set of controlled experiments . then we demonstrate the fcpr sampling is necessary to sgd in terms of system efficiency ; but the recurring batch retrieval pattern in it fosters various learning speed . in the end , we explain why the contribution of a fully trained batch is little .",
    "the problem of sgd , therefore , is neglecting the fact of aforementioned learning dynamics .",
    "we formulate the cnn training as the following optimization problem .",
    "let @xmath4 be a loss function with weight vector @xmath5 as function parameters , which takes images @xmath6 as the input .",
    "the objective of cnn training is to find an approximate solution of following optimization problem : @xmath7 a typical training iteration of cnn consists of a forward and backward pass .",
    "forward pass yields a loss that measures the discrepancy between the current predictions and the expected ; backward pass calculates the gradient , the negative of which points to the steepest descent direction .",
    "gradient descent updates the @xmath5 as follows : @xmath8 whereas evaluating the gradient over the entire dataset is extremely expensive especially for large datasets such as imagenet . to resolve this issue , stochastic gradient descent ( sgd )",
    "is proposed to approximate the entire dataset with a small randomly drawn sample @xmath9 .",
    "the upside of sgd is the efficiency of evaluating a small sample in the gradient calculation ; while the downside is abundant iterations caused by noisy gradients due to sampling .",
    "let s define a sample space @xmath10 .",
    "if @xmath11 is a random variable defined on a probability space ( @xmath12 ) , the new objective function is @xmath13 the update rule changes to @xmath14 and the following holds if @xmath15 is properly sampled , @xmath16      a convolutional neural network is a function of @xmath17 , the last layer of which is a softmax loss function calculating the cross entropy between the true prediction probabilities , @xmath18 , and the estimated prediction probabilities @xmath19 .",
    "the cross entropy between @xmath18 and @xmath19 of a batch at iteration @xmath0 is as follows @xcite :",
    "@xmath20 if weight decay is applied , the loss function turns out to be : @xmath21 where @xmath22 is a small constant ( normally around @xmath23 ) .",
    "hence the loss can not be zero with weight decay .",
    "if a batch is fully trained , the loss of it tends to be a very small float with a little fluctuation through the rest of training progression .",
    "the loss produced by the cross entropy is a good indicator of model s learning status on a batch .",
    "given a batch @xmath15 , a small cross entropy @xmath24 implies the estimated probabilities are close to the truth .",
    "please note the cross entropy not only measures the possibility of correct predictions , but also the possibilites of making false predictions .",
    "in addition , the cross entropy yields reliable gradients to solidify the learning when the existing predictions are correct yet not sound .",
    "hence we use the cross - entropy loss to assess the model s learning status on a batch @xmath15 .",
    "for example , a large loss indicates the current model underlearns the batch ; and the model needs to continue training on the batch .",
    "the term learning speed and the loss degeneration speed are used interchangeably in this paper .",
    "+        first , sampling bias is a bias in which a sample is collected in such a way that some members of the intended population are less likely to be included than others . assuming batches @xmath15 and @xmath25 are the perfect representative of the entire dataset @xmath6 , we expect losses of two consecutive iterations follows @xmath26 given proper learning rates @xmath27 and @xmath28 . whereas the equation barely holds in practice .",
    "one explanation is sampling bias .",
    "for example , training a batch of cat images has little effects on recognizing dogs .",
    "if prior iterations are over - trained by cat images , a subsequent batch of dogs will yield a large loss .",
    "existing datasets such as places @xcite , imagenet @xcite contain uneven number of images in each category , potentially resulting in the disproportional distribution of images in a batch .",
    "insufficient shuffling on the dataset is another factor contributing to sampling bias .",
    "the imprefect randomization results in the clusters of subpopulations within a batch .    to justify sampling bias differentiates batches in learning , we synthesized 10 single - class batches each of which is randomly drawn from an exclusive image category in cifar-10 ( total categories are also 10 )",
    ". therefore each batch represents a unique cifar-10 category .",
    "fig.[sgd_loss_speed ] demonstrates the loss traces of single - class batches using an exemplary convolutional neural network provided by caffe .",
    "it is interesting to see 10 loss traces degrades independently ; and each loss trace degrades at own speed .",
    "for example , batch 3 , 4 , 5 , 9 are obviously slower than others . since these batches are fully saturated with sampling bias , we conclude sampling bias contributes to various learning dynamics on batches .",
    "second , intrinsic image difference also prompts distinct learning speed on batches . intrinsic image difference indicates images from the same subpopulation are also different at pixels . to substantiate intrinsic image difference differentiates batches in learning , we conduct another controlled experiment on 10 synthetic cifar-10 batches of size 1000 ; each batch contains 100 images from category 0 , 100 images from category 1 , ... , 100 images from category 9 .",
    "this sequence is fixed for every batch so as to eliminate the potential ordering influence .",
    "therefore , we consider the batch is independent identically distributed .",
    "we adopt the same convolutional neural network used in the sampling bias experiment .",
    "fig.[uniform_batch_sgd_loss_speed ] demonstrates the loss traces of aforementioned i.i.d batches .",
    "a strong variation of losses persists through the training progression , which indicates a strong correlation among batches .",
    "however , it is still clear the losses of i.i.d batches degrade at separate speed .",
    "particularly the loss of batch 4 ( green ) is around 0.5 while batch 3 ( purple ) is around 1.3 at the epoch 400 .",
    "please note these batches are i.i.d ; and they are supposed to be approximately identical to the original dataset . whereas the learning speed of each batches are still , though correlated , different through the training progression . as the only difference among these batches are pixels , we conclude intrinsic image difference is another factor that differentiates batches .",
    "finally the cyclic batch retrieval pattern of fcpr sampling propagates the difference on batches through the training progression resulting in the distinctive learning speed .",
    "sgd relies on a key operation , uniformly drawing a batch from the entire dataset .",
    "it is simple in math but nontrivial in the system implementation .",
    "imagenet , ilsvrc2012 for example , contains 1431167 256@xmath3256 high resolution rgb images accounting for approximately 256 gb in total size . uniformly drawing a random batch from the 256 gb binary file involves significant overhead such as tlb misses or random disk i / o operations .",
    "in addition the drastic speed gap between processor and disk further deteriorates the issue .",
    "existing deep learning frameworks , such as caffe @xcite or torch @xcite , alleviates the issue by pre - permuting the entire dataset before slicing into batches : @xmath29 during the training , each iteration fetches a batch from the permuted dataset in a consecutive manner @xmath30 ; and restart fetching from the beginning @xmath31 after one epoch , creating a fixed circle batch retrieval pattern .",
    "we refer to this sampling method as fixed circle pseudo random sampling .",
    "the random reads are subsequently reduced to sequential reads on disk .",
    "therefore fcpr sampling is widely adopted by sgd .",
    "the problem of fcpr sampling is the limited sample space and the recurring batch retrieval pattern .",
    "let @xmath32 to be the size of a dataset and @xmath33 to be the batch size .",
    "the size of sample space is @xmath32/ @xmath33 ; and the batch being assigned to iteration @xmath34 is @xmath15 , where @xmath35 the model knows the upcoming batch at iteration t. and the batch will flow into the model again at iteration @xmath36 , ... , @xmath37 .",
    "experiments in sampling bias and intrinsic image difference demonstrate training on a batch has limited correlative effects on others . if the learning of a batch is dominated by the training on itself , the loss of this batch is predominately reduced at iteration @xmath38 tending to degrade at own pace .",
    "there is little learning progress made on the batch once it s fully trained .",
    "we identify a batch as fully trained if its loss remains around zero .",
    "the loss fluctuates around a near - zero float due to weight decay , the value of which depends on the size of a model ( eq.[weight_decay ] ) .",
    "the fluctuation is caused by the correlation effects from the training on other batches .",
    "fig.[uniform_batch_sgd_loss_speed ] demonstrates the i.i.d batch 2 and 9 ( yellow ) is fully trained after the epoch 1600 . whereas other i.i.d batches , batch 7 ( blue ) for example , still requires additional training .",
    "all batches are fully trained after the 2100 epoch .",
    "batches 2 and 9 ( yellow ) always stay fully trained in epoch [ 1600 , 2100 ] indicating the same training intense is unnecessary on them . the single class batch in fig.[sgd_loss_speed ]",
    "also demonstrate the same fact .",
    "batch 8 ( yellow ) and 9 ( orange ) are fully trained around 1400 while batch 6 ( dark blue ) , 7 ( purple ) and the rest are still under trained .",
    "batch 6 and 7 , in together with batch 8 and 9 , are subsequently fully trained around 1600 epoch .",
    "although batch 8 and 9 exhibit high variation in epoch [ 1400 1600 ] , it only takes a small portion of their total training time to counteract the correlation effects .",
    "therefore , most training on them are useless .",
    "some batches are fast to learn while some are slow .",
    "thereby a new variant of sgd that rebalances training iterations with respect to a batch s learning status will improve the computational efficiency . in this section ,",
    "we present inconsistent stochastic gradient descent that achieves this goal .",
    "the inconsistency is reflected by the inconsistent training effort spent on each batches .",
    "the first question is how to identify a slow or undertrained batch during the training .",
    "we adopt the control chart to monitor the training process .",
    "if the loss of a batch exceeds the upper control limit , it is identified as a undertrained batch at the moment .",
    "the second question is how to accelerate a undertrained batch .",
    "we solve a new optimization on the batch , the objective of which is to reduce the discrepancy between the undertrained batch and others without drastic parameters changes . finally , we study the effects of isgd batch size toward the convergence rate , parallelism and synchronization cost .",
    "we conclude the optimal isgd batch size is machine dependent .",
    "isgd adopt concepts in statistical process control to identify a undertrained batch .",
    "we assume the normal distribution on the loss of each batch .",
    "the training is regarded as gradually reducing the mean of population tile the model converges . at any iteration @xmath0",
    ", isgd keeps a loss queue tracking the losses produced by iterations in @xmath1 $ ] where @xmath2 is the size of sample space ( or the number of batches in an epoch ) .",
    "the queue calculates two descriptive statistics , which are the average loss @xmath39 and the standard deviation @xmath40 .",
    "the calculation is as follows : @xmath41 @xmath42 ^ 2 } \\ ] ] @xmath43 the @xmath44 control limit is a popular method to identify abnormalities .",
    "since the objective is to reduce losses , isgd uses the upper control limit as a bar to differentiate a undertrained batch .",
    "fig.[control_chart ] demonstrates the loss distribution of batches per epoch .",
    "the red line is the control limit calculated with eq.[control_limits ] .",
    "please note isgd calculates the limit per iteration instead of epoch .",
    "if the loss of current iteration @xmath0 @xmath45 @xmath46 is a undertrained batch .      the core concept of our algorithm is to accelerate a undertrained batch by solving a new optimization problem , the goal of which is to reduce the loss discrepancy between a undertrained batch and the average without drastic parameter changes .",
    "if a batch is undertrained , isgd stays on the batch tile solving the following optimization problem : @xmath47 where @xmath48 is the number of weight parameters in the network .",
    "the first term minimizes the differences between the loss of current undertrained batch @xmath46 and the control limit to accelerate the training on it .",
    "the second term is a conservative constraint avoiding drastic weight changes that may deviate the current model .",
    "then the first order derivative of eq.[new_optimization ] is : @xmath49\\nabla\\psi_{\\mathbf{w}}(\\mathbf{d_t } ) \\\\                                           & + \\frac{\\mathbf{w } - \\mathbf{w_{t-1}}}{n_w }                                      \\\\ \\end{split}\\ ] ] please note @xmath50 , @xmath51 and @xmath46 are constants . solving eq.[new_optimization ] precisely incurs the significant computation and communication overhead , which offsets the benefit of it . in practice , we approximate the solution to the new subproblem , eq.[new_optimization ] , with early stopping .",
    "this avoids the huge searching time wasted on hovering around the optimal solution .",
    "a few iterations , 5 for example , is good enough to achieve the acceleration effects .",
    "thereby we recommend approximating the solution by early stopping .",
    "alg.[ugd_alg ] demonstrates the details of isgd .",
    "since the learning status of a batch is measured by the loss , isgd identifies a batch as undertrained if the loss is larger than the control limit @xmath52 ( line 20 ) .",
    "a stringent limit imposes a tight control triggering eq.[new_optimization ] more frequently .",
    "this leads increasing computation latency spent on a batch .",
    "therefore the stringent limit decreases distinctive batches flowing into a model in a fixed time , which is unwanted . a soft margin , 2 or 3 @xmath53 , is preferred in practice ; and this margin is also widely applied in statistical process control to detect abnormalities in a process .",
    "we recommend users adjusting the margin according to the specific problem .",
    "isgd adopts a loss queue to dynamically track the losses in an epoch so that the average loss , @xmath54 , is calculated in o(1 ) time ( line 17 ) .",
    "the loss queue tracks iterations in the last epoch ; the length of it equals to the length one epoch . therefore , calculating @xmath53 is also in o(1 ) time ( line 18 ) .",
    "we do not initiate the conservative problem until after the first epoch to build up a reliable limit ( line 22 condition iter >",
    "n ) .    alg.[chase_alg ] outlines the procedures to solve the conservative subproblem on a undertrained batch .",
    "the conservative subproblem intends to accelerate the undertrained batch without drastic weight changes ; the update equation in line 7 corresponds to eq.[new_optimization_derivative ] .",
    "@xmath55\\nabla\\psi_{\\mathbf{w}}(\\mathbf{d_t})$ ] is the gradient of @xmath56 to accelerate the training of a undertrained batch ; the second term , @xmath57 , is the gradient of @xmath58 bounding significant weight changes .",
    "the @xmath50 is the same upper control threshold in alg.[ugd_alg ] .",
    "@xmath59 specifies the maximal approximate iterations , which is usually around 5 ; @xmath59 also prevents the excessive training improving computation efficiency .",
    "@xmath60 is a another small constant learning rate .",
    "isgd also intends to scale on the distributed or multigpu system using the data parallelism scheme .",
    "let s assume there are @xmath61 computing nodes , each of which is a gpu or a server in a cluster .",
    "each node contains a model duplicate .",
    "a node fetches an independent segment of the original batch referred to as the subbatch .",
    "subsequently all nodes simultaneously calculate sub - gradients and sub - losses with the assigned sub - batches .",
    "once the calculation is done , the algorithm reduces sub - gradients and sub - losses ( line 10 - 12 ) to a master node so as to acquire a global gradient and loss .",
    "then the master node updates network weights ( line 21 ) and broadcasts the latest weights to each node .",
    "therefore , isgd separates the algorithm from the system configurations by using mpi style broadcast and reduce .",
    "since mpi is an industrial and academia standard , isgd is highly portable on various heterogeneous distributed system .      training a neural network",
    "needs to reduce the learning rate to ensure convergences @xcite .",
    "sgd decreases the learning rate based on the training iterations as the time spent on an iteration is identical . whereas iterations of isgd is inconsistent requiring a new method to decrease the learning rate .",
    "isgd guides the learning rate with the average loss of entire dataset .",
    "the loss is a better indicator than iterations as it directly reflects the training status of the model .",
    "calculating the average loss of a dataset is expensive .",
    "since the average loss in eq.[average_loss ] is from the lastest scan of dataset ( or losses in an epoch ) , it is approximately same to the average loss of dataset at any iteration t. hence we decrement the learning rate with respect to this average loss .",
    "for example , we used the average loss 2 as a learning rate turning point in training alexnet .",
    "please note the average loss may fluctuate around 2 .",
    "our experiments indicate a irreversible learning rate policy improves the performance .",
    "batch size is the key factor to the parallelism of isgd . as operations on a batch",
    "is independent , scaling isgd on systems with massive computing power prefers a sufficiently large batch . in this section",
    ", we present the optimal batch size is machine dependent .",
    "first , we demonstrates a large batch converges faster than a small batch using iterations as the metric .",
    "although increasing the batch size incurs the additional computation , it does not necessarily cost more time per iteration .",
    "this is contingent upon system configurations .",
    "since isgd is a variant of sgd , we assume both have the same convergence properties .",
    "then we analyze the convergence rate with respect to time by incorporating two system factors network and image processing speed .",
    "the result indicates neither an extremely small nor extremely large batch is optimal to a system of the fixed processing power and network latency . the optimal batch size is a tradeoff between the batch exploitation and exploration .      the benefit of using a sample to approximate the population is the significantly less computation per iteration ; while the downside is the noisy gradient .",
    "please note the convergence rate in this section is measured by iterations . to analyze the dynamics inside an iteration , we need define a lyapunov process @xmath62 that measures the distance between the current solution @xmath63 and the optimal solution @xmath64 .",
    "@xmath65 is a random variable .",
    "hence the convergence rate of sgd can be derived using eq.[gd_update ] and eq.[lyapunov_process ] : @xmath66 @xmath9 is a random sample of @xmath6 in sample space @xmath10 ; and @xmath67 is a random variable that depends on the drawn sample @xmath15 and learning rate @xmath27 .",
    "therefore the expectation of eq.[convergence_rate ] yields the average convergence rate at the precision of an iteration @xmath68 to simplify the analysis of eq.[convergence_rate_exp ] , let s assume convexity on @xmath11 implying that @xmath69 @xmath70 our goal is to minimize @xmath71 so that each iteration reduces the discrepancy between the current step and the optimal .",
    "statistically @xmath72 is the unbiased estimate of @xmath73 leaving @xmath74 being the key to improve the convergence rate .",
    "first , increasing the batch size reduces @xmath75 .",
    "bienayme formula @xcite indicates @xmath75 is inverse proportional to the batch size .",
    "a large batch is less susceptible to be the insufficient representation of entire dataset .",
    "according to law of large numbers , increasing batch size improves the estimation of @xmath76 .",
    "second , enlarging batch size expedites the training with a better learning rate .",
    "the learning rate is a critical factor to decide the convergence speed .",
    "the gradient essentially provides the descent direction ; and the learning rate provides the step length in that direction .",
    "whereas the learning rate @xmath27 needs to satisfy the following condition @xcite due to the high sampling variance in an insufficiently representative batch : @xmath77 as batch size increases , the improving @xmath75 leads eq.[learning_rate ] less strict enabling us to use a better learning rate .      a unwieldy large batch size , however , is unwanted under the limited computing budget .",
    "existing convergence rate analysis use iterations as the performance metric .",
    "the problem is an iterationwise faster algorithm may cost more time than the slower counterpart . hence it is practical if we extend the analysis to be time based .",
    "let s assume the maximal processing capability of a system is @xmath78 images per second , and the network cost for broadcast and all - reduce is @xmath79 seconds .",
    "network cost is a constant because it is model dependent instead of batch size .",
    "then a gradient update ( not iteration ) essentially costs : @xmath80 where @xmath2 is the batch size . given fixed time @xmath0 ,",
    "the number of gradient updates can be computed on this system is @xmath81 after t gradient updates , the loss is bounded by @xcite @xmath82 let s assume equality in eq.[loss_bound ] and substitute eq.[iterations_time ] into it .",
    "subsequently we get the relationship among loss , @xmath4 , time @xmath0 and system configurations : @xmath83     at different batch sizes.,width=288 ]    fig.[convergence_rate_by_time ] presents the training time calculated by eq.[loss_by_time ] with respect to different batch sizes , @xmath84 . by fixing @xmath4",
    ", the equation approximates the total training time under different batches .",
    "each line represents a system configuration .",
    "we assume the first system processes up to 2000 images per second , @xmath85 , and the network cost @xmath86 is 0.3 seconds ; the second system processes up to 1000 images per second having the same network cost .",
    "the figure demonstrates the optimal batch size of the first and second system are 500 and 1000 respectively . in this case",
    ", a faster system needs a large batch conforming to our proposition .",
    "then performance of both systems deteriorate afterwards .",
    "the figure clearly indicates the optimal batch size is contingent upon the system configuration .",
    "we implement isgd in caffe @xcite , which already provides the data parallelism on multigpus .",
    "fig.[data_parallelism ] demonstrates the data parallelization scheme of caffe .",
    "it divides a batch into @xmath61 even sub - batches , where n is the gpu count .",
    "each gpu has a model duplicate ; and a gpu fetches a sub - batch to compute a sub - gradient and a sub - loss by doing an independent forward and backward propagation .",
    "subsequently gpus reduce sub - gradients and sub - losses to a global gradient and loss that tie to the original batch .",
    "a barrier is placed after the reduce to ensure the mathematical rigorousness .",
    "then the master gpu updates the weight vector ; and broadcast the latest weight to others .",
    "we can also easily migrate the algorithm to work for distributed heterogeneous system by replacing the reduce and broadcast operations .",
    "caffe adopts the tree - reduction @xcite to implement the reduce and broadcast operation .",
    "this implementation needs @xmath87 communication steps , where @xmath61 is the computing nodes .",
    "the algorithm only works well with a few gpus ; however , future effort to implement efficient reduce and broadcast operations is necessary .",
    "in this section , we demonstrate the performance of isgd against sgd using several convolutional neural networks on a variety of datasets including mnist , cifar-10 and imagenet .",
    "first , we present isgd consistently outperforms sgd under various scenarios .",
    "second , we demonstrate the scalability of isgd at different batches .",
    "in particular , isgd gets alexnet trained to 56.5% top1 accuracy in 11.5 hours with 4 titan x gpu , 15.35% faster than the nvidia s claim .",
    ".the dataset and networks used in experiments . [ cols=\"^,^,<\",options=\"header \" , ]      we benchmark on 3 widely recognized image datasets : mnist @xcite , cifar-10 @xcite , and imagenet @xcite .",
    "mnist has 60000 handwritten digits ranging from 0 to 9 .",
    "cifar-10 has 60000 32@xmath332 rgb images categorized in 10 classes .",
    "imagenet , ilsvrc 2012 , has 1431167 256 @xmath3256 rgb images depicting 1000 object categories .",
    "we adopt lenet , caffe cifar-10 quick , and alexnet to train on mnist , cifar-10 , and imagenet respectively .",
    "the complexity of networks is proportional to the size of datasets .",
    "therefore , our benchmarks cover the small , middle , and large scale cnn training .",
    "table 1 presents network details for each datasets , the architecture of which is available in caffe .",
    "+      the experimental setups of isgd and sgd are ensured to be same .",
    "the learning rate for mnist and cifar are constant 0.01 and 0.001 during the training . while the learning rate for imagenet has 3 possibilities , lr = 0.015 when average loss ( al ) in @xmath88 $ ] ; lr = 0.0015 when al in @xmath89 ; and lr = 0.00015 when al in @xmath90 .",
    "the batch size is same for both isgd and sgd in the 3 test cases .",
    "particularly the batch size for imagenet is 1000 .",
    "since an iteration of isgd is inconsistent , we conduct testing every other 2 , 6 , 900 seconds ( only count training time , excluding test time ) on mnist , cifar and imagenet respectively . besides",
    ", the machine was exclusively owned by us during the benchmark to ensure the fairness .",
    "the bechmark results is the average of 3 runs to eliminate other random factors .",
    "therefore , this is a single factor experiment between isgd v.s sgd .",
    "isgd consistently outperforms sgd in all test cases .",
    "the complexity of recognition tasks on these datasets are mnist < cifar < imagenet . in the imagenet test case",
    ", isgd demonstrates superior convergence advantages to sgd .",
    "for example , sgd takes 23.75 hours to reach the 80.9% top 5 accuracy ( please note the reported highest top 1 and 5 accuracy of alexnet in caffe is 56.9% and 80.01% @xcite ) ; while isgd only takes 18.75 hours demonstrating 21.05% performance gain . in the cifar test case , the validation accuracy of isgd is consistently higher than sgd .",
    "the expected top accuracy of cifar - quick is 75% .",
    "after 392 seconds , the test accuracy of sgd is steadily above 75% .",
    "the number for isgd is 287 seconds demonstrating 26.79% performance gain .",
    "although isgd is slightly faster than sgd in the mnist test case , isgd tends to be more robust than sgd .",
    "particularly sgd has a dent around the 25th testing .",
    "isgd is more efficient than sgd . to substantiate this point",
    ", we use the training dataset to train and test .",
    "since the training set of imagenet 256 gb is too large to be tested , we conduct the experiments on mnist and cifar . both fig.[mnist_train ] and fig.[cifar_train ] demonstrate isgd yields a lower training error than sgd in a fixed time , which also explains the higher accuracy of isgd .",
    "the advantages of isgd arise from the inconsistent training , which spares more training effort on undertrained batches . whereas sgd treats a undertrained batch as same as those fully trained resulting in the poor load balancing in training .",
    "the batch size is the predominant factor for the isgd scalability .",
    "we claim that the optimal batch size is system dependent ; and it is neither too large nor too small .",
    "we conduct the experiments on the same machine used in the performance benchmark .",
    "figure [ scalability ] demonstrates the effect of batch size to the scalability on the datasets mnist , cifar and imagenet . in these experiments",
    ", we also empirically increase the learning rate along with the batch size .",
    "the figures reflect the following conclusions .",
    "first , a sufficient large batch is necessary to the multigpu training .",
    "the single gpu training only has @xmath91 ; while the multigpu training has the additional term @xmath92 for synchronization .",
    "the batch size of single gpu training is small to frequently conduct gradient updates . if keep using such a small batch size in multigpu training , @xmath92 and the poor device saturation may lead the worse performance than the single gpu training",
    "this is most obvious on fig.[imagenet_scal ] due to the size of alexnet .",
    "secondly , a unwieldy batch size also decreases the scalability . increasing the batch size offsets its benefit if @xmath91 gains faster than the benefit brought forth by the convergence rate .",
    "for example , assuming a model needs 20 iteration to converge at the batch size of 100 . for each iteration",
    ", it takes 1 second .",
    "now we increase the batch size to 200 ; and it takes 18 iterations to converge .",
    "however , the 100 batch case only takes 20 seconds while the 200 batch case needs 2 * 18 ( assuming the latency of an iteration is proportional to the batch size ) .",
    "fig.[mnist_scal ] , fig.[cifar_scal ] and fig.[imagenet_scal ] also demonstrate an extremely large batch needs more time to converge than the smaller bathes .",
    "therefore , our claim is valid .",
    "/@xmath93 . , width=201 ]      @xmath92 and @xmath91 are the metrics to measure the system performance .",
    "eq.[performance ] indicates an iteration consists of @xmath91 and @xmath92 .",
    "@xmath91 includes a forward and backward propagation and a weight update while @xmath92 includes a reduce and a broadcast .",
    "@xmath91 is contigent upon the network architecture , the batch size and the system processing power .",
    "@xmath92 depends on the weight vector and network latency . @xmath92 and",
    "@xmath91 enable us to independently evaluate the overhead of communication and computation , which provides useful information to the system occupancy .",
    "fig.[occupancy ] demonstrates increasing batch size improves the system occupancy .",
    "if the size of network is small , @xmath91 dominates .",
    "thereby the synchronization cost of caffe - quick ( 0.58 mb ) always stays low . enlarging the network also increases @xmath92 .",
    "it is undesired to stay a small batch as @xmath91 is small in terms of @xmath92 . at the batch size of 500",
    ", the synchronization cost of lenet ( 1.72 mb ) and alexnet(243.86 mb ) accounts for @xmath94 and @xmath95 total execution time ; while the number is 10% and 16% at batch size of 3000 .",
    "increasing the batch size improves @xmath91 to offset @xmath92 .",
    "the trend is more obvious on bigger networks .",
    "fig.[saturation ] demonstrates the batch size is also important to the saturation of computing units . in this experiment set ,",
    "we measure the average latency of processing 1000 images in mnist , cifar , and imagenet on 2 titan - x at different batch size .",
    "an image in mnist , cifar , and imagenet is the size of @xmath96 , @xmath97 and @xmath98 respectively .",
    "therefore the required images to fully saturate 4 gpus is @xmath99 .",
    "fig.[saturation ] indicates a gpu requires 500 , 300 and 150 batch size to achieve full saturation on mnist , cifar and imagenet . and the numbers for 2 gpus are 2500 , 600 , and 500",
    ". please note the latency includes both @xmath92 and @xmath91 .",
    "therefore the batch size for 2 gpus is higher than @xmath100 the single gpu case .",
    "these results substantiate an appropriate large batch size is necessary to fully saturate computing devices .",
    "in terms of algorithm , a variety of approches has been discussed to accelerate cnn training .",
    "batch normalization @xcite is a method to reduce the internal covariance shift within a network .",
    "this method has proved to be an effective approach to alleviate the changes in the distribution of network activations , which is an important factor to increase the convergence speed @xcite . as discussed in the section 3.4 ,",
    "reducing the gradient variance is critical to the convergence speed of sgd .",
    "peilin @xcite and chong @xcite adopt the method of importance sampling and control variate , widely used in monte carol simulation , to reduce the gradient variance respectively .",
    "rie @xcite also proposed an explicit variance reduction method for sgd referred to as svrg .",
    "both datasets and networks are growing bigger and bigger .",
    "this trend has initiated active research in parallelizing the dnn training on the distributed or multigpu system @xcite @xcite @xcite .",
    "tensor flow @xcite , malt @xcite , and petuum @xcite are popular distributed ml frameworks . in section 5.3 , we have presented nontrivial synchronization cost involved in training .",
    "these frameworks address the issue by asynchronous sgd ( asgd ) @xcite , which overlaps the gradient computation with the communication by relaxing the math of sgd .",
    "another approach to address the synchronization problem while ensuring the mathematic rigorousness is using a large batch size .",
    "however , an increase in batch size decreases the convergence rate of sgd .",
    "mu propose solving a conservatively regularized objective function on within each batch to counteract the decreasing convergence rate @xcite .",
    "isgd is fundamentally different from existing approaches by considering the learning dynamics on batches .",
    "we have demonstrated sampling bias , intrinsic images and fcpr sampling result in distintive learning speeds on batches . whereas sgd fails to consider these factors .",
    "isgd rebalances the training effort on batches with respect to their learning statuses .",
    "therefore it is more efficient than sgd .",
    "in this paper , we consider the unbiased treatment of batches involved in sgd as a problem .",
    "factors such as sampling bias , intrinsic image difference and fcpr sampling differentiate batches in training .",
    "such differences propagate iteration by iteration rendering different learning speeds on batches . whereas sgd fails to consider such learning variation by spending identical training effort on each batch .",
    "specifically it trains every batch once in an epoch .",
    "then we propose inconsistent sgd to rebalance the training effort according to the learning status of batches .",
    "isgd adopts techniques in stochastic process control to identify a undertrained batch in the process of training .",
    "if a batch is under trained , isgd solves a subproblem including a chase logic and a conservative constraint to accelerate the training on this particular batch without drastic model parameter changes .",
    "experiments on a variety dataset demonstrate isgd converges faster than sgd .",
    "gimpel , kevin , dipanjan das , and noah a. smith .",
    "`` distributed asynchronous online learning for natural language processing .",
    "'' proceedings of the fourteenth conference on computational natural language learning .",
    "association for computational linguistics , 2010 .",
    "donahue , jeff , et al .",
    "`` long - term recurrent convolutional networks for visual recognition and description . ''",
    "arxiv preprint arxiv:1411.4389 ( 2014 ) .",
    "sarwar , badrul m. , et al .",
    "`` recommender systems for large - scale e - commerce : scalable neighborhood formation using clustering . ''",
    "proceedings of the fifth international conference on computer and information technology .",
    "byrd , richard h. , et al .",
    "`` sample size selection in optimization methods for machine learning . '' mathematical programming 134.1 ( 2012 ) : 127 - 155 .",
    "love , m. `` probability theory .",
    "bottou , lon .",
    "`` online learning and stochastic approximations . '' on - line learning in neural networks 17.9 ( 1998 ) : 25",
    ". de boer , pieter - tjerk , et al .",
    "`` a tutorial on the cross - entropy method . ''",
    "annals of operations research 134.1 ( 2005 ) : 19 - 67 .",
    "zhou , bolei , et al .",
    "`` learning deep features for scene recognition using places database . ''",
    "advances in neural information processing systems . 2014 .",
    "deng , jia , et al .",
    "`` imagenet : a large - scale hierarchical image database . '' computer vision and pattern recognition , 2009 .",
    "cvpr 2009 .",
    "ieee conference on .",
    "ieee , 2009 .",
    "jia , yangqing , et al .",
    "`` caffe : convolutional architecture for fast feature embedding . ''",
    "proceedings of the acm international conference on multimedia .",
    "acm , 2014 .",
    "collobert , ronan , koray kavukcuoglu , and clment farabet .",
    "`` torch7 : a matlab - like environment for machine learning . ''",
    "biglearn , nips workshop .",
    "epfl - conf-192376 .",
    "sanders , peter , jochen speck , and jesper larsson trff .",
    "`` two - tree algorithms for full bandwidth broadcast , reduction and scan . ''",
    "parallel computing 35.12 ( 2009 ) : 581 - 594 .",
    "lecun , yann , et al .",
    "`` gradient - based learning applied to document recognition . ''",
    "proceedings of the ieee 86.11 ( 1998 ) : 2278 - 2324 .",
    "krizhevsky , alex , and geoffrey hinton .",
    "`` learning multiple layers of features from tiny images . ''",
    "dekel , ofer , et al .",
    "`` optimal distributed online prediction using mini - batches . '' the journal of machine learning research 13.1 ( 2012 ) : 165 - 202 .",
    "ioffe , sergey , and christian szegedy .",
    "`` batch normalization : accelerating deep network training by reducing internal covariate shift . ''",
    "proceedings of the 32nd international conference on machine learning . 2015 .",
    "orr , genevieve b. , and klaus - robert mller , eds .",
    "neural networks : tricks of the trade .",
    "springer , 2003 .",
    "zhao , peilin , and tong zhang .",
    "`` stochastic optimization with importance sampling for regularized loss minimization .",
    "'' proceedings of the 32nd international conference on machine learning ( icml-15 ) . 2015 .",
    "wang , chong , et al .",
    "`` variance reduction for stochastic gradient optimization .",
    "'' advances in neural information processing systems . 2013 .",
    "johnson , rie , and tong zhang .",
    "`` accelerating stochastic gradient descent using predictive variance reduction . ''",
    "advances in neural information processing systems . 2013 .",
    "wang , linnan , et al .",
    "`` blasx : a high performance level-3 blas library for heterogeneous multi - gpu computing . ''",
    "arxiv preprint arxiv:1510.05041 ( 2015 ) .",
    "wang , linnan , et al .",
    "`` large scale artificial neural network training using multi - gpus . '' supercomputing , ( 2015 ) .",
    "coates , adam , et al .",
    "`` deep learning with cots hpc systems . '' proceedings of the 30th international conference on machine learning .",
    "abadi , martn , et al .",
    "`` tensorflow : large - scale machine learning on heterogeneous systems , 2015 . ''",
    "software available from tensorflow .",
    "xing , eric p. , et al .",
    "`` petuum : a new platform for distributed machine learning on big data . '' big data , ieee transactions on 1.2 ( 2015 ) : 49 - 67 .",
    "li , hao , et al . `` malt : distributed data - parallelism for existing ml applications . '' proceedings of the tenth european conference on computer systems .",
    "acm , 2015 .",
    "dean , jeffrey , et al .",
    "`` large scale distributed deep networks .",
    "'' advances in neural information processing systems .",
    "li , mu , et al .",
    "`` efficient mini - batch training for stochastic optimization . ''",
    "proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining .",
    "acm , 2014 ."
  ],
  "abstract_text": [
    "<S> the growth of training datasets into enormity has fostered the development of deeper and wider convolutional neural networks ( cnn ) . </S>",
    "<S> this has imposed great computation challenges in training cnn at a large scale . </S>",
    "<S> sgd is the widely adopted method to train cnn . </S>",
    "<S> conceptually it approximates the population with a randomly sampled batch ; then it evenly trains batches by conducting a gradient update on every batch in an epoch . in this paper </S>",
    "<S> , we demonstrate sampling bias , intrinsic image difference and fixed cycle pseudo random sampling differentiate batches in training , which then affect learning speeds on them . </S>",
    "<S> because of this , the unbiased treatment of batches involved in sgd creates improper load balancing . to address this issue , we present inconsistent stochastic gradient descent ( isgd ) to dynamically vary training effort according to learning statuses on batches . </S>",
    "<S> specifically isgd leverages techniques in statistical process control to identify a undertrained batch . </S>",
    "<S> once a batch is undertrained , isgd solves a new subproblem , a chasing logic plus a conservative constraint , to accelerate the training on the batch while avoid drastic parameter changes . </S>",
    "<S> extensive experiments on a variety of datasets demonstrate isgd converges faster than sgd . in training </S>",
    "<S> alexnet , isgd is 21.05% faster than sgd to reach 56% top1 accuracy under the exactly same experiment setup </S>",
    "<S> . we also extend isgd to work on multigpu or heterogeneous distributed system based on data parallelism , enabling the batch size to be the key to scalability . </S>",
    "<S> then we present the study of isgd batch size to the learning rate , parallelism , synchronization cost , system saturation and scalability . </S>",
    "<S> we conclude the optimal isgd batch size is machine dependent . </S>",
    "<S> various experiments on a multigpu system validate our claim . </S>",
    "<S> in particular , isgd trains alexnet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4 nvidia titan x at the batch size of 1536 . </S>"
  ]
}