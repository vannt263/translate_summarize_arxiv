{
  "article_text": [
    "in our big data era , various networks generate massive amounts of streaming data .",
    "examples include wireless sensor networks , where a large number of inexpensive sensors cooperate to monitor , e.g. the environment @xcite , or data centers , where a group of servers collaboratively handles dynamic user requests @xcite .",
    "since a single node has limited computational resources , decentralized information processing is preferable as the networksize scales up @xcite . in this paper",
    ", we focus on a decentralized linear regression setup , and develop computation- and communication - efficient decentralized recursive least - squares ( d - rls ) algorithms .    the main tool we adopt to reduce computation and communication costs is data - adaptive censoring , which leverages the redundancy present especially in big data . upon receiving an observation",
    ", nodes determine whether it is informative or not .",
    "less informative observations are discarded , while messages among neighboring nodes are exchanged only when necessary .",
    "we propose three censoring - based ( c)d - rls algorithms that can achieve estimation accuracy comparable to d - rls without censoring , while significantly reducing the computation and communication overhead .",
    "the merits of rls algorithms in solving centralized linear regression problems are well recognized @xcite . when streaming observations that depend linearly on a set of unknown parameters become available , rls yileds the least - squares parameter estimates online .",
    "rls reduces the computational burden of finding a batch estimate per iteration , and can even allow for tracking time - varying parameters .",
    "the computational cost can be further reduced by data - adaptive censoring @xcite , where less informative data are discarded . on the other hand , decentralized versions of rls without censoring have been advocated to solve linear regression tasks over networks @xcite . in d - rls ,",
    "a node updates its estimate that is common to the entire network by fusing its local observations with the local estimates of its neighbors . as time evolves , all local estimates consent on the centralized rls solution .",
    "this paper builds on both @xcite and @xcite by developing censoring - based decentralized rls algorithms , thus catering to efficient online linear regression over large - scale networks .",
    "different from our in - network setting where operation is fully decentralized and nodes are only able to communicate with their neighbors , most of the existing distributed censoring algorithms apply to star topology networks that rely on a fusion center @xcite .",
    "their basic idea is that each node transmits data to the fusion center for further processing only when its local likelihood ratio exceeds a threshold @xcite ; see also @xcite where communication constraints are also taken into account .",
    "information fusion over fading channels is considered in @xcite .",
    "practical issues such as joint dependence of sensor decision rules , randomization of decision strategies and partially known distributions are reported in @xcite , while @xcite also explores quantization jointly with censoring .",
    "other than the star topology studied in the aforementioned works , @xcite investigates censoring for a tree structure .",
    "if a node s local likelihood ratio exceeds a threshold , its local data is sent to its parent node for fusion .",
    "a fully decentralized setting is considered in @xcite , where each node determines whether to transmit its local estimate to its neighbors by comparing the local estimate with the weighted average of its neighbors .",
    "nevertheless , @xcite aims at mitigating only the communication cost , while the present work also considers reduction of the computational cost across the network .",
    "furthermore , the censoring - based decentralized linear regression algorithm in @xcite deals with optimal full - complexity estimation when observations are partially known or corrupted .",
    "this is different from our context , where censoring is deliberately introduced to reduce computation and communication costs for decentralized linear regression .",
    "the present paper introduces three data - adaptive online censoring strategies for decentralized linear regression .",
    "the resultant cd - rls algorithms incur low computation and communication costs , and are thus attractive for large - scale network applications requiring decentralized solvers of linear regressions . unlike most related works that specifically target wireless sensor networks ( wsns ) , the proposed algorithms may be used in a broader context of decentralized linear regression using multiple computing platforms . of particular interest",
    "are cases where a regression dataset is not available at a single machine , but it is distributed over a network of computing agents that are interested in accurately estimating the regression coefficients in an efficient manner .    in section [ sec : algo ] , we formulate the decentralized online linear regression problem ( section [ sec : algo - problem ] ) , and recast the d - rls in @xcite into a new form ( section [ sec : d - rls ] ) that prompts the development of three censoring strategies ( section [ sec : c - d - rls ] ) .",
    "section [ sec : theoretical ] develops the first censoring strategy ( section [ sec : derivation ] ) , analyzes all three censoring strategies ( section [ sec : convergence ] ) , and discusses how to set the censoring thresholds ( section [ sec : threshold ] ) . numerical experiments in section [ sec : numerical ] demonstrate the effectiveness of the novel cd - rls algorithms .    _ notation .",
    "_ lower ( upper ) case boldface letters denote column vectors ( matrices ) .",
    "@xmath0 , @xmath1 , @xmath2 and @xmath3 $ ] stand for transpose , 2-norm , induced matrix 2-norm and expectation , respectively .",
    "symbols @xmath4 , @xmath5 and @xmath6 are used for the trace , minimum eigenvalue and maximal eigenvalue of matrix @xmath7 , respectively .",
    "kronecker product is denoted by @xmath8 and the uniform distribution over @xmath9 $ ] by @xmath10 , and the gaussian probability distribution function ( pdf ) with mean @xmath11 and variance @xmath12 by @xmath13 .",
    "the standardized gaussian pdf is @xmath14 , and its the associated complementary cumulative distribution function is represented by @xmath15 .",
    "this section outlines the online linear regression setup over networks , and takes a fresh look at the d - rls algorithm .",
    "three strategies are then developed using data - adaptive censoring to reduce the computation and communication costs of d - rls .",
    "consider a bidirectionally connected network with @xmath16 nodes , described by a graph @xmath17 , where @xmath18 is the set of nodes with cardinality @xmath19 , and @xmath20 denotes the set of edges .",
    "each node @xmath21 only communicates with its one - hop neighbors , collected in the set @xmath22 .",
    "the decentralized network is deployed to estimate a real vector @xmath23 . per time",
    "slot @xmath24 , node @xmath21 receives a real scalar observation @xmath25 involving the wanted @xmath26 with a regression row @xmath27 , where @xmath0 stands for transposition , so that @xmath28 , with @xmath29 .",
    "our goal is to devise efficient decentralized online algorithms to solve the following exponentially - weighted least - squares ( ewls ) problem @xmath30 ^ 2\\end{aligned}\\ ] ] where @xmath31 is the ewls estimate at slot @xmath32 , and @xmath33 $ ] is a forgetting factor that de - emphasizes the importance of past measurements , and thus enables tracking of a non - stationary process . when @xmath34 , boils down to a standard decentralized online least - squares estimate .",
    "the d - rls algorithm of @xcite solves as follows . per time",
    "slot @xmath32 , node @xmath21 receives @xmath25 and @xmath27 and uses them to update the per - node inverse @xmath35 covariance matrix as @xmath36 along with the per - node @xmath37 cross - covariance vector as @xmath38 using @xmath39 and @xmath40 , node @xmath21 then updates its local parameter estimate using @xmath41 \\label{eq : d - rls - s}\\end{aligned}\\ ] ] where @xmath42 denotes the lagrange multiplier of node @xmath21 corresponding to its neighbor @xmath43 at slot @xmath44 , that captures the accumulated differences of neighboring estimates , recursively obtained as ( @xmath45 is a step - size ) @xmath46 .",
    "\\label{eq : d - rls - v}\\end{aligned}\\ ] ] next , we develop an equivalent novel form of d - rls recursions  that is convenient for our incorporation of data - adaptive censoring",
    ". detailed derivation of the equivalence can be found in appendix [ sec : eq - drls ] . the inverse covariance matrix is updated as in . however , the update of @xmath47 in is replaced by @xmath48\\nonumber\\\\ & -\\rho{\\mathbf{\\phi}}_j^{-1}(t){\\bm{\\delta}}_j(t-1)\\label{eq : new - d - rls - s}\\end{aligned}\\ ] ] where @xmath49 stands for a lagrange multiplier conveying network - wide information that is updated as @xmath50 \\nonumber \\\\ & - \\lambda \\sum_{j'\\in{\\mathcal{n}}_j}[{{\\mathbf{s}}}_j(t-1)-{{\\mathbf{s}}}_{j'}(t-1)].\\label{eq : new - d - rls - delta}\\end{aligned}\\ ] ] observe that @xmath49 stores the weighted sum of differences between the local estimate of node @xmath21 , and all estimates of its neighbors . interestingly ,",
    "if the network is disconnected and the nodes are isolated , then @xmath51 so long as @xmath52 , and the update of @xmath47 in basically boils down to the centralized rls one @xcite .",
    "that is , the current estimate is modified from its previous value using the prediction error @xmath53 , which is known as the incoming data _",
    "innovation_. if on the other hand the network is connected , nodes can leverage estimates of their neighbors ( captured by @xmath49 ) , which provide new information from the network other than its own observations @xmath54 .",
    "the term @xmath55 can be viewed as a laplacian smoothing regularizer , which encourages all nodes of the graph to reach consensus on their estimates .    * remark 1*. in d - rls , incurs computational complexity @xmath56 , since calculating the products @xmath57 and @xmath58 requires @xmath56 multiplications . similarly , incurs _",
    "computational _ complexity @xmath56 , that is dominated by the matrix - vector multiplications @xmath59 and @xmath60 .",
    "the cost of carrying out is relatively minor . regarding _",
    "cost per slot @xmath32 , node @xmath21 needs to transmit its local estimate @xmath47 to its neighbors and receive estimates @xmath61 from all neighbors @xmath62 .",
    "the computational burden of d - rls recursions  is comparable to that of , and , with the cost of being the same as what requires .",
    "meanwhile , the original form requires neighboring nodes @xmath21 and @xmath43 to exchange @xmath63 and @xmath64 in addition to @xmath47 and @xmath61 , which doubles the communication cost relative to and .",
    "the d - rls algorithm has well documented merits for decentralized online linear regression @xcite . however , its computation and communication costs per iteration are fixed , regardless of whether observations and/or the estimates from neighboring nodes are informative or not .",
    "this fact motivates our idea of permeating benefits of data - adaptive censoring to _ decentralized _ rls , through three novel censoring - based ( c)d - rls strategies .",
    "they are different from the rls algorithms in @xcite , where the focus is on _ centralized _ online linear regression .    our first censoring strategy ( cd - rls-1 )",
    "can be intuitively motivated as follows .",
    "if a given datum @xmath65 is not informative enough , we do not have to use it since its contribution to the local estimate of node @xmath21 , as well as to those of all network nodes , is limited . with @xmath66 specifying proper thresholds to be discussed later , this intuition can be realized using a censoring indicator variable @xmath67 if the absolute value of the innovation is less than @xmath68 , then @xmath65 is censored ; otherwise @xmath65 is used",
    ". section [ sec : threshold ] will provide rules for selecting the threshold @xmath69 along with the local noise variance @xmath70 , whose computations are lightweight .",
    "if data censoring is in effect , we simply throw away the current datum by letting @xmath71 in , to obtain @xmath72 likewise , letting @xmath73 and @xmath74 in , yields @xmath75 cd - rls-1 is summarized in algorithm 1 .",
    "if censoring is in effect , computation cost per node and per slot is a fraction @xmath76 of the d - rls in and without censoring . to recognize why",
    ", observe that the scalar - matrix multiplication @xmath77 in is not necessary as the update of @xmath39 can be merged to wherever it is needed , e.g. , in and the next slot . in addition , carrying out the @xmath56 multiplications to obtain @xmath59 is no longer necessary , while the @xmath56 multiplications required to obtain @xmath60 remain the same .",
    "the first censoring strategy still requires nodes to communicate with neighbors per time slot ; hence , the communication cost remains the same .",
    "reducing this communication cost , motivates our second censoring strategy ( cd - rls-2 ) , where each node does not perform extra computations relative to cd - rls1 , but only receives neighboring estimates if its current datum is censored .",
    "the intuition behind this strategy is that if a datum is censored , then very likely the current local estimate is sufficiently accurate , and the node does not need to account for estimates from its neighbors .",
    "estimates from neighbors , are only stored for future usage . likewise , @xmath78 neighbors do not need node @xmath21 s current estimate either , because they have already received a very similar estimate .",
    "cd - rls-2 is summarized in algorithm 2 .",
    "the third censoring strategy ( cd - rls-3 ) given by algorithm 3 is more aggressive than the second one .",
    "if a node has its datum censored at a certain slot , then it neither transmits to nor receives from its neighbors , and in that sense it remains `` isolated '' from the rest of the network in this slot .",
    "apparently , we should not allow any node to be forever isolated .",
    "to this end , we can force each node to receive the local estimate from any of its neighbors at least once every @xmath79 slots , which upper bounds the delay of information exchange to @xmath79 .",
    "interestingly , the ensuing section will prove convergence of all three strategies to the optimal argument in the mean - square deviation sense under mild conditions .",
    "initialize @xmath80 , @xmath81 and @xmath82 all @xmath83 : update @xmath84 using update @xmath47 using update @xmath84 using update @xmath47 using transmit @xmath47 to and receive @xmath61 from all @xmath85 compute @xmath49 using    initialize @xmath80 , @xmath81 and @xmath82 all @xmath83 : receives @xmath61 from all @xmath85 set @xmath86 as recently received ones from all @xmath85 update @xmath84 using update @xmath47 using transmit @xmath47 to and receive @xmath61 from all @xmath85 compute @xmath49 using    initialize @xmath80 , @xmath81 and @xmath82 all @xmath83 : stay idle set @xmath86 as recently received ones from all @xmath85 update @xmath84 using update @xmath47 using transmit @xmath47 to and receive @xmath61 from all @xmath85 compute @xmath49 using receive @xmath61",
    "this section starts with a criterion - based development of cd - rls-1 .",
    "convergence analysis of all three censoring strategies will follow , before developing practical means of setting the censoring threshold @xmath87 .",
    "consider the following truncated quadratic cost that is similar to the one used in the censoring - based but centralized rls @xcite @xmath88 ^ 2-\\frac{1}{2}\\tau^2\\sigma_j(t)^2,&|x_j(t)-{\\mathbf{h}}_j^t(t){{\\mathbf{s}}}| > \\tau\\sigma_j(t ) .",
    "\\nonumber \\end{cases}\\end{aligned}\\ ] ] using to replace the quadratic loss @xmath89 ^ 2 $ ] in , our cd - rls-1 criterion is @xmath90 to solve in a decentralized manner , we introduce a local estimate @xmath91 per node @xmath21 , along with auxiliary vectors @xmath92 and @xmath93 per edge @xmath94 . by constraining all local estimates of neighbors to consent , we arrive at the following equivalent separable convex program per slot @xmath32 @xmath95 next , we employ alternating minimization and the stochastic newton iteration to derive our first censoring - based solver of . to this end , consider the lagrangian of that is given by @xmath96\\end{aligned}\\ ] ] where @xmath97 and @xmath98 are primal variables , while @xmath99 and @xmath100 are dual variables .",
    "consider also the augmented lagrangian of , namely @xmath101\\end{aligned}\\ ] ] where @xmath102 is a positive regularization scale .",
    "note that the constraints on @xmath103 are not dualized , but they are collected in the set @xmath104 .    to minimize per slot @xmath105",
    ", we rely on alternating minimization @xcite in an online manner , which entails an iterative procedure consisting of three steps .    * [ s1 ] local estimate updates : * @xmath106    * [ s2 ] auxiliary variable updates : * @xmath107    * [ s3 ] multiplier updates : * @xmath108\\\\ { { \\mathbf{u}}}_j^{j'}(t ) & = { { \\mathbf{u}}}_j^{j'}(t-1)+\\rho \\big [ { { \\mathbf{s}}}_{j'}(t)-{\\tilde{\\mathbf{z}}}_j^{j'}(t ) \\big].\\end{aligned}\\ ] ]    observe that [ s2 ] is a linearly constrained quadratic program , for which if @xmath109 , we always have @xmath110 therefore , the initial values of @xmath111 and @xmath112 in [ s3 ] are selected to satisfy @xmath113 ( the simplest choice is @xmath114 ) .",
    "it then holds for @xmath115 that @xmath116 using the latter to eliminate @xmath112 in [ s3 ] , we obtain @xmath117 \\nonumber \\\\                & = { { \\mathbf{v}}}_j^{j'}(t-1 ) + \\frac{\\rho}{2 } \\big[{{\\mathbf{s}}}_j(t)-{{\\mathbf{s}}}_{j'}(t)\\big]\\end{aligned}\\ ] ] where the first equality comes from subtracting the two lines in [ s3 ] , and the second equality is due to @xmath118 . the auxiliary variables @xmath93 and @xmath92 can be also eliminated .",
    "when @xmath111 is initialized by @xmath119 , summing up both sides of from @xmath120 to @xmath121 , we arrive , after telescopic cancellation , at @xmath122.\\end{aligned}\\ ] ] moving on to [ s1 ] , observe that it can be split into @xmath16 per - node subproblems @xmath123^t{{\\mathbf{s}}}_j.\\end{aligned}\\ ] ] before solving with the stochastic newton iteration @xcite , eliminate @xmath111 using to obtain @xmath124^t{{\\mathbf{s}}}_j\\end{aligned}\\ ] ] which after manipulating the double sum yields @xmath125^t{{\\mathbf{s}}}_j.\\end{aligned}\\ ] ] if the update in is initialized with @xmath52 , summing up both sides from @xmath126 to @xmath127 , we find after telescopic cancellation @xmath128.\\end{aligned}\\ ] ] thus , optimization of @xmath47 reduces to @xmath129 where the instantaneous cost per slot @xmath32 is @xmath130 the stochastic gradient of the latter is given by @xmath131 + \\rho { \\bm{\\delta}}_j(t-1).\\end{aligned}\\ ] ] in the stochastic newton method , the hessian matrix is given by @xmath132=e[c_j(t){\\mathbf{h}}_j(t){\\mathbf{h}}_j^t(t)]\\ ] ] where the second equality comes from and .",
    "a reasonable approximation of the expectation is provided by sample averaging .",
    "however , presence of @xmath133 affects attenuation of regressors , which leads to @xmath134 applying the matrix inversion lemma , we obtain @xmath135 \\nonumber\\end{aligned}\\ ] ] and after adopting a diminishing step size @xmath136 , the stochastic newton update becomes @xmath137 for rational convenience , let @xmath138 , and rewrite as ( cf . ) @xmath139 substituting @xmath140 and @xmath39 into the stochastic netwon iteration yields ( cf . )",
    "@xmath141\\\\ & -\\rho{\\mathbf{\\phi}}_j^{-1}(t){\\bm{\\delta}}_j(t-1)\\end{aligned}\\ ] ] which completes the development of cd - rls-1 .      here",
    "we establish convergence of all three novel strategies for @xmath142 . with @xmath143",
    ", the ewls estimator can even adapt to time - varying parameter vectors , but analyzing its tracking performance goes beyond the scope of this paper . for the time - invariant case ( @xmath34 )",
    ", we will rely on the following assumption .",
    "( * as1 * ) _ observations obey the linear model @xmath144 , where @xmath145 is independent across @xmath21 and @xmath32 .",
    "rows @xmath27 are uniformly bounded and independent of @xmath146 .",
    "covariance matrices @xmath147 \\succ\\mathbf{0}_{p\\times p}$ ] are constant and positive definite .",
    "process @xmath148 is ergodic , while @xmath149 and @xmath150 are independent .",
    "_    we are interested in the global mean - square deviation ( gmsd ) metric @xcite , defined as @xmath151.\\end{aligned}\\ ] ] under ( as1 ) , convergence of cd - rls-1 and cd - rls-2 is asserted as follows ; see appendix [ sec : proof-1 ] for the proof .",
    "[ thm : convergence ] for cd - rls-1 and cd - rls-2 algorithms 1 and 2 , set @xmath152 and @xmath153 per node @xmath21 .",
    "let @xmath154 , and suppose @xmath155 for cd - rls-1 and correspondingly @xmath156 for cd - rls-2 , while @xmath157 is the network laplacian and the constant @xmath158 depends on @xmath159 , and the upper bound of @xmath160 . under ( as1 ) , there exists @xmath161 for which it holds for @xmath162 that @xmath163 \\nonumber",
    "\\\\ \\leq & \\sum_{j=1}^{j}\\frac{\\gamma^{-1}||{{\\mathbf{s}}}_j(0)-{{\\mathbf{s}}}_0||^2 + \\gamma t_0\\sigma_j^2\\mathrm{tr}({\\mathbf{r}_{h_j}})}{2q(\\tau)\\mu t}\\nonumber\\\\ + &   \\frac{\\gamma \\sigma_j^2\\lambda_{\\max}({\\mathbf{r}_{h_j}}^{-1})\\mathrm{tr}({\\mathbf{r}_{h_j}})\\ln(t)}{4q^2(\\tau)\\mu t}. \\label{eq : main}\\end{aligned}\\ ] ]    theorem [ thm : convergence ] establishes that the gmsd in converges to zero at a rate @xmath164 .",
    "the constant of the convergence rate is related to @xmath165 through @xmath166 , @xmath167 and @xmath11 ; the noise covariance @xmath168 , and the threshold @xmath69 through @xmath169 .",
    "theorem [ thm : convergence ] also indicates the impact of the initial states ( determined by @xmath170 and @xmath171 ) , which disappears at a faster rate of @xmath172 . to guarantee convergence ,",
    "the step size @xmath102 must be small enough .",
    "the proof for cd - rls-3 is more challenging . because a node does not receive any information from its neighbors when",
    "censoring is in effect , it has to rely on outdated neighboring estimates when the incoming datum is not censored .",
    "this delay in percolating information may cause computational instability .",
    "for this reason , we will impose an additional constraint to guarantee that all local estimates do not grow unbounded . in practice , this can be realized by truncating local estimates when they exceed a certain threshold .",
    "( * as2 * ) _ local estimates @xmath173 are uniformly bounded @xmath174 .",
    "_    convergence of cd - rls-3 is then asserted as follows .",
    "similar to cd - rls-1 and cd - rls-2 , the gmsd of cd - rls-3 converges to zero with rate @xmath164 , as stated in the following theorem . the proof is given in appendix [ sec : proof-2 ] .",
    "[ thm : convergence3 ] for cd - rls-3 given by algorithms 3 , set @xmath152 and @xmath175 per node @xmath21 . under ( as1 ) and ( as2 ) with @xmath176 as in theorem [ thm : convergence ] , there exists @xmath177 for which it holds @xmath178 , that @xmath163 \\leq \\frac{a+b\\ln(t)}{t } \\label{eq : extra}\\end{aligned}\\ ] ] where @xmath179 and @xmath180 are positive constants that depend on the upper bounds of @xmath160 and @xmath47 , parameters @xmath102 and @xmath69 , the covariance @xmath181 , the laplacian matrix @xmath157 , and @xmath182 .      the threshold @xmath69 influences considerably the performance of all cd - rls algorithms .",
    "its value trades off estimation accuracy for computation and communication overhead .",
    "we provide a simple criterion for setting @xmath69 using the average censoring ratio @xmath183 , which is defined as the number of censored data over the total number of data @xcite .",
    "the goal is to choose @xmath69 so that the actual censoring ratio approaches @xmath183 as @xmath32 goes to infinity  since we are dealing with streaming big data , such an asymptotic property is certainly relevant .",
    "when @xmath32 is large enough , @xmath184 is very close to @xmath26 ; thus , the innovation @xmath185 . as a consequence , @xmath186 , where the last equality holds because @xmath187 .",
    "therefore , @xmath188\\approx 1 - 2q(\\tau)$ ] , which implies that @xmath189    if the variances @xmath190 were known , one could simply choose @xmath152 .",
    "however , @xmath191 in practice is often unknown . in this case",
    ", we consider the running average @xmath192 ^ 2 = ( t-1)\\sigma_j^2(t)/t+[x_j(t+1)-{\\mathbf{h}}_j^t(t+1){{\\mathbf{s}}}_0]^2/t$ ] , which suggests the recursive variance estimate @xmath193 ^ 2/t\\;.\\ ] ]",
    "this section provides numerical results to validate the effectiveness of our novel censoring strategies .",
    "we simulate a network of @xmath194 nodes , which are uniformly randomly deployed over a @xmath195 square .",
    "two nodes within communication range @xmath196 are deemed as being neighbors .",
    "the resultant network topology is depicted in fig .",
    "[ eps : network ] .",
    "we compare five algorithms : the centralized adaptive censoring ( ac)-rls that runs in every node independently , d - rls without censoring @xcite , and the three censoring - based d - rls algorithms , namely cd - rls-1 , cd - rls-2 and cd - rls-3 .",
    "all algorithms are evaluated on two data sets , one synthetic and one real .",
    "the empirical gmsd is used as performance metric .    for the synthetic data set",
    ", the unknown @xmath26 is @xmath197-dimensional with @xmath198 .",
    "the setting is the one in @xcite , where wsn - based decentralized power spectrum estimation is sought for a signal modeled as an autoregressive process . in this context , consider an auxiliary sequence @xmath199 that evolves according to @xmath200 . starting from @xmath199 ,",
    "the row @xmath27 is formed by taking the next @xmath197 observations , namely @xmath201 $ ] .",
    "parameters are selected as @xmath202 , @xmath203 , and also uniformly distributed driving white noise @xmath204 with @xmath205 .",
    "observation of node @xmath21 is subject to additive white gaussian noise , with covariance @xmath206 , where @xmath207 .",
    "the true signal vector is @xmath208 , for which @xmath34 is set for all algorithms . for d - rls , cd - rls-1 ,",
    "cd - rls-2 and cd - rls-3 , the step size @xmath209 and @xmath210 where @xmath211 , leading to fastest convergence of d - rls . regarding the four censoring - based algorithms ac - rls , cd - rls-1 , cd - rls-2 and cd - rls-3 , we set the average censoring ratio to @xmath212 , which is approached using @xmath213 .",
    "the variances @xmath168 are estimated in an online manner as described in section [ sec : threshold ] .",
    "ac - rls uses @xmath210 , where @xmath214 leads to the fastest convergence . for all curves obtained by running the algorithms ,",
    "the ensemble averages are approximated via sample averaging over 100 monte carlo runs .",
    "[ eps : iteration ] depicts the gmsd versus the number of iterations .",
    "not surprisingly , since d - rls does not censor data , its convergence rate with respect to the number of iterations is the fastest . among the three proposed cd - rls algorithms , cd - rls-2 and cd - rls-3 are slower than cd - rls-1 , because the former two incur smaller communication cost than the latter .",
    "though cd - rls-3 adopts a more aggressive censoring strategy than cd - rls-2 , its convergence does not degrade as confirmed by fig .",
    "[ eps : iteration ] .",
    "ac - rls is the slowest among all , because it is run at all nodes independently , without sharing information over the network .",
    "the merits of censoring are further appreciated when one considers computational costs .",
    "recall that the target average censoring ratio is @xmath212 , meaning that @xmath215 of the data are discarded ( actual values are @xmath216 for ac - rls , @xmath217 for cd - rls-1 , @xmath218 for cd - rls-2 , and @xmath219 for cd - rls-3 , averaged over 100 runs ) .",
    "as confirmed by fig .",
    "[ eps : computation ] , the three cd - rls algorithms consume considerably less computational resources relative to d - rls that does not censor data .",
    "indeed , whenever a datum is censored , cd - rls-1 only requires @xmath220 of the computations relative to d - rls , while cd - rls-2 and cd - rls-3 incur minimal computational overhead .",
    "although ac - rls is the most computationally efficient algorithm at the beginning , absence of collaboration undermines its performance in steady state .    regarding the amount of data exchanged to communicate local estimates in a unicast mode ,",
    "cd - rls-1 is the worst because nodes need to transmit their local estimate to neighbors , no matter whether local data are censored or not .",
    "[ eps : transmission ] corroborates that cd - rls-2 and cd - rls-3 show significant improvement over d - rls , demonstrating their potential for reducing both communication and computation costs in solving decentralized linear regression problems over large - scale networks .",
    "next , we vary @xmath221 and evaluate its impact on gmsd , as shown in fig . [",
    "eps : msd - cr ] . when @xmath221 is close to @xmath222 , meaning about @xmath223 of the data is censored ,",
    "the three proposed cd - rls algorithms are still able to reach gmsd of @xmath224 , which is the limit of d - rls without censoring . among the three algorithms , cd - rls-1 exhibits the best gmsd curve , but its computation and communication costs are the highest .",
    "ac - rls does not perform well especially for low censoring ratios due to the lack of network - wide collaboration .",
    "cd - rls-2 and cd - rls-3 perform comparably in this experiment .",
    "the effectiveness of the novel censoring - based strategies is further assessed on a real data set of protein tertiary structures @xcite .",
    "the premise here is that a given dataset is not available at a single location , but it is distributed over a network whose nodes are interested in obtaining accurate regression coefficients while suppressing the communication and computational overhead .",
    "again , the graph in fig . [ eps : network ] is used to model the network of regression - performing agents .",
    "the number of control variables is @xmath225 .",
    "the first @xmath226 ( out of @xmath227 ) observations are normalized and divided evenly into @xmath194 parts , one per node . for cd - rls-1 , cd - rls-2 and cd - rls-3 , we set @xmath228 and @xmath229 , while for ac - rls we choose @xmath230 .",
    "the ground truth vector @xmath26 is estimated by solving a batch least - squares problem on the entire data set .",
    "similar to what we deduced from fig .",
    "[ eps : msd - cr ] in the synthetic data set , the novel cd - rls algorithms outperform ac - rls in terms of gmsd , as one varies the average censoring ratio from @xmath231 to nearly @xmath232 in fig .",
    "[ eps : msd - real ] .",
    "this paper introduced three data - adaptive censoring strategies that significantly reduce the computation and communication costs of the rls algorithm over large - scale networks .",
    "the basic idea behind these strategies is to avoid inefficient computation and communication when the local observations and/or the neighboring messages are not informative .",
    "we proved convergence of the resulting algorithms in the mean - square deviation sense .",
    "numerical experiments validated the merits of the novel schemes .",
    "the notion of identifying and discarding less informative observations can be widely used in various large - scale online machine learning tasks including nonlinear regression , matrix completion , clustering and classification , to name a few .",
    "these constitute our future research directions .",
    "here we prove that d - rls recursions - are equivalent to , and .",
    "it follows from that @xmath233 \\nonumber \\\\ & -\\lambda \\big[{\\bm{\\psi}}_j(t-1)-\\frac{1}{2}\\sum_{j'\\in { \\mathcal{n}}_j}({{\\mathbf{v}}}_j^{j'}(t-2)-{{\\mathbf{v}}}_{j'}^{j}(t-2 ) ) \\big ] .",
    "\\nonumber\\end{aligned}\\ ] ] applying the matrix inversion lemma to yields @xmath234 substituting @xmath235 from and @xmath236 from into , leads to @xmath237 = { \\mathbf{h}}_j(t)\\big[x_j(t)-{\\mathbf{h}}_j^t(t){{\\mathbf{s}}}_j(t-1 ) \\big ] \\nonumber \\\\ & \\hspace{6em } - \\frac{1}{2}\\sum_{j'\\in { \\mathcal{n}}_j}({{\\mathbf{v}}}_j^{j'}(t-1 ) - \\lambda { { \\mathbf{v}}}_j^{j'}(t-2 ) ) \\nonumber \\\\ & \\hspace{6em } + \\frac{1}{2}\\sum_{j'\\in { \\mathcal{n}}_j}({{\\mathbf{v}}}_{j'}^{j}(t-1 ) - \\lambda{{\\mathbf{v}}}_{j'}^{j}(t-2)).\\end{aligned}\\ ] ] next , we will show that if @xmath238 is defined as @xmath239 then its update is exactly .",
    "this can be done by taking the difference between slots @xmath32 and @xmath44 for , and substituting the update of @xmath111 in . due to",
    ", it follows that is equivalent to @xmath240 = { \\mathbf{h}}_j(t)\\big[x_j(t)-{\\mathbf{h}}_j^t(t){{\\mathbf{s}}}_j(t-1 ) \\big ] \\nonumber \\\\ & \\hspace{10.3em } - \\rho { \\bm{\\delta}}(t-1).\\end{aligned}\\ ] ] left multiplying with @xmath39 , yields the update of @xmath91 in , and completes the proof .",
    "starting with cd - rls-1 , the proof proceeds in five stages .    * stage 1 .",
    "* we first investigate the spectral properties of @xmath241 when @xmath32 is sufficiently large . letting @xmath34 and applying the matrix inversion lemma to the censoring form , we have @xmath242 summing up from @xmath120 to @xmath121 and using the telescopic cancellation , yields @xmath243 thanks to the strong law of large numbers",
    ", @xmath244 converges to @xmath245 $ ] almost surely as @xmath246 . clearly , @xmath245",
    "\\preceq e[{\\mathbf{h}}_j(t){\\mathbf{h}}_j^t(t ) ] = { \\mathbf{r}_{h_j}}$ ] . on the other hand , given @xmath160 , @xmath247 and @xmath248 defined in",
    ", the probability of @xmath249 is @xmath250}^{\\tau+\\sigma_j^{-1}[{\\mathbf{h}}_j^t(t)({{\\mathbf{s}}}_j(t-1)-{{\\mathbf{s}}}_0)]}\\phi(x)dx \\nonumber \\\\ \\geq & 1-\\int_{-\\tau}^{\\tau}\\phi(x)dx = 2q(\\tau).\\end{aligned}\\ ] ] consequently , it holds that @xmath251 \\\\ = & e\\big[{\\mathbf{h}}_j(t){\\mathbf{h}}_j^t(t)e[c_j(t)|{\\mathbf{h}}_j(t),{{\\mathbf{s}}}_j(t-1)]\\big]\\\\ = & e[{\\mathbf{h}}_j(t){\\mathbf{h}}_j^t(t ) \\pr(c_j(t)=1|{\\mathbf{h}}_j(t),{{\\mathbf{s}}}_j(t-1))]\\\\ \\succeq & 2q(\\tau ) e[{\\mathbf{h}}_j(t){\\mathbf{h}}_j^t(t ) ] = 2q(\\tau){\\mathbf{r}_{h_j}}.\\end{aligned}\\ ] ] thus , there exists @xmath252,for which it holds @xmath178 that @xmath253 as @xmath32 grows , the eigenvalues of @xmath241 are thus within the interval @xmath254 $ ] . consequently , the eigenvalues of @xmath255 are within the interval @xmath256 $ ] .",
    "* stage 2 .",
    "* rewrite the update of @xmath91 as @xmath257 \\\\ & -\\rho{\\mathbf{\\phi}}_j^{-1}(t){\\bm{\\delta}}_j(t-1).\\end{aligned}\\ ] ] note also that for @xmath34 , the update of @xmath258 is equivalent to ( cf . )",
    "@xmath259.\\end{aligned}\\ ] ] letting @xmath260 , the estimation error obeys the recursion @xmath261 \\\\ &",
    "-\\rho{\\mathbf{\\phi}}_j^{-1}(t)\\sum_{j'\\in{\\mathcal{n}}_j } \\big [ { \\mathbf{e}}_j(t-1)-{\\mathbf{e}}_{j'}(t-1 ) \\big].\\end{aligned}\\ ] ] substituting @xmath144 to eliminate @xmath247 , we obtain @xmath262.\\end{aligned}\\ ] ] left multiplying with @xmath241 yields @xmath263 \\nonumber \\\\",
    "= & { \\mathbf{\\phi}}_j(t-1){\\mathbf{e}}_j(t-1 )   \\nonumber\\\\    + & c_j(t){\\mathbf{h}}_j(t)\\epsilon_j(t)-\\rho\\sum_{j'\\in{\\mathcal{n}}_j } \\big [ { \\mathbf{e}}_j(t-1)-{\\mathbf{e}}_{j'}(t-1 )    \\big ] . \\nonumber\\end{aligned}\\ ] ] our convergence analysis result will rely on a matrix form of that accounts for all nodes @xmath21 . define vectors @xmath264^t",
    "\\in \\mathbb{r}^{jp}$ ] , @xmath265^t \\in \\mathbb{r}^j$ ] , as well as diagonal matrices @xmath266 , @xmath267 , and @xmath268 . then can be written in matrix form as @xmath269{\\mathbf{e}}(t-1)+{\\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t)\\end{aligned}\\ ] ] which after left multiplication with @xmath270 yields @xmath271{\\mathbf{e}}(t-1)\\nonumber \\\\ + & { \\mathbf{\\phi}}(t)^{-\\frac{1}{2}}{\\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t).\\end{aligned}\\ ] ] from , we have ( @xmath8 denotes kronecker product ) @xmath272 \\nonumber\\\\   = &   e[{\\mathbf{e}}^t(t-1)({\\mathbf{\\phi}}(t-1)-\\rho{\\mathbf{l}}\\otimes\\mathbf{i}_p)^t{\\mathbf{\\phi}}^{-1}(t)\\nonumber\\\\ & \\hspace{1em } \\times ( { \\mathbf{\\phi}}(t-1)-\\rho{\\mathbf{l}}\\otimes\\mathbf{i}_p){\\mathbf{e}}(t-1)]\\nonumber\\\\ + & 2e[{\\mathbf{e}}^t(t-1)({\\mathbf{\\phi}}(t-1)-\\rho{\\mathbf{l}}\\otimes\\mathbf{i}_p)^t{\\mathbf{\\phi}}^{-1}(t ) { \\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t)]\\nonumber\\\\ + & e[{\\bm{\\epsilon}}^t(t){\\mathbf{c}}^t(t){\\mathbf{h}}^t(t){\\mathbf{\\phi}}^{-1}(t){\\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t)].\\end{aligned}\\ ] ] since @xmath273 and @xmath274 are independent under ( as1 ) , the second term on the right hand side is zero ; hence , @xmath275",
    "\\nonumber\\\\ = & e[{\\mathbf{e}}^t(t-1)({\\mathbf{\\phi}}(t-1)-\\rho{\\mathbf{l}}\\otimes\\mathbf{i}_p)^t{\\mathbf{\\phi}}^{-1}(t)\\nonumber\\\\   \\times & ( { \\mathbf{\\phi}}(t-1)-\\rho{\\mathbf{l}}\\otimes\\mathbf{i}_p){\\mathbf{e}}(t-1)]\\nonumber\\\\   + &   e[{\\bm{\\epsilon}}^t(t){\\mathbf{c}}^t(t){\\mathbf{h}}^t(t){\\mathbf{\\phi}}^{-1}(t){\\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t)].\\end{aligned}\\ ] ] * stage 3 . * consider the first term on the right hand side of . since @xmath157 is positive semi - definite , we can find a matrix @xmath276 such that @xmath277 . by the matrix inversion lemma",
    ", it holds that @xmath278 for @xmath142 , it follows from that @xmath279 . since @xmath280",
    ", it holds that @xmath281 for all @xmath282 , and consequently @xmath283 if @xmath284 , then for all @xmath285 it follows that @xmath286 this implies that the second term of is positive definite .",
    "thus , we have @xmath287 and hence , the first term on the right hand side of is bounded by @xmath288\\nonumber\\\\ \\leq & e[{\\mathbf{e}}^t(t-1)({\\mathbf{\\phi}}(t-1)-\\rho{\\mathbf{l}}\\otimes\\mathbf{i}_p)^t { \\mathbf{e}}(t-1)]\\nonumber\\\\ \\leq & e[{\\mathbf{e}}^t(t-1 ) { \\mathbf{\\phi}}(t-1)^t { \\mathbf{e}}(t-1)].\\end{aligned}\\ ] ] * stage 4 .",
    "* now consider the second term on the right hand side of .",
    "manipulating the expectation yields @xmath289\\\\ = & e[\\mathrm{tr}({\\bm{\\epsilon}}^t(t){\\mathbf{c}}^t(t){\\mathbf{h}}^t(t){\\mathbf{\\phi}}^{-1}(t){\\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t))]\\\\ = & e[\\mathrm{tr}({\\mathbf{c}}^t(t){\\mathbf{h}}^t(t){\\mathbf{\\phi}}^{-1}(t){\\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t){\\bm{\\epsilon}}^t(t))]\\\\ = & e[\\mathrm{tr}({\\mathbf{c}}^t(t){\\mathbf{h}}^t(t){\\mathbf{\\phi}}^{-1}(t){\\mathbf{h}}(t){\\mathbf{c}}(t)\\mathrm{diag}(\\{\\sigma_j^2\\})].\\end{aligned}\\ ] ] where @xmath290 is a diagonal matrix constructed with @xmath291 on its diagonal . expanding the matrix multiplications and noting that @xmath292 , we obtain @xmath293 \\nonumber \\\\",
    "\\leq & \\sum_{j=1}^{j}\\sigma_j^2 e[{\\mathbf{h}}_j^t(t){\\mathbf{\\phi}}_j^{-1}(t){\\mathbf{h}}_j(t)].\\end{aligned}\\ ] ] since the eigenvalues of @xmath255 are no larger than @xmath294 @xmath162 , it holds that @xmath295 \\leq \\frac{\\lambda_{\\max}({\\mathbf{r}_{h_j}}^{-1})}{2q(\\tau)t } e[{\\mathbf{h}}_j^t(t){\\mathbf{h}}_j(t)].\\end{aligned}\\ ] ] which after using that @xmath296 = \\mathrm{tr}(e[{\\mathbf{h}}_j(t){\\mathbf{h}}_j^t(t)])$ ] @xmath297 , allows one to deduce from that @xmath298 \\nonumber \\\\ \\leq & \\frac{1}{2q(\\tau)t}\\sum_{j=1}^{j}\\sigma_j^2\\lambda_{\\max}({\\mathbf{r}_{h_j}}^{-1})\\mathrm{tr}({\\mathbf{r}_{h_j}})\\end{aligned}\\ ] ] holds @xmath178 .    for @xmath299",
    ", we have @xmath300 because to , and thus @xmath301 \\\\ \\leq & \\sum_{j=1}^{j}\\gamma \\sigma_j^2 e[{\\mathbf{h}}_j^t(t){\\mathbf{h}}_j(t)]= \\gamma\\sum_{j=1}^{j}\\sigma_j^2 tr({\\mathbf{r}_{h_j}}).\\end{aligned}\\ ] ] therefore , for @xmath302 yields @xmath303 \\nonumber \\\\ \\leq & \\gamma\\sum_{j=1}^{j}\\sigma_j^2 \\mathrm{tr}({\\mathbf{r}_{h_j}}).\\end{aligned}\\ ] ] * stage 5 . * substituting , and into implies for @xmath304 that @xmath305\\nonumber\\\\ \\leq & e[{\\mathbf{e}}^t(t-1){\\mathbf{\\phi}}(t-1){\\mathbf{e}}(t-1)]\\nonumber\\\\ + & \\frac{1}{2q(\\tau)t}\\sum_{j=1}^{j}\\sigma_j^2\\lambda_{\\max}({\\mathbf{r}_{h_j}}^{-1})\\mathrm{tr}({\\mathbf{r}_{h_j}})\\end{aligned}\\ ] ] while for @xmath302 @xmath306\\nonumber\\\\ \\leq & e[{\\mathbf{e}}^t(t-1){\\mathbf{\\phi}}(t-1){\\mathbf{e}}(t-1 ) ] + \\gamma \\sum_{j=1}^{j}\\sigma_j^2 \\mathrm{tr}({\\mathbf{r}_{h_j}}).\\end{aligned}\\ ] ] summing from @xmath307 to @xmath121 and from @xmath120 to @xmath308 , applying telescopic cancellation , and noticing that @xmath309 , yields for @xmath304 @xmath272 \\\\ \\leq & \\gamma^{-1}||{\\mathbf{e}}(0)||^2 + ( \\gamma t_0 + \\sum_{r = t_0}^{t}\\frac{\\lambda_{\\max}({\\mathbf{r}_{h_j}}^{-1})}{2q(\\tau)t})\\sum_{j=1}^{j}\\sigma_j^2\\mathrm{tr}({\\mathbf{r}_{h_j}})\\\\ \\leq & \\gamma^{-1}||{\\mathbf{e}}(0)||^2+(\\gamma t_0+\\frac{\\lambda_{\\max}({\\mathbf{r}_{h_j}}^{-1})}{2q(\\tau)}\\ln(t))\\sum_{j=1}^{j}\\sigma_j^2\\mathrm{tr}({\\mathbf{r}_{h_j}}).\\end{aligned}\\ ] ] using @xmath310 in and the definition @xmath311 , establishes that @xmath312\\leq e[{\\mathbf{e}}^t(t){\\mathbf{\\phi}}(t){\\mathbf{e}}(t ) ] , \\quad t \\geq t_0\\end{aligned}\\ ] ] which in turn implies that @xmath313 \\\\ \\leq & \\gamma^{-1}||{\\mathbf{e}}(0)||^2+(\\gamma t_0 + \\frac{\\lambda_{\\max}({\\mathbf{r}_{h_j}}^{-1})}{2q(\\tau)}\\ln(t))\\sum_{j=1}^{j}\\sigma_j^2\\mathrm{tr}({\\mathbf{r}_{h_j } } ) .",
    "\\nonumber\\end{aligned}\\ ] ] finally , with @xmath314 this leads to , which completes the proof of cd - rls-1 .    consider next cd - rls-2 .",
    "stage 1 of the proof remains the same , while for stage 2 , @xmath315 is replaced by @xmath316 $ ] in to arrive at @xmath317 .",
    "\\nonumber\\end{aligned}\\ ] ] its matrix form can be expressed as @xmath318 \\nonumber\\\\ = & e[{\\mathbf{e}}^t(t-1)({\\mathbf{\\phi}}(t-1)-\\rho({\\mathbf{c}}(t){\\mathbf{l}})\\otimes\\mathbf{i}_p)^t{\\mathbf{\\phi}}^{-1}(t)\\nonumber\\\\ & \\hspace{1em } \\times ( { \\mathbf{\\phi}}(t-1)-\\rho({\\mathbf{c}}(t){\\mathbf{l}})\\otimes\\mathbf{i}_p){\\mathbf{e}}(t-1)]\\nonumber\\\\ + &   e[{\\bm{\\epsilon}}^t(t){\\mathbf{c}}^t(t){\\mathbf{h}}^t(t){\\mathbf{\\phi}}^{-1}(t){\\mathbf{h}}(t){\\mathbf{c}}(t){\\bm{\\epsilon}}(t)].\\end{aligned}\\ ] ] observe that the right hand sides of and are only different in their first terms .",
    "similar to stage 3 ( cf . ) , we need to show that the first term satisfies @xmath319\\nonumber\\\\ \\leq & e[{\\mathbf{e}}^t(t-1){\\mathbf{\\phi}}(t-1){\\mathbf{e}}(t-1)].\\end{aligned}\\ ] ] substituting the update with @xmath142 into , it suffices to prove that @xmath320\\nonumber\\\\ \\geq & \\rho e[{\\mathbf{e}}^t(t-1 ) \\mathbf{w } { \\mathbf{e}}(t-1 ) ] \\nonumber\\end{aligned}\\ ] ] where @xmath321 for the left hand side of , use the lower bound of the conditional expectation @xmath322 $ ] to eliminate @xmath273 , and arrive at @xmath323 \\nonumber \\\\ \\geq & 2q(\\tau ) e[{\\mathbf{e}}^t(t-1 ) { \\mathbf{h}}(t){\\mathbf{h}}^t(t ) \\nonumber \\\\ & \\hspace{1em } \\times ( \\mathbf{i}_j+{\\mathbf{h}}^t(t){\\mathbf{\\phi}}^{-1}(t-1){\\mathbf{h}}(t))^{-1}\\otimes\\mathbf{i}_p{\\mathbf{e}}(t-1 ) ] . \\nonumber\\end{aligned}\\ ] ] by , it holds that @xmath324 , and thus @xmath325^{-1 } \\succeq \\left[\\mathbf{i}_j+ \\gamma   { \\mathbf{h}}^t(t ) { \\mathbf{h}}(t)\\right]^{-1}.\\end{aligned}\\ ] ] by assumption @xmath326 are uniformly bounded .",
    "if @xmath327 for all @xmath328 , we find @xmath329^{-1 } \\succeq \\frac{1}{1+\\gamma k^2 }   \\mathbf{i}_j.\\end{aligned}\\ ] ] substituting into , we obtain a lower bound for the left hand side of given by @xmath330 \\nonumber \\\\ \\geq & \\frac{2q(\\tau)}{1+\\gamma k^2}e[{\\mathbf{e}}^t(t-1){\\mathbf{h}}(t){\\mathbf{h}}^t(t){\\mathbf{e}}(t-1 ) ] \\nonumber \\\\ = & \\frac{2q(\\tau)}{1+\\gamma k^2}e[{\\mathbf{e}}^t(t-1)\\text{diag}\\{{\\mathbf{r}_{h_j}}\\}{\\mathbf{e}}(t-1 ) ] \\nonumber \\\\ \\geq & \\frac{2q(\\tau)\\mu}{1+\\gamma k^2}e[||{\\mathbf{e}}(t-1)||^2 ] .",
    "\\nonumber\\end{aligned}\\ ] ] as for the right hand side of , it is upper bounded by @xmath331 \\\\",
    "\\leq   & \\rho e[(2||\\mathbf{w}_1||_2+||\\mathbf{w}_2||_2 + 2||{\\mathbf{l}}||_2 \\nonumber \\\\ & \\hspace{1em } + \\rho||{\\mathbf{l}}||_2 ^ 2||{\\mathbf{\\phi}}^{-1}(t-1)||_2)||{\\mathbf{e}}(t-1)||^2 ] \\nonumber\\end{aligned}\\ ] ] where we used that all the diagonal elements @xmath248 of @xmath273 are within the range @xmath332 $ ] while @xmath333 is upper bounded by @xmath334 , @xmath335 by assumption , @xmath336 , @xmath337 and @xmath338 , we find that @xmath339 similarly , @xmath340 is upper bounded by @xmath341 therefore , reduces to @xmath342 \\\\ \\leq & \\rho(2\\gamma\\lambda_{\\max}({\\mathbf{l}})k^2+\\gamma^2\\lambda_{\\max}({\\mathbf{l}})^2k^2 + 2\\lambda_{\\max}({\\mathbf{l } } ) \\nonumber \\\\ + & \\rho \\gamma \\lambda_{\\max}({\\mathbf{l}})^2)e[||{\\mathbf{e}}(t-1)||^2 ] .",
    "\\nonumber\\end{aligned}\\ ] ] considering a positive constant @xmath343 and combining with , we see that if @xmath102 is chosen within @xmath344 $ ] , then holds for all @xmath282 ; and so does .",
    "the update of @xmath347 for cd - rls-3 is ( cf . for cd - rls-1 ) @xmath348 per time @xmath32 , @xmath349 is the latest time slot when node @xmath21 received information from its neighbor @xmath43 .",
    "therefore , @xmath350 can be viewed as network delay caused by the censoring strategy .",
    "then we have @xmath351||\\\\ \\leq & ||{\\mathbf{\\phi}}_j^{-1}(t)||_2 \\big[||{\\mathbf{h}}_j(t)||^2||{\\mathbf{e}}_j(t-1)||+||{\\mathbf{h}}_j(t)|||\\epsilon_j(t)|\\\\ + & \\rho\\sum_{j'\\in{\\mathcal{n}}_j}(||{\\mathbf{e}}_j(t-1)||+||{\\mathbf{e}}_{j'}(t - d_j^{j'}(t))||)\\big].\\end{aligned}\\ ] ] in deriving the inequality we use the fact that @xmath352 .    according to in the proof of theorem [ thm : convergence ] , which also holds true for cd - rls-3",
    ", there exists @xmath252 , such that @xmath310 for all @xmath162 and @xmath353 .",
    "thus , @xmath354 is upper bounded by @xmath355 where @xmath356 is a positive constant determined by @xmath169 and the smallest eigenvalue of @xmath241 . by ( as1 ) and",
    "( as2 ) , @xmath357 , @xmath358 and @xmath359 are also upper bounded .",
    "therefore , there exist constants @xmath360 , such that @xmath361.\\end{aligned}\\ ] ] taking expectations on both sides yields .",
    "rewrite the update of @xmath347 for cd - rls-3 in to @xmath362 multiplying @xmath241 on both sides , we have @xmath363 using the same notations as in the proof of theorem [ thm : convergence ] , we obtain an matrix form @xmath364 where @xmath365 and its @xmath21th block is @xmath366 .",
    "observe that @xmath367 contains the differences between the local estimates and their delayed values , and hence plays a critical role in the convergence proof .",
    "below we look for an upper bound for @xmath368 $ ] .    by the cauchy - schwarz inequality , we have latexmath:[\\ ] ] observing that @xmath398 $ ] is bounded because @xmath399 is bounded by ( as2 ) , the right hand side of is in the order of @xmath400 . following the argument in step 5 of the proof of theorem [ thm : convergence ]",
    ", @xmath401 is in the order of @xmath389 when @xmath162 .",
    "therefore , @xmath402 $ ] is in the order of @xmath403 , which completes the proof of theorem [ thm : convergence3 ] .",
    "r. arroyo - valles , s. maleki , and g. leus , `` a censoring strategy for decentralized estimation in energy - constrained adaptive diffusion networks , '' _ proc . of intl . work . on signal processing advances in wireless communications _",
    ", germany , june 2013 .",
    "d. berberidis , v. kekatos , and g. b. giannakis , `` online censoring for large - scale regressions with application to streaming big data , '' _ ieee transactions on signal processing _ , vol .",
    "64 , pp . 38543867 , aug .",
    "v. cevher , s. becker , and m. schmidt , `` convex optimization for big data : scalable , randomized , and parallel algorithms for big data analytics , '' _ ieee signal processing magazine _ , vol .",
    "31 , pp . 3243 , sept .",
    "g. b. giannakis , q. ling , g. mateos , i. d. schizas , and h. zhu , `` decentralized learning for wireless communications and networking , '' _ splitting methods in communication and imaging , science and engineering _ , r. glowinski , s. osher , and w. yin ( eds . ) , springer , 2016 .",
    "r. jiang , y. lin , b. chen , and b. suter , `` distributed sensor censoring for detection in sensor networks under communication constraints , '' _ proc . of asilomar conf . on signals , systems and computers _",
    ", pacific grove , ca , nov .",
    "2005 .",
    "g. mateos , i. d. schizas and g. b. giannakis , `` distributed recursive least - squares for consensus - based in - network adaptive estimation , '' _ ieee transactions on signal processing _ , vol .",
    "57 , pp . 45834588 , nov .",
    "2009 .",
    "c. rago , p. willett , and y. bar - shalom , `` censoring sensors : a low - communication - rate scheme for distributed detection , '' _ ieee transactions on aerospace and electronic systems _ , vol .",
    "554568 , apr .",
    "m. a. sharkh , m. jammal , a. shami , and a. ouda , `` resource allocation in a network - based cloud computing environment : design challenges , '' _ ieee communications magazine _ , vol .",
    "51 , pp . 4652 , nov .",
    "k. slavakis , s. j. kim , g. mateos , and g. b. giannakis , `` stochastic approximation vis - a - vis online learning for big data analytics , '' _ ieee signal processing magazine _ , vol .",
    "124129 , nov .",
    "2014 .",
    "z. wang , z. yu , q. ling , d. berberidis , and g. b. giannakis , `` distributed recursive least - squares with data - adaptive censoring , '' _ proc .",
    "conf . on acoustics , speech , and signal processing _",
    ", new orleans , march 2017 ."
  ],
  "abstract_text": [
    "<S> the deluge of networked data motivates the development of algorithms for computation- and communication - efficient information processing . in this context , </S>",
    "<S> three data - adaptive censoring strategies are introduced to considerably reduce the computation and communication overhead of decentralized recursive least - squares ( d - rls ) solvers . </S>",
    "<S> the first relies on alternating minimization and the stochastic newton iteration to minimize a network - wide cost , which discards observations with small innovations . in the resultant algorithm , </S>",
    "<S> each node performs local data - adaptive censoring to reduce computations , while exchanging its local estimate with neighbors so as to consent on a network - wide solution . </S>",
    "<S> the communication cost is further reduced by the second strategy , which prevents a node from transmitting its local estimate to neighbors when the innovation it induces to incoming data is minimal . in the third strategy , not only transmitting , but also receiving estimates from neighbors is prohibited when data - adaptive censoring is in effect . for all strategies , </S>",
    "<S> a simple criterion is provided for selecting the threshold of innovation to reach a prescribed average data reduction . the novel censoring - based ( c)d - rls algorithms are proved convergent to the optimal argument in the mean - square deviation sense . </S>",
    "<S> numerical experiments validate the effectiveness of the proposed algorithms in reducing computation and communication overhead .    </S>",
    "<S> decentralized estimation , networks , recursive least - squares ( rls ) , data - adaptive censoring </S>"
  ]
}