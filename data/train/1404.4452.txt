{
  "article_text": [
    "let @xmath1 be a standard brownian motion . for @xmath2 , consider the stochastic differential equation @xmath3 the solution to this equation is @xmath4 with @xmath5 a process which returns to 0 at time 1 almost surely when @xmath6 .",
    "the process @xmath7 is known as an @xmath0-brownian bridge , @xmath0-wiener bridge or scaled brownian bridge and can be viewed as a flexible alternative to the standard brownian bridge .",
    "it includes brownian motion ( @xmath8 ) and the usual brownian bridge ( @xmath9 as special cases .",
    "this paper is concerned with estimation of the scaling parameter @xmath0 given a sample path observed until time @xmath10 .",
    "estimating @xmath0 is of interest since @xmath0 determines _ how _ the process tends to 0 : if @xmath11 the process tends to 0 slower than the brownian bridge and if @xmath12 it tends to 0 faster than the brownian bridge ( see figure  [ fig1 ] ) .",
    "on the `` expected future '' for different values of @xmath0 . ]    the @xmath0-brownian bridge was introduced by @xcite , who used it to model arbitrage profit in the absence of transaction costs .",
    "it has also been used to model exchange rate dynamics @xcite and the degree of interventionist policies in the run - up to a monetary union @xcite . a possible further application in financial theory",
    "is the following .",
    "it has been reported that stock prices tend to end up at strike prices of heavily traded vanilla options at the time of their maturity ; see for example  @xcite and the references therein .",
    "brownian bridges have been used to model this phenomenon . however",
    ", the rate of convergence to the strike price will likely depend on the behavior of the market . in a cautious market",
    ", traders will start early on to push the stock price to the strike price .",
    "then an @xmath0-brownian bridge with an @xmath13 will be more suitable to model the pinning behavior than the usual brownian bridge . in an incautious market",
    ", traders will start late to push the stock price to the strike price and an @xmath0-brownian bridge with an @xmath14 will be more suitable to model the behavior .",
    "but the correct value of @xmath0 is required in order to develop optimal selling strategies for stocks showing the pinning phenomenon . in this context",
    "we mention that the optimal stopping problem for the usual brownian bridge was solved in  @xcite .",
    "other possible areas of applications include for instance modeling of animal movements , where the @xmath0-brownian bridge seems like a strong candidate to replace the brownian bridge model proposed by @xcite .",
    "starting with a paper by @xcite , the @xmath0-brownian bridge has attracted considerable interest in the stochastics community .",
    "estimation was discussed by @xcite , @xcite and @xcite , all of whom studied some properties of the maximum likelihood estimator ( mle ) of @xmath0 .",
    "@xcite studied the least squares estimator of @xmath0 when @xmath15 is replaced by a fractional brownian motion . @xcite and",
    "@xcite studied hypothesis testing problems for @xmath0 .",
    "previous studies of estimators of @xmath0 have focused on asymptotic properties , i.e. the behavior of the mle as @xmath16 . in section [ bias ]",
    "we derive the bias of the mle for @xmath10 , which is found to be surprisingly large . in section [ estimators ]",
    "we propose a bias - correction of the mle and introduce some bayesian estimators of @xmath0 .",
    "we then evaluate the properties of the bayesian estimators and the bias - corrected mle in a simulation study .",
    "finally , some open problems are discussed in section [ discussion ] and proofs are given in an appendix .",
    "we mention here that we may just as well define the @xmath0-brownian bridge on an interval @xmath17 $ ] : let @xmath18 be the strong solution of the stochastic differential equation @xmath19 then , for @xmath20 , we have @xmath21 .",
    "the @xmath0-brownian bridge is self - similar : @xmath22 } = _ d \\left ( \\sqrt{s } x^{(\\alpha , 1)}_{t / s } \\right)_{t \\in [ 0 , s]}.\\ ] ] from this self - similarity the results in this paper easily extend to @xmath0-brownian bridges on an interval @xmath17 $ ] , so it suffices to study the simpler setting where @xmath23 .",
    "the mle of @xmath0 based on a trajectory observed until time @xmath24 is given by @xmath25 @xcite showed that @xmath26 is a strongly consistent estimator of @xmath0 .",
    "it is also the least squares estimator @xcite .",
    "* section 2.2 ) showed that the mle can be written as @xmath27 which allows for a straightforward computation of @xmath26 given the sample path .",
    "next , we present a result for the expected value of the mle , the proof of which is given in the appendix .",
    "[ prop1 ] the expected value of the maximum likelihood estimator @xmath26 of @xmath0 based on the observations up to time @xmath10 is @xmath28 & = \\frac{1}{2 } + \\frac{(1-t)^{\\frac{1 - 2\\alpha}{4 } } } { \\sqrt{2 } } \\int_{\\frac{1 - 2\\alpha}{2}}^\\infty \\frac { ( 1-t)^u - ( 1-t)^{-u } } { \\left((1 + \\frac{1 - 2\\alpha}{2u})(1-t)^u + ( 1 - \\frac{1 - 2\\alpha}{2u})(1-t)^{-u}\\right)^{3/2 } } du \\label{e : gen_form } \\\\      & \\qquad - \\frac{\\ln(1-t)(1-t)^{\\frac{1 - 2\\alpha}{4 } } } { \\sqrt{2 } } \\int_{\\frac{1 - 2\\alpha}{2}}^\\infty \\frac{u}{\\sqrt{(1 + \\frac{1 - 2\\alpha}{2u})(1-t)^u + ( 1 - \\frac{1 - 2\\alpha}{2u})(1-t)^{-u } } } du .",
    "\\notag    \\end{aligned}\\ ] ]    the expectation   and the corresponding bias are shown in figure [ fig2 ] for @xmath29 and @xmath30 .",
    "the bias follows the same pattern for different @xmath31 , and is increasing in @xmath31 .",
    "we can see an almost constant behavior of the bias of @xmath26 for larger values of @xmath0 .",
    "the following result gives an explanation of this .",
    "we use the notation @xmath32 for @xmath33 = 0 $ ] .    .",
    "]    [ prop2 ] for the bias @xmath34 $ ] of the mle based on the observations up to time @xmath10 we have @xmath35 - \\alpha \\sim - \\frac{2}{\\ln(1-t)}.\\ ] ]    a proof is found in the appendix .",
    "when @xmath36 we can obtain a simpler expression for the bias .",
    "specializing formula   to the case @xmath36 we obtain @xmath35 = \\frac{1}{2 } + \\frac{1}{\\sqrt{2 } } \\int_0^\\infty \\frac { ( 1-t)^u - ( 1-t)^{-u } } { \\left((1-t)^u + ( 1-t)^{-u}\\right)^{3/2 } } du - \\frac{\\ln(1-t)}{\\sqrt{2 } } \\int_0^\\infty \\frac{u}{\\sqrt{(1-t)^u + ( 1-t)^{-u } } } du.\\ ] ] the substitution @xmath37 in both integrals gives @xmath38 = \\frac{1}{2 } + \\frac{1}{\\sqrt{2 } \\ln(1-t ) } \\int_0^\\infty \\frac { e^v - e^{-v } } { \\left(e^v + e^{-v}\\right)^{3/2 } } dv - \\frac{1}{\\sqrt{2}\\ln(1-t ) } \\int_0^\\infty \\frac{v}{\\sqrt{e^v + e^{-v } } } dv.\\ ] ] the first integral yields @xmath39_0^\\infty = \\sqrt{2},\\ ] ] and the second integral may be rewritten as @xmath40 where @xmath41 plugging   and   into   we obtain @xmath42 = \\frac{1}{2 } + \\frac{1 - a/2}{\\ln(1-t)}.\\ ] ] the bias of the mle when @xmath36 and @xmath43 is shown in figure [ fig3 ] . as can be seen , it is quite substantial unless @xmath31 is very close to 1 .",
    "in particular , @xmath44>1 $ ] when @xmath45 , meaning that on average the incautious market with @xmath36 will be mistaken for a cautious market with @xmath12 .     and @xmath43 .",
    "a bias - correction of the mle can be obtained by inverting the expectation   numerically , letting the corrected estimator be @xmath46,\\ ] ] so that @xmath47 is the @xmath0 for which @xmath48 $ ] is the observed value of @xmath26 .",
    "other alternative estimators are bayesian estimators of @xmath0 .",
    "we will study the mean and median of posterior distributions based on the jeffreys and @xmath49 priors .",
    "these can be viewed either as bayesian estimators or regularized frequentist estimators .",
    "the non - informative @xcite prior @xmath50 is proportional to @xmath51 where @xmath52 is the fisher information . from (",
    "* lemma 10 ) it follows that @xmath53    the jeffreys prior for @xmath0 is very heavy - tailed .",
    "its median is roughly 98 when @xmath54 and 94 when @xmath55 , meaning that much of the prior probability mass is concentrated on values of @xmath0 that seem very unlikely to occur in practice . in applications where no prior information is available , it might therefore be preferable to use a `` low - informative '' prior with bounded support , such as the @xmath49 prior .",
    "the posterior distributions are computed by bayes formula using the likelihood function , which is @xmath56 see ( * ? ? ?",
    "* section 2.1 ) .      to estimate the bias and mse of the alternative estimators we performed a simulation study ,",
    "in which for 100,000 realizations of the process were simulated @xmath57 .",
    "each realization was observed in 300 points and the estimators were computed by approximating the integrals in and using the rectangle rule .",
    "the bias and mse of the alternative estimators are compared to that of the mle when @xmath54 in figure [ fig4 ] .",
    "the figure is qualitatively similar for other values of @xmath31 .    from figure [ fig4 ]",
    "we conclude that @xmath47 is nearly unbiased and has a lower mse than the mle , thereby improving upon the mle considerably .",
    "the bayesian estimators based on the jeffreys prior are biased , but except when @xmath0 is close to 0 the bias is lower than that of the mle .",
    "they also have lower mse s , which is close to that of the corrected mle .",
    "the bayesian estimators based on the @xmath49 prior shrinks the estimate towards 5 and have the best performance in that particular region of the parameter space .",
    "when @xmath0 is close to 0 or 10 they are heavily biased . among the five estimators ,",
    "only the corrected mle is nearly unbiased when @xmath0 is close to 1 , meaning that it is the only estimator that reliably identifies both cautious and incautious markets .     and @xmath54.,title=\"fig : \" ]   and @xmath54.,title=\"fig : \" ]",
    "we have shown both analytically and numerically that the mle of @xmath0 is heavily biased , but that it is possible to correct for this bias .",
    "the corrected estimator is nearly unbiased and has a lower mse .",
    "it can be recommended for use instead of the mle .",
    "if one for some reason is unwilling to apply the bias - correction , it might be preferable to use a bayesian estimator based on the jeffreys prior instead .",
    "we have also seen that when the jeffreys and @xmath49 priors are used for bayesian estimation of @xmath0 , the posterior mean and the posterior median often fail to identify incautious markets , that is , situations where @xmath58 .",
    "we note that although such long - run properties are of a frequentist nature , they still play a part in calibration of bayesian procedures , for instance if one wishes to apply these estimators for repeated financial decisions .",
    "a bias - correction such as that applied to the mle makes little sense in a bayesian context , but there may be a truly bayesian way to obtain estimators with better long - run properties .",
    "the estimators in our study are motivated by the squared error loss and the absolute value loss , respectively . for the @xmath0-brownian bridge it might prove fruitful to instead use a loss function that penalizes overestimation more than underestimation .",
    "we have however not pursued this idea further .",
    "although estimation and hypothesis testing for the @xmath0-brownian bridge has been studied extensively , interval estimation of @xmath0 remains an open problem .",
    "@xcite derived the asymptotic distribution of the mle , which can be used to construct a confidence interval , but based on our investigation of the properties of this estimator we doubt that the confidence interval based on the mle will have good performance .",
    "our investigation casts similar doubts on how well bayesian credible sets based on the jeffreys and @xmath49 priors will perform . as confidence intervals",
    "are considerably more informative than point estimates , we believe this problem to be of great interest .",
    "a further motivation for studying confidence intervals is their connection to hypothesis testing .",
    "papers dealing with hypothesis tests for @xmath0 @xcite have focused on tests of point hypotheses of the type @xmath59 versus @xmath60 . in many situations",
    "it would be of greater interest to test against a composite hypothesis , e.g. @xmath59 versus @xmath61 or @xmath62 versus @xmath63 .",
    "this is possible to do by inverting a well - behaved confidence interval , not only in the frequentist setting , but also in bayesian inference @xcite .",
    "in this appendix we give the proofs of propositions [ prop1]-[prop2 ] . ignoring the superscript @xmath64 and setting @xmath65",
    "we obtain the following simplified expression for the mle .",
    "@xmath66 from   we have @xmath67 + \\frac{1}{2 } - \\frac{\\ln(1-t)}{2 } { \\mbox{e}}_{\\alpha } [ 1/i_t],\\ ] ] where @xmath68 denotes expectation under @xmath0 .",
    "[ p : gen_quot ] let @xmath71 and @xmath72 be two positive random variables and let @xmath73 be their joint laplace transform . then , for @xmath74 @xmath75 = \\gamma(k)^{-1 } \\int_0^\\infty t^{k-1 } \\lim_{s \\searrow 0 } \\frac{\\partial^j}{\\partial s^j } m_{y , z}(s ,- t ) dt.\\ ] ]    it was shown in theorem  21 in  @xcite that the joint laplace transform @xmath76 of @xmath77 and @xmath78 is @xmath79 where @xmath80 .",
    "applying proposition  [ p : gen_quot ] with @xmath81 , @xmath82 , and @xmath83 it follows that @xmath84 = \\int_0^\\infty \\frac{(1-t)^{(1 - 2\\alpha)/4}}{\\sqrt{\\cosh ( u(t ) \\ln(1-t ) ) + \\frac{1 - 2\\alpha}{2u(t ) } \\sinh ( u(t ) \\ln(1-t ) ) } } dt.\\ ] ] the substitution @xmath85 yields @xmath86 = \\sqrt{2 } ( 1-t)^{\\frac{1 - 2\\alpha } { 4 } } \\int_{\\frac{1 - 2\\alpha } { 2}}^\\infty \\frac{u du}{\\sqrt{(1 + \\frac{1 - 2\\alpha } { 2u})(1-t)^u + ( 1 - \\frac{1 - 2\\alpha}{2u})(1-t)^{-u}}},\\ ] ] where we also used @xmath87    the derivative of @xmath88 with respect to @xmath89 is given by @xmath90 applying proposition  [ p : gen_quot ] once again with @xmath91 , @xmath81 , @xmath92 , and @xmath83 it follows that @xmath93 = \\int_0^\\infty \\frac{-\\frac{(1-t)^{(5 - 2\\alpha)/4 } } { u(t ) } \\sinh\\left ( u(t ) \\ln(1-t ) \\right ) } { \\left ( \\cosh ( u(t ) \\ln(1-t ) ) + \\frac{1 - 2\\alpha}{2u(t ) } \\sinh ( u(t ) \\ln(1-t ) ) \\right)^{3/2 } } dt.\\ ] ] again , the substitution @xmath85 yields @xmath94 = - \\sqrt{2 } ( 1-t)^{\\frac{5 - 2\\alpha}{4 } } \\int_{\\frac{1 - 2\\alpha } { 2}}^\\infty \\frac { ( 1-t)^u - ( 1-t)^{-u } } { \\left((1 + \\frac{1 - 2\\alpha } { 2u})(1-t)^u + ( 1 - \\frac{1 - 2\\alpha}{2u})(1-t)^{-u}\\right)^{3/2 } } du.\\ ] ]      using proposition  [ p : gen_quot ] we could also find a formula for the mean squared error of @xmath26 since @xmath95 & = ( \\alpha - 1/2)^2 + \\frac{\\alpha - 1/2}{1-t } { \\mbox{e}}_{\\alpha}[x_t^2/i_t ] + ( \\alpha - 1/2 ) \\ln(1-t ) { \\mbox{e}}_{\\alpha } [ 1/i_t ] \\\\        & \\qquad   + \\frac{1}{4(1-t)^2 } { \\mbox{e}}_{\\alpha}[x_t^4/i_t^2 ] + ( \\ln(1-t))^2 { \\mbox{e}}_{\\alpha}[1/i_t^2 ] + \\frac{\\ln(1-t)}{2(1-t ) } { \\mbox{e}}_{\\alpha}[x_t^2/i_t^2 ] .",
    "\\end{aligned}\\ ] ] however , at this time we do not see a way to simplify the occurring integrals significantly and thus we do not pursue this further .",
    "the substitution @xmath99 in @xmath100 yields @xmath101 the terms @xmath102 in the nominator and @xmath103 in the denominator of the integrand vanish as @xmath0 tends to infinity and thus @xmath104 by partial integration we obtain @xmath105_1^\\infty \\\\      & \\qquad + \\frac{2\\alpha-1}{2\\sqrt{2 } } \\int_1^\\infty \\frac { 3 \\cdot 4 ( 1-t)^{(v-1)(2\\alpha-1)/4 } } { 2 \\left(1 + \\frac{1}{v } \\right)^{5/2 } v^2 ( 2\\alpha-1 ) \\ln(1-t ) } dv \\\\     & = \\frac{1}{2 \\ln(1-t ) } + \\frac{3}{\\ln(1-t ) } \\int_1^\\infty \\frac{(1-t)^{(v-1)(2\\alpha-1)/4}}{\\left(1 + \\frac{1}{v } \\right)^{5/2 } v^2 } dv.\\end{aligned}\\ ] ] the latter integral vanishes for large @xmath0 and thus @xmath106    for the integral @xmath107 we proceed in a similar way .",
    "the substitution @xmath99 yields @xmath108 the term @xmath109 vanishes as @xmath0 tends to infinity and thus @xmath110 partial integration yields @xmath111_1^\\infty \\\\      & \\qquad- \\frac{\\ln(1-t)}{\\sqrt{2 } } \\left(\\frac{2\\alpha-1}{2}\\right)^2 \\int_1^\\infty \\frac{4 v^2(2v+3 ) ( 1-t)^{(v-1)(2\\alpha-1)/4}}{2\\left ( \\frac{v^3}{v+1 } \\right)^{1/2 } ( v+1)^2 ( 2\\alpha-1 )",
    "\\ln(1-t ) } dv \\\\     & = - \\alpha + \\frac{1}{2 } - \\frac{2\\alpha-1}{2 \\sqrt{2 } } \\int_1^\\infty \\frac{\\sqrt{v } ( 2v+3)}{(v+1)^{3/2 } } ( 1-t)^{(v-1)(2\\alpha-1)/4 } dv.\\end{aligned}\\ ] ] integrating by parts once again gives @xmath112_1^\\infty \\\\      & \\qquad+ \\frac{2\\alpha-1}{2 \\sqrt{2 } } \\int_1^\\infty \\frac{3 \\cdot 4 ( 1-t)^{(v-1)(2\\alpha-1)/4}}{2 \\sqrt{v } ( v+1)^{5/2 } ( 2\\alpha-1 ) \\ln(1-t ) } dv   \\\\     & = - \\alpha + \\frac{1}{2 } + \\frac{5}{2 \\ln(1-t ) } + \\frac{3}{\\sqrt{2 } \\ln(1-t ) } \\int_1^\\infty \\frac{(1-t)^{(v-1)(2\\alpha-1)/4}}{\\sqrt{v } ( v+1)^{5/2 } } dv.\\end{aligned}\\ ] ] the latter integral vanishes for large @xmath0 and thus @xmath113"
  ],
  "abstract_text": [
    "<S> the @xmath0-brownian bridge , or scaled brownian bridge , is a generalization of the brownian bridge with a scaling parameter that determines how strong the force that pulls the process back to 0 is . </S>",
    "<S> the bias of the maximum likelihood estimator of the parameter @xmath0 is derived and a bias - correction that improves the estimator substantially is proposed . </S>",
    "<S> the properties of the bias - corrected estimator and four bayesian estimators based on non - informative priors are evaluated in a simulation study . + * keywords : * @xmath0-brownian bridge , bias - correction , estimation , scaled brownian bridge . </S>"
  ]
}