{
  "article_text": [
    "in this paper , we consider the statistical analysis of high dimensional structured data in two close setups : vectors with small support and matrices with low rank . in the first setup , known as compressed sensing ( cs ) @xcite , the aim is to reconstruct a high dimensional vector with only few non - zero coefficients , based on a small number of linear measurements . in the second setup ,",
    "called matrix completion @xcite , we aim at reconstructing a small rank matrix from the observations of only a few entries .",
    "both problems are motivated by many practical applications in many different domains ( medical  @xcite , imaging  @xcite , seismology  @xcite , recommending systems such as the netflix prize , etc . ) as well as theoretical challenges in many different fields of mathematics ( random matrices , geometry of banach spaces , harmonic analysis , empirical processes theory , etc . ) . from an algorithmic viewpoint ,",
    "one central idea is the convex relaxation of the @xmath0-functional ( the function giving the number of non - zero coefficients of a vector ) and of the rank function .",
    "this idea gave birth to two well - known algorithms : the basis pursuit algorithm  @xcite   and nuclear norm minimization  @xcite .",
    "many results have been obtained for these two algorithms and we refer the reader to the next sections for more details . here",
    "we will be interested in weighted versions of these algorithms , see  @xcite in the cs setup .",
    "in particular , we will be interested in finding theoretical explanation underlying the fact that , empirically , it is observed that weighted basis pursuit outperforms classical basis pursuit .",
    "we will also propose a way to export the idea of reweighting into the matrix completion problem .",
    "one way of setting the cs problem is to ask the following question . starting with a @xmath1 matrix @xmath2 , called a _ sensing _ or _ measurement _ matrix , and with a vector @xmath3 in @xmath4 , is it possible to reconstruct @xmath3 from the linear measurements @xmath5 ?",
    "classical linear algebra theory tells that we need at least @xmath6 to recover @xmath3 from @xmath5 in order to find a unique solution to the linear system .",
    "but , if more is known on @xmath3 , then , hopefully , a smaller number @xmath7 of measurements may be enough .",
    "in the theory of cs , it is now well - understood that it is indeed possible to recover sparse signals ( signals with a small support , the support being the set of non - zeros entries ) from a small number of linear measurements . if @xmath3 is a sparse vector and @xmath2 a `` good '' measurement matrix ( in a sense to be clarified later ) , then looking for a vector @xmath8 with the smallest support and satisfying @xmath9 can recover @xmath3 exactly .",
    "this procedure , called the @xmath0 or support minimization procedure , is known to be the best theoretical procedure to recover any @xmath10-sparse vector @xmath3 ( vectors with a support size smaller than @xmath10 ) from @xmath5 as long as @xmath2 is injective on the set of all @xmath10-sparse vectors . however",
    ", this problem is np - hard , and alternatives are suitable in practice , in part because the function @xmath11 ( @xmath12 stands for the cardinality of the support of @xmath3 ) is not convex .",
    "a natural remedy to this problem is convex relaxation . in  @xcite",
    ", the authors propose to minimize the @xmath13-norm as the convex envelope of this non - convex function , leading to the so - called basis - pursuit algorithm ( bp ) .",
    "the bp algorithm minimizes the @xmath13 norm on the affine space @xmath14 .",
    "namely , consider , for any @xmath15 : @xmath16 so that @xmath17 is a candidate for the reconstruction of @xmath3 based on @xmath18 .",
    "we say that @xmath3 is exactly reconstructed by @xmath19 , namely @xmath20 , when @xmath3 is the unique solution of the minimization problem   when @xmath21 .",
    "note that other algorithms have been introduced in the cs literature .",
    "for instance , @xmath22-minimization algorithms for @xmath23 are considered in  @xcite .",
    "some greedy algorithms based on the ideas of the matching pursuit algorithm of  @xcite have been used in cs , see  @xcite for instance .    in the present paper",
    ", we consider weighted-@xmath13 minimization over @xmath24 .",
    "this algorithm was introduced in  @xcite . since then , it has drawn a particular attention because it is now acknowledged , although mainly only empirically observed , that a proper weighted basis - pursuit algorithm can improve a lot upon basic basis - pursuit .",
    "this is illustrated in figure  [ fig : phase - cs ] , and many other numerical experiments can be found in  @xcite . however , theoretical explanations of this fact are still lacking .",
    "some results that go in this direction are given in  @xcite , @xcite , @xcite .",
    "but , the results given in these papers are of a different nature than ours , since they are using a random model for the unknown vector @xmath3 , such as a vector with i.i.d @xmath25 non - zero entries , with a distribution support which is uniform conditionally on the sparsity . in the statement of our results",
    ", @xmath3 is an arbitrary deterministic sparse vector . in  @xcite an iteratively reweighted least - squares procedure",
    "is studied , as an approximation of basis - pursuit .",
    "we introduce the weighted algorithm : for any @xmath15 and any sequence @xmath26 of non - negative weights , @xmath27 we use the convention @xmath28 when @xmath29 and @xmath30 .",
    "note that , under this convention , the algorithm   is defined according to the support @xmath31 of @xmath32 by @xmath33 where if @xmath34 and @xmath35 , we denote by @xmath36 the vector such that @xmath37 if @xmath38 and @xmath39 if @xmath40 . once again , we say that @xmath3 is exactly reconstructed by @xmath41 , namely @xmath42 , when @xmath3 is the unique solution of the minimization problem   when @xmath21 .",
    "in particular , this requires that the support of @xmath3 is included in the support of @xmath32 .      note that when the weight vector @xmath32 is close to @xmath3 , then @xmath43 is close to @xmath12 .",
    "moreover , for `` reasonable '' matrices @xmath2 , the vector @xmath3 is the one with the shortest support in the affine space @xmath44 .",
    "so , a natural choice for @xmath32 in   is @xmath45 the next theorem proves that @xmath46 is at least as good as the basis pursuit algorithm @xmath19 .",
    "[ thm : a ] let @xmath47 .",
    "if @xmath48 , then @xmath49 .    the proof of theorem  [ thm : a ]",
    "is based on the well - known null space property and dual characterization of  @xcite , see section  [ sec : proofs ] below .",
    "however , it was observed empirically in  @xcite that it is better to consider positive weights , and thus , to consider , for some @xmath50 , the weights @xmath51 , @xmath52 while @xmath53 , then @xmath54 is also equal to @xmath55 and there is no hope to recover @xmath3 using @xmath46 as well . by adding an extra @xmath56 term to each weights , the necessary support condition @xmath57 to reconstruct @xmath3 from @xmath58",
    "is satisfied ( see for instance proposition  [ prop : equivalence - reconstruction - exacte ] in section  [ sec : proofs ] ) .",
    "the choice of @xmath50 can be done in a data - driven way , see  @xcite .      in figure",
    "[ fig : phase - cs ] , we give a simple illustration of the fact that weighted basis - pursuit can improve a lot upon basic basis - pursuit , using a simple numerical experiment .",
    "for many combinations of @xmath7 ( @xmath8-axis ) and @xmath10 ( @xmath3-axis ) , we repeat the following experiment 50 times : draw at random a sensing matrix @xmath2 with i.i.d @xmath59 entries and draw at random a vector with @xmath10 non - zero coordinates chosen uniformly , with i.i.d @xmath25 non - zero entries .",
    "then , compute @xmath60 and @xmath61 ( here we take @xmath62 without further investigation ) , where @xmath63 is computed iteratively , using @xmath64 then , we count the number of exact reconstructions achieved by @xmath65 and @xmath66 over the 50 repetitions .",
    "the plots on the left are the exact recovery counts of @xmath65 ( black means exact recovery over the 50 repetitions ) while the plots on the right are the exact recovery counts of @xmath66 . in these figures ,",
    "exact recovery is declared exact when @xmath67 , where we take @xmath68 on the first line and @xmath69 on the second line .",
    "the red curve is a theoretical `` phase - transition '' threshold @xmath70 .",
    "we observe in these figures that @xmath66 improves a lot upon @xmath65 , in particular when @xmath69 .",
    "-axis is the sparsity ( @xmath10 ) and the @xmath8-axis is the number of measurements ( @xmath7 ) . exact recovery is declared with a tolerance equal to @xmath71 on the first line , and equal to @xmath72 on the second line .",
    "the red curve is a theoretical phase - transition threshold @xmath73,title=\"fig : \" ] -axis is the sparsity ( @xmath10 ) and the @xmath8-axis is the number of measurements ( @xmath7 ) .",
    "exact recovery is declared with a tolerance equal to @xmath71 on the first line , and equal to @xmath72 on the second line .",
    "the red curve is a theoretical phase - transition threshold @xmath73,title=\"fig : \" ] + -axis is the sparsity ( @xmath10 ) and the @xmath8-axis is the number of measurements ( @xmath7 ) . exact recovery is declared with a tolerance equal to @xmath71 on the first line , and equal to @xmath72 on the second line .",
    "the red curve is a theoretical phase - transition threshold @xmath73,title=\"fig : \" ] -axis is the sparsity ( @xmath10 ) and the @xmath8-axis is the number of measurements ( @xmath7 ) .",
    "exact recovery is declared with a tolerance equal to @xmath71 on the first line , and equal to @xmath72 on the second line .",
    "the red curve is a theoretical phase - transition threshold @xmath73,title=\"fig : \" ]      now , we want to understand if @xmath46 can do better than @xmath19 , and why .",
    "in particular , if @xmath17 is close to @xmath3 ( but fails to reconstruct exactly @xmath3 ) , under which condition do we get @xmath49 ? in general , given a weight vector @xmath74 , what conditions on @xmath32 can insure that @xmath75 ? in theorem  [ thm : b ] below , we use the duality argument of  @xcite to prove that the condition @xmath76 where @xmath77 is the support of @xmath3 and @xmath78 is such that @xmath79 where @xmath80 and @xmath81 are , respectively , the restricted isometry and incoherency constants  @xcite of the matrix @xmath2 , ensure that the @xmath32-weighted algorithm @xmath41 recovers exactly @xmath3 given @xmath5 .",
    "it is interesting to note that , so far , only random matrices are able to satisfy the incoherency and isometry properties for small values of @xmath7 .",
    "thus , if one wants the number @xmath7 of measurements to be of the order ( up to some logarithmic factor ) of the sparsity of the vector to recover , one has to consider random matrices .",
    "this leads to results in compressed sensing that hold with a large probability , with respect to the randomness involved in the construction of the sensing matrix . in practice , however , the most interesting sensing matrices are structured matrices , like the fourier or the walsh matrices ( see  @xcite ) , since these matrices can be stored and constructed by efficient algorithms .",
    "a lot of research go in this direction , and we do nt consider this problem here , but rather focus on weighted algorithms .",
    "therefore , we will state our probabilistic results for a simple ( and somehow universal ) sensing matrix @xmath2 with entries being i.i.d . centered gaussian variables with variance @xmath82 .",
    "[ thm : b ] let @xmath83 and denote by @xmath77 its support and by @xmath10 the cardinality of @xmath77 .",
    "let @xmath84 and @xmath85 .",
    "assume that @xmath86 \\;\\mbox { and } \\",
    ";   c \\leq \\frac{1 - \\delta}{\\mu},\\ ] ] where @xmath87 is a purely numerical constant .",
    "consider the event @xmath88 and let @xmath2 be a @xmath89 matrix with entries being i.i.d .",
    "centered gaussian random variables with variance @xmath82 .",
    "then , with probability larger than @xmath90,\\ ] ] the vector @xmath3 is exactly reconstructed by @xmath58 .",
    "theorem  [ thm : b ] gives an explicit condition , linking the incoherency constant @xmath81 , the restricted isometry constant @xmath80 , and the constant @xmath91 from condition @xmath92 on the weights @xmath32 that ensures the exact reconstruction of @xmath3 using @xmath41 .",
    "this is the first result of this nature for weighted basis pursuit .",
    "when @xmath93 then @xmath94 holds with @xmath95 , so that one can take @xmath96 and @xmath97 .",
    "this is the case for @xmath98 when @xmath48 .",
    "this condition is also satisfied when the weights vector @xmath32 is close enough to @xmath99 and when the absolute value of the non - zero coordinates of @xmath99 are sufficiently large .",
    "for instance , @xmath94 holds when @xmath100 indeed , if we denote @xmath101 then @xmath94 follows from   since @xmath102 and @xmath103 in particular , if @xmath92 is satisfied with @xmath104 , for some constant @xmath105 , then a proportional to @xmath10 number of gaussian measurements will be enough to get @xmath106 with a large probability .    in figure",
    "[ fig : a0-verif ] below , we give an empirical illustration of the fact that @xmath92 is indeed a relevant condition for exact reconstruction of weighted basis - pursuit .",
    "we consider exactly the same experiment as what we did in section  [ sec : empirical - evidence ] , but this time we fix the number of measurements to @xmath107 and the sparsity of @xmath3 to @xmath108 . for this combination of @xmath7 and @xmath10 ,",
    "the phase transition occurs , namely basis pursuit can either work or not , see figure  [ fig : phase - cs ] , so we can expect for these values a strong improvement of weighted basis - pursuit over non - weighted one .",
    "on the left - side of figure  [ fig : a0-verif ] , we show the value of the constant @xmath91 over the reweighting iterations .",
    "namely , if @xmath77 is the support of the true unknown vector @xmath3 , we compute for @xmath109 the values of @xmath110 where @xmath111 over the 10 repetitions ( differentiated by different colors ) , where we recall that @xmath63 is given by   and where we choose @xmath112 . on the right - side of figure",
    "[ fig : a0-verif ] , we show the logarithm of relative reconstruction errors over the iterations , namely @xmath113 ( we take the logarithm only for illustrational purpose , so that we can see the cases when exact reconstructions occurs ) . each repetition of the experiment is represented with a different color .",
    "what we observe is a direct correspondence between the constant @xmath91 from assumption @xmath92 and the quality of reconstruction of weighted basis pursuit along the iterations .",
    "this tells that assumption @xmath92 indeed explains ( at least in the considered configuration ) when exact reconstruction can or can not happen using weighted basis pursuit .     from assumption  @xmath92 ( left ) and logarithm of the relative reconstruction error of weighted basis pursuit over the iterations ( right).,title=\"fig : \" ]   from assumption  @xmath92 ( left ) and logarithm of the relative reconstruction error of weighted basis pursuit over the iterations ( right).,title=\"fig : \" ]    note that uniform results can also be derived for the weighted-@xmath13 algorithm .",
    "indeed , by using classical machinery , it can be proved that 1 ) implies 2 ) implies 3 ) where :    1 .   for all @xmath114",
    ", @xmath115 satisfies @xmath116 and @xmath117 , 2 .",
    "@xmath118 and @xmath119 , 3 .   for any @xmath120 .",
    "but , it is not clear why , for instance when @xmath121 , it would be easier for the matrix @xmath122 to satisfy @xmath123 than for @xmath2 itself .",
    "the same remark also holds for the euclidean section of @xmath124 by the kernel of @xmath125 or @xmath2 .",
    "these approaches look too crude to perform a study of @xmath13-weighted algorithms , where most of the gain can be done only on the absolute multiplying constant in front of the minimal number of measurements @xmath7 needed for exact reconstruction .",
    "thanks to theorem  [ thm : a ] , it is easy to test if we were able to reconstruct exactly a vector @xmath3 given @xmath5 .",
    "so far , we have to rely on the theory to insure that with a high probability , we have @xmath20 . using  , we can verify this belief .",
    "indeed , theorem  [ thm : a ] entails that @xmath126 when @xmath20 .",
    "in particular , if @xmath127 , then we are sure that we did nt perform the exact reconstruction of @xmath3 using @xmath17 . then , we can iterate the mechanism and define for any @xmath128 @xmath129 leading to a sequence @xmath130 if the sequence   does not become constant after a certain number of iterations , then it is very likely that none of the algorithm @xmath131 reconstructed exactly @xmath3 .",
    "we also have the following reverse statement .",
    "denote by @xmath132 the set of all @xmath133-sparse vectors in @xmath4 .",
    "[ thm : c ] let @xmath2 be a @xmath1 injective matrix on @xmath134 and let @xmath135 .",
    "the following statements are equivalent :    1 .",
    "there exists an integer @xmath136 such that @xmath137 , 2 .",
    "the sequence @xmath138 becomes constantly equal to a @xmath139-sparse vector after a certain number of iterations .",
    "note that the matrix with i.i.d .",
    "standard gaussian entries is injective on @xmath134 with probability one .",
    "thus , we propose to compute the sequence   as an empirical test for the exact reconstruction of a vector @xmath3 from @xmath5 .",
    "in many applications , data can be represented as a database with missing entries .",
    "the problem is then to fill the missing values of the database , leading to the so - called  _ matrix completion _ problem .",
    "for instance , collaborative filtering aims at doing automatic predictions of the taste of users , using the collected tastes of every users at the same time  @xcite .",
    "the popular netflix prize is a popular application of this problem .",
    "other applications include machine - learning  @xcite , control  @xcite , quantum state tomography  @xcite , structure from motion  @xcite , among many others",
    ". this problem can be understood as a non - commutative extension of the compressed sensing problem .",
    "so , a natural question is the following : _ does the principle of iterative weighting of the @xmath13-norm work also for matrix completion ? _ in this section , we prove empirically that the answer to this question is yes .",
    "we prove that one can improve the convex relaxation principle for matrices , which is based on the nuclear norm @xcite ,  @xcite , by using a weighted nuclear norm , in the same way as we did for vectors in section  [ sec : cs - vectors ] .",
    "however , note that there is , as explained below , a major difference between the vectors and matrices cases at this point , since a weighted nuclear norm is not convex in general , while a weighted @xmath13-norm is .",
    "let us first recall standard definitions and notations . let @xmath140 be a matrix with @xmath141 rows and @xmath142 columns .",
    "the matrix @xmath143 is not fully observed .",
    "what we observe is a given subset @xmath144 of cardinality @xmath7 of the entries of @xmath143 , where @xmath145 . for any matrix @xmath146",
    ", we define the _ masking _ operator @xmath147 such that @xmath148 when @xmath149 and @xmath150 when @xmath151 .",
    "we define also @xmath152 .    since we consider the case where @xmath153 , the matrix completion problem is in general severely ill - posed .",
    "so , one needs to impose a complexity or sparsity assumption on the unknown matrix @xmath143 .",
    "this is done by assuming that @xmath143 has low rank , which is the natural extension of the sparsity assumption for vectors to the spectrum of a matrix . for the problem of exact reconstruction ,",
    "other geometrical assumptions are necessary ( such as the incoherency assumption , see @xcite ) . under such assumptions ,",
    "it is now well - understood that the principle of convex relaxation of the rank function is able to reconstruct exactly the unknown matrix from few measurements , see @xcite .",
    "indeed , a natural approach would be to solve the problem @xmath154 but this minimization problem is known to be very hard to solve in practice even for small matrices , see for instance @xcite .",
    "the convex envelope of the rank function over the unit ball of the operator norm is the nuclear norm , see  @xcite , which is given by @xmath155 ( it is the bi - conjugate of the rank function over the unit ball of the operator norm ) , where @xmath156 are the singular values of @xmath2 in decreasing order .",
    "so , the convex relaxation of   is @xmath157 this problem has received a lot of attention quite recently , see @xcite , among many others .",
    "the point is that , in the same way as the basis pursuit for vectors ,   is able to recover exactly @xmath143 with a large probability , based on an almost minimal number of samples ( under some geometrical assumption ) .    in literature",
    "concerned about computational problems @xcite , @xcite , @xcite , among others , the relaxed version of   is considered , since it is easier to construct a solver for it ( one can apply generic first - order optimal methods , such as proximal forward - backward splitting @xcite , among many other methods ) and since it is more stable in the presence of noise . note that the svt algorithm of  @xcite gives a solution under equality constraints for an objective function with an extra ridge term @xmath158 .",
    "the relaxed problem is simply formulated as penalized least - squares : @xmath159 where @xmath160 is a parameter balancing goodness - of - fit and complexity , measured by the nuclear norm .",
    "before we go on , we need some notations .",
    "the vector of singular values of @xmath2 is denoted by @xmath161 , sorted in non - increasing order , where @xmath136 is the rank of @xmath2 .",
    "we define , for @xmath162 , the @xmath163-schatten norm by @xmath164 which is the @xmath22 norm of @xmath165 .",
    "we shall denote also by @xmath166 the operator norm of @xmath2 , and note that @xmath167 is the frobenius norm , associated to the euclidean inner product @xmath168 , where @xmath169 stands for the trace of @xmath2 . for any matrix @xmath2 its singular values decomposition ( svd )",
    "writes as @xmath170 , where @xmath171 is the diagonal matrix with @xmath165 on its diagonal , and @xmath172 and @xmath173 are , respectively @xmath174 and @xmath175 orthonormal matrices .",
    "we have in mind to do the same as we did in section  [ sec : cs - vectors ] for the reconstruction of sparse vectors . for a given weight vector @xmath176 , with @xmath177 , we consider @xmath178 where @xmath179 is the weighted nuclear - norm @xmath180 with the convention @xmath181 .",
    "now , we would like to use the idea of reweighting using previous estimates , in the same as we did in section  [ sec : cs - vectors ] : if @xmath182 is a solution to  , we want to use for instance @xmath183 and find a solution to the problem   for this choice of weights .",
    "but , let us stress the fact that , while we call @xmath184 the weighted nuclear norm , it is not a norm , since it is not a convex function in general !",
    "a simple counter - example is as follows .",
    "if @xmath185 ( which is usually the case since singular values are taken in a non - increasing order ) then for @xmath186 and @xmath187 , we have @xmath188 hence @xmath184 is not convex .",
    "moreover , since the aim of @xmath189 is to promote low - rank matrices , the weight vector @xmath32 should be chosen non - increasing , corresponding precisely to the case where @xmath189 is non - convex ( note that when @xmath190 , it is easy to prove that @xmath184 is a norm ) . consequently ,   is not a convex minimization problem in general , and a minimization algorithm is very likely to be stuck at a local minimum .",
    "but we would like to stick to the idea of reweighting , since it worked well for cs .",
    "the first idea that may come to mind is to use a convex relaxation of the non - convex function @xmath189 ( just as convex relaxation of the rank function led to the nuclear norm ) , but it simply leads back to the nuclear norm itself ! indeed , it can be proved that if @xmath191 , the convex envelope of @xmath184 on the ball @xmath192 is simply @xmath193 .",
    "let us go back to the original problem  .",
    "it turns out that   is equivalent to the fact that @xmath182 satisfies the following fixed - point equation : @xmath194 where @xmath195 is the spectral soft - thresholding operator defined for every @xmath196 by @xmath197 where @xmath198 is the svd of @xmath199 , with @xmath200 .",
    "this fact is easily explained .",
    "indeed , define @xmath201 , which is a differentiable function with gradient @xmath202 and @xmath203 , which is a non - differentiable convex function .",
    "we will denote by @xmath204 the subdifferential of @xmath205 at @xmath2 . the fact that @xmath206 is equivalent to the fact that @xmath207 ( for the minkowskii s addition of sets ) , that we rewrite in the following way : @xmath208 on the other hand , a standard tool in convex analysis is the _ proximal _ operator , @xcite ,  @xcite .",
    "the proximal operator of a convex function , for instance @xmath205 , is given , for every @xmath196 , by @xmath209 the minimizer being unique since @xmath210 is strongly convex .",
    "but , since @xmath211 , the point @xmath212 is uniquely determined by the inclusion @xmath213 so , choosing @xmath214 in   and identifying with   leads to the fact that @xmath215 satisfies the fixed - point equation @xmath216 which leads to   on this particular case , since we know that @xmath217 ( see proposition  [ prop : weighted - nuclear - prox ] below ) .",
    "note that the same argument proves that , if we add a ridge term to the nuclear norm penalization , namely @xmath218 for any @xmath219 , then and equivalent formulation is the fixed point equation @xmath220 and the minimizer is unique this time , since the objective function is now strongly convex .",
    "the argument given above is at the core of the proximal operator theory , and leads to the so - called proximal forward - backward splitting algorithms , see  @xcite and  @xcite .",
    "since these algorithm are optimal among the class of first - order algorithms , they drawn a large attention in the machine learning community , see for instance the survey  @xcite .",
    "another advantage in the case of matrix completion is that such an algorithm can handle large scale matrices , see remark  [ rem : large - scale ] below .",
    "so , we have seen that   and  , or   and   are equivalent formulations of the same problem .",
    "so , instead of considering  , we could consider the corresponding fixed - point problem .",
    "unfortunately , since @xmath189 is non - convex , the above arguments based on the subdifferential does not make sense anymore .",
    "but still , we can consider an estimator defined as a fixed point equation for the weighted soft - thresholding operator .",
    "[ prop : existence - and - unicity ] assume that @xmath221 and @xmath222 .",
    "let us define the matrix @xmath223 as the solution of the fixed - point equation @xmath224 where @xmath225 is the weighted soft - thresholding operator given by @xmath226 where @xmath227 is the svd of @xmath199 .",
    "then , the solution to   exists and is unique .",
    "theorem  [ prop : existence - and - unicity ] is proved in section  [ sec : proofs - cs - matrices ] below , and is a by - product of our analysis of the iterative scheme to approximate the solution of  .",
    "the parameter @xmath221 can be arbitrarily small ( in our numerical experiments we take it equal to zero , see section  [ sec : mc - numerical - study ] ) , but it ensures unicity and convergence of the iterative scheme proposed below . once again , let us stress the fact that   ( with @xmath228 ) is not equivalent to   in general , since @xmath229 is not convex .",
    "the consideration of   has several advantages : we guarantee unicity of the solution , while the problem   may have several solutions , and it is easy to solve the fixed - point problem   using iterations . even further , from a numerical point of view , it can be easily used together with a continuation algorithm , as explained in section  [ sec : mc - numerical - study ] below , to compute a set of solutions for several values of the smoothing parameter @xmath230 .",
    "the next theorem proves that iterates of the fixed - point equation   converges exponentially fast to the solution .",
    "[ thm : algorithm_convergence ] take @xmath231 as the matrix with zero entries and define for any @xmath232 : @xmath233 then , for any @xmath234 , one has : @xmath235 where @xmath236 is the solution of  .",
    "the proof of theorem  [ thm : algorithm_convergence ] is given in section  [ sec : proofs - cs - matrices ] .",
    "the main step of the proof is to establish the lipshitz property of the weighted soft - thresholding operator , see proposition  [ prop : s - w - lipshitz ]",
    ". since @xmath237 is not a proximal operator ( the objective function is not convex ) , we can not use directly the property of firm - expansivity , which is a direct consequence of the definition of a proximal operator , see the discussion in section  [ sec : proofs - cs - matrices ] .        in this section",
    "we compare empirically the quality of reconstruction using nuclear norm minimization   ( nnm ) , or equivalently  , and weighted spectral soft - thresholding   ( wsst ) . to compute the nnm we use the accelerated proximal gradient ( apg ) algorithm of  @xcite using the ` matlab ` package  ` nnls ` , which is a state - of - the - art solver for the minimization problem  .",
    "this algorithm is based on an accelerated proximal gradient algorithm , itself based on the accelerated gradient of nesterov , see  @xcite and the fista algorithm , see @xcite and see also  @xcite for a similar algorithm . in the apg algorithm , we use the linesearch and the continuation techniques , see @xcite , but we do nt use truncation , since it led to poor results in the problems considered here .",
    "the target value of @xmath230 for nnm and wsst ( see   and  ) is simply taken as @xmath238 , with @xmath239 or @xmath240 depending on the problem , see below .",
    "the solution coming out of the apg algorithm is denoted by @xmath241 .",
    "note that we could have used the fpc  @xcite or svt  @xcite algorithms instead , but it led in our experiments to poorer results compared to the apg ( in particular when looking for solutions with a rank of order , say , 100 on `` real '' matrices , like in the inpainting or recommanding systems , see below ) .",
    "the wsst is computed following the algorithm  [ alg : wsst ] below .",
    "the first while loop is a continuation loop , that goes progressively to @xmath242 .",
    "doing this instead of using @xmath242 directly is known to improve stability and rate of convergence of the algorithm .",
    "it does not take more time than using @xmath242 directly ( actually , it usually takes less time ) , since we use warm starts : when taking a smaller @xmath230 , we use the previous value @xmath243 ( the solution with the previous @xmath230 ) as a starting point .",
    "once we reached @xmath242 , we obtain a first solution of the fixed point problem  , denoted by @xmath244 .",
    "then , we update the weights by taking @xmath245 , and we start all over .",
    "we do nt use a continuation loop again , since we are already at the desired value of @xmath230 .",
    "we keep the parameter @xmath230 fixed , we only repeat the process of updating the weights and finding the solution to the fixed point   @xmath246 times . by doing this , we are typically going to decrease ( eventually a lot ) the final rank of the wsst , while keeping a good reconstruction accuracy .",
    "this process of updating the weights is usually not long .",
    "typically , after a small number of iterations , two fixed - point solutions before and after an update are very close , so that our choice @xmath247 is typically too large , but we keep it this way to ensure a good stability of the final solution",
    ".    note that in algorithm  [ alg : wsst ] we use the iterations   with @xmath248 , since it gives satisfactory results .",
    "we use a simple stopping rule @xmath249 with @xmath250 or @xmath251 depending on the scaling of the problem , see below .",
    "we used in all our computations @xmath252 and @xmath253 . for a fair comparison",
    ", we always use , for a reconstruction problem , the same parameters @xmath254 and @xmath230 for both nnm and wsst .",
    "of course , for the wsst we need to rescale @xmath230 by multiplying it by @xmath255 ( the first coordinate of the weights vector , which is equal to @xmath256 at the first iteration ) .",
    "[ rem : large - scale ] a good point with wsst is that it can handle large scale matrices , since at each iteration one only needs to store @xmath257 , which is a low rank matrix ( coming out of a previous spectral soft - thresholding ) and @xmath258 , which is a sparse matrix .",
    "the overall computational cost of wsst is obviously much longer than the one of nnm , since we use @xmath246 iterations , and since we do nt use accelerated gradient , linesearch and other accelerating recipes in our implementation of wsst .",
    "this is done purposely : we want to compare the quality of reconstruction of the `` pure '' wsst , without helping computational tricks , that usually improves rate of convergence , but accuracy of reconstruction as well ( this is the case if one compares nnm with and without these tools ) .",
    "put @xmath259 , @xmath260 and take @xmath261    put @xmath262    @xmath263      in figure  [ fig : phase - transision ] , we give a first empirical evidence of the fact that wsst improves a lot upon nnm .",
    "for each @xmath264 , we repeat the following experiment 50 times .",
    "we draw at random @xmath172 and @xmath173 as @xmath265 matrices with @xmath25 i.i.d entries , and put @xmath266 ( which is rank @xmath136 a.s . ) .",
    "then , we choose uniformly at random @xmath267 of the entries of @xmath143 , and compute the nnm and the wsst based on this matrix . in figure  [ fig : phase - transision ] , we show , for each @xmath136 ( x - axis ) , the boxplots of the relative reconstruction errors @xmath268 over the 50 repetitions for @xmath269 nnm ( top - left ) and @xmath270 = wsst ( top - right ) . on this example , we observe that nnm is not able to recover matrices with a rank larger than 35 , while wsst can recover matrices with a rank up to 70 .",
    "the boxplots of the ranks recovered by nnm and wsst are on the second line , where we observe that wsst always recovers the true rank up to a rank of order @xmath271 , while nnm correctly recovers the rank ( only most of the time ) up to a rank @xmath272 , and overestimates it a lot for larger ranks .",
    "so , on this simulated example , we observe a serious improvement of nnm using wsst , since the latter has the exact reconstruction property for matrices with twice a larger rank ( @xmath271 instead of @xmath272 ) .",
    "rank @xmath136 matrix with @xmath136 between @xmath273 and @xmath274 ( x - axis),title=\"fig : \" ]   rank @xmath136 matrix with @xmath136 between @xmath273 and @xmath274 ( x - axis),title=\"fig : \" ] +   rank @xmath136 matrix with @xmath136 between @xmath273 and @xmath274 ( x - axis),title=\"fig : \" ]   rank @xmath136 matrix with @xmath136 between @xmath273 and @xmath274 ( x - axis),title=\"fig : \" ]      in figure  [ fig : inpainting - examples ] , we consider the reconstruction of four test images ( `` lenna '' , `` fingerprint '' , `` flinstones '' and `` boat '' ) . each test image has @xmath275 pixels , and is of rank @xmath276 .",
    "we only observe @xmath267 of the pixels , picked uniformly at random , with no noise .",
    "the observations are given in the first line of figure  [ fig : inpainting - examples ] , where non - observed pixels are represented by white .",
    "the second line gives the reconstruction obtained using nnm .",
    "the third line shows the difference between the true image and the recovery by nnm , where blue is perfect recovery and red is bad recovery .",
    "the fourth line shows the reconstruction using wsst and the fifth shows the difference between the true image and recovery by wsst .    on all four images ,",
    "the recovery is much better using wsst , in particular on the fingerprint and flinstones images .",
    "this can be understood form the fact that these two are very structured images .",
    "the most surprising fact is that all the four reconstructions using nnm have rank 150 ( because of the way we choose @xmath230 , see above ) , while the rank of the reconstructions obtained with wsst is never more than 90 ( with the same choice of @xmath230 ) .",
    "so , wsst leads to simpler ( with a lower rank , which is better in terms of compression / description ) and more accurate reconstructions .",
    "in particular , we observe that wsst is able to recover in a more precise way the underlying geometry of the true images ( for instance , on the third line , first column , we can recognize the shape of lenna , while this is not the case with wsst ) .",
    "+         +         +         +         +      now , we consider matrix completion for a real dataset : the movielens data .",
    "it contains 3 datasets , available on ` http://www.grouplens.org/ ` :    * ` movie-100k : ` 100,000 ratings for 1682 movies by 943 users * ` movie-1 m : ` 1 million ratings for 3900 movies by 6040 users * ` movie-10 m : ` 10 million ratings and 100,000 tags for 10681 movies by 71567 users    the ranks of the users are integers between @xmath277 and @xmath273 . in each 3 datasets ,",
    "each user has rated at least @xmath278 movies . for our experiments , we simply choose uniformly at random half of the ratings of each user to form a subset @xmath279 of the entire subset @xmath280 or ratings . then , based on the ratings in @xmath279 , we try to predict the ratings in @xmath281 .",
    "since many entries are missing , we measure the accuracy of completion by computing the relative error in @xmath281 . if @xmath270 is a reconstruction matrix , we reproduce in table  [ tab : collaborative ] below the values of @xmath282 together with the rank used for the reconstruction .",
    "we observe in table  [ tab : collaborative ] that wsst improves a lot upon nnm on each datasets .",
    "the most surprising fact is that the rank used by wsst is much smaller than the one used by nnm , while leading at the same time to strong prediction improvements . for ` movie-1 m ` for instance , the prediction error of wsst is @xmath267 better than nnm , while nnm solution has rank 200 and the wsst has rank 40 .",
    "once again , we can conclude on this example that wsst gives both much simpler reconstructions , and better prediction accuracy .",
    "note that we considered a maximum rank equal to 200 for the ` movie-100k ` and ` movie-1 m ` datasets , and equal to 50 for ` movie-10 m ` ( to make this problem computationally tractable on a normal computer ) .",
    "lcccccc + & & & & + & @xmath283 & @xmath7 & & + & & & nnm & wsst & nnm & wsst + ` movie-100k : ` & 943/1682 & 1.00e+5 & 3.92e-01 & 3.30e-01 & 128 & 33 + ` movie-1 m : ` & 6040/3702 & 1.00e+6 & 3.83e-01 & 2.70e-01 & 200 & 40 + ` movie-10 m : ` & 71567/10674 & 9.91e+6 & 2.76e-01 & 2.36e-01 & 50 & 5 +",
    "we denote by @xmath284 the space @xmath285 endowed with the @xmath22 norm .",
    "the unit ball there is denoted by @xmath286 .",
    "we also denote the unit euclidean sphere in @xmath285 by @xmath287 .",
    "we denote by @xmath288 the canonical basis of @xmath4 and for any @xmath289 denote by @xmath290 the subspace of @xmath4 spanned by @xmath291 .",
    "let @xmath292 $ ] be a matrix from @xmath4 to @xmath293 , where @xmath294 denotes the @xmath295-th column vector of @xmath2 .",
    "let @xmath47 and @xmath77 an arbitrary subset of @xmath296 .",
    "we define @xmath297 $ ] the matrix from @xmath298 to @xmath293 with columns vectors @xmath294 for @xmath299 .",
    "we denote by @xmath300 the vector in @xmath298 with coordinates @xmath301 for @xmath299 , where @xmath301 is the @xmath295-th coordinate of @xmath3 .",
    "we denote by @xmath302 the vector of @xmath4 such that @xmath303 when @xmath304 and @xmath305 when @xmath299 .",
    "if @xmath74 has non negative coordinates , we denote by @xmath306 the vector @xmath307 and by @xmath308 the vector @xmath309 with the previous convention in case where @xmath310 for some @xmath295 .",
    "we denote by @xmath99 the vector @xmath311 .",
    "the support of @xmath3 is denoted by @xmath312 , this is the set of all @xmath313 such that @xmath314 .",
    "we also consider the @xmath32-weighted @xmath315-norm @xmath316 note that @xmath317 is a norm only when restricted to @xmath318 , where @xmath31 is the support of @xmath32 .",
    "it follows from   that , under each one of the three conditions , we have @xmath324 . therefore , to simply notations , we can work as if the ambient space were @xmath318 .",
    "hence , without loss of generality , we assume that @xmath328 .",
    "we also denote by @xmath329 the support of @xmath3 .    [ point  2 .",
    "entails point  1 .",
    "] using standard arguments ( see for instance  @xcite ) , we can see that the subgradient of @xmath317 at @xmath47 is the set @xmath330 using the definition of the subgradient of @xmath317 at @xmath3 , it follows that for any @xmath331 , @xmath332 thus , if point  2 holds then for any @xmath333 such that @xmath334 , @xmath335 and thus point  1 is satisfied .",
    "[ point  1 . entails point  3 . ]",
    "this follows from classical results on the minimization of a convex function over a convex set ( cf .  @xcite ) .",
    "nevertheless , we provide a direct proof following the argument of @xcite .",
    "denote by @xmath342 the canonical basis in @xmath4 and by @xmath343 the unit ball associated to the @xmath32-weighted @xmath315-norm : @xmath344 if @xmath3 is the unique solution of   then @xmath345 . then by a duality argument ( for instance hahn - banach theorem for the separation of convex sets )",
    ", there exists @xmath346 such that @xmath347 , where @xmath348 and @xmath349 , where @xmath350 .",
    "introduce @xmath351 , the face of @xmath352 containing @xmath3 . by moving the hyperplan @xmath353",
    ", we can assume that @xmath354 .",
    "since @xmath355 , we have @xmath356 thus @xmath357",
    ". moreover , @xmath358 so @xmath359 because @xmath360 .",
    "this is the equality case in hlder s inequality , so it follows that @xmath361 .",
    "then , for any @xmath40 , @xmath362 , thus @xmath363 and @xmath364 , so @xmath365 thus @xmath366 .",
    "that is , @xmath367 .",
    "finally , for any @xmath368 , @xmath369 , thus @xmath370 and @xmath371 .",
    "then , we normalize @xmath372 by @xmath373 to obtain point  3 .",
    "both criterions  2 and  3 in proposition  [ prop : equivalence - reconstruction - exacte ] can be used to characterize the exact reconstruction of a vector @xmath3 by the @xmath13-weighted algorithm .",
    "the vector @xmath372 of criterion  3 is now called an _ exact dual certificate _ ( cf .",
    "we will use criterion  3 and the construction of an exact dual certificate from  @xcite to prove theorems  [ thm : a ] and  [ thm : b ] . note that criterion  2 together with the construction of an _ inexact dual certificate _ ( cf .",
    "@xcite ) can also be used .",
    "nevertheless , we do not present this construction here since it does not improve the statement of theorem  [ thm : b ] .",
    "in the same way as we did in the proof of proposition  [ prop : equivalence - reconstruction - exacte ] , we can work as if the ambient space were @xmath318 and assume , without loss of generality , that @xmath328 .",
    "we denote by @xmath77 the support of @xmath3 .",
    "we prove first that when @xmath20 , then @xmath374 is injective . indeed , suppose that there exists some @xmath375 such that @xmath376 and @xmath377 .",
    "denote by @xmath378 the vector such that @xmath379 and @xmath380 .",
    "we have @xmath381 and @xmath382 .",
    "in particular , for any @xmath383 , @xmath384 .",
    "therefore , since @xmath3 is the unique solution of the basis pursuit algorithm , it follows from point  2 of proposition  [ prop : equivalence - reconstruction - exacte ] ( applied to the weight vector @xmath385 ) , that , for every @xmath386 , @xmath387 this is not possible , so @xmath374 is injective .",
    "since @xmath48 , the decoder @xmath46 is given here by @xmath388 therefore , according to  , we have @xmath389 for any @xmath40 , that is @xmath390 .",
    "as a consequence @xmath391 and @xmath374 is injective thus , @xmath392 .",
    "since @xmath393 , we have @xmath394 .",
    "we adapt to our setup the `` dual certificate '' introduced in  @xcite and consider @xmath395 in particular , we have @xmath396 and @xmath397 thus , we have @xmath398 . in view of proposition",
    "[ prop : equivalence - reconstruction - exacte ] , it only remains to prove that @xmath399 with high probability .",
    "for @xmath85 and @xmath400 , we consider the events @xmath401 and @xmath402 first , note that since @xmath403 is hermitian , we have @xmath404 thus , on @xmath405 , we have @xmath406 and so for any @xmath407 , @xmath408 . in particular , @xmath409 then , it follows that , on @xmath410 and under condition @xmath411 , @xmath412 then , theorem  [ thm : b ] follows from the probability estimates of @xmath413 provided in the next lemma .",
    "[ lem : proba - estimates ] let @xmath414 be a @xmath89 matrix where the @xmath415 s are i.i.d .",
    "standard gaussian variables .",
    "assume that @xmath416.\\ ] ] with probability larger than @xmath417 , we have @xmath418 and @xmath419    for the sake of completeness , we recall here the classical @xmath56-net argument to prove the first statement of lemma  [ lem : proba - estimates ] .",
    "it is enough to prove that @xmath420 , where @xmath421 is the set of unit vectors of @xmath422 supported on @xmath77 .",
    "first , note that @xmath423 where @xmath424 is the symmetric operator @xmath425 .",
    "let @xmath426 be a @xmath427-net of @xmath428 for the @xmath429 metric with a cardinality smaller than @xmath430 ( the existence of such a net follows from a volumetric argument , see  @xcite ) .",
    "for any @xmath431 , there exists @xmath432 such that @xmath433 with @xmath434 and therefore , @xmath435 hence , @xmath436 , and it is enough to control the supremum of @xmath437 over @xmath438 instead of @xmath421 .",
    "let @xmath439 .",
    "we denote by @xmath440 the row vectors of @xmath2 where @xmath441 are @xmath7 independent standard gaussian vectors of @xmath4 .",
    "we have @xmath442 .",
    "since @xmath443 , it follows from bernstein inequality for @xmath444 random variables  @xcite that @xmath445 \\geq 1 - 2\\exp(-c_1 m\\delta^2),\\ ] ] and a union bound yields @xmath446 \\geq      1 - 2\\exp ( s \\log 9 - c_1 m \\delta^2).\\ ] ] combining the @xmath56-net argument with this probability estimate we obtain that when @xmath447 then @xmath448 with probability at least @xmath449 .",
    "now , we turn to the second part of the statement .",
    "let @xmath450 .",
    "the @xmath295-th column vector of @xmath2 is @xmath451 where the @xmath452 s are independent standard gaussian vectors of @xmath293 .",
    "let @xmath453 to be chosen later . by markov inequality , @xmath454={\\mathbb p}\\big[\\big|\\sum_{j=1}^m g_{ij}g_{ji}\\big|_2\\geq      m\\mu\\big]\\leq ( m\\mu)^{-q}{\\mathbb{e}}\\big|\\sum_{j=1}^mg_{ij}g_{ji}\\big|_2^q.\\ ] ] now , we use the vectorial version of khintchine inequality conditionally to @xmath455 , to obtain , for some absolute constant @xmath456 , @xmath457",
    "it follows that @xmath458 hence , in ( [ eq : markov - theob ] ) for @xmath459 , we obtain @xmath460\\leq      \\exp\\big(-\\frac{\\mu^2 m \\log 2}{s(2c_4 ^ 2)^2}\\big).\\ ] ] the result follows now from an union bound .",
    "assume that @xmath461 and define @xmath462 .",
    "by construction of @xmath8 , we have @xmath463 and @xmath464 .",
    "so , since @xmath2 is injective on @xmath134 and @xmath465 , we have @xmath466 .",
    "this proves that @xmath467 , and that the sequence @xmath468 is constant and equal to a @xmath139-sparse vector starting from the @xmath136-th iteration .",
    "the next proposition shows that weighted spectral soft - thresholding achieves the minimum of the weighted nuclear norm plus a proximity term .",
    "note that , however , weighted spectral soft - thresholding is not a proximal operator , since the weighted nuclear norm is not convex .",
    "this entails in particular that the proofs below use a direct analysis , since we can not use arguments based on subdifferential computations here .",
    "[ prop : weighted - nuclear - prox ] let @xmath472 , @xmath473 and @xmath474 .",
    "then the minimization problem @xmath475 has a unique solution , given by @xmath476 , where @xmath477 is the weighted soft - thresholding operator  .",
    "denote for short @xmath478 and write the svd of @xmath2 as @xmath479 where @xmath480 $ ] , @xmath481 $ ] and @xmath482 .",
    "we have @xmath483 so that we want to minimize the function @xmath484 over @xmath485 with the constraints @xmath486 , @xmath487 and @xmath488 .",
    "using the variational characterization of singular values , if @xmath489 is the svd of @xmath199 , where @xmath490 $ ] , @xmath491 $ ] , @xmath492 , we know that the maximum of @xmath493 over all vectors @xmath494 and @xmath495 subject to @xmath496 and @xmath494 orthogonal to @xmath497 and @xmath495 orthogonal to @xmath498 is achieved at @xmath499 and @xmath500 , and is equal to @xmath501 .",
    "so the maximum of @xmath502 is achieved at @xmath503 and @xmath504 , and @xmath505 it is easy to see that for each @xmath506 the the minimum over @xmath507 is achieved at @xmath508 , which is non - increasing .    as mentioned before",
    ", @xmath237 is not a proximal operator .",
    "a nice property about proximal operators is that they are firmly non - expansive , see @xcite .",
    "namely , if @xmath509 is the proximal operator of some convex function over an hilbert space @xmath510 , then we have @xmath511 for any @xmath512 .",
    "however , it turns out that we can prove , using a direct analysis , that @xmath237 is non - expansive .",
    "once again , the proof uses a direct and technical analysis ( since we can not use arguments based on subdifferential computations ) , while the property of firm - nonexpansivity of proximal operators is an easy consequence of their definition .",
    "let us assume without loss of generality that @xmath516 . write the svd of @xmath2 and @xmath199 as @xmath517 and @xmath518 where @xmath519 $ ] , @xmath520 $ ] and @xmath521 ( resp .",
    "@xmath522 ) stands for the rank of @xmath2 ( resp .",
    "@xmath199 ) .",
    "we also write for short @xmath523 and @xmath524 where @xmath525 $ ] and @xmath526 $ ] .",
    "we want to prove that @xmath527 .",
    "first use the decomposition @xmath528 where we take @xmath529 such that @xmath530 for @xmath531 and @xmath532 for @xmath533 , and similarly for @xmath534 .",
    "we decompose @xmath535 using von neumann s trace inequality @xmath536 ( see for instance @xcite , section  7.4.13 ) , it follows for the first term of   that @xmath537 using the same argument for the two other terms of  , we obtain @xmath538 we explore the case @xmath539 and @xmath540 ; the other cases follow the same argument .",
    "we have @xmath541 so , an easy computation leads to @xmath542 we obviously have @xmath543 . by definition of @xmath534 and @xmath529",
    ", we have @xmath544 for any @xmath545 .",
    "hence , we have @xmath546 which concludes the proof of proposition  [ prop : s - w - lipshitz ] .",
    "consider the sequence @xmath547 defined in  .",
    "using proposition  [ prop : s - w - lipshitz ] we have for any @xmath548 @xmath549 so that @xmath550 this proves that @xmath551 , so the limit of @xmath547 exists and is given by @xmath552 now , by continuity of @xmath237 and @xmath553 , taking the limit on both sides of  , we obtain that @xmath554 satisfies the fixed - point equation @xmath555 so we have found at least one solution .",
    "let us show now that it is unique , so that @xmath556 : consider a matrix @xmath199 satisfying the same fixed point equation .",
    "we have @xmath557 therefore @xmath558 .",
    "geoffrey davis , stephane mallat , and zhifeng zhang .",
    "adaptive time - frequency approximations with matching pursuits . in _ wavelets : theory , algorithms , and applications ( taormina , 1993 ) _ , volume  5 of _ wavelet anal .",
    "_ , pages 271293 . academic press , san diego , ca , 1994 .",
    "shuiwang ji and jieping ye .",
    "an accelerated gradient method for trace norm minimization . in _ proceedings of the 26th annual international conference on machine learning _ , icml 09 , pages 457464 , new york , ny , usa , 2009 ."
  ],
  "abstract_text": [
    "<S> this paper is about iteratively reweighted basis - pursuit algorithms for compressed sensing and matrix completion problems . in a first part </S>",
    "<S> , we give a theoretical explanation of the fact that reweighted basis pursuit can improve a lot upon basis pursuit for exact recovery in compressed sensing . </S>",
    "<S> we exhibit a condition that links the accuracy of the weights to the rip and incoherency constants , which ensures exact recovery . in a second part </S>",
    "<S> , we introduce a new algorithm for matrix completion , based on the idea of iterative reweighting . </S>",
    "<S> since a weighted nuclear `` norm '' is typically non - convex , it can not be used easily as an objective function . </S>",
    "<S> so , we define a new estimator based on a fixed - point equation . we give empirical evidences of the fact that this new algorithm leads to strong improvements over nuclear norm minimization on simulated and real matrix completion problems </S>",
    "<S> . + _ keywords . _ </S>",
    "<S> compressed sensing ; weighted basis - pursuit ; matrix completion </S>"
  ]
}