{
  "article_text": [
    "when creating a classifier , a natural inclination is to only use variables that are obviously relevant since irrelevant variables typically decrease the accuracy of a classifier . on the other hand",
    ", this paper shows that the harm from irrelevant variables can be much less than the benefit from relevant variables and therefore it is possible to learn very accurate classifiers even when almost all of the variables are irrelevant",
    ". it can be advantageous to continue adding variables , even as their prospects for being relevant fade away .",
    " we show this with theoretical analysis and experiments using artificially generated data .",
    "    we provide an illustrative analysis that isolates the effects of relevant and irrelevant variables on a classifier s accuracy .",
    "we analyze the case in which variables complement one another , which we formalize using the common assumption of conditional independence given the class label .",
    "we focus on the situation where relatively few of the many variables are relevant , and the relevant variables are only weakly predictive . under these conditions , algorithms that cast a wide net can succeed while more selective algorithms fail .",
    "we prove upper bounds on the error rate of a very simple learning algorithm that may include many irrelevant variables in its hypothesis .",
    "we also prove a contrasting lower bound on the error of every learning algorithm that uses mostly relevant variables .",
    "the combination of these results show that the simple algorithm s error rate approaches zero in situations where every algorithm that predicts with mostly relevant variables has an error rate greater than a positive constant .    over the past decade or so , a number of empirical and theoretical findings have challenged the traditional rule of thumb described by @xcite as follows .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ one rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple ( say 5 or 10 ) of the number of adaptive parameters in the model .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the support vector machine literature ( see * ? ? ?",
    "* ) views algorithms that compute apparently complicated functions of a given set of variables as linear classifiers applied to an expanded , even infinite , set of features .",
    "these empirically perform well on test data , and theoretical accounts have been given for this .",
    "boosting and bagging algorithms also generalize well , despite combining large numbers of simple classifiers  even if the number of such `` base classifiers '' is much more than the number of training examples @xcite .",
    "this is despite the fact that @xcite showed the behavior of such classifiers is closely related to performing logistic regression on a potentially vast set of features ( one for each possible decision tree , for example ) .",
    "similar effects are sometimes found even when the features added are restricted to the original `` raw '' variables .",
    "figure  [ f : shrunkencentroids ] , which is reproduced from @xcite , is one example .",
    "the curve labelled `` te '' is the test - set error , and this error is plotted as a function of the number of features selected by the shrunken centroids algorithm .",
    "the best accuracy is obtained using a classifier that depends on the expression level of well over 1000 genes , despite the fact that there are only a few dozen training examples .",
    "( along the bottom).,title=\"fig:\",width=384 ] +    it is impossible to tell if most of the variables used by the most accurate classifier in figure  [ f : shrunkencentroids ] are irrelevant .",
    "however , we do know which variables are relevant and irrelevant in synthetic data ( and can generate as many test examples as desired ) .",
    "consider for the moment a simple algorithm applied to a simple source .",
    "each of two classes is equally likely , and there are @xmath1 relevant boolean variables , @xmath2 of which agree with the class label with probability @xmath3 , and @xmath2 which disagree with the class label with probability @xmath3 .",
    "another @xmath4 boolean variables are irrelevant .",
    "the algorithm is equally simple : it has a parameter @xmath5 , and outputs the majority vote over those features ( variables or their negations ) that agree with the class label on a @xmath6 fraction of the training examples .",
    "figure  [ f : synth ] plots three runs of this algorithm with @xmath7 training examples , and @xmath1 test examples .",
    "both the accuracy of the classifier and the fraction of relevant variables are plotted against the number of variables used in the model , for various values of @xmath5 .",
    "each time , the best accuracy is achieved when an overwhelming majority of the variables used in the model are irrelevant , and those models with few ( @xmath8 ) irrelevant variables perform far worse .",
    "furthermore , the best accuracy is obtained with a model that uses many more variables than there are training examples .",
    "also , accuracy over 90% is achieved even though there are few training examples and the correlation of the individual variables with the class label is weak .",
    "in fact , the number of examples is so small and the correlations are so weak that , for any individual feature , it is impossible to confidently tell whether or not the feature is relevant .",
    "assume classifier @xmath9 consists of a vote over @xmath10 variables that are conditionally independent given the class label .",
    "let @xmath11 of the variables agree with the class label with probability @xmath12 , and the remaining @xmath13 variables agree with the label with probability @xmath14 .",
    "then the probability that @xmath9 is incorrect is at most @xmath15 ( as shown in section  [ s : basic ] ) . the error bound decreases exponentially in the _ square _ of the number of relevant variables .",
    "the competing factor increases only _ linearly _ with the number of irrelevant variables .",
    "thus , a very accurate classifier can be obtained with a feature set consisting predominantly of irrelevant variables .    in section  [",
    "s : learning ] we consider learning from training data where the variables are conditionally independent given the class label .",
    "whereas equation  ( [ e : nolearn.intro ] ) bounded the error as a function of the number of variables @xmath10 and relevant variables @xmath11 in the _ model _ , we now use capital @xmath16 and capital @xmath17 for the total number of variables and number of relevant variables in the _ data_. the @xmath18 irrelevant variables are independent of the label , agreeing with it with probability @xmath14 .",
    "the @xmath17 relevant variables either agree with the label with probability @xmath12 or with probability @xmath19 .",
    "we analyze an algorithm that chooses a value @xmath20 and outputs a majority vote over all features that agree with the class label on at least @xmath21 of the training examples ( as before , each feature is either a variable or its negation ) .",
    "our theorem  [ t : learning.beta ] shows that if @xmath22 and the algorithm is given @xmath23 training examples , then the probability that it makes an incorrect prediction on an independent test example is at most @xmath24_+^2 }               { 1 + 8 ( n / k ) e^{-2 \\beta^2 m } + \\gamma }                    \\right ) \\right),\\ ] ] where @xmath25_+ { \\ensuremath{\\mathrel{\\stackrel{\\mathrm{def}}{=}}}}\\max \\ { z , 0 \\}$ ] .",
    "( throughout the paper , the `` big oh '' and other asymptotic notation will be for the case where @xmath26 is small , @xmath27 is large , and @xmath28 is large .",
    "thus the edge of the relevant features and the fraction of features that are relevant both approach zero while the total number of relevant features increases .",
    "if @xmath17 is not large relative to @xmath29 , even the bayes optimal classifier is not accurate .",
    "no other assumptions about the relationship between the parameters are needed . )    when @xmath30 and the number @xmath23 of training examples satisfies @xmath31 for an absolute constant @xmath32 , we also show in theorem  [ t : beta.cgamma ] that the error probability is at most @xmath33 if @xmath34 , this error probability goes to zero . with only @xmath35 examples ,",
    "an algorithm can not even tell with high confidence whether a relevant variable is positively or negatively associated with the class label , much less solve the more difficult problem of determining whether or not a variable is relevant .",
    "indeed , this error bound is also achieved using @xmath36 , when , for each variable @xmath37 , the algorithm includes either @xmath37 or its negation in the vote.to be precise , the algorithm includes each variable or its negation when @xmath36 and @xmath23 is odd , and includes both the variable and its negation when @xmath23 is even and the variable agrees with the class label exactly half the time .",
    "but , any time both a variable and its negation are included , their votes cancel . we will always use the smaller equivalent model obtained by removing such canceling votes . ]",
    "because bound   holds even when @xmath36 , it can be achieved by an algorithm that does not use knowledge of @xmath26 or @xmath17 .",
    "our upper bounds illustrate the potential rewards for algorithms that are `` inclusive '' , using many of the available variables in their classifiers  even when this means that most variables in the model are irrelevant .",
    "we also prove a complementary lower bound that illustrates the potential cost when algorithms are `` exclusive '' .",
    "we say that an algorithm is @xmath38-exclusive if the expectation of the fraction of the variables used in its model that are relevant is at least @xmath38 .",
    "we show that any @xmath38-exclusive policy has an error probability bounded below by @xmath39 as @xmath17 and @xmath28 go to infinity and @xmath26 goes to @xmath0 in such a way that the error rate obtained by the more `` inclusive '' setting @xmath40 goes to @xmath0 .",
    "in particular , no @xmath38-exclusive algorithm ( where @xmath38 is a positive constant ) can achieve a bound like ( [ e : inclusive.intro ] ) .",
    "[ [ relationship - to - previous - work ] ] relationship to previous work + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    donoho and jin ( see * ? ? ? * ; * ? ? ? * ) and fan and fan @xcite , building on a line of research on joint testing of multiple hypotheses ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , performed analyses and simulations using sources with elements in common with the model studied here , including conditionally independent variables and a weak association between the variables and the class labels .",
    "donoho and jin also pointed out that their algorithm can produce accurate hypotheses while using many more irrelevant features than relevant ones .",
    "the main theoretical results proved in their papers describe conditions that imply that , if the relevant variables are too small a fraction of all the variables , and the number of examples is too small , then learning is impossible .",
    "the emphasis of our theoretical analysis is the opposite : algorithms can tolerate a large number of irrelevant variables , while using a small number of examples , and algorithms that avoid irrelevant variables , even to a limited extent , can not learn as effectively as algorithms that cast a wider net .",
    "in particular , ours is the first analysis that we are aware of to have a result qualitatively like theorem  [ t : lower.lambda ] , which demonstrates the limitations of exclusive algorithms .    for the sources studied in this paper ,",
    "there is a linear classifier that classifies most random examples correctly with a large margin , i.e.  most examples are not close to the decision boundary .",
    "the main motivation for our analysis was to understand the effects of relevant and irrelevant variables on generalization , but it is interesting to note that we get meaningful bounds in the extreme case that @xmath41 , whereas the margin - based bounds that we are aware of ( such as @xcite ) are vacuous in this case .",
    "( since these other bounds hold more generally , their overall strength is incomparable to our results . )",
    "@xcite showed that the naive bayes algorithm ( which ignores class - conditional dependencies ) converges relatively quickly , justifying its use when there are few examples .",
    "but their bound for naive bayes is also vacuous when @xmath42 .",
    "@xcite studied the case in which the class conditional distributions are gaussians , and showed how an algorithm which does not model class conditional dependencies can perform nearly optimally in this case , especially when the number of variables is large .",
    "bhlmann and yu @xcite analyzed the variance - reduction benefits of bagging with primary focus on the benefits of the smoother classifier that is obtained when ragged classifiers are averaged . as such it takes a different form than our analysis .",
    "our analysis demonstrates that certain effects are possible , but how important this is depends on how closely natural learning settings resemble our theoretical setting and the extent to which our analysis can be generalized .",
    "the conditional independence assumption is one way to express the intuitive notion that variables are not too redundant .",
    "a limit on the redundancy is needed for results like ours since , for example , a collection of @xmath43 perfectly correlated irrelevant variables would swamp the votes of the @xmath11 relevant variables . on the other hand , many boosting algorithms minimize the potential for this kind of effect by choosing features in later iterations that make errors on different examples then the previously chosen features .",
    "one relaxation of the conditional independence assumption is to allow each variable to conditionally depend on a limited number @xmath44 of other variables , as is done in the formulation of the lovasz local lemma ( see * ? ? ?",
    "as partial illustration of the robustness of the effects analyzed here , we generalize upper bound  ( [ e : nolearn.intro ] ) to this case in section  [ s : dependent ] .",
    "there we prove an error bound of @xmath45 when each variable depends on most @xmath44 others .",
    "there are a number of ways that one could imagine relaxing the conditional independence assumption while still proving theorems of a similar flavor .",
    "another obvious direction for generalization is to relax the strict categorization of variables into irrelevant and @xmath46-relevant classes .",
    "we believe that many extensions of this work with different coverage and interpretability tradeoffs are possible .",
    "for example , our proof techniques easily give similar theorems when each relevant variable has a probability between @xmath47 and @xmath48 of agreeing with the class label ( as discussed in section  [ s : different ] ) .",
    "most of this paper uses the cleanest and simplest setting in order to focus attention on the main ideas .",
    "we state some useful tail bounds in the next section , and section  [ s : basic ] analyzes the error of simple voting classifiers .",
    "section  [ s : learning ] gives bounds on the expected error of hypotheses learned from training data while section  [ s : lower ] shows that , in certain situations , any exclusive algorithm must have high error while the error of some inclusive algorithms goes to 0 . in section  [ s : dependent ] we bound the accuracy of voting classifiers under a weakened independence assumption and in section  [ s : different ] we consider relaxation of the assumption that all relevant variables have the same edge .",
    "this section gathers together the several tail bounds that will be used in various places in the analysis .",
    "these bounds all assume that @xmath49 are @xmath50 independent @xmath51-valued random variables and @xmath52 .",
    "we start with some upper bounds .    *",
    "the hoeffding bound , ( see * ? ? ?",
    "* ) : @xmath53 }    \\leq e^{-2 \\eta^2 \\ell}.\\ ] ] * the chernoff bound , ( * ? ? ?",
    "* ; * ? ? ?",
    "* see ) and appendix  [ a : chernoff ] . for any @xmath54 : @xmath55 } <",
    "\\exp\\left(-(1+\\eta){\\mathbb{e}}(u ) \\ln \\left(\\frac{1+\\eta}{e}\\right ) \\right).\\end{aligned}\\ ] ] * for any @xmath56 ( see appendix  [ a : chernoff ] ) : @xmath57 }    < \\exp\\left(-\\eta^2 { \\mathbb{e}}(u)/4 \\right).\\ ] ] * for any @xmath58 ( see appendix  [ a : highconf ] ) : @xmath59 } < \\delta .\\ ] ]    we also use the following lower bounds on the tails of distributions .    * if @xmath60 } = 1/2 $ ] for all @xmath61 , @xmath62 , and @xmath63 then ( see appendix  [ a : lower.tail ] ) : @xmath64 }    \\geq \\frac{1}{7 \\eta \\sqrt{\\ell } }            \\exp\\left(-2 \\eta^2 \\ell \\right )              - \\frac{1}{\\sqrt{\\ell}}.\\ ] ] * if @xmath65 } = 1/2 $ ] for all @xmath61 , then for all @xmath66 such that @xmath67 is an integer ( see appendix  [ a : lower.fair ] ) : @xmath68 } \\geq \\frac{1}{5 } e^{-16 \\eta^2 \\ell}.\\ ] ] * a consequence of slud s inequality  @xcite gives the following ( see appendix  [ a : unfair ] ) . if @xmath69 and @xmath70 } = 1/2 + \\eta$ ] for all @xmath61 then : @xmath71 } \\geq \\frac{1}{4 } e^{-5 \\eta^2 \\ell}.\\ ] ]    note that the constants in the above bounds were chosen to be simple and illustrative , rather than the best possible .",
    "in this section we analyze the accuracy of the models ( hypotheses ) produced by the algorithms in section  [ s : learning ] . each example",
    "is represented by a vector of @xmath16 binary _ variables _ and a class designation .",
    "we use the following generative model :    * a random class designation from @xmath72 is chosen , with both classes equally likely , then * each of @xmath17 _ relevant _ variables are equal to the class designation with probability @xmath12 ( or with probability @xmath19 ) , and * the remaining @xmath18 _ irrelevant _ variables are equal to the class label with probability @xmath14 ; * all variables are conditionally independent given the class designation .    which variables are relevant and whether each one is positively or negatively correlated with the class designations are chosen arbitrarily ahead of time .",
    "a _ feature _ is either a variable or its complement .",
    "the @xmath73 _ irrelevant _ features come from the irrelevant variables , the @xmath17 _ relevant _ features agree with the class labels with probability @xmath74 , and the @xmath17 _ misleading _ features agree with the class labels with probability @xmath75 .",
    "we now consider models @xmath76 predicting with a majority vote over a subset of the features .",
    "we use @xmath10 for the total number of features in model @xmath76 , @xmath11 for the number of relevant features , and @xmath50 for the number of misleading features ( leaving @xmath77 irrelevant features ) .",
    "since the votes of a variable and its negation `` cancel out , '' we assume without loss of generality that models include at most one feature for each variable . recall that @xmath78_{+ } { \\ensuremath{\\mathrel{\\stackrel{\\mathrm{def}}{=}}}}\\max \\{z,0\\}$ ] .",
    "[ t : no.learning.misleading ] let @xmath76 be a majority vote of @xmath10 features , @xmath11 of which are relevant and @xmath50 of which are misleading ( and @xmath77 are irrelevant ) . the probability that @xmath76 predicts incorrectly is at most @xmath79_+^2 } { n } \\right)$ ] .    * proof * : if @xmath80 then the exponent is @xmath0 and the bound trivially holds .",
    "suppose @xmath81 .",
    "model @xmath76 predicts incorrectly only when at most half of its features are correct .",
    "the expected fraction of correct voters is @xmath82 , so , for @xmath76 s prediction to be incorrect , the fraction of correct voters must be at least @xmath83 less than its expectation .",
    "applying  ( [ e : hoeffding ] ) , this probability is at most @xmath84 @xmath85    the next corollary shows that even models where most of the features are irrelevant can be highly accurate .",
    "[ c : mostly.irrelevant ] if @xmath26 is a constant , @xmath86 and @xmath87 , then the accuracy of the model approaches @xmath88 while its fraction of irrelevant variables approaches  @xmath89 ( as @xmath90 ) .    for example",
    ", the conditions of corollary  [ c : mostly.irrelevant ] are satisfied when @xmath91 , @xmath92 and @xmath93 .",
    "we now consider the problem of learning a model @xmath76 from data .",
    "we assume that the algorithm receives @xmath23 i.i.d .",
    "examples generated as described in section  [ s : basic ] .",
    "one test example is independently generated from the same distribution , and we evaluate the algorithm s _ expected error _ : the probability over training set and test example that its model makes an incorrect prediction on the test example ( the `` prediction model '' of @xcite ) .",
    "we define @xmath94 to be the majority vote is empty or the vote is tied then any default prediction , such as 1 , will do .",
    "] of all features that equal the class label on at least @xmath21 of the training examples . to keep the analysis as clean as possible ,",
    "our results in this section apply to algorithms that chose @xmath5 as a function of the number of features @xmath16 , the number of relevant features @xmath17 , the edge of the relevant features @xmath26 , and training set size @xmath23 , and then predict with @xmath95 .",
    "note that this includes the algorithm that always choses @xmath96 regardless of @xmath16 , @xmath17 , @xmath26 and @xmath23 .",
    "recall that asymptotic notation will concern the case in which @xmath26 is small , @xmath27 is large , and @xmath28 is large .",
    "this section proves two theorems bounding the expected error rates of learned models .",
    "one can compare these bounds with a similar bound on the bayes optimal predictor that `` knows '' which features are relevant .",
    "this bayes optimal predictor for our generative model is a majority vote of the @xmath17 relevant features , and has an error rate bounded by @xmath97 ( a bound as tight as the hoeffding bound ) .",
    "[ t : learning.beta ] if @xmath98 , then the expected error rate of @xmath99 is at most @xmath100_+^2 }               { 1 + 8 ( n / k ) e^{-2 \\beta^2 m } + \\gamma }                    \\right ) \\right).\\ ] ]    our proof of theorem  [ t : learning.beta ] starts with lemmas bounding the number of misleading , irrelevant , and relevant features in @xmath99 .",
    "these lemmas use a quantity @xmath101 that will be determined later in the analysis .",
    "[ l : few.misleading ] with probability at least @xmath102 , the number of misleading features in @xmath99 is at most @xmath103    * proof * : for a particular misleading feature to be included in @xmath99 , algorithm @xmath104 must overestimate the probability that misleading feature equals the class label by at least @xmath105 . applying  ( [ e : hoeffding ] ) , this happens with probability at most @xmath106 , so the expected number of misleading features in @xmath99 is at most @xmath107 .",
    "since each misleading feature is associated with a different independent variable , we can apply  ( [ e : highconf ] ) with @xmath108 to get the desired result .",
    "[ l : few.irrelevant ] with probability at least @xmath109 , the number of irrelevant features in @xmath99 is at most @xmath110    * proof * : for a particular positive irrelevant feature to be included in @xmath99 , algorithm @xmath104 must overestimate the probability that the positive irrelevant feature equals the class label by @xmath5 . applying  ( [ e : hoeffding ] )",
    ", this happens with probability at most @xmath111 , so the expected number of irrelevant positive features in @xmath99 is at most @xmath112 .",
    "all of the events that variables agree with the label , for various variables , and various examples , are independent .",
    "so the events that various irrelevant variables are included in @xmath99 are independent .",
    "applying  ( [ e : highconf ] ) with @xmath113 gives that , with probability at least @xmath114 , the number of irrelevant positive features in @xmath99 is at most @xmath115 .",
    "a symmetric analysis establishes the same bound on the number of negative irrelevant features in @xmath99 .",
    "adding these up completes the proof .",
    "[ l : many.relevant ] with probability at least @xmath102 , the number of relevant features in @xmath99 is at least @xmath116    * proof * : for a particular relevant feature to be excluded from @xmath99 , algorithm @xmath104 must underestimate the probability that the relevant feature equals the class label by at least @xmath117 . applying  ( [ e : hoeffding ] )",
    ", this happens with probability at most @xmath118 , so the expected number of relevant variables excluded from @xmath99 is at most @xmath119 . applying  ( [ e : highconf ] ) as in the preceding two lemmas completes the proof .",
    "[ l : error.rate ] the probability that @xmath99 makes an error is at most @xmath120_+^2 }          { k + 8n e^{-2\\beta^2 m } + 6\\ln ( 1/\\delta ) }             \\right )     + 4 \\delta.\\ ] ] for any @xmath101 and @xmath121 .",
    "* proof * : the bounds of lemmas  [ l : few.misleading ] , [ l : few.irrelevant ] , and [ l : many.relevant ] simultaneously hold with probability at least @xmath122 .",
    "thus the error probability of @xmath99 is at most @xmath123 plus the probability of error given that all three bounds hold . plugging the three bounds into theorem  [ t : no.learning.misleading ] , ( and over - estimating the number @xmath10 of variables in the model with @xmath17 plus the bound of lemma  [ l : few.irrelevant ] on the number of irrelevant variables ) gives a bound on @xmath99 s error probability of @xmath124_{+}^2 }           { k+ 8 n e^{-2 \\beta^2 m } + 6 \\ln ( 1/\\delta ) }        \\right)\\ ] ] when all three bounds hold .",
    "under - approximating @xmath125 with @xmath126 and simplifying yields : @xmath127_{+}^2 }           { k+ 8 n e^{-2 \\beta^2 m } + 6 \\ln ( 1/\\delta ) }        \\right ) .\\end{aligned}\\ ] ] adding @xmath128 completes the proof .",
    "we are now ready to prove theorem  [ t : learning.beta ] .",
    "* proof * ( of theorem  [ t : learning.beta ] ) : using @xmath129 in lemma  [ l : error.rate ] bounds the probability that @xmath99 makes a mistake by    @xmath130_{+}^2 }           { k+ 8 n e^{-2 \\beta^2 m } + \\gamma k }        \\right ) + 4   \\exp\\left(- \\frac{\\gamma k}{6 } \\right ) \\\\",
    "<   \\exp \\left (       \\frac{-2 \\gamma^2   k \\left [ 1 -   8 e^{-2 ( \\gamma - \\beta)^2 m } - \\gamma \\right]_{+}^2 }           { 1 + \\frac{8 n}{k }   e^{-2 \\beta^2 m } + \\gamma }        \\right ) + 4   \\exp\\left(- \\frac{\\gamma k}{6 } \\right ) .",
    "\\end{gathered}\\ ] ]    the first term is at least @xmath131 , and @xmath132 as @xmath133 and @xmath134 , so  ( [ e : twoterms ] ) implies the bound @xmath135_+^2 }                     { 1 + \\frac{8 n}{k } e^{-2 \\beta^2 m } +   \\gamma } \\right)\\ ] ] as desired .",
    "@xmath85    the following theorem bounds the error in terms of just @xmath17 , @xmath16 , and @xmath26 when @xmath23 is sufficiently large .",
    "[ t : beta.cgamma ] suppose algorithm @xmath104 produces models @xmath99 where @xmath136 for a constant @xmath137 .",
    "* then there is a constant @xmath138 ( depending only on @xmath32 ) such that whenever @xmath139 the error of @xmath104 s model is at most @xmath140 . *",
    "if @xmath141 then the error of @xmath104 s model is at most @xmath142 .    combining lemmas  [ l : few.misleading ] and  [ l : many.relevant ] with the upper bound of @xmath16 on the number of features in @xmath99 as in lemma  [ l : error.rate ] s proof gives the following error bound on @xmath99 @xmath143_{+}^2 } { n }        \\right )   +   2 \\delta\\ ] ] for any @xmath144 .",
    "setting @xmath129 and continuing as in the proof of theorem  [ t : learning.beta ] gives the bound @xmath145_{+}^2 }           { n }        \\right).\\ ] ] for the first part of the theorem , it suffices to show that the @xmath146_+^2 $ ] term is at least @xmath14 .",
    "recalling that our analysis is for small @xmath26 , the term inside the @xmath146_+$ ] of ( [ e : launching.pad ] ) is at least @xmath147 when @xmath148 this term is at least @xmath149 , and thus its square is at least 1/2 for small enough @xmath26 , completing the proof of the first part of the theorem .    to see the second part of the theorem , since @xmath150 , the term of ( [ e : launching.pad ] ) inside the @xmath146_+$ ] is @xmath151 .    by examining inequality  ( [ e : mthreshold ] )",
    ", we see that the constant @xmath138 in theorem  [ t : beta.cgamma ] can be set to @xmath152 .",
    "[ l : many.irrel ] the expected number of irrelevant variables in @xmath99 is at least @xmath153",
    ".    follows from inequality  ( [ e : lower.fair ] ) .",
    "[ c : inclusive.good ] if  @xmath17 , @xmath16 , and @xmath23 are functions of @xmath26 such that @xmath154 then if an algorithm outputs @xmath155 using a @xmath5 in @xmath156 $ ] , it has an error that decreases super - polynomially ( in @xmath26 ) , while the expected fraction of irrelevant variables in the model goes to @xmath89 .",
    "note that theorem  [ t : beta.cgamma ] and corollary  [ c : inclusive.good ] include non - trivial error bounds on the model @xmath157 that votes all @xmath16 variables ( for odd sample size @xmath23 ) .",
    "here we show that any algorithm with an error guarantee like theorem  [ t : beta.cgamma ] must include many irrelevant features in its model .",
    "the preliminary version of this paper @xcite contains a related lower bound for algorithms that choose @xmath5 as a function of @xmath16 , @xmath17 , @xmath23 , and @xmath26 , and predict with @xmath155 . here",
    "we present a more general lower bound that applies to algorithms outputting arbitrary hypotheses .",
    "this includes algorithms that use weighted voting ( perhaps with @xmath158 regularization ) . in this section",
    "we    * set the number of features @xmath16 , number of relevant features @xmath17 , and sample size @xmath23 as a functions of @xmath26 in such a way that corollary  [ c : inclusive.good ] applies , and * prove a constant lower bound for these combinations of values that holds for `` exclusive '' algorithms ( defined below ) when @xmath26 is small enough .",
    "thus , in this situation ,  inclusive \" algorithms relying on many irrelevant variables have error rates going to zero while every `` exclusive '' algorithm has an error rate bounded below by a constant .",
    "the proofs in this section assume that all relevant variables are positively correlated with the class designation , so each relevant variable agrees with the class designation with probability @xmath159 .",
    "although not essential for the results , this assumption simplifies the definitions and notation gives a special case of the generative model described in section  [ s : learning ] , so the lower bounds proven here also apply to that more general setting . ] .",
    "we also set @xmath160 .",
    "this satisfies the assumption of theorem  [ t : beta.cgamma ] when @xmath30 ( see inequality  ( [ e : mthreshold ] ) ) .",
    "we say a classifier @xmath9 a variable @xmath161 if there is an input @xmath162 such that @xmath163 let @xmath164 be the set of variables included in @xmath9 .    for a training set @xmath165",
    ", we will refer to the classifier output by algorithm @xmath104 on @xmath165 as @xmath166 .",
    "let @xmath167 be the set of relevant variables .",
    "we say that an algorithm @xmath104 is - exclusive . ]",
    "if for every positive @xmath16 , @xmath17 , @xmath26 , and @xmath23 , the expected fraction of the variables included in its hypothesis that are relevant is at least @xmath38 , i.e.   @xmath168    our main lower bound theorem is the following .",
    "[ t : lower.lambda ] if @xmath169 then for any constant @xmath170 and any @xmath38-exclusive algorithm @xmath104 , the error rate of @xmath104 is lower bounded by @xmath171 as @xmath26 goes to 0 .",
    "notice that this theorem provides a sharp contrast to corollary  [ c : inclusive.good ] .",
    "corollary  [ c : inclusive.good ] shows that inclusive @xmath104 using models @xmath99 for any @xmath172 have error rates that goes to zero super - polynomially fast ( in @xmath173 ) under the assumptions of theorem  [ t : lower.lambda ] .",
    "the values of @xmath17 and @xmath16 in theorem  [ t : lower.lambda ] are chosen to make the proof convenient , but other values would work .",
    "for example , decreasing @xmath17 and/or increasing @xmath16 would make the lower bound part of theorem  [ t : lower.lambda ] easier to prove .",
    "there is some slack to do so while continuing to ensure that the upper bound of corollary  [ c : inclusive.good ] goes to @xmath0 .",
    "as the correlation of variables with the label over the sample plays a central role in our analysis , we will use the following definition .",
    "[ d : empirical.edge ] if a variable agrees with the class label on @xmath174 of the training set then it has _ ( empirical ) edge _ @xmath175 .",
    "the proof of theorem  [ t : lower.lambda ] uses a critical value of @xmath5 , namely @xmath176 , with the property that both : @xmath177 as @xmath133 .",
    "intuitively , means that any algorithm that uses most of the variables having empirical edge at least @xmath178 can not be @xmath38-exclusive . on the other hand , implies that if the algorithm restricts itself to variables with empirical edges greater than @xmath178 then it does not include enough relevant variables to be accurate .",
    "the proof must show that _ arbitrary _ algorithms frequently include either too many irrelevant variables to be @xmath38-exclusive or too few relevant ones to be accurate .",
    "see figure  [ f : facts ] for some useful facts about @xmath26 , @xmath23 , and @xmath178 .    to prove the lower bound , borrowing a technique from @xcite",
    ", we will assume that the @xmath17 relevant variables are randomly selected from the @xmath16 variables , and lower bound the error with respect to this random choice , along with the training and test data .",
    "this will then imply that , for each algorithm , there will be a choice of the @xmath17 relevant variables giving the same lower bound with respect only to the random choice of the training and test data .",
    "we will always use relevant variables that are positively associated with the class label , agreeing with it with probability @xmath12 .    fix any learning algorithm @xmath104 , and",
    "let @xmath166 be the hypothesis produced by @xmath104 from sample @xmath165 .",
    "let @xmath179 be the number of variables included in @xmath166 and let @xmath180 be the @xmath179th largest empirical ( w.r.t .",
    "@xmath165 ) edge of a variable .",
    "let @xmath181 be the probability that @xmath182 .",
    "we will show in section  [ s : exclusive ] that if @xmath104 is @xmath38-exclusive then @xmath183 ( as @xmath26 goes to 0 ) .",
    "we will also show in section  [ s : error ] that the expected error of @xmath104 is at least @xmath184 as @xmath26 goes to 0 .",
    "therefore any @xmath38-exclusive algorithm @xmath104 has an expected error rate at least @xmath171 as @xmath26 goes to 0 .    before attacking the two parts of the proof alluded to above",
    ", we need a subsection providing some basic results about relevant variables and optimal algorithms .",
    "this section proves some useful facts about relevant variables and good hypotheses .",
    "the first lemma is a lower bound on the accuracy of a model in terms of the number of relevant variables .",
    "[ l : lower.relevant ] if @xmath185 $ ] then any classifier using @xmath11 relevant variables has an error probability at least @xmath186 .",
    "* proof * : the usual naive bayes calculation ( see * ? ? ?",
    "* ) implies that the optimal classifier over a certain set @xmath187 of variables is a majority vote over @xmath188 .",
    "applying the lower tail bound ( [ e : unfair ] ) then completes the proof .",
    "@xmath85    our next lemma shows that , given a sample , the probability that a variable is relevant ( positively correlated with the class label ) is monotonically increasing in its empirical edge .",
    "[ l : relevant.increasing ] for two variables @xmath161 and @xmath189 , and any training set @xmath165 of @xmath23 examples ,    * @xmath190 } > { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   x_j \\text { relevant } \\mid s \\right]}$ ] if and only if the empirical edge of @xmath161 in @xmath165 is greater than the empirical edge of @xmath189 in @xmath165 , and * @xmath190 } = { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   x_j \\text { relevant } \\mid s \\right]}$ ] if and only if the empirical edge of @xmath161 in @xmath165 is equal to the empirical edge of @xmath189 in @xmath165 .    since the random choice of @xmath167 does not effect that marginal distribution over the labels , we can generate @xmath165 by picking the labels for all the examples first , then @xmath167 , and finally the values of the variables on all the examples .",
    "thus if we can prove the lemma after conditioning on the values of the class labels , then scaling all of the probabilities by @xmath191 would complete the proof .",
    "so , let us fix the values of the class labels , and evaluate probabilities only with respect to the random choice of the relevant variables @xmath167 , and the values of the variables .",
    "let @xmath192 } - { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   x_j \\in { { \\cal r } } | s \\right]}.\\ ] ] first , by subtracting off the probabilities that both variables are relevant , we have @xmath193 }                  - { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    x_i \\not\\in { { \\cal r } } , x_j \\in { { \\cal r } } \\big| s \\right]}.\\ ] ]    let @xmath194 be the event that exactly one of @xmath161 or @xmath189 is relevant",
    ". then @xmath195 }             - { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    x_i \\not\\in { { \\cal r } } , x_j \\in { { \\cal r } } \\big| s , { \\mathrm{one}}\\right ] } ) { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   { \\mathrm{one}}\\right]}.\\ ] ] so @xmath196 if and only if @xmath197 }               - { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    x_i \\not\\in { { \\cal r } } , x_j \\in { { \\cal r } } \\big| s , { \\mathrm{one}}\\right ] } > 0\\ ] ] ( and similarly for @xmath198 if and only if @xmath199 ) .",
    "if @xmath200 is the distribution obtained by conditioning on @xmath194 , then @xmath201 }               - { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [    x_i \\not\\in { { \\cal r } } , x_j \\in { { \\cal r } } \\big| s \\right]}.\\ ] ]    let @xmath202 be the values of variable @xmath61 in @xmath165 , and define @xmath203 similarly for variable @xmath204 .",
    "let @xmath205 be the values of the other variables .",
    "since we have already conditioned on the labels , after also conditioning on @xmath194 ( i.e. , under the distribution @xmath200 ) , the pair @xmath206 is independent of @xmath205 . for each @xmath202",
    "we have @xmath207 } = { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [   s_i \\mid x_i \\not\\in { { \\cal r } } \\right]}$ ] .",
    "furthermore , by symmetry , @xmath208 } = { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [    x_i \\not\\in { { \\cal r } } , x_j \\in { { \\cal r } } \\big| s ' \\right ] } = \\frac{1}{2}.\\ ] ] thus , by using bayes rule on each term , we have @xmath209 }                 - { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [    x_i \\not\\in { { \\cal r } } , x_j \\in { { \\cal r } } \\big| s_i , s_j , s ' \\right ] } \\\\    & = &     \\frac { { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [    s_i , s_j\\big| x_i \\in { { \\cal r } } , x_j \\not \\in { { \\cal r } } , s ' \\right ] }                 - { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [    s_i , s_j \\big| x_i \\not\\in { { \\cal r } } , x_j \\in { { \\cal r } } , s ' \\right ] } }         { 2 { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [   s_i , s_j | s ' \\right ] } } \\\\    & = &     \\frac{(1/2 + \\gamma)^{m_i}(1/2 - \\gamma)^{m - m_i }          - ( 1/2 + \\gamma)^{m_j}(1/2 - \\gamma)^{m - m_j } }         { 2^{m+1 } { \\mspace{2mu } { \\mathbb{q } }   \\mspace{-2mu } \\left [   s_i , s_j \\right]}},\\end{aligned}\\ ] ] where @xmath210 and @xmath211 are the numbers of times that variables @xmath161 and @xmath189 agree with the label in sample @xmath165",
    ". the proof concludes by observing that @xmath212 is positive exactly when @xmath213 and zero exactly when @xmath214 .",
    "because , in this lower bound proof , relevant variables are always positively associated with the class label , we will use a variant of @xmath155 which only considers positive features .",
    "let @xmath215 be a vote over the variables with empirical edge at least @xmath5 .",
    "when there is no chance of confusion , we will refer to the set of variables in @xmath215 also as @xmath215 ( rather than @xmath216 ) .",
    "we now establish lower bounds on the probability of variables being included in @xmath215 ( here @xmath5 can be a function of @xmath26 , but does not depend on the particular sample @xmath165 ) .",
    "[ l : irrelevant.lower ] if @xmath217 and @xmath218 then the probability that a given variable has empirical edge at least @xmath5 is at least @xmath219 if in addition @xmath220 , then the probability that a given variable has empirical edge at least @xmath5 is at least @xmath221    * proof * : since relevant variables agree with the class label with probability @xmath74 , the probability that a relevant variable has empirical edge at least @xmath5 is lower bounded by the probability that an irrelevant variable has empirical edge at least @xmath5 .",
    "an irrelevant variable has empirical edge at least @xmath5 only when it agrees with the class on @xmath222 of the sample .",
    "applying  bound  ( [ e : lower.fair ] ) , this happens with probability at least @xmath223 the second part uses  bound  ( [ e : lower.tail ] ) instead of ( [ e : lower.fair ] ) .",
    "@xmath85    we now upper bound the probability of a relevant variable being included in @xmath215 , again for @xmath5 that does not depend on @xmath165 .",
    "[ l : relevant.upper ] if @xmath224 , the probability that a given relevant variable has empirical edge at least @xmath5 is at most @xmath225 .    * proof * : use  ( [ e : hoeffding ] ) to bound the probability that a relevant feature agrees with the class label @xmath226 more often than its expected fraction of times .",
    "recall that @xmath179 is the number of variables used by @xmath166 , and @xmath180 is the edge of the variable whose rank , when the variables are ordered by their empirical edges , is @xmath179 .",
    "we will show that : if @xmath166 is @xmath38-exclusive , then there is reasonable probability that @xmath180 is at least the critical value @xmath227 . specifically ,",
    "if @xmath104 is @xmath38-exclusive , then , for any small enough @xmath26 , we have @xmath228 } > \\lambda/2 $ ] .",
    "suppose , given the training set @xmath165 , the variables are sorted in decreasing order of empirical edge ( breaking ties arbitrarily , say using the variable index ) .",
    "let @xmath229 consist of the first @xmath11 variables in this sorted order , the `` top @xmath11 '' variables .",
    "since for each sample @xmath165 and each variable @xmath161 , the probability @xmath230}$ ] decreases as the empirical edge of @xmath161 decreases ( lemma  [ l : relevant.increasing ] ) , the expectation @xmath231 is non - increasing with @xmath11 .",
    "furthermore , lemma  [ l : relevant.increasing ] also implies that for each sample @xmath165 , we have @xmath232 therefore , by averaging over samples , for each @xmath26 we have @xmath233    note that the numerators in the expectations are never greater than the denominators .",
    "we will next give upper bounds on @xmath234 and lower bounds on @xmath235 that each hold with probability @xmath236 .",
    "the next step is a high - confidence upper bound on @xmath234 . from lemma  [ l : relevant.upper ] ,",
    "the probability that a particular relevant variable is in @xmath237 is at most ( recall that @xmath238 ) @xmath239    let @xmath240 be this upper bound , and note that @xmath241 drops to 0 as @xmath26 goes to 0 , but a rate slower than @xmath242 for any @xmath243 .",
    "the number of relevant variables in @xmath237 has a binomial distribution with parameters @xmath17 and @xmath244 where @xmath245 .",
    "the standard deviation of this distribution is @xmath246    using the chebyshev bound , @xmath247 } \\leq \\frac{1}{a^2}\\ ] ] with @xmath248 gives that @xmath249}&\\leq & \\gamma \\\\ \\label{e : two.term } { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    | {   { \\cal v } _ { { { \\beta^ * } } } } \\cap { { \\cal r } } |   > k { p_{\\text{rel}}}+ \\frac{\\sigma}{\\sqrt{\\gamma } }   \\right]}&\\leq & \\gamma.\\end{aligned}\\ ] ]    since @xmath250 by  ( [ e : sigma.bound ] ) , we have @xmath251 . substituting the values of @xmath17 and @xmath241 into the square - root yields @xmath252 for small enough @xmath26 . combining with ( [ e : two.term ] ) , we get that @xmath253 } \\leq \\gamma\\ ] ] holds for small enough @xmath26 .    using similar reasoning",
    ", we now obtain a lower bound on the expected number of variables in @xmath237 .",
    "lemma  [ l : irrelevant.lower ] shows that , for each variable , the probability of the variable having empirical edge @xmath178 is at least @xmath254 for sufficiently small @xmath26 . since the empirical edges of different variables are independent , the probability that at least @xmath10 variables have empirical edge at least @xmath178 is lower bounded by the probability of at least @xmath10 successes from the binomial distribution with parameters @xmath16 and @xmath255 where @xmath256 if , now , we define @xmath257 to be the standard deviation of this binomial distribution , then , like before , @xmath258 , and @xmath259 so that , for small enough @xmath26 , @xmath260 . therefore applying the chebyshev bound  ( [ e : chebyshev ] ) with @xmath261 gives ( for sufficiently small @xmath26 ) @xmath262 }   \\leq { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    | {   { \\cal v } _ { { { \\beta^ * } } } } | < n { p_{\\text{irrel}}}- \\sigma/\\sqrt{\\gamma }   \\right ] }   < \\gamma.\\ ] ]    recall that @xmath263 }   = { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   n(s ) \\leq \\vert {   { \\cal v } _ { { { \\beta^ * } } } } \\vert \\right]}.\\ ] ] if @xmath104 is @xmath38-exclusive then , using ( [ e : by.m ] ) , we have @xmath264 where we use the upper and lower bounds from equations  ( [ e : mbr.upper ] ) and  ( [ e : mb.lower ] ) that each hold with probability @xmath236 . note that the ratio    @xmath265    which goes to 0 as @xmath26 goes to 0 .",
    "therefore , @xmath266 which implies that , @xmath267 } \\geq \\lambda -o(1)\\ ] ] as @xmath26 goes to 0 .",
    "call a variable _ good _ if it is relevant and its empirical edge is at least @xmath178 in the sample . let @xmath244 be the probability that a relevant variable is good",
    ". thus the number of good variables is binomially distributed with parameters @xmath17 and @xmath244 .",
    "we have that the expected number of good variables is @xmath268 and the variance is @xmath269 . by chebyshev s inequality",
    ", we have @xmath270 }   \\leq { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    \\text{\\ # good vars } \\geq kp + a \\sqrt{kp(1-p ) }    \\right ] } \\leq   \\frac{1}{a^2},\\ ] ] and setting @xmath271 , this gives @xmath272 } \\leq \\frac{1}{kp}.\\ ] ]    by lemma  [ l : relevant.upper ] , @xmath273 , so @xmath274 so for small enough @xmath26 , @xmath275 and thus @xmath276 .",
    "so if @xmath277 , then with probability at least @xmath278 , there are less than @xmath279 good variables . on the other hand ,",
    "if @xmath280 , then , setting @xmath281 in bound  ( [ e : a.bound ] ) gives that the probability that there are more than @xmath282 good variables is at most @xmath26 .",
    "so in either case the probability that there are more than @xmath283 good variables is at most @xmath26 ( for small enough @xmath26 ) .",
    "so if @xmath284 } \\geq q_\\gamma$ ] , then with probability at least @xmath285 algorithm @xmath104 is using a hypothesis with at most @xmath283 relevant variables . applying lemma  [ l : lower.relevant ] yields the following lower bound on the probability of error :",
    "@xmath286 since the limit of ( [ e : final.lower ] ) for small @xmath26 is @xmath287 , this completes the proof of theorem  [ t : lower.lambda ] .",
    "to keep the analysis clean , and facilitate the interpretation of the results , we have analyzed an idealized model . in this section ,",
    "we briefly consider the consequences of some relaxations of our assumptions .",
    "theorem  [ t : no.learning.misleading ] can be generalized to the case in which there is limited dependence among the variables , after conditioning on the class designation , in a variety of ways .",
    "for example , suppose that there is a degree-@xmath44 graph @xmath288 whose nodes are variables , and such that , conditioned on the label , each variable is independent of all variables not connected to it by an edge in @xmath288 .",
    "assume that @xmath11 variables agree with the label with probability @xmath289 , and the @xmath13 agree with the label with probability @xmath14 .",
    "let us say that a source like this _ has @xmath44-local dependence_. then applying a chernoff - hoeffding bound for such sets of random variables due to @xcite , if @xmath290 , one gets a bound of @xmath45 the probability of error .",
    "we have previously assumed that all relevant variables are equally strongly associated with the class label . our analysis is easily generalized to the situation when the strengths of associations fall in an interval @xmath291 $ ] .",
    "thus relevant variables agree with the class label with probability at least @xmath292 and misleading variables agree with the class label with probability at least @xmath293 . although a sophisticated analysis would take each variable s degree of association into account , it is possible to leverage our previous analysis with a simpler approach . using the @xmath292 and",
    "@xmath293 underestimates on the probability that relevant variables and misleading variables agree with the class label leads to an analog of theorem  [ t : no.learning.misleading ] .",
    "this analog says that models voting @xmath10 variables , @xmath11 of which are relevant and @xmath50 of which are misleading , have error probabilities bounded by @xmath294_+^2 } { n } \\right).\\ ] ]    we can also use the upper and lower bounds on association to get high - confidence bounds ( like those of lemmas  [ l : few.misleading ] and [ l : many.relevant ] ) on the numbers of relevant and misleading features in models @xmath99 .",
    "this leads to an analog of theorem  [ t : learning.beta ] bounding the expected error rate of @xmath99 by @xmath295_+^2 }                     { 1 + \\frac{8 n}{k } e^{-2 \\beta^2 m } +   { \\gamma_{\\text{\\rm min } } } } \\right)\\ ] ] when @xmath121 and @xmath296 .",
    "note that @xmath26 in theorem  [ t : learning.beta ] is replaced by @xmath297 here , and @xmath298 only appears in the @xmath299 factor ( which replaces an `` 8 '' in the original theorem ) .    continuing to mimic our previous analysis gives analogs to theorem  [ t : beta.cgamma ] and corollary  [ c : inclusive.good ] .",
    "these analogs imply that if @xmath300 is bounded then algorithms using small @xmath5 perform well in the same limiting situations used in section  [ s : lower ] to bound the effectiveness of exclusive algorithms .",
    "a more sophisticated analysis keeping better track of the degree of association between relevant variables and the class label may produce better bounds .",
    "in addition , if the variables have varying strengths then it makes sense to consider classifiers that assign different voting weights to the variables based on their estimated strength of association with the class label .",
    "an analysis that takes account of these issues is a potentially interesting subject for further research .",
    "we analyzed learning when there are few examples , a small fraction of the variables are relevant , and the relevant variables are only weakly correlated with the class label .",
    "in this situation , algorithms that produce hypotheses consisting predominately of irrelevant variables can be highly accurate ( with error rates going to 0 ) .",
    "furthermore , this inclusion of many irrelevant variables is essential .",
    "any algorithm limiting the expected fraction of irrelevant variables in its hypotheses has an error rate bounded below by a constant .",
    "this is in stark contrast with many feature selection heuristics that limit the number of features to a small multiple of the number of examples , or that limit the classifier to use variables that pass stringent statistical tests of association with the class label .",
    "these results have two implications on the practice of machine learning .",
    "first , they show that the engineering practice of producing models that include enormous numbers of variables is sometimes justified .",
    "second , they run counter to the intuitively appealing view that accurate class prediction `` validates '' the variables used by the predictor .",
    "we thank aravind srinivasan for his help .",
    "equation  4.1 from @xcite is @xmath301 }     < \\left(\\frac{e^\\eta}{(1+\\eta)^{1+\\eta } } \\right)^{{\\mathbb{e}}(u)}\\ ] ] and holds for independent 0 - 1 valued @xmath302 s each with ( possibly different ) probabilities @xmath303 where @xmath304 and @xmath62 . taking the logarithm of the rhs",
    ", we get @xmath305 which implies ( [ e : chernoff ] ) . from  ( [ e : chernoff.mr ] ) , when @xmath306 ( since @xmath307 there ) , @xmath308 }    < \\exp\\left(-\\eta^2 { \\mathbb{e}}(u)/4 \\right ) $ ] showing  ( [ e : leq4 ] ) .      using ( [ e : chernoff ] ) with @xmath309 gives @xmath310 }   & < \\exp\\left(-(4 { \\mathbb{e}}(u)+ 3 \\ln \\delta ) \\ln \\left(\\frac{4 + 3 \\ln(1/\\delta)/{\\mathbb{e}}(u ) } { e}\\right ) \\right ) \\\\     &",
    "< \\exp\\left(- ( 3 \\ln(1/\\delta ) \\ln \\left(\\frac{4}{e}\\right ) \\right )    \\\\ & < \\exp\\left(- \\ln(1/\\delta ) \\right ) = \\delta\\end{aligned}\\ ] ] using the fact that @xmath311 .",
    "[ l : be ] under the assumptions of section  [ s : tools ] with each @xmath65 } = 1/2 $ ] , let : @xmath312 then for all @xmath175 , we have @xmath313 } - { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    z > \\eta   \\right ] } \\bigr\\rvert \\leq \\frac{1}{\\sqrt{\\ell}}. $ ]      now , to prove ( [ e : lower.tail ] ) , let @xmath317 and let @xmath314 be a standard normal random variable . then lemma  [ l : be ] implies that",
    ", for all @xmath318 @xmath319 } - { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   z > \\kappa \\right ] } \\right|   \\leq \\frac{1}{\\sqrt{\\ell}}.\\ ] ] using @xmath320 , @xmath321 }     \\geq { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   z > 2 \\eta \\sqrt{\\ell }   \\right ] }               - \\frac{1}{\\sqrt{\\ell}}.\\ ] ] applying lemma  [ l : gaussian.lower ] , we get @xmath322 }    \\;\\ ; \\geq \\;\\ ; \\frac{1}{\\sqrt{2 \\pi } }        \\left(\\frac{1}{2 \\eta \\sqrt{\\ell } }             - \\left(\\frac{1}{2 \\eta \\sqrt{\\ell}}\\right)^3                   \\right )         e^{-2 \\eta^2 \\ell}.\\ ] ] since @xmath63 , we get @xmath323 }    \\;\\ ; & \\geq \\;\\ ; \\frac{1}{\\sqrt{2 \\pi } }       \\left(\\frac{1}{2 } - \\frac{1}{8 } \\right )       \\frac{1}{\\eta \\sqrt{\\ell } }         e^{-2 \\eta^2 \\ell } \\\\ & \\geq \\;\\",
    ";    \\frac{1}{7 \\eta \\sqrt{\\ell } }    e^{-2 \\eta^2 \\ell}.\\end{aligned}\\ ] ] combining with ( [ e : bin.by.gaussian ] ) completes the proof of  ( [ e : lower.tail ] ) .",
    "@xmath85        for @xmath10 even , let @xmath324 be i.i.d .",
    "rvs with @xmath325 } = { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [   u_1 = 1 \\right ] } = 1/2 $ ] and @xmath326 .",
    "then for integer @xmath327 $ ] , @xmath328 } \\geq \\frac{1}{5 } e^{-16",
    "t^2 / n } .\\ ] ]    let integer @xmath329 . @xmath330 } & = 2^{-2 m } \\sum_{j = t}^m { 2 m \\choose m+j } \\\\ & \\geq 2^{-2 m } \\sum_{j = t}^{2t-1 } { 2 m \\choose m+j } \\\\ & = 2^{-2 m } \\sum_{j = t}^{2t-1 } { 2 m \\choose m } \\cdot \\frac{m}{m+j } \\cdot \\frac{m-1}{m+j-1 } \\cdots \\frac{m - j+1}{m+1 } \\\\ & \\geq \\frac{1}{2 \\sqrt{m } }    \\sum_{j = t}^{2t-1 } \\prod_{i=1}^{j } \\left (   1- \\frac{j}{m+1 }   \\right )                   \\hspace{0.3 in } \\mbox{using $ { 2 m \\choose m } \\geq   2^{2 m } / 2 \\sqrt{m}$ } \\\\ & \\geq \\frac{t}{2 \\sqrt{m } }   \\left (   1- \\frac{2t}{m }    \\right)^{2 t } \\\\ & \\geq \\frac{t}{2 \\sqrt{m } }   e^{- 8t^2 / m }                   \\hspace{0.3 in } \\mbox { since $ 1-x \\geq e^{-2x}$ for $ 0 \\leq x \\leq 1/2$.}\\end{aligned}\\ ] ] for @xmath331 , the last expression is at least @xmath332 .    note that @xmath333 } = 2^{-2 m } { 2 m \\choose m } \\leq 1 / \\sqrt{\\pi m } $ ] .",
    "thus for @xmath334 , we have @xmath330 } & \\geq \\frac{1}{2 } - t { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    u = m   \\right ] } \\\\ & \\geq \\frac{1}{2 } - \\frac{1}{2}\\sqrt{m } \\frac{1}{\\sqrt{\\pi m } }    \\\\ & \\geq \\frac{1}{2 } - \\frac{1}{2 \\sqrt{\\pi } } \\approx 0.218 \\geq \\frac{1}{5 } \\geq \\frac{1}{5 } e^{-16 ^ 2/ n}\\end{aligned}\\ ] ] thus the bound @xmath335 holds for all @xmath336 .",
    "[ l : slud ] let @xmath337 be a binomial @xmath338 random variable with @xmath339 .",
    "then for @xmath340 , @xmath341 } \\geq { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    z \\geq \\frac{j-\\ell p}{\\sqrt{\\ell p(1-p ) } }   \\right]}\\ ] ] where @xmath314 is a standard normal random variable",
    ".      recall that in  ( [ e : unfair ] ) @xmath344 the sum of the @xmath50 i.i.d .",
    "boolean random variables , each of which is @xmath89 with probability @xmath345 . let @xmath337 be a random variable with the binomial @xmath346 distribution .",
    "@xmath347 } & = { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    b \\geq \\ell / 2   \\right ] } \\\\ & \\geq { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    n \\geq \\frac{\\ell/2-\\ell(1/2 - \\eta)}{\\sqrt{\\ell ( 1/2 + \\eta ) ( 1/2 - \\eta ) } }   \\right ] }   & \\text{slud 's inequality } \\\\ & = { \\mspace{2mu } { \\mathbb{p } }   \\mspace{-2mu } \\left [    n \\geq \\frac{2 \\eta \\sqrt{\\ell } } { \\sqrt{(1 - 4\\eta^2 ) } }   \\right ] }   \\\\ & \\geq \\frac{1}{2 } \\left ( 1- \\sqrt{1- \\exp \\left (    - \\frac{4 \\eta^2 \\ell } { 1 - 4\\eta^2 }   \\right ) }    \\right ) \\\\ & \\geq \\frac{1}{4 } \\exp \\left (   -   \\frac{4 \\eta^2 \\ell } { 1 - 4\\eta^2 }   \\right )    & \\text{since $ 1-\\sqrt{1-x } > x/2 $ } \\\\ & \\geq \\frac{1}{4 } \\exp \\left (   -   5 \\eta^2 \\ell   \\right )    & \\text{when $ \\eta \\leq 1/5 $ }   \\end{aligned}\\ ] ]                  p.  bickel and e.  levina . some theory of fisher s linear discriminant function , ` naive bayes ' , and some alternatives when there are many more variables than observations .",
    "_ bernoulli _ , 100 ( 6):0 9891010 , 2004 ."
  ],
  "abstract_text": [
    "<S> this work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers . </S>",
    "<S> the analysis uses the assumption that the variables are conditionally independent given the class , and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing . </S>",
    "<S> the main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to @xmath0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant . </S>",
    "<S> we also show that accurate learning is possible even when there are so few examples that one can not determine with high confidence whether or not any individual variable is relevant .    </S>",
    "<S> feature selection , generalization , learning theory </S>"
  ]
}