{
  "article_text": [
    "since geophysical inverse problems are almost always underdetermined , regularization techniques are essential to obtain a meaningful solution .",
    "two major classes of techniques exist .",
    "the first one , named ` mollifying ' in the mathematical literature , or ` optimally localized averaging ' ( ola ) in helioseismology , can be traced back to the groundbreaking work of backus and gilbert @xcite in geophysics . in this approach one",
    "searches for the size of an averaging volume that can produce a local average of the model parameter with an acceptable variance .",
    "since this method is computationally very expensive , it has found little application in large - scale geophysical inversions such as seismic tomography . to limit the computational effort , seismic tomographers instead",
    "search for a biased ( ` damped ' ) solution .",
    "this has the disadvantage of introducing a systematic error  the bias  in lieu of the random error caused by the propagation of data errors .",
    "it can be turned into an advantage if the bias actually reflects a justified disposition of the scientist to prefer certain models over others , as long as the data are fit within their error bars .",
    "simple @xmath0-norm damping , which biases model perturbations towards zero in the absence of information based on the data , is generally a bad choice to regularize the inverse problem for seismic tomography as it tends to introduce structures reflecting ray coverage into the images .",
    "for that reason , most tomographers prefer to bias the model towards ` smooth ' anomalies , in effect trying to forge a compromise between backus - gilbert theory and the efficiency of damped inversions .",
    "the smoothness of the images has the advantage that large structures become easily visible .",
    "sharp discontinuities , however , are blurred , and smaller structures , even when resolved , may be diluted beyond recognition .",
    "recently , @xcite  hereafter referred to as paper i  introduced a third option for the bias in geophysical inversions : to minimize the @xmath1-norm of the wavelet decomposition .",
    "this also biases the model towards zero , but it turns out that such reconstructions always have many ( or most ) wavelet coefficients _ exactly _ equal to zero ( i.e. they are sparse ) . in a synthetic 2d experiment using surface wave data , we showed how structurally coherent features ( in a geophysical sense ) , were more faithfully reproduced using this technique than with a simple @xmath0-norm damping .",
    "in addition , as a result of their inherent sparsity , @xmath1 reconstructions exhibit much less noise than their @xmath0 counterparts .",
    "though paper i clearly showed the feasibility of wavelet - based @xmath1 regularization it left a number of questions unanswered , in particular which wavelet families work best , how they would perform against more sophisticated @xmath0 norms ( e.g. smoothness damping ) and whether the computational feasibility as well as the positive conclusions for wavelet regularization do scale up to large , 3d models .    in this paper we therefore aim to refine the original conclusions of paper  i and to enlarge the scope of the investigation .",
    "we extend tests to 3d inversions of body wave travel times and investigate the use of different families of wavelets ( haar , d4 , dual tree ) .",
    "we include a comparison with smoothness damping , and with a fourth option named ` total variation ' damping .",
    "unlike paper  i , we also discuss so - called @xmath2 constrained recovery .",
    "in contrast to the @xmath1 norm technique which relies on iterative soft - thresholding , the @xmath2 recovery method uses iterative _ hard_-thresholding of wavelet coefficients @xcite .",
    "the ( salt dome ) model that we try to reconstruct here is more realistic than the model in paper  i and includes a wide range of length scales .",
    "the problem described in paper  i was also of a very limited size : there were only about @xmath3 degrees of freedom in the reconstructed models . here",
    "we perform 3d reconstructions , and increase the number of degrees of freedom by an order of magnitude to about @xmath4 .",
    "the number of data also increases accordingly to @xmath5 .",
    "our approach is complementary to that of @xcite who expand the frchet kernels into wavelets to obtain a significant reduction in the memory requirements to store the kernel .",
    "one disadvantage of the @xmath1-norm , @xmath2 and total variation penalties is that they lack the convenient linearity of the more conventional @xmath0-norm minimizations . making use of recent algorithmic improvements",
    ", we do demonstrate that finding a ( nonlinear ) sparse model reconstruction is not necessarily more expensive , computation - wise , than @xmath0 based ( linear ) reconstructions .",
    "the use of @xmath1 norms in seismic tomography was to the best of our knowledge first proposed in @xcite in the form of an iteratively reweighted least squares method ( irls , see also @xcite ) , but has never found much favor , possibly because the convergence of irls was not guaranteed . besides its use in seismic tomography @xmath1 norms have found application in other geophysical contexts such as deconvolution and interpolation @xcite .",
    "the use of @xmath1 norms in combination with a carefully chosen basis ( such as wavelets ) is , however , more recent and is largely inspired by the recent development of compressed sensing @xcite , that shows that under certain conditions an exact reconstruction can be obtained by solving an @xmath1 regularized inverse problem , provided there is an underlying basis in which the desired model is sparse .",
    "this emphasizes the importance of studying different `` dictionaries '' as we do here .",
    "recently @xcite has successfully applied the compressed sensing idea to wavefield reconstruction , albeit on small - scale problems only .",
    "the success and promise of compressed sensing has therefore also increased interest in the speed - up of such @xmath1 problems to be able to handle practical geophysical applications ( e.g. @xcite ) .",
    "we plan to test the regularization methods on a synthetic data set generated for a salt dome model .",
    "since the main goal of this paper is to evaluate and compare a number of algorithms , numerical efficiency is more important than the wish to have a tomographic problem at hand that is fully realistic .",
    "we have thus taken a few shortcuts to be able to run inversions quickly in matlab on a single processor .",
    "however , we took pains to ensure that we would invert for a model that has a large range of length scales , and that the ray coverage would encompass both dense and sparse regions .    in dimensionless variables ,",
    "the expression for a finite frequency sensitivity kernel corresponding to a constant background and a gaussian power spectrum is given by the formula : @xmath6 where @xmath7 .",
    "here @xmath8 is the distance between source ( earthquake ) and receiver ( station ) , and @xmath9 are the distances to source and to receiver , measured from the point @xmath10 .",
    "@xmath11 is the dominant wavelength and @xmath12 denotes the hermite polynomial of order 5 .",
    "equation ( [ kernel ] ) can be derived from the expressions for the frchet kernel in a homogeneous medium using an analytical expression for the spectral integration ( see @xcite ) .",
    "the constant background model that we use here is not at all realistic from a physical point of view for the salt dome input model that we will use in section [ reconstructionsection ] : in case of of large velocity contrasts kernels are bent rather than straight as in expression ( [ kernel ] ) . however , as we will use the same constant background kernels ( [ kernel ] ) for generating synthetic data as well as for reconstructing models from these data , we believe it is possible to accurately evaluate the effects of the regularization technique on the inversions . obviously , reconstructing a model from actual measurements requires kernels of a more complicated shape than ( [ kernel ] ) but their resolving power is not fundamentally different from those used in our simple application .    for each source - receiver pair and each dominant wavelength ,",
    "the travel time differential and the model perturbation @xmath13 are connected by the linear integral relation @xcite instead of @xmath14 as is often done . ]",
    "@xmath15 given sufficiently many data , the aim of seismic tomography is to reconstruct the model @xmath13 from a noisy version of the data vector @xmath16 containing many travel time differentials corresponding to many source - receiver pairs .    in section [ reconstructionsection ]",
    "we will perform a number of seismic reconstructions .",
    "the domain on which we will do this is the cube @xmath17 ^ 3 $ ] . for discretization , this domain is subdivided in @xmath18 voxels , a convenient choice for the digital reconstruction of a model @xmath13 , leading to @xmath19 degrees of freedom in @xmath13 . in order to be able to produce meaningful reconstructions",
    ", we expect to need at least @xmath3 data ( about @xmath20 datum for @xmath21 degrees of freedom ) .",
    "hence we will choose @xmath22 sources - receiver pairs and @xmath23 different dominant wavelengths so as to yield @xmath5 data .",
    "this represents an overparametrization by a factor of more than 10 .",
    "thus regularization will be an essential requirement for any data inversion .",
    "a very efficient set - up of our numerical experiment was obtained as follows : we first choose @xmath24 source - receiver pairs in random positions on the surface of the cube , while making sure source and receiver are never on the same face ( the kernels ( [ kernel ] ) are not curved and would not be able to cross the model domain very much if source and receiver were on the same face ) . from these initial @xmath24 pairs , we construct the full set of @xmath22 pairs by using the @xmath25 symmetry transformations of the cube .",
    "these @xmath25 operations are constructed from the @xmath26 permutations of the coordinates @xmath10 , and by the @xmath27 reflections @xmath28 , as listed in table [ transformationtable ] . in other words , starting from one initial kernel @xmath29 we easily obtain @xmath30 other kernels : @xmath31 , @xmath32 ,  , corresponding to differently positioned source - receiver pairs .",
    "the random nature of the positions of the original @xmath24 source - receiver pairs ( i.e. no coordinate is exactly zero or exactly equal to plus or minus another coordinate ) ensures us that none of the @xmath22 source - receiver pairs are identical .",
    "the initial @xmath24 source - receiver pairs and the final @xmath22 pairs are shown in figure [ transformationpic ] .",
    "@xmath33    ^3 $ ] with the initial @xmath24 source - receiver pairs ( black = source , red = receiver ) . center : the @xmath22 source - receiver pairs one obtains by applying the @xmath25 symmetry transformations of table [ transformationtable ] on the initial @xmath24 pairs .",
    "right : a single source - receiver pair joined by a straight line and its @xmath25 transformations . plotting all @xmath22 rays obtained in this way would fill the whole cube . ]    for each of the @xmath22 source - receiver pairs we will construct five finite frequency sensitivity kernels of type ( [ kernel ] ) corresponding to five different dominant wavelengths @xmath34 .",
    "thus , in total there will be @xmath5 kernels at our disposal . because of the symmetry transformations used and the random choice of the initial 100 source - receiver pairs , the coverage of the domain by these 24000 kernels is quite uniform . a picture that illustrates this property is too large to include here , but it is available in the online supplementary material in figure a.1 .    in order to give the reader an idea of the size of the fresnel zones of these finite frequency kernels , we include in figure  [ kernelpic ] and in one of the panels of figures  [ saltdomepic1blowup ] and [ saltdomepic2blowup ] a cross - sectional view of five kernels .",
    "each of these kernels corresponds to one of the five wavelengths ( @xmath35 ) and source - receiver distance @xmath36 .",
    "the width of the fresnel zone is proportional to @xmath37 @xcite .",
    "the cross sections in fig .",
    "[ kernelpic ] illustrates not only the typical widths of the kernels , but also their relative amplitudes .",
    "additionally we construct a second operator containing only @xmath38 out of the @xmath5 kernels .",
    "we choose to remove the @xmath39 kernels for which the line connecting source and receiver ( i.e. the central ray ) comes closest to the point @xmath40 . in this way",
    ", we end up with an operator that has a ` hole ' in its coverage of the cube ( see figure a.1 , right ) . using this operator",
    "we will be able to study the effect of non - uniform coverage on reconstructions .",
    "to discretize the model into voxels we calculate the integral of the sensitivity kernels over each voxel ( using a riemann sum with @xmath41 terms / voxel ) for each source - receiver pair and for each of the five dominant wavelengths we consider .",
    "the resulting values make up the operator @xmath42 we want to invert : because there are @xmath5 kernels and @xmath43 voxels , the matrix @xmath42 will have @xmath5 rows and @xmath19 columns .",
    "the use of the @xmath25 symmetry transformations of the cube allows us to save a factor of @xmath25 in memory requirements for our calculations , i.e. we need to compute and store only @xmath44 kernels corresponding to the initial @xmath24 source - receiver pairs and @xmath23 wavelengths .",
    "the remaining ones are easily ( and quickly ) generated from these @xmath44 kernels by the symmetry transformations of the cube ( quick rearrangements of the elements in a 3d array ) .    an additional saving in memory requirements",
    "is obtained by exploiting the fact that most kernels are well localized ( i.e. they are thin ) , and thus that the @xmath45 are practically zero for many voxels . in other words , each row of the matrix @xmath42 is relatively sparse .",
    "although this set - up is completely unrealistic from a physical perspective , this configuration of rays provides for an easy way to compare dense and partial data coverage of the model .",
    "it also allows us to focus on the relative merits of the inversion methods rather than on the difficulties a physically faithful modeling would entail .    in reality ,",
    "sensor coverage is limited to the surface and a few boreholes .",
    "high frequency data with narrow fresnel zones therefore leave significant areas at depth not illuminated by acoustic waves .",
    "low frequency data have wider fresnel zones but suffer a reduced sensitivity to small length scales .",
    "illumination itself does not guarantee resolution as becomes clear when one regards the singular value spectrum of @xmath42 : though the spectrum depends strongly on the experimental set - up , it often has a rapid drop - off and a significant fraction of eigenvalues is either zero or too small to be useful . in the present synthetic experiment , we chose to have about 1 datum for every 10 degrees of freedom in the model",
    ". the uniform random distribution of sources and receivers on the surface of the cube has a favorable influence on the singular value spectrum of @xmath42 ( we were able to confirm this on a scaled - downed version of the present operator @xmath42 , but not on the full @xmath46 matrix ) , and will aid in the reconstruction .",
    "this will allow us to focus on the characteristics of the different reconstruction methods , rather than on the lack of data .",
    "reconstructing the model @xmath13 from the data @xmath16 is done , in principle , by solving the linear system @xmath47 , where @xmath42 denotes , as before , the matrix containing the kernels discretized in the voxel basis .",
    "this system may contain incompatible equations ( due to noise ) , and at the same time be underdetermined ( not enough data to reconstruct all of @xmath13 ) .",
    "the problem of incompatible data can be solved by replacing the original problem with the minimization of a data fidelity term : @xmath48 here and in the following @xmath49 ( without subscripts ) always denotes the usual 2-norm of u : @xmath50 .",
    "although a minimizer always exists ( because of the quadratic nature of the functional ) , it may not always be unique .",
    "in other words , the problem is still underdetermined and an iterative numerical scheme for finding a minimizer of ( [ datamisfit ] ) may diverge .",
    "in fact , because of the existence of data errors we are not even looking for the exact minimizer ( [ datamisfit ] ) .",
    "we rather augment the functional in ( [ datamisfit ] ) by a term that will penalize a whole category of models that is thought to be unphysical .",
    "the prime example of this kind of method is tikhonov regularization @xcite whereby a penalty proportional to the @xmath0-norm of the model is imposed : @xmath51 this will effectively prevent the model from growing unboundedly due to noise in @xmath16 and ill - conditioning of the matrix @xmath42 .",
    "another , closely related , possibility is to impose a penalty consisting of the @xmath0-norm of the ( discrete ) laplacian @xmath52 of @xmath13 : @xmath53 as we will see in section [ reconstructionsection ] , this will enforce a certain degree of smoothness on the reconstructed model .",
    "the laplacian used here is defined as the difference of the model with the local average over the six nearest neighbors : @xmath54 .",
    "the above two versions of the tikhonov regularization method have the advantage of being solvable by linear equations .",
    "the variational equations that determine the minimizers of ( [ l2functional ] ) and ( [ l2deltafunctional ] ) are : @xmath55 and @xmath56 respectively ( with suitable treatment of boundary voxels , @xmath57 in the latter case ) . for these linear equations",
    ", we can use the conjugate gradient algorithm . with @xmath58arbitrary and @xmath59 , we set @xmath60 where @xmath61 is either the unit matrix @xmath62 ( in case we seek to minimize functional ( [ l2functional ] ) ) or @xmath63 ( in case we use functional ( [ l2deltafunctional ] ) ) .",
    "the model estimates @xmath64 converge to the minimizer ( [ l2functional ] ) or ( [ l2deltafunctional ] ) , respectively as @xmath65 increases .",
    "another and much more recent method of regularization consists of imposing a carefully chosen @xmath1-norm penalty @xcite .",
    "it can be shown that this leads to a sparse model , i.e. a model with few nonzero components @xcite .",
    "it would therefore _ not _ be a good idea to apply this technique to the model in the voxel basis ( there is no reason to assume the model would be sparse in that basis ) ; we would rather use this penalty on the coefficients of the model in a different basis in which we believe the model to be sparse .",
    "harmonic functions would allow us to select resolvable scales , but the complete lack of localization of these functions makes them even worse candidates than voxels .",
    "3d wavelets offer a compromise between the concentration of power in both scale and location , and are intuitively more suitable to build geophysically reasonable models .",
    "in fact , our earlier experience ( paper i ) showed the advantages of using a wavelet basis , and constructing the model by finding the minimum of the functional @xmath66 where @xmath67 are the wavelet coefficients of @xmath13 .",
    "this minimization problem can be rewritten as : @xmath68 @xmath69 is the wavelet decomposition matrix and @xmath70 the wavelet synthesis operator .",
    "this type of @xmath1-norm penalty leads to a model that has a sparse wavelet representation , i.e. a model with ( very ) few nonzero wavelet coefficients .",
    "the aim is thus to rely on the properties of the wavelet basis to be able to represent the desired solution with few nonzero components . in geophysics wavelets",
    "are a good choice for seismic reconstruction as they allow for sparse representations of overall smooth functions , while still capable of taking into account the possibility of isolated sharp features @xcite .",
    "another advantage of the @xmath1 method is that they yield noise - free model reconstructions .",
    "this is a consequence of the simple fact that noise in the model can not be represented in a sparse way ( in any reasonable basis ) .",
    "this method is thus able to produce clean models without the need for additional smoothing .",
    "it is important to note that the least squares functional ( [ l1functional ] ) is convex .",
    "this implies that a local minimum of ( [ l1functional ] ) is always a global minimum as well .    in section [ reconstructionsection ]",
    ", we shall consider a number of different choices of orthonormal wavelet bases .",
    "for each of these choices , we have @xmath71 , which we will implicitly assume hereafter .    in order to find the minimizer @xmath72 of the @xmath1 penalized functional ( [ l1functional ] )",
    ", one may use the iterative soft - thresholding algorithm @xcite : @xmath73 with @xmath74 , \\label{top}\\ ] ] and where @xmath75 may be chosen arbitrarily .",
    "the soft - thresholding @xmath76 function operates component - wise and is defined by @xmath77 this algorithm was used in a 2d seismic tomography toy problem in paper i , to which we also refer for a brief but elementary derivation ( section 2 of paper i ) . in effect , it is a simple gradient descent algorithm ( with fixed step length @xmath78 ) where the additional soft - thresholding operation @xmath79 is a mathematical consequence of the @xmath1-norm term present in functional ( [ l1functional ] ) .",
    "the constant @xmath78 should be chosen such that @xmath80 is smaller than or equal to @xmath20 ( @xmath81 is defined as the largest eigenvalue of @xmath82)@xcite . in our calculation",
    "we always choose @xmath83 .",
    "the wavelet transform @xmath69 and its inverse @xmath84 are fast transforms .",
    "this means that they cost only a fraction of the computer time needed to perform one application of @xmath42 or @xmath85 .",
    "in other words , working in a wavelet basis does not significantly change the computational complexity of the reconstruction algorithm .    as the iterative soft - thresholding algorithm ( [ ist ] )",
    "can be slow in practice , we have opted here for using the so - called fast iterative soft - thresholding algorithm ( fista ) @xcite ( see also earlier work of nesterov @xcite ) : @xmath86 with @xmath87 and @xmath88 .",
    "the fista algorithm has practically the same computational complexity as the iterative soft - thresholding algorithm ( [ ist ] ) .",
    "it only requires one additional vector addition . with this algorithm",
    "the @xmath1 penalized cost function ( [ l1functional ] ) , evaluated at @xmath89 , is bounded by @xmath90 from its limiting value : @xmath91 as opposed to the @xmath92 decrease : @xmath93 that can be proven for algorithm ( [ ist ] ) .",
    "these upper bounds are valid non - asymptotically , i.e. even for small @xmath65 @xcite .",
    "recently , mathematical advances on direct ways of constraining the number of nonzero components in a reconstructed model have appeared . in @xcite",
    "an iterative hard - thresholding algorithm is proposed of the following form : @xmath94 with @xmath95 \\label{tildetop}\\ ] ] and where the hard - thresholding operation @xmath96 sets all but the largest ( in absolute value ) @xmath97 components of @xmath98 to zero .",
    "this algorithm converges to a _",
    "local _ minimum of @xmath99 here @xmath100 denotes the number of nonzero coefficients of @xmath101 .",
    "just as with the @xmath1 penalty method , we shall apply this technique using a wavelet basis .",
    "the underlying reason is again the suitability of a wavelet basis to represent a physically acceptable model with few nonzero wavelet coefficients .",
    "we stress that the appearance of the hard - thresholding operation @xmath102 is a mathematical consequence of the constraint @xmath103 , just as soft - thresholding in ( [ top ] ) is a mathematical consequence of the @xmath1 term in ( [ l1functional ] ) @xcite .",
    "the algorithm ( [ iht ] ) converges very slowly .",
    "we therefore propose the following fista / nesterov - like modification : @xmath104 with @xmath87 and @xmath88 .",
    "as far as the authors know , this is the first time this algorithm is proposed .",
    "although there is no proof of convergence yet , we found that it worked quite well on the examples that we studied ( see section [ reconstructionsection ] ) .",
    "we used the same choice for the step - length @xmath78 as with the @xmath1 algorithm : @xmath83 .",
    "the choice of the number of nonzero model wavelet coefficients @xmath97 in method ( [ l0functional ] ) is discussed in section [ choicesection ] .    on a side note , it would also be possible to calculate a local minimizer of the ( non - convex ) functional @xmath105 using component - wise hard - thresholding ( with a fixed threshold @xmath106 ) @xcite .",
    "however , @xcite seems to prefer the formulation ( [ l0functional ] ) that imposes a _ fixed _ number of nonzeros in each step of the iteration , rather than a fixed lower bound @xmath106 for the absolute values of the wavelet coefficients .",
    "although no proof of convergence of algorithm ( [ fiht ] ) is given , we shall refer to it by the name ` @xmath2 method ' ( instead of calling it ` hard - thresholded nesterov accelerated gradient descent algorithm ' ) .      a final penalty term we will consider is the so - called ` total variation ' ( tv ) penalty : @xmath107 with @xmath108 and similarly for @xmath109 and @xmath110 .",
    "this penalization will favor piece - wise constant models in the voxel basis .",
    "unfortunately , the equations that determine the minimizer ( [ tvfunctional ] ) are again nonlinear .",
    "we will use a reweighed conjugate gradient method @xcite to determine the minimum of the tv functional ( [ tvfunctional ] ) . more specifically , defining the weights @xmath111 , we shall use algorithm ( [ conjgrad ] ) where we choose @xmath112 ( which depends on the iteration @xmath65 ) and use that @xmath113 . because of the non - quadratic nature of functional ( [ tvfunctional ] ) , the conjugate gradient algorithm no longer preserves conjugacy between successive search directions @xmath114 as @xmath65 grows .",
    "for this reason , the iteration is also reset every so often ( in accordance with @xcite ) .",
    "an algorithm similar to ( [ ist ] ) and ( [ fista ] ) may also be used to find the minimizer of the tv penalized functional ( [ tvfunctional ] ) @xcite .",
    "in fact , in @xcite it is shown how the @xmath1 norm penalty and the tv penalty can be combined in a single functional .",
    "as such , the minimizers defined by ( [ l2functional ] ) , ( [ l2deltafunctional ] ) , ( [ l1functional ] ) and ( [ tvfunctional ] ) still depend on the penalty parameter @xmath115 , or in case of the hard - thresholding algorithm ( [ fiht ] ) on the parameter @xmath97 . in the reconstructions below",
    ", we select this parameter by requiring in each instance that the reconstructed model @xmath116 fits the data @xmath16 as well as possible , but not any better than the noise level : @xmath117 with @xmath118 representing the noise vector . in other words ,",
    "the discrepancy principle tells us to choose the penalty parameter @xmath115 ( or @xmath97 ) such that @xmath119 where @xmath120 is the noise variance ( if different data have different variance , it is simplest to divide from the outset each row as well as the right hand side by the standard deviation of its datum , and set @xmath121 in ( [ penaltychoice ] ) ) . in practice , this means that we will have to perform the minimization several times , until a suitable value of @xmath115 or @xmath97 is determined for each reconstruction .",
    "in this section we present some sample reconstructions using the algorithms mentioned in section [ algorithmsection ] , applied to the finite frequency tomography problem described in section [ setupsection ] .",
    "first we will consider a simple checkerboard input model that we will try to reconstruct from incomplete and noisy data .",
    "we will also look at a more complicated salt dome model which we obtained from bp america , inc .    in each case",
    ", our procedure will be the following .",
    "we start from a known input model @xmath122 from which we construct synthetic noisy data @xmath16 : @xmath123 the noise @xmath118 is taken from a gaussian distribution , with zero mean and variance @xmath120 chosen in such a way that @xmath124 ; in other words , we add @xmath125 noise to the noiseless data .",
    "the goal is then to try to reconstruct @xmath122 as well as possible from @xmath16 and @xmath42 .",
    "for this we will use methods ( [ l2functional ] ) , ( [ l2deltafunctional ] ) , ( [ l1functional ] ) , ( [ l0functional ] ) and ( [ tvfunctional ] ) , and compare the results . since we know the noise variance @xmath120",
    ", we can use the criterion ( [ penaltychoice ] ) to choose the penalty parameter @xmath115 or @xmath97 .",
    "this parameter will be different for the various synthetic data and for the various penalties that we impose .    for the wavelet based methods ( [ l1functional ] ) and ( [ l0functional ] ) , we also need to choose a specific wavelet family .",
    "there are many wavelet bases , with varying degrees of smoothness and approximation properties @xcite .",
    "this gives us the opportunity to adapt our choice of wavelet basis to the model : we will choose the basis in which we suspect the model to be sparse .",
    "in particular , for the checkerboard reconstruction we will use haar wavelets because we know that the true solution is very sparse in that basis .",
    "we thus expect a very accurate reconstruction in that case . for the salt dome input model , we will compare the effects of different choices of wavelets bases on the reconstruction .",
    "our choice will include haar wavelets , d4 wavelets and also directional dual tree wavelets .",
    "haar wavelets are the least smooth , and directional wavelets are the most smooth of these three .",
    "the first example consists of a checkerboard pattern ; in other words the input model @xmath122 is piecewise constant ( @xmath126 ) on small cubes of @xmath127 by @xmath127 by @xmath127 voxels .",
    "it mainly serves as a proof of principle for the @xmath1 wavelet method ( [ l1functional ] ) because we know that this input model is sparse in the haar wavelet basis @xcite .",
    "the haar wavelets are piecewise constant .",
    "in fact , the model only has @xmath128 nonzero coefficients in that basis ( out of @xmath19 ) .",
    "hence , we may expect that the @xmath1 method will work very well with this model and this basis .    a single horizontal slice of the checkerboard input model and its four reconstructions are shown in figure [ checkerpic1 ] .",
    "these four reconstructions use all @xmath5 kernels so that no particular region in the model is favored or disadvantaged .",
    "the @xmath1 reconstruction ( [ l1functional ] ) with haar wavelets is visually the most faithful to the original , closely followed by the @xmath2-haar reconstruction .",
    "the @xmath0-reconstructions ( [ l2functional],[l2deltafunctional ] ) and the tv reconstruction ( [ tvfunctional ] ) display smooth transitions between @xmath129 and @xmath130 .",
    "the reconstruction result of the simple @xmath0 method ( [ l2functional ] ) is quite noisy , which is not the case for the other methods . for completeness and viewing convenience ,",
    "the online supplementary material includes a picture ( figure a.2 ) of all the horizontal slices of the input model and its various reconstructions .",
    "the smoothing effect of the reconstructions can quantitatively be seen from the histogram of the reconstructed model amplitudes ( see figure [ histpic ] ) .",
    "the @xmath0 reconstruction takes on mostly values around zero , whereas the input model only has amplitudes @xmath129 and @xmath130 ( vertical blue lines ) . in this case , the @xmath1 haar reconstruction does a very good job at recovering the @xmath126 amplitude distribution , as it is naturally well suited for the particular checkerboard model used .",
    "the @xmath2 method does second best and the tv method ( [ tvfunctional ] ) does third best from this point of view .",
    "it is surprising that the @xmath1 method outperforms the @xmath2 method in this case .",
    "the checkerboard model shows that good reconstructions are possible if one has very good prior knowledge on the model .",
    "when using the @xmath2 and @xmath1 methods this requires a basis in which the desired model is very sparse .",
    "the checkerboard model that was used here aligns optimally with the chosen haar basis .",
    "the results of @xmath1 and @xmath2 reconstructions deteriorate when the checkerboard pattern is shifted w.r.t the haar basis or when the size of the fields are changed ( the haar basis decomposition of such a checkerboard would seize to be sparse ) .",
    "as such , the checkerboard model we choose is very particular . for realistic reconstruction scenarios one should not expect such excellent results .",
    "a discussion of the mean square error of the various reconstructions is given in section [ compaspectssection ] , where other computational aspects are also discussed .     data .",
    "the whitish transitions in several of the reconstructions are the result of smoothing between @xmath129 and @xmath130 of the input model .",
    "as the input checkerboard has fields of size @xmath131 with constant model value @xmath132 , the smoothing occurs with period @xmath127 as well .",
    "in addition , the @xmath0 reconstruction has a distinctive noisy appearance .",
    "the @xmath1 reconstruction using haar wavelets is visually indistinguishable from the input model .",
    "the @xmath2 method does almost as well . ]      in this section we try to reconstruct a complex 3d model of a realistic salt body in the subsurface .",
    "the complex salt dome model was obtained from a prestack depth - migration of field seismic data in the deep - water part of the gulf of mexico , and was kindly provided to us by bp america , inc . to better accommodate the straight - ray tomography used in this paper , the sediment velocities surrounding the salt dome model that were present in the original model provided to us , were replaced with a constant velocity .",
    "this model was superimposed on a background model with long - wavelength variations ( smoothed gaussian ) .",
    "the model has a rather sharp contrast between the velocity in the salt and in the surrounding background model , providing for sharp edges .",
    "a single horizontal slice through the resulting salt dome model is pictured in figures [ saltdomepic1blowup ] and [ saltdomepic2blowup ] . for completeness",
    ", all 64 horizontal slices are shown in figure a.3 ( right ) of the online supplementary material ( as well as the smoothed gaussian that was added in ; figure a.3 , center ) .",
    "three contour plots of this model are shown in figure [ saltdome3dpic ] , corresponding to the model values @xmath133 ,",
    "@xmath134 and @xmath135 .",
    "the model contains @xmath136 voxels as in the checkerboard examples .    ) . ]",
    "we perform the same type of experiment as before : we construct synthetic data and add @xmath125 gaussian noise to it . from this noisy data",
    ", we try to reconstruct the input model .",
    "there are two differences with the checkerboard reconstructions .",
    "firstly , we will compare several different wavelet families for the @xmath1 and @xmath2 reconstructions ( in this case , there is no obvious reason to prefer haar wavelets over other wavelet bases ) .",
    "secondly , we will repeat the reconstruction experiment for an operator that has only @xmath38 kernels instead of @xmath5 , to simulate imperfect coverage of the model domain by the kernels . in other words , with the @xmath38 kernel reconstruction , a region of the model is ill resolved .",
    "the wavelet families used are , in order of increasing smoothness : haar @xcite , d4 @xcite and so - called directional dual tree ( dt ) wavelets @xcite .",
    "the haar and d4 wavelet transform on the cube are direct products of the corresponding wavelet transforms in 1d .",
    "the dt wavelet transform is not and it has , by construction , better directional sensitivity . the d4 wavelets that we will use do not suffer from edge effects as they do not use periodic boundary conditions , but follow the interval scheme proposed in @xcite .",
    "other model parameterizations that could be used are shearlets or curvelets ( they are particularly suited to sparsely represent models with singularities along curves or surfaces ) , but we did not include them in our study @xcite .    judging the success of an algorithm to reconstitute the input model invariably involves a degree of subjectiveness , even if one designs a numerical measure for goodness of model fit .",
    "such measure might also depend on the goal of the scientific experiment conducted .",
    "for example , if one deducts temperatures from velocity variations , it is more important that the amplitudes are correct and less important that sharp edges of an anomaly are preserved , but a structural geologist may be more interested in the edges and may wish to involve the misfit of the gradient , for example .    in table",
    "[ tablemisfit ] , we list the amplitude misfit ( @xmath137 ) and judge the fit to other features visually . a single horizontal slice ( number @xmath138 ) of the different reconstructions is pictured in figure [ saltdomepic1blowup ] and all @xmath139 horizontal slices are shown in figure a.4 in the online supplementary material .",
    "for the reconstructions using all @xmath5 kernels , the tv method works best based on the final resulting error , as well as visual inspection .",
    "it is closely followed by the @xmath1 method using dual tree wavelets ( @xmath1-dt ) and by the @xmath0 method with laplacian smoothing ( @xmath0-@xmath63 ) .",
    "the @xmath1 method with d4 wavelets does better than with haar wavelets , that has a relative reconstruction error almost as bad as obtained using the simple @xmath0 penalized method .",
    "however , the `` top three '' methods ( tv , @xmath1-dt , @xmath0-@xmath63 ) produce much smoother models than the input model . in case the correct sharpness of features is a desirable characteristic of the solution , the @xmath1 reconstructions with haar or d4 wavelets are more faithful to the input data . in this case",
    "one may well prefer d4 over haar to avoid the rather blocky nature of the shapes .",
    "the qualitative differences with the noisy @xmath0 reconstruction are obvious .",
    "the @xmath2 reconstructions that were obtained using iterative hard - thresholding , are less appealing .",
    "numerically the @xmath2-haar and @xmath2-d4 perform worst of all reconstructions , whereas @xmath2-dt comes in fifth .",
    "the reconstructions with only @xmath38 kernels are shown in figure [ saltdomepic2blowup ] ( a single horizontal slice ) and figure a.5 in the supplementary material ( all horizontal slices ) .",
    "the most interesting comparisons are again done visually .",
    "the lack of data coverage affects most strongly the areas around voxel @xmath140 in slice 25 ( figure [ saltdomepic2blowup ] ) and the lower left corner of slices 1344 ( third and fourth row in fig . a.5 especially ) .",
    "not surprisingly , none of the algorithms is able to ` recreate ' the model where there are no data at all .",
    "but close inspection of the model near the edge of the region affected by the data gap shows that the haar and d4 wavelets produce the model that is least contaminated by smoothing effects beyond the gap , with d4 occasionally trying to correctly ` fill in ' .",
    "the @xmath2-d4 reconstruction creates a distinctive artifact in this area .",
    "we speculate that this is caused by the non - convex nature of the @xmath2 problem ( [ l0functional ] ) .",
    "apart from the aspect of the visual reconstruction quality it is also important to compare reconstruction times .",
    "the four numerical algorithms that were used conjugate gradient , fast iterative soft and hard thresholding , and reweighed conjugate gradient all need one application of @xmath42 and one application of @xmath85 per iteration step .",
    "these dominate the other , but much faster , operations such as addition of vectors , vector norms etc that are also present in each iteration step .",
    "the forward and inverse wavelet transforms that are used in some of the methods ( via the fast wavelet transform algorithm ) also take a negligible time compared to an application of @xmath42 and @xmath85 .",
    "it follows that it is sufficient to compare the number of iterations when we evaluate the efficiency of different reconstructions .",
    "the number of iterations and corresponding relative reconstruction errors @xmath137 are given in table [ tablemisfit ] .",
    "in all cases , the iterative reconstruction algorithms were started from @xmath141 or @xmath142 .",
    "for the checkerboard model the data in table [ tablemisfit ] show that the @xmath1 and @xmath2 methods do extraordinarily well , with a mean square error far below what could be expected based on the data noise level of @xmath125 . in the same sense ,",
    "the total variation minimization and the laplacian penalization perform somewhat better than the simple @xmath0 method ; a large number of simulations with different noise realizations would be necessary to verify whether this is a statistically significant difference . in case of the checkerboard reconstructions ,",
    "only @xmath24 iterations were performed for each method .",
    "this shows that the @xmath1 and @xmath2 methods can be very successful if the sought after model is very sparse in the basis used , even with a limited number of iterations .",
    "+ we have also verified that the functionals ( [ l2functional ] ) , ( [ l2deltafunctional ] ) , ( [ l1functional ] ) , ( [ l0functional ] ) and ( [ tvfunctional ] ) remain almost constant after this number of iterations , as did the relative distance to the input model .    in case of the salt dome reconstructions , the @xmath2-haar , @xmath2-d4 and the simple @xmath0 method",
    "do worst ( in terms of reconstruction error ) closely followed by the @xmath1-haar method .",
    "the latter is due to the inappropriateness of the haar basis to represent the salt dome model in a sparse fashion .",
    "the other three methods ( @xmath1-d4 , @xmath1-dt and tv ) do about equally as the @xmath143 method for the salt dome reconstruction with @xmath5 data .    to gauge wether",
    "there is a significant difference in reconstruction error between the tv , @xmath1-dt and @xmath144 methods ( and possibly the @xmath2-dt technique ) , one would also need to repeat the numerical experiment with many noise realizations .",
    "the nonlinear reconstructions with @xmath5 data were done with @xmath145 iterations .",
    "we used formula ( [ fistabound ] ) to derive a rough upper bound on the relative error remaining in the functional ( [ l1functional ] ) w.r.t to the minimum after this number of iterations : @xmath146 where we used @xmath141 and approximated @xmath147 by @xmath148 .",
    "in other words , the calculated value of the minimum of the functional is accurate up to three decimal places ( this bound is valid for the three wavelet families ) . a simple plot of @xmath149 as a function of @xmath65 also reveals that the functional is virtually constant after @xmath145 iterations , a conclusion which also holds for the tv method , the @xmath2 method and for the @xmath0 methods ( after @xmath24 iterations ) .",
    "the corresponding reconstructions with @xmath38 data were done with only @xmath24 iterations . in this way",
    "we demonstrate that a reasonable result can already be obtained without an excessively long computation time .",
    "this is evident by comparing figures [ saltdomepic1blowup ] and [ saltdomepic2blowup ] ( or figures a.4 and a.5 in the supplementary material ) : apart from the unresolved region near @xmath150 the reconstructions are pairwise almost identical , despite the significant differences in number of iterations . in other words ,",
    "the @xmath1 and tv algorithms already succeed , after a small number of iterations , in producing qualitatively quite characteristic reconstructions .",
    "+ in case of the @xmath2 reconstructions the differences ( far from the unresolved region ) are somewhat larger , we believe , because the @xmath2 method only finds a local minimum of ( [ l0functional ] ) .    as a result of the thresholding ,",
    "the @xmath2 and @xmath1 algorithms provides sparse models at every iteration step ( not just in the limit @xmath151 ) . in other words",
    "it is not necessary , or desirable , to run the fista / nesterov style algorithm for a very long time .",
    "even after a small number of iteration , they will provide a sparse model that fits the data to within its error bars .",
    ".the number of iterations and the resulting reconstruction error for the various models and methods . [ cols= \" < , > , > , > , > , > , > \" , ]",
    "in paper i we showed how a large scale anomaly could be reconstituted even where it was ill resolved because of the selective nature of the wavelet coefficients and the @xmath1 criterion : one wavelet coefficient reconstituting a large , circular , anomaly gave a better optimization than a couple of coefficients reconstituting only the resolved part . with the results of the much more complex salt dome model at hand , we must now conclude that this probably represents more a ( lucky ) exception than a rule .",
    "there is no magical solution for the absence of data .    for the checkerboard reconstructions ,",
    "the @xmath1 method with haar wavelets is able to do very well much better than could be expected based on the data themselves because the haar wavelets are very efficient in representing _ this particular _ checkerboard pattern in a sparse way .",
    "the success of the @xmath1 method thus depends heavily on the choice of a suitable basis . for realistic models",
    "it is much more difficult to find a good sparsifying basis , and the reconstruction errors will be much larger . for the 3d salt dome reconstruction , one could argue that the @xmath1-dt method does well because it has good directional sensitivity and is therefore able to adapt to the `` curvy '' nature of the outline of the salt body , as opposed to @xmath1-haar and @xmath1-d4 methods .",
    "the @xmath0-@xmath63 method does well because the gaussian background that is present in the model is smooth ` noise ' and this is exactly the prior information put into the minimization criterion .",
    "the tv method does well as the main part of the salt dome model is roughly piecewise constant and tv favors that .",
    "the @xmath2 methods do not perform particularly well , both from a quantitative as a qualitative side .",
    "the wavelets , however , do have the distinctive quality of retaining sharp features even when regularizing by penalizing highly oscillatory models .",
    "if the preservation of sharp boundaries is not as important as the correct estimation of amplitudes , the smoothed solution , using the @xmath0-@xmath63 method , is to be preferred as it is fully linear and efficient to solve with conjugate gradients .",
    "methods using wavelets with small support , however , are able to retain sharp features , despite their regularization effect that penalizes highly oscillatory models .",
    "these methods are thus preferable when edges are important ; our preference would go to the @xmath1-d4 algorithm which gives less blocky solutions than @xmath1-haar . in no cases",
    "should one use simple norm damping ( @xmath0 method ) . without imposing additional smoothing , i.e. while still allowing for sharp transitions , the @xmath1 methods yield models which do not show signs of noise .",
    "the @xmath2 methods , which use hard - thresholding of wavelet coefficients rather than soft - thresholding , can not outperform the @xmath1 methods . in some cases they appear to produce severe artifacts .",
    "another reason not to favor @xmath2 penalties is that they only produce a local minimum of the functional ( [ l0functional ] ) .",
    "this may lead to larger variability in the reconstructions ( depending on the starting point of the iteration ) .",
    "there is currently no proven technique to tackle the minimization more efficiently that algorithm ( [ iht ] ) .",
    "the ( unproven ) method ( [ fiht ] ) proposed in this paper is , as far as the authors can tell , new .",
    "we conclude that using hard - thresholding is less appealing than using soft - thresholding of wavelet coefficients : the mathematical theory is less developed , the hard - thresholded reconstruction may exhibit significant artifacts and the reconstructions are not better than the ones obtained with soft - thresholding ( @xmath1 method ) .",
    "speed - wise the nonlinear methods can not do better than the conjugate gradient algorithm for the @xmath0 methods .",
    "many applied mathematics groups @xcite are currently working on speeding up the iterative soft - thresholding algorithm ( [ ist ] ) , but it is still at least as time - consuming to use the @xmath1 norm as it is to use the @xmath0 norm for penalization , especially for severely ill - conditioned matrices and low noise conditions @xcite .    in case",
    "the data is heavily contaminated by noise , it follows from relation ( [ penaltychoice ] ) that a large value of the penalty parameter @xmath115 must be chosen . in @xcite",
    "it was demonstrated that many competing algorithms for minimizing an @xmath1 penalized functional converge quickly in such a case .",
    "we therefore expect that such methods remain competitive with the traditional @xmath0 smoothing methods in case of travel time seismic tomography where the data noise level may reach 50% .",
    "based on the results in this paper , we can conclude that the nonlinear methods offer a way to invert data and denoise the resulting model in a single procedure without necessarily smoothing the model too much .",
    "the two salt dome examples also show that a good reconstruction , clearly showing the characteristic effects of the penalizations used , is still possible with a very limited number of iterations : this is a consequence of the fista algorithm producing sparse models at every iteration step .",
    "sparse models can therefore be constructed with few iterations and little computer time ( see @xcite for a discussion of the number of iterations used as a regularization parameter ) .    as an alternative to d4 or complex dt wavelets one could consider using curvelets or shearlets , as they are naturally designed to sparsely represent singularities along smooth curves , such as , e.g. , the sediment salt interface in our model . in this work",
    "we have not studied how the different regularization methods behave in conjunction with these particular choices of dictionaries .",
    "the authors would like to thank bp america inc . for kindly providing the 3d salt dome velocity model and several referees for their suggestions that helped improve the manuscript .",
    "part of this research has been supported by the francqui foundation ( il ) , the vub - goa 062 grant ( i d , il ) , the fwo - vlaanderen grant g.0564.09n ( i d , il ) and nsf grant dms-0530865 ( i d ) .          i.  loris , g.  nolet , i.  daubechies , f.  a. dahlen , tomographic inversion using @xmath1-norm regularization of wavelet coefficients , geophysical journal international 170  ( 1 ) ( 2007 ) 359370 . http://dx.doi.org/10.1111/j.1365-246x.2007.03409.x [ ] .",
    "s.  chevrot , l.  zhao , multiscale finite frequency rayleigh wave tomography of the kaapvaal craton , geophysical journal international 169 ( 2007 ) 201215 .",
    "http://dx.doi.org/10.1111/j.1365-246x.2006.03289.x [ ] .",
    "e.  j. cands , j.  k. romberg , t.  tao , stable signal recovery from incomplete and inaccurate measurements , communications on pure and applied mathematics 59  ( 8) ( 2006 ) 12071223 . http://dx.doi.org/10.1002/cpa.20124 [ ] .",
    "d.  l. donoho , for most large underdetermined systems of linear equations the minimal @xmath1-norm solution is also the sparsest solution , communications on pure and applied mathematics 59 ( 2006 ) 797829 .",
    "http://dx.doi.org/10.1002/cpa.20132 [ ] .",
    "f.  j. herrmann , g.  hennenfent , non - parametric seismic data recovery with curvelet frames , geophysical journal international 173  ( 1 ) ( 2008 ) 233248 .",
    "http://dx.doi.org/10.1111/j.1365-246x.2007.03698.x [ ] .",
    "n.  favier , s.  chevrot , sensitivity kernels for shear wave splitting in transverse isotropic media , geophysical journal international 153 ( 2003 ) 213228 .",
    "http://dx.doi.org/10.1046/j.1365-246x.2003.01894.x [ ] .",
    "i.  daubechies , m.  defrise , c.  de  mol , an iterative thresholding algorithm for linear inverse problems with a sparsity constraint , communications on pure and applied mathematics 57  ( 11 ) ( 2004 ) 14131457 . http://dx.doi.org/10.1002/cpa.20042 [ ] .",
    "j.  oliveira , j.  bioucas - dias , m.  figueiredo , adaptive total variation image deblurring : a majorization - minimization approach , signal processing 89  ( 9 ) ( 2009 ) 16831693 . http://dx.doi.org/10.1016/j.sigpro.2009.03.018 [ ] .",
    "j.  bect , l.  blanc - fraud , g.  aubert , a.  chambolle , a @xmath154-unified variational framework for image restoration , in : t.  pajdla , j.  matas ( eds . ) , proceedings of the 8th european conference on computer vision , vol .",
    "iv , springer verlag , 2004 , pp .",
    "113 .    j.",
    "bioucas - dias , m.  figueiredo , a new twist : two - step iterative shrinkage / thresholding algorithms for image restoration , ieee transactions on image processing 16 ( 2007 ) 29802991 . http://dx.doi.org/10.1109/tip.2007.909319 [ ] .",
    "a.  cohen , i.  daubechies , p.  vial , wavelets on the interval and fast wavelet transforms , journal for applied and computational harmonic analysis 1 ( 1993 ) 5481 .",
    "http://dx.doi.org/10.1006/acha.1993.1005 [ ] .",
    "n.  kingsbury , image processing with complex wavelets , philosophical transactions of the royal society a : mathematical , physical and engineering sciences 357  ( 1760 ) ( 1999 ) 25432560 .",
    "http://dx.doi.org/10.1098/rsta.1999.0447 [ ] .",
    "m.  a.  t. figueiredo , r.  d. nowak , s.  j. wright , gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , ieee journal of selected topics in signal processing ( special issue on convex optimization methods for signal processing ) 1 ( 2007 ) 586598 .",
    "http://dx.doi.org/10.1109/jstsp.2007.910281 [ ] .",
    "m.  elad , b.  matalon , m.  zibulevsky , coordinate and subspace optimization methods for linear least squares with non - quadratic regularization , applied and computational harmonic analysis 23  ( 3 ) ( 2007 ) 346367 . http://dx.doi.org/10.1016/j.acha.2007.02.002 [ ] .",
    "kim , k.  koh , m.  lustig , s.  boyd , d.  gorinevsky , an interior - point method for large - scale @xmath1-regularized least squares , ieee journal of selected topics in signal processing 1  ( 4 ) ( 2007 ) 606617 .",
    "http://dx.doi.org/10.1109/jstsp.2007.910971 [ ] .",
    "w.  yin , s.  osher , d.  goldfarb , j.  darbon , bregman iterative algorithms for @xmath1-minimization with applications to compressed sensing , siam journal on imaging sciences 1  ( 1 ) ( 2008 ) 143168 . http://dx.doi.org/10.1137/070703983 [ ] .",
    "m.  defrise , c.  de  mol , a note on stopping rules for iterative regularization methods and filtered svd , in : p.  c. sabatier ( ed . ) , inverse problems : an interdisciplinary study , academic press , 1987 , pp ."
  ],
  "abstract_text": [
    "<S> the effects of several nonlinear regularization techniques are discussed in the framework of 3d seismic tomography . </S>",
    "<S> traditional , linear , @xmath0 penalties are compared to so - called sparsity promoting @xmath1 and @xmath2 penalties , and a total variation penalty . which of these algorithms is judged optimal depends on the specific requirements of the scientific experiment . </S>",
    "<S> if the correct reproduction of model amplitudes is important , classical damping towards a smooth model using an @xmath0 norm works almost as well as minimizing the total variation but is much more efficient . if gradients ( edges of anomalies ) should be resolved with a minimum of distortion , we prefer @xmath1 damping of daubechies-4 wavelet coefficients </S>",
    "<S> . it has the additional advantage of yielding a noiseless reconstruction , contrary to simple @xmath0 minimization ( ` tikhonov regularization ' ) which should be avoided . in some of our examples , </S>",
    "<S> the @xmath2 method produced notable artifacts . </S>",
    "<S> in addition we show how nonlinear @xmath1 methods for finding sparse models can be competitive in speed with the widely used @xmath0 methods , certainly under noisy conditions , so that there is no need to shun @xmath1 penalizations . </S>"
  ]
}