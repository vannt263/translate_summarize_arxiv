{
  "article_text": [
    "strassen s algorithm  @xcite was the first sub - cubic algorithm for matrix multiplication .",
    "its improvement by winograd  @xcite led to a highly practical algorithm .",
    "the best asymptotic complexity for this computation has been successively improved since then , down to @xmath0 in @xcite ( see @xcite for a review ) , but strassen - winograd s still remains one of the most practicable .",
    "former studies on how to turn this algorithm into practice can be found in  @xcite and references therein for numerical computation and in @xcite for computations over a finite field .",
    "+ in this paper , we propose new schedules of the algorithm , that reduce the extra memory allocation , by three different means : by introducing a few pre - additions , by overwriting the input matrices , or by using a first recursive level of classical multiplication .",
    "these schedules can prove useful for instance for memory efficient computations of the rank , determinant , nullspace basis , system resolution , matrix inversion ... indeed , the matrix multiplication based lqup factorization of  @xcite can be computed with no other temporary allocations than the ones involved in its block matrix multiplications  @xcite . therefore the improvements on the memory requirements of the matrix multiplication , used together for instance with cache optimization strategies  @xcite , will directly improve these higher level computations .",
    "we only consider here the computational complexity and space complexity , counting the number of arithmetic operations and memory allocations .",
    "the focus here is neither on stability issues , nor really on speed improvements .",
    "we rather study potential memory space savings .",
    "further studies have thus to be made to assess for some gains for in - core computations or to use these schedules for numerical computations .",
    "they are nonetheless already useful for exact computations , for instance on integer / rational or finite field applications  @xcite .",
    "the remainder of this paper is organized as follows : we review strassen - winograd s algorithm and existing memory schedules in sections [ sec : algo ] and [ ssec : constinput ] .",
    "we then present in section [ sec : pebble ] the dynamic program we used to search for schedules .",
    "this allows us to give several schedules overwriting their inputs in section [ sec : overwrite ] , and then a new schedule for @xmath1 using only two extra temporaries in section [ sec : hyb ] , all of them preserving the leading term of the arithmetic complexity .",
    "finally , in section [ sec : mix ] , we present a generic way of transforming non in - place matrix multiplication algorithms into in - place ones ( without any extra temporary space ) , with a small constant factor overhead",
    ". then we recapitulate in table [ tab : resume ] the different available schedules and give their respective features .",
    "we first review strassen - winograd s algorithm , and setup the notations that will be used throughout the paper .",
    "+ let @xmath2 and @xmath3 be powers of @xmath4 .",
    "let @xmath5 and @xmath6 be two matrices of dimension @xmath7 and @xmath8 and let @xmath9 . consider the natural block decomposition : @xmath10 where @xmath11 and @xmath12 respectively have dimensions @xmath13 and @xmath14 .",
    "winograd s algorithm computes the @xmath15 matrix @xmath9 with the following 22 block operations : +    8 additions : @xmath16    7 recursive multiplications : @xmath17    7 final additions : @xmath18    the result is the matrix : @xmath19 $ ] .",
    "figure  [ fig : winotask ] illustrates the dependencies between these tasks .",
    "unlike the classic multiplication algorithm , winograd s algorithm requires some extra temporary memory allocations to perform its 22 block operations .",
    "we first consider the basic operation @xmath20 .",
    "the best known schedule for this case was given by @xcite .",
    "we reproduce a similar schedule in table  [ tab : schedule : ab ] .     1 & @xmath21 & @xmath22 & 12 & @xmath23 & @xmath22 + 2 & @xmath24 & @xmath25 & 13 & @xmath26 & @xmath27 + 3 & @xmath28 & @xmath29 & 14 & @xmath30 & @xmath29 + 4 & @xmath31 & @xmath22 & 15 & @xmath32 & @xmath27 + 5 & @xmath33 & @xmath25 & 16 & @xmath34 & @xmath35 + 6 & @xmath36 & @xmath35 & 17 & @xmath37 & @xmath27 + 7 & @xmath38 & @xmath22 & 18 & @xmath39 & @xmath25 + 8 & @xmath40 & @xmath25 & 19 & @xmath41 & @xmath42 + 9 & @xmath43 & @xmath27 & 20 & @xmath44 & @xmath29 + 10 & @xmath45 & @xmath22 & 21 & @xmath46 & @xmath42 + 11 & @xmath47 & @xmath42 & 22 & @xmath48 & @xmath42 +    it requires two temporary blocks @xmath22 and @xmath25 whose dimensions are respectively equal to @xmath49 and @xmath50 .",
    "thus the extra memory used is : @xmath51 summing these temporary allocations over every recursive levels leads to a total amount of memory , where for brevity @xmath52 : @xmath53 we can prove in the same manner the following lemma :    [ lem : sum ] let @xmath54 , @xmath3 and @xmath55 be powers of two , @xmath56 be homogeneous , @xmath57 and @xmath58 be a function such that @xmath59 then @xmath60 .    in the remainder of the paper",
    ", we use @xmath61 to denote the amount of extra memory used in table number  @xmath62 .",
    "the amount of extra memory we consider is always the sum up to the last recursion level .",
    "finally , assuming @xmath63 gives a total extra memory requirement of @xmath64      for the more general operation @xmath65 , a first nave method would compute the product @xmath66 using the scheduling of table  [ tab : schedule : ab ] , into a temporary matrix @xmath67 and finally compute @xmath68 .",
    "it would require @xmath69 extra memory allocations in the square case .",
    "+ now the schedule of table  [ tab : schedule : abc ] due to  @xcite only requires 3 temporary blocks for the same number of operations ( @xmath70 multiplications and @xmath71 additions ) .     1 & @xmath31 & @xmath22 & 12 & @xmath45 & @xmath22 + 2 & @xmath33 & @xmath25 & 13 & @xmath39 & @xmath25 + 3 & @xmath72 & @xmath73 & 14 & @xmath74 & @xmath27 + 4 & @xmath75 & @xmath35 & 15 & @xmath76 & @xmath27 + 5 & @xmath77 & @xmath27 & 16 & @xmath78 & @xmath29 + 6 & @xmath38 & @xmath22 & 17 & @xmath21 & @xmath22 + 7 & @xmath79 & @xmath25 & 18 & @xmath24 & @xmath25 + 8 & @xmath80 & @xmath73 & 19 & @xmath81 & @xmath73 + 9 & @xmath82 & @xmath42 & 20 & @xmath83 & @xmath35 + 10 & @xmath84 & @xmath73 & 21 & @xmath85 & @xmath29 + 11&@xmath86 & @xmath42 & 22 & & +    the required three temporary blocks @xmath87 have dimensions @xmath88 , @xmath50 and @xmath89 . since the two temporary blocks in schedule  [ tab : schedule : ab ] are smaller than the three ones here , we have @xmath90 .",
    "hence , using lemma  [ lem : sum ] , we get @xmath91 with @xmath63 , this gives @xmath92 + we propose in table  [ tab : abc:2tmp ] a new schedule for the same operation @xmath93 only requiring two temporary blocks . +",
    "our new schedule is more efficient if some inner calls overwrite their temporary input matrices .",
    "we now present some overwriting schedules and the dynamic program we used to find them .",
    "we used a brute force search algorithm to get some of the new schedules that will be presented in the following sections .",
    "it is very similar to the pebble game of huss - lederman et al .",
    "@xcite . + a sequence of computations is represented as a directed graph , just like figure  [ fig : winotask ] is built from winograd s algorithm . +",
    "a node represents a program variable .",
    "the nodes can be classified as initials ( when they correspond to inputs ) , temporaries ( for intermediate computations ) or finals ( results or nodes that we want to keep , such as ready - only inputs ) .",
    "+ the edges represent the operations ; they point from the operands to the result . + a pebble represents an allocated memory .",
    "we can put pebbles on any nodes , move or remove them according to a set of simple rules shown below .",
    "+ when a pebble arrives to a node , the computation at the associated variable starts , and can be `` partially '' or `` fully '' executed .",
    "if not specified , it is assumed that the computation is fully executed .",
    "+ edges can be removed , when the corresponding operation has been computed .",
    "+ the last two points are especially useful for accumulation operations : for example , it is possible to try schedule the multiplication separately from the addition in an otherwise recursive @xmath94 call ; the edges involved in the multiplication operation would then be removed first and the accumulated part later .",
    "they are also useful if we do not want to fix the way some additions are performed : if @xmath95 the associativity allows different ways of computing the sum and we let the program explore these possibilities . at the beginning of the exploration , each initial node has a pebble and we may have a few extra available pebbles .",
    "the program then tries to apply the following rules , in order , on each node .",
    "the program stops when every final node has a pebble or when no further moves of pebbles are possible :    _ computing a result / removing edges .",
    "_ if a node has a pebble and parents with pebbles , then the operation can be performed and the corresponding edges removed .",
    "the node is then at least partially computed .    _",
    "freeing some memory / removing a pebble . _",
    "if a node is isolated and not final , its pebble is freed .",
    "this means that we can reclaim the memory here because this node has been fully computed ( no edge pointing to it ) and is no longer in use as an operand ( no edge initiating from  it ) .",
    "_ computing in place / moving a pebble . _",
    "if a node @xmath96 has a full pebble and a single empty child node @xmath97 and if other parents of @xmath97 have pebbles on them , then the pebble on @xmath96 may be transferred to @xmath97 ( corresponding edges are removed ) .",
    "this means an operation has been made in place in the parent @xmath96 s pebble .    _ using more memory / adding a pebble . _",
    "if parents of an empty node @xmath98 have pebbles and a free pebble is available , then this pebble can be assigned to @xmath98 and the corresponding edges are removed .",
    "this means that the operation is computed in a new memory location .    _ copying some memory / duplicating a pebble .",
    "_ a computed node having a pebble can be duplicated .",
    "the edges pointed to or from the original node are then rearranged between them .",
    "this means that a temporary result has been copied into some free place to allow more flexibility .",
    "we now relax some constraints on the previous problem : the input matrices @xmath5 and @xmath6 can be overwritten , as proposed by  @xcite . for the sake of simplicity , we first give schedules only working for square matrices ( i.e. @xmath63 and any memory location",
    "is supposed to be able to receive any result of any size ) .",
    "we nevertheless give the memory requirements of each schedule as a function of @xmath54 ; @xmath3 and @xmath55 .",
    "therefore it is easier in the last part of this section to adapt the proposed schedules partially for the general case . in the tables , the notation @xmath99 ( resp .",
    "@xmath100 denotes the use of the algorithm from table  [ tab : schedule : ab ] ( resp .",
    "table  [ tab : schedule : abc ] ) as a subroutine .",
    "otherwise we use the notation @xmath101 to denote a recursive call or the use of one of our new schedules as a subroutine .",
    "we propose in table  [ tab : ab : inplace ] a new schedule that computes the product @xmath20 without any temporary memory allocation .",
    "the idea here is to find an ordering where the recursive calls can be made also in place such that the operands of a multiplication are no longer in use after the multiplication has completed because they are overwritten .",
    "an exhaustive search showed that no schedule exists overwriting less than four sub - blocks .     1 & @xmath21 & @xmath42 & 12 & @xmath45 & @xmath102 + 2 & @xmath31 & @xmath103 & 13 & @xmath104 & @xmath35 + 3 & @xmath33 & @xmath35 & 14 & @xmath26 & @xmath35 + 4 & @xmath24 & @xmath105 & 15 & @xmath106 & @xmath27 + 5 & @xmath107 ) & @xmath29 & 16 & @xmath48 & * @xmath42 + 6 & @xmath38 & @xmath27 & 17 & @xmath32 & @xmath27 + 7 & @xmath108 & @xmath42 & 18 & @xmath30 & @xmath35 + 8 & @xmath40 & @xmath12 & 19 & @xmath44 & * @xmath29 + 9 & @xmath109 & @xmath11 & 20 & @xmath34 & * @xmath35 + 10 & @xmath39 & @xmath35 & 21 & @xmath110 & @xmath111 + 11 & @xmath112 & @xmath103 & 22 & @xmath37 & * @xmath27 + * * * *    note that this schedule uses only two blocks of @xmath6 and the whole of @xmath5 but overwrites all of @xmath5 and @xmath6 . for instance",
    "the recursive computation of @xmath113 requires overwriting parts of @xmath111 and @xmath114 too . using another schedule as well as back - ups of overwritten parts into some available memory in the following",
    ", we will denote by ` ip`for ` i`nplace , either one of these two schedules .",
    "+ we present in tables  [ tab : ab : ipleft ] and [ tab : ab : ipright ] two new schedules overwriting only one of the two input matrices , but requiring an extra temporary space .",
    "these two schedules are denoted ` ovl`and ` ovr ` .",
    "the exhaustive search also showed that no schedule exists overwriting only one of @xmath5 and @xmath6 and using no extra temporary .     1 & @xmath21 & @xmath35 & 12 & @xmath115 & @xmath29 + 2 & @xmath31 & @xmath103 & 13 & @xmath39 & @xmath11 + 3 & @xmath38 & @xmath27 & 14 & @xmath26 & @xmath29 + 4 & @xmath33 & @xmath29 & 15 & @xmath32 & @xmath27 + 5 & @xmath116 & @xmath42 & 16 & @xmath30 & @xmath29 + 6 & @xmath24 & @xmath11 & 17 & @xmath34 & * @xmath35 + 7 & @xmath117 & @xmath22 & 18 & @xmath37 & * @xmath27 + 8 & @xmath40 & @xmath11 & 19 & @xmath118 & @xmath22 + 9 & @xmath109 & @xmath35 & 20 & @xmath48 & * @xmath42 + 10 & @xmath45 & @xmath29 & 21 & @xmath112 & @xmath103 + 11 & @xmath119 & @xmath103 & 22 & @xmath44 & * @xmath29 + * * * *     1 & @xmath21 & @xmath35 & 12 & @xmath120 & @xmath105 + 2 & @xmath31 & @xmath29 & 13 & @xmath45 & @xmath12 + 3 & @xmath33 & @xmath27 & 14 & @xmath26 & @xmath29 + 4 & @xmath121 & @xmath42 & 15 & @xmath32 & @xmath27 + 5 & @xmath38 & @xmath12 & 16 & @xmath30 & @xmath29 + 6 & @xmath24 & @xmath105 & 17 & @xmath34 & * @xmath35 + 7 & @xmath117 & @xmath22 & 18 & @xmath44 & * @xmath29 + 8 & @xmath40 & @xmath105 & 19 & @xmath110 & @xmath105 + 9 & @xmath109 & @xmath35 & 20 & @xmath37 & * @xmath27 + 10 & @xmath39 & @xmath27 & 21 & @xmath122 & @xmath105 + 11 & @xmath123 & @xmath29 & 22 & @xmath48 & * @xmath42 + * * * *    we note that we can overwrite only two blocks of @xmath5 in ` ovl`when the schedule is modified as follows :     18bis & @xmath124 & @xmath103 + 19bis & @xmath125 & @xmath111 + 21 & @xmath120 & @xmath103 +    similarly , for ` ovr ` , we can overwrite only two blocks of @xmath6 using copies on lines 20 and 21 and ` ovl`on line 19 . +",
    "we now compute the extra memory needed for the schedule of table  [ tab : ab : ipright ] .",
    "the size of the temporary block @xmath22 is @xmath126 , the extra memory required for table  [ tab : ab : ipright ] hence satisfies : @xmath127 .",
    "we now consider the operation @xmath128 , where the input matrices @xmath5 and @xmath6 can be overwritten .",
    "we propose in table  [ tab : abc : overwrite ] a schedule that only requires @xmath4 temporary block matrices , instead of the @xmath129 in table  [ tab : schedule : abc ] .",
    "this is achieved by overwriting the inputs and by using two additional pre - additions ( @xmath130 and @xmath131 ) on the matrix @xmath132 .     1 & @xmath133 & @xmath35 & 13 & @xmath134 & @xmath29 + 2 & @xmath31 & @xmath22 & 14 & @xmath45 & @xmath102 + 3 & @xmath33 & @xmath25 & 15 & @xmath135 & @xmath22 + 4 & @xmath136 & @xmath29 & 16 & @xmath137 & @xmath42 + 5 & @xmath24 & @xmath105 & 17 & @xmath48 & @xmath42 + 6 & @xmath21 & @xmath103 & 18 & @xmath26 & @xmath22 + 5 & @xmath138 & @xmath35 & 17 & @xmath30 & @xmath35 + 8 & @xmath38 & @xmath103 & 20 & @xmath32 & @xmath22 + 9 & @xmath40 & @xmath105 & 21 & @xmath44 & @xmath29 + 10 & @xmath139 & @xmath27 & 22 & @xmath34 & @xmath35 + 11 & @xmath140 & @xmath25 & 23 & @xmath141 & @xmath27 + 12 & @xmath39 & @xmath22 & 24 & @xmath37 & @xmath27 +    we also propose in table  [ tab : abc : overright ] a schedule similar to table  [ tab : abc : overwrite ] overwriting only for instance the right input matrix .",
    "it also uses only two temporaries , but has to call the ` ovr`schedule . the extra memory required by @xmath22 and @xmath25 in table  [ tab : abc : overwrite ]",
    "is @xmath142 .",
    "hence , using lemma  [ lem : sum ] : @xmath143     1 & @xmath144 & @xmath35 & 13 & @xmath145 & @xmath42 + 2 & @xmath33 & @xmath22 & 14 & @xmath38 & @xmath25 + 3 & @xmath136 & @xmath29 & 15 & @xmath146 & @xmath114 + 4 & @xmath24 & @xmath105 & 16 & @xmath45 & @xmath25 + 5 & @xmath147 & @xmath25 & 17 & @xmath26 & @xmath114 + 6 & @xmath148 & @xmath35 & 18 & @xmath30 & @xmath35 + 7 & @xmath31 & @xmath25 & 19 & @xmath32 & @xmath114 + 8 & @xmath40 & @xmath105 & 20 & @xmath44 & @xmath29 + 9 & @xmath149 & @xmath27 & 21 & @xmath48 & @xmath42 + 10 & @xmath39 & @xmath22 & 22 & @xmath34 & @xmath35 + 11 & @xmath150 & @xmath29 & 23 & @xmath141 & @xmath27 + 12 & @xmath151 & @xmath22 & 24 & @xmath37 & @xmath27 +    the extra memory @xmath152 required for table  [ tab : abc : overright ] in the top level of recursion is : @xmath153 we clearly have @xmath154 and : @xmath155 compared with the schedule of table  [ tab : schedule : abc ] , the possibility to overwrite the input matrices makes it possible to have further in place calls and replace recursive calls with accumulation by calls without accumulation .",
    "we show in theorem  [ thm : cost ] that this enables us to almost compensate for the extra additions performed .",
    "we now examine the sizes of the temporary locations used , when the matrices involved do not have identical sizes .",
    "we want to make use of table  [ tab : ab : inplace ] for the general case .",
    "+ firstly , the sizes of @xmath5 and @xmath6 must not be bigger than that of @xmath132 ( we need @xmath156 ) .",
    "indeed , let s play a pebble game that we start with pebbles on the inputs and @xmath157 extra pebbles that are the size of a @xmath158 .",
    "no initial pebble can be moved since at least two edges initiate from the initial nodes .",
    "if the size of @xmath159 is larger that the size of the free pebbles , then we can not put a free pebble on the @xmath160 nodes ( they are too large ) .",
    "we can not put either a pebble on @xmath161 or @xmath113 since their operands would be overwritten .",
    "so the size of @xmath159 is smaller or equal than that of @xmath158 .",
    "the same reasoning applies for @xmath162 .",
    "+ then , if we consider a pebble game that was successful , we can prove in the same fashion that either the size of @xmath5 or the size of @xmath6 can not be smaller that of @xmath132 ( so one of them has the same size as @xmath132 ) .",
    "+ finally , table  [ tab : ab : inplace ] shows that this is indeed possible , with @xmath163 .",
    "it is also possible to switch the roles of @xmath54 and @xmath55 .",
    "+ now in tables  4 to  7 , we need that @xmath5 , @xmath6 and @xmath132 have the same size .",
    "generalizing table  [ tab : ab : inplace ] whenever we do not have a dedicated in - place schedule can then done by cutting the larger matrices in squares of dimension @xmath164 and doing the multiplications / product with accumulations on these smaller matrices using algorithm  1 to  7 and free space from @xmath5 , @xmath6 or @xmath132.since algorithms  1 to  7 require less than @xmath165 extra memory , we can use them as soon as one small matrix is free .",
    "+ we now propose an example in algorithm [ alg : ipmm0 ] for the case @xmath166 :    let @xmath167 and @xmath168 .",
    "split @xmath169 $ ] , @xmath170 $ ] and @xmath171 $ ] @xmath172 now we use @xmath173 as temporary space .",
    "@xmath174 @xmath175    algorithm  [ alg : ipmm0 ] computes the product @xmath176 in place , overwriting @xmath5 and @xmath6 .",
    "finally , we generalize the accumulation operation from table  [ tab : abc : overright ] to the rectangular case",
    ". we can no longer use dedicated square algorithms .",
    "this is done in table  [ tab : abc : overright : gen ] , overwriting only one of the inputs and using only two temporaries , but with 5 recursive accumulation calls :     1 & @xmath144 & @xmath35 & 13 & @xmath177 & @xmath42 + 2 & @xmath33 & @xmath22 & 14 & @xmath48 & @xmath42 + 3 & @xmath136 & @xmath29 & 15 & @xmath38 & @xmath25 + 4 & @xmath24 & @xmath105 & 16 & @xmath178 & @xmath22 + 5 & @xmath21 & @xmath25 & 17 & @xmath30 & @xmath35 + 6 & @xmath179 & @xmath35 & 18 & @xmath44 & @xmath29 + 7 & @xmath31 & @xmath25 & 19 & @xmath34 & @xmath35 + 8 & @xmath40 & @xmath105 & 20 & @xmath32 & @xmath22 + 9 & @xmath180 & @xmath27 & 21 & @xmath45 & @xmath25 + 10 & @xmath39 & @xmath22 & 22 & @xmath181 & @xmath27 + 11 & @xmath182 & @xmath29 & 23 & @xmath37 & @xmath27 + 12 & @xmath80 & @xmath22 & 24 & & +    for instance , in table [ tab : abc : overright : gen ] , the last multiplication ( line 22 , @xmath181 ) could have been made by a call to the in place algorithm , would @xmath27 be large enough .",
    "this is not always the case in a rectangular setting .",
    "now , the size of the extra temporaries required in table [ tab : abc : overright : gen ] is @xmath183 and @xmath184 is equal to : @xmath185 if @xmath186 or @xmath187 , then @xmath188 : @xmath189 otherwise @xmath190 and : @xmath191 in the square case , this simplifies into @xmath192 + in addition , if the size of @xmath6 is bigger than that of @xmath5 , then one can store @xmath193 , for instance within @xmath105 , and separate the recursive call @xmath194 into a multiplication and an addition , which reduces the arithmetic complexity .",
    "otherwise , a scheduling with only 4 recursive calls exists too , but we need for instance to recompute @xmath195 at step @xmath196 .",
    "by combining techniques from sections  [ ssec : constinput ] and [ sec : overwrite ] , we now propose in table  [ tab : abc:2tmp ] a hybrid algorithm that performs the computation @xmath128 with constant input matrices @xmath5 and @xmath6 , with a lower extra memory requirement than the scheduling of  @xcite ( table  [ tab : schedule : abc ] ) . we have to pay a price of order @xmath197 extra operations , as we need to compute the temporary variable @xmath198 twice .     1 & @xmath133 & @xmath35 & 14 & @xmath199 & @xmath42 + 2 & @xmath200 & @xmath27 & 15 & @xmath48 & @xmath42 + 3 & @xmath31 & @xmath22 & 16 & @xmath201 & @xmath27 + 4 & @xmath33 & @xmath25 & 17 & @xmath21 & @xmath22 + 5 & @xmath202 & @xmath27 & 18 & @xmath24 & @xmath25 + 6 & @xmath38 & @xmath22 & 19 & @xmath203 & @xmath29 + 7 & @xmath79 & @xmath25 & & @xmath204 & + 8 & @xmath205 & @xmath29 & 20 & @xmath206 & @xmath35 + 9 & @xmath45 & @xmath22 & 21 & @xmath207 & @xmath25 + 10 & @xmath208 & @xmath35 & 22 & @xmath209 & @xmath25 + 11 & @xmath210 & @xmath27 & 23 & @xmath211 & @xmath25 + 12 & @xmath80 & @xmath22 & 24 & @xmath212 & @xmath29 + 13 & @xmath213 & @xmath29 & & @xmath214 & +    again , the two temporary blocks @xmath22 and @xmath25 have dimensions @xmath215 so that : @xmath216 in all cases , @xmath217 but @xmath218 is not as large as the size of the two temporaries in table  [ tab : abc : overwrite ] . we therefore get : @xmath219 assuming @xmath63 , one gets @xmath220 which is smaller than the extra memory requirement of table [ tab : schedule : abc ] . +",
    "following the improvements of the previous section , the question was raised whether extra memory allocation was intrinsic to sub - cubic matrix multiplication algorithms .",
    "more precisely , is there a matrix multiplication algorithm computing @xmath20 in @xmath221 arithmetic operations without extra memory allocation and without overwriting its input arguments ?",
    "we show in this section that a combination of winograd s algorithm and a classic block algorithm provides a positive answer .",
    "furthermore this algorithm also improves the extra memory requirement for the product with accumulation @xmath128 .",
    "the key idea is to split the result matrix @xmath132 into four quadrants of dimension @xmath222 .",
    "the first three quadrants @xmath223 and @xmath29 are computed using fast rectangular matrix multiplication , which accounts for @xmath224 standard winograd multiplications on blocks of dimension @xmath222 .",
    "the temporary memory for these computations is stored in @xmath35 .",
    "lastly , the block @xmath35 is computed recursively up to a base case , as shown on algorithm  [ alg : ipmm ] .",
    "this base case , when the matrix is too small to benefit from the fast routine , is then computed with the classical matrix multiplication .",
    "split @xmath225 , @xmath226 $ ] and @xmath227 $ ]    where each @xmath228 and @xmath229 have dimension @xmath222 .",
    "@xmath230 @xmath231 @xmath232 @xmath233 @xmath234 @xmath235 @xmath236    the complexity of algorithm  [ alg : ipmm ] is : @xmath237 when @xmath238 .",
    "recall that the cost of winograd s algorithm for square matrices is @xmath239 for the operation @xmath20 and @xmath240 for the operation @xmath241 .",
    "the cost @xmath242 of algorithm  [ alg : ipmm ] is given by the relation @xmath243 the base case being a classical dot product : @xmath244 .",
    "thus , @xmath245 .    for any @xmath54 ,",
    "@xmath55 and @xmath3 , algorithm  [ alg : ipmm ] is in place .",
    "w.l.o.g , we assume that @xmath246 ( otherwise we could use the transpose ) . the exact amount of extra memory from algorithms in table  [ tab : schedule : ab ] and  [ tab : schedule : abc ] is respectively given by eq .",
    "( [ eq : schedule : ab ] ) and  ( [ eq : schedule : abc ] ) .",
    "+ if we cut @xmath6 into @xmath247 stripes at recursion level @xmath62 , then the sizes for the involved submatrices of @xmath5 ( resp .",
    "@xmath6 ) are @xmath248 ( reps .",
    "@xmath249 ) . the lower right corner submatrix of @xmath132 that we would like to use as temporary space has a size @xmath250 .",
    "thus we need to ensure that the following inequality holds : @xmath251 it is clear that @xmath252 which simplifies the previous inequality .",
    "let us now write @xmath253 , @xmath254 and @xmath255 .",
    "we need to find , for every @xmath62 an integer @xmath256 so that eq .",
    "( [ eq : aim ] ) holds . in other words , let us show that there exists some @xmath257 such that , for any @xmath258 , the inequality @xmath259 holds . then the fact that @xmath260 provides at least one such @xmath261 .",
    "+ as the requirements in algorithm  [ alg : ipmm ] ensure that @xmath262 and @xmath263 , there just remains to prove that @xmath264 . since @xmath265 and again @xmath266 , algorithm  [ alg : ipmm ] is indeed in place .",
    "hence a fully in - place @xmath221 algorithm is obtained for matrix multiplication .",
    "the overhead of this approach appears in the multiplicative constant of the leading term of the complexity , growing from @xmath267 to @xmath268 . +",
    "this approach extends to the case of matrices with general dimensions , using for instance peeling or padding techniques . + it is also useful if any sub - cubic algorithm is used instead of winograd s .",
    "for instance , in the square case , one can use the product with accumulation in table  [ tab : abc:2tmp ] instead of table  [ tab : schedule : abc ] .      in the case of computing the product with accumulation",
    ", the matrix @xmath132 can no longer be used as temporary storage , and extra memory allocation can not be avoided .",
    "again we can use the idea of the classical block matrix multiplication at the higher level and call winograd algorithm for the block multiplications . as in the previous subsection",
    ", @xmath132 can be divided into four blocks and then the product can be made with 8 calls to winograd algorithm for the smaller blocks , with only one extra temporary block of dimension @xmath269 . + more generally , for square @xmath270 matrices , @xmath132 can be divided in @xmath271 blocks of dimension @xmath272 .",
    "then one can compute each block with winograd algorithm using only one extra memory chunk of size @xmath273 .",
    "the complexity is changed to @xmath274 which is @xmath275 for an accumulation product with winograd s algorithm . using the parameter @xmath276 , one can then balance the memory usage and the extra arithmetic operations .",
    "for example , with @xmath277 , @xmath278 and with @xmath279 , @xmath280 note that one can use the algorithm of table  [ tab : abc:2tmp ] instead of the classical winograd accumulation as the base case algorithm .",
    "then the memory overhead drops down to @xmath281 and the arithmetic complexity increases to @xmath282 .",
    "with constant input matrices , we reduced the number of extra memory allocations for the operation @xmath283 from @xmath165 to @xmath284 , by introducing two extra pre - additions .",
    "as shown below , the overhead induced by these supplementary additions is amortized by the gains in number of memory allocations .    if the input matrices can be overwritten , we proposed a fully _ in - place _",
    "schedule for the operation @xmath20 without any extra operations .",
    "we also proposed variants for the operation @xmath285 , where only one of the input matrices is being overwritten and one temporary is required .",
    "these subroutines allow us to reduce the extra memory allocations required for the @xmath286 operation without overwrite : the extra required temporary space drops from @xmath165 to only @xmath284 , at a negligible cost .    some algorithms with an even more reduced memory usage , but with some increase in arithmetic complexity , are also shown .",
    "table  [ tab : resume ] gives a summary of the features of each schedule that has been presented .",
    "the complexities are given only for @xmath287 being a power of @xmath4 .",
    "& total extra memory & total # of extra allocations & arithmetic complexity + & & & & & & + & table  [ tab : schedule : ab ] @xcite & constant & @xmath4 & @xmath288 & @xmath289 & @xmath290 + & table  [ tab : ab : inplace ] & both overwritten & @xmath291 & @xmath291 & @xmath291 & @xmath290 + & table  [ tab : ab : ipleft ] or  [ tab : ab : ipright ] & @xmath5 or @xmath6 overwritten & @xmath292 & @xmath293 & @xmath294 & @xmath290 + & [ sec : fully ] & constant & @xmath291 & @xmath291 & @xmath291 & @xmath295 + & & & & & & + & table  [ tab : schedule : abc ] @xcite & constant & @xmath129 & @xmath165 & @xmath296 & @xmath297 + & table  [ tab : abc : overwrite ] & both overwritten & @xmath4 & @xmath288 & @xmath298 & @xmath299 + & table  [ tab : abc : overright ] & @xmath6 overwritten & @xmath4 & @xmath288 & @xmath300 & @xmath299 + & table  [ tab : abc:2tmp ] & constant & @xmath4 & @xmath288 & @xmath301 & @xmath302 + &  [ sec : red ] & constant & n / a & @xmath303 & @xmath303 & @xmath304 + &  [ sec : red ] & constant & n / a & @xmath305 & @xmath305 & @xmath306 +              for @xmath318 , the arithmetic complexity of @xcitesatisfies @xmath319 hence @xmath320 ; its memory overhead satisfies @xmath321 which is @xmath322 ; its total number of allocations satisfies @xmath323 which is @xmath324    the arithmetic complexity of the schedule of table  [ tab : abc : overwrite ] satisfies @xmath325",
    "so that @xmath326 ; its number of extra memory satisfies @xmath327 which is @xmath328 ; its total number of allocations satisfies @xmath329 which is @xmath330 .",
    "the arithmetic complexity of table  [ tab : abc : overright ] schedule satisfies @xmath331 so that @xmath332 ; its number of extra memory satisfies @xmath333 which is @xmath334 ; its total number of allocations satisfies @xmath335 which is @xmath336 .",
    "the arithmetic complexity of the schedule of table  [ tab : abc:2tmp ] satisfies @xmath337 so that @xmath338 ; its number of extra memory satisfies @xmath339 which is @xmath340 ; its total number of allocations satisfies @xmath341 which is @xmath342 .",
    "for instance , by adding up allocations and arithmetic operations in table  [ tab : resume ] , one sees that the overhead in arithmetic operations of the schedule of table  [ tab : abc:2tmp ] is somehow amortized by the decrease of memory allocations .",
    "thus it makes it theoretically competitive with the algorithm of  @xcite as soon as @xmath343 .",
    "also , problems with dimensions that are not powers of two can be handled by combining the cuttings of algorithms [ alg : ipmm0 ] and [ alg : ipmm ] with peeling or padding techniques .",
    "moreover , some cut - off can be set in order to stop the recursion and switch to the classical algorithm .",
    "the use of these cut - offs will in general decrease both the extra memory requirements and the arithmetic complexity overhead . for instance we show on table [",
    "tab : ipmmperf ] the relative speed of different multiplication procedures for some double floating point rectangular matrices .",
    "we use atlas-3.9.4 for the blas and a cut - off of 1024 .",
    "we see that pour new schedules perform quite competitively with the previous ones and that the savings in memory enable larger computations ( mt for memory thrashing ) .",
    "s.  huss - lederman , e.  m. jacobson , j.  r. johnson , a.  tsao , and t.  turnbull .",
    "implementation of strassen s algorithm for matrix multiplication . in acm ,",
    "editor , _ supercomputing 96 conference proceedings : november 1722 , pittsburgh , pa_. acm press and ieee computer society press , 1996 .",
    "www.supercomp.org / sc96/proceedings / sc96proc / jacobson/.    s.  huss - lederman , e.  m. jacobson , j.  r. johnson , a.  tsao , and t.  turnbull .",
    "s algorithm for matrix multiplication  : modeling analysis , and implementation .",
    "technical report , center for computing sciences , nov .",
    "ccs - tr-96 - 17 .",
    "a.  kreczmar . on memory requirements of strassen s algorithms . in a.",
    "mazurkiewicz , editor , _ proceedings of the 5th symposium on mathematical foundations of computer science _ , volume  45 of _ lncs _ , pages 404407 , gdask , poland , sept ."
  ],
  "abstract_text": [
    "<S> we propose several new schedules for strassen - winograd s matrix multiplication algorithm , they reduce the extra memory allocation requirements by three different means : by introducing a few pre - additions , by overwriting the input matrices , or by using a first recursive level of classical multiplication . in particular , we show two fully in - place schedules : one having the same number of operations , if the input matrices can be overwritten ; the other one , slightly increasing the constant of the leading term of the complexity , if the input matrices are read - only . many of these schedules have been found by an implementation of an exhaustive search algorithm based on a pebble game .    </S>",
    "<S> * keywords : * matrix multiplication , strassen - winograd s algorithm , memory placement . </S>"
  ]
}