{
  "article_text": [
    "in this paper , we study network visualizations as a means of enhancing the interpretability of probabilistic topic models for insight discovery .",
    "we focus on what is perhaps the most popular and prevalently - used topic model : _ latent dirichlet allocation _ or lda @xcite .",
    "topic modeling algorithms like lda discover latent themes ( topics ) in document collections and represent documents as a combination of these themes .",
    "thus , they are critical tools for exploring text data across many domains . indeed , it is often the case that users must _ discover _ the subject matter buried within large and unfamiliar document sets ( sensemaking in text data ) .",
    "keyword searches are inadequate here , as it is unclear on where to even begin searching .",
    "topic discovery techniques such as lda are a boon to users in such scenarios , as they reveal the content in an unsupervised and automated fashion .",
    "automated topic organization can potentially facilitate the comprehension of unfamiliar document data on even a massive scale .    however , it is often quite challenging to obtain a `` big picture '' view of the larger trends in a document collection from only the raw output of an lda model .",
    "lda is fundamentally a statistical tool that returns a probability distribution for each document showing the relative presence ( or absence ) of various discovered topics .",
    "these topics , in turn , are represented as probability distributions over words ( typically unigrams ) .",
    "words with the highest estimated probabilities for a discovered topic are used as a _ label _ for the topic .",
    "exploring text corpora using only these raw outputs is considerably challenging . in order to derive insights and identify larger trends within the document collection , one",
    "is left to inspect these numerical distributions , which can be difficult , non - trivial , and far from straightforward .",
    "the problem is exacerbated as document collections under consideration grow .",
    "for instance , with the existence of scalable , mapreduce implementations of lda ( @xcite ) , it is now possible to train an lda model on massive text corpora with many latent topics ( big data ) .",
    "the inferred topics discovered by these lda implementations , can themselves pose their own unique data challenge .",
    "it is often unclear on how best to effectively browse these topics to discover information of interest .",
    "this , in fact , tends to be a significant challenge even for large data ( as opposed to `` big data '' )  document collections on the order of tens of thousands or hundreds of thousands . in the present work ,",
    "we investigate the use of what we refer to as _ topic similarity networks _ to address these challenges .",
    "_ topic similarity networks _ are graphs in which nodes represent latent topics in text collections and links represent similarity among topics .",
    "we describe efficient and effective methods to both building and labeling such networks .",
    "+ * summary of contributions . * our contributions in both the areas of topic visualization and topic labeling are summarized below .    1 .",
    "_ constructing topic similarity networks : _ in section [ sec : topicsim ] , we describe the construction of _ topic similarity networks _",
    ", our approach to big data visualization .",
    "we exploit these networks to discover how topics form larger themes .",
    "we employ the use of community detection in network visualizations to discover such macro - level themes including the sometimes subtle connections among these themes . 2 .",
    "_ labeling topic similarity networks : _ in section [ sec : labeling ] , we describe an approach to expressively labeling discovered topics .",
    "our method , based on keyphrase extraction , is purely unsupervised , extractive , and demonstrably efficient .",
    "these labels are , then , employed as node labels in our _ topic similarity networks _ to enable better characterization of large document sets .",
    "it is surprising to note that , to the best of our knowledge , few of the existing works on topic visualization ( discussed in the next section ) make use of automated topic labeling methods .",
    "our work , then , represents one of the first examinations of the efficacy of automated topic labeling in actual topic visualizations of large , real - world data .",
    "there has been a wave of recent work to address challenges in both visualizing topics and labeling topics  each of which we discuss separately in light of our work .",
    "a number of both graphical and text - based visualizations and user interfaces have been proposed in the existing literature to browse topics ( @xcite ) .",
    "several , like _ topicviz _ by eisenstein et al . @xcite and _ topicnets _ by gretarsson et al.@xcite , are quite innovative and make significant strides towards improving the interpretability of learned topic models . however , most of these existing methods focus on shedding light on the relationships between topics and documents ( or attributes of documents ) .",
    "although some ( @xcite ) support the inference of pair - wise similarity between topics , they do not provide insights into how topics come together to form larger themes or the subtle connections among seemingly disparate groups of topics .",
    "such insights are important in obtaining a `` big picture '' view of ill - understood document collections .",
    "an exception to this rule is work on _ correlated topic models _ ( or ctms ) and its variants ( @xcite ) . ctms model and",
    "infer associations among topics .",
    "these associations can be further mined to produce clusters of topics that represent larger themes for incorporation into visualizations .",
    "these models , however , reveal certain challenges when applied in real - world scenarios .",
    "first , existing visualizations based on ctm and its variants do not appear to easily lend themselves to extracting the kinds of insights mentioned above .",
    "this is due both to the way in which the topic relations are constructed and depicted and also the way in which the topic nodes are labeled ( topic labeling is discussed in the next section ) .",
    "one may refer to @xcite for examples of these existing visualizations and for comparison to our visualizations shown later .",
    "second , some approaches , such as @xcite , artificially constrict the topic relation structure with specification of what are referred to as supertopics , which can hinder a view of the subtle connections among different and seemingly disparate groups of topics and subtopics .",
    "a third issue is related to practical scalability .",
    "chen et al .",
    "@xcite showed that ctm is unable to process a corpus of 285k documents in any reasonable time frame ( it will not finish within a week ) .",
    "similarly , an approach to infer topic hierarchies proposed by @xcite is limited to short texts only .",
    "scactm , a parallelized extension to ctm , was shown to be substantially more scalable given a cluster of 40 machines @xcite .",
    "but , for certain domains , such machine clusters may not be available at sites of deployment .",
    "in fact , it is often the case that only a single multi - core machine is available to process millions of documents , as the storage capacity of today s machines often outstrips their processing capacity . even in scenarios where one has access to a large machine cluster ,",
    "lda is _ significantly _ more scalable and efficient because it does not learn the correlation structure among topics .",
    "( see @xcite for a time complexity analysis of ctm , scactm , and lda ) . given these aforementioned issues and the clear scalability , efficiency , and also prevalence of lda , our objective in this work is to infer these topic associations in an organic fashion from the raw output of the _ original _ lda model . as we will describe in section [ sec : topicsim ] , we do so by constructing",
    "_ topic similarity networks _ : networks depicting the similarity ( represented as links ) among topics ( represented as nodes ) .",
    "next , we discuss existing work on the labeling of topics .",
    "a topic similarity network is only useful as a visualization tool if the identity of network nodes are easily discernible .",
    "several visualization schemes label topics by simply using the most probable word ( or words ) from the topic model ( @xcite ) .",
    "however , lda - derived labels have been observed to not always be adequately expressive of the topic ( see @xcite ) . as a result",
    ", a number of methods have been proposed to better label topics in an automated fashion ( @xcite ) .",
    "unfortunately , for a variety of reasons , most of these existing techniques are unable to handle the large text corpora we consider in this work . in section [ sec : labeling ] , we describe our own method to label topics to address gaps in this existing literature on topic labeling . to better motivate the use of our own labeling method",
    ", we describe several goals that must be met by any labeling scheme for a topic similarity network in light of existing work on topic labeling .",
    "+ * unsupervised .",
    "* the labeling method must be unsupervised , as obtaining a training set for a supervised labeling method can be prohibitively expensive and time - consuming .",
    "+ * extractive . *",
    "the labeling method must be extractive .",
    "that is , labels must be generated directly from the terms within the corpus under consideration , as opposed to an external reference corpus such as wikipedia .",
    "this is especially important for the government and corporate domains , which often deal with document collections describing sensitive or proprietary information , state - of - the - art `` bleeding edge '' technology , or otherwise esoteric subject matter .",
    "such information may not reside in publicly available reference corpora like wikipedia .",
    "this requirement prevents us from utilizing methods such as @xcite , which employs the use of reference corpora when labeling topics .",
    "+ * supportive of user interactivity .",
    "* topic similarity networks are intended for use with _ interactive _ systems utilizing full - text search and faceted navigation of documents ( solr search engine ) . under these scenarios ,",
    "the documents comprising topics may be filtered in various ways _ after _ creation of the topic model .",
    "for instance , in the government domain , only those documents containing certain markings might be deemed of interest and selected in a visualization .",
    "labels heavily associated with documents that have been filtered out may no longer adequately describe the remaining documents pertaining to important sub - topics .",
    "labeling methods that are tightly coupled with the topic model ( @xcite ) can not cope well with such dynamic scenarios .",
    "moreover , it is prohibitively expensive to re - generate the topic model on the filtered document collection . for these reasons , our labeling method , described in section [ sec : labeling ] , is purposefully de - coupled from the output of lda .",
    "hence , it can re - label topics in a filtered document collection without having to re - generate the topic model .",
    "our labeling method , then , can best be characterized as a cluster labeling approach to topic labeling .",
    "+ * efficient .",
    "* dynamic filtering of document collections , as described above , also necessitates a need for efficiency in the labeling approach .",
    "as the document collection is filtered in various ways , the labeling method might be repeatedly executed on a large document collection , which can be problematic for some existing labeling methods .",
    "for instance , we were unsuccessful in executing the approach by @xcite on the document sets of interest in this work .",
    "the approaches by @xcite and @xcite also do not appear to scale as easily or as well to larger collections of longer documents .",
    "the method from @xcite , for example , was designed only for very short texts ( titles only ) .",
    "+   +   + these aforementioned issues motivate our development of a custom labeling method for use with topic similarity networks  a method that can scale to even massive collections of documents .",
    "we begin a discussion of our work with a brief overview of lda and the notation and symbols used throughout this paper .",
    "let @xmath0 represent a document collection of interest and let @xmath1 be the number of topics or themes in @xmath2 .",
    "each document is composed of a sequence of words : @xmath3 , where @xmath4 is the number of words in @xmath5 and @xmath6 .",
    "let @xmath7 be the vocabulary of @xmath2 , where @xmath8 takes a sequence of elements and returns a set .",
    "probabilistic topic models like lda take @xmath2 and @xmath1 as input and produce two matrices as output .",
    "the matrix @xmath9 is the document - topic distribution matrix and shows the distribution of topics within each document .",
    "the matrix @xmath10 is the topic - word distribution matrix and shows the distribution of words in each topic .",
    "each row of these matrices represents a probability distribution . for any topic @xmath11 ,",
    "the @xmath12 terms with the highest probability in distribution @xmath13 are typically used as thematic labels for the topic .",
    "we use these lda - derived labels as a baseline for comparison in our work . but first , we describe construction of the topic similarity network .",
    "lda captures the degree to which both documents and words are topically related .",
    "however , relations among the topics themselves are _ not _ explicitly captured .",
    "as we will show shortly , such topic - level relations can be used to construct network representations of text corpora .",
    "these representations , in turn , can be used to better understand , characterize , and visualize the themes in a document collection . in the present work ,",
    "we define these relations based on topic similarity .",
    "+ * measuring topic similarity . *",
    "recall that topics are represented as probability distributions over vocabulary @xmath14 and captured by the matrix @xmath15 .",
    "thus , the similarity for any two topics can be directly computed by comparing the word distributions from @xmath15 .",
    "the kullback - leibler ( kl ) divergence , a distance measure of two probability distributions , is often used to make such comparisons ( @xcite , @xcite ) .",
    "however , kl divergence satisfies neither the triangle inequality nor symmetry and is , therefore , not a metric . as such",
    ", it is less appropriate for defining network links based on similarity ( the complement of distance ) .",
    "although symmetric versions of kl divergence exist , we instead employ the hellinger distance metric to compute topic similarity .",
    "specifically , for any two topics @xmath16 , the hellinger similarity is measured as : @xmath17    a topic similarity network @xmath18 can be constructed where @xmath19 is the set of nodes representing discovered topics and @xmath20 is the set of edges representing similarities among topics .",
    "for any two topics @xmath16 , an edge @xmath21 exists if and only if @xmath22 is greater than some pre - defined threshold , @xmath23 .",
    "+ * measuring topic similarity in mapreduce . *",
    "note that , when constructing a topic similarity network as just described , the number of computed similarities scales quadratically with @xmath1 .",
    "however , since @xmath24 , the method remains computationally viable even for very large document collections .",
    "moreover , with some well - placed substitutions , @xmath15 can be represented using a sparse matrix format for efficient in - memory processing of massive document sets .",
    "( we currently employ a compressed sparse row format for storing and manipulating @xmath15 . ) nevertheless , for scenarios when even sparse representations of @xmath15 are unwieldy and a high degree of parallelization is desired , we propose a mapreduce implementation of the topic similarity computation .",
    "when breaking down problems into distributable units of work under the mapreduce model for parallelization , key - value pairs are employed as the core data structure @xcite . in our case , each cell in the matrix @xmath15 can be represented as a key - value pair of the form ( @xmath25 : ( @xmath26 , @xmath27 ) ) , where @xmath28 is the index of a word ( column ) , @xmath29 is the index of the topic ( row ) , and @xmath27 is the probability of word @xmath25 appearing in topic @xmath26 . if grouping by key , we obtain a key - value representation of each column in @xmath15 .",
    "that is , the values list for any key @xmath28 comprises the set of tuples @xmath30 ) .",
    "the _ map _",
    "operation accepts these key - value pairs as input and outputs key - value pairs of the form ( @xmath31 : @xmath32 ) , where the new key @xmath16 are pairs of topics appearing in the aforementioned values list and the value @xmath33 , for each word @xmath28 .",
    "thus , the _ map _ operation completes the inner expression for hellinger similarity ( shown in equation [ eq : hell ] ) for every word represented in @xmath15 .",
    "the _ reduce _ operation simply sums these values for every pair of topics and completes the hellinger similarity computation by taking the square root of this sum , multiplying by @xmath34 , and subtracting from one . the resultant network , constructed as described above , can be exploited to discover insights , trends , and patterns among the topics in @xmath2 . for the present work ,",
    "we employ the use of a community detection algorithm to discover insights into how topics are related to each other and form larger themes .     + * discovering larger themes .",
    "* a _ community _ can be loosely defined as a set of nodes more densely connected among themselves than to other nodes in the network @xcite . within the context of a topic similarity network",
    ", such communities should represent groups of highly - related topics , which we refer to as _ topic groups_. to detect these communities ( or topic groups ) , we employ the use of the louvain algorithm , a heuristic method based on modularity optimization @xcite .",
    "modularity measures the fraction of links falling within communities as compared to the expected fraction if links were distributed evenly in the network @xcite .",
    "the algorithm initially assigns each topic node to its own community . at each iteration , in a local and greedy fashion , topic nodes are re - assigned to communities with which it achieves the highest modularity . as a greedy optimization method ,",
    "the louvain algorithm is exceptionally efficient and fast , even with a large number of topics . as the authors of @xcite note ,",
    "the computational complexity of the method is unknown , but it experimentally appears to run in @xmath35 time .",
    "when the nodes in these constructed topic similarity networks are marked by their inferred community affiliation and labeled to express the topics they represent , the networks become powerful tools for exploration and discovery in large and heterogeneous text corpora .",
    "we discuss labeling of topic nodes next .",
    "-0.1 in    an algorithm capable of generating expressive thematic labels for any subset of documents in a corpus can greatly facilitate both characterization and navigation of document collections . here , we employ such an algorithm to label nodes in a topic similarity network , as each node is a topic comprising a subset of documents in the corpus .",
    "our approach , referred to as docsetlabeler , is a purely unsupervised , extractive method and shown in algorithm [ alg1].@xmath36 of algorithm [ alg1 ] are a variation of the kera algorithm described in @xcite . ] docsetlabelertakes @xmath37 , a subset of corpus @xmath2 , as input , where @xmath37 consists of all documents associated with some lda - discovered topic @xmath38 .",
    "this subset can be constructed in one of two ways .",
    "the first is to populate @xmath37 with all documents @xmath5 ( where @xmath6 ) for which the topic proportion @xmath39 is greater than some pre - defined threshold ( @xmath40 was used in @xcite ) .",
    "the second is to construct @xmath37 by transforming topics into mutually - exclusive clusters , where the topic cluster for document @xmath5 is @xmath41 .",
    "we employ the latter approach , as it better eliminates noise contributed by foreign topics ( @xmath42 } ) .",
    "labels for topic @xmath43 are , then , extracted by docsetlabelerdirectly from the text constituting the documents in @xmath37 .",
    "@xmath44 , a subset of corpus @xmath2 @xmath45 , the number of candidate terms to consider @xmath12 , the number of labels to return for document set ( @xmath46 ) @xmath47 , list of terms to filter out @xmath48 = a hash table @xmath49 = a hash table @xmath50 @xmath51 @xmath52 @xmath53 @xmath54c@xmath55 @xmath56 ( weight of term @xmath57 ) @xmath58 @xmath59}$ ] = top @xmath45 terms based on weight @xmath60}$ ] = top @xmath45 terms based on weight # compute information gain for each label @xmath61 ( score of label @xmath61 ) @xmath62 @xmath63 top @xmath45 labels based on information gain # optionally re - sort final top candidates @xmath64 return top @xmath12 labels from @xmath65    docsetlabeleris essentially a _ descriptive _ model of topic labeling that follows naturally from four observed characteristics of high - quality , topic - representative labels : _ expressivity _ , _ prominence _ , _ prevalence _ , and _",
    "discriminability_.     + * expressivity . * _",
    "expressivity _ captures the extent to which labels express and represent themes .",
    "previous works have noted that human - assigned labels tend towards multi - word noun phrases , as they are more expressive than unigrams ( see @xcite ) .",
    "the term `` information retrieval , '' for instance , is more expressive than just `` information '' or `` retrieval '' alone .",
    "unigrams tend to most often be expressive when denoting uniqueness ( a proper noun ) .",
    "this is especially true of research reports , our domain of interest , as proper noun unigrams denote important concepts , systems , techniques , or programs ( `` linearsvm , '' `` f-22 '' ) .",
    "lines 4 - 6 in algorithm [ alg1 ] explicitly extract terms conforming to the above principles .",
    "noun phrases and proper nouns are extracted using _ hunpos _ , an open - source , hmm - based , part - of - speech tagger .",
    "the @xmath66 function uses likelihood ratio tests to extract phrases of multiple words that occur together more often than chance . for a bigram of words @xmath67 and @xmath68",
    ", this association , @xmath69 , is measured as :    @xmath70    where @xmath71 are the observed frequencies of the bigram from the contingency table for @xmath67 and @xmath68 and @xmath72 are the expected frequencies assuming that the bigram is independent @xcite . only phrases with a p - value less than @xmath73 are extracted .",
    "these tests can also be used to measure associations of words within n - grams where @xmath74 ( trigrams ) .",
    "we limit phrases to the @xmath75 cases to save space in the visualizations .     +   + * prominence . * _ prominence _ captures the degree to which labels are featured prominently within individual documents .",
    "intuitively , prominent terms tend to make their first appearance earlier and also appear more frequently .",
    "thus , we weight candidate labels by both frequency and position using the harmonic mean , as shown in line 11 of algorithm [ alg1 ]",
    ".     + * prevalence and discriminability . *",
    "good labels for a particular topic appear in many documents pertaining to that topic ( _ prevalence _ ) and appear rarely in other un - related topics ( _ discriminability _ ) .",
    "this was also recently observed by @xcite and @xcite .",
    "the concept of _ information gain _ from the field of information theory simultaneously captures both prevalence and discriminability .",
    "consider a document collection @xmath2 where documents belong to either a positive or negative category .",
    "the _ entropy _",
    "@xmath76 of @xmath2 measures impurity as follows : @xmath77 , where @xmath78 and @xmath79 are the proportions of positive and negative documents in @xmath2 , respectively .",
    "is taken to be @xmath80 .",
    "] for instance , if all documents are positive ( or negative ) , @xmath81 , while a perfectly even split of positive and negative documents has entropy of @xmath82 . in algorithm [ alg1 ] , we assign @xmath37 as positive and @xmath83 as negative .",
    "the information gain @xmath84 of a candidate label @xmath61 in @xmath2 , then , is the expected entropy reduction due to segmenting on @xmath61 : @xmath85 where @xmath86 is the set of documents in @xmath2 from which label @xmath61 was extracted .",
    "thus , labels with the highest information gain for @xmath37 are expected to be simultaneously common in @xmath37 ( prevalence ) and rare in @xmath83 ( discriminability ) .",
    "information gain is computed by the @xmath87 function in algorithm [ alg1 ] .",
    "+ * final sorting .",
    "* at the end of the previous step , we are left with a small number of candidate labels ( @xmath88 ) for each topic .",
    "there are several options for choosing the final label for the topic node .",
    "for instance , one could simply select the label with the highest information gain ( the existing sorting ) .",
    "one might also select the label most frequently extracted from the documents pertaining to the topic .",
    "yet another option is to include word probabilities from @xmath15 into the final weighting .",
    "all three approaches generally yield good ( albeit slightly different ) results . for the present work , based on some preliminary testing",
    ", we choose to sort labels based on a combination of the latter two approaches , as indicated in line 25 of algorithm [ alg1 ] .",
    "more specifically , we sort labels based on the mean of the normalized frequency and the combined @xmath15 probabilities for each word comprising the label .",
    "+ to conclude , we briefly comment on the efficiency and scalability of our current docsetlabelerimplementation . note that , in algorithm [ alg1 ] , lines @xmath82@xmath89 process documents in an online fashion and can be easily parallelized .",
    "computing information gain also scales well to larger collections of longer documents , as it is a simple computation of different combinations of independent and dependent variables .",
    "moreover , it deals with a substantially reduced representation of the data ( generally , @xmath90 for all @xmath91 ) .",
    "for these reasons , it is fairly straightforward to implement docsetlabelerin a variety of different parallel processing models ( mapreduce , multi - core processing ) .",
    "lines @xmath82@xmath89 , for instance , can be implemented as a map - only job with either zero reducers or an identity reducer . on the other hand , for execution on single - node , multi - core , shared - memory systems ( as opposed to clusters ) ,",
    "documents can be processed in an online fashion and passed to as many processors available on the system .",
    "-0.1 in    as a realistic and informative case study , we utilize our methods to characterize and visualize basic research funded by the national science foundation ( nsf ) .",
    "the corpus considered in this case study consists of 132,372 titles and abstracts describing nsf awards for basic research between the years 1990 and 2003  @xcite .",
    "we executed the mallet implementation of lda @xcite on this corpus using @xmath92 as the number of topics and @xmath93 as the number of iterations .",
    "all other parameters were left as defaults . for topic similarity",
    ", we experimentally set @xmath23 as @xmath94 to yield a graph density of approximately @xmath95 .",
    "for the labeling of topic nodes in the network using docsetlabeler , we set @xmath88 and @xmath96 .",
    "we did not find the choice of @xmath45 to affect results significantly .",
    "this is possibly due to the fact that , as described previously , we prune out candidates with no statistical significance , as measured by a likelihood ratio test .",
    "+ * topic labeling of nsf grants .",
    "* table [ tab : nsflabels ] shows the labels generated for a sample of ten discovered topics by both docsetlabelerand lda . as can be seen , labels produced by docsetlabelerare more expressive and representative of the true themes of each topic .",
    "we assigned two judges to evaluate labels for all topics . for a fair comparison",
    ", we showed six unigram labels from lda but only three labels ( mostly bigrams ) from docsetlabelerfor each topic .",
    "as shown in table [ tab : nsfkappa ] , both judged the labels by docsetlabelerto be generally superior ( @xmath97=@xmath98 , @xmath99<@xmath100 ) with an inter - judge agreement of @xmath101 , as measured by cohen s kappa coefficient .",
    "-0.25 in     +   + * visualizing nsf grants . *",
    "a topic similarity network was constructed , with each node representing a topic and labeled using the highest ranked term returned by docsetlabeler .",
    "the network concisely presents a comprehensive and holistic view of 14 years of nsf - funded research and can be navigated and explored using any available network visualization software ( gephi , cytoscape ) .",
    "the entire network is shown in figure [ fig : nsf ] , where both expected and unexpected trends are revealed . as can be seen , the visualization encapsulates the major research funding efforts for scientific research in addition to the subtle connections among them .",
    "major funding efforts for education and conference support are also displayed ( towards the bottom ) . in this network and",
    "all networks shown in this paper , node sizes indicate the number of documents pertaining to the topic represented by the node .",
    "node colors indicate the community ( or topic group ) affiliation . using this network",
    ", one can better understand how topics form larger themes , discover and characterize information of interest , and derive insights into how best to search and explore the corpus further .",
    "it is difficult to quantitatively evaluate visualization schemes such as this .",
    "thus , we present illustrative examples of the patterns and trends discovered using our topic similarity network .",
    "figure [ fig : nsf - math ] shows one small corner of the `` topic universe ''  a `` social clique '' of math topics discovered by community detection within the larger network of all topics . note that each node in the network represents hundreds of documents ( or more ) .",
    "thus , this visualization of math topics clearly and concisely summarizes over @xmath102 documents .",
    "such visualizations also provide insights into relations between topic groups .",
    "for instance , figure [ fig : nsf - bio ] shows a community of biology - related topics ( shown in pink ) . here , we see peripheral connections to another life science theme ( shown in yellow ) containing topics such as _ genetic variation _ , _ population dynamics _ , and _ food webs_. we also see a peripheral connection to a material science theme ( shown in red ) , illuminating research areas dedicated to developing materials based on biological and organic components and also the mutual interest in molecular recognition . as a final example ,",
    "figure [ fig : nsf - astro ] shows a connected component of astronomical research topics that appears separate from the larger network .",
    "this last example illustrates one possible way to use these visualizations to identify outliers ( topics that are comparatively more different than the larger corpus based on their set of similarity scores ) .",
    "-0.4 in    -0.2 in    -0.4 in",
    "for our second case study , we apply our method to visualize wikipedia topics . the corpus considered here",
    "was obtained from the university of alberta and comprises the entire english portion of wikipedia .",
    "it contains over 3.3 million documents spanning a range of different topics .",
    "we executed the mallet implementation of lda @xcite on this corpus using @xmath103 as the number of topics and @xmath93 as the number of iterations .",
    "all other parameters were left as defaults . for topic similarity",
    ", we experimentally set @xmath23 as @xmath104 to yield a graph density of approximately @xmath95 .",
    "for the labeling of topic nodes in the network using docsetlabeler , we again set @xmath88 and @xmath96 .",
    "+ * labeling wikipedia topics . *",
    "table [ tab : wikilabels ] shows a sample of ten wikipedia topics and the labels generated for each by both lda and docsetlabeler .",
    "as we did with the nsf grants , we conducted a user evaluation of the labels generated for all wikipedia topics by both lda and our method . from the results shown in table [ tab : wikikappa ]",
    ", we again see that docsetlabeleroutperforms lda ( @xmath97=@xmath105 , @xmath99<@xmath100 ) with an inter - judge agreement of @xmath106 , as measured by cohen s kappa coefficient .. however , we also see that lda performs significantly better here than on the nsf grants .",
    "we elaborate on this observation further in section [ sec : limitations ] .",
    "-0.25 in     + * visualizing wikipedia . *",
    "a topic similarity network was constructed for wikipedia , with nodes labeled using the highest ranked label generated from docsetlabelerfor each topic . due to space constraints",
    ", we do not present the entire wikipedia topic similarity network in this paper .",
    "rather , we provide illustrative examples of some of the major trends discovered by our method .",
    "two of the most salient and well - defined topic groups ( macro - level themes ) emerging from our visualization are _ sports _ and _ music / dance _ , shown in figures [ fig : wiki - sport ] and [ fig : wiki - music ] , respectively .",
    "we posit that this is due to the fact that authorship and editing of wikipedia articles are crowd - sourced and the subjects of _ sports _ and",
    "_ music / dance _ both have enormous fan bases .",
    "it should follow that television and film should also appear as salient topic groups , and this is precisely what we see in figure [ fig : tvfilm ] .",
    "also shown in figure [ fig : tvfilm ] are the peripheral connections to topic nodes from other related communities ( and _ love story _ from a writing theme in green , _ daily newspaper _ and _ monthly magazine _ from a news media theme in yellow ) .",
    "+    -0.15 in    -0.4 in",
    "in both our two case studies , docsetlabelerwas observed to outperform lda on topic labeling tasks . however , comparing the two case studies , we see the performance differential was less for wikipedia topics and greater for the highly technical and scientific topics present in the nsf grants corpus .",
    "we attribute this to the fact that wikipedia is an encyclopedia with many topics that are very general and broad in nature . on those topics that are so broad and general",
    "that they are best summarized with a single word ( , _ tennis _ , _ bbc _ ) , lda performs quite well  albeit sometimes less well than docsetlabeler . in cases where there is not an equivalently expressive bigram ( two - word phrase ) or proper unigram , lda will perform better than our method , since docsetlabelercurrently focuses only on bigrams and _ proper _ unigrams .",
    "one example of the latter case is the _ motorcycle _ topic in wikipedia shown in table [ tab : wikilabels ] .",
    "the top - ranked labels generated by docsetlabelerare simply not as expressive as the simple label `` motorcycle '' produced by lda . addressing such cases is an area for future work .",
    "however , we find these cases to be in the minority  especially with respect to mining content from scientific and technical documents , which is our current and primary area of interest .    a second limitation is related to short texts .",
    "both lda and docsetlabelerare optimized for articles , summaries , and reports , such as the corpora considered in this work .",
    "shorter documents such as abstracts are also handled well by both algorithms , as evidenced by performance on the nsf grant abstracts .",
    "however , extremely short texts can cause difficulties .",
    "this was observed to a certain degree in some wikipedia topics containing many so - called `` stub '' articles of only a single sentence ( one - sentence descriptions of minor fictional characters , small towns , or persons of minor notability ) .",
    "one solution might be to replace lda and docsetlabelerwith algorithms specifically designed to handle short texts such as twitter - lda @xcite and keyword extraction algorithms designed for short snippets of text @xcite .",
    "we leave an investigation of this for future work .",
    "we have investigated the use of _ topic similarity networks _ as a practical approach to improving the interpretability of lda topic models .",
    "we described both how to construct such networks and an approach to labeling nodes in the network .",
    "these methods were combined and employed to effectively characterize and explore 14 years of nsf - funded basic research and the english portion of wikipedia using network analysis . for future work , we plan on incorporating these visualizations into a larger , facet - based , text analytic system previously developed for the u.s .",
    "department of defense ( see @xcite for more details on this system ) .",
    "jason chuang , daniel ramage , christopher manning , and jeffrey heer . .",
    "in _ proceedings of the sigchi conference on human factors in computing systems _ , chi 12 , pages 443452 , new york , ny , usa , 2012 .",
    "acm .",
    "jey  h. lau , karl grieser , david newman , and timothy baldwin . .",
    "in _ proceedings of the 49th annual meeting of the association for computational linguistics : human language technologies - volume 1 _ , hlt 11 , pages 15361545 , stroudsburg , pa , usa , 2011 .",
    "association for computational linguistics .",
    "arun  s. maiya , john  p. thompson , francisco  l. lemos , and robert  m. rolfe . .",
    "in _ proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 13 , pages 13751383 , new york , ny , usa , 2013 .",
    "acm .",
    "qiaozhu mei , xuehua shen , and chengxiang zhai . .",
    "in _ proceedings of the 13th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 07 , pages 490499 , new york , ny , usa , 2007 .",
    "acm .",
    "chi wang , marina danilevsky , nihit desai , yinan zhang , phuong nguyen , thrivikrama taula , and jiawei han . . in _ proceedings of the 19th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 13 , pages 437445 , new york , ny , usa , 2013 .",
    "xuerui wang , andrew mccallum , and xing wei . .",
    "in _ proceedings of the 2007 seventh ieee international conference on data mining _ , icdm 07 , pages 697702 , washington , dc , usa , 2007 .",
    "ieee computer society .",
    "yi  wang , hongjie bai , matt stanton , wen  y. chen , and edward  y. chang . .",
    "in _ proceedings of the 5th international conference on algorithmic aspects in information and management _ , volume 5564 of _ aaim 09 _ , pages 301314 , berlin , heidelberg , 2009 .",
    "springer - verlag .",
    "furu wei , shixia liu , yangqiu song , shimei pan , michelle  x. zhou , weihong qian , lei shi , li  tan , and qiang zhang . . in _ proceedings of the 16th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 10 , pages 153162 , new york , ny , usa , 2010 .",
    "waynexin zhao , jing jiang , jianshu weng , jing he , ee - peng lim , hongfei yan , and xiaoming li . .",
    "in paul clough , colum foley , cathal gurrin , garethj jones , wessel kraaij , hyowon lee , and vanessa mudoch , editors , _ advances in information retrieval _ , volume 6611 of _ lecture notes in computer science _ , chapter  34 , pages 338349 .",
    "springer berlin heidelberg , berlin , heidelberg , 2011 ."
  ],
  "abstract_text": [
    "<S> we investigate ways in which to improve the interpretability of lda topic models by better analyzing and visualizing their outputs . </S>",
    "<S> we focus on examining what we refer to as _ topic similarity networks _ : graphs in which nodes represent latent topics in text collections and links represent similarity among topics . </S>",
    "<S> we describe efficient and effective approaches to both building and labeling such networks . </S>",
    "<S> visualizations of topic models based on these networks are shown to be a powerful means of exploring , characterizing , and summarizing large collections of unstructured text documents . </S>",
    "<S> they help to `` tease out '' non - obvious connections among different sets of documents and provide insights into how topics form larger themes . </S>",
    "<S> we demonstrate the efficacy and practicality of these approaches through two case studies : 1 ) nsf grants for basic research spanning a 14 year period and 2 ) the entire english portion of wikipedia . </S>"
  ]
}