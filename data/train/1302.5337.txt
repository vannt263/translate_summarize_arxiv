{
  "article_text": [
    "matrix completion is the task to reconstruct low - rank matrices from a subset of its entries and occurs naturally in many practically relevant problems , such as missing feature imputation , multi - task learning  @xcite , transductive learning  @xcite , or collaborative filtering and link prediction  @xcite .",
    "almost all known methods performing matrix completion are optimization methods such as the max - norm and nuclear norm heuristics  @xcite , or optspace  @xcite , to name a few amongst many .",
    "these methods have in common that in general ( a ) they reconstruct the whole matrix and ( b ) error bounds are given for all of the matrix , not single entries .",
    "these two properties of existing methods are in particular unsatisfactory in the scenario when one is interested only in predicting ( resp .  imputing ) one single missing entry or a set of interesting missing entries instead of all - which is for real data a more natural task than imputing all missing entries , in particular in the presence of large scale data ( resp .",
    "big data ) .",
    "indeed the design of such a method is not only desirable but also feasible , as the results of @xcite suggest by relating algebraic combinatorial properties and the low - rank setting to the reconstructability of the data .",
    "namely , the authors provide algorithms which can decide for one entry if it can be - in principle - reconstructed or not , thus yielding a statement of trustability for the output of any algorithm .    in this paper",
    ", we demonstrate the first time how algebraic combinatorial techniques , combined with stochastic error minimization , can be applied to ( a ) reconstruct single missing entries of a matrix and ( b ) provide lower variance bounds for the error of any algorithm resp .",
    "estimator for that particular entry - where the error bound can be obtained without actually reconstructing the entry in question . in detail ,",
    "our contributions include :    * the construction of a variance - minimal and unbiased estimator for any fixed missing entry of a rank - one - matrix , under the assumption of known noise variances * an explicit form for the variance of that estimator , being a lower bound for the variance of any unbiased estimation of any fixed missing entry and thus yielding a quantiative measure on the trustability of that entry reconstructed from any algorithm * the description of a strategy to generalize the above to any rank * comparison of the estimator with two state - of - the - art optimization algorithms ( optspace and nuclear norm ) , and error assessment of the three matrix completion methods with the variance bound    note that most of the methods and algorithms presented in this paper restrict to rank one .",
    "this is not , however , inherent in the overall scheme , which is general .",
    "we depend on rank one only in the sense that we understand the combinatorial - algebraic structure of rank - one - matrix completion exactly , whereas the behavior in higher rank is not yet as well understood .",
    "nonetheless , it is , in principle accessible , and , once available will can be `` plugged in '' to the results here without changing the complexity much .",
    "in  @xcite , an intricate connection between the algebraic combinatorial structure , asymptotics of graphs and analytical reconstruction bounds has been exposed .",
    "we will refine some of the theoretical concepts presented in that paper which will allow us to construct the entry - wise estimator .",
    "an matrix @xmath0 is called mask . if @xmath1 is a partially known matrix , then the mask of @xmath1 is the mask which has @xmath2-s in exactly the positions which are known in @xmath1 ; and @xmath3-s otherwise .",
    "let @xmath4 be an @xmath5 mask .",
    "we will call the unique bipartite graph @xmath6 which has @xmath4 as bipartite adjacency matrix the _ completion graph _ of @xmath4",
    ". we will refer to the @xmath7 vertices of @xmath6 corresponding to the rows of @xmath4 as blue vertices , and to the @xmath8 vertices of @xmath6 corresponding to the columns as red vertices . if @xmath9 is an edge in @xmath10 ( where @xmath10 is the complete bipartite graph with @xmath7 blue and @xmath8 red vertices ) , we will also write @xmath11 instead of @xmath12 and for any @xmath5 matrix @xmath1 .    a fundamental result , ( * ? ?",
    "* theorem 2.3.5 ) , says that identifiability and reconstructability are , up to a null set , graph properties .",
    "[ thm : closure ] let @xmath1 be a generic is sampled from a continuous density , then the set of non - generic @xmath1 is a null set . ] and partially known @xmath5 matrix of rank @xmath13 , let @xmath4 be the mask of @xmath1 , let @xmath14 be integers .",
    "whether @xmath12 is reconstructible ( uniquely , or up to finite choice ) depends only on @xmath4 and the true rank @xmath13 ; in particular , it does not depend on the true @xmath1 .    for rank one ,",
    "as opposed to higher rank , the set of reconstructible entries is easily obtainable from @xmath6 by combinatorial means :    [ thm : closure ] let @xmath15 be the completion graph of a partially known @xmath5 matrix @xmath1 .",
    "then the set of uniquely reconstructible entries of @xmath1 is exactly the set @xmath11 , with @xmath16 in the transitive closure of @xmath17 . in particular , all of @xmath1 is reconstructible if and only if @xmath17 is connected .",
    "we extend theorem  [ thm : closure ] s theoretical reconstruction guarantee by describing an explicit , algebraic algorithm for actually doing the reconstruction",
    ". this algorithm will be the basis of an entry - wise , variance - optimal estimator in the noisy case . in any rank",
    ", such a reconstruction rule can be obtained by exposing equations which explicitly give known and unknown entries in terms of only known entries due to the fact that the set of low - rank matrices is an irreducible variety ( the common vanishing locus of finitely many polynomial equations ) .",
    "we are able to derive the reconstruction equations for rank one .",
    "let @xmath18 ( resp .",
    "@xmath19 ) be a path ( resp .",
    "cycle ) , with a fixed start and end ( resp .",
    "traversal order ) .",
    "we will denote by @xmath20 be the set of edges in @xmath21 ( resp . @xmath22 and @xmath23 ) traversed from blue vertex to a red one , and by @xmath24 the set of edges traversed from a red vertex to a blue one . that directs all edges from blue to red , and then taking @xmath20 to be the set of edges traversed forwards and @xmath24 the set of edges traversed backwards .",
    "this convention is convenient notationally , but any initial orientation of @xmath10 will give us the same result . ] from now on , when we speak of `` oriented paths '' or `` oriented cycles '' , we mean with this sign convention and some fixed traversal order .",
    "let @xmath25 be a @xmath5 matrix of rank @xmath2 , and identify the entries @xmath12 with the edges of @xmath10 .",
    "for an oriented cycle @xmath23 , we define the polynomials @xmath26 where for negative entries of @xmath1 , we fix a branch of the complex logarithm .",
    "[ thm : circpoly ] let @xmath25 be a generic @xmath5 matrix of rank @xmath2 .",
    "let @xmath19 be an oriented cycle .",
    "then , @xmath27 .",
    "_ proof : _ the determinantal ideal of rank one is a binomial ideal generated by the @xmath28 minors of @xmath1 ( where entries of @xmath1 are considered as variables ) .",
    "the minor equations are exactly @xmath29 , where @xmath23 is an elementary oriented four - cycle ; if @xmath23 is an elementary @xmath30-cycle , denote its edges by @xmath31 , @xmath32 , @xmath33 , @xmath34 , with @xmath35 .",
    "let @xmath36 be the collection of the elementary @xmath30-cycles , and define @xmath37 and @xmath38 . by sending the term @xmath39 to a formal variable @xmath40",
    ", we see that the free @xmath41-group generated by the @xmath42 is isomorphic to @xmath43 . with this equivalence , it is straightforward that , for any oriented cycle @xmath44 , @xmath45 lies in the @xmath41-span of elements of @xmath46 and , therefore , formally , @xmath47 with the @xmath48 .",
    "thus @xmath49 vanishes when @xmath1 is rank one , since the r.h.s .",
    "exponentiating , we see that @xmath50 if @xmath1 is generic and rank one , the r.h.s .",
    "evaluates to one , implying that @xmath51 vanishes.@xmath52    [ cor : circpoly ] let @xmath25 be a @xmath5 matrix of rank @xmath2 .",
    "let @xmath53 be two vertices in @xmath10 .",
    "let @xmath54 be two oriented paths in @xmath10 starting at @xmath55 and ending at @xmath56 .",
    "then , for all @xmath1 , it holds that @xmath57 .",
    "it is possible to prove that the set of @xmath58 forms the set of polynomials vanishing on the entries of @xmath1 which is minimal with respect to certain properties .",
    "namely , the @xmath58 form a universal grbner basis for the determinantal ideal of rank @xmath2 , which implies the converse of theorem  [ thm : circpoly ] . from this , one can deduce that the estimators presented in section  [ sec : est.estim ] are variance - minimal amongst all unbiased ones .",
    "in this section , we will construct an estimator for matrix completion which ( a ) is able to complete single missing entries and ( b ) gives universal error estimates for that entry that are independent of the reconstruction algorithm .      in all of the following",
    ", we will assume that the observations arise from the following sampling process :    [ ass : sampling ] there is an unknown fixed , rank one , matrix @xmath1 which is generic , and an @xmath5 mask @xmath0 which is known .",
    "there is a ( stochastic ) noise matrix @xmath59 whose entries are uncorrelated and which is multiplicatively centered with finite variance , non - zero . ]",
    "variance ; i.e. , @xmath60 and @xmath61 for all @xmath62 and @xmath63 .",
    "the observed data is the matrix @xmath64 , where @xmath65 denotes the hadamard ( i.e. , component - wise ) product .",
    "that is , the observation is a matrix with entries @xmath66 .",
    "the assumption of multiplicative noise is a necessary precaution in order for the presented estimator ( and in fact , any estimator ) for the missing entries to have bounded variance , as shown in example  [ ex : variance ] below .",
    "this is not , in practice , a restriction since an infinitesimal additive error @xmath67 on an entry of @xmath1 is equivalent to an infinitesimal multiplicative error @xmath68 , and additive variances can be directly translated into multiplicative variances if the density function for the noise is known ; this adds an independent combinatorial problem for the estimation of the sign which can be done by maximum likelihood . in order to keep the exposition short and easy",
    ", we did not include this into the exposition . ] .",
    "the previous observation implies that the multiplicative noise model is as powerful as any additive one that allows bounded variance estimates .",
    "[ ex : variance ] consider the rank one matrix @xmath69 the unique equation between the entries is @xmath70 solving for any entry will have another entry in the denominator , for example @xmath71 thus we get an estimator for @xmath72 when substituting observed and noisy entries for @xmath73 .",
    "when @xmath74 approaches zero , the estimation error for @xmath72 approaches infinity . in particular ,",
    "if the density function of the error @xmath75 of @xmath74 is too dense around the value @xmath76 , then the estimate for @xmath72 given by the equation will have unbounded variance . in such a case , one can show that no estimator for @xmath72 has bounded variance .      in this section ,",
    "we construct the unbiased estimator for the entries of a rank - one - matrix with minimal variance .",
    "first , we define some notation to ease the exposition :    we will denote by @xmath77 and @xmath78 the logarithmic entries and noise . thus , for some path @xmath21 in @xmath10 we obtain @xmath79 denote by @xmath80 the logarithmic ( observed ) entries , and @xmath81 the ( incomplete ) matrix which has the ( observed ) @xmath82 as entries .",
    "denote by @xmath83    the components of the estimator will be built from the @xmath84 :    [ lem : explpb ] let @xmath85 be the graph of the mask @xmath4 .",
    "let @xmath86 be any edge with @xmath55 red .",
    "let @xmath21 be an oriented path , then @xmath21 can also be the path consisting of the single edge @xmath16 . ] in @xmath6 starting at @xmath55 and ending at @xmath56 .",
    "then , @xmath87 is an unbiased estimator for @xmath88 with variance @xmath89    _ proof : _ by linearity of expectation and centeredness of @xmath90 , it follows that @xmath91 thus @xmath92 is unbiased .",
    "since the @xmath93 are uncorrelated , the @xmath94 also are ; thus , by bienaym s formula , we obtain @xmath95 and the statement follows from the definition of @xmath96 + in the following , we will consider the following parametric estimator as a candidate for estimating @xmath97 :    [ not : calpxalpha ] fix an edge @xmath86 .",
    "let @xmath98 be a basis for the set of all oriented paths starting at @xmath55 and ending at @xmath56 , in the free abelian group generated by the @xmath40 , subject to the relations @xmath99 for all cycles @xmath23 in @xmath100 .",
    "independence can be taken as linear independence of the coefficient vectors of the @xmath101 . ] and denote @xmath102 by @xmath103 .",
    "for @xmath104 , set @xmath105    furthermore , we will denote by @xmath106 the @xmath8-vector of ones .",
    "the following lemma follows immediately from lemma  [ lem : explpb ] and theorem  [ thm : circpoly ] :    @xmath107 in particular , @xmath108 is an unbiased estimator for @xmath109 if and only if @xmath110 .",
    "we will now show that minimizing the variance of @xmath108 can be formulated as a quadratic program with coefficients entirely determined by @xmath88 , the measurements @xmath94 and the graph @xmath6 .",
    "in particular , we will expose an explicit formula for the @xmath111 minimizing the variance . before stating the theorem",
    ", we define a suitable kernel :    let @xmath112 be an edge . for an edge @xmath16 and a path @xmath21 , set @xmath113 if @xmath114 otherwise @xmath115 let @xmath116 be any fixed oriented paths .",
    "define the ( weighted ) _ path kernel _",
    "@xmath117 by @xmath118    under our assumption that @xmath119 for all @xmath112 , the path kernel is positive definite , since it is a sum of @xmath103 independent positive semi - definite functions ; in particular , its kernel matrix has full rank .",
    "here is the variance - minimizing unbiased estimator :    [ prop : varest ] let @xmath120 be a pair of vertices , and @xmath98 a basis for the @xmath121@xmath122 path space in @xmath17 with @xmath103 elements .",
    "let @xmath123 be the @xmath124 kernel matrix of the path kernel with respect to the basis @xmath98 .",
    "for any @xmath104 , @xmath125 moreover , under the condition @xmath126 , the variance @xmath127 is minimized by @xmath128    _ proof : _ by inserting definitions , we obtain @xmath129 writing @xmath130 as vectors , and @xmath131 as matrices , we obtain @xmath132 by using that @xmath133 for any scalar @xmath134 , and independence of the @xmath94 , an elementary calculation yields @xmath135 in order to determine the minimum of the variance in @xmath111 , consider the lagrangian @xmath136 where the slack term models the condition @xmath137 .",
    "an elementary calculation yields @xmath138 where @xmath106 is the vector of ones . due to positive definiteness of @xmath123",
    "the function @xmath127 is convex , thus @xmath139 will be the unique @xmath111 minimizing the variance while satisfying @xmath126.@xmath52    the above setup works in wider generality : ( i ) if @xmath140 is allowed and there is an @xmath121@xmath122 path of all zero variance edges , the path kernel becomes positive semi - definite ; ( ii ) similarly if @xmath141 is replaced with any set of paths at all , the same may occur . in both cases",
    ", we may replace @xmath142 with the moore - penrose pseudo - inverse and the proposition still holds : ( i ) reduces to the exact reconstruction case of theorem  [ thm : closure ] ; ( ii ) produces the optimal estimator with respect to @xmath141 , which is optimal provided that @xmath141 is spanning , and adding paths to @xmath141 does not make the estimate worse .",
    "an estimator for rank @xmath143 and higher , together with a variance analysis , can be constructed similarly once all polynomials known which relate the entries under each other .",
    "the main difficulty lies in the fact that these polynomials are not parameterized by cycles anymore , but specific subgraphs of @xmath6 , see  ( * ? ? ?",
    "* section 2.5 ) . were these polynomials known , an estimator similar to @xmath108 as in notation  [ not : calpxalpha ] could be constructed , and a subsequent variance ( resp .",
    "perturbation ) analysis performed .      in this section ,",
    "we describe the two main algorithms which calculate the variance - minimizing estimate @xmath144 for any fixed entry @xmath12 of an @xmath5 matrix @xmath1 , which is observed with noise , and the variance bound for the estimate @xmath144 .",
    "it is important to note that @xmath12 does not necessarily need to be an entry which is missing in the observation , it can also be any entry which has been observed . in the latter case ,",
    "algorithm  [ alg : xalpha ] will give an improved estimate of the observed entry , and algorithm  [ alg : var ] will give the trustworthiness bound on this estimate .",
    "since the the path matrix @xmath23 , the path kernel matrix @xmath123 , and the optimal @xmath111 is required for both , we first describe algorithm  [ alg : alpha ] which determines those .    find a linearly independent set of paths @xmath98 in the graph @xmath6 , starting from @xmath62 and ending at @xmath63.[alg : alpha.step1 ]",
    "determine the matrix @xmath145 with @xmath146 set @xmath147 if @xmath114 , otherwise @xmath115    define a diagonal matrix @xmath148 with @xmath149 for @xmath150    compute the kernel matrix @xmath151    calculate @xmath152    output @xmath153 and @xmath111 .",
    "the steps of the algorithm follow the exposition in section  [ sec : est.estim ] , correctness follows from the statements presented there .",
    "the only task in algorithm  [ alg : alpha ] that is nt straightforward is the computation of a linearly independent set of paths in step  [ alg : alpha.step1 ] .",
    "we can do this time linear in the number of observed entries in the mask @xmath4 with the following method .",
    "to keep the notational manageable , we will conflate formal sums of the @xmath40 , cycles in @xmath154 and their representations as vectors in @xmath155 , since there is no chance of confusion .",
    "if @xmath156 is not an edge of @xmath4 , and @xmath62 and @xmath63 are in different connected components , then @xmath98 is empty .",
    "output @xmath157 .",
    "otherwise , if @xmath156 is not an edge , of @xmath4 , add a `` dummy '' copy .",
    "compute a spanning forest @xmath158 of @xmath4 that does not contain @xmath156 , if possible .    for each edge",
    "@xmath159 , compute the fundamental cycle @xmath160 of @xmath16 in @xmath158 .",
    "if @xmath156 is an edge in @xmath4 , output @xmath161 .    otherwise , let @xmath162 .",
    "output @xmath163 .",
    "we prove the correctness of algorithm  [ alg : calp ] .",
    "algorithms  [ alg : xalpha ] and  [ alg : var ] then can make use of the calculated @xmath164 to determine an estimate for any entry @xmath12 and its minimum variance bound .",
    "the algorithms follow the exposition in section  [ sec : est.estim ] , from where correctness follows ; algorithm  [ alg : xalpha ] additionally provides treatment for the sign of the entries .",
    "calculate @xmath23 and @xmath111 with algorithm  [ alg : alpha ] .",
    "store @xmath81 as a vector @xmath165 and a sign vector @xmath166 with @xmath167 .",
    "calculate @xmath168 the sign is @xmath169 if each column of @xmath170 ( @xmath171 component - wise ) contains an odd number of entries @xmath172 , else @xmath173 .",
    "return @xmath174    calculate @xmath123 and @xmath111 with algorithm  [ alg : alpha ] .",
    "return @xmath175 .",
    "note that even if observations are not available , algorithm  [ alg : var ] can be used to obtain the variance bound .",
    "the variance bound is relative , due to its multiplicativity , and can be used to approximate absolute bounds when any reconstruction estimate @xmath144 is available - which does not necessarily need to be the one from algorithm  [ alg : xalpha ] , but can be the estimation result of any reconstruction .",
    "namely , if @xmath176 is the estimated variance of the log , we obtain an upper confidence bound ( resp .",
    "deviation ) bound @xmath177 for @xmath178 and a lower confidence bound ( resp .",
    "deviation ) bound @xmath179 , corresponding to the log - confidence @xmath180 also note that if @xmath12 is not reconstructible from the mask @xmath4 ( i.e. , if the edge @xmath156 is not in the transitive closure of @xmath6 , see theorem  [ thm : closure ] ) , then the deviation bounds will be infinite .",
    "for three different masks , we calculated the predicted minimum variance for each entry of the mask .",
    "the multiplicative noise was assumed to be @xmath181 for each entry .",
    "figure  [ fig : heatmaps ] shows the predicted a - priori minimum variances for each of the masks .",
    "notice how the structure of the mask affects the expected error ; known entries generally have least variance , while it is interesting to note that in general it is less than the starting variance of @xmath2 .",
    "i.e. , tracking back through the paths can be successfully used even to denoise known entries .",
    "the particular structure of the mask is mirrored in the pattern of the predicted errors ; a diffuse mask gives a similar error on each missing entry , while the more structured masks have structured error which is determined by combinatorial properties of the completion graph and the paths therein .",
    "+   +      we generated @xmath182 random mask of size @xmath183 with @xmath184 entries sampled uniformly and a random @xmath185 matrix of rank one .",
    "the multiplicative noise was chosen entry - wise independent , with variance @xmath186 for each entry .",
    "figure [ fig:3algs.noisemse ] compares the mean squared error ( mse ) for three algorithms : nuclear norm ( using the implementation @xcite ) , optspace @xcite , and algorithm  [ alg : xalpha ] .",
    "it can be seen that on this particular mask , algorithm  [ alg : xalpha ] is competitive with the other methods and even outperforms them for low noise .",
    "the data are the same as in section  [ sec : noise ] , as are the compared algorithm .",
    "figure [ fig:3algs.linearity ] compares the error of each of the methods with the variance predicted by algorithm  [ alg : var ] each time the noise level changed .",
    "the figure shows that for any of the algorithms , the mean of the actual error increases with the predicted error , showing that the error estimate is useful for a - priori prediction of the actual error - independently of the particular algorithm .",
    "note that by construction of the data this statement holds in particular for entry - wise predictions .",
    "furthermore , in quantitative comparison algorithm  [ alg : var ] also outperforms the other two in each of the bins .",
    "in this paper , we have introduced an algebraic combinatorics based method for reconstructing and denoising single entries of an incomplete and noisy matrix , and for calculating confidence bounds of single entry estimations for arbitrary algorithms .",
    "we have evaluated these methods against state - of - the art matrix completion methods .",
    "the results of section  [ sec : exp ] show that our reconstruction method is competitive and that - for the first time - our variance estimate provides a reliable prediction of the error on each single entry which is an a - priori estimate , i.e. , depending only on the noise model and the position of the known entries .",
    "furthermore , our method allows to obtain the reconstruction and the error estimate for a single entry which existing methods are not capable of , possibly using only a small subset of neighboring entries - a property which makes our method unique and particularly attractive for application to large scale data .",
    "we thus argue that the investigation of the algebraic combinatorial properties of matrix completion , in particular in rank @xmath143 and higher where these are not yet completely understood , is crucial for the future understanding and practical treatment of big data .",
    "9 [ 1]#1 [ 1]`#1 ` urlstyle[1]doi : # 1    e.  acar , d.m .",
    "dunlavy , and t.g .",
    "link prediction on evolving data using matrix and tensor factorizations . in _ data mining workshops , 2009 .",
    "ieee international conference on _ , pages 262269 .",
    "ieee , 2009 .",
    "a.  argyriou , c.  a. micchelli , m.  pontil , and y.  ying .",
    "a spectral regularization framework for multi - task structure learning . in j.c .",
    "platt , d.  koller , y.  singer , and s.  roweis , editors , _ advances in nips 20 _ , pages 2532 .",
    "mit press , cambridge , ma , 2008 .",
    "emmanuel  j. cands and benjamin recht .",
    "exact matrix completion via convex optimization . _ found .",
    "comput . math .",
    "_ , 90 ( 6):0 717772 , 2009 .",
    "issn 1615 - 3375 .",
    "doi : 10.1007/s10208 - 009 - 9045 - 5 .",
    "url http://dx.doi.org/10.1007/s10208-009-9045-5 .",
    "a.  goldberg , x.  zhu , b.  recht , j.  xu , and r.  nowak .",
    "transduction with matrix completion : three birds with one stone . in j.",
    "lafferty , c.  k.  i. williams , j.  shawe - taylor , r.s .",
    "zemel , and a.  culotta , editors , _ advances in neural information processing systems 23 _ , pages 757765 . 2010 .",
    "raghunandan  h. keshavan , andrea montanari , and sewoong oh . matrix completion from a few entries .",
    "_ ieee trans .",
    "inform . theory _",
    ", 560 ( 6):0 29802998 , 2010 .",
    "issn 0018 - 9448 .",
    "doi : 10.1109/tit.2010.2046205 .",
    "url http://dx.doi.org/10.1109/tit.2010.2046205 .",
    "franz  j. kirly , louis theran , ryota tomioka , and takeaki uno . the algebraic combinatorial approach for low - rank matrix completion .",
    "preprint , arxiv:1211.4116v3 , 2012 .",
    "url http://arxiv.org/abs/1211.4116 .",
    "a.  menon and c.  elkan .",
    "link prediction via matrix factorization .",
    "_ machine learning and knowledge discovery in databases _ , pages 437452 , 2011 .",
    "n.  srebro , j.  d.  m. rennie , and t.  s. jaakkola .",
    "maximum - margin matrix factorization . in lawrence",
    "k. saul , yair weiss , and lon bottou , editors , _ advances in nips 17 _ , pages 13291336 . mit press , cambridge , ma , 2005 .",
    "ryota tomioka , kohei hayashi , and hisashi kashima . on the extension of trace norm to tensors . in",
    "_ nips workshop on tensors , kernels , and machine learning _ , 2010 .",
    "we adopt the conventions of section  [ sec : theory ] , so that @xmath17 is a bipartite graph with @xmath7 blue vertices , @xmath8 red ones , and @xmath16 edges oriented from blue to red .",
    "recall the isomorphism , observed in the proof of theorem  [ thm : circpoly ] of the @xmath187-group of the polynomials @xmath188 and the oriented cycle space @xmath154 .",
    "define @xmath189 ( the first betti number of the graph ) .",
    "some standard facts are that : ( i ) the rank of @xmath154 is @xmath190 ; ( ii ) we can obtain a basis for @xmath154 consisting only of simple cycles by picking any spanning forest @xmath158 of @xmath17 and then using as basis elements the fundamental cycles @xmath160 of the edges @xmath191 .",
    "this justifies step 4 .",
    "let @xmath156 be an edge of @xmath17 .",
    "define an @xmath62@xmath63 to be the set of subgraphs such that , for generic rank one @xmath1 , @xmath192 . by theorem  [ thm : circpoly ]",
    ", we can write these as @xmath187-linear combinations of @xmath193 and oriented cycles . from this , we see that the rank of the path space is @xmath194 and the graph theoretic identification of elements in the path space with subgraphs that have even degree at every vertex except @xmath62 and @xmath63 . thus , if @xmath156 is an edge of @xmath17 , step 5 is justified , completing the proof of correctness in this case .    if @xmath156 was not an edge , step 1 guarantees that the dummy copy of @xmath156 that we added is not in the spanning tree computed in step 3 .",
    "thus , the element @xmath162 computed in step 6 is a simple path from @xmath62 to @xmath63 .",
    "the collection of elements generated in step 6 is independent by the same fact in @xmath195 and has rank @xmath196 and does not put a positive coefficient on the dummy generator @xmath193 . @xmath52"
  ],
  "abstract_text": [
    "<S> we propose a general framework for reconstructing and denoising single entries of incomplete and noisy entries . </S>",
    "<S> we describe : effective algorithms for deciding if and entry can be reconstructed and , if so , for reconstructing and denoising it ; and a priori bounds on the error of each entry , individually . in the noiseless case </S>",
    "<S> our algorithm is exact . for rank - one matrices , </S>",
    "<S> the new algorithm is fast , admits a highly - parallel implementation , and produces an error minimizing estimate that is qualitatively close to our theoretical and the state - of - the - are nuclear norm and optspace methods .    </S>",
    "<S> = 1 </S>"
  ]
}