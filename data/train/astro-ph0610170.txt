{
  "article_text": [
    "we consider the standard local model of primordial cmb non - gaussianity where sky maps are modeled as @xmath1 with @xmath2 a zero - mean gaussian vector with covariance matrix @xmath3 and @xmath4 .",
    "the covariance matrix is defined by the covariance function of the random field .",
    "the objective is to use the data vector @xmath5 to estimate @xmath0 .    as opposed to the spectrum / moment estimators considered by creminelli et al .",
    "@xcite and babich @xcite , we will also consider maximum likelihood and shrinkage versions of the estimators .",
    "in addition , we will focus on studying their performance under the assumption that the leading order model is correct .",
    "that is , while these authors considered perturbation models of , we will assume the local model is correct .",
    "the reason is that to understand properties of perturbations , it is important to understand properties under the ideal case where the leading order model is correct and standard statistical tools can be used .",
    "extending the statistical methods to perturbation models is much more difficult ; it is not clear to this author what methods will remain valid and to what degree .    to compare estimators",
    "we will mainly use the mean square error so as to take into account bias and variance . however , to make a proper comparison we will have to address some technical issues regarding statistical estimation in general that were raised at the workshop on non - gaussianity in cosmology held in trieste in july 2006 .",
    "for example , some of the comments and talks at this workshop , as well as in published papers , seem to give the message that the cramer - rao ( cr ) inequality is an ideal way to decide the optimality of estimators ; that this inequality can be used to decide if an estimator provides all the available information about an unknown parameter , and that one should strive to find unbiased estimators of minimum variance . however , none of these statements is quite correct",
    ". we will show this with simple examples as well as with the problem of estimating @xmath0 to test cmb non - gaussianity .    in order not to keep repeating the same references for the different statistical concepts mentioned in this note ( in italics ) , we refer the reader to two excellent general references where",
    "all the definitions can be found : lehmann & casella @xcite and casella & berger @xcite .",
    "we start by recalling the scalar version of the cr inequality : it states that under ` regularity ' conditions the variance of an _ unbiased estimator _",
    "@xmath6 of @xmath7 satisfies @xmath8 where the _ fisher information _ @xmath9 is defined by @xmath10 and @xmath11 is the _ likelihood function_. the first problem when trying to apply this bound to estimates of @xmath0 is in the regularity conditions ; one of the conditions is to be able to pass the derivative @xmath12 of the likelihood function under the integral @xmath13 .",
    "we will see that this can not be done for the @xmath0 problem .",
    "however , even if the regularity conditions were satisfied , we would still have to determine if there are unbiased estimators of @xmath0 .",
    "the answer again is negative . finally ,",
    "if the conditions were met and there were unbiased estimators matching the bound , it still would not follow that the estimators would be any good .",
    "but before we consider these problems , we believe it is important to start with two simple examples that can be completely worked out and that illustrate some of the basic issues we will address : + * example . * consider the uniform distribution on the interval @xmath14 .",
    "the goal is to estimate @xmath7 .",
    "the probability density function ( pdf ) is @xmath15 where @xmath16 stands for the indicator function of the set @xmath17 .",
    "the likelihood function of @xmath7 is @xmath18 the first two derivatives of the log - likelihood are : @xmath19 first , note that the derivative @xmath12 can not be passed under the integral sign : @xmath20 this means that the conditions required for the cr inequality are not satisfied and therefore it is not necessarily valid . in fact , since the likelihood function is constant in @xmath21 with probability 1 , it follows that @xmath22 . or , formally , @xmath23 - \\left[\\,{{\\rm e}}\\left(\\,\\partial/\\partial\\theta \\log l(\\theta;x ) \\,\\right)\\,\\right]^2\\\\ &   = & \\frac{1}{\\theta^2 } - \\frac{1}{\\theta^2 } = 0;\\end{aligned}\\ ] ] thus the cr bound is infinite",
    "yet , @xmath24 is an unbiased estimator of @xmath7 with finite variance @xmath25 .",
    "the ml estimator is @xmath26 .",
    "this estimator is biased but its _ mean square error _",
    "( mse ) is smaller than that of the unbiased estimator : @xmath27 more generally , for a sample of size @xmath28 , the sample mean @xmath29 is a _ consistent estimator _ of @xmath7 ( i.e. , it is unbiased and its variance decreases to 0 as @xmath30 ) : @xmath31    it is also easy to see that the ml estimator is @xmath32 , where @xmath33 is the largest of the @xmath34 .",
    "the bias and variance of this estimator go to zero as the sample size increases @xmath35 which shows that the ml estimate is also consistent ; the requirements for consistency of ml estimates are different from those of the cr inequality .",
    "the variables @xmath36 are usually called _ order statistics _ and make their appearance here because the conditions @xmath37, ... ,@xmath38 are equivalent to the single requirement @xmath39 .    the estimator @xmath40 is known as a _ moment estimator _ because it is obtained by first solving for @xmath7 as a function of the moments of @xmath21 and then substituting the sample moments for the population ones ( correlation functions , and thus the bispectrum , are examples of moment estimators ) .",
    "it is well known that moment estimators can usually be improved by conditioning on what are called _ sufficient statistics _ ( functions that contain all the information about @xmath7 that is provided by the data ) .",
    "moment estimators are usually not given in terms of sufficient statistics while ml estimators are .",
    "for example , to improve @xmath40 we condition on the sufficient statistic @xmath33 and obtain the estimator @xmath41 this estimator is unbiased and its variance is smaller than that of the original @xmath40 : @xmath42 this confirms that the moment estimate did not contain all the information about @xmath7 .    finally , recall that under regularity conditions @xmath9 can also be computed as @xmath43 but not for our simple example for the same reason the cr inequality is not valid : we have @xmath44    * example .",
    "* we have seen that the cr inequality does not have to hold if the support of the pdf depends on the parameter .",
    "but , could it hold for values of @xmath7 for which the support is ` essentially ' @xmath45 ?",
    "this is not necessarily even in the limit is a finite interval .",
    "here is an example : we make a slight change to the pdf in the previous example and use the uniform @xmath46 .",
    "the support of the pdf still depends on @xmath7 and so the cr inequality is not valid . but suppose we are only interested in @xmath47 . in the limit as @xmath48 ,",
    "the support becomes @xmath49 .",
    "hence , it may be tempting to conclude that the cr inequality is ` approximately ' valid for small @xmath7 ; it is not . just as before , the fisher information is @xmath22 and the bound is again infinite . and , just as before , there are unbiased estimators ( e.g. , @xmath50 ) of finite variance . the problem of estimating @xmath0 is even worse because the likelihood function blows up at the boundaries",
    ". + the lessons to learn from these simple examples are : ( i ) the cr bound does not have to be valid if the support of the pdf depends on the parameters to be estimated ; ( ii ) the cr bound is not always applicable and even when the bound is infinite , there may be consistent unbiased estimators of finite variance ; ( iii ) an unbiased estimator is not necessarily better than a biased one .",
    "in fact , there are many cases where no unbiased estimators exist , or where a biased estimator is better than the best unbiased estimator .",
    "furthermore , although not shown in this example , there are many examples where an unbiased estimator has a variance that matches the cr bound and yet its mse is larger than that of a biased estimator ; ( iv ) moment estimators ( such as the bispectrum ) do not usually contain all the information about the unknown parameter that the data provide and therefore can be improved by conditioning on sufficient statistics .",
    "we will show that the problem of estimating @xmath0 is similar to that of estimating @xmath7 in the examples above but it is slightly worse in that the likelihood function is unbounded , it has more than one maximizer and there are no finite variance unbiased estimators of @xmath0 .",
    "the rest of the note is organized as follows : in section [ sec : toy ] , we start with a particular case of the local model to show that the conditions for the cr inequality of @xmath0 are not satisfied .",
    "we determine some of the particular characteristics of the likelihood function and derive simple expressions for estimates based on maximum likelihood . in section [ sec : gen ]",
    "we return to the general local model to extend the results found for the simple model .",
    "again we find simple expressions for the ml estimates but find that neither the moment nor the ml estimators are better for all values of @xmath51 .",
    "we then make shrinkage modifications to the estimators to improve their performance for small values of @xmath0 .",
    "some technical details are left to the appendix .",
    "we now return to the local model and start with a particular case described in section iii of babich @xcite .",
    "this model is based on independent and identically distributed observations @xmath52 where the @xmath53 are independent @xmath54 . for simplicity ,",
    "we will write @xmath51 instead of @xmath0 .      to derive the pdf and study the cr bound , it is enough to consider the case of a single @xmath55 , which can be written as @xmath56 where @xmath57 by definition @xmath58 has a _ noncentral _",
    "@xmath59 distribution with noncentrality parameter @xmath60 and thus its pdf is @xmath61 the pdf of @xmath62 can be easily derived as @xmath62 is just a rescaled and shifted @xmath58 .",
    "the problem is the condition @xmath63 ; this boundary makes the support of the pdf of @xmath62 depend on the unknown parameter @xmath51 .",
    "the pdf can be written as @xmath64 where @xmath65 with the sets @xmath66 defined as @xmath67 and @xmath68 .",
    "in particular , for @xmath69 the support in @xmath70 of @xmath71 has a left boundary defined by @xmath72 , which depends on @xmath51 .",
    "this is a problem for the cr inequality because the derivative @xmath73 can not be passed under the integral @xmath74 .",
    "in fact , having a pdf whose support does not depend on the unknown parameters is often stated as a requirement for the cr inequality ( e.g. , bickel & doksum @xcite ) .",
    "this is an important point that can not be overlooked .    in our toy problem",
    "the support of @xmath71 not only depends on @xmath51 but the likelihood function also diverges to infinity at the boundaries of @xmath66 .",
    "however , one can still define reasonably good ml - based estimates .",
    "in fact , we will show below that the boundaries actually lead to simple estimates that in the general case depend only on @xmath75 and not on the full covariance matrix .    .",
    "[ fig : likfun],width=340 ]      the shape of the likelihood function depends on the value of @xmath70 . on the region",
    "@xmath69 the likelihood is not zero only if @xmath70 is such that @xmath51 belongs to @xmath72 ; on @xmath76 we need a y such that @xmath51 belongs to @xmath77 .",
    "it is easy to see that when @xmath78 ( or when @xmath79 if we normalize the data to estimate @xmath80 ) , the likelihood function is just @xmath81 this function has a unique maximum and the ml estimate is well defined .",
    "for example , figure [ fig : likfun ] show the likelihood function of @xmath80 for @xmath82 .",
    "the case @xmath83 is more pathological . for @xmath69 and @xmath84",
    ", we need @xmath85 which delimits the boundary of @xmath72 . since the likelihood function blows up at the two limits , it is maximized by setting @xmath51 to be either of the limits .",
    "hence the maximum likelihood estimate is not well defined .",
    "a similar thing happens for @xmath76 and @xmath86 .",
    "however , this does not mean that neither of the two possibilities is good .",
    "in fact , we get a reasonably good estimate by averaging them : define the ml - based estimate of @xmath51 as @xmath87 it may seem strange that the estimate is positive when @xmath62 is negative and conversely ( it also happens in figure [ fig : likfun ] ) but it can be explained heuristically as follows : if @xmath88 , ten @xmath89 , that is , @xmath90 but since @xmath91 is gaussian @xmath54 , it is in the interval @xmath92 with probability 68% . hence , about 70% of the time we expect @xmath93 and @xmath94 and thus the right hand side in is negative and for the left side to have the same sign we need @xmath69 .",
    "similarly , we need @xmath76 when @xmath95 .",
    "a better estimate is obtained by taking in each case the quadratic root closest to zero @xmath96 in the section below we compare the mse of these two estimators to that of some moment estimates .",
    "we now compare ml to moment estimators of @xmath51 .",
    "to make our point , it is again sufficient to consider the scalar case of a single @xmath62 .",
    "we know that the odd moments of a gaussian variable @xmath97 are zero while the even moments are given by @xmath98 it follows that @xmath99 where the sums are over even and odd indices , respectively , and @xmath100 as we have assumed that @xmath101 is known , we focus on estimators of the dimensionless parameter @xmath80 using the normalized random variable @xmath102 .",
    "for example , the first two estimators based on the third and fifth moments are : @xmath103 the question we want to address now is whether @xmath104 is better than any of the other moment estimators .",
    "we use the mse to compare @xmath104 , @xmath105 and the ml estimator .",
    "the variance and bias of @xmath104 and @xmath105 are easily derived using and .",
    "the mse of the ml estimates is calculated through simulations .",
    "figure [ fig : momest ] shows the mse of @xmath104 , @xmath104 and the two ml estimators @xmath106 and @xmath107 .",
    "we see that @xmath104 is better than @xmath105 except for small values @xmath108 . for values",
    "@xmath109 , @xmath105 is better than the first ml based estimate @xmath110 .",
    "the ml estimate @xmath107 has much smaller mse than the moment estimates .",
    "however , we will see that the general case is not as clear cut .     for the toy model .",
    "[ fig : momest],width=340 ]      in the simple example discussed in the introduction , the cr bound is infinite but there are finite variance unbiased estimators of the unknown parameter .",
    "however , unbiased estimators do not always exist and our toy model provides one such example .",
    "suppose there is a finite variance unbiased estimator @xmath111 of @xmath51 based on @xmath112 .",
    "that is , @xmath113 for all @xmath114 and any @xmath115 .",
    "then , by dividing each @xmath116 in by a constant @xmath117 we obtain @xmath118 where @xmath119 , @xmath120 and @xmath121 .",
    "but since @xmath111 is an unbiased estimator , we must have @xmath122 and therefore @xmath123 is an unbiased estimator of @xmath51 for any @xmath117 .",
    "this implies that @xmath124 for any @xmath117 .",
    "hence , the inverse of the left hand side has to be a monomial in @xmath125 of degree @xmath126 .",
    "in particular , it is a constant for @xmath127 .",
    "we consider the case @xmath127 in the appendix and show that the condition is impossible to meet .",
    "hence , there are no unbiased estimators of finite variance based on @xmath62 .",
    "of course , one would still have to check if there are unbiased estimators based on more that one @xmath116 ( the number of such @xmath116 is called the _ degree _ of the estimator ) .",
    "we now return to the general case . as in the scalar case",
    ", a simple transformation leads to @xmath128 with @xmath129 where ` . *",
    "' stands for point - wise multiplication , and @xmath130 .",
    "the pdf of @xmath131 is of the form @xmath132 for some ` nice ' exponential function @xmath133 that will be defined below .",
    "it follows that the pdf of @xmath112 is @xmath134 where @xmath135 .    just as in the scalar case ,",
    "the support of the pdf of @xmath112 depends on @xmath51 ; it is defined by the two sets @xmath136 where @xmath137 and @xmath138 denote the smallest and largest values of the sample .",
    "hence , the cr bound is again not applicable but just as in the toy local model , there are simple expressions for estimates of @xmath51 based on maximum likelihood .",
    "or @xmath139 as a function of @xmath140 for independent samples of sizes @xmath141 and @xmath142 of the local model .",
    "the right panel shows the probability that @xmath143 and @xmath139 . [ fig : orderprobs],width=529 ]      to find ml estimates of @xmath51 in the general case , we first need to derive the function @xmath133 mentioned in the previous section . this is easy to do by computing the probability @xmath144 $ ] .",
    "the final result is : @xmath145 @xmath146 @xmath147 to be consistent with section [ sec : pdf ] , write the likelihood function of @xmath51 as @xmath148 with @xmath149 as in the scalar case , to find the maximum likelihood estimates we need to consider different cases .",
    "for @xmath150 the likelihood blows up at the roots of @xmath151 , and there are two values of @xmath51 for which this happens .",
    "a similar thing happens with @xmath152 and the two roots of @xmath153 .",
    "what is good about these cases is that they do not require the use of the full covariance matrix @xmath3 , only the variance @xmath75 is needed .",
    "the covariance matrix is needed only when @xmath154 and @xmath155 are both less than @xmath101 . in this case",
    "the likelihood remains finite and has to be maximized .",
    "however , in a large sample one expects @xmath137 to be small and @xmath138 to be large .",
    "thus the case @xmath156 should be rare in large samples .",
    "on the other hand , depending on @xmath51 , it is not always possible to have @xmath157 .",
    "for example , it can not happen for @xmath158 .",
    "however , with high probability @xmath150 or @xmath152 and in either case the ml estimates are roots of the quadratic equations that only require @xmath75 .",
    "for example , the left panel in figure [ fig : orderprobs ] shows the probability that @xmath150 or @xmath152 based on independent samples of size @xmath141 and @xmath142 .",
    "we see that with only @xmath141 , the probability is at least 82% that either @xmath150 or @xmath152 . with @xmath142",
    "the probability is very close to one .",
    "the right panel shows the probability that both cases will happen .",
    "we see that for large samples the two cases do not happen together only when @xmath159 .",
    "but of course , in the general case the samples @xmath116 are correlated because @xmath91 has a full covariance matrix but for sky maps with thousands of pixels we expect more than 40 ` degrees of freedom ' so we still expect the same behavior of small and large values of @xmath137 and @xmath138 , respectively .    ,",
    "@xmath105 and the ml estimator .",
    "the left panel shows the results for a sample size @xmath160 and the right panel for @xmath161 .",
    "[ fig : momestgen],width=566 ]    the selection of the appropriate quadratic roots depends on the range of values of @xmath162 under consideration .",
    "however , a simple geometric argument can be used to select them : if @xmath69 , then , as a function of @xmath91 , the parabola @xmath163 has a minimum at @xmath164 with minimum @xmath62 value @xmath165 and therefore @xmath51 is a solution of @xmath166 .",
    "there are two solutions of this equation , depending on whether @xmath167 or @xmath168 .",
    "these solutions are functions of @xmath169 .",
    "we have a similar situation when @xmath76 , in which case we find @xmath51 as a function of @xmath170 .",
    "hence , we have the following ml estimator @xmath171 where the roots are defined as @xmath172 for example , let us return to the case where the @xmath116 are independent . define the third and fifth moment estimates as in but averaging over the @xmath28 observations .",
    "figure [ fig : momestgen ] shows the mse of the ml and moment estimates for @xmath160 ( left panel ) and @xmath161 ( right panel ) .",
    "we make the following observations : ( i ) the fifth and third moment estimates , especially the former , provide better estimates for values @xmath108 but otherwise the ml estimator has much smaller mse ; ( ii ) the decrease in the mse with increasing sample size is much better for the ml estimators .",
    "this decrease is hardly noticeable in the moment estimates for larger values of @xmath140 where the estimates are dominated by the bias .",
    ", of the mse of moment estimators @xmath173 as a function of order @xmath174 .",
    "it shows that @xmath105 has the smallest mse near @xmath175 .",
    "the right panel shows the decrease with sample size @xmath28 of the mse of @xmath138 as an estimator of @xmath170 ( as defined in the text ) .",
    "it explains the decrease of the mse of the ml estimates in figure [ fig : momestgen ] .",
    "[ fig : mommse],width=566 ]    near @xmath175 the smallest mse is that of @xmath105 because as @xmath176 , the mse of @xmath173 is @xmath177 the left panel in figure [ fig : mommse ] shows the plot of this mse ( for fixed @xmath28 ) as a function of moment order @xmath174 ; the minimum value is at @xmath178 .",
    "thus , for small @xmath51 , @xmath105 is better than @xmath104 and the other higher order moment estimators .",
    "the large mse of @xmath179 near the origin comes from ml providing estimates that are , on the average , greater than @xmath140 ( in absolute value ) . from the appendix , we know that there are problems near the origin .",
    "it is also clear that as @xmath176 the parabola , used to choose the roots , becomes a line and the problem is ill - posed .",
    "it should be possible to improve ml estimates near the origin using _ shrinkage estimators _ or _",
    "penalized likelihood_. we return to this below .",
    "the fast decrease in mse of the ml estimate is interesting and can be explained as follows . estimating @xmath51 reduces to estimating @xmath169 ( for @xmath69 ) and @xmath170 ( for @xmath76 ) with @xmath137 and @xmath138 , respectively .",
    "the convergence of the corresponding estimates is then explained by the convergence of @xmath137 to @xmath169 and @xmath138 to @xmath170 , respectively .",
    "for example , the right panel in figure [ fig : mommse ] shows the mse of @xmath138 as an estimator of @xmath170 as a function of @xmath28 for @xmath180 .",
    "we see the same decrease ( as @xmath181 ) in mse as in figure [ fig : momestgen ] .",
    "it is not difficult to study the convergence rate and asymptotic distributions of @xmath138 and @xmath137 using extreme value theory ( e.g. , galambos @xcite ) .",
    "+      we now focus on small values of @xmath51 to determine if any of the estimators discussed above is better than the simple zero estimator : @xmath182 .",
    "figure [ fig : momestgent ] shows the results for @xmath183 and sample sizes @xmath160 ( left ) and @xmath161 ( right ) . this time we show a relative measure of the error defined by @xmath184 ( @xmath185 ) .",
    "estimators that are better than @xmath186 should have @xmath187 .",
    "the figure shows that with @xmath160 neither of the three estimators improves on the zero estimator .",
    "for the larger sample , @xmath105 and @xmath104 are an improvement but slight and only for @xmath188 . however , this is not the best we can do .",
    "our estimators can be improved near the origin using ideas based on shrinkage estimators .",
    "for example , to estimate a mean vector @xmath189 with observations @xmath190 , one can minimize @xmath191 ( where @xmath192 is fixed ) over @xmath193 and end up with @xmath194 .",
    "this estimator ` shrinks ' toward the origin and it is the result of minimizing a goodness of fit norm that ` penalizes ' vectors of large norms .",
    "this is a typical procedure used to obtain solution of ill - posed inverse problems ( e.g. , osullivan @xcite , tenorio @xcite ) .",
    "another example is the well - known _ james - stein shrinkage estimator _ of a multivariate gaussian mean , which improves on the minimum variance unbiased estimator .",
    "the idea is then to define estimators of the form @xmath195 , where @xmath196 takes values in the interval @xmath49 .",
    "we get the estimators @xmath197 and @xmath186 , respectively , in the limits as @xmath198 and @xmath199 .",
    "the problem is then to select a good shrinkage factor . in our case , even the simple choice of a constant factor @xmath200 already leads to improved estimates .",
    "we have chosen the factor that gives the best results for each estimator .",
    "the dashed lines in figure [ fig : momestgent ] show the value of @xmath58 for shrinkage versions of @xmath105 and @xmath201 . for the small sample ,",
    "only the shrunk ml improves on the zero estimator and only for @xmath202 . for the larger sample ,",
    "the best results are those of the shrunk @xmath105 and @xmath179 .",
    "both improve on @xmath186 for @xmath203 but ml leads to a much smaller mse .",
    "one should be able to get even better results with more sophisticated choices of @xmath196 .",
    "we have compared higher order moments to maximum likelihood ( ml ) estimators of @xmath0 and found that no estimator is better than the others over all values of @xmath51 .",
    "the selection of an appropriate estimator will depend on the relevant range of values of @xmath51 .",
    "for very small values , the fifth moment leads to the estimator with smallest mean square error ( mse ) , followed by the third moment and ml .",
    "but we have also shown that even for small @xmath0 , the fifth moment is not the best as all the estimators considered here improve with the introduction of a shrinkage factor . in practical applications ,",
    "the difference between the estimators will depend on the size of the sample and the covariance structure that determines its ` effective degrees of freedom ' .",
    "the selection of shrinkage factors will also depend on the particular covariance structure and sample size .",
    "we found that away from the origin the ml estimate has the smallest mse .",
    "however , while we have found a very fast decay of the mse with increasing sample size , the asymptotics considered here were for the case of independent observations .",
    "a correct asymptotic analysis should study the in - field asymptotics ; an excellent example with applications to cosmology is marinucci @xcite . for finite samples ,",
    "the performance of the estimators should be studied for the particular covariance matrices of the cosmological random fields under consideration .",
    "in addition , a more careful study should determine the robustness of the estimators against deviations from the local model coming from higher order terms .",
    "we have compared estimators mainly through the mse and its decrease with increasing sample size .",
    "we do not think that the cramer - rao ( cr ) inequality is the appropriate tool to select optimal estimators : matching the cr bound is not enough to guarantee a good estimator .",
    "for example , shrinkage estimators are not found by matching a cr bound .",
    "we are aware of a certain bias in favor of unbiased estimators in the community . however",
    ", all the estimators we have considered here are biased .",
    "i hope this note helps us remember that : unbiased estimators do not have to exist .",
    "when they exist , they may be difficult to find .",
    "if we find one , it may not be very good .",
    "+ * acknowledgments*. the author thanks the organizers of the workshop on non - gaussianity in cosmology ( trieste , july 2006 ) for an interesting and well organized meeting .",
    "he is also grateful to d. babich , p.a .",
    "martin , w. navidi and p.b .",
    "stark for helpful comments .     but using the relative measure of error @xmath184 ( nstead of mse ) for @xmath160 and @xmath161 .",
    "the dashed lines show ` shrunk ' versions of @xmath105 and @xmath204 as defined in the text .",
    "[ fig : momestgent],width=566 ]    10 , _ optimal estimation of non - gaussianity _ , phys .",
    "d , 72 ( 2005 ) , p.  043003 .    , _ mathematical statistics _ , 2nd edn .",
    "1 , new jersey : prentice hall , 2001 .    , _ asymptotic expansions of integrals _ , new york : dover , 1986 .    , _ statistical inference _ , 2nd edn , pacific grove , ca : duxbury press , 2002 .    , _ estimators for local non - gaussianities _ , astro - ph/0606001 .    , _ asymptotic theory of extreme order statistics _ , new york : wiley & sons , 1978 .    , _ theory of point estimation _ , 2nd edn . , new york : springer - verlag , 1998 .    , _ high resolution asymptotics for the angular bispectrum of spherical random fields _ , annals of statistics , 34 ( 2006 ) , pp .",
    "141 .    , _ a statistical perspective on ill - posed inverse problems _ , statistical science , 1 ( 1986 ) , pp",
    ".  502527 .    , _ statistical regularization of inverse problems _ , siam review , 43 ( 2001 ) , pp .  347366 .",
    "we assume that @xmath197 is a finite variance unbiased estimator of @xmath51 based on @xmath62 and that @xmath205 , as defined in , is an unbiased estimator of @xmath51 for any @xmath206 and reach a contradiction .",
    "this will show that there are no finite variance unbiased estimators of @xmath51 based on @xmath62 .          _",
    "proof : _ define @xmath214 the we can write @xmath215 clearly , the first factor decreases as @xmath125 increases , as does the second factor when multiplied by the decreasing exponential of the hyperbolic cosine .",
    "all we have left to show is that @xmath216 is also decreasing . to show this",
    ", it is enough to show that the following difference decreases @xmath217 but since @xmath218 , we have @xmath219 hence @xmath220 is also a decreasing function of @xmath125 and so is @xmath212 .",
    "finally , since the first exponential in the definition of @xmath212 has a negative exponent and @xmath221 as shown above , it follows that @xmath222 as @xmath223 .",
    "+      _ proof : _ since @xmath225 we show that the right hand side goes to zero as @xmath125 increases . by the last result , @xmath212 is a decreasing function of @xmath125 and therefore @xmath226 for any @xmath227 and @xmath228 .",
    "but the right hand side has a finite integral over @xmath229 because @xmath230 is a finite variance estimator .",
    "hence , by lebesgue s dominated convergence theorem @xmath231      _ proof : _ first note that for any @xmath234 and @xmath235 @xmath236 where @xmath237 .",
    "but since @xmath238 is bounded on @xmath239 , there are constants @xmath240 and @xmath241 such that @xmath242 for all @xmath235 and @xmath234 .",
    "it follows that @xmath243 since @xmath232 is bounded in a neighborhood of zero , there is a constant @xmath244 such that for @xmath125 large enough @xmath245 as @xmath223 .",
    "* result 3 ( b ) * if near the origin @xmath232 behaves as @xmath246 for some @xmath247 , then @xmath233 _ proof : _ for @xmath125 large enough we have the integral @xmath248 as @xmath223 for @xmath249 . +",
    "* remark : * the condition that @xmath197 behaves as @xmath246 near the origin is not very restrictive for the following reason .",
    "we have just shown that @xmath250 for any @xmath247 and since @xmath197 has finite variance , it follows from cauchy - schwarz inequality that @xmath251 it follows from this inequality that @xmath252 which already provides enough information to arrive at the same conclusion of result 3(b ) using a mellin transform analysis ( e.g. , bleistein & handelsman @xcite ) . +"
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating the parameter @xmath0 in the standard local model of primordial cmb non - gaussianity . </S>",
    "<S> we determine the properties of maximum likelihood ( ml ) estimates and show that the problem is not the typical ml estimation problem as there are subtle issues involved . in particular , the cramer - rao inequality is not applicable and the likelihood function is unbounded with several points of maxima . </S>",
    "<S> however , the particular characteristics of the likelihood function lead to ml estimates that are simple and easy to compute . we compare their performance to that of moment estimators . </S>",
    "<S> we find that ml is better than the latter for values @xmath0 away from the origin . for small values of @xmath0 , </S>",
    "<S> the fifth order moment is better than ml and the other moment estimators . </S>",
    "<S> however , we show how for small @xmath0 , one can easily improve the estimators by a simple shrinkage procedure . </S>",
    "<S> this is clearly important when the goal is to estimate a very small @xmath0 . in the process of studying the inference problem </S>",
    "<S> , we address some basic issues regarding statistical estimation in general that were raised at the workshop on non - gaussianity in cosmology held in trieste in july 2006 . </S>"
  ]
}