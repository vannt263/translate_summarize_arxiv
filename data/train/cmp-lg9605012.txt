{
  "article_text": [
    "lexical information has been shown to be crucial for many parsing decisions , such as prepositional - phrase attachment ( for example @xcite ) . however ,",
    "early approaches to probabilistic parsing @xcite conditioned probabilities on non - terminal labels and part of speech tags alone . the spatter parser @xcite does use lexical information , and recovers labeled constituents in wall street journal text with above 84% accuracy  as far as we know the best published results on this task .",
    "this paper describes a new parser which is much simpler than spatter , yet performs at least as well when trained and tested on the same wall street journal data .",
    "the method uses lexical information directly by modeling head - modifier relations between pairs of words . in this way it is similar",
    "to link grammars @xcite , and dependency grammars in general .",
    "the aim of a parser is to take a tagged sentence as input ( for example figure  [ fig - overview](a ) ) and produce a phrase - structure tree as output ( figure  [ fig - overview](b ) ) . a statistical approach to this problem consists of two components .",
    "first , the _ statistical model _ assigns a probability to every candidate parse tree for a sentence .",
    "formally , given a sentence @xmath0 and a tree @xmath1 , the model estimates the conditional probability @xmath2 .",
    "the most likely parse under the model is then :    @xmath3    ( a )    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ john / nnp smith / nnp , the / dt president / nn of / in ibm / nnp , announced / vbd his / prp$ resignation / nn yesterday / nn . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    ( b )    ( c )    ( d )    second , the _ parser _ is a method for finding @xmath4 .",
    "this section describes the statistical model , while section  [ sec - parser ] describes the parser .",
    "the key to the statistical model is that any tree such as figure  [ fig - overview](b ) can be represented as a set of * basenps * and a set of * dependencies * as in figure  [ fig - overview](c ) .",
    "we call the set of basenps @xmath5 , and the set of dependencies @xmath6 ; figure  [ fig - overview](d ) shows @xmath7 and @xmath8 for this example .",
    "for the purposes of our model , , and : @xmath9    @xmath0 is the sentence with words tagged for part of speech .",
    "that is , @xmath10 . for pos",
    "tagging we use a maximum - entropy tagger described in @xcite .",
    "the tagger performs at around 97% accuracy on wall street journal text , and is trained on the first 40,000 sentences of the penn treebank @xcite .",
    "given @xmath0 and @xmath7 , the * reduced sentence * @xmath11 is defined as the subsequence of @xmath0 which is formed by removing punctuation and reducing all basenps to their head - word alone .",
    "thus the reduced sentence is an array of word / tag pairs , , where @xmath12 .",
    "for example for figure  [ fig - overview](a )    @xmath13 [ sent2 ]    sections 2.1 to 2.4 describe the dependency model .",
    "section 2.5 then describes the basenp model , which uses bigram tagging techniques similar to @xcite .",
    "the dependency model is limited to relationships between words in * reduced * sentences such as example  [ sent2 ] .",
    "the mapping from trees to dependency structures is central to the dependency model .",
    "it is defined in two steps :    * 1 . * for each constituent in the parse tree a simple set of rules and @xmath7 to @xmath11 . ]",
    "identifies which of the children @xmath14 is the ` head - child ' of @xmath15 .",
    "for example , nn would be identified as the head - child of , vp would be identified as the head - child of .",
    "head - words propagate up through the tree , each parent receiving its head - word from its head - child .",
    "for example , in , s gets its head - word , @xmath16 , from its head - child , the vp .",
    "head - modifier relationships are now extracted from the tree in figure  [ fig - howhead ] .",
    "figure  [ fig - howdepend ] illustrates how each constituent contributes a set of dependency relationships .",
    "vbd is identified as the head - child of .",
    "the head - words of the two nps , @xmath17 and @xmath18 , both modify the head - word of the vbd , @xmath16 .",
    "dependencies are labeled by the modifier non - terminal , np in both of these cases , the parent non - terminal , vp , and finally the head - child non - terminal , vbd .",
    "the triple of non - terminals at the start , middle and end of the arrow specify the nature of the dependency relationship  @xmath19np , s , vp@xmath20 represents a subject - verb dependency , @xmath19pp , np , np@xmath20 denotes prepositional phrase modification of an np , and so on .",
    "each word in the reduced sentence , with the exception of the sentential head ` announced ' , modifies exactly one other word .",
    "we use the notation @xmath21 to state that the @xmath22th word in the reduced sentence is a modifier to the @xmath23th word , with relationship @xmath24=@xmath19label of the root of the parse tree @xmath20 .",
    "so in this case , @xmath25 . ] .",
    "@xmath26 stands for ` arrow from ' .",
    "@xmath24 is the triple of labels at the start , middle and end of the arrow .",
    "for example , @xmath27 in this sentence , and @xmath28 , so .",
    "@xmath8 is now defined as the m - tuple of dependencies : @xmath29 .",
    "the model assumes that the dependencies are independent , so that :    @xmath30      this section describes the way @xmath31 is estimated . the same sentence is very unlikely to appear both in training and test data , so we need to back - off from the entire sentence context .",
    "we believe that lexical information is crucial to attachment decisions , so it is natural to condition on the words and tags .",
    "let @xmath32 be the vocabulary of all words seen in training data , @xmath33 be the set of all part - of - speech tags , and @xmath34 be the training set , a set of reduced sentences .",
    "we define the following functions :    @xmath35 @xmath36 for @xmath37 , and @xmath38 is the number of times @xmath39 and @xmath40 are seen in the same reduced sentence in training data .",
    "formally , @xmath41=\\langle a , b \\rangle \\ , , \\ , \\bar{s}[l]=\\langle c , d \\rangle \\right)\\end{aligned}\\ ] ] where @xmath42 is an indicator function which is @xmath43 if @xmath44 is true , @xmath45 if @xmath44 is false .",
    "@xmath35 @xmath46 is the number of times @xmath39 and @xmath40 are seen in the same reduced sentence in training data , and @xmath39 modifies @xmath40 with relationship @xmath47 .",
    "formally , @xmath48=\\langle a , b \\rangle \\ , , \\ , \\bar{s}[l]=\\langle c , d \\rangle \\ , , \\,af(k)=(l , r ) ) \\nonumber\\\\ & & \\end{aligned}\\ ] ]    @xmath35 @xmath49 is the probability that @xmath39 modifies @xmath40 with relationship @xmath47 , given that @xmath39 and @xmath40 appear in the same reduced sentence .",
    "the maximum - likelihood estimate of @xmath49 is : @xmath50 we can now make the following approximation : @xmath51 where @xmath52 is the set of all triples of non - terminals .",
    "the denominator is a normalising factor which ensures that @xmath53 from  ( [ eq - pdsm ] )  and  ( [ eq - mods ] ) : @xmath54 the denominator of  ( [ eq - pdms2 ] ) is constant , so maximising @xmath55 over @xmath8 for fixed @xmath56 is equivalent to maximising the product of the numerators , @xmath57 .",
    "( this considerably simplifies the parsing process ) :    @xmath58      an estimate based on the identities of the two tokens alone is problematic .",
    "additional context , in particular the relative order of the two words and the distance between them , will also strongly influence the likelihood of one word modifying the other .",
    "for example consider the relationship between ` sales ' and the three tokens of ` of ' :    shaw , based in dalton , ga . , has annual * sales of * about $ 1.18 billion , and has economies * of * scale and lower raw - material costs that are expected to boost the profitability * of * armstrong s brands , sold under the armstrong and evans - black names .",
    "in this sentence ` sales ' and ` of ' co - occur three times .",
    "the parse tree in training data indicates a relationship in only one of these cases , so this sentence would contribute an estimate of @xmath59 that the two words are related .",
    "this seems unreasonably low given that ` sales of ' is a strong collocation .",
    "the latter two instances of ` of ' are so distant from ` sales ' that it is unlikely that there will be a dependency .",
    "this suggests that distance is a crucial variable when deciding whether two words are related .",
    "it is included in the model by defining an extra ` distance ' variable , @xmath60 , and extending @xmath61 , @xmath62 and @xmath63 to include this variable .",
    "for example , @xmath64 is the number of times @xmath39 and @xmath40 appear in the same sentence at a distance @xmath60 apart .",
    "( [ eq - finaln ] ) is then maximised instead of  ( [ eq - numer ] ) : @xmath65 a simple example of @xmath66 would be .",
    "however , other features of a sentence , such as punctuation , are also useful when deciding if two words are related .",
    "we have developed a heuristic ` distance ' measure which takes several such features into account the current distance measure @xmath66 is the combination of 6 features , or questions ( we motivate the choice of these questions qualitatively  section  [ sec - discussion ] gives quantitative results showing their merit ) :    * question 1 * does the @xmath67th word precede or follow the @xmath22th word ?",
    "english is a language with strong word order , so the order of the two words in surface text will clearly affect their dependency statistics .",
    "* question 2 * are the @xmath67th word and the @xmath22th word adjacent ?",
    "english is largely right - branching and head - initial , which leads to a large proportion of dependencies being between adjacent words .",
    "table  [ table - dist ] shows just how local most dependencies are .",
    ".percentage of dependencies vs. distance between the head words involved .",
    "these figures count basenps as a single word , and are taken from wsj training data .",
    "[ cols= \" < , > , > , > , > \" , ]",
    "we have shown that a simple statistical model based on dependencies between words can parse wall street journal news text with high accuracy .",
    "the method is equally applicable to tree or dependency representations of syntactic structures .",
    "there are many possibilities for improvement , which is encouraging .",
    "more sophisticated estimation techniques such as deleted interpolation should be tried .",
    "estimates based on relaxing the distance measure could also be used for smoothing  at present we only back - off on words .",
    "the distance measure could be extended to capture more context , such as other words or tags in the sentence .",
    "finally , the model makes no account of valency .",
    "i would like to thank mitch marcus , jason eisner , dan melamed and adwait ratnaparkhi for many useful discussions , and for comments on earlier versions of this paper .",
    "i would also like to thank david magerman for his help with testing spatter .",
    "f. jelinek , j. lafferty , d. magerman , r. mercer , a. ratnaparkhi , s. roukos .",
    "decision tree parsing using a hidden derivation model .",
    "_ proceedings of the 1994 human language technology workshop _ , pages 272 - 277 .",
    "j. lafferty , d. sleator and , d. temperley .",
    "grammatical trigrams : a probabilistic model of link grammar .",
    "_ proceedings of the 1992 aaai fall symposium on probabilistic approaches to natural language . _"
  ],
  "abstract_text": [
    "<S> this paper describes a new statistical parser which is based on probabilities of dependencies between head - words in the parse tree . </S>",
    "<S> standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words . </S>",
    "<S> tests using wall street journal data show that the method performs at least as well as spatter @xcite , which has the best published results for a statistical parser on this task . </S>",
    "<S> the simplicity of the approach means the model trains on 40,000 sentences in under 15 minutes . with a beam search strategy parsing speed </S>",
    "<S> can be improved to over 200 sentences a minute with negligible loss in accuracy . </S>"
  ]
}