{
  "article_text": [
    "support vector machines ( svms ) @xcite have been extensively used as linear classifiers either in the space where the patterns originally reside or in high dimensional feature spaces induced by kernels .",
    "they appear to be very successful at addressing the classification problem expressed as the minimization of an objective function involving the empirical risk while at the same time keeping low the complexity of the classifier . as measures of the empirical risk various quantities",
    "have been proposed with the 1- and 2-norm loss functions being the most widely accepted ones giving rise to the optimization problems known as l1- and l2-svms @xcite .",
    "svms typically treat the problem as a constrained quadratic optimization in the dual space . at the early stages of svm development",
    "their efficient implementation was hindered by the quadratic dependence of their memory requirements on the number of training examples a fact which rendered prohibitive the processing of large datasets .",
    "the idea of applying optimization only to a subset of the training set in order to overcome this difficulty resulted in the development of decomposition methods @xcite .",
    "although such methods led to improved convergence rates , in practice their superlinear dependence on the number of examples , which can be even cubic , can still lead to excessive runtimes when dealing with massive datasets .",
    "recently , the so - called linear svms @xcite taking advantage of linear kernels in order to allow parts of them to be written in primal notation succeeded in outperforming decomposition svms .    the above considerations motivated research in alternative algorithms naturally formulated in primal space long before the advent of linear svms mostly in connection with the large margin classification of linearly separable datasets a problem directly related to the l2-svm . indeed , in the case that the 2-norm loss takes the place of the empirical risk an equivalent formulation exists which renders the dataset linearly separable in a high dimensional feature space",
    "such alternative algorithms ( @xcite and references therein ) are mostly based on the perceptron @xcite , the simplest online learning algorithm for binary linear classification , with their key characteristic being that they work in the primal space in an online manner , i.e. , processing one example at a time .",
    "cycling repeatedly through the patterns they update their internal state stored in the weight vector each time an appropriate condition is satisfied . this way , due to their ability to process one example at a time , such algorithms succeed in sparing time and memory resources and consequently become able to handle large datasets .",
    "since the l1-svm problem is not known to admit an equivalent maximum margin interpretation via a mapping to an appropriate space fully primal large margin perceptron - like algorithms appear unable to deal with such a task .",
    "nevertheless , a somewhat different approach giving rise to online algorithms was developed which focuses on the minimization of the regularized 1-norm soft margin loss through stochastic gradient descent ( sgd ) .",
    "notable representatives of this approach are the pioneer norma @xcite ( see also @xcite ) and pegasos @xcite ( see also @xcite ) .",
    "sgd gives rise to a kind of perceptron - like update having as an important ingredient the  shrinking \" of the current weight vector .",
    "shrinking always takes place when a pattern is presented to the algorithm with it being the only modification suffered by the weight vector if no loss is incurred .",
    "thus , due to lack of a meaningful stopping criterion the algorithm without user intervention keeps running forever . in that sense the algorithms in question are fundamentally different from the mistake - driven large margin perceptron - like classifiers which terminate after a finite number of updates .",
    "there is no proof even for their asymptotic convergence when they use as output the final hypothesis but they do exist probabilistic convergence results or results in terms of the average hypothesis .    in the present work we reconsider the straightforward version of sgd for the primal unconstrained l1-svm problem assuming a learning rate inversely proportional to the number of steps .",
    "therefore , such an algorithm can be regarded either as norma with a specific dependence of the learning rate on the number of steps or as pegasos with no projection step in the update and with a single example contributing to the ( sub)gradient ( @xmath0 ) .",
    "we observe here that this algorithm may be transformed into a classical perceptron with margin @xcite in which the margin threshold increases linearly with the number of steps .",
    "the obvious gain from this observation is that the shrinking of the weight vector at each step amounts to nothing but an increase of the step counter by one unit instead of the costly multiplication of all the components of the generally non - sparse weight vector with a scalar .",
    "another benefit arising from the above simplified description is that we are able to demonstrate easily that if we cycle through the data in complete epochs the dual variables defined naturally via the expansion of the weight vector as a linear combination of the patterns on which margin errors were made satisfy automatically the box constraints of the dual optimization .",
    "an important consequence of this unexpected result is that the relevant dual lagrangian which is expressed in terms of the total number of margin errors , the number of complete epochs and the length of the current weight vector provides during the run a lower bound on the primal objective function and gives us a measure of the progress made in the optimization process . indeed , by virtue of the strong duality theorem the dual lagrangian and the primal objective coincide at optimality .",
    "therefore , assuming convergence to the optimum an upper bound on the relative accuracy involving the dual lagrangian may be defined which offers a useful and practically achievable stopping criterion .",
    "moreover , we may now provide evidence in favor of the asymptotic convergence to the optimum by testing experimentally the vanishing of the duality gap . finally , aiming at performing more updates at the expense of only one costly inner product calculation",
    "we propose a mechanism of presenting the same pattern repeatedly to the algorithm consistently with the above interesting properties .",
    "the paper is organized as follows .",
    "section 2 describes the algorithm and its properties . in section 3",
    "we give implementational details and deliver our experimental results .",
    "finally , section 4 contains our conclusions .",
    "assume we are given a training set @xmath1 , with vectors @xmath2 and labels @xmath3 .",
    "this set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality @xcite . by placing @xmath4 in the same position at a distance @xmath5 in an additional dimension , i.e. , by extending @xmath4 to @xmath6 $ ]",
    ", we construct an embedding of our data into the so - called augmented space @xcite .",
    "the advantage of this embedding is that the linear hypothesis in the augmented space becomes homogeneous . following the augmentation , a reflection with respect to the origin of the negatively labeled patterns",
    "is performed allowing for a uniform treatment of both categories of patterns .",
    "we define @xmath7 with @xmath8 $ ] the @xmath9-th augmented and reflected pattern .",
    "let us consider the regularized empirical risk @xmath10 involving the 1-norm soft margin loss @xmath11 for the pattern @xmath12 and the regularization parameter @xmath13 controlling the complexity of the classifier @xmath14 .",
    "for a given dataset of size @xmath15 minimization of the regularized empirical risk with respect to @xmath14 is equivalent to the minimization of the objective function @xmath16 where the `` penalty '' parameter @xmath17 is related to @xmath18 as @xmath19 this is the l1-svm problem expressed as an unconstrained optimization .",
    "the algorithms we are concerned with are classical sgd algorithms .",
    "the term stochastic refers to the fact that they perform gradient descent with respect to the objective function in which the empirical risk @xmath20 is approximated by the instantaneous risk @xmath21 on a single example .",
    "the general form of the update rule is then @xmath22 \\enspace,\\ ] ] where @xmath23 is the learning rate and @xmath24 stands for a subgradient with respect to @xmath25 since the 1-norm soft margin loss is only piecewise differentiable ( @xmath26 ) .",
    "we choose a learning rate @xmath27 which satisfies the conditions @xmath28 and @xmath29 usually imposed in the convergence analysis of stochastic approximations .",
    "then , noticing that @xmath30 , we obtain the update @xmath31 whenever @xmath32 and @xmath33 otherwise . in deriving the above update rule we made the choice @xmath34 for the subgradient at the point @xmath35 where the 1-norm soft margin loss is not differentiable .",
    "we assume that @xmath36 .",
    "we see that if @xmath37 the update consists of a pure shrinking of the current weight vector by the factor @xmath38 .",
    "the update rule may be simplified considerably if we perform the change of variable @xmath39 for @xmath40 and @xmath41 for @xmath42 . in terms of the new weight vector @xmath43",
    "the update rule becomes @xmath44 whenever @xmath45 and @xmath46 otherwise .",
    "( [ cond2 ] ) becomes @xmath47 instead of @xmath48 which is obtained from ( [ cond1 ] ) with @xmath49 .",
    "since both are satisfied with @xmath50 ( [ cond2 ] ) may be used for all @xmath51 . ]",
    "this is the update of the classical perceptron algorithm with margin in which , however , the margin threshold in condition ( [ cond2 ] ) increases linearly with the number of presentations of patterns to the algorithm independent of whether they lead to a change in the weight vector @xmath52 .",
    "thus , @xmath51 counts the number of times any pattern is presented to the algorithm which corresponds to the number of updates ( including the pure shrinkings ( [ upd2 ] ) ) of the weight vector @xmath25 .",
    "instead , the weight vector @xmath52 is updated only if ( [ cond2 ] ) is satisfied meaning that a margin error is made on @xmath53 .    in the original formulation of pegasos @xcite the update",
    "is completed with a projection step in order to enforce the bound @xmath54 which holds for the optimal solution .",
    "we show now that this is dynamically achieved to any desired accuracy after the elapse of sufficient time . in practice , however , it is in almost all cases achieved after one pass over the data .",
    "for @xmath40 the norm of the weight vector @xmath55 is bounded from above as follows @xmath56    l0.55    from the update rule ( [ upd3 ] ) taking into account condition ( [ cond2 ] ) under which the update takes place we get @xmath57 obviously , this is trivially satisfied if ( [ cond2 ] ) is violated and ( [ upd4 ] ) holds .",
    "a repeated application of the above inequality with @xmath50 gives @xmath58 from where using ( [ var ] ) and taking the square root we obtain ( [ bound ] ) .",
    "combining ( [ bound ] ) with the initial choice @xmath36 we see that for all @xmath51 the weaker bound @xmath59 previously derived in @xcite holds .",
    "sgd gives naturally rise to online algorithms .",
    "therefore , we may choose the examples to be presented to the algorithm at random . however , the l1-svm optimization task is a batch learning problem which may be better tackled by online algorithms via the classical conversion of such algorithms to the batch setting .",
    "this is done by cycling repeatedly through the possibly randomly permuted training dataset and using the last hypothesis for prediction .",
    "this traditional procedure of presenting the training data to the algorithm in complete epochs has in our case , as we will see shortly , the additional advantage that there exists a lower bound on the optimal value of the objective function to be minimized which is expressed in terms of quantities available during the run .",
    "the existence of such a lower bound provides an estimate of the relative accuracy achieved by the algorithm .",
    "[ prop2 ] let us assume that at some stage the whole training set has been presented to the algorithm exactly @xmath60 times .",
    "then , it holds that @xmath61 where @xmath62 is the total number of margin errors made up to that stage and @xmath63 the weight vector at @xmath64 with @xmath15 being the size of the training set .",
    "let @xmath65 denote the number of margin errors made on the pattern @xmath53 up to time @xmath51 such that @xmath66 .",
    "obviously , it holds that @xmath67 since @xmath53 up to time @xmath64 has been presented to the algorithm exactly @xmath60 times .",
    "then , taking into account ( [ var ] ) we see that at time @xmath51 the dual variable @xmath68 associated with @xmath53 is @xmath69 and consequently the dual variable @xmath70 after @xmath60 complete epochs is given by @xmath71 with use of ( [ dv1 ] ) we readily conclude that the dual variables after @xmath60 complete epochs automatically satisfy the box constraints @xmath72 from the weak duality theorem it follows that @xmath73 where @xmath74 is the dual lagrangian subject to the constraints @xmath75 is the dual of the primal l1-svm problem expressed as a constrained minimization . ] and the variables @xmath76 obey the box constraints @xmath75 .",
    "thus , setting @xmath77 in the above inequality , noticing that @xmath78 @xmath79 @xmath80 and substituting @xmath81 with @xmath82 we obtain @xmath83 which is equivalent to ( [ lbound ] ) .",
    "in the course of proving proposition [ prop2 ] we saw that although the algorithm is fully primal the dual variables @xmath68 defined through the expansion @xmath84 of the weight vector @xmath55 as a linear combination of the patterns on which margin errors were made obey after @xmath60 complete epochs automatically the box constraints ( [ box ] ) encountered in dual optimization .",
    "if the patterns presented to the algorithm are selected randomly with equal probability since asymptotically they will all be selected an equal number of times . ]",
    "this surprising result allows us to construct the dual lagrangian @xmath85 which provides a lower bound on the optimal value @xmath86 of the objective @xmath87 and assuming @xmath88 to obtain an upper bound @xmath89 on the relative accuracy @xmath90 achieved as the algorithm keeps running .",
    "thus , we have for the first time a primal sgd algorithm which may use the relative accuracy as stopping criterion . which is needed in order to decide whether condition ( [ cond2 ] ) is satisfied .",
    "if this approximate calculation gives a value of the relative accuracy which is not larger than @xmath91 times the one set as stopping criterion we proceed to a proper calculation of the primal objective .",
    "the comparison coefficient @xmath91 is given empirically a value close to 1 .",
    "] it is also worth noticing that @xmath85 involves only the total number @xmath62 of margin errors and does not require that we keep the values of the individual dual variables during the run .",
    "although the automatic satisfaction of the box constraints by the dual variables is very important it is by no means sufficient to ensure vanishing of the duality gap and consequently convergence to the optimal solution . to demonstrate convergence to the optimum relying on dual optimization theory",
    "we must make sure that the karush - kuhn - tucker ( kkt ) conditions @xcite are satisfied .",
    "their approximate satisfaction demands that the only patterns which have a substantial loss be the ones which have dual variables equal or at least extremely close to @xmath92 ( bound support vectors ) and moreover that the patterns which have zero loss and margin considerably larger than @xmath93 should have vanishingly small dual variables .",
    "patterns with margin very close to @xmath93 may    l0.55    have dual variables with values between 0 and @xmath92 and play the role of the non - bound support vectors . from ( [ dv2 ] )",
    "we see that the dual variable associated with the @xmath9-th pattern is equal to @xmath94 where @xmath95 is the number of epochs for which the @xmath9-th pattern was found to be a margin error .",
    "it is apparent that if there exists a number of epochs no matter how large it may be after which a pattern is consistently found to be a margin error then in the limit @xmath96 we will have @xmath97 and the dual variable associated with it will asymptotically approach @xmath92 .",
    "in contrast , if a pattern after a specific number of epochs is never found to be a margin error then @xmath98 and its dual variable will tend asymptotically to zero reflecting the accumulated effect of the shrinking that the weight vector suffers each time a pattern is presented to the algorithm .",
    "therefore , the algorithm has the necessary ingredients for asymptotic satisfaction of the kkt conditions for the vanishing of the duality gap .",
    "the potential danger remains , however , that they may exist patterns with margin not very close to @xmath93 which do not belong to any of the above categories and occasionally either become margin errors although most of the time are not or become classified with sufficiently large margin despite of the fact that they are most of the time margin errors .",
    "the hope is that with time the changes in the weight vector @xmath55 will become smaller and smaller and such events will become more and more rare leading eventually to convergence to the optimal solution .",
    "the above discussion can not be regarded as a formal proof of the asymptotic convergence of the algorithm .",
    "we believe , however , that it does provide a convincing argument that assuming convergence ( not necessarily to the optimum ) the duality gap will eventually tend to zero and the lower bound @xmath85 on the primal objective @xmath99 given in proposition [ prop2 ] will approach the optimal primal objective @xmath86 , thereby proving that convergence to the optimum has been achieved .",
    "if , instead , we make the stronger assumption of convergence to the optimum then , of course , the vanishing of the duality gap follows from the strong duality theorem . in any case the stopping criterion exploiting the upper bound @xmath89 on the relative accuracy @xmath90 is a meaningful one .",
    "our discussion so far assumes that in an epoch each pattern is presented only once to the algorithm .",
    "we may , however , consider the option of presenting the same pattern @xmath53 repeatedly @xmath100 times to the algorithm pure regularizer - induced @xmath55 shrinkings . ]",
    "aiming at performing more updates at the expense of only one calculation of the costly inner product @xmath101 .",
    "proposition [ prop2 ] and the analysis following it will still be valid on the condition that all patterns in each epoch are presented exactly the same number @xmath100 of times to the algorithm .",
    "then , such an epoch should be regarded as equivalent to @xmath100 usual epochs with single presentations of patterns to the algorithm and will have as a result the increase of @xmath51 by an amount equal to @xmath102 .",
    "it is , of course , important to be able to decide in terms of just the initial value of @xmath101 how many , let us say @xmath103 , out of these @xmath100 consecutive presentations of the pattern @xmath53 to the algorithm will lead to a margin error , i.e. , to an update of @xmath43 , with each of the remaining @xmath104 presentations necessarily corresponding to just an increase of @xmath51 by 1 which amounts to a pure shrinking of @xmath55 .",
    "[ propm ] let the pattern @xmath53 be presented at time @xmath51 repeatedly @xmath100 times to the algorithm .",
    "also let @xmath105 then , the number @xmath103 of times that @xmath53 will be found to be a margin error is given by the following formula @xmath106 + 1\\right \\ }   \\enspace.\\end{aligned}\\ ] ] here @xmath107 $ ] denotes the integer part of @xmath108 .    for the sake of brevity we call a plus - step a presentation of the pattern @xmath53 to the algorithm which leads to a margin error and a minus - step a presentation which does not .",
    "if at time @xmath51 a plus - step takes place @xmath109 while if a minus - step takes place @xmath110 .",
    "thus , a plus - step adds to @xmath111 the quantity @xmath112 while a minus - step the quantity @xmath113 . clearly , after @xmath100 consecutive presentations of @xmath53 to the algorithm it holds that @xmath114 .",
    "if @xmath115 it follows that @xmath116 which means that after @xmath117 consecutive minus - steps condition ( [ cond2 ] ) is still violated and an additional minus - step must take place .",
    "thus , @xmath118 and @xmath119 .    for @xmath120",
    "we first treat the subcase @xmath121 .",
    "if @xmath122 and @xmath123 condition ( [ cond2 ] ) is initially satisfied and will still be satisfied after any number of plus - steps since the quantity @xmath112 that is added to @xmath111 with a plus - step is non - positive .",
    "thus , @xmath124 .",
    "this is in accordance with ( [ mult ] ) since @xmath125 or @xmath126 + 1\\ge \\ell$ ] leading to @xmath124 .",
    "it remains for @xmath122 to consider @xmath111 in the interval @xmath127 which can be further subdivided as @xmath128 with the integer @xmath129 satisfying @xmath130 . for @xmath111",
    "belonging to such a subinterval condition ( [ cond2 ] ) is initially violated and will still be violated after @xmath131 minus - steps while after one more minus - step will be satisfied .",
    "it will still be satisfied after any number of additional plus - steps because the quantity @xmath112 that is added to @xmath111 with a plus - step is non - positive .",
    "thus , @xmath132 and @xmath133 .",
    "this is in accordance with ( [ mult ] ) since @xmath134 leads to @xmath135 + 1=\\ell-\\ell_1 $ ] .",
    "the subcase @xmath136 of the case @xmath120 is far more complicated .",
    "if @xmath136 with @xmath137 condition ( [ cond2 ] ) is initially satisfied and will still be satisfied after @xmath117 plus - steps since @xmath138 .",
    "thus , @xmath124 .",
    "this is consistent with ( [ mult ] ) because @xmath139 or @xmath140 + 1 \\ge \\ell$ ] leading to @xmath124 .",
    "it remains to be examined the case @xmath136 with @xmath111 in the interval @xmath141 .",
    "the above interval can be expressed as a union of subintervals @xmath142 with the integer @xmath129 satisfying @xmath130 .",
    "let @xmath111 belong to such a subinterval .",
    "let us also assume that the pattern @xmath12 has been presented @xmath143 consecutive times to the algorithm as a result of which @xmath144 plus - steps and @xmath145 minus - steps have taken place and the quantity @xmath146 has been added to @xmath111",
    ". then @xmath147 satisfies @xmath148 .",
    "as @xmath149 increases either @xmath144 will first reach the value @xmath129 with @xmath150 or @xmath145 will first reach the value @xmath151 with @xmath152 . in the former case @xmath153 .",
    "this means that condition ( [ cond2 ] ) is violated and will continue being violated until the number of minus - steps becomes equal to @xmath154 in which case one more minus - step must take place .",
    "thus , all steps taking place after @xmath144 has reached the value @xmath129 are minus - steps . in the latter case @xmath155 .",
    "this means that condition ( [ cond2 ] ) is satisfied and will continue being satisfied until the number of plus - steps becomes equal to @xmath131 in which case one more plus - step must take place .",
    "thus , all steps taking place after @xmath145 has reached the value @xmath151 are plus - steps . in both cases @xmath156 .",
    "this is again in accordance with ( [ mult ] ) because @xmath157 or @xmath140 + 1 = \\ell_1 $ ] .    with @xmath103 given in proposition [ propm ] the update of multiplicity @xmath100 of the weight vector @xmath43",
    "is written formally as @xmath158",
    "we implement three types of sgd algorithms along the lines of the previous section .",
    "the first is the plain algorithm with random selection of examples , denoted sgd - r , which terminates when the maximum number @xmath159 of steps is reached .",
    "its pseudocode is given in section 2 .",
    "the dual variables in this case do not satisfy the box constraints as a result of which relative accuracy can not be used as stopping criterion . the sgd algorithm with relative accuracy @xmath160 , the pseudocode of which is also given in section 2 , is denoted sgd - s where s designates that in an epoch each pattern is presented a single time to the algorithm .",
    "it terminates when the relative deviation of the primal objective @xmath87 from the dual lagrangian @xmath85 just falls below @xmath160 provided the maximum number @xmath161 of full epochs is not exhausted .",
    "a variation of this algorithm , denoted sgd - m , replaces in the @xmath60-th epoch the usual update with the multiple update ( [ updm ] ) of multiplicity @xmath162 only if @xmath163 .",
    "for both sgd - s and sgd - m the comparison coefficient takes the value @xmath164 unless otherwise explicitly stated .",
    "algorithms performing sgd on the primal objective are expected to perform better if linear kernels are employed .",
    "therefore the feature space in our experiments will be chosen to be the original instance space . as a consequence , our algorithms should most naturally be compared with linear svms . among them",
    "we choose @xmath165 @xcite , the first cutting - plane algorithm for training linear svms , the optimized cutting plane algorithm for svms ( ocas ) @xcite , the dual coordinate descent ( dcd ) algorithm @xcite and the margin perceptron with unlearning ( mpu ) @xcite .",
    "we also include in our study pegasos ( @xmath0 ) . finally , we briefly considered the svmsgd @xcite and sgd - qn @xcite algorithms implemented in single precision .",
    "the datasets we used for training are the binary adult and web datasets as compiled by platt , the training set of the kdd04 physics dataset ( with 70 attributes after removing the 8 columns containing missing features ) , the real - sim , news20 and webspam ( unigram treatment ) datasets , the multiclass covertype uci dataset and the full reuters rcv1 dataset .",
    "their number of instances and attributes are listed in table [ table1 ] . in the case of the covertype dataset",
    "we study the binary classification problem of the first class versus rest while for the rcv1 we consider both the binary text classification tasks of the c11 and ccat classes versus rest .",
    "the physics and covertype datasets were rescaled by multiplying all the features with 0.001 .",
    "the experiments were conducted on a 2.5 ghz intel core 2 duo processor with 3 gb ram running windows vista .",
    "the c++ codes were compiled using the g++ compiler under cygwin .",
    "first we perform an experiment aiming at demonstrating that our sgd algorithms are able to obtain extremely accurate solutions .",
    "more specifically , with the algorithm sgd - s employing single updating we attempt to diminish the gap between the primal objective @xmath87 and the dual lagrangian @xmath85 setting as a goal a relative deviation @xmath166 @xmath167 for @xmath168 . in the present and in all subsequent experiments",
    "we do not include a bias term in any of the algorithms ( i.e. , in our case we assign to the augmentation parameter the value @xmath169 ) . in order to keep the number @xmath60 of complete epochs as low as possible we increase the comparison coefficient @xmath91 until the number of epochs required gets stabilized .",
    "this procedure does not entail , of course , the shortest training time but this is not our concern in this experiment . in table",
    "[ table1 ] we give the values of both @xmath87 and @xmath85 and the number @xmath60 of epochs needed to achieve these values . if multiple updates are used a larger number of epochs is , in general , required due to the slower increase of @xmath85 .",
    "thus , sgd - s achieves , in general , relative accuracy closer to @xmath160 than sgd - m does .",
    "this is confirmed by subsequent experiments .            in our comparative experimental investigations",
    "we aim at achieving relative accuracy @xmath170 for various values of the penalty parameter @xmath92 assuming knowledge of the value of @xmath86 .",
    "for pegasos and sgd - r we use as stopping criterion the exhaustion of the maximum number of steps ( iterations ) @xmath159 which , however , is given values which are multiples of the dataset size @xmath15 .",
    "the ratio @xmath171 may be considered analogous to the number @xmath60 of epochs of the algorithm sgd - s since equal values of these two quantities indicate identical numbers of @xmath172 updates .",
    "the input parameter for sgd - s and sgd - m is the ( upper bound on ) the relative accuracy @xmath160 . for mpu",
    "we use the parameter @xmath173 , where @xmath174 is the before - run relative accuracy and @xmath175 the stopping threshold for the after - run relative accuracy . for @xmath165 and",
    "dcd we use as input their parameter @xmath160 while for ocas the primal objective value @xmath176 ( not given in the tables ) with the relative tolerance taking the default value @xmath177 .",
    "any difference in training time between pegasos and sgd - r for equal values of @xmath171 should be attributed to the difference in the implementations .",
    "any difference between @xmath171 for sgd - r and @xmath60 for sgd - s is to be attributed to the different procedure of choosing the patterns that are presented to the algorithm .",
    "finally , the difference in the number @xmath60 of epochs between sgd - s and sgd - m reflects the effect of multiple updates .",
    "it should be noted that in the runtime of sgd - s and sgd - m several calculations of the primal and the dual objective are included which are required for checking the satisfaction of the stopping criterion .",
    "if sgd - s and sgd - m were using the exhaustion of the maximum number @xmath161 of epochs as stopping criterion their runtimes would certainly be shorter .",
    "tables [ table2 ] and [ table3 ] contain the results of the experiments involving the sgd algorithms and linear svms for @xmath178 .",
    "we observe that , in general , there is a progressive decrease in training time as we move from pegasos to sgd - m through sgd - r and sgd - s due to the additive effect of several factors .",
    "these factors are the more efficient implementation of our algorithms exploiting the change of variable given by ( [ var ] ) , the presentation of the patterns to sgd - s and sgd - m in complete epochs ( see also @xcite ) and the use by sgd - m of multiple updating . the overall improvement made by sgd - m over pegasos is quite substantial .",
    "dcd and mpu are certainly statistically faster but their differences from sgd - m are not very large especially for the largest datasets",
    ". moreover , sgd - s and sgd - m are considerably faster than @xmath165 and statistically faster than ocas .",
    "pegasos failed to process the covertype dataset due to numerical problems .",
    "tables [ table4 ] and [ table5 ] contain the results of the experiments involving the sgd algorithms and linear svms for @xmath179 .",
    "although the general characteristics resemble the ones of the previous case the differences are magnified due to the intensity of the optimization task .",
    "certainly , the training time of linear svms scales much better as @xmath92 increases .",
    "moreover , mpu clearly outperforms dcd and ocas for most datasets .",
    "sgd - m is still statistically faster than @xmath165 but slower than ocas . finally , pegasos runs more often into numerical problems .",
    "in contrast , as @xmath92 decreases the differences among the algorithms are alleviated .",
    "this is apparent from the results for @xmath180 reported in tables [ table6 ] and [ table7 ] .",
    "sgd - r , sgd - s and sgd - m all appear statistically faster than the linear svms .",
    "also pegasos outperforms @xmath165 for all datasets and ocas for the majority of them .",
    "seemingly , lowering @xmath92 favors the sgd algorithms .        before concluding our experimental investigation we also consider the sgd algorithms svmsgd and sgd - qn both implemented in single precision . for a fair comparison we implemented the algorithms sgd - m and mpu in single precision as well .",
    "svmsgd and sgd - qn do not perform random permutations of the dataset but rather assume that it has already been shuffled .",
    "thus , we provided them with the dataset produced by sgd - m as a result of the first random permutation of the dataset given .",
    "the fact that the computational cost of the random permutation is not included in the runtime of svmsgd and sgd - qn gives these algorithms a certain advantage which becomes more crucial for tasks requiring short runtimes . for this reason we did not consider values of the penalty parameter @xmath92 much smaller than 1",
    ". table [ table8 ] contains the results of the experiments involving the algorithms svmsgd , sgd - qn , sgd - m and mpu for @xmath178 .",
    "we observe that statistically svmsgd is the slowest and mpu the fastest .",
    "moreover , sgd - m statistically outperforms sgd - qn with a preference for sparse multidimentional datasets . in the case of",
    "the news20 dataset sgd - qn failed to reach the required relative accuracy .",
    "we reexamined the classical sgd approach to the primal unconstrained l1-svm optimization task and made some contributions concerning both theoretical and practical issues . assuming a learning rate inversely proportional to the number of steps a simple change of variable allowed us to simplify the algorithmic description and",
    "demonstrate that in a scheme presenting the patterns to the algorithm in complete epochs the naturally defined dual variables satisfy automatically the box constraints of the dual optimization .",
    "this opened the way to obtaining an estimate of the progress made in the optimization process and enabled the adoption of a meaningful stopping criterion , something the sgd algorithms were lacking .",
    "moreover , it made possible a qualitative discussion of how the kkt conditions will be asymptotically satisfied provided the weight vector @xmath55 gets stabilized . besides , we showed that in the limit @xmath181 even without a projection step in the update it holds that @xmath54 , a bound known to be obeyed by the optimal solution . on the more practical side by exploiting our simplified algorithmic description and employing a mechanism of multiple updating we succeeded in substantially improving the performance of sgd algorithms .",
    "for optimization tasks of low or medium intensity the algorithms constructed are comparable to or even faster than the state - of - the - art linear svms ."
  ],
  "abstract_text": [
    "<S> we reconsider the stochastic ( sub)gradient approach to the unconstrained primal l1-svm optimization . we observe that if the learning rate is inversely proportional to the number of steps , i.e. , the number of times any training pattern is presented to the algorithm </S>",
    "<S> , the update rule may be transformed into the one of the classical perceptron with margin in which the margin threshold increases linearly with the number of steps . </S>",
    "<S> moreover , if we cycle repeatedly through the possibly randomly permuted training set the dual variables defined naturally via the expansion of the weight vector as a linear combination of the patterns on which margin errors were made are shown to obey at the end of each complete cycle automatically the box constraints arising in dual optimization . </S>",
    "<S> this renders the dual lagrangian a running lower bound on the primal objective tending to it at the optimum and makes available an upper bound on the relative accuracy achieved which provides a meaningful stopping criterion . </S>",
    "<S> in addition , we propose a mechanism of presenting the same pattern repeatedly to the algorithm which maintains the above properties . finally , we give experimental evidence that algorithms constructed along these lines exhibit a considerably improved performance . </S>"
  ]
}