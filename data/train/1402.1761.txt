{
  "article_text": [
    "the dependence of network performance on the number of nodes in the network is an emerging area of interest , as there is an increasing desire to expand wireless networks to larger sizes .",
    "rendering the scalability of networks more difficult is the widespread advent of imagery and streaming video , which means the users must transmit or receive larger volumes of data .",
    "the question of scalability was first considered over ten years ago in the context of sensor networks .",
    "the question was raised as to whether performance could be improved by increasing the density of the sensors or motes . addressing this question ,",
    "the work @xcite showed how the capacity scales as the number of nodes is increased within a fixed area two dimensional area . later works",
    "@xcite consider fixed node density _ or _ fixed network area .",
    "most of the previous works in the literature , while yielding precise results given their assumptions , do not include direct consideration of two crucial factors that in practice often dominate the network scaling laws : bottlenecks and erasures . in this article",
    ", we show how bottlenecks and erasures can dominate the scaling behavior in many networks , and thereby yield _ different _ scaling laws than those derived previously .",
    "we also discuss potential mitigation techniques for bottlenecks and erasures .",
    "we provide an overview of representative works in the scaling literature by using intuitive arguments to explain the mathematical results .",
    "our goal is to provide a didactic perspective on scaling which highlights the issues of its practical application .    in section  [ bottleneck ] , scaling with a range of bottleneck types is illustrated .",
    "potential methods to circumvent bottlenecks are discussed in section  [ no_bottleneck ] . the impact on scaling of protocols designed to handle erasures",
    "is discussed in section  [ erasures ] .",
    "some previous scaling results , as well as new results presented here , can be viewed as a consequence of simple bottlenecks .",
    "such bottlenecks can arise at most nodes as in figure  [ unicast_fig]a , a group of nodes as in figure  [ unicast_fig]b , or at designated cluster heads as in figure  [ unicast_fig]c . scaling laws for each of these types of bottlenecks are found by determining how many flows must share a bottleneck node .      in this section , multiple unicast transmissions in a multi - hop network",
    "are considered , when there are no packet losses nor mobility , as in @xcite .",
    "we use a simple heuristic argument to illustrate the results of @xcite , and then extend it to three dimensions .",
    "we first consider a network which consists of @xmath0 nodes , each of which has a message to send to another one of the @xmath0 nodes , all of which lie in a fixed two - dimensional circular area . when @xmath0 is large , as @xmath0 increases , the diameter of the region grows as @xmath1 . in order to minimize interference to other nodes",
    ", each node relays its message across a path of hops between neighboring nodes towards its destination , as illustrated in figure  [ unicast_fig]a .",
    "we consider the case that in order for most messages to reach their destinations , they must traverse a number of hops that scales as the diameter , or @xmath1 .",
    "various deterministic and random node placements and source - destination pair selection processes , such as a uniform distribution of nodes and node pairings , could yield this scaling with the diameter .",
    "since each of the @xmath0 messages must be transmitted by approximately @xmath1 relay nodes , a total of about @xmath2 relaying transmissions must be made for the messages to reach their destinations .",
    "the case with minimum congestion would have these relaying transmissions distributed uniformly across the @xmath0 nodes , so that each node performs about @xmath1 transmissions , one for each message it must relay .",
    "figure  [ unicast_fig]a illustrates the bottleneck that forms at one example relay node , as multiple messages pass through it . in the best case ,",
    "each node can relay one of the approximately @xmath1 messages in each unit of time .",
    "more generally , a node may not be able to successfully relay a message to its next hop in each time unit , owing to interference .",
    "therefore , each flow receives , at best , a capacity that scales as @xmath3 , as in @xcite .",
    "[ h ! ]    in a three dimensional network most source - destination paths scale as @xmath4 hops in length .",
    "hence the total number of relaying transmissions for all @xmath0 messages would grow as @xmath5 . in the best case , these transmissions are uniformly distributed among the @xmath0 nodes , so that each node would need to relay about @xmath4 messages , yielding a per flow capacity that decreases at least as fast as @xmath6 . therefore , capacity as a function of the number of nodes in a fixed three dimensional region is increased relative to that in a fixed two dimensional region .",
    "henceforth , in this article we consider two - dimensional space .",
    "the above analysis assumes the total link capacity does not change as the hop length changes .",
    "the same scaling behavior found here also holds if the density of nodes is held fixed , and @xmath0 is instead increased by extending the area of the network .",
    "we now discuss scaling as a function of the number of transmitting nodes .",
    "first , we consider the multipoint - to - point case , as is common when many nodes must communicate with a single base station or access point , for example . it is assumed that @xmath0 nodes each send a message to a single common destination node .",
    "hence , the destination is a bottleneck at reception , which can allocate at most @xmath7 of its capacity to each of the @xmath0 messages it is sent .",
    "we next consider multipoint - to - multipoint connections in which @xmath0 transmitting nodes each send a message to a common multicast group _ of any size _ up to the broadcast size of @xmath0 . in this case , each node in the common multicast group needs to receive @xmath0 messages .",
    "hence , each receiving node can allocate at most @xmath7 of its capacity to each transmitted message .",
    "these results can be extended to the case in which the @xmath0 transmitting nodes send messages to different multicast groups : the approximate @xmath7 capacity scaling holds for communication to any receiving node that is a destination for some fixed fraction of the @xmath0 transmitting nodes .    in the cases discussed in this section , the _ receiving nodes _ are the bottlenecks .",
    "these results hold regardless of mobility , topology , or reception capabilities .",
    "we consider networks which have the following common topology trait : somewhere in the network there is a bottleneck consisting of @xmath8 nodes , where @xmath8 is some constant independent of @xmath0 , and this bottleneck is located such that some fraction @xmath9 , also independent of @xmath0 , of the messages must flow through it .",
    "an example of such a topology is shown in figure  [ unicast_fig]b .",
    "each bottleneck node is thus shared by @xmath10 flows .",
    "therefore , each flow receives a fraction of the capacity that decreases as @xmath7 .",
    "this result does not depend on connection type , mobility , nor the precise topology .",
    "clustering and hierarchical structures are common in military networks , and questions arise as to whether these structures can improve scalability .",
    "we first consider communication among nodes , each of which is assigned to a cluster .",
    "for the broadcast case , the argument focusing on the @xmath0 messages each receiving node must process , and the resulting scaling law , are the same as in section  [ transmit ] .",
    "we next turn to unicast traffic .",
    "all inter - cluster communication must transit through cluster leaders , where a single leader is assigned in each cluster .",
    "we allow for the formation of @xmath11 clusters , each of which would consist of @xmath12 nodes , as illustrated in figure  [ unicast_fig]c , where @xmath13 .    we assume some fraction @xmath9 , independent of @xmath0 , of traffic is inter - cluster communication .",
    "these @xmath10 messages must be relayed through the cluster leaders , which are indicated as red nodes in figure  [ unicast_fig]c .",
    "typical inter - cluster path lengths for these inter - cluster messages scale as the diameter , or about @xmath14 cluster leader hops .",
    "the situation for clustering is analogous to figure  [ unicast_fig]a , except that the flows that went through each node in figure  [ unicast_fig]a are now constrained to traverse only the cluster leaders in figure  [ unicast_fig]c .",
    "therefore , the cluster leaders are now the bottlenecks .",
    "the scaling argument for cluster leaders is analogous to that in section  [ fixed ] :    the @xmath10 inter - cluster messages must collectively be relayed a total of @xmath15 hops . in the best case ,",
    "this relaying is uniformly distributed among the @xmath11 cluster leaders , resulting in each cluster leader needing to relay about @xmath16 messages .",
    "therefore , in the best case , the inter - cluster traffic is allocated a fraction of the cluster leader s link capacity that scales as @xmath17 .",
    "this capacity is seen to be maximized for @xmath18 , which corresponds to clusters of size 1 node ; in this case , the capacity obtained in the best case scales as @xmath19 , which is the same as that discussed in section  [ fixed ] .",
    "forming clusters of a fixed size that is independent of @xmath0 will also yield the @xmath18 result .",
    "if larger clusters are formed by increasing @xmath20 for a given @xmath0 , the capacity is _ decreased _ , according to the scaling law @xmath17 .",
    "forming clusters of size that increases with @xmath0 means fewer cluster leaders must carry the same total @xmath10 flows , and hence each cluster leader would then have more flows among which it must divide its capacity .",
    "the case @xmath21 occurs when all @xmath0 nodes form a single cluster , in which case the scaling of capacity above reduces to @xmath7 , which is the same scaling found in sections  [ transmit ] and [ bottle ] . in those sections",
    ", the single cluster leader corresponds to the base station or bottleneck node respectively .",
    "thus we have shown that the bottlenecks discussed in sections  [ fixed ] and [ transmit ] plus [ bottle ] can be viewed as two extreme limiting cases of clustering for clusters of size 1 and @xmath0 respectively .",
    "given @xmath0 , the length of hops between cluster leaders increases as @xmath20 increases ; hence , in power limited networks , the _ total _ per link capacity decreases as @xmath20 increases .",
    "therefore , in a power limited network , the total capacity per inter - cluster message will actually decrease even more rapidly with @xmath20 than @xmath17 .",
    "we now shift focus to a multi - layer hierarchical topology which consists of @xmath0 nodes that communicate with a single base station in the hierarchy .",
    "regardless of how the nodes are clustered in each layer , or how the inter - layer flows are regulated , capacity is still limited by the bottleneck of the base station needing to receive @xmath0 flows , as described in section  [ transmit ] .",
    "therefore , the per flow capacity scales as @xmath7 , or worse , if the total per link capacity depends on @xmath0 .    in summary , imposing clustering or hierarchical structure , does not improve the scaling laws , and actually _ decreases _ the capacity in point - to - point communications when cluster size grows with @xmath0 , owing to the resulting bottlenecks formed .",
    "we explore the potential of some methods that appeared recently in the literature to increase scalability . in particular",
    ", we consider whether these methods can avoid the bottlenecks discussed in section  [ bottleneck ] in practical networks .",
    "first , we discuss mobility as a means to allow nodes to circumvent bottlenecks .",
    "the work @xcite considered the case in which each node moves independently according to a stationary ergodic process . in this model",
    ", each node has a potential opportunity to transmit its packets to another nearby node at each unit in time ; figure  [ mobility_fig]a illustrates transmission for one source node to one intermediate node . _",
    "each _ such intermediate node can henceforth serve as a potential relay node .",
    "as the nodes move , there will be many such potential relay nodes for each source node , and hence it is likely that at least one of the potential relay nodes will _ eventually _ become close to the destination node .",
    "figure  [ mobility_fig]b illustrates a later point in time when one relay node comes within range of a destination node .",
    "each node thus sends and receives @xmath22 traffic per unit time , as shown in @xcite ; thus capacity scales with network size .",
    "however , the packets arrive at their destinations in a random order",
    ". moreover , the _ delay _ grows with the number of nodes , rendering such a scheme impractical in most large networks .",
    "[ h ! ]",
    "recent works in the literature , such as @xcite and @xcite , suggest that hierarchical networks can improve scaling by cooperation . in @xcite , for example , hierarchical cooperation is achieved by long distance mimo transmissions across the network .",
    "the sending and receiving mimo arrays are virtual antenna arrays , each consisting of a cluster of a large number of nodes .",
    "we consider the implementability of these large scale virtual mimo networks , as presented in @xcite .",
    "first , note that in order to achieve the mimo capacities , the channel must include multiple propagation paths .",
    "most importantly , attaining the mimo capacity is dependent on receivers knowledge of the channel .",
    "there are @xmath23 channel conditions that most be transmitted periodically . however , with the mimo setup , there are only @xmath24 parallel simultaneous communications that can occur .",
    "hence , if the channel update rate is independent of @xmath0 , the bandwidth required for these updates will grow with @xmath0 .",
    "this issue is not addressed in @xcite , as static channels are assumed there .",
    "we next consider additional assumptions of @xcite .",
    "before a long distance mimo transmission commences , each node must distribute data to all other nodes in its cluster .",
    "this stage is illustrated for one source cluster in figure  [ mimo_fig]a .",
    "it is assumed @xcite that many source clusters are performing these initial distributions in parallel , based on the assumption @xcite of spectrum segregation among clusters .",
    "we note that as node density increases , the assumption of spectrum segregation of clusters that are within range of one another requires a corresponding increase in bandwidth ; at a high enough node density , the required bandwidth can become untenable .",
    "[ h ! ]    after the long distance mimo transmission , which is illustrated in figure  [ mimo_fig]b , the nodes cooperate to decode , as depicted in figure  [ mimo_fig]c .",
    "the assumption @xcite of scale invariance is used twice in this decoding stage : first , it is assumed that the combined signal each node receives for each destination node in its cluster can be quantized to a number of bits that is independent of @xmath0 .",
    "we note that when @xmath0 is increased by extending the area of the network , thereby extending the distances of the mimo transmissions in figure  [ mimo_fig]b , the snr received at each node in the receive clusters decreases . in this case ,",
    "quantization with insufficient granularity would cause distortion . secondly ,",
    "even with sufficiently fine scaled quantization , the resulting low snr from mimo transmission of figure  [ mimo_fig]b in extended networks can itself hamper the decoding of figure  [ mimo_fig]c .",
    "the implications described above of the spectrum segregation and scale invariance assumptions in the dense and extended network types are summarized in table 1 .",
    "it is seen that as the number of nodes becomes large , each of these assumptions complicates implementation of a large scale mimo scheme for a different network type .",
    "hence , in many practical scenarios without infinite bandwidth or infinite power , scaling laws with clustering would instead follow those derived in section  [ cluster ]",
    ".    .indication of whether each assumption used in a large hierarchical mimo scheme , such as in @xcite , can be implemented without very large power or bandwidth in dense or extended networks , as the number of nodes grows very large . [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "the issue of low snr discussed in the last section leads more generally to the question of the impact of erasures on scalability .",
    "this question was not addressed in earlier sections here , nor in much of the literature .",
    "we now discuss how various erasure recovery protocols affect scalability .",
    "furthermore , we illustrate the cost of adding more receiving nodes to multicast transmissions , when erasures are considered .",
    "end - to - end acknowledgements are used in protocols such as tcp .",
    "if the erasure probability on each link is @xmath25 , then the probability a packet reaches its destination at the end of the path is given by @xmath26 , since a packet must not be lost at each of the @xmath27 hops , where @xmath28 is a constant independent of @xmath0 .",
    "the decrease in probability of a message reaching successively distant hops is illustrated in figure  [ endtoend ] .",
    "therefore , since @xmath29 , each packet s probability of reaching its destination approaches zero ! the throughput per flow decreases exponentially as @xmath30 .",
    "thus in this case , the scaling is dominated by the erasures only being corrected on an end - to - end basis , and the throughput decreases much more rapidly than the model used in section  [ fixed ] with no erasures .",
    "we have assumed @xmath25 is independent of @xmath0 , which is always the case in an extended network , as more nodes are added in that case by increasing the area .",
    "this assumption also holds in a dense network , if the decrease in the nodes transmit powers necessitated to avoid the interference of the added nodes is matched to yield the original erasure probability .",
    "[ h ! ]",
    "we now consider the case in which acknowledgments are sent on a hop - by - hop basis .",
    "assuming an erasure probability @xmath25 on each link , the throughput is reduced by @xmath31 ; hence , it scales as @xmath32 .",
    "this result obeys the same scaling law as the case of no erasures .",
    "scaling of multicast message delivery time with the number of receiving nodes can be dominated by the recovery of erasures .",
    "for best effort delivery , there is no penalty to adding any number of receiving nodes , as there is no erasure recovery .",
    "in contrast , for reliable delivery , additional packets must be sent when there are erasures , until _ every node _ receives all packets .",
    "we consider a multicast group of @xmath0 nodes that must receive a file from a single transmitting node .    in @xcite an efficient method using a predictive model and coding",
    "are used for multicast delivery , so that erasure recovery is accomplished by the preemptive transmission of the expected number of coded packets needed to complete transmission to all recipients with high probability .",
    "any given coded packet can be used by different recipients to recover from different erased packets . with this method",
    ", the total completion time for all nodes to receive a file depends only very weakly on @xmath0 @xcite .",
    "for example , in order to obtain a @xmath33 probability that @xmath34 users all receive a file of length 100 time slots , when the channel s packet erasure probability is @xmath35 , a total of @xmath36 time slots would be required @xcite .",
    "if the same file were sent over the same channel , but the number of recipients were increased from @xmath37 to @xmath38 , then the total time needed to complete delivery to all nodes with @xmath33 probability would increase to only @xmath39 time slots @xcite .",
    "it is further shown @xcite that for all practical values of @xmath0 , the dependence on file size , rather than dependence on the number of users , dominates the behavior of completion time .",
    "hence , it is important when focusing on scaling with the number of users to also consider if other factors may actually contribute more to the metric of interest",
    ".    the average per packet completion time for reception by all users decreases with increasing file size with efficient multicast transmissions @xcite .",
    "future research could leverage this favorable scaling with file size to artificially construct larger files for more efficient channel use .",
    "for example , we now depart from the case of a single source to consider the case in which @xmath40 transmitting nodes have files to send to a multicast group .",
    "if transmitting node @xmath41 has a block of size @xmath42 , a total of @xmath40 transmitting nodes can collectively create a block of size @xmath43 .",
    "each of the @xmath44 nodes that transmits after the first node transmits would code together the packets it receives from the preceding transmitting nodes with its own source packets so as to synthesize a larger block of size @xmath45 .",
    "the cost of using this method to decrease the mean per packet completion time to the entire group is an increase in delay for the some of the @xmath0 nodes to completely receive the blocks from the earlier transmitting nodes .",
    "this synthesis of a large block size through collaboration has the additional advantage of allowing the first transmitting nodes to cease retransmissions , once another transmitting node has received their information .",
    "furthermore , it can assist in an extended network so that closer nodes can serve to distribute the data of distant nodes , and can thereby also conserve battery power .",
    "detailed exploration of the benefits of such a scheme are left as future work .",
    "we have provided a simple overview of some representative studies on scaling . we have emphasized the import of bottlenecks and erasures , to show that scaling laws depend significantly on the protocols and assumptions used .",
    "we have illustrated how some of the scaling laws derived in the literature will be difficult to implement in practice , owing to their idealized assumptions .",
    "furthermore , we have shown that under a range of practical scenarios that include multicast traffic , topological bottlenecks , and hierarchical structures , the per node capacity scales as @xmath7 .",
    "a number of additional scaling issues not discussed here are a subject for future investigation . while this article analyzed scalability in terms of the data",
    ", there still remains the issue of scalability of network control messages .",
    "in addition , we note that the bottlenecks discussed here also contribute to a potential scaling problem of storage or memory at the bottleneck nodes , as well as associated queueing delays ."
  ],
  "abstract_text": [
    "<S> _ _ an intuitive overview of the scalability of a variety of types of wireless networks is presented . </S>",
    "<S> simple heuristic arguments are demonstrated here for scaling laws presented in other works , as well as for conditions not previously considered in the literature . </S>",
    "<S> unicast and multicast messages , topology , hierarchy , and effects of reliability protocols are discussed . </S>",
    "<S> we show how two key factors , bottlenecks and erasures , can often dominate the network scaling behavior . </S>",
    "<S> scaling of throughput or delay with the number of transmitting nodes , the number of receiving nodes , and the file size is described . </S>"
  ]
}