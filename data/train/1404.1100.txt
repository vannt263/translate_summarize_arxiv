{
  "article_text": [
    "principal component analysis ( pca ) is a standard tool in modern data analysis - in diverse fields from neuroscience to computer graphics - because it is a simple , non - parametric method for extracting relevant information from confusing data sets . with minimal effort pca",
    "provides a roadmap for how to reduce a complex data set to a lower dimension to reveal the sometimes hidden , simplified structures that often underlie it .",
    "the goal of this tutorial is to provide both an intuitive feel for pca , and a thorough discussion of this topic .",
    "we will begin with a simple example and provide an intuitive explanation of the goal of pca .",
    "we will continue by adding mathematical rigor to place it within the framework of linear algebra to provide an explicit solution .",
    "we will see how and why pca is intimately related to the mathematical technique of singular value decomposition ( svd ) .",
    "this understanding will lead us to a prescription for how to apply pca in the real world and an appreciation for the underlying assumptions .",
    "my hope is that a thorough understanding of pca provides a foundation for approaching the fields of machine learning and dimensional reduction .",
    "the discussion and explanations in this paper are informal in the spirit of a tutorial .",
    "the goal of this paper is to _ educate_. occasionally , rigorous mathematical proofs are necessary although relegated to the appendix . although not as vital to the tutorial , the proofs are presented for the adventurous reader who desires a more complete understanding of the math .",
    "my only assumption is that the reader has a working knowledge of linear algebra .",
    "my goal is to provide a thorough discussion by largely building on ideas from linear algebra and avoiding challenging topics in statistics and optimization theory ( but see discussion )",
    ". please feel free to contact me with any suggestions , corrections or comments .",
    "here is the perspective : we are an experimenter .",
    "we are trying to understand some phenomenon by measuring various quantities ( e.g. spectra , voltages , velocities , etc . ) in our system .",
    "unfortunately , we can not figure out what is happening because the data appears clouded , unclear and even redundant .",
    "this is not a trivial problem , but rather a fundamental obstacle in empirical science .",
    "examples abound from complex systems such as neuroscience , web indexing , meteorology and oceanography - the number of variables to measure can be unwieldy and at times even _ deceptive _ , because the underlying relationships can often be quite simple .",
    "take for example a simple toy problem from physics diagrammed in figure  [ diagram : toy ] .",
    "pretend we are studying the motion of the physicist s ideal spring .",
    "this system consists of a ball of mass @xmath0 attached to a massless , frictionless spring .",
    "the ball is released a small distance away from equilibrium ( i.e. the spring is stretched ) .",
    "because the spring is ideal , it oscillates indefinitely along the @xmath1-axis about its equilibrium at a set frequency .",
    "this is a standard problem in physics in which the motion along the @xmath1 direction is solved by an explicit function of time . in other words ,",
    "the underlying dynamics can be expressed as a function of a single variable @xmath1 .    however , being ignorant experimenters we do not know any of this .",
    "we do not know which , let alone how many , axes and dimensions are important to measure .",
    "thus , we decide to measure the ball s position in a three - dimensional space ( since we live in a three dimensional world ) .",
    "specifically , we place three movie cameras around our system of interest . at each movie camera",
    "records an image indicating a two dimensional position of the ball ( a projection ) .",
    "unfortunately , because of our ignorance , we do not even know what are the real @xmath1 , @xmath2 and @xmath3 axes , so we choose three camera positions @xmath4 and @xmath5 at some arbitrary angles with respect to the system .",
    "the angles between our measurements might not even be @xmath6 ! now , we record with the cameras for several minutes .",
    "the big question remains : _ how do we get from this data set to a simple equation of @xmath1 ? _    we know a - priori that if we were smart experimenters , we would have just measured the position along the @xmath1-axis with one camera .",
    "but this is not what happens in the real world .",
    "we often do not know which measurements best reflect the dynamics of our system in question .",
    "furthermore , we sometimes record more dimensions than we actually need .",
    "also , we have to deal with that pesky , real - world problem of noise . in the toy example",
    "this means that we need to deal with air , imperfect cameras or even friction in a less - than - ideal spring .",
    "noise contaminates our data set only serving to obfuscate the dynamics further .",
    "_ this toy example is the challenge experimenters face everyday . _",
    "keep this example in mind as we delve further into abstract concepts .",
    "hopefully , by the end of this paper we will have a good understanding of how to systematically extract @xmath1 using principal component analysis .",
    "the goal of principal component analysis is to identify the most meaningful basis to re - express a data set .",
    "the hope is that this new basis will filter out the noise and reveal hidden structure . in the example of the spring",
    ", the explicit goal of pca is to determine : `` the dynamics are along the @xmath1-axis . '' in other words , the goal of pca is to determine that @xmath7 , i.e. the unit basis vector along the @xmath1-axis , is the important dimension .",
    "determining this fact allows an experimenter to discern which dynamics are important , redundant or noise .      with a more precise definition of our goal , we need a more precise definition of our data as well .",
    "we treat every time sample ( or experimental trial ) as an individual sample in our data set . at each time sample",
    "we record a set of data consisting of multiple measurements ( e.g. voltage , position , etc . ) . in our data",
    "set , at one point in time , camera _ a _ records a corresponding ball position @xmath8 .",
    "one sample or trial can then be expressed as a 6 dimensional column vector @xmath9\\ ] ] where each camera contributes a 2-dimensional projection of the ball s position to the entire vector @xmath10 .",
    "if we record the ball s position for 10 minutes at 120 hz , then we have recorded @xmath11 of these vectors .    with this concrete example ,",
    "let us recast this problem in abstract terms .",
    "each sample @xmath10 is an @xmath0-dimensional vector , where @xmath0 is the number of measurement types .",
    "equivalently , every sample is a vector that lies in an @xmath0-dimensional vector space spanned by some orthonormal basis . from linear algebra",
    "we know that all measurement vectors form a linear combination of this set of unit length basis vectors . what is this orthonormal basis ?",
    "this question is usually a tacit assumption often overlooked .",
    "pretend we gathered our toy example data above , but only looked at camera @xmath12 .",
    "what is an orthonormal basis for @xmath13 ? a naive choice would be @xmath14 , but why select this basis over @xmath15 or any other arbitrary rotation ?",
    "the reason is that the _ naive basis reflects the method we gathered the data .",
    "_ pretend we record the position @xmath16 .",
    "we did not record @xmath17 in the @xmath18 direction and @xmath19 in the perpendicular direction . rather , we recorded the position @xmath16 on our camera meaning 2 units up and 2 units to the left in our camera window .",
    "thus our original basis reflects the method we measured our data .",
    "how do we express this naive basis in linear algebra ?",
    "in the two dimensional case , @xmath14 can be recast as individual row vectors .",
    "a matrix constructed out of these row vectors is the @xmath20 identity matrix @xmath21 .",
    "we can generalize this to the @xmath0-dimensional case by constructing an @xmath22 identity matrix @xmath23 =   \\left [ \\begin{array}{cccc } 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\\\ \\end{array } \\right ] = \\mathbf{i}\\ ] ] where each _ row _ is an orthornormal basis vector @xmath24 with @xmath0 components .",
    "we can consider our naive basis as the effective starting point .",
    "all of our data has been recorded in this basis and thus it can be trivially expressed as a linear combination of @xmath25 .      with this rigor we may now state more precisely what pca asks : _ is there another basis , which is a linear combination of the original basis , that best re - expresses our data set ?",
    "_    a close reader might have noticed the conspicuous addition of the word _",
    "linear_. indeed , pca makes one stringent but powerful assumption : linearity .",
    "linearity vastly simplifies the problem by restricting the set of potential bases . with this assumption pca",
    "is now limited to re - expressing the data as a _ linear combination _ of its basis vectors .",
    "let @xmath26 be the original data set , where each @xmath27 is a single sample ( or moment in time ) of our data set ( i.e. @xmath10 ) .",
    "in the toy example @xmath26 is an @xmath28 matrix where @xmath29 and @xmath30 .",
    "let @xmath31 be another @xmath32 matrix related by a linear transformation @xmath33 .",
    "@xmath26 is the original recorded data set and @xmath31 is a new representation of that data set . @xmath34 also let us define the following quantities .",
    "and @xmath35 are _ column _ vectors , but be forewarned . in all other sections",
    "@xmath36 and @xmath35 are _ row _ vectors . ]",
    "* @xmath37 are the rows of @xmath33 * @xmath36 are the columns of @xmath26 ( or individual @xmath38 .",
    "* @xmath35 are the columns of @xmath31 .",
    "equation  [ eqn : basis - transform ] represents a change of basis and thus can have many interpretations .    1 .",
    "@xmath33 is a matrix that transforms @xmath26 into @xmath31 .",
    "2 .   geometrically , @xmath33 is a rotation and a stretch which again transforms @xmath26 into @xmath31 .",
    "the rows of @xmath33 , @xmath39 , are a set of new basis vectors for expressing the columns of @xmath26 .",
    "the latter interpretation is not obvious but can be seen by writing out the explicit dot products of @xmath40 .",
    "@xmath41 \\left [ \\begin{array}{ccc } \\mathbf{x_1 } & \\cdots & \\mathbf{x_n } \\\\ \\end{array } \\right ] \\\\",
    "\\mathbf{y }   & = & \\left [ \\begin{array}{ccc } \\mathbf{p_1 \\cdot x_1 } & \\cdots & \\mathbf{p_1 \\cdot x_n } \\\\",
    "\\vdots & \\ddots & \\vdots \\\\",
    "\\mathbf{p_m \\cdot x_1 } & \\cdots & \\mathbf{p_m \\cdot x_n } \\\\",
    "\\end{array } \\right ] \\\\",
    "\\end{aligned}\\ ] ] we can note the form of each column of @xmath31 .",
    "@xmath42\\ ] ] we recognize that each coefficient of @xmath35 is a dot - product of @xmath36 with the corresponding row in @xmath33 . in other words , the @xmath43 coefficient of @xmath35 is a projection on to the @xmath43 row of @xmath33 .",
    "this is in fact the very form of an equation where @xmath35 is a projection on to the basis of @xmath39 .",
    "therefore , the rows of @xmath33 are a new set of basis vectors for representing of columns of @xmath26 .      by assuming linearity the problem reduces to finding",
    "the appropriate _ change of basis_. the row vectors @xmath39 in this transformation will become the _ principal components _ of @xmath26 .",
    "several questions now arise .    * what is the best way to re - express @xmath26 ? * what is a good choice of basis @xmath33 ?    these questions must be answered by next asking ourselves _",
    "what features we would like @xmath31 to exhibit_. evidently , additional assumptions beyond linearity are required to arrive at a reasonable result .",
    "the selection of these assumptions is the subject of the next section .",
    "now comes the most important question : what does _ best express _ the data mean ?",
    "this section will build up an intuitive answer to this question and along the way tack on additional assumptions .       for camera",
    "_ a_. the signal and noise variances @xmath44 and @xmath45 are graphically represented by the two lines subtending the cloud of data .",
    "note that the largest direction of variance does not lie along the basis of the recording @xmath13 but rather along the best - fit line.,scaledwidth=25.0% ]    measurement noise in any data set must be low or else , no matter the analysis technique , no information about a signal can be extracted .",
    "there exists no absolute scale for noise but rather all noise is quantified relative to the signal strength .",
    "a common measure is the _ signal - to - noise ratio _ ( _ snr _ ) , or a ratio of variances @xmath46 , @xmath47 a high snr ( @xmath48 ) indicates a high precision measurement , while a low snr indicates very noisy data .",
    "let s take a closer examination of the data from camera _ a _ in figure  [ fig : snr ] .",
    "remembering that the spring travels in a straight line , every individual camera should record motion in a straight line as well .",
    "therefore , any spread deviating from straight - line motion is noise .",
    "the variance due to the signal and noise are indicated by each line in the diagram .",
    "the ratio of the two lengths measures how skinny the cloud is : possibilities include a thin line ( snr @xmath48 ) , a circle ( snr @xmath49 ) or even worse . by positing reasonably good measurements",
    ", quantitatively we assume that directions with largest variances in our measurement space contain the dynamics of interest . in figure",
    "[ fig : snr ] the direction with the largest variance is not @xmath50 nor @xmath51 , but the direction along the long axis of the cloud .",
    "thus , by assumption the dynamics of interest exist along directions with largest variance and presumably highest snr .",
    "our assumption suggests that the basis for which we are searching is not the naive basis because these directions ( i.e. @xmath13 ) do not correspond to the directions of largest variance . maximizing the variance ( and by assumption the snr )",
    "corresponds to finding the appropriate rotation of the naive basis .",
    "this intuition corresponds to finding the direction indicated by the line @xmath52 in figure  [ fig : snr ] . in the 2-dimensional case of figure  [ fig : snr ]",
    "the direction of largest variance corresponds to the best - fit line for the data cloud .",
    "thus , rotating the naive basis to lie parallel to the best - fit line would reveal the direction of motion of the spring for the 2-d case .",
    "how do we generalize this notion to an arbitrary number of dimensions ?",
    "before we approach this question we need to examine this issue from a second perspective .       and @xmath53",
    ". the two measurements on the left are uncorrelated because one can not predict one from the other .",
    "conversely , the two measurements on the right are highly correlated indicating highly redundant measurements.,scaledwidth=47.0% ]    figure  [ fig : snr ] hints at an additional confounding factor in our data - redundancy .",
    "this issue is particularly evident in the example of the spring . in this case",
    "multiple sensors record the same dynamic information .",
    "reexamine figure  [ fig : snr ] and ask whether it was really necessary to record 2 variables . figure  [ fig : redundancy ]",
    "might reflect a range of possibile plots between two arbitrary measurement types @xmath54 and @xmath53 .",
    "the left - hand panel depicts two recordings with no apparent relationship . because one can not predict @xmath54 from @xmath53",
    ", one says that @xmath54 and @xmath53 are uncorrelated .    on the other extreme",
    ", the right - hand panel of figure  [ fig : redundancy ] depicts highly correlated recordings .",
    "this extremity might be achieved by several means :    * a plot of @xmath55 if cameras _ a _ and _ b _ are very nearby . * a plot of @xmath56 where @xmath57 is in meters and @xmath58 is in inches .",
    "clearly in the right panel of figure  [ fig : redundancy ] it would be more meaningful to just have recorded a single variable , not both .",
    "why ? because one can calculate @xmath54 from @xmath53 ( or vice versa ) using the best - fit line .",
    "recording solely one response would express the data more concisely and reduce the number of sensor recordings ( @xmath59 variables ) .",
    "indeed , this is the central idea behind dimensional reduction .      in a 2 variable case",
    "it is simple to identify redundant cases by finding the slope of the best - fit line and judging the quality of the fit .",
    "how do we quantify and generalize these notions to arbitrarily higher dimensions ?",
    "consider two sets of measurements with zero means @xmath60 where the subscript denotes the sample number .",
    "the variance of @xmath12 and @xmath61 are individually defined as , @xmath62 the _ covariance _ between @xmath12 and @xmath61 is a straight - forward generalization .",
    "@xmath63 the covariance measures the degree of the linear relationship between two variables .",
    "a large positive value indicates positively correlated data .",
    "likewise , a large negative value denotes negatively correlated data .",
    "the absolute magnitude of the covariance measures the degree of redundancy .",
    "some additional facts about the covariance .",
    "* @xmath64 is zero if and only if @xmath12 and @xmath61 are uncorrelated ( e.g. figure  [ fig : snr ] , left panel ) . *",
    "@xmath65 if @xmath66 .",
    "we can equivalently convert @xmath12 and @xmath61 into corresponding row vectors . @xmath67 \\\\ \\mathbf{b } & = & \\left[b_1\\;b_2\\;\\ldots\\;b_n\\right]\\end{aligned}\\ ] ] so that we may express the covariance as a dot product matrix computation .",
    "is calculated as @xmath68 .",
    "the slight change in normalization constant arises from estimation theory , but that is beyond the scope of this tutorial . ]",
    "@xmath69    finally , we can generalize from two vectors to an arbitrary number .",
    "rename the row vectors @xmath70 and @xmath71 as @xmath72 and @xmath73 , respectively , and consider additional indexed row vectors @xmath74 .",
    "define a new @xmath32 matrix @xmath26 .",
    "@xmath75\\ ] ] one interpretation of @xmath26 is the following . each _",
    "row _ of @xmath26 corresponds to all measurements of a particular type .",
    "column _ of @xmath26 corresponds to a set of measurements from one particular trial ( this is @xmath10 from section 3.1 ) .",
    "we now arrive at a definition for the _ covariance matrix _ @xmath76 .",
    "@xmath77 consider the matrix @xmath78 .",
    "the @xmath79 element of @xmath76 is the dot product between the vector of the @xmath80 measurement type with the vector of the @xmath43 measurement type .",
    "we can summarize several properties of @xmath76 :    * @xmath76 is a square symmetric @xmath81 matrix ( theorem 2 of appendix a ) * the diagonal terms of @xmath76 are the _ variance _ of particular measurement types . * the off - diagonal terms of @xmath76 are the _ covariance _ between measurement types .",
    "@xmath76 captures the covariance between all possible pairs of measurements .",
    "the covariance values reflect the noise and redundancy in our measurements .",
    "* in the diagonal terms , by assumption , large values correspond to interesting structure . * in the off - diagonal terms large magnitudes correspond to high redundancy .",
    "pretend we have the option of manipulating @xmath76 .",
    "we will suggestively define our manipulated covariance matrix @xmath82 .",
    "what features do we want to optimize in @xmath82 ?",
    "we can summarize the last two sections by stating that our goals are ( 1 ) to minimize redundancy , measured by the magnitude of the covariance , and ( 2 ) maximize the signal , measured by the variance .",
    "what would the optimized covariance matrix @xmath82 look like ?",
    "* all off - diagonal terms in @xmath82 should be zero .",
    "thus , @xmath82 must be a diagonal matrix . or",
    ", said another way , @xmath31 is decorrelated . * each successive dimension in @xmath31",
    "should be rank - ordered according to variance .",
    "there are many methods for diagonalizing @xmath82 .",
    "it is curious to note that pca arguably selects the easiest method : pca assumes that all basis vectors @xmath39 are orthonormal , i.e. @xmath33 is an _",
    "orthonormal matrix_. why is this assumption easiest ?    envision how pca works . in our simple example in figure  [ fig : snr ] , @xmath33 acts as a generalized rotation to align a basis with the axis of maximal variance . in multiple dimensions",
    "this could be performed by a simple algorithm :    1 .",
    "select a normalized direction in @xmath0-dimensional space along which the variance in @xmath26 is maximized . save this vector as @xmath83 .",
    "2 .   find another direction along which variance is maximized , however , because of the orthonormality condition , restrict the search to all directions orthogonal to all previous selected directions .",
    "save this vector as @xmath37 3 .",
    "repeat this procedure until @xmath0 vectors are selected .",
    "the resulting ordered set of @xmath84 s are the _",
    "principal components_.    in principle this simple algorithm works , however that would bely the true reason why the orthonormality assumption is judicious .",
    "the true benefit to this assumption is that there exists an efficient , analytical solution to this problem .",
    "we will discuss two solutions in the following sections .",
    "notice what we gained with the stipulation of rank - ordered variance .",
    "we have a method for judging the importance of the principal direction .",
    "namely , the variances associated with each direction @xmath37 quantify how `` principal '' each direction is by rank - ordering each basis vector @xmath37 according to the corresponding variances.we will now pause to review the implications of all the assumptions made to arrive at this mathematical goal .",
    "this section provides a summary of the assumptions behind pca and hint at when these assumptions might perform poorly .",
    "_ linearity _ + linearity frames the problem as a change of basis .",
    "several areas of research have explored how extending these notions to nonlinear regimes ( see discussion ) .",
    "_ large variances have important structure . _",
    "+ this assumption also encompasses the belief that the data has a high snr .",
    "hence , principal components with larger associated variances represent interesting structure , while those with lower variances represent noise . note that this is a strong , and sometimes , incorrect assumption ( see discussion ) .    _",
    "the principal components are orthogonal . _",
    "+ this assumption provides an intuitive simplification that makes pca soluble with linear algebra decomposition techniques .",
    "these techniques are highlighted in the two following sections .",
    "we have discussed all aspects of deriving pca - what remain are the linear algebra solutions .",
    "the first solution is somewhat straightforward while the second solution involves understanding an important algebraic decomposition .",
    "we derive our first algebraic solution to pca based on an important property of eigenvector decomposition .",
    "once again , the data set is @xmath26 , an @xmath32 matrix , where @xmath0 is the number of measurement types and @xmath85 is the number of samples .",
    "the goal is summarized as follows .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ find some orthonormal matrix @xmath33 in such that is a diagonal matrix .",
    "the rows of @xmath33 are the _ principal components _ of @xmath26 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    we begin by rewriting @xmath82 in terms of the unknown variable .",
    "@xmath86 note that we have identified the covariance matrix of @xmath26 in the last line .",
    "our plan is to recognize that any symmetric matrix @xmath87 is diagonalized by an orthogonal matrix of its eigenvectors ( by theorems 3 and 4 from appendix a ) .",
    "for a symmetric matrix @xmath87 theorem 4 provides @xmath88 , where @xmath89 is a diagonal matrix and @xmath90 is a matrix of eigenvectors of @xmath87 arranged as columns .",
    "might have @xmath91 orthonormal eigenvectors where @xmath92 is the rank of the matrix .",
    "when the rank of @xmath87 is less than @xmath0 , @xmath87 is _ degenerate _ or all data occupy a subspace of dimension @xmath91 . maintaining the constraint of orthogonality",
    ", we can remedy this situation by selecting @xmath93 additional orthonormal vectors to `` fill up '' the matrix @xmath90 .",
    "these additional vectors do not effect the final solution because the variances associated with these directions are zero . ]",
    "now comes the trick .",
    "_ we select the matrix @xmath33 to be a matrix where each row @xmath37 is an eigenvector of @xmath94 .",
    "_ by this selection , @xmath95 . with this relation and theorem 1 of appendix a ( @xmath96 )",
    "we can finish evaluating @xmath82 .",
    "@xmath97 it is evident that the choice of @xmath33 diagonalizes @xmath82 .",
    "this was the goal for pca .",
    "we can summarize the results of pca in the matrices @xmath33 and @xmath82 .",
    "* the principal components of @xmath26 are the eigenvectors of @xmath98 . *",
    "the @xmath80 diagonal value of @xmath82 is the variance of @xmath26 along @xmath37 .    in practice computing pca of a data",
    "set @xmath26 entails ( 1 ) subtracting off the mean of each measurement type and ( 2 ) computing the eigenvectors of @xmath76 .",
    "this solution is demonstrated in matlab code included in appendix b.",
    "this section is the most mathematically involved and can be skipped without much loss of continuity .",
    "it is presented solely for completeness .",
    "we derive another algebraic solution for pca and in the process , find that pca is closely related to singular value decomposition ( svd ) .",
    "in fact , the two are so intimately related that the names are often used interchangeably .",
    "what we will see though is that svd is a more general method of understanding _ change of basis_.    we begin by quickly deriving the decomposition . in the following section we interpret the decomposition and in the last section we relate these results to _",
    "let @xmath26 be an arbitrary @xmath99 matrix to @xmath99 .",
    "the reason for this derivation will become clear in section 6.3 . ] and @xmath100 be a rank @xmath92 , square , symmetric @xmath81 matrix . in a seemingly unmotivated fashion , let us define all of the quantities of interest .    * @xmath101 is the set of _ orthonormal _ @xmath102 eigenvectors with associated eigenvalues @xmath103 for the symmetric matrix @xmath100 . @xmath104",
    "* @xmath105 are positive real and termed the _",
    "singular values_. * @xmath106 is the set of @xmath107 vectors defined by .",
    "the final definition includes two new and unexpected properties .",
    "* @xmath108i = j@xmath109 *     these properties are both proven in theorem 5 .",
    "we now have all of the pieces to construct the decomposition .",
    "the scalar version of singular value decomposition is just a restatement of the third definition .",
    "@xmath110 this result says a quite a bit .",
    "@xmath26 multiplied by an eigenvector of @xmath111 is equal to a scalar times another vector .",
    "the set of eigenvectors and the set of vectors are both orthonormal sets or bases in @xmath92-dimensional space",
    ".    we can summarize this result for all vectors in one matrix multiplication by following the prescribed construction in figure  [ diagram : svd - construction ] .",
    "we start by constructing a new diagonal matrix @xmath112 .",
    "@xmath113\\ ] ] where are the rank - ordered set of singular values .",
    "likewise we construct accompanying orthogonal matrices , @xmath114 \\\\ \\mathbf{u } & = & \\left[\\mathbf{\\hat{u}_{\\tilde{1}}}\\;\\mathbf{\\hat{u}_{\\tilde{2}}}\\;\\ldots\\;\\mathbf{\\hat{u}_{\\tilde{n } } } \\right ] \\end{aligned}\\ ] ] where we have appended an additional @xmath93 and @xmath115 orthonormal vectors to `` fill up '' the matrices for @xmath116 and @xmath117 respectively ( i.e. to deal with degeneracy issues ) .",
    "figure  [ diagram : svd - construction ] provides a graphical representation of how all of the pieces fit together to form the matrix version of _",
    "@xmath118 where each column of @xmath116 and @xmath117 perform the scalar version of the decomposition ( equation  [ eqn : value - svd ] ) . because @xmath116 is orthogonal",
    ", we can multiply both sides by @xmath119 to arrive at the final form of the decomposition .",
    "@xmath120 although derived without motivation , this decomposition is quite powerful .",
    "equation  [ eqn : svd - matrix ] states that _ any _ arbitrary matrix @xmath26 can be converted to an orthogonal matrix , a diagonal matrix and another orthogonal matrix ( or a rotation , a stretch and a second rotation ) . making sense of equation  [ eqn : svd - matrix ]",
    "is the subject of the next section .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the scalar form of svd is expressed in equation  [ eqn : value - svd ] . @xmath121",
    "the mathematical intuition behind the construction of the matrix form is that we want to express all @xmath85 scalar equations in just one equation .",
    "it is easiest to understand this process graphically .",
    "drawing the matrices of equation  [ eqn : value - svd ] looks likes the following .",
    "we can construct three new matrices @xmath116 , @xmath117 and @xmath112 .",
    "all singular values are first rank - ordered , and the corresponding vectors are indexed in the same rank order . each pair of associated vectors @xmath122 and @xmath123 is stacked in the @xmath80 column along their respective matrices .",
    "the corresponding singular value @xmath124 is placed along the diagonal ( the @xmath125 position ) of @xmath112 .",
    "this generates the equation @xmath126 , which looks like the following .",
    "the matrices @xmath116 and @xmath117 are @xmath81 and @xmath127 matrices respectively and @xmath112 is a diagonal matrix with a few non - zero values ( represented by the checkerboard ) along its diagonal .",
    "solving this single matrix equation solves all @xmath85 `` value '' form equations .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _      the final form of svd is a concise but thick statement . instead let us reinterpret equation  [ eqn : value - svd ] as @xmath128 where @xmath70 and @xmath129 are column vectors and",
    "@xmath130 is a scalar constant .",
    "the set is analogous to @xmath70 and the set is analogous to @xmath129 .",
    "what is unique though is that and are orthonormal sets of vectors which _ span _ an @xmath0 or @xmath85 dimensional space , respectively . in particular ,",
    "loosely speaking these sets appear to span all possible `` inputs '' ( i.e. @xmath70 ) and `` outputs '' ( i.e. @xmath129 ) .",
    "can we formalize the view that and span all possible `` inputs '' and `` outputs '' ?",
    "we can manipulate equation  [ eqn : svd - matrix ] to make this fuzzy hypothesis more precise .",
    "@xmath131 where we have defined @xmath132 .",
    "note that the previous columns are now rows in @xmath133 . comparing this equation to equation  [ eqn : basis - transform ] ,",
    "perform the same role as .",
    "hence , @xmath133 is a _ change of basis _ from @xmath26 to @xmath134 . just as before",
    ", we were transforming column vectors , we can again infer that we are transforming column vectors .",
    "the fact that the orthonormal basis @xmath133 ( or @xmath33 ) transforms column vectors means that @xmath133 is a basis that spans the columns of @xmath26 .",
    "bases that span the columns are termed the _ column space _ of @xmath26 .",
    "the column space formalizes the notion of what are the possible `` outputs '' of any matrix .    there is a funny symmetry to svd such that we can define a similar quantity - the _ row space_. @xmath135 where we have defined @xmath136 .",
    "again the rows of @xmath137 ( or the columns of @xmath116 ) are an orthonormal basis for transforming @xmath138 into @xmath134 . because of the transpose on @xmath26",
    ", it follows that @xmath116 is an orthonormal basis spanning the _ row space _ of @xmath26 .",
    "the row space likewise formalizes the notion of what are possible `` inputs '' into an arbitrary matrix .    we are only scratching the surface for understanding the full implications of svd . for the purposes of this tutorial",
    "though , we have enough information to understand how pca will fall within this framework .",
    "it is evident that pca and svd are intimately related .",
    "let us return to the original @xmath32 data matrix @xmath26 .",
    "we can define a new matrix @xmath31 as an @xmath99 matrix .",
    "is of the appropriate @xmath99 dimensions laid out in the derivation of section 6.1 .",
    "this is the reason for the `` flipping '' of dimensions in 6.1 and figure 4 . ]",
    "@xmath139 where each _ column _ of @xmath31 has zero mean .",
    "the choice of @xmath31 becomes clear by analyzing @xmath140 .",
    "@xmath141 by construction @xmath140 equals the covariance matrix of @xmath26 . from section 5",
    "we know that the principal components of @xmath26 are the eigenvectors of @xmath76 .",
    "if we calculate the svd of @xmath31 , the columns of matrix @xmath116 contain the eigenvectors of @xmath142 .",
    "_ therefore , the columns of @xmath116 are the principal components of @xmath26_. this second algorithm is encapsulated in matlab code included in appendix b.    what does this mean ? @xmath116 spans the row space of @xmath143 .",
    "therefore , @xmath116 must also span the column space of @xmath144 .",
    "we can conclude that finding the principal components amounts to finding an orthonormal basis that spans the _ column space _ of @xmath26 .",
    "then we can calculate it directly without constructing @xmath31 . by symmetry the columns of @xmath117 produced by the svd of @xmath144",
    "must also be the principal components . ]",
    "principal component analysis ( pca ) has widespread applications because it reveals simple underlying structures in complex data sets using analytical solutions from linear algebra .",
    "figure  [ fig : summary ] provides a brief summary for implementing pca .",
    "a primary benefit of pca arises from quantifying the importance of each dimension for describing the variability of a data set .",
    "in particular , the measurement of the variance along each principle component provides a means for comparing the relative importance of each dimension .",
    "an implicit hope behind employing this method is that the variance along a small number of principal components ( i.e. less than the number of measurement types ) provides a reasonable characterization of the complete data set .",
    "this statement is the precise intuition behind any method of _ dimensional reduction _  a vast arena of active research . in the example of the spring",
    ", pca identifies that a majority of variation exists along a single dimension ( the direction of motion @xmath7 ) , eventhough 6 dimensions are recorded .",
    "although pca `` works '' on a multitude of real world problems , any diligent scientist or engineer must ask _ when does pca fail ?",
    "_ before we answer this question , let us note a remarkable feature of this algorithm .",
    "pca is completely _ non - parametric _ : any data set can be plugged in and an answer comes out , requiring no parameters to tweak and no regard for how the data was recorded . from one perspective",
    ", the fact that pca is non - parametric ( or plug - and - play ) can be considered a positive feature because the answer is unique and independent of the user . from another perspective",
    "the fact that pca is agnostic to the source of the data is also a weakness .",
    "for instance , consider tracking a person on a ferris wheel in figure  [ fig : failures]a .",
    "the data points can be cleanly described by a single variable , the precession angle of the wheel @xmath145 , however pca would fail to recover this variable .",
    ", a non - linear combination of the naive basis . ( b ) in this example data set , non - gaussian distributed data and non - orthogonal axes causes pca to fail .",
    "the axes with the largest variance do not correspond to the appropriate answer.,scaledwidth=47.0% ]    a deeper appreciation of the limits of pca requires some consideration about the underlying assumptions and in tandem , a more rigorous description of the source of data . generally speaking",
    ", the primary motivation behind this method is to decorrelate the data set , i.e. remove second - order dependencies .",
    "the manner of approaching this goal is loosely akin to how one might explore a town in the western united states : drive down the longest road running through the town .",
    "when one sees another big road , turn left or right and drive down this road , and so forth . in this analogy , pca requires that each new road explored must be perpendicular to the previous , but clearly this requirement is overly stringent and the data ( or town ) might be arranged along non - orthogonal axes , such as figure  [ fig : failures]b .",
    "figure  [ fig : failures ] provides two examples of this type of data where pca provides unsatisfying results .",
    "to address these problems , we must define what we consider optimal results . in the context of dimensional reduction ,",
    "one measure of success is the degree to which a reduced representation can predict the original data . in statistical terms",
    ", we must define an error function ( or loss function ) .",
    "it can be proved that under a common loss function , mean squared error ( i.e. @xmath146 norm ) , pca provides the optimal reduced representation of the data .",
    "this means that selecting orthogonal directions for principal components is the best solution to predicting the original data .",
    "given the examples of figure  [ fig : failures ] , how could this statement be true ?",
    "our intuitions from figure  [ fig : failures ] suggest that this result is somehow misleading .",
    "the solution to this paradox lies in the goal we selected for the analysis .",
    "the goal of the analysis is to decorrelate the data , or said in other terms , the goal is to remove second - order dependencies in the data . in the data sets of figure  [ fig : failures ] ,",
    "higher order dependencies exist between the variables . therefore",
    ", removing second - order dependencies is insufficient at revealing all structure in the data .",
    "multiple solutions exist for removing higher - order dependencies .",
    "for instance , if prior knowledge is known about the problem , then a nonlinearity ( i.e. _ kernel _ ) might be applied to the data to transform the data to a more appropriate naive basis .",
    "for instance , in figure  [ fig : failures]a , one might examine the polar coordinate representation of the data .",
    "this parametric approach is often termed _",
    "kernel pca_.    another direction is to impose more general statistical definitions of dependency within a data set , e.g. requiring that data along reduced dimensions be _ statistically independent_. this class of algorithms , termed , _ independent component analysis _",
    "( ica ) , has been demonstrated to succeed in many domains where pca fails .",
    "ica has been applied to many areas of signal and image processing , but suffers from the fact that solutions are ( sometimes ) difficult to compute .    writing",
    "this paper has been an extremely instructional experience for me .",
    "i hope that this paper helps to demystify the motivation and results of pca , and the underlying assumptions behind this important analysis technique .",
    "please send me a note if this has been useful to you as it inspires me to keep writing !",
    "this section proves a few unapparent theorems in linear algebra , which are crucial to this paper . + * 1 .",
    "the inverse of an orthogonal matrix is its transpose . *",
    "+ let @xmath87 be an @xmath32 orthogonal matrix where @xmath147 is the @xmath80 column vector .",
    "the @xmath79 element of @xmath148 is @xmath149 therefore , because @xmath150 , it follows that @xmath151 . + * 2 . for any matrix @xmath87 , @xmath148 and @xmath152",
    "are symmetric . *",
    "+ @xmath153 * 3 .",
    "a matrix is symmetric if and only if it is orthogonally diagonalizable . *",
    "+ because this statement is bi - directional , it requires a two - part `` if - and - only - if '' proof .",
    "one needs to prove the forward and the backwards `` if - then '' cases .",
    "let us start with the forward case .",
    "if @xmath87 is orthogonally diagonalizable , then @xmath87 is a symmetric matrix . by hypothesis ,",
    "orthogonally diagonalizable means that there exists some @xmath90 such that @xmath154 , where @xmath89 is a diagonal matrix and @xmath90 is some special matrix which diagonalizes @xmath87 .",
    "let us compute @xmath155 . @xmath156    evidently , if @xmath87 is orthogonally diagonalizable , it must also be symmetric",
    ".    the reverse case is more involved and less clean so it will be left to the reader . in lieu of this ,",
    "hopefully the `` forward '' case is suggestive if not somewhat convincing . + * 4 .",
    "a symmetric matrix is diagonalized by a matrix of its orthonormal eigenvectors . *",
    "+ let @xmath87 be a square symmetric matrix with associated eigenvectors @xmath157 . let @xmath158}$ ] where the @xmath80 column of @xmath90 is the eigenvector @xmath159 .",
    "this theorem asserts that there exists a diagonal matrix @xmath89 such that @xmath154 .",
    "this proof is in two parts . in the first part",
    ", we see that the any matrix can be orthogonally diagonalized if and only if it that matrix s eigenvectors are all linearly independent .",
    "in the second part of the proof , we see that a symmetric matrix has the special property that all of its eigenvectors are not just linearly independent but also orthogonal , thus completing our proof .    in the first part of the proof ,",
    "let @xmath87 be just some matrix , not necessarily symmetric , and let it have independent eigenvectors ( i.e. no degeneracy ) .",
    "furthermore , let @xmath158}$ ] be the matrix of eigenvectors placed in the columns .",
    "let @xmath89 be a diagonal matrix where the @xmath80 eigenvalue is placed in the @xmath125 position .",
    "we will now show that @xmath160 .",
    "we can examine the columns of the right - hand and left - hand sides of the equation . @xmath161 \\\\",
    "\\mathsf{right\\;hand\\;side : } & \\mathbf{ed } & = & [ \\lambda_{1}\\mathbf{e_1}\\:\\lambda_{2}\\mathbf{e_2}\\:\\ldots\\:\\lambda_{n}\\mathbf{e_n } ] \\end{array}\\ ] ] evidently , if @xmath160 then @xmath162 for all @xmath163 .",
    "this equation is the definition of the eigenvalue equation .",
    "therefore , it must be that @xmath160 .",
    "a little rearrangement provides @xmath164 , completing the first part the proof .    for the second part of the proof ,",
    "we show that a symmetric matrix always has orthogonal eigenvectors . for some symmetric matrix ,",
    "let @xmath165 and @xmath166 be distinct eigenvalues for eigenvectors @xmath167 and @xmath168 .",
    "@xmath169 by the last relation we can equate that .",
    "since we have conjectured that the eigenvalues are in fact unique , it must be the case that @xmath170 .",
    "therefore , the eigenvectors of a symmetric matrix are orthogonal .",
    "let us back up now to our original postulate that @xmath87 is a symmetric matrix .",
    "by the second part of the proof , we know that the eigenvectors of @xmath87 are all orthonormal ( we choose the eigenvectors to be normalized ) .",
    "this means that @xmath90 is an orthogonal matrix so by theorem 1 , @xmath171 and we can rewrite the final result . @xmath172 .",
    "thus , a symmetric matrix is diagonalized by a matrix of its eigenvectors .",
    "+ * 5 . for any arbitrary @xmath32 matrix @xmath26",
    ", the symmetric matrix @xmath111 has a set of orthonormal eigenvectors of @xmath173 and a set of associated eigenvalues @xmath174 .",
    "the set of vectors @xmath175 then form an orthogonal basis , where each vector @xmath176 is of length @xmath177 .",
    "* + all of these properties arise from the dot product of any two vectors from this set .",
    "@xmath178 the last relation arises because the set of eigenvectors of @xmath26 is orthogonal resulting in the kronecker delta . in more simpler terms",
    "the last relation states : @xmath179 this equation states that any two vectors in the set are orthogonal .",
    "the second property arises from the above equation by realizing that the length squared of each vector is defined as : @xmath180",
    "this code is written for matlab 6.5 ( release 13 ) from mathworks .",
    "the code is not computationally efficient but explanatory ( terse comments begin with a % ) .",
    "this first version follows section 5 by examining the covariance of the data set ."
  ],
  "abstract_text": [
    "<S> principal component analysis ( pca ) is a mainstay of modern data analysis - a black box that is widely used but ( sometimes ) poorly understood . </S>",
    "<S> the goal of this paper is to dispel the magic behind this black box . </S>",
    "<S> this manuscript focuses on building a solid intuition for how and why principal component analysis works . </S>",
    "<S> this manuscript crystallizes this knowledge by deriving from simple intuitions , the mathematics behind pca . </S>",
    "<S> this tutorial does not shy away from explaining the ideas informally , nor does it shy away from the mathematics . </S>",
    "<S> the hope is that by addressing both aspects , readers of all levels will be able to gain a better understanding of pca as well as the when , the how and the why of applying this technique . </S>"
  ]
}