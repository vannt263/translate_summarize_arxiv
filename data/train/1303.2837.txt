{
  "article_text": [
    "consider a network represented by a set @xmath0 of agents seeking to solve the following optimization problem on a euclidean space @xmath1 : @xmath2 where @xmath3 is a convex real function known by agent @xmath4 only .",
    "function @xmath3 can be interpreted as the price payed by an agent @xmath4 when the global network state is equal to @xmath5 .",
    "this problem arises for instance in _ cloud learning _",
    "applications where massive data sets are distributed in a network and processed by distinct virtual machines @xcite .",
    "we investigate distributed optimization algorithms : agents iteratively update a local estimate using their private objective @xmath3 and , simultaneously , exchange information with their neighbors in order to eventually reach a consensus on the global solution .",
    "standard algorithms are generally _ synchronous _ :",
    "all agents are supposed to complete their local computations synchronously at each tick of an external clock , and then synchronously merge their local results .",
    "however , in many situations , one faces variable sizes of the local data sets along with heterogeneous computational abilities of the virtual machines .",
    "synchronism then becomes a burden , as the global convergence rate is expected to depend on the local computation times of the slowest agents .",
    "it is crucial to introduce asynchronous methods which allow the estimates to be updated in a non - coordinated fashion , rather than all together or in some frozen order .",
    "the literature contains at least three classes of distributed optimization methods for solving  ( [ eq : pb ] ) . the first one is based on the simultaneous use of a local _ first - order _ optimization algorithm ( subgradient algorithm  , nesterov - like method @xcite ) and a gossip process which drives the network to a consensus .",
    "a second class of methods is formed by distributed newton - raphson methods @xcite .",
    "this paper focuses on a third class of methods derived from proximal splitting methods  @xcite .",
    "perhaps the most emblematic proximal splitting method is the so - called _ alternating direction method of multipliers _ ( admm ) recently popularized to multiagent systems by the monograph  @xcite .",
    "_ demonstrated the remarkable potential of admm to handle distributed optimization problems and introduce a useful framework to encompass graph - constrained communications @xcite .",
    "we also refer to @xcite for recent contributions .",
    "however , all of these works share a common perspective : algorithms are synchronous .",
    "they require a significant amount of coordination or scheduling between agents . in @xcite , agents operate in parallel , whereas @xcite proposes a sequential version of admm where agents operate one after the other in a predetermined order .",
    "* contributions .",
    "* this paper introduces a novel class of distributed algorithms to solve  ( [ eq : pb ] ) .",
    "the algorithms are asynchronous in the sense that some components of the network are allowed to wake up at random and perform local updates , while the rest of the network stands still .",
    "no coordinator or global clock is needed .",
    "the frequency of activation of the various network components is likely to vary .",
    "the algorithms rely on the introduction of _ randomized _ gauss - seidel iterations of a douglas - rachford monotone operator .",
    "we prove that the latter iterations provides a new powerful method for finding the zeros of a sum of two monotone operators .",
    "application of our method to problem  ( [ eq : pb ] ) yields a randomized admm - like algorithm , which is proved to converge to the sought minimizers .",
    "the paper is organized as follows .",
    "the distributed optimization problem is rigorously stated in section [ sec : admm ] . the synchronous admm algorithm that solves this problem",
    "is then described in section  [ sec : sync - admm ] .",
    "section  [ sec : randprox ] forms the core of the paper .",
    "after quickly recalling the monotone operator formalism , the random gauss - seidel form of the proximal algorithm is described and its convergence is shown there .",
    "these results will eventually lead to an asynchronous version of the well - known douglas - rachford splitting algorithm . in section",
    "[ sec : randadmm ] , the results of section  [ sec : randprox ] are applied towards developing an asynchronous version of the admm algorithm .",
    "an implementation example is finally provided in section  [ sec : num ] along with some simulations in section  [ sec : simus ] .",
    "consider a non - directed graph @xmath6 where @xmath0 is a set of vertices and @xmath7 a set of edges .",
    "we sometimes note @xmath8 for @xmath9 . for any @xmath10 , we denote by @xmath11 the subgraph of @xmath12 induced by @xmath13 ( _ i.e. _ , @xmath11 has vertices @xmath13 and for any @xmath14 , @xmath15 is an edge of @xmath11 if and only if it is an edge of @xmath12 ) .",
    "let @xmath1 be a euclidean space .",
    "we denote by @xmath16 the set of functions on @xmath17 .",
    "it is endowed with the inner product @xmath18 where @xmath19 is the inner product on  @xmath1 .",
    "we will omit subscripts @xmath20 and @xmath21 when no confusion occurs .",
    "for any finite collection @xmath22 , we endow the space @xmath23 with the scalar product @xmath24 for any @xmath25 and @xmath26 .",
    "we denote by @xmath27 the restriction of @xmath5 to @xmath13 _ i.e. _ , @xmath28 is the linear operator defined for any @xmath29 as @xmath30 .",
    "we denote by @xmath31 the constant function equal to one and by @xmath32 the linear span of @xmath33 _ i.e. _ , the set of constant functions on @xmath13 .",
    "notation @xmath34 represents the cardinal of a set @xmath13 .    for a closed proper convex function @xmath35 $ ]",
    "we define @xmath36 .",
    "consider a network of agents represented by a non - oriented graph @xmath6 where @xmath0 is a finite set of vertices ( _ i.e. _ , the agents ) and @xmath7 is a set of edges .",
    "each agent @xmath37 has a private cost function @xmath38 $ ] where @xmath1 is a euclidean space .",
    "we make the following assumption on functions @xmath3 .",
    "[ hyp : f]@xmath39 + _ i ) _ for all @xmath37 , @xmath3 is a proper closed convex function .",
    "+ _ ii ) _ the infimum in  ( [ eq : pb ] ) is finite and is attained at some point  @xmath40 .    in order to solve the optimization problem  ( [ eq : pb ] ) on the graph @xmath12 , we first provide an equivalent formulation of  ( [ eq : pb ] ) that will be revealed useful . for some integer @xmath41 , consider a finite collection @xmath42 of subsets of @xmath0 which we shall refer to as _ components_. we assume the following condition .    _",
    "i ) _ @xmath43 .",
    "+ _ ii ) _ @xmath44 is connected .",
    "[ hyp : subg ]    assumption  [ hyp : subg]_i ) _ implies that any vertex appears in one of the components @xmath45 at least .",
    "we stress the fact that two distinct components @xmath46 and @xmath47 are not necessarily disjoint , though . assumption  [ hyp : subg]_ii ) _ means that the union of all subgraphs is connected .",
    "as the latter union is also a subgraph of @xmath12 , this implies that @xmath12 is connected .",
    "as will be made clear below , our algorithms shall assume that all agents in the same component are able to perform simple operations in a coordinated fashion ( _ i.e. _ , compute a local average over a component ) .",
    "thus , in practice , it is reasonable to require that each subgraph @xmath48 is itself connected .",
    "we introduce some notations .",
    "we set for any @xmath29 , @xmath49 for any @xmath50 , we define the closed proper convex function @xmath51 where @xmath52 is the indicator function of a set @xmath53 ( equal to zero on @xmath53 and to @xmath54 outside ) . here",
    "@xmath55 is equal to zero if for any @xmath56 , @xmath57 is constant .",
    "otherwise , @xmath55 is infinite . for any @xmath29",
    ", we define @xmath58 .",
    "we consider the following optimization problem : @xmath59    under assumption  [ hyp : subg ] , @xmath5 is a minimizer of  ( [ eq : pbequiv ] ) if and only if @xmath60 where @xmath61 is a minimizer of  ( [ eq : pb ] ) .",
    "let @xmath62 such that @xmath63 is finite .",
    "then @xmath5 is constant on each component .",
    "let @xmath64 be two arbitrary vertices in @xmath0 .",
    "there exists a path in @xmath44 connecting @xmath4 and @xmath65 .",
    "each edge of this path connects two vertices which belong to a common component .",
    "thus , @xmath5 is constant on two consecutive vertices of the path .",
    "this proves that @xmath66 .",
    "thus , @xmath5 is constant and the result follows .    as noted in  @xcite ,",
    "solving problem  ( [ eq : pbequiv ] ) is equivalent to the search of the zeros of two monotone operators .",
    "one of possible approaches for that sake is to use admm .",
    "although the choice of the sets @xmath45 does not change the minimizers of the initial problem , it has an impact on the particular form of admm used to find these minimizers , as we shall see below .    in order to be more explicit ,",
    "we provide in this section two important examples of possible choices for the components @xmath45 .",
    "let @xmath67 and @xmath68 .",
    "problem  ( [ eq : pbequiv ] ) writes @xmath69 in this case , the formulation is identical to ( * ? ? ?",
    "* chapter 7 ) .",
    "[ ex : pairwise ] let @xmath70 and @xmath71 .",
    "that is , each set @xmath46 is a pair of vertices @xmath15 such that @xmath15 is an edge .",
    "problem  ( [ eq : pbequiv ] ) writes @xmath72{@{}c@ { } }      x(v ) \\\\",
    "x(w )    \\end{array}\\right)\\ ] ] where @xmath73 stands for the vector @xmath74 .",
    "we now apply the standard admm to problem  ( [ eq : pbequiv ] ) .",
    "perhaps the most direct way to describe admm is to reformulate the unconstrained problem  ( [ eq : pbequiv ] ) into the following constrained problem : minimize @xmath75 subject to @xmath76 . for any @xmath29 , @xmath77 ,",
    "the augmented lagrangian is given by @xmath78 where @xmath79 is a constant .",
    "admm consists of the iterations    [ eq : ladmm ] @xmath80    from ( * ? ? ?",
    "3.2 ) , the following result is immediate .    under assumption  [ hyp : f ]",
    ", the sequence @xmath81 defined in ( [ eq : ladmmx ] ) converges to a minimizer of  .",
    "one should now make  ( [ eq : ladmm ] ) more explicit and convince the reader that the iterations are indeed amenable to distributed implementation . due to the specific form of function @xmath82",
    ", it is clear from  ( [ eq : ladmmz ] ) that all components @xmath83 of @xmath84 are constant .",
    "otherwise stated , @xmath85 for some constants @xmath86 . for any @xmath37 , we set @xmath87 now consider the first update equation  ( [ eq : ladmmx ] ) .",
    "getting rid of all quantities in @xmath88 which do not depend on the @xmath4th component of @xmath5 , we obtain for any @xmath37 @xmath89 after some algebra , the above equation further simplifies to @xmath90 where we introduced the following constants : @xmath91 it is straightforward to show that the second update equation ( [ eq : ladmmz ] ) admits as well a simple decomposable form .",
    "after some algebra , we obtain that for any @xmath92 , @xmath93 finally , for all @xmath92 and @xmath94 , equation  ( [ eq : ladmml ] ) reads @xmath95 averaging  ( [ eq:4 ] ) w.r.t . @xmath4 and using  ( [ eq:2 ] ) yields @xmath96 .",
    "thus , the second term in the rhs of  ( [ eq:2 ] ) can be deleted . finally , averaging  ( [ eq:4 ] ) w.r.t .",
    "@xmath56 leads to @xmath97    * synchronous admm * : + at each iteration @xmath98 , + for each agent @xmath4 , compute @xmath99 using  ( [ eq : proxx ] ) .",
    "+ in each components @xmath92 , compute @xmath100 for each agent @xmath4 , compute @xmath101 and @xmath102 using  ( [ eq : z ] ) and  ( [ eq : ladmmlter ] ) respectively .",
    "the above algorithm implicitly requires the existence of a routine for computing an average , in each component @xmath46 .",
    "this requirement is mild when the components coincide with edges of the graph as in example  2 . in this case ,",
    "one only needs that the two vertices of an edge share their current estimate and find an agreement on the average . in the general case ,",
    "the objective can be achieved by selecting a leader in each component whose role is to gather the estimates , compute the average and send the result to all agents in this component .",
    "it is worth noting that in the case of example  1 , the synchronous admm described above coincides with the algorithm of @xcite .",
    "an operator @xmath103 on a euclidean space @xmath104 is a set valued mapping @xmath105 .",
    "an operator can be equivalently identified with a subset of @xmath106 , and we write @xmath107 when @xmath108 . given two operators @xmath109 and @xmath110 on @xmath104 and two real numbers @xmath111 and @xmath112 , the operator @xmath113 is defined as @xmath114 .",
    "the identity operator is @xmath115 and the inverse of the operator @xmath103 is @xmath116 .",
    "the operator @xmath103 is said _ monotone _ if @xmath117 a monotone operator is said _ maximal _ if it is not strictly contained in any monotone operator ( as a subset of @xmath106 ) .",
    "finally , @xmath103 is said _ firmly non - expansive _ if @xmath118 the typical example of a monotone operator is the subdifferential @xmath119 of a convex function @xmath120 . finding a minimum of @xmath121 amounts",
    "to finding a point in @xmath122 , where @xmath123 is the set of zeroes of an operator @xmath103 . a common technique for finding a zero of a maximal monotone operator @xmath103 is the so - called _ proximal point algorithm _ @xcite that we now describe .",
    "the _ resolvent _ of @xmath103 is the operator @xmath124 for @xmath79 .",
    "one key result ( see _ e.g. _ @xcite ) says that @xmath103 is maximal monotone if and only if @xmath125 is firmly non expansive and its domain is @xmath104 . observe that a firmly non expansive operator is single valued and denote by @xmath126 the set of fixed points of @xmath125 .",
    "it is clear that @xmath127 .",
    "the firm non expansiveness of @xmath125 plays a central role in the proof of the following result :    [ lemma : prox ] if @xmath103 is a maximal monotone operator and @xmath79 , then the iterates @xmath128 starting at any point of @xmath104 converge to a point of @xmath126 whenever this set is non - empty .",
    "assume now that the euclidean space @xmath104 is a cartesian product of euclidean spaces of the form @xmath129 where @xmath130 is a given integer , and write any @xmath131 as @xmath132 where @xmath133 for @xmath134 .",
    "let @xmath135 be a firmly non expansive operator on @xmath104 and write @xmath136 where @xmath137 . for @xmath138",
    ", define the single valued operator @xmath139 as @xmath140 considering an iterative algorithm of the form @xmath141 , its _ gauss - seidel _ version would be an algorithm of the form @xmath142 .",
    "we are interested here in a _ randomized version _ of these iterates . on a probability space @xmath143 ,",
    "let @xmath144 be a random process satisfying the following assumption :    [ hyp : xi ] the random variables @xmath145 are independent and identically distributed .",
    "they are valued in the set @xmath146 with @xmath147 = p_\\ell > 0 $ ] for all @xmath134 .",
    "we are interested here in the convergence of the random iterates @xmath148 towards a ( generally random ) point of @xmath149 , provided this set is non empty :    [ theo : main ] let @xmath135 is a firmly non - expansive operator on @xmath104 with domain @xmath104 .",
    "let @xmath150 be a sequence of random variables satisfying assumption [ hyp : xi ] .",
    "assume that @xmath151 .",
    "then for any initial value @xmath152 , the sequence of iterates @xmath153 converges almost surely to a random variable supported by @xmath149 .",
    "denote by @xmath154 the inner product of @xmath104 , and by @xmath155 its associated squared norm .",
    "define a new inner product @xmath156 on @xmath104 , and let @xmath157 be its associated squared norm .",
    "fix @xmath158 in @xmath149 .",
    "conditionally to the sigma - field @xmath159 we have @xmath160   = \\sum_{\\ell=1}^l p_\\ell        { \\left|\\!\\left|\\!\\left|}\\hat{{\\mathsf{s}}}_{\\ell } ( \\zeta^k ) -   \\zeta^\\star{\\right|\\!\\right|\\!\\right|}^2 \\\\   & = \\sum_{\\ell=1}^l p_\\ell   \\bigl (    \\frac{1}{p_\\ell }   \\| { \\mathsf{s}}_{\\ell}(\\zeta^k ) - \\zeta_\\ell^\\star\\|^2_{{\\mathsf{y}}_\\ell } +    \\sum_{\\underset{i \\neq \\ell}{i=1}}^l   \\frac{1}{p_i }    \\| \\zeta_i^k -   \\zeta_i^\\star\\|^2_{{\\mathsf{y}}_i } \\bigr )    \\\\   & =   \\| { \\mathsf{s}}(\\zeta^k ) - \\zeta^\\star\\|^2 +    \\sum_{\\ell=1}^l   \\frac{1-p_\\ell}{p_\\ell }    \\| \\zeta_\\ell^k - \\zeta_\\ell^\\star\\|^2_{{\\mathsf{y}}_\\ell }    \\\\   & =    { \\left|\\!\\left|\\!\\left|}\\zeta^k -   \\zeta^\\star{\\right|\\!\\right|\\!\\right|}^2 +    \\| { \\mathsf{s}}(\\zeta^k ) - \\zeta^\\star\\|^2 - \\| \\zeta^k -   \\zeta^\\star\\|^2    \\end{aligned}\\ ] ] since @xmath161 , we have @xmath162 where the inequality comes from the easily verifiable fact that @xmath163 is firmly non - expansive when @xmath135 is .",
    "this leads to the inequality @xmath164     \\leq   { \\left|\\!\\left|\\!\\left|}\\zeta^k -   \\zeta^\\star{\\right|\\!\\right|\\!\\right|}^2 -                                      \\| { \\mathsf{s}}(\\zeta^k ) - \\zeta^k\\|^2\\ ] ] which shows that @xmath165 is a nonnegative supermartingale with respect to the filtration @xmath166 . as such",
    ", it converges with probability one towards a random variable @xmath167 satisfying @xmath168 almost everywhere .",
    "given a countable dense subset @xmath53 of @xmath149 , there is a probability one set on which @xmath169 for all @xmath170 .",
    "let @xmath171 , let @xmath172 , and choose @xmath170 such that @xmath173 . with probability one",
    ", we have @xmath174 for @xmath98 large enough .",
    "similarly , @xmath175 for @xmath98 large enough .",
    "we therefore obtain :    * c1 : * : :    there is a probability one set on which    @xmath176    converges for every    @xmath171 .    getting back to inequality , taking the expectations on both sides of this inequality and iterating over @xmath98 , we obtain @xmath177 \\leq   ( \\zeta^0 - \\zeta^\\star)^2 .\\ ] ] by markov s inequality and borel cantelli s lemma , we therefore obtain :    * c2 : * : :    @xmath178 almost surely .",
    "we now consider an elementary event in the probability one set where * c1 * and * c2 * hold . on this event , since @xmath176 converges for @xmath171 , the sequence @xmath179 is bounded .",
    "since @xmath135 is firmly non expansive , it is continuous , and * c2 * shows that all the accumulation points of @xmath179 are in @xmath149 .",
    "it remains to show that these accumulation points reduce to one point .",
    "assume that @xmath180 is an accumulation point . by * c1",
    "* , @xmath181 converges .",
    "therefore , @xmath182 , which shows that @xmath183 is unique .",
    "we now return to the optimization problem  ( [ eq : pbequiv ] ) .",
    "it is a well known fact that the standard admm can be seen as special case of the so - called douglas - rachford algorithm  @xcite .",
    "the douglas - rachford algorithm can itself be seen as a special case of a proximal point algorithm . by the results of the previous section",
    ", this suggests that random gauss - seidel iterations applied to the douglas - rachford operator produce a sequence which eventually converges to the sought solutions .",
    "it turns out that the latter random iterations can be written under the form of practical asynchronous admm - like algorithm .",
    "consider the following dual problem associated with  ( [ eq : pbequiv ] ) @xmath184 where @xmath185 are the fenchel conjugates of @xmath121 and @xmath82 and @xmath186 is the adjoint of @xmath187 . by assumption",
    "[ hyp : f ] along with  ( * ? ? ?",
    "* th.3.3.5 ) , the minimum in  ( [ eq : dual ] ) is attained and its opposite coincides with the minimum of  ( [ eq : pbequiv ] ) .",
    "note that @xmath188 is a minimizer of  ( [ eq : dual ] ) iff zero belongs to the subdifferential of the objective function in  ( [ eq : dual ] ) . by (",
    "* th.3.3.5 ) again , this reads @xmath189 .",
    "otherwise stated , finding minimizers of the dual problem  ( [ eq : dual ] ) boils down to searching zeros of the sum of two maximal monotone operators @xmath190 defined by @xmath191 and @xmath192 . for a fixed @xmath79 , the douglas - rachford  /  lions - mercier operator @xmath193 is defined as @xmath194",
    "the following lemma is an immediate consequence of @xcite .",
    "[ lemma : lm ] under assumption  [ hyp : f ] , @xmath193 is maximal monotone , and @xmath195 .",
    "moreover , @xmath196 for any @xmath197 .",
    "lemma  [ lemma : lm ] implies that the search for a zero of @xmath190 boils down to the search of a zero of @xmath193 up to a resolvent step @xmath198 . to that end ,",
    "a standard approach is to use a proximal point algorithm of the form @xmath199 . by  @xcite",
    ", it can be shown that this approach is equivalent to the admm derived in section  [ sec : admm ] . here",
    ", our aim is different .",
    "we shall consider random gauss - seidel iterations in order to derive an asynchronous version of the admm .",
    "define @xmath200 as the resolvent associated with the douglas - rachford operator @xmath193 .",
    "on the space @xmath201 , define the operator @xmath202 as in for any @xmath92 .",
    "let @xmath150 be a random process satisfying assumption  [ hyp : xi ] .",
    "the following result is a consequence of theorem  [ theo : main ] combined with lemma  [ lemma : lm ] .",
    "let assumptions  [ hyp : f ] , [ hyp : subg ] and [ hyp : xi ] hold true .",
    "consider the sequence @xmath203 defined by @xmath204 .",
    "then for any initial value @xmath152 , the sequence @xmath205 converges almost surely to a minimizer of  ( [ eq : dual ] ) .",
    "in order to complete the above result , we still must justify the fact that , as claimed , the above iterations can be seen as an asynchronous distributed algorithm .",
    "we make the above random gauss - seidel iterations more explicit . in the sequel",
    "we shall always denote by @xmath206 the @xmath56th component of a function @xmath207 _ i.e. _ , @xmath208 . for any @xmath56",
    ", we introduce the average @xmath209 .",
    "lemma  [ lem : sl ] below states that any @xmath207 is uniquely represented by a couple @xmath210 whose expression is provided .",
    "moreover , it provides the explicit form of the @xmath56th block @xmath211 of the resolvent @xmath135 .",
    "this shall be the basis of our asynchronous distributed algorithm .    for any @xmath207",
    ", the following holds true .",
    "i ) _ there exist a unique @xmath210 such that @xmath212 .",
    "+ _ ii ) _ @xmath213 .",
    "+ _ iii ) _ for any @xmath214 , @xmath215 and @xmath216 + _ iv ) _ for any @xmath214 , and any @xmath94 @xmath217 where @xmath218 is defined by @xmath219 [ lem : sl ]    _ i)-ii ) _ _ existence _ : let us define @xmath220 and @xmath221 .",
    "trivially , @xmath212 . as @xmath222",
    ", we deduce that @xmath223 .",
    "_ uniqueness _ : for a fixed @xmath224 satisfying @xmath212 , one has @xmath225 and thus @xmath220 . as a consequence , @xmath221 .",
    "_ iii ) _ we use @xmath226 ( see  ( * ? ? ? * th . 14.3 ) ) .",
    "as @xmath82 is the indicator function of the set @xmath227 , @xmath228 coincides with the projection operator onto that set .",
    "thus , for any @xmath56 , @xmath215 . the expression of @xmath229 follows from @xmath221 .    _",
    "iv ) _ operator @xmath230 can be written as @xmath231 moreover , as @xmath193 is monotone , @xmath232 is a singleton . representing @xmath233 with @xmath223",
    ", it follows from the above expression of @xmath135 that @xmath234 where @xmath235 is such that @xmath236 for some @xmath237 . using @xmath191 , condition",
    "@xmath237 translates to : there exists @xmath238 s.t .",
    "the output - resolvent is obtained by @xmath240 . for a given component @xmath56 ,",
    "this boils down to equation  ( [ eq : slzeta ] ) .",
    "the remaining task is to provide the expression of @xmath5 . by the fenchel - young equality @xmath241  ( * ? ? ?",
    "* prop.3.3.4 ) , condition @xmath238 is equivalent to @xmath242 . using that @xmath243",
    ", we obtain @xmath244 .",
    "otherwise stated , @xmath245 where @xmath88 is the augmented lagrangian defined in  ( [ eq : alag ] ) . using the results of section  [ sec : sync - admm ] ,",
    "@xmath218 is given by ( [ eq : slx ] ) for any  @xmath4 .",
    "we are now in position to state the main algorithm .",
    "it simply consists in an explicit writing of the random gauss - seidel iterations @xmath204 using lemma  [ lem : sl]_iv)_. note that , by lemma  [ lem : sl]_i ) _ , the definition of a sequence @xmath246 on @xmath247 is equivalent to the definition of two sequences @xmath248 such that @xmath249 .",
    "moreover , by lemma  [ lem : sl]_iii ) _ , each component @xmath250 of @xmath84 is a constant .",
    "the definition of @xmath84 thus reduces to the definition of @xmath130 constants @xmath251 in @xmath1 .",
    "* asynchronous admm * : + at each iteration @xmath98 , draw r.v .",
    "@xmath252 . + for @xmath253 , set for any @xmath94 : @xmath254 for any @xmath255 , set @xmath256 .",
    "+ for any @xmath257 , set @xmath258 .",
    "in order to illustrate our results , we consider herein an asynchronous version of the admm algorithm in the context of section  [ sec : admm]-example  [ ex : pairwise ] .",
    "the scenario is the following : first , agent @xmath259 wakes up at time @xmath260 with the probability @xmath261 . denoting by @xmath262 the neighborhood of agent @xmath4 in the graph @xmath12 ,",
    "this agent then chooses one of its neighbors , say @xmath65 , with the probability @xmath263 and sends an activation message to @xmath65 . in this setting ,",
    "the edge @xmath264 coincides with one of the @xmath46 of example  [ ex : pairwise ] in section  [ sec : admm ] .",
    "it is easy to see that the samples of the activation process @xmath145 who is of course valued in @xmath7 are governed by the probability law @xmath265 = \\frac{q_v}{|{\\mathcal n}_v| }   + \\frac{q_w}{|{\\mathcal n}_w| } > 0 .\\ ] ] when the edge @xmath15 is activated , the following two @xmath266 operations are performed by the agents : @xmath267 the two agents exchange then the values @xmath99 and @xmath268 and perform the following operations : @xmath269 we remark that this communication scheme is reminiscent of the so - called _ random gossip _",
    "algorithm introduced in @xcite in the context of distributed averaging .",
    "we consider a network with @xmath270 and with @xmath271 .",
    "we evaluate the behavior of : i ) the _ synchronous admm _ ii ) the _ asynchronous admm _ and iii ) the _ distributed gradient descent _ with @xmath272 stepsize @xcite using _ random gossip _ as a communication algorithm@xcite .",
    "each agent maintains a different quadratic convex function and their goal is to reach consensus over the minimizer of problem ( [ eq : pb ] ) .    in figure",
    "[ fig : cdc ] , we plot the squared error versus the number of primal updates for the three considered algorithms . we observe that our algorithm clearly outperforms the distributed gradient descent .",
    "joao f.  c. mota , joao m.  f. xavier , pedro m.  q. aguiar , and markus puschel , `` distributed admm for model predictive control and congestion control , '' in _ proc .",
    "51st ieee conference on decision and control ( cdc ) _ , 2012 , pp ."
  ],
  "abstract_text": [
    "<S> consider a set of networked agents endowed with private cost functions and seeking to find a consensus on the minimizer of the aggregate cost . a new class of random asynchronous distributed optimization methods </S>",
    "<S> is introduced . </S>",
    "<S> the methods generalize the standard alternating direction method of multipliers ( admm ) to an asynchronous setting where isolated components of the network are activated in an uncoordinated fashion . </S>",
    "<S> the algorithms rely on the introduction of _ randomized _ gauss - seidel iterations of a douglas - rachford operator for finding zeros of a sum of two monotone operators . convergence to the sought minimizers is provided under mild connectivity conditions . </S>",
    "<S> numerical results sustain our claims . </S>"
  ]
}