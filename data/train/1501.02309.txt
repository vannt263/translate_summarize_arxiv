{
  "article_text": [
    "with a rapid increase in the number of application domains , such as data integration , information extraction , sensor networks , scientific measurements etc . , where uncertain data are generated in an unprecedented speed , managing , analyzing and query processing over such data has become a major challenge and have received significant attentions .",
    "we study one important problem in this domain , building data structures for uncertain data for efficiently answering certain range queries . the problem has been studied extensively with a wide range of applications @xcite .",
    "we formally define the problems below .",
    "let @xmath7 be any real line ( e.g. , the @xmath8-axis ) . in the ( traditional ) deterministic version of this problem",
    ", we are given a set @xmath0 of @xmath1 deterministic points on @xmath7 , and the goal is to build a data structure ( also called `` index '' in database ) such that given a range , specified by an interval @xmath9 , one point ( or all points ) in @xmath2 can be retrieved efficiently .",
    "it is well known that a simple solution for this problem is a binary search tree over all points which is of linear size and can support logarithmic ( plus output size ) query time . however , in many applications , the location of each point may be uncertain and the uncertainty is represented in the form of probability distributions @xcite . in particular , an _ uncertain point _",
    "@xmath10 is specified by its probability density function ( pdf ) @xmath11 .",
    "let @xmath0 be the set of @xmath1 uncertain points in @xmath7 ( with pdfs specified as input ) . our goal is to build data structures to quickly answer range queries on @xmath0 . in this paper",
    ", we consider the following three types of range queries , each of which involves a query interval @xmath12 $ ] . for any point @xmath13",
    ", we use @xmath14 $ ] to denote the probability that @xmath10 is contained in @xmath2 .",
    "top-1 query : : :    return the point @xmath10 of @xmath0 such that    @xmath14 $ ] is the largest .",
    "top-@xmath4 query : : :    given any integer @xmath4 , @xmath15 , as    part of the query , return the @xmath4 points @xmath10    of @xmath0 such that @xmath14 $ ] are the    largest .",
    "threshold query : : :    given a threshold @xmath6 , as part of the query , return all    points @xmath10 of @xmath0 such that    @xmath16\\geq \\tau$ ] .",
    "we assume @xmath17 is a step function , i.e. , a _ histogram _ consisting of at most @xmath18 pieces ( or intervals ) for some integer @xmath19 ( e.g. , see fig .  [",
    "fig : histogram ] ) .",
    "more specifically , @xmath20 for @xmath21 , @xmath22 , with @xmath23 , @xmath24 , and @xmath25 . throughout the paper ,",
    "we assume @xmath18 is a constant .",
    "the cumulative distribution function ( cdf ) @xmath26 is a monotone piecewise - linear function consisting of @xmath18 pieces ( e.g. , see fig .",
    "[ fig : cdf ] ) .",
    "note that @xmath27 , and for any interval @xmath12 $ ] the probability @xmath14 $ ] is @xmath28 . from a geometric point of view , each interval of @xmath17 defines a rectangle with the @xmath8-axis , and the sum of the areas of all these rectangles of @xmath17 is exactly one .",
    "further , the cdf value @xmath29 is the sum of the areas of the subsets of these rectangles to the left of the vertical line through @xmath8 ( e.g. , see fig .  [",
    "fig : cdfvalue ] ) , and the probability @xmath14 $ ] is the sum of the areas of the subsets of these rectangles between the two vertical lines through @xmath30 and @xmath31 , respectively ( e.g. , see fig .  [",
    "fig : probvalue ] ) .    as discussed in @xcite , the histogram model can be used to approximate most pdfs with arbitrary precision in practice .",
    "in addition , the _ discrete _ pdf where each uncertain point can appear in a few locations , each with a certain probability , can be viewed as a special case of the histogram model because we can use infinitesimal pieces around these locations .    .",
    "]    . ]",
    "we also study an important special case where the pdf @xmath17 is a uniform distribution function , i.e. , @xmath32 is associated with an interval @xmath33 $ ] such that @xmath34 if @xmath35 $ ] and @xmath36 otherwise . clearly , the cdf @xmath37 if @xmath38 $ ] , @xmath39 if @xmath40 , and @xmath41 if @xmath42 .",
    "uniform distributions have been used as a major representation of uncertainty in some previous work ( e.g. , @xcite ) .",
    "we refer to this special case the _ uniform case _ and the more general case where @xmath17 is a histogram distribution function as the _ histogram _ case .    throughout the paper",
    ", we will always use @xmath12 $ ] to denote the query interval .",
    "the query interval @xmath2 is _ unbounded _ if either @xmath43 or @xmath44 ( otherwise , @xmath2 is _ bounded _ ) . for the threshold query",
    ", we will always use @xmath45 to denote the output size of the query , i.e. , the number of points @xmath10 of @xmath0 such that @xmath14\\geq \\tau$ ] .",
    "range reporting on uncertain data has many applications @xcite , as shown in @xcite , our problems are also useful even in some applications that involve only deterministic data .",
    "for example , consider the movie rating system in imdb where each reviewer gives a rating from 1 to 10 . a top-@xmath4 query on @xmath46 would find `` the @xmath4 movies such that the percentages of the ratings they receive at least 7 are the largest '' ; a threshold query on @xmath46 and @xmath47 would find `` all the movies such that at least 85% of the ratings they receive are larger than or equal to 7 '' .",
    "note that in the above examples the interval @xmath2 is unbounded , and thus , it would also be interesting to have data structures particularly for quickly answering queries with unbounded query intervals .",
    "$ ] is equal to the sum of the areas of the shaded rectangles . ]",
    "$ ] is equal to the sum of the areas of the shaded rectangles . ]",
    "the threshold query was first introduced by cheng _",
    "_ @xcite . using r - trees , they @xcite gave heuristic algorithms for the histogram case , without any theoretical performance guarantees .",
    "for the uniform case , if @xmath6 is fixed for any query , they proposed a data structure of @xmath48 size with @xmath49 query time @xcite .",
    "these bounds depend on @xmath50 , which can be arbitrarily large .",
    "agarwal _ et al .",
    "_ @xcite made a significant theoretical step on solving the threshold queries for the histogram case : if the threshold @xmath6 is fixed , their approach can build an @xmath51 size data structure in @xmath52 time , with @xmath53 query time ; if @xmath6 is not fixed , they built an @xmath54 size data structure in @xmath55 expected time that can answer each query in @xmath56 time .",
    "@xcite considered the threshold queries in two and higher dimensions .",
    "they provided heuristic results and a query takes @xmath51 time in the worst case .",
    "heuristic solutions were also given elsewhere , e.g. @xcite .",
    "recently , abdullah _",
    "_ @xcite extended the notion of _ geometric coresets _ to uncertain data for range queries in order to obtain efficient approximate solutions .",
    "our work falls into the broad area of managing and analyzing uncertain data which has attracted significant attentions recently in database community .",
    "this line of work has spanned a range of issues from theoretical foundation of data models and data languages , algorithmic problems for efficiently answering various queries , to system implementation issues .",
    "probabilistic database systems have emerged as a major platform for this purpose and several prototype systems have been built to address different aspects / challenges in managing probabilistic data , e.g. mystiq  @xcite , trio  @xcite , orion  @xcite , maybms  @xcite , prdb  @xcite , mcdb  @xcite .",
    "besides of the range queries we mentioned above , there has also been much work on efficiently answering different types of queries over probabilistic data , such as conjunctive queries ( or the union of conjunctive queries )  @xcite , aggregates  @xcite , top-@xmath4 and ranking  @xcite , clustering  @xcite , nearest neighbors  @xcite , and so on .",
    "we refer interested readers to the recent book @xcite for more information .",
    "as discussed in @xcite , our uncertain model is an analogue of the _ attribute - level uncertainty model _ in the probabilistic database literature .",
    "another popular model is the _ tuple - level uncertainty model _",
    "@xcite , where a tuple has fixed attribute values but its existence is uncertain .",
    "the range query under the latter model is much easier since a @xmath57-dimensional range searching over uncertain data can be transformed to a ( @xmath58)-dimensional range searching problem over certain data @xcite .",
    "in contrast , the problem under the former model is more challenging , partly because it is unclear how to transform it to an instance on certain data .      based on our above discussion",
    ", the problem has four variations : the _ uniform unbounded _ case where each pdf @xmath17 is a uniform distribution function and each query interval @xmath2 is unbounded , the _ uniform bounded _ case where each pdf @xmath17 is a uniform distribution function and each query interval @xmath2 is bounded , the _ histogram unbounded _ case where each pdf @xmath17 is a general histogram distribution function and each query interval @xmath2 is unbounded , and the _ uniform bounded _ case where each pdf @xmath17 is a general histogram distribution function and each query interval @xmath2 is bounded . refer to table [ tab : results ] for a summary of our results on the four cases .",
    "note that we also present solutions to the most general case ( i.e. , the histogram bounded case ) , which were originally left as open problems in the preliminary version of this paper @xcite .",
    "0.866llllll & top-1 queries   & top-@xmath4 queries  & threshold queries + & & preprocessing time  & @xmath52 & @xmath52 & @xmath52 + & & space & @xmath51 & @xmath51 & @xmath51 + & & query time & @xmath59   & @xmath60   & @xmath61 + & & preprocessing time & @xmath52 & @xmath52 & @xmath52 + & & space & @xmath51 & @xmath51 & @xmath51 + & & query time & @xmath59 & @xmath62 & @xmath61 + & & preprocessing time & @xmath52 & @xmath54 & @xmath54 + & & space & @xmath51 & @xmath52 & @xmath52 + & & query time & @xmath59 & @xmath62 & @xmath61 + & & preprocessing time & @xmath55 & @xmath63 & @xmath63 @xcite + & & space & @xmath54 & @xmath54 & @xmath54 @xcite + & & query time & @xmath64 & @xmath65  & @xmath66 @xcite +    we say the _ complexity _ of a data structure is @xmath67 if can be built in @xmath68 time and its size is @xmath69 .",
    "* for the uniform unbounded case , the complexities of our data structures for the three types of queries are all @xmath70 .",
    "the top-@xmath3 query time is @xmath59 ; the top-@xmath4 query time is @xmath71 ; the threshold query time is @xmath61 .",
    "* for the histogram unbounded case , our results are the same as the above uniform unbounded case except that the time for each top-@xmath4 query is @xmath72 if @xmath73 and @xmath74 otherwise ( i.e. , for large @xmath4 , the algorithm has a better performance ) .",
    "* for the uniform bounded case , the complexity of our top-1 data structure is @xmath75 , with query time @xmath59 . for the other two types of queries ,",
    "the complexities of our data structures are both @xmath76 ; the top-@xmath4 query time is @xmath72 if @xmath73 and @xmath74 otherwise , and the threshold query time is @xmath61 .",
    "* for the histogram bounded case , for threshold queries , agarwal _ et al . _",
    "@xcite built a data structure of size @xmath54 in @xmath55 expected time , with @xmath77 query time .",
    "note that our results on the threshold queries for the two uniform cases and the histogram unbounded case are clearly better than the above solution in @xcite .",
    "for top-1 queries , we build a data structure of @xmath54 size in @xmath55 ( deterministic ) time , with @xmath64 query time . for top-@xmath4 queries ,",
    "we build a data structure of @xmath54 size in @xmath55 expected time , with @xmath78 query time .",
    "note that all above results are based on the assumption that @xmath18 is a constant ; otherwise these results still hold with replacing @xmath1 by @xmath79 except that for the histogram bounded case the results hold with replacing @xmath1 by @xmath80 .    the rest of the paper is organized as follows .",
    "we first introduce the notations and some observations in section [ sec : pre ] .",
    "we present our results for the uniform case in section [ sec : uniform ] .",
    "the histogram case is discussed in section [ sec : nonuniform ] .",
    "we conclude the paper in section [ sec : conclusions ] .",
    "recall that an _ uncertain point _",
    "@xmath10 is specified by its pdf @xmath11 and the corresponding cdf is @xmath26 is a monotone piecewise - linear function ( with at most @xmath18 pieces ) . for each uncertain point @xmath10 , we call @xmath14 $ ] the _ @xmath2-probability _ of @xmath10",
    ". let @xmath81 be the set of the cdfs of all points of @xmath0 .",
    "since each cdf is an increasing piecewise linear function , depending on the context , @xmath81 may also refer to the set of the @xmath51 line segments of all cdfs . recall that @xmath12 $ ] is the query interval .",
    "we start with an easy observation .",
    "[ lem:10 ] if @xmath43 , then for any uncertain point @xmath10 , @xmath14=f_p(x_r)$ ] .    due to",
    "@xmath43 , @xmath14=\\int_{-\\infty}^{x_r}f_p(t)dt$ ] , which is exactly @xmath82 .",
    "let @xmath83 be the vertical line with @xmath8-coordinate @xmath31 . since each cdf @xmath84 is a monotonically increasing function , there is only one intersection between @xmath84 and @xmath83 .",
    "it is easy to know that for each cdf @xmath84 of @xmath81 , the @xmath85-coordinate of the intersection of @xmath84 and @xmath83 is @xmath82 , which is the @xmath2-probability of @xmath10 by lemma [ lem:10 ] .",
    "for each point in any cdf of @xmath81 , we call its @xmath85-coordinate the _ height _ of the point .    in the uniform case , each cdf @xmath84 has three segments : the leftmost one is a horizontal segment with two endpoints @xmath86 and @xmath87 , the middle one , whose slope is @xmath88 , has two endpoints @xmath87 and @xmath89 , and the rightmost one is a horizontal segment with two endpoints @xmath89 and @xmath90 .",
    "we transform each @xmath84 to the line @xmath91 containing the middle segment of @xmath84 .",
    "consider an unbounded interval @xmath2 with @xmath43 .",
    "we can use @xmath91 to compute @xmath14 $ ] in the following way .",
    "suppose the height of the intersection of @xmath83 and @xmath91 is @xmath85 .",
    "then , @xmath14=0 $ ] if @xmath92 , @xmath14=y$ ] if @xmath93 , @xmath14=1 $ ] if @xmath94 . therefore , once we know @xmath95 , we can obtain @xmath14 $ ] in constant time .",
    "hence , we can use @xmath91 instead of @xmath84 to determine the @xmath2-probability of @xmath10 .",
    "the advantage of using @xmath91 is that lines are usually easier to deal with than line segments .",
    "below , with a little abuse of notation , for the uniform case we simply use @xmath84 to denote the line @xmath91 for any @xmath96 and now @xmath81 is a set of lines .",
    "fix the query interval @xmath12 $ ] . for",
    "each @xmath97 , @xmath98 , denote by @xmath99 the point of @xmath0 whose @xmath2-probability is the @xmath97-th largest .",
    "based on the above discussion , we obtain lemma [ lem:20 ] , which holds for both the histogram and uniform cases .",
    "[ lem:20 ] if @xmath43 , then for each @xmath98 , @xmath99 is the point of @xmath0 such that @xmath100 is the @xmath97-th highest among the intersections of @xmath83 and all cdfs of @xmath81 .",
    "suppose @xmath43 . based on lemma [ lem:20 ] , to answer the top-1 query on @xmath2",
    ", it is sufficient to find the cdf of @xmath81 whose intersection with @xmath83 is the highest ; to answer the top-@xmath4 query , it is sufficient to find the @xmath4 cdfs of @xmath81 whose intersections with @xmath83 are the highest ; to answer the threshold query on @xmath2 and @xmath6 , it is sufficient to find the cdfs of @xmath81 whose intersections with @xmath83 have @xmath85-coordinates @xmath101 .    * half - plane range reporting : * as the half - plane range reporting data structure  @xcite is important for our later developments , we briefly discuss it in the dual setting .",
    "let @xmath102 be a set of @xmath1 lines .",
    "given any point @xmath103 , the goal is to report all lines of @xmath102 that are above @xmath103 .",
    "an @xmath51-size data structure can be built in @xmath52 time that can answer each query in @xmath104 time , where @xmath105 is the number of lines above the query point @xmath103 @xcite .",
    "the data structure can be built as follows .",
    "let @xmath106 be the upper envelope of @xmath102 ( e.g. , see fig .",
    "[ fig : layers ] ) .",
    "we represent @xmath106 as an array of lines @xmath107 ordered as they appear on @xmath106 from left to right . for each line @xmath108",
    ", @xmath109 is its _ left neighbor _ and @xmath110 is its _",
    "right neighbor_. we partition @xmath102 into a sequence @xmath111 , of subsets , called _ layers _ ( e.g. , see fig .",
    "[ fig : layers ] ) .",
    "the first layer @xmath112 consists of the lines that appear on @xmath106 . for @xmath113 ,",
    "@xmath114 consists of the lines that appear on the upper envelope of the lines in @xmath115 .",
    "each layer @xmath114 is represented in the same way as @xmath106 . to answer a half - plane range reporting query on a point @xmath103 , let @xmath116 be the vertical line through @xmath103 .",
    "we first determine the line @xmath108 of @xmath117 whose intersection with @xmath116 is on the upper envelope of @xmath117 , by doing binary search on the array of lines of @xmath117 .",
    "then , starting from @xmath108 , we walk on the upper envelope of @xmath117 in both directions to report the lines of @xmath117 above the point @xmath103 , in linear time with respect to the output size .",
    "next , we find the line of @xmath118 whose intersection with @xmath116 is on the upper envelope of @xmath118 .",
    "we use the same procedure as for @xmath117 to report the lines of @xmath118 above @xmath103 .",
    "similarly , we continue on the layers @xmath119 , until no line is reported in a certain layer . by using fractional cascading @xcite , after determining the line @xmath108 of @xmath117 in @xmath59 time by binary search , the data structure @xcite can report all lines above @xmath103 in constant time each .",
    "for any vertical line @xmath120 , for each layer @xmath114 , denote by @xmath121 the line of @xmath114 whose intersection with @xmath120 is on the upper envelope of @xmath114 . by fractional cascading @xcite , we have the following lemma for the data structure @xcite .",
    "[ lem:30]_@xcite _ for any vertical line @xmath120 , after the line @xmath122 is known , we can obtain the lines @xmath123 in this order in @xmath124 time each .     into three layers : @xmath125 , @xmath126 , @xmath127 .",
    "the thick polygonal chain is the upper envelope of @xmath102 . ]",
    "in this section , we present our results for the uniform case . we first discuss our data structures for the unbounded case in section [ sec : unbounded ] , which will also be needed in our data structures for the bounded case in section [ sec : uniformbounded ] .",
    "further , the results in section [ sec : unbounded ] will also be useful in our data structures for the histogram case in section [ sec : nonuniform ]",
    ".    recall that in the uniform case @xmath81 is a set of lines .",
    "we first discuss the unbounded case where @xmath12 $ ] is unbounded and some techniques introduced here will also be used later for the bounded case . without loss of generality ,",
    "we assume @xmath43 , and the other case where @xmath44 can be solved similarly . recall that @xmath83 is the vertical line with @xmath8-coordinate @xmath31 .    for top-1 queries , by lemma [ lem:20 ] , we only need to maintain the upper envelope of @xmath81 , which can be computed in @xmath52 time and @xmath51 space . for each query ,",
    "it is sufficient to determine the intersection of @xmath83 with the upper envelope of @xmath81 , which can be done in @xmath59 time . next , we consider top-@xmath4 queries .",
    "given @xmath2 and @xmath4 , by lemma [ lem:20 ] , it suffices to find the @xmath4 lines of @xmath81 whose intersections with @xmath83 are the highest , and we let @xmath128 denote the set of the above @xmath4 lines . as preprocessing , we build the half - plane range reporting data structure ( see section [ sec : pre ] ) on @xmath81 , in @xmath52 time and @xmath51 space .",
    "suppose the layers of @xmath81 are @xmath129 .",
    "in the sequel , we compute the set @xmath128 .",
    "let the lines in @xmath128 be @xmath130 ordered from top to bottom by their intersections with @xmath83 .",
    "let @xmath131 be the line of @xmath132 which intersects @xmath83 on the upper envelope of the layer @xmath132 , for @xmath133 .",
    "we first compute @xmath134 in @xmath59 time by binary search on the upper envelope of @xmath135 . clearly , @xmath136 is @xmath134 .",
    "next , we determine @xmath137 .",
    "let the set @xmath138 consist of the following three lines : @xmath139 , the left neighbor ( if any ) of @xmath134 in @xmath135 , and the right neighbor ( if any ) of @xmath134 in @xmath135 .    [ lem:40 ] @xmath137 is the line in @xmath138 whose intersection with @xmath83 is the highest .    note that @xmath137 is the line of @xmath140 whose intersection with @xmath83 is the highest .",
    "we distinguish two cases :    1 .",
    "if @xmath137 is in @xmath135 , since the slopes of the lines of @xmath135 from left to right are increasing , @xmath137 must be a neighbor of @xmath136 .",
    "hence , @xmath137 must be either the left neighbor or the right neighbor of @xmath136 in @xmath135 .",
    "if @xmath137 is not in @xmath135 , then @xmath139 must be the line of @xmath141 whose intersection with @xmath83 is the highest . according to the definition of the layers of @xmath81 the upper envelope of @xmath142 is also the upper envelope of @xmath141 .",
    "therefore , @xmath139 is the line of @xmath141 whose intersection with @xmath83 is the highest .",
    "hence , @xmath137 must be @xmath143 .",
    "the lemma thus follows .",
    "we refer to @xmath138 as the _ candidate set_. by lemma [ lem:40 ] , we find @xmath137 in @xmath138 in @xmath124 time .",
    "we remove @xmath137 from @xmath138 , and below we insert at most three lines into @xmath138 such that @xmath144 must be in @xmath138 . specifically ,",
    "if @xmath137 is @xmath139 , we insert the following three lines into @xmath138 : @xmath145 , the left neighbor of @xmath139 , and the right neighbor of @xmath139 . if @xmath137 is the left ( resp . ,",
    "right ) neighbor @xmath120 of @xmath134 , we insert the left ( resp .",
    ", right ) neighbor of @xmath120 in @xmath135 into @xmath138 . by generalizing lemma [ lem:40 ]",
    ", we can show @xmath144 must be in @xmath138 ( the details are omitted ) .",
    "we repeat the same algorithm until we find @xmath146 . to facilitate the implementation",
    ", we use a heap to store the lines of @xmath138 whose `` keys '' in the heap are the heights of the intersections of @xmath83 and the lines of @xmath138 .",
    "[ lem : new50 ] the set @xmath128 can be found in @xmath74 time .    according to our algorithm",
    ", there are @xmath72 insertions and `` extract - max '' operations ( i.e. , finding the element of @xmath138 with the largest key and remove the element from @xmath138 ) on the heap @xmath138 . the size of @xmath138 is always bounded by @xmath72 during the algorithm . hence all operations on @xmath138 take @xmath147 time .",
    "further , after finding @xmath134 in @xmath59 time , due to lemma [ lem:30 ] , the lines that are inserted into @xmath138 can be found in constant time each .",
    "hence , the total time for finding @xmath128 is @xmath148 .",
    "we can improve the algorithm to @xmath60 time by using the selection algorithm in @xcite for sorted arrays .",
    "the key idea is that we can implicitly obtain @xmath149 sorted arrays of @xmath72 size each and @xmath128 can be computed by finding the largest @xmath4 elements in these arrays .",
    "the details are given in lemma [ lem : new60 ] .",
    "[ lem : new60 ] the set @xmath128 can be found in @xmath60 time .",
    "consider any layer @xmath132 .",
    "suppose the array of lines of @xmath132 is @xmath107 and let @xmath150 be the line @xmath131 .",
    "the intersections of the lines @xmath151 with @xmath83 are sorted in decreasing order of their heights , and the intersections of the lines @xmath152 with @xmath83 are also sorted in decreasing order of their heights . once @xmath150 is known , we can implicitly obtain the following two arrays @xmath153 and @xmath154 : the @xmath155-th element of @xmath153 ( resp . , @xmath154 )",
    "is the height of the intersection of @xmath156 ( resp . , @xmath157 ) and @xmath83 .",
    "since these lines are explicitly maintained in the layer @xmath132 , given any index @xmath155 , we can obtain the @xmath155-th element of @xmath153 ( resp . , @xmath154 ) in @xmath124 time .    to compute the set @xmath128",
    ", we first find the lines @xmath158 for @xmath159 , which can be done in @xmath59 time due to lemma [ lem:30 ] .",
    "consequently , we obtain the @xmath149 arrays @xmath160 and @xmath161 for @xmath162 , implicitly .",
    "in fact we only need to consider the first @xmath4 elements of each such array , and below we let @xmath160 and @xmath161 denote the arrays only consisting of the first @xmath4 elements .",
    "an easy observation is that the heights of the intersections of @xmath83 and the lines of @xmath128 are exactly the largest @xmath4 elements of @xmath163 .    in light of the above discussion , to compute @xmath128 , we do the following : ( 1 ) find the @xmath4-th largest element @xmath6 of @xmath164 ; ( 2 ) find the lines of @xmath164 whose intersections with @xmath83 have heights at least @xmath6 , which can be done in @xmath72 time by checking the above @xmath149 sorted arrays with @xmath6 in their index orders .",
    "below , we show that we can compute @xmath6 in @xmath72 time .",
    "recall that @xmath164 contains @xmath149 sorted arrays and each array has @xmath4 elements .",
    "further , for any array , given any index @xmath155 , we can obtain its @xmath155-th element in constant time .",
    "hence , we can find the @xmath4-th largest element of @xmath164 in @xmath72 time by using the selection algorithm given in @xcite for matrices with sorted columns ( each sorted array in our problem can be viewed as a sorted column of a @xmath165 matrix ) .",
    "the lemma thus follows .    hence , we obtain the following result .",
    "[ theo : uniformunboundedtopk ] for the uniform case , we can build in @xmath52 time an @xmath51 size data structure on @xmath0 that can answer each top-@xmath4 query with an unbounded query interval in @xmath166 time .    for the threshold query",
    ", we are given @xmath2 and a threshold @xmath6 .",
    "we again build the half - plane range reporting data structure on @xmath81 . to answer the query , as discussed in section [ sec : pre ]",
    ", we only need to find all lines of @xmath81 whose intersections with @xmath83 have @xmath85-coordinates larger than or equal to @xmath6 .",
    "we first determine the line @xmath134 by doing binary search on the upper envelope of @xmath135 .",
    "then , by lemma [ lem:30 ] , we find all lines @xmath167 whose intersections have @xmath85-coordinates larger than or equal to @xmath6 . for each @xmath97 with @xmath168",
    ", we walk on the upper envelope of @xmath132 , starting from @xmath131 , on both directions in time linear to the output size to find the lines whose intersections have @xmath85-coordinates larger than or equal to @xmath6 .",
    "hence , the running time for answering the query is @xmath61 .",
    "now we assume @xmath12 $ ] is bounded .",
    "consider any point @xmath13 .",
    "recall that @xmath10 is associated with an interval @xmath169 $ ] in the uniform case . depending on the positions of @xmath12",
    "$ ] and @xmath169 $ ] , we classify @xmath33 $ ] and the point @xmath10 into the following three types with respect to @xmath2 .",
    "l - type : : :    @xmath33 $ ] and @xmath10 are l - type if    @xmath170 .",
    "r - type : : :    @xmath33 $ ] and @xmath10 are r - type if    @xmath171 .",
    "m - type : : :    @xmath33 $ ] and @xmath10 are m - type if    @xmath172 .",
    "denote by @xmath173 , @xmath174 , and @xmath175 the sets of all @xmath83-type , @xmath176-type , and @xmath177-type of points of @xmath0 , respectively . in the following , for each kind of query , we will build an data structure such that the different types of points will be searched separately ( note that we will not explicitly compute the three subsets @xmath173 , @xmath174 , and @xmath175 ) .",
    "for each point @xmath13 , we refer to @xmath178 as the _ left endpoint _ of the interval @xmath33 $ ] and refer to @xmath179 as the _",
    "right endpoint_. for simplicity of discussion , we assume that no two interval endpoints of the points of @xmath0 have the same value .      for any point @xmath13 , denote by @xmath180 the set of the cdfs of the points of @xmath0 whose intervals have left endpoints larger than or equal to @xmath178 . again ,",
    "as discussed in section [ sec : pre ] we transform each cdf of @xmath180 to a line .",
    "we aim to maintain the upper envelope of @xmath180 for each @xmath13 .",
    "if we computed the @xmath1 upper envelopes explicitly , we would have an data structure of size @xmath181 . to reduce the space , we choose to use the persistent data structure @xcite to maintain them implicitly such that data structure size",
    "is @xmath51 .",
    "the details are given below .",
    "we sort the points of @xmath0 by the left endpoints of their intervals from left to right , and let the sorted list be @xmath182 . for each @xmath97 with @xmath183 , observe that the set @xmath184 has exactly one more line than @xmath185 . if we maintain the upper envelope of @xmath185 by a balanced binary search tree ( e.g. , a red - black tree ) , then by updating it we can obtain the upper envelope of @xmath184 by an insertion and a number of deletions on the tree , and each tree operation takes @xmath59 time .",
    "an easy observation is that there are @xmath51 tree operations in total to compute the upper envelopes of all sets @xmath186 .",
    "further , by making the red - black tree persistent @xcite , we can maintain all upper envelopes in @xmath52 time and @xmath51 space .",
    "we use @xmath187 to denote the above data structure .",
    "we can use @xmath187 to find the point of @xmath173 with the largest @xmath2-probability in @xmath59 time , as follows .",
    "first , we find the point @xmath188 such that @xmath189 .",
    "it is easy to see that @xmath190 .",
    "consider the unbounded interval @xmath191 $ ] .",
    "consider any point @xmath10 whose cdf is in @xmath185 .",
    "due to @xmath192 , we can obtain that @xmath14=\\pr[p\\in i']$ ] .",
    "hence , the point @xmath10 of @xmath185 with the largest value @xmath14 $ ] also has the largest value @xmath193 $ ] .",
    "this implies that we can instead use the unbounded interval @xmath194 as the query interval on the upper envelope of @xmath185 , in the same way as in section [ sec : unbounded ] .",
    "the persistent data structure @xmath187 maintains the upper envelope of @xmath185 such that we can find in @xmath59 time the point @xmath10 of @xmath185 with the largest value @xmath193 $ ] .",
    "similarly , we can build a data structure @xmath195 of @xmath51 space in @xmath52 time that can find the point of @xmath174 with the largest @xmath2-probability in @xmath59 time . to find the point of @xmath175 with the largest @xmath2-probability , the approach for @xmath173 and @xmath174 does not work because we can not reduce the query to another query with an unbounded interval .",
    "instead , we reduce the problem to a `` segment dragging query '' by dragging a line segment out of a corner in the plane , as follows .    for each point @xmath10 of @xmath0 , we define a point @xmath196 in the plane , and we say that @xmath10 _ corresponds to _ @xmath103 .",
    "similar transformation was also used in @xcite .",
    "let @xmath197 be the set of the @xmath1 points defined by the points of @xmath0 .",
    "for the query interval @xmath12 $ ] , we also define a point @xmath198 ( this is different from @xcite , where @xmath2 defines a point @xmath199 ) .",
    "if we partition the plane into four quadrants with respect to @xmath200 , then we have the following lemma .",
    "[ obser:10 ] the points of @xmath175 correspond to the points of @xmath197 that strictly lie in the second quadrant ( i.e. , the northwest quadrant ) of @xmath200 .",
    "consider any point @xmath13 .",
    "let @xmath196 be the point defined by @xmath10 .",
    "on the one hand , @xmath10 is in @xmath175 if and only if @xmath201 , i.e. , @xmath202 and @xmath203 .",
    "on the other hand , @xmath202 and @xmath203 if and only if @xmath103 is in the second quarter of @xmath198 .",
    "the lemma thus follows .",
    "let @xmath204 be the upwards ray originating from @xmath200 and let @xmath205 be the leftwards ray originating from @xmath200 .",
    "imagine that starting from the point @xmath200 and towards northwest , we drag a segment of slope @xmath3 with two endpoints on @xmath204 and @xmath205 respectively , and let @xmath206 be the point of @xmath197 hit first by the segment ( e.g. , see fig .",
    "[ fig : segdrag ] ) .     out of the corner at @xmath200",
    ": @xmath206 is the first point that will be hit by the segment . ]",
    "[ lem:50 ] the point of @xmath0 that defines @xmath206 is in @xmath175 and has the largest @xmath2-probability among all points in @xmath175 .",
    "first of all , by lemma [ obser:10 ] , @xmath206 must be in @xmath175 .",
    "consider any point @xmath103 in the second quadrant of @xmath200 , and let @xmath10 be the point of @xmath0 that defines @xmath103 .",
    "since the interval of @xmath10 contains the interval @xmath2 , we have @xmath207=\\frac{x_r - x_l}{x_r(p)-x_l(p)}$ ] .",
    "based on the definition of @xmath206 , @xmath206 is the point @xmath103 of @xmath197 in the second quadrant of @xmath200 that has the smallest value @xmath208 .",
    "therefore , @xmath206 is the point @xmath103 of @xmath197 in the second quadrant of @xmath200 that has the largest value @xmath209 .",
    "the lemma thus follows .",
    "based on lemma [ lem:50 ] , to determine the point of @xmath175 with the largest @xmath2-probability , we only need to solve the above query on @xmath197 by dragging a segment out of a corner .",
    "more specifically , we need to build a data structure on @xmath197 to answer the following _ out - of - corner segment - dragging queries _ : given a point @xmath103 , find the first point of @xmath197 hit by dragging a segment of slope @xmath3 from @xmath103 and towards the northwest direction with the two endpoints on the two rays @xmath210 and @xmath211 , respectively , where @xmath210 is the upwards ray originating from @xmath103 and @xmath211 is the leftwards ray originating from @xmath103 . by using mitchell s result in @xcite ( reducing the problem to a point location problem ) , we can build an @xmath51 size data structure on @xmath197 in @xmath52 time that can answer each such query in @xmath59 time .    for the uniform case",
    ", we can build in @xmath52 time an @xmath51 size data structure on @xmath0 that can answer each top-@xmath3 query in @xmath59 time .      to answer a top - k query",
    ", we will do the following .",
    "first , we find the top-@xmath4 points in @xmath173 ( i.e. , the @xmath4 points of @xmath173 whose @xmath2-probabilities are the largest ) , the top-@xmath4 points in @xmath174 , and the top-@xmath4 points in @xmath175 .",
    "then , we find the top-@xmath4 points of @xmath0 from the above @xmath212 points .",
    "below we build three data structures for computing the top-@xmath4 points in @xmath173 , @xmath174 , and @xmath175 , respectively .",
    "we first build the data structure for @xmath173 . again , let @xmath182 be the list of the points of @xmath0 sorted by the left endpoints of their intervals from left to right . we construct a complete binary search tree @xmath213 whose leaves from left to right store the @xmath1 intervals of the points @xmath182 . for each internal node @xmath214 ,",
    "let @xmath215 denote the set of points whose intervals are stored in the leaves of the subtree rooted at @xmath214 .",
    "we build the half - plane range reporting data structure discussed in section [ sec : pre ] on @xmath215 , denoted by @xmath216 . since the size of @xmath216 is @xmath217 , the total size of the data structure @xmath213 is @xmath52 , and @xmath213",
    "can be built in @xmath54 time .",
    "we use @xmath213 to compute the top-@xmath4 points in @xmath173 as follows . by the standard approach and using @xmath30 , we find in @xmath59 time a set @xmath218 of @xmath219 nodes of @xmath213 such that @xmath220 and no node of @xmath218 is an ancestor of another node .",
    "then , we can determine the top-@xmath4 points of @xmath173 in similarly as in section [ sec : unbounded ] .",
    "however , since we now have @xmath59 data structures @xmath216 , we need to maintain the candidate sets for all such @xmath216 s .",
    "specifically , after we find the top-1 point in @xmath216 for each @xmath221 , we use a heap @xmath138 to maintain them where the `` keys '' are the @xmath2-probabilities of the points .",
    "let @xmath10 be the point of @xmath138 with the largest key . clearly , @xmath10 is the top-1 point of @xmath173 ; assume @xmath10 is from @xmath216 for some @xmath221 .",
    "we remove @xmath10 from @xmath138 and insert at most three new points from @xmath216 into @xmath138 , in a similar way as in section [ sec : unbounded ] .",
    "we repeat the same procedure until we find all top-@xmath4 points of @xmath173 .    to analyze the running time , for each node @xmath221",
    ", we can determine in @xmath59 time the line in the first layer of @xmath216 whose intersection with @xmath83 is on the upper envelope of the first layer , and subsequent operations on @xmath216 each takes @xmath124 time due to fractional cascading .",
    "hence , the total time for this step in the entire algorithm is @xmath222 .",
    "however , we can do better by building a fractional cascading structure @xcite on the first layers of @xmath216 for all nodes @xmath214 of the tree @xmath213 . in this way , the above step only takes @xmath59 time in the entire algorithm , i.e. , do binary search only at the root of @xmath213 .",
    "in addition , building the heap @xmath138 initially takes @xmath59 time . note that the additional fractional cascading structure on @xmath213 does not change the size and construction time of @xmath213 asymptotically @xcite .",
    "the entire query algorithm has @xmath72 operations on @xmath138 in total and the size of @xmath138 is @xmath60 . hence , the total time for finding the top-@xmath4 points of @xmath173 is @xmath223 , which is @xmath74 by lemma [ lem : time ] .",
    "[ lem : time ] @xmath224 .    to simplify the notation ,",
    "let @xmath225 , and our goal is to prove @xmath226 . depending on whether @xmath227 , there are two cases .    1 .",
    "if @xmath228 , then @xmath229 , implying that @xmath230 . thus , @xmath231 + hence , we obtain that @xmath226 .",
    "2 .   if @xmath232 , then @xmath233 . + hence , we obtain that @xmath226 .",
    "the lemma thus follows .",
    "if @xmath73 , we have a better result in lemma [ lem:100 ] .",
    "note that comparing with lemma [ lem : new60 ] , we need to use other techniques to obtain lemma [ lem:100 ] since the problem here involves @xmath59 half - plane range reporting data structures @xmath216 while lemma [ lem : new60 ] only needs to deal with one such data structure .",
    "[ lem:100 ] if @xmath73 , we can compute the top-@xmath4 points in @xmath173 in @xmath72 time .",
    "we assume @xmath73 .",
    "recall that @xmath83 is the vertical line with @xmath8-coordinate @xmath31 .",
    "let @xmath218 be the set of @xmath59 nodes of @xmath213 as defined before .",
    "consider any node @xmath221 , which is associated with a half - plane range reporting data structure @xmath216 on the cdfs of the point set @xmath215 .",
    "let @xmath234 be the cdfs of the points of @xmath215 .",
    "let @xmath235 .",
    "our goal is to find the @xmath4 lines of @xmath236 whose intersections with @xmath83 are the highest , and denote by @xmath128 the above @xmath4 lines that we seek .",
    "we let @xmath237 be the layers of @xmath234 , and for each layer @xmath238 , denote by @xmath239 the line of @xmath238 whose intersection with @xmath83 is on the upper envelope of the layer @xmath238 .",
    "for each layer @xmath238 , we define two arrays @xmath240 and @xmath241 in the same way as in the proof of lemma [ lem : new60 ] ( we omit the details ) . for each node @xmath214",
    ", we define another array @xmath242 of size @xmath4 as follows : for each @xmath162 , the @xmath97-th element of @xmath242 is the height of the intersection of @xmath239 and @xmath83 . hence , the elements of @xmath242 are sorted in decreasing order .    our algorithm for computing @xmath128 has two main steps . in the first main step",
    ", we will find a set @xmath243 of the largest @xmath4 elements in @xmath244 . for each @xmath221 ,",
    "let @xmath245 be the number of elements of @xmath242 that are contained in @xmath243 , i.e. , the first @xmath245 elements of @xmath242 are in @xmath243 .",
    "an easy observation is that the heights of the intersections of @xmath83 and the sought lines of @xmath128 are the largest @xmath4 elements of @xmath246 , which contains @xmath149 sorted arrays . in the second main step",
    ", we will find the largest @xmath4 elements of @xmath164 and thus obtain the set @xmath128 .",
    "below , we show that the above two main steps can be done in @xmath72 time .",
    "we consider the first main step .",
    "for simplicity of discussion , we assume no two elements of @xmath247 are equal .",
    "first of all , as discussed above , in @xmath59 time we can determine the lines @xmath248 for all @xmath221 , and thus the first elements of all arrays @xmath242 for @xmath221 are determined .",
    "further , for each array @xmath242 , due to the fractional cascading , we can obtain the next element in constant time each , and in other words , we can obtain the first @xmath97 elements of @xmath242 in @xmath249 time . to compute the set @xmath243 ,",
    "i.e. , the largest @xmath4 elements of @xmath247 , since @xmath247 contains @xmath250 sorted arrays , one may want to use the selection algorithm in @xcite again , as in lemma [ lem : new60 ] .",
    "however , here we can not use that algorithm because for each array in @xmath247 , given any data structure , we can not obtain the corresponding element in constant time .",
    "instead , we propose the following approach .",
    "recall that @xmath251 . for simplicity of discussion ,",
    "we assume @xmath252 and @xmath253 .",
    "let @xmath254 .",
    "let @xmath138 be a max - heap .",
    "initially @xmath255 . during the algorithm",
    ", we will maintain a set @xmath102 of elements .",
    "initially @xmath256 , and after the algorithm stops , @xmath102 contains at most @xmath257 elements .",
    "first of all , for each array @xmath242 , we compute its first @xmath258 elements and then insert only the @xmath258-th element of @xmath242 into @xmath138 .",
    "now @xmath138 contains @xmath252 elements .",
    "we do an `` extract - max '' operation on @xmath138 , i.e. , remove the largest element from @xmath138 .",
    "suppose the element removed above is from @xmath242 for a node @xmath214 .",
    "then , we add the first @xmath258 elements of @xmath242 into @xmath102 .",
    "if @xmath259 , the algorithm stops ; otherwise , we compute the next @xmath258 elements of @xmath242 and insert only the @xmath260-th element of @xmath242 into @xmath138 .    in general ,",
    "suppose we do an `` extract - max '' operation on @xmath138 and let the removed element by the operation be the @xmath261-th element of the array @xmath242 for a node @xmath214 .",
    "then , we add the elements of @xmath242 with indices from @xmath262 to @xmath263 to @xmath102 .",
    "if @xmath259 , the algorithm stops ; otherwise , we compute the next @xmath258 elements of @xmath242 and insert the @xmath264$]-th element of @xmath242 into @xmath138 .",
    "after the algorithm stops , we do the following .",
    "consider any element in the current heap @xmath138 , and suppose it is the @xmath265-th element of the array @xmath242 for a node @xmath214 .",
    "then , according to our algorithm , the elements of @xmath242 with indices from @xmath266 to @xmath267 are not in @xmath102 , and we call these @xmath258 elements the _ red elements_. since the current @xmath138 has at most @xmath268 elements , there are at most @xmath269 red elements , and we let @xmath270 be the union of @xmath102 and all red elements .",
    "we claim that the largest @xmath4 elements of @xmath247 must be in @xmath270 , i.e. , @xmath271 .",
    "we prove the claim as follows .",
    "let @xmath272 be the element that is removed from @xmath138 in the last extract - max operation on @xmath138 in the above algorithm , i.e. , after @xmath272 is removed , the algorithm stops .",
    "let @xmath138 be the heap after the algorithm stops . according to our algorithm , since each array @xmath242 is sorted decreasingly , @xmath272 is the smallest element in @xmath102 . since @xmath259 and @xmath273 , any element of @xmath243 must be larger than @xmath272 .",
    "if @xmath274 , then the claim is proved .",
    "otherwise , suppose an element @xmath275 is in @xmath276 .",
    "since @xmath277 and all elements of @xmath138 are smaller than @xmath272 , @xmath275 must be a red element , and thus @xmath275 is in @xmath270 .",
    "the claim is proved .    in light of the above claim",
    ", @xmath243 can be easily obtained after we find the @xmath4-th largest element of @xmath270 .    in the sequel",
    ", we analyze the running time of the above algorithm for the first main step .",
    "recall that @xmath254 .",
    "first of all , the size of the heap @xmath138 is at most @xmath252 at any time during the algorithm .",
    "clearly , the algorithm will stop after @xmath278 extract - max operations .",
    "the number of insertions is at most @xmath279 .",
    "therefore , the running time of all operations on @xmath138 in the entire algorithm is @xmath280 , which is @xmath72 due to @xmath251 . on the other hand ,",
    "the number of red elements is at most @xmath281 , and thus @xmath282 , which is @xmath72 due to @xmath251 .",
    "notice that the elements of @xmath247 that have been computed during the entire algorithm are exactly those in @xmath270 , and thus the time for computing these elements is @xmath283 .",
    "finally , since @xmath284 , we can find the @xmath4-th largest element in @xmath270 in @xmath72 time by using the well - known linear time selection algorithm .    as a summary ,",
    "the first main step can find the set @xmath243 in @xmath72 time .",
    "the second main step is to compute the largest @xmath4 elements in @xmath164 , which contains @xmath149 arrays . as in the proof of lemma [ lem : new60 ]",
    ", we can obtain any arbitrary element of these arrays in constant time , without computing these arrays explicitly .",
    "hence , by using the same approach as in lemma [ lem : new60 ] , we can compute the largest @xmath4 elements of @xmath164 in @xmath72 time .",
    "consequently , the set @xmath128 can be obtained .",
    "the lemma thus follows .    to compute the top-@xmath4 points of @xmath174",
    ", we build a similar data structure @xmath285 , in a symmetric way as @xmath213 , and we omit the details . finally , to compute the top-@xmath4 points in @xmath175 , we do the following transformation . for each point @xmath13",
    ", we define a point @xmath286 in the 3-d space with @xmath8- , @xmath85- , and @xmath287-axes .",
    "let @xmath197 be the set of all points in the 3-d space thus defined .",
    "let the query interval @xmath2 define an unbounded query box ( or 3d rectangle ) @xmath288 .",
    "similar to lemma [ obser:10 ] in section [ sec : unbounded ] , the points of @xmath175 correspond exactly to the points of @xmath289 .",
    "further , the top-@xmath4 points of @xmath175 correspond to the @xmath4 points of @xmath289 whose @xmath287-coordinates are the largest .",
    "denote by @xmath290 the @xmath4 points of @xmath289 whose @xmath287-coordinates are the largest . below",
    "we build a data structure on @xmath197 for computing the set @xmath290 for any query interval @xmath2 and thus finding the top-@xmath4 points of @xmath175 .",
    "we build a complete binary search tree @xmath291 whose leaves from left to right store all points of @xmath197 ordered by the increasing @xmath8-coordinate . for each internal node @xmath214 of @xmath291",
    ", we build an auxiliary data structure @xmath216 as follows .",
    "let @xmath292 be the set of the points of @xmath197 stored in the leaves of the subtree of @xmath291 rooted at @xmath214 .",
    "suppose all points of @xmath292 have @xmath8-coordinates less than @xmath30 .",
    "let @xmath293 be the points of @xmath292 whose @xmath85-coordinates are larger than @xmath31 .",
    "the purpose of the auxiliary data structure @xmath216 is to report the points of @xmath294 in the decreasing @xmath287-coordinate order in constant time each after the point of @xmath295 is found , where @xmath295 is the point of @xmath293 with the largest @xmath287-coordinate . to achieve this goal",
    ", we use the data structure given by chazelle and guibas @xcite ( the one for subproblem p1 in section 5 ) , and the data structure is a _ hive graph _",
    "@xcite , which can be viewed as the preliminary version of the fractional cascading techniques @xcite . by using the result in @xcite",
    ", we can build such a data structure @xmath216 of size @xmath296 in @xmath297 time that can first compute @xmath295 in @xmath298 time and then report other points of @xmath294 in the decreasing @xmath287-coordinate order in constant time each . since the size of @xmath216 is @xmath299 ,",
    "the size of the tree @xmath291 is @xmath52 , and @xmath291 can be built in @xmath54 time .    using @xmath291",
    ", we find the set @xmath290 as follows .",
    "we first determine the set @xmath218 of @xmath59 nodes of @xmath291 such that @xmath300 consists of all points of @xmath197 whose @xmath8-coordinates less than @xmath30 and no point of @xmath218 is an ancestor of another point of @xmath218 .",
    "then , for each node @xmath221 , by using @xmath216 , we find @xmath295 , i.e. , the point of @xmath292 with the largest @xmath287-coordinate , and insert @xmath295 into a heap @xmath138 , where the key of each point is its @xmath287-coordinate .",
    "we find the point in @xmath138 with the largest key and remove it from @xmath138 ; denote the above point by @xmath301 .",
    "clearly , @xmath302 is the point of @xmath290 with the largest @xmath287-coordinate .",
    "suppose @xmath302 is in a node @xmath221 .",
    "we proceed on @xmath216 to find the point of @xmath292 with the second largest @xmath287-coordinate and insert it into @xmath138 .",
    "now the point of @xmath138 with the largest key is the point of @xmath290 with the second largest @xmath287-coordinate .",
    "we repeat the above procedure until we find all @xmath4 points of @xmath290 .    to analyze the query time , finding the set @xmath218 takes @xmath59 time . for each node @xmath221",
    ", the search for @xmath295 on @xmath216 takes @xmath59 time plus the time linear to the number of points of @xmath216 in @xmath290 .",
    "hence , the total time for searching @xmath295 for all vertices @xmath221 is @xmath222 time .",
    "similarly as before , we can remove a logarithmic factor by building a fractional cascading structure on the nodes of @xmath291 for searching such points @xmath295 s , in exactly the same way as in @xcite . with the help of the fractional cascading structure , all these @xmath295 s for @xmath221",
    "can be found in @xmath59 time .",
    "note that building the fractional cascading structure does not change the construction time and the size of @xmath291 asymptotically @xcite .",
    "in addition , building the heap @xmath138 initially takes @xmath59 time . in the entire algorithm",
    "there are @xmath72 operations on @xmath138 in total and the size of @xmath138 is always bounded by @xmath166 . therefore , the running time of the query algorithm is @xmath223 , which is @xmath74 by lemma [ lem : time ] .    using similar techniques as in lemma [ lem:100 ] ,",
    "we obtain the following result .",
    "[ lem:110 ] if @xmath251 , we can compute the top-@xmath4 points in @xmath175 in @xmath72 time .",
    "consider any point @xmath214 in @xmath218 , which is associated with a set @xmath292 and a data structure @xmath216 .",
    "define an array @xmath242 of size @xmath4 as follows : for each @xmath162 , the @xmath97-th element of @xmath242 is the @xmath97-th largest @xmath287-coordinate of the points of @xmath292 . as discussed above , in @xmath59 time we can obtain the first elements of @xmath242 for all @xmath221 , and after that , we can obtain the next element of each array @xmath242 in constant time each by using the data structure @xmath216 . our goal is to find the point set @xmath290 .",
    "let @xmath244 .",
    "an easy observation is that the @xmath287-coordinates of the points of @xmath290 are exactly the largest @xmath4 elements in @xmath247 . since @xmath251 ,",
    "computing the largest @xmath4 elements of @xmath247 can be done in @xmath72 time in the same way as the first main step of the algorithm in the proof of lemma [ lem:100 ] , and we omit the details .    the lemma is thus proved .",
    "we summarize our results for the top-@xmath4 queries below .",
    "[ theo : uniformtopk ] for the uniform case , we can build in @xmath54 time an @xmath52 size data structure on @xmath0 that can answer each top-@xmath4 query in @xmath72 time if @xmath73 and @xmath148 time otherwise .      to answer the threshold queries",
    ", we build the same data structure as in theorem [ theo : uniformtopk ] , i.e. , the three trees @xmath213 , @xmath291 , and @xmath285 . the tree @xmath213 is used for finding the points @xmath10 of @xmath173 with @xmath207\\geq \\tau$ ] ; @xmath285 is for finding the points @xmath10 of @xmath174 with @xmath207\\geq \\tau$ ] ; @xmath291 is for finding the points @xmath10 of @xmath175 with @xmath207\\geq \\tau$ ] .",
    "the three trees @xmath213 , @xmath285 , and @xmath291 are exactly the same as those for theorem [ theo : uniformtopk ] .",
    "we can compute them in @xmath54 time and @xmath52 space .",
    "below , we discuss the query algorithms on the three trees .",
    "let @xmath303 , @xmath304 , and @xmath305 be the number of points in @xmath173 , @xmath174 , and @xmath175 whose @xmath2-probabilities are at least @xmath6 , respectively .",
    "hence , @xmath306 .    to find the points @xmath10 of @xmath173 with @xmath207\\geq \\tau$ ] , we first determine the set @xmath218 of @xmath59 nodes of @xmath213 such that @xmath307 and no node of @xmath218 is an ancestor of another node of @xmath218 . recall that each node @xmath214 of @xmath213 is associated with a half - plane range reporting data structure @xmath216 . for each node @xmath221 , by using @xmath216 , we can find the points @xmath10 of @xmath215 with @xmath14\\geq \\tau$ ] in @xmath308 time , where @xmath309 is the output size .",
    "note that @xmath215 and @xmath310 are disjoint for any two nodes @xmath214 and @xmath311 of @xmath218 .",
    "hence , @xmath312 .",
    "as there are @xmath59 nodes in @xmath218 , it takes @xmath313 time to find all points @xmath10 of @xmath173 with @xmath14\\geq \\tau$ ] , and again the @xmath222 time factor can be reduced to @xmath59 by using fractional cascading @xcite .",
    "hence , the total query time is @xmath314 .",
    "we can use the similar approach to find all points @xmath10 of @xmath174 with @xmath14\\geq \\tau$ ] in @xmath315 time , by using @xmath285 .",
    "we omit the details .    finally , we find the points @xmath10 of @xmath175 with @xmath14\\geq \\tau$ ] , by using @xmath291 . as in section",
    "[ sec : uniformtopk ] , for each point @xmath13 , we define in the 3-d space a point @xmath316 .",
    "let @xmath197 be the set of all @xmath1 points defined above .",
    "let the interval @xmath12 $ ] and @xmath6 together define an unbounded 3-d box query @xmath317 .",
    "let @xmath318 .",
    "hence , the points @xmath10 of @xmath175 with @xmath14\\geq \\tau$ ] correspond to the points of @xmath290 , and thus @xmath319 .    by using the tree @xmath291",
    ", we can find @xmath290 in @xmath320 time , as follows .",
    "we first determine the set @xmath218 of @xmath219 nodes of @xmath291 such that @xmath300 consists of all points of @xmath197 whose @xmath8-coordinates are less than @xmath30 and no node of @xmath218 is an ancestor of another node of @xmath218 .",
    "consider any node @xmath221 .",
    "let @xmath293 be the points of @xmath292 whose @xmath85-coordinates are larger than @xmath31 , and @xmath295 be the point @xmath293 with the largest @xmath287-coordinate . recall that after @xmath295 is found @xmath216 can report other points of @xmath294 in the decreasing @xmath287-coordinate order in constant time each .",
    "hence after @xmath295 is known we can report the points of @xmath292 in the query box @xmath321 in time linear to the output size .",
    "again , with the help of fractional cascading , the nodes @xmath295 for all @xmath221 can be found in @xmath59 time .",
    "therefore , we can find all points of @xmath290 in @xmath320 time .",
    "in other words , with @xmath291 , we can find the points of of @xmath175 whose @xmath2-probabilities at least @xmath6 in @xmath320 time .",
    "hence , we obtain theorem [ theo : uniformthreshold ] .",
    "[ theo : uniformthreshold ] for the uniform case , we can build in @xmath54 time an @xmath52 size data structure that can answer each threshold query in @xmath53 time , where @xmath45 is the output size of the query .",
    "in this section , we present our data structures for the histogram case . in the histogram case ,",
    "the cdf of each point @xmath13 has @xmath18 pieces ; recall that we assumed @xmath18 is a constant , and thus @xmath81 is still a set of @xmath51 line segments .",
    "we first discuss our data structures for the unbounded case in section [ sec : unboundedhist ] and then present our results for the bounded case in section [ sec : boundedhist ] .",
    "again , we assume w.l.o.g . that @xmath43 .",
    "recall that @xmath83 is the vertical line with @xmath8-coordinate @xmath31 .",
    "note that lemmas [ lem:10 ] and [ lem:20 ] are still applicable .      for the top-1 queries , as in section [ sec : unbounded ] it is sufficient to maintain the upper envelope of @xmath81 .",
    "although @xmath81 now is a set of line segments , its upper envelope is still of size @xmath51 and can be computed in @xmath52 time @xcite . given the query interval @xmath2 , we can compute in @xmath59 time the cdf of @xmath81 whose intersection with @xmath83 is on the upper envelope of @xmath81 .",
    "[ theo : nonuniformtop1 ] in the histogram case , we can build in @xmath52 time an @xmath51 size data structure on @xmath0 that can answer each top-1 query with an unbounded query interval in @xmath59 time .      for the threshold query , as discussed in section [ sec : pre ] we only need to find the cdfs of @xmath81 whose intersections with @xmath83 have @xmath85-coordinates at least @xmath6 .",
    "let @xmath200 be the point @xmath322 on @xmath83 .",
    "a line segment is _ vertically above _",
    "@xmath200 if the segment intersects @xmath83 and the intersection is at least as high as @xmath200 . hence , to answer the threshold query on @xmath2 , it is sufficient to find the segments of @xmath81 that are vertically above @xmath200 .",
    "@xcite gave the following result on the _ segment - below - point queries_. for a set @xmath102 of @xmath51 line segments in the plane , a data structure of @xmath51 size can be computed in @xmath52 time that can report the segments of @xmath102 vertically _ below _ a query point @xmath103 in @xmath323 time , where @xmath105 is the output size . in our problem , we need a data structure on @xmath81 to solve the _ segments - above - point queries _ , which can be solved by using the same approach as @xcite .",
    "therefore , we can build in @xmath52 time an @xmath51 data structure on @xmath0 that can answer each threshold query with an unbounded query interval in @xmath53 time .",
    "[ theo : nonuniformthreshold ] in the histogram case , we can build in @xmath52 time an @xmath51 size data structure on @xmath0 that can answer each threshold query with an unbounded query interval in @xmath53 time , where @xmath45 is the output size of the query .      for the top-@xmath4 queries , we only need to find the @xmath4 segments of @xmath81 whose intersections with @xmath83 are the highest .",
    "to this end , we can slightly modify the data structure for the segment - below - point queries given in @xcite .",
    "the data structure in @xcite is a binary tree structure that maintains a number of sets of lines ( each such line contains a segment of @xmath81 ) . for each such set of lines",
    ", a half - plane range reporting data structure similar to that in section [ sec : pre ] is built , where the lower envelopes ( instead of the upper envelopes as we discussed in section [ sec : pre ] ) of the layers of the lines are maintained . for our purpose ,",
    "we replace it by our half - plane range reporting data structure in section [ sec : pre ] ( i.e. , maintain the upper envelopes ) . with this modification ,",
    "we can answer the segments - above - point queries in the following way .    consider a query point @xmath324 ( i.e. , the lower infinite endpoint of @xmath83 ) , and suppose we want to find the segments of @xmath81 vertically above @xmath103 , which are also the segments intersecting @xmath83 . by using the data structure @xcite",
    "modified as above , the query algorithm works as follows .",
    "first , with the help of fractional cascading , in @xmath59 time , the query algorithm will find @xmath59 half - plane range reporting data structures such that for each such data structure @xmath325 the segment intersecting @xmath83 on the upper envelope of @xmath325 is known .",
    "second , for each such half - plane range reporting data structure @xmath325 , from the above known segment intersecting @xmath83 , by using the fractional cascading and walking on the upper envelopes of the layers of @xmath325 , we can report all lines of @xmath325 higher than @xmath103 in constant time each .",
    "the first step takes @xmath59 time , and the second step takes @xmath326 time , where @xmath105 is the total output size .    for our problem , we only need to report the highest @xmath4 segments of @xmath81 that are vertically above @xmath103 .",
    "to this end , we will modify the query algorithm such that the segments of @xmath81 vertically above @xmath103 will be reported in order from top to bottom , and once @xmath4 segments are reported , we will terminate the algorithm .",
    "we use a heap @xmath138 in a similar way as in section [ sec : unbounded ] for the uniform case .",
    "specifically , in the first step , we find the @xmath59 half - plane range reporting data structures , and for each such data structure @xmath138 , the highest segment of @xmath325 intersecting @xmath83 is known . in the second step ,",
    "we build a heap @xmath138 on these @xmath59 segments where the keys are the @xmath85-coordinates of their intersections with @xmath83 .",
    "the segment in @xmath138 with the largest key must be the highest segment of @xmath81 intersecting @xmath83 .",
    "we remove the segment from @xmath138 , and let @xmath325 be the half - plane range reporting data structure that contains the segment . as in section [ sec : unbounded ] for the uniform case , we determine in constant time at most three segments from @xmath325 and insert them to @xmath138 . now",
    "the segment of @xmath138 with the largest key is the second highest segment of @xmath81 intersecting @xmath83 .",
    "we repeat the above procedure until we have reported @xmath4 segments .",
    "to analyze the running time , the first step takes @xmath59 time . in the second step",
    ", we have @xmath72 operations on @xmath138 and the segments that are inserted to @xmath138 can be found in constant time each by the range - reporting data structures .",
    "the size of the heap @xmath138 in the entire query algorithm is @xmath166 .",
    "hence , the running time of the query algorithm is @xmath327 , which is @xmath148 by lemma [ lem : time ] .",
    "similarly , if @xmath73 , we can answer the top-@xmath4 query in @xmath72 time , as follows . as discussed above , in @xmath59 time we find the @xmath59 half - plane range reporting data structures , and for each such data structure @xmath325 , the highest segment of @xmath325 intersecting @xmath83 is known .",
    "the answer to the top-@xmath4 query is the highest @xmath4 intersections of @xmath83 and the lines in these @xmath59 half - plane range reporting data structures .",
    "this is exactly the same situation as in lemma [ lem:100 ] , where we also have @xmath59 half - plane range reporting data structures .",
    "hence , the algorithm in lemma [ lem:100 ] is applicable here , which runs in @xmath72 time .    in summary",
    ", we obtain the following results .",
    "[ theo : nonuniformtopk ] we can build in @xmath52 time an @xmath51 size data structure on @xmath0 that can answer each top-@xmath4 query with an unbounded query interval in @xmath72 time if @xmath73 and @xmath148 time otherwise .      in this case , the query interval @xmath12 $ ] is bounded .",
    "for this case , agarwal _ et al . _",
    "@xcite built a data structure of size @xmath54 in @xmath55 expected time , which can answer each threshold query in @xmath77 time .",
    "we first briefly discuss this data structure ( refer to section 4 of @xcite for more details ) because our data structures for top-1 and top-@xmath4 queries also use some of their techniques .",
    "agarwal _ et al . _",
    "@xcite built a data structure ( a binary search tree ) , denoted by @xmath328 , which maintains a family of _ canonical sets _ of planes in 3d ( defined by the uncertain points of @xmath0 ) .",
    "consider any query interval @xmath12 $ ] with a threshold value @xmath6 .",
    "let @xmath329 be the point with coordinates @xmath330 in 3d , and let @xmath331 be the line through @xmath103 and parallel to the @xmath287-axis .",
    "using @xmath328 , one can determine a family @xmath332 of @xmath222 canonical sets of @xmath328 with the following property : each uncertain point @xmath10 defines one and only one plane in @xmath332 such that the @xmath287-coordinate of the intersection of the plane with @xmath331 is the probability @xmath14 $ ] .",
    "note that the canonical sets of @xmath332 are pairwise disjoint .    to answer the threshold query on @xmath2 and @xmath6 , it is sufficient to report the planes in each canonical set of @xmath332 that lie above the point @xmath329 . to this end , for each canonical set @xmath102 of @xmath328 , agarwal _ et al . _",
    "@xcite constructed a halfspace range - reporting data structure given by afshani and chan @xcite on the planes in @xmath102 in @xmath333 space and @xmath334 expected time , such that given any point @xmath103 , one can report the planes of @xmath102 above @xmath103 in @xmath335 time , where @xmath177 is the output size . in this way , because there are @xmath222 canonical sets in @xmath332 , the threshold query can be answered in @xmath336 time .",
    "the total space of @xmath328 including the halfspace range - reporting data structures is @xmath54 and @xmath328 can be built in @xmath55 expected time .      consider the top-1 query on the above query interval @xmath2 . to answer the query",
    ", it suffices to find the plane in @xmath332 whose intersection with @xmath331 is the highest . to this end , it is sufficient to know the intersection of @xmath331 with the upper envelope of each canonical set of @xmath332 .",
    "therefore , for each canonical set @xmath102 of @xmath328 , instead of constructing a halfspace range - reporting data structure , we compute the upper envelope of the planes of @xmath102 @xcite and build a point location data structure @xcite on the upper envelope , which can be done in @xmath333 space and @xmath334 time . in this way , for each canonical set @xmath102 of @xmath332 , in @xmath59 time we can determine the plane intersecting @xmath331 in the upper envelope of @xmath102 .",
    "hence , the top-1 query can be answered in @xmath64 time since @xmath332 has @xmath222 canonical sets .    comparing with the original data structure in @xcite ,",
    "since we spend @xmath333 space and @xmath334 time on each canonical set @xmath102 of @xmath328 , the entire data structure can be constructed in @xmath337 space and @xmath55 ( deterministic ) time .",
    "we summarize the result for the top-1 queries in the follow theorem .",
    "[ theo : nonuniformtop1bounded ] in the histogram case , we can build in @xmath55 time an @xmath338 size data structure on @xmath0 that can answer each top-1 query with a bounded query interval in @xmath64 time .      consider the top-@xmath4 query on the query interval @xmath2 . to answer the query",
    ", it suffices to find the @xmath4 planes in @xmath332 whose intersections with @xmath331 are the highest . to this end , for each canonical set @xmath102 of @xmath328 , we build a _",
    "@xmath155-highest plane _ data structure given by afshani and chan @xcite on the planes of @xmath102 in @xmath333 space and @xmath334 expected time , such that given any integer @xmath155 and any query line @xmath83 parallel to the @xmath287-axis , the @xmath155 highest planes of @xmath102 at @xmath83 can be found in @xmath339 time . comparing with the original data structure in @xcite ,",
    "since we spend asymptotically the same space and time on each canonical set of @xmath328 , our data structure can be constructed in @xmath337 space and @xmath55 expected time .    to answer the top-@xmath4 query on @xmath2 , one straightforward way works as follows .",
    "for each canonical set @xmath102 of @xmath332 , by using the @xmath155-highest plane data structure with @xmath340 , we compute the highest @xmath4 planes of @xmath102 at @xmath331 . since there are @xmath222 canonical sets in @xmath332 , the above computes @xmath341 planes , and among them the highest @xmath4 planes at @xmath331 are the answer to the top-@xmath4 query .",
    "the query time is @xmath342 . in the following , we present an improved query algorithm with time @xmath78 .    in the following discussion , for simplicity ,",
    "whenever we refer to the relative order the planes ( e.g. , highest , lowest , higher , lower ) , we refer to their intersections with the line @xmath331 . for example , by `` a plane is higher than another plane '' , we mean that the first plane has a higher intersection with @xmath331 than the second plane . for ease of exposition",
    ", we assume the intersection points of the planes of @xmath332 with @xmath331 are distinct .",
    "note that @xmath332 is a family of canonical sets ; but by slightly abusing the notation , when we say `` a plane of @xmath332 '' , we really mean that the plane is in a canonical set of @xmath332 .",
    "we make use of some idea from lemma [ lem:100 ] although the details are quite different .",
    "our algorithm has two steps : a main algorithm and a post - processing algorithm .",
    "we discuss the main algorithm first .",
    "let @xmath343 , and thus @xmath344 .",
    "let @xmath345 be the canonical sets of @xmath332 .",
    "for each canonical set",
    "@xmath346 , let @xmath347 denote the set of the highest @xmath348 planes of @xmath346 for @xmath349 , and we let @xmath350 for @xmath351 . for each @xmath346 ,",
    "our main algorithm maintains a subset @xmath352 and an integer @xmath353 , such that @xmath354 .",
    "we also maintain a max - heap @xmath138 that contains the lowest plane in the subset @xmath355 for each @xmath356 .",
    "hence , the size of @xmath138 is @xmath222 .",
    "the `` keys '' of the planes in @xmath138 are the @xmath287-coordinates of their intersections with @xmath331 .",
    "in addition , our algorithm maintains an integer @xmath357 , which is the size of a set @xmath176 of planes .",
    "before the main algorithm stops , @xmath358 ( after the main algorithm stops , the definition of @xmath176 is slightly different ; see the details below ) .",
    "note that our algorithm does not maintain @xmath176 explicitly , and we use @xmath176 only to argue the correctness of the algorithm . during the main algorithm ,",
    "@xmath357 will get increased , and the main algorithm stops once @xmath359 ( at which moment we have identified a set of @xmath78 planes , and among them the highest @xmath4 planes are the answer to our top-@xmath4 query , which will be found later by the post - processing algorithm ) .    initially , for each canonical set @xmath346 of @xmath332 , by using the @xmath155-highest plane data structure with @xmath360 , we compute @xmath361 , and further we find the lowest plane in @xmath355 and insert it into @xmath138 ; the above can be done in @xmath362 time , which is @xmath363 time due to @xmath364 . also , initially we set @xmath365 ( @xmath176 is implicitly set to @xmath366 ) , and set @xmath367 for each @xmath97 with @xmath368 .",
    "next , we do an `` extract - max '' operation on @xmath138 to find the highest plane in @xmath138 and remove it from @xmath138 .",
    "suppose the above plane is from a canonical set @xmath346 for some @xmath97 .",
    "then , we let @xmath369 and set @xmath370 .",
    "further , by using the @xmath155-highest plane data structure with @xmath371 , we compute @xmath372 , and then we find the lowest plane in @xmath355 and insert it into @xmath138 ; again , the above can be done in @xmath363 time .",
    "finally , we update @xmath373 .",
    "in general , we do an extract - max operation on the current @xmath138 and suppose the removed plane is from a canonical set @xmath346 for some @xmath97 .",
    "we let @xmath374 ( note that @xmath375 , and thus this just adds those planes of @xmath376 that are not in @xmath377 to @xmath176 ) , and set @xmath378 . again",
    ", we do not explicitly maintain @xmath176 but explicitly maintain @xmath357 .",
    "if @xmath359 , then we stop the main algorithm .",
    "otherwise , by using the @xmath155-highest plane data structure with @xmath379 , we compute @xmath380 ( and the previous @xmath355 is discarded ) , and further we find the lowest plane in @xmath381 and insert it into @xmath138 ; again , the above can be done in @xmath363 time .",
    "note that for ease of exposition , we assume @xmath382 ( otherwise , we can solve the problem by similar techniques with more tedious discussions )",
    ". finally , we increase @xmath353 by one .",
    "the above finishes the main algorithm .",
    "after it stops , let @xmath383 .",
    "let @xmath384 be the set of @xmath4 highest planes in @xmath332 , i.e. , @xmath384 is the answer to our top-@xmath4 query on @xmath2 .",
    "we have the following lemma .    [ lem:120 ] @xmath385 , @xmath386 , and @xmath387 .",
    "we first show @xmath385 .",
    "suppose the last extract - max operation on @xmath138 in the main algorithm removes a plane from the canonical set @xmath388 for @xmath389 .",
    "hence , the algorithm stops after we have @xmath390 .",
    "thus , @xmath391 .",
    "consider any other canonical set @xmath346 with @xmath392 . according to our algorithm",
    ", it always holds that @xmath393 .",
    "therefore , we have the following : @xmath394    since @xmath375 for any @xmath97 , we obtain that @xmath385 .",
    "next , we show that @xmath395 .    indeed ,",
    "since the algorithm stops right after @xmath396 and @xmath390 , the original value of @xmath357 before the above increasing is less than @xmath4 . in other words , @xmath397 . for each @xmath346 of @xmath332 , if @xmath367 , then @xmath398 and @xmath399 ; otherwise , @xmath400 .",
    "therefore , we obtain @xmath401 because @xmath402 .    finally , we prove @xmath386 .",
    "let @xmath403 denote the plane removed by the last extract - max operation on @xmath138 in the main algorithm .",
    "we claim that @xmath403 is the lowest plane in @xmath176 .",
    "we prove the claim below .    according to our algorithm , the planes removed by the extract - max operations on @xmath138",
    "follow the order from high to low .",
    "consider any plane @xmath404 .",
    "to prove the claim , it is sufficient to show that @xmath403 is not higher than @xmath405 . according to our algorithm , the first time @xmath405 is added in @xmath176 must be due to an operation on @xmath176 : @xmath374 after an extract - max operation removes a plane @xmath406 from @xmath138 and @xmath406 is from a canonical set @xmath346 .",
    "this implies that @xmath407 . according to our algorithm",
    ", @xmath406 is the lowest plane in the above @xmath376 , and thus , @xmath406 is not higher than @xmath405 . on the other hand ,",
    "since @xmath403 is the last plane removed by the extract - min operations , @xmath403 is not higher than @xmath406 .",
    "therefore , @xmath403 is not higher than @xmath405 , and the above claim is proved .",
    "consider any plane @xmath408 .",
    "to show @xmath386 , it suffices to prove @xmath409 . if @xmath405 is in @xmath176 , then since @xmath385 , @xmath409 is true . below we assume @xmath410 , and thus @xmath411 .",
    "note that @xmath408 implies that there are at most @xmath412 planes of @xmath332 higher than @xmath405 . since @xmath413 and @xmath403 is the lowest plane in @xmath176 , @xmath405 must be higher than @xmath403 since otherwise all planes in @xmath176 would be higher than @xmath405 , contradicting with that there are at most @xmath412 planes higher than @xmath405 .",
    "assume @xmath405 is in a canonical set @xmath346 for some @xmath97 .",
    "recall that @xmath403 is from the canonical set @xmath414 .",
    "note that all planes of @xmath414 higher than @xmath403 are in @xmath415 . by our definition of @xmath176 ,",
    "since @xmath405 is higher than @xmath403 and @xmath410 , we can obtain @xmath392 . according to our algorithm ,",
    "after the algorithm stops , @xmath138 contains a plane @xmath417 from @xmath346 , and @xmath417 is the lowest plane in @xmath376 .",
    "recall that , @xmath418 . this implies that all planes of @xmath346 higher than @xmath417 are in @xmath419 . since @xmath403 is removed by an extract - min operation and after the operation @xmath417 is still in @xmath138 , @xmath403 must be higher than @xmath417 . because @xmath405 is higher than @xmath403 , @xmath405 is higher than @xmath417 .    in summary ,",
    "the above discussion obtains the following : @xmath405 is in @xmath346 ; @xmath405 is higher than @xmath417 ; all planes of @xmath346 higher than @xmath417 are in @xmath419 .",
    "thus , we obtain that @xmath405 is in @xmath419 .",
    "therefore , we conclude that @xmath386 , and the lemma follows .    based on lemma [ lem:120 ] ,",
    "if we have the set @xmath419 explicitly , then we can compute @xmath384 in additional @xmath78 time by using the linear time selection algorithm @xcite .",
    "however , the above main algorithm does not explicitly compute @xmath419 , but it has maintained @xmath353 for each @xmath356 . since @xmath420 , we can compute @xmath419 by using a @xmath155-highest plane query with @xmath421 on each canonical set @xmath346 of @xmath332 .",
    "the following lemma gives the running time of our entire top-@xmath4 query algorithm .",
    "the time complexity of our top-@xmath4 query algorithm is @xmath78 .",
    "we first analyze the main algorithm , whose running time mainly depends on the time of the @xmath155-highest plane queries and the time of the operations on the heap @xmath138 .",
    "we first give a bound on the time of the @xmath155-highest plane queries , with the help of lemma [ lem:120 ] .",
    "note that after each @xmath155-highest plane query in the main algorithm , we always find the lowest plane in the output planes of the query , whose time is only linear to the number of output planes and is upper bounded by the above query time . in the following , we focus on analyzing the time of the @xmath155-highest plane queries .",
    "consider any canonical set @xmath356 . according to our algorithm , for each @xmath422 with @xmath423 , the main algorithm performs a @xmath155-highest plane query on @xmath346 with @xmath424 to compute @xmath347 , which takes @xmath425 time ( we ignore the @xmath426 factor in the query time because @xmath427 ) .",
    "hence , the total time of the @xmath155-highest plane queries on @xmath346 in the main algorithm is @xmath428 .",
    "note that @xmath429    recall that @xmath430 .",
    "hence , the total time on the @xmath155-highest plane queries in the entire main algorithm is @xmath431 , which is @xmath432 by lemma [ lem:120 ] .",
    "next , we analyze the time we spent on the heap @xmath138 . recall that the size of @xmath138 is @xmath222 .",
    "initially , we build @xmath138 on @xmath222 planes , which can be done in @xmath222 time . later in the algorithm ,",
    "the operations on @xmath138 include the extract - max and insertion operations .",
    "we need to figure out how many operations were performed on @xmath138 in the main algorithm .",
    "consider any extract - max operation on @xmath138 , and suppose the removed plane is from set @xmath346 .",
    "then , after the operation , we have @xmath374 , and since @xmath433 , the above increases @xmath176 by at least @xmath426 planes .",
    "after that , there is at most one insertion operation on @xmath138 .",
    "since the main step stops once @xmath434 , the total number of extract - max operations is at most @xmath435 .",
    "the number of insertion operations is also at most @xmath435 . since @xmath436 , each operation on @xmath138 takes @xmath437 time .",
    "the total time on @xmath138 is @xmath438 .",
    "therefore , the total time of the main algorithm is @xmath432 .",
    "finally , we analyze the running time of the post - processing step , which computes @xmath419 and finds the highest @xmath4 planes in @xmath419 .",
    "computing @xmath419 is done by doing a @xmath155-highest plane query with @xmath439 on each set @xmath346 .",
    "therefore , as above , the total time is at most @xmath440 , which is @xmath432 .",
    "finding the highest @xmath4 planes in @xmath419 takes @xmath431 time by using the linear time selection algorithm @xcite .",
    "thus , the total time of our top-@xmath4 query algorithm is @xmath432 .",
    "the above discussion leads to the following theorem .",
    "[ theo : nonuniformtopkbounded ] we can build in @xmath55 expected time an @xmath54 size data structure on @xmath0 that can answer each top-@xmath4 query with a bounded query interval in @xmath441 time .",
    "note that the planes reported by our top-@xmath4 query algorithm are not in any sorted order .",
    "in this paper we present a number of data structures for answering a variety of range queries over uncertain data in one dimensional space . in general , our data structures have linear or nearly linear sizes and can support efficient queries .",
    "while it would be interesting to develop better solutions , an interesting but challenging open problem is whether we can generalize our techniques to solve the corresponding problems in higher dimensions , for which only heuristic results have been proposed @xcite .",
    "the authors would like to thank sariel har - peled for several insightful comments , and for suggesting us the canonical set idea ( somewhat similar to that in @xcite ) , which leads us to the solutions for the histogram bounded case of the problem .",
    "p.  agrawal , o.  benjelloun , a.  das sarma , c.  hayworth , s.  nabar , t.  sugihara , and j.  widom .",
    "trio : a system for data , uncertainty , and lineage . in _ proc . of the 32nd international conference on very large data bases",
    "_ , pages 11511154 , 2006 .",
    "r.  cheng , j.  chen , m.  mokbel , and c.  chow .",
    "probabilistic verifiers : evaluating constrained nearest - neighbor queries over uncertain data . in _",
    "ieee international conference on data engineering ( icde ) _ , pages 973982 , 2008 .",
    "r.  cheng , y.  xia , s.  prabhakar , r.  shah , and j.s .",
    "efficient indexing methods for probabilistic threshold queries over uncertain data . in _ proc . of the 30th international conference on very large data bases _ , pages 876887 , 2004 .",
    "t.  s. jayram , a.  mcgregor , s.  muthukrishnan , and e.  vee .",
    "estimating statistical aggregates on probabilistic data streams . in _",
    "acm sigmod - sigact - sigart symposium on principles of database systems ( pods ) _ , pages 243252 , 2007 .",
    "y.  tao , r.  cheng , x.  xiao , w.k .",
    "ngai , b.  kao , and s.  prabhakar .",
    "indexing multi - dimensional uncertain data with arbitrary probability density functions . in _ proc . of the 31st international conference on very large data bases ( vldb ) _ , pages 922933 , 2005 ."
  ],
  "abstract_text": [
    "<S> given a set @xmath0 of @xmath1 uncertain points on the real line , each represented by its one - dimensional probability density function , we consider the problem of building data structures on @xmath0 to answer range queries of the following three types for any query interval @xmath2 : ( 1 ) top-@xmath3 query : find the point in @xmath0 that lies in @xmath2 with the highest probability , ( 2 ) top-@xmath4 query : given any integer @xmath5 as part of the query , return the @xmath4 points in @xmath0 that lie in @xmath2 with the highest probabilities , and ( 3 ) threshold query : given any threshold @xmath6 as part of the query , return all points of @xmath0 that lie in @xmath2 with probabilities at least @xmath6 . </S>",
    "<S> we present data structures for these range queries with linear or nearly linear space and efficient query time . </S>"
  ]
}