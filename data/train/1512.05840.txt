{
  "article_text": [
    "digital newsroom is rapidly changing . with the introduction of social media ,",
    "the new public sphere offers greater mutual visibility and diversity .",
    "however , a full understanding of how the new system operates is still out of complete grasp . in particular , the problem is made further challenging with data sparsity , where individual records about responses to any particular news event is limited .",
    "we believe mapping the development of analytical framework that integrates different domains of data can significantly help us characterize behavioral patterns of journalists and understand how they operate in social media according to different news topics and events .",
    "figure 1 outlines a possible data mapping .",
    "ingesting data from published feeds by news organizations and map corresponding journalists to social media and observe their activities will undoubtedly enable a clearer mapping of individual behaviors related to news event progression .",
    "based on this data mapping , we can incorporate general response by normal users on social media to construct a data feedback loop .",
    "-0.1 in        -0.2 in    0.1 in        -0.1 in    we are interested in aggregated behaviors as well as individual behaviors that interact with the ecosystem .",
    "a way to achieve this aim is to design a mathematical model that can map the latent process that characterize individual behavior as well as feature analysis on aggregate level . in this work ,",
    "we propose a model building on top of probabilistic matrix factorization , called poisson factorization machine , to describe the latent factors that behavioral features are embedded upon .",
    "in particular , we first create a data design matrix by inferring links between data instances from different domains .",
    "figure 2 illustrates such a representation , where each row is a news article published by news organization .",
    "the columns denote the features associated with the story , which can include topics written in the news , name entities mentioned , as well as social media engagement associated with journalists twitter accounts and general users interactions .",
    "then , we perform our poisson factorization machine to learn a lower dimensional embedding of the latent factors that map to these features .    using this framework",
    ", we can answer several key questions : * query- * given any data instance or any subset of features as queries , can we understand potentially related instances and features by the learned latent feature mapping ?",
    "* factor analysis- * can we describe the learned latent factors in a meaningful way to better understand relations between features ?",
    "* prediction- * given new data instances , can we use its latent dimensional embedding to predict the instance s response at some unobserved features ( e.g. features that have not yet occurred so we can not record ) .",
    "our proposed factorization model offers several advantages .",
    "first , the model bases on a hierarchical generative structure with poisson distribution , which provide better statistical strength sharing among factors and model human behavior data better , being discrete output .",
    "second , our model generalizes earlier works on poisson matrix factorization by extending the model to include response variables to enable a form of supervised guidance .",
    "we offer batch as well as online variational inference to accommodate large - scale datasets .",
    "in addition , we show that it is possible to stack units of pfm to make a deep architecture .",
    "third , we further extend our model to incorporate multi - way interactions among features .",
    "this provides natural feature grouping for more interpretable feature exploration .",
    "[ submission ]",
    "in this section , we describe the details for the proposed poisson factorization machine , where we discuss the formal definition of the model , and inference with prediction procedures .",
    "consider a set of observed news articles , where for each article , we have mapped the authors to their respective twitter handles and extracted a set of behavior patterns around the time of posting the article .",
    "we denote this data matrix by @xmath0 @xmath1 @xmath2 , where each entry @xmath3 is a d dimensional feature associated with that article .",
    "also , we have response variables @xmath4 @xmath5 with total of c response types . in this work , we set c = 1 , but the reasoning can easily apply to the higher dimension cases .",
    "imagine there are @xmath6 \\{1,@xmath7,@xmath8 } latent factors from the matrix factorization model .",
    "we define the poisson factorization machine with the following generative process : @xmath9 where @xmath10 is the @xmath11th latent factor mapping for feature vector , @xmath12 denotes the factor weights .",
    "note the hyperparameters of the model @xmath13 , and @xmath14 .",
    "here , @xmath15 is a scalar used to tune the weights . to generate the response variable , for each instance , we use the corresponding factor weights as input for normal linear model for regression .",
    "figure shows the graphical model . in this work",
    ", we leave @xmath16 predefined , and update other hyperparameters during model fitting .",
    "notice the introduction of auxiliary variable @xmath17 .",
    "this is employed to facilitate posterior variational inference by helping us to develop closed coordinate ascent updates for the parameters .",
    "more specifically , by the superposition theorem [ 4 ] , the poisson distribution dictated by @xmath18 can be composed of @xmath8 independent poisson variables @xmath19 where @xmath20 .",
    "effectively , when we are looking at the data likelihood , it can be viewed as marginalizing @xmath17 ( if we denote all hidden parameters by @xmath21 ) , but keep in mind that having @xmath17 not marginalized maintains flexibility for inference later on :    @xmath22    the intuition for selecting poisson model comes from the statistical property that poisson distribution models count data , which is more suitable for sparse behavioral patterns than traditional gaussian - based matrix factorization .",
    "also , poisson factorization avoids the problem of data sparsity since it penalizes 0 less strongly than gaussian distribution , which makes it desirable for weakly labeled data[1 ] .",
    "the selection of gamma prior as originally suggested by [ 3][5 ] .",
    "notice that response variable need not be gaussian , and can vary depending on the nature of inputting dataset .",
    "in fact , since our proposed learning and inference procedure is very similar as in [ 1 ] , our model can generalize to generalized linear models , which include other distributions such as poisson , multinomial , etc .",
    "0.1 in    = [ circle , minimum size = 7 mm , thick , draw = black!80 , node distance = 12 mm ] = [ -latex , thick ] = [ rectangle , draw = black!100 ] ( a ) [ label = below:@xmath23 ; ( theta ) [ right = of a , label = below:@xmath24 ; ( y ) [ right = of theta , label = below:@xmath25 ; ( eta ) [ right = of y , label = below:@xmath26 ; ( x ) [ above = of y , label = below:@xmath27 ; ( beta ) [ right = of x , label = below:@xmath28 ; ( b ) [ right = of beta , label = below:@xmath29 ; ( a ) edge [ connect ] ( theta ) ( theta ) edge [ connect ] ( x ) ( theta ) edge [ connect ] ( y ) ( eta ) edge [ connect ] ( y ) ( beta ) edge [ connect ] ( x ) ( b ) edge [ connect ] ( beta ) ; ; ; ; ; ; ;    -0.1 in      for posterior inference , we adopt variational em algorithm , which was employed in supervised topic model [ 1 ] . after showing the derivation , we will also present a stochastic variational inference analogue for the inference algorithm .",
    "first , we consider a fully factorized variational distribution for the parameters : @xmath30 for the above variational distribution , we choose the following distributions to reflect the generative process in the poisson factorization model : @xmath31 the log likelihood for the data features is calculated and bounded by : @xmath32 - \\mathbb{e_q}[ln\\ q(\\theta , \\beta ) ] \\\\ & = \\mathbb{e_q}[ln\\ p(\\theta , \\beta , x |",
    "\\pi ) ] + h(q)\\end{aligned}\\ ] ] where @xmath33 is the entropy of the variational distribution @xmath34 and the resulting lower bound is our evidence lower bound ( elbo ) .      the elbo lower bounds the log - likelihood for all article items in the data . for the e - step in variational em procedure",
    ", we run variational inference on the variational parameters . in the m - step",
    ", we optimize according to the elbo with respect to model parameters @xmath15 ( with @xmath35 , @xmath36 held fixed ) , as well as regression parameters @xmath37 and @xmath38 .",
    "* variational e - step * to calculate the updates , we try to first further break down @xmath39 $ ] and draw analogy to earlier works on poisson matrix factorization [ 2][3][5][7 ] to illustrate how we can directly leverage on previous results to derive coordinate ascent update rules for the proposed model . notice that : @xmath40 = \\mathbb{e_q}[ln\\ p(\\beta | b , b ) ] \\\\+ \\mathbb{e_q}[ln\\ p(\\theta | a , ac ) ]   + \\sum_{n=1}^n { \\mathbb{e_q}[ln\\ p(z | x , \\theta , \\beta)]}\\\\ + \\sum_{n=1}^n{\\mathbb{e_q}[ln\\ p(x | \\theta , \\beta)]+\\mathbb{e_q}[ln\\ p(y | \\theta , \\eta",
    ", \\sigma ) ] } \\end{gathered}\\ ] ] in fact , for variational parameters , when taking partial derivatives , the expression is identical to batch inference algorithm in ordinary poisson matrix factorization , so we arrive at the following updates for @xmath41 : @xmath42)}{\\sum_{k=1}^k{exp(\\mathbb{e_q}[ln\\ \\theta_{ik}\\beta_{kd}])}}\\ ] ] @xmath43 } \\\\",
    "\\nu_{k , d } & = b + \\displaystyle \\sum_{i=1}^n{x_{id}\\phi_{idk } } \\\\",
    "\\lambda_{k , d } & = b + \\displaystyle \\sum_{i=1}^n{\\mathbb{e_q}[\\theta_{ik}]}\\end{aligned}\\ ] ] expectations for @xmath44 $ ] and @xmath45 $ ] are @xmath46 and @xmath47 respectively ( where @xmath48 is the digamma function ) .    * variational m - step * in the m - step , we maximize the article - level elbo with respect to @xmath15 , @xmath37 , and @xmath38 .",
    "we update @xmath15 according to @xmath49 \\ \\ \\",
    "\\rightarrow \\ \\ \\ c^{-1 } = \\frac{1}{nk}\\sum_{i , k } \\mathbb{e_q}[\\theta_{ik}].\\end{aligned}\\ ] ] for regression parameters , we take partial derivatives with respect to @xmath37 and @xmath38 , yielding for @xmath37 : @xmath50y_i - \\mathbb e[\\frac{\\eta^t\\theta_i\\theta_i^t\\eta_i}{2 } ] \\\\ & = \\frac{1}{\\sigma}\\displaystyle \\sum_{i=1}^n \\mathbb e[\\theta_i ] y_i-\\eta^t\\mathbb e[\\theta_i\\theta_i^t ] \\end{aligned}\\ ] ] where the latter term @xmath51 $ ] is evaluated as @xmath52 setting the partial derivative to zero and we arrive at the updating rule for @xmath37 , as we collapse all instances @xmath53 into vector : @xmath54\\big)^{-1 } \\mathbb e[\\theta]^t y\\ ] ] for derivative of @xmath38 , we have : @xmath55y_i - \\mathbb e[\\frac{\\eta^t\\theta_i\\theta_i^t\\eta_i}{2 } ] \\rightarrow \\\\ \\sigma & = \\frac{1}{n}\\big \\{y^ty - y^t\\mathbb e[\\theta]\\big(\\mathbb e[\\theta^t\\theta]\\big)^{-1 } \\mathbb e[\\theta]^t y \\big\\}\\end{aligned}\\ ] ] to complete the procedure , we iterate between updating and optimizing parameters by following e - step and m - step until convergence . algorithm 1 outlines the entire process",
    ".      batch variational em algorithm might take multiple passes at the dataset before reaching convergence . in real world applications",
    ", this may not be desirable since social media data is more dynamic and large in scale .",
    "recently , hoffman et al .",
    "developed stochastic variational inference where each iteration we work with a subset of data instead , averaging over the noisy variational objective @xmath56 .",
    "we optimize by iterating between global variables and local variables .",
    "the global objective is based on @xmath57 , and at each iteration @xmath58 , with data subset @xmath59,we have : @xmath60 + \\mathbb e_q[ln\\ p(\\beta ) ] + h(q)}.\\ ] ] the main difference in the inference procedure for stochastic variational inference is in the updating part , where parameters @xmath15 , @xmath37 , and @xmath38 are updated with the average empirically according to the mini - batch samples ( where @xmath61 , @xmath62 ) : @xmath63 \\\\ \\eta^{(t ) } & = \\big(\\mathbb e[\\widehat{\\theta}^t\\widehat{\\theta}]\\big)^{-1 } \\mathbb e[\\widehat{\\theta}]^t \\widehat{y } \\\\",
    "\\sigma^{(t ) } & = \\frac{1}{|b_t|}\\big \\{\\widehat{y}^t\\widehat{y } -\\widehat{y}^t\\mathbb e[\\widehat{\\theta}]\\big(\\mathbb e[\\widehat{\\theta}^t\\widehat{\\theta}]\\big)^{-1 } \\mathbb e[\\widehat{\\theta}]^t \\widehat{y } \\big\\}\\end{aligned}\\ ] ] as for @xmath57 , since it is also global variable , we update by taking a natural gradient step based on the fisher information matrix of variational distribution @xmath64 .",
    "we have the variational parameters for @xmath57 updated as : @xmath65}\\big)\\end{aligned}\\ ] ] where @xmath66 is a positive interpolation factor and ensuring convergence .",
    "we adopt @xmath67 for @xmath68 and @xmath69 $ ] which is shown to correspond to natural gradient step in information geometry with stochastic optimization[3][5 ] .",
    "algorithm 2 summaries the stochastic variational inference procedure .      for",
    "any given query @xmath3 or @xmath70 , we can infer the corresponding latent factor distribution by computing @xmath71 $ ] as well as @xmath72 $ ] given that @xmath73 $ ] is already learned by the model .",
    "prediction follows by combining both latent factors to generate the response .",
    "determine instance set @xmath74 for iteration :      notice that we can enforce each @xmath75 to be an @xmath76 dimensional vector .",
    "in this way , we create an f - way interaction among latent factors .",
    "we can further impose a sparse deterministic prior @xmath77 with binary entries where each data entry encodes whether the two latent factors should interact .",
    "we can also approach by using a diffusion tree prior , which consider a hierarchy of prior interactions . under this construct",
    ", we can flatmap the tensor data structure to two dimensional structure and apply the factor interactions to encode a higher level of feature dependency .",
    "given the development of poisson factorization machines , here we show that it is possible to stack layers of latent gamma units on top of pfm to introduce a deep architecture . in particular , for latent factors @xmath57 and @xmath78 , a layering ( for layer l ) parent variables @xmath79 with corresponding weights @xmath80 are fed with link function @xmath81 specified by @xmath82 . as described in the recent work on deep exponential families [ 6 ] with exponential family in the form @xmath83 , we can condition the sufficient statistics , the mean to be particular , controlled by the link function @xmath84 via the gradient of the log - normalizer .",
    "@xmath85 = \\bigtriangledown_{\\eta}a(g_l(z_{l+1}^tw_{l , k})).\\]]figure 4 shows the schematics of stacking layers of latent variables on pfm latent states .    *",
    "gamma variables * the gamma distribution is an exponential family distribution with the probability density controlled by natural parameters @xmath86 where @xmath87 the link functions for @xmath88 and @xmath57 are respectively @xmath89 where @xmath90 denotes the shape parameter at layer @xmath91 , and @xmath92 is simply drawn from a gamma distribution .",
    "= [ circle , minimum size = 7 mm , thick , draw = black!80 , node distance = 12 mm ] = [ -latex , thick ] = [ rectangle , draw = black!100 ] ( a ) [ ] pfm ; ( theta ) [ above = of a , label = right:@xmath93 ; ( y ) [ left = of theta , label = below:@xmath80 ] ; ( eta ) [ above = of theta , label = right:@xmath95 ; ( x ) [ left = of eta , label = below:@xmath96 ; ; ; ( theta ) edge [ connect ] ( a ) ( y ) edge [ connect ] ( theta ) ( eta ) edge [ connect ] ( theta ) ( x ) edge [ connect ] ( eta ) ; ; ;      * inference * for inference , since all variables in each layer follows precisely for the inference procedures derived earlier , we focus on the stacking variables .",
    "the elbo for the generative process now includes layer - wise @xmath97 and @xmath17 . for each stacking @xmath17 and corresponding @xmath97",
    ", we have : @xmath98 for parameter updates , taking the gradient for the variational bound @xmath99 can be used to update each stacking layer s parameters , respectively ."
  ],
  "abstract_text": [
    "<S> newsroom in online ecosystem is difficult to untangle . with prevalence of social media , interactions between journalists and individuals become visible , but lack of understanding to inner processing of information feedback loop in public sphere leave most journalists baffled . </S>",
    "<S> can we provide an organized view to characterize journalist behaviors on individual level to know better of the ecosystem ? to this end , </S>",
    "<S> i propose poisson factorization machine ( pfm ) , a bayesian analogue to matrix factorization that assumes poisson distribution for generative process . </S>",
    "<S> the model generalizes recent studies on poisson matrix factorization to account temporal interaction which involves tensor - like structure , and label information . </S>",
    "<S> two inference procedures are designed , one based on batch variational em and another stochastic variational inference scheme that efficiently scales with data size . </S>",
    "<S> an important novelty in this note is that i show how to stack layers of pfm to introduce a deep architecture . </S>",
    "<S> this work discusses some potential results applying the model and explains how such latent factors may be useful for analyzing latent behaviors for data exploration . </S>"
  ]
}