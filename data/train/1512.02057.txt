{
  "article_text": [
    "the algorithmic randomness ( or kolmogorov complexity ) @xmath12 of a binary string @xmath13 is defined as the length of its shortest compression .",
    "more precisely , @xmath12 is the length of the shortest program on a universal turing machine that generates @xmath13 and stops then @xcite . in a seminal paper ,",
    "bennett @xcite proposed to consider @xmath12 as the _ thermodynamic entropy _ of a microscopic state of a physical system when @xmath13 describes the latter with respect to some standard binary encoding after sufficiently fine discretization of the phase space .",
    "this assumes an ` internal ' perspective ( followed here ) , where the microscopic state is perfectly known to the observer .",
    "although @xmath12 is in principle uncomputable , it can be estimated from the boltzmann entropy in many - particle systems , given that the microscopic state is typical in a set of states satisfying some macroscopic constraints @xcite .",
    "that is , in practice one needs to rely on more conventional definitions of physical entropy .",
    "from a theoretical and fundamental perspective , however , it is appealing to have a definition of entropy that neither relies on missing knowledge like the statistical shannon / von - neumann entropy @xcite nor on the separation between microscopic vs.  macroscopic states which becomes problematic on the mesoscopic scale like the boltzmann entropy @xcite . for imperfect knowledge of the microscopic state",
    ", zurek @xcite considers thermodynamic entropy as the sum of statistical entropy and kolmogorov complexity @xcite , which thus unifies the statistical and the algorithmic perspective of physical entropy .    to discuss how @xmath12 behaves under hamiltonian dynamics , notice that dynamics on a continuous space is usually not compatible with discretization , which immediately introduces also statistical entropy in addition to the algorithmic term particularly for chaotic systems @xcite in agreement with standard entropy increase by coarse - graining @xcite .",
    "remarkably , however , @xmath12 can also increase by applying a one - to - one map @xmath14 on a _ discrete _ space @xcite .",
    "then @xmath15 is the tightest upper bound for @xmath16 that holds for the general case . for a system starting in a simple initial state @xmath13 and evolving by the repeated application of some simple map @xmath17 , the description of @xmath18 essentially amounts to describing @xmath19 and zurek derives logarithmic entropy increase until the scale of the recurrence time is reached @xcite .",
    "although logarithmic growth is rather weak @xcite , it is worth mentioning that the arrow of time here emerges from assuming that the system starts in a _",
    "simple _ state .",
    "we will later argue that this is just a special case of the general assumption that it starts in a state that is independent of @xmath14 .",
    "the fact that @xmath20 depends on the turing machine could arguably spoil its use in physics , but , in the spirit of deutsch s idea that the laws of physics determine the laws of computation @xcite , future research may define a more physical version of @xmath20 by computation models whose elementary steps directly use physically realistic particle interactions , see e.g. @xcite .",
    "moreover , _ quantum _",
    "thermodynamics @xcite should rather rely on _ quantum _ kolmogorov complexity @xcite .",
    "reichenbach s principle @xcite states that every statistical dependence between two variables @xmath21 must involve some sort of causation : either direct causation ( @xmath5 causes @xmath6 or vice - versa ) or a common cause for both @xmath5 and @xmath6 .",
    "conversely , variables without causal relation are statistically independent .",
    "surprisingly , however , statistical ensembles are not necessarily required for drawing causal conclusions . as argued in ref .",
    "@xcite , two binary words @xmath22 , representing two causally disconnected objects should also be algorithmically independent , where algorithmic dependence is measured by @xcite @xmath23 as common in algorithmic information theory @xcite , the plus sign above ( in)equalities or signs denote errors of order @xmath24 , i.e. , they do not depend on the length of the binary strings @xmath22 .",
    "further , @xmath25 denotes the conditional kolmogorov complexity , i.e. , the length of the shortest program that generates @xmath26 from @xmath27 ( denoting the shortest compression of @xmath28 ) .",
    "thus , @xmath29 is the number of bits saved when @xmath26 and @xmath28 are compressed jointly rather than independently , or , equivalently , the number of bits that the description of @xmath26 can be shortened when the shortest description of @xmath28 is known .",
    "remarkably , algorithmic information can be used to infer whether @xmath5 causes @xmath6 ( denoted by @xmath30 ) or @xmath6 causes @xmath5 from their joint distribution , given that exactly one of the alternatives is true . if @xmath0 and @xmath1 are ` independently chosen by nature ' and thus causally unrelated , their algorithmic mutual information is also zero @xcite . here",
    ", it is implicitly assumed , for sake of simplicity , that the joint probability distribution is known and that it is computable .",
    "this is , for instance the case if we replace ` distribution ' with the empirical frequencies obtained via sufficiently fine binning of some observed data set with sufficiently large sampling .",
    "first , consider linear non - gaussian additive noise models @xcite .",
    "then , the joint distribution @xmath4 of two random variables @xmath21 admits the linear model @xmath31 where @xmath32 and @xmath33 is an unobserved noise term that is statistically independent of @xmath5 .",
    "are called statistically independent if @xmath34 , which is stronger than being uncorrelated , i.e. , @xmath35=\\exp [ z ] \\exp [ w]$ ] .",
    "] whenever @xmath5 or @xmath33 is non - gaussian , it follows that for every model of the form @xmath36 , the noise term @xmath37 and @xmath6 are statistically dependent , although they may be uncorrelated .",
    "that is , except for gaussian variables , a linear model with independent noise can hold in one direction at most . within that context , ref .",
    "@xcite infers the direction with additive independent noise to be the causal one . to justify this reasoning , ref .",
    "@xcite argues that whenever holds , the densities of @xmath38 and @xmath39 are related by the differential equation @xmath40 therefore , knowing @xmath39 enables a short description of @xmath38 .",
    "whenever @xmath38 has actually high description length , we should thus reject @xmath41 as a causal explanation .",
    "second , consider the information - geometric approach to causal inference @xcite .",
    "assume that @xmath5 and @xmath6 are random variables with values in @xmath42 $ ] , deterministically related by @xmath43 and @xmath44 , where @xmath45 is a monotonically increasing one - to - one mapping of @xmath42 $ ] . ref .",
    "@xcite proposes to infer @xmath30 whenever the derivative of @xmath45 and the probability density of @xmath8 satisfy @xmath46 and infer @xmath41 if the integral above is positive .",
    "the reason is that negative values are typical if @xmath8 is generated by a random procedure that is independent of @xmath45 @xcite , while positive values require @xmath47 to be particularly large in regions where @xmath45 has large slope .",
    "then , knowing @xmath45 ( here describing the conditional distribution @xmath48 ) constrains the distribution of @xmath5 @xcite and thus makes its description shorter .",
    "to provide a unifying foundation connecting thermodynamics and causal inference we propose the following postulate :    [ post : aim ] let @xmath13 be the initial state of a system and @xmath49 a mechanism mapping @xmath13 to a final state . if the preparation of @xmath13 is done without any knowledge of @xmath49 , then @xmath50 that is , knowing @xmath13 does not enable a shorter description of @xmath49 and vice versa .    the postulate is entailed by the assumption that there is no algorithmic dependence in nature without an underlying causal relation . by overloading notation ,",
    "we have identified mechanism and state with their encodings into binary strings .",
    "generalizations of algorithmic mutual information for infinite strings can be found in @xcite , which then allows to apply postulate  [ post : aim ] to continuous physical state spaces . here ,",
    "however , we consider finite strings describing states after sufficiently fine discretizations of the state space instead , neglecting issues from chaotic systems @xcite for sake of conciseness .",
    "to show the first consequence of postulate , consider a physical system whose state space is a finite set @xmath51 . assuming that the dynamics @xmath14 is a bijective map of @xmath51",
    ", it follows that the entropy can not decrease :    [ thm : entropyincr ] if the dynamics of a system is a one - to - one mapping @xmath14 of a discrete set @xmath51 of states then postulate  [ post : aim ] implies that the kolmogorov complexity can never decrease when applying @xmath14 to the initial state @xmath13 , i.e. , @xmath52    algorithmic independence of @xmath13 and @xmath14 amounts to @xmath53 . for any known bijection @xmath14",
    ", @xmath13 can be computed from @xmath54 and vice versa implying that @xmath55 .",
    "thus , @xmath56 , concluding the proof .",
    "@xmath14 may also be considered as the @xmath19-fold concatenation of the same map @xmath17 , where @xmath17 has negligible description length .",
    "then , @xmath57 amounts to @xmath58 .",
    "theorem  [ thm : entropyincr ] then implies that @xmath59 whenever @xmath19 and @xmath13 are algorithmically independent .",
    "that is , while ref .",
    "@xcite derives entropy increase for a simple initial state @xmath13 , we have derived it for all states @xmath13 that are independent of @xmath19 .    to further illustrate theorem  [ thm : entropyincr ] , consider a toy model of a physical system consisting of @xmath60 cells , each being occupied or not with a particle , see figure  [ fig : caordered ] .",
    "its state is described by a binary word @xmath13 with @xmath61 digits . for generic @xmath13",
    ", we have @xmath62 , while figure  [ fig : caordered ] , left , shows a simple state where all particles are in the left uppermost corner containing @xmath63 cells .",
    "a description of this state @xmath13 consists essentially of describing @xmath64 and @xmath65 ( up to negligible extra information specifying that @xmath64 and @xmath65 describe the size of the occupied region ) , which requires @xmath66 bits .",
    "left : cellular automaton starting in a state with small description length .",
    "right : the state @xmath67 obtained from @xmath13 by application of the dynamical law @xmath14 . ]",
    "consider now that the dynamical evolution @xmath14 transforms @xmath13 into @xmath68 where @xmath67 looks ` more generic ' , as shown in figure  [ fig : caordered ] , right . in principle",
    ", we can not exclude that @xmath67 is equally simple as @xmath13 due to some non - obvious pattern .",
    "however , excluding this possibility as unlike , theorem  [ thm : entropyincr ] rules out any scenario where @xmath67 is the initial state and @xmath13 the final state of _ any _ bijective mapping @xmath14 that is algorithmically independent of @xmath67 .",
    "the transition from @xmath13 to @xmath67 can be seen as a natural model of a mixing process of a gas , as described by popular toy models like lattice gases @xcite .",
    "these observations are consistent with standard results of statistical mechanics saying that mixing is the typical behaviour , while de - mixing requires some rather specific tuning of microscopic states . here",
    "we propose to formalize specific by means of algorithmic dependencies between the initial state and the dynamics .",
    "this view does not necessarily generate novel insights for typical scenarios of statistical physics , but it introduces a link to crucial concepts in the field of causal inference .",
    "since applying to closed systems reproduces the standard thermodynamic law of non - decrease of entropy , it is appealing to state algorithmic independence for closed system dynamics only and then obtain conditions under which the independence for open system follows .",
    "let @xmath14 be a one - to - one map transforming the initial joint state @xmath69 of system and environment into the final state @xmath70 . for fixed @xmath71 , define the open system dynamics @xmath72 .",
    "if @xmath13 is algorithmically independent of the pair @xmath73 ( which is true , for instance when @xmath74 is negligible and @xmath13 and @xmath14 are independent ) , independence of @xmath13 and @xmath49 follows because algorithmic independence of two strings @xmath75 implies independence of @xmath76 whenever @xmath77 can be computed from @xmath78 via a program of length @xmath24 @xcite .",
    "further , we can extend the argument above to statistical ensembles : consider @xmath79 systems with identical state space @xmath51 , each coupled to an environment with identical state space @xmath80 .",
    "let @xmath81 be the initial state of the @xmath82th copy .",
    "then @xmath83 and @xmath84 define empirical distributions @xmath85 on @xmath51 and @xmath86 on @xmath80 , respectively , counting the number of occurrences in the respective @xmath79-tuple . for fixed @xmath86 , the dynamics",
    "@xmath14 acting on each copy @xmath87 defines a conditional distribution for the final state @xmath88 of one copy , given its initial state @xmath89 via @xmath90 where the sum runs over all @xmath71 with @xmath91 for some @xmath92 .",
    "if @xmath93 is algorithmically independent of @xmath94 , we can conclude that @xmath85 and @xmath95 are algorithmically independent , because they are derived from two algorithmically independent objects via a program of length @xmath24 . defining the variable @xmath96 by the initial state of one copy @xmath51 and @xmath97 as the final state , we have thus derived the algorithmic independence of @xmath0 and @xmath1 .",
    "notice that it is not essential in the reasoning above that cause and effect describe initial and final states of the same physical system , one could as well consider a tripartite instead of a bipartite system .",
    "to illustrate how the independence of @xmath0 and @xmath1 is inherited from algorithmic independence of initial state and dynamics of a closed system , figure  [ fig : wall ]     physical system generating a non - linear deterministic causal relation : a particle travelling towards a structured wall with momentum orthogonal to the wall , where it is backscattered in a slightly different direction .",
    "@xmath26 and @xmath28 denote the positions where @xmath98 crosses a vertical line before and after the scattering process , respectively . ]",
    "shows a simple 2-dimensional system with a particle @xmath98 travelling towards a wall @xmath99 perpendicular to the momentum of @xmath98 .",
    "@xmath98 crosses a line @xmath100 parallel to @xmath99 at some position @xmath101 $ ] .",
    "let the surface of @xmath99 be structured such that @xmath98 hits the wall with an incident angle that depends on its vertical position",
    ". then @xmath98 crosses @xmath100 again at some position @xmath28 .",
    "assume that @xmath100 is so close to @xmath99 that the mapping @xmath102 is one - to - one .",
    "also , assume that @xmath103 is mapped to @xmath103 and @xmath104 to @xmath104 and the experiment is repeated with particles having the same momenta but with different positions such that @xmath26 is distributed according to some probability density @xmath47 .",
    "it is plausible that the initial distribution of momenta and positions do not contain information about the structure of @xmath99 , while the final one does . due to theorem  [ thm : entropyincr ] ,",
    "the scattering process increases the kolmogorov complexity of the state .",
    "further , this process is thermodynamically irreversible for every thermodynamic machine that has no access to the structure of @xmath99 .",
    "we now focus on a restricted aspect of this physical process , namely the one leading from @xmath47 to @xmath105 via the function @xmath45 , so we can directly apply the information - geometric approach to causal inference @xcite .",
    "again , the initial state @xmath47 is algorithmically independent of @xmath45 , while @xmath105 contains information about @xmath45 ( and thus about @xmath106 ) .",
    "hence , algorithmic dependence indicates the time ( and causal ) direction of the process .",
    "notice that the shannon entropy of @xmath105 is smaller than the entropy of @xmath47 because the function @xmath45 will typically make the distribution less uniform @xcite , while making it more uniform requires fine - tuning @xmath45 relative to @xmath47 .",
    "this could lead to misleading conclusions such as inferring the time direction from @xmath105 to @xmath47 .",
    "this is of particular relevance in scenarios where simple criteria like entropy increase / decrease are inapplicable , at least without accounting for the description of the entire physical system ( that often may be not available , e.g. , if the momentum of the particle is not measured ) .",
    "the example above suggests how the algorithmic independence provides a new tool for the inference of time direction in such scenarios .",
    "already reichenbach linked asymmetries between cause and effect to the arrow of time when he argued that the statistical dependence patterns induced by causal structures @xmath107 ( common cause ) vs.  @xmath108 ( common effect ) naturally emerges from the time direction of appropriate mixing processes @xcite . in this work",
    "we provide a new foundational principle describing additional asymmetries that appear when algorithmic rather than only statistical information is taken into account . as a consequence it follows naturally the non - decrease of algorithmic entropy and the independence between @xmath10 and @xmath11 , thus providing further relations between thermodynamics and causal inference .",
    "intuitively , our principle resembles the standard way to obtain markovian dynamics of open systems , coupling a system to a statistically independent environment @xcite . in this sense",
    ", our principle can be understood as a notion of markovianity that is stronger in two respects : first , the initial state of the system is not only statistically but also algorithmically independent of the environment , and second , it is also algorithmically independent of the dynamical law .",
    "it thus provides a useful new rationale for finding the most plausible causal explanation for given observations arising in study of open systems .",
    "furthermore , given the recent connections between the phenomenon of quantum nonlocality @xcite with algorithmic information @xcite and causality  @xcite , our results may also point new directions for research in the foundations of quantum physics .",
    "the authors would like to thank johan berg and philipp geiger for helpful remarks on an earlier version of the manuscript .",
    "rc acknowledges financial support from the excellence initiative of the german federal and state governments ( grants zuk 43 & 81 ) , the us army research office under contracts w911nf-14 - 1 - 0098 and w911nf-14 - 1 - 0133 ( quantum characterization , verification , and validation ) , the dfg ( gro 4334 & spp 1798 ) .",
    "j.  peters , d.  janzing , a.  gretton , and b.  schlkopf . detecting the direction of causal time series . in _ proceedings of the 26th international conference on machine learning , montreal ,",
    "acm international conference proceeding series _ ,",
    "volume 382 , pages 801808 , new york , ny , usa , 2009 .",
    "pickup , z.  pan , d.  wei , y.  shih , c.  zhang , a.  zisserman , b.  schlkopf , and w.t .",
    "freeman . seeing the arrow of time . in _",
    "computer vision and pattern recognition ( cvpr ) , 2014 ieee conference on _ , pages 20432050 , june 2014 .",
    "y.  kano and s.  shimizu .",
    "causal inference using nonnormality . in _ proceedings of the international symposium on science of modeling , the 30th anniversary of the information criterion _ , pages 261270 , tokyo , japan ( 2003 ) .",
    "p.  daniusis , d.  janzing , j.  m. mooij , j.  zscheischler , b.  steudel , k.  zhang , and b.  schlkopf .",
    "inferring deterministic causal relations . in _ proceedings of the 26th annual conference on uncertainty in artificial intelligence ( uai ) _ ,",
    "pages 143150 .",
    "auai press , ( 2010 ) .",
    "d.  janzing , b.  steudel , n.  shajarisales , and b.  schlkopf . .",
    "in v.  vovk , papadopolous h. , and a.  gammerman , editors , _ measures of complexity _ , festschrift for alexey chervonencis , pages 253265 .",
    "springer verlag , heidelberg ( 2015 ) ."
  ],
  "abstract_text": [
    "<S> we postulate a principle stating that the initial condition of a physical system is typically algorithmically independent of the dynamical law . </S>",
    "<S> we argue that this links thermodynamics and causal inference . on the one hand </S>",
    "<S> , it entails behaviour that is similar to the usual arrow of time . on the other hand </S>",
    "<S> , it motivates a statistical asymmetry between cause and effect that has recently postulated in the field of causal inference , namely , that the probability distribution @xmath0 contains no information about the conditional distribution @xmath1 and vice versa , while @xmath2 may contain information about @xmath3 .    </S>",
    "<S> drawing causal conclusions from statistical data is at the heart of modern scientific research . </S>",
    "<S> while it is generally accepted that _ active _ interventions to a system ( e.g. randomized trials in medicine ) reveal causal relations , statisticians have widely shied away from drawing causal conclusions from _ passive _ observations . meanwhile , however , the increasing interdisciplinary field of causal inference has shown that also the latter is possible even without information about time order if appropriate assumptions that link causality and statistics are made  @xcite , with applications in biology  @xcite , psychology  @xcite , and economy  @xcite . </S>",
    "<S> more recently , also foundational questions of quantum physics have been revisited in light of the formal language and paradigms of causal inference  @xcite .    </S>",
    "<S> remarkably , recent results from causal inference have also provided new insights about the thorny issue of the arrow of time . </S>",
    "<S> contrary to a wide - spread belief , the joint distribution @xmath4 of two variables @xmath5 and @xmath6 sometimes indicates whether @xmath5 causes @xmath6 or vice versa @xcite . </S>",
    "<S> more conventional methods rely on conditional independencies and thus require statistical information of at least @xmath7 observed variables @xcite . </S>",
    "<S> the idea behind the new approach is that if @xmath5 causes @xmath6 , @xmath8 contains no information about @xmath9 and vice versa . like the asymmetries between cause and effect , </S>",
    "<S> similar asymmetries between past and future are also manifest even in stationary time series @xcite which can sometimes be used to infer the direction of empirical time series ( e.g. in finance or brain research ) or to infer the time direction of movies  @xcite . </S>",
    "<S> altogether , these results suggest a deeper connection for the asymmetries between cause vs.  effect and past vs.  future . </S>",
    "<S> in particular , a physical toy model relating such asymmetries to the usual thermodynamic arrow of time has been proposed  @xcite .    motivated by these insights , we propose a foundational principle for both types of asymmetries , basically stating that the initial state of a physical system and the dynamical law to which it is subjected to should be algorithmically independent . </S>",
    "<S> as we show , it implies for a closed system the non - decrease of physical entropy if the latter is identified with the kolgomorov complexity , in agreement with ref .  </S>",
    "<S> @xcite . </S>",
    "<S> similarly , the independence of @xmath10 and @xmath11 postulated in causal inference naturally follows , if we identify cause and effect with the initial and final states of a physical system . </S>",
    "<S> moreover , we employ our principle to understand open system dynamics and apply it to a toy model representing typical cause - effect relations . </S>"
  ]
}