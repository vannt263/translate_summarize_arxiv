{
  "article_text": [
    "we introduce a new optimization methods for approximating a global minimum of composite objective function @xmath3 @xmath4 where @xmath5 is smooth and @xmath6 is convex lower semicontinuous .",
    "this class of problems is rich enough to encompass many smooth / nonsmooth , convex / nonconvex optimization problems considered in practice .",
    "applications can be found in various fields throughout science and engineering , including signal / image processing @xcite and machine learning @xcite .",
    "succesful algorithms for these types of problems include for example fista method @xcite and forward - backward method @xcite .",
    "the goal of this paper is to investigate to which extent extragradient method can be used to tackle similar problems .",
    "the extragradient method was initially proposed by korpelevich @xcite .",
    "it has become a classical method for solving variational inequality problems , finding @xmath7 such that @xmath8 where @xmath9 is a nonempty , closed and convex subset of @xmath10 , @xmath11 is a monotone mapping , and @xmath12 denotes the euclidean inner product in @xmath10 .",
    "the extragradient method , generates a sequence of estimates based on the following recursion , which requires two orthogonal projections onto @xmath9 at each iteration , @xmath13 after korpelevich s work , a number of authors extended the extragradient method for variational inequality problems ( for example , see @xcite , @xcite ) . in the context of convex constrained optimization ,",
    "@xcite considered the performances of the extragradient method under error bounds assumptions . in this",
    "setting , luo and tseng have described asymptotic linear convergence of the extragradient method applied to constrained problems of the form , @xmath14 where @xmath15 is a convex set of @xmath10 and @xmath16 is a smooth convex function on @xmath10 . to our knowledge",
    ", this is the only attempt to analyse the method sepecifically in an optimization setting .",
    "a distinguished feature of the extragradient method is the use of an additional projected gradient step which can be seen as a guide during the optimization process .",
    "intuitively , this additional iteration allows to _ foresee _ the geometry of the problem and take into account curvature information , one of the most important bottlenecks for first order methods .",
    "motivated by this observation , our goal is to extend and understand further the extragradient method in the specific setting of [ eq : prob ] .",
    "appart from the work of luo and tseng , the literature on this topic is quite scarce .",
    "for example , the nonconvex case is not considered at all .",
    "we combine the work of @xcite , @xcite and recent extensions for first - order descent methods , ( see ) , and propose the extented extragradient method * ( eeg ) * to tackle problem [ eq : prob ] .",
    "the classical extragradient method relies on orthogonal projections .",
    "we extend it by considering more general nonsmooth convex functions , the * ( eeg ) * method is given by the following recursion , @xmath17 where @xmath18 are positive real number .",
    "an important challenge in this context is to ballance the magnitude of these two parameters to maintain desirable convergence properties .",
    "we devise conditions which allow to prove convergence of the method when @xmath5 is nonconvex .",
    "in addition , we describe two different rates of convergence when @xmath5 is convex .",
    "following we heavily rely on the kurdyka - lojasiewicz ( kl ) inequality to study the nonconvex setting .",
    "the kl inequality @xcite has a long history in convergence analysis and nonsmooth optimization .",
    "furthermore , recent generalizations @xcite have shown the important versatility of this approach as the inequality holds true for the vast majority of models encountered in practice . this opened the possibility to devise",
    "general and abstract [ comma removed ] convergence results for first order methods @xcite , which constitute an important ingredient of our analysis . based on this approach",
    ", we derive a general convergence result for the proposed * ( eeg ) * method .    when the function @xmath5 is convex , problem [ eq : prob ] becomes remove `` itself '' convex and we may consider global convergence rates .",
    "we first describe a @xmath0 nonasymptotic [ note : either always separate non convex , non asymptotic or always join nonconvex , nonasymptotic etc .",
    "] rate in terms of objective function .",
    "this is related to classical results from the analysis of first order methods in convex optimization , see for example the analysis of forward backward method in @xcite .",
    "furthermore , we show that the _ small - prox _ result of also applies to * ( eeg ) * method which echoes the error bound framework of luo and tseng @xcite and opens the door to more refined complexity results when further properties of the objective function are available .    as already mentioned , a distinguished aspect of the extragradient method is the use of an additional proximal gradient step at each iteration .",
    "the intuition behind this mechanism is the incorporation of curvature information in the optimization process .",
    "it is expected that one of the effects of this additional step is to allow taking larger step sizes . with this in mind , the analysis of * ( eeg ) * method is the occasion to describe an exact line search variant of the method : @xmath19 although computing the solution to the exact line search is a nonconvex problem , potentially hard in the general case , we describe an active set method to tackle it for the specific and very popular case of @xmath1 regularized least squares . in this setting the computational overhead of the exact line search",
    "has a magnitude roughly similar to that of a gradient computation ( discarding additional logarithmic terms ) .    on the practical side ,",
    "we compare the performance of the proposed * ( eeg ) * method ( and its line search variant ) to those of fista and forward - backward methods on the @xmath1 regularized least squares problem .",
    "the numerical results suggest that in the setting of ill conditioned problems , both * ( eeg ) * and forward - backward , when combined with exact line search , constitute promising alternatives to fista .",
    "[ [ structure - of - the - paper . ] ] structure of the paper .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    section 2 introduces the problem and our main assumptions .",
    "we also recall important definitions and notations which will be used throughout the text .",
    "section 3 contains the main convergence results of this paper .",
    "more precisely , in subsection 3.3 , we present the convergence and finite length property under kl assumption in the nonconvex case .",
    "subsection 3.4 , contains both a proof of sublinear convergence rate and the application of the _ small - prox _ result for * ( eeg ) * method leading to improved complexity analysis under kurdyka - lojasiewicz inequality assumption .",
    "section 4 describes exact line search for proximal gradient steps in the context of @xmath1 penalized least - squares and results from numerical experiments .",
    "we are interested in solving minimization problems of the form @xmath20 where @xmath21 are extended value functions from @xmath10 to @xmath22 $ ] .",
    "we make the following standing assumptions :    * @xmath23 and we note @xmath24 * @xmath6 is a lower semi - continuous , convex , proper function .",
    "* @xmath5 is differentiable with @xmath25-lipschitz continuous gradient , where @xmath26 .",
    "let us give two classical examples fitting these assumptions .",
    "* constrained minimization .",
    "* let @xmath15 be a closed convex set of @xmath10 .",
    "define @xmath6 to be the indicator function ( @xmath27 ) of the set @xmath15 : @xmath28 then , the unconstrained minimization of composite function is equivalent to minimize the function @xmath5 over the set @xmath15 .",
    "* regularized least squares . *",
    "the @xmath1 regularized least squares problem consists in the minimization of the following nonsmooth objective function : @xmath29 where @xmath30 is a real matrix , @xmath31 be is real vector , @xmath32 is a positive real and @xmath33 denotes the @xmath34-norm , the sum of coordinates absolute value .",
    "many approaches based on @xmath1 regularized least squares are very popular in signal processing and statistics .      in this subsection ,",
    "we recall the definitions , notations and some well - known results from nonsmooth analysis which are going to be used throughout the paper . we will use notations from @xcite ( see also @xcite ) .",
    "let @xmath35}$ ] be a proper , lower - semicontinuous function . for each @xmath36 ,",
    "the frchet subdifferential of @xmath37 at @xmath38 , written @xmath39 , is the set of vectors @xmath40 which satisfy @xmath41 when @xmath42 , we set @xmath43 .",
    "we will use the following set @xmath44 the subdifferential of @xmath37 at @xmath36 is defined by the following closure process @xmath45 @xmath46 is defined similarly as @xmath47 .",
    "when @xmath37 is convex , the above definition coincides with the usual notion of subdifferential in convex analysis @xmath48 independently from the definition , when @xmath37 is smooth at @xmath38 then the subdifferential is a singleton , @xmath49 .",
    "+ we can deduce from its definition the following closedness property of the subdifferential : if a sequence @xmath50 , converges to @xmath51 , and @xmath52 converges to @xmath53 then @xmath54 .",
    "the set @xmath55 is called the set of critical points of @xmath37 . in this nonsmooth context ,",
    "the fermat s rule remains unchanged : a necessary condition for @xmath38 to be local minimizer of @xmath37 is that @xmath56 ( * ? ? ? * theorem 10.1 ) .    under our standing assumption",
    ", @xmath5 is a smooth function and we have subdifferential sum rule ( [ exercise 10.10]@xcite ) @xmath57 we recall a well known important property of smooth functions which have @xmath25-lipschitz continuous gradient , see ( * ? ? ?",
    "* lemma 1.2.3 ) .",
    "[ lem1 ] for any @xmath58 , we have @xmath59    for the rest of this paragraph , we suppose that @xmath37 is a convex function .",
    "given @xmath60 and @xmath61 , the proximal operator associated to @xmath37 , which we denote by @xmath62 , is defined as the unique minimizer of function @xmath63 , i.e : @xmath64 using fermat s rule , @xmath62 is characterized as the unique solution of the inclusion @xmath65 we can check that when h is convex then @xmath66 is lipschitz continuous with constant 1 ( see ( * ? ? ?",
    "* proposition 12.27 ) ) . as an illustration ,",
    "let @xmath67 be a closed , convex and nonempty set , then @xmath68 is the orthogonal projection operator onto @xmath15 .",
    "the following property of the @xmath69 mapping will be used in the analysis , see ( * ? ? ?",
    "* lemma 1.4 ) .",
    "[ proximal ] let @xmath70 , and @xmath71 , then @xmath72      in this subsection , we present the nonsmooth kurdyka - ojasiewicz inequality introduced in @xcite ( see also @xcite , and the fundamental works @xcite ) .",
    "we note @xmath73=\\{x\\in { \\mathbb{r}}^n : h(x)<\\mu\\}$ ] and @xmath74=\\{x\\in { \\mathbb{r}}^n:\\eta < h(x)<\\mu\\}$ ] .",
    "let @xmath75 and set @xmath76    the function @xmath37 satisfies the _ kurdyka - ojasiewicz ( kl ) inequality _ ( or has the kl _ property _ ) locally at @xmath77 if there exist @xmath75 , @xmath78 and a neighborhood @xmath79 such that @xmath80 for all @xmath81 $ ] .",
    "we say that @xmath82 is a _ desingularizing function _ for @xmath3 at @xmath83 .",
    "the function @xmath37 has the kl property on @xmath9 if it does so at each point of @xmath9 .",
    "when @xmath37 is smooth and @xmath84 then ( [ defkl ] ) can be rewritten as @xmath85.\\ ] ] this inequality may be interpreted as follows : the function @xmath37 can be made sharp locally by a reparameterization of its values through a function @xmath78 for some @xmath86 .",
    "the kl inequality is obviously satisfied at any noncritical point @xmath87 and will thus be useful only for critical points , @xmath88 .",
    "the _ ojasiewicz gradient inequality _ corresponds to the case when @xmath89 for some @xmath90 and @xmath91 . the class of functions which satisfy kl inequality is extremely vast .",
    "typical kl functions are semi - algebraic functions , but there exists many extensions , see @xcite .",
    "if @xmath37 has the kl property and admits the same desingularizing function @xmath92 at _ every point _",
    ", then we say that @xmath92 is a _",
    "global _ desingularizing function for @xmath5 .",
    "the following lemma is similar to ( * ? ? ?",
    "* lemma 3.6 ) .",
    "[ klunif ] let @xmath93 be a compact set and let @xmath94 $ ] be a proper and lower semicontinuous function .",
    "we assume that @xmath37 is constant on @xmath93 and satisfies the kl property at each point of @xmath93 .",
    "then there exist @xmath95 and @xmath92 such that for all @xmath96 , one has @xmath97 for all @xmath98.$ ]",
    "we now describe our extragradient method dedicated to the minimization of problem ( * p)*. recall that the method is defined , given an initial estimate @xmath99 , by the following recursion , for @xmath100 ,    * ( eeg ) * y_k=_s_k g(x_k - s_kf(x_k)),[def1 ] + x_k+1=__k g(x_k-_kf(y_k))[def2 ] .    where @xmath101 are positive step size sequences .",
    "we introduce relevant quantities , @xmath102 , and @xmath103 for @xmath104 . throughout the paper",
    ", we will consider the following condition on the two step size sequence , @xmath105 depending on the context , additional restrictions will be imposed on the step size sequences .",
    "we introduce in this subsection two technical properties of sequences produced by(*eeg ) * method .",
    "they will play a crucial role in the proofs of our main convergence and complexity results .",
    "we begin with a descent property .",
    "[ proper1 ] for any @xmath106 , we have @xmath107    we fix an arbitrary @xmath104 . applying lemma  [ proximal ] for ( [ def1 ] ) , with @xmath108 , @xmath109 and @xmath110 , we obtain @xmath111 combining with the descent lemma , @xmath112 , we get @xmath113 similarly , applying lemma  [ proximal ] for ( [ def2 ] ) , with @xmath114 and @xmath115 we obtain @xmath116 on the other hand , we have from the descent lemma that @xmath117 summing up the last two inequalities , we have @xmath118 combining inequalities ( [ p1 ] ) and ( [ p2 ] ) , we obtain @xmath119 which concludes the proof    [ rem : remarkdecrease ] if we combine the constraint that @xmath120 for all @xmath104 with condition * ( c ) * , we deduce from proposition [ proper1 ] that , for all @xmath104 , @xmath121 , and @xmath122 under this condition , we have that * ( eeg ) * is a descent method in the sense that it will produce a decreasing sequence of objsective value .",
    "we now establish a second property of sequences produced by ( * eeg * ) method which is interpreted as a subgradient step property .",
    "[ condgra ] assume that @xmath123 satisfy condition * ( c)*. then for any @xmath106 , there exists @xmath124 such that @xmath125    we write the optimality condition for @xmath126 , @xmath127 therefore , there exists @xmath128 such that @xmath129 this implies that @xmath130 since @xmath131 is @xmath25-lipschitz continuous , it follows that @xmath132 denote @xmath133 , since the @xmath134 is 1-lipschitz continuous , we get @xmath135 and therefore @xmath136 on the other hand , @xmath6 is convex , thus in view of the definition of @xmath137 , @xmath138 similarly , from ( [ opt ] ) and convexity of @xmath6 , we get @xmath139 adding the last two inequalities , we obtain @xmath140 or equivalently @xmath141 it follows that @xmath142 using cauchy - schwarz inequality , we get @xmath143 since from condition * ( c ) * , @xmath144 , this is equivalent to @xmath145 this inequality asserts that the product of two terms is nonpositive .",
    "hence one of the terms must be nonpositive and the other one must be nonnegative . from condition * ( c ) * , we have @xmath146 , the last term is bigger than the first one and hence must be nonnegative .",
    "this yields @xmath147 by combining the latter inequality with ( [ ineq1 ] ) , we get @xmath148 similarly , from the definitions of @xmath149 and the convexity of @xmath6 , we obtain that @xmath150 and @xmath151 summing the last two inequalities , we have that @xmath152 using the condition @xmath153 and cauchy - schwarz inequality , we get @xmath154 using lipschitz continuity of @xmath131 , we have that @xmath155 combining this inequality with ( [ ineq2 ] ) , we obtain @xmath156 combining ( [ ineq3 ] ) with ( [ equa3 ] ) , we get @xmath157 and the result is proved    combining remark [ rem : remarkdecrease ] and proposition [ condgra ] above , we have the following corollary which underlines the fact that ( * eeg * ) is actually an approximate gradient method in the sense of @xcite .    [ condc1 ]",
    "assume that @xmath158 satisfy the following @xmath159 then , for all @xmath104    i ) : :    @xmath160 ii ) : :    there exists @xmath161 such    that @xmath162    where , @xmath163      in this subsection , we analyse the convergence of ( * eeg ) * method in the nonconvex setting .",
    "the main result is stated in theorem [ rate ] , which also describes the asymptotic rate of convergence .",
    "this result is based on the assumptions that @xmath3 has the kl property on @xmath164 and that @xmath165 satisfy conditions * ( c1 ) * from corollary [ condc1 ] .",
    "we will also assume that the sequence @xmath166 generated by ( * eeg * ) is bounded .",
    "this boundedness assumption is not very restrictive here , since under condition * ( c1 ) * , corollary [ condc1 ] ensures that it is satisfied for any coercive objective function .",
    "similarly to ( * ? ? ?",
    "* lemma  3.5 ) , we first give some properties of @xmath3 on the set of accumulation points of @xmath167 .",
    "[ const ] assume that @xmath165 satisfy condition * ( c1 ) * and that @xmath167 is bounded .",
    "let @xmath168 be the set of limit points of the sequence @xmath167 .",
    "it holds that @xmath168 is compact and nonempty , @xmath169 , @xmath170 and @xmath171 for all @xmath172 .    from the boundedness assumption , it is clear that @xmath168 is nonempty . in view of corollary [ condc1 ] * i ) * , it follows that @xmath173 is nonincreasing .",
    "furthermore , @xmath174 is bounded from below by @xmath175 , hence there exists @xmath176 such that @xmath177 .",
    "in addition , we have @xmath178 therefore @xmath179 converges , thus @xmath180 .",
    "we now fix an arbitrary point @xmath181 , which means that there exists a subsequence @xmath182 of @xmath167 such that @xmath183 , therefore , by lower semicontinuity of @xmath6 and continuity of @xmath5 , @xmath184 from the definition of @xmath185 and condition * ( c1 ) * , we get for all @xmath186 , @xmath187 let @xmath188 , it follows that @xmath189 , thus , in view of ( [ lsc ] ) , @xmath190 , therefore @xmath191 .",
    "since @xmath174 is nonincreasing , @xmath192 , and we deduce that @xmath193 . since @xmath194 was arbitrary in @xmath168 , it holds that @xmath3 is constant on @xmath168 .",
    "now , thanks to corollary [ condc1 ] * ii ) * , there exist @xmath161 , such that @xmath195 under condition ( * c1 * ) , it holds that @xmath196 remains bounded .",
    "since @xmath197 it holds that @xmath198 . combining with the closedness of @xmath199 , this implies that @xmath200 , hence @xmath201 .",
    "since @xmath194 was taken arbitrarily in @xmath168 , this means that @xmath202 .",
    "the compactness of @xmath168 is implied by ( * ? ? ?",
    "* lemma 3.5 ) . combining the boundedness of @xmath167 and the compactness of @xmath168",
    ", we deduce that @xmath203 which concludes the proof .",
    "we are now in a position to prove the convergence of the extra - gradient method in the non - convex case",
    ".    [ rate ] assume that @xmath165 satisfy condition * ( c1 ) * , that @xmath3 has the kl property on @xmath164 and that @xmath167 is bounded .",
    "then the sequence @xmath167 converges to @xmath201 , moreover @xmath204    note that lemma [ const ] can be applied here and we will use the same notations .",
    "we write @xmath205 and let @xmath168 be the set of limit points of @xmath167 . combining the kl assumption and lemma  [ klunif ] , there exists @xmath206 and a desingularizing function @xmath207 such that @xmath208 for all @xmath209 $ ] .",
    "denote by @xmath210 the first index such that @xmath211 or @xmath212 .",
    "if such an @xmath210 exists , one has @xmath213 and @xmath214 , for all @xmath215 which shows that the result holds true .",
    "for the rest of the proof , we will assume that @xmath216 and @xmath217 for all @xmath218 .",
    "from lemma [ const ] , we have @xmath203 and @xmath219 , therefore there exists @xmath220 such that for all @xmath221 , @xmath222 using the concavity of @xmath92 and corollary  [ condc1 ] we obtain @xmath223 since @xmath3 has kl property on @xmath168 , using again corollary  [ condc1 ] , we get @xmath224 combining the last two inequalities , we obtain for all @xmath221 , @xmath225 where @xmath226 is given in corollary [ condc1 ] .",
    "this implies that @xmath227 therefore , the series @xmath228 is bounded and hence converges . by cauchy criterion",
    ", it follows that the sequence @xmath167 converges to some point @xmath229 .",
    "furthermore , from lemma [ const ] , @xmath230 which concludes the proof .",
    "when the kl desingularizing function of @xmath3 is of the form @xmath231 , where @xmath232 is a positive constant and @xmath233 $ ] , then we can estimate the rate of convergence of the sequence @xmath167 , as follows ( see theorem 2 , @xcite ) .",
    "* @xmath234 then the sequence @xmath235 converges in a \f finite number of steps .",
    "* @xmath236 $ ] then there exist @xmath237 and @xmath238 such that @xmath239 * @xmath240 then there exist @xmath237 such that @xmath241      throughout this section , we suppose that the function @xmath5 is convex and we focus on complexity and non asymptotic convergence rate analysis .",
    "we begin with a technical lemma which introduces more restrictive step size conditions .",
    "[ lem : stepconvex ] assume that @xmath242 satisfy the following @xmath243 then for all @xmath104 , @xmath244    first , we note that if @xmath245 satisfy * ( c2 ) * then they also satisfy condition * ( c1 ) * and proposition [ condgra ] applies . thanks to inequality ( [ ineq3 ] ) from the proof of proposition [ condgra ] , we get @xmath246 in addition , it can be checked using elementary calculation that @xmath247 is equivalent to @xmath248}{2l}\\leq \\alpha_k\\leq \\frac{(1-ls_k)\\left[(1+ls_k)+\\sqrt{(1+ls_k)^2 - 4l^2s_k^2}\\right]}{2l}.\\ ] ] note that , for @xmath249 then @xmath250 . using this inequality , with the condition @xmath251 , we get @xmath252 .",
    "thus , @xmath253}{2l}\\leq \\frac{(1-ls_k)\\left[(1+ls_k)-(1-ls_k)\\right]}{2l}=(1-ls_k)s_k\\leq s_k\\notag\\\\      \\displaystyle\\frac{(1-ls_k)\\left[(1+ls_k)+\\sqrt{(1+ls_k)^2 - 4l^2s_k^2}\\right]}{2l}\\geq \\frac{(1-ls_k)\\left[(1+ls_k)+(1-ls_k)\\right]}{2l}=\\frac{1}{l}-s_k.\\notag \\end{cases}\\ ] ] putting things together , condition * ( c2 ) * implies that @xmath254    with a similar method as in @xcite , we prove a sublinear convergence rate for @xmath255 in the convex case .",
    "suppose that @xmath245 satisfy condition * ( c2 ) * and that @xmath5 is convex , then , for any @xmath256 , we have @xmath257    we first fix arbirary @xmath104 and @xmath258 .",
    "applying lemma [ proximal ] with @xmath259 , @xmath260 and @xmath261 , we obtain @xmath262 where the last inequality is due to lemma [ lem1 ] .",
    "it follows that @xmath263 since @xmath5 is convex , @xmath264 , and the above inequality implies that @xmath265 using the fact that @xmath174 is noninreasing and bounded from bellow by @xmath266 , it follows from lemma [ lem : stepconvex ] that @xmath267 summing this inequality for @xmath268 gives @xmath269 coming back to corollary  [ condc1 ] , it is easy to see that the sequence @xmath270 is nonincreasing , then @xmath271 . combining with ( [ complex1 ] ) , we get @xmath272 it follows that @xmath273      we now study the complexity of * ( eeg ) * method when @xmath3 has , in addition , the kl property on @xmath164 .",
    "first , using the convexity of @xmath5 , proposition  [ proper1 ] , can be improved by using the following result .",
    "[ descentcd2 ] assume that @xmath5 is convex and @xmath245 satisfy condition * ( c ) * , then for all @xmath106 , we have @xmath274 where @xmath275    fix an arbitrary @xmath104 .",
    "applying lemma  [ proximal ] , with @xmath276 , @xmath277 , @xmath260 and @xmath261 , we get @xmath278 where the last inequality follows from lemma [ lem1 ] .",
    "this implies that @xmath279 since @xmath5 is convex , @xmath280 , which leads to @xmath281 from condition * ( c ) * , proposition [ condgra ] holds and in particular , inequality ( [ ineq3 ] ) . combining inequality ( [ descentcd3 ] ) with ( [ ineq3 ] ) , we get the desired result , @xmath282 \\|x_k - x_{k+1}\\|^2.\\ ] ]    we now consider another step size condition .",
    "[ lem : stepsize3 ] suppose that @xmath18 satisfy the following condition @xmath283 then , for all @xmath104 , @xmath284    first , one can check that @xmath285 if and only if @xmath286 \\cup [ 2 , + \\infty),\\ ] ] and the bound @xmath287 is a necessary condition which ensures that there exists @xmath288 which satisfies @xmath289 .",
    "we now turn to the lower bound under condition * ( c3)*. set @xmath290 where one can think of @xmath291 satisfying @xmath292 $ ] .",
    "the maximum of @xmath293 is attained for @xmath294 and hence @xmath295 is increasing on @xmath296 $ ] , and therefore , for all @xmath104 , @xmath297 which is the desired result .",
    "we can check that , when condition * ( c3 ) * is satisfied , one has @xmath298 combining this with proposition  [ condgra ] , proposition [ descentcd2 ] and lemma [ lem : stepsize3 ] , we obtain the following corollary .    [",
    "cor : gradmethod2 ] suppose that @xmath242 satisfy condition * ( c3 ) * and that @xmath5 is convex , then    i ) : :    @xmath299 ii ) : :    there exists @xmath161 such    that    @xmath300    where @xmath15 is given in lemma [ lem : stepsize3 ] and @xmath301 .",
    "we now consider the complexity for * ( eeg ) * method under nonsmooth kl inequality in the form of a _ small prox _ result as in .",
    "first , we recall some definitions from .",
    "let @xmath302 , we assume that @xmath3 has the kl property on @xmath303 $ ] with desingularizing function @xmath304 set @xmath305 and consider the function @xmath306})^{-1}:[0,\\beta_0]\\to [ 0,r_0]$ ] , which is increasing and convex .",
    "we add the assumption that @xmath307 is lipschitz continuous ( on @xmath308 $ ] ) with constant @xmath309 and @xmath310 .",
    "set @xmath311 starting from @xmath312 , we define the sequence @xmath313 by @xmath314 it is easy to prove that @xmath315 is decreasing and converges to zero . by continuity , @xmath316 .",
    "now , applying the result of , we have the complexity of * ( eeg ) * method in the form of a _ small prox _ result .",
    "assume that @xmath245 satisfy condition * ( c3 ) * and @xmath5 is convex .",
    "then , the sequence @xmath167 converges to @xmath258 , and @xmath317 moreover , @xmath318 where @xmath319 and @xmath15 are given in corollary [ cor : gradmethod2 ] .",
    "in this section , we compare the extra - gradient method with standard algorithms in numerical optimization : forward - backward and fista .",
    "we describe the problem of interest , details about exact line search in this context and numerical results .",
    "we let @xmath320 be a real matrix , @xmath31 be a real vector and @xmath32 be a scalar , all of them given and fxed .",
    "following the notations of the previous section , we define @xmath321 and @xmath322 ( the sum of absolute values of the entries ) . with these notations ,",
    "the optimization problem ( * p * ) becomes @xmath323 solutions of problem of the form of ( [ eq : l1leastsquares ] ) ( as well as many extensions ) are extensively used in statistics and signal processing @xcite . for this problem",
    ", we introduce the proximal gradient mapping , a specialization of the proximal gradient step to problem ( [ eq : l1leastsquares ] ) .",
    "this is the main building block of all the algorithms presented in the numerical experiment . @xmath324 where @xmath325 ( @xmath326 ) is the soft - thresholding operator which acts coordinatewise and satisfies for @xmath327 @xmath328_i =   \\begin{cases } 0 , & \\text{if } |x_i| \\leq a\\\\ x_i - a { \\rm sign}(x_i ) , & \\text{otherwise}. \\end{cases } \\end{aligned}\\ ] ]      one intuition behind extragradient - method for optimization is the use of an additional iteration as a guide or a scout to provide an estimate of the gradient that better suits the geometry of the problem .",
    "this should eventually translate in the possibility of taking larger steps leading to faster convergence . in this section",
    "we briefly describe a strategy which allows to perform exact line search in the context of @xmath1-regularized least squares . up to our knowledge , this was not decribed before in the literature .",
    "furthermore , this strategy may be extended to more general least squares problems with nonsmooth regularizers . for the rest of this section , we assume that @xmath329 is fixed .",
    "we heavily rely on the two simple facts :      we consider the following function @xmath332 it can be deduced from the properties of @xmath5 , @xmath6 and @xmath333 that @xmath334 is continuous and piecewise quadratic . in classical implementation of proximal splitting methods , the step - size parameter @xmath335 is a well chosen constant which depends on the problem , or alternatively it is estimated using backtracking .",
    "the alternative which we propose is to choose the step - size parameter @xmath335 minimizing @xmath334 .",
    "since @xmath334 is piecewise quadratic , this only requires to know the values of @xmath335 for which @xmath334 is not differentiable and the expression of @xmath334 as a quadratic form between these values .",
    "the nonsmooth points of @xmath334 are given by the following set @xmath336 and correspond to limiting values for which coordinates of @xmath337 are null .",
    "we assume that the elements of @xmath338 are ordered nondecrasingly ( letting potential ties appear several times ) . the comments that we have made so far lead to the following .",
    "* @xmath338 contains no more than @xmath339 elements .",
    "* given @xmath38 and @xmath340 , computing @xmath338 is as costly as computing @xmath131 .",
    "* @xmath334 is quadratic between two consecutive elements of @xmath338 .    in order to minimize @xmath334 ,",
    "the only task that should be performed is to keep track of its value ( or equivalently of its quadratic expression ) between consecutive elements of @xmath338 . here",
    ", we can use the fact that elements of @xmath338 corresponds to values of @xmath335 for which one coordinate of @xmath337 goes to zero or becomes active ( non - zero ) .",
    "a careful implementation of the minimization of @xmath334 amounts to sort the values in @xmath338 , screen them in increasing order , keep track of the corresponding quadratic expression and the minimal value .",
    "we provide a few details for completeness .",
    "* the vector @xmath341_i}{\\partial s } \\right)_{i=1}^n \\in { \\mathbb{r}}^n$ ] is constant between consecutive elements of @xmath338 .",
    "furthermore the elements of @xmath338 ( counted with multiple ties ) corresponds to value of @xmath335 for which a single coordinate of @xmath342 is modified . *",
    "suppose that @xmath343 are two consecutive elements of @xmath338 .",
    "then for all @xmath344 $ ] , letting @xmath345 on this segment , we have @xmath346 , hence , @xmath347 where @xmath348 is a vector which depends on the sign pattern of @xmath349 and @xmath350 . * for @xmath351 ,",
    "the sign pattern of @xmath352 and the corresponding value of @xmath350 and @xmath353 ( for the next interval ) are modified only at a single coordinate , the same for the three of them .",
    "in other words , updating the quadratic expression of @xmath334 at @xmath354 only requires the knowledge of this coordinate , the value of the corresponding column in @xmath355 and can be done by computing inner products in @xmath356 .",
    "this requires @xmath357 operations . *",
    "given these properties , we can perform minimization of @xmath334 by an active set strategy , keeping track only of the sign pattern of @xmath358 , the value of @xmath359 , the value of @xmath360 , @xmath361 and @xmath362 which cost is of the order of @xmath357 .",
    "this should not be repeated more than @xmath339 times .",
    "using this active set procedure provides the quadratic expression of @xmath334 for all intervals represented by consecutive values in @xmath338 . from these expressions , it is not difficult to compute the global minimum of @xmath334 .",
    "the overall cost of this operation when properly implemented is of the order of @xmath363 plus the cost of sorting @xmath339 elements in @xmath364 .",
    "this is comparable to the cost of computing the gradient of @xmath5 .",
    "hence in this specific setting , performing exact line search does not add much overhead in term of computational cost compared to existing step - size strategies .    ) as a function of time for simulated @xmath1 regularized least squares data .",
    "fb stands for forward - backward and eg for extra - gradient .",
    "the indications in parenthesis indicate the step sizes strategy that is used . ]    ) as a function of the iteration counter for simulated @xmath1 regularized least squares data .",
    "fb stands for forward - backward and eg for extra - gradient .",
    "the indications in parenthesis indicate the step sizes strategy that is used . ]        * set @xmath365 and @xmath366 * set @xmath367 where @xmath368 has standard gaussian independent entries and @xmath369 is a diagonal matrix which @xmath210-th diagonal entry is @xmath370 . *",
    "choose @xmath371 in @xmath10 with the @xmath372 first entries being independent standard gaussian and the remaining ones are null .",
    "* set @xmath373 where @xmath374 has independant gaussian entries . *",
    "we choose @xmath375      * forward - backward with step - size parameter @xmath376 ( see for example @xcite ) .",
    "* forward - backward with step - size parameter determined by exact line search .",
    "* extra - gradient ( discussed in the present paper ) with step size parameter @xmath377 and @xmath378 . * extra - gradient with step size parameter @xmath377 and @xmath335 determined by exact line search .",
    "* fista as described in @xcite .",
    "the exact line search active set procedure is implemented in compiled ` c ` code in order keep a reasonable level of efficiency compared to linear algebra operations which have efficient implementations .",
    "all algorithm are initialized at the same point .",
    "we keep track of decrease of the objective value , the iteration counter @xmath379 and the total spent since initialization .",
    "the iteration counter is related to analytical complexity while the total time spent is related to the arithmetical complexity ( see the introduction in @xcite for more details ) .",
    "comparing algorithms in term of analytical complexity does not reflect the fact that iterations are more costlty for some of them compared to others .",
    "computational times are presented in figure [ fig : computtime ] and iteration counters in figure [ fig : computiter ] .",
    "the main comment is that the exact line search procedure improves uppon fixed step size parameters while the induced computational overhead remains reasonable .",
    "indeed , both forward - backward and extra - gradient , when implemented using the exact line search procedure produce results which are comparable to fista , a reference in terms of performances for this composite problem . on the other hand , there is not much differentce between forward - backward and extra - gradient , neither for fixed step sizes , nor for exact line search .",
    "this work is sponsored by a grant from the air force office of scientific research , air force material command ( grant number fa9550 - 15 - 1 - 0500 ) .",
    "any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the united states air force research laboratory .",
    "the collaboration with emile richard mostly took place during his postdoctoral stay in mines paristech , paris , france in 2013 .",
    "attouch h. , bolte j. , redont p. , soubeyran a. : proximal alternating minimization and projection methods for nonconvex problems . an approach based on the kurdyka - ojasiewicz inequality , _ math .",
    "oper . res .",
    "35 , no2 , 438457 ( 2010 ) .",
    "_    attouch h. , bolte j. , svaiter b.f .",
    ": convergence of descent methods for semi - algebraic and tame problems : proximal algorithms , forward - backward splitting , and regularized gauss - seidel methods , _ math .",
    "137 , no . 1 - 2 , 91129 ( 2013 ) . _            bolte j. , daniilidis a. , lewis a. , shiota m. : clarke subgradients of stratifiable functions , _ siam j. optim .",
    "2 , 556572 ( 2007 ) . _ bolte j. , sabach s. , teboulle m. : proximal alternating linearized minimization for nonconvex and nonsmooth problems , _ mathematical programming , series a , 146 , 116 ( 2013)_.          combettes , p.l . ,",
    "pesquet , j .- c .",
    ": proximal splitting methods in signal processing , _ fixed - point algorithms for inverse problems in science and engineering springer optimization and its applications , 49 , 185212 ( 2011)_.    combettes , p.l . ,",
    "wajs , v.r . : signal recovery by proximal forward - backward splitting . , _ multiscale model .",
    "simul . , 4 , 11681200 ( 2005)_. g.m .",
    "korpelevich , the extragradient method for finding saddle points and other problems , _ ekon .",
    "metody , translated into english as matecon 12 ( 1976 ) 747 - 756_. kurdyka k. : on gradients of functions definable in o - minimal structures , _ ann .",
    "fourier 48 , 769783 ( 1998 ) . _    ojasiewicz s. : une proprit topologique des sous - ensembles analytiques rels , in : _ les quations aux drives partielles , pp . 8789 , ditions du centre national de la recherche scientifique , paris ( 1963 ) . _      monteiro renato d. c. , svaiter b. f : complexity of variants of tseng s modified f - b splitting and korpelevich s methods for hemivariational inequalities with applications to saddle - point and convex optimization problems , _ siam j. optim . , 21(4 ) , 16881720 .",
    "_      rockafellar , r.t . ,",
    "wets , r. : _ variational analysis_. grundlehren der mathematischen , wissenschaften , vol . 317 .",
    "springer , heidelberg ( 1998 ) .",
    "tibshirani , r. : regression shrinkage and selection via the lasso .",
    "_ journal of the royal statistical society , series b , 58(1):267288 ( 1996 ) _"
  ],
  "abstract_text": [
    "<S> we consider the extragradient method to minimize the sum of two functions , the first one being smooth and the second being convex . under kurdyka - lojasiewicz assumption , we prove that the sequence produced by the extragradient method converges to a critical point of the problem and has finite length . </S>",
    "<S> the analysis is extended to the case when both functions are convex . </S>",
    "<S> we provide a @xmath0 convergence rate which is classical for gradient methods . </S>",
    "<S> furthermore , we show that the recent _ small - prox _ complexity result can be applied to this method . considering the extragradient method is the occasion to describe exact line search for proximal decomposition methods . </S>",
    "<S> we provide details for the implementation of this scheme for the @xmath1 regularized least squares problem and give numerical results which suggest that combining nonaccelerated methods with exact line search can be a competitive choice .    </S>",
    "<S> * key words : * extragradient , descent method , forward - backward splitting , kurdyka - lojasiewicz inequality , complexity , first order method , @xmath2-regularized least squares . </S>"
  ]
}