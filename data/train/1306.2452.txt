{
  "article_text": [
    "the central result in this paper is the development of a new generic procedure for simulation of conditioned diffusions , also called diffusion bridges or pinned diffusions .",
    "more specifically , for some given diffusion process",
    "@xmath1 , we aim to simulate the functional @xmath2,\\ ] ] where @xmath3 , @xmath4 is some set that may consist of only one point , @xmath5 is an arbitrarily given suitable test function and @xmath6 is a given state . in recent years , the problem of computing terms such as ( [ cp ] ) has attracted a lot of attention in the literature , sparked by several applications .",
    "indeed , many relevant properties of a diffusion process @xmath1 can be advantageously analyzed by considering the process conditioned on certain appropriate events .",
    "one so allows `` to study rare events by conditioning on the event happening or to analyze the behaviour of a composite system when only some of its components can be observed , '' as is eloquently put by @xcite .",
    "for instance , in statistical inference based on a continuous time model , discrete time observations can be enriched to continuous time observations by sampling from the diffusion bridges between the discrete time data ; see @xcite and @xcite for more information .",
    "conditional diffusions have further been successfully used for critical calculations in rare event situations . as an example from computational chemistry",
    ", we refer to the review paper of @xcite , where diffusion bridges are used for detection of the transition state surface between two stable regions @xmath4 and @xmath7 in configuration space . here",
    ", standard monte carlo simulation is prohibitively costly , as the event of such a transition is rare , provided that the `` walls '' in the energy surface between @xmath4 and @xmath7 are high .",
    "however , by studying the process conditioned on starting in @xmath4 and ending in @xmath7 , one can efficiently observe on which paths the configuration typically travels from @xmath4 to @xmath7 .",
    "other possible applications appear in the field of stochastic environmental models , for instance , regarding the concentration evolutions of pollution in water ; for example see @xcite and references therein for a related problem .",
    "several approaches for simulation of diffusion bridges have already been studied in the literature . for the theory of diffusion bridges we refer to @xcite and the references therein .",
    "many existing approaches utilize known radon  nikodym densities of the law of the diffusion @xmath1 conditioned on initial and terminal values , with respect to the law of a standard diffusion bridge process ( e.g. , wiener bridge ) on path - space [ as a radon  nikodym derivative obtained by doob s h - transform ; see , e.g. , @xcite or @xcite ] .",
    "several other approaches are based on ( partial ) knowledge of the transition densities of the unconditional diffusion ( that is not generically available , of course ) . for an overview of many different techniques ,",
    "we refer to .",
    "first , let us mention the work by @xcite who construct a general , rejection - based algorithm for solutions of _ one - dimensional _ sdes , based on the radon  nikodym derivative of the law of the solution with respect to the wiener measure .",
    "the algorithm gives ( in finite , but random time ) discrete samples of the exact solution of the sde .",
    "a  simple adaption of this algorithm gives samples of the exact diffusion process conditioned on @xmath8 , by using the law of the corresponding brownian bridge as reference measure ( instead of the wiener measure ) .",
    "an overview of related importance sampling techniques is given by @xcite .",
    "on the other hand , by relying on knowledge of the transition densities of @xmath1 , @xcite use a sequential weighted monte carlo framework , including resampling with optimal priority scores .    another general technique used for simulation of diffusion bridges",
    "is the markov chain monte carlo method .",
    "indeed , @xcite and @xcite show how the law of a ( multi - dimensional , uniformly elliptic , additive - noise ) diffusion @xmath1 conditioned on @xmath8 can be regarded as the invariant distribution of a stochastic differential equation of langevin type on path - space , that is , of a langevin - type stochastic partial differential equation ( spde ) .",
    "thus , in principle mcmc methods are applicable as explored by @xcite and @xcite . however , this requires the numerical solution of the spde involved .",
    "it should be noted that in @xcite the uniform ellipticity condition is relaxed leading to a fourth order parabolic spde rather than a second order one .",
    "other notable approaches include those of @xcite , which treat the case of physically relevant functionals of wiener integrals with respect to brownian bridges , and @xcite , who uses an mcmc approach based on successive modifications of the drift of the diffusion process .",
    "another approach is the one of @xcite developed for one - dimensional diffusions . in order to obtain a sample from the process @xmath1 conditioned on @xmath9 and @xmath8 ,",
    "@xcite start a path of the diffusion from @xmath10 and another path of the diffusion in _ reversed time _ at @xmath11 . if these paths hit at time @xmath12 , consider the concatenated path @xmath13 .",
    "the distribution of the process @xmath13 ( conditional on @xmath14 ) equals the distribution of the bridge conditional on being hit by an independent path of the underlying diffusion with initial distribution @xmath15 .",
    "as proved by @xcite , the probability of this event approaches @xmath16 when @xmath17 .",
    "finally , in order to improve the accuracy , @xmath13 is used as initial value of an mcmc algorithm on path space , converging to a sample from the true diffusion bridge .",
    "a more general approach is given by @xcite which relies on the explicit radon  nikodym derivative of the diffusion @xmath1 conditioned on its initial and terminal values and another diffusion @xmath18 , which is modeled like the brownian bridge .",
    "in fact , @xmath18 has the same dynamics as @xmath1 , except for an extra term @xmath19 in the drift , which enforces @xmath20 . under certain regularity conditions ",
    "in particular invertibility of the diffusion matrix @xmath21@xcite provide a girsanov - type theorem , which leads to a representation of the form @xmath22 = \\mathbb{e } \\bigl [ g(y ) z(y ) \\bigr]\\ ] ] for functionals @xmath5 defined on path - space and a factor @xmath23 explicitly given as a functional of the path @xmath18 together with quadratic variations of functions of @xmath18 .",
    "as such this approach allows for direct monte carlo simulation of ( [ cp ] ) . however , we stress that @xmath23 explicitly depends on @xmath24 which does not exist in many hypo - elliptic applications . on the other hand , simulation of the bridge - type process @xmath18",
    "is numerically troublesome because of the exploding drift term .",
    "the new method presented in this article is inspired by the forward - reverse estimator for the transition density @xmath25 constructed by @xcite . given a grid @xmath26",
    ", we prove that @xmath27\\ ] ] equals @xmath28 } { \\mathbb{e } [ k_{\\varepsilon } ( y(\\widehat{t}_l ) - x(t^\\ast )   ) \\mathcal{y}(\\widehat{t}_l )   ] } , \\hspace*{-20pt}\\ ] ] which can be implemented by monte carlo simulation for any @xmath29 . in ( [ eq : for - rev - intro ] ) @xmath30 is a given grid - point chosen by the user .",
    "the process @xmath1 solves the original sde with initial value @xmath31 on the time - interval @xmath32 $ ] . on the other hand",
    ", @xmath18 is an ( independent ) _ reverse process _ as defined in section  [ main ] started at @xmath33 and simulated until time @xmath34 , not on the original grid , but on a `` perturbed '' grid defined in ( [ eq : hat - grid ] ) .",
    "[ note that @xmath18 is different from the time - reversed diffusion in the sense of @xcite that explicitly requires the transition density of @xmath1 .",
    "] indeed , the dynamics of @xmath18 are explicitly given below in terms of the dynamics of @xmath1not relying on the transition density  and , usually , share the same regularity properties ; see ( [ mc12 ] ) and ( [ revc ] ) .",
    "next , we weight the trajectories according to the distance between @xmath35 and @xmath36 using a kernel @xmath37 with bandwidth @xmath38 .",
    "finally , we have an exponential weighting factor @xmath39 , similar to the radon  nikodym derivative in @xcite .",
    "the denominator in  ( [ eq : for - rev - intro ] ) actually corresponds to the forward - reverse estimator for the transition density @xmath40 of @xmath1 introduced by @xcite .",
    "the details of the monte carlo simulation are spelled out in section  [ analysis ] , but we note that ( [ eq : for - rev - intro ] ) can be computed to an accuracy of @xmath41 with a complexity of @xmath42 in any dimension less or equal to four ( disregarding possible discretization errors due to the construction of samples @xmath1 , @xmath18 and  @xmath39 ) . thus our algorithm essentially achieves the optimal rate of convergence for monte carlo algorithms .",
    "we underline that the forward - reverse algorithm for ( [ cp ] ) presented here is not a straightforward extension of the forward - reverse algorithm for transition densities of @xcite .",
    "the main difficulty lies in the extension of the representation from just one intermediate time @xmath43 to an arbitrary time grid @xmath44 with @xmath45 . in the nonautonomous case",
    "this issue is further complicated due to the fact that the dynamics of the reverse process as defined in @xcite depends explicitly on both @xmath46 and @xmath47 .",
    "obviously , the different structure also requires a different error analysis .",
    "in particular , we need sharper error bounds than @xcite .    in comparison to the other methods mentioned above , our new procedure has the following main features :    the method applies to multidimensional diffusions .",
    "it is based on simulation of _ unconditional _ diffusions only , hence technical simulation problems due to exploding drifts in sdes that govern particular diffusion bridges are avoided .",
    "the vector fields determining the ( forward ) sde that governs @xmath1 only need to satisfy a hrmander - type condition guaranteeing sufficient regularity and exponential decay of the transition densities .",
    "in particular , the diffusion matrix of @xmath1 may be degenerate .",
    "the estimator corresponding to the developed stochastic representation for  ( [ cp ] ) is root-@xmath0 consistent , that is the mean square estimation accuracy is of order @xmath48 with @xmath0 being the number of trajectories that need to be simulated .    as a matter of fact , the methods for simulating diffusion bridges known in the literature so far , do not cover all the features ( i)(iv ) simultaneously .",
    "for example , @xcite require that either the diffusion matrix is invertible , or impose some very specific structural conditions on the drift and diffusion matrix of the process @xmath1 .",
    "moreover , the exploding drift terms in their process @xmath18 makes simulation of the auxiliary process @xmath18 nontrivial . on the other hand ,",
    "the method of @xcite in germ carries some ideas related to our approach , but they need to impose balance restrictions on the transition density of @xmath1 , and moreover their method  together with several others  is only one dimensional .",
    "the methods of @xcite and the related papers mentioned above also involve some further structural assumptions and , in addition , require numerical solutions of spdes .",
    "moreover , we complement our algorithm by an adaptation , which allows us to treat the more general problem of conditioning at final time @xmath47 not on all , but just on some components of the vector @xmath49 .",
    "more precisely , we present a variant of the algorithm for computing conditional expectations where @xmath49 is conditioned to lie in a `` simple '' set @xmath4 , that is , either @xmath4 has positive measure both under the lebesgue measure and the distribution of @xmath49 , or @xmath4 is an affine plane of dimension @xmath50 . in order to achieve this extension , we need to prove ( lebesgue ) integrable error bounds for the forward - reverse algorithm for the case where @xmath49 is conditioned to a value  @xmath51 .",
    "the structure of the paper is as follows . in section  [ recap ]",
    "we recap the essential facts concerning the reverse diffusion system of @xcite .",
    "the main representation theorems for the diffusion conditioned on reaching a fixed state , or conditioned on reaching some borel set , are derived in section  [ main ] .",
    "a detailed accuracy analysis concerning the monte carlo estimators for the respective conditioned diffusions is provided in section  [ analysis ] , including the precise required regularity assumptions given in conditions [ ass : bound - density ] , [ ass : kernel - order ] and [ ass : convenience ] .",
    "limitations of the method are discussed in section  [ sec : limit - forw - reverse ] , while section  [ num ] provides a numerical study involving a heston - type stochastic volatility model .",
    "in this section we recapitulate shortly the main ingredients in the approach by @xcite .",
    "let us consider the sde @xmath52 \\\\[-8pt ]   \\eqntext{0\\leq s\\leq t , x_{t , x}(t)=x,}\\end{aligned}\\ ] ] where @xmath53 , @xmath54\\times\\mathbb { r}^{d}\\rightarrow \\mathbb{r}^{d}$ ] , @xmath55\\times\\mathbb{r}^{d}\\rightarrow\\mathbb { r}^{d\\times m}$ ] , @xmath56 is an @xmath57-dimensional standard wiener process and @xmath58 . at this stage",
    ", we only assume that @xmath1 admits a @xmath59 transition density @xmath60 and that the coefficients of ( [ in1 ] ) are @xmath59 as well .",
    "along with the ( forward ) process @xmath1 given by ( [ in1 ] ) , @xcite introduced an associated process @xmath61 in @xmath62 , @xmath63 , termed _",
    "reverse _ process on the interval @xmath64 $ ] , that solves the sde @xmath65 with @xmath66 being a ( from @xmath56 independent ) @xmath57-dimensional wiener process , and @xmath67 despite its name , we stress that @xmath68 is the solution of an ordinary sde _",
    "forward _ in time on the interval @xmath64 $ ] .",
    "one of the central results in @xcite is the following theorem .",
    "[ mt ] for fixed @xmath69 and @xmath70 , and any bi - variate test function @xmath71we have @xmath72 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad= { \\int\\!\\!\\!\\int}p\\bigl(t , x , t^{\\ast},x^{\\prime}\\bigr)p\\bigl(t^{\\ast } , y^{\\prime } , t , y \\bigr)f\\bigl(x^{\\prime},y^{\\prime}\\bigr)\\,dx^{\\prime}\\,dy^{\\prime},\\end{aligned}\\ ] ] where @xmath73 satisfies the forward equation ( [ in1 ] ) , and @xmath74 , @xmath75 , is the solution of the reverse system ( [ mc12 ] ) .",
    "[ cor : mt ] by taking @xmath76 , ( [ jf ] ) yields @xmath77=\\int p \\bigl(t^{\\ast},y^{\\prime } , t , y\\bigr)\\,dy^{\\prime } , \\label{jf1}\\ ] ] which obviously extends to @xmath78 . by next taking @xmath79 ( while abusing notation slightly ) we obtain from ( [ jf ] ) , using ( [ jf1 ] ) and the independence of @xmath1 and @xmath68 , @xmath80=\\int p\\bigl(t , x , t^{\\ast},x^{\\prime } \\bigr)f \\bigl(x^{\\prime}\\bigr)\\,dx^{\\prime},\\ ] ] which obviously extends to @xmath81 , that is , the standard forward stochastic representation for @xmath82 . on the other hand , by taking @xmath83",
    "we obtain the so called reverse stochastic representation@xmath84=\\int p\\bigl(t^{\\ast},y^{\\prime},t , y\\bigr)f \\bigl(y^{\\prime}\\bigr)\\,dy^{\\prime } , \\label{mt1}\\ ] ] which obviously extends to @xmath78 .",
    "it should be noted that in @xcite the time domain of the reverse process was considered fixed . for our purposes",
    "however , it turns out to be more effective ( in particular regarding the proof of theorem [ key ] below ) to consider reverse processes suitably defined on different time domains . in particular",
    "it turns out be fruitful to formulate the forward - reverse representations of the previous section in terms of reverse processes defined on @xmath85 $ ] for suitable @xmath86 .",
    "we therefore introduce the reverseprocess @xmath87 that starts at time @xmath88 at a generic state @xmath89 , is defined on an interval @xmath85 $ ] and satisfies ( [ mc12 ] ) with coefficients ( [ revc ] ) for @xmath90 , that is , ( [ so ] ) solves the sde@xmath91 \\\\[-8pt ] \\nonumber \\mathcal{y}(s)&=&\\exp \\biggl ( \\int_{0}^{s}c_{0,t } \\bigl(u , y(u)\\bigr)\\,du \\biggr).\\end{aligned}\\ ] ] as a result , we have for any fixed @xmath92 , @xmath93 , that @xmath94 whence ( [ jf ] ) and ( [ mt1 ] ) may be equivalently written as@xmath95 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad= { \\int\\!\\!\\!\\int}p\\bigl(t , x , t^{\\ast},x^{\\prime}\\bigr)p\\bigl(t^{\\ast } , y^{\\prime},t , y \\bigr)f\\bigl(x^{\\prime},y^{\\prime}\\bigr)\\,dx^{\\prime}\\,dy^{\\prime}\\end{aligned}\\ ] ] and@xmath96=\\int p \\bigl(t^{\\ast},y^{\\prime},t , y\\bigr)f\\bigl(y^{\\prime } \\bigr)\\,dy^{\\prime } , \\label{mt1a}\\ ] ] respectively .",
    "the main benefit is that the reverse process used in representations  ( [ jf ] ) and ( [ mt1 ] ) depend on both @xmath97 and @xmath47 , while the one used in ( [ jf1a ] ) and ( [ mt1a ] ) depends on @xmath47 only . in particular , ( [ mt1a ] ) may be considered as a reverse representation for all @xmath98 in terms of one and the same reverse process @xmath99 .",
    "let us start with the following lemma .",
    "[ lem ] for any @xmath100 it holds that @xmath101    the first statement is directly obvious from ( [ so1 ] ) . from this the second statement follows by @xmath102    we are now ready to state the following key theorem .",
    "[ key ] given a grid @xmath103 , it holds that @xmath104 \\\\ & & \\qquad= \\int_{\\mathbb{r}^{d\\times l}}f(y_{0},y_{1 } , \\ldots , y_{l-1})\\prod_{i=1}^{l}p(t_{i-1},y_{i-1},t_{i},y_{i})\\,dy_{i-1}\\end{aligned}\\ ] ] with @xmath105 and @xmath106 .",
    "the following proof  much shorter than the proof in a previous version of the paper  has essentially been pointed out to us by an anonymous referee .",
    "we fix @xmath107 ( @xmath108 ) and use induction on @xmath109 . for @xmath110 the statement boils down to  ( [ mt1a ] ) with @xmath111 .",
    "suppose the statement is proved for some @xmath112 .",
    "for the grid @xmath113 we next consider for any test function @xmath114 , @xmath115 \\\\ & & \\qquad=\\mathbb{e } \\bigl [ \\mathbb{e } \\bigl [ f \\bigl ( y_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{0 } ) , y_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{1 } ) , \\ldots , \\\\ & & \\hspace*{68pt}\\qquad\\quad y_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{l-1 } ) , y_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(0 ) \\bigr)\\\\ & & \\hspace*{24pt}\\qquad\\quad { } \\times \\mathcal { y}_{y;t_{l+1}}(t_{l+1}-t_{l } ) \\mathcal{y}_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{0 } ) \\vert\\\\ & & \\hspace*{166pt } y_{y;t_{l+1}}(t_{l+1}-t_{l } ) , \\mathcal{y}_{y;t_{l+1}}(t_{l+1}-t_{l } ) \\bigr ] \\bigr ] \\\\ & & \\qquad=\\mathbb{e } \\bigl [ \\mathcal{y}_{y;t_{l+1}}(t_{l+1}-t_{l } ) \\\\ & & \\hspace*{43pt}{}\\times\\mathbb { e } \\bigl [ f \\bigl(y_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{0 } ) , y_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{1 } ) , \\ldots , \\\\ & & \\hspace*{93pt } \\qquad\\quad y_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{l-1 } ) , y_{y;t_{l+1}}(t_{l+1}-t_{l})\\bigr ) \\\\ & & \\hspace*{97pt}\\qquad\\quad{}\\times\\mathcal{y}_{y_{y;t_{l+1}}(t_{l+1}-t_{l});t_{l}}(t_{l}-t_{0 } ) \\vert y_{y;t_{l+1}}(t_{l+1}-t_{l } ) \\bigr ] \\bigr].\\end{aligned}\\ ] ] by the induction hypothesis , we have that @xmath116 \\\\ & & \\qquad= \\mathbb{e } \\bigl [ f\\bigl(y_{z;t_{l}}(t_{l}-t_{0 } ) , y_{z;t_{l}}(t_{l}-t_{1 } ) , \\ldots , y_{z;t_{l}}(t_{l}-t_{l-1}),z\\bigr ) \\mathcal{y}_{z;t_{l}}(t_{l}-t_{0})\\bigr ] \\\\ & & \\qquad= \\int_{\\mathbb{r}^{d\\times l}}f(y_{0},y_{1 } , \\ldots , y_{l-1},z ) \\prod_{i=1}^{l}p(t_{i-1},y_{i-1},t_{i},y_{i } ) \\,dy_{i-1 } \\eqqcolon f(z)\\end{aligned}\\ ] ] with @xmath117 , and so we obtain @xmath115 \\\\ & & \\qquad=\\mathbb{e } \\bigl [ \\mathcal{y}_{y;t_{l+1}}(t_{l+1}-t_{l})f \\bigl ( y_{y;t_{l+1}}(t_{l+1}-t_{l } ) \\bigr ) \\bigr ] \\\\ & & \\hspace*{-3pt}\\qquad\\stackrel{\\scriptsize{(\\ref{mt1a})}}{= } \\int p(t_{l},z , t_{l+1},y)f(z)\\,dz \\\\ & & \\qquad=\\int_{\\mathbb{r}^{d\\times ( l+1 ) } } f(y_{0},y_{1},\\ldots , y_{l})\\prod_{i=1}^{l+1}p(t_{i-1},y_{i-1},t_{i},y_{i})\\,dy_{i-1},\\end{aligned}\\ ] ] where @xmath118 and the integration variable @xmath119 is renamed to @xmath120 .    for the next theorem , we consider an extended time grid @xmath121 for convenience , we also introduce the notation @xmath122 moreover , we fix a starting point @xmath123 .",
    "[ gen1 ] for any @xmath124 and grids ( [ eq : full - grid ] ) together with ( [ eq : hat - grid ] ) , we have @xmath125 \\\\ & & \\qquad= \\int_{\\mathbb{r}^{d\\times(k+l ) } } f(x_{1},\\ldots , x_{k},y_{0},y_{1 } , \\ldots , y_{l-1 } ) \\\\ & & \\hspace*{75pt}{}\\times\\prod_{i=1}^{k } p(s_{i-1},x_{i-1},s_{i},x_{i})\\,dx_{i } \\prod_{i=1}^{l } p(t_{i-1},y_{i-1},t_{i},y_{i})\\,dy_{i-1}\\end{aligned}\\ ] ] with @xmath126 , @xmath127 , and the processes @xmath1 and @xmath68 being independent .    theorem [ gen1 ]",
    "follows directly from theorem [ key ] by a standard conditioning argument and the chapman  kolmogorov equation .",
    "note that for @xmath128 , theorem [ gen1 ] collapses to theorem [ mt ] .",
    "we are now ready to derive a forward - reverse stochastic representation for the finite dimensional distributions of the process @xmath129 , conditional on @xmath130 , for fixed @xmath131 , and fixed @xmath132 . to this end",
    "we henceforth assume that @xmath133 we also need to assume continuity of @xmath60 .",
    "let us take a bounded measurable test function @xmath134 and consider the conditional expectation @xmath135 \\\\[-8pt ] \\nonumber & & \\hspace*{257pt } { } x_{s_{0},x}(t)=y \\bigr].\\end{aligned}\\ ] ] the distribution of the diffusion @xmath129 conditional on @xmath136 is completely determined by the totality of conditional expectations of the form ( [ ce ] ) .",
    "these conditional expectations may be obtained due to theorem [ thr : conditional - dist - general - grid ] below .",
    "[ thr : conditional - dist - general - grid ] consider the forward process @xmath1 and its reverse process @xmath137 as before and the grids as specified in ( [ eq : full - grid ] ) and ( [ eq : hat - grid ] ) .",
    "let @xmath138 with @xmath37 being integrable on @xmath139 and @xmath140 .",
    "hence , formally @xmath141 converges to the delta function @xmath142 on @xmath139 ( in distribution sense ) as @xmath143 .",
    "then , since @xmath144 by assumption , for any bounded measurable function @xmath145 , we have @xmath146 \\nonumber\\\\ & & \\qquad= \\frac{1}{p(s_{0},x , t , y ) } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad\\quad{}\\times\\lim_{{\\varepsilon}\\downarrow0 } \\mathbb{e } \\bigl [ g \\bigl(x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x } \\bigl(t^{\\ast}\\bigr ) , y_{y;t}(\\widehat{t}_{l-1 } ) , \\ldots , y_{y;t}(\\widehat{t}_{1})\\bigr ) \\\\ & & \\hspace*{113pt}\\qquad\\quad\\times k_{{\\varepsilon } } \\bigl ( y_{y;t}(\\widehat{t}_l)-x_{s_{0},x } \\bigl(t^{\\ast } \\bigr ) \\bigr ) \\mathcal{y}_{y;t } ( \\widehat{t}_l ) \\bigr].\\nonumber\\end{aligned}\\ ] ]    by applying theorem [ gen1 ] to @xmath147 we obtain @xmath148 \\nonumber \\\\ & & \\qquad= \\int_{\\mathbb{r}^{d\\times(k+l ) } } g(x_{1 } , \\ldots , x_{k } , y_{1},\\ldots , y_{l-1 } ) k_{{\\varepsilon}}(y_{1}-x_{k } ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\hspace*{42pt}\\qquad\\quad { } \\times\\prod_{i=1}^{k } p(s_{i-1},x_{i-1},s_{i},x_{i } ) \\,dx_{i } \\prod_{i=1}^{l } p(t_{i-1},y_{i-1},t_{i},y_{i } ) \\,dy_{i-1 } \\\\ & & \\qquad= \\int_{\\mathbb{r}^{d\\times(k+l ) } } g(x_{1 } , \\ldots , x_{k},y_{1},\\ldots , y_{l-1 } ) k(v)\\,dv\\ , p \\bigl(t^\\ast , x_k + { \\varepsilon}v , t_1,y_{2 } \\bigr ) \\nonumber \\\\ & & \\hspace*{42pt}\\qquad\\quad { } \\times\\prod_{i=1}^{k } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } \\prod_{i=2}^{l } p(t_{i-1},y_{i-1},t_{l},y_{i})\\,dy_{i-1}. \\nonumber\\end{aligned}\\ ] ] by sending @xmath38 to zero , ( [ rh ] ) clearly converges to @xmath149 from which ( [ cdy1 ] ) easily follows .",
    "if the original grid @xmath150 is equidistant , then the transformed grid @xmath151 is obtained by a translation with @xmath152 , which leads to the following corollary .",
    "[ cor - equidist - grid ] if the time grid @xmath150 is equidistant , we have @xmath153 \\\\ & & \\qquad=\\frac{1}{p(s_{0},x , t , y)}\\\\ & & \\qquad\\quad{}\\times\\lim_{{\\varepsilon}\\downarrow0}\\mathbb{e } \\bigl [ g \\bigl(x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x } \\bigl(t^{\\ast } \\bigr),y_{y;t}\\bigl(t_{l-1}-t^\\ast \\bigr ) , \\ldots , y_{y;t}\\bigl(t_{1}-t^\\ast\\bigr ) \\bigr ) \\\\ & & \\hspace*{110pt}\\qquad\\quad{}\\times k_{{\\varepsilon } }",
    "\\bigl ( y_{y;t}\\bigl(t - t^\\ast \\bigr)-x_{s_{0},x}\\bigl(t^{\\ast}\\bigr ) \\bigr ) \\mathcal{y}_{y;t } \\bigl(t - t^\\ast\\bigr ) \\bigr].\\end{aligned}\\ ] ] moreover , by setting @xmath154 , we retrieve the forward - reverse representation of the transition density in @xcite , @xmath155,\\label{eq : forward - reverse - density}\\ ] ] expressed with the variant of the reverse process introduced in ( [ so1 ] ) above .",
    "[ rem : cond - diff - pathdep ] for fixed @xmath132 and @xmath156 as before , let us define a process @xmath13 by @xmath157 the idea is that we run along the reverse diffusion @xmath18 backwards in time . then",
    "( [ cdy1 ] ) reads @xmath158 \\\\ & & \\qquad=\\frac{1}{p(s_{0},x , t , y)}\\lim_{{\\varepsilon}\\downarrow0}\\mathbb{e}\\bigl[g \\bigl(x_{s_{0},x}(s_{1}),\\ldots , x_{s_{0},x } \\bigl(t^{\\ast } \\bigr),z(t_{1}),\\ldots , z(t_{l-1})\\bigr ) \\\\ & & \\hspace*{127pt}\\qquad\\quad{}\\times k_{{\\varepsilon } } \\bigl ( z\\bigl(t^{\\ast}\\bigr)-x_{s_{0},x } \\bigl(t^{\\ast}\\bigr ) \\bigr ) \\mathcal{y}_{y;t } \\bigl(t - t^\\ast\\bigr ) \\bigr].\\end{aligned}\\ ] ]      now let us assume that we are interested in the conditional expectation of a functional @xmath159 given @xmath160 for some borel set @xmath4 .",
    "it is assumed for simplicity that either @xmath4 is a subset of @xmath139 with positive lebesgue measure and with @xmath161 , or @xmath4 is an affine hyperplane of dimension @xmath162 , @xmath163 . as a further simplification in the latter case , although without further loss of generality , we assume that @xmath4 is of the form @xmath164 for @xmath163 we consider the `` restricted '' lebesgue measure @xmath165 which coincides with the ordinary lebesgue measure if @xmath166 , and with a dirac point measure if @xmath167 . we next introduce a random variable @xmath168 with support in @xmath4 independent from @xmath1 and @xmath18 , whose law has a density @xmath169 with respect to @xmath170 .",
    "let further @xmath171 denote the reverse process starting at the random location @xmath172 at time @xmath97 . here",
    ", we replace condition ( [ assp ] ) on the positivity of the transition density by @xmath173    [ thr : cond - dist - set ] let the kernel function @xmath37 be as in theorem [ thr : conditional - dist - general - grid ] , and let there be given a time grid of the form ( [ eq : full - grid ] ) .",
    "the conditional expectation of @xmath174 given @xmath175 with @xmath4 being a borel set , either with positive probability or a hyperplane of the form ( [ hyp ] ) , and @xmath5 being a bounded measurable test function , has the stochastic representation @xmath176 \\\\ & & \\qquad=\\lim_{{\\varepsilon}\\downarrow0 } \\mathbb{e } \\biggl[g \\bigl ( x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x}\\bigl(t^{\\ast}\\bigr ) , y_{\\xi;t } ( \\widehat{t}_{l-1 } ) , \\ldots , y_{\\xi;t}(\\widehat{t}_{1 } ) \\bigr ) \\\\ & & \\hspace*{96pt}\\qquad\\quad{}\\times k_{{\\varepsilon } } \\bigl ( y_{\\xi;t}(\\widehat{t}_l)-x_{s_{0},x } \\bigl(t^{\\ast}\\bigr ) \\bigr ) \\frac{\\mathcal{y}_{\\xi;t}(\\widehat{t}_l)}{\\varphi(\\xi ) } \\biggr].\\end{aligned}\\ ] ] in particular , by setting @xmath154 we obtain a stochastic representation for the factor @xmath177.\\ ] ]    let us abbreviate @xmath178 , \\\\",
    "h(y ) & \\coloneqq&\\mathbb{e } \\bigl [   g \\bigl ( x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x}(t_{l-1 } ) \\bigr ) { \\vert}x_{s_{0},x}(t)=y \\bigr],\\end{aligned}\\ ] ] and consider the density of the conditional distribution of @xmath179 given @xmath175 with respect to the measure @xmath170 , that is , @xmath180 recall ( [ eq : assp2 ] ) and the construction ( [ a ] ) of @xmath181 .",
    "then we have @xmath182 q(y)\\lambda_{a}(dy ) \\\\ & = & \\int_{a}h(y)q(y)\\lambda_{a}(dy ) \\\\ & = & \\mathbb{e } \\biggl [ \\frac{h(\\xi)q(\\xi ) } { \\varphi(\\xi ) } \\biggr ] \\\\ & = & \\frac{\\mathbb{e } [ ( { p(s_{0},x , t,\\xi)}/{\\varphi(\\xi)})\\mathbb{e }   [   g ( x_{s_{0},x}(s_{1}),\\ldots , x_{s_{0},x}(t_{l-1 } ) ) { \\vert}x_{s_{0},x}(t)=\\xi ]   ] } { \\int_{a}p(s_{0},x , t , z)\\lambda_{a}(dz)}.\\end{aligned}\\ ] ] hence , denoting @xmath183 , @xmath184 \\biggr ] \\\\ & & \\qquad=\\lim_{{\\varepsilon}\\downarrow0 } \\mathbb{e } \\biggl[g \\bigl ( x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x}\\bigl(t^{\\ast}\\bigr ) , y_{\\xi;t } ( \\widehat{t}_{l-1 } ) , \\ldots , y_{\\xi;t}(\\widehat{t}_{1 } ) \\bigr ) \\\\ & & \\hspace*{95pt}\\qquad\\quad { } \\times k_{{\\varepsilon } } \\bigl ( y_{\\xi;t}(\\widehat{t}_l ) - x_{s_{0},x}\\bigl(t^{\\ast}\\bigr ) \\bigr ) \\frac{\\mathcal{y}_{\\xi;t}(\\widehat{t}_l)}{\\varphi(\\xi ) } \\biggr ] . \\ ] ]    [ cor : cond - dist - comp ] the conditional expectation of @xmath185 given @xmath186 has the stochastic representation @xmath187\\\\ & & \\qquad= \\int_{\\mathbb{r}^{d-1}}p\\bigl(s_{0},x , t , c^{1},y \\bigr)\\,dy \\\\ & & \\quad\\qquad{}\\times \\mathbb{e } \\bigl [   g \\bigl ( x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x}(t_{l-1 } ) \\bigr ) { \\vert}x_{s_{0},x}^{1}(t)=c^{1 } \\bigr]\\end{aligned}\\ ] ] for any @xmath168 taking values in the hyperplane @xmath188 such that @xmath189 is the density of the law of @xmath168 with respect to @xmath170 defined accordingly . in particular , by setting @xmath154 , we obtain a stochastic representation for the marginal density @xmath190 \\\\ & & \\qquad=\\int _ { \\mathbb{r}^{d-1}}p\\bigl(s_{0},x , t , c^{1},y^{2 } , \\ldots , y^{d}\\bigr)\\,dy^{2}\\cdots dy^{d}.\\end{aligned}\\ ] ]    [ rem : markov - chain ] without doubt it is possible to construct analogous stochastic representations for conditional markov chains in the spirit of @xcite .",
    "the details , however , are considered beyond the scope of the present paper .",
    "[ sec : constr - analys - forw ] the stochastic representations for the conditional diffusion problem ( [ cp ] ) , derived in the previous section , naturally lead to respective monte carlo estimators . in this section",
    "we analyze the accuracy of these estimators , under the following assumptions .",
    "first we need suitably regularity of the transition densities of both forward and reverse processes .",
    "[ ass : bound - density ] we assume that the diffusion @xmath1 as well as the reverse diffusion @xmath18 ( not including @xmath39 ) defined in ( [ mc12 ] ) have @xmath191 transition densities @xmath192 and @xmath193 , respectively .",
    "moreover , for fixed @xmath194 , there are constants @xmath195 , @xmath196 , @xmath197 , @xmath198 and @xmath199 such that for any multi - indices @xmath200 with @xmath201 we have @xmath202 uniformly for @xmath203\\times\\mathbb{r}^{d}\\times\\mathbb { r}^{d}$ ] and similarly for @xmath204 .",
    "[ rem : bound - density - restriction ] in fact , for the theorems formulated as below , we only need condition [ ass : bound - density ] for @xmath205 .",
    "higher order versions only become necessary in the context of remark [ rem : higher - order - kernel ] .",
    "[ rem : ass_bound_density_autonomous ] by the results of @xcite , corollary  3.25 , condition [ ass : bound - density ] is satisfied _ in the autonomous case _ provided that ( the vector fields driving ) the forward diffusion @xmath1 and @xmath18 satisfy a uniform hrmander condition , and @xmath206 and @xmath207 are bounded , and @xmath191 bounded ; that is , all the derivatives are bounded as well .",
    "we know of no similar study for nonautonomous stochastic differential equations . of course , the seminal work by @xcite gives upper ( and lower ) gaussian bounds for the transition density of time - dependent , but uniformly elliptic stochastic differential equations .",
    "moreover , @xcite prove the existence and smoothness of transition densities for time - dependent sdes under hrmander conditions .    in any case ,",
    "an extension of the kusuoka ",
    "stroock result to the time - inhomogeneous case seems entirely possible , in particular since we do not consider time - derivatives , for instance , by first considering the case of piecewise constant coefficients .",
    "[ ass : kernel - order ] the kernel @xmath37 satisfies @xmath208 and@xmath209 .",
    "moreover , it has lighter tails than a gaussian density in the sense that there are constants @xmath210 and @xmath211 such that @xmath212    in many applications , one would probably choose a compactly supported kernel , which trivially satisfies the above tail - condition .",
    "finally , we also introduce some further assumptions put forth for convenience , which could be easily relaxed .    [",
    "ass : convenience ] the functional @xmath213 together with its gradient and its hessian are bounded .",
    "moreover , the coefficient @xmath214 in ( [ mc12 ] ) is bounded .",
    "[ rem : convenience ] condition [ ass : convenience ] could be replaced by a requirement of polynomial boundedness .",
    "let us consider @xmath215 \\\\[-8pt ] \\nonumber & & \\hspace*{38pt}\\qquad{}\\times{\\varepsilon}^{-d } k \\biggl ( \\frac{y_{y;t}(\\widehat{t}_l)-x_{s_{0},x}(t^{\\ast})}{{\\varepsilon } } \\biggr ) \\mathcal{y}_{y;t}(\\widehat{t}_l ) \\biggr],\\end{aligned}\\ ] ] which can  and will  be computed using monte carlo simulation . here",
    ", we recall the definition of @xmath216 given in ( [ eq : hat - grid ] ) . by theorem [ thr : conditional - dist - general - grid ] , @xmath217 converges to @xmath218.\\ ] ]    [ bias ] assuming conditions [ ass : bound - density ] , [ ass : kernel - order ] and [ ass : convenience ] , there are constants @xmath219 such that the bias of the approximation @xmath220 can be bounded by @xmath221    changing variables @xmath222 in theorem [ gen1 ] , we arrive at @xmath223 in particular , we have that @xmath224 .",
    "consider @xmath225 in the following , we use the notation @xmath226 , for @xmath227 , @xmath228 . by taylor s formula , conditions [ ass : kernel - order ] and",
    "[ ass : bound - density ] , we get @xmath229 \\,dv \\\\ & = & \\int k(v ) \\bigl [ { \\varepsilon}\\partial_{x } p\\bigl(t^{\\ast } , x_{k } , t_{1 } , y_{1}\\bigr ) \\cdot v \\bigr ] \\,dv \\\\ & & { } + \\sum_{|\\beta| = 2 } \\frac{2}{\\beta ! } { \\varepsilon}^{2 } \\int\\!\\!\\!\\int_{0}^{1 } ( 1-t ) \\partial^{\\beta}_{x } p\\bigl(t^{\\ast},x_{k } + t { \\varepsilon}v , t_{1 } , y_{1}\\bigr ) \\cdot v^{\\beta } \\,dt k(v ) \\,dv\\end{aligned}\\ ] ] implying that @xmath230 where @xmath231 , @xmath232 as given in condition [ ass : bound - density ] , @xmath233 and @xmath234 is chosen such that @xmath235 , which is possible by condition [ ass : kernel - order ] . since @xmath236 we can further compute , using @xmath237",
    ", @xmath238 defining @xmath239 , we get the bound @xmath240 with @xmath241 , which is positive for @xmath242 . consequently ,",
    "for @xmath243 , we can interpret @xmath244 as a ( gaussian ) transition density , which has moments of all orders , for a suitable normalization constant @xmath245 ,  for which we can derive explicit upper bounds .",
    "thus we finally obtain @xmath246 provided that @xmath243 , as the last expression can be interpreted as @xmath247\\ ] ] for a markov process @xmath13 with transition densities @xmath248 , @xmath249 , @xmath250 , @xmath251 , @xmath252 , which admits finite moments of all orders by construction .",
    "note that the constant @xmath253 in the above statement can be explicitly bounded in terms of the bound on @xmath5 , the constants appearing in condition [ ass : bound - density ] and @xmath254 .    in the spirit of @xcite",
    "we now introduce a monte carlo estimator @xmath255 for the quantity @xmath220 introduced in ( [ eq : def - heps ] ) .",
    "let us denote @xmath256 \\\\[-8pt ] \\nonumber & & { } \\times k \\biggl ( \\frac{y_{y;t}^{m}(\\widehat{t}_l ) - x_{s_{0},x}^{n}(t^{\\ast})}{{\\varepsilon } } \\biggr ) \\mathcal{y}_{y;t}^{m } ( \\widehat{t}_l).\\end{aligned}\\ ] ] note that @xmath257 = h_{{\\varepsilon}}$ ] .",
    "the monte carlo estimator is now defined by @xmath258 where the superscripts @xmath259 and @xmath57 denote different , independent realizations of the corresponding processes .",
    "we are left to analyze the variance of the estimator @xmath260 . to this end",
    ", we consider the expectation @xmath261 $ ] for various combinations of @xmath262 and @xmath263 .    for the remainder of the section , we omit the sub - scripts in @xmath1 , @xmath18 and @xmath39 as we keep the initial times and values fixed .",
    "[ lem : z - m - mprime ] for @xmath264 we obtain @xmath265 { \\vert}_{{\\varepsilon}=0 } \\\\ & & \\qquad= \\int g(x_{1 } , \\ldots , x_{k } , y_{1 } , \\ldots , y_{l-1 } ) g \\bigl(x_{1 } , \\ldots , x_{k } , y_{1}^{\\prime } , \\ldots , y_{l-1}^{\\prime}\\bigr ) \\\\ & & \\hspace*{9pt}\\qquad\\quad{}\\times p\\bigl(t^{\\ast } , x_{k } , t_{1 } , y_{1}\\bigr ) p\\bigl(t^{\\ast } , x_{k } , t_{1 } , y_{1}^{\\prime}\\bigr ) \\\\ & & \\hspace*{9pt}\\qquad\\quad{}\\times\\prod_{i=1}^{k } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } \\prod_{i=2}^{l } p(t_{i-1 } , y_{i-1 } , t_{i } , y_{i } ) \\,dy_{i-1 } \\\\ & & \\hspace*{9pt}\\quad\\qquad{}\\times\\prod_{i=2}^{l } p \\bigl(t_{i-1 } , y_{i-1}^{\\prime } , t_{i } , y_{i}^{\\prime}\\bigr ) \\,dy_{i-1}^{\\prime}.\\end{aligned}\\ ] ] moreover , we can bound @xmath266 - \\mathbb{e } \\bigl[z^{\\varepsilon}_{nm } z^{\\varepsilon}_{nm^{\\prime}}\\bigr ] { \\vert}_{{\\varepsilon}=0 } \\bigr{\\vert}\\le c { \\varepsilon}^{2}.\\ ] ]    in what follows , @xmath253 is a positive constant , which may change from line to line .",
    "we have @xmath267 \\\\ & & \\qquad= { \\varepsilon}^{-2d } e \\biggl [ g \\bigl ( x_{s_{1}}^{n } , \\ldots , x_{s_{k}}^{n } , y_{\\widehat{t}_{l-1}}^{m } , \\ldots , y_{\\widehat{t}_{1}}^{m } \\bigr ) g \\bigl ( x_{s_{1}}^{n } , \\ldots , x_{s_{k}}^{n } , y_{\\widehat{t}_{l-1}}^{m^{\\prime } } , \\ldots , y_{\\widehat{t}_{1}}^{m^{\\prime } } \\bigr ) \\\\ & & \\hspace*{129pt}\\qquad\\quad { } \\times k \\biggl ( \\frac{y_{\\widehat{t}_l}^{m}-x_{t^{\\ast}}^{n}}{{\\varepsilon } } \\biggr ) k \\biggl ( \\frac{y_{\\widehat{t}_l}^{m^{\\prime}}-x_{t^{\\ast}}^{n}}{{\\varepsilon } } \\biggr ) \\mathcal{y}_{\\widehat{t}_l}^{m } \\mathcal{y}_{\\widehat{t}_l}^{m^{\\prime } } \\biggr ] \\\\ & & \\qquad = { \\varepsilon}^{-2d } \\int g(x_{1 } , \\ldots , x_{k } , y_{1 } , \\ldots , y_{l-1 } ) g\\bigl(x_{1 } , \\ldots , x_{k } , y_{1}^{\\prime } , \\ldots , y_{l-1}^{\\prime } \\bigr ) \\\\ & & \\hspace*{32pt}\\qquad\\quad{}\\times k \\biggl ( \\frac{y_{0}-x_{k}}{{\\varepsilon } } \\biggr ) k \\biggl ( \\frac{y_{0}^{\\prime}-x_{k}}{{\\varepsilon } } \\biggr ) \\prod_{i=1}^{k } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } \\\\ & & \\hspace*{32pt}\\qquad\\quad{}\\times\\prod_{i=1}^{l } p(t_{i-1 } , y_{i-1 } , t_{i } , y_{i } ) \\,dy_{i-1 } \\prod_{i=1}^{l } p \\bigl(t_{i-1 } , y_{i-1}^{\\prime } , t_{i } , y_{i}^{\\prime}\\bigr ) \\,dy_{i-1}^{\\prime } \\\\ & & \\qquad = \\int g(x_{1 } , \\ldots , x_{k } , y_{1 } , \\ldots , y_{l-1 } ) g\\bigl(x_{1 } , \\ldots , x_{k } , y_{1}^{\\prime } , \\ldots , y_{l-1}^{\\prime}\\bigr ) \\\\ & & \\hspace*{8pt}\\qquad\\quad { } \\times k(v ) k\\bigl(v^{\\prime}\\bigr ) p\\bigl(t^\\ast , x_{k}+{\\varepsilon}v , t_{1 } , y_{1}\\bigr ) \\,dv p \\bigl(t^{\\ast } , x_{k } + { \\varepsilon}v^{\\prime } , t_{1 } , y_{1}^{\\prime}\\bigr ) \\,dv^{\\prime } \\\\ & & \\hspace*{8pt}\\qquad\\quad { } \\times\\prod_{i=1}^{k } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i}\\\\ & & \\hspace*{8pt}\\qquad\\quad { } \\times \\prod_{i=2}^{l } p(t_{i-1 } , y_{i-1 } , t_{i } , y_{i } ) \\,dy_{i-1 } \\\\ & & \\hspace*{8pt}\\qquad\\quad { } \\times\\prod_{i=2}^{l } p \\bigl(t_{i-1 } , y_{i-1}^{\\prime } , t_{i } , y_{i}^{\\prime}\\bigr ) \\,dy_{i-1}^{\\prime},\\end{aligned}\\ ] ] where we changed variables @xmath268 and @xmath269 .",
    "thus , for @xmath270 , we arrive at the above expression , which is treated as a problem - dependent constant .",
    "using condition [ ass : kernel - order ] [ and the short - hand notation @xmath271 , we now consider @xmath272 \\,dv \\,dv^{\\prime } \\\\ & & \\hspace*{-6pt}\\qquad = { \\varepsilon}^{2 } \\int k(v ) k\\bigl(v^\\prime\\bigr)\\\\ & & \\hspace*{-6pt}\\hspace*{21pt}\\qquad\\quad{}\\times \\int _ { 0}^{1 } ( 1-t ) \\biggl [ \\sum _ { i=1}^{d } \\partial_{x}^{2e_{i}}p(x_{k}+t { \\varepsilon}v , y_{1 } ) p\\bigl(x_{k}+t{\\varepsilon}v^{\\prime } , y_{1}^{\\prime}\\bigr ) v_{i}^{2 } \\\\ & & \\hspace*{-6pt}\\hspace*{84pt}\\qquad\\quad { } + \\sum_{i=1}^{d } p(x_{k}+t { \\varepsilon}v , y_{1 } ) \\\\ & & \\hspace*{-6pt}\\hspace*{122pt}\\qquad{}\\times\\partial_{x}^{2e_{i } } p \\bigl(x_{k}+t{\\varepsilon}v^{\\prime } , y_{1}^{\\prime } \\bigr ) \\bigl(v^{\\prime}_{i}\\bigr)^{2 } \\\\ & & \\hspace*{-6pt}\\hspace*{-6pt}\\hspace*{84pt}\\qquad\\quad { } + 2 \\sum_{i , j=1}^{d } \\partial_{x}^{e_{i } } p(x_{k}+t{\\varepsilon}v , y_{1 } ) \\\\ & & \\hspace*{-6pt}\\hspace*{156pt}{}\\times\\partial_{x}^{e_{j } } p\\bigl(x_{k}+t { \\varepsilon}v^{\\prime } , y_{1}^{\\prime}\\bigr ) v_{i } v^{\\prime}_{j } \\,dv \\,dv^{\\prime } \\biggr ] \\,dt \\,dv \\,dv^{\\prime},\\end{aligned}\\ ] ] where , for instance , @xmath273 and @xmath274 . by similar techniques as in the proof of theorem [ bias ] , relying once more on the uniform bounds of condition [ ass : bound - density ] , we arrive at an upper bound @xmath275 for a transition density @xmath276 with gaussian bounds .",
    "consequently , we obtain @xmath277 - \\mathbb{e } \\bigl[z^{\\varepsilon}_{nm } z^{\\varepsilon}_{nm^{\\prime}}\\bigr ] { \\vert}_{{\\varepsilon}=0 } \\bigr{\\vert}\\\\ & & \\qquad\\le c { \\varepsilon}^{2 } \\int\\bigl|g(x_{1 } , \\ldots , x_{k } , y_{1 } , \\ldots , y_{l-1})\\bigr| \\\\ & & \\hspace*{30pt}\\qquad\\quad{}\\times\\bigl|g\\bigl(x_{1 } , \\ldots , x_{k } , y_{1}^{\\prime } , \\ldots , y_{l-1}^{\\prime } \\bigr)\\bigr|\\\\   & & \\hspace*{30pt}\\qquad\\quad{}\\times\\prod _ { i=1}^{k } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } s_{{\\varepsilon}}^{(1,2)}(x_{k } , y_{1 } ) \\,dy_{1 } \\\\ & & \\hspace*{30pt}\\qquad\\quad{}\\times\\prod_{i=3}^{l } p(t_{i-1 } , y_{i-1 } , t_{i } , y_{i } ) \\,dy_{i-1 } \\times s_{{\\varepsilon}}^{(1,2)}\\bigl(x_{k } , y_{1}^{\\prime}\\bigr ) \\,dy_{1}^{\\prime}\\\\ & & \\hspace*{30pt}\\qquad\\quad{}\\times\\prod _ { i=3}^{l } p\\bigl(t_{i-1 } , y_{i-1}^{\\prime } , t_{i } , y_{i}^{\\prime } \\bigr ) \\,dy_{i-1}^{\\prime},\\end{aligned}\\ ] ] which can be bounded by @xmath278 by boundedness of @xmath5 .",
    "in fact , we can find densities @xmath279 and @xmath280 with gaussian tails such that @xmath281 - \\mathbb{e } \\bigl[z^{\\varepsilon}_{nm } z^{\\varepsilon}_{nm^{\\prime}}\\bigr ] { \\vert}_{{\\varepsilon}=0 } \\bigr{\\vert}\\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad \\le c { \\varepsilon}^{2 } \\int \\widetilde{p}\\bigl(s_{0},x , t^{\\ast},x_{k}\\bigr ) \\widetilde{q}\\bigl(t^{\\ast},x_{k},t , y\\bigr)^{2 } \\,dx_{k}.\\ ] ]    when we consider @xmath282 $ ] , we have to take care of terms @xmath283 appearing in the expectation . to this end , let us introduce @xmath284.\\end{aligned}\\ ] ] in what follows , we replace @xmath283 by its conditional expectation @xmath285 and re - write the expectation as an integral w.r.t .",
    "the transition density of the reverse diffusion @xmath18 ; by independence of @xmath1 and @xmath68 , we do not need to condition on @xmath1 as well . note that by condition [ ass : convenience ]",
    ", @xmath286 is a bounded function , and the transition densities @xmath204 of the reverse process @xmath18 satisfy the bounds provided by condition [ ass : bound - density ] as well .    [",
    "lem : zn - nprime ] for @xmath287 we have @xmath288 { \\vert}_{{\\varepsilon}=0 } \\\\ & & \\qquad= \\int g ( x_{1 } , \\ldots , x_{k-1 } , y_{0 } , \\ldots , y_{l-1 } ) g \\bigl ( x_{1}^{\\prime } , \\ldots , x_{k-1}^{\\prime } , y_{0 } , \\ldots , y_{l-1 } \\bigr ) \\\\ & & \\hspace*{9pt}\\qquad\\quad{}\\times\\mu_{2}(y_{0 } , \\ldots , y_{l-1 } ) \\prod _ { i=1}^{k-1 } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } \\\\ & & \\hspace*{9pt}\\qquad\\quad{}\\times{}\\prod",
    "_ { i=1}^{k-1 } p\\bigl(s_{i-1 } , x_{i-1}^{\\prime } , s_{i } , x_{i}^{\\prime } \\bigr ) \\,dx_{i}^{\\prime}\\times \\\\ & & \\hspace*{9pt}\\qquad\\quad{}\\times p(s_{k-1 } , x_{k-1 } , s_{k } , y_{0 } ) p\\bigl(s_{k-1 } , x_{k-1}^{\\prime } , s_{k } , y_{0}\\bigr)\\\\ & & \\hspace*{9pt}\\qquad\\quad{}\\times \\prod_{i=1}^{l } q(\\widehat{t}_{i-1 } , y_{i } , \\widehat{t}_{i } , y_{i-1 } ) \\,dy_{i-1}.\\end{aligned}\\ ] ] moreover , there is a constant @xmath253 such that @xmath289 - \\mathbb{e } \\bigl[z^{\\varepsilon}_{nm } z^{\\varepsilon}_{n^{\\prime}m}\\bigr]{\\vert}_{{\\varepsilon}=0 }   \\bigr{\\vert}\\le { \\varepsilon}^{2 } c.\\ ] ]    we first note that @xmath290 \\\\ & & \\quad= { \\varepsilon}^{-2d } \\mathbb{e } \\biggl [ g \\bigl(x^n_{s_1 } , \\ldots , x^n_{s_k } , y^m_{\\widehat{t}_{l-1 } } , \\ldots , y^m_{\\widehat{t}_1 } \\bigr ) g \\bigl(x^{n'}_{s_1 } , \\ldots , x^{n'}_{s_k } , y^m_{\\widehat{t}_{l-1 } } , \\ldots , y^m_{\\widehat{t}_1 } \\bigr ) \\\\ & & \\hspace*{133pt}\\qquad { } \\times k \\biggl ( { \\frac{y^m_{\\widehat{t}_l } - x^n_{t^\\ast}}{{\\varepsilon } } } \\biggr ) k \\biggl ( { \\frac{y^m_{\\widehat{t}_l } - x^{n'}_{t^\\ast}}{{\\varepsilon } } } \\biggr ) \\bigl ( \\mathcal{y}^m_{\\widehat{t}_l } \\bigr)^2 \\biggr ] \\\\ & & \\quad= { \\varepsilon}^{-2d } \\mathbb{e } \\biggl [ g \\bigl(x^n_{s_1 } , \\ldots , x^n_{s_k } , y^m_{\\widehat{t}_{l-1 } } , \\ldots , y^m_{\\widehat{t}_1 } \\bigr ) g \\bigl(x^{n'}_{s_1 } , \\ldots , x^{n'}_{s_k } , y^m_{\\widehat{t}_{l-1 } } , \\ldots , y^m_{\\widehat{t}_1 } \\bigr ) \\\\ & & \\hspace*{87pt}\\qquad { } \\times k \\bigl ( { \\frac{y^m_{\\widehat{t}_l } - x^n_{t^\\ast}}{{\\varepsilon } } } \\bigr ) k \\biggl ( { \\frac{y^m_{\\widehat{t}_l } - x^{n'}_{t^\\ast}}{{\\varepsilon } } } \\biggr ) \\mu_2 \\biggl ( y^m_{\\widehat{t}_l } , \\ldots , y^m_{\\widehat{t}_1 } \\biggr ) \\biggr].\\end{aligned}\\ ] ]    by a similar approach as in lemma [ lem : z - m - mprime ] , but changing variables @xmath291 and @xmath292 , we arrive at @xmath293\\\\ & & \\qquad = \\int g ( x_{1 } , \\ldots , x_{k-1 } , y_{0}- { \\varepsilon}v , y_{1 } , \\ldots , y_{l-1 } ) \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times g \\bigl ( x_{1}^{\\prime } , \\ldots , x_{k-1}^{\\prime } , y_{0}-{\\varepsilon}v^{\\prime } , y_{1 } , \\ldots , y_{l-1 } \\bigr ) \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times   k ( v ) k \\bigl ( v^{\\prime } \\bigr ) \\mu_{2}(y_{0 } , \\ldots , y_{l-1 } ) \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times\\prod_{i=1}^{k-1 } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } \\prod_{i=1}^{k-1 } p \\bigl(s_{i-1 } , x_{i-1}^{\\prime } , s_{i } , x_{i}^{\\prime}\\bigr ) \\,dx_{i}^{\\prime } \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times p(s_{k-1 } , x_{k-1 } , s_{k } , y_{0}-{\\varepsilon}v ) \\,dv \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times p\\bigl(s_{k-1 } , x_{k-1}^{\\prime } , s_{k } , y_{0}-{\\varepsilon}v^{\\prime}\\bigr ) \\,dv^{\\prime}\\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times\\prod_{i=1}^{l } q ( \\widehat{t}_{i-1 } , y_{i } , \\widehat{t}_{i } , y_{i-1 } ) \\,dy_{i-1}.\\end{aligned}\\ ] ] for @xmath270 , condition [ ass : kernel - order ] implies @xmath294 { \\vert}_{{\\varepsilon}=0 } \\\\ & & \\qquad= \\int g ( x_{1 } , \\ldots , x_{k-1 } , y_{0 } , \\ldots , y_{l-1 } ) g \\bigl ( x_{1}^{\\prime } , \\ldots , x_{k-1}^{\\prime } , y_{0 } , \\ldots , y_{l-1 } \\bigr ) \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times\\mu_{2}(y_{0 } , \\ldots , y_{l-1 } ) \\prod _ { i=1}^{k-1 } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times\\prod",
    "_ { i=1}^{k-1 } p\\bigl(s_{i-1 } , x_{i-1}^{\\prime } , s_{i } , x_{i}^{\\prime } \\bigr ) \\,dx_{i}^{\\prime } \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times p(s_{k-1 } , x_{k-1 } , s_{k } , y_{0 } ) p\\bigl(s_{k-1 } , x_{k-1}^{\\prime } , s_{k } , y_{0}\\bigr ) \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times\\prod_{i=1}^{l } q(\\widehat{t}_{i-1 } , y_{i } , \\widehat{t}_{i } , y_{i-1 } ) \\,dy_{i-1},\\end{aligned}\\ ] ] which gives the formula from the statement of the lemma .",
    "for the bound on the difference , note once again that @xmath295 k(v ) k\\bigl(v^{\\prime}\\bigr ) \\,dv \\,dv^{\\prime}\\end{aligned}\\ ] ] can be bounded in the sense that @xmath296 for transition densities @xmath297 with gaussian tails , so that @xmath298 - \\mathbb{e } \\bigl[z^{\\varepsilon}_{nm } z^{\\varepsilon}_{n^{\\prime}m}\\bigr ] { \\vert}_{{\\varepsilon}=0 } \\bigr{\\vert}\\\\ & & \\qquad \\le c { \\varepsilon}^{2 } \\int\\mu_{2}(y_{0 } , \\ldots , y_{l-1 } ) \\prod_{i=1}^{k-1 } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) \\,dx_{i } \\\\ & & \\qquad\\quad{}\\times\\prod_{i=1}^{k-1 } p \\bigl(s_{i-1 } , x_{i-1}^{\\prime } , s_{i } , x_{i}^{\\prime}\\bigr ) \\,dx_{i}^{\\prime } s_{{\\varepsilon}}^{(2,1)}(x_{k-1 } , y_{0 } ) s_{{\\varepsilon}}^{(2,1)}\\bigl(x_{k-1}^{\\prime},y_{0 } \\bigr ) \\\\ & & \\qquad\\quad{}\\times \\prod_{i=1}^{l } q(\\widehat { t}_{i-1 } , y_{i } , \\widehat{t}_{i } , y_{i-1 } ) \\,dy_{i-1}.\\end{aligned}\\ ] ] if @xmath204 was symmetric , that is , @xmath299 , then this expression would already have the desired form . while symmetry of @xmath204 would be a very strong assumption ,",
    "note that condition [ ass : bound - density ] allows us to bound @xmath300 by a gaussian transition density @xmath301 which is naturally symmetric .",
    "absorbing @xmath302 and @xmath303 into the constant @xmath253 and denoting ( by a mild abuse of notation ) @xmath304 the chapman ",
    "kolmogorov equation implies that @xmath305 - \\mathbb{e } \\bigl[z^{\\varepsilon}_{nm } z^{\\varepsilon}_{n^{\\prime}m}\\bigr ] { \\vert}_{{\\varepsilon}=0 } \\bigr{\\vert}\\nonumber\\\\ & & \\qquad \\le c { \\varepsilon}^{2 } \\int\\widetilde{p}\\bigl(s_{0},x , t^{\\ast},y_{0 } \\bigr)^{2 } \\widetilde{q}\\bigl(t^{\\ast } , y_{0 } , t , y \\bigr ) \\,dy_{0 } \\\\ & & \\qquad\\le c { \\varepsilon}^{2 } \\int\\widetilde{p}\\bigl(s_{0},x , t^{\\ast},y_{0 } \\bigr ) \\widetilde { q}\\bigl(t^{\\ast } , y_{0 } , t , y\\bigr ) \\,dy_{0}.\\nonumber \\ ] ]    [ lem : znm ] we have @xmath306\\\\ & & \\qquad = \\int k(v)^{2 } \\,dv \\int g(x_{1 } , \\ldots , x_{k-1 } , y_{0 } , y_{1 } , \\ldots , y_{l-1 } ) \\\\ & & \\hspace*{63pt}\\qquad\\quad{}\\times\\mu_{2}(y_{0 } , y_{1 } , \\ldots , y_{l-1 } ) \\\\ & & \\hspace*{63pt}\\qquad\\quad{}\\times\\prod_{i=1}^{k-1 } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) p(s_{k-1 } , x_{k-1 } , s_{k } , y_{0 } ) \\\\ & & \\hspace*{63pt}\\qquad\\quad{}\\times\\prod_{i=1}^{l } q ( \\widehat{t}_{i-1 } , y_{i } , \\widehat{t}_{i } , y_{i-1 } ) \\,dx_{1 } \\cdots dx_{k-1 } \\,dy_{0 } \\,dy_{1 } \\cdots dy_{l-1}.\\end{aligned}\\ ] ] moreover , there is a constant @xmath307 such that @xmath308 - \\lim_{{\\varepsilon}\\to0 } { \\varepsilon}^{d } \\mathbb{e } \\bigl [ \\bigl(z^{\\varepsilon}_{nm}\\bigr)^{2 } \\bigr ] \\bigr{\\vert}\\le c { \\varepsilon}^{2}.\\ ] ]    substituting @xmath291 , we obtain @xmath306 \\\\ & & \\qquad=\\int g(x_{1 } , \\ldots , x_{k-1 } , y_{0}-{\\varepsilon}v , y_{1 } , \\ldots , y_{l-1 } ) \\mu_{2}(y_{0 } , y_{1 } , \\ldots , y_{l-1 } ) \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times k(v)^{2 } \\prod_{i=1}^{k-1 } p(s_{i-1 } , x_{i-1 } , s_{i } , x_{i } ) p(s_{k-1 } , x_{k-1 } , s_{k } , y_{0 } - { \\varepsilon}v ) \\\\ & & \\hspace*{8pt}\\qquad\\quad{}\\times\\prod_{i=1}^{l } q ( \\widehat{t}_{i-1 } , y_{i } , \\widehat{t}_{i } , y_{i-1 } ) \\times dx_{1 } \\cdots dx_{k-1 } \\,dv \\,dy_{0 } \\,dy_{1 } \\cdots dy_{l-1}.\\end{aligned}\\ ] ] for @xmath309 the right - hand side gives the statement from the lemma .    for the difference , consider @xmath310 \\,dv.\\end{aligned}\\ ] ] following the procedure established in the previous lemmas",
    ", we obtain @xmath311 and by the argument used in the proof of lemma [ lem : zn - nprime ] , we obtain transition densities function @xmath312 and @xmath313 such that @xmath314 - \\lim _ { { \\varepsilon}\\to0 } { \\varepsilon}^{d } \\mathbb{e } \\bigl [ \\bigl(z_{nm}^{\\varepsilon}\\bigr)^{2 } \\bigr ] \\bigr{\\vert}\\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad \\le c { \\varepsilon}^{2 } \\int\\widetilde{p}\\bigl(s_{0},x , t^{\\ast},y_{0 } \\bigr ) \\widetilde{q}\\bigl(t^{\\ast},y_{0 } , t , y\\bigr ) \\,dy_{0}. \\ ] ]    in what follows , we simplify the notation by the following conventions :    * the constant in theorem [ bias ] is denoted by @xmath315 , that is , @xmath316 ; * for @xmath264 , we set @xmath317 \\eqqcolon h_{{\\varepsilon}}^{(1,2)}$ ] and denote the constant for the difference by @xmath318 , that is , @xmath319 ; * for @xmath287 , we set @xmath282 \\eqqcolon h_{{\\varepsilon}}^{(2,1)}$ ] and denote the constant for the difference by @xmath320 , that is , @xmath321 ; * we set @xmath322 \\eqqcolon h_{{\\varepsilon}}^{(1,1)}$ ] and denote the constant for the difference by @xmath323 , that is , @xmath324 .",
    "[ lem : variance ] the variance of the estimator is given by @xmath325    lemma [ lem : variance ] gives a clarification of the intuitive fact that the variance of @xmath260 explodes as @xmath309 ( and , hence , @xmath326 ) .",
    "indeed , as all the @xmath327 terms have a finite limit , the explosion is exclusively caused by the contribution of @xmath328 = { \\varepsilon}^{-d } h^{(1,1)}_{\\varepsilon}$ ] .",
    "finally , the exploding term @xmath329 will be compensated by the factor @xmath330 .",
    "proof of lemma [ lem : variance ] the result follows immediately by ( [ eq : def - z ] ) , independence of @xmath331 and @xmath332 when both @xmath333 and @xmath264 and the notation introduced above , noting that @xmath334 = h_{{\\varepsilon}}$ ] .",
    "we immediately obtain the following :    [ lem : mse - h ] we assume conditions [ ass : bound - density ] , [ ass : kernel - order ] and [ ass : convenience ] hold .",
    "then the mean square error of the estimator @xmath260 introduced in ( [ eq : hat - heps ] ) for the term @xmath335 defined in  ( [ eq : def - h ] ) satisfies @xmath336\\\\ & & \\qquad \\le \\frac{1-n - m}{nm } h^{2 } + \\frac{m-1}{nm } h^{(1,2)}_{0 } + \\frac { n-1}{nm } h^{(2,1)}_{0 } + \\frac{{\\varepsilon}^{-d}}{nm } h^{(1,1)}_{0 } \\\\ & & \\qquad\\quad{}+ \\frac{{\\varepsilon}^{-d+2}}{nm } c_{1,1 } + { \\varepsilon}^{2 } \\biggl [ 2 \\frac{1-n - m}{nm } c h + \\frac{m-1}{nm } c_{1,2 } + \\frac{n-1}{nm } c_{2,1 } \\biggr]\\\\ & & \\quad\\qquad { } + \\frac{(n-1)(m-1)}{nm } c_0^{2 } { \\varepsilon}^{4}.\\end{aligned}\\ ] ]    similar to @xcite , we can now choose @xmath337 and the bandwidth @xmath38 so as to obtain convergence proportional to @xmath338 in rmse - sense .",
    "[ thr : mse - h - order ] assume conditions [ ass : bound - density ] , [ ass : kernel - order ] and [ ass : convenience ] and set @xmath339 , and @xmath340 dependent on @xmath0 .",
    "* if @xmath341 , choose @xmath342 for some @xmath343 . then we have @xmath344 = \\mathcal{o}(n^{-1})$ ] , so we achieve the optimal convergence rate @xmath345 . * for @xmath346 ,",
    "choose @xmath347 , and we obtain @xmath348 = \\mathcal{o}(n^{-8/(4+d)})$ ] .",
    "insert @xmath349 and the respective choice of @xmath350 in lemma [ lem : mse - h ] .",
    "[ rem : higher - order - kernel ] by replacing the kernel @xmath37 by _ higher order _ kernels , is the order of the lowest order ( nonconstant ) monomial @xmath351 such that @xmath352 . ]",
    "one could retain the convergence rate @xmath345 even in higher dimensions , as higher order kernels lead to higher order estimates ( in @xmath38 ) in lemmas [ lem : z - m - mprime ] , [ lem : zn - nprime ] and  [ lem : znm ] .",
    "so far , we have only computed the quantity @xmath335 as given in ( [ eq : def - h ] ) .",
    "however , finally we want to compute the conditional expectation @xmath353.\\ ] ] as @xmath354 with @xmath335 defined in ( [ eq : def - h ] ) , we need to divide the estimator for @xmath335 by an appropriate estimator for @xmath355in fact , we choose the forward reverse estimator with @xmath356 .",
    "note that we have assumed that @xmath357 . to rule out large error contributions when the denominator is small",
    ", we will discard experiments which give too small estimates for the transition density .",
    "more precisely , we choose our final estimator to be @xmath358 \\\\[-8pt ] \\nonumber & & { } \\times\\mathbf{1}_{({1}/{(nm ) } ) { \\varepsilon}^{-d } \\sum_{n=1}^{n } \\sum_{m=1}^{m } k ( ( { y_{\\widehat{t}_l}^{m } - x_{t^{\\ast}}^{n})}/{{\\varepsilon } } ) \\mathcal{y}_{\\widehat{t}_l}^{m } > \\overline{p}/2},\\end{aligned}\\ ] ] where @xmath359 is a lower bound for @xmath355 ( for fixed @xmath360 ) , which is assumed to be known . and then taking a value at the lower end of a required confidence interval .",
    "see remark  [ rem : nullfolge ] below for a different version of the theorem . in any case",
    ", our numerical experiments suggest that the cut - off can be safely omitted in practice .",
    "keep in mind , however , that the ratio of the asymptotic distributions for numerator and denominator may not have finite moments . ]",
    "[ thr : mse - h ] assume conditions [ ass : bound - density ] , [ ass : kernel - order ] and [ ass : convenience ] and set @xmath339 and @xmath340 dependent on @xmath0 .    * if @xmath341 ( or @xmath361 and higher order kernels are used ) , choose @xmath342 , @xmath362 . then we have @xmath363 = \\mathcal{o}(n^{-1})$ ] , so we achieve the optimal convergence rate @xmath345 . * for @xmath346 ,",
    "choose @xmath347 , and we obtain @xmath363 = \\mathcal{o}(n^{-8/(4+d)})$ ] .",
    "let @xmath364 , and , similarly , let @xmath365 denote the estimator in the denominator , including the normalization factor",
    ". moreover , let @xmath366 as defined in ( [ eq : def - h ] ) and let @xmath367 .",
    "then we have already established in theorem [ thr : mse - h - order ] that @xmath368 & = & \\mathcal{o } \\bigl(n^{-p}\\bigr ) , \\\\",
    "\\mathbb{e } \\bigl [ |y_{n } - y|^{2 } \\bigr ] & = & \\mathcal{o } \\bigl(n^{-p}\\bigr),\\end{aligned}\\ ] ] where @xmath369 for @xmath341 and @xmath370 when @xmath346 .",
    "moreover , we have obtained in lemma [ lem : variance ] that @xmath371 and @xmath372 .",
    "we will now estimate the mean square error for the quotient by splitting it into two contributions , depending on whether @xmath373 is small or large . to this end , let @xmath374 for a constant @xmath375 to be specified below satisfying @xmath376 $ ] ( in fact , for @xmath0 large enough , this constant may be chosen to be @xmath377 ) .",
    "then we have @xmath378 & = & \\mathbb{e } \\biggl [ \\frac { ( x_{n } y - y_{n } x   ) ^{2}}{(y_{n}y)^{2 } } \\mathbf{1}_{y_{n } > d_{n } } \\biggr ] \\nonumber \\\\ & \\le&\\frac{\\mathbb{e } [   ( y(x_{n}-x ) + x(y - y_{n } )   ) ^{2 } ] } { y^{2 } d_{n}^{2 } } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & \\le&2 \\frac{y^{2 } \\mathbb{e}[(x_{n}-x)^{2 } ] + x^{2 } \\mathbb{e } [ ( y - y_{n})^{2 }   ] } { y^{2 } d_{n}^{2 } } \\\\ & \\le&\\frac{c^{1}_{x , y}}{d_{n}^{2 } n^{p}},\\nonumber\\end{aligned}\\ ] ] where we used the estimates on the mses for numerator and denominator . on the other hand , we have , using that @xmath379 , chebyshev s inequality and our estimate on the variance of @xmath373 , @xmath380 \\\\[-8pt ] \\nonumber & \\le&\\frac{\\operatorname{var } y_{n } } { ( \\mathbb{e } y_{n } - d_{n } ) ^{2 } } \\\\ & \\le&\\frac{c^{2}_{y } } { ( \\mathbb{e } y_{n } - d_{n }   ) ^{2 } n^{p}}.\\nonumber\\end{aligned}\\ ] ] finally , consider @xmath381 & = & \\mathbb{e } \\biggl [ \\biggl ( \\zeta_{n } - \\frac{x}{y } \\mathbf{1}_{y_{n } > d_{n } } - \\frac{x}{y } \\mathbf{1}_{y_{n } \\le d_{n } } \\biggr ) ^{2 } \\biggr ] \\nonumber \\\\ & = & \\mathbb{e } \\biggl [ \\biggl ( \\zeta_{n } - \\frac{x}{y } \\mathbf{1}_{y_{n}>d_{n } } \\biggr)^{2 } \\biggr ] + \\frac{x^{2}}{y^{2 } } \\mathbb{p } ( y_{n } \\le d_{n } ) \\\\",
    "& \\le&\\frac{c^{1}_{x , y}}{d_{n}^{2 } n^{p } } + \\frac{c^{2}_{y } x^{2 } } { ( \\mathbb{e } y_{n } - d_{n }   ) ^{2 } y^{2 } n^{p}},\\nonumber\\end{aligned}\\ ] ] where we have combined ( [ eq:2 ] ) and ( [ eq:4 ] ) .",
    "now choose @xmath382 for @xmath0 large enough .",
    "as @xmath383 , ( [ eq:6 ] ) implies that @xmath384 = \\mathcal{o } \\bigl(n^{-p}\\bigr ) . \\ ] ]    [ rem : nullfolge ] alternatively , we could replace the cut - off @xmath377 in ( [ eq : h - hat - def ] ) by some sequence @xmath385 . in that case , the mse of the estimator is of order @xmath386 , which can be chosen as close to @xmath387 as desired by proper choices of ( slowly convergent ) sequences @xmath375 .",
    "note that finally @xmath388 in the proof of theorem [ thr : mse - h ] , as @xmath389 by assumption .      in theorem [ thr : cond - dist - set ] and corollary [ cor : cond - dist - comp ] we have derived a representation of the conditional expectation of a functional @xmath5 of the process @xmath1 given that @xmath390 ( for a borel set @xmath4 with positive probability ) or given @xmath391 . in analogy to the first part of this section",
    ", one can construct monte carlo estimators for these conditional expectations and analyze their bias and variance . in what follows",
    ", we assume that @xmath4 is either a general borel set with positive probability or an affine surface , that is , we treat both cases distinguished above together .",
    "recall that we represented the conditional expectation as @xmath392 \\\\ & & \\qquad= \\int_{a } p(s_{0},x , t , y ) \\lambda_{a}(dy ) \\mathbb{e } \\bigl [   g \\bigl ( x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x}(t_{l-1 } ) \\bigr ) { \\vert}x_{s_{0},x}(t ) \\in a \\bigr],\\end{aligned}\\ ] ] where @xmath168 is an independent random variable taking values in @xmath4 with density @xmath393 with respect to @xmath170 . in order to arrive at an estimator with bounded variance",
    ", we need to restrict the choice of @xmath393 and , consequently , @xmath168 .",
    "[ ass : varphi ] the density @xmath393 has ( strictly ) super - gaussian tails , that is , there are constants @xmath394 such that @xmath395    we define the following monte carlo estimator for the conditional expectation @#1@@@(#1@italiccorr ) @xmath396 where @xmath397 , @xmath398 , are independent samples from the solution of the forward process @xmath1 started at @xmath399 and @xmath400 together with @xmath401 , @xmath402 , are independent samples from the reverse process @xmath403 started at @xmath404 , @xmath405 , for an independent sequence of samples @xmath406 from the distribution  @xmath168 .",
    "apart from the term @xmath407 , the difference to estimator ( [ eq : h - hat - def ] ) is the randomness of the initial values of the reverse process .",
    "again , @xmath408 , and remark  [ rem : nullfolge ] applies .",
    "the analysis of ( [ eq : h - comp - def ] ) , however , works along the lines of the analysis of ( [ eq : h - hat - def ] ) . indeed , in all the expectations considered in theorem [ bias ] and in lemmas  [ lem : z - m - mprime][lem : znm ]",
    ", we obtain the same kind of results by the following steps :    condition on @xmath168 and pull out the factor @xmath409 ( possibly with indices @xmath57 and/or @xmath263 ) ;    use the results obtained in section  [ sec : forw - reverse - estim - state ] , with constants depending on the value of @xmath168 ;    move @xmath410 back in and take the expectation in @xmath168 .",
    "[ thr : mse - comp ] set @xmath349 and assume condition [ ass : varphi ] and , as usual , condition [ ass : bound - density ] , [ ass : kernel - order ] and [ ass : convenience ] .",
    "* if @xmath341 , choose @xmath342 , @xmath411 .",
    "then the mse of the forward - reverse estimator @xmath412 is @xmath413 . * for @xmath346 , choose @xmath347 .",
    "then the mse of the forward - reverse estimator @xmath412 is @xmath414 .    in this proof ,",
    "the constant @xmath253 may change from line to line .",
    "define @xmath415 , \\\\",
    "h^{\\xi}_{{\\varepsilon } } & \\coloneqq&\\mathbb{e } \\biggl[g \\bigl ( x_{s_{0},x}(s_{1 } ) , \\ldots , x_{s_{0},x } \\bigl(t^{\\ast}\\bigr),y_{\\xi;t}(\\widehat{t}_{l-1}),\\ldots , y_{\\xi;t}(\\widehat{t}_{i } ) \\bigr ) \\\\ & & \\hspace*{80pt } { } \\times k_{{\\varepsilon } } \\bigl ( y_{\\xi;t}(\\widehat{t}_l)-x_{s_{0},x } \\bigl(t^{\\ast } \\bigr ) \\bigr ) \\frac{\\mathcal{y}_{\\xi;t}(\\widehat{t}_l)}{\\varphi(\\xi ) } \\biggr ] , \\\\ z^{{\\varepsilon},\\xi}_{nm } & \\coloneqq&\\frac{1}{{\\varepsilon}^{d } } g \\bigl ( x_{s_{0},x}^{n}(s_{1 } ) , \\ldots , x_{s_{0},x}^{n}(s_{k } ) , y_{\\xi^{m};t}^{m}(\\widehat{t}_{l-1 } ) , \\ldots , y_{\\xi^{m};t}^{m}(\\widehat{t}_{1 } )",
    "\\bigr ) \\\\ & & { } \\times k \\biggl ( \\frac{y_{\\xi^{m};t}^{m}(\\widehat{t}_l ) - x_{s_{0},x}^{n}(t^{\\ast } ) } { { \\varepsilon } } \\biggr ) \\frac{\\mathcal{y}^{m}_{\\xi^{m};t}(\\widehat{t}_l)}{\\varphi(\\xi^{m})},\\end{aligned}\\ ] ] and notice that the result will follow if we can establish the bounds of theorem [ bias ] and lemmas [ lem : z - m - mprime ] , [ lem : zn - nprime ] and [ lem : znm ] for @xmath335 , @xmath220 and @xmath331 replaced by @xmath416 , @xmath417 and @xmath418 , respectively .    for the bias , ( [ eq : bias - bound - explicit ] )",
    "implies a bound @xmath419 for some density @xmath279 in @xmath51 , where we make the dependence of @xmath335 and @xmath220 on @xmath51 explicit .",
    "consequently , conditioning on @xmath168 first , we have @xmath420 \\biggr{\\vert}\\nonumber \\\\ & \\le&\\mathbb{e } \\biggl [ \\frac{|h(\\xi ) - h_{{\\varepsilon}}(\\xi)|}{\\varphi(\\xi ) } \\biggr ] \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & \\le & c { \\varepsilon}^{2 } \\int\\frac{\\widetilde{p}(s_{0},x , t,\\xi)}{\\varphi(\\xi ) } \\varphi(\\xi ) \\,d\\xi \\\\ & \\le & c { \\varepsilon}^{2}.\\nonumber\\end{aligned}\\ ] ]    similarly , using the estimate from lemma [ lem : z - m - mprime ] , denoting @xmath421 $ ] , where we assume @xmath422 and @xmath423 , we get , using a simple adaptation of ( [ eq : zm - mprime ] ) for different terminal values @xmath51 and @xmath424 , @xmath425 -   \\mathbb{e } \\bigl [ z_{nm}^{{\\varepsilon},\\xi}z_{nm^{\\prime}}^{\\xi } \\bigr ] | _ { { \\varepsilon}=0 } \\bigr{\\vert}\\nonumber \\\\ & & \\qquad\\le\\mathbb{e } \\biggl [ \\frac{{\\vert}z^{\\varepsilon}_{n , m , m^{\\prime } } ( \\xi^{m},\\xi^{m^{\\prime } } ) - z^{\\varepsilon}_{n , m , m^{\\prime}}(\\xi^{m},\\xi^{m^{\\prime } } ) { \\vert}_{{\\varepsilon}=0 } { \\vert}}{\\varphi(\\xi^{m } ) \\varphi(\\xi^{m^{\\prime } } ) } \\biggr ] \\nonumber \\\\ & & \\qquad \\le c { \\varepsilon}^{2 } \\mathbb{e } \\biggl [ \\frac{\\int\\widetilde{p}(s_{0 } , x , t^{\\ast},x_{k } ) \\widetilde{q}(t^{\\ast},x_{k } , t , \\xi^{m } ) \\widetilde { q}(t^{\\ast},x_{k},t,\\xi^{m^{\\prime } } ) \\,dx_{k}}{\\varphi(\\xi^{m } ) \\varphi ( \\xi^{m^{\\prime } } ) } \\biggr ] \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad = c{\\varepsilon}^{2 } \\int\\widetilde{p}\\bigl(s_{0},x , t^{\\ast},x_{k } \\bigr ) \\widetilde { q}\\bigl(t^{\\ast},x_{k } , t , y\\bigr)\\\\ & & \\hspace*{28pt}\\qquad\\quad{}\\times \\widetilde{q}\\bigl(t^{\\ast},x_{k},t , y^{\\prime}\\bigr ) \\,dx_{k } \\lambda_{a}(dy ) \\lambda_{a } \\bigl(dy^{\\prime}\\bigr ) \\nonumber \\\\ & & \\qquad \\le c { \\varepsilon}^{2}.\\nonumber\\end{aligned}\\ ] ]    adopting the above notation for the case @xmath426 covered in lemma [ lem : zn - nprime ] and using ( [ eq : zn - nprime ] ) , we get @xmath427 -   \\mathbb{e } \\bigl [ z_{nm}^{{\\varepsilon},\\xi}z_{n^{\\prime}m}^{{\\varepsilon},\\xi } \\bigr ] { \\vert}_{{\\varepsilon}=0}\\bigr { \\vert}\\\\ & & \\qquad \\le\\mathbb{e } \\biggl [ \\frac{{\\vert}z^{\\varepsilon}_{n , n^{\\prime } , m}(\\xi^{m},\\xi^{m } ) -   z^{\\varepsilon}_{n , n^{\\prime},m}(\\xi^{m},\\xi ^{m } ) { \\vert}_{{\\varepsilon}=0 } { \\vert}}{\\varphi(\\xi^{m } ) \\varphi(\\xi^{m } ) } \\biggr ] \\\\ & & \\qquad \\le c { \\varepsilon}^{2 } \\int\\frac{\\widetilde{p}(s_{0},x , t^{\\ast},y_{1 } ) \\widetilde{q}(t^{\\ast},y_{1 } , t , y)}{\\varphi(y ) } \\,dy_{1 } \\lambda_{a}(dy).\\end{aligned}\\ ] ] by assumption the density @xmath428 has gaussian tails , whereas @xmath393 was assumed to have strictly sub - gaussian tails .",
    "this implies that the above integral is finite , and we get the bound @xmath429 -   \\mathbb{e } \\bigl [ z_{nm}^{{\\varepsilon},\\xi } z_{n^{\\prime}m}^{{\\varepsilon},\\xi } \\bigr ] { \\vert}_{{\\varepsilon}=0 } \\bigr{\\vert}\\le c { \\varepsilon}^{2}.\\ ] ]    in a similar way , using ( [ eq : znm ] ) , we get the bound @xmath430 - \\lim _ { { \\varepsilon}\\to0 } { \\varepsilon}^{d } \\mathbb{e } \\bigl [ \\bigl(z_{nm}^{{\\varepsilon},\\xi}\\bigr)^{2 } \\bigr ] \\bigr{\\vert}\\le c { \\varepsilon}^{2}.\\ ] ]    the respective versions of lemmas [ lem : variance ] , [ lem : mse - h ] and theorem [ thr : mse - h - order ] follow immediately from the bounds ( [ eq : bias - bound - xi ] ) , ( [ eq : zm - mprime - bound - xi ] ) , ( [ eq : zn - nprime - bound - xi ] ) and ( [ eq : znm - bound - xi ] ) , and we can repeat the proof of theorem [ thr : mse - h ] , arriving at the conclusion .",
    "we again stress that the nonoptimal complexity rate in theorem [ thr : mse - comp ] can be improved to the optimal one even for @xmath346 by remark [ rem : higher - order - kernel ] .",
    "theorems [ thr : mse - h ] and [ thr : mse - comp ] above present the asymptotic analysis of the mse for the forward - reverse estimator . in practice , for many methods with very good asymptotic rates , limitations arise due to potentially high constants , and the forward - reverse estimator is no exception .",
    "in fact , this can be already seen in a very simple example , where all the estimates can be given explicitly .    for @xmath431 , consider the one - dimensional ornstein  uhlenbeck process @xmath432 for @xmath433 .",
    "the corresponding reverse process satisfies @xmath434 for a brownian motion @xmath435 .",
    "moreover , @xmath436 .",
    "we first discuss the estimator @xmath437 introduced in ( [ eq : hat - heps ] ) for the numerator of the forward - reverse estimator @xmath438 for @xmath439 with @xmath440 .",
    "of course , we expect that the findings for this special case carry over to situations with nonconstant @xmath5 and @xmath441 .",
    "after elementary but tedious calculations [ milstein , schoenmakers and spokoiny ( @xcite ) , section  4 ] one arrives at @xmath442 = { \\frac{1}{\\sqrt { 2 \\pi \\bigl ( { \\varepsilon}^2 e^{-2\\alpha(t - t^\\ast ) } + \\sigma^2_t \\bigr ) } } } \\exp \\biggl ( - { \\frac { ( e^{-\\alpha t}x - y ) ^2}{2 ( { \\varepsilon}^2 e^{-2\\alpha(t - t^\\ast ) } + \\sigma^2_t ) } } \\biggr)\\ ] ] and @xmath443 where @xmath444 thus , all the terms in the mse [ composed of the square of ( [ eq : ou - example - mean ] ) and ( [ eq : ou - example - var ] ) ] exhibit fairly moderate constants , except for the last term in ( [ eq : ou - example - var ] ) . indeed ,",
    "when @xmath445 , we have @xmath446 , unless @xmath447 . in other words , the constant in theorem  [ thr : mse - h - order ] will be quite large if @xmath448 and @xmath449 . that observation is quite intuitive in view of ( [ eq : ou - example ] ) and ( [ eq : ou - example - reverse ] ) : @xmath450 is contracting to @xmath451 as time increases , whereas @xmath452 is exponentially expanding away from @xmath51 .",
    "thus , the probability of @xmath453 and @xmath454 be close to each other is very small .",
    "[ rem : ou - example-1 ] note that the last term in ( [ eq : ou - example - var ] ) is the term estimated in lemma  [ lem : znm ] .",
    "the constant in the lemma depends on the constant in condition  [ ass : bound - density ] for the derivatives of the transition density @xmath455 with respect to the @xmath456-variable . for the ornstein  uhlenbeck process",
    ", the density is given by @xmath457 therefore , we see that derivatives with respect to @xmath456 ( and , hence , the corresponding constants ) are considerably larger than derivatives with respect to @xmath458 .",
    "this explains why the last term ( and no other term ) in ( [ eq : ou - example - var ] ) causes problems for @xmath459 large .",
    "[ rem : ou - example - gen ] there is also a source of error due to the form of @xmath460 as a fraction of two terms .",
    "the error of an approximation @xmath461 of a quantity of interest @xmath462 by the fraction of the approximations @xmath463 for @xmath464 and @xmath465 for @xmath466 with corresponding ( absolute ) errors @xmath467 and @xmath468 is controlled by the _ relative _",
    "errors for @xmath464 and @xmath466 . indeed , assume for simplicity that @xmath469 and @xmath470 , then @xmath471 which may be close to @xmath16 if the relative error @xmath472 for the denominator @xmath466 is large .",
    "some care is necessary when implementing the forward reverse estimators ( [ eq : h - hat - def ] ) and ( [ eq : h - comp - def ] ) for expectations of a functional of the diffusion bridge between two points or a point and a subset .",
    "this especially concerns the evaluation of the double sum .",
    "indeed , straightforward computation would require the cost of @xmath473 kernel evaluations which would be tremendous , for example , when @xmath474 .",
    "but , fortunately , by using kernels with an ( in some sense ) small support we can get around this difficulty as outlined below ; see also @xcite for a similar discussion .",
    "we here assume that the kernel @xmath475 used in ( [ eq : h - hat - def ] ) and ( [ eq : h - comp - def ] ) , respectively , has bounded support contained in some ball of radius @xmath476 , an assumption which is easily fulfilled in practice .",
    "for instance , even though the gaussian kernel @xmath477 has unbounded support , in practice @xmath475 is negligible outside a finite ball ( with exponential decay of the value as function of the radius ) . therefore , it is easy to choose a ball @xmath478 such that @xmath37 is smaller than some error tolerance @xmath479 outside the ball .",
    "depends on the size of the constants in the mse bound .",
    "] then , due to the small support of @xmath37 , the following monte carlo algorithm for the kernel estimator is feasible . for simplicity , we take @xmath480 .",
    "[ we present the algorithm only for the case of ( [ eq : h - hat - def ] ) , the analysis being virtually equal for ( [ eq : h - comp - def ] ) . ] here , the input variable @xmath481 denotes the grid ( [ eq : full - grid ] ) .",
    "simulate @xmath0 trajectories @xmath482 of the forward process on @xmath483 .",
    "simulate @xmath0 trajectories @xmath484 of the reverse process on @xmath485 .",
    "find the sub - sample @xmath486 evaluate ( [ eq : h - hat - def ] ) by    the complexity of the simulation steps ( 2 ) and ( 3 ) in algorithm [ alg : algorithm ] is @xmath487 and @xmath488 elementary computations , respectively .",
    "the size @xmath489 of the intersection in step ( 5 ) of algorithm [ alg : algorithm ] is , on average , proportional to @xmath490 .",
    "the search procedure itself can be done at a cost of order @xmath491 ( neglecting the cost of comparison between two integers ) .",
    "thus , we get the complexity bounds summarized in theorem [ thr : complexity ] below .",
    "[ thr : complexity ] assume that samples from the forward process @xmath1 and the reverse process @xmath137 can be obtained at constant cost .",
    "furthermore , assume that the cost of checking for equality of integers carries negligible cost .",
    "then the following asymptotic bounds hold for the complexity of algorithm  [ alg : algorithm ] :    * if @xmath341 , we choose @xmath492 , implying that the mse of the output of the algorithm is @xmath413 with a complexity @xmath493 ; * if @xmath346 , we choose @xmath494 and obtain an mse of @xmath414 with a complexity @xmath493 .",
    "we present two numerical studies : in the first example , the forward process is a two - dimensional brownian motion , with the standard brownian bridge as the conditional diffusion . in the second example , we consider a heston model whose stock price component is conditioned to end in a certain value . in both examples , we actually use a gaussian kernel @xmath495 and the simulation as well as the functional @xmath5 of interest are defined on a uniform grid @xmath496 with @xmath497 and @xmath498 for @xmath499 and @xmath500 .    [ ex : brownian_bridge ] we consider @xmath501 , a two - dimensional standard brownian motion , which we condition on starting at @xmath502 and ending at @xmath503 , that is , the conditioned diffusion is a classical two - dimensional brownian bridge . in particular , the reverse process @xmath504 is also a standard brownian motion , and @xmath505 .",
    "we consider the functional @xmath506 where @xmath507 . in this simple toy - example",
    ", we can actually compute the true solution @xmath508 = \\frac{1}{6 } \\frac{l+1}{l-1}.\\ ] ] as evaluation of the functional @xmath5 is cheap in this case , we use a naive algorithm calculating the full double sum .",
    "we choose @xmath339 and @xmath509 , which still gives the rate of convergence obtained in theorem [ thr : mse - h ] .    .",
    "dashed lines are reference lines proportional to @xmath510 . ]    in figure  [ fig : ex_bb ] , we show the results for @xmath511 , with the choices @xmath512 and @xmath513 , that is , with @xmath514 and @xmath515 , respectively . in both case , we observe the asymptotic relation @xmath516 predicted by theorem [ thr : mse - h ] .",
    "the mse is slightly lower when @xmath46 is closer to the middle of the interval @xmath517 $ ] [ case ( b ) ] as compared to the situation when @xmath46 is close to the boundary [ case ( a ) ] .",
    "intuitively , one would expect such an effect , as in the latter case the forward process can only accumulate a considerably smaller variance as compared to the reverse process .",
    "however , it should be noted that the effect is rather small . or @xmath518 .",
    "mathematically , this is a consequence of the transition densities getting singular . ]",
    "[ ex : heston_bridge ] let us consider the stock price @xmath519 in a heston model : @xmath520 , that is , the stock price together with its ( stochastic ) volatility satisfies the stochastic differential equation @xmath521 we have @xmath522 as this process is time - homogeneous , we have @xmath523 , and the remaining coefficients of the sde for the reverse process are given by @xmath524 as path - dependent functional we consider the _ realized variance _ of the stock - price , that is , for a grid as above we consider @xmath525 ( dependence of the functional @xmath5 on the final value @xmath51 obviously changes nothing in the theorems of sections  [ main ] and  [ analysis ] . ) we choose @xmath526 and @xmath527 .",
    "this time , however , we only condition on the value of the stock component at final time @xmath47 .",
    "for the calculations , we use the following , arbitrarily chosen parameters : @xmath528 , @xmath529 , @xmath530 , @xmath531 , @xmath532 .",
    "the initial stock price and the initial variance were set to @xmath533 and @xmath534 , respectively .",
    "moreover , the realized variance was computed conditionally on @xmath535 , and we choose the standard normal density for @xmath393 , despite condition  [ ass : varphi ] .",
    "contrary to example [ ex : brownian_bridge ] , we can not produce samples from the exact distributions of either the forward or the reverse processes @xmath536 or @xmath537 .",
    "thus , we approximate the corresponding paths using the euler  maruyama scheme on a uniform grid with mesh @xmath538 , so that the mse for the solution of the corresponding sde is itself @xmath413 , implying that the asymptotic order of the mse of our quantity of interest is not affected by the numerical approximation of the forward and backward processes .",
    "moreover , evaluation of the functional @xmath5 is quite costly due to the numerous calls of the @xmath539-function .",
    "thus , we use the cut - off procedure introduced above , so that the individual terms in the double sum are only included when the value of the kernel @xmath540 is larger than @xmath541 .",
    "the main parameters of the forward - reverse algorithm are chosen as @xmath339 and @xmath542 , so that we are in the regime of theorem [ thr : mse - comp ] .    .",
    "dashed lines are reference lines proportional to  @xmath510 . ]",
    "the numerical results in figure  [ fig : ex_heston ] confirm the rate of convergence for the mse established in theorem [ thr : mse - comp ] .",
    "again , there is no significant advantage of choosing @xmath46 in the middle of the relevant interval @xmath85 $ ] .",
    "the `` exact '' reference value was computed using the forward - reverse algorithm with very large @xmath0 , corresponding small @xmath38 and a very fine grid for the euler scheme .",
    "note that figure  [ fig : ex_heston ] depicts the `` relative mse , '' that is , the mse normalized by the squared reference value .",
    "we are very grateful to an anonymous referee , who has pointed out to us the way to a much shorter and more transparent proof of the main theorem [ key ] .",
    "moreover , the paper has profited from various comments made by the referee , which improved the notation and general presentation of the paper .",
    "we are also grateful to g. n. milstein for providing us with enlightening references ."
  ],
  "abstract_text": [
    "<S> in this paper we derive stochastic representations for the finite dimensional distributions of a multidimensional diffusion on a fixed time interval , conditioned on the terminal state </S>",
    "<S> . the conditioning can be with respect to a fixed point or more generally with respect to some subset . </S>",
    "<S> the representations rely on a reverse process connected with the given ( forward ) diffusion as introduced in milstein , schoenmakers and spokoiny [ _ bernoulli _ * 10 * ( 2004 ) 281312 ] in the context of a forward - reverse transition density estimator . </S>",
    "<S> the corresponding monte carlo estimators have essentially root-@xmath0 accuracy , and hence they do not suffer from the curse of dimensionality . we provide a detailed convergence analysis and give a numerical example involving the realized variance in a stochastic volatility asset model conditioned on a fixed terminal value of the asset . </S>"
  ]
}