{
  "article_text": [
    "causality is commonly defined based on the widely accepted assumption that an effect is always preceded by its cause .",
    "@xcite postulates a measure of causal influence between two time series ( _ granger causality _ ) . in a nutshell , time series @xmath1",
    "granger - causes time series @xmath2 if knowledge of past values of @xmath1 improves the prediction of @xmath2 ( compared to only using past values of @xmath2 ) . in the case of a set @xmath3 of time series , the pairwise analysis may lead to spurious detection of a causal relation . for this reason it is advisable to include the set @xmath4 of all additional observable time series in both prediction tasks .",
    "note that this approach resolves the problem of common hidden factors @xmath5 if @xmath6 .",
    "( if common factors are not observable , granger causality fails and we refer to @xcite for a detailed discussion and a remedy in form of the phase slope index . ) while this approach , to which we refer as _ complete _ granger causality , is practical , a more elegant way to deal with multivariate data is to handle all potential causal relations between all time series at once . in this paper , we assume a linear dynamics of the underlying system , which leads to the vector autoregressive ( var ) model .    in many applications the true causality graph is assumed to be sparse , i.e. only a few causal interactions between time series are expected .",
    "however , both ordinary least squares ( ols ) and ridge regression , which are usually used for fitting var models , are known for producing dense coefficients . only recently @xcite",
    "have proposed to enforce estimation of sparse ar coefficients using @xmath7-norm regularized models such as the lasso @xcite .    in this paper",
    "we propose a novel sparse approach which  unlike lasso ",
    "accounts for the fact that the absence of a causal relation between @xmath1 and @xmath2 requires all ar coefficients belonging to that certain pair of time series to be jointly zero .",
    "furthermore , we consider ridge regression in combination with the multiple statistical testing procedure provided by @xcite .",
    "more details on the methodology are given in section [ sec : methods ] .",
    "these methods are evaluated and compared to standard approaches in extensive simulations .",
    "in this section , we briefly summarize related approaches to estimate sparse vector autoregressive models in the context of causal discovery .",
    "we roughly distinguish between sparse estimation methods and testing strategies .    given a multivariate time series @xmath8 , a linear vector autoregressive process of order @xmath9",
    "is defined as    @xmath10    where @xmath11 , @xmath12 and @xmath13 indicates time .",
    "hence , the signal at time @xmath14 is modeled as a linear combination of its @xmath9 past values and gaussian measurement noise .",
    "inspired by the initial assumption that the cause should always precede the effect , we suggest the following definition of causality in the case of vector autoregressive models .",
    "we say that time series @xmath1 has a causal influence on time series @xmath2 if for at least one @xmath15 , the coefficient @xmath16 corresponding to the interaction between @xmath2 and @xmath1 at the @xmath17th time - lag is nonzero .",
    "thus , causal inference may be conducted by estimating the matrices @xmath18 from a sample @xmath19 .",
    "let us introduce the following shortcuts .",
    "we denote by @xmath20 the matrix of all var coefficients and set @xmath21 , @xmath22 , @xmath23 . here",
    "@xmath24 denotes the vectorization operation .",
    "probably the most straightforward way to estimate a sparse var is to use @xmath7-regularization on the set of coefficients , @xmath25    recently , @xcite proposed a combination of var - estimation and the lasso @xcite .",
    "while @xcite only consider a var model of order @xmath26 , there have been extensions to higher orders ( e.g. * ? ? ?",
    "* ) . however , we note in the latter case , lasso is not used on the var coefficients directly , but that the problem is transformed into the task of estimating partial correlation coefficients between time - lagged copies of the time series ( see also * ? ? ?",
    "just as in the case of sparse methods , it is often suggested to transform the regression task into the estimation of the matrix of partial correlation coefficients between time - lagged copies of the time series . while @xcite estimate the correlation matrix in an unregularized way , @xcite propose a shrinkage estimator , which is superior in the case of high - dimensional data @xcite .",
    "afterwards , significant partial correlations are detected by controlling false discovery rates .",
    "while the latter approach is only tested for @xmath27 , it is straightforward to extend it to higher order var s .",
    "in the following , we provide the details regarding the groupwise sparsity and the alternative testing strategy respectively .      under the assumption of gaussian white noise",
    "it is natural to estimate the ar coefficients using regularized least squares , and probably the most straightforward way to do so is to use ridge regression , @xmath28 thanks to the ridge penalty , eq",
    ".   delivers solutions with small coefficients , which , however , are in general never exactly zero . in the strict sense of granger , this corresponds to a fully - connected dependency graph , rendering ridge regression an improper candidate for sparse causal recovery . on the other side , many of the estimated coefficients are expected to be non - significant .",
    "hence , we propose a sparsification through statistical testing .",
    "in contrast to to e.g. bootstrapping , we derive @xmath17-values explicitly using the approximate distribution of the coefficients .",
    "it is apparant from eq .   that the estimation can be done independently for each column of @xmath29 , and so does the testing .",
    "let therefore @xmath30 denote the @xmath31th column of @xmath29 and let @xmath32 .",
    "neglecting the dependency of @xmath33 and @xmath34 , the ridge coefficients depend linearly on @xmath34 , and we can conclude that under the null - hypothesis @xmath35 , we have @xmath36 with @xmath37 furthermore , setting @xmath38 an estimate of the model variance @xmath39 is given by @xmath40 using eq .",
    "we can now construct normalized test statistics @xmath41 which are jointly normally distributed with @xmath42 and @xmath43 .",
    "suppose we want to test all individual hypotheses @xmath44 simultaneously , then , according to @xcite , the adjusted p - values are @xmath45 .",
    "we reject a hypothesis , if the @xmath17-value is below the predefined significance level @xmath46 . here , @xmath47 and @xmath48 is the density function of the multivariate normal distribution @xmath49 .",
    "sparse causal discovery using ridge regression is a two - step procedure and may possibly suffer from the aggregation of assumptions that enter in each step .",
    "direct estimation of sparse var coefficients ( e.g. via lasso ) is therefore desirable , as this would allow omission of the multiple significance testing step .",
    "however , for higher order models , this approach is prone to selecting a different set of causal interactions for each of the @xmath9 time lags .",
    "we here suggest that this behavior can be overcome by enforcing _",
    "joint sparsity _ of the coefficient vectors that belong to a certain pair of time series .",
    "this corresponds to incorporating the prior belief that causal influences between time series are not restricted to only one particular time lag into the estimation .",
    "the positive effect of such modeling can be verified in figure  [ fig2 ] ( see section [ sec : experiments ] for more details ) .",
    "the idea of imposing groupwise sparse coefficients leads to @xmath50-norm regularized regression also known as the _ group lasso _",
    "@xcite , which has also applications in multiple kernel learning @xcite and the eeg / meg inverse problem ( e.g. * ? ? ?",
    "the term @xmath50-norm corresponds to an @xmath51-norm of a vector of @xmath52-norms .",
    "our proposed objective is given by @xmath53 this penalty leads to a groupwise variable selection , i.e. a whole block of coefficients is jointly zero .",
    "note that the first term in eq .",
    "penalizes all @xmath54 coefficients describing univariate relations . in this way",
    ", those coefficients are shrunk and hence , overfitting is avoided .",
    "furthermore , we remark that it is also conceivable to to split the estimation of @xmath29 into @xmath55 subproblems ( as suggested in subsection  [ subsec : ridge ] ) , which is desirable in large - scale scenarios .    eqs .   and define a non - differentiable but convex optimization problem which can be solved by means of second - order cone programming ( socp ) . for problems with sparse expected structure ,",
    "however , the optimization can be carried out much more efficiently using the results of @xcite . by keeping a set of active coefficient groups ,",
    "their algorithm needs to call the socp solver only for problem sizes far smaller than the original problem  leading to a considerable reduction of memory usage and computation time . in the experiments , we employ the active - set algorithm of @xcite in combination with a freely available socp solver @xcite",
    "we conduct a series of experiments in which the causal structure of simulated data has to be recovered .",
    "we compare the group lasso , standard lasso , ridge regression with multiple testing and complete granger causality based on ar models .",
    "all four approaches are applied both with and without knowledge of the true model order . in the latter case @xmath56",
    "is chosen for the reconstruction .",
    "for all methods considered , it is also possible to estimate the model order @xmath9 , e.g. via cross - validation .",
    "each simulated data set consists of a multivariate time series with parameters @xmath57 and @xmath58 that is generated by a random var process of order @xmath59 according to .",
    "the distribution of the noise component @xmath60 is chosen to be the standard normal distribution .",
    "the var coefficients for all but 10 randomly chosen pairs of time series are set to zero , yielding exactly 10 causal interactions .",
    "the non - zero coefficients are drawn randomly from @xmath61 .",
    "each set of var coefficients is tested for the stability of its induced dynamical system by looking at the eigenvalues of the corresponding transition matrix . only coefficients leading to stable systems ( i.e those with transition matrices with eigenvalues of at most 1 ) are accepted .",
    "we consider the following three types of problems , for each of which we create 10 instances : 1 ) no noise is added to the data generated by the var model 2 ) the data is superimposed by gaussian noise of approximately the same strength , which is uncorrelated ( white ) both across time and sensors 3 ) the data is superimposed by mixed noise of approximately the same strength , which is generated as a random instantaneous mixture of @xmath55 univariate ar processes of order 20 .",
    "note that in none of these cases the noise itself possesses a causal structure which would superimpose the true structure .",
    "for measuring performance we consider receiver operating characteristics ( roc ) curves , which allow objective assessment of the performance in different regimes ( e.g. very few false positives ) . as an additional measure of absolute performance",
    "we calculate the area under curve ( auc ) .",
    "roc curves and auc values are averaged across the 10 problem instances and standard errors are computed for auc .",
    "granger causality is calculated using the levinson - wiggens - robinson algorithm for fitting ar models @xcite , which is available in the open source biosig toolbox @xcite .",
    "note that for this particular method , we use granger s original definition of causal influence instead of our coefficient - based approach . that is , for a pair of time series @xmath1 and @xmath2 we calculate the logarithm of the ratio of the residuals of the two ar models 1 ) including interactions and 2 ) excluding interactions between @xmath1 and @xmath2 ( _ granger score _ ) .",
    "this score is divided by its standard deviations as estimated by the jackknife . to obtain a roc - curve ,",
    "the granger score is threshold at different values , ranging from completely sparse to completely dense solutions .    for ridge regression ,",
    "the regularization parameter @xmath62 is chosen via @xmath63-fold cross - validation ( with respect to prediction accuracy ) . for this value of @xmath62",
    ", we derive the test statistics defined in subsection[subsec : ridge ] .",
    "the multidimensional integrals in eq .   are computed using monte carlo sampling according to @xcite .",
    "roc - curves are constructed by varying the significance level @xmath46 .    for lasso and group lasso , solutions ranging from completely sparse to completely dense",
    "are obtained through variation of the regularizing constant @xmath62 and @xmath64 respectively .",
    "first , we illustrate the different behavior of the investigated methods in figure [ fig2 ] .",
    "this example corresponds to the situation without noise and with known model order @xmath59 .",
    "the left figure shows the true underlying causal structure , with a black box indicating a causal interaction .",
    "the reconstructions for the different methods are based on a single estimate of the var coefficients . for granger causality",
    ", we use a threshold of 2 . for ridge regression , we use a significance level of @xmath65 .",
    "for lasso , ridge regression and group lasso , the regularizing constant is fixed by using 10-fold cross - validation ( with respect to prediction accuracy ) .",
    "we display the binary influence matrix in figure [ fig2 ] . in this example",
    "ridge regression exhibits perfect reconstruction and outperforms all other methods .",
    "group lasso comes second .",
    "note that , due to the strong tendency of group lasso to select the same influences for each time lag , its estimated causal dependency matrix is sparser than that of lasso .    [",
    "cols=\"^,^,^,^,^ \" , ]",
    "we presented a novel approach for causal discovery in multivariate time series which is based on the group lasso . as an alternative we also discussed ridge regression with subsequent multiple testing according to @xcite which is also novel in the context of var modeling .",
    "both approaches were shown to outperform standard methods in simulated scenarios .",
    "future research will aim at applying our techniques to real - world problems .",
    "given that the sparsity assumption is correct , our group lasso approach should be able to handle much larger problems than the ones that were considered here by 1 ) splitting the problem into @xmath55 independent subproblems and 2 ) using the active set solver of @xcite in combination with strong regularization that ensures staying in the sparse regime .",
    "we expect that this will allow large - scale applications such as the estimation of cerebral information flow from functional magnetic resonance tomography ( fmri ) recordings to benefit from the improved accuracy of our approach .",
    "this work was supported in part by the german bmbf ( fkz 01gq0850 , 01-is07007a and 16sv2234 ) and the fp7-ict programme of the european community under the pascal2 network of excellence , ict-216886 .",
    "we thank thorsten dickhaus for discussions ."
  ],
  "abstract_text": [
    "<S> our goal is to estimate causal interactions in multivariate time series . using vector autoregressive ( var ) models , </S>",
    "<S> these can be defined based on non - vanishing coefficients belonging to respective time - lagged instances . as in most cases </S>",
    "<S> a parsimonious causality structure is assumed , a promising approach to causal discovery consists in fitting var models with an additional sparsity - promoting regularization . along this line </S>",
    "<S> we here propose that sparsity should be enforced for the subgroups of coefficients that belong to each pair of time series , as the absence of a causal relation requires the coefficients for all time - lags to become jointly zero . </S>",
    "<S> such behavior can be achieved by means of @xmath0-norm regularized regression , for which an efficient active set solver has been proposed recently . </S>",
    "<S> our method is shown to outperform standard methods in recovering simulated causality graphs . </S>",
    "<S> the results are on par with a second novel approach which uses multiple statistical testing .    </S>",
    "<S> * keywords * vector autoregressive model , granger causality , group lasso , multiple testing </S>"
  ]
}