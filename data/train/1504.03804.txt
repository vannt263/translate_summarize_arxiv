{
  "article_text": [
    "in a classification problem with @xmath1 classes , we usually have @xmath2 labeled observations @xmath3 from the @xmath4-th class ( @xmath5 ) , and we use these @xmath6 observations to construct a decision rule for classifying a new unlabeled observation @xmath7 to one of these @xmath1 pre - defined classes . if @xmath8 and @xmath9 respectively denote the prior probability and the probability density function of the @xmath4-th class , and @xmath10 denotes the corresponding posterior probability , the optimal _ bayes classifier _ assigns @xmath7 to the class @xmath11 , where @xmath12 . however , the @xmath13 s ( or , the @xmath10 s ) are unknown in practice , and one needs to estimate them from the training sample of labeled observations .",
    "popular parametric classifiers like linear discriminant analysis ( lda ) and quadratic discriminant analysis ( qda ) are motivated by parametric model assumptions on the underlying class distributions .",
    "so , they may lead to poor classification when the model assumptions fail to hold , and the class boundaries of the bayes classifier have complex geometry . on the other hand ,",
    "nonparametric classifiers like those based on @xmath14-nearest neighbors ( @xmath14-nn ) and kernel density estimates ( kde ) are more flexible and free from such model assumptions .",
    "but , they suffer from the curse of dimensionality and are often not suitable for high - dimensional data .",
    "consider two examples denoted by * e1 * and * e2 * , respectively .",
    "* e1 * involves a classification problem with two classes in @xmath15 , where the distribution of the first class is an equal mixture of n@xmath16 and n@xmath17 , and that for the second class is n@xmath18 . here",
    "n@xmath19 denotes the @xmath20-variate normal distribution , @xmath21 and @xmath22 is the @xmath23 identity matrix . in * e2",
    "* , each class distribution is an equal mixture of two uniform distributions .",
    "the distribution for the first ( respectively , the second ) class is a mixture of u@xmath24 and u@xmath25 ( respectively , u@xmath26 and u@xmath27 ) .",
    "here u@xmath28 denotes the uniform distribution over the region @xmath29 with @xmath30 .",
    "figure 1 shows the class boundaries of the bayes classifier for these two examples when @xmath31 , and @xmath32 .",
    "the regions colored grey ( respectively , black ) correspond to observations classified to the first ( respectively , the second ) class by the bayes classifier .",
    "it is clear that classifiers like lda and qda or any other classifier with linear or quadratic class boundaries will deviate significantly from the bayes classifier in both examples .",
    "a natural question then is how standard nonparametric classifiers like those based on @xmath14-nn and kde perform in such examples . in figure 2 , we have plotted average misclassification rates of these two classifiers along with the bayes risks for different values of @xmath20 .",
    "these classifiers were trained on a sample of size 100 generated from each class distribution , and the misclassification rates were computed based on a sample of size 250 from each class . this procedure was repeated 500 times to calculate the average misclassification rate . smoothing parameters associated with @xmath14-nn and kde ( i.e. , the @xmath14 in @xmath14-nn and the bandwidth in kde ) were chosen by minimizing leave - one - out cross - validation estimates of misclassification rates @xcite .",
    "figure 2 shows that in * e1 * , the bayes risk decreases to zero as @xmath20 grows .",
    "since the class distributions in * e2 * have disjoint supports , the bayes risk is zero irrespective of the value of @xmath20 .",
    "but in both examples , the misclassification rates of these two nonparametric classifiers increased to almost 50% as @xmath20 increased .",
    "these two examples clearly show the necessity to develop new classifiers to cope with such situations .",
    "over the last three decades , data depth ( see , e.g. , @xcite ) has emerged as a powerful tool for data analysis with applications in many areas including supervised and unsupervised classification ( see @xcite ) . spatial depth ( also known as the @xmath33 depth )",
    "is a popular notion of data depth that was introduced and studied in @xcite and @xcite . the _ spatial depth _ ( spd ) of an observation @xmath34 w.r.t .",
    "a distribution function @xmath35 on @xmath15 is defined as @xmath36 where @xmath37 and @xmath38 is the multivariate sign function given by @xmath39 if @xmath40 , and @xmath41 .",
    "henceforth , @xmath42 will denote the euclidean norm .",
    "spatial depth is often computed on the standardized version of the data . in that case",
    ", spd is defined as @xmath43 where @xmath44 is a scatter matrix associated with @xmath35 .",
    "if @xmath44 has the affine equivariance property , this version of spd is affine invariant .",
    "like other depth functions , spd provides a centre - outward ordering of multivariate data .",
    "an observation has higher ( respectively , lower ) depth if it lies close to ( respectively , away from ) the centre of the distribution . in other words ,",
    "given an observation @xmath7 and a pair of probability distributions @xmath45 and @xmath46 , if spd@xmath47 is larger than spd@xmath48 , one would expect @xmath7 to come from @xmath45 instead of @xmath46 .",
    "based on this simple idea , the _ maximum depth classifier _ was developed in @xcite . for a @xmath1-class problem involving distributions @xmath49 , it classifies an observation @xmath7 to the @xmath50-th class , where @xmath51 .",
    "an important property of spd ( see lemma 1 in appendix ) is that when the class distribution @xmath35 is unimodal and spherically symmetric , the class density function turns out to be a monotonically increasing function of spd . in both examples * e1 * and * e2 * , the class distributions are spherical .",
    "consequently , spd@xmath52 is a function of @xmath53 in view of the rotational invariance of spd@xmath52 . in figure 3 , we have plotted @xmath54 and @xmath55 for different values of @xmath53 for examples * e1 * and * e2 * , where @xmath45 and @xmath46 are the two class distributions and @xmath56 .",
    "it is transparent from the plots that the maximum depth classifier based on spd will fail in both examples . in example * e1 * , for all values of @xmath53 smaller ( respectively , greater ) than a constant close to 4 , the observations will be classified to the first ( respectively , the second ) class by the maximum spd classifier . on the other hand",
    ", this classifier will classify all observations to the second class in example * e2*.    in section 2 , we develop a modified classifier based on spd to overcome this limitation of the maximum depth classifier .",
    "most of the existing modified depth based classifiers are developed mainly for two class problems ( see , e.g. , @xcite ) . for classification problems involving @xmath57 classes ,",
    "one usually solves @xmath58 binary classification problems taking one pair of classes at a time and then uses majority votes to make the final classification .",
    "our proposed classification method based on spd addresses the @xmath1 class problem directly .",
    "almost all depth based classifiers proposed in the literature require ellipticity of class distributions to achieve bayes optimality . in order to cope with possible multimodal as well as non - elliptic population distributions",
    ", we construct a localized version of spd ( henceforth referred to as lspd ) in section 3 . in section 4",
    ", we develop a multiscale classifier based on lspd .",
    "relevant theoretical results on spd , lspd and the resulting classifiers have also been studied in these sections .",
    "an advantage of spd over other depth functions is its computational simplicity .",
    "classifiers based on spd and lspd can be constructed even when the dimension of the data exceeds the sample size .",
    "we deal with such high - dimensional low sample size cases in section 5 , and show that both classifiers turn out to be optimal under a fairly general framework . in sections 6 and 7",
    ", some simulated and benchmark data sets are analyzed to establish the usefulness of our classification methods .",
    "section 8 contains a brief summary of the work and some concluding remarks .",
    "all proofs and mathematical details are given in the appendix .",
    "let us assume that @xmath59 are the density functions of @xmath1 elliptically symmetric distributions on @xmath15 , where @xmath60 for @xmath5 .",
    "here @xmath61 , @xmath62 is a @xmath23 positive definite matrix , and @xmath63 is a probability density function on @xmath15 for @xmath5 . for such classification problems involving general elliptic populations with equal or unequal priors ,",
    "the next theorem establishes the bayes optimality of a classifier , which is based on @xmath64spd@xmath65 , @xmath66 , spd@xmath67 , the vector of spd .",
    "in particular , it follows from this theorem that for examples * e1 * and * e2 * discussed at the beginning of section 1 , the class boundaries ( see figure 1 ) of the bayes classifiers are functions of @xmath68spd@xmath65 , spd@xmath69 .",
    "_ if the densities of the @xmath1 competing classes are elliptically symmetric , the posterior probabilities of these classes satisfy the logistic regression model given by @xmath70 } ~\\mbox{for}~~ 1 \\leq j \\leq ( j-1)~\\ ] ] @xmath71}.\\ ] ] here @xmath72 , and @xmath73s are appropriate real - valued functions of real variables . consequently , the bayes rule assigns an observation @xmath7 to the class @xmath50 , where @xmath74 . _",
    "theorem 1 shows that the bayes classifier is based on a nonparametric multinomial additive logistic regression model for the posterior probabilities , which is a special case of generalized additive models ( gam ) @xcite .",
    "if the prior probabilities of the @xmath1 classes are equal , and @xmath75 are all elliptic and unimodal differing only in their locations , this bayes classifier reduces to the maximum spd classifier @xcite ( see remark 1 after the proof of theorem 1 in the appendix ) .    for any fixed @xmath76 and @xmath4 , one can calculate the @xmath1-dimensional vector @xmath77 , where @xmath78 is the @xmath76-th training sample observation in the @xmath4-th class for @xmath79 and @xmath5 .",
    "these @xmath77s can be viewed as realizations of the vector of co - variates in a nonparametric multinomial additive logistic regression model , where the response corresponds to the class label that belongs to @xmath80 .",
    "so , a classifier based on spd can be constructed by fitting a generalized additive model with the logistic link function . in practice ,",
    "when we compute spd of @xmath7 from the data @xmath81 generated from @xmath35 , we use its empirical version as @xmath82 . for the standardized version of the data , it is defined as @xmath83 where @xmath84 is an estimate of @xmath44 , and",
    "@xmath85 is the empirical distribution of the data @xmath86 .",
    "the resulting classifier worked well in examples * e1 * and * e2 * , and we shall see it in section 6 .",
    "under elliptic symmetry , the density function of a class can be expressed as a function of spd , and hence the spd contours coincide with the density contours .",
    "this is the main mathematical argument used in the proof of theorem 1 .",
    "now , for certain non - elliptic distributions , where the density function can not be expressed as a function of spd , such mathematical arguments are no longer valid .",
    "for instance , consider an equal mixture of n@xmath87 , n@xmath88 and n@xmath89 , where @xmath90 .",
    "we have plotted its spd contours in figure 4 when @xmath31 . for this trimodal distribution ,",
    "the spd contours fail to match the density contours . as a second example , we consider a @xmath20-dimensional distribution with independent components , where the @xmath76-th component is exponential with the scale parameter @xmath91 for @xmath92 .",
    "we have plotted its spd contours in figure 5 when @xmath31 . even in this example , the spd contours differ significantly from the density contours . to cope with this issue , we suggest a _ localization _ of spd ( see the third contour plots @xmath93 in figures 4 and 5 ) . as we shall see later",
    ", this localized spd relates to the underlying density function , and the resulting classifier turns out to be the bayes classifier ( in a limiting sense ) in a general nonparametric setup with arbitrary class densities .",
    "note that @xmath94 is constructed by assigning the same weight to each unit vector @xmath95 ignoring the significance of distance between @xmath7 and @xmath96 . by introducing a weight function , which depends on this distance",
    ", one can extract important features related to the local geometry of the data . to capture these local features ,",
    "we introduce a kernel function @xmath97 as a weight and define @xmath98 - \\| e_f[k_h({\\mbox{\\bf t } } ) u({\\mbox{\\bf t } } ) ] \\|,\\ ] ] where @xmath99 and @xmath100 . here",
    "@xmath101 is chosen to be a bounded continuous density function on @xmath15 such that @xmath102 is a decreasing function of @xmath103 and @xmath104 as @xmath105 .",
    "the gaussian kernel @xmath106 is a possible choice .",
    "it is desirable that the localized version of spd approximates the class density or a monotone function of it for small values of @xmath107 .",
    "this will ensure that the class densities and hence , the class posterior probabilities become functions of the local depth as @xmath108 .",
    "on the other hand , one should expect that as @xmath109 , the localized version of spd should tend to @xmath110 or a monotone function of it .",
    "however , @xmath111 as @xmath109 .",
    "so , we re - scale @xmath112 by an appropriate factor to define the _ localized spatial depth _ ( lspd ) function as follows : @xmath113 using @xmath114 in the definition of @xmath115 , one gets lspd on standardized data , which is affine invariant if @xmath44 is affine equivariant .",
    "@xmath116 defined this way is a continuous function of @xmath107 , and @xmath117lspd@xmath118 , @xmath66 , lspd@xmath119 has the desired behavior as shown in theorem 2 .    _",
    "consider a kernel function @xmath102 that satisfy @xmath120 .",
    "if @xmath59 are continuous density functions with bounded first derivatives , and the scatter matrix @xmath121 corresponding to @xmath13 exists for all @xmath5 , then _    @xmath122 , and    @xmath123    now , we construct a classifier by plugging in lspd@xmath124 instead of spd in the gam discussed in section 2 .",
    "so , we consider the following model for the posterior probabilities @xmath125 } , ~\\mbox{for}~1 \\le j < ( j-1),\\ ] ] @xmath126}.\\ ] ]    the main implication of part @xmath127 of theorem 2 is that the classifier constructed using gam and @xmath128 as the covariate tends to the bayes classifier in a general nonparametric setup as @xmath108 . on the other hand ,",
    "part @xmath129 of theorem 2 implies that for elliptic class distributions , the same classifier tends to the bayes classifier when @xmath109 . when we fit gam , the functions @xmath130s are estimated nonparametrically .",
    "flexibility of such nonparametric estimates automatically takes care of the constants @xmath131 for @xmath5 and @xmath132 in the expressions of the limiting values of @xmath128 in parts @xmath127 and @xmath129 of theorem 2 , respectively .    the empirical version of @xmath112 , denoted by @xmath133 ,",
    "is defined as @xmath134 where @xmath135 ( or , @xmath136 if we use standardized version of the data ) for @xmath137 . then @xmath138 is defined using @xmath139 with @xmath140 replaced by @xmath133 .",
    "theorem 3 below shows the almost sure uniform convergence of @xmath138 to its population counterpart @xmath141 .",
    "similar convergence result for the empirical version of spd has been proved in the literature ( see , e.g. , @xcite ) .",
    "_ suppose that the density function @xmath142 and the kernel @xmath101 are bounded , and @xmath101 has bounded first derivatives .",
    "then , for any fixed @xmath143 , @xmath144 @xmath145 .",
    "_    from the proof of theorem 3 , it is easy to check that this almost sure uniform convergence also holds when @xmath109 . under additional moment conditions on @xmath142 and @xmath101 ,",
    "this holds for the @xmath108 case as well if @xmath146 as @xmath147 ( see remarks 2 and 3 after the proof of theorem 3 in the appendix ) .",
    "so , the result stated in parts ( a ) and ( b ) of theorem 2 continue to hold for the empirical version of lspd under appropriate assumptions .",
    "localization and kernelization of different notions of data depth have been considered in the literature @xcite .",
    "the fact that lspd@xmath124 tends to a constant multiple of the probability density function as @xmath108 is a crucial requirement for limiting bayes optimality of classifiers based on this localized depth function . in @xcite ,",
    "the authors proposed localized versions of simplicial depth and half - space depth , but the relationship between the local depth and the probability density function has been established only in the univariate case .",
    "a depth function based on inter - point distances has been developed in @xcite to capture multimodality in a data set .",
    "@xcite defined kernelized spatial depth in a reproducing kernel hilbert space . in @xcite , the authors considered a generalized notion of mahalanobis depth in reproducing kernel hilbert spaces .",
    "however , there is no result connecting them to the probability density function .",
    "infact , the kernelized spatial depth function becomes degenerate at the value @xmath148 as the tuning parameter goes to zero .",
    "consequently , it becomes non - informative for small values of the tuning parameter .",
    "it will be appropriate to note here that none of the preceding authors used their proposed depth functions for constructing classifiers .",
    "recently , in @xcite , the authors proposed a notion of local depth and used it for supervised classification .",
    "but , their proposed version of local depth does not relate to the underlying density function either .",
    "when the class distributions are elliptic , part @xmath129 of theorem 2 implies that lspd@xmath124 with appropriately large choices of @xmath107 lead to good classifiers",
    ". these large values may not be appropriate for non - elliptic class distributions , but part @xmath127 of theorem 2 implies that lspd@xmath124 with appropriately small choices of @xmath107 lead to good classifiers for general nonparametric models for class densities .",
    "however , for small values of @xmath107 , the empirical version of lspd@xmath124 behaves like a nonparametric density estimate , and it suffers from the curse of dimensionality .",
    "so , the resulting classifier may have its statistical limitations for high - dimensional data .     for different values of @xmath107.,width=264,height=226 ]",
    "we now consider two examples to demonstrate the above points .",
    "the first example ( we call it * e3 * ) involves two multivariate normal distributions n@xmath149 and n@xmath150 . in the second example ( we call it * e4 * )",
    ", both distributions are trimodal .",
    "the first class has the same density as in figure 4 ( i.e. , an equal mixture of n@xmath87 , n@xmath88 and n@xmath89 ) , while the second class is an equal mixture of n@xmath151 , n@xmath152 and n@xmath153 .",
    "we consider the case @xmath154 for * e3 * and @xmath31 for * e4*. for each of these two examples , we generated a training sample of size 100 from each class .",
    "the misclassification rate for the classifier based on @xmath155 was computed based on a test sample of size 500 ( 250 from each class ) .",
    "this procedure was repeated 100 times to calculate the average misclassification rate for different values of @xmath107 .",
    "figure 6 shows that the large ( respectively , small ) values of @xmath107 yielded low misclassification rates in * e3 * ( respectively , * e4 * ) . for small values of @xmath107 ,",
    "empirical lspd@xmath124 behaved like a nonparametric density estimate that suffered from the curse of dimensionality in * e4*. consequently , its performance deterioratesd .",
    "but , for large @xmath107 , the underlying elliptic structure was captured well by the proposed classifier .",
    "this provides a strong motivation for using a multi - scale approach in constructing the final classifier so that one can harness the strength of different classifiers corresponding to different levels of localization of spd .",
    "one would expect that when aggregated , classifiers corresponding to different values of @xmath107 will lead to improved misclassification rates .",
    "usefulness of the multi - scale approach in combining different classifiers has been discussed in the classification literature by several authors including @xcite .",
    "a popular way of aggregation is to consider the weighted average of the estimated posterior probabilities computed for different values of @xmath107 .",
    "there are various proposals for the choice of the weight function in the literature .",
    "following @xcite , one can compute @xmath156 , the leave - one - out estimate of the misclassification rate of the classifier based on lspd@xmath124 and use @xmath157}\\ ] ] as the weight function , where @xmath158 .",
    "the exponential function helps to appropriately weighing up ( respectively , down ) the promising ( respectively , poor ) classifier resulting from different choices of the smoothing parameter @xmath107 .",
    "however , @xmath159 or @xmath160 may not be finite for some choices of @xmath161 .",
    "so , here we use a slightly modified weight function @xmath162 , where @xmath163 is a univariate cauchy density with a large scale parameter and support restricted to have positive values only .",
    "our final classifier , which we call the lspd classifier , assigns an observation @xmath7 to the @xmath50-th class , where @xmath164 here @xmath165 is as in equations ( 4 ) and ( 5 ) in section 3 . in practice",
    ", we first generate @xmath166 independent observations @xmath167 from @xmath163 .",
    "for any given @xmath4 and @xmath7 , we approximate @xmath168 by @xmath169 .",
    "the use of the cauchy distribution with a large scale parameter ( we use 100 in this article ) helps us to generate small as well as large values of @xmath107 .",
    "this is desirable in view of theorem 2 .",
    "a serious practical limitation of many existing depth based classifiers is their computational complexity in high dimensions , and this makes such classifiers impossible to use even for moderately large dimensional data . besides , depth functions that are based on random simplices formed by the data points ( see @xcite ) , can not be defined in a meaningful way if dimension of the data exceeds the sample size .",
    "projection depth and tukey s half - space depth ( see , e.g. , @xcite ) both become degenerate at zero for such high - dimensional data .",
    "classification of high - dimensional data presents a substantial challenge to many nonparametric classification tools as well .",
    "we have seen in examples * e1 * and * e2 * ( see figure 2 ) that nonparametric classifiers like those based on @xmath14-nn and kde can yield poor performance when data dimension is large",
    ". some limitations of support vector machines for classification of high - dimensional data have also been noted in @xcite .",
    "one of our primary motivations behind using spd is its computational tractability , especially when the dimension is large .",
    "we now investigate the behavior of classifiers based on spd and lspd for such high - dimensional data . for this investigation",
    ", we assume that the observations are all standardized by a common positive definite matrix @xmath44 for all @xmath1 classes , and the following conditions are stated for those standardized random vectors , which are written as @xmath96s for notational convenience .",
    "( c1 ) consider a random vector @xmath170 @xmath171 .",
    "assume that @xmath172 @xmath173 exists for @xmath5 , and @xmath174    ( c2 ) consider two independent random vectors @xmath175 and @xmath176 .",
    "assume that @xmath177 @xmath178 exists , and @xmath179 for all @xmath180 .",
    "it is not difficult to verify that for @xmath181 ( @xmath5 ) , if we assume that the sequence of variables @xmath182 centered at their means are independent with uniformly bounded eighth moments ( see theorem 1 ( 2 ) in @xcite , p. 4110 ) , or if we assume that they are @xmath183-dependent random variables with some appropriate conditions ( see theorem 2 in @xcite , p. 350 ) , then the almost sure convergence in ( c1 ) as well as ( c2 ) holds . as a matter of fact , the almost sure convergence stated in ( c1 ) and ( c2 ) holds if we assume that for all @xmath180 , the sequences @xmath184 and @xmath185 , where @xmath181 and @xmath186 , are _ mixingales _ satisfying some appropriate conditions ( see , e.g. , theorem 2 in @xcite , p . 350 ) .",
    "define @xmath187 and @xmath188 . for the random vector @xmath181",
    ", @xmath189 is the limit of @xmath190 as @xmath191 , where @xmath192 denotes the variance of a random variable @xmath193 . if we consider a second independent random vector @xmath186 with @xmath194 , then @xmath195 is the limit of @xmath196 as @xmath191 . in @xcite",
    ", the authors assumed a similar set of conditions to study the performance of the classifier based on support vector machines ( svm ) with a linear kernel and the @xmath14-nn classifier with @xmath197 as the data dimension grows to infinity .",
    "similar conditions on observation vectors were also considered in @xcite to study the consistency of principal components of the sample dispersion matrix for high - dimensional data . under ( c1 ) and ( c2 ) , the following theorem describes the behavior of @xmath198 and @xmath128 as @xmath20 grows to infinity .    _",
    "suppose that the conditions ( c1)-(c2 ) hold , and @xmath199 @xmath200 .",
    "_    \\(a ) @xmath201 , where @xmath202 and @xmath203 for @xmath204 .",
    "\\(b ) assume that @xmath109 and @xmath191 in such a way that @xmath205 .",
    "then , @xmath206 depending on whether @xmath207 , respectively . here",
    "@xmath208 , @xmath209 and @xmath210 for @xmath211 .",
    "\\(c ) assume that @xmath212 , and @xmath213 as @xmath191 .",
    "then , @xmath214 .",
    "the @xmath215s as well as the @xmath216s in the statement of theorem 4 are distinct for all @xmath5 whenever either @xmath217 or @xmath218 for all @xmath204 ( see lemma 2 in appendix ) .",
    "in such a case , part @xmath127 of theorem 4 implies that for large @xmath20 , @xmath198 has good discriminatory power , and our classifier based on spd can discriminate well among the @xmath1 populations .",
    "further , it follows from part @xmath129 that when both @xmath20 and @xmath107 grow to infinity in such a way that @xmath219 or a positive constant , @xmath128 has good discriminatory power as well , and our classifier based on lspd@xmath124 can yield low misclassification probability .",
    "however , part @xmath93 shows that if @xmath220 grows at a rate faster than @xmath107 , @xmath221 becomes non - informative .",
    "consequently , the classifier based on lspd@xmath124 lead to high misclassification probability in this case .",
    "we analysed some data sets simulated from elliptic as well as non - elliptic distributions . in each example , taking an equal number of observations from each of the two classes , we generated 500 training and test sets , each of size 200 and 500 , respectively .",
    "we considered examples in dimensions 5 and 100 . for classifiers based on spd and lspd",
    ", we used the usual sample dispersion matrix of the @xmath4-th ( @xmath222 ) class as @xmath223 when @xmath224 . for @xmath225 , due to statistical instability of the sample dispersion matrix , we standardized each variable in a class by its sample standard deviation .",
    "average test set misclassification rates of different classifiers ( over 500 test sets ) are reported in table 1 along with their corresponding standard errors . to facilitate comparison ,",
    "the corresponding bayes risks are reported as well .",
    "we compared our proposed classifiers with a pool of classifiers that include parametric classifiers like lda and qda , and nonparametric classifiers like those based on @xmath14-nn ( with the euclidean metric as the distance function ) and kde ( with the gaussian kernel ) .",
    "for the implementation of lda and qda in dimension @xmath226 , we used diagonal estimates of dispersion matrices as in the cases of spd and lspd . for @xmath14-nn and kde , we used pooled versions of the scatter matrix estimates , which were chosen to be diagonal for @xmath225 . in table 1",
    ", we report results for the multiscale methods of @xmath14-nn @xcite and kde @xcite using the same weight function as described in section 4 . to facilitate comparison , we also considered svm having the linear kernel and the radial basis function ( rbf ) kernel ( i.e. , @xmath227 with the default value @xmath228 as in http://www.csie.ntu.edu.tw/@xmath229cjlin/libsvm/ ) ; the classifier based on classification and regression trees ( cart ) and a boosted version of cart known as random forest ( rf ) . for the implementation of svm , cart and rf , we used the r codes available in the libraries e1071 @xcite , tree @xcite and randomforest @xcite , respectively . for classifiers based on spd and lspd , we wrote our own r codes using the library vgam @xcite , and the codes are available at https://sites.google.com/site/tijahbus/home/lspd .",
    "in addition , we compared the performance of our classifiers with two depth based classification methods ; the classifier based on depth - depth plots ( dd ) @xcite and the classifier based on maximum local depth @xcite ( ld ) . the dd classifier uses a polynomial of class depths ( usually , half - space depth or projection depth is used , and depth is computed based on several random projections ) to construct the separating surface .",
    "we used polynomials of different degrees and reported the best result in table 1 . for the ld classifier",
    ", we used the r package depthproc and considered the best result obtained over a range of values for the localization parameter .",
    "however , in almost all cases , the performance of the ld classifier was inferior to that of the dd classifier .",
    "so , we did not report its misclassification rates in table 1 .",
    "recall examples * e1 * and * e2 * in section 2 and example * e3 * in section 4 involving elliptic class distributions . in *",
    "e1 * with @xmath224 , the dd classifier led to the lowest misclassification rate closely followed by spd and lspd classifiers , but in the case of @xmath225 , spd and lspd classifiers significantly outperformed all other classifiers considered here ( see table 1 ) .",
    "the superiority of these two classifiers was evident in * e2 * as well . in the case of @xmath224 ,",
    "the difference between their misclassification rates was statistically insignificant , though the former had an edge .",
    "since the class distributions were elliptic , dominance of the spd classifier over the lspd classifier was quite expected .",
    "however , this difference was found to be statistically significant when @xmath225 . in view of the normality of the class distributions ,",
    "qda was expected to have the best performance in * e3 * , and we observed the same . for @xmath224 ,",
    "the dd classifier ranked second here , while the performance of spd and lspd classifiers was satisfactory .",
    "however , in the case of @xmath225 , spd and lspd classifiers again outperformed the dd classifier , and they correctly classified all the test set observations .    * 13|@c@|    data &  bayes   &  lda   & qda &  svm   & svm & @xmath14-nn & kde &  cart   &  rf   & dd & spd & lspd + set & risk & & & ( lin ) & ( rbf ) & & & & & & & +     +    * e1 * & 26.50 & 50.00 & 52.53 & 45.46 & 30.03 & 40.65 & 39.16 & 36.90 &  31.32   & * 27.92*@xmath230 & 28.32 & 28.54 + & & ( 0.20 ) & ( 0.19 ) & ( 0.11 ) & ( 0.09 ) & ( 0.13 ) & ( 0.11 ) & ( 0.13 ) &  ( 0.09 )   & ( 0.11 ) & ( 0.10 ) & ( 0.11 ) +    * e2 * & 0.00 & 47.43 & 42.44 & 43.92 & 38.06 & 37.64 & 34.29 & 39.10 & 34.26 & 26.68 & *   9.26 * @xmath230 & * 9.42 * + & & ( 0.15 ) & ( 0.06 ) & ( 0.12 ) & ( 0.09 ) & ( 0.16 ) & ( 0.11 ) & ( 0.11 ) & ( 0.08 ) & ( 0.09 ) & ( 0.09 ) & ( 0.10 ) +    * e3 * & 10.14 & 21.56 & *   11.09 * @xmath230 & 22.09 & 11.74 & 18.16 & 16.94 & 19.18 & 13.77 & * 11.17 * & 11.49 & 11.64 + & & ( 0.09 ) & ( 0.07 ) & ( 0.09 ) & ( 0.07 ) & ( 0.09 ) & ( 0.08 ) & ( 0.13 ) & ( 0.08 ) & ( 0.07 ) & ( 0.07 ) & ( 0.07 ) +    * e4 * & 2.10 & 40.52 & 42.41 & 36.16 & 25.08 & * 2.42 * @xmath230 & * 2.55 * & 15.52 & 4.98 & 33.04 & 10.07 & * 2.58 * + & & ( 0.09 ) & ( 0.08 ) & ( 0.10 ) & ( 0.13 ) & ( 0.03 ) & ( 0.03 ) & ( 0.09 ) & ( 0.06 ) & ( 0.12 ) & ( 0.07 ) & ( 0.03 ) +    * e5 * & 2.04 & 41.17 & 5.97 & 32.14 & 8.12 & 9.44 & 9.26 & 4.82 & * 2.84 * @xmath230 & 5.82 & 5.65 & 5.52 + & & ( 0.15 ) & ( 0.05 ) & ( 0.34 ) & ( 0.07 ) & ( 0.08 ) & ( 0.07 ) & ( 0.08 ) & ( 0.03 ) & ( 0.05 ) & ( 0.06 ) & ( 0.06 ) +     +    * e1 * & 0.48 & 50.29 & 50.67 & 46.85 & 24.97 & 44.57 & 49.99 & 35.72 & 25.14 & 24.99 & *   1.60 * @xmath230 & 2.34 + & & ( 0.10 ) & ( 0.13 ) & ( 0.11 ) & ( 0.06 ) & ( 0.08 ) & ( 0.10 ) & ( 0.12 ) & ( 0.12 ) & ( 0.10 ) & ( 0.11 ) & ( 0.12 ) +    * e2 * & 0.00 & 43.77 & 46.13 & 43.99 & 40.32 & 49.96 & 49.22 & 40.30 & 32.36 & 27.56 & *   2.90 * @xmath230 & 3.18 + & & ( 0.09 ) & ( 0.04 ) & ( 0.09 ) & ( 0.06 ) & ( 0.02 ) & ( 0.06 ) & ( 0.11 ) & ( 0.10 ) & ( 0.09 ) & ( 0.08 ) & ( 0.09 ) +    * e3 * & 0.00 & 0.46 & *   0.00 * @xmath230 & 3.21 & *   0.00 * @xmath230 & 49.99 & 49.98 & 17.40 & 0.02 & 1.92 & *   0.00 * @xmath230 & *   0.00 * @xmath230 + & & ( 0.01 ) & ( 0.00 ) & ( 0.05 ) & ( 0.00 ) & ( 0.00 ) & ( 0.00 ) & ( 0.12 ) & ( 0.00 ) & ( 0.02 ) & ( 0.00 ) & ( 0.00 ) +    * e4 * & 0.00 & 33.40 & 33.40 & 46.28 & 19.43 & *   0.00 * @xmath230 & *   0.00 * @xmath230 & 17.28 & *   0.00 * @xmath230 & 23.15 & *   0.00 * @xmath230 & *   0.00 * @xmath230 + & & ( 0.00 ) & ( 0.00 ) & ( 0.10 ) & ( 0.09 ) & ( 0.00 ) & ( 0.00 ) & ( 0.00 ) & ( 0.09 ) & ( 0.10 ) & ( 0.00 ) & ( 0.00 ) +    * e5 * & 0.00 & 46.74 & *   0.00 * @xmath230 & 44.45 & 7.83 & 44.01 & 49.98 & 3.32 & *   0.00 * @xmath230 & 3.12 & *   0.00 * @xmath230 & *   0.00 * @xmath230 + & & ( 0.29 ) & ( 0.00 ) & ( 0.31 ) & ( 0.15 ) & ( 0.21 ) & ( 0.04 ) & ( 0.11 ) & ( 0.00 ) & ( 0.10 ) & ( 0.00 ) & ( 0.00 ) +    in all these examples , the bayes classifier had non - linear class boundaries .",
    "so , lda and svm with linear kernel could not perform well .",
    "the performance of svm with the rbf kernel was relatively better . in * e3 * , it had competitive misclassification rates for both values of @xmath20 .",
    "@xmath14-nn and kde had comparable performance in the case of @xmath224 , but in the high - dimensional case ( @xmath225 ) , they misclassified almost half of the test cases",
    ". in @xcite , the authors derived some conditions under which the @xmath14-nn classifier tends to classify all observations to a single class when the data dimension increases to infinity .",
    "these conditions hold in this example .",
    "it can also be shown that the classifier based on kde with equal prior probabilities have the same problem in high dimensions .",
    "recall the trimodal example * e4 * discussed in section 4 . in this example , the lspd classifier and the nonparametric classifiers based on @xmath14-nn and kde significantly outperformed all other classifiers in the case of @xmath224 .",
    "the differences between the misclassification rates of these three classifiers was statistically insignificant .",
    "interestingly , along with these classifiers , the spd classifier also led to zero misclassification rate for @xmath225 .",
    "the dd classifier , lda , qda and svm did not have satisfactory performance in this example .",
    "the final example ( we call it * e5 * ) is with exponential distributions , where the component variables are independently distributed in both classes .",
    "the @xmath76-th variable in the first ( respectively , the second ) class is exponential with scale parameter @xmath91 ( respectively , @xmath231 ) .",
    "further , the second class has a location shift such that the difference between the mean vectors of the two classes is @xmath232 .",
    "recall that figure 5 shows the density contours of the first class when @xmath31 . in this example , the rf classifier had the best performance followed by cart when @xmath224 .",
    "dd , spd and lspd classifiers also performed well , and their misclassification rates were significantly lower than all other classifiers .",
    "linear classifiers like lda and svm with linear kernel failed to perform well .",
    "note that as @xmath20 increases , the difference between the locations of these two classes shrinks to zero .",
    "this results in high misclassification rates for these linear classifiers when @xmath225 .",
    "qda performed reasonably well , and like spd , lspd and rf classifiers , it correctly classified all the test cases when @xmath225 .",
    "the dd classifier led to an average misclassification rate of 3.12% .",
    "again , the classifiers based on @xmath14-nn and kde had poor performance for @xmath225 .",
    "this is due to the same reason as in * e3 * ( see also @xcite ) .",
    "note that even in these examples with non - elliptic distributions , the spd classifier performed well for high - dimensional data .",
    "this can be explained using part @xmath127 of theorem 4 .",
    "these examples also demonstrate that for non - elliptic or multimodal data , if not better , our lspd classifier can perform as good as popular nonparametric classifiers .",
    "in fact , this adjustment of lspd classifier is automatic in view of the multiscale approach developed in section 4 .",
    "we analyzed some benchmark data sets for further evaluation of our proposed classifiers .",
    "the biomedical data set is taken from the cmu data archive ( http://lib.stat.cmu.edu/datasets/ ) , the growth data set is obtained from @xcite , the colon data set is available in @xcite ( and also at the r - package ` rda ' ) , and the lightning 2 data set is taken from the ucr time series classification archive ( http://www.cs.ucr.edu/@xmath229eamonn/time_series_data/ ) .",
    "the remaining data sets are taken from the uci machine learning repository ( http://archive.ics.uci.edu/ml/ ) .",
    "descriptions of these data sets are available at these sources . in the case of biomedical data , we did not consider observations with missing values .",
    "satellite image ( satimage ) data set has specific training and test samples . for this data set ,",
    "we report the test set misclassification rates of different classifiers . if a classifier had misclassification rate @xmath233 , its standard error was computed as @xmath234 . in all other data sets , we formed the training and the test sets by randomly partitioning the data , and this random partitioning was repeated 500 times to generate new training and test sets .",
    "the average test set misclassification rates of different classifiers are reported in table 2 along with their corresponding standard errors .",
    "the sizes of the training and the test sets in each partition are also reported in this table .",
    "since the codes for the dd classifier are available only for two class problems , we could use it only in cases of biomedical and parkinson s data , where it yielded misclassification rates of 12.54% and 14.48% , respectively , with corresponding standard error of 0.18% and 0.15% . in the case of growth data ,",
    "where training sample size from each class was smaller than the dimension , the values of randomized versions of half - space depth and projection depth were zero for almost all observations . due to this problem",
    ", the dd classifier could not be used .",
    "we used the maximum ld classifier on these real data sets , but in most of the cases , its performance was not satisfactory .",
    "so , we do not report them in table 2 .    * 7|@c@|*3|@c@| data set & biomed & parkinson s & wine & waveform & vehicle & satimage & growth & lightning 2 & colon +    dimension ( @xmath20 ) & 4 & 22 & 13 & 21 & 18 & 36 & 31 & 637 & 2000 + classes ( @xmath1 ) & 2 & 2 & 3 & 3 & 4 & 6 & 2 & 2 & 2 + training size & 100 & 97 & 100 & 300 & 423 & 4435 & 46 & 60 & 31 + test size & 94 & 98 & 78 & 501 & 423 & 2000 & 47 & 61 & 31 +    lda & 15.66 & 30.93 & 2.00 & 19.90 & 22.49 & 16.06 & 29.15 & 32.51 & *   14.03 * @xmath230 + & ( 0.14 ) & ( 0.12 ) & ( 0.06 ) & ( 0.15 ) & ( 0.07 ) & ( 0.82 ) & ( 0.34 ) & ( 0.35 ) & ( 0.20 ) + qda & * 12.57 * & xxxx & 2.46 & 21.12 & * 16.38 * & 14.14 & xxxx & xxxx & xxxx + & ( 0.13 ) & xxxx & ( 0.09 ) & ( 0.15 ) & ( 0.07 ) & ( 0.78 ) & xxxx & xxxx & xxxx +    svm ( lin ) & 22.03 & 15.31 & 3.64 & 18.88 & 21.20 & 15.18 & 5.16 & 35.64 & 16.38 + & ( 0.13 ) & ( 0.12 ) & ( 0.09 ) & ( 0.07 ) & ( 0.07 ) & ( 0.80 ) & ( 0.12 ) & ( 0.35 ) & ( 0.23 ) + svm ( rbf ) & * 12.76 * & 13.69 & 1.86 & 16.08 & 25.57 & 30.99 & *   4.66 * @xmath230 & 28.73 & 35.48 + & ( 0.13 ) & ( 0.10 ) & ( 0.06 ) & ( 0.07 ) & ( 0.08 ) & ( 1.03 ) & ( 0.11 ) & ( 0.32 ) & ( 0.00 ) +    @xmath14-nn & 17.74 & 14.42 & 1.98 & 16.37 & 21.80 & *   9.23 * @xmath230 & * 4.48 * & 30.09 & 22.47 + & ( 0.15 ) & ( 0.16 ) & ( 0.06 ) & ( 0.08 ) & ( 0.08 ) & ( 0.65 ) & ( 0.10 ) & ( 0.20 ) & ( 0.27 )",
    "+ kde & 16.67 & *   11.01 @xmath230 * & *   1.36 * @xmath230 & 23.83 & 21.21 & 19.81 & * 4.79 * & 28.11 & 23.20 + & ( 0.14 ) & ( 0.12 ) & ( 0.05 ) & ( 0.03 ) & ( 0.07 ) & ( 0.89 ) & ( 0.13 ) & ( 0.30 ) & ( 0.28 ) +    cart & 17.69 & 16.63 & 10.99 & 56.61 & 31.41 & 53.43 & 17.40 & 33.96 & 28.78 + & ( 0.18 ) & ( 0.20 ) & ( 0.22 ) & ( 0.12 ) & ( 0.10 ) & ( 0.56 ) & ( 0.25 ) & ( 0.34 ) & ( 0.35 ) + rf & 13.23 & 11.58 & 2.12 & 57.02 & 25.52 & 30.91 & 9.67 & * 22.08*@xmath230 & 19.10 + & ( 0.14 ) & ( 0.15 ) & ( 0.06 ) & ( 0.12 ) & ( 0.07 ) & ( 0.48 ) & ( 0.25 ) & ( 0.34 ) & ( 0.30 ) +    spd & * 12.53 * & 15.44 & 2.34 & *   15.12 * @xmath230 & *   16.35 * @xmath230 & 12.59 & 14.64 & 27.70 & 19.98 + & ( 0.21 ) & ( 0.15 ) & ( 0.08 ) & ( 0.06 ) & ( 0.08 ) & ( 0.74 ) & ( 0.28 ) & ( 0.30 ) & ( 0.31 ) + lspd & *   12.49 * @xmath230 & * 11.35 * & 1.85 & * 15.36 * & 17.15 & 12.59 & 5.10 & 27.46 & 20.51 + & ( 0.15 ) & ( 0.11 ) & ( 0.07 ) & ( 0.06 ) & ( 0.08 ) & ( 0.74 ) & ( 0.12 ) & ( 0.30 ) & ( 0.33 ) +    the figure marked by ` @xmath230 ' is the best misclassification rate observed for a data set . the other figures in bold ( if any ) are the misclassification rates whose differences with the best misclassification rate are statistically insignificant at the 5% level .",
    "because of the singularity of the estimated class dispersion matrices , qda could note be used in some cases and those are marked by ` xxxx ' .    in biomedical and vehicle data sets ,",
    "scatter matrices of the competing classes were very different .",
    "so , qda had significant improvement over lda .",
    "in fact , its misclassification rates of qda were close to the best ones . in both of these data sets ,",
    "the class distributions were nearly elliptic ( this can be verified using the diagnostic plots suggested in @xcite ) .",
    "the spd classifiers utilized the ellipticity of the class distributions to outperform the nonparametric classifiers .",
    "the lspd classifier could compete with the spd classifier in the biomedical data .",
    "but in the vehicle data , where the evidence of ellipticity was much stronger , it had a slightly higher misclassification rate . in the parkinson s data set , we could not use qda because of the singularity of the estimated class dispersion matrices .",
    "so , we used the estimated pooled dispersion matrix for standardization in our classifiers . in this data set , all nonparametric classifiers had significantly lower misclassification rates than lda . among them ,",
    "the classifier based on kde had the lowest misclassification rate .",
    "the performance of lspd classifier was also competitive .",
    "since the underlying distributions were non - elliptic , the lspd classifier significantly outperformed the spd classifier .",
    "we observed almost the same phenomenon in the wine data set as well , where the classifier based on kde yielded the best misclassification rate followed by the lspd classifier .",
    "in these two data sets , although the data dimension was quite high , all competing classes had low intrinsic dimensions ( can be estimated using @xcite ) .",
    "so , the nonparametric methods like kde were not much affected by the curse of dimensionality .",
    "recall that for small values of @xmath107 , lspd@xmath124 performs like kde .",
    "therefore , the difference between the misclassification rates of kde and lspd classifiers was statistically insignificant .    in the waveform data set , the spd classifier had the best misclassification rate . in this data set ,",
    "the class distributions were nearly elliptic .",
    "so , the spd classifier was expected to perform well . as the lspd classifier is quite flexible , it yielded competitive misclassification rates . here , the class distributions were not normal ( can be checked using the method in @xcite ) , and they did not have low intrinsic dimensions . as a result , other parametric as well as nonparametric classifiers had relatively higher misclassification rates",
    ".    recall that in the satimage data set , results are based on a single training and a single test set .",
    "so , the standard errors of the misclassification rates were high for all classifiers , and this makes it difficult to compare the performance of different classifiers . in this data set , @xmath14-nn classifiers led to the lowest misclassification rate , but spd and lspd classifiers performed better than all other classifiers .",
    "nonlinear svm , cart and rf had quite high misclassification rates .",
    "we further analyzed some data sets , where the sample size was quite small compared to data dimension . in these data sets",
    ", we worked with unstandardized observations . instead of using the estimated pooled dispersion matrix",
    ", we used the identity matrix for implementation of lda .",
    "the growth data set contains growth curves of males and females , which are smooth and monotonically increasing functions . because of high dependence among the measurement variables , the class distributions had low intrinsic dimensions , and they were non - elliptic .",
    "as a result , the nonparametric classifiers performed well .",
    "svm with the rbf kernel had the best misclassification rate , but those of @xmath14-nn , kde and lspd classifiers were also comparable .",
    "good performance of the linear svm classifier indicates that there was a good linear separability between the two classes , but lda failed to figure it out .",
    "the lightning 2 data set consists of observations that are realizations of time series . in this data set , rf had the best performance followed by the lspd classifier . here also , we observed non - elliptic class distributions with low intrinsic dimensions @xcite .",
    "this justifies the good performance of the classifiers based on @xmath14-nn and kde .",
    "the spd classifier also had competitive misclassification rates because of the flexibility of gam .",
    "in fact , it yielded the third best performance in this data set .    finally , we analyzed the colon data set , which contains micro - array expressions of 2000 genes for some ` normal ' and ` colon cancer ' tissues . in this data",
    "set , there was good linear separability among the observations from the two classes .",
    "so , lda and linear svm had lower misclassification rates than all other classifiers . among the nonparametric classifiers , rf had the best performance closely followed by the spd classifier .",
    "these two classifiers were less affected by the curse of dimensionality . recall that lspd@xmath124 with large bandwidth @xmath107 approximates spd .",
    "because of this automatic adjustment , the lspd classifier could nearly match the performance of the spd classifier .        to compare the overall performance of different classifiers , following the idea of @xcite , we computed their efficiency scores on different data sets . for a data set , if @xmath235 classifiers have misclassification rates @xmath236 , the efficiency of the @xmath237-th classifier ( @xmath238 ) is defined as @xmath239 , where @xmath240 .",
    "clearly , in any data , the best classifier has @xmath241 , while a lower value of @xmath238 indicates the lack of efficiency of the @xmath237-th classifier . in each of these benchmark data sets",
    ", we computed this ratio for all classifiers , and they are graphically represented by box plots in figure 7 .",
    "this figure clearly shows the superiority of the lspd classifier ( with the highest median value of 0.88 ) over its competitors .",
    "we did not consider qda for comparison because it could not be used for some of the data sets .",
    "in this article , we develop and study classifiers constructed by fitting a nonparametric additive logistic regression model to features extracted from the data using spd as well as its localized version , lspd .",
    "the spd classifier can be viewed as a generalization of parametric classifiers like lda and qda .",
    "when the underlying class distributions are elliptic , it has bayes optimality . for large values of @xmath107",
    ", while lspd@xmath124 behaves like spd , for small values of @xmath107 , it captures the underlying density .",
    "so , the multiscale classifier based on lspd is flexible , and it overcomes several drawbacks associated with spd and other existing depth based classifiers . when the underlying class distributions are elliptic but not normal , both spd and lspd classifiers outperform popular parametric classifiers like lda and qda as well as nonparametric classifiers . in the case of non - elliptic or multi - modal",
    "distributions , while spd may fail to extract meaningful discriminatory features , the lspd classifier can compete with other nonparametric methods . moreover ,",
    "for high - dimensional data , while traditional nonparametric methods suffer from the curse of dimensionality , both spd and lspd classifiers can lead to low misclassification probabilities . analyzing several simulated and benchmark data sets",
    ", we have amply demonstrated this . in high - dimensional benchmark data sets , the class distributions had low intrinsic dimensions due to high correlation among the the measurement variables @xcite .",
    "moreover , the competing classes differed mainly in their locations . as a consequence , though the proposed lspd classifier had the best overall performance in benchmark data sets , its superiority over other nonparametric methods was not as prominent as it was in the simulated examples .",
    "* lemma 1 :* if @xmath35 has a spherically symmetric density @xmath242 on @xmath15 with @xmath243 , then @xmath244\\|$ ] is a non - negative monotonically increasing function of@xmath53 .",
    "* proof of lemma 1 :* in view of spherical symmetry of @xmath245 , @xmath246\\|$ ] is invariant under orthogonal transformations of @xmath7 .",
    "consequently , @xmath247 for some non - negative function @xmath248 .",
    "consider now @xmath249 and @xmath250 such that @xmath251 .",
    "using spherical symmetry of @xmath245 , without loss of generality , we can assume @xmath252 for @xmath253 such that @xmath254 .",
    "for any @xmath255 , we have @xmath256 \\biggr|,\\ ] ] due to spherical symmetry of @xmath245 .",
    "note also that for any @xmath34 with @xmath243 , @xmath257 $ ] is a strictly convex function of @xmath7 in this case .",
    "consequently , it is a strictly convex function of @xmath237 .",
    "observe now that @xmath258 with this choice of @xmath7 is the absolute value of the derivative of @xmath257 $ ] w.r.t .",
    "@xmath237 . this derivative is a symmetric function of @xmath237 that vanishes at @xmath259 .",
    "hence , @xmath258 is an increasing function of @xmath260 , and this proves that @xmath261 . @xmath262    * proof of theorem 1 :* if the population distribution @xmath13 ( @xmath5 ) is elliptically symmetric , we have @xmath263 , where @xmath264 @xmath265 .",
    "since spd@xmath266 is affine invariant , it is a function of @xmath267 , the mahalanobis distance .",
    "again , since @xmath268 has a spherically symmetric distribution with its center at the origin , from lemma 1 it follows that spd@xmath269 is a monotonically decreasing function of @xmath267 .",
    "so , @xmath267 is also a function of spd@xmath269 .",
    "therefore , @xmath13 , which is a function of @xmath267 , can also be expressed as @xmath270 where @xmath271 is an appropriate real - valued function that depends on @xmath272 .",
    "now , one can check that @xmath273 for @xmath274 .",
    "now , for @xmath275 , define @xmath276 and @xmath277 .",
    "so , if we define @xmath278 , the proof of the theorem is complete .",
    "* remark 1 * : if @xmath13 is unimodal , @xmath279 is monotonically increasing for @xmath5 .",
    "moreover , if the distributions differ only in their locations , the @xmath279s are same for all class . in that case , @xmath280 for @xmath281 , and hence the classifier turns out to be the maximum depth classifier .    * proof of theorem 2 ( a ) :* let @xmath282 . for any fixed @xmath34 and the distribution function @xmath283",
    ", we have @xmath284 - \\| e_{f_j } [ k_{h}({{\\mbox{\\bf t } } } ) u({\\mbox{\\bf t } } ) ] \\|,$ ] where @xmath285 for @xmath5 .",
    "for the first term in the expression of lspd@xmath286 above , we have @xmath287 = \\int_{\\mathbb{r}^d } { h}^{-d } k_{h}({\\mbox{\\boldmath $ \\sigma$}}_{j}^{-1/2}({\\mbox{\\bf x}}-{\\mbox{\\bf v } } ) ) f_j({\\mbox{\\bf v } } ) d{\\mbox{\\bf v}}= |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } \\int_{\\mathbb{r}^d } k({\\mbox{\\bf y } } ) f_j({\\mbox{\\bf x}}-h{\\mbox{\\boldmath $ \\sigma$}}_{j}^{1/2}{\\mbox{\\bf y } } ) d{\\mbox{\\bf y } } , \\nonumber\\end{aligned}\\ ] ] where @xmath288 .",
    "so , using taylor s expansion of @xmath13 , we get @xmath287 = |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } f_j({\\mbox{\\bf x } } ) - h |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } \\int_{\\mathbb{r}^d } k({\\mbox{\\bf y } } ) ~({\\mbox{\\boldmath $ \\sigma$}}_{j}^{1/2}{\\mbox{\\bf y}})^{\\prime } \\nabla f_j({\\mbox{\\boldmath $ \\xi$ } } ) d{\\mbox{\\bf y } } , \\nonumber \\ ] ] where @xmath289 lies on the line joining @xmath7 and @xmath290 .",
    "so , using the cauchy - scawartz inequality , one gets @xmath291 - |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } f_j({\\mbox{\\bf x}})\\bigr| \\leq h |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } \\lambda_{j}^{1/2 } m^{\\circ}_{j } m_k$ ] , where @xmath292 , @xmath293 , and @xmath294 is the largest eigenvalue of @xmath121 for @xmath5 .",
    "this implies @xmath291 - |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } f_j({\\mbox{\\bf x}})\\bigr| \\rightarrow 0 $ ] as @xmath108 for @xmath5 .    for the second term in the expression of lspd@xmath286 , a similar argument yields @xmath295 = |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } \\int_{\\mathbb{r}^d } k({\\mbox{\\bf y } } ) u({{\\mbox{\\bf y } } } ) f_j({\\mbox{\\bf x}}-h{\\mbox{\\boldmath $ \\sigma$}}_{j}^{1/2}{\\mbox{\\bf y } } ) d{\\mbox{\\bf y}}~~~~~~~~~~~~~~~~~~~~~~~ \\nonumber \\\\ & & ~~~~~~~~~~~~~~~~= - h |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } \\int_{\\mathbb{r}^d } k({\\mbox{\\bf y } } ) u({{\\mbox{\\bf y } } } ) ~({\\mbox{\\boldmath $ \\sigma$}}_{j}^{1/2}{\\mbox{\\bf y}})^{\\prime } \\nabla f_j({\\mbox{\\boldmath $ \\xi$ } } ) d{\\mbox{\\bf y}}~~(\\mbox{since } \\int k({\\mbox{\\bf y}})u({\\mbox{\\bf y}})d{\\mbox{\\bf y}}={\\bf 0 } ) .",
    "\\nonumber \\ ] ] so , @xmath296\\| \\leq h |{\\mbox{\\boldmath $ \\sigma$}}_{j}|^{1/2 } \\lambda_{j}^{1/2 } m^{\\circ}_{j } m_k \\rightarrow 0~\\mbox{as } ~h~ \\rightarrow 0 $ ] , and hence , @xmath297 consequently , @xmath128lspd@xmath118 , @xmath66 , lspd@xmath298 as @xmath108 . @xmath299    * proof of theorem 2 ( b ) :* here we consider the case @xmath212 . consider any fixed @xmath34 and any fixed @xmath4 ( @xmath5 ) .",
    "for any fixed @xmath300 , since @xmath301 as @xmath109 , using dominated convergence theorem ( note that @xmath101 is bounded ) , one can show that @xmath302 so , @xmath128lspd@xmath303 @xmath304 , @xmath66 , lspd@xmath305spd@xmath306spd@xmath307 as @xmath109 .",
    "@xmath299    * proof of theorem 3 :* define the sets @xmath308 , and @xmath309 .",
    "clearly @xmath310 , the set @xmath311 is a closed ball and the set @xmath312 has cardinality @xmath313 .",
    "we will prove the almost sure ( a.s . )",
    "uniform convergence on the three sets : ( i ) on @xmath312 ( ii ) on @xmath314 , and ( iii ) on @xmath315 .",
    "consider any fixed @xmath316 $ ] . recall that for this choice of @xmath107 , lspd@xmath317 ( see equation ( 3 ) ) and lspd@xmath318 are defined as follows : @xmath319 @xmath320 - h^{-d}\\|e[k(h^{-1}({{\\mbox{\\bf x}}- { \\mbox{\\bf x } } } ) ) ~u({\\mbox{\\bf x}}- { \\mbox{\\bf x}})]\\|.\\ ] ]    \\(i ) define @xmath321 $ ] for @xmath137 .",
    "note that @xmath322s are independent and identically distributed ( i.i.d . ) with @xmath323 and @xmath324 .",
    "using the exponential inequality for sums of i.i.d .",
    "random vectors ( see p. 491 of @xcite ) , for any fixed @xmath325 , we get @xmath326 , where @xmath327 is a positive constant that depends on @xmath132 and @xmath233 .",
    "this now implies that @xmath328 \\bigr\\| \\ge \\epsilon\\biggr ) \\\\",
    "\\vspace{-0.05 in } \\leq p\\biggl(\\bigl\\|\\frac{1}{nh^d } \\sum_{i=1}^{n}k(h^{-1}({{\\mbox{\\bf x}}- { \\mbox{\\bf x}}_i } ) ) u({\\mbox{\\bf x}}- { \\mbox{\\bf x}}_i ) - h^{-d}e[k(h^{-1}({{\\mbox{\\bf x}}- { \\mbox{\\bf x } } } ) ) u({\\mbox{\\bf x}}- { \\mbox{\\bf x } } ) ]",
    "\\bigr\\| \\ge \\epsilon\\biggr)\\end{array}\\ ] ] @xmath329 for a fixed value of @xmath107 , since @xmath330 is a sum of i.i.d . bounded random variables , using bernstein s inequality , we also have @xmath331\\bigr|\\ge \\epsilon \\biggr ) \\le 2 e^{-c_1n\\epsilon^2}\\ ] ] for some suitable positive constant @xmath332 .",
    "this implies @xmath333\\bigr|\\ge \\epsilon\\biggr ) \\le 2 e^{-c_1nh^{2d}\\epsilon^2}. \\vspace{-0.1in}\\ ] ] combining equations ( 6 ) and ( 7 ) , we get @xmath334 for some suitable constants @xmath335 and @xmath336 .",
    "since the cardinality of @xmath312 is @xmath337 , we have @xmath338 now , @xmath339 .",
    "so , a simple application of borel - cantelli lemma implies that @xmath340 ( ii ) consider the set @xmath314 .",
    "note that given any @xmath7 in @xmath314 , there exists @xmath341 such that @xmath342 .",
    "first we will show that @xmath343 as @xmath147 . using the mid - value theorem",
    ", one gets where @xmath289 lies on the line joining @xmath7 and @xmath344 .",
    "note that the right hand side is less than @xmath345 , where @xmath346 .",
    "this upper bound is free of @xmath7 , and goes to @xmath347 as @xmath147 . now , @xmath348",
    "@xmath349 \\right\\| \\vspace{-0.1in}\\ ] ] @xmath350 \\right| + k({\\bf 0 } ) \\left\\| \\frac{1}{nh^d } \\sum_{i=1}^n   \\{u({\\mbox{\\bf x}}- { \\mbox{\\bf x}}_i ) - u({\\mbox{\\bf y}}- { \\mbox{\\bf x}}_i)\\ } \\right \\|.\\ ] ] we have already proved that the first part converges to @xmath347 in a.s . sense . for the second part ,",
    "consider a ball of radius @xmath351 around @xmath7 ( say , @xmath352 ) . now , @xmath353",
    "@xmath354 @xmath355    note that @xmath356 are i.i.d",
    ". bounded random variables with expectation @xmath357 .",
    "so , the a.s .",
    "convergence of the first term follows from bernstein s inequality . since @xmath358 ( where @xmath359 ) , the second term converges to @xmath347",
    "the third term also converges to @xmath347 as @xmath360 .",
    "therefore , we have @xmath361 as @xmath360 .",
    "similarly , one can prove that @xmath362 as @xmath360 .",
    "note that in the arguments above , all bounds are free from @xmath7 and @xmath344 .",
    "we have also proved that @xmath363 as @xmath360 .",
    "so , combining these results , we have @xmath364      fix any @xmath365 .",
    "we can choose two constants @xmath366 and @xmath367 such that @xmath368 and @xmath369 when @xmath370 .",
    "now , one can check that @xmath371 \\le h^{-d}e\\left[k(h^{-1}({\\mbox{\\bf x}}-{\\mbox{\\bf x}}))i(\\|{\\mbox{\\bf x}}\\|\\le m_1)\\right ] + h^{-d}k({\\bf 0})p(\\|{\\mbox{\\bf x}}\\| > m_1 ) .",
    "\\vspace{-0.05in}\\ ] ] note that if @xmath372 and @xmath373 , @xmath374 .",
    "now , choose @xmath375 large enough such that @xmath376 , and this implies @xmath377 .",
    "so , we get @xmath371 \\le",
    "\\epsilon/2 + h^{-d}k({\\bf 0})p(\\|{\\mbox{\\bf x}}\\| > m_1)\\le \\epsilon,~\\mbox{and}\\ ] ] @xmath378 @xmath379 the glivenko - cantelli theorem implies that the last term on the right hand side converges to @xmath347 as @xmath147 .",
    "so , we have @xmath380          * remark 3 : * the result continues to hold when @xmath108 as well .",
    "however , for the a.s .",
    "convergence in part ( i ) , ( more specifically , to use the borel - cantelli lemma ) , we require @xmath146 as @xmath147 . in part ( iii ) , we need @xmath366 and @xmath367 to vary with @xmath375 .",
    "assume the first moment of @xmath142 to be finite , and @xmath383 ( which implies @xmath384 as @xmath385 ) .",
    "also assume that @xmath146 as @xmath147 .",
    "we can now choose @xmath386 to ensure that both @xmath368 and @xmath369 for @xmath370 hold for sufficiently large @xmath375 .",
    "* proof of theorem 4 ( a ) :* consider two independent random vectors @xmath387 , @xmath388 and @xmath389 , where @xmath5 .",
    "it follows from ( c1 ) and ( c2 ) that @xmath390 so , for almost every realization @xmath7 of @xmath199 , @xmath391 next , consider two independent random vectors @xmath199 and @xmath392 for @xmath281",
    ". using ( c1 ) and ( c2 ) , we get @xmath393 consequently , for almost every realization @xmath7 of @xmath199 @xmath394    let us next consider @xmath395 , where @xmath199 , @xmath396 are independent random vectors , and @xmath397 denotes the inner product in @xmath15 . therefore , for almost every realization @xmath7 of @xmath96 , arguments similar to those used in ( 8) and ( 9 ) yield @xmath398 observe now that @xmath399 \\|^2 = \\langle e_{f_j}[u({\\mbox{\\bf x}}- { \\mbox{\\bf x}}_1 ) ] , e_{f_j}[u({\\mbox{\\bf x}}- { \\mbox{\\bf x}}_{2 } ) ] \\rangle = e_{f_j } \\ { \\langle u({\\mbox{\\bf x}}- { \\mbox{\\bf x}}_{1 } ) , u({\\mbox{\\bf x}}- { \\mbox{\\bf x}}_{2 } ) \\rangle \\},$ ] where @xmath400 are independent random vectors for @xmath5 .",
    "since here we are dealing with expectations of random vectors with bounded norm , a simple application of dominated convergence theorem implies that for almost every realization @xmath7 of @xmath199 @xmath401 , as @xmath191 , @xmath402 therefore , for @xmath199 , we get @xmath403 as @xmath191.@xmath299 * proof of theorem 4 ( b ) :* recall that for @xmath404 , @xmath405 - \\| e_f[h^dk_h({\\mbox{\\bf t } } ) u({\\mbox{\\bf t } } ) ] \\|$ ] , and since we have assumed @xmath96s to be standardized , here we have @xmath406 .",
    "let @xmath37 and @xmath407 where @xmath408 .",
    "then , using ( 8) and ( 9 ) above , and the continuity of @xmath163 , for almost every realization @xmath7 of @xmath199 , one gets @xmath409 depending on whether @xmath410 , for almost every realization @xmath7 of @xmath199 .",
    "the proof now follows from a simple application of dominated convergence theorem .",
    "@xmath299    * proof of theorem 4 ( c ) :* since @xmath411 as @xmath412 , using the same argument as used in the proof of theorem 3(b ) , for @xmath407 and almost every realization @xmath7 of @xmath199 , we have @xmath413 the proof now follows from a simple application of dominated convergence theorem . @xmath299    * lemma 2 :* recall @xmath215 and @xmath414 for @xmath5 defined in theorem 3 ( a ) and ( b ) , respectively . for any @xmath204 ,",
    "@xmath415 if and only if @xmath416 and @xmath417 . similarly , @xmath418 if and only if @xmath416 and @xmath417 .",
    "* proof of lemma 2 :* the ` if ' part is easy to check in both cases .",
    "so , it is enough to prove the ` only if ' part and that too for the case of @xmath419 .",
    "note that if @xmath420 and @xmath421 are equal , we have @xmath422 these two equations hold simultaneously only if @xmath423 and @xmath424 .    now consider the case @xmath425 .",
    "recall that @xmath426 , @xmath427 , @xmath428 and @xmath429 .",
    "if possible , assume that @xmath430 .",
    "this implies that @xmath431 and hence @xmath432 also , if @xmath430 , we must have @xmath433 combining ( 13 ) and ( 14 ) , we have @xmath434 , and this implies @xmath435 . similarly , if @xmath436 , we get @xmath437 and hence @xmath435 . again , if @xmath438 but @xmath439 , similar arguments lead to @xmath435 .",
    "this completes the proof of the lemma .",
    "@xmath299        alon , u. , barkai , n. , notterman , d. a. , gish , k. , mack , d. and leine , a. j. ( 1999 ) broad pattern of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays .",
    ", usa _ , * 96 * , 6745 - 6750 .",
    "dimitriadou , e. , hornik , k. , leisch , f. , meyer , d. and weingessel , a. ( 2011 ) e1071 : misc functions of the department of statistics ( e1071 ) , tu wien .",
    "r package version 1.5 - 27 .",
    "_ http://cran.r-project.org/package=e1071 _"
  ],
  "abstract_text": [
    "<S> in this article , we develop and investigate a new classifier based on features extracted using spatial depth . </S>",
    "<S> our construction is based on fitting a generalized additive model to the posterior probabilities of the different competing classes . to cope with possible multi - modal as well as non - elliptic population distributions , we develop a localized version of spatial depth and use that with varying degrees of localization to build the classifier . </S>",
    "<S> final classification is done by aggregating several posterior probability estimates each of which is obtained using localized spatial depth with a fixed scale of localization . </S>",
    "<S> the proposed classifier can be conveniently used even when the dimension is larger than the sample size , and its good discriminatory power for such data has been established using theoretical as well as numerical results . * keywords :* bayes classifier , elliptic and non - elliptic distributions , hdlss asymptotics , uniform strong consistency , weighted aggregation of posteriors .     </S>",
    "<S> + @xmath0theoretical statistics and mathematics unit , indian statistical institute , 203 , b. t. road , kolkata 700108 , india . </S>"
  ]
}