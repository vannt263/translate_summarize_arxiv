{
  "article_text": [
    "markov decision processes ( mdps ) @xcite comprise a popular framework for sequential decision - making in a random dynamic environment . at each time step , an agent observes the state of the system of interest and chooses an action .",
    "the system then transitions to its next state , with the transition probability determined by the current state and the action taken .",
    "there is a ( possibly time - varying ) cost associated with each admissible state - action pair , and a policy ( feedback law ) for mapping states to actions is selected to minimize average cost . in the basic mdp framework",
    ", it is assumed that the cost functions and the transition probabilities are known in advance , the policy is designed `` offline '' ( e.g. , using dynamic programming ) , and the optimality criterion is forward - looking , taking into account the effect of past actions on future costs . in many practical problems ,",
    "however , this degree of advance knowledge is unavailable . when neither the transition probability nor the cost functions are known in advance ,",
    "various reinforcement learning ( rl ) methods , such as the celebrated @xmath0-learning algorithm @xcite and its variants , can be used to learn an optimal policy in an online regime . however , the key assumptions underlying rl are that the agent is operating in a stochastically stable environment , and that the state - action costs ( or at least their expected values with respect to any environmental randomness ) do not vary with time .",
    "these assumptions are needed to ensure that the agent is eventually able to learn an optimal stationary control policy .",
    "another framework for sequential decision - making , dating back to the seminal work of robbins @xcite and hannan @xcite and now widely used in the machine learning community @xcite , deals with nonstochastic , unpredictable environments . in this _ online learning _ ( or _ sequential prediction _ ) framework , the effects of the environment are modeled by an arbitrarily varying sequence of cost functions , where the cost function at each time step is revealed to the agent only _",
    "after _ an action has been taken .",
    "there is no state , and the goal of the agent is to minimize _ regret _ , i.e. , the difference between the total cost incurred using causally available information and the total cost of the best single action that could have been chosen in hindsight .",
    "in contrast with mdps , the regret - based optimality criterion is necessarily myopic and backward - looking , since the cost incurred at each time step depends only on the action taken at that time step , so past actions have no effect on future costs .",
    "there is also a more stringent model of online learning , in which the agent observes not the entire cost function for each time step , but only the value of this cost at the currently taken action @xcite .",
    "this model is inspired by the celebrated _ multiarmed bandit _",
    "problem first introduced by robbins @xcite , and is referred to as the _ nonstochastic bandit problem_. one widely used way of constructing regret - minimizing strategies for such bandit problems is to randomize the agent s actions ( _ exploration _ ) so that the random cost value revealed to the agent can be used to construct an unbiased estimate of the full cost function , which is then fed into a suitable strategy that minimizes regret under the assumption of full information ( _ exploitation _ ) .",
    "we will not consider nonstochastic bandit problems in this paper .",
    "instead , we refer the reader to a recent survey by bubeck and cesa - bianchi @xcite that discusses both stochastic and nonstochastic bandit problems .    recent work by even - dar et al .",
    "@xcite and yu et al .",
    "@xcite combines the mdp and the online learning frameworks into what may be described as _",
    "online mdps _ with finite state and action spaces . like in the traditional mdp setting",
    ", the agent observes the current state and chooses an action , and the system transitions to the next state according to a fixed and known markov law .",
    "however , like in the online framework , the one - step cost functions form an arbitrarily varying sequence , and the cost function corresponding to each time step is revealed to the agent after the action has been taken .",
    "the objective of the agent is to minimize regret relative to the best stationary markov policy that could have been selected with full knowledge of the cost function sequence over the horizon of interest .",
    "the time - varying cost functions may represent unmodeled aspects of the environment or collective ( and possibly irrational ) behavior of any other agents that may be present ; the regret minimization viewpoint then ensures that the agent s _ online _ policy is robust against these effects .",
    "we give here a brief statement of the problem of interest in order to fix ideas ; a more detailed formulation is given later on .",
    "the reader may wish to consult section  [ ssec : notation ] for notation .",
    "the set - up considered in @xcite is motivated by problems in machine learning and artificial intelligence , where the actions are the main object of interest , and the state merely represents memory effects present in the system . in this paper , we take a more control - oriented view : the emphasis is on steering the system along a desirable state trajectory through actions selected according to a state feedback law .",
    "following the formulation proposed recently by todorov @xcite , we allow the agent to modulate the state transitions directly , so that actions ( resp .",
    ", state feedback laws ) correspond to probability distributions ( resp .",
    ", markov kernels ) on the underlying state space . as in @xcite , the one - step cost is a sum of two terms : the state cost , which measures how `` desirable '' each state is , and the control cost , which measures the deviation of the transition probabilities specified by the chosen action from some fixed _ default _ or _ passive dynamics_. ( we also refer the reader to a recent paper by kappen et al .",
    "@xcite , which interprets todorov s set - up as an inference problem for probabilistic graphical models . )",
    "more precisely , we consider an mdp with a finite state space @xmath1 , where the action space @xmath2 is the simplex @xmath3 of probability distributions over @xmath1 . a fixed markov matrix ( transition kernel ) @xmath4_{x , y \\in \\sx}$ ] is given .",
    "a stationary markov policy ( state feedback law ) is a mapping @xmath5 , so if the system is in state @xmath6 , then the transition to the next state is stochastic , as determined by the probability distribution @xmath7 . in other words , if we denote the next state by @xmath8 , then the state transitions induced by the action @xmath9 are governed by the conditional probability law @xmath10(x^+).\\ ] ] the one - step state - action cost @xmath11 consists of two terms , the _ state cost _ @xmath12 , where @xmath13 is a given function , and the _ control cost _ , which penalizes any deviation of the next - state distribution @xmath14 from the one prescribed by @xmath15 , the row of @xmath16 corresponding to @xmath17 . to motivate the introduction of such control costs",
    ", we can imagine the situation , in which implementing the state transitions according to @xmath16 can be done `` for free . ''",
    "however , it may very well be the case that following @xmath16 will be in conflict with the goal of keeping the state cost low . from this perspective",
    ", it may actually be desirable to deviate from @xmath16 .",
    "any such deviation may be viewed as an _ active perturbation _ of the _ passive dynamics _ prescribed by @xmath16 , and the agent should attempt to balance the tendency to keep the state costs low against allowing too strong of a perturbation of @xmath16 .",
    "our choice of control cost is inspired by the work of todorov @xcite , and is given by the _",
    "kullback  leibler divergence _ ( or the _ relative entropy _ ) @xcite @xmath18 between the proposed next - state distribution @xmath19 and the next - state distribution prescribed by the passive dynamics @xmath16 .",
    "one useful property of this control cost is that it automatically forbids all those state transitions that are already forbidden by @xmath16 . indeed ,",
    "if for a given @xmath6 there exists some @xmath20 such that @xmath21(y ) > 0 $ ] , while @xmath22 , then @xmath23 .",
    "thus , the overall one - step state - action cost is given by @xmath24 in the online version of this problem ( detailed in section  [ ssec : model ] ) , the state costs form an arbitrarily varying sequence @xmath25 , and the agent learns the state cost for each time step only after having selected the transition law to determine the next state .",
    "for any given value of the horizon , the regret is computed with respect to the best stationary markov policy ( state feedback law ) that could have been chosen in hindsight .",
    "the precise definition of regret is given in section  [ ssec : regret ] .",
    "since this is a nonstandard set - up , we take a moment to situate it in the context of usual models of mdps . in a standard mdp with finite state and action spaces",
    ", we have a finite collection of markov matrices @xmath26 on @xmath1 indexed by the actions @xmath27 .",
    "state feedback laws are functions @xmath28 , and the set of all such functions is finite with cardinality @xmath29",
    ". therefore , in each state @xmath6 the agent has at most @xmath30 choices for the distribution of the next state @xmath8 , and we may equivalently represent each state feedback law @xmath31 as a mapping from @xmath1 into @xmath3 with @xmath32 . since the state space @xmath2 is finite , the range of this mapping is a finite subset of the probability simplex @xmath3 .",
    "the criterion for selecting this next - state distribution pertains to minimization of the expectation of the immediate state - action cost plus a suitable value function that accounts for the effect of the current action on future costs . in many cases ,",
    "the one - step state - action cost @xmath11 decomposes into a sum of state cost @xmath12 and control cost @xmath33 , where @xmath12 quantifies the ( un)desirability of the state @xmath17 , while @xmath33 represents the effort required to apply action @xmath9 in state @xmath17 .    in the set - up of @xcite ,",
    "the collection of all possible next - state distributions is unrestricted . as a consequence ,",
    "any mapping @xmath5 is a feasible stationary markov policy .",
    "since any markov matrix @xmath34 on @xmath1 can be equivalently represented as a mapping from @xmath1 into @xmath3 with @xmath35 , there is thus a one - to - one correspondence between state feedback laws and markov matrices on @xmath1 .",
    "in contrast to the case when the agent may choose among a finite set of actions , the probability simplex @xmath3 is an uncountable set , so the agent has considerably greater freedom to choose the next state distribution .",
    "as before , we introduce a state - action cost @xmath36 , where @xmath12 measures the ( un)desirability of state @xmath17 , while @xmath33 quantifies the difficulty of executing action @xmath9 in state @xmath17 .",
    "since actions @xmath9 now correspond to probability distributions , and we choose the kullback ",
    "leibler control cost @xmath37 , where @xmath16 is a fixed markov matrix on the state space @xmath1 that may represent , e.g. , the `` free '' dynamics of the system in the absence of external controls .",
    "the kullback ",
    "leibler divergence is widely used in stochastic control and inference .",
    "first of all , it has many desirable properties , such as nonnegativity and convexity @xcite .",
    "secondly , if we adopt the viewpoint that the purpose of a control policy is to shape the joint distribution of all relevant variables describing the closed - loop behavior of the system , then using the relative entropy to compare the distribution induced by any control law to some reference model leads to functional equations for the optimal policy that are often easier to solve than the corresponding dynamical programming recursion @xcite ( e.g. , see @xcite for an alternative derivation of the optimal controller in an lqg problem using relative entropy instead of dynamic programming ) ; similar ideas are fruitful in the context of robust control , where the relative entropy is used to quantify the radius of uncertainty around some nominal system @xcite .",
    "moreover , the relative entropy is a canonical regularization functional for stochastic nonlinear filtering problems @xcite : an optimal bayesian filter is the solution of a variational problem that entails minimization of the sum of expected negative log - likelihood ( which can be interpreted as state cost ) and a relative entropy with respect to the prior measure on the state space .    to further motivate our interest in problems of this sort ,",
    "let us consider two examples .",
    "one is _ target tracking _ with an arbitrarily moving target ( or multiple targets ) . in this example",
    ", the state space @xmath1 is the vertex set of an undirected graph , and the passive dynamics @xmath16 specifies some default _ random walk _ on this graph .",
    "the tracker s discrete - time motion is constrained by the topology of the graph , while the targets motions are not . at each time",
    "@xmath38 , the state cost @xmath39 is the tracking error , which quantifies how far the tracker is from the targets .",
    "for instance , it may be given by the graph distance ( length of shortest path ) between the tracker s current location and the location of the closest target .",
    "other possibilities can also be considered , including some based on noisy information on the location of the targets .",
    "the control cost penalizes the tracker s deviation from @xmath16 as it attempts to track the targets .",
    "the passive dynamics @xmath16 can be seen as the tracker s prior model for the targets motion . moreover ,",
    "if @xmath16 is sufficiently rapidly mixing , then any tracker that follows @xmath16 will visit every vertex of the graph infinitely often with probability one ; however , there is no guarantee that the tracker s prior model is correct ( i.e. , that the tracker will be anywhere near the targets ) .",
    "hence , the state - action cost will trade off the tendency of the tracker to `` cover '' the graph as much as possible ( exploration ) against the tendency to follow a potentially faulty model of the targets ( exploitation ) .",
    "another example setting is real - time control of a _ brain - machine interface_. there , the state space @xmath1 may be the set of possible positions or modes of a neural prosthetic device , and the passive dynamics @xmath16 may encode the `` natural '' ( free ) dynamics of the device in the absence of user control ; we may assume , for instance , that the state transitions prescribed by @xmath16 correspond to `` minimum - energy '' operating mode of the device .",
    "if the user wishes to make the device execute some trajectory , the state cost @xmath39 at time @xmath38 may represent the deviation of the current point on the trajectory from the one intended by the user .",
    "since the user is a human operator with conscious intent , we may not want to ascribe an a priori model to her intended trajectory , and instead treat it as an individual sequence modulating the state costs @xmath25 . in this setting , the kullback ",
    "leibler control cost penalizes significant deviations from the free dynamics @xmath16 , since these will typically be associated with energy expenditures .",
    "the common thread running through these two examples ( and it is certainly possible to construct many others ) is that they model real - time interaction of a particular system with some well - defined `` reference '' or  nominal \" dynamics @xmath16 with a potentially unpredictable environment ( which may include hard - to - model adversaries or rational agents , etc . ) , and we must balance the tendency to respond to immediate changes in the environment against the need to operate the system near the nominal mode . since no offline policy design is possible in such circumstances ,",
    "the regret minimization framework offers a meaningful alternative .      in this paper",
    ", we give an explicit construction of a strategy for the agent , such that the regret relative to any uniformly ergodic class of stationary markov policies grows _ sublinearly _ as a function of the horizon .",
    "the only regularity conditions needed for this result to hold are ( a ) uniform boundedness of the state costs ( the agent need not know the bound , only that it exists ) ; and ( b ) ergodicity of the passive dynamics . moreover , our strategy is computationally efficient : the time is divided into phases of increasing length , and during each phase the agent applies a stationary markov policy optimized for the average of the state cost functions revealed during all of the preceding phases .",
    "thus , our strategy belongs to the class of so - called `` lazy '' strategies for online decision - making problems @xcite ; a similar approach was also taken by yu et al .",
    "@xcite in their paper on online mdps with finite state and action spaces .",
    "the main advantage of lazy strategies is their computational efficiency , which , however , comes at the price of suboptimal scaling of the regret with the time horizon .",
    "we comment on this issue further in the sequel .",
    "our main contribution is an extension of the theory of online mdps to a wide class of control problems that lie outside the scope of existing approaches @xcite . more specifically :    1 .   while in @xcite both the state and the action spaces are finite",
    ", we only assume this for the state space .",
    "our action space is the simplex of probability distributions on the state space , which is a compact subset of a euclidean space .",
    "hence , the techniques used in the existing literature are no longer directly applicable .",
    "( it is also possible to extend our approach to continuous state spaces , but additional regularity conditions will be needed .",
    "this extension will be the focus of our future work . )",
    "@xcite assume that the underlying mdp is unichain ( * ? ? ?",
    "8.3 ) and satisfies a certain uniform ergodicity condition ( a similar assumption is also needed by even - dar et al .",
    "their assumption is rather strong , since it places significant simultaneous restrictions on an exponentially large family of markov chains on the state space ( each chain corresponds to a particular choice of state feedback law , and there are @xmath30 such laws ) .",
    "it is also difficult to verify , since the problem of determining whether an mdp is unichain is np - hard @xcite .",
    "by contrast , our ergodicity assumption pertains to only _",
    "one _ markov chain ( the passive dynamics @xmath16 ) , it can be efficiently verified in polynomial time , and we prove that it automatically implies uniform ergodicity of all stationary control laws that could possibly be invoked by our strategy .",
    "because these stationary control laws correspond to solutions of certain average - cost optimality equations ( acoes ) in the set - up of todorov @xcite , we establish and subsequently exploit several useful and previously unknown results concerning the continuity and uniform ergodicity of optimal policies for todorov s problem .",
    "these results , as well as the techniques used to prove them , play a very important role in our overall contribution .",
    "indeed , in the online setting , the state cost functions are revealed to the agent in real time .",
    "hence , any policy used by the agent must rely on estimates ( or forecasts ) of future state costs based on currently available information .",
    "our new results on todorov s optimal control laws provide sharp bounds on the sensitivity of these laws to misspecification of state costs , and may be of independent interest .",
    "4 .   in @xcite",
    ", the policy computation at the beginning of each phase requires solving a linear program and then adding a carefully tuned random perturbation to the solution . as a result ,",
    "the performance analysis in @xcite is rather lengthy and technical ( in particular , it invokes several advanced results from perturbation theory for linear programs ) .",
    "by contrast , even though we are working with a continuous action space , all policy computations in our case reduce to solving finite - dimensional eigenvalue problems , without any need for additional randomization .",
    "moreover , even though the overall scheme of our analysis is similar to the one in @xcite ( which , in turn , is inspired by existing work on lazy strategies @xcite ) , the proof is self - contained and much less technical , relying on our new results pertaining to todorov - type optimal control laws .",
    "a preliminary version of this work has appeared in a conference publication @xcite , and most of the proofs were omitted due to space limitations . since",
    "a major part of our contribution is a set of probabilistic analysis techniques for mdps with kullback  leibler control cost ( in both online and offline settings ) , the present paper fills in the missing details .",
    "in addition , most of our new results on the sensitivity of todorov - type optimal controllers to perturbations of state costs were omitted from @xcite .",
    "the present paper not only gives a self - contained treatment of these results , but also demonstrates their crucial role in performance analysis of online strategies for todorov - type mdps .",
    "finally , compared to @xcite , the present paper reports a more thorough and improved empirical evaluation of our proposed strategy in the context of target tracking on a large graph .",
    "in particular , we report the results of monte - carlo simulation of our strategy ( with error bars ) and compare it to two baseline strategies : ( a ) the best stationary policy that could be chosen with full prior knowledge of the state cost sequence and ( b ) the best stationary policy chosen from a large pool of randomly sampled policies _ without _ advance knowledge of state costs . in @xcite , we only compared our strategy to the passive dynamics @xmath16 .",
    "the experimental results reported here show that ( a ) the regret of our strategy w.r.t .",
    "the best stationary policy chosen in hindsight is nonnegative and grows sublinearly with time ( thus validating our theoretical bound ) , and ( b ) in simulations , our strategy performs strictly better than any randomly sampled stationary policy .",
    "the remainder of the paper is organized as follows .",
    "we close this section with a brief summary of frequently used notation .",
    "section  [ sec : problem ] contains precise formulation of the online mdp problem and presents our main result , theorem  [ thm : main_r ] . in preparation for the proof of the theorem",
    ", section  [ sec : prelims ] contains preliminaries on mdps with kl control cost @xcite , including a number of new results pertaining to optimal policies .",
    "section  [ sec : strategy ] then describes our proposed strategy , whose performance is then analyzed in section  [ sec : proof ] in order to prove theorem  [ thm : main_r ] .",
    "some simulation results are presented in section  [ sec : simulations ] .",
    "we close by summarizing our contributions and outlining some directions for future work .",
    "proofs of all intermediate results are relegated to the appendix .",
    "we will denote the underlying finite state space by @xmath1 .",
    "a matrix @xmath40_{x , y \\in \\sx}$ ] with nonnegative entries , and with the rows and the columns indexed by the elements of @xmath1 , is called _ stochastic _ ( or _ markov _ ) if its rows sum to one : @xmath41 .",
    "we will denote the set of all such stochastic matrices by @xmath42 , the set of all probability distributions over @xmath1 by @xmath3 , the set of all functions @xmath43 by @xmath44 , and the cone of all nonnegative functions @xmath45 by @xmath46 .",
    "we will represent the elements of @xmath3 by row vectors and denote them by @xmath47 , etc .",
    ", and the elements of @xmath44 by column vectors and denote them by @xmath48 , etc .",
    "the total variation ( or @xmath49 ) distance between @xmath50 is @xmath51 the _ kullback  leibler divergence _",
    "( or _ relative entropy _ ) @xcite between @xmath52 and @xmath53 is @xmath54 where @xmath55 is the _ support _ of @xmath52 . here and in the sequel",
    ", we work with natural logarithms .",
    "the _ span seminorm _ ( also called the _ oscillation _ ) of @xmath56 is defined as @xmath57 note that @xmath58 if and only if @xmath59 for some constant @xmath60 and all @xmath6 ; @xmath61 for any @xmath56 and @xmath60 .",
    "we also define the _ sup norm _ @xmath62 and note that @xmath63 .    any markov matrix @xmath64 acts on probability distributions from the right and on functions from the left : @xmath65 we say that @xmath34 is _ unichain _ @xcite if the corresponding markov chain has a single recurrent class of states ( plus a possibly empty transient class ) .",
    "the is equivalent to @xmath34 having a unique invariant distribution @xmath66 ( i.e. @xmath67 ) @xcite .",
    "we will denote the set of all such markov matrices over @xmath1 by @xmath68 .",
    "given @xmath69 $ ] , we say that @xmath34 is @xmath70-_contractive _ if @xmath71 ( in fact , every @xmath64 is 1-contractive ) .",
    "we will denote the set of @xmath70-contractive markov matrices by @xmath72 .",
    "it is easy to show that , for every @xmath73 , @xmath74 .",
    "the _ dobrushin ergodicity coefficient _ @xcite of @xmath75 is given by @xmath76 and it can be shown that any @xmath64 is @xmath77-contractive @xcite .",
    "finally , for any @xmath78 we define the supremum distance @xmath79",
    "given the finite state space @xmath1 , let @xmath80 be a fixed subset of @xmath46 , and let @xmath81 be a fixed initial state .",
    "consider an agent ( a ) performing a controlled random walk on @xmath1 in response to a dynamic environment ( e ) . the interaction between a and e proceeds as follows :    [ cols= \" < \" , ]     since we use the same policy throughout each phase , the evolution of the state induced by the above algorithm is described by the following inhomogeneous markov chain : @xmath82 the implementation of this strategy reduces to solving a finite - dimensional frobenius ",
    "perron eigenvalue ( fpe ) problem @xcite at the beginning of each phase to obtain a todorov - type relative value function .",
    "the corresponding twisted kernel then determines the stationary policy to be followed throughout that phase .",
    "an efficient method for solving fpe problems was recently developed by chanchana @xcite .",
    "this method makes use of the well - known collatz formula for the fpe @xcite and elsner s inverse iteration algorithm for computing the spectral radius of a nonnegative irreducible matrix @xcite .",
    "it is an iterative algorithm , which at each iteration performs an lu factorization of an @xmath83 matrix .",
    "the time complexity of each iteration is @xmath84 .",
    "chanchana s algorithm outperforms the three best known algorithms for solving fpe problems , which all rely on elsner s inverse iteration and have quadratic convergence .",
    "numerical experimental results can be found in ( * ? ? ?",
    "* section  3.5 ) .",
    "following the general outline in @xcite , the proof of theorem  [ thm : main_r ] can be divided into four major steps .",
    "the first step is to show that there is no loss of generality in considering a different notion of regret , i.e. , the _ steady - state regret _ , which is the difference between the cumulative cost of the proposed strategy and the steady - state cost of a fixed stationary policy .",
    "the second step is to bound the difference between the expected total cost of our strategy and the sum of expected steady - state costs within each phase .",
    "that is , for each @xmath85 , the steady - state expectation of the cost incurred in phase @xmath85 is taken w.r.t .  the unique invariant distribution of @xmath86 .",
    "after this step , we may only concentrate on expectations over invariant state distributions , which renders the problem much easier . for the third step ,",
    "we show that the sum of steady - state expected costs is not much worse than what we would get if , at the start of each phase @xmath85 , we also knew all the state cost functions to be revealed during phase @xmath85 , i.e. , if we used the `` clairvoyant '' policy @xmath87 in phase @xmath85 . in the fourth step ,",
    "we consider the sum of expected costs in each phase that could be attained if we knew all the state cost functions in advance and used the optimal policy w.r.t .",
    "the average of all the state cost functions throughout all the phases .",
    "we show that this expected cost is actually greater than the sum of expected costs of each phase when we only know the state cost functions one phase ahead .",
    "we then assemble the bounds obtained in these four steps to obtain the final bound on the regret of our strategy .      before proceeding to the proof of theorem  [ thm : main_r ] , we present two lemmas that will be used throughout .",
    "the proofs of the lemmas rely heavily on the results of section  [ ssec : todorov_properties ] , and are detailed in appendices  [ app : proof_uniform_bounds3 ] and [ app : proof_policy_cont ] .",
    "[ lem : uniform_bounds3 ] there exists constants @xmath88 , @xmath89 and @xmath90 , such that , for every @xmath91 and every @xmath92 , @xmath93 moreover , the bound @xmath94 holds for all @xmath95 , such that @xmath96 for all @xmath6 .",
    "[ lem : policy_cont ] there exists a constant @xmath97 , such that , for every @xmath92 , @xmath98 and @xmath99 where @xmath100 is the unique invariant distribution of @xmath86",
    ". moreover , there exists a constant @xmath101 , such that for @xmath102 , we have @xmath103    _ as will be evident from the proof below , we can specify the precise form of the regret bound in using the constants from the above lemmas : @xmath104 _      we are now ready to present the detailed proof of theorem  [ thm : main_r ] .",
    "* step 1 : reduction to the steady - state case . * for any @xmath95 , let us define the _ steady - state regret _ of our strategy @xmath105 w.r.t .",
    "@xmath34 by @xmath106,\\end{aligned}\\ ] ] which is the difference between the actual cumulative cost of @xmath105 and the steady - state cost of the stationary unichain policy @xmath34 initialized with @xmath66 .",
    "now let us fix some @xmath107 and consider an arbitrary @xmath108 , where without loss of generality we can assume @xmath96 for all @xmath6 . for each @xmath109 ,",
    "let @xmath110 be the distribution of @xmath111 in the markov chain induced by the transition matrix @xmath34 and initial state @xmath112 .",
    "for any @xmath113 , we have @xmath114 - \\e_{\\pi_p } \\left [ \\sum_{t=1}^{t } c_t(x , p ) \\right ] \\bigg \\rvert \\nonumber \\\\ & = \\bigg \\lvert \\displaystyle \\sum_{t=1}^{t }   \\lbrace \\e_{\\nu_t } [ c_t(x_t , p ) ] - \\e_{\\pi_p } [ c_t(x , p ) ] \\rbrace \\bigg \\rvert   \\nonumber \\\\ & \\le \\displaystyle \\sum_{t=1}^{t } \\| c_t(\\cdot , p ) \\|_\\infty \\| \\nu_t - \\pi_p \\|_1 \\nonumber \\\\ & \\le 2 k_0 \\displaystyle \\sum_{t=1}^{t } \\rho^{t-1 } \\le \\frac{2k_0}{1 - \\rho }   \\label{eq : step_1},\\end{aligned}\\ ] ] where the second inequality is by lemma  [ lem : uniform_bounds3 ] and the fact that @xmath108 .",
    "therefore , it suffices to show that the bound in holds with @xmath115 in place of @xmath116 .",
    "* step 2 : steady - state approximation within phases . * in this step , we approximate the cumulative cost within each phase by its steady - state value .",
    "let @xmath117 denote the number of complete phases up to time @xmath113 , i.e. @xmath118 ( simple algebra gives @xmath119 ) .",
    "then we can decompose the total cost as @xmath120 where the inequality is by lemma  [ lem : uniform_bounds3 ] .",
    "since all state costs are nonnegative by hypothesis , @xmath121 \\ge \\e_{\\pi_p } \\left [ \\sum_{t=1}^{\\tau_{1:m } } c_t(x , p )   \\right],\\end{aligned}\\ ] ] which implies that @xmath122 for every time step @xmath38 , let @xmath123 be the state distribution induced by our strategy when starting from initial state distribution @xmath124 .",
    "note that the transition matrix at time @xmath38 is @xmath125 if @xmath126 .",
    "we can decompose the expected cost in the first @xmath117 phases as @xmath127,\\end{aligned}\\ ] ] and for every @xmath126 we have @xmath128   \\\\      & \\le \\e_{\\pi^{(m ) } } \\left[c_t(x , p^{(m)})\\right ] + \\| c_t(\\cdot , p^{(m ) } ) \\|_\\infty \\| \\mu_t - \\pi^{(m ) } \\|_1 \\\\      & \\le \\e_{\\pi^{(m ) } } \\left[c_t(x , p^{(m)})\\right ] + k_0 \\| \\mu_t - \\pi^{(m ) } \\|_1,\\end{aligned}\\ ] ] where the last step is by lemma  [ lem : uniform_bounds3 ] .",
    "in addition , for every @xmath129 , we have @xmath130 where the first inequality is due to lemma  [ lem : uniform_bounds3 ] .",
    "hence , @xmath131 \\\\      & \\le \\sum_{t \\in { \\mathcal t}_m } \\e_{\\pi^{(m ) } } \\left[c_t(x , p^{(m)})\\right ] + 2k_0 \\displaystyle \\sum_{k=0}^{\\tau_m -1 } \\alpha^k \\\\      & \\le \\sum_{t \\in { \\mathcal t}_m } \\e_{\\pi^{(m ) } } \\left[c_t(x , p^{(m)})\\right ] + \\frac { 2k_0}{1-\\alpha}.\\end{aligned}\\ ] ] substituting this into , we have @xmath132 + \\frac { 2k_0 m}{1-\\alpha}.\\end{aligned}\\ ] ] * step 3 : looking one phase ahead . * in this step , we show that the steady - state cost in each phase is not much worse than what we could get if we knew everything one phase ahead . for every @xmath133 , we have @xmath134 \\\\      & \\le \\sum_{t \\in { \\mathcal t}_m } \\e_{\\pi^{(m+1 ) } } \\left[c_t(x , p^{(m)})\\right ] + k_0 \\tau_m \\| \\pi^{(m+1 ) } - \\pi^{(m ) } \\|_1   \\\\      & \\le \\tau_m \\e_{\\pi^{(m+1 ) } } \\left[\\wh f^{(m ) } + d^{(m)}\\right ] + \\frac{k_0 k_2 \\tau_{m}^2}{(1-\\alpha ) \\tau_{1:m } } \\\\      & = \\tau_m \\bar j_{\\wh f^{(m)}}(p^{(m+1 ) } ) + \\tau_m \\e_{\\pi^{(m+1 ) } } \\left[d^{(m ) } - d^{(m+1)}\\right ] \\\\      &",
    "\\qquad + \\frac{k_0 k_2 \\tau_{m}^2}{(1-\\alpha ) \\tau_{1:m } } \\\\      & \\le \\tau_m \\bar j_{\\wh f^{(m)}}(p^{(m+1 ) } ) + \\left(\\frac{k_0 k_2}{1 - \\alpha } + k_3\\right ) \\frac{\\tau_{m}^2}{\\tau_{1:m } } , \\end{aligned}\\ ] ] where the first inequality is by lemma  [ lem : uniform_bounds3 ] , the second inequality is by lemma  [ lem : policy_cont ] , and the last inequality is due to in lemma  [ lem : policy_cont ] .",
    "so we now have @xmath135 * step 4 : looking @xmath117 phases ahead .",
    "* in this step , we consider the fictitious situation where we know everything @xmath117 phases ahead , and show that the resulting steady - state value is actually greater than what we could get if we knew everything just one phase ahead .",
    "in other words , we claim that @xmath136 to see that this claim is true , we apply backward induction : @xmath137 where the second equality is due to the fact that @xmath138 , while the inequality is by proposition  [ pps : jproof ] and the fact that @xmath139 , where @xmath140 is the relative value function for state cost @xmath141 . repeating this argument , we obtain .",
    "moreover , @xmath142 .",
    "\\label{eq : comparator}\\end{aligned}\\ ] ]    after these four steps , we are finally in a position to bound the expected steady - state regret . combining  , we can write @xmath143 \\\\      & \\qquad + \\sum_{m=1}^{m}\\left(\\frac{k_0 k_2}{1 - \\alpha } + k_3\\right ) \\frac{\\tau_{m}^2}{\\tau_{1:m } } + \\frac { 2k_0 m}{1-\\alpha}.\\end{aligned}\\ ] ] therefore , @xmath144   \\nonumber \\\\      & \\le \\e c_{\\tau_{1:m } } - \\inf_{p \\in \\cm_1(\\sx ) } \\e_{\\pi_p } \\left [   \\sum_{t=1}^{\\tau_{1:m } } c_t(x , p )   \\right ]   \\nonumber \\\\      & \\le   \\sum_{m=1}^{m}\\left(\\frac{k_0 k_2}{1 - \\alpha } + k_3\\right ) \\frac{\\tau_{m}^2}{\\tau_{1:m } } + \\frac { 2k_0 m}{1-\\alpha}. \\label{eq : phasemregret}\\end{aligned}\\ ] ] next we show that the right - hand side of can be bounded by a quantity that is sublinear in @xmath113 . from",
    ", we have @xmath145 due to our construction of the phases , @xmath146 and @xmath147 if @xmath148 .",
    "moreover , it is a matter of routine but tedious algebraic calculations to show that the choice @xmath149 for @xmath150 for any @xmath151 is sufficient to guarantee that @xmath152 .",
    "thus , we obtain @xmath153 therefore , recalling , we finally obtain @xmath154 which completes the proof of theorem  [ thm : main_r ] .",
    "in this section , we demonstrate the performance of our proposed strategy on a simulated problem involving online ( real - time ) tracking of a moving target on a large , connected , undirected graph @xmath155 , which models a terrain with obstacles .",
    "the state space is the set of all vertices ( nodes ) of @xmath155 .",
    "the target is executing a stationary random walk on @xmath155 with a randomly sampled transition probability matrix , which is different from the one that governs the passive dynamics @xmath16 .",
    "the motion of both the tracking agent and the target must conform to the topology of @xmath155 , in the sense that both can only move between neighboring vertices .",
    "the graph used in our simulation has @xmath156 vertices .    to make sure that assumptions  [ as : irred ] and [ as : dobrushin ] are satisfied , we construct the passive dynamics in the form @xmath157 for some @xmath158",
    "here , @xmath159 is a random walk that represents environmental constraints , allowing the agent to go from a given node either to any adjacent node ( with equal probability ) or to remain at the current location . to ensure that the agent is sufficiently mobile ,",
    "the probability of not moving is chosen to be relatively small ( in our case , @xmath160 ) compared to the probability of transitioning to any of the neighboring nodes . since the underlying graph is connected , the random walk @xmath159 is irreducible ; it is also aperiodic since @xmath161 for all vertices @xmath17 .",
    "we also add a perturbation random walk @xmath162 , which has a fixed column of ones ( we can think of the node indexing that column as a `` home base '' for the agent ) , and zeros elsewhere . the `` size '' of the perturbation is controlled by @xmath163 , which is set to be small ( we have chosen @xmath164 ) , so the agent only has a slight chance of returning to `` home base '' from any given node within one step .",
    "this perturbation ensures that no two rows of @xmath16 are orthogonal , and @xmath165 .",
    "the simulation consists of a number of independent experiments .",
    "each individual experiment runs for @xmath166 time steps .",
    "we first randomly sample a transition matrix for the target motion . after simulating the target s random walk for @xmath113 steps , we record the target locations and use them to generate a sequence of state cost functions @xmath167 .",
    "then we feed these @xmath168 state cost functions sequentially to our online algorithm and compute the resulting cumulative cost @xmath169 . at each time",
    "@xmath38 , the tracking agent is in state ( location ) @xmath170 , the target is at location @xmath171 , and the agent s action is @xmath172 .",
    "the cumulative cost after @xmath113 time steps is @xmath173 , \\end{aligned}\\ ] ] with state costs @xmath174 , where @xmath175 is the graph distance ( number of edges in the shortest path ) between the agent s current location and the location of the target , normalized by the diameter of @xmath155 .",
    "then we compute the best stationary policy @xmath34 in hindsight for the average of all the state costs by solving the mpe @xmath176 for the relative value function @xmath177 , where @xmath178 , and then setting @xmath179 the regret is then computed with respect to the steady - state cost of this best stationary policy : @xmath180,\\end{aligned}\\ ] ] where @xmath181 and @xmath66 is the unique invariant distribution of @xmath34 .    to plot the regret versus time with error bars , we implement the experiment @xmath182 times and compute the empirical average of the regret across experiments . for each realization",
    "the agent was initialized with the same starting state .",
    "the evolution of the regret versus time is shown in figure  [ fig : subfig1 ] , where the regret at time @xmath38 is defined as the total cost of our strategy up to time @xmath38 minus the total cost of the best stationary policy up to time @xmath38 .",
    "we can see that the regret is growing sublinearly , as stated in theorem  [ thm : main_r ] .",
    "we also compare the total cost of our strategy to that of the best stationary baseline policy among a set @xmath183 of @xmath184 randomly sampled stationary policies . once again",
    ", each experiment runs for @xmath166 time steps .",
    "the baseline policy @xmath185 is the one that has the smallest total cost @xmath186.\\end{aligned}\\ ] ] among the @xmath184 randomly sampled policies .",
    "the regret of our adaptive strategy is thus given by @xmath187 .",
    "as before , there are @xmath182 independent experiments , where in each experiment the agent using our strategy and the agent using the best sampled stationary policy were initialized with the same starting state .",
    "the evolution of the regret versus time is shown in figure  [ fig : subfig ] , where the regret at time @xmath38 is defined as the total cost of our strategy up to time @xmath38 minus the total cost of the best sampled stationary policy up to time @xmath38 .",
    "we can see that the regret is negative , which implies that our strategy outperforms the best sampled stationary policy for each particular realization of the state cost sequence .",
    "the problem studied in this paper combines aspects of both stochastic control and online learning . in particular , our construction of a hannan - consistent strategy ( a concept from the theory of online learning @xcite ) uses several ideas and techniques from the theory of mdps with average cost criterion , including some new results concerning optimal policies for mdps with kl control costs @xcite .",
    "we have proved that , for any horizon @xmath113 , our strategy achieves sublinear @xmath188 regret relative to any uniformly ergodic class of stationary policies , which is similar to the results of yu et al .",
    "@xcite for online mdps with finite state and action spaces .",
    "however , while our strategy ( like that of @xcite ) is computationally efficient , we believe that the @xmath188 scaling of regret with @xmath113 is suboptimal . indeed , in the case when both the state and the action spaces are finite , even - dar et al .",
    "@xcite present a strategy that achieves a much better @xmath189 regret .",
    "of course , the strategy of @xcite involves recomputing the policy at _ every _ time step ( rather than in phases , as is done here and in @xcite ) , which results in a significant loss of efficiency . an interesting open question , which we plan to address in our future work , is whether it is possible to attain @xmath189 regret for online mdps with kl control costs . a related challenge is to study these online mdps in the ( nonstochastic ) bandit setting , where at each time step the agent only learns the value @xmath190 of the state cost at time @xmath38 at the current state @xmath111 , rather than the full state cost function @xmath191 .",
    "while this bandit setting is more realistic , very little is known about it even for online mdps with finite state and action spaces  neu et al .",
    "@xcite constructed a strategy that achieves @xmath192 regret , but it is not known whether this is optimal .",
    "another promising avenue for further research has to do with the apparent duality between our set - up and the theory of _ risk - sensitive control _ of markov processes @xcite .",
    "indeed , the acoe can be viewed as a special case of the isaacs equation for a certain dynamic two - player game with average cost criterion , in which player 1 generates state cost functions , while player 2 generates distributions over the state space ( cf .",
    ", e.g. , @xcite ) . in the set - up of our section  [ sec : problem ] , player 1 would correspond to the environment e , while player 2 would be the agent a. we plan to explore this connection further .    finally , as mentioned in the introduction",
    ", we would like to extend our results to more general ( e.g. , compact ) state spaces .",
    "this will require more sophisticated machinery , e.g. , foster ",
    "lyapunov criteria and ergodicity w.r.t .",
    "weighted norms @xcite , as well as spectral theory of the mpe for markov chains with general state spaces @xcite .",
    "consider the matrix @xmath193 with entries @xmath194 .",
    "for any @xmath195 and any @xmath196 , @xmath197 . since @xmath16 is irreducible ( assumption  [ as : irred ] ) , for any pair @xmath196 of states there exists some @xmath195 , such that @xmath198 .",
    "but then @xmath199 as well , which means that @xmath200 is also irreducible . therefore , by the frobenius",
    " perron theorem @xcite , @xmath200 has a strictly positive right eigenvector @xmath201 with a positive eigenvalue @xmath202 ( the frobenius - perron eigenvalue ) : @xmath203 .",
    "thus , @xmath204 . moreover , the fp eigenvalue is simple , and @xmath200 has no nonnegative right eigenvectors other than the positive multiples of @xmath201 @xcite .",
    "this proves the existence and uniqueness part .",
    "now , using the fact that @xmath205 solves the mpe , we can show that @xmath206 whence it follows that @xmath207 as was just proved , @xmath200 is irreducible , and @xmath201 is strictly positive .",
    "hence , for any pair @xmath208 there exists some @xmath195 , such that @xmath209 as well .",
    "this proves the irreducibility of @xmath210 .",
    "now , since @xmath200 is irreducible , the frobenius ",
    "perron theorem says that there exists a unique strictly positive @xmath211 , such that @xmath212 @xcite .",
    "now define @xmath213 through @xmath214 a straightforward calculation shows that @xmath215 is an invariant distribution of @xmath210 .",
    "the uniqueness of @xmath215 follows from the irreducibility of @xmath210 .",
    "we essentially follow the proof of theorem  3.2 in @xcite , with some simplifications . for each @xmath216 , define the function @xmath217 via @xmath218,\\ ] ] where @xmath219 $ ] denotes the expectation w.r.t .  the markov chain @xmath220 with initial state @xmath221 and transition matrix @xmath16 .",
    "then a simple inductive argument shows that @xmath222 indeed , for each @xmath38 let @xmath223 .",
    "then , since @xmath224 by , we can write @xmath225 \\label{eq : w_ind_1 } \\\\          & = e^{-t\\lambda_f } \\e_x \\left[\\psi_t \\e[v_f(x_{t+1})|x_t]\\right ] \\label{eq : w_ind_2}\\\\          & = e^{-t\\lambda_f } \\e_x \\left[\\psi_t \\pd v_f(x_t)\\right ] \\label{eq : w_ind_3}\\\\          & = e^{-t\\lambda_f } \\e_x \\left [ \\psi_{t-1 } v_f(x_t)\\right ] , \\label{eq : w_ind_4 }      \\end{aligned}\\ ] ] where follows from definitions , and use the markov property , and again follows from definitions .",
    "proceeding backwards , we get @xmath226 & = \\e_x[\\psi_1 v_f(x_2 ) ] = \\e_x [ \\psi_1 \\pd v_f(x_1 ) ] \\\\      & = v_f(x ) = e^{-h_f(x)}.\\end{aligned}\\ ] ] substituting this into , we get , which in turn implies that @xmath227 for all @xmath228 . since @xmath229",
    ", we can write @xmath230 . let @xmath53 ( respectively , @xmath231 ) be the distribution of @xmath232 in the markov chain with transition matrix @xmath16 and initial state @xmath221 ( respectively , @xmath233 ) .",
    "then @xmath234 for every @xmath20 .",
    "consequently , for any @xmath235 we have @xmath236 \\label{eq : w_diff_1}\\\\      & \\ge e^{-\\bar{n } \\| f \\|_\\infty } \\e_x \\left [ e^{-\\sum^t_{t=\\bar{n}+1 } f(x_t)-h_f(x_{t+1})}\\right ] \\label{eq : w_diff_2 } \\\\      & = e^{-\\bar{n } \\| f",
    "\\|_\\infty } \\e_{x^\\circ}\\left [   e^{-\\sum^t_{t=\\bar{n}+1}f(x_t)-h_f(x_{t+1 } ) } \\frac{\\nu(x_{\\bar{n}+1})}{\\nu^\\circ(x_{\\bar{n}+1})}\\right ] \\label{eq : w_diff_3}\\\\      & \\ge \\theta e^{-\\bar{n } \\| f \\|_\\infty } \\e_{x^\\circ } \\left [   e^{-\\sum^t_{t=1}f(x_t)-h_f(x_{t+1 } ) } \\right ] \\label{eq : w_diff_4}\\\\      & = \\theta e^{-\\bar{n } \\| f \\|_\\infty } e^{-w_t(x^\\circ ) } , \\label{eq : w_diff_5}\\end{aligned}\\ ] ] where is by definition , follows from the markov property and a change of measure , follows from and from the fact that @xmath237 , and is again by definition",
    ". taking logarithms , we get @xmath238 .",
    "interchanging the roles of @xmath17 and @xmath239 , we get @xmath240 .",
    "this proves ; follows immediately .",
    "the basic idea is as follows . for",
    "a given @xmath241 , let us introduce the dynamic programming operator @xmath242 that maps any @xmath243 to @xmath244 , where @xmath245 @xmath246 then we can express the acoe as @xmath247 .",
    "hence , for any @xmath248 , @xmath249 where uses the fact that the span seminorm is unchanged after adding a constant , and is by the triangle inequality .",
    "we will then show the following :    1 .",
    "for any @xmath243 and any @xmath248 , @xmath250 2 .   for a fixed @xmath241 , the dynamic programming operator",
    "@xmath251 is a contraction in the span seminorm : for every @xmath252 , there exists a constant @xmath253 , such that for any two @xmath254 with @xmath255 we have @xmath256    assuming that items 1 ) and 2 ) above are proved , we proceed as follows .",
    "first of all , the first term in is bounded by @xmath257 by .",
    "next , since @xmath258 , proposition  [ pps : h_bounded ] guarantees that there exists some @xmath259 , such that @xmath260 .",
    "therefore , there exists a constant @xmath261 , such that the second term in is bounded by @xmath262 .",
    "therefore , @xmath263 , which gives with @xmath264 .",
    "we now prove 1 ) and 2 ) . for any function @xmath243 and any two @xmath248",
    ", we have @xmath265 \\\\ & \\qquad \\qquad - \\left[g(x ) + \\inf_{\\mu \\in \\cp(\\sx)}\\left\\{\\e_\\mu \\varphi + d(\\mu\\|p^*(x,\\cdot ) ) \\right\\ } \\right]\\bigg\\ } \\\\ & = \\max_{x \\in \\sx } \\left [ f(x ) - g(x)\\right].\\end{aligned}\\ ] ] similarly , we get @xmath266 $ ] . thus , @xmath267 ,",
    "so we have proved .",
    "to establish , we follow the proof of proposition  2.2 in @xcite with some simplifications .",
    "pick any @xmath268 and let @xmath269 where explicitly @xmath270 and @xmath271 .",
    "then @xmath272 -     \\left [ \\dpop_f \\varphi(x ' ) - \\dpop_f \\varphi'(x')\\right ] \\\\ & = \\inf_{\\mu \\in \\cp(\\sx ) } \\left\\{\\e_\\mu \\varphi + d(\\mu \\| p^*(x,\\cdot ) ) \\right\\ } \\\\ & \\qquad - \\inf_{\\mu \\in \\cp(\\sx ) } \\left\\ { \\e_\\mu \\varphi ' + d(\\mu \\| p^*(x,\\cdot))\\right\\ } \\\\ & \\qquad - \\inf_{\\mu \\in \\cp(\\sx ) } \\left\\ { \\e_\\mu \\varphi + d(\\mu \\| p^*(x',\\cdot ) ) \\right\\ } \\\\ & \\qquad + \\inf_{\\mu \\in \\cp(\\sx ) } \\left\\ { \\e_\\mu\\varphi ' + d(\\mu \\| p^*(x',\\cdot))\\right\\ } \\\\ & \\le \\e_\\nu \\varphi + d(\\nu \\| p^*(x,\\cdot ) ) - \\e_\\nu \\varphi ' - d(\\nu \\| p^*(x,\\cdot ) ) \\\\ & \\qquad -\\e_{\\nu'}\\varphi - d(\\nu ' \\| p^*(x',\\cdot ) ) + \\e_{\\nu'}\\varphi ' + d(\\nu ' \\|   p^*(x',\\cdot ) ) \\\\ & = \\int ( \\varphi - \\varphi ' ) \\mathrm{d}(\\nu-\\nu').\\end{aligned}\\ ] ] a standard argument shows that that @xmath273 .",
    "consequently , @xmath274 then the proof of will be complete if we can show that @xmath275 suppose that does not hold .",
    "then there exist sequences @xmath276 , @xmath277 of functions with @xmath278 , a set @xmath279 , and a pair of points @xmath268 , such that @xmath280 = 1,\\ ] ] where for any @xmath64 we denote @xmath281 .",
    "this implies in turn that @xmath282 since @xmath283 , implies that @xmath284 .",
    "but this means that @xmath285 , which contradicts assumption  [ as : dobrushin ] .",
    "hence , holds .",
    "we begin with . from definitions",
    ", we have @xmath286 + \\log \\frac{\\lambda_{\\varphi'}(x)}{\\lambda_{\\varphi}(x)}\\label{eq : pps1_ineq}.      \\end{aligned}\\ ] ] a simple change - of - measure calculation shows that @xmath287 .",
    "\\label{eq : lambda_ratio }      \\end{aligned}\\ ] ] to bound the right - hand side of , we recall the well - known _ hoeffding bound _",
    "@xcite , which for our purposes can be stated as follows : for any @xmath211 and any @xmath288 , @xmath289 applying this bound gives @xmath290 + \\frac{\\| \\varphi - \\varphi ' \\|_s^2 } { 8}.\\end{aligned}\\ ] ] substituting this bound into , we see that the terms involving the expectation of the difference @xmath291 cancel , and we are left with . to prove , we use pinsker s inequality , @xmath292 @xcite . to prove , we follow essentially the same strategy as in the proof of proposition  [ pps : h_cont ] to show that @xmath293 for every @xmath294 .",
    "fix some @xmath95 .",
    "if there exists some @xmath6 such that @xmath295 and @xmath296 , then proposition  [ pps : jproof ] holds trivially .",
    "thus , there is no loss of generality if we assume that @xmath297 .",
    "then @xmath298 \\nonumber\\\\      & = \\sum_{x \\in \\sx } \\pi_p(x ) \\bigg [ f(x ) + \\sum_{y \\in \\sx } p(x , y ) \\log \\frac{p(x , y)}{\\twp_{h_f}(x , y ) } \\nonumber \\\\      & \\qquad + \\sum_{y \\in \\sx } p(x , y ) \\log \\frac{\\twp_{h_f}(x , y)}{\\pd(x , y ) }   \\bigg ] \\nonumber \\\\      & = \\sum_{x \\in \\sx } \\pi_p(x ) \\bigg [ f(x ) + \\e_{\\pi_p } d(p(x,\\cdot ) \\| \\twp_{h_f}(x , \\cdot ) ) \\nonumber \\\\      & \\qquad + \\sum_{y \\in \\sx } p(x , y ) \\log \\frac{e^{-h_f(y)}}{\\lambda_{h_f}(x ) }   \\bigg ] \\nonumber \\\\      & \\ge \\sum_{x \\in \\sx } \\pi_p(x ) \\bigg [ f(x ) + \\sum_{y \\in \\sx } p(x , y ) \\log \\frac{e^{-h_f(y)}}{\\lambda_{h_f}(x ) }   \\bigg ] \\nonumber \\\\      & = \\e_{\\pi_p } [ f(x ) -ph_f(x ) - \\log \\lambda_{h_f}(x ) ]   \\nonumber \\\\      & = \\e_{\\pi_p } [ f(x ) -h_f(x ) - \\log \\lambda_{h_f}(x ) ] , \\end{aligned}\\ ] ] where the inequality is due to the fact that the kl divergence is always nonnegative , and the last step is due to the fact that @xmath66 is the invariant distribution of @xmath34 . by the acoe ,",
    "we know that @xmath299 for every @xmath6 .",
    "so we have @xmath300 .",
    "note that if we take the expectation @xmath301 $ ] of both sides of the acoe , we get @xmath302   \\\\      & = \\e_{\\twpi_f } [ f(x ) + d(\\twp_{h_f}(x , \\cdot ) \\| \\pd(x , \\cdot ) ) + \\twp_{h_f } h_f(x ) ]   \\\\      & = \\e_{\\twpi_f}[f(x ) + d(\\twp_{h_f}(x , \\cdot ) \\| \\pd(x , \\cdot ) ) ] + \\e_{\\twpi_f } [ h_f(x ) ] , \\end{aligned}\\ ] ] where the last equality is due to the fact that @xmath215 is the invariant distribution of @xmath210 .",
    "therefore , @xmath303 =   \\bar j_f(\\twp_{h_f } ) = \\lambda_f$ ] .",
    "so we now have @xmath304 for any @xmath95 , which completes the proof of proposition  [ pps : jproof ] .      for every state @xmath6 ,",
    "let @xmath305 denote the set of states that can be reached from @xmath17 in one step by the passive dynamics @xmath16 , i.e. , @xmath306 . let us also define @xmath307 and @xmath308 . since @xmath309 , and @xmath310 is bounded by proposition  [ pps : h_bounded ] , we have @xmath311 .",
    "therefore , @xmath312 and for any @xmath91 @xmath313 thus , the first bound of lemma  [ lem : uniform_bounds3 ] holds with @xmath314 .",
    "the same argument works for any @xmath95 that satisfies @xmath315 .",
    "the second bound holds by proposition  [ pps : h_bounded ] , where @xmath316 .",
    "the third bound follows from the second bound , @xmath317 , and by proposition  [ pps : twisted_kernels ] with @xmath318 .",
    "let us recall that each @xmath86 is given by the twisted kernel @xmath319 , where the relative value function @xmath310 arises from the solution of the mpe with state cost @xmath320 .",
    "then @xmath321 where the first step is by proposition  [ pps : twisted_kernels ] , the second by proposition  [ pps : h_cont ] with @xmath322 , and the third by lemma  4.3 in @xcite .",
    "this proves .",
    "moreover , proposition  [ pps : mpe_sol ] guarantees that @xmath309 has a unique invariant distribution @xmath100 .",
    "therefore , @xmath323 where the third inequality follows from . rearranging",
    ", we get .    next ,",
    "from the form of @xmath86 and @xmath87 we have @xmath324 - \\e_x^{(m ) } [ h^{(m ) } ] + \\log \\frac{\\lambda_{h^{(m+1 ) } } ( x)}{\\lambda_{h^{(m ) } } ( x)}\\label{eq : d_diffphase},\\end{aligned}\\ ] ] where @xmath325 $ ] denotes expectation w.r.t .",
    "@xmath326 , and we can follow the same steps we have used in to show that @xmath327.\\end{aligned}\\ ] ] using the hoeffding bound @xcite , we can write @xmath328 + \\frac{\\| h^{(m ) } - h^{(m+1 ) } \\|_s^2 } { 8}\\end{aligned}\\ ] ] substituting into and simplifying , we get @xmath329 - \\e_x^{(m ) } [ h^{(m+1 ) } ] + \\frac{1}{8 } \\| h^{(m ) } - h^{(m+1 ) } \\|_s^2 \\\\ & \\le \\| h^{(m+1 ) } \\|_s \\cdot \\| p^{(m)}(x , \\cdot ) - p^{(m+1)}(x , \\cdot ) \\|_1 \\nonumber \\\\ & \\qquad + \\frac{1}{8 } \\| h^{(m ) } - h^{(m+1 ) } \\|_s^2   \\nonumber \\\\ &",
    "k_2 \\tau_{m}}{\\tau_{1:m}}+ \\frac{1}{8 } \\| h^{(m ) } - h^{(m+1 ) } \\|_s^2   \\nonumber \\\\ & \\le \\frac{k_1 k_2 \\tau_{m}}{\\tau_{1:m}}+ \\frac{k_2 ^ 2 \\tau_{m}^2 } { 2 \\tau_{1:m}^2 }   \\nonumber \\\\ & \\le   \\left(k_1 k_2 + \\frac{k_2 ^ 2}{2}\\right)\\frac{\\tau_{m}}{\\tau_{1:m}}. \\label{eq : d_diffphase3}\\end{aligned}\\ ] ] here , the third step uses the fact that @xmath317 ( lemma  [ lem : policy_cont ] ) and , the fourth also uses , and the last is due to the fact that @xmath330 . letting @xmath331 , we get .",
    "a.  arapostathis , v.  s. borkar , e.  fernndez - gaucherand , m.  k. ghosh , and s.  i. marcus , `` discrete - time controlled markov processes with average cost criterion : a survey , '' _ siam j. control optim .",
    "_ , vol .  31 , no .  2 , pp .",
    "282344 , 1993 .",
    "h.  robbins , `` asymptotically subminimax solutions of compound statistical decision problems , '' in _ proc .",
    "2nd berkeley symposium on mathematical statistics and probability 1950_.1em plus 0.5em minus 0.4em berkeley , ca : university of california press , 1951 , pp .",
    "131148 .",
    "e.  todorov , `` linearly - solvable markov decision problems , '' in _ advances in neural information processing systems 19 _ , b.  schlkopf , j.  platt , and t.  hoffman , eds.1em plus 0.5em minus 0.4emcambridge , ma : mit press , 2007 , pp . 13691376 .",
    "i.  r. petersen , m.  r. james , and p.  dupuis , `` minimax optimal control of stochastic systems with relative entropy constraints , '' _ ieee trans .",
    "45 , no .  3 , pp .",
    "398412 , march 2000 .    c.  d. charalambous and f.  rezaei , `` stochastic uncertain systems subject to relative entropy constraints : induced norms and monotonicity properties of minimax games , '' _ ieee trans .",
    "52 , no .  4 , pp .",
    "647660 , april 2007 .",
    "g.  neu , a.  gyrgy , c.  szepesvri , and a.  antos , `` online markov decision processes under bandit feedback , '' in _ advances in neural information processing systems 23 _ , j.  lafferty , c.  k.  i. williams , j.  shawe - taylor , r.  zemel , and a.  culotta , eds .",
    ", 2010 , pp . 18041812 .",
    "peng guan received the b.e .  and m.sc .",
    "degrees in department of automation from tsinghua university , beijing , china , in 2006 and 2009 , respectively .",
    "he is currently working towards the ph.d .",
    "degree in electrical and computer engineering at duke university .",
    "his research interests include stochastic control and online learning .",
    "maxim raginsky received the b.s .  and m.s .",
    "degrees in 2000 and the ph.d .",
    "degree in 2002 from northwestern university , evanston , il , all in electrical engineering .",
    "he has held research positions with northwestern , the university of illinois at urbana - champaign ( where he was a beckman foundation fellow from 2004 to 2007 ) , and duke university . in 2012 ,",
    "he has returned to uiuc , where he is currently an assistant professor with the department of electrical and computer engineering and the coordinated science laboratory . in 2013 , prof .",
    "raginsky has received a faculty early career development ( career ) award from the national science foundation .",
    "his research interests lie at the intersection of information theory , machine learning , and control .",
    "rebecca willett is an associate professor of electrical and computer engineering at the university of wisconsin - madison .",
    "she completed her phd in electrical and computer engineering at rice university in 2005 and was an assistant then associate professor of electrical and computer engineering at duke university from 2005 to 2013 .",
    "willett received the national science foundation career award in 2007 , is a member of the darpa computer science study group , and received an air force office of scientific research young investigator program award in 2010 .",
    "willett has also held visiting researcher positions at the institute for pure and applied mathematics at ucla in 2004 , the university of wisconsin - madison 2003 - 2005 , the french national institute for research in computer science and control ( inria ) in 2003 , and the applied science research and development laboratory at ge healthcare in 2002 .",
    "her research interests include network and imaging science with applications in medical imaging , neural coding , astronomy , and social networks .",
    "additional information , including publications and software , are available online at http://willett.ece.wisc.edu ."
  ],
  "abstract_text": [
    "<S> this paper considers an online ( real - time ) control problem that involves an agent performing a discrete - time random walk over a finite state space . </S>",
    "<S> the agent s action at each time step is to specify the probability distribution for the next state given the current state . </S>",
    "<S> following the set - up of todorov , the state - action cost at each time step is a sum of a state cost and a control cost given by the kullback - leibler ( kl ) divergence between the agent s next - state distribution and that determined by some fixed passive dynamics . </S>",
    "<S> the online aspect of the problem is due to the fact that the state cost functions are generated by a dynamic environment , and the agent learns the current state cost only after selecting an action . </S>",
    "<S> an explicit construction of a computationally efficient strategy with small regret ( i.e. , expected difference between its actual total cost and the smallest cost attainable using noncausal knowledge of the state costs ) under mild regularity conditions is presented , along with a demonstration of the performance of the proposed strategy on a simulated target tracking problem . </S>",
    "<S> a number of new results on markov decision processes with kl control cost are also obtained . </S>"
  ]
}