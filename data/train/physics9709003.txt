{
  "article_text": [
    "n - body simulations are one of the most important tools by which contemporary theoretical cosmologists try to investigate the evolution of the large scale structure of the universe . due to the long - range nature of the gravitational force ,",
    "the number of particles required to reach a significant mass resolution is a few orders of magnitude larger than those allowed even by present - day state - of - the - art massively parallel supercomputers ( hereafter mpp ) .",
    "work- and data - sharing programming techniques are customary tools exploited in the development of many parallel implementations of the n - body problem @xcite @xcite @xcite .",
    "the most popular algorithms are generally based on grid methods like the @xmath0 .",
    "the main problem with this method lies in the fact that the grid has typically a fixed mesh size , while the cosmological problem is inherently highly irregular , because a highly clustered large - scale structure develops starting from a nearly homogeneous initial distribution . on the other hand",
    "the barnes & hut ( 1986 ) oct - tree recursive method is inherently adaptive , and allows one to achieve a higher mass resolution than grid - based methods when clusters of galaxies form . because of these features , however",
    ", the computational problem can easily run into unbalance causing a performance degradation .",
    "for this reason , we have undertaken a study of the optimal work- and data - sharing distribution for our parallel treecode .",
    "+ our work- and data - sharing parallel tree - code ( hereafter wdsh - ptc ) is based on this algorithm tree scheme , which we have modified to run on a shared - memory mpps @xcite @xcite .",
    "we have adopted the cray research corporation craft environment @xcite to share work and data among the pes involved in the run .",
    "+ to optimize the performances of the wdsh - ptc in order to run simulations with a very high number of particles , we have carried out a study on the optimal data distribution in the t3d global memory @xcite .",
    "the obtained results allow us to determine an optimal strategy for the dynamic load balance ( dlb ) , that is the main purpose of this work .",
    "generally all tests and measurements were carried out using a @xmath1 value equal to 0.8 , considering this value as the optimal value for the simulations that we would like to run .",
    "the barnes - hut tree algorithm is a @xmath2 procedure to compute the gravitational force through a hierarchical subdivision of the computational domain in a set of cubic nested regions .",
    "the evolution of the system involves the advancement of the trajectories of all particles , and this is carried out through a discrete integration of the trajectories of each particle . at each timestep",
    "the force and the trajectory of each body are updated . in practice one",
    "does not adopt the `` nude '' gravitational potential , in order to avoid the formation of binary systems , but a potential smoothed on a sufficiently small scale .",
    "a more detailed discussion on the bh tree method can be found in @xcite . + for our purposes , we can distinguish three main phases in each timestep .",
    "the first is the tree_formation ( hereafter tf ) phase where the tree structure is built starting from the whole computational domain included in a cubic region , that is the `` root - cell '' ( i.e. the zero level ) of the tree . using the orb ( orthogonal recursive bisection ) technique , at the begining the root - cell",
    "is subdivided into 8 cubic nested regions ( subcells ) each including a portion of the computational domain .",
    "this subdivision creates the second level of the tree .",
    "the orb technique is then recursively applied to each new subcell , so that several levels of the tree are formed until all the final cells contain at most one body .",
    "tree cells containing more than one body are called `` internal cells '' ( icells ) , and those containg only one body are called `` terminal cells '' ( fcells ) . for each icell the total mass , the center of mass and the quadrupole moment are computed .",
    "+ the second phase is the force_compute  ( hereafter fc ) , during which the forces acting on each body of the system are computed . in the tree_inspection ( hereafter ti ) subphase , for each body one makes an `` interaction list '' containing pointers to cells with which the particle will interact , formed according to the following criteria . starting from the root - cell the @xmath3 ratio is compared with a threshold parameter @xmath1 ( the opening angle parameter ) , `` c '' being the cell - size and `` d '' the distance between the particle and the cell center of mass .",
    "+ if the @xmath3 ratio is smaller than @xmath1 , the cell is considered as a single region that contributes a force component to the particle , and its subcells are not further investigated .",
    "otherwise one checks all the subcells using the same criterion , until one arrives at cells containing only one particle . at the end , all the tree cells satisfying the criterion form the `` body interaction list '' for the given body .",
    "+ in general , using the value @xmath1 = 1 , the acceleration approximation has an error of about 1% .",
    "typical values of @xmath1 for cosmological simulations are in the range 0.5 - 1.2 : the larger the @xmath1 the smaller the length of the interaction list .",
    "the average length of the `` interaction list '' is proportional to @xmath4 , so that the total computation complexity in a serial code based on the barnes - hut tree code scales as @xmath5 .",
    "after the ti subphase , using the interaction list , the acc_components ( hereafter ac ) subphase is executed and the particle acceleration is computed . +    at the end of the fc phase there is a synchronization point followed by the update_position phase , the last phase of the timestep , when the bodies positions are updated ( fig .",
    "the tests carried out on the original serial n - body code , kindly provided to us by dr .",
    "l. hernquist , confirm the results shown by salmon ( 1990 ) concerning the complexity .",
    "in particular the total time spent in the tf phase ranges from 7% to 10% of the overall time according to the particles initial condition ; i.e. , uniform or clustered distributions .",
    "whereas the ti ranges from 65% to 70% , the ac subphase last for the remaining 20% - 25% of the total timestep .",
    "the wdsh - ptc uses the work - sharing technique in order that all pes cooperate in the tf phase , during which the tree is formed and the cell properties are computed , and the synchronization is implicit @xcite .",
    "+ during the fc phase each pe computes the acceleration components for each body in asynchronous mode and only at the end of the phase an explicit barrier statement is set .",
    "+ the wdsh - ptc parallelism level reached is very high , more than 90% of the work is performed in a parallel region as shown by apprentice , a `` performance analysis tool '' designed for cray mpp systems that allows one to investigate the effective performances reached by a code . with various pool configurations of pes ranging from 16 to 128 , our results show that the most time - consuming phases ( tf , ti and ac ) are executed in a parallel regime . + in the following sections we will discuss the performances obtained using different ways for data distribution in the memory and how a strategy of dynamical load balancing ( hereafter dlb ) can be devised .",
    "several strategies to share data in the t3d pes memory @xcite @xcite were investigated to find the best data distribution , in order to maximize the number of computed particles per second .",
    "we have considered the two main kinds of data , tree data ( cells pointers and cells properties ) and body data , and we have observed the code performances by varying the tree data and the body data distribution .",
    "+ tests were carried out , fixing the constraint that each pe executes the fc phase only for all bodies residing in the local memory .",
    "a bodies data distribution ranging from contiguous blocks ( coarse grain : cdir$ shared pos(:block , : ) ) to a fine grain distribution ( cdir$ shared pos(:block(1 ) , : ) ) was adopted .",
    "we studied different tree data distributions ranging from assigning to contiguous blocks a number of cells equal to the expected number of internal cells ( ntotcell ) , as described in j. salmon ( 1990 ) ( coarse grain : cdir$ shared pos_cell(block(:ntotcell / n$pes ) , : ) ) , to a simple fine grain distribution ( cdir$ shared pos_cell(:block(1 ) , : ) ) . + all the tests were performed for two different set of initial conditions , namely uniform and clustered distribution having @xmath6 particles each .",
    "the tests were carried out using from 16 to 128 pes , and in tab .",
    "1 we report only the most significant results obtained with 128 pes and using coarse grain and fine grain data distribution , although the same trend was obtained using 16 and 64 pes .      the data ( i.e. particles and cells ) distribution greatly affects the overall code performance and an accurate study has to be carried out to obtain high gain from the mpp machines for this kind of code .",
    "one possible approach was adopted by @xcite , and is based on the so - called `` locally essential tree '' ( let ) introduced by @xcite , where each pe builds a `` local tree '' for the particles assigned to it , and the force acting on each particle is then computed after all the pes have builded a let assembling together the pointers to all the cells from other pes which do contribute ( according to the above mentioned @xmath1-criterion ) to the force on all their particles .",
    "the main problem with this approach is that the memory requirements grow very quickly with the number of particles @xmath7 .",
    "we have then preferred to keep a single tree shared among all the pes and to look for the optimal distribution of tree s cells and particles .",
    "+ the tree data distribution greatly affects the overall code performance and the distribution must be thoroughly carried out to obtain high gain from the mpp machines for this kind of code .",
    "our results show that the best tree data distribution is obtained using a block factor equal to 1 , degenerate in the second dimension ( cdir$ shared pos_cell(:block(1 ) , : ) ) .",
    "this is in accordance with what we expected , and in order to understand this point we notice that there are two aspects of the problem to be considered .",
    "+ the first is related to the cell inspection performed during the fc phase . during the tf phase , cells properties are determined level by level , starting from level 0 ( containing only the root cell ) and descending down to deeper levels of depth l ( each l level has @xmath8 cells ) .",
    "the cells are numbered progressively starting from the root cell . considering that all the cells belonging to the first four low levels generally include a large part of the domain , they are inspected by each pe @xmath9 times during each timestep because that is the average number of bodies that each pe has to treat .",
    "a fine grain tree data distribution involves that each pe have the same number of cells ( @xmath10 ) and cells belonging to low levels are distributed over the pes local memory . using this kind of tree data distribution",
    ", we obtain that for each pe the execution time of the fc phase is almost the same , because all the pes spend on average the same amount of time spent in the tree data access .",
    "results in tab .1 show that , a coarse grain tree data distribution increases the duration of the tf phase and the number of particles per second , executed in a timestep , up to a factor of five .",
    "this is due to the network contention to access to `` critical resources '' . in the case of a coarse grain tree data distribution ,",
    "all the cells belonging to the first levels are located in the first pe ( or in the first two pes ) , and all pes access them at the same time , during the fc phase . +",
    "another aspect is related to the highly dynamical evolution of tree s properties .",
    "each timestep produces a different tree , making it very difficult to determine rules for an optimal tree data distribution which can minimize the access time on the t3d toroidal architecture .",
    "because of the overhead due to data redistribution during a run , as well as the fact that the block factor power of two ( imposed by craft in the t3d ) we deem it inconvenient to further examine this point .",
    "so it is possible to conclude that , as shown in tab .",
    "1 , a fine grain tree data distribution should be used for this kind of codes .      bodies are labelled in such a way that close bodies have adjacent numbers , and the properties are shared among the pes using the craft directive cdir$ shared pos(:block(n ) , : ) n ranging between @xmath11 and @xmath9 .",
    "+ the fine grain bodies data distribution ( bf ) is obtained using a block factor @xmath12 ; i.e. , bodies are shared among the pe but there is no spatial relation in the body set residing in the same pe local memory .",
    "the medium grain bodies data distribution ( bm ) is obtained using a block factor @xmath13 . using this kind of distribution",
    "each pe has two data block of bodies properties residing in the local memory , each block having a close bodies set . at the end",
    "the coarse grain bodies data distribution ( bc ) is obtained using a block factor @xmath14 ; i.e. , each pe has one close data set block of bodies residing in the local memory . in any case , each pe executes the fc phase only for those bodies residing in the local memory .",
    "the results reported in tab . 1 show that the best bodies data distribution , having the highest code performance in terms of particles per second , is obtained using the block factor n = nbodies / n$pes as expected .",
    "the most time - consuming subphases are the ti and the ac . during the first phase the body interaction list is formed , containing some tree cells and close bodies .",
    "generally , tree cells are shared among all pes , whereas bodies are residing in the same pe or in the nearest pes .",
    "this fact reduces the access time of the bodies properties included in the interaction list .",
    "therefore the obtained results confirm that by distributing the close bodies in the same pe ( coarse grain ) we obtain the best performance .",
    "+ this effect depends on the order of the bodies so that nearest bodies have nearest numbering in the arrays containing bodies properties . if necessary",
    ", a sorting of the array can be performed , to obtain higher performances .",
    "as stated above , the best choice is to have a fine grain tree data distribution and a coarse grain bodies data distribution . as emphasized by the unbalance factor in tab .",
    "1 , it is very important to adopt a strategy allowing to increase the load balance and obtain higher performances . at the beginning @xcite",
    ", we adopted a dlb technique based on the concet of `` pe executor '' , i.e. , the pe executing the fc phase for the body .",
    "the pe executor was re - assigned at each time step in order to balance the load .",
    "+ although this usually brings some advantages , sometimes the overhead due to the pe executor re - assignment may greatly reduce the usefulness of this scheme . here",
    "we present the results of a new dlb strategy , that allows us to avoid any large overhead , because no explicit control mechanism is necessary . the total time spent in a parallel region @xmath15",
    ", can be considered as the sum of the following terms + @xmath16    where @xmath17 is the number of processors executing the job , @xmath18 is the time spent in the serial portion of the code ( a typical master region ) , @xmath19 is the time spent by a single processor ( @xmath20 ) to execute the parallel region , @xmath21 the overhead time due to the remote data access and to the synchronization points , and @xmath22 is a costant .",
    "+        & pe # & p / sec & fc phase & t - step & uf + 1mun_tf_bf & 128 & 4129 & 230.05 & 249.5 & 4.22 + 1mcl_tf_bf & 128 & 3832 & 250.32 & 268.81 & 4.57 + 1mun_tf_bm & 128 & 3547 & 270.51 & 290.45 & 5.90 + 1mcl_tf_bm & 128 & 3308 & 291.63 & 312.26 & 6.32 + 1mun_tf_bc & 128 & 4875 & 186.31 & 205.32 & 4.14 + 1mcl_tf_bc & 128 & 4490 & 203.37 & 222.72 & 4.38 + 1mun_tc_bc & 128 & 837 & 1051.93 & 1230.0 & 16.33 + 1mcl_tc_bc & 128 & 750 & 1173.24 & 1373.4 & 17.62 +    tab .1    ll    legend : & 1mun - 1 million of particles in uniform initial conditions ; + & 1mcl - 1 million of particles in clustered initial conditions ; + & tf - tree fine grain data distribution + & tc - tree coarse grain data distribution + & bf - bodies fine grain data distribution + & bm - bodies medium grain data distribution + & bc - bodies coarse grain data distribution +   + & pe # : pes number + & p / sec : particle per second + & fc : total fc pashe duration in second + & t - step : timestep duration in second + & uf : unbalance factor ( variance of fc phase duration ) +   +    in the fc phase , there are no serial regions , so the @xmath19 term is proportional to the length of the interaction list needed to compute the force acting on each body , the interaction list average length being @xmath23 . using a coarse grain subdivision ,",
    "each pe has a block of close bodies in the local memory ( @xmath24 ) ; in a uniform distribution initial condition , the pes having extreme numeration in the pool of available pes , have residing bodies near the border of the computational domain .",
    "owing to the lack of bodies besides the border line of the computational domain ( i.e. these bodies have a smaller interaction list than bodies in the center of the domain ) , the pes having extreme numeration have a lower load at each timestep .",
    "this kind of effect may be enhanced , if a clustered initial condition is used .",
    "+    the @xmath21 term depends principally on the latency time , on the bandwidth of the internal network , on the code synchronization point , and on the network contention . when the number of pes involved in the simulation increases the data dispersion on the t3d torus increases .",
    "2 we plot the total length of the interaction list , and the total number of cells and bodies forming the interaction list , using @xmath25 .",
    "these data were obtained making several tests with uniform and clustered initial conditions and the results are in accordance with plots reported in salmon ( 1990 ) .",
    "the number of internal cells included in the interaction list ranges from @xmath26 to @xmath27 of its total length .",
    "the tree cells have a fine grain distribution as stated above , and then data access to these elements increases with raising pe number which means the @xmath21 term in ( 1 ) increases . +    the figs . 3 , 4 and 5 report the total time ( @xmath15 in ( 1 ) ) in seconds spent in the fc phase ( t_fc ) by each pe , for different values of the total length of the interaction list ( tl_il ) , using 16 , 32 and 64 pes , respectively with 1 million of clustered particles @xmath25 .",
    "tl_il value is the sum of all the interaction lists obtained during the fc phase performed by the single pe .",
    "6 shows a similar result obtained by running a test with only 1 pe on the t3d machine .",
    "in this case all bodies and tree cells properties are located in the pe local memory .",
    "the results show a linear dependence between tl_il and the @xmath19 term . a comparison between data reported in fig . 6 and figs .",
    "3 , 4 and 5 leads us to the following consideration .",
    "when 16 pes are used ( fig .",
    "3 ) a relationship between the t_fc and tl_il terms may be found .",
    "the overhead time @xmath21 , ranging from 60% up to 70% of the @xmath28 term in ( 1 ) , can be calculated from the difference between data reported in figs .",
    "3 and 6 . when the pe number increases the relationship",
    "is lost as shown fig .",
    "the @xmath21 increases as the pe number increases , whereas the code performances increase when the load balance is optimized .",
    "+    the adopted technique is to perform a load redistribution among the pes so that all pes have the same load in the fc phase .",
    "we force each pe to execute this phase _ only _ for a fixed portion nb_lp of the bodies residing in the local memory .",
    "the nb_lp value is calculated as    @xmath29    the p_lp being a constant ranging from @xmath30 to @xmath11 .",
    "the fc phase for all the remaining bodies    @xmath31    is executed from all the pes that have finished the fc phase for the nb_lp bodies . no correlation between the pe memory , where the body properties are residing , and the pe , executing the fc phase for it , is found .",
    "+ if p_lp=1 all pes execute the fc phase only for bodies residing in the local memory .",
    "@xmath32 is equal to 0 and the body locality is totally involved in the overall performances , on the contrary if p_lp=0 , @xmath33 ( nb_lp = 0 ) , the pes execute only nfree bodies and the locality is not taken into account . + a p_lp value lower than 1 gives an automatic dynamic load balance mechanism to the system ; i.e. , each pe works until all the nfree bodies are computed . on the other hand ,",
    "if p_lp value is equal to 0 , it gives the maximum load balance and the maximum degree of automatism in the fc phase .",
    "+ several tests were performed with p_lp value ranging from 0 to 1 and pes ranging from 16 to 128 , using several initial conditions from 1000 up to 2 million particles uniform and clustered .",
    "7 - 10 show the results obtained only with a high ( @xmath34 ) number of particles , but the results obtained with smaller numbers of particles show the same trend .",
    "+ using the reported results and fixing the pe number , it is possible to determine the p_lp value allowing the best code performances . in particular , note that for a simulation using a high number of pes ( more than 32 ) , it is convenient to fix the p_lp value near to @xmath30 , that is maximize the load balance among the pes rather than the bodies locality .",
    "this is due to the network contention of the system that becomes relevant for the remote data access , mainly for the data access time of the tree cells .",
    "+    on the other hand , using a lower pe number ( from 16 to 32 ) we observe a different effect .",
    "we have a lower data dispersion among the pes , so the remote loads are fewer and have a shorter access time , thus the code performance takes advantage of the data locality . from figs .",
    "7 - 10 it is possible to fix a p_lp value combining the `` load remote '' effect and the `` data locality '' effect to maximize the computed number of particles per second , thus improving the code performances .",
    "+    the figures show that , fixing the pes number and the particles number , the same p_lp value gives the best performance both in uniform and clustered conditions .",
    "this means that it is possible to fix a p_lp value and this can be usefully adopted during all the running time : it is not necessary to recompute the p_lp value to have good performances .",
    "hence an automatic load balance mechanism is found without recalculating the p_lp value and without introducing any overhead time to obtain a good dynamic load balance .",
    "as stated in @xcite the memory occupancy of this kind of n - body code is lower than the n - body local essential tree - based codes .",
    "the total memory occupancy of wdsh - ptc , at present , allows to execute large simulation with more than 80 million of particles on the cineca t3e machine having 128 pes each with 16 mword ram .",
    "moreover the total memory occupancy may be reduced in order to run larger simulations with a little degradation of code performances .",
    "11 shows the code speed ( preliminary data ) in number of particles per second , computed at each timestep , vs. the number of pes , using two @xmath1 values : @xmath35 and @xmath36 , for a clustered configuration of @xmath37 particles , using the periodic boundary conditions and adopting a _ grouping _",
    "strategy @xcite , @xcite , @xcite i.e. building an interaction list for a group of bodies ( included in a cell ) .",
    "+    the results obtained using the wdsh - ptc code , at present , give performances comparable to those obtained with different approaches like let @xcite , with the advantage of avoiding the let and an excessive demand for memory .",
    "the study carried out on the cray t3d machines at the cineca ( casalecchio di reno , italy ) , allows us to propose a criterion for the optimal data distribution of bodies and tree cells properties . a strategy for",
    "the _ automatic _ dynamic load balance has been described , which does not introduce a significant overhead .",
    "the results of this work will allow us to obtain , in the next future , a wdsh - ptc version for the cray t3e system , using the hpf - craft and the shmem library .",
    "the new version will include an enhanced grouping strategy and periodic boundary conditions , and will allow us to run large simulations with very high performances @xcite . using the cineca visualization laboratory experience and resources",
    ", we plan also to develop an ad hoc visualization package for the scientific visualization of the simulation results    we would like to thank the cineca for the supercomputing time grant , mr .",
    "s. bassini for the availability of the cineca support and dr .",
    "l. calori , both from cineca , and dr .",
    "f. dur from silicon graphics for useful suggestions and comments .",
    "a. pagliaro wish to tank the epcc staff .            v. antonuccio - delogu , u. becciani , _",
    "`` a parallel tree n - body code for heterogeneous clusters '' _ , in : j. dongarra and j. wasniewsky , eds.,_parallel sciebtific computing - para 94 _ ( springer verlag : 1994 ) , 17"
  ],
  "abstract_text": [
    "<S> n - body algorithms for long - range unscreened interactions like gravity belong to a class of highly irregular problems whose optimal solution is a challenging task for present - day massively parallel computers . in this paper </S>",
    "<S> we describe a strategy for optimal memory and work distribution which we have applied to our parallel implementation of the barnes & hut ( 1986 ) recursive tree scheme on a cray t3d using the craft programming environment . </S>",
    "<S> we have performed a series of tests to find an _ </S>",
    "<S> optimal data distribution _ in the t3d memory , and to identify a strategy for the _ dynamic load balance _ in order to obtain good performances when running large simulations ( more than 10 million particles ) . </S>",
    "<S> the results of tests show that the step duration depends on two main factors : the data locality and the t3d network contention . </S>",
    "<S> increasing data locality we are able to minimize the step duration if the closest bodies ( direct interaction ) tend to be located in the same pe local memory ( contiguous block subdivison , high granularity ) , whereas the tree properties have a fine grain distribution . in a very large simulation , due to network contention , </S>",
    "<S> an unbalanced load arises . to remedy this </S>",
    "<S> we have devised an automatic work redistribution mechanism which provided a good dynamic load balance at the price of an insignificant overhead .    ,    ,    ,    ,    , and </S>"
  ]
}