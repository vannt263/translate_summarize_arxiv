{
  "article_text": [
    "in 1969 stuart kauffman started to study random boolean networks as simple models of genetic regulatory networks @xcite .",
    "random boolean networks that consists of a set of boolean _ gates _ that are capable of storing a single boolean value . at discrete time",
    "steps these gates store a new value according to an initially chosen random boolean function , which receives its inputs from random chosen gates .",
    "we will give a more formal definition later .",
    "kauffman made numerical studies of random networks , where the functions are chosen from the set of all boolean functions with @xmath0 arguments ( the so called _",
    "@xmath2-networks _ ) .",
    "he recognised that if @xmath3 , the random networks exhibit a remarkable form of ordered behaviour : the limit cycles are small , the number of _ ineffective gates _ , which are gates that can be perturbed without changing the asymptotic behaviour , and the number of _ freezing gates _ that stop changing their state is large .",
    "in contrast if @xmath4 , the networks do not exhibit this kind of ordered behaviour ( see @xcite ) . the first analytical proof for this _",
    "phase transition _ was given by derrida and pomeau ( see @xcite ) by studying the evolution of the hamming distance of random chosen initial states by means of so called _",
    "annealed approximation_. the first proof for the number of freezing and ineffective gates was given by james lynch ( see @xcite , although slightly weaker results appeared earlier @xcite )",
    ". depending on a parameter @xmath1 , that depends on the probabilities of the boolean functions , he showed that if @xmath5 almost all gates are ineffective and freezing , otherwise not .",
    "although his analysis is very general , until now it was only applied to networks with connectivity @xmath6 and non - uniform probabilities for the boolean function : if the probability of choosing a constant function is larger or equal the probability of choosing a non - constant non - canalizing function ( namely the xor- or the inverted xor - function ) , @xmath1 is less or equal to one .",
    "but it turns out that in some cases @xmath1 is equal to the expectation of the average sensitivity .",
    "therefore we will first study the average sensitivity in section [ sec:02 ] .",
    "afterwards it will be shown in section [ sec:03 ] how to use the results from the previous section to apply lynch s analysis to classical @xmath2-networks and _ biased _ random boolean networks .",
    "but first we will give some basic definition used throughout the paper in section [ sec:01 ] .",
    "in the following @xmath7 denotes the galois field of two elements , where addition , denoted by @xmath8 , is defined modulo 2 .",
    "the set of vectors of length @xmath0 over @xmath9 will be denoted by @xmath10 .",
    "if @xmath11 is a vector from @xmath10 , its @xmath12th component will be denoted by @xmath13 . with @xmath14",
    "we will denote the _ unit vector _ which has all components zero except component @xmath12 which is one .",
    "the hamming weight of @xmath15 is defined as @xmath16 and the hamming distance of @xmath17 as @xmath18 a boolean function is a mapping @xmath19 .",
    "a function @xmath20 may be represented by its _ truth table _ @xmath21 , that is , a vector in @xmath22 , where each component of the truth table gives the value of @xmath20 for one of the @xmath23 possible arguments . to fix an order on the components of the truth table , suppose that its @xmath12th component equals the value of the corresponding function , given the binary representation ( to @xmath0 bits ) of @xmath12 as an argument .",
    "in this section we will focus on the _ average sensitivity_. the average sensitivity is a known complexity measure for boolean functions , see for example @xcite .",
    "it was already used to study boolean and random boolean networks for example in @xcite .",
    "let @xmath20 denote a boolean function @xmath24 and @xmath25 a unit vector .    1 .",
    "the sensitivity @xmath26 is defined as : @xmath27 2 .",
    "the average sensitivity @xmath28 is defined as the average of @xmath26 over all @xmath29 : @xmath30    now consider the random variable @xmath31 , where @xmath32 denotes the set a all @xmath33 boolean function with @xmath0 arguments .",
    "the probability measure is given by @xmath34 .",
    "the expected value of the average sensitivity of this random variable is denoted by @xmath35 , and is given by @xmath36 the expected value was already derived in @xcite , and is given by :     + let the random variable @xmath37 be defined as above , then @xmath38 [ theo:01 ]    we will now concentrate on biased boolean functions .",
    "the bias of a boolean function @xmath19 is defined as the number of @xmath39 in the functions truth table divided by @xmath40 . to define the bias of a random boolean function two definitions are possible .",
    "first we can assumes that the truth tables of the boolean functions are produced by independent bernoulli trials with probability @xmath41 for a one ( this should be called _",
    "mean _ bias , used for example in @xcite ) .",
    "therefore consider the random variable @xmath42 .",
    "the probability of choosing a function @xmath20 is given by @xmath43 for @xmath44 this is equivalent to the definition of @xmath37 .    as a second possibility , we can only choose functions which have bias @xmath41 whereas to all other functions we assign probability 0 ( we will call this _ fixed _",
    "bias ) . therefore consider the random variables @xmath45 .",
    "denote the truth table of a function @xmath20 by @xmath21 .",
    "further denote the set of all boolean functions @xmath20 with @xmath0 arguments and @xmath46 with @xmath47 .",
    "the probability for a certain function chosen according @xmath48 is given by @xmath49    both definitions ensure that the expectation to get a one is equal to @xmath41 if the input of a function is chosen at random ( with respect to uniform distribution ) . but it will turn out that these two different methods of creating biased boolean functions , have a major impact on the average sensitivity .",
    "the expectation of the average sensitivity of @xmath42 was derived in @xcite :    let the random variable @xmath50 be defined as above : @xmath51 [ theo : expectedsensitivityb ]    for the random variable @xmath48 we will now proof the following theorem :    let the random variable @xmath48 be defined as above : @xmath52 [ theo : expectedsensitivity ] [ cor:01 ]    to find @xmath53 we will first consider the random variable @xmath54 where @xmath55 and the probability of a function is given by @xmath56    consider the boolean functions as functions into @xmath57 by identifying @xmath58 with @xmath59 .",
    "then we get or the function @xmath20 : @xmath60 where @xmath25 again denotes the unit vector with @xmath12th component set to @xmath39 .",
    "hence by the linearity of the expectation @xmath61    now we form a matrix with the truth tables of all functions with hamming weight @xmath62 as column vectors : @xmath63 @xmath64 has exactly @xmath65 columns and @xmath23 rows .",
    "each entry @xmath66 in the @xmath12th row and @xmath67th column equals the value of function @xmath68 given the binary representation of @xmath12 as input .",
    "hence @xmath69 is determined by the number of @xmath39 in the row associated with @xmath70 divided by the length of the row .",
    "consider an arbitrary row @xmath12 .",
    "this row has a one at position @xmath67 if the corresponding column @xmath71 has a one at position @xmath12 .",
    "but there are @xmath72 column vectors with a @xmath39 at position @xmath12",
    ". it follows : @xmath73 as this holds for all @xmath70 , we have @xmath74    to find an expression for @xmath75 we consider two arbitrary rows @xmath76 ( @xmath77 ) .",
    "define the following sum : @xmath78 obviously @xmath79 only if we have a @xmath39 in both rows at position @xmath12 .",
    "this means for the column vectors @xmath80 of @xmath64 , we have @xmath81 .",
    "but there are exactly @xmath82 such column vectors in @xmath64 .",
    "therefore we have @xmath83 as @xmath84 for all @xmath85 it follows : @xmath86 hence substituting equations , and into equation leads to @xmath87 finally the claimed expression for @xmath53 can be obtained from the above equation by a substitution of @xmath62 : @xmath88 .",
    "[ comment ] it should be noted , that the theorems [ theo:01 ] and [ theo : expectedsensitivityb ] can be proved using in a similar way .",
    "also worth noting is the fact , that if the functions are chosen according @xmath89 or @xmath42 the expectation of the sensitivity of a fixed vector @xmath70 ( namely the expectation of @xmath90 ) is independent of @xmath70 ( see equation , , and ) .",
    "hence the following lemma holds    if @xmath91 or @xmath42 , then @xmath92 [ lemma:01 ]    before proceeding to the next section , it should be noted , that using the same arguments as in the proof of theorem [ theo : expectedsensitivity ] , we can also prove the expectation of average sensitivity of order @xmath93 , defined as @xmath94 in this case , instead of summing up all unit vectors in equation , we sum up all vectors of hamming weight @xmath93 . as the equations and hold for all @xmath29 we conclude that @xmath95 and by similar arguments @xmath96 respectively @xmath97",
    "as already mentioned james lynch gave a very general analysis of randomly constructed boolean networks ( see @xcite ) . before stating his results we give a formal definition for boolean networks a boolean network * b *  is a 4-tuple @xmath98 where @xmath99 is a set of natural numbers , @xmath100 is a set of labeled edges on @xmath101 , @xmath102 is a ordered set of boolean functions such that for each @xmath103 the number of arguments of @xmath104 is the _ in - degree _ of @xmath105 in @xmath100 , these edges are labeled with @xmath106 , and @xmath107 .",
    "suppose that a vertex @xmath12 has @xmath108 in - edges from vertices @xmath109 .",
    "for @xmath110 we define @xmath111 the state of * b *  at time 0 is called the _ initial state _ @xmath11 , so we define @xmath112 . for time",
    "@xmath113 the state is inductively defined as @xmath114 .",
    "hence we can in interpret @xmath101 as set of gates , @xmath100 and @xmath115 describes their functional dependence and @xmath11 is the networks initial state .",
    "assume some ordering @xmath116 on the set of all boolean functions @xmath117 , where each function @xmath118 depends on @xmath108 arguments .",
    "further a random variable @xmath119 with probabilities @xmath120 such that @xmath121 and @xmath122 .",
    "now a random boolean network consisting of @xmath123 _ gates _ is constructed as follows : for each gate a boolean function is chosen independently , where the probability of choosing @xmath118 is given by @xmath124 .",
    "suppose a function @xmath20 was chosen that has @xmath0 arguments , these arguments are chosen at random from all @xmath125 equally likely possibilities . at last",
    "an initial state is chosen at random from the set on all equally likely states .",
    "if the boolean functions are chosen according to our previously defined random variable @xmath37 we will call this networks @xmath2-networks with connectivity @xmath0 . if the functions are chosen according to @xmath48 or @xmath42 we will call this networks _ biased random boolean networks _ with connectivity @xmath0 and fixed bias @xmath41 respectively mean bias @xmath41 .",
    "let us now state lynch s results .",
    "his analysis depends on a parameter @xmath126 depending only on the functions and their probabilities .",
    "we will define @xmath1 later in definition [ def : lambda ] .",
    "first we have to state lynch s definition of _ freezing _ and _ ineffective _ gates :     + let @xmath127 and @xmath103 .    1 .",
    "gate @xmath105 freezes to @xmath110 in @xmath62 steps on input @xmath11 if @xmath128 for all @xmath129 .",
    "2 .   let @xmath130 .",
    "+ a gate @xmath105 is @xmath62-ineffective at input @xmath15 if @xmath131 .",
    "now we will state the main result .",
    "+ let @xmath132 , @xmath133 be positive constants satisfying @xmath134 and @xmath135 where @xmath136 .    1 .",
    "there is a constant @xmath137 such that for all @xmath127 @xmath138 when @xmath5 , @xmath139 and when @xmath140 , @xmath141 .",
    "there is a constant @xmath137 such that for all @xmath127 @xmath142 when @xmath5 , @xmath139 and when @xmath140 , @xmath141 .",
    "the above theorem shows that if @xmath5 almost all gates are freezing and ineffective and otherwise not .",
    "the next corollary gives us more information what happens if @xmath140 :    let @xmath140 . for almost all random boolean networks    1 .",
    "if gate @xmath105 is not @xmath143-ineffective , there is a positive constant @xmath144 such that for @xmath145 , the number of gates affected by @xmath105 at time @xmath62 is asymptotic to @xmath146 , 2 .",
    "if gate @xmath105 is not freezing in @xmath143 steps , there is a positive constant @xmath144 such that for @xmath145 , the number of gates that affect @xmath105 at time @xmath62 is asymptotic to @xmath146 .",
    "now we will state the definition of @xmath1 for boolean networks :    [ def : lambda ] let @xmath20 be a boolean function of @xmath0 arguments . for @xmath147 , we say that argument @xmath12 directly affects @xmath20 on input @xmath29 if @xmath148 .",
    "now put @xmath149 as the number of @xmath12 s that directly affect @xmath20 on input @xmath70 . given a constant @xmath150 $ ] , we define @xmath151    obviously @xmath149 is identical to @xmath90 which will be used instead in the further discussion .",
    "the constant @xmath152 is the probability that a random gate is one ( at infinite time ) given that all gates at time @xmath153 have probability @xmath154 of being one .",
    "( see ( * ? ? ? * definiton 2 ) ) .",
    "assume that we choose the functions according a random variable @xmath155 which should be either @xmath37 , @xmath48 or @xmath42 .",
    "the functions are chosen out the set @xmath156 , we denote a function s probability with @xmath157 .",
    "it follows that @xmath158 @xmath159 denotes the expectation of the sensitivity for a fixed @xmath70 , equation follows from lemma [ lemma:01 ] . therefore , together with theorem [ theo:01 ] and theorem [ cor:01 ] we proved the following :    for random boolean networks , if    1 .",
    "the functions are chosen according random variable @xmath42 , it follows that @xmath160 2 .",
    "the functions are chosen according random variable @xmath48 , it follows that @xmath161    [ theo:02 ]    as a special case of the above theorem we get ( or by using theorem [ theo:01 ] )    in random boolean networks , where the functions are chosen according to the random variable @xmath162 @xmath163",
    "the results about @xmath2-networks are consistent with experimental results .",
    "in fact if @xmath164 almost all networks almost all gates are freezing and almost all gates are ineffective and otherwise not ( see @xcite ) .",
    "obviously , the border between the ordered and disordered phase is given by @xmath165 .",
    "the resulting phase diagram for biased random boolean networks , where the functions are chosen according to @xmath48 and @xmath42 is shown in figure [ fig:01 ] .",
    "it it interesting to note that if the functions are chosen with fixed bias , then also boolean networks with connectivity @xmath166 can become unstable . this conclusion can be drawn from lynch s original result already .",
    "as mentioned in the introduction , he showed for @xmath166 , that @xmath140 if the probability of choosing a non - constant non - canalizing function , namely the xor or the inverted xor function , is larger than the probability of choosing a constant function .",
    "for example if the bias is @xmath154 , the probability of choosing a constant function is zero , whereas both xor and inverted xor function have probability greater zero , hence @xmath140 .",
    "( dashed ) and @xmath48 ( solid),title=\"fig : \" ]    it is interesting to compare our results with previous results obtained first by derrida and pomeau using the so called _ annealed approximation _ ( see @xcite ) . in their _ annealed model _ the functions and connections",
    "are chosen at random at each time step .",
    "considering two instances of the same annealed network starting in two randomly chosen initial states @xmath167 they show that @xmath168 where @xmath169 if @xmath170 and @xmath171 otherwise .",
    "it is remarkable that the two models behave similar , but it is unclear whether this holds in general .",
    "we would like to thank our colleges georg schmidt and stephan stiglmayr for proofreading and uwe schoening for useful hints ."
  ],
  "abstract_text": [
    "<S> in this work we consider random boolean networks that provide a general model for _ genetic regulatory networks_. we extend the analysis of james lynch who was able to proof kauffman s conjecture that in the _ ordered phase _ of random networks , the number of _ ineffective _ and _ freezing _ gates is large , where as in the _ disordered phase _ their number is small . </S>",
    "<S> lynch proved the conjecture only for networks with connectivity two and non - uniform probabilities for the boolean functions . </S>",
    "<S> we show how to apply the proof to networks with arbitrary connectivity @xmath0 and to random networks with _ biased _ boolean functions . </S>",
    "<S> it turns out that in these cases lynch s parameter @xmath1 is equivalent to the expectation of _ average sensitivity _ of the boolean functions used to construct the network . </S>",
    "<S> hence we can apply a known theorem for the expectation of the average sensitivity . in order to prove the results for networks with biased functions , we deduct the expectation of the average sensitivity when only functions with specific connectivity and specific bias are chosen at random .    </S>",
    "<S> random boolean networks , phase transition , average sensitivity    pacs numbers : 02.10.eb , 05.45.+b , 87.10.+e </S>"
  ]
}