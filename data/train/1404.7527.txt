{
  "article_text": [
    "[ sec : introduction ]    this paper is set in the framework of _ inductive inference _ , a branch of ( algorithmic ) learning theory .",
    "this branch analyzes the problem of algorithmically learning a description for a formal language ( a computably enumerable subset of the set of natural numbers ) when presented successively all and only the elements of that language . for example",
    ", a learner @xmath0 might be presented more and more even numbers .",
    "after each new number , @xmath0 outputs a description for a language as its conjecture . the learner",
    "@xmath0 might decide to output a program for the set of all multiples of @xmath1 , as long as all numbers presented are divisible by  @xmath1 .",
    "later , when @xmath0 sees an even number not divisible by @xmath1 , it might change this guess to a program for the set of all multiples of  @xmath2 .",
    "many criteria for deciding whether a learner @xmath0 is _ successful _ on a language  @xmath3 have been proposed in the literature .",
    "gold , in his seminal paper @xcite , gave a first , simple learning criterion , _",
    "_ @xmath4-learning _ _ stands for learning from a _ text _ of positive examples ; @xmath5 stands for gold , who introduced this model , and is used to to indicate full - information learning ; @xmath6 stands for _ explanatory_. ] , where a learner is _ successful _",
    "iff , on every _ text _ for @xmath3 ( listing of all and only the elements of @xmath3 ) it eventually stops changing its conjectures , and its final conjecture is a correct description for the input sequence .",
    "trivially , each single , describable language @xmath3 has a suitable constant function as a @xmath4-learner ( this learner constantly outputs a description for @xmath3 ) .",
    "thus , we are interested in analyzing for which _ classes of languages _",
    "@xmath7 there is a _ single learner",
    "_ @xmath0 learning _",
    "each _ member of @xmath7 .",
    "this framework is also sometimes known as _ language learning in the limit _ and has been studied extensively , using a wide range of learning criteria similar to @xmath4-learning ( see , for example , the textbook @xcite ) .",
    "a wealth of learning criteria can be derived from @xmath4-learning by adding restrictions on the intermediate conjectures and how they should relate to each other and the data .",
    "for example , one could require that a conjecture which is consistent with the data must not be changed ; this is known as _ conservative _ learning and known to restrict what classes of languages can be learned ( @xcite , we use @xmath8 to denote the restriction of conservative learning ) .",
    "additionally to conservative learning , the following learning restrictions are considered in this paper ( see section  [ sec : learningcriteria ] for a formal definition of learning criteria including these learning restrictions ) .    in _ cautious _ learning ( @xmath9 , @xcite ) the learner is not allowed to ever give a conjecture for a strict subset of a previously conjectured set . in _ non - u - shaped _ learning ( @xmath10 , @xcite ) a learner may never _ semantically _ abandon a correct conjecture ; in _ strongly non - u - shaped _ learning ( @xmath11 , @xcite ) not even syntactic changes are allowed after giving a correct conjecture .    in",
    "_ decisive _ learning ( @xmath12 , @xcite ) , a learner may never ( semantically ) return to a _ semantically _ abandoned conjecture ; in _ strongly decisive _ learning ( @xmath13 , @xcite ) the learner may not even ( semantically ) return to _ syntactically _ abandoned conjectures .",
    "finally , a number of monotonicity requirements are studied ( @xcite ) : in _ strongly monotone _ learning ( @xmath14 ) the conjectured sets may only grow ; in _ monotone _ learning ( @xmath15 ) only incorrect data may be removed ; and in _ weakly monotone _ learning ( @xmath16 ) the conjectured set may only grow while it is consistent .",
    "the main question is now whether and how these different restrictions reduce learning power .",
    "for example , non - u - shaped learning is known not to restrict the learning power @xcite , and the same for strongly non - u - shaped learning @xcite ; on the other hand , decisive learning _ is _ restrictive @xcite .",
    "the relations of the different monotone learning restriction were given in @xcite .",
    "conservativeness is long known to restrict learning power @xcite , but also known to be equivalent to weakly monotone learning @xcite .",
    "cautious learning was shown to be a restriction but not when added to conservativeness in @xcite , similarly the relationship between decisive and conservative learning was given . in exercise 4.5.4b of @xcite",
    "it is claimed ( without proof ) that cautious learners can not be made conservative ; we claim the opposite in theorem  [ thm : cautvarconv ] .",
    "this list of previously known results leaves a number of relations between the learning criteria open , even when adding trivial inclusion results ( we call an inclusion trivial iff it follows straight from the definition of the restriction without considering the learning model , for example strongly decisive learning is included in decisive learning ; formally , trivial inclusion is inclusion on the level of learning restrictions as predicates , see section  [ sec : learningcriteria ] ) . with this paper",
    "we now give the complete picture of these learning restrictions .",
    "the result is shown as a map in figure  [ fig : goldrelations ] .",
    "a solid black line indicates a trivial inclusion ( the lower criterion is included in the higher ) ; a dashed black line indicates inclusion ( which is not trivial ) .",
    "a gray box around criteria indicates equality of ( learning of ) these criteria .",
    "a different way of depicting the same results is given in figure  [ fig : partialordergold ] ( where solid lines indicate any kind of inclusion ) .",
    "results involving monotone learning can be found in section  [ sec : monotone ] , all others in section  [ sec : caution ] .    at ( -5,-1 ) @xmath5 ;    ( nothing ) at ( 0,0 ) * t * @xmath10 @xmath11 ; ( dec ) at ( 0,-1.5 ) @xmath12 ; ( sdec ) at ( 0,-3 ) @xmath13 ; ( mon ) at ( 2.5,-4.5 ) @xmath15 ; ( wmon ) at ( -2.5,-4.5 ) @xmath9 @xmath16 @xmath8 ; ( smon ) at ( 0,-6 ) @xmath14 ;    ( nothing )  ( dec ) ; ( dec )  ( sdec ) ; ( sdec )  ( wmon ) ; ( sdec )  ( mon ) ; ( mon )  ( smon ) ; ( wmon )  ( smon ) ;    for the important restriction of conservative learning we give the characterization of being equivalent to cautious learning .",
    "furthermore , we show that even two weak versions of cautiousness are equivalent to conservative learning .",
    "recall that cautiousness forbids to return to a strict subset of a previously conjectured set .",
    "if we now weaken this restriction to forbid to return to _ finite _ subsets of a previously conjectured set we get a restriction still equivalent to conservative learning .",
    "if we forbid to go down to a correct conjecture , effectively forbidding to ever conjecture a superset of the target language , we also obtain a restriction equivalent to conservative learning . on the other hand ,",
    "if we weaken it so as to only forbid going to _ infinite _ subsets of previously conjectured sets , we obtain a restriction equivalent to no restriction .",
    "these results can be found in section  [ sec : caution ] .    in _",
    "set - driven _",
    "learning @xcite the learner does not get the full information about what data has been presented in what order and multiplicity ; instead , the learner only gets the set of data presented so far . for this learning model",
    "it is known that , surprisingly , conservative learning is no restriction @xcite !",
    "we complete the picture for set driven learning by showing that set - driven learners can always be assumed conservative , strongly decisive and cautious , and by showing that the hierarchy of monotone and strongly monotone learning also holds for set - driven learning .",
    "the situation is depicted in figure  [ fig : hierarchysetdriven ] .",
    "these results can be found in section  [ sec : setdriven ] .    at ( -5,-1 ) @xmath17 ;    ( nothing ) at ( 0,0 ) ; ( mon ) at ( 0,-1.5 ) @xmath15 ; ( smon ) at ( 0,-3 ) @xmath14 ;    ( nothing ) ",
    "( mon ) ; ( mon )  ( smon ) ;      a major emphasis of this paper is on the techniques used to get our results .",
    "these techniques include specific techniques for specific problems , as well as general theorems which are applicable in many different settings .",
    "the general techniques are given in section  [ sec : techniques ] , one main general result is as follows .",
    "it is well - known that any @xmath4-learner @xmath0 learning a language @xmath3 has a _ locking sequence _ , a sequence @xmath18 of data from @xmath3 such that , for any further data from @xmath3 , the conjecture does not change and is correct .",
    "however , there might be texts such that no initial sequence of the text is a locking sequence .",
    "we call a learner such that any text for a target language contains a locking sequence _ strongly locking _ , a property which is very handy to have in many proofs .",
    "fulk @xcite showed that , without loss of generality , a @xmath4-learner can be assumed strongly locking , as well as having many other useful properties ( we call this the _ fulk normal form _ , see definition  [ defn : fulknormalform ] ) . for many learning criteria considered in this paper it might be too much to hope for",
    "that all of them allow for learning by a learner in fulk normal form . however , we show in corollary  [ cor : sinklocking ] that we can always assume our learners to be strongly locking , total , and what we call _ syntactically decisive _ , never _ syntactically _ returning to syntactically abandoned hypotheses .",
    "the main technique we use to show that something is decisively learnable , for example in theorem  [ thm : natnumsdec ] , is what we call _ poisoning _ of conjectures . in the proof of theorem  [ thm : natnumsdec ]",
    "we show that a class of languages is decisively learnable by simulating a given monotone learner @xmath0 , but changing conjectures as follows .",
    "given a conjecture @xmath19 made by @xmath0 , if there is no mind change in the future with data from conjecture @xmath19 , the new conjecture is equivalent to @xmath19 ; otherwise it is suitably changed , _ poisoned _ , to make sure that the resulting learner is decisive .",
    "this technique was also used in @xcite to show strongly non - u - shaped learnability .    finally , for showing classes of languages to be not ( strongly ) decisively learnable , we adapt a technique known in computability theory as a `` priority argument '' ( note , though , that we do not deal with oracle computations ) .",
    "we use this technique to reprove that decisiveness is a restriction to @xmath4-learning ( as shown in @xcite ) , and then use a variation of the proof to show that strongly decisive learning is a restriction to decisive learning .",
    "[ sec : mathprelim ]    unintroduced notation follows @xcite , a textbook on computability theory .",
    "@xmath20 denotes the set of natural numbers , @xmath21 .",
    "the symbols @xmath22 , @xmath23 , @xmath24 , @xmath25 respectively denote the subset , proper subset , superset and proper superset relation between sets ; @xmath26 denotes set difference . @xmath27 and @xmath28 denote the empty set and the empty sequence , respectively .",
    "the quantifier @xmath29 means `` for all but finitely many @xmath30 '' . with @xmath31 and @xmath32",
    "we denote , respectively , domain and range of a given function .",
    "whenever we consider tuples of natural numbers as input to a function , it is understood that the general coding function @xmath33 is used to code the tuples into a single natural number .",
    "we similarly fix a coding for finite sets and sequences , so that we can use those as input as well . for finite sequences ,",
    "we suppose that for any @xmath34 we have that the code number of @xmath18 is at most the code number of @xmath35 .",
    "we let @xmath36 denote the set of all ( finite ) sequences , and @xmath37 the ( finite ) set of all sequences of length at most @xmath38 using only elements @xmath39 .",
    "if a function @xmath40 is not defined for some argument @xmath30 , then we denote this fact by @xmath41 , and we say that @xmath40 on @xmath30 _ diverges _ ; the opposite is denoted by @xmath42 , and we say that @xmath40 on @xmath30 _",
    "converges_. if @xmath40 on @xmath30 converges to @xmath43 , then we denote this fact by @xmath44 .",
    "we let @xmath45 denote the set of all partial functions @xmath46 and @xmath47 the set of all total such functions .",
    "@xmath48 and @xmath49 denote , respectively , the set of all partial computable and the set of all total computable functions ( mapping @xmath46 ) .",
    "we let @xmath50 be any fixed acceptable programming system for @xmath48 ( an acceptable programming system could , for example , be based on a natural programming language such as c or java , or on turing machines ) .",
    "further , we let @xmath51 denote the partial computable function computed by the @xmath50-program with code number @xmath43 .",
    "a set @xmath52 is _ computably enumerable ( ` ce ` ) _ iff it is the domain of a computable function .",
    "let @xmath53 denote the set of all ` ce `  sets .",
    "we let @xmath54 be the mapping such that @xmath55 . for each @xmath19",
    ", we write @xmath56 instead of @xmath57 .",
    "@xmath54 is , then , a mapping from @xmath20 _ onto _ @xmath53 .",
    "we say that @xmath19 is an index , or program , ( in @xmath54 ) for @xmath56 .",
    "we let @xmath58 be a blum complexity measure associated with @xmath50 ( for example , for each @xmath19 and @xmath30 , @xmath59 could denote the number of steps that program @xmath19 takes on input @xmath30 before terminating ) . for all @xmath19 and @xmath38 we let @xmath60 ( note that a complete description for the finite set @xmath61 is computable from @xmath19 and  @xmath38 ) .",
    "the symbol @xmath62 is pronounced _ pause _ and is used to symbolize `` no new input data '' in a text .",
    "for each ( possibly infinite ) sequence @xmath63 with its range contained in @xmath64 , let @xmath65 ) . by using an appropriate coding ,",
    "we assume that @xmath66 and @xmath62 can be handled by computable functions . for any function @xmath67 and all @xmath68",
    ", we use @xmath69 $ ] to denote the sequence @xmath70 ,  , @xmath71 ( the empty sequence if @xmath72 and undefined , if any of these values is undefined ) .",
    "we will use case s _ operator recursion theorem _ ( * ort * ) , providing _",
    "infinitary _ self - and - other program reference @xcite .",
    "* ort *  itself states that , for all operators @xmath73 there are @xmath40 with @xmath74 and @xmath75 , @xmath76      [ sec : learningcriteria ]    in this section we formally introduce our setting of learning in the limit and associated learning criteria .",
    "we follow @xcite in its `` building - blocks '' approach for defining learning criteria .",
    "a _ learner _ is a partial computable function @xmath77 .",
    "a _ language _ is a ` ce `  set @xmath52 .",
    "any total function @xmath78 is called a _",
    "text_. for any given language @xmath3 , a _ text for @xmath3 _ is a text @xmath67 such that @xmath79 .",
    "initial parts of this kind of text is what learners usually get as information .",
    "an _ interaction operator _ is an operator @xmath80 taking as arguments a function @xmath0 ( the learner ) and a text @xmath67 , and that outputs a function @xmath43 .",
    "we call @xmath43 the _ learning sequence _ ( or _ sequence of hypotheses _ ) of @xmath0 given @xmath67 .",
    "intuitively , @xmath80 defines how a learner can interact with a given text to produce a sequence of conjectures .",
    "we define the interaction operators @xmath5 , @xmath81 ( partially set - driven learning , @xcite ) and @xmath17 ( set - driven learning , @xcite ) as follows . for all learners @xmath0 , texts @xmath67 and all @xmath68 , @xmath82);\\\\ { \\mathbf{psd}}(h , t)(i )   & = & h({\\mathrm{content}}(t[i]),i);\\\\ { \\mathbf{sd}}(h , t)(i )    & = & h({\\mathrm{content}}(t[i])).\\end{aligned}\\ ] ] thus , in set - driven learning",
    ", the learner has access to the set of all previous data , but not to the sequence as in @xmath5-learning . in partially set - driven learning , the learner has the set of data and the current iteration number .",
    "successful learning requires the learner to observe certain restrictions , for example convergence to a correct index .",
    "these restrictions are formalized in our next definition .",
    "a _ learning restriction _ is a predicate @xmath83 on a learning sequence and a text . we give the important example of explanatory learning ( @xmath6 , @xcite ) defined such that , for all sequences of hypotheses @xmath43 and all texts @xmath67 , @xmath84.\\end{aligned}\\ ] ] furthermore",
    ", we formally define the restrictions discussed in section  [ sec : introduction ] in figure  [ fig : definitionsoflearningrestrictions ] ( where we implicitly require the learning sequence @xmath43 to be total , as in @xmath6-learning ; note that this is a technicality without major importance ) .",
    "@xmath85 )     \\subseteq w_{p(i ) } \\rightarrow p(i ) = p(i+1)];\\\\ { \\mathbf{caut}}(p , t )",
    "\\leftrightarrow & \\ ; [ \\forall i , j : w_{p(i ) }     \\subset w_{p(j ) } \\rightarrow i <",
    "j];\\\\ { \\mathbf{nu}}(p , t ) \\leftrightarrow & \\ ; [ \\forall i , j , k : i \\leq j \\leq k \\ ; \\wedge \\ ;     w_{p(i ) } = w_{p(k ) } = { \\mathrm{content}}(t ) \\rightarrow w_{p(j ) } = w_{p(i)}];\\\\ { \\mathbf{dec}}(p , t ) \\leftrightarrow & \\ ; [ \\forall i , j , k : i \\leq j \\leq k \\ ; \\wedge \\ ;     w_{p(i ) } = w_{p(k ) } \\rightarrow w_{p(j ) } = w_{p(i)}];\\\\ { \\mathbf{snu}}(p , t ) \\leftrightarrow & \\ ; [ \\forall i , j , k : i \\leq j",
    "\\leq k \\ ; \\wedge \\ ;     w_{p(i ) } = w_{p(k ) } = { \\mathrm{content}}(t ) \\rightarrow p(j ) = p(i)];\\\\ { \\mathbf{sdec}}(p , t ) \\leftrightarrow & \\ ; [ \\forall i , j , k : i \\leq j \\leq k \\ ; \\wedge \\ ;     w_{p(i ) } = w_{p(k ) } \\rightarrow p(j ) = p(i)];\\\\ { \\mathbf{smon}}(p , t ) \\leftrightarrow & \\ ; [ \\forall i , j : i < j \\rightarrow w_{p(i ) }      \\subseteq w_{p(j)}];\\\\ { \\mathbf{mon}}(p , t ) \\leftrightarrow & \\ ; [ \\forall i , j : i < j \\rightarrow w_{p(i ) }    \\cap{\\mathrm{content}}(t ) \\subseteq w_{p(j)}\\cap{\\mathrm{content}}(t)];\\\\ { \\mathbf{wmon}}(p , t ) \\leftrightarrow & \\ ; [ \\forall i , j : i < j \\wedge { \\mathrm{content}}(t[j ] )    \\subseteq w_{p(i ) } \\rightarrow w_{p(i ) } \\subseteq w_{p(j)}].\\end{aligned}\\ ] ]    a variant on decisiveness is _ syntactic decisiveness _ , @xmath86 , a technically useful property defined as follows .",
    "@xmath87.\\ ] ] we combine any two sequence acceptance criteria @xmath83 and @xmath88 by intersecting them ; we denote this by juxtaposition ( for example , all the restrictions given in figure  [ fig : definitionsoflearningrestrictions ] are meant to be always used together with @xmath6 ) .",
    "with @xmath89 we denote the always true sequence acceptance criterion ( no restriction on learning ) .",
    "a _ learning criterion _ is a tuple @xmath90 , where @xmath91 is a set of learners ( the admissible learners ) , @xmath80 is an interaction operator and @xmath83 is a learning restriction ; we usually write @xmath92 to denote the learning criterion , omitting @xmath91 in case of @xmath93 .",
    "we say that a learner @xmath94 _ @xmath92-learns _ a language @xmath3 iff , for all texts @xmath67 for @xmath3 , @xmath95 .",
    "the set of languages @xmath92-learned by @xmath94 is denoted by @xmath96 .",
    "we write @xmath97 $ ] to denote the set of all @xmath92-learnable classes ( learnable by some learner in @xmath91 ) .",
    "[ sec : techniques ]    in this section we present technically useful results which show that learners can always be assumed to be in some normal form .",
    "we will later always assume our learners to be in the normal form established by corollary  [ cor : sinklocking ] , the main result of this section .",
    "we start with the definition of _",
    "delayable_. intuitively , a learning criterion @xmath83 is delayable iff the output of a hypothesis can be arbitrarily ( but not indefinitely ) delayed .",
    "[ defn : delayable ] let @xmath98 be the set of all non - decreasing @xmath99 with infinite limit inferior , i.e.  for all @xmath100 we have @xmath101 .    a learning restriction @xmath83 is _ delayable _ iff , for all texts @xmath67 and @xmath102 with @xmath103 , all @xmath43 and all @xmath104 , if @xmath105 and @xmath106 ) \\subseteq { \\mathrm{content}}(t'[n])$ ] , then @xmath107 .",
    "intuitively , as long as the learner has at least as much data as was used for a given conjecture , then the conjecture is permissible .",
    "note that this condition holds for @xmath108 if @xmath109 .",
    "note that the intersection of two delayable learning criteria is again delayable and that _ all _ learning restrictions considered in this paper are delayable .",
    "as the name suggests , we can apply _ delaying tricks _ ( tricks which delay updates of the conjecture ) in order to achieve fast computation times in each iteration ( but of course in the limit we still spend an infinite amount of time ) .",
    "this gives us equally powerful but total learners , as shown in the next theorem .",
    "while it is well - known that , for many learning criteria , the learner can be assumed total , this theorem explicitly formalizes conditions under which totality can be assumed ( note that there are also natural learning criteria where totality can not be assumed , such as consistent learning @xcite ) .",
    "[ thm : total ] for any delayable learning restriction @xmath83 , we have [ @xmath110 = [ @xmath111 .",
    "let @xmath0 be a @xmath112-learner and @xmath19 such that @xmath113 .",
    "we define a function @xmath114 such that , for all @xmath18 , @xmath115 we let @xmath116 be the learner such that , for all @xmath18 , @xmath117 as @xmath0 is required to have only total learning sequences , we have that @xmath118 ; thus , @xmath116 is total computable using that @xmath114 is total computable .",
    "let @xmath119 , @xmath120 and let @xmath67 be a text for @xmath3 .",
    "let @xmath121))|$ ]",
    ". then we have , for all @xmath122 , @xmath123 ) = h(t[r(n)])$ ] . thus ,",
    "if we show that @xmath104 we get that @xmath116 @xmath112-learns @xmath3 from @xmath67 using @xmath83 delayable . from the definition of @xmath114 we get",
    "that @xmath124 is non - decreasing and , for all @xmath122 , @xmath125 . for any given @xmath100",
    "there are @xmath126 with @xmath127 such that @xmath128 ) \\leq n'$ ] .",
    "thus , we have @xmath129 and , as @xmath124 is non - decreasing , we get @xmath130 as desired .",
    "next we define another useful property , which can always be assumed for delayable learning restrictions .",
    "[ defn : stronglylocking ] a _ locking sequence for a learner @xmath0 on a language @xmath3 _ is any finite sequence @xmath18 of elements from @xmath3 such that @xmath131 is a correct hypothesis for @xmath3 and , for sequences @xmath35 with elements from @xmath3 , @xmath132@xcite . it is well known that every learner @xmath0 learning a language @xmath3 has a locking sequence on @xmath3 .",
    "we say that a learning criterion @xmath133 _ allows for strongly locking learning _",
    "iff , for each @xmath133-learnable class of languages @xmath7 there is a learner @xmath0 such that @xmath0 @xmath133-learns @xmath7 and , for each @xmath120 and any text @xmath67 for @xmath3 , there is an @xmath122 such that @xmath134 $ ] is a locking sequence of @xmath0 on @xmath3 ( we call such a learner @xmath0 _ strongly locking _ ) .    with this definition",
    "we can give the following theorem .",
    "[ thm : delaystronglylocking ] let @xmath83 be a delayable learning criterion",
    ". then @xmath135 allows for strongly locking learning .",
    "let @xmath7 and @xmath136 be such that @xmath0 @xmath135-learns @xmath7 .",
    "we define a set @xmath137 , for all @xmath138 and @xmath18 such that @xmath139 thus , @xmath114 contains sequences with elements from @xmath140 such that @xmath0 makes a mind change on @xmath18 extended with such a sequence .",
    "additionally , we define a function @xmath40 recursively such that , for all @xmath141 and @xmath67 , @xmath142)}.\\end{aligned}\\ ] ] intuitively , @xmath40 searches for longer and longer sequences which are _ not _ locking sequences .",
    "we let @xmath116 be the learner such that , for all @xmath18 , @xmath143 note that @xmath40 is total ( as @xmath0 is total ) , and thus @xmath116 is total .",
    "let @xmath120 and @xmath67 be a text for @xmath3 .",
    "we will show now that @xmath144 converges to a finite sequence .",
    "we have that @xmath144 is finite .    by way of contradiction , suppose that @xmath144 is infinite , and let @xmath145 .",
    "as @xmath144 is infinite we get , for every @xmath122 , an @xmath146 such that @xmath147 ) \\neq f(t[n])$ ]",
    ". then we have @xmath148 ) \\subseteq { \\mathrm{content}}(f(t[m])).\\ ] ] as this holds for every @xmath122 , we get @xmath149 . from the construction of @xmath40 we know that @xmath150 .",
    "thus , @xmath144 is a text for @xmath3 . from the construction of @xmath114",
    "we get that @xmath0 does not @xmath4-learns @xmath3 from @xmath102 as @xmath0 changes infinitely often its mind , a contradiction .",
    "next , we will show that @xmath116 converges on @xmath67 and @xmath116 is strongly locking . as @xmath144 is finite",
    ", there is @xmath151 such that , for all @xmath152 , @xmath153 ) = f(t[n_0]).\\end{aligned}\\ ] ]    as @xmath144 converges to @xmath154)$ ] , we get from the construction of @xmath114 that @xmath154)$ ] is a locking sequence of @xmath0 on @xmath3 . therefore we get that , for all @xmath155 , @xmath156 ) = f(t[n_0 ] \\diamond \\tau)\\ ] ] and therefore @xmath157 ) = h'(t[n_0 ] \\diamond \\tau).\\ ] ] thus , @xmath116 is strongly locking and converges on @xmath67 .    to show that @xmath116 fulfills the @xmath83-restriction , we let @xmath158 ) \\diamond t$ ] be a text for @xmath3 starting with @xmath154)$ ] .",
    "let @xmath124 be such that @xmath159)| , & \\text{if } n \\leq n_0 ; \\\\",
    "r(n_0 ) + n - n_0 , & \\text{otherwise . }",
    "\\end{cases}\\]]we now show @xmath160 ) = h'(t[n]).\\ ] ]    _ case 1 : _",
    "then we get @xmath162 ) & = h(t'[|f(t[n])| ] ) \\\\      & = h(f(t[n ] ) ) & \\text{as $ t ' = f(t[n_0 ] ) \\diamond t$ } \\\\      & = h'(t[n]).\\end{aligned}\\ ] ]    _ case 2 : _",
    "then we get @xmath162 ) & = h(t'[r(n_0 ) + n - n_0])\\\\      & = h(t'[|f(t[n_0])| + n - n_0 ] ) \\\\       & = h(f(t[n_0])\\diamond t[n - n_0 ] ) & \\text{as $ t ' = f(t[n_0 ] ) \\diamond t$}\\\\      & = h(f(t[n_0 ] ) ) & \\text{\\hspace{-10mm}as $ f(t[n_0])$ is a locking sequence of $ h$ } \\\\      & = h'(t[n]).\\end{aligned}\\ ] ] thus , all that remains to be shown is that @xmath104 . obviously , @xmath124 is non - decreasing .",
    "especially , we have that @xmath124 is strongly monotone increasing for all @xmath163 .",
    "thus we have , for all @xmath100 , @xmath130 .",
    "finally we show that @xmath164 ) \\subseteq { \\mathrm{content}}(t[n])$ ] . from the construction of @xmath40 we have , for all @xmath161 , @xmath165)| ] ) \\subseteq { \\mathrm{content}}(t[n])$ ] . from the construction of @xmath124 and @xmath102",
    "we get that , for all @xmath163 , @xmath166 .",
    "thus we get , for all @xmath122 , @xmath164 ) \\subseteq { \\mathrm{content}}(t[n])$ ] .",
    "next we define semantic and pseudo - semantic restrictions introduced in  @xcite .",
    "intuitively , semantic restrictions allow for replacing hypotheses by equivalent ones ; pseudo - sematic restrictions allow the same , as long as no new mind changes are introduced .",
    "[ defn : semanticrestriction ] for all total functions @xmath167 , we let @xmath168    a sequence acceptance criterion @xmath83 is said to be a _ semantic restriction _",
    "iff , for all @xmath169 and @xmath170 , @xmath171 .",
    "a sequence acceptance criterion @xmath83 is said to be a _ pseudo - semantic restriction _",
    "iff , for all @xmath169 and @xmath172 , @xmath171 .",
    "we note that the intersection of two ( pseudo- ) semantic learning restrictions is again ( pseudo- ) semantic .",
    "all learning restrictions considered in this paper are pseudo - semantic , and all except @xmath8 , @xmath11 , @xmath13 and @xmath6 are semantic .",
    "the next lemma shows that , for every pseudo - semantic learning restriction , learning can be done syntactically decisively .",
    "[ lem : syndec ] let @xmath83 be a pseudo - semantic learning criterion .",
    "then we have @xmath173 = [ { \\mathcal{r}}{\\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{syndec}}\\delta].\\ ] ]    let a @xmath112-learner @xmath136 be given .",
    "we define a learner @xmath174 such that , for all @xmath18 , @xmath175 the correctness of this construction is straightforward to check .    as @xmath86 is a delayable learning criterion ,",
    "we get the following corollary by taking theorems  [ thm : total ] and  [ thm : delaystronglylocking ] and lemma  [ lem : syndec ] together .",
    "we will always assume our learners to be in this normal form in this paper .",
    "[ cor : sinklocking ] let @xmath83 be pseudo - semantic and delayable",
    ". then @xmath176 allows for strongly locking learning by a syntactically decisive total learner .",
    "fulk showed that any @xmath4-learner can be ( effectively ) turned into an equivalent learner with many useful properties , including strongly locking learning @xcite .",
    "one of the properties is called _ order - independence _ , meaning that on any two texts for a target language the learner converges to the same hypothesis .",
    "another property is called _ rearrangement - independence _ , where a learner @xmath0 is rearrangement - independent if there is a function @xmath40 such that , for all sequences @xmath18 , @xmath177 ( intuitively , rearrangement independence is equivalent to the existence of a partially set - driven learner for the same language ) .",
    "we define the collection of all the properties which fulk showed a learner can have to be the _",
    "fulk normal form _ as follows .",
    "[ defn : fulknormalform ] we say a @xmath4-learner @xmath0 is in _ fulk normal form _ if @xmath178 hold .    1 .",
    "@xmath0 is order - independent .",
    "@xmath0 is rearrangement - independent .",
    "3 .   if @xmath0 @xmath4-learns a language @xmath3 from some text , then @xmath0 @xmath4-learns  @xmath3 .",
    "4 .   if there is a locking sequence of @xmath0 for some @xmath3 , then @xmath0 @xmath4-learns @xmath3 .",
    "5 .   for all @xmath179 , @xmath0 is strongly locking on @xmath7 .",
    "the following theorem is somewhat weaker than what fulk states himself .",
    "[ thm : fulknormalform ] every @xmath4-learnable set of languages has a @xmath4-learner in fulk normal form .",
    "[ sec : caution ]    in this section we consider various versions of cautious learning and show that all of our variants are either no restriction to learning , or equivalent to conservative learning as is shown in figure  [ fig : goldcautiousv ] .",
    "additionally , we will show that every cautious @xmath4-learnable language is conservative @xmath4-learnable which implies that @xmath180 $ ] , @xmath181 $ ] and @xmath182 $ ] are equivalent .",
    "last , we will separate these three learning criteria from strongly decisive @xmath4-learning and show that @xmath183 $ ] is a proper superset .    [",
    "thm : convinsdec ] we have that any conservative learner can be assumed cautious and strongly decisive , i.e. @xmath184 = [ \\textbf{txtgconvsdeccautex}].\\ ] ]    let @xmath136 and @xmath185 be such that @xmath0 * txtgconvex*-learns @xmath185 .",
    "we define , for all @xmath18 , a set @xmath186 as follows @xmath187 we let @xmath188 let @xmath67 be a text for a language @xmath189 .",
    "we first show that @xmath116 * txtgex*-learns @xmath3 from the text @xmath67 . as @xmath0 * txtgconvex*-learns @xmath3 , there are @xmath122 and @xmath19 such that @xmath190 ) = h(t[n ' ] ) = e$ ] and @xmath191 .",
    "thus , there is @xmath192 such that @xmath193 ) : \\phi_{h(t[n])}(x ) \\leq m$ ] and therefore @xmath194)=h'(t[m'])=e$ ] .",
    "next we show that @xmath116 is strongly decisive and conservative ; for that we show that , with every mind change , there is a new element of the target included in the conjecture which is currently not included but is included in all future conjectures ; it is easy to see that this property implies both caution and strong decisiveness .",
    "let @xmath68 and @xmath195 be such that @xmath196 ) ) = t[i]$ ] .",
    "this implies that @xmath197 ) \\subseteq w_{h'(t[i'])}.\\ ] ] let @xmath198 such that @xmath199 ) \\neq h'(t[j'])$ ]",
    ". then there is @xmath200 such that @xmath201 ) ) = t[j]$ ] and therefore @xmath202 ) \\subseteq w_{h'(t[j'])}.\\ ] ] note that in the following diagram @xmath203 could also be between @xmath68 and @xmath195 .",
    "( left ) at ( -1,0 ) [ ] ; ( right ) at ( 12,0 ) [ ] ; ( labelil ) at ( 4,-1)[]@xmath199 ) = h(t[i])$ ] ; ( labeljl ) at ( 10,-1)[]@xmath204 ) = h(t[j])$ ] ; at ( 4,-1.5 ) @xmath205 ) \\subseteq w_{h(t[i])}$ ] ; at ( 10,-1.5 ) @xmath206 ) \\subseteq w_{h(t[j])}$ ] ;    ( left )  ( right ) ; ( 1,1pt )  ( 1,-1pt ) node[anchor = south]@xmath68 node[anchor = north]mind change @xmath0 ; ( 4,1pt )  ( 4,-1pt ) node[anchor = south]@xmath195 node[anchor = north]mind change @xmath116 ; ( 7,1pt ) ",
    "( 7,-1pt ) node[anchor = south]@xmath203 node[anchor = north]mind change @xmath0 ; ( 10,1pt ) ",
    "( 10,-1pt ) node[anchor = south]@xmath207 node[anchor = north]mind change @xmath116 ; ( 4,0.5 ) ",
    "( 10,0.5 ) node[midway , above , yshift=12pt,]no mind change @xmath116 ;    as @xmath0 is conservative and @xmath205 ) \\subseteq w_{h(t[i])}$ ] , there exists @xmath208 such that @xmath209 and @xmath210)}$ ] .",
    "then we have @xmath211)}$ ] as @xmath212)}$ ] .",
    "obviously @xmath116 is conservative as it only outputs ( delayed ) hypotheses of @xmath0 ( and maybe skip some ) and @xmath0 is conservative .    in the following we consider three new learning restrictions .",
    "the learning restriction @xmath213 means that the learner never returns a hypothesis for a finite set that is a proper subset of a previous hypothesis .",
    "@xmath214 is the same restriction for infinite hypotheses .",
    "with @xmath215 the learner is not allowed to ever output a hypothesis that is a proper superset of the target language that is learned .",
    "[ defn : cautvariations ] @xmath216\\\\ { \\mathbf{caut}}_{\\infty}(p , t ) & \\leftrightarrow [ \\forall i < j : w_{p(j ) } \\subset w_{p(i ) } \\rightarrow w_{p(j ) } \\text { is finite}]\\\\ { \\mathbf{caut}}_{\\mathbf{tar}}(p , t ) & \\leftrightarrow [ \\forall i : \\neg ( { \\mathrm{content}}(t ) \\subset w_{p(i ) } ) ] \\end{aligned}\\ ] ]    ( nothing ) at ( 1,0 ) * t * ; ( caut ) at ( 1,-3.5 ) @xmath9 ;    ( cautinf ) at ( -1.5,-1 ) @xmath214 ; ( cauttar ) at ( 1,-2 ) @xmath217 ; ( cautfin ) at ( 3.5,-2 ) @xmath218 ;    ( caut )  ( cautfin ) ; ( caut )  ( cauttar ) ; ( caut )  ( cautinf ) ; ( cautinf )  ( nothing ) ; ( cauttar )  ( nothing ) ; ( cautfin )  ( nothing ) ;    ( -3,-1.2 )  ( 5,-1.2 ) ;    the proof of the following theorem is essentially the same as given in @xcite to show that cautious learning is a proper restriction of @xmath4-learning , we now extend it to strongly decisive learning .",
    "note that a different extension was given in @xcite ( with an elegant proof exploiting the undecidability of the halting problem ) , pertaining to _ behaviorally correct _ learning .",
    "the proof in @xcite as well as our proof would also carry over to the combination of these two extensions .    [ thm : convwmonint ] there is a class of languages that is @xmath219-learnable , but not @xmath220-learnable .",
    "let @xmath0 be a @xmath81-learner as follows , @xmath221 and @xmath222 .",
    "suppose @xmath7 is @xmath220-learnable through learner @xmath174 .",
    "we define , for all @xmath18 and @xmath38 , the total computable predicate @xmath223 as @xmath224    we let @xmath225 such that , for every set @xmath226 , @xmath227 . using @xmath228",
    "we define @xmath43 and @xmath75 strongly monotone increasing such that for all @xmath122 and @xmath38 , @xmath229 ) ) , & \\text{if } q(e[n+1],t ) ; \\\\ p , & \\text{otherwise . } \\end{cases}\\end{aligned}\\ ] ] _ case 1 : _ for all @xmath122 and @xmath38 , @xmath230,t)$ ] does not hold .",
    "then we have @xmath231 for all @xmath232 .",
    "thus @xmath233 as for any @xmath234 , @xmath235 . but @xmath116 does not @xmath220-learns @xmath236 from text @xmath19 as for all @xmath122 and @xmath38 , @xmath237)$ ] is not a proper subset of @xmath238)}$ ] in @xmath38 steps although @xmath236 is infinite .",
    "_ case 2 : _ there are @xmath122 and @xmath38 such that @xmath230,t)$ ] holds .",
    "then we have @xmath239 ) \\in { \\mathcal{l}}$ ] as we will show now .",
    "let @xmath67 be a text for @xmath239)$ ] .",
    "as @xmath19 is monotone increasing we have that @xmath240 is the maximal element in @xmath239)$ ] . additionally , for all @xmath241",
    ", we have @xmath242))$ ] . as @xmath0 makes only one mind change",
    "the strongly decisive and monotone conditions hold .",
    "thus , there is @xmath151 such that , for all @xmath152 , @xmath243),n ) = h({\\mathrm{content}}(t[n_0]),n_0 ) = { \\mathrm{ind}}({\\mathrm{content}}(e[n+1]))$ ] , i.e.  @xmath239 ) \\in { \\mathcal{l}}$ ] .",
    "the learner @xmath116 does not @xmath220-learn @xmath239)$ ] as we know from the predicate @xmath244 that @xmath239 ) \\subset w_{h'({\\mathrm{content}}(e[n+1]))}$ ] and the cautious learner @xmath116 must not change to a proper subset of a previous hypothesis .",
    "the following theorem contradicts a theorem given as an exercise in @xcite ( exercise 4.5.4b ) .",
    "[ thm : cautvarconv ] for @xmath245 we have @xmath246 = [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{conv}}{\\mathbf{ex}}].\\ ] ]    we get the inclusion [ * txtgconvex * ] @xmath22 [ * txtgcautex * ] as a direct consequence from theorem  [ thm : convinsdec ] .",
    "obviously we have @xmath182 \\subseteq [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{caut}}_\\mathbf{tar}{\\mathbf{ex}}]$ ] and @xmath182 \\subseteq [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{caut}}_\\mathbf{fin}{\\mathbf{ex}}]$ ] .",
    "thus , it suffices to show @xmath247 \\subseteq [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{conv}}{\\mathbf{ex}}]$ ] .",
    "let @xmath185 be @xmath176-learnable by a syntactically decisive learner @xmath248 ( see corollary  [ cor : sinklocking ] ) .",
    "using the s - m - n theorem we get a function @xmath249 such that @xmath250 we let @xmath244 be the following computable predicate . @xmath251 for given sequences @xmath18 and @xmath35 we say @xmath252 if @xmath253 this means that , for every @xmath18 , the set of all @xmath35 such that @xmath252 is finite and computable .",
    "we define a learner @xmath116 such that @xmath254 where @xmath255 using recursion . for a given sequence @xmath256 let @xmath257 be such that @xmath258 . @xmath259",
    "this means @xmath116 only changes its hypothesis if @xmath244 ensures that @xmath0 made a mind change and that the previous hypothesis does not contain something of the new input data .",
    "we first show that @xmath116 is conservative .",
    "let @xmath18 and @xmath257 be such that @xmath260 and let @xmath252 be such that @xmath261 .",
    "then we have , for all @xmath262 with @xmath263 , @xmath264 , \\text { which is equivalent to } \\\\ & \\exists \\rho   \\in ( w_{h(\\hat{\\sigma})}^t)^ * , |\\hat{\\sigma } \\diamond \\rho| \\leq t : h(\\hat{\\sigma } \\diamond \\rho ) \\neq h(\\hat{\\sigma});\\end{aligned}\\ ] ] as there is @xmath138 such that @xmath265 .",
    "therefore , we get @xmath266 , as @xmath267 is monotone non - decreasing in @xmath38 .",
    "thus , @xmath116 is conservative .",
    "second , we will show that @xmath116 converges on any text @xmath67 for a language @xmath189 .",
    "let @xmath189 and @xmath67 be a text for @xmath3 .",
    "thus , @xmath0 converges on @xmath67 .",
    "suppose @xmath116 does not converge on @xmath67 .",
    "let @xmath268 the corresponding sequence of hypotheses .",
    "then @xmath269 is a text for @xmath3 as for every @xmath270 , @xmath271 .",
    "as @xmath116 infinitely often changes its mind , we have that , for infinitely many @xmath272 , there is , for each @xmath68 , @xmath273 such that @xmath274 with @xmath275 holds . as @xmath275 means that @xmath276 , @xmath0 diverges on @xmath102 , a contradiction .",
    "third we will show that @xmath116 converges to a correct hypothesis .",
    "let @xmath18 be such that @xmath116 converges to @xmath277 on @xmath67 . in the following we consider two cases for this @xmath18 .",
    "_ case 1 : _ if @xmath18 is a locking sequence of @xmath0 on @xmath3 we have , for all @xmath155 , @xmath132 and especially for all @xmath278 with @xmath279 , @xmath280 .",
    "thus , @xmath281 .",
    "_ case 2 : _",
    "suppose @xmath18 is not a locking sequence .",
    "as @xmath79 and @xmath116 converges , we have for all @xmath122 and @xmath35 with @xmath282 $ ] , @xmath283 .",
    "this means that , for all @xmath35 with elements of @xmath3 and @xmath284 @xmath285 , i.e. @xmath286 we now show @xmath287 .",
    "if we have , for all @xmath155 , @xmath288 , we get this directly from equation  ( [ eq : notqsigmatau ] ) .",
    "otherwise , let @xmath35 be such that @xmath289 .",
    "let @xmath290 .",
    "thus , @xmath291 , as @xmath0 is syntactically decisive . from @xmath292 we can conclude that @xmath293 .",
    "therefore we have , for all @xmath290 , @xmath294 and thus @xmath295 .",
    "additionally we will show now that @xmath296 .",
    "obviously we have @xmath297 to show that @xmath298 suppose there is @xmath294 such that @xmath299 .",
    "then there is a minimal @xmath38 such that @xmath300 but there is @xmath301 such that @xmath302 and therefore @xmath303 as we have @xmath304 which is equivalent to @xmath305 and we supposed that @xmath306 it follows that @xmath307 this is a contradiction as @xmath308 thus , for all @xmath290 we have @xmath309 and from @xmath287 we get @xmath298 .",
    "\\(a ) @xmath310 we have that the learner must not change to a proper subset of a previous hypothesis and this means that @xmath311 .",
    "\\(b ) @xmath312 the learner @xmath0 never returns a hypothesis which is a proper superset of the language that is learned .",
    "thus @xmath311 .",
    "\\(c ) @xmath313 as @xmath0 must not change to a finite subset of a previous hypothesis , we suppose that @xmath314 and both @xmath315 and @xmath3 are infinite .",
    "this means there is a sequence @xmath316 such that @xmath317 .",
    "thus , @xmath318 is finite , a contradiction to @xmath315 being infinite .",
    "therefore we have @xmath311 .    from the definitions of the learning criteria we have @xmath180 \\subseteq [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{wmon}}{\\mathbf{ex}}]$ ] .",
    "using theorem  [ thm : cautvarconv ] and the equivalence of weakly monotone and conservative learning ( using @xmath5 ) @xcite , we get the following .",
    "[ cor : convwmoncaut ] we have @xmath319 = [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{wmon}}{\\mathbf{ex } } ] = [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{caut}}{\\mathbf{ex}}].\\ ] ]    using corollary  [ cor : convwmoncaut ] and theorem  [ thm : convinsdec ] we get that weakly monotone @xmath4-learning is included in strongly decisive @xmath4-learning .",
    "theorem  [ thm : convwmonint ] shows that this inclusion is proper .",
    "[ cor : wmoninsdec ] we have @xmath320 \\subset [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{sdec}}{\\mathbf{ex}}].\\ ] ]    the next theorem is the last theorem of this section and shows that forbidding to go down to strict _",
    "infinite _ subsets of previously conjectures sets is no restriction .",
    "[ thm : cautinft ] we have @xmath321 = [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{ex}}].\\ ] ]    obviously we have @xmath322 \\subseteq [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{ex}}]$ ] .",
    "thus , we have to show that @xmath323 \\subseteq [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{caut}}_\\mathbf{\\infty}{\\mathbf{ex}}]$ ] .",
    "let @xmath7 be a set of languages and @xmath0 be a learner such that @xmath0 @xmath4-learns @xmath7 and @xmath0 is strongly locking on @xmath7 ( see corollary  [ cor : sinklocking ] ) .",
    "we define , for all @xmath18 and @xmath38 , the set @xmath324 such that @xmath325 using the s - m - n theorem we get a function @xmath249 such that @xmath326 we define a learner @xmath116 as @xmath327 we will show now that the learner @xmath116 @xmath328-learns @xmath7 . let an @xmath120 and a text @xmath67 for @xmath3 be given . as @xmath0 is strongly locking there is @xmath151 such that for all @xmath155 , @xmath329 \\diamond \\tau ) = h(t[n_0])$ ] and @xmath330 ) } = l$ ] .",
    "thus we have , for all @xmath152 , @xmath123 ) = h'(t[n_0])$ ] and @xmath331 ) } = w_{p(t[n_0 ] ) } = w_{h(t[n_0 ] ) } = l$ ] . to show that the learning restriction @xmath332 holds , we assume that there are @xmath333 such that @xmath334 ) } \\subset w_{h'(t[i])}$ ] and @xmath334)}$ ] is infinite . w.l.o.g .",
    "@xmath203 is the first time that @xmath116 returns the hypothesis @xmath334)}$ ] .",
    "let @xmath35 be such that @xmath69 \\diamond \\tau = t[j]$ ] . from the definition of the function",
    "@xmath43 we get that @xmath206 ) \\subseteq w_{h'(t[j ] ) } \\subseteq w_{h'(t[i])}$ ] .",
    "thus , @xmath335 ) } = w_{p(t[i])}$ ] and therefore @xmath336)}$ ] is finite , a contradiction to the assumption that @xmath334)}$ ] is infinite .",
    "[ sec : decisiveness ]    in this section the goal is to show that decisive and strongly decisive learning separate ( see theorem  [ thm : stronglydecisivelearning ] ) . for this proof",
    "we adapt a technique known in computability theory as a `` priority argument '' ( note , though , that we are not dealing with oracle computations ) . in order to illustrate the proof with a simpler version , we first reprove that decisiveness is a restriction to @xmath4-learning ( as shown in @xcite ) .    for both proofs",
    "we need the following lemma , a variant of which is given in @xcite for the case of decisive learning ; it is easy to see that the proof from @xcite also works for the cases we consider here .",
    "[ lem : notnatnum ] let @xmath7 be such that @xmath337 and , for each finite set @xmath226 , there are only finitely many @xmath120 with @xmath338 .",
    "let @xmath339 . then , if @xmath7 is @xmath176-learnable , it is so learnable by a learner which never outputs an index for @xmath20",
    ".    now we get to the theorem regarding decisiveness .",
    "its proof is an adaptation of the proof given in @xcite , rephrased as a priority argument .",
    "this rephrased version will be modified later to prove the separation of decisive and strongly decisive learning .",
    "[ thm : decisivelearning ] we have @xmath340 \\subset [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{ex}}].\\ ] ]    for this proof we will employ a technique from computability theory known as _ priority argument_. for this technique , one has a set of _ requirements _ ( we will have one for each @xmath341 ) and a _ priority _ on requirements ( we will prioritize smaller @xmath19 over larger ) .",
    "one then tries to fulfill requirements one after the other in an iterative manner ( fulfilling the unfulfilled requirement of highest priority without violating requirements of higher priority ) so that , in the limit , the entire infinite list of requirements will be fulfilled .",
    "we apply this technique in order to construct a learner @xmath77 ( and a corresponding set of learned sets @xmath342 ) .",
    "thus , we will give requirements which will depend on the @xmath0 to be constructed .",
    "in particular , we will use a list of requirement @xmath343 , where lower @xmath19 have higher priority . for each @xmath19",
    ", @xmath344 will correspond to the fact that learner @xmath345 is not a suitable decisive learner for @xmath7 .",
    "we proceed with the formal argument .    for each @xmath19 ,",
    "let requirement @xmath344 be the disjunction of the following three predicates depending on the @xmath0 to be constructed .",
    "a.   @xmath346 : @xmath347 and @xmath0 learns @xmath348 .",
    "b.   @xmath349 and @xmath0 learns @xmath350 and some @xmath226 with @xmath351 . c.   @xmath352 .",
    "if all @xmath343 hold , then every learner which never outputs an index for @xmath20 fails to learn @xmath7 decisively as follows . for each learner",
    "@xmath345 which never outputs an index for @xmath20 , either ( i ) of @xmath344 holds , implying that some co - singleton is learned by @xmath0 but not by @xmath345 . or ( ii ) holds , then there is a @xmath18 on which @xmath345 generalizes , but will later have to abandon this correct conjecture @xmath353 in order to learn some finite set @xmath226 ; as , after the change to a hypothesis for @xmath226 , the text can still be extended to a text for @xmath236 , the learner is not decisive .",
    "however , the price for avoiding it is to output a conjecture for @xmath20 . ]",
    "thus , all that remains is to construct @xmath0 in a way that all of @xmath343 are fulfilled . in order to coordinate the different requirements when constructing @xmath0 on different inputs , we will divide the set of all possible input sequences into infinitely many segments ,",
    "of which every requirement can `` claim '' up to two at any point of the algorithm defining @xmath0 ; the chosen segments can change over the course of the construction , and requirements of higher priority might `` take away '' segments from requirements with lower priority ( but not vice versa ) .",
    "we follow  @xcite with the division of segments : for any set @xmath354 we let @xmath355 be the _ i d of @xmath356 _ ; for ease of notation , for each finite sequence @xmath18 , we let @xmath357 . for each @xmath358 ,",
    "the @xmath358th segment contains all @xmath18 with @xmath359 .",
    "we note that @xmath360 is _ monotone _ ,",
    "i.e. @xmath361 the first way of ensuring some requirement @xmath344 is via ( i ) ; as this part itself is not decidable , we will check a `` bounded '' version thereof .",
    "we define , for all @xmath362 , @xmath363 for any @xmath19 , if we can find an @xmath358 such that , for all @xmath38 , we have @xmath364 , then it suffices to make @xmath0 learn @xmath365 in order to fulfill @xmath344 via part ( i ) ; this requires control over segment @xmath358 in defining @xmath0 .",
    "note that , if we ever can not take control over some segment because some requirement with higher priority is already in control , then we will try out different @xmath358 ( only finitely many are blocked ) .",
    "if we ever find a @xmath38 such that @xmath366 , then we can work on fulfilling @xmath344 via ( ii ) , as we directly get a @xmath18 where @xmath345 over the content generalizes . in order to fulfill @xmath344 via ( ii ) we have to choose a finite set @xmath226 with @xmath351 .",
    "we will then take control over the segments corresponding to @xmath367 and @xmath368 ( for growing @xmath38 ) , _ but not necessarily over segment @xmath358 _ , and thus establish @xmath344 via ( ii ) . note that , again , the segments we desire might be blocked ; but only finitely many are blocked , and we require control over @xmath367 and @xmath368 , both of which are at least @xmath358 ( this follows from @xmath360 being monotone , see equation  ( [ eq : idmonotone ] ) , and from @xmath369 ) ; thus , we can always find an @xmath358 for which we can either follow our strategy for ( i ) or for ( ii ) as just described .",
    "it is tempting to choose simply @xmath370 , this fulfills all desired properties .",
    "the main danger now comes from the possibility of @xmath371 being an index for @xmath20 : this will imply that , for growing @xmath38 , @xmath372 will also be growing indefinitely .",
    "of course , there is no problem with satisfying @xmath344 , it now holds via ( iii ) ; but as soon as at least two requirements will take control over segments @xmath373 for indefinitely growing @xmath373 , they might start blocking each other ( more precisely , the requirement of higher priority will block the one of lower priority ) .",
    "we now need to know something about our later analysis : we will want to make sure that every requirement @xmath344 either ( a ) converges in which segments to control or ( b ) for all @xmath122 , there is a time @xmath38 in the definition of @xmath0 after which @xmath344 will never have control over any segment corresponding to ids @xmath374 ; in fact , we will show this later by induction ( see claim  [ claim : inductionproof ] ) . any requirement which takes control over segments @xmath373 for indefinitely growing @xmath373",
    "might be blocked infinitely often , and thus forced to try out different @xmath358 for fulfilling @xmath344 , including returning to @xmath358 that were abandoned previously because of ( back then ) being blocked by a requirement of higher priority .",
    "thus , such a requirement would fulfill neither ( a ) nor ( b ) from above .",
    "we will avoid this problem by _ not _ choosing @xmath370 , but instead choosing a @xmath226 which grows in i d along with the corresponding @xmath375 .",
    "the idea is to start with @xmath370 and then , as @xmath375 grows , add more elements . for this",
    "we make some definitions as follows .    for a finite sequence @xmath18 we let @xmath376 be the least element not in @xmath140 which is larger than all elements of @xmath140 . for any finite sequence @xmath18 and @xmath377",
    "we let @xmath378 be such that @xmath379 for all @xmath380 and @xmath18 with @xmath381 we have @xmath382 thus , we will use the sets @xmath378 to satisfy ( ii ) of @xmath344 ( in place of @xmath226 ) .    we now have all parts that are required to start giving the construction for @xmath0 . in that construction",
    "we will make use of a subroutine which takes as inputs a set @xmath383 of blocked indices , a requirement @xmath19 and a time bound @xmath38 , and which finds triples @xmath384 with @xmath385 such that @xmath386.\\ ] ] we call @xmath384 fulfilling equation  ( [ eq : defntwitness ] ) for given @xmath38 and @xmath19 a _ @xmath38-witness for @xmath344_. the subroutine is called ` findwitness `  and is given in algorithm  [ alg : priorityargumentdecsubroutine ] .",
    "` error `    we now formally show termination and correctness of our subroutine .",
    "let @xmath380 and a finite set @xmath383 be given .",
    "the algorithm ` findwitness `  on @xmath387 terminates and returns a @xmath38-witness @xmath384 for @xmath344 such that @xmath385 .    from the condition in line  [ line : pcondition ]",
    "we see that the search in line  [ line : sigmasearch ] is necessarily successful , showing termination . using the monotonicity of @xmath360 from equation  ( [ eq : idmonotone ] ) on equation  ( [ eq : defdtsigma ] ) we have that the subroutine ` findwitness `  can not return ` error ` on any arguments @xmath387 : for @xmath388 , we either have @xmath364 or the @xmath30 and @xmath373 chosen are larger than @xmath389 .    with the subroutine given above",
    ", we now turn to the priority construction for defining @xmath0 detailed in algorithm  [ alg : priorityargumentdec ] .",
    "this algorithm assigns witness tuples to more and more requirements , trying to make sure that they are @xmath38-witnesses , for larger and larger @xmath38 . for each @xmath19",
    ", @xmath390 will be the witness tuple associated with @xmath344 after @xmath38 iterations ( defined for all @xmath391 ) .",
    "we say that a requirement @xmath344 _ blocks _ an i d @xmath122 iff @xmath392 for the witness tuple @xmath393 currently associated with @xmath344 .",
    "we say that a tuple @xmath384 is _",
    "@xmath394-legal _ iff it is a @xmath38-witness for @xmath344 and @xmath30 and @xmath373 are not blocked by any @xmath395 with @xmath396 .",
    "clearly , it is decidable whether a triple is @xmath394-legal .    in order to define the learner @xmath0 we will need some functions giving us indices for the languages to be learned . to that end ,",
    "let @xmath397 ( using the s - m - n theorem ) be such that @xmath398 to increase readability , we allow assignments to values of @xmath0 for arguments on which @xmath0 was already defined previously ; in this case , the new assignment has no effect",
    ".    regarding algorithm  [ alg : priorityargumentdec ] , note that lines 38 make sure that we have an appropriate witness tuple .",
    "we will later show that the sequence of assigned witness tuples will converge ( for learners never giving a conjecture for @xmath20 ) .",
    "lines 911 will try to establish the requirement @xmath344 via ( i ) , once this fails it will be established in lines 1216 via ( ii ) .    after this construction of @xmath0 , we let @xmath342 be the target to be learned . first note that the ids blocked by different requirements are always disjoint ( at the end of an iteration of @xmath38 ) . as the major part of the analysis , we show the following claim by induction , showing that , for each @xmath19 , either the triple associated with @xmath344 converges or it grows arbitrarily in both its @xmath30 and @xmath373 value ( this is what we earlier had to carefully choose the @xmath226 for ) .    [",
    "claim : inductionproof ] for all @xmath19 we have @xmath344 and , for all @xmath122 , there is @xmath399 such that either @xmath400 or @xmath401    as our induction hypothesis , let @xmath19 be given such that the claim holds for all @xmath396 .",
    "case 1 : there is @xmath399 such that @xmath402 .",
    "+ then , for all @xmath38 , @xmath403 is a @xmath38-witness for @xmath344 ; in the case of @xmath404 , we have that , for all but finitely many @xmath35 with @xmath405 , @xmath406 , and index for @xmath348 ; this implies @xmath407 , which shows @xmath344 .",
    "otherwise we have , for all @xmath408 , @xmath409 .",
    "furthermore we get , for all but finitely many @xmath35 with @xmath410 , @xmath411 , and index for @xmath412 ; this implies @xmath413 .",
    "consider now all those @xmath35 with @xmath414 .",
    "if @xmath415 , then @xmath0 is already be defined on infinitely many such @xmath35 , namely in case of @xmath410 .",
    "however , we have that @xmath412 is a _ proper _ subset of @xmath350 , which shows that , on any text for @xmath350 , @xmath0 will eventually only output @xmath371 , which gives @xmath416 as desired and , thus , @xmath344 .",
    "case 2 : otherwise .",
    "+ for each i d @xmath358 there exists at most finitely many @xmath18 with @xmath359 and @xmath18 is used in the witness triple for @xmath344 ; this follows from the choice of @xmath18 in the subroutine ` findwitness `  as a minimum , where , for larger @xmath38 , all previously considered @xmath18 are still considered ( so that the chosen minimum might be smaller for larger @xmath38 , but never go up , which shows convergence ) .",
    "a triple is only abandoned if it is not legal any more ; this means it is either blocked or it is not a @xmath38-witness triple for some @xmath38 . using the induction hypothesis , the first can only happen finitely many times for any given tuple ; the second implies the desired increase in both the @xmath30 and the @xmath373 value of the witness tuple .",
    "for this we also use our specific choice of @xmath226 as growing along with the i d of the associated @xmath375 and we use that any witness tuple with a @xmath18 with @xmath359 has @xmath30 and @xmath373 value of at least @xmath358 , due to the monotonicity of @xmath360 .    to show @xmath344 ( we will show ( 3 ) ) , let @xmath417 be the maximum over all @xmath399 existing for the converging @xmath396 by the induction hypothesis and @xmath19 . let @xmath418 be the @xmath417-witness triple chosen for @xmath344 in iteration @xmath417 .",
    "suppose , by way of contradiction , that @xmath419 is not an index for @xmath20 ; let @xmath420 .",
    "let @xmath421 be the maximum over all @xmath399 found by the induction hypothesis for all @xmath396 with the chosen @xmath122 .",
    "since the triple @xmath384 is @xmath394-legal for all @xmath422 , we get a contradiction to the unbounded growth of the witness triple .",
    "this shows that @xmath419 is an index for @xmath20 , and thus we have @xmath344 .    with the last claim",
    "we now see that all requirement are satisfied .",
    "this implies that @xmath7 can not be @xmath423-learned by a learner never using an index for @xmath20 as conjecture .",
    "we have that @xmath337 .",
    "furthermore , for any i d @xmath358 , there are only finitely many sets in @xmath7 with that i d ; this implies that , for every finite set @xmath226 , there are only finitely many elements @xmath120 with @xmath338 .",
    "thus , using lemma  [ lem : notnatnum ] , @xmath7 is not decisively learnable at all .",
    "while the previous theorem showed that decisiveness poses a restriction on @xmath4-learning , the next theorem shows that the requirement of strong decisiveness is even more restrictive .",
    "the proof follows the proof of theorem  [ thm : decisivelearning ] , with some modifications .",
    "[ thm : stronglydecisivelearning ] we have @xmath424 \\subset [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{dec}}{\\mathbf{ex}}].\\ ] ]    we use the same language and definitions as in the proof of theorem  [ thm : decisivelearning ] .",
    "the idea of this proof is as follows .",
    "we build a set @xmath7 with a priority construction just as in the proof of theorem  [ thm : decisivelearning ] , the only essential change being in the definition of the hypothesis @xmath425 : the change from @xmath371 to @xmath425 and back to @xmath371 on texts for @xmath350 is what made @xmath7 not decisively learnable .",
    "thus , we will change @xmath425 to be a hypothesis for @xmath350 as well  _ as soon as @xmath345 changed its hypothesis on an extension of @xmath18 _ , and otherwise it is a hypothesis for @xmath426 as before .",
    "this will make @xmath0 decisive on texts for @xmath350 , but @xmath371 will not be strongly decisive .    furthermore , we will make sure that for sequences with i d @xmath358 , only conjectures for sets with i d @xmath358 are used , so that indecisiveness can only possibly happen within a segment",
    ". now the last source of @xmath7 not being decisively learnable is as follows .",
    "when different requirements take turns with being in control over the segment , they might introduce returns to abandoned conjectures . to counteract this ,",
    "we make sure that any conjecture which is ever abandoned on a segment of i d @xmath358 is for @xmath365 , which will give decisiveness .",
    "we first define an alternative @xmath427 for the function @xmath43 from that proof with the s - m - n theorem such that , for all @xmath428 , @xmath429 as we have @xmath430 , this is a valid application of the s - m - n theorem .",
    "we also want to replace the output of @xmath0 according to line  [ line : folowe ] of algorithm  [ alg : priorityargumentdec ] .",
    "to that end , let @xmath431 be as given by the s - m - n theorem such that , for all @xmath19 and @xmath18 , @xmath432    we construct now a learner @xmath0 again according to a priority construction , as given in algorithm  [ alg : priorityargumentsdec ] .",
    "note that lines 1[line : elseline ] are identical with the construction from algorithm  [ alg : priorityargumentdec ] and lines 38 again make sure that we have an appropriate witness tuple and lines 911 try to establish the requirement @xmath344 via ( i ) .",
    "the main difference lies in the way that @xmath344 is established once this fails in lines 1218 via ( ii ) : here we need to check for a mind change and adjust what language @xmath0 should learn accordingly .",
    "it is easy to check that @xmath0 , on any sequence @xmath18 , gives conjectures for languages of the same i d as that of @xmath18 .",
    "thus , indecisiveness of @xmath0 can only occur within a segment .",
    "next we will modify @xmath0 to avoid indecisiveness from different requirements taking turns controlling the same segment .    with the s - m - n theorem",
    "we let @xmath433 be such that , for all @xmath18 , @xmath434 let @xmath116 be such that , for all @xmath18 , @xmath435 we now let @xmath436 .",
    "it is easy to see that @xmath116 is decisive on all texts where it always makes an output , since indecisiveness can again only happen within a segment , and @xmath40 _ poisons _ any possible non - final conjectures within a segment .",
    "let a strongly decisive learner @xmath437 for @xmath7 be given which never makes a conjecture for @xmath20 ( we are reasoning with lemma  [ lem : notnatnum ] again ) .",
    "let @xmath19 be such that @xmath438 .",
    "reasoning as in the proof of theorem  [ thm : decisivelearning ] , we see that there is a triple @xmath384 such that @xmath439 converges to that triple in the construction of @xmath116 .",
    "if , for all @xmath38 , @xmath440 , then we have that @xmath407 ( on any sequences with i d @xmath30 , @xmath116 gives an output for @xmath348 , and it converges ) .",
    "assume now that there is @xmath399 such that , for all @xmath408 , we have @xmath441 .    case 1 : there is @xmath35 with @xmath442 such that @xmath443 .",
    "+ let @xmath67 be a text for @xmath444 .",
    "then @xmath116 on @xmath67 converges to an index for @xmath3 , giving @xmath120 . but",
    "this shows that @xmath445 was not strongly decisive on any text for @xmath3 starting with @xmath446 , a contradiction .",
    "case 2 : otherwise .",
    "+ let @xmath67 be a text for @xmath447 .",
    "then @xmath116 on @xmath67 converges to an index for @xmath3 , giving @xmath120 . but",
    "@xmath445 converges on any text for @xmath3 starting with @xmath18 to @xmath371 , a contradiction to @xmath448 ( so the convergence is not to a correct hypothesis ) .    in both cases",
    "we get the desired contradiction .",
    "[ sec : setdriven ]    in this section we give theorems regarding set - driven learning . for this",
    "we build on the result that set - driven learning can always be done conservatively @xcite .",
    "first we show that any conservative set - driven learner can be assumed to be cautious and syntactically decisive , an important technical lemma .",
    "[ thm : sdsyntdec ] we have @xmath449 = [ { \\mathbf{txt}^{}}{\\mathbf{sd}}{\\mathbf{conv}}{\\mathbf{syndec}}{\\mathbf{ex}}].\\ ] ] in other words , every set - driven learner can be assumed syntactically decisive .",
    "let a set - driven learner @xmath0 be given .",
    "following @xcite we can @xmath0 assume to be conservative .",
    "we define a learner @xmath116 such that , for all finite sets @xmath450 , @xmath451 let @xmath452 .",
    "we will show that @xmath116 is syntactically decisive and @xmath453-learns @xmath7 .",
    "let @xmath120 be given and let @xmath67 be a text for @xmath3 .",
    "first , we show that @xmath116 @xmath454-learns @xmath3 from @xmath67 . as @xmath0 is a set driven learner",
    "there is @xmath151 such that @xmath455 ) ) = h({\\mathrm{content}}(t[n]))$ ] and @xmath456 ) ) } = l$ ] .",
    "we will show that , for all @xmath134 $ ] with @xmath152 , the first condition in the definition of @xmath116 holds .",
    "let @xmath152 and suppose there are @xmath226 and @xmath457 with @xmath458 ) , \\\\",
    "h(d ) & = h({\\mathrm{content}}(t[n ] ) ) = h({\\mathrm{content}}(t[n_0]))\\end{aligned}\\ ] ] and @xmath459 ) , \\\\ h(d ) & \\neq h(d').\\end{aligned}\\ ] ] as @xmath460 and @xmath0 is conservative , @xmath0 must not change its hypothesis .",
    "thus , for all @xmath457 with @xmath461 we get @xmath462 , a contradiction .",
    "thus we have , for all @xmath152 , @xmath463 ) ) & = h'({\\mathrm{content}}(t[n_0 ] ) ) \\\\ & = { \\mathrm{pad}}(h({\\mathrm{content}}(t[n_0])),0)\\end{aligned}\\ ] ] and @xmath464 ) ) } = w_{{\\mathrm{pad}}(h({\\mathrm{content}}(t[n_0])),0 ) } = l$ ] , i.e.  @xmath116 @xmath4-learns @xmath3 .",
    "second , we will show that @xmath116 is conservative .",
    "whenever @xmath0 makes a mind change , @xmath116 will also make a mind change ; as , for all @xmath122 , @xmath465 ) ) } = w_{h'({\\mathrm{content}}(t[n]))}$ ] , we have that @xmath116 is conservative in these cases .",
    "thus , we have to show that @xmath116 is conservative whenever it changes its mind because the first condition in the definition does not hold .",
    "let @xmath122 such that @xmath466 ) ) \\neq h'({\\mathrm{content}}(t[n-1]))\\ ] ] because the first condition in the definition of @xmath116 is violated .",
    "let @xmath467)$ ] . thus , there are @xmath226 and @xmath457 with @xmath468 such that @xmath469 and @xmath470 .",
    "we consider the case that @xmath471 ) = h(t[n-1])$ ] as otherwise @xmath116 is obviously conservative . as @xmath0 is conservative we can conclude that there is @xmath472 such that @xmath473 .",
    "if not we could construct a text @xmath102 with elements of @xmath226 on which @xmath0 would not be conservative . thus there is @xmath474 such that @xmath475 ) } = w_{h(t[n-1 ] ) } = w_{h'(t[n-1])}\\ ] ] and therefore @xmath116 is still conservative if it changes its mind .    to show that @xmath116 is syntactically decisive let @xmath476 such that @xmath477 and @xmath478 .",
    "this implies that @xmath479 .",
    "thus @xmath480 and therefore the second component in @xmath481 is different for @xmath450 and @xmath482 .",
    "this implies that @xmath483 as @xmath481 is injective .",
    "the following theorem is the main result of this section , showing that set - driven learning can be done not just conservatively , but also strongly decisively and cautiously _ at the same time_.    [ thm : sdconvcautsdec ] we have @xmath449 = [ { \\mathbf{txt}^{}}{\\mathbf{sd}}{\\mathbf{conv}}{\\mathbf{sdec}}{\\mathbf{caut}}{\\mathbf{ex}}].\\ ] ]    following @xcite we can assume a set - driven learner to be conservative .",
    "let @xmath0 and @xmath185 be such that @xmath0 * txtsdconvex*-learns @xmath185 and suppose that @xmath0 is syntactically decisive using lemma [ thm : sdsyntdec ] .",
    "we define a function @xmath43 using the s - m - n theorem such that , for every set @xmath226 and @xmath19 , @xmath484 we define a function @xmath485 such that , for any finite set @xmath226 , @xmath486 we define @xmath116 , for all finite sets @xmath226 , as @xmath487 let @xmath189 be given and let @xmath67 be a text for @xmath3 .",
    "we first show that @xmath116 @xmath488-learns @xmath3 from @xmath67 . as @xmath0 * txtsdex*-learns @xmath3 we know that @xmath0 is strongly locking on @xmath67 ( this was shown in  @xcite ) .",
    "thus there is @xmath151 such that @xmath489 $ ] is a locking sequence .",
    "let @xmath490)$ ] be minimal with @xmath491))$ ] .",
    "thus we have , for all @xmath152 , @xmath492 ) ) ) = d'$ ] . from the construction of @xmath43 and @xmath0",
    "syntactically decisive we get @xmath493 this shows that @xmath116 @xmath488-learns @xmath3 .",
    "next we show the following claim .",
    "[ claim : syndecconcl ] @xmath494    as @xmath0 is syntactically decisive we have that , for all @xmath495 with @xmath496 , @xmath497 therefore we get @xmath498 suppose , by way of contradiction , @xmath499 .",
    "this implies that there is @xmath38 such that @xmath500 with @xmath501 , according to the definitions of @xmath116 and @xmath43 .",
    "but , as @xmath502 , this is a contradiction to @xmath0 being syntactically decisive .",
    "let @xmath503 be such that @xmath504 ) ) \\neq h'({\\mathrm{content}}(t[j]))$ ] . to increase readability we let @xmath505)$ ] and @xmath506)$ ] . as @xmath0 is syntactically decisive , @xmath116 only changes its mind if @xmath0 changed its mind before .",
    "thus we have @xmath507 as @xmath508 and @xmath509 we get from claim  [ claim : syndecconcl ] ( with @xmath510 and @xmath511 ) that @xmath512 this shows that @xmath116 is conservative .",
    "we will now show that @xmath513 as this implies that @xmath116 is cautious and strongly decisive .    from the construction of @xmath116",
    "we get that there is @xmath514 with @xmath515 such that @xmath116 is consistent on @xmath383 , i.e.  @xmath516 using claim  [ claim : syndecconcl ] again ( this time with @xmath517 , @xmath518 and @xmath511 ) , we see that there is @xmath519 which shows that @xmath520 .",
    "[ sec : monotone ]    in this section we show the hierarchies regarding monotone and strongly monotone learning , simultaneously for the settings of @xmath5 and @xmath17 in theorems  [ thm : smon ] and  [ thm : wmonnotmon ] . with theorems  [ thm : natnumsdec ] and",
    "[ thm : moninsdec ] we establish that monotone learnabilty implies strongly decisive learnability .",
    "this is a standard proof which we include for completeness .",
    "let @xmath524 and @xmath525 .",
    "let @xmath19 such that @xmath526 and @xmath43 using the s - m - n theorem such that , for all @xmath527 , @xmath528 we first show that @xmath7 is @xmath521-learnable .",
    "we let a learner @xmath0 such that , for all @xmath18 , @xmath529 let @xmath530 and @xmath67 be a text for @xmath531 .",
    "thus , there is @xmath151 such that @xmath532 and any element in @xmath533)$ ] is even .",
    "then , we have , for all @xmath152 , @xmath534 ) ) = h({\\mathrm{content}}(t[n]))$ ] and @xmath535 ) } = w_{p(k ) } = l_k$ ] .",
    "it is easy to see that @xmath0 makes exactly one mind change on @xmath67 and this is at @xmath151 .",
    "we have @xmath536 is a subset of @xmath537 as @xmath538 .",
    "thus @xmath0 is monotone .",
    "additionally @xmath0 is weakly monotone as it change its mind only if the first time a odd element is presented in the text and the previous hypotheses are @xmath539 .    now , suppose that there is @xmath174 and @xmath116 @xmath522-learns @xmath7 .",
    "let @xmath18 be a locking sequence of @xmath116 on @xmath539 and @xmath527 such that , for all @xmath540 .",
    "we let @xmath67 be a text for @xmath531 starting with @xmath18 .",
    "as @xmath541 we have that @xmath116 is not strongly monotone on @xmath67 or @xmath0 does not @xmath4-learns @xmath531 from @xmath67 .",
    "this is a standard proof which we include for completeness .",
    "let @xmath544 and @xmath545 .",
    "let @xmath19 such that @xmath526 and @xmath43 using the s - m - n theorem such that , for all @xmath527 , @xmath528 we define , for all @xmath18 , a learner @xmath0 such that @xmath546 let @xmath530 and a @xmath67 be a text for @xmath531 .",
    "then , there is @xmath151 such that @xmath547)$ ] for the first time .",
    "thus we have that for all @xmath548 ) ) = h({\\mathrm{content}}(t[n]))$ ] and @xmath456 ) ) } = w_{p(k ) } = l_k$ ] .",
    "obviously @xmath0 learns @xmath531 weakly mononote as the learner only change its mind if a greater odd element appears in the text .",
    "suppose now there is a learner @xmath174 such that @xmath116 @xmath543-learns @xmath7 . let @xmath18 be a locking sequence of @xmath116 on @xmath539 and @xmath527 such that , for all @xmath549 , @xmath550 .",
    "let @xmath551 a locking sequence of @xmath116 on @xmath552 and @xmath67 be a text for @xmath553 starting with @xmath554 .",
    "let @xmath555 be a locking sequence of @xmath116 on @xmath553 .",
    "then , we have @xmath556 as the datum @xmath557 is in @xmath539 and in @xmath553 but not in @xmath531 , @xmath116 is not monotone on the text @xmath67 for @xmath553 .",
    "let @xmath0 be a learner in fulk normal form such that @xmath0 @xmath4-learns @xmath7 with @xmath558 . as @xmath0 is strongly locking on @xmath7",
    "there is a locking sequence of @xmath0 on @xmath20 . using this locking sequence we get an uniformly enumerable sequence @xmath560 of languages such that ,          we will use the @xmath277 as hypotheses .",
    "note that any hypothesis @xmath277 is either semantically equivalent to @xmath131 or , if @xmath18 is not a locking sequence of @xmath0 for any language , @xmath277 is an index for a finite superset of @xmath573 . in the latter case",
    "we call the hypothesis @xmath277 _",
    "poisoned_.      let @xmath120 and @xmath67 be a text for @xmath3 .",
    "as @xmath0 is strongly locking and @xmath0 @xmath4-learns @xmath7 there is @xmath151 such that , for all @xmath575 , @xmath329 ) = h(t[n_0 ] \\diamond \\sigma)$ ] and @xmath330 ) } = l$ ] .",
    "thus , there is @xmath576 such that , for all @xmath577)$ ] , @xmath578)}(x ) \\leq n_1 $ ] .",
    "this implies that , for all @xmath579 , @xmath580 ) = h'(t[n])$ ] and @xmath581 ) } = w_{p(\\max(m(t[n_1 ] ) ) ) } = \\bigcup_{t \\in \\mathbb{n } } w_{h(t[n_0])}^t = l.\\ ] ]    next , we will show that @xmath116 is strongly decisive .",
    "suppose there are @xmath582 such that @xmath583 ) } = w_{h'(t[k])}$ ] and @xmath584 ) \\neq h'(t[j])$ ] . from the construction of the learner @xmath116 we get @xmath585 )",
    "\\neq h(t[j])$ ] .",
    "_ case 1 : _",
    "@xmath584)$ ] is _ not _ a poisoned hypothesis . independently of whether @xmath586)$ ]",
    "is poisoned or not , there is @xmath587 $ ] such that @xmath588)}$ ] .",
    "( @xmath589 $ ] if the hypothesis is poisend , @xmath590))$ ] otherwise . ) as @xmath584)$ ] is not poisened and @xmath585 ) \\neq h(t[k])$ ] we get through the construction of @xmath43 that @xmath591)}$ ] .",
    "thus , we have @xmath583 ) } \\neq w_{h'(t[k])}$ ] , a contradiction .",
    "_ case 2.1 : _",
    "@xmath586)$ ] is _ not _ poisoned .",
    "thus , @xmath589 $ ] is a locking sequence on @xmath0 for a language @xmath592 and @xmath593 ) } \\in { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{ex}}(h)$ ] .",
    "as @xmath584)$ ] is poisoned we have @xmath583 ) } \\notin { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{ex}}(h)$ ] .",
    "thus , we get @xmath583 ) } \\neq w_{h'(t[k])}$ ] , a contradiction .",
    "[ thm : moninsdec ] we have that any monotone @xmath4-learnable class of languages is strongly decisive learnable , while the converse does not hold , i.e. @xmath596 \\subset [ { \\mathbf{txt}^{}}{\\mathbf{g}}{\\mathbf{sdec}}{\\mathbf{ex}}].\\ ] ]      _ case 1 : _",
    "@xmath7 is dense .",
    "we will show now that @xmath0 @xmath522-learns the class @xmath7 . let @xmath120 and @xmath67 be a text for @xmath3 .",
    "suppose there are @xmath68 and @xmath203 with @xmath333 such that @xmath598 ) } \\nsubseteq w_{h(t[j])}$ ] .",
    "thus , we have @xmath598)}\\backslash w_{h(t[j ] ) } \\neq \\emptyset$ ] .",
    "let @xmath599)}\\backslash w_{h(t[j])}$ ] . as @xmath7 is dense",
    "there is a language @xmath600 such that @xmath206 ) \\cup \\{x\\ } \\in l'$ ] .",
    "let @xmath102 be a text for @xmath601 and @xmath602 be such that @xmath603 \\diamond t'$ ] .",
    "obviously , @xmath602 is a text for @xmath601 .",
    "we have that @xmath604)}$ ] but @xmath605)}$ ] which is a contradiction as @xmath0 is monotone .",
    "thus , @xmath0 @xmath522-learns @xmath7 , which implies that @xmath0 @xmath606-learns @xmath7 . using corollary  [ cor : wmoninsdec ]",
    "we get that @xmath7 is @xmath559-learnable .",
    "k.  jantke .",
    "monotonic and non - monotonic inductive inference of functions and patterns . in j.  dix ,",
    "k.  jantke , and p.  schmitt , editors , _ nonmonotonic and inductive logic _ ,",
    "volume 543 of _ lecture notes in computer science _ , pages 161177 ."
  ],
  "abstract_text": [
    "<S> we investigate how different learning restrictions reduce learning power and how the different restrictions relate to one another . </S>",
    "<S> we give a complete map for nine different restrictions both for the cases of complete information learning and set - driven learning . </S>",
    "<S> this completes the picture for these well - studied _ delayable _ learning restrictions . </S>",
    "<S> a further insight is gained by different characterizations of _ conservative _ learning in terms of variants of _ cautious _ learning .    </S>",
    "<S> our analyses greatly benefit from general theorems we give , for example showing that learners with exclusively delayable restrictions can always be assumed total . </S>"
  ]
}