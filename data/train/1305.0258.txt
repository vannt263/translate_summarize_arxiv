{
  "article_text": [
    "the construction of parametrizations of low dimensional data in high dimension is an area of intense research ( e.g. , @xcite ) .",
    "a major limitation of these methods is that they are only defined on a discrete set of data . as a result ,",
    "the inverse mapping is also only defined on the data .",
    "there are well known strategies to extend the forward map to new points  for example , the nystrm extension is a common approach to solve this _ out - of - sample extension _ problem ( see e.g. , @xcite and references therein ) .",
    "however , the problem of extending the inverse map ( i.e. the _ preimage problem _ ) has received little attention so far ( but see @xcite ) .",
    "the nature of the preimage problem precludes application of the nystrm extension , since it does not involve extension of eigenvectors .",
    "we present a method to numerically invert a general smooth bi - lipschitz nonlinear dimensionality reduction mapping over all points in the image of the forward map .",
    "the method relies on interpolation via radial basis functions of the coordinate functions that parametrize the manifold in high dimension .",
    "the contributions of this paper are twofold .",
    "primarily , this paper addresses a fundamental problem for the analysis of datasets : given the construction of an adaptive parametrization of the data in terms of a small number of coordinates , how does one synthesize new data using new values of the coordinates ? we provide a simple and elegant solution to solve the `` preimage problem '' .",
    "our approach is scale - free and numerically stable and can be applied to any nonlinear dimension reduction technique .",
    "the second contribution is a novel interpretation of the nystrm extension as a properly rescaled radial basis function interpolant . a precise analysis of this similarity yields a critique of the nystrm extension , as well as suggestions for improvement .",
    "we consider a finite set of @xmath0 datapoints @xmath1 that lie on a bounded low - dimensional smooth manifold @xmath2 , and we assume that a nonlinear mapping has been defined for each point @xmath3 , @xmath4 we further assume that the map @xmath5 converges toward a limiting continuous function , @xmath6 , when the number of samples goes to infinity .",
    "such limiting maps exist for algorithms such as the laplacian eigenmaps @xcite .    in practice ,",
    "the construction of the map @xmath5 is usually only the first step . indeed , one is often interested in exploring the configuration space in @xmath7 , and one needs an inverse map to synthesize a new measurement @xmath8 for a new configuration @xmath9 in the coordinate domain ( see e.g. , @xcite ) . in other words , we would like to define an inverse map @xmath10 at any point @xmath11 .",
    "unfortunately , unlike linear methods ( such as pca ) , nonlinear dimension reduction algorithms only provide an explicit mapping for the original discrete dataset @xmath12 .",
    "therefore , the inverse mapping @xmath13 is only defined on these data .",
    "the goal of the present work is to generate a numerical extension of @xmath13 to all of @xmath14 . to simplify the problem",
    ", we assume the mapping @xmath5 coincides with the limiting map @xmath15 on the data , @xmath16 for @xmath17 .",
    "this assumption allows us to rephrase the problem as follows : we seek an extension of the map @xmath18 everywhere on @xmath19 , given the knowledge that @xmath20 .",
    "we address this problem using interpolation , and we construct an approximate inverse @xmath21 , which converges toward the true inverse as the number of samples , @xmath0 , goes to infinity , @xmath22 using terminology from geometry , we call @xmath19 the _ coordinate domain _ , and @xmath18 a _ coordinate map _ that parametrizes the manifold @xmath23 .",
    "the components of @xmath24 are the _",
    "coordinate functions_. we note that the focus of the paper is not the construction of new points @xmath25 in the coordinate domain , but rather the computation of the coordinate functions everywhere in @xmath19 .",
    "given the knowledge of the inverse at the points @xmath26 , we wish to interpolate @xmath18 over @xmath27 .",
    "we propose to interpolate each coordinate function , @xmath28 independently of each other .",
    "we are thus facing the problem of interpolating a function of several variables defined on the manifold @xmath19 .",
    "most interpolation techniques that are designed for single variable functions can only be extended using tensor products , and have very poor performance in several dimensions .",
    "for instance , we know from mairhuber theorem ( e.g. , @xcite ) that we should not use a basis independent of the nodes ( for example , polynomial ) to interpolate scattered data in dimension @xmath29 . as a result , few options exist for multivariate interpolation .",
    "some of the most successful interpolation methods involve radial basis functions ( rbfs ) @xcite .",
    "therefore , we propose to use rbfs to construct the inverse mapping .",
    "similar methods have been explored in @xcite to interpolate data on a low - dimensional manifold .",
    "we note that while kriging @xcite is another common approach for interpolating scattered data , most kriging techniques are equivalent to rbf interpolants @xcite .",
    "in fact , because in our application we lack specialized information about the covariance structure of the inverse map , kriging is identical to rbf interpolation .",
    "we focus our attention on two basis functions : the gaussian and the cubic .",
    "these functions are representative of the two main classes of radial functions : scale dependent , and scale invariant . in the experimental section",
    "we compare the rbf methods to shepard s method @xcite , an approach for multivariate interpolation and approximation that is used extensively in computer graphics @xcite , and which was recently proposed in @xcite to compute a similar inverse map .    for each coordinate function @xmath30 , we define @xmath31 to be the rbf interpolant to the data @xmath32 , @xmath33 the reader will notice that we dropped the dependency on @xmath0 ( number of samples ) in @xmath34 to ease readability .",
    "the function @xmath35 in ( [ one - d ] ) is the kernel that defines the radial basis functions , @xmath36 .",
    "the weights , @xmath37 , are determined by imposing the fact that the interpolant be exact at the nodes @xmath38 , and thus are given by the solution of the linear system @xmath39 we can combine the @xmath40 linear systems ( [ one - coord ] ) by concatenating all the coordinates in the right - hand side of ( [ one - coord ] ) , and the corresponding unknown weights on the left - hand side of ( [ one - coord ] ) to form the system of equations , @xmath41 which takes the form @xmath42 , where @xmath43 , @xmath44 , and @xmath45 .",
    "let us define the vector @xmath46 .",
    "the approximate inverse at a point @xmath47 is given by @xmath48",
    "the approximate inverse ( [ rbf_eqn ] ) is obtained by interpolating the original data @xmath32 using rbfs . in order to assess the quality of this inverse",
    ", three questions must be addressed : 1 ) given the set of interpolation nodes , @xmath49 , is the interpolation matrix @xmath50 in ( [ gauss_rbf ] ) necessarily non - singular and well - conditioned ?",
    "2 ) how well does the interpolant ( [ rbf_eqn ] ) approximate the true inverse @xmath18 ?",
    "3 ) what convergence rate can we expect as we populate the domain with additional nodes ? in this section we provide elements of answers to these three questions . for a detailed treatment ,",
    "see @xcite .",
    "in order to interpolate with a radial basis function @xmath51 , the system ( [ gauss_rbf ] ) should have a unique solution and be well - conditioned . in the case of the gaussian defined by @xmath52 the eigenvalues of @xmath50 in ( [ gauss_rbf ] )",
    "follow patterns in the powers of @xmath53 that increase with successive eigenvalues , which leads to rapid ill - conditioning of @xmath50 with increasing @xmath0 ( e.g. , @xcite ; see also @xcite for a discussion of the numerical rank of the gaussian kernel ) .",
    "the resulting interpolant will exhibit numerical _",
    "saturation error_. this issue is common among many scale - dependent rbf interpolants .",
    "the gaussian scale parameter , @xmath53 , must be selected to match the spacing of the interpolation nodes .",
    "one commonly used measure of node spacing is the _ fill distance _ , the maximum distance from an interpolation node .    for the domain @xmath54 and a set of interpolation nodes @xmath55 the _ fill distance _ , @xmath56 , is defined by @xmath57     in ( [ gauss_rbf ] ) , for the gaussian ( @xmath58 ) and the cubic ( @xmath59 ) as a function of the fill distance for a fixed scale @xmath60 .",
    "points are randomly scattered on the first quadrant of the unit sphere in @xmath61 , for @xmath62 from left to right .",
    "note : the same range of @xmath0 , from 10 to 1000 , was used in each dimension . in high dimension",
    ", it takes a large number of points to reduce fill distance .",
    "however , the condition number of @xmath50 still grows rapidly for increasing @xmath0 .",
    ", scaledwidth=80.0% ]     in ( [ gauss_rbf ] ) , for the gaussian ( @xmath58 ) and the cubic (  ) as a function of the scale @xmath53 , for a fixed fill distance .",
    "@xmath63 points are randomly scattered on the first quadrant of the unit sphere in @xmath61 , @xmath62 from left to right .",
    "[ cond_w_ep],scaledwidth=80.0% ]    owing to the difficulty in precisely establishing the boundary of a domain @xmath54 defined by a discrete set of sampled data , estimating the fill distance @xmath56 is somewhat difficult in practice .",
    "additionally , the fill distance is a measure of the `` worst case '' , and may not be representative of the `` typical '' spacing between nodes .",
    "thus , we consider a proxy for fill distance which depends only on mutual distances between the data points .",
    "we define the _ local fill distance _",
    ", @xmath64 , to denote the average distance to a nearest neighbor , @xmath65 the relationship between the condition number of @xmath50 and the spacing of interpolation nodes is explored in fig .",
    "[ cond_w_fill ] , where we observe rapid ill - conditioning of @xmath50 with respect to decreasing local fill distance , @xmath64 .",
    "conversely , if @xmath64 remains constant while @xmath53 is reduced , the resulting interpolant improves until ill - conditioning of the @xmath50 matrix leads to propagation of numerical errors , as is shown in fig .",
    "[ cond_w_ep ] .",
    "when interpolating with the gaussian kernel , the choice of the scale parameter @xmath53 is difficult . on the one hand , smaller values of @xmath53",
    "likely lead to a better interpolant .",
    "for example , in 1-@xmath66 , a gaussian rbf interpolant will converge to the lagrange interpolating polynomial in the limit as @xmath67 @xcite .",
    "on the other hand , the interpolation matrix becomes rapidly ill - conditioned for decreasing @xmath53 . while some stable algorithms have been recently proposed to generate rbf interpolants ( e.g. , @xcite , and references therein ) these sophisticated algorithms are more computationally intensive and algorithmically complex than the rbf - direct method used in this paper , making them undesirable for the inverse - mapping interpolation task .",
    "saturation error can be avoided by using the scale - free rbf kernel @xmath68 , one instance from the set of rbf kernels known as the _ radial powers _",
    ", @xmath69 together with the _ thin plate splines _ , @xmath70 they form the family of rbfs known as the _ polyharmonic splines_.    because it is a monotonically increasing function , the cubic kernel , @xmath71 , may appear less intuitive than the gaussian .",
    "the importance of the cubic kernel stems from the fact that the space generated by linear combinations of shifted copies of the kernel is composed of splines . in one dimension ,",
    "one recovers the cubic spline interpolant .",
    "one should note that the behavior of the interpolant in the far field ( away from the boundaries of the convex hull of the samples ) can be made linear ( by adding constants and linear polynomials ) as a function of the distance , and therefore diverges much more slowly than @xmath72 @xcite .    in order to prove the existence and uniqueness of an interpolant of the form , @xmath73",
    "we require that the set @xmath74 be a @xmath75-unisolvent set in @xmath7 , where _ @xmath76-unisolvency _ is as follows .",
    "the set of nodes @xmath77 is called _",
    "@xmath76-unisolvent _ if the unique polynomial of total degree at most @xmath76 interpolating zero data on @xmath78 is the zero polynomial .    for our problem",
    ", the condition that the set of nodes @xmath79 be 1-unisolvent is equivalent to the condition that the matrix @xmath80 have rank @xmath81 ( we assume that @xmath82 ) .",
    "this condition is easily satisfied .",
    "indeed , the rows @xmath83 of ( [ polymatrix ] ) are formed by the orthogonal eigenvectors of @xmath84 . additionally , the first eigenvector , @xmath85 , has constant sign . as a result ,",
    "@xmath86 are linearly independent of any other vector of constant sign , in particular @xmath87 . in figures",
    "[ cond_w_fill ] and [ cond_w_ep ] we see that the cubic rbf system exhibits much better conditioning than the gaussian .",
    "we now consider the second question : can the interpolant ( [ one - d ] ) approximate the true inverse @xmath18 to arbitrary precision ?",
    "as we might expect , an rbf interpolant will converge to functions contained in the completion of the space of linear combinations of the kernel , @xmath88 .",
    "this space is called the _ native space_.",
    "we note that the completion is defined with respect to the @xmath35-norm , which is induced by the inner - product given by the reproducing kernel @xmath35 on the pre - hilbert space @xmath89 @xcite .",
    "it turns out that the native space for the gaussian rbf is a very small space of functions whose fourier transforms decay faster than a gaussian @xcite . in practice ,",
    "numerical issues usually prevent convergence of gaussian rbf interpolants , even within the native space , and therefore we are not concerned with this issue .",
    "the native space of the cubic rbf , on the other hand , is an extremely large space .",
    "when the dimension , @xmath66 , is odd , the native space of the cubic rbf is the beppo levi space on @xmath7 of order @xmath90 @xcite .",
    "we recall the definition of a _",
    "beppo levi space _ of order @xmath91 .    for @xmath92",
    ", the linear space @xmath93 , equipped with the inner product @xmath94 , is called the _",
    "beppo levi space on @xmath7 of order @xmath91 _ , where @xmath95 denotes the weak derivative of ( multi - index ) order @xmath96 on @xmath7 .    for even dimension ,",
    "the beppo levi space on @xmath7 of order @xmath97 corresponds to the native space of the thin plate spline @xmath98 @xcite .",
    "because we assume that the inverse map @xmath18 is smooth , we expect that it belongs to any of the beppo levi spaces . despite the fact that we lack a theoretical characterization of the native space for the cubic rbf in even dimension ,",
    "all of our numerical experiments have demonstrated equal or better performance of the cubic rbf relative to the thin plate spline in all dimensions ( see also @xcite for similar conclusions ) .",
    "thus , to promote algorithmic simplicity for practical applications , we have chosen to work solely with the cubic rbf .",
    "the gaussian rbf interpolant converges ( in @xmath99 norm ) exponentially fast toward functions in the native space , as a function of the decreasing fill distance @xmath56 @xcite .",
    "however , as observed above , rapid ill - conditioning of the interpolation matrix makes such theoretical results irrelevant without resorting to more costly stable algorithms .",
    "the cubic interpolant converges at least as fast as @xmath100 in the respective native space @xcite .",
    "in practice , we have experienced faster rates of algebraic convergence , as shown in the experimental section .",
    "we first conduct experiments on a synthetic manifold , and we then provide evidence of the performance of our approach on real data . for all experiments",
    "we quantify the performance of the interpolation using a `` leave - one - out reconstruction '' approach : we compute @xmath101 , for @xmath102 , using the remaining @xmath103 points : @xmath104 , @xmath105 and their coordinates in @xmath61 , @xmath106 .",
    "the average performance is then measured using the average leave - one - out @xmath107 reconstruction error , @xmath108 in order to quantify the effect of the sampling density on the reconstruction error , we compute @xmath109 as a function of @xmath64 , which is defined by ( [ localfill ] ) .",
    "the two rbf interpolants are compared to shepard s method , a multivariate interpolation / approximation method used extensively in computer graphics @xcite .",
    "shepard s method computes the optimal constant function that minimizes the sum of squared errors within a neighborhood @xmath110 of @xmath25 in @xmath7 , weighted according to their proximity to @xmath25 .",
    "the solution to this moving least squares approximation is given by @xmath111 the relative impact of neighboring function values is controlled by the scale parameter @xmath53 , which we choose to be a multiple of @xmath112 .      for our synthetic manifold example , we sampled @xmath0 points @xmath113 from the uniform distribution on the unit sphere @xmath114 , then embedded these data in @xmath115 via a random unitary transformation .",
    "the data are mapped to @xmath116 using the first five non - trivial eigenvectors of the graph laplacian .",
    "the minimum of the total number of available neighbors , @xmath103 , and 200 neighbors was used to compute the interpolant . for each local fill distance , @xmath64",
    ", the average reconstruction error is computed using ( [ error ] ) .",
    "the performances of the cubic rbf , gaussian rbf , and shepard s method versus @xmath64 are shown in fig .",
    "[ unitsphere ] .",
    "we note that the interpolation error based on the cubic rbf is lowest , and appears to scale approximately with @xmath117 , an improvement over the @xmath100 bound @xcite .",
    "in fact , the cubic rbf proves to be extremely accurate , even with a very sparsely populated domain : the largest @xmath64 corresponds to 10 points scattered on @xmath114 .    , on @xmath114 embedded in @xmath115 , using the cubic ( left ) , the gaussian ( center ) , and shepard s method ( right ) .",
    "note the difference in the range of @xmath118-axis.,scaledwidth=80.0% ]    .reconstruction error @xmath119 for each digit ( 0 - 9 ) .",
    "red denotes lowest average reconstruction residual . [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     three representative reconstructions for the digit `` 3 '' .",
    "the optimal scales ( according to table [ digits_table ] ) were chosen for both the gaussian rbf and shepard s methods .",
    "the cubic rbf outperforms the gaussian rbf and shepard s method in all cases , with the lowest average error ( table  [ digits_table ] ) , and with the most `` noise - like '' reconstruction residual ( fig .",
    "[ three_example ] ) .",
    "results suggest that a poor choice of scale parameter with the gaussian can corrupt the reconstruction .",
    "the scale parameter in shepard s method must be carefully selected to avoid the two extremes of either reconstructing solely from a single nearest neighbor , or reconstructing a blurry , equally weighted , average of all neighbors .",
    "finally , the performance of the inverse mapping algorithms was also assessed on the frey face dataset @xcite , which consists of digital images of brendan frey s face taken from sequential frames of a short video .",
    "the dataset is composed of @xmath120 gray scale images .",
    "each image was normalized to have unit @xmath107 norm , providing a dataset of 1,965 points in @xmath121 .",
    "a 15-dimensional representation of the frey face dataset was generated via laplacian eigenmaps .",
    "the inverse mapping techniques were tested on all images in the set .",
    "table  [ frey_table ] shows the mean leave - one - out reconstruction errors for the three methods .",
    "[ frey_example ] shows three representative reconstructions using the different techniques .",
    "the optimal scales ( according to table [ frey_table ] ) were chosen for both the gaussian rbf and shepard s methods .",
    "again , the cubic rbf outperforms the gaussian rbf and shepard s method in all cases , with the lowest average error ( table  [ frey_table ] ) , and with the most `` noise - like '' reconstruction residual ( fig .",
    "[ frey_example ] ) .",
    "inspired by the rbf interpolation method , we provide in the following a novel interpretation of the nystrm extension : the nystrm extension interpolates the eigenvectors of the ( symmetric ) normalized laplacian matrix using a slightly modified rbf interpolation scheme .",
    "while several authors have mentioned the apparent similarity of nystrm method to rbf interpolation , the novel and detailed analysis provided below provides a completely new insight into the limitations and potential pitfalls of the nystrm extension .",
    "consistent with laplacian eigenmaps , we consider the symmetric normalized kernel @xmath122 , where @xmath123 is a radial function measuring the similarity between @xmath3 and @xmath124 , and @xmath40 is the degree matrix ( diagonal matrix consisting of the row sums of @xmath50 )",
    ".        given an eigenvector @xmath125 of @xmath126 ( associated with a nontrivial eigenvalue @xmath127 ) defined over the points @xmath3 , the nystrm extension of @xmath125 to an arbitrary new point @xmath8 is given by the interpolant @xmath128 where @xmath129 is the coordinate @xmath130 of the eigenvector @xmath131 .",
    "we now proceed by re - writing @xmath132 in ( [ nystrom1 ] ) , using the notation @xmath133 , where @xmath134 , and @xmath135 . @xmath136^t       =   \\tilde { { \\bm k } } ( { { \\bm x } } , \\cdot)^t { \\bm{\\phi}}\\lambda^{-1 } { \\bm{\\phi}}^t { \\bm{\\phi}}_l   \\\\ & =      \\tilde { { \\bm k } } ( { { \\bm x } } , \\cdot)^t \\widetilde k^{-1 } { \\bm{\\phi}}_l   =        \\frac{1}{\\sqrt{d ( { { \\bm x } } ) } }       \\begin{bmatrix }        k ( { { \\bm x } } , { { \\bm x } } ^{(1 ) } ) & \\ldots & k ( { { \\bm x } } , { { \\bm x } } ^{(n ) } )       \\end{bmatrix }      d^{-1/2 }   ( d^{1/2 } k^{-1 } d^{1/2 } ) { \\bm{\\phi}}_l\\\\      & =   \\frac{1}{\\sqrt{d ( { { \\bm x } } ) } }      { { \\bm k } } ( { { \\bm x } } , \\cdot)^t k^{-1 } ( d^{1/2 } { \\bm{\\phi}}_l ) .",
    "\\end{split }    \\label{nystrom2}\\ ] ] if we compare the last line of ( [ nystrom2 ] ) to ( [ rbf_eqn ] ) , we conclude that in the case of laplacian eigenmaps , with a nonsingular kernel similarity matrix @xmath50 , the nystrm extension is computed using a radial basis function interpolation of @xmath125 after a pre - rescaling of @xmath125 by @xmath137 , and post - rescaling by @xmath138 .",
    "although the entire procedure it is not exactly an rbf interpolant , it is very similar and this interpretation provides new insight into some potential pitfalls of the nystrm method .",
    "the first important observation concerns the sensitivity of the interpolation to the scale parameter in the kernel @xmath35 . as we have explained in section [ conditioning ] , the choice of the optimal scale parameter @xmath53 for the gaussian rbf is quite difficult . in fact , this issue has recently received a lot of attention ( e.g. @xcite ) .",
    "the second observation involves the dangers of sparsifying the similarity matrix . in many nonlinear dimensionality reduction applications , it is typical to sparsify the kernel matrix @xmath50 by either thresholding the matrix , or keeping only the entries associated with the nearest neighbors of each @xmath139 .",
    "if the nystrm extension is applied to a thresholded gaussian kernel matrix , then the components of @xmath140 as well as @xmath141 are discontinuous functions of @xmath8 . as a result",
    ", @xmath142 , the nystrm extension of the eigenvector @xmath125 will also be a discontinuous function of @xmath8 , as demonstrated in fig .",
    "[ truncated ] . in the nearest neighbor approach ,",
    "the extension of the kernel function @xmath143 to a new point @xmath8 is highly unstable and poorly defined . given this larger issue , the nystrm extension should not be used in this case . in order to interpolate eigenvectors of a sparse similarity matrix , a better interpolation scheme such as a true ( non - truncated ) gaussian rbf , or a cubic rbf interpolant could provide a better alternative to nystrm",
    ". a local implementation of the interpolation algorithm may provide significant computational savings in certain scenarios .",
    "the authors would like to thank the three anonymous reviewers for their excellent comments .",
    "ndm was supported by nsf grant dms 0941476 ; bf was supported by nsf grant dms 0914647 ; fgm was partially supported by nsf grant dms 0941476 , and doe award de - scoo04096 ."
  ],
  "abstract_text": [
    "<S> nonlinear dimensionality reduction embeddings computed from datasets do not provide a mechanism to compute the inverse map . in this paper , we address the problem of computing a stable inverse map to such a general bi - lipschitz map . </S>",
    "<S> our approach relies on radial basis functions ( rbfs ) to interpolate the inverse map everywhere on the low - dimensional image of the forward map . </S>",
    "<S> we demonstrate that the scale - free cubic rbf kernel performs better than the gaussian kernel : it does not suffer from ill - conditioning , and does not require the choice of a scale . </S>",
    "<S> the proposed construction is shown to be similar to the nystrm extension of the eigenvectors of the symmetric normalized graph laplacian matrix . </S>",
    "<S> based on this observation , we provide a new interpretation of the nystrm extension with suggestions for improvement .    inverse map , nonlinear dimensionality reduction , radial basis function , interpolation , nystrm extension </S>"
  ]
}