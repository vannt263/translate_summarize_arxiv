{
  "article_text": [
    "most machine learning settings take feature vectors as input . these features",
    "are often acquired using some process resulting in less than optimal data quality . in many situations ,",
    "the data quality depends on the resources allocated for the data acquisition process .",
    "examples of possible resources are sample rate , total sample time , cpu allocated to some costly pre - processing , and transmitted power .",
    "several approaches have been proposed in order to deal with some uncertainty in learning schemas ( for example@xcite ) . in many cases ,",
    "however , one does not merely deal with existing uncertainty , but can sometimes `` shape '' the uncertainty to meet one s needs .",
    "this is often the case when several sensors share a common resource .",
    "for example , mobile applications use sensors that share power , cpu and bandwidth .",
    "each of those resources can be divided between sensors according to the designer wish .",
    "another example is the design of a system with fixed budget ( money wise ) , each type of sensor incorporated can have a variety of qualities ( with a price tag to match ) . which sensor is `` worth '' investing in ?    in this work we explore the following problem : several sensors that share a common resource acquire inputs that will be used for classification .",
    "what is the best way to divide the resource between the sensors ?",
    "the resources allocated for each sensor affect the quality of the data it collects .",
    "we wish to maximize classification performance by correctly allocating the available resources .",
    "we emphasize that different resource allocation schemes may result in different optimal classifiers .",
    "this coupling increases the complexity of the problem .",
    "we present a framework for  uncertainty management \" : this framework formulates the presented problem as an optimization problem . the direct formulation , however , is not easily solvable so we derive an equivalent solvable problems for various scenarios .",
    "we further bound the benefit that may arise from optimally allocating the resources . based on the results presented we devise an algorithm for deriving the optimal resource allocation and present some simulation results that show the potential benefits .",
    "an application domain of such an approach is that of sensor management ( see @xcite ) , where mostly state - estimation problems have been investigated . among the most studied applications",
    "is the real - time allocation of radar resources ( for example @xcite ) .",
    "however , other applications such as multi sensor management @xcite have also been studied .",
    "one more emerging application is the use of services like mechanical turk in order to extract features ( for example subjective features regarding an image or a text ) .",
    "the more averaging performed , the more accurate the features are .",
    "however , not all features require the same accuracy .    in our model , collected features are corrupted by some disturbance .",
    "we explore two types of disturbances : stochastic and adversarial .",
    "a stochastic disturbance corresponds to common situations where features are corrupted by some , typically additive , noise .",
    "an adversarial disturbance concerns the worst possible deterministic loss maximizing noise corresponding to `` worst - case '' scenarios .",
    "we assume that special effort is made so that the training data are of the highest quality . during the test phase , however , resources are limited and should be allocated sparingly .",
    "this is often the case in applications where the number of samples to be classified is larger by several orders of magnitude than the training set size .",
    "this work focuses on methods for controlling uncertainty in problems of binary classification with real valued features .",
    "we consider support vector machines ( svm ) style classification @xcite due to its many beneficial properties ( for example @xcite and @xcite ) .",
    "however , our method can be easily adapted to a wide variety of learning schemes .",
    "we further explore a second scenario in which we assume that the training data are noisy while during the test phase data quality is superb .",
    "this can occur for a number of reasons .",
    "one example is some difficulty to gather information in the learning phase which do not exist in the test - phase .",
    "for example , patients may be more willing to conduct a ct scan when some serious illness is suspected but convincing them to perform one for the sake of experimentation require the use of less radiation therefore more noise@xcite .",
    "another example is when the learning data - set is `` sensitized '' by artificiality adding noise in order to comply with privacy issues .",
    "scenarios in which noise arise in both training and testing phase can be accommodated by a combination of the methods presented .",
    "in most of the paper we assume that the relation between the resources to be allocated and the disturbance is known .",
    "this scenario is quite reasonable , examples include influence of sampling rate on temporal features , sampling time on spectral features , power on channel error rate in communication and many more .",
    "however , since there are also cases where this relation is unknown we introduce an algorithm that is _ completely _ data - driven .",
    "we do not assume gaussian noise .",
    "however , in many areas of control and signal processing gaussian noise is used to model sensors noise .",
    "for that reason the examples given consider gaussian noise",
    ".    * related works . *",
    "the problem of resource allocation between sensors has been investigated in several disciplines and from several perspectives .",
    "most works come from an adaptive control perspective .",
    "almost half a century ago , meier @xcite defined a setting where sensors parameters can be controlled .",
    "the control perspective has been studied extensively since , mostly for the special case of sensor switching , namely dynamically choosing one sensor from several available ones ; see @xcite and many others .",
    "in contrast with those works we are dealing with classification problem . the existence of some decision boundary makes the problem more involved and the control theory framework inadequate .",
    "in addition , this line of research generally assumes full knowledge of the underline model , an assumption we would like to avoid .    in @xcite the authors considered the problem of finding an optimal least - squares linear regressor as well as noise parameters of a static estimation problem when the underlying model is known .",
    "they explore the spacial case of estimating a scalar using square loss . a mild extension to this spacial case",
    "is given in @xcite .",
    "we generally follow the same approach , although our problem definition is more general .",
    "we fortunately have the privilege of enjoying a later rich body of research concerning dealing with known uncertainty in learning scenarios ( e.g. , @xcite ) .",
    "classification problems in this context were considered by trying to maximize some measure of information in the data . in",
    "this setting one tries to optimize some information measures like sample conditional entropy or the kullback - leibler ( kl ) divergence ( for example @xcite ) .",
    "such methods lead to an elegant solution but are heuristic and ignore knowledge about the desired utility function , so that some information `` quantity '' is optimized instead of the relevance to classification .",
    "resource efficient learning is a growing field of research in recent years .",
    "most research is focused on dynamic acquisition of features where different features are acquired for different samples .",
    "multiple models were proposed including trees @xcite , cascades@xcite and markov decision processes @xcite .",
    "our work explores the situation where features are acquired simultaneously and not sequentially .",
    "some work had also considered introducing resource awareness into the classifier learning process .",
    "this is usually done using some greedy process where features are added to a classifier until the resource budget run out@xcite .",
    "similar methods which treat the learning scheme as `` black - box '' are wrapper feature selection @xcite .",
    "some work had explored similar issues when resources are scarce in the learning phase instead of the testing phase @xcite .",
    "while our work shares a similar motivation with those fields , our decision space is continuous and not discrete .",
    "we are inspired by problems in which sensors use a physical resource which need to be allocated ( time , power , bandwidth , etc . ) .",
    "existing methods can not support such problems .",
    "in addition , the use of a continuous decision space circumvents the need to solve complex combinatorial problems and allows the use of various tools from optimization theory .",
    "another setting which had been explored is on - line learning in the presence of noise .",
    "an algorithm for on - line learning from noisy data is presented in @xcite .",
    "we improve the algorithm presented there by allowing on - line control of features quality and show that learning can be done more efficiently .    * contributions . *",
    "the contributions of this paper are :    * we develop a framework for considering feature acquisition quality as a resource allocation problem in classification . *",
    "we derive algorithms for optimal resource allocation and optimal classification for a variety of scenarios .",
    "* we analyse the performance gain that can be achieved . *",
    "we demonstrate the benefit that can arise from using those methods in simulation .",
    "the structure of this paper is as following : section [ section : statistical ] introduces the framework of uncertainty management and provides a method for determining the optimal resource allocation for stochastic disturbances .",
    "section [ section : adversarial ] explores the case of adversarial noise .",
    "the results presented in those sections characterize the optimal allocation for a wide array of problems .",
    "section [ sec : unknown ] proposes an algorithm for the scenario where the disturbance characteristics is unknown and gives a theoretical guarantee on its regret .",
    "section [ sec : learning ] explores the case where the training set is noisy and provides an efficient algorithm for the special case of linear classifier with gaussian noise and square loss .",
    "section [ section : simulation ] presents some simulations that demonstrate the feasibility of the results and section [ section : conclusion ] concludes with some final thoughts .",
    "proofs for all of the theorems in this paper can be found in the appendix .",
    "this section explores the case in which the disturbance is stochastic .",
    "we assume that m samples @xmath0 are generated from some joint distribution ( i.i.d . ) .",
    "denote by @xmath1 the @xmath2th feature of sample @xmath3 .",
    "each @xmath1 is measured with some disturbance @xmath4 .",
    "the disturbance is generated from a distribution with some vector of parameters ( resources ) @xmath5 .",
    "denote the resulting vector of disturbances in sample @xmath3 as @xmath6 .",
    "we follow the empirical risk minimization framework @xcite .",
    "let @xmath7 be the cost incurred when the disturbance is generated using resource vector @xmath8 .",
    "@xmath9 our objective is to optimize _ both _ the resource vector @xmath10 and the classifier @xmath11 such that @xmath7 is minimized .    for simplicity , we focus our attention on the spacial case of linear classifiers . however , the framework presented in this paper can be easily extended to other families of classifiers .",
    "also , we assume that the noise is independent between samples , namely that each @xmath4 is generated i.i.d . using a distribution with parameter @xmath12 .",
    "we note that @xmath7 can also be written as @xmath13 where @xmath14 .",
    "the variable @xmath15 is a measure of the noise influence on the cost function .",
    "for example , for linear classifiers it is often the standard deviation of the noise in the axis perpendicular to the decision boundary .",
    "this is helpful since many loss function may be defined this way .",
    "we give details of such an example below ( example 1 ) . for a linear classifier",
    "we define ,    @xmath16    assume that @xmath17 is a convex function in @xmath8 , positive and strictly decreasing in each element for @xmath18 .",
    "also , assume that @xmath19 is strictly increasing and convex in @xmath20 .",
    "we refer to a loss function that satisfies these assumptions as an _ acceptable loss function_. those assumption can be informally interpreted as assuming that more resources provide better accuracy and that increasing performance provide diminishing return .",
    "the problem can now be stated as : @xmath21    [ [ example-1 ] ] example 1 + + + + + + + + +    consider the case in which @xmath4 is gaussian with zero mean and standard deviation @xmath22 , where @xmath22 is a convex strictly decreasing function .",
    "assume also that @xmath23 . in this case",
    ", @xmath20 is the standard deviation of the distance from the decision boundary , namely , @xmath24",
    ".    now , there are two natural loss functions we can explore : hinge loss and square loss . for the hinge loss @xmath25 . in such case @xmath26 can be calculated directly : @xmath27    for the square loss @xmath28 .",
    "in this case a simple calculation shows that the overall loss is : @xmath29    similarly , one can use other loss functions and obtain a numerical if not exact expressions .",
    "the following theorem characterizes the optimal resource allocation for problem ( [ problem : stat ] ) .",
    "according to theorem [ optimal - noise ] the resource allocation depends only on @xmath30 such that one can derive the optimal resource allocation even without knowing @xmath26 .",
    "the proof of the theorem and all other proofs appear in the appendix .    [",
    "optimal - noise ] suppose that @xmath31 is an acceptable loss function . for the optimal solution @xmath32 of problem ( [ problem : stat ] )",
    "there exists @xmath33 such that @xmath34 , and for every @xmath3 it holds that @xmath35    using theorem [ optimal - noise ] and greedy search over @xmath36 problem ( [ problem : stat ] ) can be solved .",
    "we now outline a few examples of optimal allocation of resources for different relations between the resources and the noise variance . while all of the examples relate to zero mean gaussian noise , theorem [ optimal - noise ]",
    "is general and can be applied for other distributions as long as their variance is finite .",
    "we explore the scenario in which the standard deviation is proportional to the inverse of the resources allocated .",
    "namely , @xmath37 .",
    "this is the case , for example , when the resource is the sampling rate and the features measured are timing of various events . in this case : @xmath38    [ [ example-3-variance - proportional - to - inverse - of - resources ] ] example 3 : variance proportional to inverse of resources + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a popular relation between resources and noise is when the variance is proportional to the inverse of resources allocated .",
    "namely , @xmath39 .",
    "this is the case in many situations including : power in active sensors , duration of sampling for spectral features and number of measurements taken when averaging ( for example if features are extracted using a mechanical turk ) . in this case , the optimal allocation can be easily computed to be : @xmath40    [ coll : ridge ] in the case of square - loss and uniform allocation of resources @xmath41 , it follows that @xmath42 .",
    "[ coll : lasso ] when applying optimal allocation of resources according to ( [ optimal_allocation ] ) it results that @xmath43 .",
    "interestingly , the optimization problem derived for square - loss ( [ lforsquereloss ] ) with uniform allocation of resources is equivalent to the optimization problem derived when performing ridge regression . similarly , using optimal allocation of resources",
    "is similar to performing lasso regularization .",
    "this support claims that using lasso regularization produce classifiers which are more robust to noise than other regularization techniques @xcite    since for the square - loss optimizing ( [ lforsquereloss ] ) is equivalent to performing lasso , one can use complicity bounds derived for this case .",
    "it is known that a bound on the error resulting from lasso regularization @xmath44 is increasing in @xmath45 @xcite . since @xmath45 is decreasing with @xmath46 , it is increasing with @xmath47 .",
    "this surprisingly implies that _ less resources require less examples to learn_. this can be explained in the following manner : with less resources there is more noise in the decision making phase , the larger the noise the less impact small changes in the classifier makes ( in the limit , there are no resources and therefore the noise is infinite and there is nothing to learn ) .",
    "[ [ example-4-quantization - noise ] ] example 4 : quantization noise + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is known that rounding quantization noise can be treated as gaussian with standard deviation of @xmath48 where the @xmath49 is the accuracy of the least significant bit @xcite . consider a scenario in which we would like to maintain the number of bits used to represent all features under some threshold @xmath47 .",
    "we will first disregard the fact that @xmath8 must be an integer and derive the solution for @xmath50 while @xmath51 for all @xmath3 .",
    "the solution of ( [ problem : stat ] ) for any fixed @xmath52 will be    @xmath53    notice that we still need to transform @xmath54 into integers",
    ". this can be done by `` searching '' in the vicinity of the optimal vector @xmath8 .      to gain some insight about the expected benefit of using this method",
    "we explore the special case of square loss with @xmath55 .",
    "observe that @xmath56 is decreasing in @xmath47 . from corollary [ coll : ridge ] and",
    "[ coll : lasso ] we know that finding an optimal @xmath52 for uniform allocation is equivalent to performing ridge regression while optimizing ( [ lforsquereloss ] ) is equivalent to performing lasso .",
    "we ask the following question : for the same expected loss how much resources can we save by using the method presented ? in order to answer this question we start by fixing @xmath52 and analyze the expected loss for different resources allocations .    for every admissible @xmath57 denote by @xmath58 the resource budget that holds @xmath59 when @xmath60 . also , denote by @xmath61 the resource budget that holds @xmath62 when @xmath63 .",
    "the following result bounds the ratio between resources required for achieving the same loss .",
    "[ th : r_improve ] for every @xmath52 and for @xmath64 it holds that @xmath65 .",
    "the proof can be found in the appendix .",
    "denote by @xmath66 the optimal classifier when resources are allocated optimally and by @xmath67 the optimal classifier when resources are allocated uniformly .",
    "the next corollary follows directly from theorem [ th : r_improve ] ; it bounds the total benefit that can arise from the joint optimization of both resource allocation and classifier .",
    "it holds since @xmath68 and @xmath69 .",
    "[ cor : performence ] for every @xmath52 and for @xmath64 it holds that @xmath70    it is clear from corollary [ cor : performence ] that in cases where some features hold little information ( small coefficients in the classifier ) the benefit of optimized resource allocation can be very large . it should be noted that in extreme cases this is equivalent to using feature selection ( meaning , choosing which features should be allocated zero resources )",
    ". however , in many cases even when considering only relevant features the variance of their influence is significant . in such cases our method provides considerable benefit .",
    "we now consider the case where the disturbance is adversarial .",
    "several models for adversarial disturbance have been considered in the literature , we will adopt the model from @xcite .",
    "formally , consider some samples @xmath71 where @xmath72 and @xmath73 .",
    "we only have access to some corrupted version of this set @xmath74 .",
    "the disturbances @xmath6 are determined by an adversary , however the adversary can only affect samples in a certain way . formally , the vector @xmath75 is in a set defined by : @xmath76 , where @xmath77 is some symmetric uncertainty set that contains the origin . in our setting , we wish to optimize both the classifier parameters @xmath78 and the shape of @xmath79 under the constraint of available power ( or budget ) for the adversary .",
    "the main difference here from other works ( see @xcite and follow - ups ) is that we can _ optimize _ over @xmath77 out of a family of sets ( set of sets ) .",
    "such a family can be , for example , the set of ellipsoid sets while maintaining some constant fixed resource budget .",
    "@xmath80    formally , the problem is optimizing @xmath81 for some @xmath82 that defines the problem .",
    "we will focus our attention on the hinge loss , @xmath83 . for hinge loss",
    "the following result is given in @xcite :    ( xu et al .",
    "2009@xcite ) [ xu_lemma ] assume @xmath84 are non - separable then the following min - max problem @xmath85 is equivalent to the following optimization problem @xmath86    we use this result in order to derive the following theorem :    [ theorem_adversrial ] consider the solution @xmath87 for the problem @xmath88 then the solution satisfies @xmath89    theorem [ theorem_adversrial ] is analogous to theorem [ optimal - noise ] and allows to optimize resource allocation in the adversarial setting .",
    "[ [ example-4 ] ] example 4 + + + + + + + + +    gaussian noise is a popular modelling choice in many domains .",
    "we wish to find some constraint which will create in the adversarial setting an effect that reassembles gaussian noise . for this purpose",
    "we use an ellipsoid uncertainty set . instead of assuming gaussian noise we bound the uncertainty to a fixed width of standard deviations .",
    "consider the model presented in @xcite with an ellipsoid uncertainty set namely , @xmath90 .",
    "the function @xmath91 can be any of the former examples .",
    "now , under non separability assumption the solution of the problem @xmath92    satisfies , @xmath93    fixing @xmath94 allows the derivation of @xmath78 by solving the conic optimization problem @xmath95    theorems [ optimal - noise ] and [ theorem_adversrial ] allow to solve either ( [ problem : stat ] ) or ( [ optimization - adv ] ) using alternating optimization .",
    "moreover , in the special case where @xmath39 the optimal allocation of resources is analogous to lasso regression : @xmath96",
    "in this section we consider the case of stochastic disturbance that is unknown .",
    "we wish do devise a data - driven algorithm that finds the optimal resource allocation even when the disturbance is initially unknown .",
    "we use stochastic gradient descent in order to minimize the cumulative loss function . in this section",
    "we explore the special case of square - loss with some assumptions on the structure of the disturbance .",
    "we derive a concrete algorithm and a corresponding bound for this special case .",
    "it is possible to easily extend this algorithm to various other scenarios .",
    "we make the following assumptions on the structure of the disturbance :    * the disturbance and data - points are independent ( @xmath97 is independent from @xmath6 ) . *",
    "the disturbance is independent between features . *",
    "the distribution of the disturbance in each feature is symmetric . * the second moment of the disturbance ,",
    "@xmath98 is convex in @xmath54 .",
    "the last assumption is reasonable since we expect diminishing return from increasing allocated resources .",
    "we use ridge regularization and bound the possible set of classifiers by @xmath99 .",
    "the optimization problem can be stated as : @xmath100    the gradient is given by : @xmath101    since @xmath102 is unknown we will approximate it using the kiefer - wolfowitz procedure @xcite .",
    "this results in @xmath103 .",
    "we will denote by @xmath104 the projection of classifier @xmath52 and resource vector @xmath8 into the set of feasible solutions @xmath105 .",
    "we further denote the maximum distance between two vectors in this set by @xmath106 .",
    "it is now possible to use standard stochastic gradient descent . following the kiefer - wolfowitz procedure , at each step measure two data points with two slightly different resource allocations .",
    "then , estimate the gradient and update the classifier and resource allocation accordingly .",
    "finally , project the solution into the feasible solutions space and continue to the next step .",
    "the resulting algorithm is presented as algorithm [ alg : unknown ] .",
    "@xmath107 initialize @xmath108 , @xmath109 receive @xmath110 using resource distribution @xmath111 receive @xmath112 using resource distribution @xmath113 @xmath114 @xmath115 @xmath116}{\\epsilon}$ ] @xmath117;@xmath104 is the projection into the feasible solutions space    it is easy to verify that the estimated gradient is indeed unbiased .",
    "notice that unlike standard on - line learning the measurement @xmath118 are not i.i.d .",
    "since choosing @xmath8 creates a coupling between measurements .",
    "however , the `` noise '' of the estimated gradient is a martingale difference sequence and therefore stochastic estimation theory can be easily applied .",
    "we proceed to bound the regret which arise from algorithm [ alg : unknown ] .",
    "since we use keifer - wolfowitz procedure the regret must be measured in comparison to the biased functions created by the procedure .",
    "namely , @xmath119 and @xmath120 .",
    "when @xmath121 is small enough @xmath122 is approximately @xmath123 .",
    "it is now possible to derive a bound on the regret .",
    "[ th : unknown ] if @xmath124 is jointly convex in @xmath125 for every @xmath126 , @xmath127 , @xmath128 , @xmath129 , @xmath130 and @xmath131 then @xmath132    where , @xmath133    the proof follows similar lines to that used to derive a bound in @xcite and can be found in the appendix .",
    "theorem [ th : unknown ] implies that the optimal classifier and optimal resource allocation can be learned with sub - linear regret .",
    "note that decreasing @xmath121 , which is the step - size used to estimate the gradient , will increase learning time .",
    "this is since we assume that noise is independent between samples . in this setting",
    "decreasing @xmath121 increases the noise level in estimating the gradient . choosing large @xmath121 , however , can result in large bias from the optimal solution .",
    "the next two remarks show that assuming some dependence between samples may _ reduce _ learning time significantly",
    ".    the term @xmath134 can be quite large . for reducing the variance in the learning process it is possible at some cases during training to sample multiple times the same data point .",
    "in such cases it is possible to derive a much better bound in which @xmath135 .    in many cases the measurements noise of the same sample with different resources",
    "is correlated .",
    "this is for example the case when the resource is cpu time and the disturbance is caused from processing only part of the data .",
    "two acquisitions of the same sample share a vast amount of common data . in such cases",
    "the difference between measurements with @xmath136 and @xmath8 can be bounded much more tightly then the bound used in theorem [ th : unknown ] .",
    "if @xmath137 then @xmath138 in theorem [ th : unknown ] can be rewritten as @xmath139 .",
    "in this section we explore the situation where the learning set is noisy while the test set is of perfect quality .",
    "this is the case in certain medical examinations where in the learning phase it is difficult to persuade a subject to go through extensive testing while at test time a patient suspected of having a serious disease will agree to such testing @xcite .",
    "we adopt the framework in @xcite that considered learning from noisy data . in our setting , however , the noise distribution can be controlled ( under some resource constraints ) by the learner . as we will show",
    "this control can produce a more efficient learning process .",
    "the on - line learning scheme fits this scenario since the optimal noise allocation depends on the classifier .",
    "we will focus our attention on the case of squared - loss . in @xcite",
    "the authors develop an algorithm for online learning from noisy data .",
    "their algorithm uses stochastic gradient descent in order to optimize the expected loss .",
    "our algorithm is a modification of the one presented in @xcite to include the control over resources .",
    "we will use lasso regularization in order to bound the set of classifiers , namely @xmath140 .",
    "the algorithm is presented as algorithm [ alg : efficientlearn ] .",
    "the algorithm receives as input the step size @xmath141 , the lasso parameter @xmath142 and some function which assign optimal resources for a known classifier @xmath143 .",
    "examples for possible @xmath143 had been given in section [ section : statistical ] .",
    "the covariance matrix of the disturbance which results from using resources vector @xmath8 is denoted by @xmath144 .",
    "notice that @xmath144 is diagonal and assumed known .",
    "we focus on the case where the disturbance is gaussian with standard deviation @xmath145 .",
    "@xmath146 initialize @xmath108 , @xmath109 receive @xmath147 using resource distribution @xmath111 @xmath148 @xmath149 @xmath150 @xmath151    our results are based on the following lemma which is an adaptation of theorem 2 from @xcite .",
    "assume @xmath152 and set @xmath153 then the regret of algorithm [ alg : efficientlearn ] satisfy @xmath154 .",
    "since the proof of this lemma is very similar to the one used to produce the results in @xcite we refer the reader to @xcite .",
    "we now move on to show that a proper choice of resources may improve learning .",
    "we assume the problem is normalized such that @xmath155 , @xmath156,@xmath127 and @xmath128 .",
    "we further denote @xmath129 .",
    "the following two theorems show that proper allocation of resources can improve the efficiency of learning by @xmath157 .",
    "more specifically the regret will be @xmath158 instead of @xmath159 .    [",
    "th : uniform ] assume @xmath160 and @xmath153 . then @xmath161    where",
    ",    @xmath162    however , in case resources are allocated efficiently the corresponding bound is given by the following theorem .",
    "[ th : efficent ] assume @xmath163 and @xmath153 then @xmath164 where , @xmath165    notice that efficient learning requires some balance between two terms .",
    "the term @xmath166 is required for estimating @xmath167 while the term @xmath168 is required for estimating @xmath169 .",
    "we have created @xmath170 by balancing those two terms evenly .",
    "it is possible that a different balance will provide better results .    when @xmath52 is dense the efficient allocation is almost uniform .",
    "therefore , the regret of the two resources allocation schemes should be similar .",
    "this is not evident from the bounds provided .",
    "the reason is that the proof of theorem [ th : uniform ] uses the fact that in the worst case @xmath171 . in cases where @xmath52 is dense",
    "this is loose . using a tighter bound , @xmath172 results in a bound with order @xmath173 for the uniform allocation case , similar to that received for efficient allocation of resources .",
    "we tested the method on three datasets , one synthetic and two real - life problems from the uci repository .",
    "noise was added to all data artificially according to the relation @xmath174 .",
    "for all datasets , measurement noise was created using the normal distribution with parameters @xmath175 and was added to the test samples .",
    "we applied the algorithm from the previous section to derive both an optimal classifier and an optimal resource allocation . the result given in eq .",
    "( [ lasso ] ) was used to derive the optimal resource allocation for a fixed classifier .",
    "we used hinge - loss as the loss function to be minimized and approximated @xmath26 by using an adversarial ellipsoid uncertainty - set .",
    "optimization was performed using the commercially available mosek solver @xcite .",
    "* synthetic problem .",
    "* we generated 240000 samples uniformly distributed in a box in @xmath176 .",
    "we used @xmath177 as the divider and created a data - set with labels that obey @xmath178 , where @xmath179 is some small gaussian noise we added in order to make the data - set non - separable .",
    "a random subset of 10000 samples was used for learning while performance was measured on the rest",
    ". tenfold cross validation was performed .",
    "the result for different @xmath47 values is depicted at figure [ error_for_simulated ] .",
    "the method results in about 50% reduction in resources required for meeting the same error rate . in this case , the optimal classifier is similar to the classifier derived without noise and the benefit arise mainly from the redistribution of noise .",
    "we wish to confirm the result of theorem [ th : r_improve ] using similar synthetic data - sets . for this purpose ,",
    "we have generated nine data - sets each using as a divider @xmath180 for @xmath181 .",
    "for each data - set we have extracted the resources needed for achieving an error rate of @xmath182 .",
    "we calculated the ratio between the total resources required when resources are allocated optimally and those required when resources are allocated uniformly . when @xmath183 the optimal allocation is uniform and we expect no benefit ( the ratio equals one ) .",
    "as we increase @xmath184 , more resources should be allocated to @xmath185 and therefore the ratio is improving ( decreasing ) .",
    "figure [ ratio_of_improvment ] shows the resulting graph compared with the theoretical result of theorem [ th : r_improve ] ( using the optimal classifier ) .",
    "it can be seen that the simulation result is almost identical to the theoretical one , though contrary to the assumptions of theorem [ th : r_improve ] we are optimizing the hinge - loss and measuring error - rate .",
    "observe in figure [ ratio_of_improvment ] that considerable benefits arise even when the differentiation between features is rather small .",
    "* real data sets .",
    "* next , we tested the method on real - life databases from the uci repository .",
    "we started with the skin segmentation data set @xcite where rgb pixels are classified as skin or non - skin .",
    "noise was added artificially to each pixel from the 245057 available samples , a random subset of 10000 was used for learning while the rest was used to estimate performance .",
    "ten - fold cross validation was performed .",
    "the results for different @xmath47 values can be seen in figure [ error_for_skin ] .",
    "it can be seen that the method results in about 30 % reduction in resources .",
    "we tested the method on the breast cancer data set from the uci repository@xcite .",
    "this data - set contains 9 features that represent measurements from a biopsy and classified each sample as malignant or benign .",
    "the 683 samples were randomly divided , 2/3 of the data was used for training and the remaining 1/3 for testing .",
    "the results are depicted in figure [ error_for_breast ] .",
    "the optimal classifier is different than the zero - noise classifier . in order to demonstrate what portion of the benefit arise from the resource allocation and what portion from the difference between the classifiers we added a plot of the error - rate of the zero - noise classifier when resources are allocated optimally .",
    "most of the benefit comes from the correct allocation of resources .",
    "we presented a method for optimal resource allocation in classification problems along with an analysis of the expected benefits from using this method .",
    "our framework is general and we specialized it for the important special case of linear classifiers with gaussian noise or with certain adversarial disturbances .",
    "the framework we presented opens up several directions for future research .",
    "first , a natural extension of our work is to consider non - linear classifiers .",
    "this can be easily done using the  kernel - trick \" computationally . however , while the disturbance ( stochastic or adversarial ) has a comfortable shape in the input space , this does not necessarily happen in the feature space .",
    "this can probably be accommodated using the same techniques as @xcite to obtain performance bounds .",
    "second , an expansion of the framework presented is the case where resources can be further divided between samples such that `` hard '' to classify examples will receive more resources .",
    "the key observation for this is the fact that allocation of resources between features is local in nature .",
    "the global cost function @xmath7 can be replaced by @xmath186 and therefore allows deciding on the allocation of resources for each sample separately .",
    "the optimal allocation creates a function @xmath187 that can be used in the method presented in @xcite to produce optimal allocation between samples .",
    "finally , the simulation results in this paper include only noise that was artificially generated .",
    "this is due to the complexity of creating a closed - loop system that controls the acquisition process .",
    "we believe that closing a complete feedback loop in applications such as sensor networks and radar will provide similar benefit to that presented as long as the noise is appropriately modelled .",
    "[ [ proof - of - theorem - optimal - noise ] ] proof of theorem [ optimal - noise ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +          the first term is a positive semi - definite matrix that is multiplied by a positive factor ( since @xmath56 is convex ) .",
    "the second term is the hessian of @xmath20 which is positive semi - definite ( since @xmath189 is convex ) multiplied by a positive factor ( since @xmath56 is increasing ) .",
    "therefore , the hessian is positive semi - definite and @xmath56 is convex in @xmath8 .",
    "we now continue to prove the theorem by noting that problem ( [ problem : stat ] ) can be rewritten as @xmath190 the inner optimization is convex , therefore necessary and sufficient conditions are given by karush - khun - tucker @xmath191                                                rcl ( ||_t||_2 ^ 2)&=&e_t||2(<w_t,_t>-y_t)_t-_tw_t||_2 ^ 2 + & & 8(||(<w_t,_t>-y_t)_t||^2)+2||_tw_t||^2 + & & 16(||(<w_t , x_t>+<w_t , n>))(x_t+n)||^2)+16e(||y_t(x_t+n)||^2)+2||_tw_t||^2 + & & 32(||(<w_t , x_t>))x_t||^2)+32(||(<w_t , n>))n||^2 ) + & & + 32(||(<w_t , x_t>))n||^2)+32(||(<w_t , n>))x_t||^2 ) + & & + 16(||y_tx_t||^2)+16e(||y_tn||^2)+2||_tw_t||^2 + & & 32b_w^2 + 98b_w^2 + 32b_w^2 + 32b_w^2 + 16 + 32b_w^2b_x^4 + 16=g"
  ],
  "abstract_text": [
    "<S> we study classification problems where features are corrupted by noise and where the magnitude of the noise in each feature is influenced by the resources allocated to its acquisition . </S>",
    "<S> this is the case , for example , when multiple sensors share a common resource ( power , bandwidth , attention , etc . ) . </S>",
    "<S> we develop a method for computing the optimal resource allocation for a variety of scenarios and derive theoretical bounds concerning the benefit that may arise by non - uniform allocation . </S>",
    "<S> we further demonstrate the effectiveness of the developed method in simulations . </S>"
  ]
}