{
  "article_text": [
    "the new yorker cartoon caption contest has been running for more than 10 years .",
    "each week , the editors post a cartoon ( cf . figures  [ cartoon31 ] and  [ cartoon32 ] ) and ask readers to come up with a funny caption for it .",
    "they pick the top 3 submitted captions and ask the readers to pick the weekly winner .",
    "the contest has become a cultural phenomenon and has generated a lot of discussion as to what makes a cartoon funny ( at least , to the readers of the new yorker ) . in this paper , we take a computational approach to studying the contest to gain insights into what differentiates funny captions from the rest .",
    "we developed a set of unsupervised methods for ranking captions based on features such as originality , centrality , sentiment , concreteness , grammaticality , human - centeredness , etc .",
    "we used each of these methods to independently rank all captions from our corpus and selected the top captions for each method .",
    "then , we performed amazon mechanical turk experiments in which we asked turkers to judge which of the selected captions is funnier .",
    "in early work , mihalcea and strapparava investigate whether classification techniques can distinguish between humorous and non - humorous text .",
    "training data consisted of humorous one - liners ( 15 words or less ) , and non - humorous one - liners , which are derived from reuters news titles , proverbs , and sentences from the british national corpus .",
    "they looked at features such as alliteration , antonymy and adult slang .",
    "mihalcea and pullman @xcite took this work further .",
    "they looked at four semantic classes relevant to human - centeredness : persons , social groups , social relationships , and personal pronouns .",
    "they showed that social relationships and personal pronouns have high prevalence in humor .",
    "mihalcea and pullman also looked at sentiment ; they found that humor tends to have a strong negative orientation ( especially in the case of long satirical text , but regular text also shows some tendency toward the negative ) .",
    "reyes et al .",
    "@xcite used these same features as well as others to build a humor taxonomy .",
    "raz @xcite classified tweets by type and topic , while barberi @xcite focused on classifying tweets into irony , education , humour , and politics .",
    "zhang et al @xcite , also looking at tweets , used a set of manually crafted features based on influential humor theories , linguistic norms , and affective dimensions .",
    "our work differs from previous research in several ways .",
    "first , most previous work has focused on automatically distinguishing between humorous and non - humorous text . in our case ,",
    "the goal is to rank humorous texts ( and assess _ why _ they are funny ) , not perform binary classification .",
    "second , we re not aware of any work that deals specifically with cartoon captions , and although our methods are not specific to captions , we include features based on the objects depicted in the cartoons .",
    "we have access to a corpus of more than 2 m captions for more than 400 contests run since 2005 . for our experiments we picked a subset of 50 cartoons and 298,224 captions .",
    "our data set includes , for each contest , the following :    * the cartoon itself * 5,000 + captions , tokenized using clearnlp 2.0 @xcite * the three selected captions , including the winning caption * the most frequent n - grams in the captions * manually labeled objects that are visible in the cartoon * tfidf scores for all captions *  antijokes \" from two sites ( alinla and radosh ) , devoted to  unfunny \" captions",
    "we developed more than a dozen unsupervised methods for ranking the submissions for a given contest . as controls ,",
    "we use the three captions selected by the editors of the new yorker as well as antijokes . for all methods , we broke ties randomly .",
    "some of our methods can be used in two different directions ( e.g. , cu2 favors the most positive captions whereas cu2r the most negative ones ) .",
    "the methods and baselines are split into five groups : or@xmath0originality based , ge@xmath0generic , cu@xmath0content , ny@xmath0original new yorker contest , co@xmath0control .    *",
    "( or1 & or1r ) similarity to contest centroid * ( or2 & or2r ) highest / lowest lexrank * ( or3 & or3r ) largest / smallest cluster * ( or4 ) highest average tfidf * ( cu1 ) presence of freebase entities @xcite * ( cu2 & cu2r ) caption sentiment * ( cu3 ) human - centeredness * ( ge1 ) most syntactically complex * ( ge2 ) most concrete ( i.e. , refers to objects present in the cartoon ) * ( ge3 & ge3r ) unusually formatted text * ( ny1 ) first place official * ( ny2 ) second place official * ( ny3 ) third place official * ( co2 ) antijokes      we built a lexical network out of the captions for each contest .",
    "we used lexrank to identify the most central caption in each contest ( method or1 ) and the one with the highest lexrank score ( method or2 ) .",
    "we also used a graph clustering method , previously used in king et al .",
    ", to cluster the captions in each contest thematically ; the sizes of these clusters comprise method or3 .",
    "the tfidf scores used to build the lexical network are used in method or4 .    ....   0 0     if that 's theseus , i ' m not here .   1 0",
    "if it 's theseus , tell him i 'll be back in the labyrinth just as soon as happy hour is over .   2 0     if that 's theseus , i just left .   3 0",
    "if it 's theseus , tell him to get lost .   4 1     if that 's elsie , you have n't seen me .   5 2",
    "if that 's bessie , tell her i ' ve moooooved on !   6 3     if its my wife , tell her i ' m in a china shop .",
    "7 3     i got kicked out of the china shop .",
    "8 5     if that 's merrill lynch , tell them i quit and went to pamplona .",
    "9 5     if that 's my wife , tell her i went to pamplona .",
    "10 4     if it 's my wife , tell her that i ran into an old minotaur friend .",
    "11 4     if that 's my wife tell her i 'll be home in a minotaur .",
    "jeez ! what 's a minotaur got to do to get a drink around here ?",
    "13 4     if i hear that ' a guy and a minotaur go into a bar ' joke one more time ...   14 5",
    "if that 's merrill lynch , tell them i 'll be back when i ' m good and ready .",
    "15 5     if it 's my wife , i was working late on a merrill - lynch commercial .   16 5",
    "if that 's my cow , tell her i left for pamplona .",
    "17 3     this 'll be the last one .",
    "i need to get back to the china shop .",
    "18 6     if that 's my matador , tell him i ' m not here .   19 5     if that 's merrill or lynch , tell ' em i ' m not here .   ....    figure  [ circularplot ] shows the pairwise similarities for the captions in the mini - corpus .",
    "the seven clusters are identified by the louvain method .",
    "solid lines represent high cosine similarity between a pair of captions .",
    "the captions in the mini - corpus are shown in figure  [ mini - corpus ] .",
    "the seven clusters in figure  [ minotaurgraph ] are identified by the louvain method .",
    "solid lines represent high cosine similarity between a pair of captions .              for cu1 , we annotated the captions for freebase entities by querying noun - phrases ( within a caption ) over freebase indexed entities .",
    "we scored each caption using idf @xmath1 freebase score , where the freebase score captures relevance .    to compute the sentiment polarity of each caption ( method cu2 ) , we used stanford corenlp @xcite to annotate each sentence with its sentiment from 0 ( very negative ) to 4 ( very positive ) .",
    "only 13.20% had positive polarity ; 51.09% had negative polarity , and the rest were neutral .    for human - centeredness ( method cu3 ) , we followed the method described in mihalcea and pullman @xcite .",
    "we used wordnet @xcite to list all the word forms derived from the \\{_person , individual , someone , somebody , mortal , human , soul _ } synset ( `` people '' set ) , as well as those belonging to the \\{_relative , relation _ } synset ( `` relatives '' set ) .",
    "we excluded personal pronouns , as 75.96% of the captions contained at least one .",
    "we also accounted for any proper names as part of the `` people '' set .",
    "25.33% of the captions mentioned at least one `` person '' , but only 3.60% contained a word from the `` relatives '' set .",
    "we computed syntactic complexity ( ge1 ) using @xcite .",
    "for concreteness ( ge2 ) , two of the authors of this paper labeled all the objects in each of the 50 cartoons used in our evaluation .",
    "we then computed how often any of those objects were referred to ( with a nominal np ) in each caption .",
    "we computed ge3 by counting punctuation marks and unusually formatted ( e.g. very long ) words in each caption .",
    "[ cols=\"<,^,>,>,>,>,>,>,>,>\",options=\"header \" , ]",
    "we used amazon mechanical turk ( amt ) to compare the outputs of the different methods and the baselines .",
    "each amt hit consisted of one cartoon as well as two captions , a and b ( produced by one of the 18 methods and baselines ) .",
    "the turkers had to determine which of the two captions is funnier .",
    "they were given four options - `` a is funnier '' , `` b is funnier '' , `` both are funny '' , `` neither is funny '' .",
    "they did not know which method was used to produce caption a or b. all pairs of captions from our methods were compared for each cartoon , and each hit ( pair ) was assessed by 7 turkers .",
    "we report on three evaluations in table  [ results ] .",
    "each evaluation ( @xmath2 , @xmath3 pair ) corresponds to the number of votes in favor of the given method minus the number of votes against .",
    "so the first set corresponds to pairs in which , out of seven judges , there was a difference of at least 4 votes in favor of one or the other caption .",
    "this level of significant agreement happened in 5,594/15,154 cases ( 36.9% of the time ) .",
    "a difference of at least 3 votes happened in 8,131/15,154 pairs ( 53.6% ) .",
    "the third evaluation corresponds to all pairwise comparisons , including ties .",
    "@xmath2 refers to the number of times the above constraint for @xmath4 is met and score @xmath3 is calculated by averaging the number of votes in favor minus the number of votes against for each @xmath2 .",
    "the probability that a random process will generate a difference of at least 4 votes ( excluding ties ) is 12.5% .",
    "we compared over a dozen methods for selecting the funniest caption among 5,000 submissions to the new yorker caption contest . using side by side funniness assessments from amt",
    ", we found that the methods that consistently select funnier captions are negative sentiment , human - centeredness , and lexical centrality .",
    "not surprisingly , knowing the traditions of the new yorker cartoons , negative captions were funnier than positive captions .",
    "captions that relate to people were consistently deemed funnier .",
    "the first two methods ( negative sentiment and human - centeredness ) are consistent with the findings in mihalcea and pullman @xcite .",
    "more interestingly , we also showed that captions that reflect the collective wisdom of the contest participants outperformed semantic outliers .",
    "the next two strongest features were positive sentiment and proper formatting .",
    "we are making our corpus public for research and for a shared task on funniness detection .",
    "the corpus includes our 50 selected cartoons , more than 5,000 captions per cartoon , manual annotations of the entities in the cartoons , automatically extracted topics from each contest , and the funniness scores .",
    "in this paper , we used unsupervised methods for funniness detection .",
    "we will next explore supervised and ensemble methods .",
    "( however , ensemble methods may not work for this task as captions may be funny in different ways ; for example , of two equally funny captions , one may be funny - absurd and the other funny - ironic . )",
    "we will also explore pun recognition ( e.g. , `` tell my wife i ll be home in a _",
    "minotaur_. '' ) , other creative uses of language , as well as more semantic features .",
    "christopher  d. manning , mihai surdeanu , john bauer , jenny finkel , steven  j. bethard , and david mcclosky . the stanford corenlp natural language processing toolkit . in _ proceedings of the acl _ , pages 5560 , 2014 ."
  ],
  "abstract_text": [
    "<S> the new yorker publishes a weekly captionless cartoon . </S>",
    "<S> more than 5,000 readers submit captions for it . </S>",
    "<S> the editors select three of them and ask the readers to pick the funniest one . </S>",
    "<S> we describe an experiment that compares a dozen automatic methods for selecting the funniest caption . </S>",
    "<S> we show that negative sentiment , human - centeredness , and lexical centrality most strongly match the funniest captions , followed by positive sentiment . </S>",
    "<S> these results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal ( vision+text ) systems . as part of this work , a large set of cartoons and captions </S>",
    "<S> is being made available to the community . </S>"
  ]
}