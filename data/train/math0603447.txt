{
  "article_text": [
    "let @xmath0 be a measurable space .",
    "we consider a random variable @xmath1 on @xmath2 with probability distribution denoted by @xmath3 .",
    "denote by @xmath4 the marginal of @xmath3 on @xmath5 and by @xmath6 the conditional probability function of @xmath7 , knowing that @xmath8 .",
    "we have @xmath9 i.i.d .",
    "observations of the couple @xmath1 denoted by @xmath10 .",
    "the aim is to predict the output label @xmath11 for any input @xmath12 in @xmath5 from the observations @xmath13 .",
    "we recall some usual notation for the classification framework .",
    "a * prediction rule * is a measurable function @xmath14 .",
    "the * misclassification error * associated with @xmath15 is @xmath16 it is well known ( see , e.g. , devroye _ et al . _",
    "@xcite ) that @xmath17 where the prediction rule @xmath18 , called the * bayes rule * , is defined by @xmath19 the minimal risk @xmath20 is called the * bayes risk*. a * classifier * is a function , @xmath21 , measurable with respect to @xmath13 and @xmath12 with values in @xmath22 , that assigns to the sample @xmath13 a prediction rule @xmath23 .",
    "a key characteristic of @xmath24 is the * generalization error * @xmath25 $ ] , where @xmath26 the aim of statistical learning is to construct a classifier @xmath24 such that @xmath25 $ ] is as close to @xmath20 as possible .",
    "accuracy of a classifier @xmath24 is measured by the value @xmath27 $ ] , called the * excess bayes risk * of @xmath24 .",
    "we say that the classifier @xmath24 learns with the convergence rate @xmath28 , where @xmath29 is a decreasing sequence , if there exists an absolute constant @xmath30 such that for any integer @xmath9 , @xmath31\\leq c\\psi(n)$ ] .",
    "given a convergence rate , theorem 7.2 of devroye _ et al . _",
    "@xcite shows that no classifier can learn at least as fast as this rate for any arbitrary underlying probability distribution @xmath3 . to achieve rates of convergence ,",
    "we need a complexity assumption on the set which the bayes rule @xmath18 belongs to .",
    "for instance , yang @xcite provide examples of classifiers learning with a given convergence rate under complexity assumptions .",
    "these rates can not be faster than @xmath32 ( cf .",
    "nevertheless , they can be as fast as @xmath33 if we add a control on the behavior of the conditional probability function @xmath34 at the level @xmath35 ( the distance @xmath36 is sometimes called the * margin * ) . for the problem of discriminant analysis , which is close to our classification problem ,",
    "mammen and tsybakov @xcite and tsybakov @xcite have introduced the following assumption .",
    "the probability distribution @xmath3 on the space @xmath37 satisfies @xmath38 ) with @xmath39 if there exists @xmath40 such that @xmath41\\leq c_0 \\bigl(r(f)-r^ { * } \\bigr)^{1/\\kappa } , \\ ] ] for any measurable function @xmath15 with values in @xmath42    according to tsybakov @xcite and boucheron _ et al . _",
    "@xcite , this assumption is equivalent to a control on the margin given by @xmath43\\leq c t^{\\alpha}\\qquad\\forall0\\leq t < 1.\\ ] ] several example of * fast rates * , that is , rates faster than @xmath32 , can be found in blanchard _",
    "@xcite , steinwart and scovel @xcite , massart @xcite , massart and ndlec @xcite , massart @xcite and audibert and tsybakov @xcite .",
    "the paper is organized as follows . in section , [ sectiondef ]",
    "we introduce definitions and procedures which are used throughout the paper .",
    "section [ sectionoptimalhinge ] contains oracle inequalities for our aggregation procedures w.r.t .",
    "the excess hinge risk .",
    "section [ sectionexcessrisk ] contains similar results for the excess bayes risk .",
    "proofs are postponed to section [ proofs ] .",
    "convex surrogates @xmath44 for the classification loss are often used in algorithm ( cortes and vapnic @xcite , freund and schapire @xcite , lugosi and vayatis @xcite , friedman _ et al . _",
    "@xcite , bhlman and yu @xcite , bartlett _",
    "let us introduce some notation .",
    "take @xmath44 to be a measurable function from @xmath45 to @xmath45 . the risk associated with the loss function @xmath44",
    "is called the @xmath46-*risk * and is defined by @xmath47,\\ ] ] where @xmath48 is a measurable function .",
    "the * empirical * @xmath44-*risk * is defined by @xmath49 and we denote by @xmath50 the infimum over all real - valued functions @xmath51 .",
    "classifiers obtained by minimization of the empirical @xmath44-risk , for different convex losses , have been proven to have very good statistical properties ( cf .",
    "lugosi and vayatis @xcite , blanchard _ et al . _",
    "@xcite , zhang @xcite , steinwart and scovel @xcite and bartlett _ et al . _",
    "a wide variety of classification methods in machine learning are based on this idea , in particular , on using the convex loss @xmath52 associated with support vector machines ( cortes and vapnik  @xcite , schlkopf and smola @xcite ) , called the * hinge loss*. the corresponding risk is called the * hinge risk * and is defined by @xmath53,\\ ] ] for any measurable function @xmath48 .",
    "the * optimal hinge risk * is defined by @xmath54 it is easy to check that the bayes rule @xmath18 attains the infimum in ( [ ohr ] ) and that @xmath55 for any measurable function @xmath15 with values in @xmath45 ( cf .",
    "lin  @xcite and generalizations in zhang @xcite and bartlett _ et al . _",
    "@xcite ) , where we extend the definition of @xmath56 to the class of real - valued functions by @xmath57 .",
    "thus , minimization of the * excess hinge risk * , @xmath58 , provides a reasonable alternative for minimization of the excess bayes risk , @xmath59 .",
    "now , we introduce the problem of aggregation and the aggregation procedures which will be studied in this paper .",
    "suppose that we have @xmath60 different classifiers @xmath61 taking values in @xmath22 .",
    "the problem of model selection type aggregation , as studied in nemirovski @xcite , yang @xcite , catoni @xcite and tsybakov @xcite , consists of the construction of a new classifier @xmath62 ( called an * aggregate * ) which approximately mimics the best classifier among @xmath61 . in most of these papers",
    "the aggregation is based on splitting the sample into two independent subsamples , @xmath63 and @xmath64 , of sizes @xmath65 and @xmath66 , respectively , where @xmath67 .",
    "the first subsample , @xmath68 , is used to construct the classifiers @xmath61 and the second subsample , @xmath69 , is used to aggregate them , that is to construct a new classifier that mimics , in a certain sense , the behavior of the best among the classifiers @xmath70 .    in this paper",
    ", we will not consider the sample splitting and will concentrate only on the construction of aggregates ( following juditsky and nemirovski @xcite , tsybakov @xcite , birg @xcite , bunea _ et al . _",
    "thus , the first subsample is fixed and , instead of classifiers @xmath61 , we have fixed prediction rules @xmath71 . rather than working with a part of the initial sample",
    "we will suppose , for notational simplicity , that the whole sample @xmath13 of size @xmath9 is used for the aggregation step instead of a subsample @xmath72 .",
    "let @xmath73 be a finite set of real - valued functions , where @xmath60 .",
    "an * aggregate * is a real - valued statistic of the form @xmath74 where the weights @xmath75 satisfy @xmath76 let @xmath44 be a convex loss for classification . the empirical risk minimization aggregate ( * erm * ) is defined by the weights @xmath77 the erm aggregate is denoted by @xmath78 .",
    "the * averaged erm * aggregate is defined by the weights @xmath79 where @xmath80 is the number of functions in @xmath81 minimizing the empirical @xmath44-risk .",
    "the averaged erm aggregate is denoted by @xmath82 .",
    "the aggregation with exponential weights aggregate ( * aew * ) is defined by the weights @xmath83 the aew aggregate is denoted by @xmath84 .",
    "the * cumulative aew * aggregate is an on - line procedure defined by the weights @xmath85 the cumulative aew aggregate is denoted by @xmath86 .",
    "when @xmath81 is a class of prediction rules , intuitively , the aew aggregate is more robust than the erm aggregate w.r.t .",
    "the problem of overfitting .",
    "if the classifier with smallest empirical risk is overfitted , that is , if it fits too many to the observations , then the erm aggregate will be overfitted .",
    "but , if other classifiers in @xmath87 are good classifiers , then the aggregate with exponential weights will consider their `` opinions '' in the final decision procedure and these opinions can balance with the opinion of the overfitted classifier in @xmath81 , which can be false because of its overfitting property .",
    "the erm only considers the `` opinion '' of the classifier with the smallest risk , whereas the aew takes into account all of the opinions of the classifiers in the set @xmath81 .",
    "the exponential weights , defined in ( [ aew ] ) , can be found in several situations .",
    "first , one can check that the solution of the minimization problem @xmath88 for all @xmath89 is @xmath90 thus , for @xmath91 , we find the exponential weights used for the aew aggregate .",
    "second , these weights can also be found in the theory of prediction of individual sequences ( cf .",
    "vovk @xcite ) .",
    "now , we introduce a concept of optimality for an aggregation procedure and for rates of aggregation , in the same spirit as in tsybakov @xcite ( where the regression problem is treated ) . our aim is to prove that the aggregates introduced above are optimal in the following sense .",
    "we denote by @xmath92 the set of all probability measures @xmath3 on @xmath2 satisfying @xmath93 ) .",
    "[ defoptimality ] let @xmath44 be a loss function .",
    "the remainder term @xmath94 is called an * optimal rate of model selection type aggregation ( ms - aggregation ) for the @xmath44-risk * if the two following inequalities hold :    a.   @xmath95 , there exists a statistic @xmath62 , depending on @xmath81 , such that @xmath96 , @xmath97 @xmath98\\leq \\min_{f\\in\\mathcal{f}}\\bigl(a^{(\\phi)}(f)-a^{(\\phi ) * } \\bigr)+c_1\\gamma(n , m,\\kappa,\\mathcal{f},\\pi);\\ ] ] b.   @xmath99 such that for any statistic @xmath100 , @xmath101 , @xmath102 @xmath103\\geq \\min_{f\\in\\mathcal{f}}\\bigl(a^{(\\phi)}(f)-a^{(\\phi ) * } \\bigr)+c_2\\gamma(n , m,\\kappa,\\mathcal{f},\\pi).\\ ] ]    here , @xmath104 and @xmath105 are positive constants which may depend on @xmath106 .",
    "moreover , when these inequalities are satisfied , we say that the procedure @xmath62 , appearing in ( [ deforacle ] ) , is an * optimalms - aggregate for the @xmath44-risk*. if @xmath107 denotes the convex hull of @xmath81 and if ( [ deforacle ] ) and ( [ deflower ] ) are satisfied with @xmath108 replaced by @xmath109 , then we say that @xmath94 is an * optimal rate of convex aggregation type for the * @xmath44-*risk * and @xmath62 is an * optimal convex aggregation procedure for the * @xmath44-*risk*.    in tsybakov @xcite , the optimal rate of aggregation depends only on @xmath110 and @xmath9 . in our case , the residual term may be a function of the underlying probability measure @xmath3 , of the class @xmath81 and of the margin parameter @xmath106 .",
    "note that , without any margin assumption , we obtain @xmath111 for the residual , which is free from @xmath3 and @xmath81 . under the margin assumption",
    ", we obtain a residual term dependent of @xmath3 and @xmath81 and it should be interpreted as a normalizing factor in the ratio @xmath112- \\min_{f\\in\\mathcal{f}}(a^{(\\phi)}(f)-a^{(\\phi ) * } ) } { \\gamma(n , m,\\kappa,\\mathcal{f},\\pi)}.\\ ] ] in that case , our definition does not imply the uniqueness of the residual .",
    "observe that a linear function achieves its maximum over a convex polygon at one of the vertices of the polygon .",
    "the hinge loss is linear on @xmath113 $ ] and @xmath107 is a convex set , thus ms - aggregation or convex aggregation of functions with values in @xmath113 $ ] are identical problems when we use the hinge loss .",
    "that is , we have @xmath114",
    "take @xmath110 functions @xmath115 with values in @xmath113 $ ] . consider the convex hull @xmath116 .",
    "we want to mimic the best function in @xmath107 using the hinge risk and working under the margin assumption .",
    "we first introduce a margin assumption w.r.t . the hinge loss .",
    "the probability distribution @xmath3 on the space @xmath37 satisfies the margin assumption for hinge risk @xmath117 with parameter @xmath39 if there exists @xmath118 such that @xmath119\\leq c\\bigl(a(f)-a^ * \\bigr)^{1/\\kappa}\\ ] ] for any function @xmath15 on @xmath5 with values in @xmath113 $ ] .",
    "[ proporac2 ] the assumption @xmath120 ) is equivalent to the margin assumption @xmath93 ) .    in what follows",
    ", we will assume that @xmath93 ) holds and thus also that @xmath120 ) holds .",
    "the aew aggregate of @xmath110 functions @xmath115 with values in @xmath113 $ ] , introduced in ( [ aew ] ) for a general loss , has a simple form for the case of the hinge loss , given by @xmath121\\\\[-8pt ] \\nonumber&&\\qquad\\mbox{where } w^{(n)}(f_j)=\\frac{\\exp ( \\sum_{i=1}^ny_if_j(x_i))}{\\sum_{k=1}^m\\exp ( \\sum_{i=1}^ny_if_k(x_i))}\\ \\forall j=1,\\ldots , m.\\end{aligned}\\ ] ]    in theorems [ oraclea ] and [ optimalkappa ] , we state the optimality of our aggregates in the sense of definition [ defoptimality ] .    [ oraclea ] let @xmath122 .",
    "we assume that @xmath3 satisfies @xmath93 ) .",
    "we denote by @xmath107 the convex hull of a finite set @xmath81 of functions @xmath115 with values in @xmath113 $ ] .",
    "let @xmath62 be either of the four aggregates introduced in section [ subsectionaggregationprocedures ] .",
    "then , for any integers @xmath123 , @xmath62 satisfies the inequality @xmath124&\\leq & \\min_{f\\in\\mathcal{c}}\\bigl(a(f)-a^*\\bigr)\\\\ & & { } + c\\biggl(\\sqrt{\\frac{\\min_{f\\in \\mathcal{c}}(a(f)-a^*)^{{1}/{\\kappa}}\\log m}{n}}+\\biggl(\\frac{\\log m}{n}\\biggr)^{{\\kappa}/{(2\\kappa-1)}}\\biggr),\\end{aligned}\\ ] ] where @xmath125 for the erm , aerm and aew aggregates with @xmath126 , @xmath118 is the constant in ( [ mahinge ] ) and @xmath127 for the caew aggregate with @xmath128 . for @xmath129 , the caew",
    "aggregate satisfies @xmath130&\\leq & \\min_{f\\in\\mathcal{c}}\\bigl(a(f)-a^*\\bigr)\\\\ & & { } + 2c\\biggl(\\sqrt{\\frac{\\min_{f\\in \\mathcal{c}}(a(f)-a^*)\\log m}{n}}+\\frac{(\\log m)\\log n}{n}\\biggr).\\end{aligned}\\ ] ]    [ optimalkappa ] let @xmath126 and let @xmath131 be two integers such that @xmath132 .",
    "we assume that the input space @xmath5 is infinite .",
    "there exists an absolute constant @xmath30 , depending only on @xmath106 and @xmath133 , and a set of prediction rules @xmath73 such that for any real - valued procedure @xmath100 , there exists a probability measure @xmath3 satisfying @xmath93 ) , for which @xmath134&\\geq & \\min_{f\\in\\mathcal { c}}\\bigl(a(f)-a^*\\bigr ) \\\\ & & { } + c\\biggl(\\sqrt{\\frac{(\\min_{f\\in\\mathcal { c}}a(f)-a^*)^{1/\\kappa}\\log m}{n}}+\\biggl(\\frac{\\log m}{n}\\biggr)^{{\\kappa}/{(2\\kappa-1)}}\\biggr),\\end{aligned}\\ ] ] where @xmath135 and @xmath118 is the constant in ( [ mahinge ] ) .    combining the exact oracle inequality of theorem [ oraclea ] and the lower bound of theorem [ optimalkappa ]",
    ", we see that the residual @xmath136 is an optimal rate of convex aggregation of @xmath110 functions with values in @xmath113 $ ] for the hinge loss .",
    "moreover , for any real - valued function @xmath15 , we have @xmath137 for all @xmath138 and @xmath139 , thus @xmath140 thus , by aggregating @xmath141 , it is easy to check that @xmath142 is an optimal rate of model - selection aggregation of @xmath110 real - valued functions @xmath115 w.r.t . the hinge loss . in both cases",
    ", the aggregate with exponential weights , as well as erm and aerm , attains these optimal rates and the caew aggregate attains the optimal rate if @xmath128 .",
    "applications and learning properties of the aew procedure can be found in lecu @xcite ( in particular , adaptive svm classifiers are constructed by aggregating only @xmath143 svm estimators ) . in theorem",
    "[ oraclea ] , the aew procedure satisfies an exact oracle inequality with an optimal residual term whereas in lecu @xcite and lecu @xcite the oracle inequalities satisfied by the aew procedure are not exact ( there is a multiplying factor greater than @xmath144 in front of the bias term ) and in lecu @xcite , the residual is not optimal . in lecu @xcite , it is proved that for any finite set @xmath81 of functions @xmath115 with values in @xmath113 $ ] and any @xmath89 , there exists an absolute constant @xmath145 such that , for @xmath107 the convex hull of @xmath81 , @xmath146\\leq ( 1+\\epsilon ) \\min_{f\\in\\mathcal{c}}\\bigl(a(f)-a^*\\bigr)+c(\\epsilon ) \\biggl(\\frac{\\log m}{n}\\biggr)^{{\\kappa}/{(2\\kappa-1)}}.\\ ] ] this oracle inequality is good enough for several applications ( see the examples in lecu @xcite ) .",
    "nevertheless , ( [ equaleccolt ] ) can be easily deduced from theorem [ oraclea ] using lemma [ lemtsysara ] and may be inefficient for constructing adaptive estimators with exact constants ( because of the factor greater than @xmath144 in front of @xmath147 ) .",
    "moreover , oracle inequalities with a factor greater than @xmath144 in front of the oracle @xmath147 do not characterize the real behavior of the technique of aggregation which we are using .",
    "for instance , for any strictly convex loss @xmath44 , the erm procedure satisfies ( cf . chesneau and lecu @xcite ) @xmath148\\leq(1+\\epsilon )",
    "\\min_{f\\in\\mathcal{f}}\\bigl(a^{(\\phi)}(f)-a^{(\\phi)*}\\bigr)+c(\\epsilon ) \\frac{\\log m}{n}.\\ ] ] but , it has been recently proven , in lecu @xcite , that the erm procedure can not mimic the oracle faster than @xmath111 , whereas , for strictly convex losses , the caew procedure can mimic the oracle at the rate @xmath149 ( cf .",
    "thus , for strictly convex losses , it is better to use the aggregation procedure with exponential weights than erm ( or even penalized erm procedures ( cf . lecu @xcite ) ) to mimic the oracle .",
    "non - exact oracle inequalities of the form ( [ equanonexcatoraclphi ] ) can not tell us which procedure is better to use since both erm and caew procedures satisfy this inequality .",
    "it is interesting to note that the rate of aggregation ( [ rateofaggregation ] ) depends on both the class @xmath81 and @xmath3 through the term @xmath150 .",
    "this is different from the regression problem ( cf .",
    "tsybakov @xcite ) , where the optimal aggregation rates depend only on @xmath110 and @xmath9 .",
    "three cases can be considered , where @xmath151 denotes @xmath152 and @xmath110 may depend on @xmath9 ( i.e. , for function classes @xmath81 depending on @xmath9 ) :    1 .   if @xmath153 , for an absolute constant @xmath154 , then the hinge risk of our aggregates attains @xmath150 with the rate @xmath155 , which can be @xmath156 in the case @xmath157 ; 2 .",
    "if @xmath158 for some constants @xmath159 , then our aggregates mimic the best prediction rule in @xmath107 with a rate slower than @xmath160 , but faster than @xmath161 ; 3 .",
    "if @xmath162 , where @xmath154 is a constant , then the rate of aggregation is @xmath163 as in the case of no margin assumption .",
    "we can explain this behavior by the fact that not only @xmath106 , but also @xmath150 , measures the difficulty of classification .",
    "for instance , in the extreme case where @xmath164 , which means that @xmath107 contains the bayes rule , we have the fastest rate @xmath160 . in the worst cases , which are realized when @xmath106 tends to @xmath165 or @xmath166 , where @xmath154 is an absolute constant ,",
    "the optimal rate of aggregation is the slow rate @xmath167",
    "we now provide oracle inequalities and lower bounds for the excess bayes risk .",
    "first , we can deduce , from theorem [ oraclea ] and [ optimalkappa ] , ` almost optimal rates of aggregation ' for the excess bayes risk achieved by the aew aggregate .",
    "second , using the erm aggregate , we obtain optimal rates of model selection aggregation for the excess bayes risk .    using inequality ( [ e1 ] )",
    ", we can derive , from theorem [ oraclea ] , an oracle inequality for the excess bayes risk .",
    "the lower bound is obtained using the same proof as in theorem [ optimalkappa ] .",
    "[ oracler ] let @xmath73 be a finite set of prediction rules for an integer @xmath168 and @xmath126 .",
    "we assume that @xmath3 satisfies @xmath93 ) .",
    "denote by @xmath62 either the erm , the aerm or the aew aggregate . for any number @xmath154 and any integer @xmath9",
    ", @xmath62 then satisfies @xmath169&\\leq & 2(1+a)\\min_{j=1,\\ldots , m}\\bigl(r(f_j)-r^*\\bigr)\\nonumber\\\\[-8pt]\\\\[-8pt ] \\nonumber & & { } + \\big[c+(c^{2\\kappa } /a)^{1/(2\\kappa-1)}\\big ] \\biggl(\\frac{\\log m}{n}\\biggr)^{{\\kappa}/{(2\\kappa-1)}},\\end{aligned}\\ ] ] where @xmath125 .",
    "the caew aggregate satisfies the same inequality with @xmath127 when @xmath128 . for @xmath129 , the caew aggregate satisfies ( [ approxoracler ] ) , where we need to multiply the residual by @xmath170 .",
    "moreover , there exists a finite set of prediction rules @xmath73 such that , for any classifier @xmath100 , there exists a probability measure @xmath3 on @xmath171 satisfying @xmath93 ) , such that , for any @xmath172 , @xmath173\\geq 2(1+a)\\min_{f\\in\\mathcal{f}}\\bigl(r(f)-r^ * \\bigr)+c(a)\\biggl(\\frac{\\log m}{n } \\biggr)^{{\\kappa}/{(2\\kappa-1)}},\\ ] ] where @xmath174 is a constant depending only on @xmath175 .",
    "due to corollary [ oracler ] , @xmath176 is an almost optimal rate of ms - aggregation for the excess risk and the aew aggregate achieves this rate .",
    "the word `` almost '' is used here because @xmath177 is multiplied by a constant greater than @xmath144 .",
    "oracle inequality ( [ approxoracler ] ) is not exact since the minimal excess risk over @xmath81 is multiplied by the constant @xmath178 .",
    "this is not the case when using the erm aggregate , as explained in the following theorem .",
    "[ theooracler ] let @xmath126 .",
    "we assume that @xmath3 satisfies @xmath93 ) .",
    "we denote by @xmath73 a set of prediction rules .",
    "the erm aggregate over @xmath81 satisfies , for any integer @xmath179 , @xmath180&\\leq & \\min_{f\\in\\mathcal{f}}\\bigl(r(f)-r^*\\bigr)\\\\ & & { } + c\\biggl(\\sqrt{\\frac{\\min_{f\\in \\mathcal{f}}(r(f)-r^*)^{{1}/{\\kappa}}\\log m}{n}}+\\biggl(\\frac{\\log m}{n}\\biggr)^{\\kappa/(2\\kappa-1)}\\biggr),\\end{aligned}\\ ] ] where @xmath181 and @xmath182 is the constant appearing in @xmath93 ) .    using lemma [ lemtsysara ]",
    ", we can deduce the results of herbei and wegkamp @xcite from theorem [ theooracler ] .",
    "oracle inequalities under @xmath93 ) have already been stated in massart @xcite ( cf .",
    "@xcite ) , but the remainder term obtained is worse than the one obtained in theorem [ theooracler ] .    according to definition [ defoptimality ] , combining theorem [ theooracler ] and the following theorem , the rate @xmath183 is an optimal rate of ms - aggregation w.r.t .",
    "the excess bayes risk .",
    "the erm aggregate achieves this rate .",
    "[ optimalkappar ] let @xmath168 and @xmath9 be two integers such that @xmath132 and @xmath126 .",
    "assume that @xmath5 is infinite .",
    "there exists an absolute constant @xmath30 and a set of prediction rules @xmath73 such that for any procedure @xmath100 with values in @xmath45 , there exists a probability measure @xmath3 satisfying @xmath93 ) , for which @xmath184&\\geq&\\min_{f\\in\\mathcal { f}}\\bigl(r(f)-r^*\\bigr)\\\\ & & + { } c\\biggl(\\sqrt{\\frac{(\\min_{f\\in\\mathcal{f}}r(f)-r^*)^ { { 1}/{\\kappa}}\\log m}{n}}+\\biggl(\\frac{\\log m}{n}\\biggr)^{\\kappa/(2\\kappa-1)}\\biggr),\\end{aligned}\\ ] ] where @xmath185 and @xmath182 is the constant appearing in @xmath93 ) .",
    "proof of proposition [ proporac2 ] since , for any function @xmath15 from @xmath5 to @xmath22 , we have @xmath186 , it follows that @xmath93 ) is implied by mah(@xmath106 ) .",
    "assume that @xmath93 ) holds .",
    "we first explore the case @xmath128 , where @xmath93 ) implies that there exists a constant @xmath187 such that @xmath188 for any @xmath189 ( cf .",
    "let @xmath15 be a function from @xmath5 to @xmath113 $ ] .",
    "we have , for any @xmath189 , @xmath190 \\\\ & \\geq & t\\mathbb{e}\\bigl[|f(x)-f^*(x)|{\\mathbh{1}}_{|2\\eta(x)-1|\\geq t } \\bigr]\\\\ & \\geq & t\\bigl(\\mathbb{e}[|f(x)-f^*(x)|]-2\\mathbb{p } \\bigl(|2\\eta(x)-1|\\leq t \\bigr ) \\bigr)\\\\ & \\geq & t\\bigl(\\mathbb{e}[|f(x)-f^*(x)|]-2c_1t^{1/(\\kappa-1 ) } \\bigr).\\end{aligned}\\ ] ] for @xmath191^{\\kappa-1}$ ] , we obtain @xmath192^{\\kappa}.\\ ] ]    for the case @xmath129 , @xmath193 ) implies that there exists @xmath194 such that @xmath195 a.s . indeed ,",
    "if for any @xmath196 ( the set of all positive integers ) , there exists @xmath197 ( the @xmath198-algebra on @xmath5 ) such that @xmath199 and @xmath200 , then , for @xmath201 we obtain @xmath202 and @xmath203=2p^x(a_n),$ ] and there is no constant @xmath40 such that @xmath204 for all @xmath196 .",
    "so , assumption @xmath193 ) does not hold if no @xmath194 satisfies @xmath195 a.s .",
    "thus , for any @xmath15 from @xmath5 to @xmath113 $ ] , we have @xmath205\\geq h \\mathbb{e}[|f(x)-f^*(x)| ] .$ ]    proof of theorem [ oraclea ] we start with a general result which says that if @xmath44 is a convex loss , then the aggregation procedures with the weights @xmath206 , introduced in ( [ aew ] ) satisfy @xmath207 indeed , take @xmath44 to be a convex loss .",
    "we have @xmath208 thus @xmath209 any @xmath210 satisfies @xmath211 thus , by averaging this equality over the @xmath212 and using @xmath213 , where @xmath214 denotes the kullback  leibler divergence between the weights @xmath215 and the uniform weights @xmath216 , we obtain the first inequality of  ( [ agregatvserm1 ] ) . using the convexity of @xmath44 , we obtain a similar result for the aerm aggregate .",
    "let @xmath89 .",
    "we consider @xmath218 , where @xmath219 .",
    "let @xmath220 . if @xmath221 then , for any @xmath222 , we have @xmath223 because @xmath224 .",
    "hence , @xmath225\\\\ & & \\quad \\leq \\mathbb{p}\\biggl[\\sup_{f\\in\\mathcal{d}}\\frac { a(f)-a^*-(a_n(f)-a_n(f^*))}{a(f)-a^*+x } > \\frac{\\epsilon}{a_\\mathcal{c}-a^*+2\\epsilon+x } \\biggr]\\nonumber.\\end{aligned}\\ ] ]    according to ( [ hinglinearhingeconvex ] ) , for @xmath226 such that @xmath227 , we have @xmath228 . according to ( [ agregatvserm ] )",
    ", we have @xmath229 thus , if we assume that @xmath230 , then , by definition , we have @xmath231 and thus there exists @xmath222 such that @xmath232 . according to ( [ equapremineg ] ) , we have @xmath233 \\\\ & & \\quad   \\leq \\mathbb{p}\\biggl[\\inf_{f\\in\\mathcal{d}}a_n(f)-a_n(f^ * ) \\leq a_n(f')-a_n(f^*)+\\frac{\\log m}{n } \\biggr ] \\\\ & & \\quad \\leq \\mathbb{p}\\biggl[\\inf_{f\\in\\mathcal{d } } a_n(f)-a_n(f^*)\\leq a_\\mathcal{c}-a^*+\\epsilon\\biggr ] \\\\ & & \\qquad{}+\\mathbb{p}\\biggl[a_n(f')-a_n(f^*)\\geq a_\\mathcal{c}-a^*+ \\epsilon-\\frac{\\log m}{n } \\biggr ] \\\\ & & \\quad \\leq \\mathbb{p}\\biggl[\\sup_{f\\in\\mathcal{c}}\\frac { a(f)-a^*-(a_n(f)-a_n(f^*))}{a(f)-a^*+x } > \\frac{\\epsilon}{a_\\mathcal{c}-a^*+2\\epsilon+x } \\biggr]\\\\ & & \\qquad { } + \\mathbb{p}\\biggl[a_n(f')-a_n(f^*)\\geq a_\\mathcal{c}-a^*+ \\epsilon-\\frac{\\log m}{n } \\biggr].\\end{aligned}\\ ] ] if we assume that @xmath234 then there exists @xmath235 ( where @xmath236 and @xmath237 ) such that @xmath238 the linearity of the hinge loss on @xmath113 $ ] leads to @xmath239}{\\sum_{j=1}^mw_j[a(f_j)-a^*+x]}\\end{aligned}\\ ] ] and , according to lemma [ petitlemme ] , we have @xmath240    we now use the relative concentration inequality of lemma [ lemdevrela ] to obtain @xmath241\\\\ & & \\quad \\leq m\\biggl ( 1+\\frac{8c(a_\\mathcal{c}-a^*+2\\epsilon+x)^2x^{1/\\kappa}}{n(\\epsilon x)^2}\\biggr ) \\exp\\biggl(-\\frac{n(\\epsilon x)^2}{8c(a_\\mathcal{c}-a^*+2\\epsilon + x)^2x^{1/\\kappa } } \\biggr)\\\\ & & \\qquad { } + m\\biggl(1+\\frac{16(a_\\mathcal{c}-a^*+2\\epsilon+x)}{3n\\epsilon x } \\biggr)\\exp\\biggl(-\\frac{3n\\epsilon x}{16(a_\\mathcal { c}-a^*+2\\epsilon+x ) } \\biggr).\\end{aligned}\\ ] ] using proposition [ proporac2 ] and lemma [ lemvariancemargin ] to upper bound the variance term and applying bernstein s inequality , we get @xmath242\\\\ & & \\quad\\leq \\exp\\biggl(-\\frac{n(\\epsilon-(\\log m ) /n)^2}{4c(a_\\mathcal{c}-a^*)^{1/\\kappa}+(8/3)(\\epsilon-(\\log m)/n ) } \\biggr)\\end{aligned}\\ ] ] for any @xmath243 . we take @xmath244 , then , for any @xmath245 , we have @xmath246 thus , for @xmath247 , we have @xmath248\\leq2u+2 \\int_{u/2}^1\\bigl[t_1(\\epsilon)+m\\bigl(t_2(\\epsilon)+t_3(\\epsilon)\\bigr ) \\bigr]\\,\\mathrm{d}\\epsilon,\\ ] ] where @xmath249 and @xmath250      1 .   _ the case _ @xmath253 .",
    "denote by @xmath254 the solution of @xmath255 .",
    "we have @xmath256 .",
    "take @xmath257 such that @xmath258 . using the definitions of case ( c@xmath144 ) and @xmath254 , we get @xmath259 .",
    "moreover , @xmath260 , thus @xmath261 using lemma [ intalpha ] and the inequality @xmath262 , we obtain @xmath263\\\\[-8pt]\\nonumber & & { } \\times\\exp\\biggl(- \\frac{nu^2}{64(2c+1/3)(a_\\mathcal{c}-a^*)^{1/\\kappa}}\\biggr).\\end{aligned}\\ ] ] + we have @xmath264 .",
    "thus , using lemma [ intalpha ] , we get @xmath265 we have @xmath266 , so @xmath267 \\\\[-8pt ] \\nonumber & \\leq & \\frac{64(a_\\mathcal{c}-a^*)^{1/\\kappa}}{3nu}\\exp\\biggl(-\\frac { 3nu^2}{64(a_\\mathcal{c}-a^*)^{1/\\kappa}}\\biggr).\\end{aligned}\\ ] ] + from ( [ equa11 ] ) , ( [ equa12 ] ) , ( [ equa13 ] ) and ( [ equainit ] ) , we obtain @xmath268\\leq 2u+6m\\frac{(a_\\mathcal{c}-a^*)^{1/\\kappa}}{n\\beta_1u } \\exp\\biggl(-\\frac{n\\beta_1u}{(a_\\mathcal{c}-a^*)^{1/\\kappa } } \\biggr).\\ ] ] the definitions of @xmath257 leads to @xmath269\\leq 4\\sqrt{\\frac{(a_\\mathcal{c}-a^*)^{1/\\kappa}\\log m}{n\\beta_1}}.$ ] 2 .",
    "_ the case _",
    "we now choose @xmath257 such that @xmath271 , where @xmath272 . using the definition of case ( c@xmath252 ) and @xmath254",
    ", we get @xmath273 . using lemma [ intalpha ] and @xmath274 , @xmath275 and @xmath276 , respectively",
    ", we obtain @xmath277 \\\\[-8pt ] \\nonumber \\int_{u/2}^{1}t_2(\\epsilon)\\,\\mathrm{d}\\epsilon & \\leq & \\frac{128c}{nu^{1 - 1/\\kappa}}\\exp\\biggl(-\\frac{nu^{2 - 1/\\kappa}}{128c } \\biggr)\\end{aligned}\\ ] ] and @xmath278 from ( [ equa21 ] ) , ( [ equa23 ] ) and ( [ equainit ] ) , we obtain @xmath268\\leq 2u+6m\\frac{\\exp(-n\\beta_2 u^{(2\\kappa-1)/\\kappa } ) } { n\\beta_2 u^{1 - 1/\\kappa}}.\\ ] ] the definition of @xmath257 yields @xmath279\\leq4 ( \\frac{\\log m}{n\\beta_2 } ) ^{\\kappa/(2\\kappa-1)}.$ ]    finally , we obtain @xmath268\\leq4 \\cases{\\displaystyle { \\biggl(\\frac{\\log m}{n\\beta_2 } \\biggr)^{\\kappa/(2\\kappa-1 ) } } , & \\quad if   $ a_\\mathcal{c}-a^*\\leq\\biggl(\\dfrac{\\log m}{n\\beta_1 } \\biggr)^{\\kappa/(2\\kappa-1)}$ , \\cr \\displaystyle{\\sqrt{\\frac{(a_\\mathcal{c}-a^*)^{1/\\kappa}\\log m}{n\\beta_1 } } } , & \\quad otherwise . } \\ ] ] for the caew aggregate , it suffices to upper bound the sums by integrals in the following inequality to get the result : @xmath280 & \\leq & \\frac{1}{n}\\sum_{k=1}^n \\mathbb{e}\\bigl[a\\bigl(\\tilde{f}_k^{(\\mathrm{aew})}\\bigr)-a^ * \\bigr]\\\\   & \\leq & \\min_{f\\in\\mathcal{c}}a(f)-a^*+ c\\biggl\\{\\sqrt{(a_\\mathcal{c}-a^*)^{1/\\kappa}\\log m } \\biggl(\\frac{1}{n}\\sum_{k=1}^n\\frac{1}{\\sqrt{k}}\\biggr)\\\\ & & { } \\hspace*{92pt}+ ( \\log m)^{\\kappa/(2\\kappa-1)}\\frac{1}{n}\\sum_{k=1}^n\\frac{1 } { k^{\\kappa/(2\\kappa-1 ) } } \\biggr\\}.\\end{aligned}\\ ] ]    * proof of theorem [ optimalkappa ] .",
    "* let @xmath175 be a positive number , @xmath81 be a finite set of @xmath110 real - valued functions and @xmath115 be @xmath110 prediction rules ( which will be carefully chosen in what follows ) .",
    "using  ( [ hinglinearhingeconvex ] ) , taking @xmath73 and assuming that @xmath281 , we obtain=1 @xmath282-(1+a)\\min_{f\\in { \\operatorname{conv}}(\\mathcal{f})}\\bigl(a(f)-a^*\\bigr ) \\biggr ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\qquad\\geq \\inf_{\\hat{f}_n}\\mathop{\\sup_{\\pi\\in\\mathcal{p}}}_{\\kappa f^*\\in\\{f_1,\\ldots , f_m\\ } } \\mathbb{e}[a(\\hat{f}_n)-a^ * ] , \\end{aligned}\\ ] ] where @xmath283 is the set made of all convex combinations of elements in @xmath81 .",
    "let @xmath80 be an integer such that @xmath284 , @xmath285 be @xmath80 distinct points of @xmath5 and @xmath286 be a positive number satisfying @xmath287",
    ". denote by @xmath4 the probability measure on @xmath288 such that @xmath289 , for @xmath290 , and @xmath291 .",
    "we consider the cube @xmath292 .",
    "let @xmath293 .",
    "for all @xmath294 we consider @xmath295 for all @xmath296 , we denote by @xmath297 the probability measure on @xmath2 having @xmath4 for marginal on @xmath5 and @xmath298 for conditional probability function .",
    "we denote by @xmath303 the hamming distance on @xmath304 .",
    "let @xmath305 be such that @xmath306 .",
    "denote by @xmath307 the hellinger distance .",
    "since @xmath308 and @xmath309 the hellinger distance between the measures @xmath310 and @xmath311 satisfies @xmath312      let @xmath296 and @xmath24 be an estimator with values in @xmath113 $ ] ( according to ( [ equaprojhingerisk ] ) , we consider only estimators in @xmath113 $ ] ) . using @xmath93",
    ") , we have , conditionally on the observations @xmath13 and for @xmath316 , @xmath317 \\bigr)^\\kappa \\geq ( cw)^\\kappa\\biggl ( \\sum_{j=1}^{n-1}|\\hat{f}_n(x_j)-\\sigma_j|\\biggr)^{\\kappa}.\\ ] ] taking here the expectations , we find @xmath318\\geq(cw)^\\kappa\\mathbb{e}_{\\pi_\\sigma } [ ( \\sum_{j=1}^{n-1}|\\hat{f}_n(x_j)-\\sigma_j|)^{\\kappa}].$ ] using jensen s inequality and lemma [ lem2 ] , we obtain      now take @xmath320 , @xmath321 and @xmath322 .",
    "replace @xmath286 and @xmath80 in ( [ ineqassouadapplique ] ) by these values .",
    "thus , from ( [ ineqborninf ] ) , there exist @xmath115 ( the first @xmath323 are @xmath324 for @xmath296 and any choice is allowed for the remaining @xmath325 ) such that , for any procedure @xmath100 , there exists a probability measure @xmath3 satisfying @xmath93 ) , such that @xmath326-(1+a)\\min_{j=1,\\ldots , m}(a(f_j)-a^*)\\geq c_0 ( \\frac{\\log m}{n } ) ^{\\kappa/(2\\kappa-1)},$ ] where @xmath327 .    moreover , according to lemma [ lemtsysara ] ,",
    "we have @xmath328 thus , @xmath329&\\geq & \\min_{f\\in\\mathcal{c}}\\bigl(a(f)-a^*\\bigr)+\\frac{c_0}{2}\\biggl ( \\frac{\\log m}{n}\\biggr)^{\\kappa/(2\\kappa-1)}\\\\ & & { } + \\sqrt{2^{-1}a^{1/\\kappa}c_0 } \\sqrt{\\frac{(a_\\mathcal{c}-a^*)^{{1}/{\\kappa}}\\log m}{n}}.\\end{aligned}\\ ] ]      proof of corollary [ oracler ] the result follows from theorems [ oraclea ] and [ optimalkappa ] . using inequality ( [ e1 ] ) , lemma [ lemtsysara ] and the fact that for any prediction rule @xmath15 , we have @xmath334 , for any @xmath154 , with @xmath335 and @xmath336",
    ", we obtain the result .",
    "let @xmath220 .",
    "if @xmath340 then the same argument as in theorem [ oraclea ] yields @xmath341 for any @xmath342 .",
    "so , we have @xmath343\\\\ & & \\quad \\leq \\mathbb{p}\\biggl[\\sup_{f\\in\\mathcal{f}_\\epsilon}\\frac { r(f)-r^*-(r_n(f)-r_n(f^*))}{r(f)-r^*+x } > \\frac{\\epsilon}{r_\\mathcal{f}-r^*+2\\epsilon+x } \\biggr].\\end{aligned}\\ ] ]    we consider @xmath344 such that @xmath345 if @xmath346 , then @xmath347 , so there exists @xmath348 such that @xmath349 . hence , using the same argument as in theorem [ oraclea ] , we obtain @xmath350 & \\leq & \\mathbb{p}\\biggl[\\sup_{f\\in\\mathcal{f}}\\frac { r(f)-r^*-(r_n(f)-r_n(f^*))}{r(f)-r^*+x}\\geq \\frac{\\epsilon}{r_\\mathcal{f}-r^*+2\\epsilon+x } \\biggr]\\\\ & & { } + \\mathbb{p}[r_n(f')-r_n(f^*)>r_\\mathcal{f}-r^*+\\epsilon].\\end{aligned}\\ ] ]    we complete the proof by using lemma [ lemdevrela ] , the fact that for any @xmath15 from @xmath5 to @xmath22 , we have @xmath186 , and the same arguments as those developed at the end of the proof of theorem [ oraclea ] .    proof of theorem [ optimalkappar ] using the same argument as the one used in the beginning of the proof of theorem [ optimalkappa ] , we have , for all prediction rules @xmath115 and @xmath154 , @xmath351-(1+a)\\min_{j=1,\\ldots ,",
    "m}\\bigl(r(g_j)-r^*\\bigr ) \\biggr ) \\\\ & & \\quad\\geq \\inf_{\\hat{f}_n}\\mathop{\\sup_{\\pi\\in\\mathcal{p}_\\kappa}}\\limits_{f^*\\in\\{f_1,\\ldots , f_m\\ } } \\mathbb{e}[r(\\hat{f}_n)-r^ * ] .\\end{aligned}\\ ] ] consider the set of probability measures @xmath352 introduced in the proof of theorem [ optimalkappa ] .",
    "assume that @xmath128 .",
    "since for any @xmath296 and any classifier @xmath24 , we have , by using @xmath93 ) , @xmath353 \\geq(c_0w)^\\kappa\\mathbb{e}_{\\pi_\\sigma}\\biggl [ \\biggl ( \\sum_{j=1}^{n-1}|\\hat{f}_n(x_j)-\\sigma_j|\\biggr)^{\\kappa}\\biggr],\\ ] ] using jensen s inequality and lemma [ lem2 ] , we obtain @xmath354\\bigr)\\geq(c_0w)^\\kappa\\biggl(\\frac{n-1}{4\\mathrm{e}^2 } \\biggr)^\\kappa.\\ ] ]    by taking @xmath320 , @xmath321 and @xmath355 , there exist @xmath115 ( the first @xmath323 are @xmath324 for @xmath296 and any choice is allowed for the remaining @xmath325 ) such that for any procedure @xmath100 , there exists a probability measure @xmath3 satisfying @xmath93 ) , such that @xmath356-(1+a)\\min_{j=1,\\ldots , m}(r(f_j)-r^*)\\geq c_0 ( \\frac{\\log m}{n } ) ^{\\kappa/(2\\kappa-1)},$ ] where @xmath357 . moreover , according to lemma [ lemtsysara ] , we have @xmath358 the case @xmath129 is treated in the same way as in the proof of theorem [ optimalkappa ] .            [ lemvariancemargin ]",
    "let @xmath15 be a function from @xmath5 to @xmath113 $ ] and @xmath3 a probability measure on @xmath2 satisfying @xmath93 ) for some @xmath126 .",
    "denote by @xmath367 the symbol of variance .",
    "we have @xmath368 and @xmath369    [ lemdevrela ] let @xmath73 be a finite set of functions from @xmath5 to @xmath113 $ ] .",
    "assume that @xmath3 satisfies @xmath93 ) for some @xmath126 .",
    "we have , for any positive numbers @xmath370 and any integer @xmath9 , @xmath371 \\leq m\\biggl(\\biggl(1+\\frac{8cx^{1/\\kappa}}{n(tx)^2 } \\biggr)\\exp\\biggl ( -\\frac{n(tx)^2}{8cx^{1/\\kappa}}\\biggr)+\\biggl(1+\\frac{16}{3ntx } \\biggr)\\exp\\biggl(-\\frac{3ntx}{16 } \\biggr ) \\biggr),\\ ] ] where the constant @xmath118 appears in @xmath120 ) and @xmath372    for any integer @xmath373 , consider the set @xmath374 . using bernstein s inequality , proposition [ proporac2 ] and lemma [ lemvariancemargin ] to upper bound the variance term",
    ", we obtain @xmath375 \\\\ & & \\quad\\leq \\sum_{j=0}^{+\\infty}\\mathbb{p}\\biggl[\\max_{f\\in\\mathcal{f}_j } z_x(f)>t \\biggr ] \\\\ & & \\quad \\leq \\sum_{j=0}^{+\\infty } \\mathbb{p}\\biggl[\\max_{f\\in\\mathcal { f}_j}a(f)-a_n(f)-\\bigl(a(f^*)-a_n(f^*)\\bigr)>t(j+1)x \\biggr ] \\\\ & & \\quad \\leq m\\sum_{j=0}^{+\\infty}\\exp\\biggl(-\\frac { n[t(j+1)x]^2}{4c((j+1)x)^{1/\\kappa}+(8/3)t(j+1)x } \\biggr ) \\\\ & & \\quad\\leq m\\biggl(\\sum_{j=0}^{+\\infty}\\exp\\biggl(-\\frac { n(tx)^2(j+1)^{2 - 1/\\kappa}}{8cx^{1/\\kappa } } \\biggr ) + \\exp\\biggl(-(j+1)\\frac{3ntx}{16 } \\biggr ) \\biggr ) \\\\ & & \\quad \\leq m\\biggl(\\exp\\biggl(-\\frac{nt^2x^{2 - 1/\\kappa}}{8c}\\biggr)+\\exp \\biggl(-\\frac{3ntx}{16 } \\biggr)\\biggr ) \\\\ & & \\qquad { } + m\\int_{1}^{+\\infty}\\biggl(\\exp\\biggl(-\\frac{nt^2x^{2 - 1/\\kappa } } { 8c}u^{2 - 1/\\kappa } \\biggr)+\\exp\\biggl(-\\frac{3ntx}{16}u \\biggr)\\biggr)\\,\\mathrm{d}u .\\end{aligned}\\ ] ] lemma [ intalpha ] leads to the result .    [ lem2 ] let @xmath376 be a set of probability measures on a measurable space @xmath0 , indexed by the cube @xmath377",
    ". denote by @xmath378 the expectation under @xmath379 and by @xmath303 the hamming distance on @xmath304 .",
    "assume that @xmath380 then , @xmath381^m}\\max_{\\omega\\in\\omega}\\mathbb{e}_\\omega \\biggl[\\sum_{j=1}^m|\\hat{w_j}-w_j |\\biggr]\\geq\\frac { m}{4}\\biggl(1-\\frac{\\alpha}{2}\\biggr)^2.\\ ] ]    obviously , we can replace @xmath382^m}$ ] by @xmath383 since for all @xmath384 and @xmath385 $ ] , there exists @xmath386 ( e.g. , the projection of @xmath387 on to @xmath388 ) such that @xmath389 .",
    "we then use theorem @xmath390 of tsybakov @xcite , page 103 .",
    "blanchard , g. , bousquet , o. and massart , p. ( 2004 ) .",
    "statistical performance of support vector machines .",
    "available at http://mahery.math.u-psud.fr/~blanchard/publi/[http//mahery.math.u-psud.fr/blanchard/publi/ ] .",
    "juditsky , a. , rigollet , p. and tsybakov , a.b .",
    "learning by mirror averaging .",
    "preprint n. 1034 , laboratoire de probabilits et modle alatoires , univ .",
    "paris 6 and paris 7 .",
    "available at http://www.proba.jussieu.fr/mathdoc/preprints/index.html#2005 .",
    "tsybakov , a.b .",
    "optimal rates of aggregation . in _ computational learning theory and kernel machines _ ( b. schlkopf and m. warmuth , eds . ) .",
    "_ lecture notes in artificial intelligence _ * 2777 * 303313 .",
    "heidelberg : springer ."
  ],
  "abstract_text": [
    "<S> in the same spirit as tsybakov , we define the optimality of an aggregation procedure in the problem of classification . using an aggregate with exponential weights , </S>",
    "<S> we obtain an optimal rate of convex aggregation for the hinge risk under the margin assumption . </S>",
    "<S> moreover , we obtain an optimal rate of model selection aggregation under the margin assumption for the excess bayes risk . </S>"
  ]
}