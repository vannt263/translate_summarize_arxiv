{
  "article_text": [
    "in this paper , we consider the approximate numerical solution of very large - scale systems of linear equations @xmath0 in the case that both the number of equations @xmath1 and the number of unknowns @xmath2 have an exponential rate of increase , e.g. , @xmath3 and @xmath4 with @xmath5 and @xmath6 , standard numerical solution methods can not be applied directly without exploiting structure of the matrix such as the sparsity due to high computational and storage costs .",
    "such an exponential increase with size of the matrix is referred to as the curse of dimensionality . for example",
    ", standard numerical methods for solving high - dimensional partial differential equations often become intractable as the dimensionality of the involved operators and functions increases .",
    "we consider structured matrices with billions of rows and columns and beyond , without any assumption that the data matrix is sufficiently sparse .    in order to break the curse of dimensionality",
    ", low - rank tensor decomposition techniques are gaining growing attention in numerical computing and signal processing @xcite .",
    "a tensor of order @xmath7 is an @xmath7-dimensional array .",
    "higher - order tensors arise from various sources such as multi - dimensional data analysis @xcite and high - dimensional problems in scientific computing @xcite .",
    "tensor decomposition techniques transform such higher - order tensors into low - parametric data representation formats .",
    "comprehensive surveys about traditional tensor decomposition methods such as candecomp / parafac ( cp ) and tucker formats are provided in @xcite . however , traditional tensor decomposition methods have limitations in high - order tensor approximation and numerical computing @xcite . on the other hand ,",
    "modern tensor decomposition methods such as the hierarchical tucker ( ht ) @xcite and tensor train ( tt ) formats @xcite are promising tools for breaking down the curse of dimensionality .",
    "for instance , once large - scale matrices and vectors are reshaped into higher - order tensors and represented approximately in tt format ( e.g. , see @xcite ) , basic algebraic operations such as addition and matrix - by - vector multiplication can be performed with logarithmic storage and computational complexity @xcite .",
    "we mostly focus on the tt format , which is equivalent to the matrix product states ( mps ) with open boundary conditions ( obc ) in quantum physics @xcite .",
    "algorithms for solving optimization problems using tt formats , due to its simple linear structure of separation of variables ( or dimensions ) , are often presented in simple recursive forms , please see , e.g. , tt - rank truncation algorithm @xcite .",
    "however , previous studies on numerical algorithms for solving systems of linear equations based on tt formats focus mostly on the cases of square data matrices , i.e. , @xmath8 with @xmath9 ; see , e.g. , tt - gmres @xcite , alternating least squares ( als ) @xcite , modified alternating least squares ( mals ) @xcite , density matrix renormalization group ( dmrg ) @xcite , and alternating minimal energy ( amen ) @xcite . in order to apply the existing algorithms to the case of non - square or strongly nonsymmetric coefficient matrices ,",
    "the normal equation @xmath10 can be considered , in which the solution is equivalent in the sense of minimum of the linear least squares problem .",
    "however , the matrix product @xmath11 is often extremely ill - conditioned since the singular values of @xmath12 are squared .",
    "the ill - conditioning can slow down the convergence rate and reduce accuracy of developed algorithms .",
    "our main objective is to develop a new method to compute an approximation to the moore - penrose pseudoinverse ( mpp ) of @xmath12 , i.e. , @xmath13 and solve the preconditioned system @xmath14 when @xmath15 , or @xmath16 with the substitution @xmath17 when @xmath18 .",
    "the preconditioned matrix @xmath19 ( resp .",
    "@xmath20 ) should be square and near symmetric , and have more uniformly distributed eigenvalues possibly far away from zero to improve the convergence property of an iterative method .",
    "we can compute an approximate mpp @xmath21 by minimizing the cost function @xmath22 assuming without loss of generality that @xmath15 . the objective function with @xmath23 has been considered widely for the computation of preconditioners by , e.g. , sparse approximate inverse ( spai ) preconditioners @xcite and generalized approximate inverse ( gainv ) preconditioners @xcite .",
    "most approximate inverse techniques are claimed to be largely immune to the risk of breakdown during the preconditioner construction @xcite .",
    "the general case that @xmath24 was considered by , e.g. , the optimal low - rank regularized inverse matrix approximation @xcite . the regularization term is helpful for alleviating ill - posedness of the minimization problem and improving the convergence property of the proposed algorithm .",
    "the simulation results in section  [ sec_num_simulation ] illustrate that the regularization approach is also helpful for avoiding overestimation of tt - ranks of the pseudoinverses .",
    "we propose a new method for computing the approximate generalized inverse @xmath25 in the form of a low - rank tt decomposition .",
    "many of the sparse approximate inverse algorithms such as the spai preconditioners @xcite assume that approximate inverses are sparse , however , inverses of sparse matrices are often not sparse .",
    "on the other hand , matrices represented in tt format do not have to be sparse , instead , they are required to have relatively small tt - ranks .",
    "it has been shown that inverses and preconditioners of several classes of important matrices such as laplace - like operators and banded toeplitz matrices admit approximate low - rank tt representations @xcite .",
    "holtz , rohwedder , and schneider @xcite and oseledets and dolgov @xcite developed dmrg ( also called as mals ) methods for solving systems of linear equations with square matrices in tt formats .",
    "the application to matrix inversion is presented in @xcite . however",
    ", the matrix inversion method proposed in @xcite is applicable only to square matrices .",
    "and when we want to apply this approach , the linear matrix equation @xmath26 is converted to a larger linear least squares ( ls ) problem . on the other hand ,",
    "the proposed method can be applied to general non - square matrices , whose pseudoinverses can be efficiently computed without a need to solve a larger linear ls problem . finally ,",
    "once an approximate pseudoinverse is computed , the preconditioned linear system can be solved by applying existing tt - based optimization algorithms such as the ones developed in @xcite .",
    "numerical simulation results illustrate that the approximate generalized inverses obtained by the proposed method provide usually low tt - ranks with a sufficiently small approximation error .",
    "the resulting preconditioned linear systems can be solved by existing tt - based algorithms much faster than linear systems without preconditioning .",
    "a wide class of structured matrices in tt format are considered and demonstrated to have low tt - rank preconditioners .",
    "the paper is organized as follows . in section 2 , we describe briefly mathematical representations for tt decomposition . in section 3 , we design a new tensor network and develop a mals algorithm for computing approximate pseudoinverses . in section 4 , experimental results are presented to demonstrate the validity and effectiveness of the proposed method . in section 5 , discussion and concluding remarks are given .",
    "we will briefly describe notation for tensors and tensor operations used in this paper .",
    "we refer to @xcite for further details .",
    "scalars , vectors , and matrices are denoted by lower - case letters ( @xmath27 , @xmath28 ,  ) , lower - case bold letters ( @xmath29 , @xmath30 ,  ) , and upper - case bold letters ( @xmath12 , @xmath31 ,  ) , respectively .",
    "@xmath7th - order tensors , i.e. , @xmath7-way arrays ( for @xmath32 ) , are denoted by calligraphic letters ( @xmath33 , @xmath34 ,  ) . for a tensor @xmath35 , where @xmath36 is the size of the @xmath37th _ mode _",
    ", the @xmath38th entry of @xmath39 is denoted by @xmath40 .",
    "mode-@xmath37 fibers of a tensor @xmath41 are column vectors @xmath42 determined by fixing all the indices except for the @xmath37th index .",
    "the mode-@xmath43 contracted product of tensors @xmath44 and @xmath45 is a binary operation defined by the tensor is contracted with the mode @xmath7 of a tensor @xmath33 . ]",
    "@xmath46 with entries @xmath47 the mode-1 contracted product is a natural generalization of the matrix - by - matrix product to higher - order tensors .",
    "note that it has associativity : @xmath48 for any tensors @xmath33 , @xmath34 , and @xmath49 of proper sizes .",
    "basic symbols for tensor are shown in figure  [ diagram_tensor ] .",
    "in particular , symbols for vectors ( @xmath43st - order tensors ) , matrices ( 2nd - order tensors ) , and 3rd - order tensors are illustrated in figure  [ diagram_tensor](a ) , while the mode-1 contraction of two 3rd - order tensors is illustrated in figure  [ diagram_tensor](b ) .    [ cols=\"^,^ \" , ]",
    "we presented a new mals algorithm for the computation of approximate pseudoinverses of extremely large - scale structured matrices using low - rank tt decomposition .",
    "the proposed method can estimate the moore - penrose pseudoinverses of any nonsymmetric or nonsquare structured matrices in low - rank matrix tt format approximately , so it can be useful for preconditioning overdetermined or underdetermined large - scale systems of linear equations .",
    "the proposed method provides stability and the fast convergence speed even for very ill - conditioned large - scale matrices by regularization .",
    "the regularized solutions were shown to have relatively small tt - ranks in the numerical simulations , so the computational costs for the construction of preconditioners and solution to huge systems of linear equations were significantly smaller than without regularization .",
    "the regularization technique is especially important when the size of a data matrix is huge and ill - conditioned .",
    "the proposed algorithm converts the large - scale minimization problem into sequential smaller - scale optimization problems to which any standard optimization methods can be applied .",
    "the convergence to the desired solution is stable and relatively fast because the tt - ranks of the approximate inverses can be adaptively determined during the iteration process , and the decrease in the objective function value is monotonic .",
    "the computational cost for running the proposed mals algorithm is logarithmic in the matrix size under the assumption of boundedness of tt - ranks .",
    "the estimated pseudoinverses were applied to preconditioning of the strongly nonsymmetric matrix occurring in systems of linear equations in the convection - diffusion equation problem .",
    "several standard iterative algorithms such as gmres , bi - cgstab , and pcg showed the improved convergence in the numerical simulations , which demonstrate the effectiveness of the proposed algorithm by preconditioning and symmetrizing the coefficient matrix .",
    "the main advantage of the proposed method lies in its applicability to any rectangular huge structured matrices .",
    "moreover , the regularization technique employed in the proposed method helps to compute approximate pseudoinverses reliably for any ill - conditioned structured matrices which admit low - rank tt approximations .",
    "the developed algorithm can further be directly applied to the following areas .",
    "first , the computation of regularized moore - penrose pseudoinverses is closely related to the regularized ( filtered ) solution of systems of linear equations @xmath50 , by @xmath51 , where @xmath52 @xcite .",
    "second , the large scale generalized eigenvalue decomposition ( gevd ) problem described by @xmath53 for a square matrix @xmath12 and a nonsingular square matrix @xmath31 @xcite can be transformed to a standard eigenvalue decomposition problem @xmath54 if the large - scale inverse matrix @xmath55 can be approximately computed in tt format efficiently .",
    "once the large - scale matrices @xmath12 and @xmath55 are represented in tt format , the multiplication @xmath56 can be relatively easily performed @xcite .",
    "third , a special case of the optimization problem arises in important subspace clustering problems @xcite , which can also be efficiently solved using the proposed algorithm based on tt decompositions .",
    "r. andreev and c. tobler , _ multilevel preconditioning and low - rank tensor iteration for space - time simultaneous discretizations of parabolic pdes _ , numer .",
    "linear algebra appl . , 22 ( 2015 ) , pp .",
    "317337 . , _ the moore - penrose pseudoinverse : a tutorial review of the theory _ , braz",
    ". j. phys . , 42 ( 2012 ) ,",
    "146165 .      , _ approximation of @xmath57 by exponential sums in @xmath58 _ , i m a j. numer .",
    ", 25 ( 2005 ) , pp .  685697 . , _ a comparative study of sparse approximate inverse preconditioners _ , appl .",
    ", 30 ( 1999 ) , pp .  305340 .          ,",
    "_ era of big data processing : a new approach via tensor networks and tensor decompositions _ , arxiv:1403.2048 , 2014 .",
    ", _ tensor networks for big data analytics and large - scale optimization problems _ , arxiv:1407.3124 , 2014 .",
    ", nonnegative matrix and tensor factorizations : applications to exploratory multi - way data analysis and blind source separation , wiley , chichester , 2009 .                          , _ a literature survey of low - rank tensor approximation techniques _ , gamm - mitt",
    ". , 36 ( 2013 ) , pp .",
    "5378 . , _",
    "parallel preconditioning with sparse approximate inverses _ , siam j. sci .",
    "comput . , 18 ( 1997 ) ,",
    "838853 .        , _ on manifolds of tensors of fixed tt - rank _ ,",
    "numer . math . , 120 ( 2012 ) ,",
    ", _ the alternating linear scheme for tensor optimization in the tensor train format _ , siam j. sci .",
    ", 34 ( 2012 ) , pp .",
    "a683a713 .",
    "v. kazeev , m. khammash , m. nip , and c. schwab , _ direct solution of the chemical master equation using quantized tensor trains _ , plos comput .",
    "biol . , 10 ( 2014 ) , e1003359 .",
    "doi:10.1371/journal.pcbi.1003359              , _ low - rank tensor methods with subspace correction for symmetric eigenvalue problems _ , siam j. sci .",
    "comput . , 36 ( 2014 ) , pp .  a2346a2368 .",
    ", _ preconditioned low - rank methods for high - dimensional elliptic pde eigenvalue problems _ , comput .",
    "methods appl .",
    ", 11 ( 2011 ) , pp .",
    "363381 .                , _ solution of linear systems and matrix inversion in the tt - format _",
    ", siam j. sci .",
    "comput . , 34 ( 2012 ) , pp .  a2718a2739 . , _ breaking the curse of dimensionality , or how to use svd in many dimensions _ , siam j. sci .",
    "comput . , 31 ( 2009 ) , pp",
    ".  37443759 .",
    "d.  v. savostyanov , s.  v. dolgov , j.  m. werner , and i. kuprov , _ exact nmr simulation of protein - size spin systems using tensor train formalism _ , phys .",
    "b , 90 ( 2014 ) , 085139 .",
    "m. signoretto , q.  t. dinh , l. de lathauwer , and j.  a.  k. suykens , _ learning with tensors : a framework based on convex optimization and spectral regularization _ , machine learning , 94 ( 2014 ) , pp .",
    "303351 . , _",
    "idr(s ) : a family of simple and fast algorithms for solving large nonsymmetric linear systems _ , siam j. sci . comput . , 31 ( 2008 ) ,",
    "10351062 .",
    "l. sorber , i. domanov , m. van barel , and l. de lathauwer , _ exact line and plane search for tensor optimization _ , comput .",
    "optim . appl .",
    "( 2015 ) , pp .  122 .",
    "doi:10.1007/s10589 - 015 - 9761 - 5 , _ the density - matrix renormalization group in the age of matrix product states _ , ann . phys . , 326 ( 2011 ) ,",
    "96192 .        , _",
    "rank / norm regularization with closed - form solutions : application to subspace clustering _ , in proceedings of the twenty - seventh conference on uncertainty in artificial intelligence , f. cozman and a. pfeffer , eds .",
    ", auai press , corvallis , oregon , 2011 , pp ."
  ],
  "abstract_text": [
    "<S> we propose a new method for low - rank approximation of moore - penrose pseudoinverses ( mpps ) of large - scale matrices using tensor networks . </S>",
    "<S> the computed pseudoinverses can be useful for solving or preconditioning of large - scale overdetermined or underdetermined systems of linear equations . </S>",
    "<S> the computation is performed efficiently and stably based on the modified alternating least squares ( mals ) scheme using low - rank tensor train ( tt ) decompositions and tensor network contractions . </S>",
    "<S> the formulated large - scale optimization problem is reduced to sequential smaller - scale problems for which any standard and stable algorithms can be applied . </S>",
    "<S> regularization technique is incorporated in order to alleviate ill - posedness and obtain robust low - rank approximations . </S>",
    "<S> numerical simulation results illustrate that the regularized pseudoinverses of a wide class of non - square or nonsymmetric matrices admit good approximate low - rank tt representations . </S>",
    "<S> moreover , we demonstrated that the computational cost of the proposed method is only logarithmic in the matrix size given that the tt - ranks of a data matrix and its approximate pseudoinverse are bounded . </S>",
    "<S> it is illustrated that a strongly nonsymmetric convection - diffusion problem can be efficiently solved by using the preconditioners computed by the proposed method .    </S>",
    "<S> alternating least squares ( als ) , density matrix renormalization group ( dmrg ) , curse of dimensionality , solving of huge system of linear equations , low - rank tensor approximation , matrix product operators , matrix product states , preconditioning , generalized inverse of huge matrices , tensor networks , big data .    15a09 , 65f08 , 65f20 , 65f22 </S>"
  ]
}