{
  "article_text": [
    "throughout their development , information theory , and more generally , probability theory , have benefitted from non - negative measures of dissimilarity , or loosely speaking , distances , between pairs of probability measures defined on the same measurable space ( see , e.g. , @xcite ) .",
    "notable among those measures are ( see section  [ sec : preliminaries ] for definitions ) :    * total variation distance @xmath3 ; * relative entropy @xmath4 ; * @xmath1-divergence @xmath5 ; * hellinger divergence @xmath6 ; * rnyi divergence @xmath7 .",
    "it is useful , particularly in proving convergence results , to give bounds of one measure of dissimilarity in terms of another .",
    "the most celebrated among those bounds is pinsker s inequality : . ]",
    "@xmath8 proved by csiszr @xcite and kullback @xcite , with kemperman @xcite independently a bit later . improved and generalized versions of pinsker s inequality have been studied , among others , in @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite .    in this work , we find relationships among the various divergence measures outlined above as well as a number of other measures of dissimilarity between probability measures .",
    "the framework of @xmath0-divergences , which encompasses the foregoing measures ( rnyi divergence is a one - to - one transformation of the hellinger divergence ) serves as a convenient playground .",
    "the rich structure of the total variation distance as well as its importance in both theory and practice merits placing special attention on bounding the rest of the distance measures in terms of @xmath9 .",
    "relationships among measures of distances between probability measures have long been a focus of interest in probability theory and statistics ( e.g. , for studying the rate of convergence of measures ) .",
    "the reader is referred to surveys in ( * ? ? ?",
    "* section  3 ) , ( * ? ? ?",
    "* chapter  2 ) , @xcite and ( * ? ? ?",
    "* appendix  3 ) , which provide several relationships among useful @xmath0-divergences and other measures of dissimilarity between probability measures .",
    "some notable existing bounds among @xmath0-divergences include , in addition to :    * ( * ? ? ?",
    "* lemma 1 ) , @xcite @xmath10 * ( * ? ? ? * ( 2.2 ) ) @xmath11 * ( * ? ? ?",
    "* theorem  5 ) , ( * ? ? ?",
    "* theorem  4 ) , @xcite @xmath12 * ( * ? ? ?",
    "* corollary  5.6 ) for all @xmath13 @xmath14 the inequality in is reversed if @xmath15 $ ] , and it holds with equality if @xmath16 . * @xcite , ( * ? ? ?",
    "* ( 58 ) ) @xmath17 * @xcite @xmath18 * ( * ? ? ?",
    "* ( 2.8 ) ) @xmath19 * @xcite , ( * ? ? ?",
    "* corollary  32 ) , @xcite @xmath20 * @xcite ( cf . a generalized form in ( * ? ? ?",
    "* lemma  a.3.5 ) ) @xmath21 a generalization of is provided in ( * ? ? ?",
    "* proposition  2.15 ) : @xmath22 * ( * ? ? ?",
    "* proposition  2.35 ) if @xmath23 and @xmath24 , then @xmath25 + ( * ? ? ?",
    "* theorems 3 and 16 ) the rnyi divergence @xmath7 is monotonically increasing in @xmath26 , and @xmath27 is monotonically decreasing in @xmath28 $ ] ; the same monotonicity properties also hold for @xmath6 ( * ? ? ?",
    "* proposition  2.7 ) .",
    "* @xcite if @xmath28 $ ] , then @xmath29 * a  reverse pinsker inequality \" , providing an upper bound on the relative entropy in terms of the total variation distance , does not exist in general since we can find distributions which are arbitrarily close in total variation but with arbitrarily high relative entropy . nevertheless , it is possible to introduce constraints under which such reverse pinsker inequalities hold . in the special case of a finite alphabet @xmath30 , csiszr and",
    "talata @xcite showed that @xmath31 when @xmath32 is positive . *",
    "* theorem  3.1 ) yields that if @xmath33 is a strictly convex function , then there exists a real - valued function @xmath34 which depends on @xmath0 such that @xmath35 and and @xmath36 . ]",
    "@xmath37 is a sequence of pairs of probability measures , then @xmath38    the numerical optimization of an @xmath0-divergence subject to simultaneous constraints on @xmath39-divergences @xmath40 was recently studied in @xcite , which showed that for that purpose it is enough to restrict attention to alphabets of cardinality @xmath41 .",
    "earlier , @xcite showed that if @xmath42 , then either the solution is obtained by a pair @xmath43 on a binary alphabet , or it is a deflated version of such a point .",
    "therefore , from a purely numerical standpoint , the minimization of @xmath44 such that @xmath45 can be accomplished by a grid search on @xmath46 ^ 2 $ ] .",
    "occasionally , as in the case where @xmath47 and @xmath48 , it is actually possible to determine analytically the locus of @xmath49 ( see @xcite ) .",
    "in fact , as shown in ( * ? ? ?",
    "* ( 22 ) ) , a binary alphabet suffices if the single constraint is on the total variation distance .",
    "the same conclusion holds when minimizing the rnyi divergence @xcite .",
    "the rest of the paper is structured as follows :    section  [ sec : preliminaries ] introduces the basic definitions needed and in particular the various measures of dissimilarity between probability measures used throughout .",
    "based on functional domination , section  [ sec : functional domination ] provides a basic tool for the derivation of bounds among @xmath0-divergences . under mild regularity conditions , this approach further enables to prove the optimality of constants in those bounds .",
    "we also show instances where such optimality can be shown in the absence of regularity conditions .",
    "the basic tool used in section  [ sec : functional domination ] is exemplified in obtaining relationships among important @xmath0-divergences such as @xmath50 and @xmath9 .",
    "this approach is also useful in providing an alternative proof of samson s inequality @xcite ( an analog to pinsker s inequality , useful in proving certain concentration of measure results @xcite ) , whose constant we show can not be improved .    section  [ sec : bounded ] provides an approach for bounding @xmath0-divergence in terms of @xmath51-divergence , assuming that the relative information is lower and/or upper bounded with probability one .",
    "the approach is exemplified in bounding @xmath52 , @xmath53 , @xmath54 , and the local behavior of @xmath0-divergence ratios .",
    "we also show that bounded relative information leads to a strengthened version of jensen s inequality , which , in turn , results in upper and lower bounds on the ratio of the non - negative difference @xmath55 to @xmath56 .",
    "we also show a reverse version of samson s inequality .",
    "section  [ sec : tv - ri ] gives several useful identities linking the total variation distance with the relative information spectrum , which result in a number of upper and lower bounds on @xmath9 , some of which are tighter than pinsker s inequality in various ranges of the parameters .",
    "it also provides refined bounds on @xmath57 as a function of @xmath1 divergences and the total variation distance .",
    "section [ sec : reversep ] is devoted to proving ",
    "reverse pinsker inequalities , \" namely , lower bounds on @xmath58 as a function of @xmath59 involving either ( a ) bounds on the relative information , ( b ) lipschitz constants , or ( c ) the minimum mass of the reference measure ( in the finite alphabet case ) . in the latter case",
    ", we also examine the relationship between entropy and the total variation distance from the equiprobable distribution , as well as the exponential decay of the probability that an independent identically distributed sequence is not strongly typical .",
    "section  [ sec : eg ] focuses on the @xmath2 divergence .",
    "this @xmath0-divergence generalizes the total variation distance , and its utility in information theory has been exemplified in @xcite .",
    "section  [ sec : eg ] starts by providing an integral representation of @xmath0-divergences as a function of the @xmath2 divergence ; this representation shows that @xmath60 uniquely determines @xmath61 and @xmath6 , as well as any other @xmath0-divergence with twice differentiable @xmath0 .",
    "in addition , we show an extension of pinsker s inequality to @xmath2 divergence , which leads to a relationship between the relative information spectrum and relative entropy .",
    "section  [ sec : rd ] expresses the rnyi divergence in terms of the relative information spectrum , and gives upper and lower bounds on the rnyi divergence in terms of either the variational distance or relative entropy .",
    "particular attention is placed on the finite alphabet case .",
    "we assume throughout that the probability measures @xmath62 and @xmath63 are defined on a common measurable space @xmath64 , and @xmath65 denotes that @xmath62 is _ absolutely continuous _ with respect to @xmath63 , namely there is no event @xmath66 such that @xmath67 .",
    "[ def : ri ] if @xmath65 , the _ relative information _ provided by @xmath68 according to @xmath43 is given by denotes the radon - nikodym derivative ( or density ) of @xmath62 with respect to @xmath63 .",
    "logarithms have an arbitrary common base , and the exponent indicates the inverse function of the logarithm with that base . ]",
    "@xmath69    when the argument of the relative information is distributed according to @xmath62 , the resulting real - valued random variable is of particular interest .",
    "its cumulative distribution function and expected value are known as follows .",
    "[ def : ris ] if @xmath65 , the _ relative information spectrum _ is the cumulative distribution function @xmath70,\\end{aligned}\\ ] ] with means that @xmath71 = p ( { \\ensuremath{\\mathcal}}{f } ) $ ] for any event @xmath66 . ]",
    "@xmath72 . furthermore , if @xmath73 , then the relative entropy of @xmath62 with respect to @xmath63 is @xmath74 \\\\",
    "\\label{eq2:re1 } & = \\mathbb{e } \\bigl [ \\imath_{p \\| q}(y ) \\ ,",
    "\\exp\\bigl(\\imath_{p \\| q}(y ) \\bigr ) \\bigr].\\end{aligned}\\ ] ]      introduced by ali - silvey @xcite and csiszr ( @xcite ) , a useful generalization of the relative entropy , which retains some of its major properties ( and , in particular , the data processing inequality @xcite ) , is the class of @xmath0-divergences . a general definition of an @xmath0-divergence is given in @xcite , specialized next to the case where @xmath65 .",
    "[ def : fd ] let @xmath33 be a convex function , and suppose that @xmath65 .",
    "the _ @xmath0-divergence _ from @xmath62 to @xmath63 is given by @xmath75\\end{aligned}\\ ] ] with @xmath76 in , we take the continuous extension implies its continuity on @xmath77 . ]",
    "@xmath78.\\end{aligned}\\ ] ]    if @xmath79 and @xmath80 denote , respectively , the densities of @xmath62 and @xmath63 with respect to a @xmath81-finite measure @xmath82 ( i.e. , @xmath83 , @xmath84 ) , then we can write as @xmath85    [ remark : equivalence - fd ] different functions may lead to the same @xmath0-divergence for all @xmath43 : if for an arbitrary @xmath86 , we have @xmath87 then @xmath88    the following key property of @xmath0-divergences follows from jensen s inequality .",
    "[ prop : fgibbs ] if @xmath33 is convex and @xmath89 , @xmath65 , then @xmath90 if , furthermore , @xmath0 is strictly convex at @xmath91 , then equality in holds if and only if @xmath92 .    the reader is referred to @xcite for surveys on general properties of @xmath0-divergences , and also to the textbook by liese and vajda @xcite which provides a systematic and rigorous study of @xmath0-divergences .",
    "the assumptions of proposition  [ prop : fgibbs ] are satisfied by many interesting measures of dissimilarity between probability measures . in particular",
    ", we give the following examples which receive particular attention in this paper . as per definition",
    "[ def : fd ] , in each case the function @xmath0 is defined on @xmath77 .    1 .   _ relative entropy _",
    "@xcite : @xmath93 , @xmath94 with @xmath95 defined as @xmath96 2 .   _ relative entropy _ : ( @xmath97 ) @xmath98 , @xmath99 3 .",
    "_ jeffrey s symmetrized divergence _",
    "@xcite : ( @xmath100 ) @xmath101 , @xmath102 4 .",
    "_ @xmath1-divergence _ @xcite : @xmath103 or @xmath104 , @xmath105 consequently , the @xmath1-divergence can be expressed in the following equivalent forms : @xmath106 - 1 \\\\ \\label{eq : chi - square 4 } & = \\mathbb{e } \\bigl [ \\exp\\bigl(\\imath_{p\\|q}(x)\\bigr ) \\bigr ] - 1\\end{aligned}\\ ] ] with @xmath72 and @xmath73 .",
    "note that if @xmath107 , then from the right side of , we obtain @xmath108 with @xmath109 .",
    "_ hellinger divergence of order @xmath110 _ @xcite , ( * ? ? ?",
    "* definition  2.10 ) ( a.k.a .",
    "tsallis divergence , e.g. , ( * ? ? ?",
    "* ( 49 ) ) ) : @xmath111 with @xmath112 the @xmath1-divergence is the hellinger divergence of order  2 , while @xmath113 is usually referred to as the _ squared hellinger distance_. the analytic extension of @xmath6 at @xmath114 yields @xmath115 6 .   _",
    "total variation distance _ : setting @xmath116 results in @xmath117 \\label{eq3 : tv distance } & = 2 \\ , \\sup_{{\\ensuremath{\\mathcal}}{f } \\in \\mathscr{f } } \\bigl(p({\\ensuremath{\\mathcal}}{f } ) - q({\\ensuremath{\\mathcal}}{f})\\bigr).\\end{aligned}\\ ] ] 7 .   _ triangular discrimination ( a.k.a .",
    "the vincze - le cam distance ) _",
    "@xcite : @xmath118 with @xmath119 note that @xmath120 8 .",
    "_ jensen - shannon divergence ( a.k.a .",
    "capacitory discrimination ) _",
    "@xcite : @xmath121 with @xmath122 9 .   _",
    "@xmath2 divergence _",
    "( see , e.g. , @xcite ) : for @xmath123 , @xmath124 with @xmath125 where @xmath126 .",
    "@xmath2 is sometimes called  hockey - stick divergence \" because of the shape of @xmath127 . if @xmath128 , then @xmath129 10 .",
    "_ degroot statistical information _",
    "@xcite : for @xmath130 , @xmath131 with @xmath132 note that , from  , it follows that ( cf .",
    "* ( 77 ) ) ) @xmath133 + this measure was first proposed by degroot @xcite due to its operational meaning in bayesian statistical hypothesis testing ( see section  [ saens ] ) , and it was later identified as an @xmath0-divergence ( see ( * ? ? ?",
    "* theorem  10 ) ) .",
    "_ marton s divergence _",
    "@xcite : @xmath134 \\right]\\end{aligned}\\ ] ] where the minimum is over all probability measures @xmath135 with respective marginals @xmath136 and @xmath137 . from @xcite @xmath138 with @xmath139 note",
    "that marton s divergence satisfies the triangle inequality ( * ? ? ?",
    "* lemma  3.1 ) , and @xmath140 implies @xmath92 ; however , due to its asymmetry , it is not a distance measure .",
    "another generalization of relative entropy was introduced by rnyi @xcite in the special case of finite alphabets .",
    "the general definition ( assuming @xmath65 ) is the following .",
    "[ def : rd ] let @xmath65 .",
    "rnyi divergence of order @xmath141 _ from @xmath62 to @xmath63 is given as follows :    * if @xmath142 , then @xmath143 \\bigr ) \\\\[0.1 cm ] \\label{eq : rd3 } & = \\frac1{\\alpha-1 } \\ ; \\log \\bigl ( \\mathbb{e } \\bigl [ \\exp \\bigl((\\alpha-1 ) \\ , \\imath_{p\\|q}(x)\\bigr)\\bigr ] \\bigr)\\end{aligned}\\ ] ] with @xmath72 and @xmath73 .",
    "* if @xmath144 , then .",
    "rnyi @xcite defined @xmath145 , while we have followed @xcite defining it instead as @xmath146 . ]",
    "@xmath147 * if @xmath148 , then @xmath149 which is the analytic extension of @xmath150 at @xmath114 ( if @xmath151 , it can be verified by lhpital s rule that @xmath152 ) .",
    "* if @xmath153 then @xmath154 with @xmath73 .",
    "rnyi divergence is a one - to - one transformation of hellinger divergence of the same order @xmath155 : @xmath156 which , when particularized to order  2 becomes @xmath157 note that , , follow from and the monotonicity of the rnyi divergence in its order , which in turn yields .    the bhattacharyya distance has been introduced in @xcite , and it has been popularized in the engineering literature in @xcite .    [ definition : b distance ] the bhattacharyya distance between @xmath62 and @xmath63 , denoted by @xmath158 , is given by @xmath159    note that , if @xmath107 , then @xmath160 and @xmath161 if and only if @xmath92 , though @xmath162 does not satisfy the triangle inequality .",
    "let @xmath0 and @xmath51 be convex functions on @xmath77 with @xmath163 , and let @xmath62 and @xmath63 be probability measures defined on a measurable space @xmath64 .",
    "if , for @xmath26 , @xmath164 for all @xmath165 then , it follows from the definition of an @xmath0-divergence that @xmath166 this simple observation leads to a proof of , for example , and the left inequality in with the aid of remark  [ remark : equivalence - fd ] .",
    "we start this section by proving a general result , which will be helpful in proving various tight bounds among @xmath0-divergences .    [",
    "theorem : tight bound ] let @xmath65 , and let @xmath0 , @xmath51 be convex and non - negative functions on @xmath77 with @xmath163 . denote the function with domain @xmath77 @xmath167 and @xmath168 then ,    a.   @xmath169 b.   [ theorem : tight bound : partb ] if , in addition , @xmath51 is strictly convex at  1 , and @xmath170 , then @xmath171    a.   the bound in follows from and @xmath172 for all @xmath173 . b.   since @xmath51 is strictly convex at  1 , proposition  [ prop : fgibbs ] implies that @xmath174 if @xmath175 .",
    "the convexity of @xmath176 on @xmath77 implies their continuity ; since @xmath51 is non - negative on @xmath77 and strictly convex at  1 with @xmath177 then @xmath178 for all @xmath179 , which implies that @xmath180 in and is well defined , continuous on @xmath181 , and right continuous at  1 .",
    "+ to show , we fix an arbitrary @xmath182 and construct a sequence of pairs of probability measures whose ratio of @xmath0-divergence to @xmath51-divergence converges to @xmath183 . to that end , for sufficiently small @xmath184 , let @xmath185 and @xmath186 be parametric probability measures defined on the set @xmath187 with @xmath188 and @xmath189 .",
    "then , @xmath190 & =   \\lim_{\\varepsilon \\to 0 } \\ , \\frac{\\varepsilon \\ , f(\\nu ) + ( 1-\\varepsilon ) \\ , f\\left(\\frac{1-\\nu \\varepsilon}{1-\\varepsilon}\\right)}{\\varepsilon \\ , g(\\nu ) + ( 1-\\varepsilon ) \\ , g\\left(\\frac{1-\\nu \\varepsilon}{1-\\varepsilon}\\right ) } \\\\[0.1 cm ] & = \\lim_{\\alpha \\to 0 } \\frac{f(\\nu ) + \\frac{\\nu -1}{\\alpha } \\ ; f(1 - \\alpha)}{g(\\nu ) + \\frac{\\nu -1}{\\alpha } \\ ; g(1-\\alpha ) } \\label{arlington } \\\\[0.1 cm ] & = \\kappa(\\nu ) \\label{cun}\\end{aligned}\\ ] ] where holds by change of variable @xmath191 , and holds by the assumption on the derivatives of @xmath0 and @xmath51 at 1 . if @xmath192 we are done .",
    "if the supremum in is not attained on @xmath193 ( e.g. , it is equal to either @xmath194 or @xmath195 ) , then due to the continuity of @xmath196 on @xmath181 and right continuity at  1 , the right side of can be made arbitrarily close to @xmath197 by an appropriate choice of @xmath198 .    beyond the restrictions in proposition",
    "[ prop : fgibbs ] , the only operative restriction imposed by theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) is the differentiability of the functions @xmath0 and @xmath51 at @xmath91 .",
    "indeed , we can invoke remark  [ remark : equivalence - fd ] and add @xmath199 to @xmath200 , without changing @xmath201 ( and likewise with @xmath51 ) and thereby satisfying the condition in theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) ; the stationary point at  1 must be a minimum of both @xmath0 and @xmath51 because of the assumed convexity , which implies their non - negativity on @xmath77 .",
    "it is useful to generalize theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) by dropping the assumption on the existence of the derivatives at  1 . to that end , note that the inverse transformation used for the transition to is given by @xmath202 where @xmath184 is sufficiently small , so if @xmath203 ( resp .",
    "@xmath204 ) , then @xmath26 ( resp .",
    "@xmath205 ) .",
    "consequently , it is easy to see from that if @xmath206 , the construction in the proof can restrict to @xmath203 , in which case it is enough to require that the left derivatives of @xmath0 and @xmath51 at  1 be equal to @xmath207 . analogously , if @xmath208 , it is enough to require that the right derivatives of @xmath0 and @xmath51 at  1 be equal to @xmath207 .",
    "since the rnyi divergence of order @xmath210 is monotonically increasing in @xmath211 , yields @xmath212 inequality , which can be found in @xcite and ( * ? ? ?",
    "* theorem  5 ) , is sharpened in theorem  [ thm : d - chi ] under the assumption of bounded relative information . in view of ,",
    "an alternative way to sharpen is @xmath213 for @xmath214 , which is tight as @xmath215 .",
    "relationships between the relative entropy , total variation distance and @xmath1 divergence are derived next .",
    "a.   if @xmath65 and @xmath216 , then @xmath217 holds if @xmath218 and @xmath219 .",
    "furthermore , if @xmath220 then @xmath221 is optimal , and if @xmath222 then @xmath223 is optimal .",
    "b.   if @xmath107 and @xmath175 , then @xmath224 and the constant in the right side of is the best possible .",
    "a.   the satisfiability of with @xmath218 is equivalent to . + let @xmath65 , and @xmath73 . then , @xmath225 \\\\ \\label{eq2 : improved diaconiss96 } & \\leq \\tfrac12 \\ , \\mathbb{e } \\left [ \\left ( 1 - \\frac{\\text{d}p}{\\text{d}q } \\ , ( y ) \\right)^+ + \\left ( \\frac{\\text{d}p}{\\text{d}q } \\ , ( y ) - 1 \\right)^2 \\right ] \\ , \\log e \\\\",
    "\\label{eq3 : improved diaconiss96 } & = \\left ( \\tfrac14 \\ , |p - q| + \\tfrac12 \\ , \\chi^2(p\\|q ) \\right ) \\ , \\log e\\end{aligned}\\ ] ] where follows from the definition of relative entropy with the function @xmath226 defined in ; holds since for @xmath165 @xmath227 \\ , \\log e\\end{aligned}\\ ] ] and follows from , , and the identity @xmath228 $ ] .",
    "this proves with @xmath219 .",
    "+ next , we show that if @xmath220 then @xmath229 is the best possible constant in . to that end , let @xmath230 , and let @xmath231 .",
    "it can be verified that the function @xmath196 is monotonically decreasing on @xmath77 , so @xmath232 since @xmath233 and @xmath234 , and @xmath235 , the desired result follows from theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) .",
    "+ to show that @xmath223 is the best possible constant in if @xmath222 , we let and @xmath236 $ ] .",
    "theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) does not apply here since @xmath237 is not differentiable at @xmath91 .",
    "however , we can still construct probability measures for proving the optimality of the point @xmath238 . to that end , let @xmath239 , and define probability measures @xmath240 and @xmath241 on the set @xmath242 with @xmath243 and @xmath244 . since @xmath245 and @xmath246 , @xmath247 where holds since @xmath248 and the result follows since , in the right side of , @xmath249 .",
    "b.   we have @xmath250 with @xmath251 where is easy to verify since @xmath196 is monotonically increasing on @xmath252 , and monotonically decreasing on @xmath253 .",
    "the desired result follows since the conditions of theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) apply .",
    "inequality   strengthens the bound in ( * ? ? ? * ( 2.8 ) ) , @xmath254 note that the short outline of the suggested proof in @xcite leads to the weaker upper bound @xmath255 nats .",
    "note that implies a looser result in where the constant in the right side of is doubled .",
    "furthermore , pinsker s inequality and result in the bound @xmath256 which , although weaker than , has the same behavior for @xmath257 .",
    "the next result shows three upper bounds on the vincze - le cam distance in terms of the relative entropy ( and total variation distance ) .",
    "[ thm : re - delta - tv ] let @xmath226 be the function defined in , and let @xmath259 denote the function @xmath33 defined in .",
    "a.   if @xmath65 , then @xmath260 where @xmath261 , computed with @xmath262 furthermore , the constant @xmath263 in is the best possible .",
    "b.   if @xmath65 , then @xmath264 where @xmath265 is defined by @xmath266 furthermore , the constant @xmath267 in is the best possible .",
    "c.   if @xmath107 , then @xmath268 and the bound in is tight .",
    "a.   let @xmath269 and @xmath270 .",
    "these functions satisfy the conditions in theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) , and result in the function @xmath196 defined in .",
    "the function @xmath196 is maximal at @xmath271 with @xmath272 . since @xmath273 and @xmath274 , theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) results in with the optimality of its constant @xmath263 .",
    "b.   by the definition of @xmath267 in , it follows that for all @xmath275 @xmath276 note that holds for all @xmath277 ( i.e. , not only in @xmath252 as in ) since , for @xmath278 , is equivalent to @xmath279 which indeed holds for all @xmath280 .",
    "hence , the bound in follows from , , and . it can be verified from that @xmath265 , the maximal value of @xmath281 on @xmath282 is attained at @xmath283 and @xmath284 .",
    "the optimality of the constant @xmath267 in is shown next ( note that theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) can not be used here , since the right side of is not differentiable at  @xmath91 ) .",
    "let @xmath185 and @xmath186 be probability measures defined on @xmath187 with @xmath285 and @xmath189 for @xmath239 .",
    "then , it follows that @xmath286 \\label{eq d2 : re - delta - tv } & = \\lim_{\\varepsilon \\to 0 } \\frac{f_{\\delta}(t^\\star ) \\ , \\varepsilon \\ , \\log e + o(\\varepsilon)}{\\bigl [ r(t^\\star ) + 2c_2 \\ , ( 1-t^\\star ) \\ , \\log e \\bigr ] \\ , \\varepsilon + o(\\varepsilon ) } \\\\",
    "\\label{eq d3 : re - delta - tv } & = \\kappa_{c_2}(t^\\star ) = 1\\end{aligned}\\ ] ] where follows by the construction of @xmath185 and @xmath186 , and the use of taylor series expansions ; follows from ( note that @xmath287 ) , which yields the required optimality result in .",
    "c.   let @xmath269 and @xmath288 be defined by @xmath289 for all @xmath275 .",
    "these functions , which satisfy the conditions in theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) , yield @xmath290 and @xmath291 . theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) yields the desired result with @xmath292    we can generalize and to @xmath293 where @xmath216 and @xmath107 . in view of theorem  [",
    "theorem : tight bound ] , the locus of the allowable constant pairs @xmath294 in can be evaluated . to that end ,",
    "theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) can be used with @xmath295 , and @xmath296 for all @xmath173 .",
    "this results in a convex region , which is symmetric with respect to the straight line @xmath297 ( since @xmath298 ) .",
    "note that , and the symmetry property of this region identify , respectively , the points @xmath299 , @xmath300 and @xmath301 on its boundary ; since the sum of their coordinates is nearly constant , the boundary of this subset of the positive quadrant is nearly a straight line of slope @xmath302 .",
    "an analog of pinsker s inequality , which comes in handy for the proof of marton s conditional transportation inequality ( * ? ? ? * lemma  8.4 ) , is the following bound due to samson ( * ? ? ?",
    "* lemma  2 ) :    [ thm : samson ] if @xmath65 , then @xmath303 where @xmath304 is the distance measure defined in .",
    "we provide next an alternative proof of theorem  [ thm : samson ] , in view of theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) , with the following advantages :    a.   this proof yields the optimality of the constant in , i.e. , we prove that @xmath305 where the supremum is over all probability measures @xmath306 such that @xmath175 and @xmath107 .",
    "b.   a simple adaptation of this proof enables to derive a reverse inequality to , which holds under the boundedness assumption of the relative information ( see section  [ subsec : reverse samson s inequality ] ) .",
    "@xmath307    where , from , @xmath308 is the convex function @xmath309 and , from and , the non - negative and convex function @xmath33 in is given by @xmath310 for all @xmath275 .",
    "let @xmath311 be the non - negative and convex function @xmath226 defined in , which yields @xmath312 .",
    "note that @xmath313 , and the functions @xmath0 and @xmath311 are both differentiable at @xmath91 with @xmath314 .",
    "let @xmath315 \\to { \\ensuremath{\\mathbb{r}}}$ ] be the continuous extension of @xmath316 .",
    "the desired result follows from theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) since in this case @xmath317 as can be verified from the monotonicity of @xmath196 on @xmath252 ( increasing ) and @xmath253 ( decreasing ) .    as mentioned in @xcite , samson s inequality",
    "strengthens the pinsker - type inequality in ( * ? ? ?",
    "* lemma  3.2 ) : @xmath318 nevertheless , similarly to our alternative proof of theorem  [ thm : samson ] , one can verify that theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) yields the optimality of the constant in .",
    "let @xmath33 be a convex function with @xmath89 , and let @xmath319 be given by @xmath320 for all @xmath275 . note that @xmath321 is also convex , @xmath322 , and @xmath323 if @xmath107 . by definition ,",
    "we take @xmath324    vajda ( * ? ? ?",
    "* theorem  2 ) showed that the range of an @xmath0-divergence is given by @xmath325 where every value in this range is attainable by a suitable pair of probability measures @xmath65 .",
    "recalling remark [ remark : equivalence - fd ] , note that @xmath326 with @xmath327 defined in .",
    "* lemma  11.1 ) strengthened , showing that @xmath328 note that , provided @xmath329 and @xmath330 are finite , yields a counterpart to .",
    "next , we show that the constant in can not be improved .    [",
    "theorem : strengthened basusp ] if @xmath33 is convex with @xmath89 , then @xmath331 where the supremum is over all probability measures @xmath332 such that @xmath65 and @xmath175 .    as the first step",
    ", we give a simplified proof of ( cf .",
    "@xcite ) . in view of remark  [",
    "remark : equivalence - fd ] , it is sufficient to show that for all @xmath173 , @xmath333 if @xmath334 , reduces to @xmath335 , which holds in view of the convexity of @xmath0 and @xmath336 .",
    "if @xmath278 , we can readily check , with the aid of , that reduces to @xmath337 , which , in turn , holds because @xmath321 is convex and @xmath338 .    for the second part of the proof of",
    ", we construct a pair of probability measures @xmath240 and @xmath241 such that , for a sufficiently small @xmath184 , @xmath339 can be made arbitrarily close to the right side of . to that end , let @xmath340 $ ] , and let @xmath240 and @xmath241 be defined on the set @xmath341 with @xmath342 and @xmath343 .",
    "then , @xmath344 where holds since @xmath345 , and follows from and .",
    "csiszr ( * ? ? ?",
    "* theorem  2 ) showed that if @xmath329 and @xmath330 are finite and @xmath65 , then there exists a constant @xmath346 which depends only on @xmath0 such that @xmath347 if @xmath348 , then is superseded by where the constant is not only explicit but is the best possible according to theorem  [ theorem : strengthened basusp ] .    a direct application of theorem  [ theorem : strengthened basusp ] yields    @xmath349 \\label{eq : sup2 } & \\sup_{p \\neq q } \\frac{\\delta(p \\| q)}{|p - q| } = 1 , \\\\[0.1 cm ] \\label{eq : sup3 } & \\sup_{p \\neq q } \\frac{\\mathrm{js}(p \\| q)}{|p - q| } = \\log 2 , \\\\[0.1 cm ] \\label{eq : sup4 } & \\sup_{p \\neq q } \\frac{d_2 ^ 2(p , q)}{|p - q| } = \\frac12 , \\\\[0.1 cm ] \\label{eq : sup5 } & \\sup_{p \\neq q } \\frac{d_2",
    "^ 2(p , q ) + d_2 ^ 2(q , p)}{|p - q| } = 1\\end{aligned}\\ ] ]    where the suprema in  are over all @xmath65 with @xmath175 , and the supremum in is over all @xmath107 with @xmath175 .    the results in , and strengthen , respectively , the inequalities in ( * ? ? ?",
    "* proposition  2.35 ) , ( * ? ? ? * ( 11 ) ) and ( * ? ? ? * theorem  2 ) . the results in and form counterparts of .",
    "as the following result shows , it is possible to find bounds among @xmath0-divergences without requiring a strong condition of functional domination ( see section  [ sec : functional domination ] ) as long as the relative information is upper and/or lower bounded almost surely . assuming @xmath65 and @xmath73 , denote @xmath350 $ ] by @xmath351 \\label{eq : beta2 } \\beta_2 & \\triangleq \\operatorname*{ess\\,inf}\\frac{\\text{d}p}{\\text{d}q } \\ , ( y)\\end{aligned}\\ ] ] with the convention that if the right side of is @xmath352 , then @xmath353 . note that if @xmath107 , then @xmath354 \\label{eq : beta1-alt } \\beta_1 & = \\operatorname*{ess\\,inf}\\frac{\\text{d}q}{\\text{d}p } \\ , ( y).\\end{aligned}\\ ] ]    [ thm : fd1 ] let @xmath65 , and let @xmath33 and @xmath355 be convex functions with @xmath163 .",
    "let @xmath180 be defined as in theorem [ theorem : tight bound ] .",
    "then , @xmath356    denote @xmath357 for @xmath358 , and @xmath359 .",
    "then , @xmath360 where follows from the definition of @xmath361 , the non - negativity of @xmath51 on @xmath77 , and the fact that @xmath362 has no mass outside @xmath363 $ ] . to justify , we will consider three separate cases :    * if @xmath364 or @xmath365 , then the first term in the right side of is 0 and is satisfied with equality ; * if @xmath366 , @xmath367 , then @xmath368 and is trivially true ; * if @xmath366 , @xmath369 , and @xmath370 , then @xmath371 which , upon substitution in the left side of yields the desired result .",
    "an analogous reasoning yields .",
    "[ example : non - monotonic ] if @xmath103 and @xmath372 , we get @xmath373    [ remark : play ] in the application of theorem [ thm : fd1 ] , it is often convenient to make use of the freedom afforded by remark [ remark : equivalence - fd ] and choose the corresponding offsets such that    * the non - negativity property of @xmath51 , which is required by theorem  [ thm : fd1 ] , is satisfied ; * @xmath374 as a necessary condition for the finiteness of the right side of",
    ".    [ remark : tight constants ] similarly to the proof of theorem  [ theorem : tight bound][theorem : tight bound : partb ] ) , under the conditions therein , one can verify that the constants in theorem  [ thm : fd1 ] are the best possible among all probability measures @xmath306 with given @xmath375 ^ 2 $ ] .",
    "if @xmath196 is monotonically increasing , then theorem  [ thm : fd1 ] becomes @xmath376 in this case , if @xmath377 , sometimes it is convenient to replace @xmath377 and @xmath378 in  with @xmath379 and @xmath380 $ ] at the expense of loosening the bounds .    in the following examples , which illustrate important cases in which @xmath381 and @xmath382 are positive",
    ", we exclude the trivial case where @xmath383 .",
    "( _ gaussian distributions . _ ) [ example : gaussians ] if the variances are equal , then @xmath384 . if the means are equal and @xmath385 , then @xmath386 and @xmath366 . if the means are equal and @xmath387 , then @xmath353 and @xmath388 .",
    "( _ shifted laplace distributions . _ ) [ example : two laplacians ] let @xmath62 and @xmath63 be the probability measures whose probability density functions ( with respect to lebesgue measure ) are , respectively , given by @xmath389 and @xmath390 with @xmath391 where @xmath392 . in this case , gives @xmath393 which yields @xmath394    ( _ cramr distributions .",
    "_ ) [ example : two cramer distributions ] suppose that @xmath62 and @xmath63 have cramr probability density functions @xmath395 and @xmath396 , respectively , with @xmath397 where @xmath398 and @xmath399 . in this case , we have @xmath400 since the ratio of the probability density functions , @xmath401 , tends to @xmath402 in the limit where @xmath403 . in the special case where @xmath404 , the ratio of these probability density functions is @xmath405 at @xmath406 ; due also to the symmetry of the probability density functions around @xmath407 , it can be verified that in this special case @xmath408    ( _ cauchy distributions . _ ) [ example : two cauchy distributions ] suppose that @xmath62 and @xmath63 have cauchy probability density functions @xmath409 and @xmath410 , respectively , @xmath411 and @xmath412^{-1 } , \\quad x \\in { \\ensuremath{\\mathbb{r}}}\\end{aligned}\\ ] ] where @xmath413 . in this case",
    ", we also have @xmath400 since the ratio of the probability density functions tends to @xmath414 in the limit where @xmath403 . in the special case where @xmath415 , @xmath416    the remaining part of this section is devoted to various applications of theorem  [ thm : fd1 ] . from this point ,",
    "we make use of the definition of @xmath95 in",
    ".      an illustrative application of theorem  [ thm : fd1 ] gives upper and lower bounds on the ratio of relative entropies .",
    "[ thm : bounds re and dual ] let @xmath107 , @xmath175 , and let @xmath350 $ ] be given in - .",
    "let @xmath315 \\to [ 0 , \\infty]$ ] be the continuous extension to @xmath417 $ ] of @xmath418 defined on @xmath179 , namely , @xmath419 , @xmath420 , and @xmath421 .",
    "then , @xmath422    for @xmath173 , let @xmath423 then @xmath424 , @xmath425 , and @xmath245 in view of .",
    "the desired result follows from theorem  [ thm : fd1 ] and the monotonicity of @xmath426 shown in appendix  [ appendix : properties of kappa ] .",
    "the following result bounds the ratio of relative entropy to hellinger divergence of an arbitrary positive order @xmath428 .",
    "theorem  [ thm : improved hausslero ] extends and strengthens a result for @xmath23 by haussler and opper ( * ? ? ?",
    "* lemma  4 and ( 6 ) ) ( see also @xcite ) , which in turn generalizes the special case for @xmath429 obtained simultaneously and independently by birg and massart ( * ? ? ?",
    "* ( 7.6 ) ) .",
    "[ thm : improved hausslero ] let @xmath65 , @xmath175 , and let @xmath350 $ ] be given in -",
    ". for @xmath110 , let @xmath430 \\to [ 0 , \\infty]$ ] be the continuous extension to @xmath417 $ ] of @xmath431 defined on @xmath181 , namely @xmath432 , @xmath433 , and @xmath434 is respectively @xmath352 or @xmath207 if @xmath23 or @xmath435 .",
    "then , for @xmath23 , @xmath436 and , for @xmath437 , @xmath438    from , @xmath312 .",
    "let , for @xmath110 , @xmath439 defined for @xmath165 . since @xmath440",
    "then , for every @xmath110 , the function @xmath441 is monotonically decreasing on @xmath442 $ ] and monotonically increasing on @xmath443 ; hence , @xmath444 implies that @xmath441 is a non - negative function as required in theorem  [ thm : fd1 ] . in view of remark  [ remark : equivalence - fd ] and , we have @xmath445 .    the function @xmath430 \\to [ 0 , \\infty]$ ] in is the continuous extension of @xmath446 . as shown in appendix  [",
    "appendix : monotonicity of kappa_alpha ] , @xmath447 is monotonically increasing on @xmath417 $ ] if @xmath23 , and it is monotonically decreasing on @xmath417 $ ] if @xmath435 . the bounds in and follow , respectively , from theorem  [ thm : fd1 ] for @xmath23 and @xmath437 .",
    "theorem  [ thm : improved hausslero ] is particularly of interest for @xmath448 . in this case since , from , @xmath449 \\to [ 0 , \\infty]$ ] is monotonically increasing with @xmath450 , the left inequality in yields .    for large arguments , @xmath451 grows logarithmically .",
    "for example , if @xmath452 , it follows from that @xmath453 which is strictly smaller than the upper bound on the relative entropy in ( * ? ? ? * theorem  5 ) , given not in terms of @xmath381 but in terms of another more cumbersome quantity that controls the mass that @xmath362 may have at large values .",
    "let @xmath65 and @xmath175 .",
    "* theorem  2 ) and ( * ? ? ? * ( 11 ) ) show , respectively , @xmath457    the following result suggests a refinement in the upper bounds of and .",
    "[ thm : supertopsoell ] let @xmath65 , @xmath175 , and let @xmath350 $ ] be given in - .",
    "then , @xmath458 \\label{eq1 :",
    "superll2 } \\frac{\\delta(p\\|q)}{|p - q| } & \\leq \\max \\bigl\\{\\kappa_2(\\beta_1^{-1 } ) , \\ , \\kappa_2(\\beta_2 ) \\bigr\\}\\end{aligned}\\ ] ] with @xmath459 \\to [ 0 , \\log 2]$ ] and @xmath460 \\to [ 0 , 1]$ ] defined as the continuous extensions of @xmath461 , \\\\[0.1 cm ]",
    "\\label{eq2 : superll2 } & \\kappa_2(t ) = \\frac{|t-1|}{t+1}\\end{aligned}\\ ] ] i.e. , @xmath462    let @xmath463 , @xmath464 denote the functions @xmath33 in , and , respectively ; these functions yield @xmath9 , @xmath465 and @xmath466 as @xmath0-divergences .",
    "the functions @xmath467 and @xmath468 , as introduced in  , are the continuous extensions to @xmath417 $ ] of @xmath469 it can be verified by and that @xmath470 for @xmath471 $ ] and @xmath472 ; furthermore , @xmath467 and @xmath468 are both monotonically decreasing on @xmath46 $ ] , and monotonically increasing on @xmath473 $ ] .",
    "consequently , if @xmath474 $ ] and @xmath472 , @xmath475 in view of theorem  [ thm : fd1 ] and remark  [ remark : equivalence - fd ] , follows from , and ; follows from and .",
    "if @xmath476 or @xmath477 , referring to an unbounded relative information @xmath478 , the right inequalities in , and , respectively coincide ( due to , , and since @xmath479 ) ; otherwise , the right inequalities in , provide , respectively , sharper upper bounds than those in , .",
    "[ example : supertopsoe ] suppose that @xmath65 , @xmath175 and @xmath480 for all @xmath68 ( e.g. , @xmath481 and @xmath482 ) ; substituting @xmath483 in  gives @xmath484 \\label{eq4 : superll2 } \\frac{\\delta(p\\|q)}{|p - q| } & \\leq \\tfrac13\\end{aligned}\\ ] ] improving the upper bounds on @xmath454 and @xmath456 which are @xmath485 and  1 , respectively , according to the right inequalities in , .    for finite alphabets , ( * ? ? ?",
    "* theorem  7 ) shows @xmath486 the following theorem extends the validity of for hellinger divergences of an arbitrary order @xmath487 and for a general alphabet , while also providing a refinement of both upper and lower bounds in when the relative information @xmath478 is bounded .",
    "[ thm : supperll ] let @xmath65 , @xmath175 , @xmath155 , and let @xmath350 $ ] be given in - .",
    "let @xmath488 \\to [ 0 , \\infty)$ ] be the continuous extension of @xmath489}{t^{\\alpha}+\\alpha-1-\\alpha t } \\end{aligned}\\ ] ] i.e. , @xmath490 then , if @xmath23 , @xmath491 } \\kappa_{\\alpha}(\\beta)\\end{aligned}\\ ] ] and , if @xmath437 , @xmath492    let @xmath493 and @xmath494 denote , respectively , the functions @xmath33 in and which yield the hellinger divergence , @xmath495 , and the jensen - shannon divergence , @xmath466 , as @xmath0-divergences . from ",
    ", @xmath447 is the continuous extension to @xmath417 $ ] of @xmath496 as shown in the proof of theorem  [ thm : improved hausslero ] , for every @xmath110 and @xmath165 , we have @xmath497 ( i.e. , the denominator of @xmath498 in the right side of is non - negative , as required in theorem  [ thm : fd1 ] ) .",
    "it can be verified that @xmath430 \\to { \\ensuremath{\\mathbb{r}}}$ ] has the following monotonicity properties :    * if @xmath23 , there exists @xmath499 such that @xmath500 is monotonically increasing on @xmath501 $ ] , and it is monotonically decreasing on @xmath502 $ ] ; * if @xmath437 , @xmath447 is monotonically decreasing on @xmath417 $ ] .",
    "based on these properties of @xmath500 , for @xmath474 $ ] :    * if @xmath23 @xmath503 } \\kappa_{\\alpha}(t);\\end{aligned}\\ ] ] * if @xmath435 , @xmath504    in view of theorem  [ thm : fd1 ] , remark  [ remark : equivalence - fd ] and , the bounds in and follow respectively from and .",
    "specializing theorem  [ thm : supperll ] to @xmath505 , since @xmath506 for @xmath165 and @xmath507 achieves its global maximum at @xmath91 with @xmath508 , and imply that @xmath509 under the assumptions in example  [ example : supertopsoe ] , it follows from that @xmath510 which improves the lower bound @xmath485 in the left side of .",
    "specializing theorem  [ thm : supperll ] to @xmath16 implies that ( cf . and )",
    "@xmath511 under the assumptions in example",
    "[ example : supertopsoe ] , it follows from that @xmath512 while without any boundedness assumption , yields the weaker upper and lower bounds @xmath513      another application of theorem  [ thm : fd1 ] shows that the local behavior of @xmath0-divergences differs by only a constant , provided that the first distribution approaches the reference measure in a certain strong sense .",
    "[ thm : local behavior fd ]",
    "suppose that @xmath514 , a sequence of probability measures defined on a measurable space @xmath64 , converges to @xmath63 ( another probability measure on the same space ) in the sense that , for @xmath73 , @xmath515 where it is assumed that @xmath516 for all sufficiently large @xmath517 . under the assumptions on the functions @xmath0 , @xmath51 and @xmath196 in theorem  [ thm : fd1 ] , we have @xmath518 and @xmath519    since @xmath89 , @xmath520 } f(\\beta)\\end{aligned}\\ ] ] where we have abbreviated @xmath521 the condition in yields ( see appendix  [ appendix : proof of 2nd condition ] ) @xmath522 hence , follows from , , , , the continuity of @xmath0 at  1 ( due to its convexity ) , and since also @xmath51 has the same properties .    from - , we also have @xmath523 } \\kappa(\\beta ) \\cdot d_g(p_n \\|",
    "q ) & \\leq d_f(p_n \\| q ) \\\\ \\label{eq2 : bounds fd - pn to q } & \\leq \\sup_{\\beta",
    "\\in [ \\beta_{2,n } , \\ , \\beta_{1,n}^{-1 } ] } \\kappa(\\beta ) \\cdot d_g(p_n \\| q).\\end{aligned}\\ ] ] finally , follows from , , , and the continuity of @xmath196 .    continuing with examples  [ example : two laplacians ] , [ example : two cramer distributions ] and  [ example : two cauchy distributions ] , it is easy to check that the conditions and are satisfied in the following cases .",
    "[ example : convergent laplacians ] a sequence of laplacian probability density functions with common variance and converging means : @xmath524    a sequence of converging cramr probability density functions : [ example : convergent cramer pdfs ] @xmath525    [ example : convergent cauchy pdfs ] a sequence of converging cauchy probability density functions : @xmath526^{-1 } , \\quad x \\in { \\ensuremath{\\mathbb{r}}}\\\\ & \\lim_{n \\to \\infty } m_n = m \\in { \\ensuremath{\\mathbb{r}}}\\\\ & \\lim_{n \\to \\infty } \\gamma_n = \\gamma > 0.\\end{aligned}\\ ] ]    we will have opportunity to use the following specialization of theorem  [ thm : local behavior fd ] .",
    "[ cor : ratios ] let @xmath527 converge to @xmath63 in the sense of . then , @xmath528 and @xmath529 vanish as @xmath530 with @xmath531    the continuous extension of @xmath196 in to @xmath417 $ ] yields @xmath532 .      as mentioned in section",
    "[ section : fd ] , @xmath533 is equal to the hellinger divergence of order  2 . specializing theorem  [ thm : improved hausslero ] to the case @xmath534 results in :    [ cor : re and chi - square ]",
    "let @xmath468 be the continuous extension to @xmath535 $ ] of the function @xmath536 namely , @xmath537 , @xmath538 , and @xmath539 .",
    "then , @xmath540    corollary  [ cor : re and chi - square ] improves the upper and lower bounds in ( * ? ? ?",
    "* proposition  2 ) : @xmath541 for example , if @xmath542 , gives @xmath543 ( in nates ) whereas gives @xmath544 .",
    "note that if @xmath477 , then the upper bound in is @xmath537 whereas it is @xmath352 according to ( * ? ? ?",
    "* proposition  2 ) . in view of remark  [",
    "remark : tight constants ] , the bounds in are the best possible among all probability measures @xmath332 with given @xmath375 ^ 2 $ ] .    specializing theorem  [ thm : local behavior fd ] results in ( cf .",
    "* theorem  4.1 ) ) :    [ cor : ratios : chi ] let @xmath527 converge to @xmath63 in the sense of . then , @xmath545 and @xmath528 vanish as @xmath530 with @xmath546    the continuous extension of @xmath468 in to @xmath417 $ ] yields @xmath547 .",
    "the next result sharpens inequality   under the assumption of bounded relative information .",
    "[ thm : d - chi ] let @xmath107 and @xmath350 $ ] be defined as in .",
    "if @xmath377 , then @xmath548    the main tool for the proof of theorem [ thm : d - chi ] is the following strengthened version of jensen s inequality , which generalizes a result in ( * ? ?",
    "* theorem  1 ) .",
    "[ lemma : superjensen ] let @xmath549 be probability measures defined on a measurable space @xmath64 , and fix an arbitrary random transformation @xmath550",
    ". denote means that the marginal probability measures of the joint distribution @xmath551 are @xmath552 and @xmath553 . ]",
    "@xmath554 , and @xmath555 .",
    "let @xmath556 be a convex function , then @xmath557 ) \\right ] - f ( \\mathbb{e } [ z_\\mathtt{0 } ]   ) \\bigr ) \\leq \\mathbb{e } [ f ( \\mathbb{e } [ z_\\mathtt{1 } | x_\\mathtt{1 } ] ) ] - f ( \\mathbb{e } [ z_\\mathtt{1 } ]   ) \\end{aligned}\\ ] ] where @xmath558 , @xmath559 , and @xmath560    see appendix  [ appendix : proof superjensen ] .    letting @xmath561 , and choosing @xmath552 so that @xmath562 ( e.g.",
    ", @xmath563 is a restriction of @xmath552 to an event of @xmath552-probability less than  1 ) , becomes jensen s inequality @xmath564 ) \\leq \\mathbb{e}[f(x_\\mathtt{1})]$ ] .",
    "theorem [ thm : d - chi ] is a special case of the following result with @xmath98 @xmath565 .",
    "[ thm : gi fd ] let @xmath33 be a convex function with @xmath89 .",
    "assume that @xmath107 , let @xmath350 $ ] be defined as in  , and let @xmath72 .",
    "then , @xmath566   - f\\bigl ( 1 + \\chi^2(p\\|q ) \\bigr ) \\\\ \\label{eq : gi fd } & \\leq \\beta_1^{-1 } \\ , d_f(p \\| q).\\end{aligned}\\ ] ]    we invoke lemma [ lemma : superjensen ] with @xmath567 that is given by the deterministic transformation @xmath568 . then , @xmath569 = \\exp \\left(\\imath_{p\\|q } ( x_\\mathtt{0 } ) \\right ) $ ] .",
    "if , moreover , we let @xmath570 , we obtain @xmath571   & = 1 , \\\\ \\mathbb{e } \\left [ f ( \\mathbb{e } [ z_\\mathtt{0 } | x_\\mathtt{0 } ] ) \\right ] & = d_f ( p \\| q ) \\end{aligned}\\ ] ] and if we let @xmath572 , we have ( see ) @xmath573   & = 1 + \\chi^2 ( p \\| q ) , \\\\ \\mathbb{e } \\left [ f ( \\mathbb{e } [ z_\\mathtt{1 } | x_\\mathtt{1 } ] ) \\right ] & = \\mathbb{e } \\left [ f \\left ( \\exp ( \\imath_{p\\|q } ( x ) ) \\right ) \\right].\\end{aligned}\\ ] ] therefore , follows from lemma  [ lemma : superjensen ] .",
    "to show we use with @xmath574 and @xmath575 , and follows from lemma [ lemma : superjensen ] as well .      in view of the alternative proof of theorem  [ thm : samson ] , and the basic tool in theorem  [ thm : fd1 ] , our next result forms a reverse inequality to .",
    "[ thm : samson - refined ] let @xmath65 , and @xmath350 $ ] be given by .",
    "then , @xmath576 & \\leq \\tfrac{2}{\\log e } \\ ; d(p\\|q )   \\label{eq : samson - refined - 2}\\end{aligned}\\ ] ] where @xmath577 \\to \\bigl[0 , \\tfrac{2}{\\log e } \\bigr]$ ] is defined in .",
    "we rely on our alternative proof of theorem  [ thm : samson ] ( which already obtains the upper bound on @xmath578 ) .",
    "recall that the function @xmath196 , which is the continuous extension of to @xmath417 $ ] , is monotonically increasing on @xmath46 $ ] , and monotonically decreasing on @xmath473 $ ] . hence , @xmath579 } \\kappa(t ) & = \\min \\bigl\\{\\kappa(\\beta_1^{-1 } ) , \\ , \\kappa(\\beta_2 ) \\bigr\\ } , \\\\ \\sup_{t",
    "\\in [ \\beta_2 , \\beta_1^{-1 } ] } \\kappa(t ) & = \\kappa(1 ) = \\tfrac{2}{\\log e}\\end{aligned}\\ ] ] and the upper and lower bounds on @xmath578 ( see and ) are obtained from theorem  [ thm : fd1 ] .    from the continuous extension of",
    "to @xmath417 $ ] @xmath580 which yields that , if @xmath175 , then the left side of provides a positive lower bound on @xmath578 as long as @xmath377 .",
    "the following theorem provides several expressions of the total variation distance in terms of the relative information .",
    "these expressions prove to be useful later in this paper .",
    "[ thm : tv ] let @xmath65 , and let @xmath72 and @xmath73 be defined on a measurable space @xmath64 .",
    "then , the following equalities hold for the total variation distance between @xmath62 and @xmath63 : , and @xmath581 @xmath582 \\\\ \\label{eq : tv3 } & = 2 \\ , \\mathbb{e } \\bigl [ \\bigl ( 1 - \\exp(\\imath_{p\\|q}(y ) ) \\bigr)^- \\bigr ] \\\\ \\label{eq : tv4 } & = 2 \\ , \\mathbb{e } \\bigl [ \\bigl ( 1 - \\exp(-\\imath_{p\\|q}(x ) ) \\bigr)^+ \\bigr ] \\\\[0.1 cm ] \\label{eq : tv4a } & = 2 \\left ( \\mathbb{p}\\bigl[\\imath_{p\\|q}(x ) > 0\\bigr ] - \\mathbb{p}\\bigl[\\imath_{p\\|q}(y ) > 0\\bigr ] \\right ) \\\\ \\label{eq : tv4b } & = 2 \\left ( \\mathbb{p}\\bigl[\\imath_{p\\|q}(y ) \\leq 0\\bigr ] - \\mathbb{p}\\bigl[\\imath_{p\\|q}(x ) \\leq 0\\bigr ] \\right ) \\\\ \\label{eq : tv5 } & = 2 \\ , \\int_0 ^ 1 \\mathbb{p}\\bigl[\\imath_{p \\| q}(y )",
    "< \\log \\beta\\bigr ] \\ , \\text{d}\\beta \\\\ \\label{eq : tv6 } & = 2 \\ , \\int_0 ^ 1 \\mathbb{p}\\bigl[\\imath_{p \\| q}(x ) > \\log \\frac1{\\beta } \\bigr ] \\ ,",
    "\\text{d}\\beta \\\\ \\label{eq : tv100 } & = 2 \\ , \\int_1^{\\beta_1^{-1 } } \\beta^{-2 } \\left [ 1 -\\mathds{f}_{p\\|q}(\\log \\beta ) \\right ] \\ , \\text{d}\\beta.\\end{aligned}\\ ] ] furthermore , if @xmath107 , then @xmath583.\\end{aligned}\\ ] ]    eq",
    ".   follows from the definitions in and . since @xmath584 and @xmath585 , for all @xmath586 , and follow from and @xmath587 = \\int \\left ( 1 - \\frac{\\mathrm{d}p}{\\mathrm{d}q } \\right ) \\ , \\mathrm{d}q = 0.\\end{aligned}\\ ] ]    by change of measure , for any measurable function @xmath588 with @xmath589 < \\infty$ ] and @xmath590 < \\infty$ ] , @xmath591 & = \\mathbb{e } \\bigl[\\frac{\\text{d}p}{\\text{d}q}(y ) \\ , f(y)\\bigr ] \\\\ & = \\mathbb{e } \\bigl [ \\exp\\bigl(\\imath_{p\\|q}(y)\\bigr ) \\ , f(y)\\bigr ] .",
    "\\end{split}\\end{aligned}\\ ] ] hence , it follows from that @xmath592 & = \\mathbb{e } \\bigl[1\\bigl\\{\\imath_{p\\|q}(x ) > 0\\bigr\\}\\bigr ]   \\\\ \\label{eq : cm1 } & = \\mathbb{e } \\bigl[\\exp(\\imath_{p\\|q}(y ) ) \\ ; 1\\bigl\\{\\imath_{p\\|q}(y ) > 0\\bigr\\ } \\bigr]\\end{aligned}\\ ] ] and @xmath593 & = \\mathbb{e } \\bigl[1\\bigl\\{\\imath_{p\\|q}(x ) \\leq 0\\bigr\\}\\bigr ]   \\\\ \\label{eq : cm2 } & = \\mathbb{e } \\bigl[\\exp(\\imath_{p\\|q}(y ) ) \\ ; 1\\bigl\\{\\imath_{p\\|q}(y ) \\leq 0\\bigr\\ } \\bigr].\\end{aligned}\\ ] ]    to show , note that from and the change of measure in , we get @xmath594   \\\\ & = \\mathbb{e } \\bigl [ \\bigl ( \\exp(\\imath_{p\\|q}(y ) ) - 1 \\bigr ) \\ ; 1\\bigl\\ { \\imath_{p \\| q}(y ) > 0 \\bigr\\ } \\bigr ] \\\\ & = \\mathbb{e } \\bigl [ \\bigl ( 1 - \\exp(-\\imath_{p\\|q}(x ) ) \\bigr ) \\ ; 1\\bigl\\ { \\imath_{p \\| q}(x ) > 0 \\bigr\\ } \\bigr ] \\\\ & = \\mathbb{e } \\bigl [ \\bigl ( 1 - \\exp(-\\imath_{p\\|q}(x ) ) \\bigr)^{+ } \\bigr].\\end{aligned}\\ ] ]    to show and , we get from and @xmath595 \\\\",
    "\\label{eq2 : tv4a } & = \\mathbb{e } \\bigl [ \\bigl(\\exp(\\imath_{p\\|q}(y ) ) - 1 \\bigr ) \\ ; 1\\bigl\\{\\imath_{p\\|q}(y ) > 0\\bigr\\ } \\bigr ] \\\\ \\label{eq3 : tv4a } & = \\mathbb{p } \\bigl[\\imath_{p\\|q}(x ) > 0\\bigr ] - \\mathbb{p } \\bigl[\\imath_{p\\|q}(y ) > 0\\bigr]\\end{aligned}\\ ] ] where is , and is equivalent to .",
    "to show , we use and the notation in in order to write @xmath596   \\\\ & = \\mathbb{e } \\bigl [ ( z-1 ) \\ , 1\\{z>1\\ } \\bigr ] \\\\ \\label{zzb } & = \\int_0^{\\infty } \\mathbb{p } \\bigl [ ( z-1 ) \\ , 1\\{z>1\\ } \\geq \\beta \\bigr ] \\ , \\text{d } \\beta \\\\ & = \\int_1^{\\infty } \\mathbb{p } \\bigl [ z \\geq \\beta \\bigr ] \\ , \\text{d } \\beta \\\\ \\label{zzc } & = \\int_0 ^ 1 \\mathbb{p } \\bigl [ z < \\beta \\bigr ] \\ , \\text{d } \\beta\\end{aligned}\\ ] ] where follows from with @xmath597 in ; exploits the fact that the expectation of a non - negative random variable is the integral of its complementary cumulative distribution function ; and holds since @xmath597 is non - negative with @xmath598 = 1 $ ] .    to show , we use to write @xmath599 \\\\ & = \\int_0^\\infty \\mathbb{p } \\bigl [ \\bigl ( 1 - \\exp(-\\imath_{p\\|q}(x ) ) \\bigr)^{+ } > \\beta \\bigr ] \\ , \\text{d } \\beta \\\\ & = \\int_0 ^ 1 \\mathbb{p } \\bigl [ \\bigl ( 1 - \\exp(-\\imath_{p\\|q}(x ) ) \\bigr)^{+ } > \\beta \\bigr ] \\ , \\text{d } \\beta \\\\ & = \\int_0 ^ 1 \\mathbb{p } \\left [ \\imath_{p\\|q}(x ) > \\log \\frac1{1-\\beta } \\right ] \\ , \\text{d } \\beta \\\\ & = \\int_0 ^ 1 \\mathbb{p } \\left [ \\imath_{p\\|q}(x ) > \\log \\frac1{\\beta } \\right ] \\ , \\text{d } \\beta.\\end{aligned}\\ ] ]    to prove , a change of variable of integration in , and the fact that @xmath600 for @xmath601 give @xmath602 \\ , \\text{d}t \\\\[0.1 cm ] & = \\int_0 ^ 1 \\bigl [ 1 - \\mathds{f}_{p\\|q}\\bigl(\\log \\frac1{t}\\bigr ) \\bigr ] \\text{d}t \\\\[0.1 cm ] & = \\int_1^{\\infty } \\frac{1-\\mathds{f}_{p\\|q}(\\log \\beta)}{\\beta^2 } \\ ; \\text{d}\\beta \\\\[0.1 cm ] & = \\int_1^{\\beta_1^{-1 } } \\frac{1-\\mathds{f}_{p\\|q}(\\log \\beta)}{\\beta^2 } \\ ; \\text{d}\\beta\\end{aligned}\\ ] ] with the convention that @xmath603 if @xmath353 .",
    "assume that @xmath107 .",
    "to show simply note that , the symmetry of the total variation distance , and the anti - symmetry of the relative information where @xmath604 enable to conclude that @xmath605.\\end{aligned}\\ ] ] similarly , switching @xmath62 and @xmath63 in results in @xmath606 \\\\ & = \\mathbb{e } \\bigl [ \\bigl ( 1 - \\exp\\bigl(-\\imath_{p \\| q}(x)\\bigr ) \\bigr)^{- } \\bigr]\\end{aligned}\\ ] ] which proves .    in view of , if @xmath65 , the supremum in is a maximum which is achieved by the event @xmath607    similarly to theorem  [ thm : tv ] , the following theorem provides several expressions of the relative entropy in terms of the relative information spectrum .    [ theorem : relative entropy - exact expressions ] if @xmath151 , then @xmath608 \\label{eq2 : re } & = \\int_1^\\infty \\frac{1 - \\mathds{f}_{p\\|q}(\\log \\beta)}{\\beta } \\ , \\mathrm{d}\\beta - \\int_0 ^ 1 \\frac{\\mathds{f}_{p\\|q}(\\log \\beta)}{\\beta } \\ , \\mathrm{d}\\beta \\\\[0.1 cm ] \\label{eq3 : re } & = \\int_0^\\infty \\pr\\bigl[\\imath_{p\\|q}(y ) > \\alpha \\bigr ] \\",
    ", \\alpha e^{\\alpha } \\ , \\mathrm{d}\\alpha - \\int_{-\\infty}^0 \\pr\\bigl[\\imath_{p\\|q}(y )",
    "< \\alpha \\bigr ] \\ , \\alpha e^{\\alpha } \\ , \\mathrm{d}\\alpha\\end{aligned}\\",
    "] ] where @xmath73 , and for convenience and assume that the relative information and the resulting relative entropy are in nats .",
    "the expectation of a real - valued random variable @xmath609 is equal to @xmath610 = \\int_0^\\infty \\pr[v > t ] \\ , \\mathrm{d}t - \\int_{-\\infty}^0 \\pr[v < t ] \\ , \\mathrm{d}t\\end{aligned}\\ ] ] where we are free to substitute @xmath611 by @xmath612 , and @xmath613 by @xmath614 .",
    "if we let @xmath615 with @xmath72 , then yields provided that @xmath616 < \\infty$ ] .",
    ".   follows from by using the substitution @xmath617 when the relative entropy is expressed in nats .    to prove , let @xmath618 with @xmath73 , and let @xmath619 where @xmath95 is given in with natural logarithm .",
    "the function @xmath311 is strictly monotonically increasing on @xmath443 , on which interval we define its inverse by @xmath620 ; it is also strictly monotonically decreasing on @xmath442 $ ] , on which interval we define its inverse by @xmath621 \\to ( 0,1]$ ] .",
    "then , only the first integral on the right side of can be non - zero , and we decompose it as @xmath622 \\ , \\mathrm{d}t + \\int_0^{\\infty } \\pr\\bigl[z < 1 , \\ , r(z ) > t\\bigr ] \\ , \\mathrm{d}t \\\\ & = \\int_0^{\\infty } \\pr\\bigl[z > s_1(t)\\bigr ] \\ , \\mathrm{d}t + \\int_0 ^ 1 \\pr\\bigl[z < s_2(t ) \\bigr ] \\ , \\mathrm{d}t \\\\ \\label{eq : change of var } & = \\int_1^{\\infty } \\pr[z > v ] \\",
    ", \\log_e v \\ , \\mathrm{d}v - \\int_0 ^ 1 \\pr[z < v ] \\ , \\log_e v \\ , \\mathrm{d}v\\end{aligned}\\ ] ] where follows from the change of variable of integration @xmath623 . upon taking @xmath624 on both sides of the inequalities inside the probabilities in , and",
    "a further change of the variable of integration @xmath625 , is seen to be equal to .      in this section ,",
    "we provide three upper bounds on @xmath9 which complement .",
    "[ thm : ubtv ] if @xmath65 and @xmath72 , then @xmath626 $ ] , @xmath627 substituting @xmath628 , taking expectation of both sides of , and using give @xmath629 \\\\ & = \\mathbb{e } \\bigl [ \\imath_{p\\|q}(x ) + | \\imath_{p\\|q}(x ) | \\bigr ] \\\\ & = d(p \\| q ) + \\mathbb{e } \\bigl[|\\imath_{p\\|q}(x)| \\bigr].\\end{aligned}\\ ] ]    theorem  [ thm : ubtv ] is tighter than pinsker s bound in ( * ? ? ?",
    "* ( 2.3.14 ) ) : latexmath:[\\ ] ] where follows from , and the monotonicity of @xmath51 , and is due to .",
    "discussions with jingbo liu , vincent tan and mark m. wilde are gratefully acknowledged .",
    "e. abbe and c. sandon , `` community detection in general stochastic block models : fundamental limits and efficient recovery algorithms , '' march  2015 .",
    "[ online ] .",
    "available at http://arxiv.org/abs/1503.00609 . to appear in the _",
    "56th annual ieee symposium on foundations of computer science _ , berkeley , california , usa , october  2015 .",
    "e. abbe and c. sandon , `` recovering communities in the general stochastic block model without knowing the parameters , '' june  2015 .",
    "[ online ] .",
    "available at http://arxiv.org/abs/1506.03729 . to appear in the _ 29th annual conference on neural information processing systems _ , montral , canada , december  2015 .",
    "a. barron , `` information theory and martingales , '' presented at the _ 1991 ieee international symposium on information theory _",
    "( recent results session ) , budapest , hungary , june 2329 , 1991 .",
    "[ online ] .",
    "available at http://www.stat.yale.edu/~arb4/publications_files/information theory and maringales.pdf[http://www.stat.yale.edu/~arb4/publications_files/information theory and maringales.pdf ] .",
    "a. basu , h. shioya and c. park , `` statistical inference : the minimum distance approach , '' _ chapman & hall/ crc monographs on statistics and applied probability _ ,",
    "vol .  120 , crc press , boca raton , florida , usa , june 2011 .                c. d. charalambous , i. tzortzis , s. loyka and t. charalambous , `` extremum problems with total variation distance and their applications , '' _ ieee trans .",
    "on automatic control _ , vol .",
    "59 , no .  9 , pp .",
    "23532368 , september 2014 .",
    "i. csiszr , `` eine informationstheoretische ungleichung und ihre anwendung auf den bewis der ergodizitt von markhoffschen ketten , '' _ publ . math .",
    "_ , vol .",
    "8 , pp .  85108 , january 1963 .",
    "s. s. dragomir , `` upper and lower bounds for csiszr @xmath0-divergence in terms of the kullback - leibler distance and applications , '' _",
    "inequalities for csiszr f - divergence in information theory , rgmia monographs _ , edited by s.  s.  dragomir and t.  m.  rassias , victoria university , australia , 2000 .",
    "s. s. dragomir , `` an upper bound for the csiszr @xmath0-divergence in terms of the variational distance and applications , '' _",
    "inequalities for csiszr f - divergence in information theory , rgmia monographs _ , edited by s.  s.  dragomir and t.  m.  rassias , victoria university , australia , 2000",
    ".    s. s. dragomir and v. glucevi , `` some inequalities for the kullback - leibler and @xmath1-distances in information theory and applications , '' _ tamsui oxford journal of mathematical sciences _ ,",
    "17 , no .  2 , pp .",
    "97111 , 2001 .",
    "e. even - dar , s. m. kakade and y. mansour , `` the value of observation for monitoring dynamical systems , '' _ proceedings of the international joint conference on artificial intelligence _ , pp .  24742479 , hyderabad , india , january 2007 .",
    "h. jefrreys , `` an invariant form for the prior probability in estimation problems , '' _ proceedings of the royal society of london .",
    "series  a , mathematical and physical sciences _ , vol .",
    "186 , no .",
    "1007 , pp .  453461 , september 1946 .",
    "m. krajci , c. f. liu , l. mikes and s. m. moser , `` performance analysis of fano coding , '' _ proceedings of the ieee 2015 international symposium on information theory _ , hong kong , pp .",
    "17461750 , june  1419 , 2015 .",
    "j. liu , p. cuff and s. verd , `` resolvability in @xmath2 with applications to lossy compression and wiretap channels , '' _ proceedings of the 2015 ieee international symposium on information theory _ , pp .  755759 , hong kong , june 1419 , 2015 .",
    "j. liu , p. cuff and s. verd , `` one - shot mutual covering lemma and marton s inner bound with a common message , '' _ proceedings of the 2015 ieee international symposium on information theory _ , pp .  14571461 , hong kong , june 1419 , 2015 .",
    "k. pearson , `` on the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling , '' _ the london , edinburgh , and dublin philosophical magazine and journal of science _ ,",
    "50 , no .",
    "302 , pp .  157175 , 1900 .",
    "m.  raginsky and i.  sason , `` concentration of measure inequalities in information theory , communications and coding , '' _ foundations and trends in communications and information theory _ ,",
    "10 , no . 1 - 2 ,",
    "pp .  1250 , 2013 .",
    "( second edition , 2014 ) .",
    "i. sason and s. verd , `` upper bounds on the relative entropy and rnyi divergence as a function of total variation distance for finite alphabets , '' _ proceedings of the 2015 ieee information theory workshop _ , pp .",
    "214218 , jeju island , south korea , october  1115 , 2015 .",
    "m. tomamichel and v. y. f. tan , `` a tight upper bound for the third - order asymptotics for most discrete memoryless channels , '' _ ieee trans . on information",
    "59 , no .  11 , pp .  70417051 ,",
    "november 2013 .",
    "s. verd , `` total variation distance and the distribution of the relative information , '' _ proceedings of the 2014 information theory and applications workshop _ , pp .",
    "499501 , san - diego , california , usa , february  2014 ."
  ],
  "abstract_text": [
    "<S> this paper develops methods to obtain bounds on an @xmath0-divergence based on one or several other @xmath0-divergences , dealing with pairs of probability measures defined on arbitrary alphabets . </S>",
    "<S> functional domination is one such approach , where special emphasis is placed on finding the best possible constant upper bounding a ratio of @xmath0-divergences . </S>",
    "<S> another approach used for the derivation of bounds among @xmath0-divergences relies on moment inequalities and the logarithmic - convexity property , which is studied in this paper to obtain tight bounds on the relative entropy and bhattacharyya distance in terms of @xmath1 divergences . a rich variety of bounds </S>",
    "<S> are shown to hold under boundedness assumptions on the relative information . </S>",
    "<S> special attention is on the total variation distance and its relation to the relative information and relative entropy , including  </S>",
    "<S> reverse pinsker inequalities , \" as well as on the @xmath2 divergence , which generalizes the total variation distance . </S>",
    "<S> pinsker s inequality is extended for this type of @xmath0-divergence , a result which leads to an inequality that links the relative entropy and relative information spectrum . </S>",
    "<S> integral expressions of the rnyi divergence in terms of the relative information spectrum are derived , leading to bounds on the rnyi divergence in terms of either the variational distance or relative entropy .    * keywords * : relative entropy , total variation distance , @xmath0-divergence , rnyi divergence , pinsker s inequality , relative information . </S>"
  ]
}