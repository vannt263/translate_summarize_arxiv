{
  "article_text": [
    "the recognition that neurons are computational devices is one of the foundations of modern neuroscience @xcite .",
    "however , determining the functional form of such computation is extremely difficult , if only because while one often knows the output ( the spikes ) the input ( synaptic activity ) is almost always unknown .",
    "often , therefore , scientists must draw inferences about the computation from its results , namely the output spike trains and their statistics . in this vein , many researchers have used information theory to determine , via calculation of the entropy rate , a neuron s channel capacity , i.e. , how much information the neuron could conceivably transmit , given the distribution of observed spikes @xcite .",
    "however , entropy quantifies randomness , and says little about how much _ structure _ a spike train has , or the amount and type of computation which must have , at a minimum , taken place to produce this structure . here , and throughout this paper , we mean `` computational structure '' information - theoretically , i.e. , the most compact effective description of a process capable of _ statistically _ reproducing the observed spike trains .",
    "the complexity of this structure is the number of bits needed to describe it .",
    "this is different from the algorithmic information content of a spike train , which is the number of bits needed to reproduce the latter _ exactly _ , describing not only its regularities , but also its accidental , noisy details .",
    "our goal is to develop rigorous yet practical methods for determining the minimal computational structure necessary and sufficient to generate neural spike trains .",
    "we are able to do this through non - parametric analysis of the directly - observable spike trains , without resorting to _ a priori _ assumptions about what kind of structure they have .",
    "we do this by identifying the minimal hidden markov model ( hmm ) which can statistically predict the future of the spike train without loss of information .",
    "this hmm also generates spike trains with the same statistics as the observed train .",
    "it thus defines a program which describes the spike train s computational structure , letting us quantify , in bits , the structure s complexity .    from multiple directions , several groups , including our own ,",
    "have shown that minimal generative models of time series can be discovered by clustering histories into `` states '' , based on their conditional distributions over future events @xcite .",
    "the observed time series need _ not _ be markovian ( few spike trains are ) , but the construction always yields the minimal hmm capable of generating and predicting the original process . following @xcite , we will call such a hmm a `` causal state model '' ( csm ) . within this framework ,",
    "the model discovery algorithm called _ causal state splitting reconstruction _ , or cssr @xcite is an adaptive non - parametric method which consistently estimates a system s csm from time - series data . in this paper",
    "we adapt cssr for use in spike train analysis .",
    "cssr provides us with non - parametric estimates of the time- and history- dependent spiking probabilities found by more familiar parametric analyses . unlike those analyses ,",
    "it is also capable , in the limit of infinite data , of capturing _ all _ the information about the computational structure of the spike - generating process contained in the spikes themselves . in particular",
    ", the csm quantifies the _ complexity _ of the spike - generating process by showing how much information about the history of the spikes is relevant to their future , i.e. , how much information is needed to reproduce the spike train statistically .",
    "this is equivalent to the log of the effective number of statistically - distinct states of the process @xcite . while this is not the same as the algorithmic information content , we show that csms can also approximate the average algorithmic information content , splitting it into three parts : ( 1 ) the generative process s complexity in our sense ; ( 2 ) the _ internal entropy rate _ of the generative process , the extra information needed to describe the exact state transitions the undergone while generating the spike train ; and ( 3 ) the _ residual randomness _ in the spikes , unconstrained by the generative process .",
    "the first of these quantifies the spike train s structure , the last two its randomness .",
    "below , we give precise definitions of these quantities , both their ensemble averages (  [ subsect : alginfocont ] ) and their functional dependence on time (  [ sec : time - varying - compl ] ) .",
    "the time - dependent versions allow us to determine when the neuron is traversing states requiring complex descriptions .",
    "our methods put hard numerical lower bounds on the amount of computational structure which must be present to generate the observed spikes .",
    "they also quantify , in bits , the extent to which the neuron is driven by external forces .",
    "we demonstrate our approach using both simulated and experimentally recorded single - neuron spike trains .",
    "we discuss the interpretation of our measures , and how they add to our understanding of neuronal computation .",
    "throughout this paper we treat spike trains as stochastic binary time series , with time divided into discrete , equal - duration bins steps ( typically at one millisecond resolution ) ; `` 1 '' corresponds to a spike and `` 0 '' to no spike .",
    "our aim is to find a minimal description of the computational structure present in such a time series .",
    "heuristically , the structure present in a spike train can be described by a `` program '' which can reproduce the spikes statistically .",
    "the information needed to describe this program ( loosely speaking the program length ) quantifies the structure s complexity .",
    "our approach uses minimal , optimally predictive hmms , or _",
    "causal state models _ ( csms ) , reconstructed from the data , to describe the program .",
    "( we clarify our use of `` minimal '' below . )",
    "the csms are then used to calculate various measures of the computational structure , such as its complexity .",
    "the states are chosen so that they are optimal predictors of the spike train s future , using only the information available from the train s history .",
    "( we discuss the limitations of this below . )",
    "specifically the states @xmath0 are defined by grouping the histories of past spiking activity @xmath1 which occur in the spike train into equivalence classes , where all members of a given equivalence class are statistically equivalent in terms of predicting the future spiking @xmath2 .",
    "( @xmath3 denotes the sequence of random observables , i.e. , spikes or their absence , between @xmath4 and @xmath5 while @xmath6 denotes the random observable at time @xmath7 .",
    "the notation is similar for the states . )",
    "this construction ensures that the causal states are markovian , even if the spike train is not ( * ? ? ?",
    "* lemma 6 , p.  839 ) . therefore , at all times @xmath7 the system and its possible future evolution(s ) can be specified by the state @xmath0 .",
    "like all hmms , a csm can be represented pictorially by a directed graph , with nodes standing for the process s hidden states and directed edges the possible transitions between these states .",
    "each edge is labeled with the observable / symbol emitted during the corresponding transition ( `` 1 '' for a spike and `` 0 '' for no spike ) , and the probability of traversing that edge given that the system started in that state .",
    "the csm also specifies the time - averaged probability of occupying any state ( via the ergodic theorem for markov chains ) .",
    "the theory is described in more detail below , but at this point examples may clarify the ideas .",
    "figures [ fig:1 ] a and b show two simple csms .",
    "both are built from simulated @xmath8 hz spike trains 200 seconds in length ( 1 msec time bins , @xmath9 iid at each time when spiking is possible ) .",
    "however , spike trains generated from the csm in figure [ fig:1 ] b have a 5 msec refractory period after each spike ( when @xmath10 ) , while the spiking rate in non - refractory periods is still 40 hz ( @xmath9 ) .",
    "the refractory period is additional structure , represented by the extra states .",
    "state @xmath11 represents the status of the neuron during 40 hz spiking , outside of the refractory periods .",
    "while in this state , the neuron either emits no spike ( @xmath12 ) , staying in state @xmath11 , or emits a spike ( @xmath13 ) with probability @xmath9 and moves to state @xmath14 .",
    "the equivalence class of past spiking histories defining state @xmath11 therefore includes all past spiking histories for which the most recent five symbols are 0 , symbolically @xmath15 .",
    "state @xmath14 is the neuron s state during the first msec of the refractory period .",
    "it is defined by the set of spiking histories @xmath16 .",
    "no spike can be emitted during a refractory period so the transition to state @xmath17 is certain and the symbol emitted is always 0. in this manner the neuron proceeds through states @xmath17 to @xmath18 and back to state @xmath11 whereupon it is possible to spike again .",
    "the rest of this section is divided into four subsections .",
    "first , we briefly review the formal theory behind csms ( for details , see @xcite ) and discuss why they can be considered a good choice for understanding the structural content of spike trains .",
    "second , we describe the _ causal state splitting reconstruction ( cssr ) _ algorithm used to reconstruct csms from observed spike trains @xcite .",
    "we emphasize that cssr requires no _ a priori _ knowledge of the structure of the csm which is discovered from the spike train .",
    "third , we discuss two different notions of spike train structure , namely statistical complexity and algorithmic information content .",
    "these two measures can be interpreted as different aspects of a spike train s computational structure , and each can be related to the reconstructed csm .",
    "fourth and finally , we show how the reconstructed csm can be used to predict spiking , measure the neural response and detect the influence of external stimuli .",
    "the foundation of the theory of causal states is the concept of a _ predictively sufficient statistics_. a statistic , @xmath19 , on one random variable , @xmath20 , is sufficient for predicting another random variable , @xmath21 , when @xmath22 and @xmath20 have the same information about @xmath21 , @xmath23 = i[\\eta(x);y]$ ] .",
    "this holds if and only if @xmath20 and @xmath21 are conditionally independent given @xmath22 : @xmath24 .",
    "this is a close relative of the familiar idea of _ parametric _ sufficiency ; in bayesian statistics , where parameters are random variables , parametric sufficiency is a special case of predictive sufficiency @xcite .",
    "predictive sufficiency shares all of parametric sufficiency s optimality properties @xcite . however , a statistic s predictive sufficiency depends only on the actual joint distribution of @xmath20 and @xmath21 , not on any parametric model of that distribution .",
    "again as in the parametric case , a _ minimal _",
    "predictively sufficient statistic @xmath25 is one which is a function of every other sufficient statistic @xmath19 , i.e. , @xmath26 for some @xmath27 .",
    "minimal sufficient statistics are the most compact summaries of the data which retain all the predictively - relevant information .",
    "a basic result is that a minimal sufficient statistic always exists and is ( essentially ) unique , up to isomorphism @xcite .    in the context of stochastic processes , such as spike trains",
    ", @xmath25 is the minimal sufficient statistic of the history @xmath28 for predicting future of the process , @xmath29 .",
    "this statistic is the optimal predictor of the observations .",
    "the sequence of values of the minimal sufficient statistic , @xmath30 , is another stochastic process .",
    "this process is always a homogeneous markov chain , whether or not the @xmath6 process is @xcite .",
    "turned around , this means that the original @xmath6 process is always a random function of a homogeneous markov chain , whose latent states , named the _ causal states _ by @xcite , are optimal , minimal predictors of the future of the time series .",
    "a _ causal state model _ or _ causal state machine _ is a stochastic automaton or hmm constructed so that its markov states are minimal sufficient statistics for predicting the future of the spike train , and consequently can generate spike trains statistically identical to those observed .",
    "causal state _ reconstruction _ means inferring the causal states from the observed spike train .",
    "following @xcite , the causal states can be seen as equivalence classes of spike - train histories @xmath31 which maximize the mutual information between the state(s ) and the future of the spike train @xmath2 . because they are sufficient",
    ", they predict the future of the spike train as well as it can be predicted from its history alone . because they are minimal",
    ", the number of states or equivalence classes is as small as it can be without discarding predictive power .",
    "formally , two histories , @xmath32 and @xmath33 , are equivalent when @xmath34 .",
    "the equivalence class of @xmath32 is @xmath35 $ ] .",
    "define the function which maps histories to their equivalence classes : @xmath36}\\ ] ] @xmath37 the causal states are the possible values of @xmath25 , i.e. , the equivalence classes ; each corresponds to a distinct distribution for the future .",
    "the state at time @xmath7 is @xmath38 .",
    "clearly , @xmath39 is a sufficient statistic .",
    "it is also minimal , since if @xmath19 is sufficient , then @xmath40 implies @xmath41 .",
    "one can further show ( * ? ? ?",
    "* theorem 3 ) that @xmath25 is the _",
    "unique _ minimal sufficient statistic , meaning that any other must be isomorphic to it .",
    "in addition to being minimal sufficient statistics , the causal states have some other important properties which make them ideal for quantifying structure @xcite .",
    "( 1 ) as mentioned , @xmath42 is a markov process , and one can write the observed process @xmath20 as a random function of the causal state process , i.e. , @xmath20 has a natural hidden - markov - model representation .",
    "( 2 ) the causal states are recursively calculable ; there is a function @xmath43 such that @xmath44  see appendix [ app : filtering ] .",
    "( 3 ) csms are closely related to the `` predictive state representations '' of controlled dynamical systems @xcite ; see appendix [ sec : csts - and - psrs ] .",
    "our goal is to find a minimal sufficient statistic for the spike train , which will form a hidden markov model . as stated previously , the states of this model are equivalence classes of spiking histories @xmath45 . in practice , we need an algorithm which can both cluster histories into groups which preserve their conditional distribution of futures , and find the history length @xmath46 at which the past may be truncated while preserving the computational structure of the spike train .",
    "the former is accomplished by the cssr algorithm @xcite for inferring causal states from data by building a recursive next - step - sufficient statistic .",
    "=    i[x_{t+1};x_{-\\infty}^t]$ ] , but not necessarily for longer predictions .",
    "cssr relies on the theorem that if @xmath19 is next - step sufficient , and it is recursively calculable , then @xmath19 is sufficient for the whole of the future @xcite .",
    "cssr first finds a next - step sufficient statistic , and then refines it to be recursive .",
    "] we do the latter by minimizing schwartz s bayesian information criterion ( bic ) over @xmath46 .",
    "to save space , we just sketch the cssr algorithm here .",
    "cssr starts by treating the process as an independent , identically - distributed sequence , with one causal state .",
    "it adds states when statistical tests show that the current set of states is not sufficient .",
    "suppose we have a sequence @xmath47 of length @xmath48 from a finite alphabet @xmath49 of size @xmath50 .",
    "we wish to derive from this an estimate @xmath51 of the minimal sufficient statistic @xmath25 .",
    "we do this by finding a set @xmath52 of states , each of which will be a set of strings , or finite - length histories .",
    "the function @xmath51 will then map a history @xmath32 to whichever state contains a suffix of @xmath32 ( taking `` suffix '' in the usual string - manipulation sense ) .",
    "although each state can contain multiple suffixes , one can check @xcite that the mapping @xmath51 will never be ambiguous .",
    "the _ null hypothesis _ is that the process is markovian on the basis of the states in @xmath52 , @xmath53 for all @xmath54 . in words ,",
    "adding an extra piece of history does not change the conditional distribution for the next observation .",
    "we can check this with standard statistical tests , such as @xmath55 or kolmogorov - smirnov . in this paper , we used a ks test of size @xmath56 .",
    ", decreasing @xmath57 tends to yield simpler csms with fewer states . in a sense , it is a sort of regularization coefficient .",
    "the influence of this regularization diminishes as @xmath48 increases . for the data used in the results section of this paper , varying @xmath57 in the range @xmath58 made little difference . ]",
    "if we reject this hypothesis , we fall back on a _ restricted alternative hypothesis _ , that we have the right set of conditional distributions , but have matched them with the wrong histories .",
    "that is , @xmath59 for some @xmath60 , but @xmath61 .",
    "if this hypothesis passes a test of size @xmath57 , then @xmath62 is the state to which we assign the history exists , we chose the one for which @xmath63 differs least , in total variation distance , from @xmath64 , which is plausible and convenient .",
    "however , which state we chose is irrelevant in the limit @xmath65 , so long as the difference between the distributions is not statistically significant . ] .",
    "only if the ( [ restalthyp ] ) is itself rejected do we create a new state , with the suffix @xmath66 .",
    "the algorithm itself has three phases .",
    "phase i initializes @xmath52 to a single state , which contains only the null suffix @xmath67 .",
    "( that is , @xmath67 is a suffix of any string . )",
    "the length of the longest suffix in @xmath52 is @xmath68 ; this starts at 0 .",
    "phase ii iteratively tests the successive versions of the null hypothesis , eq",
    ".  [ nullhyp ] , and @xmath68 increases by one each iteration , until we reach some maximum length @xmath46 .",
    "at the end of ii , @xmath51 is ( approximately ) next - step sufficient .",
    "phase iii makes @xmath51 recursively calculable , by splitting the states until they have deterministic transitions . under mild technical conditions ( a finite true number of states , etc . )",
    ", cssr converges in probability on the correct csm as @xmath69 , provided only that @xmath46 is long enough to discriminate all of the states .",
    "the error of the predicted distributions of futures @xmath70 , measured by total variation distance , decays as @xmath71 .",
    "section 4 of @xcite details cssr s convergence properties .",
    "comparisons of cssr s performance with that of more traditional expectation maximization based approaches can also be found in @xcite as can time complexity bounds for the algorithm .",
    "depending upon the machine used , cssr can process an @xmath72 time series in under a minute .",
    "cssr requires no _ a priori _ knowledge of the csm s structure , but does need a choice of of @xmath46 ; here pick it by minimizing the bic of the reconstructed models over @xmath46 , i.e. , @xmath73 where @xmath74 is the likelihood , @xmath48 is the data length and @xmath75 is the number of model parameters , in our case the number of predictive states involved in describing the csm will be ( number of states)*(number of symbols - 1 ) since the sum of the outgoing probabilities for each state is constrained to be 1 .",
    "thus , for a binary alphabet , @xmath76 number of states . ]",
    "bic s logarithmic - with-@xmath48 penalty term helps keep the number of causal states from growing too quickly with increased data size , which is why we use it instead of the akaike information criterion ( aic ) .",
    "also , bic is known to be consistent for selecting the order of markov chains and variable - length markov models @xcite , both of which are sub - classes of csms .",
    "writing the observed spike train as @xmath77 , and the state sequence as @xmath78 , the total likelihood of the spike train is @xmath79 the sum over all possible causal state sequences of the joint probability of the spike train and the state sequence .",
    "since the states update recursively , @xmath80 , the starting state @xmath81 and the spike train @xmath77 fix the entire state sequence @xmath78 .",
    "thus the sum over state sequences can be replaced by a sum over initial states @xmath82 with the state probabilities @xmath83 coming from the csm . by the markov property , @xmath84",
    "selecting @xmath46 is now straightfoward : for each value of @xmath46 , we build the csm from the spike train , calculate the likelihood using eq.[eqn : likelihood - as - sum - over - initial - states ] and [ eqn : likelihood - as - factor - over - sequence ] , and pick the value , and csm , minimizing eq .",
    "[ eqn : bic - defined ] .",
    "we try all values of @xmath46 up to a model - independent upper bound . for a wide range of stochastic processes",
    ", @xcite showed that the length @xmath85 of subsequences for which probabilities can be consistently and non - parametrically estimated can grow as fast as @xmath86 , where @xmath27 is the entropy rate , but no faster .",
    "cssr estimates the distribution of the next symbol given the previous @xmath46 symbols , which is equivalent to estimating joint probabilities of blocks of length @xmath87 .",
    "thus marton and shield s result limits the usable values of @xmath46 : @xmath88 using eq .",
    "[ eqn : marton - shields ] requires the entropy rate @xmath27 .",
    "the latter can either be upper bounded as the log of the alphabet size ( here , @xmath89 ) , or by some other , less pessimistic , estimator of the entropy rate ( such as the output of cssr with @xmath90 ) . use of an upper bound on @xmath27 results in a conservative maximum value for @xmath46 .",
    "for example , a 30 minute experiment with 1 msec time bins lets us use _ at least _",
    "@xmath91 by the most pessimistic estimate of @xmath92 ; the actual maximum value of @xmath46 may be much larger .",
    "we use @xmath93 in this paper but see no indication that this ca nt be extended further , if need be .      for real neural data",
    ", the number of causal states can be very large  hundreds or more .",
    "this creates an interpretation problem , if only because it is hard to fit such a csm on a single page for inspection .",
    "we thus developed a way to reduce the full csm while still accounting for most of the spike train s structure .",
    "our `` state culling '' technique found the least - probable states and selectively removed them , appropriately redirecting state transitions and reassigning state occupation probabilities . by keeping the most probable states , we focus on the ones which contribute the most to the spike train s structure and complexity . again , we used bic as our model selection criterion .",
    "first , we sorted the states by probability , finding the least probable state ( `` remove '' state ) with a single incoming edge from a state ( its `` ancestor '' ) with outgoing transitions to two different states , the remove state and a second , `` keep '' state .",
    "we redirected both of the ancestor s outgoing edges to the keep state .",
    "second , we reassigned the remove state s outgoing transitions to the keep state .",
    "if the outgoing transitions from the keep state were still deterministic ( at most a single 0 emitting edge and a single 1 emitting edge ) , we stopped .",
    "if the transitions were non - deterministic , we merged states reached by emitting 0s with each other ( likewise those reached by 1s ) , repeating this until termination .",
    "third , we checked that there existed a state sequence of the new model which could generate the observed spikes .",
    "if there was , we accepted the new csm .",
    "if not , we rejected the new csm and chose the next lowest probability state from the original csm to remove .",
    "this culling was iterated until removing any state made it impossible for the csm to generate the spike train . at each iteration",
    ", we calculated bic ( as described in the previous section ) , and ultimate chose the culled csm with the minimum bic .",
    "this gave a culled csm for each value of @xmath46 ; the final one we used was chosen after also minimizing bic over @xmath46 .",
    "the csms shown below in the results section paper result from this minimizing of bic over @xmath46 and state culling .      while we do model selection with bic , we also want to do model checking or adequacy - testing . for the most part , we do this by using the csm to bootstrap point - wise confidence bounds on the interspike interval ( isi ) distribution , and checking their coverage of the empirical isi distribution .",
    "because this distribution is not used by cssr in reconstructing the csm , it provides a check on the latter s ability to accurately describe the spike - train s statistics .",
    "specifically , we generated confidence bounds as follows .",
    "to simulate one spike train , we picked a random starting state according to the csm s inferred state - occupation probabilities , and then ran the csm forward for @xmath48 time - steps , @xmath48 being the length of the original spike train .",
    "this gives a binary time - series , where a `` 1 '' stands for a spike and a `` 0 '' for no - spike , and gave us a sample of inter - spike intervals from the csm .",
    "this in turn gave an `` empirical '' isi distribution .",
    "repeated over @xmath94 independent runs of the csm , and taking the @xmath95 and @xmath96 quantiles of the distributions at each isi length , gives 99% pointwise confidence bounds .",
    "( pointwise bounds are necessary because of the isi distribution often modulates rapidly with isi length . ) if the csm is correct , the empirical isi will , by chance , lie outside the bounds at @xmath97 of the isi lengths .",
    "if we split the data into training and validation sets , a csm reconstructed from the training set can be used to bootstrap isi confidence bounds , which can be compared to the isi distribution of the test set .",
    "we discuss this sort of of cross validation , as well as an additional test based on the time rescaling theorem , in appendix [ appendix : cross - validate ] .",
    "the _ algorithmic information content _",
    "@xmath98 of a sequence @xmath99 is the length of the shortest complete ( input - free ) computer program which will output @xmath99 exactly and then halt @xcite.the information needed to reproduce the spike train _ statistically _ rather than _ exactly _ ( eq .  [ eqn : c - defined ] ) . see @xcite for a detailed comparison of complexity measures . ] in general , @xmath98 is strictly uncomputable , but when @xmath99 is the realization of a stochastic process @xmath100 , the ensemble - averaged algorithmic information essentially coincides with the shannon entropy ( `` brudno s theorem '' ; see @xcite ) , reflecting the fact that both are maximized for completely random sequences @xcite .",
    "both the algorithmic information and the shannon entropy can be conveniently written in terms of a minimal sufficient statistic @xmath101 : @xmath102 } } & = & h[x_1^n ] + o(n ) \\nonumber\\\\ & = & h[q ] + h[x_1^n|q ] + o(n ) \\label{eq : algorithmicinfo}\\end{aligned}\\ ] ] the equality @xmath103 = h[q ] + h[x_1^n|q]$ ] holds because @xmath101 is a function of @xmath100 , so @xmath104 = 0 $ ] .",
    "the key to determining a spike train s expected algorithmic information is thus to find a minimal sufficient statistic . by construction ,",
    "causal state models provide exactly this ; a minimal sufficient statistic for @xmath99 is the state sequence @xmath105 @xcite .",
    "thus the ensemble - averaged algorithmic information content , dropping terms @xmath106 and smaller , is @xmath102 } } & = &   h[s_0^n ] + h[x_1^n|s_0^n ] \\nonumber\\\\ & = & h[s_0 ] + \\sum_{i=1}^n{h[s_i|s_{i-1 } ] + \\sum_{i=1}^n h[x_i|s_i , s_{i-1 } ] } \\label{jointhsplit}\\end{aligned}\\ ] ] going from the first to the second line uses the causal states markov property .",
    "assuming stationarity , eq .",
    "[ jointhsplit ] becomes @xmath107 } } & = & h[s_t ] + n \\left ( h[s_{t}|s_{t-1 } ] + h[x_t | s_t , s_{t-1 } ] \\right )    \\nonumber\\\\    & = & c + n\\left ( j + r \\right ) \\label{eqn : final - decomp - of - average - complexity}\\end{aligned}\\ ] ] this separates terms representing structure from those representing randomness .    the first term in eq .",
    "[ eqn : final - decomp - of - average - complexity ] is the _ complexity _ , @xmath17 , of the spike - generating process @xcite .",
    "@xmath108 = -{\\ensuremath{\\mathbb{e}\\left [ \\log{{\\ensuremath{\\mathbb{p}}}(s_t ) } \\right ] } } \\label{eqn : c - defined}\\ ] ] @xmath17 is the entropy of the causal states , quantifying the structure present in the observed spikes .",
    "this is distinct from the entropy of the spikes themselves , which quantifies not their structure but their randomness ( and is approximated by the other two terms ) .",
    "intuitively , @xmath17 is the ( time - averaged ) amount of information about the past of the system which is relevant to predicting its future .",
    "for example , consider again the iid 40 hz bernoulli process of figure [ fig:1 ] a. with @xmath9 , this has an entropy of @xmath109 bits / msec , but because it can be described by a single state , the complexity is zero .",
    "( that state emits either a `` 0 '' or a `` 1 '' , with respective probabilities @xmath110 and @xmath111 , but either way the state transitions back to itself . ) in contrast , adding a 5 ms refractory period to the process means six states are needed to describe the spike trains ( figure [ fig:1 ] b ) . the new structure of the refractory period is quantified by the higher complexity , @xmath112 bits .",
    "the second and third terms in eq .",
    "[ eqn : final - decomp - of - average - complexity ] both describe randomness , but of distinct kinds . the second term , the _ internal entropy rate _",
    "@xmath113 , quantifies the randomness in the state transitions ; it is the entropy of the next state given the current state .",
    "@xmath114 = -{\\ensuremath{\\mathbb{e}\\left [ \\log{{\\ensuremath{\\mathbb{p}}}(s_{t+1}|s_t ) } \\right]}}\\ ] ] this is the average number of bits per time - step needed to describe the sequence of states the process moved through ( beyond those given by @xmath17 ) .",
    "the last term in eq .",
    "[ eqn : final - decomp - of - average - complexity ] accounts for any residual randomness in the spikes which is not captured by the state transitions .",
    "@xmath115 = -{\\ensuremath{\\mathbb{e}\\left [ \\log{{\\ensuremath{\\mathbb{p}}}(x_{t+1}|s_t , s_{t+1 } ) } \\right]}}\\ ] ] for long trains , the entropy of the spikes , @xmath103 $ ] , is approximately the sum of these two terms , @xmath103 \\approx n\\left(j + r \\right)$ ] .",
    "computationally , @xmath17 represents the fixed generating structure of the process , which needs to be described once , at the beginning of the time series , and @xmath116 represents the growing list of details which pick out a particular time series from the ensemble which could be generated ; this needs , on average , @xmath117 extra bits per time - step .",
    "the `` sophistication '' of @xcite . )    consider again the 40 hz bernoulli process .",
    "as there is only one state , the process always stays in that state .",
    "thus the entropy of the next state @xmath118 . however , the state sequence yields no information about the emitted symbols ( the process is iid ) , so the residual randomness @xmath119 bits / msec  as it must be , since the total entropy rate is @xmath109 bits / msec . in contrast , the states of the 5 msec refractory process are informative about the process s future .",
    "the internal entropy rate @xmath120 bits / msec and the residual randomness @xmath121 .",
    "all of the randomness is in the state transitions , because they uniquely define the output spike train .",
    "the randomness in the state transition is confined to state @xmath11 , where the process `` decides '' whether it will stay in @xmath11 , emitting no spike , or emit a spike and go to @xmath14 .",
    "the decision needs , or gives , @xmath109 bits of information .",
    "the transitions from @xmath14 through @xmath18 and back to @xmath11 are fixed and contribute 0 bits , reducing the expected @xmath113 .",
    "the important point is that the structure present in the refractory period makes the spike train less random , lowering its entropy .",
    "averaged over time , the mean firing rate of the process is @xmath122 . were the spikes iid , the entropy rate would be @xmath123 bits / msec , but in fact @xmath124 bits / msec .",
    "this is because a minimal description of a long sequence @xmath125 , the generating process only needs to be described _ once _ ( @xmath17 ) , while the internal entropy rate and randomness need to be updated at each time step ( @xmath116 ) .",
    "simply put , a complex , structured spike train can be exactly described in fewer bits than one which is entirely random .",
    "the csm lets us calculate this reduction in algorithmic information , and quantify the structure by means of the complexity .",
    "the complexity and entropy are ensemble - averaged quantities . in the previous section",
    "the ensemble was the entire time series , and the averaged complexity and entropies were analogous to a mean firing rate .",
    "the time - varying complexity and entropies are also of interest , for example their variation after stimuli .",
    "a peri - stimulus time histogram ( psth ) shows how the firing probability varies with time ; the same idea works for the complexity and entropy .",
    "since the states form a markov chain , and any one spike train stays within a single ergodic component , we can invoke the ergodic theorem @xcite , and ( almost surely ) assert that @xmath126 for arbitrary integrable functions @xmath127 .    in the case of the mean firing rate ,",
    "the function to time - average is @xmath128 . for the time averaged - complexity ,",
    "internal entropy and residual randomness , the functions ( respectively @xmath129 , @xmath130 and @xmath131 ) are @xmath132 and time - varying entropy @xmath133 .    the psth averages over an ensemble of stimulus presentations , rather than time : @xmath134 with @xmath135 being number of stimulus presentations , and @xmath7 re - set to zero at each presentation .",
    "analogously , the `` psth '' of the complexity is @xmath136 for the entropies , replace @xmath129 with @xmath130 , @xmath131 or @xmath27 as appropriate .",
    "similar calculations can be made with any well - defined ensemble of reference times , not just stimulus presentations ; we will also calculate @xmath129 and the entropies as functions of the time since the latest spike .",
    "we can estimate the error these time - dependent quantities as the standard error of the mean as a function of time , @xmath137 where @xmath138 is the sample standard deviation in each time bin @xmath7 and @xmath135 is the number of trials .",
    "the probabilities appearing in the definitions of @xmath139 , @xmath140 , @xmath141 also have some estimation errors , either because of sampling noise or , more interestingly , because the ensemble is being distorted by outside influences .",
    "the latter creates a gap between their averages ( over time or stimuli ) and what the csm predicts for those averages . in the next section ,",
    "we explain how to use this to measure the influence of external drivers .      if we know that @xmath142 , the csm predicts that firing probability is @xmath143 . by means of the csm s recursive filtering property ( appendix [ app : filtering ] ) ,",
    "once a transient regime has passed , the state is always known with certainty .",
    "thereafter , the csm predicts what the firing probability should be at all times , incorporating the effects of the spike train s history .",
    "as we show in the next section , these predictions give good matches to the actual response function in simulations where the spiking probability depends only on the spike history .",
    "but real neurons spiking rates generally also depend on external processes , e.g. , stimuli .",
    "as currently formulated , the csm is ( or , rather , converges on ) the optimal predictor of the future of the process given its _",
    "own _ past .",
    "such an `` output only '' model does not represent the ( possible ) effects of other processes , and so ignores external covariates and stimuli . presently ,",
    "determining the precise form of spike trains responses to external forces is best left to parametric models .",
    "however , we can use output - only csms to learn something about the computation : the psth - calculated entropy rate @xmath144 quantifies the extent to which external processes drive the neuron .",
    "( the psth subscript is henceforce supressed . )",
    "suppose we know the true firing probability @xmath145 . at each time step , the csm predicts the firing probability @xmath146 .",
    "if @xmath147 , then the csm correctly describes the spiking and the psth entropy rate is @xmath148 } - ( 1-\\lambda_{csm}(t))\\log{\\left[1-\\lambda_{csm}\\right(t)]}\\ ] ] however , if @xmath149 , then the csm mis - describes the spiking , because it neglects the influence of external processes .",
    "simply put , the csm has no way of knowing when the stimuli happen .",
    "the psth entropy rate calculated using the csm becomes @xmath150 } - ( 1-\\lambda_{true}(t))\\log{[1-\\lambda_{csm}(t)]}\\ ] ] solving @xmath145 , @xmath151 } } { \\log{[1-\\lambda_{csm}(t)]}-\\log{[\\lambda_{csm}(t)]}}\\ ] ]    the discrepancy between @xmath146 and @xmath145 indicates how much of the apparent randomness in the entropy rate is actually due to external driving . the true psth entropy rate @xmath152 is @xmath153 } - ( 1-\\lambda_{true}(t))\\log{[1-\\lambda_{true}(t)]}\\ ] ] the difference between @xmath154 and @xmath152 quantifies , in bits , the driving by external forces as a function of the time since stimulus presentation .",
    "@xmath155 } + ( 1-\\lambda_{true}(t))\\log{\\left[\\frac{1-\\lambda_{true}(t)}{1-\\lambda_{csm}(t)}\\right]}\\end{aligned}\\ ] ] this _ stimulus - driven entropy _",
    "@xmath156 is the relative entropy or kullback - leibler divergence @xmath157 between the true distribution of symbol emissions and that predicted by the csm .",
    "information - theoretically , this relative entropy is the error in our prediction of the next state due to assuming the neuron is running autonomously when it s actually externally driven .",
    "since every state corresponds to a distinct distribution over future behavior , this is our error in predicting the future due to ignorance of the stimulus .",
    "we now present a few examples .",
    "( all of them use a time - step of 1 millisecond . )",
    "we begin with idealized model neurons to illustrate our technique .",
    "we recover csms for the model neurons using only the simulated spike trains as input to our algorithms . from the csm",
    "we calculate the complexity , entropies , and , when appropriate , stimulus - driven entropy ( kullback - leibler divergence between the true and csm predicted firing probabilities ) of each model neuron .",
    "we then analyze spikes recorded _ in vivo _ from a neuron in layer ii / iii of rat si ( barrel ) cortex .",
    "we use spike trains recorded both with and without external stimulation of the rat s whiskers .",
    "see @xcite for experimental details .",
    "we begin with a refractory , bursting model neuron , whose spiking rate depends only on the time since the last spike .",
    "the baseline rate is 40 hz .",
    "every spike is followed by a 2 msec `` hard '' refractory period , during which spikes never occur .",
    "the spiking rate then rebounds to twice its baseline , to which it slowly decays .",
    "( see dashed line in the first panel of figure [ fig:3 ] b. ) this history dependence mimics that of a bursting neuron , and is , intuitively , more complex than the simple refractory period of the model in figure [ fig:1 ] .",
    "figure [ fig:2 ] shows the 17-state csm reconstructed from a 200 second spike train ( at 1 msec resolution ) generated by this model .",
    "it has a complexity of @xmath158 bits ( higher than that of the model in figure [ fig:1 ] , as anticipated ) , an internal entropy rate of @xmath159 bits / msec and a residual randomness of @xmath121 bits / msec .",
    "the csm was obtained with @xmath160 ( selected by bic ) .",
    "figure [ fig:3 ] a shows how the 99% isi bounds bootstrapped from the csm enclose the empirical isi distribution , with the exception of one short segment .",
    "the csm is easily interpreted .",
    "state @xmath11 is the baseline state . when it emits a spike , the csm moves to state @xmath14 .",
    "there are then two deterministic transitions , to @xmath17 and then @xmath161 , which never emit spikes ; this is the hard 2 msec refractory period .",
    "once in @xmath161 it is possible to spike again , and if that happens , the transition is back to state @xmath14 .",
    "however , if no spike is emitted , the transition is to state @xmath162 .",
    "this is repeated , with varying firing probabilities , as states @xmath162 through @xmath101 are traversed .",
    "eventually , the process returns to @xmath11 and so to baseline .",
    "figure [ fig:3 ] b plots the firing rate , complexity , and internal entropies as functions of the time since the last spike _ conditional on no subsequent spike emission_. this lets us compare the firing rate predicted by the csm ( solid line squares ) to the specification of the model which generated the spike train ( dashed line ) and a psth calculated by triggering on the last spike ( solid line ) . except at 16 and 17",
    "msec post spike , the csm - predicted firing rate agrees with both the generating model and the psth .",
    "the discrepancy arises because the csm only discerns the structure in the data , and most of the isis are shorter than 16 msec .",
    "there is much closer agreement between the csm and the psth if firing rates are plotted as a function of time since a spike without conditioning on no subsequent spike emission ( not shown ) .",
    "the second and third panels of figure [ fig:3 ] plot the time - dependent complexity and entropies .",
    "the complexity is much higher after the emission of a spike than during baseline , because the states traversed ( b - q ) are less probable , and represent the additional structures of refractoriness and bursting .",
    "the time - dependent entropies ( third panel ) show that just after a spike , the refractory period imposes temporary determinism on the spike train , but burstiness increases the randomness before the dynamics return to the baseline state .",
    "figure [ fig:4 ] shows the csm for a periodically - stimulated model neuron .",
    "this csm was reconstructed from 200 seconds of spikes with a baseline firing rate of 40 hz ( @xmath9 ) .",
    "each second , the firing rate rose over the course of 5 msec to @xmath163 spikes / msec , falling slowly back to baseline over the next 50 msec .",
    "this mimics the periodic presentation of a strong external stimulus .",
    "( the exact inhomogeneous firing rate used was @xmath164 + 0.04 $ ] with @xmath7 in msec . see figure [ fig:5 ]",
    "b , first panel , dashed line . ) in this model , the firing rate does not directly depend on the spike train s history , but there is a sort of history dependence in the stimulus time - course , and this is what cssr discovers .",
    "bic selected @xmath165 , giving a 16 state csm with @xmath166 bits , @xmath167 bits / msec and @xmath168 bits / msec .",
    "the baseline is again state @xmath11 and if no spike is emitted then the process stays in @xmath11 .",
    "spikes are either spontaneous and random , or stimulus - driven . because the stimulus is external , it is not immediately clear which of these two causes produced a given spike . thus , if a spike is emitted , the csm traverses states @xmath14 through @xmath18 , deciding , so to speak , whether or not the spike is due to a stimulus . if two spikes happen within 3 msec of each other , then the csm decides that it is being stimulated and goes to one of states @xmath169 , @xmath170 or @xmath135 .",
    "states @xmath169 through @xmath171 represent the response to the stimulus .",
    "the csm moves between these states until no spike is emitted for 3 msec , when it returns to the baseline , @xmath11 .",
    "the isi distribution from the csm matches that from the model ( figure [ fig:5 ] a ) .",
    "however , because the stimulus does nt depend on spike train s history , the csm makes inaccurate predictions during stimulation .",
    "the first panel of figure [ fig:5 ] b plots the firing rate as a function of time since stimulus presentation , comparing the model ( dashed line ) and the psth ( solid line ) with the csm s prediction ( line with squares ) .",
    "the discrepancy between these is due to the csm having no way of knowing that an external stimulus has been applied until several spikes in a row have been emitted ( represented , as we just say , by states @xmath14@xmath18 ) . despite this",
    ", @xmath139 shows that something more complex than simple random firing is happening ( second panel of figure [ fig:5 ] b ) , as do @xmath140 and @xmath141 ( third panel ) .",
    "further , something is clearly wrong with the entropy rate , because it should be upper - bounded by @xmath92 bit / msec ( when @xmath172 ) . the fact that @xmath173 exceeds this bound indicates an external force , not fully captured by the csm , is at work .    as discussed in methods (  [ subsec : influence - of - externals ] ) , drive from the stimulus can be quantified with a relative entropy ( figure [ fig:5 ] c ) .",
    "stimuli are presented at @xmath174 msec , where @xmath175 bit .",
    "it is not until @xmath176 msec post - stimulus that @xmath177 and the csm once again correctly describes the internal entropy rate .",
    "thus , as expected , the stimulus strongly influences neuronal dynamics immediately after its presentation .",
    "the true internal entropy rate @xmath152 is slightly less than 1 bit / msec shortly after stimulation , when the true spiking rate has a maximum of @xmath178 .",
    "the fact that the csm gives an inaccurate value for @xmath113 actually lets us find the number of bits of information gain supplied by the stimulus , e.g. , @xmath179 bit immediately after the stimulus is presented .",
    "we reconstructed a csm from 90 seconds of spontaneous ( no vibrissa deflection ) spiking recorded from a layer ii / iii fsu barrel cortex neuron .",
    "cssr , using @xmath180 , discovered a csm with 315 states , a complexity of @xmath181 bits , and internal entropy rate of @xmath182 bits / msec .",
    "after state culling (  [ section : condense ] ) , the reduced csm , plotted in figure [ fig:6 ] , has 14 states , @xmath183 , @xmath184 bits / msec , and residual randomness of @xmath185 bits / msec .",
    "we focus on the reduced csm from this point onwards .",
    "this csm resembles that of the spontaneously - firing model neuron of  [ sec : model - spontaneous - neuron ] and fig .",
    "[ fig:2 ] .",
    "the complexity and entropies are lower than those of our model neuron because the mean spike rate is much lower , and so simple descriptions suffice most of the time .",
    "( barrel cortex neurons exhibit notoriously low spike rates , especially during anesthesia . )",
    "there is a baseline state @xmath11 which emits a spike with probability @xmath186 , i.e. , 10 hz .",
    "when a spike is emitted , the csm moves to state @xmath14 and then on through the chain of states @xmath17 through @xmath48 , return to @xmath11 if no spike is subsequently emitted .",
    "however , the csm can emit a second or even third spike after the first , and indeed this neuron displays spike doublets and triplets . in general , emitting a spike moves the csm to @xmath14 , with some exceptions that show the structure to be more intricate than the model neuron s .",
    "figure [ fig:7 ] a shows the csm s 99% confidence bounds almost completely enclosing the empirical isi distribution .",
    "the first panel of figure [ fig:7 ] b plots the history - dependent firing probability predicted by the csm as a function of the time since the latest spike , according to both the psth and the csm s prediction .",
    "they are highly similar in the first 13 msec post - spike , indicating that the csm gets the spiking statistics right in this epoch .",
    "the csm and psth the diverge after this , for two reasons .",
    "first , as with the model neuron , there are few isis of this length .",
    "most of the isis are either shorter , due to the nueron s burstiness , or much longer , due to the low baseline firing rate . secondly",
    ", 90 seconds is not very much data .",
    "we show in figure [ fig:10 ] that a csm reconstructed from a longer spike train does capture all of the structure .",
    "we present the results of this shorter spike train to emphasize that , as a non - parametric method , cssr only uncovers the statistical structure _ in the data _ , no more , no less .",
    "finally , the second and third panels of figure [ fig:6 ] b show , respectively , the complexity and entropies as functions of the time since the latest spike . as with the model of  [",
    "sec : model - spontaneous - neuron ] , the structure in the process occurs after spiking , during the refractory and bursting periods .",
    "this is when the complexity is largest , and also when the entropies vary most .",
    "we reconstructed csms from 335 seconds of spike trains taken from the same neuron used above , but recorded while it was being periodically stimulated by vibrissa deflection .",
    "bic selected @xmath187 , giving the 29-state csm shown in figure [ fig:8 ] .",
    "( before state culling , the original csm had 1916 states , @xmath188 and @xmath189 . )",
    "the reduced csm has a complexity of @xmath190 bits , an internal entropy rate of @xmath184 bits / msec , and a residual randomness of @xmath185 bits / msec .",
    "note that @xmath17 is higher when the neuron is being stimulated as opposed to when it is spontaneously firing , indicating more structure in the spike train .",
    "while at first the csm may seem to only represents history - dependent refractoriness and bursting , ignoring the external stimulus , this is not quite true .",
    "once again , there is a baseline state @xmath11 , and most of the other states ( @xmath14@xmath20 ) comprise a refractory / bursting chain , like this neuron has during spontaneous firing . however , the transition upon @xmath11 emitting a spike is not back to @xmath14 and then down the chain again , but to either state @xmath191 , and subsequently @xmath192 , or more often to state @xmath193 .",
    "these three states represent the structure induced by the external stimulus , as we saw with the model stimulated neuron of  [ sec : model - stimulated - neuron ] and figure [ fig:4 ] .",
    "( the state @xmath193 is comparable to the state @xmath135 of the model stimulated neuron : both loop back to themselves if they emit a spike . )",
    "three states are enough because , in this experiment , barrel cortex neurons spike extremely sparsely , @xmath194@xmath195 spikes per stimulus presentation .",
    "figure [ fig:9 ] a plots the isi distribution , nicely enclosed by the bootstrapped confidence bounds .",
    "figure [ fig:9 ] b shows the firing rate , complexity and entropies as functions of the time since stimulus presentation ( averaged over all presentations ) .",
    "these plots look much like those in figure [ fig:7 ] b. however , there is a clear indication that something more complex takes place after stimulation : the csm s firing - rate predictions are wrong .",
    "the stimulus - driven entropy @xmath156 turns out to be as large as @xmath196 bits within 515 msec post - stimulus .",
    "this agrees with the known @xmath197@xmath198 msec stimulus propagation time between vibrissae and barrel cortex @xcite .",
    "the reason that @xmath156 is so much smaller for the real neuron than the stimulated model neuron of  [ sec : model - stimulated - neuron ] is that the former s firing rate is much lower . although the firing rate post - stimulus can be almost twice as large as the csm s prediction , the actual rate is still low , @xmath199 spikes / msec .",
    "most of the time the neuron does not spike , even when stimulated , so on average , the stimulus provides little information per presentation . for completeness , figure [ fig:10 ] shows the spike probability , complexity and entropies as functions of the time since the latest spike . averaged over this ensemble , the csm s predictions are highly accurate .",
    "the goal of this paper was to present methods for determining the structural content of spike trains while making minimal _ a priori _ assumptions as to the form which that structure takes .",
    "we use the cssr algorithm to build minimal , optimally predictive hidden markov models ( csms ) from spike trains , schwartz s bayesian information criterion to find the optimal history length @xmath46 of the cssr algorithm , and bootstrapped confidence bounds on the isi distribution from the csm to check goodness - of - fit .",
    "we demonstrated how csms can estimate a spike train s complexity , thus quantifying its structure , and its mean algorithmic information content , quantifying the minimal computation necessary to generate the spike train .",
    "finally we showed how to quantify , in bits , the influence of external stimuli upon the spike - generating process .",
    "we applied these methods both to simulated spike trains , for which the resulting csms agreed with intuition , and to real spike trains recorded from a layer ii / ii rat barrel cortex neuron , demonstrating increased structure , as measured by the complexity , when the neuron was being stimulated .",
    "we are unaware of any other practical techniques for quantifying the complexity and computational structure of a spike train as we define them . intuitively , neither random ( poisson ) , nor highly ordered ( e.g. , strictly periodic , as in @xcite )",
    "spike trains should be thought of as complex since they do not possess structure requiring a sophisticated program to generate .",
    "instead , complexity lies between order and disorder @xcite , in the non - random variation of the spikes .",
    "higher complexity means a greater degree of organization in neural activity than would be implied by random spiking .",
    "it is the reconstruction of the csm through cssr which allows us to calculate the complexity .",
    "our definition of complexity stands in stark contrast to other complexity measures which assign high values to highly disordered systems .",
    "some of these , such as lempel ziv complexity @xcite and context free grammar complexity @xcite have been applied to spike trains",
    ". however , both of these are measures of the amount of information required to reproduce the spike train _ exactly _ , and take on very high values for completely random sequences .",
    "these `` complexity '' measures are therefore much more similar to total algorithmic information content and even to the entropy rate than to our sort of complexity .",
    "our measure of complexity is the entropy of the distribution of causal states .",
    "this has the desired property of being maximized for structured , rather than ordered or disordered systems , because the causal states are defined statistically , as equivalence classes of histories conditioned on future events .",
    "other researchers have also calculated complexity measures which are entropies of state distributions , but have defined their states differently .",
    "@xcite uses the observables ( symbol strings ) present in the spike train to define a k - th order markov process and calls each individual length k string which appears in the spike train a state .",
    "@xcite similarly use single suffix symbol strings to define the states of a markov process . in both cases ,",
    "iid bernoulli sequences could exhibit up to @xmath200 states ( in long enough sequences ) , and possess an extremely high `` complexity '' .",
    "however , all of these states make the same prediction for the future of the process . the minimal representation is a single causal state , a csm with a complexity of zero .",
    "it should be noted that there are also many works which model spike trains using hmms , but in which the hidden states represent _ macro_-states of the system ( awake / asleep , up / down , etc . ) , and spiking rates are modeled separately in each macro - state @xcite . although the graphical representation of such hmms may look like those of csms , the two kinds of states have very different meanings .",
    "finally , there are also state space methods which model the dynamical state of the system as a continuous hidden variable , the most well known of which is the linear gaussian model with kalman filtering .",
    "these have been extensively applied to neural encoding and decoding problems @xcite .",
    "interestingly , for a univariate gaussian arma model in state - space form , the kalman filter s one - step - ahead prediction and mean - squared prediction error are , jointly , minimal - sufficient for next - step prediction , and since they can be updated recursively they in fact constitute the minimal sufficient statistic , and hence the causal state in this special case .",
    "neurons are driven by their afferent synapses .",
    "although as discussed in appendix [ sec : csts - and - psrs ] , there is a parallel `` transducer '' formalism for generating csms which take external influences into account , this is not yet computationally implemented , and our current approach reconstructs csms only from the spike train .",
    "since the history of the neuron under study is typically connected with the history of the network in which it is located , this csm will , in general , reflect more than a neuron s internal biophysical properties .",
    "nonetheless , in both our model neurons and in the real barrel cortex neuron , states not interpretable as simple refractoriness or bursting appeared when a stimulus was present , proving we can detect stimulus - driven complexity .",
    "further , we showed that the csm can be used to determine the extent ( in bits ) to which a neuron is driven by external stimuli .",
    "the methods presented here complement more established modes of spike - train analysis , which have different goals .",
    "parametric methods , such as psths or maximum likelihood estimation @xcite generally focus on determining a neuron s firing rate ( mean , instantaneous or history - dependent ) , and on how known external covariates modulate that rate .",
    "they have the advantage of requiring less data than non - parametric methods such as cssr , but the disadvantage , for our purposes , of imposing the structure of the model at the outset . when the experimenter wants to know how a neuron encodes a particular aspect of a covariate , e.g. , how neurons in the sensory periphery or primary sensory cortices encode stimuli , parametric methods have proved highly illuminating . however , in many cases the identity or even existence of relevant external covariates is uncertain .",
    "for example , one could envision using csms to analyze recordings in pre - frontal cortex during different cognitive tasks , or to perhaps compare spiking structure during different attentional states . in both cases ,",
    "the relevant external covariates are not at all clear , but csms could still be used to quantify changes in computational structure , for single neurons or for groups of them . for neural populations one can envision generating distributions ( over the population ) of complexities and examining how these distributions change in different cortical macro - states",
    ". this would be entirely analagous to analyzing distributions of firing rates or tuning curves .",
    "in addition to calculations of the complexity , the whole array of mutual - information analyses can be applied to csms , but instead of calculating mutual information between the spikes and the covariates ( which could include other spike trains ) , one can calculate the mutual information between the covariates and the _ causal states_. the advantage is that the causal states represent the behavioral patterns of the spike - generating process , and so are closer to the actual state of the system than the spikes ( output observables ) are themselves . results on calculating the mutual information between the causal states of different neurons ( informational coherence ) in a large simulated network show that synchronous neuronal dynamics are more effectively revealed than when calculated directly from the spikes @xcite .    in closing ,",
    "our methods provide a way to understand structure in spike trains , and should be considered as complements to traditional analysis methods .",
    "we rigorously define structure , and show how to discover it from the data itself .",
    "our methods go beyond those which seek to describe the observed variation in the spiking rates by also describing the underlying computational process ( in the form of a csm ) needed to generate that variation .",
    "a csm can show not only that the spike rate has changed , but also show exactly _ how _ it has changed .",
    "[ [ acknowledgments ] ] acknowledgments + + + + + + + + + + + + + + +    the authors thank mark andermann and christopher moore for the use of their data .",
    "rh thanks emery brown , anna dreyer and christopher moore for valuable discussions .",
    "crs thanks anthony brockwell , dave feldman , chris genovese , rob kass and alessandro rinaldo for valuable discussions .",
    "a common difficulty with hidden markov models is that predictions can only be made from a knowledge of the state , which must itself be guessed at from the time series , since it is , after all , hidden .",
    "this creates the _ state estimation _ or _ filtering _ problem . under strong assumptions",
    "( linear gaussian stochastic dynamics , linearly observed through iid additive gaussian noise ) the kalman filter is an optimal yet tractable solution . for non - linear processes",
    ", however , optimal filtering essentially amounts to maintaining a posterior distribution over the states and updating it via bayes s rule @xcite .",
    "( this distribution is sometimes called the process s `` information state '' . )    one convenient and important feature of csms is that this whole machinery of filtering is unnecessary , because of their recursive - updating property .",
    "given the state at time @xmath7 , @xmath0 , and the observation at time @xmath201 , @xmath202 , the state at time @xmath201 is fixed , @xmath203 for some transition function @xmath43 . clearly , if the state is known with certainty at any time , it will remain known",
    ". however , the same recursive updating property also allows us to show that the state does become certain , i.e. , that after some finite ( but possibly random ) time @xmath204 , @xmath205 is either 0 or 1 for all states @xmath206 . for markov chains of order @xmath50 , clearly @xmath207",
    "; under more general circumstances @xmath208 goes to zero exponentially or faster .",
    "thus , after a transient period , the state is completely unambiguous",
    ". this will be useful to us in multiple places , including understanding the computational structure of the process and predicting the firing rate of the neuron .",
    "it also leads to considerable numerical simplifications , compared to approaches which demand conventional filtering .",
    "further , recursive filtering is easily applied to a new spike train , not merely the one from which the csm was reconstructed .",
    "this helps in cross - validating csms , as discussed in the next appendix .",
    "it is often desirable to cross - validate a statistical model by spliting one s data set in two , using one part ( generally the larger ) as a training set for the model and the other part to validate the model by some statistical test . in the case of csms",
    "it is particularly important to check the validity of the bic used to regularize the @xmath46 control - setting .",
    "one possible test is the isi bootstrapping of  [ subsect : isibootstrap ] . a second , somewhat stronger",
    ", goodness - of - fit test is based on the time rescaling theorem of @xcite .",
    "this test rescales the interspike intervals as a function of the integrated history - dependent spiking rate over the isi : @xmath209 where the @xmath210 are the spike times and @xmath211 is the history - dependent spiking rate from the csm .",
    "if the csm describes the data well , then rescaled isi s @xmath212 should follow a uniform distribution .",
    "this can be tested using either a kolmogorov smirnov test or by plotting the empirical cdf of the rescaled times against the cdf of the uniform distribution ( kolmogorov smirnov or `` ks '' plot ) @xcite .",
    "figure [ fig:11 ] gives cross - validation results for the rat barrel cortex neuron , during both spontaneous firing and periodic vibrissae deflection .",
    "90 seconds of spontaneously firing spikes were split into a 75 second training set and a 15 second validation set .",
    "the 335 seconds of stimulus - evoked firing were split into a 270 second training set and a 65 second validation set .",
    "panels a and b show the isi bootstrapping results for the spontaneous and stimulus evoked firing respectively .",
    "the dashed lines are 99% confidence bounds from a csm reconstructed from the training set and the solid line is the isi distribution of the validation set .",
    "the isi distribution largely falls within these bounds for both the spontaneous and stimulus evoked data .",
    "panels c - f display the time rescaling test .",
    "panels c and d show the time rescaling plots for the spontaneous and stimulus evoked training data respectively .",
    "the dashed lines are 95% confidence bounds .",
    "the spontaneous ks plot largely falls within the bounds .",
    "the stimulus - evoked does not , but this is expected because , as discussed , the csm does not completely capture the imposition of the external stimulus .",
    "( the jagged `` steps '' in both plots result from the 1 msec temporal discretization . )",
    "panels e and f show the time rescaling plots for , respectively , the spontaneous and stimulus evoked validation data .",
    "the fits here are somewhat worse . in the stimulated case , this is not surprising . in the spontaneous case",
    "the cause is likely non - stationarity in the data , a problem shared with other spike train analysis techniques , such as the generalized linear model approaches described in the next appendix .",
    "it should be emphasized that the point of reconstructing csms is not to obtain perfect fits to the data , but instead to estimate the structure inherent in the spike train , and the cross - validation results should be viewed in this light .",
    "mathematically , csms can be expanded to include the influence of external stimuli on the process , yielding causal state _",
    "transducers _ , which are optimal representations of the history - dependent mapping from inputs to outputs ( * ? ? ?",
    "* ch .  7 ) .",
    "such causal state transducers are a type of partially - observable markov decision process , closely related to predictive state representations ( psrs ) @xcite . in both formalisms ,",
    "the right notion of `` state '' is a statistic , a measurable function of the observable past of the process .",
    "causal states represent this through an equivalence relation on the space of observable histories . for psrs ,",
    "the representation is through `` tests '' , i.e. , a distinguished set input / output sequence pairs ; the idea is that states can be uniquely characterized by their probabilities of producing the output sequences conditional on the input sequences .",
    "an algorithm for reconstructing causal state transducers would begin by estimating probability distributions of future histories conditioned on both the history of the spikes _ and _ the history of an external covariate @xmath21 , e.g. @xmath213 , and otherwise be entirely parallel to cssr . this has not yet implemented .",
    "klinkner , k.  l. , shalizi , c.  r. , & camperi , m.  f. ( 2006 ) .",
    "measuring shared information and coordinated activity in neuronal networks . in weiss , y. , schlkopf , b. , & platt , j.  c. ( eds . ) , _ advances in neural information processing systems 18 ( nips 2005 ) _ , ( pp .",
    "667674 ) , cambridge , massachusetts . mit press .",
    "littman , m.  l. , sutton , r.  s. , & singh , s. ( 2002 ) .",
    "predictive representations of state . in dietterich , t.  g. , becker , s. , & ghahramani , z. ( eds . ) , _ advances in neural information processing systems 14 ( nips 2001 ) _ , ( pp . 15551561 ) . ,",
    "cambridge , massachusetts .",
    "mit press .",
    "rapp , p.  e. , zimmerman , i.  d. , vining , e.  p. , cohen , n.,albano , a.  m. , jimenez - montano , m.  a. ( 1994 ) .",
    "the algorithmic complexity of neural spike trains increases during focal seizures . , _ 14 _ , 47314739 .",
    "shalizi , c.  r. & klinkner , k.  l. ( 2004 ) .",
    "blind construction of optimal nonlinear recursive predictors for discrete sequences . in chickering ,",
    "m. & halpern , j.  y. ( eds . ) , _ uncertainty in artificial intelligence : proceedings of the twentieth conference ( uai 2004 ) _ , ( pp .",
    "504511 ) . , arlington , virginia .",
    "auai press .",
    "singh , s. , littman , m.  l. , jong , n.  k. , pardoe , d. , & stone , p. ( 2003 ) learning predictive state representations . in t.",
    "fawcett and n. mishra , editors , _ proceedings of the twentieth international conference on machine learning ( icml-2003 ) _ , ( pp .",
    "712 - 719 ) .",
    "aaai press .",
    "truccolo , w. , eden , u.  t. , fellow , m.  r. , donoghue , j.  p. , & brown , e.  n. ( 2005 ) . a point process framework for relating neural spiking activity to spiking history , neural ensemble and covariate effects . , _ 93 _ , 10741089 .     which always transitions back to itself , emitting a spike with probability @xmath9 per msec .",
    "( b ) csm for 40 hz bernoulli spiking process with a 5 msec refractory period imposed after each spike .",
    "state @xmath11 again spikes with probability @xmath9 . upon spiking the csm transitions through a deterministic chain of states @xmath14@xmath18 ( squares ) which represent the refractory period .",
    "the increased structure of the refractory period requires a more complex representation . ]    , @xmath159 , @xmath121 .",
    "state @xmath11 ( circle ) is the baseline 40 hz spiking state . upon emitting a spike",
    "the transition is to state @xmath14 .",
    "states @xmath14 and @xmath17 ( squares ) are `` hard '' refractory states from which no spike may be emitted .",
    "states @xmath161 through @xmath101 ( hexagons ) compromise a refractory / bursting chain from which if a spike is emitted the transition is back to state @xmath14 . upon exiting the chain",
    "the csm returns to the baseline state @xmath11 . ]        , @xmath167 , @xmath168 .",
    "state @xmath11 is the baseline state .",
    "states @xmath14 through @xmath18 ( octagons )",
    "are `` decision '' states in which the csm evaluates whether a spike indicates a stimulus or was spontaneous .",
    "two spikes within 3 msec cause the csm to transition to states @xmath169 through @xmath171 , which represent the structure imposed by the stimulus .",
    "if no spikes are emitted within 5 ( often fewer ) sequential msec , the csm goes back to the baseline state @xmath11 . ]      , @xmath214 , @xmath185 .",
    "state @xmath11 ( circle ) is baseline 10 hz spiking .",
    "states @xmath14 through @xmath48 comprise a refractory / bursting chain similar to , but with a somewhat more intricate structure than , that of the model neuron in figure [ fig:2 ] ]        , @xmath189 , @xmath215 .",
    "most of the states are devoted to refractory / bursting behavior , however states `` c1 '' , `` c2 '' and `` zz '' represent the structure imposed by the external stimulus . see text for discussion . ]"
  ],
  "abstract_text": [
    "<S> neurons perform computations , and convey the results of those computations through the statistical structure of their output spike trains . here </S>",
    "<S> we present a practical method , grounded in the information - theoretic analysis of prediction , for inferring a minimal representation of that structure and for characterizing its complexity . </S>",
    "<S> starting from spike trains , our approach finds their _ causal state models _ ( csms ) , the minimal hidden markov models or stochastic automata capable of generating statistically - identical time series . </S>",
    "<S> we then use these csms to objectively quantify both the generalizable structure and the idiosyncratic randomness of the spike train . </S>",
    "<S> specifically , we show that the expected algorithmic information content ( the information needed to describe the spike train exactly ) can be split into three parts describing ( 1 ) the time - invariant structure ( complexity ) of the minimal spike - generating process , which describes the spike train _ statistically _ , ( 2 ) the randomness ( internal entropy rate ) of the minimal spike - generating process , and ( 3 ) a residual pure noise term not described by the minimal spike generating process . </S>",
    "<S> we use csms to approximate each of these quantities . </S>",
    "<S> the csms are inferred non - parametrically from the data , making only mild regularity assumptions , via the causal state splitting reconstruction ( cssr ) algorithm . </S>",
    "<S> the methods presented here complement more traditional spike train analyses by describing not only spiking probability , and spike train entropy , but also the complexity of a spike train s structure . </S>",
    "<S> we demonstrate our approach using both simulated spike trains and experimental data recorded in rat barrel cortex during vibrissa stimulation .    </S>",
    "<S> [ multiblock footnote omitted ]    [ multiblock footnote omitted ]    [ multiblock footnote omitted ] </S>"
  ]
}