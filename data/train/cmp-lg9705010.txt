{
  "article_text": [
    "statistical approaches to disambiguation offer the advantage of making the most likely decision on the basis of available evidence . for this purpose",
    "a large number of probabilities has to be estimated from a training corpus .",
    "however , many possible conditioning events are not present in the training data , yielding zero maximum likelihood ( ml ) estimates .",
    "this motivates the need for _ smoothing _ methods , which re - estimate the probabilities of low - count events from more reliable estimates .",
    "inductive generalization from observed to new data lies at the heart of machine - learning approaches to disambiguation . in memory - based learning ( mbl )",
    "induction is based on the use of similarity @xcite . in this paper",
    "we describe how the use of similarity between patterns embodies a solution to the sparse data problem , how it relates to backed - off smoothing methods and what advantages it offers when combining diverse and rich information sources .",
    "we illustrate the analysis by applying mbl to two tasks where combination of information sources promises to bring improved performance : pp - attachment disambiguation and part of speech tagging .",
    "the basic idea in memory - based language processing is that processing and learning are fundamentally interwoven .",
    "each language experience leaves a memory trace which can be used to guide later processing .",
    "when a new instance of a task is processed , a set of relevant instances is selected from memory , and the output is produced by analogy to that set .",
    "the techniques that are used are variants and extensions of the classic @xmath0-nearest neighbor ( @xmath0-nn ) classifier algorithm .",
    "the instances of a task are stored in a table as patterns of feature - value pairs , together with the associated `` correct '' output . when a new pattern is processed , the @xmath0 nearest neighbors of the pattern are retrieved from memory using some similarity metric .",
    "the output is then determined by extrapolation from the @xmath0 nearest neighbors , i.e. the output is chosen that has the highest relative frequency among the nearest neighbors .    note that no abstractions , such as grammatical rules , stochastic automata , or decision trees are extracted from the examples . rule - like behavior results from the linguistic regularities that are present in the patterns of usage in memory in combination with the use of an appropriate similarity metric .",
    "it is our experience that even limited forms of abstraction can harm performance on linguistic tasks , which often contain many subregularities and exceptions  @xcite .",
    "the most basic metric for patterns with symbolic features is the * overlap metric * given in equations  [ distance ] and  [ overlap ] ; where @xmath1 is the distance between patterns @xmath2 and @xmath3 , represented by @xmath4 features , @xmath5 is a weight for feature @xmath6 , and @xmath7 is the distance per feature .",
    "the @xmath0-nn algorithm with this metric , and equal weighting for all features is called ib1 @xcite .",
    "usually @xmath0 is set to 1 .",
    "@xmath8    where : + @xmath9    this metric simply counts the number of ( mis)matching feature values in both patterns .",
    "if we do not have information about the importance of features , this is a reasonable choice .",
    "but if we do have some information about feature relevance one possibility would be to add linguistic bias to weight or select different features  @xcite .",
    "an alternative  more empiricist  approach , is to look at the behavior of features in the set of examples used for training .",
    "we can compute statistics about the relevance of features by looking at which features are good predictors of the class labels .",
    "information theory gives us a useful tool for measuring feature relevance in this way  @xcite .",
    "+ * information gain * ( ig ) weighting looks at each feature in isolation , and measures how much information it contributes to our knowledge of the correct class label .",
    "the information gain of feature @xmath10 is measured by computing the difference in uncertainty ( i.e. entropy ) between the situations without and with knowledge of the value of that feature ( equation  [ iggain ] ) .",
    "@xmath11    @xmath12    where @xmath13 is the set of class labels , @xmath14 is the set of values for feature @xmath10 , and @xmath15 is the entropy of the class labels .",
    "the probabilities are estimated from relative frequencies in the training set .",
    "the normalizing factor @xmath16 ( split info ) is included to avoid a bias in favor of features with more values .",
    "it represents the amount of information needed to represent all values of the feature ( equation  [ splitinfo ] ) .",
    "the resulting ig values can then be used as weights in equation  [ distance ] . the @xmath0-nn algorithm with this metric",
    "is called ib1-ig  @xcite .",
    "+ the possibility of automatically determining the relevance of features implies that many different and possibly irrelevant features can be added to the feature set .",
    "this is a very convenient methodology if theory does not constrain the choice enough beforehand , or if we wish to measure the importance of various information sources experimentally .",
    "finally , it should be mentioned that mb - classifiers , despite their description as table - lookup algorithms here , can be implemented to work fast , using e.g.  tree - based indexing into the case - base  @xcite .",
    "the commonly used method for probabilistic classification ( the bayesian classifier ) chooses a class for a pattern @xmath2 by picking the class that has the maximum conditional probability @xmath17 .",
    "this probability is estimated from the data set by looking at the relative joint frequency of occurrence of the classes and pattern @xmath2 . if pattern @xmath2 is described by a number of feature - values @xmath18 , we can write the conditional probability as @xmath19 . if a particular pattern @xmath20 is not literally present among the examples , all classes have zero ml probability estimates .",
    "smoothing methods are needed to avoid zeroes on events that could occur in the test material .",
    "there are two main approaches to smoothing : count re - estimation smoothing such as the add - one or good - turing methods  @xcite , and back - off type methods @xcite .",
    "we will focus here on a comparison with back - off type methods , because an experimental comparison in   shows the superiority of back - off based methods over count re - estimation smoothing methods . with the back - off method the probabilities of complex conditioning events",
    "are approximated by ( a linear interpolation of ) the probabilities of more general events :    @xmath21    where @xmath22 stands for the smoothed estimate , @xmath23 for the relative frequency estimate , @xmath24 are interpolation weights , @xmath25 , and @xmath26 for all @xmath6 , where @xmath27 is a ( partial ) ordering from most specific to most general feature - sets can be read as @xmath2 is more specific than @xmath28 . ]",
    "( e.g the probabilities of trigrams ( @xmath2 ) can be approximated by bigrams ( @xmath28 ) and unigrams ( @xmath29 ) ) .",
    "the weights of the linear interpolation are estimated by maximizing the probability of held - out data ( deleted interpolation ) with the forward - backward algorithm .",
    "an alternative method to determine the interpolation weights without iterative training on held - out data is given in samuelsson ( 1996 ) .",
    "we can assume for simplicity s sake that the @xmath30 do not depend on the value of @xmath31 , but only on @xmath6 .",
    "in this case , if @xmath32 is the number of features , there are @xmath33 more general terms , and we need to estimate @xmath34 s for all of these . in most applications",
    "the interpolation method is used for tasks with clear orderings of feature - sets ( e.g. n - gram language modeling ) so that many of the @xmath33 terms can be omitted beforehand .",
    "more recently , the integration of information sources , and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods  @xcite .",
    "for such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand .    if we let the @xmath30 depend on the value of @xmath31 , the number of parameters explodes even faster .",
    "a practical solution for this is to make a smaller number of _ buckets _ for the @xmath31 , e.g. by clustering ( see e.g. ) .",
    "note that linear interpolation ( equation  [ linear_interpolation ] ) actually performs two functions . in the first place ,",
    "if the most specific terms have non - zero frequency , it still interpolates them with the more general terms .",
    "because the more general terms should never overrule the more specific ones , the @xmath30 for the more general terms should be quite small .",
    "therefore the interpolation effect is usually small or negligible .",
    "the second function is the pure back - off function : if the more specific terms have zero frequency , the probabilities of the more general terms are used instead .",
    "only if terms are of a similar specificity , the @xmath24 s truly serve to weight relevance of the interpolation terms .",
    "if we isolate the pure back - off function of the interpolation equation we get an algorithm similar to the one used in  .",
    "it is given in a schematic form in table  [ back - off - cb ] .",
    "each step consists of a back - off to a lower level of specificity .",
    "there are as many steps as features , and there are a total of @xmath35 terms , divided over all the steps . because all features are considered of equal importance , we call this the _ naive back - off _ algorithm .    if @xmath36 : + @xmath37 + else if @xmath38 : + @xmath39 + else if  : + @xmath40 + else if @xmath41 : + @xmath42 +    usually , not all features @xmath43 are equally important , so that not all back - off terms are equally relevant for the re - estimation . hence , the problem of fitting the @xmath30 parameters is replaced by a term selection task . to optimize the term selection ,",
    "an evaluation of the up to @xmath35 terms on held - out data is still necessary . in summary",
    ", the back - off method does not provide a principled and practical domain - independent method to adapt to the structure of a particular domain by determining a suitable ordering @xmath27 between events . in the next section",
    ", we will argue that a formal operationalization of similarity between events , as provided by mbl , can be used for this purpose .",
    "in mbl the similarity metric and feature weighting scheme automatically determine the implicit back - off ordering using a domain independent heuristic , with only a few parameters , in which there is no need for held - out data .",
    "if we classify pattern @xmath2 by looking at its nearest neighbors , we are in fact estimating the probability @xmath17 , by looking at the relative frequency of the class in the set defined by @xmath44 , where @xmath44 is a function from @xmath2 to the set of most similar patterns present in the training data . although the name `` @xmath0-nearest neighbor '' might mislead us by suggesting that classification is based on exactly @xmath0 training patterns , the @xmath44 function given by the overlap metric groups varying numbers of patterns into _ buckets _ of equal similarity",
    "a bucket is defined by a particular number of mismatches with respect to pattern @xmath2 .",
    "each bucket can further be decomposed into a number of _ schemata",
    "_ characterized by the position of a wildcard ( i.e. a mismatch ) .",
    "thus @xmath44 specifies a @xmath27 ordering in a collins & brooks style back - off sequence , where each bucket is a step in the sequence , and each schema is a term in the estimation formula at that step .",
    "in fact , the unweighted overlap metric specifies exactly the same ordering as the naive back - off algorithm ( table  [ back - off - cb ] ) . in figure  [ buckets ] this is shown for a four - featured pattern .",
    "the most specific schema is the schema with zero mismatches , which corresponds to the retrieval of an identical pattern from memory , the most general schema ( not shown in the figure ) has a mismatch on every feature , which corresponds to the entire memory being best neighbor .",
    "if information gain weights are used in combination with the overlap metric , individual schemata instead of buckets become the steps of the back - off sequence .",
    "the @xmath27 ordering becomes slightly more complicated now , as it depends on the number of wildcards _ and _ on the magnitude of the weights attached to those wildcards .",
    "let @xmath45 be the most specific ( zero mismatches ) schema .",
    "we can then define the @xmath27 ordering between schemata in the following equation , where @xmath1 is the distance as defined in equation  [ distance ] .",
    "@xmath46    note that this approach represents a type of implicit parallelism .",
    "the importance of the @xmath47 back - off terms is specified using only @xmath32 parameters  the ig weights , where @xmath32 is the number of features .",
    "this advantage is not restricted to the use of ig weights ; many other weighting schemes exist in the machine learning literature ( see   for an overview ) .",
    "+ using the ig weights causes the algorithm to rely on the most specific schema only .",
    "although in most applications this leads to a higher accuracy , because it rejects schemata which do not match the most important features , sometimes this constraint needs to be weakened .",
    "this is desirable when : ( i ) there are a number of schemata which are almost equally relevant , ( ii ) the top ranked schema selects too few cases to make a reliable estimate , or ( iii ) the chance that the few items instantiating the schema are mislabeled in the training material is high . in such cases we wish to include some of the lower - ranked schemata .",
    "for case ( i ) this can be done by discretizing the ig weights into bins , so that minor differences will lose their significance , in effect merging some schemata back into buckets . for ( ii ) and",
    "( iii ) , and for continuous metrics  @xcite which extrapolate from exactly @xmath0 neighbors , it might be necessary to choose a @xmath0 parameter larger than 1 .",
    "this introduces one additional parameter , which has to be tuned on held - out data .",
    "we can then use the distance between a pattern and a schema to weight its vote in the nearest neighbor extrapolation .",
    "this results in a back - off sequence in which the terms at each step in the sequence are weighted with respect to each other , but without the introduction of any additional weighting parameters .",
    "a weighted voting function that was found to work well is due to  : the nearest neighbor schema receives a weight of 1.0 , the furthest schema a weight of 0.0 , and the other neighbors are scaled linearly to the line between these two points .",
    "in this section we describe experiments with mbl on a data - set of prepositional phrase ( pp ) attachment disambiguation cases .",
    "the problem in this data - set is to disambiguate whether a pp attaches to the verb ( as in _ i ate pizza with a fork _ ) or to the noun ( as in _ i ate pizza with cheese _ ) .",
    "this is a difficult and important problem , because the semantic knowledge needed to solve the problem is very difficult to model , and the ambiguity can lead to a very large number of interpretations for sentences .",
    "we used a data - set extracted from the penn treebank wsj corpus by  .",
    "it consists of sentences containing the possibly ambiguous sequence _",
    "verb noun - phrase pp_. cases were constructed from these sentences by recording the features : verb , head noun of the first noun phrase , preposition , and head noun of the noun phrase contained in the pp .",
    "the cases were labeled with the attachment decision as made by the parse annotator of the corpus .",
    "so , for the two example sentences given above we would get the feature vectors ate , pizza , with , fork , v . and ate , pizza , with , cheese , n .",
    "the data - set contains 20801 training cases and 3097 separate test cases , and was also used in  .",
    "+ the ig weights for the four features ( v , n , p , n ) were respectively 0.03 , 0.03 , 0.10 , 0.03 .",
    "this identifies the preposition as the most important feature : its weight is higher than the sum of the other three weights .",
    "the composition of the back - off sequence following from this can be seen in the lower part of figure  [ buckets ] .",
    "the grey - colored schemata were effectively left out , because they include a mismatch on the preposition .",
    ".accuracy on the pp - attachment test set .",
    "[ cols=\"<,<\",options=\"header \" , ]     the results show that for naive back - off ( and ib1 ) the addition of more , possibly irrelevant , features quickly becomes detrimental ( decrease from 88.5 to 85.9 ) , even if these added features do make a generalisation performance increase possible ( witness the increase with ib1-ig from 88.3 to 89.8 ) .",
    "notice that we did not actually compute the @xmath48 terms of naive back - off in the pdddaaasss condition , as ib1 is guaranteed to provide statistically the same results .",
    "contrary to naive back - off and ib1 , memory - based learning with feature weighting ( ib1-ig ) manages to integrate diverse information sources by differentially assigning relevance to the different features .",
    "since noisy features will receive low ig weights , this also implies that it is much more noise - tolerant .",
    "we have analysed the relationship between back - off smoothing and memory - based learning and established a close correspondence between these two frameworks which were hitherto mostly seen as unrelated .",
    "an exception is the use of similarity for alleviating the sparse data problem in language modeling  @xcite .",
    "however , these works differ in their focus from our analysis in that the emphasis is put on similarity between _ values _ of a feature ( e.g. words ) , instead of similarity between patterns that are a ( possibly complex ) combination of many features .",
    "the comparison of mbl and back - off shows that the two approaches perform smoothing in a very similar way , i.e. by using estimates from more general patterns if specific patterns are absent in the training data .",
    "the analysis shows that mbl and back - off use exactly the same type of data and counts , and this implies that mbl can safely be incorporated into a system that is explicitly probabilistic . since the underlying @xmath0-nn classifier is a method that does not necessitate any of the common independence or distribution assumptions , this promises to be a fruitful approach .",
    "a serious advantage of the described approach , is that in mbl the back - off sequence is specified by the used similarity metric , without manual intervention or the estimation of smoothing parameters on held - out data , and requires only one parameter for each feature instead of an exponential number of parameters . with a feature - weighting metric such as information gain",
    ", mbl is particularly at an advantage for nlp tasks where conditioning events are complex , where they consist of the fusion of different information sources , or when the data is noisy .",
    "this was illustrated by the experiments on pp - attachment and pos - tagging data - sets .",
    "this research was done in the context of the `` induction of linguistic knowledge '' research programme , partially supported by the foundation for language speech and logic ( tsl ) , which is funded by the netherlands organization for scientific research ( nwo ) .",
    "we would like to thank peter berck and anders green for their help with software for the experiments .",
    "claire cardie .",
    "1996 . automatic feature set selection for case - based learning of linguistic knowledge . in _ proc . of the conference on empirical methods in natural language processing _ , may 17 - 18 , 1996 , university of pennsylvania .",
    "walter daelemans and antal van den bosch . 1992 .",
    "generalisation performance of backpropagation learning on a syllabification task . in m.",
    "f.  j.  drossaers & a.  nijholt ( eds . ) , _ twlt3 : connectionism and natural language processing_. enschede : twente university .",
    ". 2737 .",
    "walter daelemans .",
    "memory - based lexical acquisition and processing . in p.  steffens ( ed . ) , _ machine translation and the lexicon_. springer lecture notes in artificial intelligence , no . 898 .",
    "berlin : springer verlag .",
    ". 8598    walter daelemans . 1996 .",
    "abstraction considered harmful : lazy learning of language processing . in j.  van den herik and t.  weijters ( eds . ) , _ benelearn-96 .",
    "proceedings of the 6th belgian - dutch conference on machine learning_. matriks : maastricht , the netherlands , pp . 3 - 12 .",
    "walter daelemans , jakub zavrel , peter berck , and steven gillis .",
    "mbt : a memory - based part of speech tagger generator . in e.  ejerhed and i.  dagan ( eds . ) _ proc . of the fourth workshop on very large corpora _ , copenhagen : acl sigdat , pp .",
    "14 - 27 .",
    "walter daelemans , antal van den bosch , and ton weijters .",
    "igtree : using trees for compression and classification in lazy learning algorithms . in d.",
    "aha ( ed . ) _ artificial intelligence review _ , special issue on lazy learning , vol . 11(1 - 5 ) .",
    "ido dagan , fernando pereira , and lillian lee .",
    "similarity - based estimation of word cooccurrence probabilities . in _ proc .",
    "of the 32nd annual meeting of the acl _ , june 1994 , las cruces , new mexico , acl .",
    "hwee tou ng and hian beng lee .",
    "1996 . integrating multiple knowledge sources to disambiguate word sense : an exemplar - based approach . in _ proc . of the 34th annual meeting of the acl _ ,",
    "june 1996 , santa cruz , ca , acl .",
    "d.  wettschereck , d.  w.  aha , and t.  mohri .",
    "1997 . a review and comparative evaluation of feature - weighting methods for lazy learning algorithms .",
    "in d.  aha ( ed . ) _ artificial intelligence review _ , special issue on lazy learning , vol . 11(1 - 5 ) .",
    "jakub zavrel and jorn b. veenstra .",
    "1995 . the language environment and syntactic word - class acquisition . in c.koster and f.wijnen",
    "_ proc . of the groningen assembly on language acquisition ( gala95)_. center for language and cognition , groningen , pp .",
    "365 - 374 ."
  ],
  "abstract_text": [
    "<S> this paper analyses the relation between the use of similarity in memory - based learning and the notion of backed - off smoothing in statistical language modeling . </S>",
    "<S> we show that the two approaches are closely related , and we argue that feature weighting methods in the memory - based paradigm can offer the advantage of automatically specifying a suitable domain - specific hierarchy between most specific and most general conditioning information without the need for a large number of parameters . </S>",
    "<S> we report two applications of this approach : pp - attachment and pos - tagging . </S>",
    "<S> our method achieves state - of - the - art performance in both domains , and allows the easy integration of diverse information sources , such as rich lexical representations . </S>"
  ]
}