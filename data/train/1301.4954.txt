{
  "article_text": [
    "functional regression , in particular functional linear regression , has been studied extensively .",
    "recent synopses include @xcite , @xcite , and @xcite .",
    "let @xmath0 be a random process defined on @xmath1 $ ] and @xmath2 be the univariate response variable .",
    "typically , @xmath3 is restricted to a compact interval , so the assumption that @xmath4 $ ] causes no loss of generality .",
    "suppose we observe @xmath5 i.i.d .",
    "copies of @xmath6 , @xmath7 , @xmath8 .",
    "the functional linear regression model assumes that @xmath9 where @xmath10 is the coefficient constant , @xmath11\\rightarrow \\mathbb r$ ] is the slope function , and the @xmath12 are i.i.d .",
    "random errors with @xmath13 and @xmath14 .",
    "one of the popular methods for estimating functional linear models is based on functional principal component analysis ( see , e.g. , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) .",
    "in addition , methods of regularization have also been applied to the functional linear model ( see , e.g. , @xcite , @xcite , @xcite ) .    due to the limitation of the inherent linearity of ( [ equ : flr ] ) , @xcite extended this model to nonparametric functional models and @xcite discussed functional models that are additive in the functional principal component scores of the predictor functions .",
    "recently , @xcite proposed a new model called a functional generalized additive model ( fgam ) .",
    "the same model was studied by @xcite who called it the continuously additive model .",
    "we will study the special case of the fgam with the identify link and continuous errors so that @xmath15 where @xmath16 ^ 2\\rightarrow \\mathbb r$ ] is a bivariate function . because @xmath17 is nonlinear , @xmath18 can be replaced by @xmath19 for a transformation @xmath20 . since @xmath20 can be strictly increasing function from the entire real line to @xmath21 $ ] , assuming that @xmath22 $ ] also causes no loss of generality .",
    "( in @xcite , @xmath23 is allowed to depend on @xmath3 and is an estimate of the cdf of @xmath18 , but we will not pursue this refinement here . )",
    "model will be called the additive functional model and contains ( [ equ : flr ] ) as a special case with @xmath24 .",
    "the additive functional model offers increased flexibility compared to ( [ equ : flr ] ) , while still facilitating interpretation and estimation . in @xcite ,",
    "computational issues of this model were studied and @xmath17 was estimated using tensor - product b - splines with roughness penalties . in @xcite ,",
    "a piecewise constant function was fit to @xmath17 and the asymptotic properties , e.g. , consistency and asymptotic normality , of predictions based on @xmath25 were studied .    in this paper , we study the minimax prediction .",
    "the unknown bivariate function @xmath17 is assumed to reside in a rkhs @xmath26 with a reproducing kernel @xmath27 ^ 2\\times [ 0,1]^2\\rightarrow \\mathbb r$ ] .",
    "the goal of prediction is to recover the functional @xmath28 : @xmath29 based on the training sample @xmath30 , @xmath31 .",
    "let @xmath32 be an estimate of @xmath17 from the training data .",
    "then its accuracy can be naturally measured by the excess risk : @xmath33 ^ 2 - \\mathbb e^*\\big[y_{n+1 } - \\int_0 ^ 1 f_0\\big(t , x_{n+1}(t)\\big)dt\\big]^2 \\\\ = &    \\mathbb e^*\\big\\ { \\int_0 ^ 1 \\big [ \\widehat f_n(t , x_{n+1}(t ) ) - f_0(t , x_{n+1}(t ) )    \\big]dt \\big\\}^2,\\end{aligned}\\ ] ] where @xmath34 possesses the same distribution with @xmath30 and is independent with @xmath30 , @xmath31 , and @xmath35 represents taking expectation over @xmath34 only .",
    "it is interesting to study the rate of convergence of @xmath36 as the sample size @xmath5 increases , which reflects the difficulty of the prediction problem .",
    "a closed related but different problem is estimation the bivariate function @xmath17 .    the optimal rate of convergence for the prediction problem is established in this paper .",
    "the spectral theorem admits that there exist a set of orthonormalized eigenfunctions @xmath37 and a sequence of eigenvalues @xmath38 such that @xmath39 it is shown that under model ( [ equ : funadd ] ) , the difficulty of the prediction problem as measured by the minimax rate of convergence depends on the decay rate of the eigenvalues of the kernel @xmath40 ^ 2 \\times [ 0 , 1]^2 \\rightarrow \\mathbb r$ ] , and @xmath41 where @xmath42 .",
    "a minimax lower bound is first derived for the prediction problem .",
    "then a roughness - regularized predictor is introduced and is shown to attain the rate of convergence given in the lower bound .",
    "therefore , this estimator is rate - optimal .",
    "the paper is organized as follows .",
    "section 2 establishes the minimax lower bound for the rate of convergence of the excess risk .",
    "section 3 develops a predictor using a roughness regularization method and shows this predictor is rate - optimal .",
    "section 4 conducts a monte carlo study to validate the method and we also illustrate the merit of the method by using two real data examples . some discussions are provided in section 5 .",
    "the paper ends with proofs in section 6 .",
    "in this section , we establish the minimax lower bound for the rate of convergence of the excess risk .",
    "assume that the unknown @xmath17 resides in a reproducing kernel hilbert space @xmath26 with a reproducing kernel @xmath43 .",
    "it is well - known that @xmath26 is a linear functional space endowed with an inner product @xmath44 such that @xmath45 there is a one - to - one relationship between @xmath43 and @xmath26 .",
    "it follows from ( [ equ : c ] ) that @xmath46 where @xmath47 is the joint density function of @xmath48 evaluated at @xmath49 .",
    "similarly , @xmath50 admits the spectral decomposition , @xmath51 where the @xmath52 are the positive eigenvalues with a decreasing order and the @xmath53 are the corresponding orthonormal eigenfunctions .",
    "we assume @xmath54 for some constant @xmath55 , where for two sequences @xmath56 , @xmath57 means that @xmath58 is bounded away from zero and infinity as @xmath59 .",
    "[ thm : low ] suppose that the eigenvalues @xmath60 of the kernel @xmath50 in ( [ equ : c ] ) satisfy @xmath54 for some constant @xmath55 , then the excess prediction risk satisfies @xmath61 where the infimum is taken over all possible predictors @xmath62 based on @xmath63 .",
    "it is interesting to compare theorem [ thm : low ] with some of the known results when functional linear regression is the true model .",
    "if the bivariate function @xmath64 is restricted to the specific form @xmath65 , where @xmath66 belongs to a reproducing kernel hilbert space @xmath67 with the reproducing kernel @xmath68\\times [ 0 , 1]\\rightarrow \\mathbb r$ ] , then we have a functional linear regression model .",
    "assume @xmath69 where the @xmath70 are the eigenvalue and eigenfunction pairs for @xmath71 .",
    "it is not hard to see that @xmath72 where @xmath73 therefore , @xmath74 where @xmath75 is the covariance function of @xmath76 , so the eigenvalues of @xmath50 have the same decay rate as the eigenvalues of @xmath77 .",
    "this special setting coincides with those considered in @xcite and @xcite .",
    "results similar to ours have been established in these papers for this special setting .",
    "in this section , we will develop a predictor using a roughness regularization method and establish that this predictor achieves the optimal rate established in theorem [ thm : low ] .",
    "we define the estimate @xmath78 of @xmath17 as the minimizer of the functional @xmath79 where @xmath80 is the tuning parameter and @xmath81 is a squared semi - norm on @xmath26 .",
    "the first term measures the closeness of the fit to the data , the second term controls the smoothness of the estimate , and the tuning parameter @xmath80 adjusts the trade - off between these two .",
    "the estimate @xmath78 can be computed explicitly over the infinitely dimensional function space @xmath26 .",
    "this observation is important to both numerical implementation of the procedure and our asymptotic analysis .",
    "let @xmath82 be the null space of @xmath83 , i.e. , @xmath84 .",
    "assume that @xmath85 be the orthonormal basis of @xmath82 with @xmath86 .",
    "let @xmath87 be its orthogonal complement in @xmath88 such that @xmath89 .",
    "[ thm : sol ] the minimizer of ( [ equ : obj ] ) over @xmath26 can be represented by @xmath90 for some @xmath91 and @xmath92 .",
    "denote by @xmath93 the @xmath94 matrix with @xmath95 and by @xmath96 the @xmath97 matrix with @xmath98 then , ( [ equ : obj ] ) may be written as the matrix form @xmath99 where @xmath100 .",
    "it is easy to see that the solution of the linear system @xmath101 is a solution of ( [ equ : obj2 ] ) .",
    "it follows from ( [ equ:1 ] ) and ( [ equ:2 ] ) that @xmath102 .",
    "suppose @xmath96 is of full column rank .",
    "let @xmath103 be the qr - decomposition of @xmath96 with @xmath104 orthogonal and @xmath105 upper - triangular . from @xmath102 , @xmath106 , so @xmath107 , the row space of @xmath108 .",
    "since @xmath104 is orthogonal , @xmath109 , and @xmath110 because @xmath111 projects onto @xmath112 .",
    "simple algebra gives @xmath113      in this section , we turn to the asymptotic properties of the estimate @xmath78 .",
    "[ thm : upper ] assume that for any @xmath114 ^ 2)$ ] @xmath115 for a positive constant @xmath116 .",
    "then , @xmath117 when @xmath80 is of order @xmath118 .",
    "we have made an additional assumption ( [ equ : condition ] ) on @xmath76 .",
    "for the functional linear regression model when @xmath65 , condition ( [ equ : condition ] ) shows that , for any @xmath119)$ ] , @xmath120 which states that linear functionals of @xmath76 have bounded kurtosis . in general , ( [ equ : condition ] ) states that such special nonlinear functional @xmath121 of @xmath76 have bounded kurtosis .",
    "it follows from both theorem [ thm : low ] and theorem [ thm : upper ] that the minimax rate of convergence for the excess prediction @xmath36 is of order @xmath118 , which is determined by the decay rate of the eigenvalues of the kernel @xmath50 .",
    "let @xmath122 . since the regularized estimator is a linear estimator in @xmath2 , @xmath123 , where @xmath124 is called the hat matrix depending on @xmath80 .",
    "some algebra yields @xmath125 we may select the tuning parameter @xmath80 that minimizes the generalized cross - validation score @xcite , @xmath126 choosing @xmath80 by minimizing gcv worked very well in our numerical studies .",
    "in our numerical studies , we compare the numerical performance of the proposed predictor with some well - known existing predictors .    we will focus on a rkhs @xmath26 with a squared seminorm @xmath127 the function @xmath128 where @xmath129 acts like a reproducing kernel in this approach to the computation of thin - plate splines , and hence is called a semi - kernel ( @xcite , @xcite ) . in this setting , the optimal solution of the roughness - regularized estimate can be written as @xmath130 where @xmath131 for some pair of integers @xmath132 with @xmath133 and @xmath134 is the number of such pairs .",
    "let @xmath135 and @xmath136 be the estimates from the training data .",
    "then , for any random function @xmath76 , the predicted response is @xmath137 in particular , when @xmath138 , we have @xmath139 , and @xmath140 note that @xmath141 and @xmath142 . to avoid an identifiability problem , we may estimate @xmath143 by @xmath144 . in the following",
    ", we will use thin - plate splines with @xmath138 to fit the data .",
    "our first simulation study compares our estimate with other two different estimates .",
    "the first method uses the well - known functional principal component analysis ( fpca ) approach .",
    "the second method uses the p - spline approach in @xcite , where one estimates @xmath64 using tensor - product b - splines with roughness penalties .",
    "the simulation setting is the same as the setting of @xcite and @xcite .",
    "the random predictor function @xmath76 was generated as @xmath145,\\ ] ] where @xmath146 are independently sampled from the uniform distribution on @xmath147 $ ] . obviously , the @xmath148 are eigenvalues of the covariance function of @xmath76 .",
    "consider two cases for the @xmath149 : the  closely spaced `` case and the  well spaced '' case .",
    "for the well spaced case , @xmath150 with @xmath151 and @xmath152 . for the closely spaced case , @xmath153 , @xmath154 for @xmath155 , and @xmath156 for @xmath157 and @xmath158 .",
    "the true coefficient function @xmath159 was given by @xmath160.\\ ] ] the simulation study was performed when the functional linear regression model is the true model .",
    "the response variable @xmath2 is simulated from the model : @xmath161 , where the error @xmath162 , where @xmath163 and @xmath164 .",
    "the performance of different estimators is measured by the root mean squared prediction error , @xmath165 where @xmath166 is the sample size of the test data and the @xmath167 are predicted values .",
    "each training set contains @xmath168 curves and @xmath169 curves are used for the test set . for each setting , the experiment is repeated @xmath170 times .",
    "the results of simulations are summarized in table [ tab1 ] .",
    "we observe that our thin - plate spline estimator performs nearly identically to the functional pca estimator , even though this is an ideal setting for the latter since the functional linear model holds .",
    "also , our estimator slightly outperforms the p - spline estimator .",
    ".the root mean squared prediction errors ( rmspe ) of three estimators for a functional linear regression model where @xmath161 .",
    "fpca is an estimation for the functional linear model based on functional principal components analysis .",
    "p - spline is the estimator of @xcite .",
    "`` thinspline '' is our proposed estimator using a thin - plate spline . [",
    "cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ table_air ]",
    "we have established the minimax rate of convergence for prediction for the continuous functional additive model . it is shown that the optimal rate depends on the decay rate of the eigenvalues of the kernel @xmath50 , which depends on the reproducing kernel and the joint distribution of the random predictor function at any two points . the minimax theory in the existing literature on the functional linear regression model is a special setting of current study .",
    "we have focused on the additive functional model with the squared error loss in this paper .",
    "it should be noted that the method of regularization can be easily extended to handle other models such as the generalized regression model @xcite .",
    "we shall leave these extensions for future papers .    the simulation in this paper study only the estimator using thin - plate splines .",
    "for the case of univariate regression , @xcite has showed that a smoothing spline and a p - spline are asymptotically equivalent .",
    "similar asymptotic equivalent result is expected to hold for the bivariate regression too .",
    "so , it is expected that our simulation performance is similar to that of @xcite , who used the bivariate p - splines to fit the data .",
    "however , it should be pointed out that our results can be applied to the more general reproducing kernel hilbert spaces .",
    "it is worth noting that estimating @xmath17 itself is totally different problem with the prediction discussed in the current paper .",
    "for example , for the functional linear regression model , we may not estimate the coefficient function @xmath159 consistently without additional conditions linking the smoothness of @xmath159 and the curves @xmath171 @xcite . as an example of additional assumptions",
    ", one might assume the reproducing kernel @xmath43 and the covariance kernel @xmath20 are perfectly aligned , i.e. , they share the same set of eigenfunctions . under this circumstance",
    ", we may estimate @xmath159 consistently @xcite .",
    "it deserves further study when we can estimate @xmath17 consistently under the additive functional model .",
    "this issue is important and we could use this to test for linearity of @xmath17 .",
    "since any lower bound for a specific case yields immediately a lower bound for the general case , to establish lower bounds , we only study the case when the @xmath12 are i.i.d .",
    "fix @xmath175 .",
    "it follows from theorem 2.5 in @xcite that in order to establish the minimax lower bound for @xmath36 , for each @xmath5 we need to find functions @xmath176 , @xmath177 , satisfying the following three conditions :    1 .",
    "@xmath178 , @xmath179 , 2 .",
    "@xmath180dt \\big\\}^2\\ge 2s$ ] , for @xmath181 , 3 .",
    "@xmath182 , where @xmath183 denotes the joint distribution of @xmath184 when @xmath185 and @xmath186 is the kullback - leibler distance between two probability measures .",
    "@xmath195 for all @xmath196 if @xmath197 for all @xmath198 .",
    "thus , we need to show that @xmath199 .",
    "this result holds since @xmath200 we also have @xmath201 where @xmath202 for @xmath203 , and @xmath204 for @xmath205 .",
    "further , the varshamov - gilbert bound ( see @xcite , p.  104 ) shows that , for @xmath206 , there exists a subset @xmath207 such that @xmath208 , @xmath209 where @xmath210 is the hamming distance between @xmath211 and @xmath212 , and @xmath213    to verify part ( b ) , for @xmath214 , direct calculation yields that @xmath215dt \\big\\}^2\\\\ = & \\sum_{j = m+1}^{2m}\\sum_{k = m+1}^{2 m }   m^{-1 } ( \\omega_j - \\omega_j')(\\omega_k - \\omega_k')\\int\\int \\mathbb e^ * \\big [ { k^{1/2}}(\\phi_j)(t , x(t ) ) { k^{1/2}}(\\phi_k)(s , x(s))\\big]dtds\\\\ = & \\sum_{k = m+1}^{2 m }   m^{-1 } ( \\omega_k - \\omega_k')^2 \\rho_k \\ge ~   m^{-1}\\rho_{2 m } d(\\omega , \\omega ' ) \\ge ~ c_1 m^{-1 } ( 2m)^{-2r } m/8 \\ge c_2 n^{-2r/(2r+1)}\\end{aligned}\\ ] ] by , @xmath54 , and the definition of @xmath190 .",
    "hence , @xmath189 in part ( b ) is of order @xmath118 .",
    "next , observe that for any @xmath216 , @xmath217 ^ 2.\\end{aligned}\\ ] ] therefore , @xmath218 ^",
    "2\\\\ & = { n\\over 2\\sigma^2 } \\sum_{k = m+1}^{2m}m^{-1 } ( \\omega_k - \\omega_k')^2 \\rho_k \\le { n\\over 2\\sigma^2 } \\rho_m \\sum_{k = m+1}^{2m}m^{-1 } ( \\omega_k - \\omega_k')^2 \\le { n\\over 2\\sigma^2}m^{-2r } \\le c_3 n^{1/(2r+1)}.\\end{aligned}\\ ] ] since @xmath190 is the smallest integer greater than @xmath191 , this implies that @xmath219 if we choose @xmath220 and @xmath221 .",
    "this completes the proof of theorem [ thm : low ] .",
    "_ proof of theorem [ thm : sol]_. define the subspace of @xmath88 , @xmath222 note that @xmath223 is a closed linear subspace of @xmath87 . for any @xmath224",
    ", one may write @xmath225 where @xmath226 , @xmath227 and @xmath228 .",
    "observe that @xmath229 because @xmath230 further , due to orthogonality , @xmath231 and @xmath232 .",
    "therefore , the minimum of ( [ equ : obj ] ) must belong to the linear space @xmath233 .",
    "+ _ proof of theorem [ thm : upper]_. note that @xmath234 .",
    "so there exist @xmath235 and @xmath236 such that @xmath237 and @xmath238 .",
    "therefore , @xmath239 and @xmath240 where @xmath241 write @xmath242 recall that @xmath243 .",
    "denote @xmath244 @xmath245 then , @xmath246 define @xmath247 it follows from triangle inequality that @xmath248 let us first bound the first term in the right hand side of ( [ equ : g ] ) .",
    "recall that the @xmath249 are the eigenfunctions of @xmath50 .",
    "write @xmath250 .",
    "then , @xmath251 and @xmath252    next , let us bound the second term in the right hand side of ( [ equ : g ] ) . recall that @xmath253 we observe that @xmath254 we now bound five terms on the right hand side separately .",
    "direct calculation yields that @xmath255 similarly , @xmath256    next , we make use three auxiliary results whose proofs are similar to ones in cai and yuan ( 2012 ) so we omit the details . if there exists a constant @xmath257 such that @xmath258 for any @xmath259 such that @xmath260 , then @xmath261 and @xmath262 where @xmath263 stands for the usual operator norm",
    ". further , for any @xmath264 @xmath265 using ( [ equ : use1 ] ) we have @xmath266 whenever @xmath267 for some constant @xmath257 . similarly , @xmath268",
    "so , for @xmath269 , @xmath270 when @xmath271 for @xmath272 .",
    "next , @xmath273 similarly , @xmath274 it follows from ( [ equ : use3 ] ) , @xmath275 combining the facts above , we conclude that , if @xmath80 is of order @xmath276 , then @xmath277"
  ],
  "abstract_text": [
    "<S> the functional generalized additive model ( fgam ) provides a more flexible nonlinear functional regression model than the well - studied functional linear regression model . </S>",
    "<S> this paper restricts attention to the fgam with identity link and additive errors , which we will call the additive functional model , a generalization of the functional linear model . </S>",
    "<S> this paper studies the minimax rate of convergence of predictions from the additive functional model in the framework of reproducing kernel hilbert space . </S>",
    "<S> it is shown that the optimal rate is determined by the decay rate of the eigenvalues of a specific kernel function , which in turn is determined by the reproducing kernel and the joint distribution of any two points in the random predictor function . </S>",
    "<S> for the special case of the functional linear model , this kernel function is jointly determined by the covariance function of the predictor function and the reproducing kernel . </S>",
    "<S> the easily implementable roughness - regularized predictor is shown to achieve the optimal rate of convergence . </S>",
    "<S> numerical studies are carried out to illustrate the merits of the predictor . </S>",
    "<S> our simulations and real data examples demonstrate a competitive performance against the existing approach . </S>",
    "<S> + _ keywords _ : functional regression , minimax rate of convergence , principal component analysis , reproducing kernel hilbert space . </S>"
  ]
}