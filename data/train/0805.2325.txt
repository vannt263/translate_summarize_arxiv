{
  "article_text": [
    "in analyses of survey data , such as those of galaxies or quasars , @xmath0-point correlation functions are often estimated ( e.g. * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . these help to describe the structure of the observed objects , such as the filamentary structure of galaxies that has been observed , and to constrain the parameters of cosmological models .",
    "such estimates are only as important as their associated errors , since it is the errors that indicate the amount of agreement between two sets of data or between data and simulations from a model .    in spatial point processes ,",
    "the expressions for the standard errors of correlation ( and similar ) functions have been worked out only for the simplest of models .",
    "for example , @xcite found approximations for the variance of the @xmath1 function , an integral of the two - point correlation function , for the poisson process .",
    "these depend on such factors as the shape of the observation region and the type of correction method for boundary effects .",
    "@xcite , @xcite and others worked out approximations for standard errors of various estimators of the two - point correlation function under the poisson or weakly clustered models .",
    "thus , very often , approximations such as poisson errors are used instead . however , point data arising in astronomy are typically clustered and non - poisson .",
    "so while poisson errors are useful and easy to compute , they only serve as rough indications of the size of errors .    besides using poisson errors ,",
    "errors can also be estimated by using mock catalogs generated from a cosmological model .",
    "this method was employed in @xcite , where the initial conditions for the cosmological model were selected independently . in the statistics literature , this is referred to as parametric bootstrap , although the term more commonly refers to mock datasets generated from the model using parameter values fixed at the estimates from the data , instead of being independently chosen .",
    "an alternative is to use non - parametric bootstrap .",
    "this involves generating new samples , called bootstrap samples , by resampling from the actual data , and computing estimates for these new samples .",
    "the distribution of these bootstrap estimates then serves as a proxy for the actual distribution of the data estimates , so that statistical inference , such as the construction of confidence intervals , can be performed .",
    "note that the procedure does not make any specific model assumptions , thus the errors obtained by this method can serve as a check of model assumptions .",
    "due to the simplicity and flexibility of the non - parametric bootstrap , the method is attractive .",
    "what is desirable then is to make the non - parametric bootstrap procedure work as well as possible for data that is correlated , and check that it performs satisfactorily , so that it can be useful as a tool in analysis .",
    "this paper thus examines the non - parametric bootstrap , specifically bootstrap of spatial data , where the dependence present in the data is of interest .",
    "there were some early misconceptions about how bootstrap should be applied to spatial data .",
    "the naive method of resampling individual points does not work in the spatial context . in order for spatial bootstrap to be valid",
    ", the underlying dependence structure has to be preserved as much as possible when generating bootstrap samples .",
    "two common methods for doing this are the block bootstrap and subsampling where blocks of data , instead of individual data points , are resampled .",
    "we introduce these methods in section [ sect : bootspatial ] and describe their shortcomings .",
    "in section [ sect : improve ] we describe the marked point bootstrap @xcite as a way to address these shortcomings .",
    "we describe how the marked point bootstrap can be used with the two- and three - point correlation function estimators , and by extension to estimators of @xmath0-point correlation functions . in section",
    "[ sect : simstudy ] we present results of a simulation study using simple point process models comparing the empirical coverage of confidence intervals obtained using non - parametric bootstrap and using normal approximations with poisson errors .    in this paper",
    ", we restrict ourselves to constructing nominal 95% confidence intervals , i.e. these confidence intervals are supposed to contain the true value 95% of the time .",
    "the empirical coverage of the confidence intervals is the actual confidence level achieved by the confidence intervals . in a simulation study with a known model , the empirical coverage can be obtained by finding the number of confidence intervals that contain the true value and then compared with the nominal level .",
    "it is desirable , of course , for the empirical coverage to be close to the nominal level .",
    "furthermore , it is often better for the empirical coverage to be higher instead of lower than the nominal level , so that the procedure is conservative .",
    "bootstrap is a computationally intensive procedure . with the large datasets now common in astronomy , even computing",
    "the @xmath0-point correlation functions pose computational challenges .",
    "for example , @xcite avoided using the jackknife procedure for error estimation because of the size of the data they used . the way the marked point bootstrap is formulated",
    ", however , makes it much faster than subsampling ( a generalization of the jackknife ) and the block bootstrap , so that applying the procedure to large datasets is feasible as long as computing the actual estimates is feasible . in section [",
    "sect : simstudy ] we provide some time measurements of the procedure used in our simulation study .",
    "the non - parametric bootstrap was originally developed for independent data @xcite .",
    "the main idea is to draw new samples from the actual data by sampling with replacement a data point at a time .",
    "bootstrap estimates of the same statistic are computed from the bootstrap samples . with these bootstrap estimates , confidence intervals , for example",
    ", can then be constructed .",
    "this can be done in a variety of ways .",
    "suppose @xmath1 , @xmath2 and @xmath3 are respectively the quantity of interest , the estimate of @xmath1 computed from the data and the bootstrap estimates , with @xmath4 equal to the number of bootstrap samples . a simple method , called the basic bootstrap interval in @xcite , is to set @xmath5 & \\label{eqn : basicci } \\end{aligned}\\ ] ] as the @xmath6% confidence interval for @xmath1 . here ,",
    "@xmath4 , the number of bootstrap samples , is large , say , 999 , and @xmath7 is the @xmath8-th ordered values of the bootstrap statistic .",
    "so , for example , with @xmath9 bootstrap samples , a 95% confidence interval for @xmath1 is given by @xmath10 $ ] . in our simulation studies ,",
    "we use ( [ eqn : basicci ] ) to construct the confidence intervals .",
    "standard errors for @xmath2 are estimated by the standard deviation of the bootstrap estimates @xmath11 .    while there are other methods of constructing confidence intervals from bootstrap samples ( see * ?",
    "* for example ) , the interest here is in the method of generating the bootstrap samples , when the data is spatial .",
    "@xcite rightly concludes that resampling individual points do not work .",
    "if the resampled points are placed in their original positions in the observation region , there will be multiple points at single locations , which do not usually occur in most data sets .",
    "in their claim that bootstrap can not be used for analysis for clustering , @xcite were also considering bootstrap in terms of resampling individual points .    due to the success of bootstrap for resampling independent data , it has been extended to resample dependent data .",
    "most of this work is for time series , but can easily be applied to spatial data in two and three dimensions .",
    "a common method is the block bootstrap : blocks of the spatial data are sampled at random , then joined together to form a new sample @xcite .",
    "asymptotic arguments for the validity of the bootstrap involve limiting the range of dependence , increasing the observation region size and letting the resampling block size increase but at a slower rate than the observation region size . in this asymptotic",
    "setting , we then have many almost independent blocks of data , with each block itself containing a large subsample ( see e.g. * ? ? ?",
    "however , the assumed conditions necessary to make the calculations tractable also means that normal approximations work well too .",
    "some theoretical results show that the accuracy of bootstrap estimates is of a higher order than the normal approximations . whether this difference is meaningful in actual practice is less clear .",
    "we believe that the role of non - parametric bootstrap is to serve as another objective method to obtain standard errors that do not make any model assumptions .",
    "error estimates obtained using bootstrap can be used as a way to assess or compare with other estimates of errors .",
    "@xcite is a recent work in the astronomy literature that examined bootstrap for dependent data . for the non - parametric bootstrap , they focused on subsampling @xcite , which can be considered a generalization of the jackknife procedure . in subsampling ,",
    "random portions of the data are deleted , and the remaining data are treated as bootstrap samples .",
    "the standard deviation of the estimates computed from these samples serves as an estimate of the error , less a factor to adjust for the smaller subsamples and the large overlap .    when estimating correlation functions , pairs or triplets etc of points have to be counted .",
    "by joining independently resampled blocks together to form the bootstrap sample , the block bootstrap creates artifical configurations of points across the resampling blocks and distorts the dependence structure in the data .",
    "this does not matter in asymptotic arguments because the effect becomes negligible if the range of the correlation is fixed while the resampling blocks increase in size .",
    "however , @xcite found that the actual coverage achieved by confidence intervals obtained using block bootstrap can be much lower than the nominal percentage level for finite samples .    in subsampling ,",
    "no artificial configurations of points are created . however , while the correction weight accounts for the difference in sample sizes between the bootstrap samples and the actual data set , it does not account for the change in the boundary effects due to the different resampling regions . since subsampling uses smaller regions as the bootstrap observation regions , boundary effects are magnified . for subsampling ,",
    "there is the temptation to use large subsamples to try and retain more of the dependence structure , but like block bootstrap , theoretical justification of the method requires that the subsamples be small in size relative to the actual data set .",
    "@xcite also found that subsampling can yield confidence intervals that attain very low empirical coverage .",
    "they also found that the subsampling method is sensitive to the fraction of the data used for subsampling .",
    "@xcite proposed another version of spatial bootstrap , called marked point bootstrap , that reduces the effect of joining independent blocks and produces confidence intervals that achieve coverage closer to the nominal level .",
    "this is described in the next section , where we also show how it can be applied to the two- and three - point correlation function estimators commonly used in astronomy .",
    "suppose @xmath0 points are observed in a region @xmath12 .",
    "furthermore , suppose that the quantity of interest @xmath1 can be estimated using an estimator of the form @xmath13 note that each point @xmath14 has an associated quantity @xmath15 , the inner sum of equation ( [ eqn : estimator ] ) .",
    "estimators of two - point statistics can be expressed in this form . in this case , the quantity @xmath16 will depend on the distance between @xmath17 and @xmath18 . as an aside , note that estimators of three - point statistics can be written in a similar form , with the inner sum replaced by a double sum .    with point data , the term `` mark ''",
    "is used to refer to some additional information associated with a point .",
    "this is usually some actual measured value . for galaxy data , for example",
    ", marks could be quantities such as luminosity , color and so on . in this paper ,",
    "the bootstrap method considered uses marks associated with the points .",
    "however , these marks are not quantities such as luminosity that are directly measured . instead they are numerical quantities that we construct and associate with the points .",
    "the actual values of these marks are not random , but are constructed so that they relate to the statistic that is of interest . if the statistic of interest is given by equation ( [ eqn : estimator ] ) , then the mark associated with point @xmath14 , denoted by @xmath19 , is equal to @xmath15 , so that  @xmath20 . at the risk of being repetitive , suppose that @xmath21 , for some @xmath22 , is of interest .",
    "this quantity is used in estimators of the two - point correlation function , and is the number of pairs of points separated by ( roughly ) distance @xmath22 .",
    "then the mark associated with point @xmath23 is @xmath24 , the number of points that are roughly distance @xmath22 away from @xmath23 .",
    "note that the sum of all the marks gives back the value of @xmath25 .",
    "it is also important to note that to compute the estimate ( [ eqn : estimator ] ) , the marks have to be calculated anyway . in regular applications ,",
    "the algorithm doing the estimation does not individually record these marks , but keeps a running sum of the marks . in order to do the marked point bootstrap ,",
    "the difference in terms of the code is that the marks now have to be stored so that they can be used in the bootstrap step .    in the block bootstrap ,",
    "blocks of data points are resampled and then joined together , forming a new dataset from which @xmath1 is estimated using the new configuration of points that was generated , yielding @xmath11 . in the marked point bootstrap ,",
    "blocks can be used to resample points as well . however , the crucial difference is that the bootstrap estimate is computed , not from how the resampled points are positioned , but from the marks that are associated with these points . in other words ,",
    "the marked point bootstrap resamples the marks rather than the points and the bootstrap estimate is computed by summing these resampled marks .    to be more precise ,",
    "suppose that @xmath26 number of points have been resampled , with the resampled points denoted by @xmath27 .",
    "associated with each @xmath28 is a mark @xmath29 .",
    "we denote this mark @xmath29 rather than @xmath30 to emphasize the fact that these marks are sampled from the actual data , i.e.  computed from the original dataset and not from the bootstrap sample .",
    "then the bootstrap estimate of @xmath1 is given by the average of the resampled marks : @xmath31 just like @xmath2 is given by the average of the actual marks .",
    "note that in an actual implementation of the procedure , all that is required is keeping track of how many times each point is resampled .",
    "the step - by - step procedure for estimating and resampling the quantity ( [ eqn : estimator ] ) is as follows :    1 .",
    "for each point @xmath14 , calculate @xmath32 .",
    "2 .   obtain the estimate @xmath2 , using @xmath33 .",
    "3 .   resample the points .",
    "this can be done by randomly placing blocks on to the observation region and keeping track of which point is resampled .",
    "suppose point @xmath34 is resampled @xmath35 times , and @xmath36 .",
    "the bootstrap estimate is then @xmath37 5 .",
    "repeat steps 3 and 4 to get @xmath4 bootstrap estimates .",
    "6 .   construct a confidence interval using ( [ eqn : basicci ] ) .",
    "a few remarks about the procedure are in order .",
    "* remark 1 * instead of randomly placing blocks , the observation region can be divided into a number of subregions , and the regions selected randomly with replacement . this latter method is sometimes referred to as using fixed blocks as opposed to moving blocks .",
    "it is generally considered that the moving blocks bootstrap works better in terms of convergence rates in asymptotic arguments .",
    "* remark 2 * the number of blocks used is so that the total area / volume of the blocks is equal to the original area / volume of the observation region .",
    "note that in this case @xmath26 would usually not be equal to @xmath0 , though they will be of the same order of magnitude .",
    "however , this does not pose problems since the statistic @xmath2 is a mean of the marks .",
    "* remark 3 * there is no real consenus on the size of the resampling blocks to use .",
    "@xcite did some work on determining the optimal block size from data .",
    "intuitively , the procedure needs large blocks so that the correlation structure is less distorted , and a large enough number of blocks so that there is enough variability between bootstrap samples . if @xmath1 represents the number of blocks and @xmath0 the number of data points ( which is assumed to increase with the observation region size ) , theoretical work in e.g.  @xcite suggests that consistency is achieved as @xmath38 and @xmath39 .",
    "thus some trade - off is needed .",
    "a rule - of - thumb is to divide each dimension of the observation region into at least three parts , i.e.  nine blocks in 2d , 27 blocks in 3d .",
    "this would ensure enough variability between bootstrap samples .",
    "of course , for correlation functions , the maximum value of the separation distance @xmath22 at which these functions are estimated would influence the decision on block size .",
    "fortunately , @xcite found that the marked point bootstrap is less sensitive to block size than block bootstrap or subsampling : they resampled an absorber catalog using slices of the sphere and found that the bootstrap errors were similar for different sizes of the slices .",
    "our simulation results also show little difference due to different block size .",
    "there are a few advantages to this form of spatial bootstrap over the regular block bootstrap .",
    "since the bootstrap estimates are based on the resampled marks and not on marks recalculated from the bootstrap sample , the contribution to the bootstrap estimate is due to actual pairs of points in the original dataset .",
    "this helps to minimize the distortion of dependence structure in the dataset due to resampling .    furthermore , for any block of resampled points ,",
    "information about the points just outside the block ( and therefore not sampled by this particular block ) is captured by the marks associated with the points that are sampled by the block .",
    "this helps to reduce the variability of bootstrap results due to the size of the resampling blocks , compared to block bootstrap or subsampling .",
    "also , since the resampling blocks do not need to be joined together to form a contiguous region for the bootstrap sample , there is flexibility in the choice of the shape of the resampling regions .",
    "lastly , the marked point bootstrap can be performed relatively quickly compared to block bootstrap or subsampling .",
    "the marks that are associated with the points are part of the actual estimator and are already computed in the estimation step .",
    "resampling using the marked point bootstrap only involves identifying which points are resampled with each resampling region , and keeping track of how many times each point is resampled .",
    "inter - point distances and edge correction weights do not have to be recalculated . with @xmath0 data points and @xmath4 bootstrap samples ,",
    "block bootstrap will take roughly @xmath40 computations for a statistic involving pairs of points .",
    "the marked point bootstrap will involve roughly @xmath41 computations .",
    "the difference will be more marked for three - point computations .",
    "simulation studies done in @xcite showed that the empirical coverages of confidence intervals obtained using the marked point bootstrap can be much closer to the nominal 95% level than those obtained with block bootstrap or subsampling .",
    "we now describe how the marked point bootstrap can be used with estimators of the two - point correlation function .",
    "the common estimators of the two - point correlation function @xmath42 are @xmath43 which are , respectively , the natural estimator @xcite , and estimators due to @xcite , where @xmath22 is some distance of interest . in these expressions , @xmath44 and @xmath45 ,",
    "where @xmath46 , @xmath47 and @xmath48 , @xmath49 is a set of randomly generated points ( i.e.  poisson ) in the observation region @xmath12 , and @xmath0 and @xmath50 are respectively the number of points in the real and random data sets .    to apply the marked point bootstrap , assign to each point @xmath23 of the dataset marks @xmath51 and @xmath52",
    ", we then have @xmath53 and bootstrap estimates of the two - point correlation functions are then obtained by substituting the above into ( [ eqn : nat])-([eqn : hewett ] ) .",
    "if each point @xmath17 of the actual data is resampled @xmath54 times , so that @xmath55 , @xmath56 and @xmath57 can also be written as @xmath58    note that @xmath59 does not need to be resampled , since it is used as an approximation to an integral and has nothing to do with the actual data .",
    "if , as is usually the case , estimation of @xmath42 is needed for a range of values of @xmath22 , then the marks @xmath60 and @xmath61 would be vectors , containing the relevant values for each value of @xmath22 .",
    "estimators of the three - point correlation function can be bootstrapped in a similar way .",
    "for example , an estimator of the three - point correlation function is @xmath62 introduced by @xcite , where @xmath63 and @xmath64 and @xmath65 are counts of triplets of points with the desired configuration , @xmath66 with all points from the real data set and so on . the contribution to @xmath66 by any particular triplet of points",
    "is divided by 3 and assigned as marks to each of the three points .",
    "for any individual point , all these marks are summed together . for @xmath67 , the contribution by each triplet",
    "is divided by 2 and assigned to the two real data points .",
    "bootstrap proceeds by resampling the real data points and the values of @xmath68 and @xmath69 found by adding the marks of the resampled points . substituting these into ( [ eqn:3ptest ] )",
    "gives the bootstrap estimate .",
    "other similar estimators , such as the three - point estimator of @xcite or the @xmath0-point estimators of @xcite , can be bootstrapped in the same way .",
    "we performed a simple simulation study to compare the performance of confidence intervals obtained using the marked point bootstrap with those obtained using normal approximations with poisson errors , varying the observation region size , number density and point process model . for computational simplicity , we restrict to two dimensions .",
    "we also performed an additional study with a large observation region and approximately 50,000 points , showing the applicability of the marked point bootstrap to datasets of size comparable to current astronomy datasets .",
    "we used the poisson point process model and a neyman - scott model to generate the data points .",
    "the neyman - scott model is of historical interest in astronomy as a model for galaxies @xcite .",
    "it is still commonly used to model point data in other fields @xcite .",
    "we chose the neyman - scott model as it is a model for clustered data with closed - form expressions for the two - point correlation function .",
    "the neyman - scott point datasets that we used are generated as follows : parent points are distributed as a poisson point process with intensity @xmath70 .",
    "a poisson number with mean @xmath71 of offspring points are then randomly scattered about each parent point .",
    "the collection of offspring points form the point process .",
    "we set the dispersion function of offspring points about parent points to be a bivariate normal density centered at the parent point , with standard deviation @xmath72 .",
    "this specific neyman - scott model is sometimes referred to as the modified thomas model @xcite .",
    "the two - point correlation function , @xmath42 , is zero for the poisson model , while @xmath73 for the modified thomas model .",
    "thus the point pattern from a modified thomas model is more clustered if @xmath70 or @xmath72 is smaller .",
    "the quantity @xmath72 also controls the range of the correlation , with the range larger for larger values of @xmath72 .",
    "we used several values for @xmath74 and @xmath72 in our simulation study .",
    "for each point process model , we generated 500 realizations on the unit square .",
    "for each realization , we estimated @xmath42 for @xmath75 .",
    "bootstrap estimates were then produced from each realization and a nominal 95% confidence interval constructed .",
    "thus for each point process model , we have 500 95% confidence intervals .",
    "we then checked the the empirical coverage , i.e.  the proportion of these that contained the true value of @xmath42 , with proportion closer to 95% being desirable .",
    "we also constructed 500 confidence intervals using the normal approximation with poisson errors .",
    "the poisson error @xmath76 is the inverse of the pair counts for an uncorrelated data set of the same size as the actual data , as given by @xcite .",
    "the 95% confidence intervals for @xmath77 based on the normal approximation are thus given by @xmath78 ) .",
    "we then found the empirical coverage of these confidence intervals .",
    "we then repeated the procedure for the @xmath79 and @xmath80 squares .",
    "the results are summarized in figures [ fig : hampoicoverage ] to [ fig : booterrors ] .",
    "figure [ fig : hampoicoverage ] shows plots of the empirical coverage of nominal 95% confidence intervals of the two - point correlation function for the poisson process model , using the @xcite estimator .",
    "simulation results for the other estimators are similar and are not shown .",
    "the thick solid lines in the plots show the empirical coverage of confidence intervals obtained using normal approximation with poisson errors .",
    "note that poisson errors are correct in this case and we find that the empirical coverage is close to 95% for all the point densities and observation region sizes considered .",
    "the thin lines represent the empirical coverage of confidence intervals obtained from the marked point bootstrap , with the different line types representing different resampling block sizes . these were squares of lengths 0.5 , 0.33 and 0.25 for the 1 by 1 regions , of lengths 1 , 0.67 , 0.5 , 0.33 for the 2 by 2 regions and of lengths 2 , 1 , 0.67 , 0.5 for the 4 by 4 regions ( solid , dashed , dotted and dashed - dotted lines respectively for increasingly smaller blocks ) .",
    "the difference due to the block size used for resampling appear to be small . as mentioned , this was an advantage of the marked point bootstrap .",
    "@xcite found greater variation of performance with block size for subsampling and block bootstrap .    compared with the poisson empirical coverage",
    ", we find that at low densities and smaller observation region sizes ( plots towards the upper left of figure [ fig : hampoicoverage ] ) , the bootstrap method does poorly .",
    "however , the empirical coverage of the bootstrap confidence intervals quickly increases towards 95% with increasing density ( down the columns in figure [ fig : hampoicoverage ] ) and/or observation region size ( across the rows in figure [ fig : hampoicoverage ] ) , i.e. with larger sample sizes .",
    "the top left plot of figure [ fig : booterrors ] shows the poisson errors and bootstrap errors for the poisson point process model simulated on the @xmath79 square .",
    "the bootstrap errors shown in this figure are from resampling with @xmath81 squares . also shown in the plot",
    "are the true errors as obtained from the estimates from 500 realizations .",
    "notice that both the poisson and bootstrap errors are close to the true errors .",
    "figure [ fig : hamtomcoverage ] shows similar plots for various modified thomas models , each with number density 500 .",
    "the general behavior with increasing observation region size for the poisson model occurs here as well .",
    "thus to reduce the number of plots , we only include plots for the 2 by 2 observation regions , and show the relative performance of the poisson and bootstrap confidence intervals .",
    "we find that when the point pattern is only weakly clustered ( left plot , for the case @xmath82 and @xmath83 ) , the poisson confidence intervals had empirical coverage close to the nominal 95% level .",
    "however , as the other two plots in figure [ fig : hamtomcoverage ] show , the empirical coverages of the poisson confidence intervals become lower than 95% as the degree and/or range of clustering increases ( i.e.  with smaller @xmath72 or @xmath70 ) . on the other hand ,",
    "the boostrap confidence intervals attain coverage much closer to 95% for all the cases shown , regardless of the degree of clustering .",
    "plots of the poisson and bootstrap error estimates are shown in figure [ fig : booterrors ] .",
    "notice that the poisson approximation underestimates the true error as the degree of clustering increases , while the bootstrap error estimates remain close to the true errors , even for the modified thomas model .",
    "thus we find that the performance of the poisson confidence intervals is sensitive to the degree of clustering of the point pattern .",
    "if the point pattern is poisson , or weakly clustered , the empirical coverage of poisson confidence intervals is close to the nominal level , even with small sample sizes . however , performance quickly deteriorates with greater degree of clustering . on the other hand",
    ", the bootstrap confidence interval does not perform well with small sample sizes . with moderate sample sizes",
    ", however , the bootstrap method performs rather well , over a wide range in the degree of clustering .",
    "we performed an additional set of simulations using data sets of roughly 50,000 points in a @xmath84 square and estimating @xmath42 for @xmath85 to 2 .",
    "other than the restriction to 2d , the data size and range of @xmath22 is roughly of the scale found in current astronomy data .",
    "we used the modified thomas model and chose four sets of parameter values , varying the degree and range of clustering but with the same number density . a sample realization from each of the four models",
    "is shown in figure [ fig : r20thomas ] .",
    "the models corresponding to the top row in figure [ fig : r20thomas ] have higher clustering than the models on the bottom row . for models in the same row ,",
    "the strength of clustering is similar , but the model on the right has a longer correlation range .",
    "we used square resampling blocks of side length 5 , 2.5 and 2 and results were very similar .",
    "the results are summarized in figure [ fig : r20simulation ] , which show the empirical coverage of confidence intervals ( left column ) and errors ( right column ) obtained from the marked point bootstrap and with poisson errors , for each of the four thomas models .",
    "the plots qualitatively show the same relative performance between poisson errors and bootstrap as found in the earlier simulation study .",
    "when the range of clustering is large , the empirical coverage of confidence intervals based on poisson errors and the normal approximation is very low ( second and fourth plots on the left column of figure [ fig : r20simulation ] ) .",
    "the coverage of the bootstrap intervals are affected too , but by much less . when the correlation is large , the poisson errors substantially under - estimate the true errors , while the marked bootstrap errors were more realistic .",
    "at the larger values of @xmath22 , especially when @xmath77 is near 0 , the bootstrap procedure appears to be somewhat conservative , while the poisson errors become more accurate .",
    "we made some time measurements of various sections of the algorithm : the functions computing @xmath86 and @xmath87 took 1 minute and 13 minutes respectively .",
    "here , @xmath88 and we did not use any sophisticated methods ( such as tree - based algorithms ) to speed up the computation . the bootstrap function , generating 999 samples and computing the estimates , took roughly 1 minute , showing the feasibility of the procedure for large data sets .",
    "the speed of the marked point bootstrap is due to the fact that the marks that are resampled have already been computed as part of the estimation .",
    "the additional computational burden of the bootstrap is sampling the points and keeping track of the number of times each point is resampled .",
    "in this paper , we introduced the marked point bootstrap as a method to bootstrap spatial data for estimating errors without specific model assumptions .",
    "in particular , we described how the method can be applied to estimators of the two- and three - point correlation functions . with the non - parametric bootstrap",
    ", errors are obtained from the actual data .",
    "there is no need choose a model , select parameter values or generate mock catalogs using @xmath0-body simulations .",
    "thus errors obtained from non - parametric bootstrap can be used to compare with errors obtained from other methods with more specific model assumptions .    for non - parametric spatial bootstrap",
    ", we propose the marked point bootstrap over the more common block bootstrap or subsampling methods .",
    "there are several advantages of the marked point bootstrap .",
    "firstly , by using information from actual pairs or triplets of points in the data , bootstrap confidence intervals using the marked point bootstrap attain better empirical coverage than confidence intervals constructed using block bootstrap ( see @xcite for a comparison of these two methods ) .",
    "secondly , in the marked point bootstrap , it is the marks that are used to compute the bootstrap estimate .",
    "thus the resampled points do not have to be arranged to form a new point pattern .",
    "this makes it a lot easier to bootstrap data that are observed in irregularly shaped regions that are common in astronomy . in @xcite for example , bootstrap on an absorber catalog was done using slices as well as spheres , with similar results for both types of resampling regions .",
    "thirdly , the marks used for resampling are part of the original estimate and are computed during the estimation step . the only additional computation required by the marked point bootstrap involves selecting points ( that is , testing whether each point lies in a resampling region or not ) , and keeping track of the number of times each point",
    "is resampled .",
    "unlike the block bootstrap , there is no need to re - compute from scratch the estimates for each bootstrap sample .",
    "this difference in computation is even greater for higher - order statistics .",
    "these properties of the marked point bootstrap make it a computationally feasible tool for analysis .",
    "our study here suggests that non - parametric bootstrap can yield valid estimates of errors under a wide range of point patterns .",
    "the lack of specific model assumptions means that the non - parametric bootstrap method , and in particular the marked point bootstrap , can serve as an alternate and complementary method for quantifying errors .",
    "having estimates of errors obtained using poisson approximations , parametric and non - parametric bootstrap allows one to have a better sense of the size of errors involved in an analysis .",
    "the simulation study performed here shows that bootstrap confidence intervals do attain coverage close to the nominal level , even for the clustered point patterns where poisson errors are known to be inaccurate , when sample sizes are large .",
    "more specifically , bootstrap performance improves with increasing number density , and also with increasing observation region size relative to the correlation length .",
    "unfortunately , in astronomy , the correlation length may be of the same scale as the observation region .",
    "if the values of @xmath22 at which the correlation function estimates are computed are small relative to the resampling blocks ( and the observation region ) , then although the bootstrap procedure would distort the dependence structure at the large scales , it would still be valid for these smaller values of @xmath22 .    if , instead , @xmath42 , say , for @xmath22 close to the size of the observation region is of interest , then the bootstrap procedure would start to break down , in the sense that the empirical coverage of confidence may not be close to the nominal level , and the bootstrap errors not reflect the true errors . in this case , the amount of information contained in the data is smaller and the boundary effects are magnified . with respect to the marked point bootstrap ,",
    "larger blocks are needed to capture the dependence structure at this larger scale . for a fixed sample ,",
    "this can not be done without reducing the variability of the bootstrap samples .",
    "the method that might work best is parametric bootstrap , assuming that the model is correct , and that the parameter values used are close to the true values .",
    "non - parametric bootstrap can still be useful here .",
    "firstly , it is at least a better choice than poisson errors , since the latter would grossly underestimate the true errors .",
    "secondly , it can provide additional error estimates to compare with the errors obtained with the assumed model .",
    "for these most challenging instances , having a variety of methods can only be beneficial .",
    ", h.  j. , peacock , j.  a. , guzzo , l. , capak , p. , porciani , c. , scoville , n. , aussel , h. , finoguenov , a. , james , j.  b. , kitzbichler , m.  g. , koekemoer , a. , leauthaud , a. , le fvre , o. , massey , r. , mellier , y. , mobasher , b. , norberg , p. , rhodes , j. , sanders , d.  b. , sasaki , s.  s. , taniguchi , y. , thompson , d.  j. , white , s. d.  m. , & el - zant , a. 2007 , astrophysical journal supplement , 172 , 314"
  ],
  "abstract_text": [
    "<S> in this paper , we examine the validity of non - parametric spatial bootstrap as a procedure to quantify errors in estimates of @xmath0-point correlation functions . we do this by means of a small simulation study with simple point process models and estimating the two - point correlation functions and their errors . </S>",
    "<S> the coverage of confidence intervals obtained using bootstrap is compared with those obtained from assuming poisson errors . </S>",
    "<S> the bootstrap procedure considered here is adapted for use with spatial ( i.e.  dependent ) data . </S>",
    "<S> in particular , we describe a marked point bootstrap where , instead of resampling points or blocks of points , we resample marks assigned to the data points . these marks are numerical values that are based on the statistic of interest . </S>",
    "<S> we describe how the marks are defined for the two- and three - point correlation functions . by resampling marks , </S>",
    "<S> the bootstrap samples retain more of the dependence structure present in the data . </S>",
    "<S> furthermore , this method of bootstrap can be performed much quicker than some other bootstrap methods for spatial data , making it a more practical method with large datasets . </S>",
    "<S> we find that with clustered point datasets , confidence intervals obtained using the marked point bootstrap has empirical coverage closer to the nominal level than the confidence intervals obtained using poisson errors . </S>",
    "<S> the bootstrap errors were also found to be closer to the true errors for the clustered point datasets . </S>"
  ]
}