{
  "article_text": [
    "the covariance matrix adaptation evolution strategy ( cma - es ) is designed to learn dependencies between decision variables by adapting a covariance matrix which defines the sampling distribution of candidate solutions @xcite .",
    "this algorithm constantly demonstrates good performance at various platforms for comparing continuous optimizers such as the black - box optimization benchmarking ( bbob ) workshop @xcite and the special session at congress on evolutionary computation @xcite .",
    "the cma - es was also extended to noisy @xcite , expensive @xcite and multi - objective optimization @xcite .    the principle advantage of the cma - es , the learning of dependencies between @xmath2 decision variables , also forms its main practical limitations such as @xmath4 memory storage and @xmath4 computational complexity per function evaluation @xcite .",
    "these limitations may preclude the use of the cma - es for computationally cheap but large scale optimization problems ( e.g. , with @xmath5 ) if the internal computational cost of cma - es is greater than the cost of one function evaluation .",
    "on non - trivial large scale problems with @xmath6 not only the internal computational cost of the cma - es becomes significant but it is becoming simply impossible to efficiently store the covariance matrix in memory .",
    "one may argue that there are very few known continuous domain real - world problems of that huge dimensionality .",
    "this situation probably will not change much before practitioners have a set of tools that are able to efficiently search in such huge search spaces .",
    "several evolution strategies ( ess ) have been proposed to deal with large scale optimization problems : @xmath7 time and space complexity algorithms such as separable cma - es ( sep - cma - es @xcite ) and linear time natural evolution strategy ( r1-nes @xcite ) , l - cma - es @xcite with @xmath8 time and @xmath1 space complexity , where only @xmath0 dominant eigen - pairs of the covariance matrix are computed .",
    "the sep - cma - es learns only the scaling of variables .",
    "the r1-nes learns only the predominant eigen - direction",
    ". the l - cma - es learns @xmath0 dominant eigen - pairs , but its @xmath8 sampling complexity practically ends up with @xmath4 when @xmath9 as studied in @xcite for non - separable problems where multiple adaptation directions are required .    the problem of growing time and space complexity when optimizing large scale problems is not new .",
    "it was addressed in gradient - based optimization community when it became clear that for @xmath10 the storage of the approximate inverse hessian matrix precludes the use of quasi - newton methods such as broyden  fletcher - goldfarb  shanno ( bfgs ) method @xcite . as a solution",
    ", it was proposed not to store the matrix but to reconstruct it using information from the last @xmath0 iterations @xcite .",
    "the final algorithm called the limited memory bfgs algorithm ( l - bfgs or lm - bfgs ) is still considered to be the state - of - the - art of large scale gradient - based optimization @xcite . in this paper",
    ", we demonstrate that a very similar idea can be used to reconstruct the covariance matrix in the cma - es to reduce the time and space complexity to @xmath11 ) .",
    "the paper is organized as follows .",
    "section [ state ] reviews evolution strategies ( ess ) proposed for large scale optimization .",
    "the lm - cma - es algorithm is described in section [ lmcmaalgo ] .",
    "the experimental validation of lm - cma - es is reported and discussed in section [ expe ] .",
    "section [ conclusion ] concludes the paper .",
    "historically , first evolution strategies @xcite were designed to perform the search without learning dependencies between variables which is a more recent development that gradually led to the cma - es algorithm @xcite . in this section ,",
    "we discuss in detail the cma - es algorithm and its state - of - the - art derivatives for large scale optimization .",
    "for a recent comprehensible overview of evolution strategies , the interested reader is referred to @xcite .",
    "the covariance matrix adaptation evolution strategy @xcite is probably the most popular and in overall the most efficient evolution strategy .",
    "the ( @xmath12)-cma - es is outlined in * algorithm * [ cmadefault ] . at iteration @xmath13 of cma - es , a mean @xmath14 of the mutation distribution ( can be interpreted as an estimation of the optimum )",
    "is used to generate its @xmath15-th out of @xmath16 candidate solution @xmath17 ( line [ cmasampling ] ) by adding a random gaussian mutation defined by a ( positive definite ) covariance matrix @xmath18 as    @xmath19    where @xmath20 is a mutation step - size .",
    "these @xmath16 solutions then should be evaluated on an objective function @xmath21 ( line [ cmagenerateend ] ) .",
    "the old mean of the mutation distribution is stored in @xmath22 and a new mean @xmath23 is computed as a _ weighted sum _ of the best @xmath24 parent individuals selected among @xmath16 generated offspring individuals ( line [ cmacomputenewmean ] ) .",
    "the weights @xmath25 are used to control the impact of selected individuals , weights are usually higher for better ranked individuals ( line [ cmaescmagiven ] ) .    the procedure of the adaptation of the step - size @xmath20 in cma - es is inherited from the cumulative step - size adaptation evolution strategy ( csa - es ) @xcite and is controlled by evolution path @xmath26 .",
    "successful mutation steps @xmath27 ( line [ cmasigmapathupdate ] ) are tracked in the space of sampling , i.e. , in the isotropic coordinate system defined by principal components of the covariance matrix @xmath28 . to update the evolution path @xmath26 a decay / relaxation factor @xmath29",
    "is used to decrease the importance of previously performed steps with time .",
    "the step - size update rule increases the step - size if the length of the evolution path @xmath26 is longer than the expected length of the evolution path under random selection @xmath30 , and decreases otherwise ( line [ cmastepsizeupdate ] ) .",
    "expectation of @xmath31 is approximated by @xmath32 .",
    "a damping parameter @xmath33 controls the change of the step - size .",
    "the covariance matrix update consists of two parts ( line [ cmaupdate ] ) : _ rank - one update _",
    "@xcite and _ rank-@xmath24 update _ @xcite .",
    "the rank - one update computes evolution path @xmath34 of successful moves of the mean @xmath27 of the mutation distribution in the given coordinate system ( line [ cmaevopathupdate ] ) , in a similar way as the evolution path @xmath26 of the step - size . to stall the update of @xmath34",
    "when @xmath35 increases rapidly , a @xmath36 trigger is used ( line [ cmahsigma ] ) .",
    "the rank-@xmath24 update computes a covariance matrix @xmath37 as a weighted sum of covariances of successful steps of @xmath24 best individuals ( line [ cmapluscov ] ) .",
    "the update of @xmath38 itself is a replace of previously accumulated information by a new one with corresponding weights of importance ( line [ cmaupdate ] ) : @xmath39 for covariance matrix @xmath40 of rank - one update and @xmath41 for @xmath37 of rank-@xmath24 update @xcite such that @xmath42 .",
    "recently it was proposed to also take into account unsuccessful mutations in the _ `` active '' rank-@xmath24 update _ @xcite .",
    "[ cmaescmagiven ] [ cmageneratebegin ] [ cmasampling ] [ cmagenerateend ] //",
    "the symbol @xmath43 denotes @xmath44-th best individual on @xmath21 [ cmacomputenewmean ] [ cmasigmapathupdate ] [ cmahsigma ] [ cmaevopathupdate ] [ cmapluscov ] [ cmaupdate ] [ cmastepsizeupdate ]    in cma - es , the factorization of the covariance @xmath38 into @xmath45 is needed to sample the multivariate normal distribution ( line [ cmasampling ] ) .",
    "the eigendecomposition with @xmath46 complexity is used for the factorization .",
    "already in the original cma - es it was proposed to perform the eigendecomposition every @xmath47 generations ( not shown in * algorithm * [ cmadefault ] ) to reduce the complexity per function evaluation to @xmath4      the original cma - es has @xmath4 time and space complexity that precludes its applications for large scale optimization with @xmath48 . to enable the algorithm for large scale optimization , a linear time and space version called sep - cma - es was proposed in @xcite .",
    "the algorithm does not learn dependencies but the scaling of variables by restraining the covariance matrix update to the diagonal elements : @xmath49 where , for @xmath50 the @xmath51 are the diagonal elements of @xmath28 and the @xmath52 .",
    "this update reduces the computational complexity to @xmath7 and allows to exploit problem separability , thus the original property of being rotationally invariant is lost .",
    "the algorithm demonstrated good performance on separable problems and even outperformed cma - es on non - separable rosenbrock function for @xmath5 .",
    "a novel natural evolution strategy ( nes ) variant , the rank - one nes ( r1-nes ) , which uses a low rank approximation of the search distribution covariance matrix was proposed recently by @xcite .",
    "the algorithm adapts the search distribution according to the natural gradient with a particular parametrization of the covariance matrix ,    @xmath53    where @xmath54 and @xmath35 are the parameters to be adjusted .",
    "the adaptation of the predominant eigen - direction @xmath55 allows the algorithm to solve highly non - separable problems while maintaining only @xmath7 time and space complexity .",
    "a version of cma - es with a limited memory storage also called limited memory cma - es ( l - cma - es ) was proposed by @xcite .",
    "the l - cma - es uses the @xmath0 eigen - vectors and eigen - values spanning the @xmath0-dimensional dominant subspace of the @xmath2-dimensional covariance matrix @xmath56 .",
    "the authors adapted a singular value decomposition updating algorithm developed in @xcite that allowed to avoid the explicit computation and storage of the covariance matrix .",
    "for @xmath57 the performance in terms of number of function evaluations gradually decreases while enabling the search in @xmath58 for @xmath6 .",
    "however , the computational complexity of @xmath8 practically ( for @xmath0 in order of @xmath59 @xcite ) leads to the same limitations as for the original cma - es .",
    "the ( @xmath12)-cholesky - cma - es proposed in @xcite is of special interest in this paper because the lm - cma - es is based on this algorithm .",
    "the cholesky - cma represents a version of cma - es with rank - one update where instead of performing the factorization of the covariance matrix @xmath28 into @xmath60 , the cholesky factor @xmath61 and its inverse @xmath62 are iteratively updated . from * theorem 1 * @xcite it follows that if @xmath28 is updated as    @xmath63    where @xmath64 is given in the decomposition form @xmath65 , and @xmath66 , then for @xmath67 a cholesky factor of the matrix @xmath68 can be computed by    @xmath69 { { \\textit{\\textbf{z}}}^t}^t,\\ ] ]    for @xmath70 we have @xmath71 .",
    "from the * theorem 2 * @xcite it follows that if @xmath72 is the inverse of @xmath61 , then the inverse of @xmath73 can be computed by    @xmath74,\\ ] ]    for @xmath75 and by @xmath76 for @xmath77 .",
    "[ cholcmaescmagiven ] [ cholcmageneratebegin ] [ cholcmasampling1 ] [ cholcmasampling2 ] [ cholcmagenerateend ] [ cholcmacomputenewmean ] [ cholcmacomputenewzmean ] [ cholcmasigmapathupdate ] [ cholcmaevopathupdate ] [ cholaupdate ] [ cholainvupdate ] [ cholcmastepsizeupdate ]    the ( @xmath12)-cholesky - cma - es is outlined in * algorithm * [ cholcma ] . as well as in the original cma - es , cholesky - cma - es proceeds by sampling @xmath16 candidate solutions ( lines [ cholcmageneratebegin ] - [ cholcmagenerateend ] ) and taking into account the most successful @xmath24 out of @xmath16 solutions in the evolution paths adaptation ( lines [ cholcmasigmapathupdate ] and [ cholcmaevopathupdate ] ) .",
    "however , the eigen - decomposition procedure is not required anymore because the cholesky factor and its inverse are updated incrementally ( line [ cholaupdate ] and [ cholainvupdate ] ) .",
    "this simplifies a lot the implementation of the algorithm and reduces its time complexity to @xmath4 . a postponed update of the cholesky factors every @xmath7 iterations would not reduce the asymptotic complexity further ( as it does in the original cma - es ) because the quadratic complexity will remain due to matrix - vector multiplications needed to sample new individuals .",
    "the non - elitist cholesky - cma is a good alternative to the original cma - es and demonstrates a comparable performance @xcite . while it has the same computational and memory complexity , the lack of rank-@xmath24 update may deteriorate its performance on problems where it is essential .",
    "in this section , we first present main components of the computationally cheap limited memory cma - es and then introduce the algorithm itself .",
    "the components are : a procedure for reconstruction of cholesky factors of a covariance matrix using stored direction vectors , a procedure to store these vectors and a new procedure for step - size adaptation .",
    "the idea to reconstruct the inverse hessian matrix in the bfgs method @xcite enabled its application for large scale gradient - based optimization . while the cma - es is a gradient - free algorithm , the two algorithms are indeed similar with a difference that the latter estimates the gradient in a stochastic way",
    ". this observation inspired us to investigate whether a similar matrix reconstruction procedure can be used in cma - es as well to reduce its time and space complexity .",
    "as can be seen , the only use of cholesky factor @xmath61 in * algorithm * [ cholcma ] is for sampling of new solutions after @xmath78 or for its own update to @xmath73 . by setting @xmath79 and @xmath80 , one can rewrite the line ( [ cholaupdate ] ) as    @xmath81    in the following , we show how the vectors needed to sample new candidate solutions can be obtained without an explicit storage of cholesky factors . at iteration @xmath82 , @xmath83 and @xmath84 in line ( [ cholcmasampling2 ] ) of * algorithm * [ cholcma ] , the new updated cholesky factor @xmath85 . at iteration @xmath86 , @xmath87 and @xmath88 .",
    "thus , a very simple iterative procedure which scales as @xmath1 can be used to sample candidate solutions in @xmath89 according to the cholesky factor @xmath61 reconstructed from @xmath0 pairs of vectors @xmath90 and @xmath91 .    [ cholvectorgiven1 ] [ cholvectorgeneratebegin ] [ cholvector1 ] [ cholvector3 ]    [ invcholvectorgiven ] [ invcholvectorgeneratebegin2 ] [ invcholvector1 ] [ invcholvector3 ]    [ sel1 ] [ sel2 ] [ sel3 ] [ sel4 ] [ sel5 ] [ sel6 ] [ sel7 ] [ sel8 ] [ sel9 ] [ sel10 ] [ sel12 ] [ sel13 ] [ sel34 ]    the pseudo - code of the procedure to reconstruct @xmath92 from @xmath0 direction vectors evolution paths @xmath93 and their inverses @xmath94 but for brevity we say @xmath0 direction vectors ] is given in * algorithm * [ cholvector ] . at each iteration of reconstruction of @xmath92 ( lines [ cholvectorgeneratebegin ] - [ cholvector1 ] ) ,",
    "@xmath95 is updated as a sum of @xmath96-weighted version of itself and @xmath97-weighted evolution path @xmath90 scaled by the dot product of @xmath91 and @xmath95 . as can be seen , the * algorithm * uses @xmath98 indexation instead of @xmath13 .",
    "this is simply a convenient way to have references to matrices @xmath99 and @xmath100 which store @xmath90 and @xmath91 vectors , respectively . in the next subsection",
    ", we will show how to efficiently manipulate these vectors .",
    "a very similar approach can be used to reconstruct @xmath101 , for the sake of reproducibility the pseudo - code is given in * algorithm * [ invcholvector ] for @xmath102 and @xmath103 .",
    "the computational complexity of both procedures scales as @xmath1 .",
    "it is an open question how to use only @xmath104 direction vectors to obtain a comparable amount of useful information as stored in the covariance matrix of the original cma - es . for large @xmath2 and @xmath105 , evolution path vectors",
    "@xmath106 from the last @xmath0 iterations are likely to be quite similar and therefore to contain only some local information .    in this paper , we propose a simple approach which forces @xmath0 selected vectors to be at approximately the same distance from each other in terms of number of iterations , but at most with the distance of @xmath107 from each other given that the @xmath0-th vector is the one from the last iteration .",
    "this selection procedure is outlined in * algorithm * [ selection ] which outputs an array of pointers @xmath108 such that @xmath109 points out to a row in matrices @xmath99 and @xmath100 with the oldest saved vectors @xmath93 and @xmath94 which will be taken into account during the reconstruction procedure .",
    "the higher the index @xmath44 of @xmath110 the more recent the corresponding direction vector is .",
    "the index @xmath111 points out to the oldest vector which will be replaced by the newest one in the same iteration when the procedure is called .",
    "the rule to choose a vector to be replaced is the following : find a pair of consecutively saved vectors with the closest distance ( in terms of number of iterations , stored in @xmath112 ) between each other ( line [ sel3 ] ) , if this distance is smaller than @xmath107 then the most recent vector will be removed by assigning @xmath113 , otherwise the oldest vector among @xmath0 saved vectors should be removed .",
    "thus , the procedure gradually replaces vectors in a way to keep them at approximately the same distance , but at most at distance of @xmath107 iterations .      an elegant success rule for step - size adaptation called the _",
    "median success rule _ was recently proposed in @xcite .",
    "it is applicable to non - elitist multi - recombinant evolution strategies .",
    "the median success rule compares the median fitness of the population to a fitness from the previous iteration .",
    "the comparison fitness is chosen to achieve a target success rate of 1/2 .",
    "the empirical validation demonstrated that the median success rule is competitive to csa @xcite .    in practice",
    ", one should count the number @xmath114 of individuals in the current population better than some @xmath115-th best individual of the previous population , where @xmath115 depends on @xmath2 and @xmath16 but can be set to be @xmath116 @xcite .",
    "then , a normalized measurement    @xmath117    can be computed such that @xmath118 iff the median individual was successful .",
    "the step - size is adapted as    @xmath119    where @xmath120 and @xmath121 .",
    "we suppose that while being quite elegant the median success rule has a potential drawback that we will demonstrate on an example .",
    "let us suppose that fitness values ( to be minimized ) of the previous population are say @xmath122 $ ] while the fitness values of the current population are @xmath123 $ ] .",
    "according to the median success rule if @xmath115 is chosen as , e.g. , 3 , the number of successful individual ( with fitness values better than or equal to @xmath124 ) is 4 ( as @xmath125 and @xmath126 ) .",
    "the computed value of @xmath114 is then will be used to adapt the step - size .",
    "however , its computation does not take into account the values of @xmath127 for @xmath128 and even if all such @xmath127 are better than the best solution @xmath129 , this information will not be taken into account .",
    "this potential drawback is not the drawback in a sense that the _ median _ success rule was designed in this way .",
    "however , we suppose that the information omitted in the median success rule can be useful since it can provide a better estimate whether and by how much the new population is more successful than the previous one .    in this paper",
    ", we introduce _ the population success rule _ ( psr ) for step - size adaptation for non - elitist multi - recombinant evolution strategies . to estimate the success of the current population we combine fitness function values from the previous and current population into a mixed set    @xmath130    then , we rank all individual in the mixed set to define two sets @xmath131 and @xmath132 containing ranks of individuals of the previous and current populations ranked in the mixed set .",
    "we compute a normalized success measurement    @xmath133    where @xmath134 is a target success ratio .",
    "the step - size can be adapted as in ( [ mrulesigma ] ) .",
    "the proposed _",
    "population success rule _ takes into account all fitness function values from the previous and current generation",
    ". this success rule seems to represent a more general case of the 1/5th - rule which can be obtained when @xmath135 .      in the previous subsection we introduced all necessary components of the ( @xmath12)-lm - cma - es outlined in * algorithm * [ lmcma ] .",
    "the algorithm represents a computationally efficient limited memory version of cma - es , where the cholesky factor and its inverse are reconstructed from a set of stored direction vectors ( lines [ lmcmasampling2 ] and [ lmcmaevopathupdate ] ) .",
    "the mutation step - size is adapted using the population success rule ( lines [ succrule1 ] - [ lmcmaupdate ] ) .",
    "the algorithm memory and time complexity scales as @xmath1 .",
    "[ lmcmagiven ] [ lmcmageneratebegin ] [ lmcmasampling1 ] [ lmcmasampling2 ] [ lmcmagenerateend ] [ lmcmacomputenewmean ] [ lmcmaevopathupdate ] [ lmcmavecupdate ] [ lmcmaupdate ] [ lmcmadvec ] [ succrule1 ] [ lmcmasigma ]",
    "in this section , we perform a set of numerical experiments to assess the performance of the proposed lm - cma - es on large scale optimization problem with @xmath136 .",
    "we investigate the performance on three basic problems : sphere function @xmath137 , separable ellipsoid function @xmath138 and its rotated version @xmath139 , where @xmath140 is an orthogonal @xmath141 matrix with each column vector @xmath142 being a uniformly distributed unit vector implementing an angle - preserving transformation @xcite .      for the sake of reproducibility",
    ", the matlab / c++ source code of all tested algorithms is available at + https://sites.google.com / site / lmcmaeses/.    in the order to estimate the performance of ( @xmath12)-lm - cma - es , we compare it with ( @xmath12)-cholesky - cma - es and ( @xmath12)-sep - cma - es .",
    "we use the default parameters for cholesky - cma - es and sep - cma - es as given in @xcite and @xcite , respectively .",
    "the parameters of lm - cma - es are given in * algorithm * [ lmcma ] .",
    "for all problems , the mean @xmath143 is initialized in the range @xmath144^n$ ] , the population is sampled with initial step - size @xmath145 and using the same seed per run .",
    "note that in all cases we use the default population size @xmath146 .",
    "the lm - cma - es has @xmath1 memory complexity and more specifically stores @xmath147 , @xmath148 and @xmath16 solution vectors @xmath149 . for large @xmath2 and @xmath150 used in this paper , the algorithm stores approximately @xmath151 real - valued parameters .",
    "if a real - valued parameter requires @xmath152 bytes of memory , then for @xmath153 the lm - cma - es will require 5.8 megabytes while the original cma - es would start to reach its limit by requiring 1 gigabyte of memory .",
    "using the same amount of memory ( more specifically , 1.03 gigabyte ) , the lm - cma - es will able to optimize a 1 million dimensional problem .",
    "indeed , by taking @xmath154 even less memory would be needed but the latter possibility makes sense only if the performance stays at a reasonable level .",
    "timing results of lm - cma - es on the separable ellipsoid compared to sep - cma - es and cholesky - cma - es .",
    "the results were computed using at most @xmath155 function evaluations for sep - cma - es and lm - cma - es and using at most @xmath156 for cholesky - cma - es . ]",
    "figure [ fig : timing ] shows how fast cpu time per evaluation scales for different operations ( measured on a 2.0 ghz processor ) .",
    "scalar - vector multiplication of a vector with @xmath2 variables scales linearly with ca .",
    "@xmath157 seconds , evaluation of the separable ellipsoid is twice more expensive if a temporary data is used .",
    "sampling of @xmath2 normally distributed variables scales as ca .",
    "100 vectors - scalar multiplications .",
    "as can be seen in figure [ fig : timing ] , sampling of @xmath158 dominates the computational overhead of sep - cma already after @xmath159 .",
    "the lm - cma - es scales almost linearly for @xmath160 as ca .",
    "@xmath161 or ca .",
    "200 scalar - vector multiplications .",
    "matrix - vector multiplication scale quadratically with @xmath2 and cholesky - cma - es scales as ca .",
    "1.5 - 2 matrix - vector multiplications .    practically , the lm - cma - es is about 40 times faster ( in terms of its internal computation cost per function evaluation ) for @xmath162 and about 140 times faster for @xmath153 than cholesky - cma - es .",
    "the lm - cma - es is only about 2 times slower than sep - cma - es , whose cost is dominated by sampling from normal distribution .    the computation cost of cma - es with full covariance matrix learning limits its applicability for @xmath48 and makes it intractable because of memory for @xmath6",
    ".     results of lm - cma - es on the sphere function compared to sep - cma - es and cholesky - cma - es .",
    "lines show the median of 11 runs for different problem dimensions to reach the target fitness value of @xmath163 .",
    "the dotted line is an extrapolation . ]",
    "the sphere function is often viewed in evolutionary computation to be the first function to look at when benchmarking evolutionary algorithms .",
    "figure [ fig : sphere ] demonstrates a comparable performance of lm - cma - es with population success rule , sep - cma - es with csa and cholesky - cma - es with csa .",
    "it should be further studied what is the effect of the target population success rate ( set to @xmath164 ) whose value was chosen the same for all experiments in order to obtain a reasonable performance on ellipsoid functions .",
    "figure [ fig : elli]-left shows that both lm - cma - es and cholesky - cma - es are rotationally invariant and therefore they optimization runs ( one per function ) are almost coincide ( within the algorithm ) .",
    "the sep - cma - es is not rotationally invariant and therefore it performs better on the separable ellipsoid than on its rotated version where the exploitation of the separability is not that useful . importantly , the lm - cma - es often outperforms the cholesky - cma - es in the beginning of optimization , while the adaptation of the full covariance matrix makes cholesky - cma - es faster at later stages .",
    "figure [ fig : elli]-right shows that the loss of performance of lm - cma - es compared to cholesky - cma - es is in order of a factor of 3 - 4 given that for @xmath162 the lm - cma - es uses only @xmath165 direction vectors .",
    "it is important to keep in mind that for @xmath6 the cholesky - cma - es becomes intractable both due to its memory and computational complexity .",
    "then , the sep - cma - es becomes an alternative , however , it does not learn dependencies and might be therefore inefficient ( see figure [ fig : elli]-left ) .",
    "we discussed several large scale ess in this paper : l - cma - es @xcite and r1-nes @xcite .",
    "we compared the lm - cma - es indirectly by analyzing the results from @xcite and @xcite .",
    "it takes about 6000 seconds for l - cma - es to solve 200-dimensional ellipsoid after about @xmath166 function evaluations with @xmath167 and 4000 seconds after @xmath168 evaluations with @xmath169 .",
    "the lm - cma - es solves the same problem after about @xmath170 seconds and @xmath171 function evaluations with @xmath172 .",
    "the performance is comparable while the lm - cma - es is about @xmath173 times faster that is unlikely to be only due to a different processor or implementation used .",
    "the l - cma - es has @xmath8 computational complexity and therefore it is in order of @xmath0 times computationally slower than lm - cma - es .",
    "the r1-nes algorithm performs well on non - separable problems but tends to fail on problems where the learning of multiple principal components is essential , e.g. , it fails on moderate dimensional rotated ellipsoid function @xcite . on rosenbrock function",
    "the lm - cma - es is about 5 times faster ( not shown ) in terms of number of function evaluations for @xmath174 .",
    "the r1-nes also samples from the normal distribution , and therefore the lower bound of its computational complexity is predefined ( see figure [ fig : timing ] ) .",
    "we performed an experiment on 100,000-dimensional separable ellipsoid problems for 100,000 function evaluations ( i.e. , @xmath2 evaluations ) .",
    "the original cma - es and cholesky - cma - es can not be applied due to memory requirements .",
    "the applicability of l - cma - es is also limited due to its @xmath8 computational complexity .",
    "the results for sep - cma - es specifically designed for large scale optimization and the proposed lm - cma - es are shown in figure [ fig : elli100k ] .",
    "while the lm - cma - es gradually improves the fitness similarly as in figure [ fig : elli]-left , the sep - cma - es does not improve it because it diverges from the very first iterations . to investigate whether it is a mistake in our implementation ,",
    "we launched the same experiment using the sep - cma - es author s matlab implementation where the divergence was also observed .",
    "it should be noted that the separable ellipsoid can be easily solved by various evolutionary algorithms which implicitly or explicitly exploit its separability , our purpose of its usage is to investigate how the lm - cma - es performs on problems with high dependencies between variables .",
    "given that the lm - cma - es is rotationally invariant , its performance on both separable and non - separable problems is comparable , but the former is cheaper to compute .",
    "lm - cma - es and sep - cma - es on separable 100,000-dimensional ellipsoid problem .",
    "the sep - cma - es divergences after the first generation ( the best fitness is shown ) . note that the lm - cma - es is rotationally invariant , therefore a similar performance is expected on 100,000-dimensional rotated ellipsoid . ]",
    "this paper presents a new approach to efficiently store and exploit the information about dependencies between decision variables of large scale optimization problems .",
    "it allows to reconstruct the cholesky factor and its inverse using @xmath104 direction vectors that turns out to be sufficient to obtain good performance on large scale problems with highly - depended variables .",
    "the implementation of this approach in the lm - cma - es algorithm makes it possible to optimize a 1 million dimensional problem while learning dependencies between variables at a cost of about 0.1 second per function evaluation on an ordinary machine .",
    "indeed , one should not plan to easily find a global optimum in such a huge search space , but some local optimization / tuning seems reasonable , e.g. , in machine learning problems .",
    "the proposed lm - cma - es algorithm is based on the _ population success rule _ which looks promising and requires further theoretical and empirical investigations .",
    "it should be studied as well whether it can be claimed to represent a general case of the 1/5th success rule .",
    "more experiments are required to investigate whether and when the lack of rank-@xmath24 update is a limitation .    all parameters chosen for the algorithm were tuned only moderately and _ specifically _ for large @xmath2 and might require a significant revision to address a wider set of optimization problems commonly used for eas . however , we suppose that the performance on the ellipsoid function is already worth a closer scientific investigation .",
    "we envision that several directions may further improve the algorithm : i ) adaptation of @xmath0 within a fixed range , the impact of @xmath0 itself should be studied as well , ii ) since the population success rule does not make any assumptions about the sampling distribution , the gaussian sampling can be removed that would further speed - up the algorithm ( e.g. , to replace csa by psr in cma - es ) .",
    "the speculations about a possibility of having cma - es like evolutionary processes going on in nature often end up around a hypothesis that there is no such a thing in natural evolution as a full covariance matrix and its update .",
    "one may suppose that only a limited number of direction vectors is stored to adjust the mutation in promising directions .",
    "a.  auger , d.  brockhoff , n.  hansen , et  al .",
    "benchmarking the local metamodel cma - es on the noiseless bbob2013 test bed . in",
    "_ gecco ( companion ) , workshop on black - box optimization benchmarking ( bbob2013 ) _ , pages 12251232 , 2013 .",
    "n.  hansen , a.  s. niederberger , l.  guzzella , and p.  koumoutsakos .",
    "a method for handling uncertainty in evolutionary optimization with an application to feedback control of combustion .",
    ", 13(1):180197 , 2009 .",
    "i.  loshchilov , m.  schoenauer , and m.  sebag .",
    "self - adaptive surrogate - assisted covariance matrix adaptation evolution strategy . in _ genetic and evolutionary computation conference _ , pages 321328 .",
    "acm , 2012 ."
  ],
  "abstract_text": [
    "<S> we propose a computationally efficient limited memory covariance matrix adaptation evolution strategy for large scale optimization , which we call the lm - cma - es . </S>",
    "<S> the lm - cma - es is a stochastic , derivative - free algorithm for numerical optimization of non - linear , non - convex optimization problems in continuous domain . </S>",
    "<S> inspired by the limited memory bfgs method of liu and nocedal ( 1989 ) , the lm - cma - es samples candidate solutions according to a covariance matrix reproduced from @xmath0 direction vectors selected during the optimization process . </S>",
    "<S> the decomposition of the covariance matrix into cholesky factors allows to reduce the time and memory complexity of the sampling to @xmath1 , where @xmath2 is the number of decision variables . </S>",
    "<S> when @xmath2 is large ( e.g. , @xmath2 > 1000 ) , even relatively small values of @xmath0 ( e.g. , @xmath3 ) are sufficient to efficiently solve fully non - separable problems and to reduce the overall run - time . </S>"
  ]
}