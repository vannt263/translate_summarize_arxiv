{
  "article_text": [
    "computing the eigendecomposition of a symmetric tridiagonal matrix is a classic linear algebra problem , and is ubiquitous in computational science .",
    "some well - known algorithms include the qr algorithm  @xcite , the mrrr  @xcite and the divide - and - conquer ( dc ) algorithm  @xcite . according to the comparisons in  @xcite , dc and mrrr are generally faster than qr for large matrices , especially when the eigenvectors are required . in this work ,",
    "we focus on the dc algorithm , and the goal is to develop an improved version .",
    "though dc is very fast in practice which takes @xmath3 flops on average  @xcite , for some matrices with few deflations its complexity @xcite can be @xmath4 . by using hierarchically semiseparable ( hss ) matrices  @xcite ,",
    "we show that its worst case complexity can be reduced to @xmath0 , where @xmath2 is a modest number and is usually much smaller than a big @xmath1 .",
    "the main observation of this work is from the following theorem .",
    "[ thm : bns - r1 ] assume that @xmath5 such that @xmath6 , and @xmath7 is a vector and a scalar @xmath8 .",
    "then , the eigenvector corresponding to @xmath9 , an eigenvalue of @xmath10 , is @xmath11 and the eigenvalues of @xmath12 satisfy @xmath13    it shows that @xmath14 with @xmath15 is a cauchy - like matrix .",
    "recall that @xmath16 is called cauchy - like if it satisfies @xmath17 where @xmath18 and @xmath19 , which are called the _ generators _ of cauchy - like matrix @xmath20 .",
    "it is easy to check that @xmath20 is also off - diagonally low - rank . to take advantage of these two properties",
    ", we can use an hss matrix to approximate @xmath20 and then use the fast hss matrix multiplication algorithm to update the eigenvectors , like the bidiagonal svd case  @xcite .",
    "a structured low - rank approximation method is designed for a cauchy - like matrix in  @xcite , called srrsc ( _ structured rank - revealing schur - complement factorization _ ) , which can be used to construct hss matrices efficiently . by incorporating srrsc into dc , an accelerated dc ( adc ) algorithm",
    "is proposed for the singular value problem in  @xcite , where adc is 3x faster than dc in intel mkl for some large matrices .",
    "we show that this technique also works for the symmetric tridiagonal eigenvalue problem , which is presented in example 3 in section  [ sec : numer - tedc ] . in this paper",
    ", we implement the adc algorithm in parallel and it obtains even better speedups .    in this paper , we further show that the randomized hss construction algorithm  @xcite can also be used to compute the eigendecomposition reliably , and it can obtain similar speedups as using srrsc when compared with intel mkl .",
    "recent work has suggested the efficiency of the randomized algorithms  @xcite for computing a low - rank matrix approximation .",
    "martinsson  @xcite has developed a novel hss construction algorithm by using the randomized sampling technique , and for simplicity we refer to this algorithm as ` rshss ` .",
    "this method is extremely suitable for matrices with fast matrix - vector multiplication algorithms .",
    "for example , if the matrix - vector product costs @xmath21 flops , rshss has linear complexity @xmath22 , where @xmath1 is the dimension of the matrix and @xmath2 is its maximum numerical rank of off - diagonal blocks . for matrix @xmath20 in",
    ", the fast multipole method ( fmm )  @xcite can be used for the matrix - vector product in @xmath23 flops .",
    "therefore , an hss matrix approximation to @xmath20 can be constructed in @xmath22 flops if combining rshss with fmm . to use rshss , we need to know the maximum rank of off - diagonal blocks , which is difficult for general matrices .",
    "fortunately , the off - diagonal rank of the matrix @xmath20 defined in   can be estimated by using the approximation theory of function @xmath24 .",
    "we show in section  [ sec : notation ] that the estimated rank based on the exponential expansion  @xcite is quite acceptable .    by adding the hss matrix techniques to the symmetric dc algorithm  @xcite , two new accelerated dc ( adc ) algorithms",
    "are obtained .",
    "one is denoted by adc1 based on srrsc , and the other is denoted by adc2 based on rshss .",
    "similar to the analysis in  @xcite , the complexity of adcs can be shown to be @xmath25 where @xmath1 is the dimension of a symmetric tridiagonal matrix @xmath26 and @xmath2 is a modest integer , which is related to the distribution of eigenvalues . since the hss matrix construction  @xcite and multiplication  @xcite algorithms",
    "are naturally parallelizable , we further implement these two algorithms in parallel by using openmp in a shared memory multicore environment .",
    "we also simply parallelize the classical process of dc algorithm by using openmp such as solving the subproblems at the bottom level of the divide - and - conquer tree and all the secular equations .",
    "numerous experiments have been done to test these two adc algorithms .",
    "it turns out that our adcs can be about 6x times faster than the dc implementation in mkl for some large matrices with few deflations .",
    "the accuracy comparisons are also included in section  [ sec : numer - tedc ] .",
    "assume that @xmath26 is a symmetric tridiagonal matrix , @xmath27 we briefly introduce some formulae of cuppen s divide - and - conquer algorithm  @xcite .",
    "@xmath26 is decomposed into the sum of two matrices , @xmath28 where @xmath29 and @xmath30^t$ ] with ones at the @xmath31-th and @xmath32-th entries .",
    "if @xmath33 and @xmath34 , then @xmath26 can be written as @xmath35 where @xmath36 by theorem  [ thm : bns - r1 ] , we can get the eigenvectors @xmath20 of the middle matrix at the right hand side of  .",
    "the eigenvectors of @xmath26 would be @xmath37 .",
    "the matrices @xmath38 can also be divided recursively .",
    "more numerical details can be found in  @xcite and @xcite .",
    "we only point out that the eigenvectors can not be computed directly from  . in practice ,",
    "the computed @xmath39 is only an approximation to @xmath9 . to compute the eigenvectors orthogonally  @xcite",
    ", we need to use lwner s theorem  @xcite to recompute the vector @xmath40 as @xmath41 and then use   to compute the eigenvectors .",
    "to be more specific , the matrix @xmath20 is defined as @xmath42 since @xmath43 and @xmath44 are interlacing , see  , @xmath20 is usually off - diagonally low rank , and the ranks of off - diagonal blocks depend on the distribution of @xmath43 and @xmath45 .",
    "we use the following example to show that .",
    "* example 1 . *",
    "assume that a matrix @xmath20 satisfies the eigendecomposition @xmath46 , where @xmath47 , @xmath48 , @xmath49 , @xmath50 , for @xmath51 and @xmath7 is a random normalized vector .",
    "the off - diagonal low rank property of @xmath20 is shown in table  [ tab : ex1-rank ] , which includes the numerical ranks of the submatrices @xmath52 for different @xmath53 with @xmath54 .",
    "the ranks are computed by truncating the singular values less than @xmath55 .",
    ".the ranks of different off - diagonal blocks of @xmath20 [ cols=\"^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     the ranks of the off - diagonal blocks can be estimated by using the approximation theory of function @xmath56 .",
    "the element @xmath57 can be approximated by the sums of exponentials , @xmath58 assume that @xmath59 and @xmath60 belong to two different subintervals of @xmath61 $ ] , @xmath62 , @xmath63 , @xmath64 and @xmath65 .",
    "denote @xmath66 , ( in our case @xmath59 and @xmath60 are the eigenvalues of @xmath67 and @xmath12 respectively , @xmath68 and @xmath69 , the largest eigenvalue of @xmath12 ) , then @xmath70 an approximation error bound is given in  @xcite for the sums of exponentials , @xmath71 where @xmath72 is defined in   and @xmath73 $ ] .",
    "as long as the number of approximation terms satisfies @xmath74 the approximation error of the sum of exponentials is less than @xmath75 , a small constant .",
    "for example , when @xmath76 and @xmath77-@xmath78 , @xmath79-@xmath80 , @xmath81 and the off - diagonal rank estimated by   is 32 , which is close to the result in table  [ tab : ex1-rank ] .",
    "equation   would be used to estimate the rank in algorithm  [ alg : randhss ] .    to keep the orthogonality of @xmath20 ,",
    "@xmath82 must be computed by @xmath83 where @xmath84 ( the distance between @xmath9 and @xmath59 ) , and @xmath85 ( the distance between @xmath9 and @xmath86 ) , which can be returned by calling the lapack routine ` dlaed4 ` . if using fmm to compute the eigenvectors , equation   shows that some modifications of classic fmm are needed since some @xmath59 and @xmath60 may equal in double precision but @xmath87 is not zero .",
    "the hss matrices are a very important type of rank - structured matrices which share the same property that the off - diagonal blocks are low - rank .",
    "other rank - structured matrices include @xmath88-matrix @xcite , @xmath89-matrix @xcite , quasiseparable matrices @xcite , and sequentially semiseparable ( sss ) @xcite matrices .",
    "the hss matrix was first discussed in  @xcite , which can be seen as an algebraic counterpart of fmm in  @xcite . in this paper",
    "we use the hss matrix to accelerate the computation of eigenvectors , and other rank - structured matrices can be similarly used too .",
    "we follow the notation in  @xcite and briefly introduce some key concepts of the hss matrix .",
    "let @xmath90 and @xmath91 be a _",
    "postordered _ binary tree , which means the ordering of a nonleaf node @xmath92 satisfies @xmath93 , where @xmath94 is its left child and @xmath95 is its right child .",
    "each node @xmath92 is associated with a contiguous subset of @xmath96 , @xmath97 , satisfying the following conditions :    * @xmath98 and @xmath99 , for a parent node @xmath92 with left child @xmath94 and right child @xmath95 ; * @xmath100 , where @xmath101 denotes the set of all leaf nodes ; * @xmath102 , @xmath103 denotes the root of @xmath91 .",
    "a block row or column excluding the diagonal block is called an _ hss block row or column _ , denoted by @xmath104 associated with node @xmath92 .",
    "we also simply call them _",
    "hss blocks_. as in  @xcite , the maximum ( numerical ) rank of all the hss blocks is called _",
    "hss rank_.    for each node @xmath92 in @xmath91 , there are matrices @xmath105 , @xmath106 , @xmath107 and @xmath108 associated with it , called _",
    "generators _ , such that @xmath109 for a leaf node @xmath92 , @xmath110 , @xmath111 , @xmath112 .",
    "figure  [ fig : ahss ] shows a @xmath113 hss matrix @xmath114 , and it can be written as @xmath115 and figure  [ fig : hsstree ] shows its corresponding postordering hss tree .    1 .",
    "the generators of a cauchy - like matrix   can be represented by four vectors . while , the generators of an hss matrix are _",
    "matrices_. for an hss matrix , we only need to store the generators @xmath116 , @xmath117 , @xmath118 and @xmath108 , and @xmath105 , @xmath106 and @xmath107 can be constructed hierarchically when needed .",
    "the hss representation   is equivalent to the previous representations in  @xcite , but is simpler ( generators @xmath119 and @xmath120 are not introduced ) . for a parent node @xmath92 , if let @xmath121 , @xmath122 , then reduces to the form in  @xcite , @xmath123{cccc}d_{1 } & u_{1}b_{1}v_{2}^{t } & u_{1}r_{1}b_{3}w_{4}^{t}v_{4}^{t } & u_{1}r_{1}b_{3}w_{5}^{t}v_{5}^{t}\\\\ u_{2}b_{2}v_{1}^{t } & d_{2 } & u_{2}r_{2}b_{3}w_{4}^{t}v_{4}^{t } & u_{2}r_{2}b_{3}w_{5}^{t}v_{5}^{t}\\\\ u_{4}r_{4}b_{6}w_{1}^{t}v_{1}^{t } & u_{4}r_{4}b_{6}w_{2}^{t}v_{2}^{t } & d_{4 } & u_{4}b_{4}v_{5}^{t}\\\\ u_{5}r_{5}b_{6}w_{1}^{t}v_{1}^{t } & u_{5}r_{5}b_{6}w_{2}^{t}v_{2}^{t } & u_{5}b_{5}v_{4}^{t } & d_{5}\\end{array } \\right ] .",
    "\\label{eq : posterhss}\\ ] ]      the procedure of adc algorithms is expressed in the following algorithm , which is similar to the dc algorithm  @xcite .",
    "[ alg : tedc ] compute the whole eigendecomposition of a symmetric tridiagonal matrix by using the adc algorithm . let @xmath124 be a small integer constant .    ` if ` the row dimension of @xmath26 is less than @xmath53    use qr algorithm  @xcite to compute @xmath125 ;    ` return ` @xmath20 and @xmath126 ;    ` else `    form @xmath127 ;    call ` adc`(@xmath128 ) ;    call ` adc`(@xmath129 ) ;    form @xmath130 from @xmath131 ;    ` if ` the size of @xmath12 is small    find the eigenvalues @xmath126 and eigenvectors @xmath132 of @xmath12 ;    compute @xmath133 ;    ` else `    find the eigenvalues @xmath126 of @xmath12 and construct an hss matrix @xmath134 ;    compute @xmath135 via the hss matrix multiplication algorithm ;    ` end if `    ` return ` @xmath20 and",
    "@xmath126 ;    ` end if `    algorithm  [ alg : tedc ] is more like a framework for accelerating the tridiagonal dc algorithm , since the hss matrices can be replaced by other rank - structured matrices such as @xmath88-,@xmath136-matrix .",
    "the difference between algorithm  [ alg : tedc ] and the standard dc algorithm is that algorithm  [ alg : tedc ] uses the hss matrix techniques to update the eigenvectors when the size of matrix @xmath12 is large , to reduce the complexity cost .",
    "if the sizes of secular equations are always small , i.e. , most eigenvalues are computed by deflation , adc is equivalent to the standard dc .",
    "the random hss construction algorithm proposed in  @xcite is built on two low - rank approximation algorithms : _ random sampling _",
    "( rs )  @xcite and _ interpolative decomposition _ ( i d )  @xcite .",
    "the form of i d has appeared in the rank - revealing qr  @xcite and rank - revealing lu factorization  @xcite , and it also has a close relationship to the matrix skeleton and cur factorization  @xcite .",
    "we introduce rs first . for a given @xmath137 matrix @xmath138 with @xmath139",
    ", we want to find a tall matrix @xmath20 with orthogonal columns such that @xmath140 where @xmath75 is a small constant .",
    "the random sampling method right multiplies @xmath138 with a gaussian random matrix @xmath141 , and get a `` compressed '' matrix @xmath142 with much fewer columns , @xmath143 , where @xmath2 is the numerical rank of @xmath138 and @xmath144 is the oversampling parameter , usually @xmath145 or @xmath146 .",
    "then , the matrix @xmath20 can be obtained by applying the rrqr  @xcite or the truncated svd  @xcite to @xmath147 .",
    "it is shown in  @xcite that the rs algorithm computes a good low - rank approximation with quite high probability .",
    "for example , the computed @xmath20 satisfies @xmath148 with probability at least @xmath149  @xcite .    in general , the rank @xmath2 is rarely known in advance . for the symmetric tridiagonal dc algorithm",
    ", we can use   as a guide to estimate @xmath2 .",
    "the i d method computes an approximate low - rank factorization of @xmath138 such that @xmath150 where @xmath151 is a subset of the column indices of @xmath138 , @xmath152 is a @xmath153 matrix with a @xmath154 identity matrix as a submatrix and all its entries are less than one in magnitude , and @xmath155 is a permutation matrix .",
    "a stable and accurate method for computing i d is proposed in  @xcite , similar to the rrqr algorithm in  @xcite .",
    "we can combine rs with i d to get a more efficient low - rank approximation algorithm  @xcite . for",
    "a given @xmath156 matrix @xmath138 , generate an @xmath157 gaussian random matrix @xmath158 as above , and compute the row sampling and column sampling matrices @xmath142 and @xmath159 .",
    "then , use i d to determine the @xmath2 selected rows and columns of @xmath138 from @xmath147 and @xmath160 , @xmath161 = \\texttt{interpolative}(y^t ) , \\quad [ x^{col},j^{col } ] = \\texttt{interpolative}(z^t),\\ ] ] and @xmath138 can be approximated by @xmath162      the main idea is to apply the randomized i d to the row and column sampling matrices by traversing the hss tree level - by - level , from bottom to top . to illustrate it ,",
    "let @xmath114 be a matrix as defined in  , @xmath163 and @xmath164 be the sampling matrices , where @xmath165 is a gaussian random matrix for @xmath166 . to construct an hss matrix ,",
    "we need to find the low - rank approximations of all hss blocks , @xmath167 and @xmath168 .",
    "recall that @xmath167 and @xmath168 are respectively the @xmath92-th hss block row and column , satisfying @xmath169    in this subsection , we show how to obtain the low - rank approximations from @xmath147 and @xmath160 by using the randomized i d method . for a leaf node @xmath92 ,",
    "its compressed hss block row and column are , respectively , @xmath170 where @xmath171 means @xmath172 for @xmath173 and @xmath174 . by applying the",
    "i d method to @xmath175 and @xmath176 , we can easily obtain the low - rank approximations to @xmath167 and @xmath168 , respectively .",
    "for a parent node , its compressed hss blocks can be neatly obtained from those of its children recursively , see section 4.1 in  @xcite and algorithm  [ alg : randhss ] below .",
    "then , its generators can be obtained similarly by applying i d to the compressed hss blocks .",
    "[ alg : randhss ] ( random hss construction for cauchy - like matrices ) given the generators of cauchy - like matrix @xmath114 , compute its hss matrix approximation accurately .",
    "first , use   to estimate the hss rank @xmath2 of @xmath114 and generate two @xmath177 gaussian random matrices @xmath178 and @xmath174 . then , compute @xmath163 and @xmath179 .    `",
    "do ` @xmath180    ` for ` node @xmath92 at level @xmath181    ` if ` @xmath92 is a leaf node ,    1 .",
    "@xmath182 ; 2 .",
    "compute @xmath183 , @xmath184 ; 3 .",
    "compute the i d of @xmath185 , @xmath186 ; 4 .   compute @xmath187 , @xmath188 ;    ` else `    1 .",
    "store the generators @xmath189 , @xmath190 ; 2 .",
    "compute @xmath191 , @xmath192 ; 3 .",
    "compute the i d of @xmath185 , @xmath193 ; 4 .   compute @xmath194 , @xmath195 ;    ` end if `    ` end for `    ` end do `    for the root node @xmath92 , store @xmath189 , @xmath190 .",
    "it can be verified that the complexity of algorithm  [ alg : randhss ] is @xmath196 , where @xmath197 is the cost of multiplying @xmath114 with ( two ) random matrices , and @xmath2 is the hss rank of @xmath114 . in practice , we can let @xmath198 .",
    "fmm can be used to compute the sample matrices @xmath147 and @xmath160 , which only costs @xmath199 flops . for large matrices",
    ", fmm can be much faster than the plain matrix - matrix multiplications . if using fmm , the complexity of rshss is @xmath200 flops , see the reference  @xcite .",
    "this hss construction algorithm in theory can be faster than the algorithm proposed in  @xcite which costs @xmath25 flops .",
    "most time of rshss is taken to compute the sample matrices . in the sequential case it takes about @xmath201 of the construction time and about @xmath202 in the fully parallel case ,",
    "refer to table  [ tab : ex1-speedups ] . in  @xcite",
    ", it is proposed to use the subsampled random fourier ( srft ) or hadamard ( srht ) transforms to compute the sample matrices.we do not use this technique in rshss or adc2 , since the srft would introduce complex matrices , and the srht requires the dimension of matrix @xmath114 to be powers of two .",
    "furthermore , the construction algorithm is usually much faster than the hss matrix multiplication algorithm , see the results in table  [ tab : ex1-speedups ] .",
    "note that if the srht is applicable , the complexity of rshss is also about @xmath22 flops .",
    "another issue we want to mention is the accuracy of rshss .",
    "if the singular values of hss blocks do not decay rapidly , the rs method may lose a bit of accuracy .",
    "a power scheme was proposed to improve the quality of sample matrices in  @xcite , for instance compute @xmath203 .",
    "we find it is very difficult to incorporate this technique into algorithm  [ alg : randhss ] and moreover , using the power scheme would require about @xmath204 times as many operations as algorithm  [ alg : randhss ] .",
    "for accuracy , we choose a relatively large oversampling parameter @xmath144 and try to compute the i d of sampled matrices as accurately as possible . in practice , we let @xmath77-@xmath205 in   to estimate the rank , and let the oversampling parameter @xmath206 .",
    "the number of used random vectors is usually larger than the hss rank .",
    "this strategy in practice is quite robust and it does not fail for any experiments during all our tests .",
    "note that rshss still has a risk of losing accuracy , for example , if @xmath207 is smaller than the hss rank in some rare cases .",
    "the adc algorithm is consisted of three other algorithms : _ the hss construction _ and _ hss matrix multiplication algorithms _ , and the standard dc algorithm . almost all modern cpus have multiple cores , and we implement the adc algorithms in parallel to exploit the multicore architecture",
    ". we use openmp to implement these algortihms .",
    "this section introduces the parallel implementation details of these three algorithms .      as illustrated in algorithm",
    "[ alg : randhss ] and section 4.1 of  @xcite , the computations for different nodes at the same level can be performed simultaneously .",
    "we can exploit the parallelism of the hss tree , and the computations for different nodes are done by different _",
    "processes_. furthermore , the work for each node can also be done by _ multi - threads _ by calling a multithreaded blas library .",
    "recall that algorithm  [ alg : randhss ] computes three or four generators for each node , @xmath116 , @xmath117 , @xmath118 and @xmath108 .",
    "note that parent nodes do not have the generator @xmath116 , and @xmath117 is computed from the row compression , and @xmath118 is from the column compression .",
    "the generators @xmath116 and @xmath108 are submatrices of the original matrix @xmath114 , and are cauchy - like .",
    "besides the number of flops , the running time of algorithms is also determined by the amount of data movements . to have good data locality , we store the same type of generators for nodes at the same level continuously . for example",
    ", we first store all the generators @xmath117 at level @xmath181 , then the generators @xmath118 and finally the generators @xmath108 at level @xmath181 , for @xmath208 .",
    "all the generators are stored continuously in one array , name it @xmath209 , and the generators @xmath116 are stored in the front part of @xmath209 .",
    "this form of storage is good for hss matrix multiplications , see algorithm  [ alg : phssmm - omp ] below , where the computations follow the hss tree level by level , and the generators of the same type are used one after the other .",
    "for example , the computations of   use all the generators @xmath116 at the bottom level , and so do the generators @xmath118 .",
    "another point we want to mention is that the cauchy - like matrices @xmath116 and @xmath108 are computed from its generators respectively , which are four vectors , see equation  .",
    "we find that recomputing the entries of @xmath116 and @xmath108 is usually faster than subtracting them from the original matrix @xmath114 .",
    "our parallel version of rshss is similar to algorithm  [ alg : randhss ] .",
    "the only difference is that the ` do - loop ` in algorithm  [ alg : randhss ] is replaced by the following process after some computation details are ignored .",
    "` par_for ` leaf node @xmath92 ,    compute the cauchy - like matrix @xmath116 via its generators and store it in @xmath209 ;    ` end par_for `    ` do ` @xmath180    ` par_for ` node @xmath92 at level @xmath181 , compute its generator @xmath117 from @xmath175 and store @xmath117 in @xmath209 ; ` end par_for `    ` par_for ` node @xmath92 at level @xmath181 , compute its generator @xmath118 from @xmath176 and store @xmath118 in @xmath209 ; ` end par_for `    ` par_for ` node @xmath92 at level @xmath181 , compute the cauchy - like matrix @xmath108 and store it in @xmath209 ; ` end par_for `    ` end do `    the abbreviation ` par_for ` stands for ` parallel for ' , which means the following computations can be done in parallel . in practice",
    "we use   to estimate the hss rank @xmath2 , based on the partition of the original matrix @xmath114 , see figure  [ fig : ahss ] .",
    "the matrix @xmath114 is partitioned by letting all leaf nodes have roughly @xmath53 rows and columns . since",
    "@xmath43 and @xmath45 are ordered increasingly , each partition of @xmath20   can also be seen as a partition of interval @xmath61 $ ] which contains both @xmath43 and @xmath45 . for the partition in figure  [ fig : ahss ] ,",
    "the interval @xmath61 $ ] is divided into four segments , and the first @xmath210 entries of @xmath43 and @xmath45 lie in the first segment of @xmath61 $ ] , the second @xmath211 entries lie in the second segment , and so on .",
    "the rank estimated by   depends on the distance of two segments , which is defined in section  [ sec : lowrank ] .",
    "we use the distances of neighbouring segments to estimate rank , and choose the _ maximum _ rank estimated by   as the hss rank . for figure",
    "[ fig : ahss ] , there are _ three _ pairs of neighbouring segments , and the estimated ranks are of @xmath212 , @xmath213 and @xmath214 respectively , and the maximum of them is used as an estimate of hss rank @xmath2 .",
    "if some eigenvalues are clustered , i.e. , the distance between @xmath215 and @xmath216 is small , the estimated rank by   may be too large to be useful .",
    "we use the following tricks to get a more reasonable estimate of @xmath2 .    1 .",
    "if the distance between @xmath215 and @xmath216 is too small , we modify the partition of matrix @xmath114 , i.e. , move the boundary forward or backward to let the distance large . in our implementation",
    ", we modify the partition when the distance between @xmath215 and @xmath216 is less than @xmath217 , and the boundary is moved forward or backward by at most @xmath31(=5 ) rows and columns .",
    "if the computed rank by   is still too large , larger than 100 , we fix the rank to be 100 .",
    "( we find that hss rank @xmath2 is rarely larger than 100 in the tridiagonal dc algorithm . )",
    "note that these techniques are unfortunately lack of theoretical support , but they make the rank estimation method more useful and robust .",
    "after an hss matrix is represented in its hss form , there exist fast algorithms for multiplying it with a vector in @xmath218 flops ( see  @xcite ) .",
    "an hss matrix multiplication algorithm has been introduced in  @xcite for @xmath219 , where @xmath220 is an hss matrix and @xmath114 is a general matrix . for completeness",
    ", this subsection introduces the process of multiplying an hss matrix with a general matrix from right , i.e. , compute @xmath221 . from algorithm  [ alg : phssmm - omp ] it is easy to see that the hss matrix multiplication algorithms are naturally parallelizable .",
    "[ alg : phssmm - omp ] assume that the hss tree @xmath91 is a full binary tree and there are @xmath222 levels , the root is at level 0 and the leaf nodes are at level @xmath223 .",
    "let @xmath224 be the sibling of @xmath92 .",
    "let @xmath225 be a @xmath226 matrix and partition the columns of @xmath225 as @xmath227 $ ] , where @xmath228 , @xmath229 is a leaf node .    `",
    "upsweep for ` @xmath230    ` par_for ` @xmath92 at the bottom level , compute @xmath231 ; ` end par_for `    ` for ` @xmath232    ` par_for ` @xmath92 at level @xmath181 , compute @xmath233 ; ` end par_for `    ` end for `    ` downsweep for ` @xmath234    ` par_for ` @xmath92 at the second top level , compute @xmath235 ` end par_for `    ` for ` @xmath236    ` par_for ` @xmath92 at level @xmath181 , compute @xmath237 @xmath238 ` end par_for `    ` end for `    compute @xmath225    * ` par_for ` @xmath92 at the bottom level , compute @xmath239 * ` end par_for `    all the computations for the nodes at the same level are independent of each other .",
    "furthermore , almost all the operations are matrix - matrix multiplications and we can take advantage of the highly optimized routine ` dgemm ` in mkl .",
    "we explore both the parallelism in the hss tree and the parallelism from the blas operations by using mkl .",
    "table  [ tab : hss_parallel ] shows the speedups of algorithm  [ alg : phssmm - omp ] when only exploiting the parallelism in the hss tree .",
    "the dimension of the hss matrix is 10000 , which is defined in the same way as the matrix @xmath20 in example 1 , and we multiple it with a @xmath240 random matrix via algorithm  [ alg : phssmm - omp ] . the times cost by algorithm  [ alg : phssmm - omp ] are presented in the third row of table  [ tab : hss_parallel ] , and the compiled codes are linked to a sequential blas library .",
    "the results in table  [ tab : hss_parallel ] show that the scalability of algorithm  [ alg : phssmm - omp ] is good .",
    "some more numerical results are included in example 2 in section  [ sec : numer - tedc ] .",
    "[ c]|c|c|c|c|c|c|c|c|c|c| & + & @xmath241 & @xmath80 & @xmath242 & @xmath243 & @xmath244 & @xmath245 & @xmath78 & @xmath246 & @xmath205 + time(s ) & 11.34 & 4.06 & 2.63 & 2.07 & 1.73 & 1.51 & 1.40 & 1.31 & 1.20 + speedups & 1.00 & 2.79 & 4.31 & 5.48 & 6.55 & 7.51 & 8.10 & 8.66 & 9.45 +      the lapack routine ` dstevd ` implements a divide - and - conquer algorithm for symmetric tridiagonal matrices .",
    "it computes the eigenvalues and eigenvectors explicitly by calling ` dlaed0 ` .",
    "the routine ` dlaed0 ` solves each subproblem in a divide - and - conquer way . `",
    "dlaed1 ` called by ` dlaed0 ` computes the eigendecomposition of the merged subproblem , and it calls ` dlaed2 ` to deflate a diagonal matrix with rank - one modification and calls ` dlaed3 ` to update the eigenvector matrix via matrix - matrix multiplications .",
    "our implementation has the same structure as lapack .",
    "we add the hss techniques in the routine ` dlaed1 ` and rename it ` mdlaed1 ` .",
    "when the size of the deflated matrix is small , it calls ` dlaed3 ` as usual .",
    "otherwise , it calls ` mdlaed3 ` to compute the eigenvalues and update the eigenvectors . in our implementation",
    ", we use the hss matrix techniques when the size of the deflated matrix is larger than 2000 .",
    "the routine ` mdlaed3 ` is similar to ` dlaed3 ` , and it computes the eigenvalues @xmath45 , the recomputed vector @xmath247 , @xmath84 ( the distance between @xmath9 and @xmath59 ) , and @xmath85 ( the distance between @xmath9 and @xmath86 ) , for @xmath248 . the secular equation in ` mdlaed3 ` is solved in parallel by calling ` dlaed4 ` .",
    "then the eigenvector matrix of the diagonal matrix with rank - one modification is approximated by an hss matrix , and the eigenvectors of the original matrix @xmath26 are updated via fast hss matrix multiplications  @xcite , see also algorithm  [ alg : phssmm - omp ] . using the hss matrix techniques to update the eigenvectors",
    "saves a lot of flops since the complexity is reduced from @xmath4 to @xmath0 .",
    "the divide - and - conquer algorithm is also organized in a binary tree structure .",
    "figure  [ fig : comput_tree ] shows the tree structure of dc algorithm .",
    "a big problem is recursivley splitted into two small problems , and two small problems are merged together into a big one .",
    "the subproblems at the same level of dc tree can be solved in parallel .",
    "the subproblems at the bottom level are solved by using the qr algorithm in parallel , calling the lapack routine ` dlasdq ` in our implementation . for the problems at the other levels ,",
    "we had tried to use the nested parallelism of openmp to exploit both the parallelism of dc tree and blas operations , but it did not give us any speedup increases . furthermore ,",
    "if using nested parallel computing , each thread would require a lot of private memory to store the intermediate eigenvectors , and the memory cost would be greatly increased .",
    "therefore , the problems above the bottom level of dc tree are solved sequentially and we only exploit the parallelism of blas operations and hss techniques .    as is well known , solving the secular equations costs about @xmath249 operations .",
    "we further parallelize the process of solving the secular equations , which is inspired by the work  @xcite .",
    "we simply add omp parallel do directives in ` mdlaed3 ` when calling ` dlaed4 ` .",
    "our parallel implementation is simpler than that in  @xcite . in this paper",
    "we use openmp and follow the fork - join model . while , it followed a task - flow model in  @xcite and used a dynamic runtime system to schedule the tasks . comparing the numerical results in  @xcite with those in the next section",
    ", we can see that the rank - structured matrix techniques is good for the case that there are few deflations and that the task - flow model used in  @xcite is good for the case that there are a lot of deflations .",
    "the advantage of the task - flow model is that it introduces a huge level of parallelism through a fine task granularity , and that the tasks are scheduled by a runtime system , some synchronization barriers are removed .",
    "as the algorithm in  @xcite still uses plain matrix - matrix multiplications to update the eigenvectors , when the deflations are few , its the advantage decreases as the dimensions of matrices increase .",
    "therefore , a good research direction is to combine the rank - structured matrix techniques with the task - flow model .",
    "all the results are obtained on a server with 128 gb memory and an intel(r ) xeon(r ) cpu e5 - 2670 , which has two sockets , 8 cores per socket , and 16 cores in total .",
    "the codes are written in fortran 90 .",
    "for compilation we used intel fortran compiler ( ` ifort ` ) and the optimization flag ` -o2 -openmp ` , and then linked the codes to intel mkl ( composer_xe_2013.0.079 )",
    ". * example 2 . * in this example , we use the matrix defined in example 1 to show the scalability of the hss construction and matrix multiplication algorithms when implemented in parallel by using multi - threading .",
    "the dimension of this matrix is 10000 .",
    "the row dimensions of hss blocks for the leaf nodes are around @xmath250 .",
    "the scalability of the hss construction algorithm based on srrsc and rshss are tested , and the results are shown in table  [ tab : ex1-speedups ] .",
    "the results for algorithm  [ alg : phssmm - omp ] are also included .",
    "the elapsed times of hss constructions are shown in the rows denoted by ` const ` , and the times of hss multiplications are included in those denoted by ` mult ` .",
    "the row denoted by ` dgemm ` in table  [ tab : ex1-speedups ] shows the times of computing the sample matrices @xmath147 and @xmath160 .",
    "the results in table  [ tab : ex1-speedups ] are obtained by letting ` omp_num_threads ` and ` mkl_num_threads ` equal to @xmath251 and @xmath205 . from the results we can see that the hss construction algorithm is usually faster than the hss multiplication algorithm . for the rshss algorithm",
    ", we let @xmath144 equal to @xmath252 and the estimated rank by   is 79 which is larger than 57 , the hss rank computed by srrsc .",
    "most ranks of the hss blocks are around 40 .",
    "the hss matrix multiplications for rshss is slower than those for srrsc , since the ranks of hss blocks computed by rshss are usually larger than those computed by srrsc .",
    "from the results in table  [ tab : ex1-speedups ] , we can see that our parallel implementation achieves good speedups .",
    "[ c]|c|c|c|c|c|c|c|c|c|c|c| & + & @xmath241 & @xmath80 & @xmath242 & @xmath243 & @xmath244 & @xmath245 & @xmath78 & @xmath246 & @xmath205 + & const & 2.29 & 0.84 & 0.52 & 0.41 & 0.34 & 0.33 & 0.32 & 0.31 & 0.31 + & mult & 4.75 & 1.87 & 1.32 & 1.08 & 0.96 & 0.97 & 0.89 & 0.85 & 0.85 + & dgemm & 2.24 & 0.76 & 0.46 & 0.34 & 0.30 & 0.25 & 0.21 & 0.19 & 0.16 + & const & 2.85 & 1.07 & 0.75 & 0.63 & 0.57 & 0.53 & 0.48 & 0.45 & 0.42 + & mult & 8.04 & 2.98 & 1.94 & 1.54 & 1.29 & 1.14 & 1.09 & 1.02 & 0.95 +    * example 3 . * for several classes of matrices  @xcite , few or no eigenvalues are deflated in the dc algorithm .",
    "some of such matrices include the clement - type , legendre - type , laguerre - type , hermite - type and toeplitz - type matrices , which are defined as follows .",
    "we use these matrices to show the performance of the adc algorithms .",
    "the clement - type matrix  @xcite is given by @xmath253 where the off - diagonal entries are @xmath254 .",
    "the legendre - type matrix is defined as  @xcite , @xmath255 where the off - diagonal entries are @xmath256 .",
    "the laguerre - type matrix is defined as  @xcite , @xmath257    the hermite - type matrix is given as  @xcite , @xmath258    the toeplitz - type matrix is a symmetric tridiagonal matrix with diagonals 2 and off - diagonal entries 1 . in this example , we compare adcs with dc in intel mkl with ` omp_num_threads`=16 .",
    "the speedups of adc1 and adc2 over dc are similar , and the results are respectively reported in table  [ tab : ex2-srrsc ] and table  [ tab : ex2-rhss ] , see also figure  [ fig : speedup ] . since adcs require fewer flops than the standard dc , they can achieve even better speedups for larger matrices . for example , when the dimension of toeplitz - type matrix increases from @xmath259 to @xmath260 , the speedup of adc2 over dc increases from @xmath261 to @xmath262 . during our experiments ,",
    "hss techniques are only used when the size of the current secular equation is larger than 2000 , which is a parameter depending on the computer architecture , compiler and the optimized blas library .",
    "the row dimensions of the hss blocks for the leaf nodes are also around 200 , and 16 threads are used .",
    "[ c]|c|ccccccccc| & + & @xmath263 & @xmath264 & @xmath265 & @xmath266 & @xmath267 & @xmath268 & @xmath269 & @xmath270 & @xmath259 + clement & 1.32x & 1.76x & 2.22x & 2.80x & 3.10x & 3.57x & 3.95x & 4.31x & 5.03x + legendre & 1.99x & 2.11x & 2.69x & 3.27x & 3.52x & 3.92x & 4.35x & 4.79x & 4.81x + laguerre & 1.64x & 2.28x & 2.53x & 2.82x & 3.32x & 3.71x & 4.11x & 4.32x & 5.00x + hermite & 2.00x & 2.23x & 2.47x & 2.99x & 3.35x & 3.68x & 4.07x & 4.52x & 5.08x + toeplitz & 1.84x & 2.02x & 2.69x & 3.22x & 3.66x & 4.19x & 5.02x & 5.26x & 6.05x +    the dc algorithm is relatively complex , there are deflations and the secular equations are solved via iterative methods , and it is difficult to count the total number of flops cost by dc or adcs by hand .",
    "we use some tools based on event based sampling ( ebs ) technology to estimate the floating point operations .",
    "papi  @xcite and intel vtune amplifier xe  @xcite are popular performance analysis tools , which can make reasonable estimates .",
    "figure  [ fig : flops ] shows the comparisions of flops costed by adc2 and dc in mkl .",
    "these results are for the toeplitz - type matrices with different dimensions . from it",
    "we can see that dc requires nearly @xmath4 flops , while adc2 requires much fewer flops .",
    "figure  [ fig : maxerr ] and  [ fig : relerr ] shows the maximum errors and maximum relative errors of the eigenvalues computed by adc1 compared with those by dc , respectively .",
    "from the results , we can see that the computed eigenvalues by adc1 nearly have the same accuracy as those computed by mkl .",
    "note that the results for relative error are included here but the dc algorithms in general are not guaranteed to have high relative accuracy .",
    "[ c]|c|ccccccccc| & + & @xmath263 & @xmath264 & @xmath265 & @xmath266 & @xmath267 & @xmath268 & @xmath269 & @xmath270 & @xmath259 + clement & 2.05x & 2.08x & 2.54x & 3.01x & 3.41x & 3.87x & 4.26x & 4.71x & 5.12x + legendre & 2.27x & 2.37x & 2.65x & 3.25x & 3.45x & 3.84x & 4.06x & 4.68x & 4.72x + laguerre & 1.89x & 2.48x & 2.56x & 3.01x & 3.28x & 3.79x & 4.06x & 4.46x & 4.68x + hermite & 2.37x & 2.58x & 2.68x & 3.33x & 3.46x & 3.82x & 4.16x & 4.33x & 5.04x + toeplitz & 2.23x & 2.31x & 2.97x & 3.29x & 3.85x & 4.27x & 5.07x & 5.08x & 5.79x +    the results for the orthogonality of the computed eigenvectors are shown in figure  [ fig : orth - adc2 ] , which are defined as @xmath271 .",
    "figure  [ fig : back - adc2 ] shows the results for the backward error of adc2 , computed as @xmath272 . while , adc2 is a little less accurate than adc1 but adc2 can also be used reliably for applications , the orthogonality of the computed eigenvectors by adc2 is about 1@xmath273 - 12 and the maximum error of the computed eigenvalues by adc2 compared with those by intel mkl is about 1@xmath273 - 14 .",
    "one advantage of adc2 over adc1 is that it requires fewer flops when fmm or srht is applicable , which will be done in the future work .",
    "we further use all the matrices in the lapack ` stetester `  @xcite with dimensions larger than 1000 to test adc2 .",
    "figure  [ fig : lapack - tester ] shows the speedups of adc2 over dc in mkl and the relative errors of the eigenvalues computed by adc2 compared with those by mkl .",
    "the results show that for almost all matrices adc2 is faster than the dc implmentation in mkl and that the computed eigenvalues are highly accurate compared with those computed by dc in mkl .",
    "the experiments are done by letting ` omp_num_threads ` and ` mkl_num_threads ` equal to @xmath205 .",
    "for some rare matrices adc2 is a little slower than ` dstevd ` in mkl but never slower by more than @xmath274e-@xmath275 seconds .",
    "note that the hss techniques are only used when the size of secular equation is larger than 2000 . for the matrices with dimensions from 1000 to 2000 ,",
    "the speedups of adc2 over dc in mkl is due to that adc2 computes the bottom subproblems of the dc tree and the secular equations in parallel .",
    "in this paper , two accelerated tridiagonal dc algorithms are proposed by using the hss matrix techniques .",
    "one uses srrsc for the hss construction and the other uses a randomized hss construction algorithm which is first introduced in  @xcite . for the later one , we propose a method to estimate the hss rank by using the function approximation theory .",
    "the main point is using the rank - structured matrix techniques to update the eigenvectors . roughly speaking",
    ", the worst case complexity of adcs is reduced to @xmath0 for an @xmath276 symmetric tridiagonal matrix instead of @xmath4 , where @xmath2 is a modest number which depends on the property of the tridiagonal matrix .",
    "we implement adcs in parallel including the hss construction and hss matrix multiplication algorithms , and compare them with the multithreaded intel mkl library . for some matrices of large dimensions with few deflations , our adc algorithms can be more than 6x times faster than the dc algorithm in mkl .",
    "the authors would like to thank ming gu for valuable suggestions and ziyang mao , lihua chi , yihui yan , xu han , xinbiao gan and qingfeng hu for some helpful discussions .",
    "the authors also thank the referee for their valuable comments which greatly improve the presentation of this paper .",
    "this work is partial supported by national natural science foundation of china ( no . 11401580 , 611330005 and 91430218 ) , and 863 program of china under grant 2012aa01a301 ."
  ],
  "abstract_text": [
    "<S> in this paper , two accelerated divide - and - conquer algorithms are proposed for the symmetric tridiagonal eigenvalue problem , which cost @xmath0 flops in the worst case , where @xmath1 is the dimension of the matrix and @xmath2 is a modest number depending on the distribution of eigenvalues . </S>",
    "<S> both of these algorithms use hierarchically semiseparable ( hss ) matrices to approximate some intermediate eigenvector matrices which are cauchy - like matrices and are off - diagonally low - rank . </S>",
    "<S> the difference of these two versions lies in using different hss construction algorithms , one ( denoted by adc1 ) uses a structured low - rank approximation method and the other ( adc2 ) uses a randomized hss construction algorithm . for the adc2 algorithm , </S>",
    "<S> a method is proposed to estimate the off - diagonal rank . </S>",
    "<S> numerous experiments have been done to show their stability and efficiency . </S>",
    "<S> these algorithms are implemented in parallel in a shared memory environment , and some parallel implementation details are included . comparing the adcs with highly optimized multithreaded libraries such as intel mkl , we find that adcs could be more than 6x times faster for some large matrices with few deflations . </S>"
  ]
}