{
  "article_text": [
    "ambiguity is the most specific feature of natural languages , which sets them aside from programming languages , and which is at the root of the difficulty of the parsing enterprise , pervading languages at all levels : lexical , morphological , syntactic , semantic and pragmatic .",
    "unless clever techniques are developed to deal with ambiguity , the number of possible parses for an average sentence ( 20 words ) is simply intractable . in the case of prepositional phrases ,",
    "the expansion of the number of possible analysis is the catalan number series , thus the number of possible analyses grows with a function that is exponential in the number of prepositional phrase @xcite .",
    "one of the most interesting topics of debate at the moment , is the use of frequency information for automatic syntactic disambiguation .",
    "as argued in many pieces of work in the ai tradition @xcite , the exact solution of the disambiguation problem requires complex reasoning and high level syntactic and semantic knowledge .",
    "however , current work in part - of - speech tagging has succeeded in showing that it is possible to carve one particular subproblem and solve it _ by approximation _  using statistical techniques  independently of the other levels of computation .",
    "in this paper we consider the problem of prepositional phrase ( pp ) ambiguity .",
    "while there have been a number of recent studies concerning the use of statistical techniques for resolving single pp attachments , i.e. in constructions of the form [ v np pp ] , we are unaware of published work which applies these techniques to the more general , and pathological , problem of multiple pps , e.g. [ v np pp1 pp2 ... ] . in particular , the multiple pp attachment problem results in sparser data which must be used to resolve greater ambiguity : a strong test for any probabilistic approach .    we begin with an overview of techniques which have been used for pp attachment disambiguation , and then consider how one of the most successful of these , the backed - off estimation technique , can be applied to the general problem of multiple pp attachment .",
    "attempts to resolve the problem of pp attachment in computational linguistics are numerous , but the problem is hard and success rate typically depends on the domain of application . historically , the shift from attempts to resolve the problem completely , by using heuristics developed using typical ai techniques @xcite has left the place for attempts to solve the problem by less expensive means , even if only approximately . as shown by many psycholinguistic and practical studies @xcite ,",
    "lexical information is one of the main cues to pp attachment disambiguation .    in one of the earliest attempts to resolve the problem of pp attachment ambiguity using lexical measures , hindle and rooth",
    "show that a measure of mutual information limited to lexical association can correctly resolve 80% of the cases of pp attachment ambiguity , confirming the initial hypothesis that lexical information , in particular co - occurrence frequency , is central in determining the choice of attachment .",
    "the same conclusion is reached by brill and resnik .",
    "they apply transformation - based learning @xcite to the problem of learning different patterns of pp attachment . after acquiring 471 patterns of pp attachment ,",
    "the parser can correctly resolve approximately 80% of the ambiguity .",
    "if word classes @xcite are taken into account , only 266 rules are needed to perform at 80% accuracy .",
    "magerman and marcus report 54/55 correct pp attachments for pearl , a probabilistic chart parser , with earley style prediction , that integrates lexical co - occurrence knowledge into a probabilistic context - free grammar .",
    "the probabilities of the rules are conditioned on the parent rule and on the trigram centered at the first input symbol that would be covered by the rule . even if the parser has been tested only in the direction giving domain",
    ", where the behaviour of prepositions is very consistent , it shows that a mixture of lexical and structural information is needed to solve the problem successfully .",
    "collins and brooks propose a 4-gram model for pp disambiguation which exploits backed - off estimation to smooth null events ( see next section ) .",
    "their model achieves 84.5% accuracy .",
    "the authors point out that prepositions are the most informative element in the tuple , and that taking low frequency events into account improves performance by several percentage points . in other words , in solving the pp attachment problem , backing - off is not advantageous unless the tuple that is being tested is not present in the training set ( it has zero counts ) .",
    "moreover , tuples that contain prepositions are the most informative .    the second result is roughly confirmed by brill and resnik , ( ignoring the importance of n2 when it is a temporal modifier , such as _ yesterday , today _ ) . in their work , the top 20 transformations learned are primarily based on specific prepositions .",
    "the pp attachment model presented by collins and brooks determines the most likely attachment for a particular prepositional phrase by estimating the probability of the attachment .",
    "we let @xmath0 represent the attachment event , where @xmath1 indicates that the pp attaches to the verb , and @xmath2 indicates attachment to the object np .",
    "the attachment is conditioned by the relevant head words , a 4-gram , of the vp .",
    "* tuple format : ( @xmath0 , v , n1 , p , n2 ) * so : _ john read [ [ the article ] [ about the budget ] ] _ * is encoded as : ( 2 , read , article , about , budget ) +    using a simple maximal likelihood approach , the best attachment for a particular input tuple ( v , n1,p , n2 ) can now be determined from the training data via the following equation :    @xmath3    here @xmath4 denotes the frequency with which a particular tuple occurs .",
    "thus , we can estimate the probability for each configuration @xmath5 , by counting the number of times the four head words were observed in that configuration , and dividing it by the total number of times the 4-tuple appeared in the training set .    while the above equation is perfectly valid in theory , sparse data means it is rather less useful in practice .",
    "that is , for a particular sentence containing a pp attachment ambiguity , it is very likely that we will never have seen the precise ( v , n1,p , n2 ) quadruple before in the training data , or that we will have only seen it rarely . to address this problem ,",
    "they employ backed - off estimation when zero counts occur in the training data .",
    "thus if @xmath6 is zero , they ` back - off ' to an alternative estimation of @xmath7 which relies on 3-tuples rather than 4-tuples :    @xmath8    similarly , if no 3-tuples exist in the training data , they back - off further :    @xmath9    @xmath10    the above equations incorporate the proposal by collins and brooks that only tuples including the preposition should be considered , following their results that the preposition is the most informative lexical item . using this technique , collins and brooks achieve an overall accuracy of 84.5% .",
    "previous work has focussed on the problem of single pp attachment , in configurations of the form [ v np pp ] where both the np and the pp are assumed to be attached within the vp .",
    "the algorithm presented in the previous section , for example , simply determines the maximally likely attachment event ( to np or vp ) based on the supervised training provided by a parsed corpus .",
    "the broader value of this approach , however , remains suspect until it can be demonstrated to apply more generally .",
    "we now consider how this approach  and the use of lexical statistics in general  might be naturally extended to handle the more difficult problem of multiple pp attachment .",
    "in particular , we investigate the pp attachment problem in cases containing two pps , [ v np pp1 pp2 ] , and three pps , [ v np pp1 pp2 pp3 ] , with a view to determining whether n - gram based parse disambiguation models which use the backed - off estimate can be usefully applied .",
    "multiple pp attachment presents two challenges to the approach :    1 .   for a single pp ,",
    "the model must make a choice between two structures .",
    "for multiple pps , the space of possible structural configurations increases dramatically , placing increased demands on the disambiguation technique .",
    "2 .   multiple pp structures are less frequent , and contain more words , than single pp structures .",
    "this substantially increases the sparse data problems when compared with the single pp attachment case .      to carry out the investigation , training and test data were obtained from the penn tree - bank , using the tgrep tools to extract tuples for 1-pp , 2-pp , and 3-pp cases . for the single pp study ,",
    "vp attachment was coded as 1 and np attachment was coded as 2 .",
    "a database of quadruples of the form _",
    "( configuration , v , n , p ) _ was then created .",
    "the table below shows the two configurations and their frequencies in the corpus .    [ cols=\"<,<,>\",options=\"header \" , ]",
    "the backed - off estimate has been demonstrated to work successfully for single pp attachment , but the sparse data problem renders it impractical for use in more complex constructions such as multiple pp attachment ; there are too many configurations , too many head words , too few training examples . in this paper",
    "we have demonstrated , however , that the relatively rich training data obtained for the first preposition can be exploited in attaching subsequent pps .",
    "the algorithm incrementally fixes each preposition into the configuration and the more informative pp1 training data is exploited to settle the competition for possible attachments for each subsequent preposition .",
    "performance is considerably better than both chance and the naive baseline technique .",
    "the generalized backed - off estimation approach which we have presented constitutes a practical solution to the problem of multiple pp disambiguation .",
    "this further suggests that backed - off estimation may be successfully integrated into more general syntactic disambiguation systems .",
    "we gratefully acknowledge the support of the british council and the swiss national science foundation on grant 83bc044708 to the first two authors , and on grant 12 - 43283.95 and fellowship 8210 - 46569 from the swiss nsf to the first author .",
    "we thank the audiences at edinburgh and pennsylvania for their useful comments .",
    "all errors remain our responsibility .",
    "stephen crain and mark steedman .",
    "1985 . on not being led up the garden path : the use of context by the psychological parser . in david  r. dowty , lauri kartunnen , and arnold  m. zwicky , editors , _ natural language processing : psychological , computational , and theoretical perspectives _ , pages 320358 .",
    "cambridge university press , cambridge .",
    "marylin ford , joan bresnan , and ron kaplan .",
    "1982 . a competence - based theory of syntactic closure . in joan bresnan , editor , _ the mental representations of grammatical relations",
    ", pages 727796 . mit press , cambridge , ma .",
    "greg whittemore , kathleen ferrara , and hans brunner . 1990 . empirical study of predictive power of simple attachment schemes for post - modifiers prepositional phrases .",
    "pages 2330 , pittsburgh , pa .",
    "association for computational linguistics ."
  ],
  "abstract_text": [
    "<S> there has recently been considerable interest in the use of lexically - based statistical techniques to resolve prepositional phrase attachments . to our knowledge , however , these investigations have only considered the problem of attaching the first pp , i.e. , in a [ v np pp ] configuration . in this paper , we consider one technique which has been successfully applied to this problem , backed - off estimation , and demonstrate how it can be extended to deal with the problem of multiple pp attachment . </S>",
    "<S> the multiple pp attachment introduces two related problems : sparser data ( since multiple pps are naturally rarer ) , and greater syntactic ambiguity ( more attachment configurations which must be distinguished ) . we present and algorithm which solves this problem through re - use of the relatively rich data obtained from first pp training , in resolving subsequent pp attachments . </S>"
  ]
}