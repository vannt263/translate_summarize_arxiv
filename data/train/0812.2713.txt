{
  "article_text": [
    "the location of a @xmath2 source is very important to study galactic supernova explosion .",
    "the determination of neutrino incoming direction can be used to locate a supernova , especially , if the supernova is not optically visible .",
    "the method based on the inverse @xmath3 decay , @xmath4 , has been discussed in the ref.@xcite .",
    "the method can be applied to determine a reactor neutrino direction and a supernova neutrino direction .",
    "but the uncertainty of location of the @xmath2 source attainable by using the method is not small enough and almost 2 times as large as that in the super - kamiokande experiment(sk ) .",
    "so we try to apply the bayesian neural network(bnn)@xcite to locate @xmath2 sources in order to decrease the uncertainty on the measurement of the neutrino incoming direction .",
    "bnn is an algorithm of the neural networks trained by bayesian statistics .",
    "it is not only a non - linear function as neural networks , but also controls model complexity .",
    "so its flexibility makes it possible to discover more general relationships in data than the traditional statistical methods and its preferring simple models make it possible to solve the over - fitting problem better than the general neural networks@xcite .",
    "bnn has been used to particle identification and event reconstruction in the experiments of the high energy physics , such as ref.@xcite .    in this paper , it is discussed by using monte - carlo simulation that the method of bnn is applied to determine neutrino incoming direction in reactor neutrino experiments and supernova explosion location by scintillator detectors .",
    "the idea of bnn is to regard the process of training a neural network as a bayesian inference .",
    "bayes theorem is used to assign a posterior density to each point , @xmath5 , in the parameter space of the neural networks .",
    "each point @xmath5 denotes a neural network . in the method of bnn ,",
    "one performs a weighted average over all points in the parameter space of the neural network , that is , all neural networks .",
    "the methods make use of training data \\{(@xmath6,@xmath7 ) , ( @xmath8,@xmath9), ... ,(@xmath10,@xmath11 ) } , where @xmath12 is the known target value associated with data @xmath13 , which has @xmath14 components if there are @xmath14 input values in the regression .",
    "that is the set of data @xmath15(@xmath6,@xmath8, ...",
    ",@xmath10 ) which corresponds to the set of target @xmath16(@xmath7,@xmath9, ...",
    ",@xmath11 ) .",
    "the posterior density assigned to the point @xmath5 , that is , to a neural network , is given by bayes theorem    @xmath17    where data @xmath18 do not depend on @xmath5 , so @xmath19 .",
    "we need the likelihood @xmath20 and the prior density @xmath21 , in order to assign the posterior density @xmath22to a neural network defined by the point @xmath5 .",
    "@xmath23 is called evidence and plays the role of a normalizing constant , so we ignore the evidence . that is ,    @xmath24    we consider a class of neural networks defined by the function    @xmath25    the neural networks have @xmath14 inputs , a single hidden layer of @xmath26 hidden nodes and one output . in the particular bnn",
    "described here , each neural network has the same structure .",
    "the parameter @xmath27 and @xmath28 are called the weights and @xmath29 and @xmath30 are called the biases . both sets of parameters",
    "are generally referred to collectively as the weights of the bnn , @xmath5 .",
    "@xmath31 is the predicted target value .",
    "we assume that the noise on target values can be modeled by the gaussian distribution .",
    "so the likelihood of @xmath32 training events is    @xmath33=exp[-\\sum_{i=1}^{n}(t_{i}-y\\left(x_{i},\\bar{\\theta}\\right)/2\\sigma^{2})]\\ ] ]    where @xmath12 is the target value , and @xmath34 is the standard deviation of the noise .",
    "it has been assumed that the events are independent with each other .",
    "then , the likelihood of the predicted target value is computed by eq . ( 4 )",
    ".    we get the likelihood , meanwhile we need the prior to compute the posterior density . but",
    "the choice of prior is not obvious .",
    "however , experience suggests a reasonable class is the priors of gaussian class centered at zero , which prefers smaller rather than larger weights , because smaller weights yield smoother fits to data . in the paper , a gaussian prior",
    "is specified for each weight using the bnn package of radford neal .",
    "however , the variance for weights belonging to a given group(either input - to - hidden weights(@xmath27 ) , hidden -biases(@xmath29 ) , hidden - to - output weights(@xmath28 ) or output - biases(@xmath30 ) ) is chosen to be the same : @xmath35 , @xmath36 , @xmath37 , @xmath38 , respectively . however , since we do nt know , a priori , what these variances should be , their values are allowed to vary over a large range , while favoring small variances .",
    "this is done by assigning each variance a gamma prior    @xmath39    where @xmath40 , and with the mean @xmath41 and shape parameter @xmath42 set to some fixed plausible values .",
    "the gamma prior is referred to as a hyperprior and the parameter of the hyperprior is called a hyperparameter .",
    "then , the posterior density , @xmath22 , is gotten according to eqs .",
    "( 2),(4 ) and the prior of gaussian distribution . given an event with data @xmath43 , an estimate of the target value",
    "is given by the weighted average    @xmath44    currently , the only way to perform the high dimensional integral in eq .",
    "( 6 ) is to sample the density @xmath45 with the markov chain monte carlo ( mcmc ) method@xcite . in the mcmc method ,",
    "one steps through the @xmath5 parameter space in such a way that points are visited with a probability proportional to the posterior density , @xmath22 .",
    "points where @xmath22 is large will be visited more often than points where @xmath22 is small .",
    "( 6 ) approximates the integral using the average    @xmath46    where @xmath47 is the number of points @xmath5 sampled from @xmath22 .",
    "each point @xmath5 corresponds to a different neural network with the same structure .",
    "so the average is an average over neural networks , and is closer to the real value of @xmath48 , when @xmath47 is sufficiently large .",
    "in the paper , a toy detector is designed to simulate the central detector in the reactor neutrino experiment , such as daya bay experiment@xcite and double chooz experiment@xcite , with cern geant4 package@xcite .",
    "the toy detector consists of three regions , and they are the gd - doped liquid scintillator(gd - ls from now on ) , the normal liquid scintillator(ls from now on ) and the oil buffer , respectively . the toy detector of cylindrical shape like the detector modules of daya bay experiment and double chooz experiment is designed in the paper .",
    "the diameter of the gd - ls region is 2.4 meter , and its height is 2.6 meter .",
    "the thickness of the ls region is 0.35 meter , and the thickness of the oil part is 0.40 meter . in the paper ,",
    "the gd - ls and ls are the same as the scintillator adopted by the proposal of the chooz experiment@xcite .",
    "the 8-inch photomultiplier tubes ( pmt from now on ) are mounted on the inside the oil region of the detector .",
    "a total of 366 pmts are arranged in 8 rings of 30 pmts on the lateral surface of the oil region , and in 5 rings of 24 , 18 , 12 , 6 , 3 pmts on the top and bottom caps .",
    "the response of the neutrino and background events deposited in the toy detector is simulated with geant4 .",
    "although the physical properties of the scintillator and the oil ( their optical attenuation length , refractive index and so on ) are wave - length dependent , only averages@xcite ( such as the optical attenuation length of gd - ls with a uniform value is 8 meter and the one of ls is 20 meter ) are used in the detector simulation .",
    "the program could nt simulate the real detector response , but this wo nt affect the result of the comparison between the bnn and the method in the ref.@xcite .",
    "the task of the event reconstruction in the reactor neutrino experiments is to reconstruct the energy and the vertex of a signal .",
    "the maximum likelihood method ( mld ) is a standard algorithm of the event reconstruction in the reactor neutrino experiments .",
    "the likelihood is defined as the joint poisson probability of observing a measured distribution of photoelectrons over the all pmts for given ( @xmath49 ) coordinates in the detector .",
    "the ref.@xcite for the work of the chooz experiment shows the method of the reconstruction in detail .    in the paper ,",
    "the event reconstruction with the mld are performed in the similar way with the chooz experiment@xcite , but the detector is different from the detector of the chooz experiment , so compared to ref.@xcite , there are some different points in the paper :    \\(1 ) the detector in the paper consists of three regions , so the path length from a signal vertex to the pmts consist of three parts , and they are the path length in gd - ls region , the one in ls region , and the one in oil region , respectively .",
    "\\(2 ) considered that not all pmts in the detector can receive photoelectrons when a electron is deposited in the detector , the @xmath50 equation is modified in the paper and different from the one in the chooz experiment , that is , @xmath51 , where @xmath52 is the number of photoelectrons received by the j - th pmt and @xmath53 is the expected one for the j - th pmt@xcite .",
    "\\(3 ) @xmath54 and the coordinates of the charge center of gravity for the all visible photoelectrons from a signal are regarded as the starting values for the fit parameters(@xmath49 ) , where @xmath55 is the total numbers of the visible photoelectrons from a signal and @xmath56 is the proportionality constant of the energy @xmath57 , that is , @xmath58 .",
    "@xmath56 is obtained through fitting @xmath55 s of the 1 mev electron events , and is @xmath59 in the paper .",
    "according to the anti - neutrino interaction in the detector of the reactor neutrino experiments@xcite , the neutrino events from the random direction and the particular direction , ( 0.433,0.75,-0.5 ) , are generated uniformly throughout gd - ls region of the toy detector .",
    "1 shows the four important physics quantities of the monte - carlo reactor neutrino events and they are @xmath60@xmath61@xmath62@xmath63 , respectively .",
    "the selections of the neutrino events are as follows :    \\(1 ) positron energy : 1.3 mev < @xmath64 < 8 mev ;    \\(2 ) neutron energy : 6 mev < @xmath65 < 10 mev ;    \\(3 ) neutron delay : 2 @xmath41s < @xmath61@xmath66 < 100 @xmath41s ;    \\(4 ) relative positron - neutron distance : @xmath67 < 100 cm .",
    "10000 events from the random directions and 5000 events from ( 0.433,0.75,-0.5 ) are selected according to the above criteria , respectively .",
    "the events from the random direction are regarded as the training sample of bnn , and the events from ( 0.433,0.75,-0.5 ) are regarded as the test sample of bnn .",
    "the neutrino events for the random direction and the particular direction , ( 0.354,0.612,-0.707 ) , are generated uniformly throughout gd - ls region of a liquid scintillator detector with the same geometry and the same target as the toy detector in the sec .",
    "3 , according to the following supernova @xmath68 energy distribution@xcite :    @xmath69    with @xmath70 and the supernova is considered to be at @xmath71 .",
    "the number of the fixed direction neutrino events , for a supernova at @xmath71 , could be detected in a liquid scintillator experiment with mass equal to that of sk@xcite .",
    "the events from the random direction are regarded as the training sample of bnn , and the events from ( 0.354,0.612,-0.707 ) are regarded as the test sample of bnn .",
    "2 shows the four important physics quantities of the monte - carlo supernova neutrino events and they are @xmath60@xmath61@xmath62@xmath63 , respectively .",
    "the inverse-@xmath3 decay can be used to locate the neutrino source in scintillator detector experiments .",
    "the method is based on the neutron boost in the forward direction .",
    "and neutron retains a memory of the neutrino source direction .",
    "the unit vector @xmath72 , having its origin at the positron reconstructed position and pointing to the captured neutron position , is defined for each neutrino event .",
    "the distribution of the projection of this vector along the known neutrino direction is forward peaked , but its r.m.s .",
    "value is not far from that of a flat distribution(@xmath73 ) .",
    "@xmath74 is defined as the average of vectors @xmath72 , that is    @xmath75    the measured neutrino direction is the direction of @xmath74 .",
    "the neutrino direction lies along the z axis is assumed to evaluate the uncertainty in the direction of @xmath74 . from the central limit theorem @xmath74 follows that the distribution of the three components is gaussian with @xmath76 centered at ( 0,0,@xmath77 ) .",
    "therefore , the uncertainty on the measurement of the neutrino direction can be given as the cone around @xmath74 which contains 68.3% of the integral of this distribution .",
    "in the paper , the x , y , z components of the neutrino incoming direction are predicted by the three bnns , respectively .",
    "the bnns have the input layer of 6 inputs , the single hidden layer of 15 nodes and the output layer of a output . here",
    "we will explain the case of predicting the x component of the neutrino incoming direction in detail :    \\(1 ) the data format for the training sample is @xmath78@xmath61@xmath62@xmath79 ( i = x ) , where @xmath80 is the difference of @xmath81 and @xmath82 ( i = x ) .",
    "@xmath81(i = x ) is the x components of the @xmath72 in the section 6 .",
    "@xmath82(i = x ) is the x component of the known neutrino incoming direction ( @xmath83 ) .",
    "@xmath84(i = x ) is the x component of the reconstructed positron position .",
    "@xmath78@xmath61@xmath62@xmath85 are used as inputs to a bnn , and @xmath86 is the known target .",
    "the target can be obtained by eq . 10 .",
    "that is @xmath87 where    \\(2 ) the inputs of the test sample are similar with that of the train sample , but the @xmath80(i = x ) is different from that of the training sample .",
    "the @xmath74 obtained by the method in the section 6 is substituted for the known neutrino incoming direction in the process of computing @xmath80(i = x ) .",
    "the @xmath88(i = x ) is the output of the bnn , that is , it is the predicted value using the bnn .",
    "we make use of the @xmath88 value to compute the x component of neutrino incoming direction via the following equation(in fact , eq .",
    "11 is the inverse - function of eq . 10 . ) : @xmath89 where @xmath81(i = x ) is the x component of the @xmath72 .",
    "@xmath90(i = x ) is just the x component of the direction vector ( @xmath91 ) predicted by the bnn .",
    "a markov chain of neural networks is generated using the bnn package of radford neal , with the training sample , in the process of predicting the x component of neutrino incoming direction by using the bnn .",
    "one thousand iterations , of twenty mcmc steps each , are used in the paper .",
    "the neural network parameters are stored after each iteration , since the correlation between adjacent steps is very high .",
    "that is , the points in neural network parameter space are saved to lessen the correlation after twenty steps .",
    "it is also necessary to discard the initial part of the markov chain because the correlation between the initial point of the chain and the points of the part is very high .",
    "the initial three hundred iterations are discarded in the paper .",
    "certainly , the y , z components of the @xmath91 are obtained in the same method , if only i = y , z , respectively . here @xmath92 is defined as the unit vector of the @xmath91 predicted by the bnns for each event in the test sample .",
    "we can also define the direction @xmath93 as the average of the unit direction vectors predicted by the bnns in the same way as the section 6 . that is @xmath94",
    "the @xmath93 is just the neutrino incoming direction predicted by the bnns .",
    "the uncertainty in this value is evaluated in the same method as the section 6 .",
    "we can know the r.m.s .",
    "value of the distribution of the projection of the unit direction vectors predicted by the bnns in the same method as the ref.@xcite . from the central limit theorem @xmath93 follows that the distributions of its three components are gaussian with @xmath95 centered at ( 0,0,@xmath96 ) .",
    "therefore , the uncertainty on the measurement of the neutrino direction can be given as the cone around @xmath93 which contains 68.3% of the integral of this distribution .",
    "3 shows the distributions of the projections of the @xmath72 in the sec . 6 and the @xmath92 predicted by the method of bnn along the reactor neutrino incoming direction .",
    "the r.m.s .",
    "attainable by using bnn is only about 0.41 , and less than that attainable by using the method in the ref.@xcite .",
    "the results of the determination of the reactor neutrino incoming direction using the method in the ref.@xcite and the method of bnn are shown in table 1 .",
    "the uncertainty attainable by using the method in the ref.@xcite is 21.1@xmath0,and the one attainable by using bnn is 1.0@xmath0 .",
    "4 shows the distributions of the projections of the @xmath72 in the sec .",
    "6 and the @xmath92 predicted by the method of bnn along the supernova neutrino incoming direction . the r.m.s .",
    "attainable by using bnn is also about 0.35 .",
    "the results of the determination of the supernova neutrino incoming direction using the method in the ref.@xcite and the method of bnn are shown in table 2 .",
    "the uncertainty attainable by using the method in the ref.@xcite is 10.7@xmath0 , and the one attainable by using bnn is 0.6@xmath0 .",
    "so compared to the method in ref.@xcite , the uncertainty attainable by using bnn is significantly improved and reduces by a factor of about 20 ( 21@xmath0 compared to 1@xmath0 in the case of reactor neutrinos and 11@xmath0 compared to 0.6@xmath0 in the case of supernova neutrinos ) . and compared to sk , it reduces by a factor of about 8 ( 5@xmath0 compared to 0.6@xmath0 ) .",
    "why such good results can be obtained with bnn ?",
    "first , neutrino directions obtained with the method in the ref.@xcite are used as inputs to bnn , that is such good results obtained with bnn is on the base of the results of the method in the ref.@xcite ; second , bnn can extract some unknown information from its inputs and discover more general relationships in data than traditional statistical methods ; third , the over - fitting problem can be solved by using bayesian methods to control model complexity .",
    "so results obtained with bnn can be much better than that of the method in the ref.@xcite . in a word , the method of bnn can be well applied to determine neutrino incoming direction in reactor neutrino experiments and supernova explosion location by scintillator detectors .",
    "this work is supported by the national natural science foundation of china ( nsfc ) under the contract no .",
    "10605014 .",
    "p. c. bhat and h. b. prosper _ beyesian neural networks_. in : l. lyons and m. k. unel ed .",
    "_ proceedings of statistical problems in particle physics , astrophysics and cosmology , oxford ,",
    "uk 12 - 15 , september 2005_. london : imperial college press .",
    "151 - 154 y. xu , y. x. meng , and w. w. xu , journal of instrumentation 3 , p08005 ( 2008 ) , arxiv : 0808.0240 s. duane , a. d. kennedy , b. j. pendleton and d. roweth , physics letters , b195 , 216 - 222 ( 1987 )"
  ],
  "abstract_text": [
    "<S> in the paper , it is discussed by using monte - carlo simulation that the bayesian neural network(bnn ) is applied to determine neutrino incoming direction in reactor neutrino experiments and supernova explosion location by scintillator detectors . as a result , compared to the method in ref.@xcite , the uncertainty on the measurement of the neutrino direction using bnn </S>",
    "<S> is significantly improved . </S>",
    "<S> the uncertainty on the measurement of the reactor neutrino direction is about 1.0@xmath0 at the 68.3% c.l . , </S>",
    "<S> and the one in the case of supernova neutrino is about 0.6@xmath0 at the 68.3% c.l .. compared to the method in ref.@xcite , the uncertainty attainable by using bnn reduces by a factor of about 20 . and </S>",
    "<S> compared to the super - kamiokande experiment(sk ) , it reduces by a factor of about 8 .    </S>",
    "<S> @xmath1department of physics , nankai university , tianjin 300071 , the people s republic of china    bayesian neural network , neutrino incoming direction , reactor neutrino , supernova neutrino    pacs numbers : 07.05.mh , 29.85.fj , 14.60.pq , 95.85.ry </S>"
  ]
}