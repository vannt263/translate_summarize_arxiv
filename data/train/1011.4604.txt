{
  "article_text": [
    "consider the standard linear regression model : @xmath0 where @xmath1 is a vector of responses , @xmath2 is a design matrix , @xmath3 is an unknown regression vector and @xmath4 is a vector of random noises .",
    "one widely studied problem for this model is that of variable selection , that is , how to determine the support of @xmath5 ( i.e. , the indices of the nonzero entries of @xmath5 ) .",
    "when @xmath6 , this problem can be tackled by many classical approaches . in recent years , however , the situations where @xmath7 have become increasingly common in many applications such as signal processing and gene expression studies .",
    "thus , efforts have been directed at developing new variable selection methods that work for large values of @xmath8 .",
    "a few examples of such methods include the lasso @xcite , the elastic net @xcite , and the more recent dantzig selector @xcite .",
    "a dantzig selector for is a solution of the following optimization problem : @xmath9 where @xmath10 and @xmath11 is the diagonal matrix whose diagonal entries are the norm of the columns of @xmath12 .",
    "the dantzig selector was first proposed in @xcite and justified on detailed statistical grounds . in particular , it was shown that , this estimator achieves a loss within a logarithmic factor of the ideal mean squared error , i.e. , the error one would achieve if one knows the support of @xmath5 and the coordinates of @xmath5 that exceed the noise level . for more discussion of the importance of dantzig selector and its relationship with other estimators like lasso , we refer the readers to @xcite .    despite the importance of the dantzig selector and its many connections with other estimators , there are very few existing algorithms for solving .",
    "one natural way of solving is to recast it as a linear programming ( lp ) problem and solve it using lp techniques .",
    "this approach is adopted in the package @xmath13-magic @xcite , which solves the resulting lp problem via a primal - dual interior - point ( ip ) method",
    ". however , the ip methods are typically not efficient for large - scale problems as they require solving dense newton systems for each iteration .",
    "another approach of solving uses homotopy methods to compute the entire solution path of the dantzig selector ( see , for example , @xcite ) . nevertheless , as discussed in ( * ? ? ?",
    "* section  1.2 ) , these methods are also unable to deal with large - scale problems .",
    "recently , first - order methods are proposed for in @xcite , which are capable of solving large - scale problems . in @xcite ,",
    "problem   and its dual are recast into a smooth convex programming problem and an optimal first - order method proposed in @xcite is then applied to solve the resulting problem . in @xcite ,",
    "problem   is recast as a linear cone programming problem .",
    "the optimal first - order methods ( see , for example , @xcite ) are then applied to solve a smooth approximation to the dual of the latter problem . in this paper",
    ", we consider an alternative approach , namely , the alternating direction method ( adm ) , for solving . the adm and its many variants",
    "have recently been widely used to solve large - scale problems in compressed sensing , image processing and statistics ( see , for example , @xcite ) . in general",
    ", the adm can be applied to solve problems of the following form : @xmath14 where @xmath15 and @xmath16 are convex functions , @xmath17 and @xmath18 are matrices , @xmath19 is a vector , and @xmath20 and @xmath21 are closed convex sets .",
    "each iteration of the adm involves solving two subproblems successively and then updating a multiplier , and the method converges to an optimal solution of under some mild assumptions ( see , for example , @xcite ) . in this paper",
    ", we show that can be rewritten in the form of , and hence the adm can be suitably applied .",
    "moreover , we show that one of the adm subproblems has a simple closed form solution , while another one can be efficiently and approximately solved by a nonmonotone gradient method proposed recently in @xcite .",
    "we also discuss convergence of this adm .",
    "finally , we compare our method for solving with a first - order method proposed in @xcite on large - scale simulated problems .",
    "the computational results show that our approach usually outperforms that method in terms of cpu time while producing solutions of comparable quality .",
    "the rest of the paper is organized as follows . in subsection",
    "[ sec : notation ] , we define notations used in this paper . in section  [ sec : dant ] , we study the alternating direction method for solving problem   and address its convergence . finally , we conduct numerical experiments to compare our method with a first - order method proposed in @xcite in section  [ sec : numerics ] .      in this paper , @xmath22 denotes the @xmath23-dimensional euclidean space and @xmath24 denotes the set of all @xmath25 matrices with real entries . for a vector @xmath26 ,",
    "@xmath27 , @xmath28 and @xmath29 denote the @xmath30-norm , @xmath31-norm and @xmath32-norm of @xmath33 , respectively . for any vector @xmath33 in @xmath22",
    ", @xmath34 is the vector whose @xmath35th entry is @xmath36 , while @xmath37 is the vector whose @xmath35th entry is @xmath30 if @xmath38 and @xmath39 otherwise . given two vectors @xmath33 and @xmath40 in @xmath22 , @xmath41 denotes the hadamard ( entry - wise ) product of @xmath33 and @xmath40 , @xmath42 denotes the vector whose @xmath35th entry is @xmath43 .",
    "the letter @xmath44 denotes the vector of all ones , whose dimension should be clear from the context . finally , given a scalar @xmath45 , @xmath46_+$ ] denotes the positive part of @xmath45 , that is , @xmath46_+",
    "= \\max\\{0,a\\}$ ] .",
    "in this section , we study the adm for solving and discuss its convergence and implementation details .    in order to apply the adm ,",
    "we first rewrite in the form of . to this end",
    ", we introduce a new variable @xmath47 and rewrite as follows : @xmath48 then it is easy to see that is in the form of with @xmath49 , @xmath50 , @xmath51 , @xmath52 , @xmath53 , @xmath54 and @xmath55 .",
    "next , in order to describe the adm iterations , we introduce the following augmented lagrangian function for problem : @xmath56 for some @xmath57 .",
    "each iteration of the adm involves alternate minimization of @xmath58 with respect to @xmath47 and @xmath5 , followed by an update of @xmath59 .",
    "the standard adm for problem ( or , equivalently , ) is described as follows : + * alternating direction method : *    * * start : * let @xmath60 and @xmath57 be given . * * for * @xmath61 @xmath62 * end * ( for )    before discussing the convergence of the above method , we first derive the dual problem of ( or , equivalently , ) .",
    "note that @xmath63 where the third equality holds by strong duality .",
    "thus , the dual problem of is given by @xmath64 now we are ready to state a convergence result for the adm , whose proof can be found in @xcite .",
    "[ convergence ] suppose that the solution set of is nonempty and @xmath57 .",
    "let @xmath65 be a sequence generated from the above alternating direction method .",
    "then @xmath66 is convergent .",
    "furthermore , the limit of @xmath67 solves , and any accumulation point of @xmath68 solves .",
    "it is easy to observe that the first subproblem in has a closed form solution , which is given by : @xmath69 where @xmath70 is the vector consisting of the diagonal entries of @xmath11 .",
    "however , the second subproblem does not in general have a closed form solution . in practice",
    "we can choose @xmath71 to be a suitable approximate solution instead .",
    "our next proposition states that the resulting adm still converges to optimal solutions .",
    "the proof follows essentially the same arguments as ( * ? ? ?",
    "* theorem  8) and is thus omitted .",
    "suppose that the solution set of is nonempty and @xmath57 .",
    "let @xmath72 be a sequence of nonnegative numbers with @xmath73 .",
    "let @xmath66 be generated as in while @xmath68 is chosen to satisfy : @xmath74 for all @xmath75 .",
    "then @xmath66 is convergent .",
    "furthermore , the limit of @xmath67 solves , and any accumulation point of @xmath68 solves .    before ending this section , we present an iterative algorithm to solve the second subproblem in approximately .",
    "note that this subproblem can be equivalently written as @xmath76 since the objective function of is the sum of a smooth function @xmath77 and the nonsmooth convex function @xmath13-norm , the nonmonotone gradient method ii recently proposed by lu and zhang @xcite can be suitably applied to approximately solve . for ease of reference , we present the algorithm below . to simplify notations , for any vector @xmath78 and any real number @xmath79",
    ", we define @xmath80    * nonmonotone gradient method : *    * * start : * choose parameters @xmath81 , @xmath82 and integer @xmath83 .",
    "let @xmath84 be given and set @xmath85 .",
    "* * for * @xmath86 a.   let @xmath87 b.   find the largest @xmath88 such that @xmath89_+\\le i\\le l}\\left\\{f_k(u^i)+\\|u^i\\|_1\\right\\}+\\sigma\\alpha\\delta_l.\\ ] ] set @xmath90 , @xmath91 and @xmath92 .",
    "c.   update @xmath93 , where @xmath94 and @xmath95 .",
    "+ * end * ( for )",
    "in this section , we conduct numerical experiments to test the performance of the adm for solving problem . in particular , we compare our method with the default first - order method implemented in the tfocs package @xcite for .",
    "all codes are written in matlab and all experiments are performed in matlab 7.11.0 ( 2010b ) on a workstation with an intel xeon e5410 cpu ( 2.33 ghz ) and 8 gb ram running red hat enterprise linux ( kernel 2.6.18 ) .",
    "we initialize the adm by setting @xmath96 , and terminate the method once @xmath97 for some @xmath98 . for the nonmonotone gradient method subroutine",
    "used to compute @xmath71 , we set @xmath99 , @xmath100 , @xmath101 and @xmath102 , and moreover , we initialize the method by setting @xmath103 . in addition , we terminate this subroutine once @xmath104 for the same @xmath105 as above . for the first - order method implemented in the tfocs package @xcite for , we set the restarting parameter to be @xmath106 as discussed in ( * ? ? ? * section  6.1 ) .",
    "we experiment with two different smoothing parameters : @xmath107 ( at1 ) and @xmath108 ( at2 ) .",
    "we terminate the first - order method when @xmath109      in this subsection , we consider design matrices with unit column norms .",
    "similar to ( * ? ? ?",
    "* section  4.1 ) , we first generate an @xmath110 matrix @xmath12 with independent gaussian entries and then normalize each column to have norm @xmath30 .",
    "we then select a support set @xmath111 of size @xmath112 uniformly at random , and sample a vector @xmath5 on @xmath111 with i.i.d .",
    "entries according to the model @xmath113 for all @xmath35 , where @xmath114 with probability @xmath115 and @xmath116 .",
    "we finally set @xmath117 with @xmath118 .    in our experiment , we choose @xmath119 , @xmath120 , which corresponds to @xmath121 and @xmath122 noise , and @xmath123 for @xmath124 .",
    "for each @xmath125 , we randomly generate @xmath126 copies of instances as described above .",
    "we then set @xmath127 as suggested by ( * ? ? ?",
    "* theorem  1.1 ) .",
    "in addition , we set @xmath128 and @xmath129 for the adm . given an approximate solution @xmath130 of , we compute a two - stage dantzig selector @xmath131 by following the same procedure as described in ( * ? ? ?",
    "* section  1.6 ) , where we truncate all entries with magnitude below @xmath132 .",
    "we evaluate the quality of the solutions obtained from different methods by comparing the following ratios that are introduced in ( * ? ? ?",
    "* section  4.1 ) : @xmath133 for convenience , we call them the pre - processing and post - processing errors , respectively . clearly , the smaller the ratios , the higher the solution quality .",
    "the results of this experiment are reported in tables  [ t1 ] and [ t2 ] .",
    "in particular , we present the cpu time ( cpu ) , the number of iterations ( iter ) and the errors @xmath134 and @xmath135 for all methods , averaged over the @xmath126 instances .",
    "we see from both tables that our adm generally outperforms the first - order methods implemented in the tfocs package @xcite in terms of both cpu time and solution quality .",
    "for example , comparing with at2 , which produces solutions with the best quality among the first - order methods , our method is about twice as fast and produces solutions with smaller pre - processing errors and comparable post - processing errors .    in figure  [ fig1 ] , we present the result for one instance with size @xmath136 and @xmath137 .",
    "the asterisks are the true values of @xmath5 while the circles are the estimates obtained by our method before the post - processing ( the upper plot ) and after the post - processing ( the lower plot ) .",
    "we see from the plot that the latter estimates are very close to the true values of @xmath5 . the similar phenomenon can also be observed in figure  [ fig2 ] for the estimates obtained by at2 on the same instance .",
    "m.  afonso , j.  bioucas - dias and m.  figueiredo .",
    "an augmented lagrangian approach to the constrained optimization formulation of imaging inverse problems . submitted to _",
    "the ieee transactions on image processing _ ( 2009 ) .",
    "e.  cands and j.  romberg .",
    "@xmath13-magic : recovery of sparse signals via convex programming .",
    "user guide , applied & computational mathematics , california institute of technology , pasadena , ca 91125 , usa , october 2005 .",
    "available at ` www.l1-magic.org ` .",
    "z.  lu and y.  zhang .",
    "an augmented lagrangian approach for sparse principal component analysis .",
    "technical report , department of mathematics , simon fraser university , burnaby , bc , v5a 1s6 , canada , july 2009 ."
  ],
  "abstract_text": [
    "<S> in this paper , we study the alternating direction method for finding the dantzig selectors , which are first introduced in @xcite . </S>",
    "<S> in particular , at each iteration we apply the nonmonotone gradient method proposed in @xcite to approximately solve one subproblem of this method . </S>",
    "<S> we compare our approach with a first - order method proposed in @xcite . </S>",
    "<S> the computational results show that our approach usually outperforms that method in terms of cpu time while producing solutions of comparable quality .    * </S>",
    "<S> key words : * dantzig selector , alternating direction method , nonomonotone line search , gradient method . </S>"
  ]
}