{
  "article_text": [
    "selecting a suitable kernel for a kernel - based @xcite machine learning task can be a difficult task . from a statistical point of view",
    ", the problem of choosing a good kernel is a model selection task . to this end , recent research has come up with a number of _ multiple kernel learning _ ( mkl ) @xcite approaches , which allow for an automated selection of kernels from a predefined family of potential candidates .",
    "typically , mkl approaches come in one of these three different flavors :    * instead of formulating an optimization criterion with a fixed kernel @xmath0 , one leaves the choice of @xmath0 as a variable and demands that @xmath0 is taken from a linear span of base kernels @xmath1 .",
    "the actual learning procedure then optimizes not only over the parameters of the kernel classifier , but also over the @xmath2 subject to the constraint that @xmath3 for some fixed norm .",
    "this approach is taken for instance in @xcite for regression and in @xcite for classification . *",
    "a second approach optimizes over all kernel classifiers for each of the @xmath4 base kernels , but modifies the regularizer to a block norm , that is , a norm of the vector containing the individual kernel norms .",
    "this allows to trade - off the contributions of each kernel to the final classifier .",
    "this formulation was used for instance in @xcite . * finally",
    ", since it appears to be sensible to have only the best kernels contribute to the final classifier , it makes sense to encourage sparse kernel weights .",
    "one way to do so is to extend the second setting with an _",
    "elastic net _ regularizer , a linear combination of @xmath5 and @xmath6 regularizers .",
    "this approach was recently described in @xcite .    while all of these formulations are based on similar considerations , the individual formulations and used techniques vary considerably .",
    "the particular formulations are tailored more towards a specific optimization approach rather than the inherent characteristics .",
    "type ( i ) approaches , for instance , are generally solved using a partially dualized wrapper approach , ( ii ) makes use of the fact that the @xmath7-norm computes a coordinatewise maximum and ( iii ) solves mkl in the primal .",
    "this makes it hard to gain insights into the underpinnings and differences of the individual methods , to design general - purpose optimization procedures for the various criteria and to compare the different techniques empirically .    in this paper , we formulate mkl as an optimization criterion with a dual - block - norm regularizer . by using this specific form of regularization",
    ", we can incorporate all the previously mentioned formulations as special cases of a single criterion .",
    "we derive a modular dual representation of the criterion , which separates the contribution of the loss function and the regularizer .",
    "this allows practitioners to plug in specific ( dual ) loss functions and to adjust the regularizer in a flexible fashion .",
    "we show how the dual optimization problem can be solved using standard smooth optimization techniques , report on experiments on real world data , and compare the various approaches according to their ability to recover sparse kernel weights . on the theoretical side",
    ", we give a concentration inequality that bounds the generalization ability of mkl classifiers obtained in the presented framework .",
    "the bound is the first known bound to apply to mkl with elastic net regularization and it matches the best previously known bound @xcite for the special case of @xmath5 and @xmath6 regularization .",
    "in this section we cast multiple kernel learning in a unified framework .",
    "before we go into the details , we need to introduce the general setting and notation .",
    "we begin with reviewing the classical supervised learning setup . given a labeled sample @xmath8 , where the @xmath9 lie in some input space @xmath10 and @xmath11 ,",
    "the goal is to find a hypothesis @xmath12 , that generalizes well on new and unseen data .",
    "regularized risk minimization returns a minimizer @xmath13 , @xmath14 where @xmath15 is the empirical risk of hypothesis @xmath16 w.r.t .",
    "a convex loss function @xmath17 , @xmath18 is a regularizer , and @xmath19 is a trade - off parameter .",
    "we consider linear models of the form @xmath20 together with a ( possibly non - linear ) mapping @xmath21 to a hilbert space @xmath22 @xcite and constrain the regularization to be of the form @xmath23 which allows to kernelize the resulting models and algorithms .",
    "we will later make use of kernel functions @xmath24 to compute inner products in @xmath22 .",
    "when learning with multiple kernels , we are given @xmath4 different feature mappings @xmath25 , each giving rise to a reproducing kernel @xmath26 of @xmath27 .",
    "there are two main ways to formulate regularized risk minimization with mkl .",
    "the first approach introduces a linear kernel mixture @xmath28 , @xmath29 . with this , one solves @xmath30 with a blockwise weighted target vector @xmath31 .",
    "alternatively , one can omit the explicit mixture vector @xmath32 and use block - norm regularization instead . in this case , one optimizes @xmath33 where @xmath34 denotes the @xmath35 block norm . one can show that ( [ classicmkl ] ) is a special case of ( [ blocknormmkl ] ) .",
    "in particular , one can show that setting the block - norm parameter to @xmath36 is equivalent to having kernel mixture regularization with @xmath37 @xcite .",
    "this also implies that the kernel mixture formulation is strictly less general , because it can not replace block norm regularization for @xmath38 .",
    "extending the block norm criterion to also include elastic net @xcite regularization , we thus choose the following minimization problem as primary object of investigation in this paper : +   + * primal mkl optimization problem * =    @xmath39    where @xmath40 denotes the cartesian product of the @xmath41 s . using the above criterion it is possible to recover block norm regularization by setting @xmath42 and the elastic net regularizer by setting @xmath43 .",
    "optimization problems often have a considerably easier structure when studied in the dual space . in this section",
    "we derive the dual problem of the generalized mkl approach presented in the previous section .",
    "let us begin with rewriting optimization problem ( p)by expanding the decision values into slack variables as follows @xmath44 applying lagrange s theorem re - incorporates the constraints into the objective by introducing lagrangian multipliers @xmath45 .",
    "is variable over the whole range of @xmath46 since it is incorporates an equality constraint . ]",
    "the lagrangian saddle point problem is then given by @xmath47 setting the first partial derivatives of the above lagrangian to zero w.r.t .",
    "@xmath48 gives the following kkt optimality condition @xmath49 inspecting the above equation reveals the representation @xmath50 . rearranging the order of terms in the lagrangian",
    ", @xmath51 lets us express the lagrangian in terms of fenchel - legendre conjugate functions @xmath52 as follows , @xmath53 thereby removing the dependency of the lagrangian on @xmath48 .",
    "the function @xmath54 is called _ dual loss _ in the following .",
    "recall that the inf - convolution @xcite of two functions @xmath16 and @xmath55 is defined by @xmath56 and that @xmath57 , and @xmath58 .",
    "moreover , we have for the conjugate of the block norm @xmath59 @xcite where @xmath60 is the conjugate exponent , i.e. , @xmath61 . as a consequence , we obtain the following _ dual _ optimization problem +   + * dual mkl optimization problem * =    @xmath62    note that the supremum is also a maximum , if the loss function is continuous .",
    "the function @xmath63 is the so - called _ morea - yosida approximate _ @xcite and has been studied extensively both theoretically and algorithmically for its favorable regularization properties .",
    "it can `` smoothen '' an optimization problem  even if it is initially non - differentiable  and it increases the condition number of the hessian for twice differentiable problems .",
    "the above dual generalizes multiple kernel learning to arbitrary convex loss functions and regularizers . due to the mathematically clean separation of the loss and the regularization term  each loss term solely depends on a single real valued variable  we can immediately recover the corresponding dual for a specific choice of a loss / regularizer pair @xmath64 by computing the pair of conjugates @xmath65 .",
    "while formalizing multiple kernel learning with block - norm regularization offers a number of conceptual and analytical advantages , it requires an additional step in practical applications .",
    "the reason for this is that the block - norm regularized dual optimization criterion does not include explicit kernel weights . instead , this information is contained only implicitly in the optimal kernel classifier parameters , as output by the optimizer .",
    "this is a problem , for instance if one wishes to apply the induced classifier on new test instances .",
    "here we need the kernel weights to form the final kernel used for the actual prediction . to recover the underlying kernel weights , one essentially needs to identify which kernel contributed to which degree for the selection of the optimal dual solution . depending on the actual parameterization of the primal criterion , this can be done in various ways .",
    "we start by reconsidering the kkt optimality condition given by eq .   and observe that the first term on the right hand side , @xmath66 introduces a scaling of the feature maps . with this notation ,",
    "it is easy to see from eq .   that our model given by eq",
    ".   extends to @xmath67 in order to express the above model solely in terms of dual varables we have to compute @xmath2 in terms of @xmath68 .    in the following we focus on two cases",
    "first , we consider @xmath69 block norm regularization for arbritrary @xmath70 while switching the elastic net off by setting the parameter @xmath42 .",
    "then , from eq .",
    "we obtain @xmath71 resubstitution into leads to the proportionality @xmath72 note that , in the case of classification , we only need to compute @xmath2 up to a positive multiplicative constant .    for the second case , let us now consider the elastic net regularizer , i.e. , @xmath73 with @xmath74 and @xmath75 .",
    "then , the optimality condition given by eq .   translates to @xmath76 inserting the left hand side expression for @xmath77 into the right hand side leads to the non - linear system of equalities @xmath78 where we employ the notation @xmath79 . in our experiments",
    "we solve the above conditions numerically using @xmath74 .",
    "the optimal mixing coefficients @xmath80 can now be computed solely from the dual @xmath68 variables by means of eq .",
    "and  , and by the kernel matrices @xmath81 using the identity @xmath82 this enables optimization in the dual space as discussed in the next section .",
    "in this section we describe how one can solve the dual optimization problem using an efficient quasi - newton method . for our experiments ,",
    "we use the hinge loss @xmath83 , but the discussion also applies to most other convex loss functions . we first note that the dual loss of the hinge loss is @xmath84 if @xmath85 and @xmath86 elsewise @xcite . hence , for each @xmath87 the term @xmath88 of the generalized dual , i.e. , optimization problem ( d ) , translates to @xmath89 , provided that @xmath90 .",
    "employing a variable substitution of the form @xmath91 , the dual problem ( d ) becomes @xmath92 and by definition of the inf - convolution , @xmath93 we note that the representer theorem @xcite is valid for the above problem , and hence the solution of can be expressed in terms of kernel functions , i.e. , @xmath94 for certain real coefficients @xmath95 uniformly for all @xmath96 , hence @xmath97 .",
    "thus , eq .  [ eq555 ] has a representation of the form @xmath98 the above expression can be written for @xmath99 . ] in terms of kernel matrices as follows , +   + * hinge loss dual optimization problem * =    @xmath100    where we denote by @xmath101 the elementwise multiplication of two vectors and use the shorthand @xmath102 .",
    "in this section we give two uniform convergence bounds for the generalization error of the multiple kernel learning formulation presented in section [ sec - derivation ] .",
    "the results are based on the established theory on rademacher complexities .",
    "let @xmath103 be a set of independent rademacher variables , which obtain the values -1 or + 1 with the same probability 0.5 . and",
    "let @xmath104 be some space of classifiers @xmath105 .",
    "then , the _ rademacher complexity _ of @xmath104 is given by @xmath106 \\notag~.\\end{aligned}\\ ] ] if the rademacher complexity of a class of classifiers is known , it can be used to bound the generalization error .",
    "we give one result here , and refer to the literature @xcite for further results on rademacher penalization .",
    "assume the loss @xmath107 has @xmath108 , is lipschitz with constant @xmath109 and @xmath110 for all @xmath111 .",
    "then , the following holds with probability larger than @xmath112 for all classifiers @xmath113 : @xmath114 \\le \\frac{1}{n } \\sum_{i=1}^n { \\ell } ( y_i c(x_i ) ) + 2l \\mathcal{r}_{\\mathcal{c } } +   \\sqrt{\\frac{8 \\ln \\frac{2}{\\delta}}{n } } \\label{rademacherbound}~.\\end{aligned}\\ ] ]    we will now give an upper bound for the rademacher complexity of the block - norm regularized linear learning approach described above .",
    "more precisely , for @xmath115 let @xmath116 denote the norm induced by kernel @xmath117 and for @xmath118 , @xmath119 and @xmath120 with @xmath121 define @xmath122 we now give a bound for the following class of linear classifiers : @xmath123    [ maintheorem ] assume the kernels are normalized , i.e. @xmath124 for all @xmath125 and all @xmath115 .",
    "then , the rademacher complexity of the class @xmath126 of linear classifiers with block norm regularization is upper - bounded as follows : @xmath127 for the special case with @xmath128 and @xmath129 , the bound can be improved as follows : @xmath130    it is instructive to compare this result to some of the existing mkl bounds in the literature . for instance , the main result in @xcite bounds the rademacher complexity of the @xmath5-norm regularizer with a @xmath131 term .",
    "we get the same result by setting @xmath132 and @xmath43 . for the @xmath6-norm",
    "regularized setting , we can set @xmath132 and @xmath133 ( because the kernel weight formulation with @xmath6 norm corresponds to the block - norm representation with @xmath133 ) to recover their @xmath134 bound . finally , it is interesting to see how changing the @xmath135 parameter influences the generalization capacity of the elastic net regularizer ( @xmath136 ) . for @xmath137 , we essentially recover the @xmath5 regularization penalty , but as @xmath135 approaches 0 , the bound includes an additional @xmath138 term . this shows how the capacity of the elastic net regularizer increases towards the @xmath6 setting with decreasing sparsity .    using the notation @xmath139 and @xmath140",
    "it is easy to see that @xmath141 & =   \\e\\left [ \\sup_{\\|w\\|_b \\le 1 } \\left\\ { \\left ( \\begin{array}{c }   w_1 \\\\ \\vdots \\\\",
    "w_m \\end{array } \\right)^t \\left ( \\begin{array}{c } \\frac{1}{n}\\sum_{i=1}^n\\sigma_i\\phi_1(x_i ) \\\\ \\vdots \\\\",
    "\\frac{1}{n}\\sum_{i=1}^n\\sigma_i \\phi_m(x_i)\\end{array } \\right ) \\right\\ } \\right ] \\notag \\\\ & = \\e\\left [ \\left\\| \\left ( \\begin{array}{c } \\|   \\frac{1}{n}\\sum_{i=1}^n\\sigma_i\\phi_1(x_i)\\|_{\\star1 }",
    "\\\\ \\vdots \\\\ \\|",
    "\\frac{1}{n}\\sum_{i=1}^n\\sigma_i \\phi_m(x_i)\\|_{\\star m } \\end{array } \\right ) \\right\\|_o^ * \\right ] \\notag , \\end{aligned}\\ ] ] where @xmath142 denotes the dual norm of @xmath143 and we use the fact that @xmath144 @xcite , and that @xmath145 .",
    "we will show that this quantity is upper bounded by @xmath146 as a first step we prove that for any @xmath147 @xmath148 for any @xmath149 we can apply hlder s inequality to the dot product of @xmath147 and @xmath150 and obtain @xmath151 . since @xmath152",
    ", we can apply this twice on the two components of @xmath153 to get a lower bound for @xmath154 , @xmath155 in other words , for every @xmath147 with @xmath156 it holds that @xmath157 thus , @xmath158 this means we can bound the dual norm @xmath159 of @xmath153 as follows : @xmath160 this accounts for the first factor in ( [ mainres ] ) . for the second factor , we show that @xmath161 & \\le \\sqrt{\\frac{2\\ln m}{n } } + \\sqrt{\\frac{1}{n}}\\label{mainres2 } ~.\\end{aligned}\\ ] ] to do so , define @xmath162 by the independence of the rademacher variables it follows for all @xmath163 , @xmath164   = \\frac{1}{n^2}\\sum_{i=1}^n \\e\\left[k_k(x_i , x_i)\\right ]   \\le \\frac{1}{n } \\label{expectationbound}.\\end{aligned}\\ ] ]    in the next step we use a martingale argument to find an upper bound for @xmath165 $ ] where @xmath166 $ ] . for ease of notation",
    ", we write @xmath167 $ ] to denote the conditional expectation @xmath168 $ ] .",
    "we define the following martingale : @xmath169- \\e_{(r-1)}[\\sqrt{v_k } ] \\notag \\\\ & = \\e_{(r)}\\left[\\left\\|\\frac{1}{n}\\sum_{i=1}^n \\sigma_i \\phi_k(x_i)\\right\\|_{\\star k}\\right]- \\e_{(r-1)}\\left[\\left\\|\\frac{1}{n}\\sum_{i=1}^n \\sigma_i \\phi_k(x_i)\\right\\|_{\\star k}\\right ] .",
    "\\label{martingale1}\\end{aligned}\\ ] ] the range of each random variable @xmath170 is at most @xmath171 .",
    "this is because switching the sign of @xmath172 changes only one summand in the sum from @xmath173 to @xmath174 .",
    "thus , the random variable changes by at most @xmath175 .",
    "hence , we can apply hoeffding s inequality , @xmath176\\le e^{\\frac{1}{2n^2}s^2 }   .$ ]    this allows us to bound the expectation of @xmath177 as follows : @xmath178 & = \\e \\left[\\frac{1}{s } \\ln \\sup_k   e^{sw_k}\\right ] \\notag \\\\ & \\le \\e\\left[\\frac{1}{s } \\ln \\sum_{k=1}^m \\exp \\left [ s\\sum_{r=1}^n z_k^{(r)}\\right ] \\right ] \\notag \\\\ & \\le \\frac{1}{s } \\ln \\sum_{k=1}^m   \\e\\left[e^ { s \\sum_{r=1}^{n-1 } z_k^{(r)}}\\e_{(n-1)}\\left[e^{s z_k^{(n)}}\\right ] \\right ] \\notag \\\\ & \\le \\frac{1}{s } \\ln \\sum_{k=1}^m   \\left(e^{\\frac{1}{2n^2}s^2}\\right)^n\\notag\\\\ & = \\frac{\\ln m}{s } + \\frac{s}{2n}\\notag ~,\\end{aligned}\\ ] ] where we @xmath179 times applied hoeffding s inequality . setting @xmath180 yields : @xmath178 & \\le \\sqrt{\\frac{2\\ln m}{n } } \\label{varbound }",
    "~.\\end{aligned}\\ ] ] now , we can combine ( [ expectationbound ] ) and ( [ varbound ] ) : @xmath181 \\le\\e\\left [ \\sup_k w_k+ \\sqrt{\\e[v_k]}\\right ] \\le \\sqrt{\\frac{2\\ln m}{n } } + \\sqrt{\\frac{1}{n } } ~.\\ ] ] this concludes the proof of ( [ mainres2 ] ) and therewith ( [ mainbound ] ) .",
    "the special case ( [ mainresspecialcase ] ) for @xmath182 is similar . as a first step , we modify ( [ normstep ] ) to deal with the @xmath6-norm rather than the @xmath7-norm : @xmath183 to see this , observe that for any @xmath147 and any @xmath184 hlder s inequality gives @xmath185 .",
    "applying this to the two components of @xmath153 we have : @xmath186 in other words , for every @xmath147 with @xmath156 it holds that @xmath187 following the same arguments as in ( [ dualbound1 ] ) and ( [ dualbound2 ] ) we obtain ( [ secondnormbound ] ) .",
    "to finish the proof it now suffices to show that @xmath188 & \\le \\sqrt{\\frac{m}{n } } \\notag~.\\end{aligned}\\ ] ] this is can be seen by a straightforward application of ( [ expectationbound ] ) : @xmath189 & \\le \\sqrt { \\e \\left[\\sum_{k=1}^m   v_k \\right ] } \\notag   \\le \\sqrt{\\sum_{i=1}^m \\frac{1}{n } } = \\sqrt{\\frac{m}{n } } \\notag~.\\end{aligned}\\ ] ]",
    "in this section we evaluate the proposed method on artifical and real data sets .",
    "we chose the limited memory quasi - newton software l - bfgs - b @xcite to solve .",
    "l - bfgs - b approximates the hessian matrix based on the last @xmath190 gradients , where @xmath190 is a parameter to be chosen by the user .",
    "the goal of this section is to study the relationship of the level of sparsity of the true underlying function to the chosen block norm or elastic net mkl model .",
    "apart from investigating which parameter choice leads to optimal results , we are also interested in the effects of suboptimal choices of @xmath191 . to this aim we constructed several artificial data sets",
    "in which we vary the degree of sparsity in the true kernel mixture coefficients .",
    "we go from having all weight focussed on a single kernel ( the highest level of sparsity ) to uniform weights ( the least sparse scenario possible ) in several steps .",
    "we then study the statistical performance of @xmath69-block - norm mkl for different values of @xmath191 that cover the entire range @xmath192 $ ] .",
    "we follow the experimental setup of @xcite but compute classification models for @xmath193 block - norm mkl and @xmath194 elastic net mkl .",
    "the results are shown in fig .",
    "[ toy - gap50 ] and compared to the bayes error that is computed analytically from the underlying probability model .",
    "unsurprisingly , @xmath5 performs best in the sparse scenario , where only a single kernel carries the whole discriminative information of the learning problem .",
    "in contrast , the @xmath7-norm mkl performs best when all kernels are equally informative .",
    "both mkl variants reach the bayes error in their respective scenarios .",
    "the elastic net mkl performs comparable to @xmath5-block - norm mkl .",
    "the non - sparse @xmath195-norm mkl and the unweighted - sum kernel svm perform best in the balanced scenarios , i.e. , when the noise level is ranging in the interval 60%-92% .",
    "the non - sparse @xmath196-norm mkl of @xcite performs only well in the most non - sparse scenarios .",
    "intuitively , the non - sparse @xmath195-norm mkl of @xcite is the most robust mkl variant , achieving an test error of less than @xmath197 in all scenarios .",
    "the sparse @xmath5-norm mkl performs worst when the noise level is less than @xmath198 .",
    "it is worth mentioning that when considering the most challenging model / scenario combination , that is @xmath7-norm in the sparse and @xmath5-norm in the uniformly non - sparse scenario , the @xmath5-norm mkl performs much more robust than its @xmath7 counterpart .",
    "however , as witnessed in the following sections , this does not prevent @xmath7 norm mkl from performing very well in practice . in summary , we conclude that by tuning the sparsity parameter @xmath191 for each experiment , block norm mkl achieves a low test error across all scenarios .",
    "this experiment aims at detecting transcription start sites ( tss ) of rna polymerase ii binding genes in genomic dna sequences .",
    "accurate detection of the transcription start site is crucial to identify genes and their promoter regions and can be regarded as a first step in deciphering the key regulatory elements in the promoter region that determine transcription .",
    "many detectors thereby rely on a combination of feature sets which makes the learning task appealing for mkl . for our experiments",
    "we use the data set from @xcite and we employ five different kernels representing the tss signal ( weighted degree with shift ) , the promoter ( spectrum ) , the 1st exon ( spectrum ) , angles ( linear ) , and energies ( linear ) . the kernel matrices are normalized such that each feature vector has unit norm in hilbert space .",
    "we reserve 500 and 500 randomly drawn instances for holdout and test sets , respectively , and use 1,000 as the training pool from which 250 elemental training sets are drawn .",
    "table  [ tab : bio ] shows the area under the roc curve ( auc ) averaged over 250 repetitions of the experiment . thereby @xmath199 and @xmath86 block norms are approximated by @xmath200 and @xmath201 norms , respectively .",
    "for the elastic net we use an @xmath202-block - norm penalty .",
    ".results for the bioinformatics experiment . [ cols=\"<,<\",options=\"header \" , ]     table [ tab : ids ] shows the results for multiple kernel learning with various norms and elastic net parameters @xmath203 .",
    "the overall performance of all models is relatively high which is typical for intrusion detection applications .",
    "where very small false positive rates are crucial .",
    "the elastic net instantiations perform relatively similar where @xmath204 is the most accurate one .",
    "it reaches about the same level as @xmath5-block - norm mkl , which performs better than the non - sparse @xmath195-norm mkl , the @xmath205-norm mkl , and the svm with an unweighted - sum kernel . out of the block norm mkl versions  as already witnessed in the bioinformatics experiment@xmath7-norm mkl gives the best predictor .",
    "we presented a framework for multiple kernel learning , that unifies several recent lines of research in that area .",
    "we phrased the seemingly different mkl variants as a single generalized optimization criterion and derived its dual . by plugging in an arbitrary convex loss function",
    "many existing approaches can be recovered as instantiations of our model .",
    "we compared the different mkl variants in terms of their generalization performance by giving an concentration inequality for generalized mkl that matches the previous known bounds for @xmath5 and @xmath195 mkl .",
    "we showed on artificial data how the optimal choice of an mkl model depends on the properties of the true underlying scenario .",
    "we compared several existing mkl instantiations on bioinformatics and network intrusion detection data .",
    "surprisingly , our empirical analysis shows that the recent uniformly non - sparse @xmath7 mkl of @xcite outperforms its sparse and non - sparse competitors in both practical cases .",
    "it is up to future research to determine whether this empirical success also translates to other loss functions than hinge loss and other performance measures than the area under the roc curve .",
    "a.  agarwal , a.  rakhlin , and p.  bartlett .",
    "matrix regularization techniques for online multitask learning . technical report ucb / eecs-2008 - 138 , eecs department , university of california , berkeley , oct 2008 .          m.  kloft , u.  brefeld , s.  sonnenburg , p.  laskov , k",
    "mller , and a.  zien .",
    "efficient and accurate lp - norm multiple kernel learning . in y.",
    "bengio , d.  schuurmans , j.  lafferty , c.  k.  i. williams , and a.  culotta , editors , _ advances in neural information processing systems 22 _ , pages 9971005 . mit press , 2009 .",
    "m.  kloft , u.  brefeld , s.  sonnenburg , and a.  zien .",
    "non - sparse regularization and efficient training with multiple kernels .",
    "technical report ucb / eecs-2010 - 21 , eecs department , university of california , berkeley , feb 2010 .",
    "corr abs/1003.0079 .",
    "http://www.eecs.berkeley.edu/pubs/techrpts/2010/eecs-2010-21.html .",
    "m.  kloft , s.  nakajima , and u.  brefeld .",
    "feature selection for density level - sets . in w.",
    "l. buntine , m.  grobelnik , d.  mladenic , and j.  shawe - taylor , editors , _ proceedings of the european conference on machine learning and knowledge discovery in databases ( ecml / pkdd ) _ , pages 692704 , 2009 .",
    "j.  s. nath , g.  dinesh , s.  ramanand , c.  bhattacharyya , a.  ben - tal , and k.  r. ramakrishnan . on the algorithmics and applications of a mixed - norm based kernel learning formulation . in y.",
    "bengio , d.  schuurmans , j.  lafferty , c.  k.  i. williams , and a.  culotta , editors , _ advances in neural information processing systems 22 _ , pages 844852 , 2009 ."
  ],
  "abstract_text": [
    "<S> recent research on multiple kernel learning has lead to a number of approaches for combining kernels in regularized risk minimization . </S>",
    "<S> the proposed approaches include different formulations of objectives and varying regularization strategies . in this paper </S>",
    "<S> we present a unifying general optimization criterion for multiple kernel learning and show how existing formulations are subsumed as special cases . </S>",
    "<S> we also derive the criterion s dual representation , which is suitable for general smooth optimization algorithms . </S>",
    "<S> finally , we evaluate multiple kernel learning in this framework analytically using a rademacher complexity bound on the generalization error and empirically in a set of experiments . </S>"
  ]
}