{
  "article_text": [
    "ratsaby @xcite introduced a quantitative definition of the information content of a general static system ( e.g. a solid or some fixed structure ) and explained how it algorithmically interferes with input excitations thereby influencing its stability .",
    "his model is based on concepts of the theory of algorithmic information and randomness .",
    "he modeled a system as a selection rule of a finite algorithmic complexity which acts on an incoming sequence of random external excitations by selecting a subsequence as output .",
    "as postulated in @xcite a simple structure is one whose information content is small .",
    "its selection behavior is of low complexity since it can be more concisely described .",
    "consequently it is less able to deform properties of randomness of the input sequence . and",
    "vice versa , if the system is sufficiently complex it can significantly deform the randomness at the input .",
    "following @xcite there have been recent theoretical and empirical results that validate his model for specific problem domains .",
    "the first empirical proof of his model appeared in @xcite where it was shown that this inverse relationship between system complexity and randomness exists also in a real physical system .",
    "the particular system investigated consisted of a one - dimensional vibrating solid - beam to which a random sequence of external input forces is applied . in @xcite",
    "the problem of learning to predict binary sequences was shown to be an exemplar of this paradigm .",
    "the complexity of a learner s decision rule is proportional to the amount that the subsequence selected by the learner ( via his mistakes ) deviates from a truly random sequence . a first empirical investigation of this learning problem appeared in @xcite where",
    "a new measure of system complexity called the sysratio was introduced and shown to be a proper measure of a learner s decision complexity .",
    "the current paper digs further along this line and provides not only further empirical analysis and justification of the model of @xcite applied to the problem of learning but also gives new interpretations of standard learning phenomena such as model data underfitting or overfitting .",
    "it is shown that these phenomena can be interpreted as certain types of deformations of randomness of the binary mistake sequences .",
    "these deformations are measured in the @xmath1-plane ( @xmath2 stands for divergence and @xmath3 for estimated kolmogorov complexity ) .",
    "we conclude that the prediction rule obtained by learning is analogous to a physical static object that scatters a random beam of particles .",
    "we call this phenomena _ bit - scattering _ ( we discuss this phenomenon later at the end of section [ sec : experimentl - setup ] ) .",
    "the current paper is a further justification that the static algorithmic interference model defined in @xcite applies to the problem of learning to predict . before proceeding to give an introduction to the main concepts",
    "let us state the problem that we consider in the paper .",
    "_ statement of the problem _ : given a random source that generates two binary sequences , @xmath4 and @xmath5 of length @xmath6 and @xmath7 , respectively , according to a finite markov chain of unknown order @xmath8 with an unknown probability transition matrix .",
    "a learner uses @xmath4 to estimate the probability parameters of a markov model of order @xmath9 .",
    "once the model is learnt , the learner makes a prediction for every bit in @xmath5 .",
    "denote by @xmath10 the binary sequence corresponding to these predictions .",
    "denote by @xmath11 the error sequence that corresponds to the learner s predictions where the @xmath12 bit @xmath13 if the prediction differs from the true value , i.e. , @xmath14 and @xmath15 otherwise .",
    "denote by @xmath16 the subsequence of @xmath11 corresponding to those bits of @xmath10 that are @xmath17 . in this paper",
    "we study different characteristics of the error sequence @xmath16 and how they depend on the two main learner s parameters , the training sequence length @xmath6 and the model order @xmath9 .",
    "we focus on two main characteristics , the algorithmic complexity of the error sequence and the statistical deviation between the frequency of @xmath18s and the probability of seeing a @xmath18 in the sequence .",
    "we determine their interrelationship and how the probability of a prediction error depends on them .",
    "the remainder of the paper is organized as follows : in section [ sec1 ] we introduce the basic concepts of algorithmic complexity and related properties of randomness . in section [ sec : selection - rule ] we review the concept of a selection rule , in section [ sec : relationship - to - information ] we state a relationship between the complexity of a finite random binary sequence and its entropy .",
    "section [ sec : experimentl - setup ] describes the experimental setup used for the analysis followed by section [ sec : results ] which describes the results .    before continuing",
    ", we should clarify at this point that our use of the words chaoticity or chaotic is different from chaos theory . by a chaotic binary sequence",
    "we do not necessarily mean that it is generated by some dynamical system that is highly sensitive to initial conditions but that it is highly disordered , or in other words , has a high algorithmic complexity .",
    "algorithmic randomness ( see @xcite ) is a notion of randomness of an individual element ( object ) of a sample space .",
    "it reflects how chaotic , or how complicated it is to describe the object .",
    "classical probability theory assigns probabilities to sets of outcomes of random trials in an experiment .",
    "for instance , consider an experiment with @xmath7 randomly and independently drawn binary numbers @xmath19 , @xmath20 , where @xmath21 with probability @xmath22 .",
    "then any outcome such as @xmath23 has the same probability @xmath24 . however , from an algorithmic perspective , it is clear that the string @xmath25 is not random compared to some other possible string with a more complicated pattern of zeros and ones .",
    "algorithmic randomness of finite objects ( binary sequences ) aims to explain the intuitive idea that a sequence , whether finite or infinite , should be measured as being more unpredictable if it possess fewer regularities ( patterns ) .",
    "there is no formal definition of randomness but there are three main properties that a random binary string of length @xmath7 must intuitively satisfy @xcite .",
    "the first property is the so - called _ stochasticity _ or frequency stability of the sequence which means that any binary word of length @xmath26 must have the same frequency limit ( equal to @xmath27 ) .",
    "this is basically the notion of normality that borel introduced and is related to the degree of unpredictability of the sequence .",
    "the second property is _ chaoticity _ or disorderliness of the sequence .",
    "a sequence is less chaotic ( less complex ) if it has a short description , i.e. , if the minimal length of a program that generates the sequence is short .",
    "the third property is _",
    "typicalness_. a random sequence is a typical representative of the class @xmath28 of all binary sequences .",
    "it has no specific features distinguishing it from the rest of the population .",
    "an infinite binary sequence is typical if each small subset @xmath29 of @xmath28 does not contain it ( the correct definition of a small set was given by martin l@xmath30f @xcite ) .",
    "algorithmic randomness was first considered by von mises in 1919 who defined an infinite binary sequence @xmath31 of zeros and ones as random if it is unbiased , i.e. if the frequency of zeros goes to @xmath22 , and every subsequence of @xmath31 that we can extract using an admissible selection rule ( see definition below ) is also not biased .",
    "kolmogorov and loveland @xcite proposed a more permissive definition of an admissible _ selection rule _ as any ( partial ) computable process which , having read any @xmath7 bits of an infinite binary sequence @xmath31 , picks a bit that has not been read yet , decides whether it should be selected or not , and then reads its value .",
    "when subsequences selected by such a selection rule pass the unbiasedness test they are called kolmogorov - loveland stochastic ( kl - stochastic for short ) .",
    "martin l@xmath30f @xcite introduced a notion of randomness which is now considered by many as the most satisfactory notion of algorithmic randomness .",
    "his definition says precisely which infinite binary sequences are random and which are not .",
    "the definition is probabilistically convincing in that it requires each random sequence to pass every algorithmically implementable statistical test of randomness .",
    "in this paper we are concerned with random sequences that arise from the process of learning and prediction , or more specifically , from the prediction mistakes made by a learner .",
    "let @xmath32 be a sequence of binary random variables drawn according to some unknown joint probability distribution @xmath33 .",
    "consider the problem of learning to predict the next bit in a binary sequence drawn according to @xmath34 . for training , the learner is given a finite sequence @xmath4 of bits @xmath35 @xmath36 , drawn according to @xmath34 and estimates a model @xmath37 that can be used to predict the next bit of a partially observed sequence .",
    "after training , the learner is tested on another sequence @xmath5 drawn according to the same unknown distribution @xmath34 . using @xmath37 he produces the bit @xmath38 as a prediction for @xmath39 , @xmath40 .",
    "denote by @xmath11 the corresponding binary sequence of mistakes where @xmath41 if @xmath42 and is @xmath17 otherwise .",
    "denote by @xmath16 the subsequence of @xmath11 that corresponds to the times @xmath43 where the learner predicted @xmath44 .",
    "note that @xmath16 is also a subsequence of @xmath5 so we can view the process of predicting as a process of selecting a subsequence of the input @xmath5 .",
    "it is clear that the subsequence @xmath16 of mistakes should be random since the test sequence @xmath5 is random .",
    "it is reasonable to expect that the learner may implicitly vary some of the randomness characteristics of the subsequence of bits that he selects thereby cause @xmath16 to be less random than @xmath5 . in this sense",
    ", we may say that the learner deforms the randomness of the input @xmath5 producing a less random subsequence of @xmath5 . or perhaps the learner being of a finite complexity is limited in his ability to deform randomness of @xmath5 .",
    "essentially we ask what interference does a learner have on the randomness of a test sequence .",
    "it appears essential that we look not only on the randomness of the object itself ( the test sequence @xmath5 ) but also at the interfering entity  the learner , specifically , its algorithmic component that is used for prediction .",
    "let us formally define a selection rule .",
    "this is a principal concept used as part of tests of randomness of sequences ( mentioned above ) .",
    "let @xmath45 be the space of all finite binary sequences and denote by @xmath46 the set of all finite binary sequences of length @xmath7 .",
    "an admissible _ selection rule _",
    "@xmath47 is defined @xcite based on three partial recursive functions @xmath48 and @xmath49 on @xmath45 .",
    "let @xmath50 .",
    "the process of selection is recursive .",
    "it begins with an empty sequence @xmath51 .",
    "the function @xmath52 is responsible for selecting possible candidate bits of @xmath5 as elements of the subsequence to be formed .",
    "the function @xmath53 examines the value of these bits and decides whether to include them in the subsequence .",
    "thus @xmath52 does so according to the following definition : @xmath54 , and if at the current time @xmath9 a subsequence has already been selected which consists of elements @xmath55 then @xmath52 computes the index of the next element to be examined according to element @xmath56 where @xmath57 , i.e. , the next element to be examined must not be one which has already been selected ( notice that maybe @xmath58 , @xmath59 , i.e. , the selection rule can go backwards on @xmath60 ) .",
    "next , the two - valued function @xmath53 selects this element @xmath61 to be the next element of the constructed subsequence of @xmath60 if and only if @xmath62 . the role of the two - valued function @xmath49 is to decide when this process must be terminated .",
    "this subsequence selection process terminates if @xmath63 or @xmath64 .",
    "let @xmath65 denote the selected subsequence .",
    "by @xmath66 we mean the length of the shortest program computing the values of @xmath52 , @xmath53 and @xmath49 given @xmath7 .    from the above discussion , we know that there are two principal measures related to the information content in a finite sequence @xmath5 , stochasticity ( unpredictability ) and chaoticity ( complexity ) .",
    "an infinitely long binary sequence is regarded random if it satisfies the principle of stability of the frequency of @xmath18s for any of its subsequences that are obtained by an admissible selection rule @xcite .",
    "kolmogorov showed that the stochasticity of a finite binary sequence @xmath60 may be precisely expressed by the deviation of the frequency of ones from some @xmath67 , for any subsequence of @xmath5 selected by an admissible selection rule @xmath47 of finite complexity @xmath66 where for an object @xmath60 given another object @xmath68 he defined in @xcite the complexity of @xmath60 as    @xmath69    where @xmath70 is the length of the sequence @xmath71 , @xmath72 is a universal partial recursive function which acts as a description method , i.e. , when provided with input @xmath73 it gives a specification for @xmath60 ( for an introduction see section 2 of @xcite ) .",
    "the chaoticity of @xmath5 is large if its complexity is close to its length @xmath7 .",
    "the classical work of @xcite relates chaoticity to stochasticity . in @xcite",
    "it is shown that chaoticity implies stochasticity . for a binary sequence @xmath74 ,",
    "let us denote by @xmath75 the number of @xmath18s in @xmath74 , then this can be seen from the following relationship ( with @xmath76 ) :    @xmath77    where @xmath78 is the length of the subsequence selected by @xmath47 and @xmath79 is some absolute constant . apparently as the chaoticity of @xmath5 grows the stochasticity of the selected subsequence @xmath65 grows ( the bias from @xmath22 decreases ) .",
    "also , and more relevant to the context of this paper , the information content of the selection rule namely @xmath66 has a direct effect on this relationship : the lower @xmath66 the stronger the stability ( smaller deviation of the frequency of @xmath18s from @xmath22 ) .",
    "in @xcite the other direction which shows that stochasticity implies chaoticity is proved .",
    "it was recently shown in @xcite that the level of randomness of the subsequence @xmath16 of @xmath11 which corresponds to the occurrences of mistakes in predicting @xmath17s decreases relative to an increase in the complexity of the learner . the approach taken there is to represent the learner s decision as a selection rule that selects @xmath16 from @xmath11 .",
    "the rule s complexity is defined based on a combinatorial quantity rather than kolmogorov complexity but still yields a relationship of the form of ( [ eq : ineq ] ) .",
    "this relationship shows that the possibility of deviation of the frequency of @xmath18s in @xmath16 from the probability @xmath80 of seeing a @xmath18 in @xmath16 grows as the complexity of the class of possible decisions grows .",
    "the current paper investigates this experimentally .",
    "we consider a learner s prediction ( or decision ) rule which we term as _ system _ and study its influence on a random binary test sequence on which prediction decisions are made .",
    "the system is based on the maximum _ a posteriori _ probability decision where probabilities are defined by a statistical parametric model which is estimated from data .",
    "the learner of this model is a computer program that trains from a given random data sequence and then produces a decision rule by which it is able to predict ( or decide ) the value of the next bit in future ( yet unseen ) random binary sequences . as in @xcite",
    "we focus on markov source and a markov learner whose orders may differ .",
    "we now describe the connection between the concepts of entropy ( shannon entropy ) and algorithmic complexity .",
    "entropy is a measure of unpredictability of a random variable .",
    "intuitively , we expect that the more unpredictable a sequence of random variables the higher its algorithmic ( kolmogorov ) complexity .",
    "this is formally expressed as theorem 14.3.1 in @xcite which we now state : denote by @xmath81 the entropy of a random variable @xmath82 and consider a sequence of random variables @xmath83 drawn i.i.d . according to the probability mass function @xmath84 , @xmath85 , where @xmath86 is a finite alphabet .",
    "let @xmath87 . then there exists a constant @xmath88 such that @xmath89 for all @xmath7 .",
    "consequently , the expected value @xmath90 with increasing @xmath7 .",
    "this means that the expected value of the kolmogorov complexity of the sequence converges to the shannon entropy of the sequence with increasing @xmath7 .",
    "a more relevant estimate for our work here concerns the kolmogorov complexity of a specific sequence ( not the expected value over all sequences ) . in the case of a bernoulli random sequence @xmath91 with probability @xmath92",
    "its complexity relates to the binary entropy @xmath81 of any of the i.i.d .",
    "random variables of the sequence .",
    "it is based on the following statement which holds even more generally for any binary sequence of length @xmath7 ( theorem 14.2.5 of @xcite ) : let @xmath93 be a binary string then the kolmogorov complexity of @xmath5 is bounded as    @xmath94    where @xmath95 , @xmath96 is the entropy of a binary random variable with probability @xmath97 and @xmath88 is some finite positive constant independent of @xmath7 and of the sequence @xmath5 .",
    "in particular , we may compute this bound for the random mistake sequences @xmath16 that we are interested in . in section [ sec : results ] we use this as a comparison with the empirical estimated algorithmic complexity which is obtained by compression .",
    "we proceed to describe the setup .",
    "the learning problem consists of predicting the next bit in a given sequence generated by a markov chain ( model ) @xmath98 of order @xmath8 .",
    "there are @xmath99 states in the model each represented by a word of @xmath8 bits . during a learning problem ,",
    "the source s model is fixed .",
    "a learner , unaware of the source s model , has a markov model of order @xmath9 .",
    "we denote by @xmath100 the probability of transiting from state @xmath101 whose binary @xmath9-word is @xmath102 $ ] to the state whose word is @xmath103 $ ] .",
    "given a random sequence of length @xmath6 generated by the source the learner estimates its own model s parameters @xmath100 by @xmath104 , @xmath105 , which is the frequency of the event `` @xmath106 is followed by a @xmath18 '' in the training sequence .",
    "we denote by @xmath107 the learnt model with parameters @xmath104 , @xmath105 .",
    "we denote by @xmath108 the transition probability from state @xmath101 of the source model , @xmath105 .",
    "a simulation run is characterized by the parameters , @xmath9 and @xmath6 .",
    "it consists of a training and testing phases . in the training phase",
    "we show the learner a binary sequence of length @xmath6 and he estimates the transition probabilities . in the testing phase we show the learner another random sequence ( generated by the same source ) of length @xmath7 and test the learner s predictions on it . for each bit in the test sequence",
    "we record whether the learner has made a mistake .",
    "when a mistake occurs we indicate this by a @xmath18 and when there is no mistake we write a @xmath17 .",
    "the resulting sequence of length @xmath7 is the generalization mistake sequence @xmath11 .",
    "we denote by @xmath16 the binary subsequence of @xmath11 that corresponds to the mistakes that occurred only when the learner predicted a @xmath17 .",
    "its length is denoted by @xmath109 .",
    "we denote by @xmath80 the probability of mistake when predicting a @xmath17 , i.e. , @xmath80 is the probability of seeing a @xmath18 in the subsequence @xmath16 .    for",
    "a fixed @xmath9 denote by @xmath110 the number of runs with a learner of order @xmath9 and training sample of size @xmath6 .",
    "the experimental setup consists of @xmath111 runs with @xmath112 , @xmath113 with a total of @xmath114 runs .",
    "the testing sequence is of length @xmath115 .",
    "each run results in a file called _ system _ which contains a binary vector @xmath116 whose @xmath12 bit represents the maximum _ a posteriori _ decision made at state @xmath101 of the learner s model , i.e. , @xmath117 for @xmath105 .",
    "let us denote by @xmath118 , thus @xmath119 are bernoulli random variables with parameters @xmath120 , @xmath105 .",
    "the learner s system is comprised of the decision at every possible state .",
    "another file generated is the _",
    "errort0 _ which contains the mistake subsequence @xmath16 . at the end of each run",
    "we measure the lengths of the _ system _ file and its compressed length where compression is obtained either via the gzip algorithm ( a variant of @xcite ) or the ppm algorithm @xcite and compute the",
    "_ sysratio _",
    "( denoted as @xmath121 which is the ratio of the compressed to uncompressed length of the system file .",
    "note that @xmath0 is a measure of information density since it captures the number of bits of useful information ( useful for describing the system ) per bit of representation ( in the uncompressed file ) .",
    "we do similarly for the mistake - subsequence @xmath16 obtaining the length @xmath122 of the compressed file that contains @xmath16 ( henceforth referred to as the estimated algorithmic complexity of @xmath16 since it is an approximation of the kolmogorov complexity of @xmath16 , see @xcite ) .",
    "we measure the kl - divergence @xmath123 between the probability distribution @xmath124 of binary words @xmath125 of length @xmath126 and the empirical probability distribution @xmath127 as measured from the mistake subsequence @xmath16 .",
    "note , @xmath124 is defined according to the bernoulli model with parameter @xmath128 , that is , @xmath129 for a word @xmath125 with @xmath101 ones , where @xmath128 is the frequency of ones in the subsequence @xmath16 .",
    "the distribution @xmath127 equals the frequency of a word @xmath125 in @xmath16 .",
    "hence @xmath123 reflects by how much @xmath16 deviates from being random according to a bernoulli sequence with parameter @xmath80 ( the mistake probability when predicting a @xmath17 ) .",
    "we are interested in determining the relationship between the estimated algorithmic complexity @xmath122 of @xmath16 , its divergence @xmath123 and the learning performance . as the learning performance we look at the generalization error of type @xmath17 that is the error for @xmath17-predictions .",
    "we choose four different levels of learning problems , controlled by the order of the source model @xmath130 , @xmath126 , @xmath131 , @xmath132 . for each problem",
    "we choose for the source model a transition matrix of probabilities @xmath133 , @xmath134 , where for some of the states @xmath101 we set @xmath135 and for others @xmath136 , @xmath137 .",
    "thus the bayes optimal error is @xmath138 . to ensure that the problem is sufficiently challenging we set the first half of the states ( those ranging from the @xmath8-dimensional vector @xmath139 to @xmath140 ) to have @xmath135 and the second half ( @xmath141 to @xmath142 ) to have @xmath136 .",
    "this ensures that a markov model of order @xmath143 can not approximate the true transition probabilities well .",
    "that is , the infinite - sample limit estimate based on a markov model of order @xmath9 which is smaller than @xmath8 will still be @xmath144 , @xmath105 .",
    "but for a markov model of order @xmath145 the infinite - sample size estimates will converge to the true values of @xmath97 or @xmath146 .      before we start to investigate the three relationships stated above we perform a sanity check to see how the prediction generalization error ( for any of the two prediction types , not just when predicting a zero ) varies with respect to the model complexity @xmath9 and training length @xmath6 .",
    "this is the so - called learning curves in the areas of statistical pattern recognition and learning theory @xcite .",
    "figure [ fig : generalization - error - versus ] displays the contours of the error surface as a function of @xmath9 and @xmath6 for a learning problem with @xmath147 ( the bayes error is @xmath138 ) . as can be seen , when @xmath143 the error remains very high , close to @xmath148 , regardless of the training sample size @xmath6 ( this is the leftmost contour colored in red ) . for @xmath149",
    "the prediction error gets closer to the bayes @xmath138 value ( outermost contour colored in dark blue ) with increasing @xmath6 .",
    "the shape of the contours indicate the tradeoff between approximation and estimation errors whose sum is the prediction error ( standard results from learning theory , see for instance @xcite ) .",
    "the larger that @xmath9 becomes the lower the approximation error . the larger that @xmath6 becomes the smaller the estimation error .",
    "generalization error with respect to @xmath9 and @xmath6 for @xmath147 ]    we now proceed to describe the main result which concerns the relationship between the learner s performance and the mistake sequence complexity .      first we look at the relationship between the sysratio @xmath0 and @xmath9 .",
    "figure [ fig : sysratio  versus k ] shows the average of the sysratio @xmath0 as a function of @xmath9 where in figure [ fig : sysratio  versus k](a ) we used gzip as the compressor that estimates the kolmogorov complexity and in figure [ fig : sysratio  versus k ] ( b ) we used the ppm algorithm as compressor .",
    "note that the ppm compressor obtains @xmath0 values that are smaller than the gzip compressor which means that the compressed lengths of the corresponding system files is smaller when using ppm .",
    "we believe that this is due to additional cost incurred by gzip in the form of data structures that are appended to the compressed data .",
    "this is more noticeable when the file to be compressed is small ( for instance , in the plot we see that the the sysratio only goes below unity at around @xmath150 which is when the uncompressed file length goes above @xmath151 ) .",
    "the ppm compressor thus approximates the algorithmic ( kolmogorov ) complexity better than gzip when the uncompressed files are relatively small . in the remainder of the paper we decided to keep the plots with respect to both types of compressors in order to show that the results of our analysis do not significantly vary as one changes from one type of compressor to another ( in some places we put only the gzip - based results since the differences were insignificant ) .",
    "looking at the plots of figure [ fig : sysratio  versus k ] it is clear that the average sysratio decreases as the learner s model order @xmath9 increases . for the ppm compressor ,",
    "we see a critical point at the vicinity of @xmath8 where the convexity of the graph changes from concave down to concave up possibly indicating an inflection point ( this holds for learning problems with other values of @xmath8 , for instance in appendix [ sec : apa ] we show this for @xmath130 and @xmath152 ) .",
    "to explain this , first note that the uncompressed length of the system is always @xmath153 for some constant @xmath79 since the vector @xmath116 is of length @xmath154 ( see section [ sec : experimentl - setup ] ) .",
    "the length of the compressed system file also grows , but at a slower rate with respect to @xmath9 and this gives rise to the decrease in @xmath0 with respect to @xmath9 .",
    "we can explain why the rate of the compressed system file grows more slowly as follows : for values of @xmath143 the learner s model is incapable ( by design of the learning problem ) of estimating the bayes optimal prediction and the probability of the events `` @xmath106 is followed by a @xmath18 '' is @xmath155 , @xmath105 .",
    "thus the average value @xmath104 of the indicators of such events is a binomial random variable with a distribution symmetric at @xmath22 and hence from ( [ eq : zi ] ) the probability @xmath120 that @xmath156 equals @xmath22 .",
    "the components of the random vector @xmath116 are independent bernoulli random variables with parameter @xmath120 when conditioned on the sample size vector @xmath157 ( this is the vector whose components @xmath158 are the number of times that @xmath106 appeared in the training sequence , see @xcite for details ) .",
    "since in this case @xmath159 then each component has a maximum entropy @xmath160 and hence the expected value of the entropy of the vector @xmath116 ( with respect to the random sample size vector @xmath157 ) is maximal and equals @xmath161 hence the expected compressed length of the system file ( which contains the vector @xmath116 ) is large as the expected description length of any random variable is at least as large as its entropy .",
    "as @xmath9 increases beyond @xmath8 the model becomes more capable of estimating the true transition probabilities ( recall , these are either @xmath138 or @xmath162 ) and the probability @xmath100 of the events `` @xmath106 is followed by a @xmath18 '' get farther away from @xmath22 in the direction of @xmath138 or @xmath162 , depending on the particular state @xmath101 , @xmath105 .",
    "thus the average value @xmath104 of the indicators of such events is a binomial random variable with an asymmetric distribution with a mean @xmath163 ) .",
    "hence from ( [ eq : zi ] ) the probability @xmath120 that @xmath156 gets either very close to @xmath17 or @xmath18 as the training size @xmath6 increases .",
    "thus the components of the random vector @xmath116 tend to be closer to deterministic .",
    "they are still random since the training sequence length is not increasing with @xmath9 and the variance of the estimates @xmath104 does not converge to zero .",
    "therefore for each of the @xmath154 components of the vector @xmath116 the entropy is smaller than when @xmath143 .",
    "however as there are exponentially many components @xmath119 , on the whole , the entropy of @xmath116 ( and hence the expected compressed length of the system file ) still increases but at a lower rate than when @xmath143 .",
    "we can now alternatively look at the learning curves ( section [ sub : learning - curves ] ) based on the sysratio ( instead of @xmath9 ) .",
    "this is shown in figure [ fig : learning - curves - with ] .",
    "clearly , good learners are those with low value of sysratio @xmath0 ( left uppermost region which is colored dark blue ) while bad learners are those with a high sysratio @xmath0 , displayed as the rightmost contour which spans from lowest to highest @xmath6 values .",
    "generalization error with respect to @xmath0 and @xmath6 ( for @xmath147 ) ]    we proceed now to discuss the characteristics of the mistake subsequence @xmath16 .",
    "first , in section [ sub : estimated - algorithmic - complexity ] we study how its estimated algorithmic complexity @xmath122 and divergence @xmath123 depend on the learner s decision characteristics , or formally , the sysratio @xmath0 . in section [ sub : estimated - algorithmic - complexity_dif ] we fix the learner s model order @xmath9 and study how @xmath122 depends on @xmath123 . finally in sections [ sub : the - error - surface ] and [ sub : the - sysratio - surface ] we study the @xmath80 and @xmath0 surfaces over the @xmath1-plane .",
    "note , in the plots of this section we use the average sysratio which is computed by taking for each value of @xmath112 the average over the @xmath164 runs .",
    "figure [ fig : algorithmic - complexity- ] shows the graph ( with @xmath165 ) of the average estimated algorithmic complexity @xmath122 of @xmath16 versus the average system ratio @xmath0 .",
    "the dashed lines are the upper and lower envelopes of the estimated standard deviation from the mean .",
    "this variance arises from the different values of training size @xmath6 and from the fact that both the training and test sequences are random .",
    "the arrow points at the value of @xmath166 that corresponds to @xmath147 ( the source model order ) .",
    "as can be seen , for low values of @xmath0 the spread in @xmath122 is low .",
    "there is a critical point at @xmath166 where the spread around the mean value of @xmath122 increases significantly as @xmath0 increases .",
    "we know from section [ sec : selection - rule ] that the higher the algorithmic complexity of a selection rule the higher the possible deviation of the frequency of @xmath18s in the selected subsequence ( the stochastic deviation ) .",
    "as mentioned above , in @xcite it was shown that the decision rule of a learner can be represented as a selection rule that picks the subsequence corresponding to the mistakes made when predicting @xmath17s in the input test sequence .",
    "the theory predicts that the stochastic deviation of the mistake sequence @xmath16 grows as the complexity of the decision rule increases .",
    "we now validate this experimentally .",
    "figure [ fig : divergence  of ] displays the graph ( with @xmath165 ) of the average divergence @xmath123 of the mistake subsequence @xmath16 versus the average of the sysratio @xmath0 where again averages are taken over the @xmath167 runs as described above .",
    "the dashed lines are the upper and lower envelopes of the standard deviation from the mean .",
    "the arrow points at the value of @xmath166 that corresponds to @xmath8 ( the source model order ) .",
    "as can be seen , for low values of @xmath0 the spread of @xmath123 is low .",
    "similar to the previous result for @xmath122 , also here we see a relative minimum at @xmath166 where the standard deviation around the mean value of @xmath123 increases once we increase @xmath0 beyond @xmath166 . since we know there is an inverse relationship between @xmath0 and @xmath9 ( figure [ fig : sysratio  versus k ] ) then the small hook shape that appears to the left of the plot in figure [ fig : divergence  of ] indicates an increase in the @xmath123 value as @xmath9 increases beyond @xmath8 ( @xmath0 decreases below @xmath166 ) .",
    "thus data overfitting ( which occurs when @xmath168 is depicted here via this slight increase in the divergence @xmath123 as we decrease @xmath0 beyond the @xmath166 .",
    "it follows from this result that the sysratio @xmath0 ( which is a measure of information density of the learner s model @xcite ) influences how random are the mistakes made by a learner .",
    "the sysratio @xmath0 is a proper measure of complexity of a learner s decision rule since it is with respect to @xmath0 that the characteristics of the random mistake subsequence @xmath16 are consistent with the theory @xcite , namely , the higher the sysratio the more significant the deviation @xmath123 of @xmath16 from a pure bernoulli random sequence .",
    "we have so far considered @xmath0 as an independent variable . in section [ sub : the - sysratio - surface ] we study the sysratio as a dependent variable , i.e. , as a function of the estimated algorithmic complexity @xmath122 and divergence @xmath123 . before looking at that we proceed to show how @xmath122 varies with respect to the error @xmath80 which will now play the role of the independent variable .",
    "we first mention that in all the figures below we reduced the number of data points ( using simple random sampling ) for clarity of presentation .",
    "figure [ fig : error - versus - estimated ] shows the estimated algorithmic complexity @xmath122 of the mistake subsequence @xmath16 versus the probability of error @xmath80 .",
    "the curves are a second order regression .",
    "for @xmath169 there is no clear relationship but for @xmath170 ( just above @xmath171 we see a sharp rise in @xmath122 with respect to an increasing @xmath80 ( the regression polynomial is : @xmath172 ) . when @xmath173 ( double the value of @xmath171 we see a less steep increase ( the regression polynomial is : @xmath174 ) .",
    "estimated algorithmic complexity @xmath122 versus error probability , for @xmath175 , @xmath132 and @xmath176 and @xmath147 , gzip - based compressor ]      in figure [ fig:(a)-estimated - algorithmic](a ) we compare @xmath122 marked in red ( @xmath165 ) to the entropy - based estimate of ( [ eq : entropyest ] ) marked in blue ( @xmath177 ) where we substitute for @xmath7 in ( [ eq : entropyest ] ) the length @xmath109 of the sequence @xmath16 and the probability @xmath80 for the parameter @xmath97 .",
    "the value of the pearson s correlation coefficient between @xmath122 and the entropy estimate is @xmath178 indicating a high correlation ( almost linear ) .",
    "thus the entropy - based estimate appears to be good for the whole population of learners which consists of training sequences of size @xmath179 and models of order @xmath180 . in figure [ fig:(a)-estimated - algorithmic](a ) for the @xmath122 data ( marked by @xmath181 there appear to be two clusters of points ( sequences ) separated by an error probability gap at @xmath182 .",
    "the first region is for @xmath183 .",
    "we refer to it as the _ cool _ cluster . here the complexity @xmath122 values are concentrated .",
    "the other cluster ( termed _ hot _ ) is where @xmath184 . here",
    "the spread in values of @xmath122 is significantly larger than in the cool cluster .    in figure [ fig:(a)-estimated - algorithmic](b )",
    "we see that the divergence @xmath123 ( marked by the symbols @xmath185 ) and the complexity @xmath122 ( marked by @xmath165 ) are somewhat correlated ( pearson s coefficient of @xmath186 ) and it is due to the fact that the divergence values @xmath123 are also split into two clusters which are in correspondence with the two clusters of the @xmath122 values .",
    "let us look at the distribution of @xmath122 which is shown in figure [ fig : histogram - of- ] .",
    "the distribution is very similar for both types of compressors . for the gzip - based and ppm - based compressors the mean values are @xmath187 , @xmath188 and the distributions have skewness of @xmath189 , @xmath190 and kurtosis of @xmath191 , @xmath192 , respectively ( for the normal distribution the skewness and kurtosis are @xmath193 .",
    "this indicates that the distributions are positively asymmetric ( a heavier right tail ) and peaked .",
    "figure [ fig:3dplotoferrorvsdivgncalgcomp ] depicts the first central result of the paper .",
    "it displays the error probability @xmath80 as a function of the divergence @xmath123 and estimated algorithmic complexity @xmath122 ( we note that the jagged contour lines are due to the interpolation mesh being limited in size and do not reflect actual data ) . at the center bottom",
    "we see the contour level of @xmath194 ( this is approximately the bayes error level ) and the topmost contour is at a value of @xmath195 which corresponds to prediction by pure - guessing .",
    "we can ascertain the following from this interesting plot : the population of mistake sequences of lowest error probability ( close to the bayes @xmath138 value ) concentrates close to the mean value @xmath196 and has a very low divergence @xmath123 .",
    "this region corresponds to the cool cluster of figure [ fig:(a)-estimated - algorithmic ] ( we call it the cool region and it appears in blue in figure [ fig:3dplotoferrorvsdivgncalgcomp ] ) .",
    "this characteristic indicates that the sequences in the cool region are close to being truly random bernoulli sequences with parameter @xmath80 .",
    "as we start to look at a population of sequences with a higher error probability @xmath80 and walk along its fixed contour level we have a tradeoff between two possible choices : ( 1 ) to have a complexity @xmath122 value which is far from the mean ( less than or greater than @xmath196 ) and maintain a low divergence @xmath123 value or ( 2 ) to have a large divergence @xmath123 and maintain an @xmath122 which is close to the mean @xmath196 .",
    "the union of the red and orange regions in figure [ fig:3dplotoferrorvsdivgncalgcomp ] corresponds to the hot cluster that we saw in figure [ fig:(a)-estimated - algorithmic ] . by definition of the maximum _ a posteriori _ probability decision rule that we are using the error",
    "can never exceed @xmath148 so the true error surface can not exceed @xmath148 and this is why we see that the empirical error surface ends at a contour level close to @xmath148 .    an interesting point that we see here",
    "is that this surface is defined only over a part ( colored region ) of the @xmath1-plane .",
    "we term this the _ admissible _ region of the @xmath1 plane and it is induced by the error surface . in figure [ fig:3dplotoferrorvsdivgncalgcomp ]",
    "we see that the contour area is slightly larger on the right side of @xmath196 than on the left of @xmath196 which is consistent with the heavier right tail of the @xmath122 distribution in figure [ fig : histogram - of- ] .",
    "so admissibility appears to have a slight intrinsic bias towards complexity values @xmath122 that are larger than the mean @xmath196 .    if we regard sequences in the the cool region as _",
    "truly _ _ random _",
    "( i.e. , having a complexity value @xmath122 close to the mean @xmath196 and a low divergence from bernoulli ) then we can introduce a new perspective on the process of learning .",
    "when the process is perfect , it produces a bayes optimal predictor whose mistake sequence falls in the cool region .",
    "but when it is imperfect ( due to limited training size @xmath6 or improper model order @xmath9 ) the process produces a _ malformed _ sequence which is either atypically chaotic ( @xmath122 far from @xmath196 ) but stochastic ( low @xmath123 ) or typically chaotic ( @xmath122 close @xmath196 ) but atypically stochastic ( large @xmath123 ) .",
    "so far we discussed the error surface which is intrinsically a property of the random mistake sequence since @xmath80 is defined only based on the ratio of the number of @xmath18s to the length of the sequence . in the next section",
    "we examine the sysratio surface which intrinsically is a learner s characteristic since it measures the information density @xmath0 of the learner s decision rule .",
    "figure [ fig : sysratio  asfunc_dvi_alg ] displays the next central result of the paper , a contour plot of the sysratio @xmath0 over the @xmath1-plane .",
    "the outer contours ( red ) are for higher values of @xmath0 .",
    "there are two relative minima one of which is at a lower value of @xmath123 and touches the @xmath197 axis while the other appears above the @xmath198 divergence level .",
    "based on what we already know about @xmath0 versus @xmath9 ( figure [ fig : sysratio  versus k ] ) we can conclude that the lower minimum in figure [ fig : sysratio  asfunc_dvi_alg ] is in a region of the plane that corresponds to sequences generated by learners of order @xmath9 which is equal to @xmath8 or just slightly above @xmath8 ( we call this region _ @xmath199 _ for overfitting minimum ) while the upper relative minimum in figure [ fig : sysratio  asfunc_dvi_alg ] is in the region of sequences generated by learners of order @xmath9 which is slightly lower than @xmath8 ( we call this region @xmath200 for underfitting minimum ) .",
    "the remaining regions ( colored green to red ) are where the learners have an order @xmath9 significantly less than @xmath8 .",
    "thus there is a saddle point as one passes from @xmath200 to @xmath199 and cross from @xmath9 which is just under @xmath8 to @xmath201 .",
    "this is more pronounced in the gzip - based compressor than in the ppm - based compressor .",
    "based on this plot we can see that a decision rule with a high information density ( sysratio value @xmath0 ) yields an atypically chaotic random error sequence , i.e. , with an estimated algorithmic complexity value @xmath122 that is far from the mean @xmath196 . as the information density of the decision rule decreases the complexity of the error sequence moves towards a typical value ( @xmath122 closer to @xmath196 ) and its divergence from bernoulli decreases towards zero .",
    "recall from the end of section [ sec1 ] that the act of predicting bits of the input test sequence @xmath202 to be @xmath17s is equivalent to selecting from @xmath202 a subsequence @xmath16 .",
    "we are now in a position to understand that this selection process produces random binary sequences @xmath16 of different character and spreads them in different regions of the @xmath1-plane .",
    "this spreading is a consequence of what we term _ scattering _ bits of a sequence since it resembles particle scattering in physics ( it is also similar to the concept of chaotic scattering @xcite where instead of initial conditions of the learner we characterize it by its information density @xmath0 ) .",
    "given a random input sequence @xmath5 the learner ( in his decision / selection action ) effectively scatters the bits of @xmath5 in a way that resembles the binary collisions of particles in a beam with other particles that knock the beam particles into different directions . from this scattering",
    "the resulting sequence of bits is @xmath16 .",
    "the learner here acts as a static structure ( a solid of some kind ) , or a localized target such as a thin foil in a physical scattering experiment .",
    "learners with high information density@xmath0 scatter bits of the input sequence more wildly thereby producing sequences ( points in the @xmath1-plane ) that deviate from typical complexity values or have high stochastic divergence . as mentioned in section [ sec : overview ]",
    "this is in line with the model introduced in @xcite where a static structure is said to deform the randomness characteristics of an input sequence of excitations .",
    "it is interesting to ask at this point whether as a consequence of this phenomenon it may perhaps be possible to optimally fine - tune a learner s model - order @xmath9 just by observing the randomness characteristics of the mistake error @xmath16 , i.e. , adjusting @xmath9 in a direction that corresponds to decreasing @xmath0 towards the @xmath199 region .",
    "it is not yet clear whether such a scheme that monitors the random characteristics of the mistake sequence would yield better performance ( either accuracy or computational efficiency ) compared to doing standard model - selection which adjusts @xmath9 directly based on some form of estimate of the generalization error @xcite .",
    "this paper is an experimental investigation of the problem that was posed and theoretically solved in @xcite .",
    "we have reconfirmed that the sysratio @xmath0 originally introduced in @xcite is a proper measure of the complexity of a learner s decision rule as it is with respect to @xmath0 that the deformation of randomness of the mistake subsequence @xmath16 takes place in consistence with the theory , namely , the higher the value of @xmath0 the more significant the divergence @xmath123 of the mistake sequence @xmath16 relative to a pure bernoulli random sequence .",
    "the two central results introduced in the current paper depict the special structure of the error probability @xmath80 and sysratio @xmath0 surfaces over the @xmath1-plane .",
    "they imply that bad learners generate atypically complex or stochastically divergent mistake sequences while good learners generate typically complex sequences with low divergence from bernoulli .",
    "since a learner can be modeled as a selection rule we name this phenomenon bit - scattering. the idea follows the general model of static algorithmic interference introduced in @xcite whereby effectively the learner acts as a static structure whose complexity is the sysratio ( information density @xmath0 ) .",
    "it produces randomly - deformed types of mistake sequences where deformation is proportional to @xmath0 .",
    "in this section we present some additional auxiliary results pertaining to the relationship between the sysratio @xmath0 and model order @xmath9 . in section [ sub : sysratio  versus ] for a learning problem with @xmath147 we saw that for the ppm - based compressor the graph of the average @xmath0 versus @xmath9 is decreasing and has a critical point in the vicinity of @xmath8 .",
    "figure [ fig : the - critical - point ] shows that this critical point also appears in learning problems with @xmath130 .",
    "for @xmath152 there appears to be two critical points , one of which is at @xmath8 .",
    "j.  ratsaby .",
    "some consequences of the complexity of intelligent prediction",
    ". presented at _ international symposium on understanding intelligent and complex systems ( uics09 ) _ , petru maior university , 2009 .",
    "j.  ratsaby and i.  chaskalovic .",
    "random patterns and complexity in static structures . in _ proc .",
    "of international conference on artificial intelligence and pattern recognition ( aipr09 ) _ , volume iii of _ mathematics and computer science _ , pages 255261 , 2009 ."
  ],
  "abstract_text": [
    "<S> we investigate a population of binary mistake sequences that result from learning with parametric models of different order . </S>",
    "<S> we obtain estimates of their error , algorithmic complexity and divergence from a purely random bernoulli sequence . </S>",
    "<S> we study the relationship of these variables to the learner s information density parameter which is defined as the ratio between the lengths of the compressed to uncompressed files that contain the learner s decision rule . </S>",
    "<S> the results indicate that good learners have a low information density@xmath0 while bad learners have a high @xmath0 . </S>",
    "<S> bad learners generate mistake sequences that are atypically complex or diverge stochastically from a purely random bernoulli sequence . </S>",
    "<S> good learners generate typically complex sequences with low divergence from bernoulli sequences and they include mistake sequences generated by the bayes optimal predictor . based on the static algorithmic interference model of @xcite the learner here acts as a static structure which `` scatters '' the bits of an input sequence ( to be predicted ) in proportion to its information density @xmath0 thereby deforming its randomness characteristics . </S>"
  ]
}