{
  "article_text": [
    "in the application of optimal filtering for the detection of a signal buried in noise , often it is useful the procedure of whitening  @xcite .",
    "if the noise in which the signal is hidden is non stationary or non gaussian noise we can not apply anymore the optimal filter for stationary and gaussian noise .",
    "we focused this work on the application of least squares lattice  @xcite algorithm to the problem of identification of non stationary noise in stochastic process , to the procedure of whitening and to the possibility of making stationary a non stationary process .    to this aim",
    "we apply this algorithm to some toy models we built using an autoregressive non stationary model ( see section  [ sec : toy ] )  @xcite . in section  [ sec : whitening ]",
    "we show the whitening techniques based on a lattice structure , in section  [ sec : lsl ] we introduce the adaptive least squares methods and its application to simulated non stationary data . in sections  [ sec : toy ] and",
    "[ sec : voice ] we show how it is possible to whiten the data and to eliminate the non stationarity present in the noise .",
    "we apply this algorithm to simulated and real data .",
    "an auto - regressive process @xmath0 $ ] of order @xmath1 with parameter @xmath2 , from here after @xmath3 , is characterized by the relation @xmath4=\\sum _ { k=1}^{p}a_{k}x[n - k]+\\sigma w[n]\\ , , \\ ] ] being @xmath5 $ ] a white normal process .",
    "the problem of determining the ar parameters is the same of that of finding the optimal `` weights vector '' @xmath6 , for @xmath7 for the problem of linear prediction  @xcite . in the linear prediction we would predict the sample @xmath8 $ ] using the @xmath9 previous observed data @xmath10=\\{x[n-1],x[n-2]\\ldots x[n - p]\\ } $ ] building the estimate as a transversal filter : @xmath11=\\sum _ { k=1}^{p}w_{k}x[n - k]\\ , .\\ ] ]    we choose the coefficients of the linear predictor by minimizing a cost function that is the mean squares error @xmath12^{2 } ] $ ] ( @xmath13 is the operator of average on the ensemble ) , being @xmath14=x[n]-\\hat{x}[n]\\ ] ] the error we make in this prediction and obtaining the so called normal or wiener - hopf equations @xmath15-\\sum _ { k=1}^{p}w_{k}r_{xx}[-k]\\ , , \\ ] ] which are identical to the yule  walker equations  @xcite used to estimated the parameters @xmath2 from autocorrelation function with @xmath16 and @xmath17    this relationship between ar model and linear prediction assures us to obtain a filter which is stable and causal  @xcite , so we can use the @xmath18 model to reproduce stable processes in time - domain .",
    "it is this relation between ar process and linear predictor that becomes important in the building of whitening filter .",
    "the tight relation between the ar filter and the whitening filter is clear in the figure  [ fig : ar - predic ] .",
    "the figure describes how an ar process colors a white process at the input of the filter if you look at the picture from left to right .",
    "if you read the picture from right to left you see a colored process at the input that pass through the ar inverse filter coming out as a white process .    when we find the @xmath9 parameters that fit a psd of a noise process , what we are doing is to find the optimal vector of weights that let us reproduce the process at the time @xmath19 knowing the process at the @xmath9 previous times .",
    "all the methods that involve this estimation try to make the error signal ( see equation ( [ eq : error ] ) ) a white process in such a way to throw out all the correlation between the data ( which we use for the estimation of the parameters ) .",
    "the least squares based methods build their cost function using all the information contained in the error function at each step , writing it as the sum of the error at each step up to the iteration @xmath20 : @xmath21=\\sum _ { i=1}^{n}\\lambda ^{n - i}e^{2}(i|n)\\ , , \\ ] ]    being @xmath22-\\sum _ { k=1}^{n}x_{i - k}w_{k}[n],\\ ] ]    where @xmath23 is the signal to be estimated , @xmath24 are the data of the process and @xmath25 the weights of the filter .",
    "the forgetting factor @xmath26 lets us tune the learning rate of the algorithm .",
    "this coefficient can help when there are non stationary data in the time series and we want the algorithm has a short memory .",
    "if we have stationary data we fix @xmath27 , otherwise we choose @xmath28    there are two ways to implement the least squares methods for the spectral estimation : in a recursive way ( recursive least squares or kalman filters ) or in a lattice filters using fast techniques  @xcite .",
    "the first kind of algorithm , examined in  @xcite , has a computational cost proportional to the square of the order of filter , while the cost of the second one is linear in the order @xmath29    the computational cost of rls is prohibitive for an on line implementation .",
    "moreover its structure is not modular , thus forcing the choice of the order @xmath1 once for all .",
    "the algorithm with a modular structure like that of the lattice offers the advantages of giving an output of the filter at each stage @xmath30 , so in principle we can change the order of the filter by imposing some criteria on its output .",
    "the least square lattice filter is a modular filter with a computational cost proportional to the order @xmath1 .    in the least squares methods",
    "the linear prediction is made for a vector of data @xmath31 $ ] , so the natural space where developing these methods are the vectorial spaces ( a detailed insight in these techniques is reported in  @xcite ) .",
    "let @xmath32 be a hilbert @xmath33dimensional space , to which the vectors @xmath34 $ ] of acquired data belong .",
    "the @xmath30 vectors @xmath35 $ ] of length @xmath20 obtained as time translation of length @xmath30 of the vector @xmath34 $ ] @xmath36=z^{-1}\\bfx[n]&=&(0,0,x[1], ... ,x[n-1])\\ , \\nonumber\\\\    \\bfx_2[n]=z^{-2}\\bfx[n]&=&(0,0,0,x[1], ...",
    ",x[n-2])\\ , \\nonumber\\\\    \\vdots & = & \\vdots \\nonumber\\\\    \\bfx_p[n]=z^{-p}\\bfx[n]&=&(0,0,0,\\ldots , x[1], ... ,x[n - p])\\nonumber\\end{aligned}\\ ] ] form a base of this space .",
    "a vector @xmath37 which belongs to this space can be written as @xmath38=\\sum_{k=1}^{p}a_k\\bfx_k[n]\\ , .\\ ] ] the vector @xmath39 $ ] with last component @xmath0 $ ] does not belong to this space , but to a vectorial @xmath40-dimensional space @xmath41 . in the problem of linear prediction the best estimation of desired signal @xmath42 $ ] , that is @xmath39 $ ] ,",
    "is obtained using the vector lying in the space @xmath32 .",
    "therefore the least squares methods look for a vector @xmath31 $ ] which is the closest to the vector @xmath42 $ ] , by minimizing the norm of the distance between @xmath31 $ ] and @xmath42 $ ] .",
    "it can be shown that this operation corresponds to the projection of the vector @xmath42 $ ] from the @xmath40-dimensional space @xmath43 in the @xmath30-dimensional sub - space @xmath44 by a projector @xmath45 .",
    "we can decompose the vector @xmath42 $ ] as sum of the vector @xmath31 $ ] and a vector which has null component only along the vector orthogonal to the space @xmath44 .",
    "this vector is the vector @xmath46 which , by definition , is orthogonal to the data vector @xmath34 $ ] .",
    "in fact the orthogonal vector to the space @xmath44 can be obtained as    @xmath47=\\bfd[n]-{\\bf p}\\bfd[n]=\\bfd[n]-\\hat{\\bfx}[n]={\\bf e}(n|n)\\ .\\ ] ]    therefore the vector @xmath42 $ ] belongs to the vectorial space @xmath41 , direct sum of the sub - space @xmath32 and of sub - space @xmath48 defined by the vector @xmath46    @xmath49    for the ls adaptive algorithm we want to write the quantities we need for the estimation of @xmath31 $ ] at the iteration @xmath20 by means of the quantities at the iteration @xmath50 and if we use a modular structure the same quantities at the stage @xmath30 in terms of the ones at the stage @xmath51 .    using the described techniques ,",
    "if we augment the order of the filter from @xmath30 to @xmath40 , we must write the new projector @xmath52 as function of the operator @xmath53 . the new vectorial space will be the direct sum of @xmath32 of dimension @xmath30 and of the @xmath54-dimensional sub - space orthogonal to @xmath32 along which there is @xmath46 and the projector will be @xmath55 where we wrote @xmath56 to point the projector on the one - dimensional space @xmath57 .",
    "if we add a new data @xmath0 $ ] to the space @xmath58 , we introduce the vector @xmath59 $ ] orthogonal to the space @xmath60 and the new projection of the signal @xmath42 $ ] along @xmath61 will be    @xmath62\\bfd[n]=\\bfp[n-1]\\bfd[n-1]+{\\bf",
    "p}_{\\bfl[n]}\\bfd[n]=\\\\ \\nonumber \\hat{\\bfx}[n-1 ] + { \\bf p}_{\\bfl[n]}\\bfd[n]\\ .\\end{aligned}\\ ] ]    then we can write in a matrix form the relation  ( [ eq : proiec_time ] ) @xmath63=\\left(\\begin{array}{ll }        { \\bf p}[n-1]&0\\\\        0&1 \\end{array}\\right)\\ , .\\ ] ]    a useful parameter which is introduced is the angle @xmath64 $ ] between the two sub - spaces @xmath65 $ ] and @xmath66 $ ] which can be obtained from the relation @xmath67=<\\bfl[n],{\\bf p}^\\perp_p[n]\\bfl[n]>\\ , \\ ] ] where we introduced the scalar product @xmath68 between the two vectors @xmath69 $ ] @xmath70 $ ] defined as @xmath71,\\bfb[n]>=\\sum_{k=1}^{n}\\lambda^{n - k}u[k]v[k]\\ , \\ ] ] and @xmath72={\\bf i}-{\\bf p}_p[n]$ ] .",
    "let us remember that @xmath73 is the forgetting factor ; if we limit ourselves to @xmath74 the scalar product @xmath68 is simply @xmath75 .",
    "we can write an adapting relation for the projector in term of the vector @xmath76 @xmath77 & 0\\\\ 0&0 \\end{array}\\right]=\\bfp_p^\\perp[n]-\\frac{\\bfp_p^\\perp[n]\\bfp_{\\bfl}[n ] \\bfp_p^\\perp[n]}{\\gamma_p[n]}\\ .\\ ] ]    it is important to note that , thanks to the properties of the projector , the number of operation per iteration is now proportional to the order @xmath1 and not to @xmath78 as for the rls algorithm .    the lsl filter is a lattice filter characterized by recursive relation between the forward error ( fpe ) and the backward one ( bpe ) . with the new notation we can write    @xmath79=\\bfx[n]-\\hat{\\bfx}[n]=\\left[{\\bf i}-{\\bfp}_p[n]\\right]\\bfx[n]\\ .\\ ] ]    the scalar error @xmath80 $ ] can be written as the component along the direction @xmath59 $ ] of the vector perpendicular to the sub - space @xmath81 $ ] @xmath82,\\bfe_p^f[n]>\\ .\\ ] ] in a similar way we can write the backward errors .",
    "for the backward errors the space where we make the prediction is different from the sub - space @xmath83 $ ] because the base is now given by @xmath84,\\ldots , z^{-(p-1)}\\bfx[n]$ ] .",
    "if we introduce the projector @xmath85 on this new base we can write @xmath86=\\left [ { \\bf i}-\\bfp_{p-1}[n]\\right]z^{-p}\\bfx[n]\\ .\\ ] ] the scalar backward error is given by @xmath87=<\\bfl[n],\\bfe_p^b[n]>\\ , .\\ ] ] the square sum for the forward and backward errors can be written as @xmath88&=&<\\bfe_p^f[n],\\bfe_p^f[n]>\\\\    \\epsilon_p^b[n]&=&<\\bfe_p^b[n],\\bfe_p^b[n]>\\end{aligned}\\ ] ] now we can write the recursive relations for the projectors in the equations  ( [ eq : fpelsl ] ) and  ( [ eq : bpelsl ] ) @xmath89&=&e^f_p[n]+k_{p+1}^b[n]e^b_p[n-1]\\ , \\\\",
    "\\label{eq : reticolo1 } e^b_{p+1}[n]&=&e^b_p[n-1]+k_{p+1}^f[n]e^f_p[n]\\ , \\end{aligned}\\ ] ] where we introduced the forward @xmath90 and backward @xmath91 reflection coefficients defined by @xmath92&=&-\\frac{<z^{-1}\\bfe_p^b[n],\\bfe_p^f[n]>}{\\epsilon^b_p[n-1]}\\ , \\\\ & & \\nonumber\\\\   k^f_{p+1}[n]&=&-\\frac{<z^{-1}\\bfe_p^b[n],\\bfe_p^f[n]>}{\\epsilon^f_p[n]}\\ .\\end{aligned}\\ ] ] the adaptive implementation of the lsl filter  ( [ eq : reticolo ] )  ( [ eq : reticolo1 ] ) requires the updating with order @xmath30 and time @xmath20 of the reflection coefficients .",
    "so we must write the recursive relation for the quantities @xmath93 $ ] , @xmath94 $ ] and @xmath95=<z^{-1}\\bfe_p^b[n],\\bfe_p^f[n]>$ ] .",
    "this can be done using the updating formula @xmath96&=&\\lambda\\delta_{p+1}[n-1]+ \\frac{e^b_{p}[n-1]e^f_{p}[n]}{\\gamma_p[n-1]}\\ , \\\\",
    "\\epsilon^f_{p+1}[n]&=&\\lambda\\epsilon^f_{p}[n]- \\frac{\\delta^2_{p+1}[n]}{\\epsilon_p^b[n-1]}\\ , \\\\ \\epsilon^b_{p+1}[n]&=&\\lambda\\epsilon^b_p[n-1]- \\frac{\\delta^2_{p+1}[n]}{\\epsilon_p^f[n ] } \\ , \\\\ \\gamma_{p+1}[n-1]&=&\\gamma_p[n-1]- \\frac{[e_p^b[n-1]]^2}{\\epsilon_p^b[n-1]}\\ .\\end{aligned}\\ ] ]    the error @xmath97 at the last stage is the whitened sequence of the input data .",
    "so at the output of lsl filter we find the reflection coefficients we can use for the estimation of the ar parameters for fit to psd of the time series .",
    "moreover one of the output of this filter is the whitened sequence of data .",
    "the procedure described for the implementation of lsl filter is the so called aposteriori procedure  @xcite .",
    "since this algorithm involves division by updated parameters , we must be careful in avoiding division by values too small . in the aposteriori procedure ,",
    "the reflection coefficients are estimated indirectly by the estimation of @xmath98 and @xmath99 .    in the apriori implementation , the reflection coefficients are estimated directly by the forward and backward errors .    in the apriori implementation the recursive relation for the parameters",
    "are given by @xmath100 & = & \\lambda \\epsilon^f_{p-1}[n-1]+ \\gamma_{p-1 } e^f_{p-1}[n ] e^f_{p-1}[n]\\ , \\\\",
    "\\epsilon^b_{p-1}[n ] & = & \\lambda \\epsilon^b_{p-1}[n-1]+ \\gamma_{p-1 } e^b_{p-1}[n-1 ] e^b_{p-1}[n-1]\\ , \\\\ e^f_{p } [ n]&= & e^f_{p-1}[n]+ k^f_{p}[n-1 ] e^b_{p-1}[n-1]\\ , \\\\ e^b_{p}[n ] & = & e^b_{p-1}[n-1]+   k^b_{p}[n-1 ] e^f_{p-1}[n]\\ , \\\\        k^f_{p}[n ] & = & k^f_p[n-1]-\\frac{\\gamma_{p-1}e^b_{p-1}[n]e^f_{p}[n]}{\\epsilon^b_{p-1}[n]}\\ , \\\\ k^b_p[l ] & = & k^b_p[n-1]-\\frac{\\gamma_{p-1}e^f_{p-1}[n]e^b_{p}[n]}{\\epsilon^f_{p-1}[n]}\\ , \\\\ \\gamma_{p}&=&\\gamma_{p-1}-\\frac{\\gamma_{p-1}^2|e^b_{p-1}[n-1]|^2}{\\epsilon^b_{p-1}[n]}\\ . \\\\\\end{aligned}\\ ] ] since the apriori implementation is more stable with respect the aposteriori one , we choose to use the apriori recursive relation for the lsl filter to perform tests on non stationary data .",
    ".modified lsl algorithm [ cols= \" < \" , ]     in the above relations no one of the parameters gives a direct estimation of the @xmath101 of the guiding white noise process for the ar model .",
    "we have to estimate it by using the quantities @xmath98 or @xmath99 . in particular",
    "if we suspect to have non stationary data in which the overall rms of the process changes in time we have to estimate it step by step .",
    "the relation we used to estimate @xmath101 in lsl filter is : @xmath102=\\sqrt{\\epsilon^f_p[n]}/p\\ , , \\ ] ] being @xmath1 the order we choose for the filter and consequently for the ar fit to the psd .",
    "moreover we have to normalize this quantity with respect to the number of iterations we used to estimated it . if @xmath74 we used all the data to achieve the converging value for @xmath103 $ ] , so we have to divide this value at the step @xmath104 , by the length @xmath104 of the time series .",
    "if we used @xmath105 we have to normalize by the window of data we used that is equal to @xmath106 , called the memory of the filter .",
    "if the @xmath101 varies in time we well find a @xmath101 that follows the changes , choosing a good value for the parameter @xmath73 .",
    "the novelty in our algorithm is the introduction of a normalization of the output of the whitening filter in such a way to make the process white and stationary .",
    "we accomplish this task by estimating the @xmath103 $ ] at each step @xmath20 and the by diving the output @xmath107 $ ] , which is our whitened time series , by @xmath103 $ ] .    if we do not normalize the output of the whitening filter by the varying @xmath101 we will have white non stationary data , but if we divide the output by @xmath101 we will find white stationary data , that are what we need in applying optimal signal search filter for gaussian and stationary data . in the next section we show the results of the application of this modified version of lsl algorithms to simulated non stationary data .",
    "we build an ar process of order @xmath108 simulating a power spectral density in which one resonance is present and we let varying the frequency of the peak changing in time the value of one of the two parameter .",
    "the value we use are the following : @xmath109 choosing a sampling frequency of @xmath110 .",
    "moreover we let @xmath111 vary in time with the following law : @xmath112 with the values @xmath113 @xmath114 and @xmath115 .",
    "we fit this process using lsl filter and whiten the data , using @xmath116 . in figure  [ fig : ar1 ] we show the simulated time varying parameter and the estimated one . in figure",
    "[ fig : ar2 ] we show the simulated time series and the output of the lsl whitening filter .    in these data",
    "it is one of the parameters of the ar model which changes its value in time , so when we estimate the reflection coefficients from the data we find also this variation in time if we use this forgetting factor @xmath117 .",
    "so the division of the output of the process by the estimated @xmath101 does nt influence the whitening of the data , since the values of the @xmath101 is constant in time .",
    "we check these results also by plotting the psd at the input and at the output of the modified lsl filter and we plot the in figure  [ fig : ar3 ] . in this figure",
    "the peak of the psd results broadened due to the moving of the main resonance , while after the application of the lsl algorithm with forgetting factor @xmath117 the psd becomes flat , and also the non stationarity disappears .",
    "we simulated an ar noise process in which the @xmath101 of the guiding white normal noise changes in time with the following function : @xmath118 we use an ar(2 ) model with the same initial values of previous toy model , and the following values for the modulation @xmath119    in this case it is crucial the division by the estimated @xmath101 of the process if we want to have at the output of whitening filter stationary data .",
    "in fact if we plot the estimated @xmath101 of the lsl filter with @xmath120 , we find that , even if in a noisy way , the estimation follows the variations in time of the simulated @xmath101 ( see figure  [ fig : sigma ] ) .",
    "if we plot the output of the standard implementation of the lsl whitening filter in figure  [ fig : sigma1 ] , it is evident the this filter has reduced the total rms of the data , but it has not removed the modulation of the sigma of the process .",
    "if we apply the modified lsl filter , as it evident in figure  [ fig : sigma2 ] , we succeed in removing also the modulation of the data and in having a stationary whitened time series .",
    "as it is clear in figure  [ fig : sigma3 ] the whitening obtained with the modified lsl algorithm is good and the whitened psd for non stationary data results flat .",
    "in order to see the application of this algorithm on real data we perform a test on a recorded speech sound , that we convert in a time series . this is surely a non stationary time series , and we want to test if our algorithm is able to identify the speech and to remove all the features present in it .",
    "this will also mean that we can be able to reconstruct the speech from the learned parameters  @xcite . in figure",
    "[ fig : voice4 ] we report the voice time series in time domain and the outputs of the standard and lsl algorithm , using an order @xmath121 for the filter and a value @xmath122 for the forgetting factor .",
    "if we do not apply the modification of the lsl filter , we succeeded in whitening the psd ( see figure  [ fig : voice2 ] ) , but non in removing the non stationarity , as is clear in figure  [ fig : voice1 ] . in figure",
    "[ fig : voice4 ] we reports the results of the modified algorithm . the whitening results good and the variation of the rms in time has disappeared .    in figure",
    "[ fig : voice3 ] we superimpose the estimated @xmath103 $ ] on the voice time series to show that most of the variation in time of the voice are due not to the variation of the ar parameters but to the variation of the @xmath101 of the ar process .",
    "these tests show that we have a powerful method to identify non stationary process and to whiten them . if we have the estimation of the parameters in time we can reconstruct step by step the original time series .",
    "we build a whitening filter using a modified lsl algorithm to remove the non stationarity present in the rms of the driving white noise for a simulated ar process .",
    "we find that with this algorithm is able to follow the non stationarity either coming from the ar parameters or from the @xmath101 parameter and to remove them from the original data set .",
    "e. cuoco et al `` on - line power spectra identification and whitening for the noise in interferometric gravitational wave detectors  class .",
    "quantum grav .",
    "18(2001 ) 1727 - 1751 e. cuoco et al '' noise parametric estimation and whitening for ligo 40-m interferometer data  , phys . rev .",
    "d,64,122002 kay s , modern spectral estimation : theory and application , 1988 prentice hall englewood cliffs haykin s , adaptive filter theory , 1996 prentice hall alexander s t , adaptive signal processing , 1986 springer - verlag hayes m h , statistical digital signal processing and modeling , 1996 wiley widrow b and stearns s d , adaptive signal processing , 1985 prentice hall orfanidis s j , introduction to signal processing , 1996 prentice - hall e. cuoco , in preparation james r dickie and asoke k nandi , `` a comparative study of ar order selection methods  , signal processing 40 ( 2 - 3 ) 1994 pp .",
    "239 - 255 ki yong lee , souhwan jung and jaeyeal rheem , '' smoothing approach using forward - backward kalman filter with markov switching parameters for speech enhancement  , signal processing 80 ( 12 ) 2000",
    "2579 - 2588 yuanjin zheng , david b.h .",
    "tay and zhiping lin , `` modeling general distributed nonstationary process and identifying time - varying autoregressive system by wavelets : theory and application  , signal processing 81 ( 9 ) 2001 pp .",
    "1823 - 1848 martin bouchard , '' numerically stable fast convergence least - squares algorithms for multichannel active sound cancellation systems and sound deconvolution systems  , signal processing 82 ( 5 ) 2002 pp .",
    "721 - 736 dong kyoo kim and poogyeon park , \" the normalized least - squares order - recursive lattice smoother  , signal processing 82 ( 6 ) 2002 pp ."
  ],
  "abstract_text": [
    "<S> in this paper the author proposes to use the least squares lattice filter with forgetting factor to estimate time - varying parameters of the model for noise processes . </S>",
    "<S> we simulated an auto - regressive ( ar ) noise process in which we let the parameters of the ar vary in time . </S>",
    "<S> we investigate a new way of implementation of least squares lattice filter in following the non stationary time series for stochastic process . </S>",
    "<S> moreover we introduce a modified least squares lattice filter to whiten the time - series and to remove the non stationarity . </S>",
    "<S> we apply this algorithm to the identification of real times series data produced by recorded voice .    </S>",
    "<S> system identification ; gaussian time - varying ar model ; whitening ; adaptive least squares lattice algorithms ; forgetting factor </S>"
  ]
}