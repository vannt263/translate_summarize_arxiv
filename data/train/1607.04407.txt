{
  "article_text": [
    "there has been increasing demand for reliable statistics of government fund allocations , social services planning , etc . , in smaller geographic areas and sub - populations , where large samples are not available . because of the limited number of observations within each area or domain , the direct estimator based on the design - based approach , constructed only from information within each area or domain , is not reliable .",
    "the empirical bayes estimator and empirical best linear unbiased predictor ( eblup ) help make efficient inferences by borrowing information from other areas via model - based approaches to small area estimation .",
    "fay and herriot ( 1979 ) first applied this model - based approach to census data through a specific bayesian model .",
    "the model , called the fay ",
    "herriot model , has been widely used in practice . for @xmath1 @xmath2 in the above model , level 1",
    "is used to take into account the sampling distribution of the direct estimator @xmath3 for the small area @xmath4 . a true mean for small area @xmath4 , @xmath5 , is linked to provide the auxiliary variables @xmath6 in the level 2 linking model . in practice , the coefficient @xmath7-vector @xmath8 and the model variance parameter @xmath9 in the linking model are unknown , and we need to estimate them from the observed data .",
    "the assumption of a known @xmath10 often follows from the asymptotic variances of the transformed direct estimates ( efron and morris 1975 ) or from empirical variance modelling ( fay and herriot 1979 ) .",
    "this model can be viewed as the following linear mixed model : @xmath11 where @xmath12 and @xmath13 are independent of the normality assumption @xmath14 and @xmath15 .",
    "let @xmath16 define the mean squared error ( mse ) @xmath17 $ ] of the estimator or predictor @xmath18 of a small area mean @xmath5 , where the expectation is on the joint distribution of @xmath19 and @xmath20 under the fay  herriot model ( [ fh ] ) .",
    "the bayes estimator of @xmath5 is consistent with the best predictor ( bp ) in this model , with the minimum mse among all @xmath18 .",
    "it is given by @xmath21 where @xmath22 is called the shrinkage factor toward @xmath23 from the direct estimate @xmath3",
    ".    if @xmath8 is unknown , the best linear unbiased prediction ( blup ) approach yields @xmath24 in which @xmath8 of @xmath25 is replaced by @xmath26 , which minimizes the mse among all linear unbiased estimators or predictors , as follows : @xmath27 where the weighted least - square estimator of @xmath8 , @xmath28 , @xmath29 , @xmath30 and @xmath31 .",
    "+ from the fact that both @xmath8 and @xmath9 are practically unknown , the empirical best linear unbiased prediction estimator ( eblup ) @xmath32 is widely used for small area inference , where the unknown model variance parameter @xmath9 in @xmath33 is replaced by its consistent estimator @xmath34 : @xmath35 where @xmath36 and @xmath37 and the consistent estimator @xmath34 for large @xmath0 is even - translation invariant for all @xmath3 and @xmath8 . to estimate the model variance parameter @xmath9 , the method of moment estimator ( see fay and herriot 1979 ;",
    "prasad and rao 1990 ) and standard maximum likelihood estimators such as profile maximum likelihood ( ml ) estimator and residual maximum likelihood ( reml ) estimator are utilized .",
    "in particular , the reml estimator of @xmath9 is widely used in terms of higher - order asymptotic properties for large @xmath0 under some mild regularity conditions .",
    "hereafter , we indicate the reml estimator as @xmath38 , obtained from @xmath39 where the residual likelihood function @xmath40 and @xmath41 .",
    "this paper focuses on the confidence interval for @xmath5 , used widely in small area estimation as well as point estimation .",
    "let @xmath42 denote the general form of the confidence interval as follows : @xmath43 where @xmath44 and @xmath45 are , respectively , an estimator of @xmath5 and a measure of uncertainty of @xmath44 .",
    "@xmath46 is adopted as an adequate percentile point to get closer to the nominal coverage level @xmath47 .",
    "we call @xmath48% the confidence interval for @xmath5 if the coverage probability is consistent with the nominal coverage , that is , @xmath49=1-\\alpha$ ] holds exactly for a fixed @xmath8 and @xmath9 with a probability of @xmath50 , according to the fay ",
    "herriot model .",
    "we introduce several intervals traditionally used for small area estimation .",
    "the simplest confidence interval , called the direct confidence interval and denoted by @xmath51 , is constructed with the direct estimate @xmath3 , the @xmath52 value @xmath53 of the upper @xmath48% , and the sampling variance @xmath10 , substituted for @xmath44 , @xmath46 , and @xmath54 , respectively .",
    "this yields a coverage probability of exactly @xmath47 .",
    "however , this interval could yield too long length to make any reasonable conclusion when @xmath10 is large .",
    "in contrast , cox ( 1975 ) suggested the empirical bayes confidence interval for @xmath5 , choosing @xmath55 , @xmath56 , @xmath57 and plugging the anova estimator @xmath58 into @xmath34 , denoted by @xmath59 , where @xmath60 in the balanced case .",
    "although successful , because its length is not greater than that of the direct confidence interval @xmath61 , it is not accurate enough in most small area applications because the coverage error is of the order of @xmath62 for large @xmath0 .    similar to the cox interval ,",
    "the traditional empirical bayes confidence interval @xmath63 is also a common method suggested in prasad and rao ( 1990 ) , in which @xmath64 , @xmath46 , and @xmath45 in ( [ ci.form ] ) are replaced by @xmath32 , @xmath65 , and @xmath66 , along with the pr estimator using hendersons method 3 ( prasad and rao 1990 ) of @xmath9 , where the second - order unbiased estimator @xmath67 of @xmath68 is such that @xmath69=o(m^{-1})$ ] .",
    "recently , users have frequently employed the residual maximum likelihood estimator @xmath38 instead of the anova - type estimators for two such intervals .",
    "we denote these as @xmath70 and @xmath71 , with the reml estimator @xmath38 used for each empirical bayes confidence interval .",
    "the length of @xmath72 obtained is less than that of the direct confidence interval for large @xmath0 , but the coverage error is of the order of @xmath62 as with the cox interval @xmath73 .    to offset this disadvantage , datta et al .",
    "( 2002 ) and sasase and kubokawa ( 2005 ) suggested calibrating @xmath53 to @xmath46 for the cox confidence interval @xmath73 to reduce the coverage error to the order of @xmath74 for a balanced case ( @xmath75 for all @xmath4 ) in the fay ",
    "and for an unbalanced case in the nested error regression model .",
    "diao et al .",
    "( 2014 ) revealed the relationship between the percentile point @xmath46 and the measure of uncertainty of @xmath32 , @xmath45 , in constructing the second - order confidence interval . from this result , if reml is adopted as the estimator of @xmath9 , it assigns @xmath76 , @xmath77 with @xmath78 as stated in corollary 1 in diao et al . ( 2014 ) in constructing second - order confidence interval @xmath79 , @xmath80 in ( [ ci.form ] ) , where @xmath81 and @xmath82 .",
    "the second - order confidence interval can be constructed through a simulation - based method .",
    "hall and maiti ( 2006 ) , chatterjee et al .",
    "( 2008 ) , and li and lahiri ( 2010 ) achieved an adequate @xmath46 by a bootstrap method .",
    "yoshimori ( 2015 ) found , by a simulation study based on the fay ",
    "herriot model , that the interval length of chatterjee et al.(2008 ) tends to be less than that of hall and maiti ( 2006 ) .",
    "yoshimori and lahiri ( 2014 a ) suggested a second - order efficient empirical bayes confidence interval via an area - specific adjustment factor , which has three desired properties simultaneously : ( i ) the order of coverage error is @xmath74 , ( ii ) the length is always less than that of the direct confidence interval , and ( iii ) it does not rely on a simulation - based method such as the bootstrap method . hereafter , we call this the yl confidence interval , which is denoted by @xmath83 and written as follows : @xmath84 where @xmath85 yields the maximum value of the adjusted residual likelihood , @xmath86 , which is equal to the residual likelihood @xmath87 multiplied by a specific adjustment factor , @xmath88 ; that is , @xmath89 , where @xmath88 is as shown in yoshimori and lahiri ( 2014a ) .",
    "the concept of the adjusted likelihood method can be refer to lahiri and li ( 2009 ) .",
    "the authors emphasized that the yl interval method would be an alternative to the parametric bootstrap interval proposed in li and lahiri ( 2010 ) , although yl does not rely on a simulation - based method . from the result , it would be reliable in small area estimation , especially for developing countries that are not in favour of using simulation - based methods .",
    "nevertheless , their required condition for the existence of @xmath90 could be getting stronger not for large @xmath0 when at least one leverage value is high due to the following condition in practice .",
    "@xmath91 where @xmath92 is the leverage @xmath93 .",
    "this is the condition described in remark 3 of yoshimori and lahiri ( 2014a ) to ensure that @xmath90 exists .",
    "for example , this condition does not hold for the baseball batting average data shown in gelman et al .",
    "( 1995 ) with the covariate mentioned in section 4 .",
    "moreover , the yl interval method might cause confusion in two ways : ( a ) some people might not understand why @xmath0 estimates are needed for one global parameter @xmath9 ; and ( b ) with no parallel computations for large @xmath0 , considerable time might be needed for the number of iterations required to obtain area - specific @xmath0 estimates of @xmath9 , with the likelihood method used only for the global parameter @xmath9 .    to address these problems , this paper proposes , as never done before , a more reliable confidence interval given in ( [ nas ] ) satisfying the following five desired properties , by providing a new non - area - specific ( nas ) adjustment factor :    * desired properties *    ( i ) : :    the coverage error is of the order of @xmath74 ; ( ii ) : :    the length is always less than that of the direct confidence interval ; ( iii ) : :    it does not rely on a simulation - based method such as the bootstrap    method ; ( iv ) : :    it does not require calculations to obtain the estimator of    @xmath9 for all @xmath0 areas , unlike the yl method    ( 2014a ) ; ( v ) : :    it has a milder condition for the existence of the estimator of    @xmath9 than the ( [ cond.yl ] ) .",
    "we also show some simulation results for comparison with several existing intervals and adopt our method for real data analysis .",
    "now , we prepare the regularity conditions for the introduction of some theorems mentioned in the next section , which correspond to r2r4 in yoshimori and lahiri ( 2014a ) .    * regularity conditions *    r1 : :    @xmath94 is bounded for large @xmath0 values ; r2 : :    the elements of @xmath95 are uniformly bounded , implying    @xmath96 ; r3 : :    @xmath97 ,    @xmath98 ;    hereafter , we constrain the class of the adjustment factor @xmath99 with the following conditions , which correspond to conditions r1 and r5 in yoshimori and lahiri ( 2014a ) .",
    "r4 : :    the logarithm of the adjustment factor @xmath100    [ or @xmath101 is free of @xmath19 and    is five times continuously differentiable with respect to    @xmath9 .",
    "moreover , it is bounded for large @xmath0    values ; r5 : :    @xmath102 , where @xmath103 is    a generic positive constant and @xmath104 is a small    positive constant , where    @xmath105    for an adjustment factor @xmath99 .",
    "we first construct the following theorem to show the relationship between the adjustment factor @xmath99 and the uncertainty measure of @xmath106 , @xmath45 :    [ t1 ] under the regularity conditions , we have the following equation , up to the order @xmath62 for large @xmath0 , in constructing a second - order empirical bayes confidence interval ; @xmath107(a+d_i)^2}{2d_i^2}c_i .",
    "\\notag \\\\ & = \\frac{7-z^2}{4(a+d_i)}+\\frac{(1+z^{2})}{4a}-\\frac{tr[v^{-2}](a+d_i)^2}{2d_i^2}c_i , \\label{ad.diff2}\\end{aligned}\\ ] ] where the function @xmath108 is @xmath109 , @xmath110 , and @xmath111 is the first derivative of the logarithm of the adjustment factor @xmath99 with respect to @xmath9 .",
    "the proof follows from diao et al .",
    "( 2014 ) and some calculations .",
    "this theorem ensures several corollaries as follows :    1 .",
    "if we set @xmath112 as @xmath108 , that is , @xmath113 , the adequate adjustment factor @xmath114 is consistent with that in yoshimori and lahiri ( 2014a ) , which is area - specific since it is dependent on @xmath4 .",
    "when we adopt an reml estimator such as @xmath34 , a suitable @xmath108 can be derived with the right - hand side of ( [ ad.diff2 ] ) set to zero , that is , @xmath115 .",
    "unfortunately , it does not satisfy the desired property ( ii ) since the estimator of @xmath108 , @xmath116g_{3i}(\\hat{a})$ ] , could be inflated as @xmath34 getting smaller .",
    "3 .   when @xmath117 , we use @xmath118 as @xmath54 .",
    "then , the following confidence interval can be constructed : @xmath119 where @xmath120 , with @xmath121 and @xmath118 , is the mean squared error of blup , @xmath122 $ ] .",
    "this interval also satisfies properties ( i)(iii ) , since @xmath123 holds where @xmath124 , but property ( iv ) is sacrificed except in a balanced case .    to satisfy property ( iv ) as well ,",
    "our first goal is to find an adequate non - area - specific adjustment factor @xmath125 such that it is not free from @xmath9 , from corollary 1(b ) .",
    "remember that satisfying property ( iv ) does imply that iterative calculations are not required to estimate @xmath9 for the whole @xmath4 area and that a likelihood method can be used .",
    "in order to obtain our first goal , let us go back to equation ( [ ad.diff2 ] ) in theorem [ t1 ] .",
    "we now replace @xmath108 by @xmath126 since @xmath108 is related to the uncertainty measure of eblup , @xmath127 , and not that of blup .",
    "this means that the following confidence interval class can be considered with general adjusted residual maximum likelihood estimator @xmath128 and @xmath129 : @xmath130    now , the equation ( [ ad.diff2 ] ) can be written as @xmath131 where @xmath132 .",
    "the result shows that there is only one non - area - specific term @xmath133 in equation ( [ ad.diff3 ] ) .",
    "if we eliminate the remaining terms , we achieve our first goal successfully ; that is , @xmath134 it follows that @xmath129 no longer depends on @xmath34 such as @xmath135 for all @xmath4 , which implies that @xmath136 , with the non - area - specific adjustment factor @xmath137 obtained from the differential equation @xmath138 .",
    "hereafter , we denote this non - area - specific adjustment factor as @xmath139 .    according to this result",
    ", we construct the second - order confidence interval @xmath140 with the non - area - specific adjustment factor @xmath141 and fixed @xmath142 for all @xmath4 : @xmath143 and @xmath144 with non - area specific adjustment factor @xmath139 .",
    "the proof follows from theorem [ t1 ] .    under the regularity conditions , there exists at least one solution @xmath145 for @xmath124 with the condition @xmath146 .",
    "the proof is similar to that of the corollary to theorem 4 of yoshimori and lahiri ( 2014a ) .",
    "note that this existence condition for the estimator of @xmath9 is no longer dependent on leverage @xmath92 and is milder than ( [ cond.yl ] ) when @xmath147 .",
    "this implies that our interval @xmath148 has property ( v ) as well .    in the discussion",
    "so far , @xmath148 has properties ( i ) , ( iii ) , ( iv ) , and ( v ) .",
    "then , does @xmath148 have property ( ii ) simultaneously ?",
    "in general , its length is always less than @xmath149}$ ] , but not less than that of the direct confidence interval .",
    "even so , the length does not tend to be inflated significantly since @xmath150 does not depend on @xmath128 .",
    "we can also consider the possibility of another non - area - specific factor not dependent on @xmath52 , @xmath151 , with @xmath152 $ ] such that @xmath153g_{3i}(\\hat{a})}\\end{aligned}\\ ] ] where @xmath154 .",
    "however , this could increase the variability of length since the estimator of @xmath155 is dependent on @xmath34 .",
    "if @xmath156 is set to 2 for all @xmath4 in the interval class ( [ class1 ] ) , as with the traditional empirical bayes confidence interval @xmath72 , we obtain the adjustment factor as @xmath157 for the second - order confidence interval , which can not be a non - area - specific adjustment factor .",
    "we next set the second goal : construct an interval such that all desired properties described in the previous section are satisfied . to achieve the goal",
    ", we suggest the following second - order efficient confidence interval using @xmath158 given in corollary 1(c ) : @xmath159 thus , we achieve our second goal successfully .    our suggested estimators @xmath145 and @xmath160 have the following properties as well , which were used in @xmath161 .    under the regularity conditions , we have for large @xmath0 values , [ t3 ]    1 .",
    "@xmath162=\\frac{2}{tr[v^{-2}]}\\tilde{l}_{i , ad*}^{(1)}+o(m^{-1 } ) $ ] ; 2 .",
    "@xmath163= \\frac{2}{tr[v^{-2}]}+o(m^{-1})$ ] ; 3 .",
    "there exists at least one solution @xmath164 on @xmath124 for @xmath4 with the condition @xmath165 ,    where @xmath166 and @xmath167 ,  @xmath168 } for each @xmath145 , @xmath160 .",
    "the proof is straightforward from theorem 1 in yoshimori and lahiri ( 2014b ) and the proof of the corollary to theorem 4 in yoshimori and lahiri ( 2014a ) .",
    "part ( 3 ) implies that @xmath169 has property ( v ) as well .",
    "in this section , two finite sample simulation studies are implemented to investigate the performances of several confidence intervals through monte carlo simulation under the fay  herriot model ( [ fh ] ) .",
    "the first simulation study was considered a balanced case such that the leverages of all areas satisfy the required condition ( [ cond.yl ] ) for the confidence interval @xmath170 , whereas an unbalanced case with one area does not satisfy the condition ( [ cond.yl ] ) for the second study .",
    "we set @xmath171 without loss of generality through these simulation studies .",
    "for each study , we generated @xmath172 datasets from model ( [ fh ] ) .",
    "when the reml method obtained zero estimates , we truncated it to 0.01 .",
    "for the first simulation study , we considered a situation in which the number of areas was @xmath173 and the dimensions of @xmath8 were @xmath174 and @xmath175 for area @xmath4 .",
    "additionally , we generated @xmath176 independently from the uniform distribution @xmath177 once and then treated it as fixed .",
    "then , all leverages satisfied condition ( [ cond.yl ] ) since the maximum was 0.23 . in order to investigate the effect of the shrinkage factor @xmath178 in a balanced case",
    ", we considered three @xmath179 values , 0.5 , 0.7 , and 0.9 , defining the sampling variances such that @xmath180 and changing the value of @xmath9 .",
    "the competitors are the cox confidence interval with reml method , the cox - type confidence interval with @xmath90 given in ( [ yl ] ) , the calibrated traditional confidence interval proposed in diao et al .",
    "( 2014 ) , the second - order efficient confidence interval based on a non - area - specific adjustment factor , and the direct confidence interval .",
    "let them be denoted by _",
    "cox.re , cox.yl , ct , nas , and direct _ , respectively .    .simulated coverage probabilities and average length ( in parentheses ) in a balanced case such that condition ( [ cond.yl ] ) holds with 95% nominal coverage [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     the result for study 2 is displayed in table [ unb.dib ] .",
    "as in study 1 , this table reports simulated coverage probabilities with average length in parentheses for a nominal coverage of 95% as well as minimum and maximum @xmath181 and leverage values , that is , respectively 0.47 and 0.9 for @xmath181 and 0.07 and 0.64 for leverage .",
    "@xmath182 also reveals under - coverage for all situations , as in study 1 .",
    "it is noteworthy that the simulated coverage probability dramatically goes down to about 50% in combinations with ( @xmath181,leverage)=(0.9,0.64 ) for both patterns ( a ) and ( b ) from this table .",
    "similar to the result in study 1 , the simulated probability of @xmath79 is reported to be more than 95% except for such cases , while the average lengths are mostly larger than that of the direct confidence interval .",
    "a remarkable result is that the average interval length is 3.7 times as large as that of @xmath51 for ( @xmath181,leverage)=(0.47,0.07 ) in pattern ( b ) .",
    "in contrast , @xmath183 maintains more than 95% simulated coverage probabilities , and the length is always less than that of the direct confidence interval .",
    "furthermore , this study also shows that @xmath183 improves @xmath83 in terms of the leverage condition .",
    "in this section , we compare performances between two intervals @xmath183 and @xmath184 using batting average data .",
    "efron and morris ( 1975 ) provided 1970 batting average data for 18 players and analysed the batting average of the first 45 at bats to predict seasonal averages in 1970 .",
    "incidentally , they also showed the seasonal batting averages for external evaluations .",
    "gelman et al . ( 1995 ) additionally presented previous batting average data from the baseball encyclopedia . in this paper , all information on the 18 players is sorted in the order of their batting averages and previous averages .",
    "let us consider the information of covariates in model ( [ fh ] ) as @xmath185 , where previous seasonal batting average is represented as @xmath176 for @xmath4th player .",
    "following efron and morris ( 1975 ) , we consider a balanced case with @xmath186 for all players via arc - sin transformation .",
    "the maximum leverage value of 0.79 then leads to failure of condition ( [ cond.yl ] ) for the eighth player .",
    "figure [ ci.da ] provides the results of two confidence intervals ( @xmath183 and @xmath184 ) for the 8th and 14th players , whose leverage values are the maximum and minimum , respectively .",
    "the x - axis shows the batting average , and each point indicates the true seasonal batting average in 1970 .",
    "thus , we see in the figure that @xmath183 is enclosed within @xmath184 for the 14th player , while the seasonal batting average is in both intervals .",
    "we also find that @xmath183 provides a smaller length than @xmath184 for the 8th player with the seasonal batting average being in both intervals .",
    "this paper proposed a second - order efficient empirical bayes confidence interval based on the non - area - specific adjustment factor under the fay  herriot model .",
    "our method simultaneously provides five desired properties while the interval @xmath170 fulfils three properties . additionally , the overall results from the simulation and real data analysis showed that our method , @xmath183 , is superior to other confidence intervals , including @xmath83 , in terms of coverage probability and length .",
    "moreover , we also studied the simulated elapsed time with regard to computer burden .",
    "accordingly , we considered three values of @xmath0 , @xmath187 , to compare the calculation time of @xmath83 with those of @xmath73 , @xmath183 , and @xmath184 . in this study , we set @xmath188 with @xmath189 and generated a dataset of @xmath190 with @xmath191 and @xmath180 .",
    "simulated calculation time was measured as the average of five implementations in r 3.2.0 with the fisher scoring method and no parallel implementation for @xmath83 .",
    "we found that the calculation time of @xmath83 rises rapidly as @xmath0 increases .",
    "in particular , for @xmath192 , it takes about 1000 seconds , about 1000 times as long as @xmath183 , which takes about 1 second .",
    "we expect that much longer calculation times will be required for @xmath83 with even larger @xmath0 .",
    "the authors thank professor partha lahiri at university of maryland for reading an earlier draft of the paper and making constructive comments .",
    "this research was supported by jsps grant - in - aid for research activity start - up no .",
    "26880011 .",
    "datta , g. s. , ghosh , m. , smith , d. and lahiri , p. ( 2002 ) . on an asymptotic theory of conditional and unconditional coverage probabilities of empirical bayes confidence intervals .",
    "j. statist . _",
    "* 29 * 139 - 152 .",
    "diao , l. , smith , d. d. , datta , g. s. , maiti , t. and opsomer , j. d. ( 2014 ) .",
    "accurate confidence interval estimation of small area parameters under the fay  herriot model .",
    "j. stat . _",
    "* 41 * : 497515 .",
    "lahiri , p. and li , h. ( 2009 ) .",
    "generalized maximum likelihood method in linear mixed models with an application in small area estimation . _ in : proceedings of the federal committee on statistical methodology research conference . _"
  ],
  "abstract_text": [
    "<S> an empirical bayes confidence interval has high user demand in many applications . in particular , the second - order empirical bayes confidence interval , </S>",
    "<S> the coverage error of which is of the third order for large number of areas , @xmath0 , is widely used in small area estimation when the sample size within each area is not large enough to make reliable direct estimates based on a design - based approach . </S>",
    "<S> yoshimori and lahiri ( 2014a ) proposed a new type of confidence interval , called the _ second - order efficient empirical bayes confidence interval _ , whose length is less than that of the direct confidence interval based on the design - based approach . </S>",
    "<S> however , this interval still has some disadvantages : ( i ) it is hard to use when at least one leverage value is high ; ( ii ) many iterations tend to be required to obtain the estimators of one global model variance parameter as the number of areas @xmath0 getting larger , due to the area - specific adjustment factor . to prevent such issues </S>",
    "<S> , this paper proposes , as never done before , a more efficient confidence interval to allow for high leverage and reduce the number of iterations for large @xmath0 , by adopting a non - area - specific adjustment factor , maintaining the existing desired properties . </S>",
    "<S> we also reveal the relationship between the general adjustment factor and the measure of uncertainty of the empirical bayes estimator to create a second - order confidence interval . moreover </S>",
    "<S> , we present two simulation studies and a real data analysis to show the efficiency of this confidence interval .    </S>",
    "<S> * keywords * , adjusted residual maximum likelihood ; confidence interval ; empirical bayes ; linear mixed model ; small area estimation . </S>"
  ]
}