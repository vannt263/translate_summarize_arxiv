{
  "article_text": [
    "there are a few mathematical functions which are introduced for convenience . for example , the heaviside step function @xmath0 , a piecewise constant function given by : @xmath1 and the dirac delta function @xmath2  ( more precisely , a distribution  ( see , e.g. ,  @xcite ) ) , a generalized function whose discrete analog is referred to as the kronecker delta function : @xmath3    this article is on a newcomer , the shrinkage function @xmath4 , first introduced by donoho and johnstone in their landmark paper  ( @xcite , see also @xcite ) on function estimation using wavelets in the early 1990 s .",
    "recently , the shrinkage function has been heavily used in the solutions of several optimization and approximation problems of matrices ( see , e.g. ,  @xcite ) .",
    "we give an elementary treatment that is accessible to a vast group of researchers , as it only requires basic knowledge in calculus and linear algebra and show how naturally the shrinkage function can be used in solving more advanced problems .",
    "we start with a regular calculus problem .",
    "let @xmath5 and @xmath6 be given . consider the following problem : @xmath7.\\ ] ] we adopt the notation @xmath8 to mean that @xmath9 is a solution of the minimization problem @xmath10 and define : @xmath11}.\\ ] ]    [ theorem 1 ]",
    "let @xmath5 be fixed . for each @xmath12",
    ", there is one and only one solution @xmath13 , to the minimization problem  ( [ p ] ) .",
    "furthermore , @xmath14    * remark .",
    "* the function @xmath15 defined above is called the shrinkage function ( also referred to as soft shrinkage or soft threshold , @xcite ) .",
    "one may imagine that @xmath13 `` shrinks '' @xmath16 to zero when  @xmath17 a plot of @xmath4 for @xmath18 is given in figure  [ fig1 ] .     for @xmath19.,scaledwidth=50.0%",
    "]    let @xmath20 .",
    "note that @xmath21 when @xmath22 and @xmath23 is continuous on @xmath24 and differentiable everywhere except a single point @xmath25 .",
    "so , @xmath23 achieves its minimum value on @xmath24 at one of its critical points .",
    "let @xmath26 .     for different values of @xmath16 with @xmath19.,scaledwidth=50.0% ]",
    "we consider three cases .",
    "case 1 : @xmath27 .",
    "since @xmath23 is differentiable at @xmath28 and achieves its minimum , we must have @xmath29 .",
    "note that , for @xmath30 , we have @xmath31=\\lambda+(x - a).\\ ] ] so , @xmath32 which implies @xmath33 to be consistent with @xmath27 , it is necessary that @xmath34 or , equivalently , @xmath35 .",
    "case 2 : @xmath36 . by proceeding similarly as in case 1 above",
    ", we can arrive at @xmath37    case 3 : @xmath38 .",
    "note that @xmath39 is no longer differentiable at @xmath38  ( so we could not use the condition @xmath29 as before ) .",
    "but since @xmath23 has a minimum at @xmath38 and since @xmath23 is differentiable on each side of @xmath38 , it is necessary that @xmath40 + so , @xmath41 thus , @xmath42 or , equivalently , @xmath43 to summarize , we have @xmath44    since one and only one of the three cases ( 1 ) @xmath35 , ( 2 ) @xmath45 , and ( 3 ) @xmath46 holds , we obtain the uniqueness in general . with the uniqueness ,",
    "it is straightforward to verify that each of the three cases would imply the corresponding formula for @xmath47 .",
    "this completes the proof .",
    "recently , research in compressive sensing leads to the recognition of the fact that the @xmath48-norm of a vector is a good substitute for the count of the number of non - zero entries of the vector in many minimization problems . in this section ,",
    "we solve some simple minimization problems using the count of non - zero entries or the @xmath48-norm . given a vector @xmath49 ,",
    "we want to solve @xmath50,\\ ] ] where @xmath51 denotes the number of non - zero entries of @xmath52 , @xmath53 denotes the euclidean norm in @xmath54 , and @xmath55 is a given balancing parameter",
    ". we can solve problem  ( [ sp ] ) component - wise  ( in each @xmath56 ) as follows .",
    "notice that , given @xmath57 , each entry @xmath56 of @xmath58 contributes 1 to @xmath59 if @xmath56 is non - zero , and contributes 0 if @xmath56 is zero . since we are minimizing @xmath60 ,",
    "if @xmath56 is zero then the contribution to @xmath61 depending on this @xmath56 is @xmath62 ; otherwise , if @xmath56 is non - zero , then we should minimize @xmath63 for @xmath64 , which forces that @xmath65 and contributes 1 to @xmath61 as the minimum value .",
    "therefore , the solution @xmath58 to problem ( [ sp ] ) is given component - wise by @xmath66 next , we replace @xmath67 by @xmath68 in ( [ sp ] ) and solve : @xmath69,\\ ] ] where @xmath70 denotes the @xmath48 norm in @xmath71 .",
    "using theorem  [ theorem 1 ] , we can solve ( [ p2 ] ) component - wise as follows .",
    "[ theorem 2 ] @xcite let @xmath72 and @xmath73 be given and let @xmath74,\\ ] ] then @xmath75 where , @xmath76 denotes the vector whose entries are obtained by applying the shrinkage function @xmath77 to the corresponding entries of @xmath78 .",
    "if @xmath56 and @xmath79 denote the @xmath80th entry of the vectors @xmath58 and @xmath81 , respectively , @xmath82 , then we have , @xmath83 \\\\ & = & \\arg\\min_{\\mathbf{u } \\in \\mathbb{r}^{n } } \\left[\\sum_{i=1}^n\\left ( \\frac{1}{\\beta}|u_i| + \\frac{1}{2}(u_i -v_i)^2\\right)\\right].\\end{aligned}\\ ] ] since the @xmath80-th term in the summation depends only on @xmath56 , the vector @xmath84 must have components @xmath85 satisfying @xmath86,\\ ] ] for @xmath82 .",
    "but by theorem  [ theorem 1 ] , the solution to each of these problems is given precisely by @xmath87 .",
    "this yields the result",
    ".    * remark . *",
    "the previous proof still works if we replace the vectors by matrices and use the extension of the @xmath48 and @xmath88-norms to matrices _ by treating them as vectors_. thus by using the same argument we can easily show the following matrix version of the previous theorem .",
    "[ theorem 3 ]  @xcite let @xmath72 and @xmath89 be given",
    ". then @xmath90,\\ ] ] where @xmath91 is again defined component - wise .",
    "+ theorem  [ theorem 3 ] solves the problem of approximating a given matrix by a sparse matrix by using the shrinkage function .",
    "the sparse approximation as given by theorem  [ theorem 3 ] has many applications such as data compression and dimension reduction . in these areas ,",
    "one may also be interested in finding matrices of low rank .",
    "for example , given a matrix @xmath92 , we want to solve the following approximation problem : @xmath93,\\ ] ] where @xmath94 denotes the frobenius norm of matrices ( which turns out to be equivalent to the vector @xmath95 norm if we treat a matrix as a vector - see more discussion on the matrix norms in subsection 4.1 ) .    this is a harder problem since rank@xmath96 is not a convex function . a convex relaxation ( see ,",
    "e.g. , @xcite ) of the problem is provided by replacing the term @xmath97 by the nuclear norm of @xmath98 , @xmath99 , ( again , see subsection 4.1 for a discussion on the nuclear norm and its properties ) .",
    "the problem then becomes : @xmath100.\\ ] ] this problem again yields an explicit solution ( @xcite ) , but in these literatures , the formula is derived by using advanced tools from convex analysis ( `` subdifferentials '' to be more specific ) .",
    "here , we will show how we can obtain the solution by using simple ideas from the previous section",
    ".      it will be beneficial to recall the various matrix norms .",
    "many useful matrix norms can be defined in terms of the singular values of the matrices .",
    "we will deal with two of them : the nuclear norm @xmath101 and the frobenius norm @xmath102 .",
    "let @xmath103 and @xmath104 be a singular value decomposition  ( svd ) of @xmath105 with @xmath106 and @xmath107 being two orthogonal matrices ( that is , @xmath108 and @xmath109 ) and @xmath110 being a diagonal matrix such that @xmath111 .",
    "the @xmath112 s are called the singular values of @xmath105 .",
    "it is known ( @xcite ) that every matrix in @xmath113 has a svd and that svd of a matrix is not unique .",
    "then the nuclear norm of @xmath105 is given by @xmath114 and we can also define the frobenius norm of @xmath105 as @xmath115 this norm turns out to be the same as the @xmath88 norm of @xmath105 , treated as a vector in @xmath116 .",
    "this is because the nonzero singular values @xmath112 s are exactly the square root of the nonzero eigenvalues of @xmath117 or @xmath118 .",
    "so , @xmath119 = _ i=1^\\{m , n}(_i(a))^2.@xmath120 here we have used @xmath121 to denote the trace of a matrix ( which is equal to the sum of all diagonal entries of the matrix )",
    ". we will need the following simple fact about the nuclear norms of a matrix and that of its diagonal : let @xmath122 denote the diagonal matrix using the diagonal of @xmath105 .",
    "we have @xmath123 this inequality can be verified by using a svd of @xmath104 as follows .",
    "write @xmath124 , @xmath125 , and @xmath126",
    ". then @xmath127 @xmath128 where we have used the cauchy - schwarz inequality in obtaining the second inequality , and the orthogonality of @xmath129 and @xmath130 ( so that @xmath131 and @xmath132 ) in the last inequality .",
    "we will also use the fact that for any orthogonal matrices @xmath133 and @xmath134 , @xmath135 and @xmath105 have the same singular values , and therefore their frobenius norms and nuclear norms are same : @xmath136 this is known as the unitary invariance of the frobenius norm and nuclear norm .",
    "we are ready to show how problem ( [ nn ] ) is problem ( [ p ] ) in disguise . given @xmath137 , using the unitary invariance of the frobenius norm and the nuclear norm , we have @xmath138&=&\\min_{{\\mathbf x}}[\\|{\\mathbf x}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf x}-{\\mathbf u}\\tilde{\\mathbf a}{\\mathbf v}^{t } \\|_f^2]\\\\ & = & \\min_{{\\mathbf x}}[\\|{\\mathbf u}^{t}{\\mathbf x}{\\mathbf v}\\|_*+\\frac{\\beta}{2}\\|{\\mathbf u}^{t}{\\mathbf x}{\\mathbf v}-\\tilde{\\mathbf a}\\|_f^2].\\end{aligned}\\ ] ] it can be seen from the last expression that the minimum occurs when @xmath139 is diagonal since both terms in that expression get no larger when @xmath140 is replaced by its diagonal matrix ( with the help of ( [ diag ] ) ) .",
    "so , the matrix @xmath141 is a diagonal matrix .",
    "thus , @xmath142 which yields a svd of @xmath98 ( using the same matrices @xmath129 and @xmath130 as in a svd of @xmath105  ! ) . then , @xmath143&= \\min_{\\tilde{\\mathbf x}\\in { \\rm diag}}[\\|\\tilde{\\mathbf",
    "x}\\|_*+\\frac{\\beta}{2}\\|\\tilde{\\mathbf x}-\\tilde{\\mathbf a}\\|_f^2]\\\\ & = \\min_{\\tilde{\\mathbf x}\\in { \\rm diag } } [ \\sum_{i}\\tilde{x}_{ii}+\\frac{\\beta}{2}\\sum_{i}(\\tilde{x}_{ii}-\\sigma_i({\\mathbf a}))^2],\\end{aligned}\\ ] ] where @xmath144 is the set of diagonal matrices in @xmath145 . above is an optimization problem like ( [ p ] ) ( for vectors @xmath146 as @xmath140 varies )",
    "whose solution is given by @xmath147 to summarize , we have proven the following .    [ theorem 4]@xcite suppose that @xmath148 and @xmath72 are given .",
    "then the solution to the minimization problem ( [ nn ] ) is given by @xmath149 where the diagonal matrix @xmath150 has diagonal entries @xmath151 where @xmath152 is a svd of @xmath105 .",
    "* remark . * * 1 . * a recent proof of this theorem is given by cai , candes , and shen in @xcite where they give an advanced verification of the result . our proof given above has the advantage that it is elementary and allows the reader to `` discover '' the result . +",
    "there are many earlier discoveries of related results ( @xcite ) where rank@xmath153 is used instead of the nuclear norm @xmath99 .",
    "we will examine one such variant in the next section .",
    "+ * 3 . * one key ingredient in the above discussion is the unitary invariance of the norms @xmath154 and @xmath94 .",
    "it was von neumann ( see , e.g. , @xcite ) who was among the first to study the family of all unitarily invariant matrix norms in matrix approximation , @xmath94 being one of them .",
    "a closely related ( but harder ) problem is _ compressive sensing _ ( @xcite ) .",
    "readers are strongly recommended to the recently survey by bryan and leise ( @xcite ) .",
    "more problems can be solved by applying similar ideas . for example , let us consider a variant of a well - known result of schmidt  ( see , e.g. , ( * ? ? ?",
    "* section 5 ) ) , replacing the rank by the nuclear norm : for a fixed positive number @xmath155 , consider @xmath156 using similar methods as in section 3 , this problem can be transformed into the following : @xmath157 note that ,  ( [ bas ] ) is related to a lasso problem  @xcite .",
    "but unlike a lasso problem , no special assumption  ( like 0 mean ) is made on @xmath81 in  ( [ bas ] ) .",
    "as in  @xcite , one can form a _",
    "lagrangian _ of  ( [ bas ] ) and solve : @xmath158 which has a solution @xmath159 according to theorem  [ theorem 2 ] .",
    "( the reason for us to use @xmath160 instead of @xmath161 in  ( [ bas ] ) is nonessential : it is only for indicating the similarity with lasso formulation . )",
    "we now sketch the derivation of converting  ( [ ba ] ) to  ( [ bas ] ) : as before , let @xmath162 be a svd of @xmath105 .",
    "then , @xmath163 note that @xmath164 , so ( [ ba ] ) can be written as @xmath165 which , by using ( [ diag ] ) , can be further transformed to @xmath166 next , if we let @xmath58 and @xmath81 be two vectors in @xmath167 consisting of the diagonal elements of @xmath168 and @xmath169 , respectively , then ( [ bad ] ) is ( [ bas ] ) .",
    "thus we have established the following result .       +   + * acknowledgment . *",
    "this work is partially supported by a csums program of the national science foundation dms-0803059 .",
    "toby boas , katie mercier , and eric niederman contributed to this work as undergraduate students .",
    ", _ sparse and low - rank matrix decomposition via alternating direction methods _ , technical report ( available from http://www.optimization-online.org/dbfile/2009/11/2447.pdf ) , dept . of mathematics , hong kong baptist university , 2009 ."
  ],
  "abstract_text": [
    "<S> we first give an elementary derivation of the shrinkage function widely used in compressive sensing and statistical estimations . </S>",
    "<S> then we demonstrate how it can be used in solving several well - known problems and proving one new result in matrix approximations </S>",
    "<S> .    shrinkage function , singular value decomposition , low - rank approximation , sparse approximation .    65f15 , 65f30 , 65f35 , 65f50 , 65k10 </S>"
  ]
}