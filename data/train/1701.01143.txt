{
  "article_text": [
    "for years i have been using a toy experiment for introducing probabilistic reasoning .",
    "irrespective of whether my audience has been of professional physicists and engineers , high school students , teachers and general public , or even managers and senior officers attending a decision - making school , this toy model has always been an `` eye opener '' .",
    "this is how it was defined by the editors of a special issue of the american journal of physics , in which this ` experiment ' was first published @xcite .",
    "a thorough description of the game , and what can be learned from it , is given in ref .",
    "in particular , in that paper i explain the reasons why i do not let anyone see the balls in the box . at",
    "most we can make simulated extractions , in which we can ` almost ' see the game from the god s perspective : we know the box composition with certainty , and give a superior smile at the algorithm that is trying to guess it . `",
    "almost ' , because we remain uncertain about the color of future extractions .",
    "let us see then what we can learn from simulations .",
    "firstly , in order to allow readers to reproduce the results , details of the simulation are given in section 2 .",
    "for this reason , as examples of how to generate and analyze the sequences , r commands are given . in sections 3 - 5 ,",
    "some sequences are analyzed in detail . whenever possible , the numbers obtained from the probability theory algorithm are compared with those resulting from ` simple rules ' .",
    "but , as made clear in section 6 , this is not always possible . finally , in section 7 , i emphasize the fact that most real life cases  as random sequences of black and white balls extracted from a box are  might have ` astronomically ' small probabilities of occurrence , given the hypotheses of interest .",
    "but , nevertheless , the smallness of each conditional probability is irrelevant for the inference . instead , what matters are their ratios and the relative prior beliefs of the different hypotheses .",
    "thousand extractions from each of the boxes @xmath0 , @xmath1 and @xmath2 ( fig .",
    "[ fig : sixbox ] ) , since we can form an idea of what happens from the others just by ( anti-)symmetry .",
    "the r code to generate and analyze the sequences is based on that shown in footnote 31 of @xcite , but we report here also the inferential story as the extractions go on . moreover , for the benefit of the reader , who can then check the details of the results the ` seed ' of the random generator is given , equal to 20160715 , for the date of the talk upon which this paper is based ( no special , fancy sequences have been cherry - picked ) .    here",
    "are the four lines of r code to make initializations and extractions ( ` 0 ' for black and ` 1 ' for white ) : + ` n = 5 ; i = 0:n ; pii = i / n ; n = 1000 ; set.seed(20160715 ) ` + ` seq.b0 = rbinom(n , 1 , pii[1 ] ) ` + ` seq.b1 = rbinom(n , 1 , pii[2 ] ) ` + ` seq.b2 = rbinom(n , 1 , pii[3 ] ) ` + later , in order to have a feeling of the performances of the method , we can split the sequences in ` runs ' of 100 extractions and analyze them independently .",
    "the runs from @xmath0 will obviously be all equal , since that composition can only produce 0 s ( black ) , and therefore only the first run is shown ( fig . [",
    "fig : simulazione_b0 ] ) .",
    "( see text , especially for the meaning of `` laplace '' , that indeed stands for `` laplace _ rule of succession _ '' ) . ]",
    "we see ( upper plot ) that as we continue to observe black our confidence that we have ` picked ' @xmath0 steady increases .",
    "@xmath3 is obviously ruled out from the very beginning , while the probability we _ have to _ assign the compositions having both colors decreases exponentially with the increasing number of extractions ( note the log scale in the ordinates ) .",
    "( and for @xmath4 ) : @xmath5 [ fn : exponential_pbi ] , ] after 100 extractions the probability of @xmath0 differ from unity by about @xmath6 , essentially the probability of @xmath1 , while all others are less probable by tens of orders of magnitude .",
    "1.000000e+00 2.037036e-10 6.533186e-23 1.606938e-40 1.267651e-70 ` ] so , we are _ practically certain _ about @xmath0  but those who only like certainty have to remember that _ our only certainty is that @xmath3 is ruled out_.    the lower plot of fig .",
    "[ fig : simulazione_b0 ] shows , instead , the probability of white in a next extraction , that is @xmath7 ( blue circles ) .",
    "its exponential decrease results from the exponential decrease of @xmath8 , for @xmath9 .",
    ", @xmath10 with @xmath11 we obtain @xmath12 , or 1 in 25 billions ( remember that ` who ' analyses the sequence does not know the real content of the box . ) ] thus after 100 black in a row we become ` practically certain ' to observe black in the 101-th extraction , being the probability of white only @xmath12  but yet _ not impossible ! _ ( if you think that very improbable events do not occur in real life , then wait for appendix b. )    for comparison we could show in the same plot also the relative frequency of white in the @xmath13 extractions , as we shall do with the simulations from the other boxes , but , since in this case it is always zero , it is of little interest , and anyway not representable in a log scale .",
    "it is , instead , more interesting , the probability evaluated ( _ incorrectly _ ! ) applying the laplace rule of succession ( equation 15 of ref .",
    "@xcite ) , that in this case becomes @xmath14 , and , by complement @xmath15 for black .",
    "as we can see , the performance is rather poor .",
    "however is not laplace to be wrong ] _ en faisant , par example , remonter la plues ancienne poque de lhistoire ,  cinq mill ans , ou  1826213 jours , et le solei stant lev constenmment dans cet intervall ,  chaque rvolution de vinght - quatre heures ; il y a 1826214  parier contre un , quil se levera encore demain .",
    "_ mais ce nombre est incomparablement plus fort pour celui qui connaissant par lensemble des phnomnes , le principe rgulateur des jours et des saisons , voit que rien , ne peut en arrter le cours . _ _ '' [ _  thus we find that an event having occurred successively any number of times , the probability that it will happen again the next time is equal to this number increased by unity divided by the same number , increased by two units .",
    "placing the most ancient epoch of history at five thousand years ago , or at 182623 days , and the sun having risen constantly in the interval of each revolution of twenty - four hours , it is a bet of 1826214 to one that it will rise again tomorrow . _ _ but this number is incomparably greater for him who , recognizing in the totality of phenomena the principal regulator of days and seasons , sees that nothing can arrest the course of it_.  ] ( italics and underline mine ) @xcite",
    ". great laplace ! ( and please note once more the probability expressed in terms of a virtual coherent bet . ) [ fn : laplace_sole ] ] , but rather those who would use his formula a - critically , without understanding the assumptions behind it , which were discussed in detail in the text . in our specific case ,",
    "as it might be in important cases of real life , the prior of the propensity of the box to give white was not uniform between between 0 and 1 .",
    "we had instead only six possible values , and the full calculation takes into account of the real situation .",
    "for this reason the name of laplace is in quote marks in the legends of the figures , to mean `` misused laplace rule . ''",
    "the analysis of the sequences from box @xmath1 ( and , by symmetry , from @xmath16 ) is in general the most interesting and instructive , because the probabilities calculated using probability theory , taking into account all the available information properly , differ quite a lot from those obtained using intuitive heuristics , or from ` prescriptions ' based on improper use of theoretical results not fully understood ( see footnote [ fn : laplace_sole ] ) .",
    "figures [ fig : simulazione_b1_0_100 ] , [ fig : simulazione_b1_100_100 ] and [ fig : simulazione_b1_200_100 ] show the results of the inferences and of the ( probabilistic ) predictions based on three sequential runs of 100 extractions each .",
    "( run 1 : 1:100 ) . ]",
    "( run 2 : 101:200 ) . ]",
    "( run 3 : 201:300 ) . ]",
    "each story is peculiar , as real life situations are , and we see that  in the simulations we know the ` truth '  the method based on probability theory , and which take into account at best all available information , performs _ much better _ than the others .",
    "it is worth remembering that all real cases are _ unique _ and we can only rely on the quality of the methods , as well as of the data and all relevant information . as someone says",
    "( reference missing ) , in the bayesian analysis `` the result is the result . ''",
    "for example , in the first part of sequence on which of fig.[fig : simulazione_b1_0_100 ] is based , @xmath1 and @xmath2 seemed practically equally likely , and , as consequence , the probability of white in the next extraction was in between 1/5 and 2/5 . that s all .",
    "this the best we could say at that moment , but as soon as the overall relative frequencies of white approaches 20% ( frequencies are reported as black triangles in the lower plot ) there as a kind of ` attraction ' from @xmath1 : its probability suddenly rises , and the probability of white approaches rapidly 20% . once balls of both colors are observed , if the relative frequency of observed white goes under 20% the effect of ` attraction ' gets more enhanced , because the next possibility , related to box @xmath2 , is too far . , is `` too far ''  see appendix b. ]    also interesting is run 3 ( fig.[fig : simulazione_b1_200_100 ] ) , characterized by 16 black in a row . as a result , for a while we believed stronger and stronger we had picked up @xmath0 , and thus the probability of white in the next extraction was ( exponentially ) decreasing .",
    "resulting in small probability of white in the next extraction .",
    "than , suddenly , we observe white , and the probability of @xmath0 instantly drops to zero , while the probability @xmath1 jumps practically at 100% and that of a next white at 20% .",
    "0.000000e+00 9.803047e-01 1.965040e-02 4.487479e-05 9.129799e-10 ` + ` > sum ( pii * pbi ) ` + ` [ 1 ] 0.203948 ` ] it is nice , and instructive , to observe that from this extraction on , @xmath17 will always be _ slightly above 20%_. those who have a biased mind would speak about a ` biased estimator ' . in reality , it is a just logical consequence of the fact that , once we have ruled out @xmath0 , the probability of white in a future extraction , weighted average of all possible values of the propensity of the box to give white , _ has to be _ slightly above the minimum possible value of propensity , that is 1/5 . the frequency based value , as well as that from the _ laplace rule _",
    ", remains quite for a while below 20% , and than oscillates around it , in contradiction with the fact that @xmath0 has been definitively ruled out .",
    "you might object that after a very long sequence also the other evaluations will eventually converge ( see footnote 28 of ref .",
    "@xcite for remarks on the precise meaning of this term in probability theory ) , but , as it as been famously said , `` _ in the long run we are all dead_. '' let us see what happens if we analyze the full sequence of 1000 extractions ( fig .",
    "[ fig : simulazione_b1_0_1000 ] ) .",
    "( full sequence 1:1000 ) . ]",
    "the frequency based evaluations of the next observation is still oscillating around 20% , while that obtained from probability theory approaches 1/5 ( from above ! ) by @xmath18 [ rough estimate obtained extrapolating the probability of @xmath19 from the above plot ] .",
    "let us conclude this round up by also showing the results of the analysis of three runs and of the complete sequence from the box @xmath2 ( figures [ fig : simulazione_b2_0_100 ] , [ fig : simulazione_b2_100_100 ] , [ fig : simulazione_b2_200_100 ] and [ fig : simulazione_b2_0_100 ] )     ( run 1 : 1:100 ) . ]     ( run 2 : 101:200 ) . ]",
    "( run 3 : 201:300 ) . ]",
    "( full sequence 1:1000 ) . ]    without further comments , besides that in probabilistic matter , as in real life , the german dictum `` einmal ist keinmal '' applies .",
    "after having seen the comparisons of the evaluations of probabilities of white in a future extraction , someone would like to see something similar concerning the probability of the different box compositions in the light of the observed sequence .",
    "but he / she will be disappointed to learn that such a comparison is simply impossible because _ frequentists prohibit the very concept of _ @xmath20 . that s all , sorry ! ( and i am sorry for you , if you thought you were a frequentist , but , nevertheless , you also thought that such a probability had a meaning ",
    "see @xcite and references therein for details ) .",
    "an important misconception about probability is to confuse the probability of the effect given a given hypothesis with the probability of that hypothesis given the observed effect .",
    "the name `` prosecutor fallacy '' , with which this logical error is often designed gives , by itself , an idea of its relevant importance in real life . continuing with the style of this paper , i would like to touch this point using the third run of the sequence from box @xmath1 ( fig .",
    "[ fig : simulazione_b1_200_100 ] ) , which i find particular instructive .",
    "we shall analyze what we have learned after the 16th , the 17th and the 100th extraction , also giving the details of the calculations in r , which start with the usual initialization ( ` n=5 ; i=0:n ; pii = i / n ` ) :    @xmath21 ( run 3 ) : at the beginning we had 16 black in a row , resulting on the following probabilities : +    ` > n=16 ; x=0 ; ( pbi =   pii^x * ( 1-pii)^(n - x ) / sum ( pii^x * ( 1-pii)^(n - x ) ) ) ` + ` [ 1 ] 9.723559e-01 2.736939e-02 2.743123e-04 4.176237e-07 6.372432e-12 ` + ` > sum ( pii * pbi ) ` + ` [ 1 ] 0.005583852 ` +   we are 97% confident to have got @xmath0 , 2.7% to have got @xmath1 , and so on .",
    "on the other hand , the probabilities to get `` 0 white in 16 trials ''  be careful , i am trying to fool you  under the different hypotheses @xmath22 are @xmath23 , that is +   ` > ( ( 5-i)/5)^16 ` + ` [ 1 ] 1.000000e+00 2.814750e-02 2.821110e-04 4.294967e-07 6.553600e-12 0.000000e+00 `   + so it seems than that the small probability to @xmath1 is due to the small probability to get the ` observation ' @xmath24 from @xmath1 ; and similarly with the other boxes which contain white balls .",
    "@xmath25 ( run 3 ) : here is what happened after the next extraction , in which we observe white : +    ` > n=17 ; x=1 ; ( pbi =   pii^x * ( 1-pii)^(n - x ) / sum ( pii^x * ( 1-pii)^(n - x ) ) ) ` + ` [ 1 ] 0.000000e+00 9.803047e-01 1.965040e-02 4.487479e-05 9.129799e-10   0.000000e+00 ` + ` > sum ( pii * pbi ) ` + ` [ 1 ] 0.203948 `   + what is , instead , the probability of the observation , subject to the different compositions ?",
    "you might think at binomial distributions resulting in 1 success in 17 trials , with probabilities given by @xmath26 , that is +   ` >",
    "dbinom(x , n , pii ) ` + ` [ 1 ] 0.000000e+00 9.570149e-02 1.918355e-03 4.380867e-06 8.912896e-11 0.000000e+00 `   + however , this is not we have really observed , but just _ its summary_. the observation was indeed _ the _ sequence ! and the probability of the sequence is quite different : +   ` > pii^x * ( 1-pii)^(n - x ) ` + ` [ 1 ] 0.000000e+00 5.629500e-03 1.128444e-04 2.576980e-07 5.242880e-12 0.000000e+00 `    @xmath11 ( run 3 ) : as it easy to predict , the difference becomes ` dramatic ' for very large values of @xmath13 .",
    "having observed 18 white in 100 extractions , these are the probabilities of the hypotheses : +    ` > n=100 ; x=18 ; ( pbi =   pii^x * ( 1-pii)^(n - x ) / sum ( pii^x * ( 1-pii)^(n - x ) ) ) ` + ` [ 1 ] 0.000000e+00 9.999851e-01 1.491273e-05 8.011548e-17 2.938692e-39 0.000000e+00 `   + to be compared with the probability of 18 successes in 100 trials for the different boxes : +   ` > dbinom(x , n , pii ) ` + ` [ 1 ] 0.000000e+00 9.089812e-02 1.355559e-06 7.282455e-18 2.671256e-40 `   + but the conditional probabilities of what we have really observed are now strikingly different : +   ` > pii^x * ( 1-pii)^(n - x ) ` + ` [ 1 ] 0.000000e+00 2.964277e-21 4.420612e-26 2.374881e-37 8.711229e-60   0.000000e+00 `   + but indeed , the fact that this sequence _ had _",
    "@xmath27 _ chance _ ( really in the sense of a propensity ) to occur from @xmath1 _ is absolutely irrelevant_. what matters is only this probability with respect to those from the other boxes .",
    "indeed the respective conditional probabilities provide the bayes - turing factors for every pair of hypotheses . and therefore , since _ in our toy experiment _ the different compositions were initially _ equally likely _ , we get odds of @xmath1 vs @xmath2 of @xmath28 ; vs @xmath29 and @xmath16 @xmath30 and @xmath31 , respectively .",
    "these are the numbers that matter .    at this point",
    "i hope the lesson is clear , without you need to be further impressed with the numbers that we would get analyzing the full sequence of 1000 extractions :    * that fact that we can make our inference and prediction based on the number of trials and the number of successes it is because these _ summaries _ are _ ` sufficient ' _ for the purpose of the inference ( and forecasting ) ; but the real _ observation _ is the sequence ; * most of the fact of real life _ had _ very little chance to occur , if we analyze them with enough accuracy . but",
    "this implies little on the probabilities of the cause that might have produced them . what matters are the ratio of conditional probabilities : @xmath32 .",
    "having to evaluate probabilities of hypotheses and probabilities of occurrences of future events , unless you possess a crystal ball , it is hard to out - perform bayesian reasoning , if it is used consistently , and all the available pieces of information are properly taken into account .",
    "but the lesson which comes from playing with the simulated sequences goes beyond the demonstration of the power of the so called bayesian methods .",
    "for example i find it important that , in the training of _ probabilistic thinking _ , people should be exposed to the rich variety of what can occur randomly . and , therefore , most events of real life _ had _ very little chance to occur .",
    "think , for example , at a given configuration of light content in pixels , when you shoot a picture with a digital camera .",
    "more simply , and easier to calculate , consider a number to twelve decimal places that can come from a gaussian random number generator , like that obtained with the following r commands : +   ` > options(digits=14 ) ; set.seed(20160715 ) ; nd=12 ; dxm=10^(-nd)/2 ` + ` > ( xr = round(rnorm(1 ) , nd ) ) ; as.double(sprintf(\"%.2e \" , dnorm(xr , ) * 2 * dxm ) ) ` + ` [ 1 ] 1.479427401471 ` + ` [ 1 ] 1.34e-13 ` + ` > (",
    "xr = round(rnorm(1 ) , nd ) ) ; as.double(sprintf(\"%.2e \" , dnorm(xr , ) * 2 * dxm ) ) ` + ` [ 1 ] -0.762658301757 ` + ` [ 1 ] 2.98e-13 `   + ( yes , every time you repeat this line of code you will observe , _ with certainty _ , numbers which _ had _ about 1-in - trillions chance to occur ! and they all come with probability 1 from a gaussian random generator with @xmath33 and and @xmath34 )    the reason i insist on these apparently trivial considerations is that i have seen too often in the past , and even quite recently , attempts of indoctrinating people with ` statistical regularities ' .",
    "these attempts imply a misinterpretation of probability theorems and , at the same time , a refusal of the concept of a single event probability .",
    "instead , not only degrees of beliefs apply to single events , but also propensities , if we reflect on the fact that a propensity might change with time @xcite .",
    "this work was partially supported by a grant from simons foundation which allowed me a stimulating working environment during my visit at the isaac newton institute of cambridge , uk ( epsrc grant ep / k032208/1 ) .",
    "it is a pleasure to thank paolo agnoli of pangea formazione , for discussions on probabilistic matter , always with some philosophical flavor , encouragements to go ahead with the six box toy experiment , and for comments on the manuscript , which has also benefitted of comments by patricia wiltshire ."
  ],
  "abstract_text": [
    "<S> following a paper in which the fundamental aspects of probabilistic inference were introduced by means of a toy experiment , details of the analysis of simulated long sequences of extractions are shown here . </S>",
    "<S> in fact , the striking performance of probability - based inference and forecasting , compared to those obtained by simple ` rules ' , might impress those practitioners who are usually underwhelmed by the philosophical foundation of the different methods . </S>",
    "<S> the analysis of the sequences also shows how the smallness of the probability of what has actually been observed , given the hypotheses of interest , is irrelevant for the purpose of inference .    </S>",
    "<S> # 1    _ `` grown - ups like numbers '' _ + ( saint - exupry s little prince ) </S>"
  ]
}