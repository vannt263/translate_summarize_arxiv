{
  "article_text": [
    "consider the additive gaussian noise channel with feedback as depicted in figure  1 .",
    "the channel @xmath0 @xmath12 has additive gaussian noise @xmath13 where @xmath14 .",
    "we wish to communicate a message @xmath15 reliably over the channel @xmath16 .",
    "the channel output is causally fed back to the transmitter .",
    "we specify a @xmath17 code with the codewords . ]",
    "@xmath18 @xmath19 @xmath20 @xmath21 satisfying the expected power constraint @xmath22 and decoding function @xmath23 the probability of error @xmath24 is defined by @xmath25 where the message @xmath26 is independent of @xmath27 and is uniformly distributed over @xmath28 .",
    "we call the sequence @xmath29  an @xmath30-block feedback capacity sequence if for every @xmath31 , there exists a sequence of @xmath32 codes with @xmath33 as @xmath34 , and for every @xmath31 and any sequence of codes with @xmath35 codewords , @xmath24 is bounded away from zero for all @xmath30 .",
    "we define the feedback capacity @xmath36 as @xmath37 if the limit exists .",
    "this definition of feedback capacity agrees with the usual operational definition for the capacity of memoryless channels without feedback as the supremum of achievable rates  @xcite .    in @xcite ,",
    "cover and pombra characterized the @xmath30-block feedback capacity @xmath38 as @xmath39 here @xmath40 , @xmath41 and @xmath42 respectively denote the covariance matrices of @xmath43 @xmath44 and @xmath27 , and the maximization is over all @xmath45 of the form @xmath46 with a strictly lower - triangular @xmath47 matrix @xmath48 and multivariate gaussian @xmath49 independent of @xmath27 such that @xmath50 equivalently , we can rewrite ( [ fb cap 1 ] ) as @xmath51 where the maximization is over all nonnegative definite @xmath47 matrices @xmath52 and strictly lower triangular @xmath53 matrices @xmath48 such that @xmath54    when the noise process @xmath55 is stationary , the @xmath30-block capacity is super - additive in the sense that @xmath56 consequently , the feedback capacity @xmath36 is well - defined ( see , for example , plya and szeg  @xcite ) as @xmath57 to obtain a closed - form expression for the feedback capacity @xmath58 , however , we need to go further than ( [ fb cap 3 ] ) since the above characterization does not give any hint on the sequence of the optimal @xmath59 achieving @xmath38 or more importantly , its limiting behavior .    in this paper , we study in detail the case where the additive gaussian noise process @xmath60 is a moving average process of order one ( ma(1 ) ) .",
    "we define the gaussian ma(1 ) noise process @xmath60 with parameter @xmath61 as @xmath62 where @xmath63 is a white gaussian innovation process . without loss of generality",
    ", we will assume that @xmath64 , has unit variance .",
    "there are alternative ways of defining gaussian ma(1 ) processes , which we will review in section  [ sec - noise ] .",
    "note that the condition @xmath65 is not restrictive .",
    "when @xmath66 , it can be readily verified that the process @xmath3 has the same distribution as the process @xmath67 defined by @xmath68 where the moving average parameter @xmath69 is given by @xmath70 thus giving @xmath71    we state the main theorem , the proof of which will be given in section  [ sec - pf ] .    for the additive gaussian ma(1 ) noise channel @xmath72 with the gaussian ma(1 ) noise process @xmath3 defined in ( [ ma1 noise ] ) , the feedback capacity @xmath58 under the power constraint @xmath73 is given by @xmath74 where @xmath8 is the unique positive root of the fourth - order polynomial @xmath75    as will be shown later in sections 3 and 4 , the feedback capacity @xmath58 is achieved by an asymptotically stationary ergodic input process @xmath2 satisfying @xmath76 for all @xmath77 . thus by ergodic theorem , the feedback capacity does not diminish under a more restrictive power constraint @xmath78 ( see also the arguments given in  ( * ? ? ?",
    "* ; * ? ? ?",
    "* section viii ) based on the stationarity of the noise process . )",
    "the literature on gaussian feedback channels is vast .",
    "we first mention some prior work closely related to our main discussion . in earlier work , schalkwijk and kailath  @xcite",
    "( see also the discussion by wolfowitz  @xcite ) considered the feedback over the additive white gaussian noise channel , and proposed a simple linear signalling scheme that achieves the feedback capacity .",
    "the coding scheme by schalkwijk and kailath can be summarized as follows : let @xmath79 be one of @xmath80 equally spaced real numbers on some interval , say , @xmath81 $ ] . at time @xmath82 , the receiver forms the maximum likelihood estimate @xmath83 of @xmath79 . using the feedback information , at time @xmath84",
    ", we send @xmath85 , where @xmath86 is a scaling factor properly chosen to meet the power constraint .",
    "after @xmath30 transmissions , the receiver finds the value of @xmath79 among @xmath80 alternatives that is closest to @xmath87 .",
    "this simple signalling scheme , without any coding , achieves the feedback capacity .",
    "as is shown by shannon  @xcite , feedback does not increase the capacity of memoryless channels .",
    "( see also kadota et al .",
    "@xcite for continuous cases . ) the benefit of feedback , however , does not consist of the simplicity of coding only .",
    "the probability of decoding error of the schalkwijk - kailath scheme decays doubly exponentially in the duration of communication , compared to the exponential decay for the nonfeedback scenario . in fact , there exists a feedback coding scheme such that the probability of decoding error decreases more rapidly than the exponential of any order  @xcite .",
    "later schalkwijk extended his work to the center - of - gravity information feedback for higher dimensional signal spaces  @xcite .",
    "butman  @xcite generalized the linear coding scheme of schalkwijk and kailath for white noise processes to autoregressive ( ar ) noise processes . for first - order autoregressive ( ar(1 ) ) processes @xmath60 with regression parameter @xmath88 @xmath89 defined by @xmath90 he obtained a lower bound on the feedback capacity as @xmath91 , where @xmath8 is the unique positive root of the fourth - order polynomial @xmath92 this rate has been shown to be optimal among a certain class of linear feedback schemes by wolfowitz  @xcite and tiernan  @xcite and is strongly believed to be the capacity of the ar(1 ) feedback capacity .",
    "tiernan and schalkwijk  @xcite found an upper bound of the ar(1 ) feedback capacity , which meets butman s lower bound for very low and very high signal - to - noise ratio .",
    "butman  @xcite also obtained capacity upper and lower bounds for ar processes with higher order .",
    "for the case of moving average ( ma ) noise processes , there are far fewer results in the literature , although ma processes are usually more tractable than ar processes of the same order .",
    "ozarow  @xcite gave upper and lower bounds of the feedback capacity for ar(1 ) and ma(1 ) channels and showed that feedback strictly increases the capacity .",
    "substantial progress was made by ordentlich  @xcite ; he observed that @xmath93 in ( [ fb cap 2 ] ) is at most of rank @xmath82 for a ma noise process with order @xmath82 .",
    "he also showed that the optimal @xmath94 necessarily has the property that the current input signal @xmath95 is orthogonal to the past outputs @xmath96 .",
    "for the special case of ma(1 ) processes , this development , combined with the arguments given in  @xcite , suggests that a linear signalling scheme similar to the schalkwijk - kailath scheme be optimal , which is proved by our theorem  1 .",
    "a recent report by yang , kavi , and tatikonda  @xcite ( see also yang s thesis  @xcite ) studies the feedback capacity of the general arma(@xmath82 ) case using the state - space model and offers a conjecture on the feedback capacity as a solution to an optimization problem that does not depend on the horizon @xmath30 . for the special case @xmath97 with the noise process @xmath60 defined by @xmath98 they conjecture that the schalkwijk - kailath - butman scheme is optimal . the corresponding achievable rate can be written in a closed form as @xmath91 , where @xmath8 is the unique positive root of the fourth - order polynomial @xmath99 and @xmath100 by taking @xmath101 or @xmath102 , we can easily recover and , respectively . thus , in the special case @xmath101 , our theorem  1 confirms the yang - kavi - tatikonda conjecture .    to conclude this section",
    ", we review , in a rather incomplete manner , previous work on the gaussian feedback channel in addition to aforementioned results , and then point out where the current work lies in the literature .",
    "the standard literature on the gaussian feedback channel and associated simple feedback coding schemes traces back to a 1956 paper by elias  @xcite and its sequels  @xcite .",
    "turin  @xcite , horstein  @xcite , khasminskii  @xcite , and ferguson  @xcite studied a sequential binary signalling scheme over the gaussian feedback channel with symbol - by - symbol decoding that achieves the feedback capacity with an error exponent better than the nonfeedback case .",
    "as mentioned above , schalkwijk and kailath  @xcite made a major breakthrough by showing that a simple linear feedback coding scheme achieves the feedback capacity with doubly exponentially decreasing probability of decoding error .",
    "this fascinating result has been extended in many directions .",
    "omura  @xcite reformulated the feedback communication problem as a stochastic - control problem and applied this approach to multiplicative and additive noise channels with noiseless feedback and to additive noise channels with noisy feedback .",
    "pinsker  @xcite , kramer  @xcite , and zigangirov  @xcite studied feedback coding schemes under which the probability of decoding error decays as the exponential of arbitrary high order .",
    "wyner  @xcite and kramer  @xcite studied the performance of the schalkwijk - kailath scheme under a peak power constraint and reported the singly exponential behavior of the probability of decoding error under a peak power constraint .",
    "the actual error exponent of the gaussian feedback channel under the peak power constraint was later obtained by schalkwijk and barron  @xcite .",
    "kashyap  @xcite , lavenberg  @xcite and kramer  @xcite looked at the case of noisy or intermittent feedback .",
    "the more natural question of transmitting a gaussian source over a gaussian feedback channel was studied by kailath  @xcite , cruise  @xcite , schalkwijk and bluestein  @xcite , ovseevich  @xcite , and ihara  @xcite .",
    "there are also many notable extensions of the schalkwijk - kailath scheme in the area of multiple user information theory . using the schalkwijk - kailath scheme , ozarow and",
    "leung - yan - cheong  @xcite showed that feedback increases the capacity region of _ stochastically _ degraded broadcast channels , which is rather surprising since feedback does _ not _ increase the capacity region of _ physically _ degraded broadcast channels , as shown by el gamal  @xcite .",
    "ozarow  @xcite also established the feedback capacity region of two - user white gaussian multiple access channel through a very innovative application of the schalkwijk - kailath coding scheme .",
    "the extension to a larger number of users was attempted by kramer  @xcite , where he also showed that feedback increases the capacity region of strong interference channels .    following these results on the white gaussian noise channel on hand , the next focus was on the feedback capacity of the colored gaussian noise channel .",
    "butman  @xcite extended the schalkwijk - kailath coding scheme to autoregressive noise channels .",
    "subsequently , tiernan and schalkwijk  @xcite , wolfowitz  @xcite , ozarow  @xcite , dembo  @xcite , and yang et al .",
    "@xcite studied the feedback capacity of finite - order arma additive gaussian noise channels and obtained many interesting upper and lower bounds . using an asymptotic equipartition theorem for nonstationary nonergodic gaussian noise processes , cover and pombra",
    "@xcite obtained the @xmath30-block capacity ( [ fb cap 2 ] ) for the arbitrary colored gaussian channel with or without feedback .",
    "( we can take @xmath103 in for the nonfeedback case . ) using matrix inequalities , they also showed that feedback does not increase the capacity much ; namely , feedback increases the capacity at most twice ( a result obtained by pinsker  @xcite and ebert  @xcite ) , and feedback increases the capacity at most by half a bit .    the extensions and refinements of the result by cover and pombra abound .",
    "dembo  @xcite showed that the feedback does not increase the capacity at very low signal - to - noise ratio or very high signal - to - noise ratio .",
    "as mentioned above , ordentlich  @xcite examined the properties of the optimal solution @xmath94 in ( [ fb cap 2 ] ) and found the rank condition of @xmath93 for finite - order ma noise processes .",
    "chen and yanagi  @xcite studied cover s conjecture  @xcite that the feedback capacity is at most as large as the nonfeedback capacity with twice the power , and made several refinements on the upper bounds by cover and pombra .",
    "thomas  @xcite , pombra and cover  @xcite , and ordentlich  @xcite extended the factor - of - two bound result to the colored gaussian multiple access channels with feedback . recently",
    "yang , kavi , and tatikonda  @xcite revived the control - theoretic approach ( @xcite ) to the stationary arma(@xmath82 ) gaussian feedback capacity problem .",
    "although one - sentence summary would not do justice to their contribution , yang et reformulated the feedback capacity problem as a stochastic control problem and used dynamic programming for the numerical computation of the @xmath30-block feedback capacity . in a series of papers",
    "@xcite , ihara obtained coding theorems for continuous - time gaussian channels with feedback and showed that the factor - of - two bound on the feedback capacity is tight by considering cleverly constructed nonstationary channels both in discrete time  @xcite and continuous time  @xcite .",
    "( see also  ( * ? ? ?",
    "* ; * ? ? ?",
    "* examples 5.7.2 and 6.8.1 ) . ) in fact , besides the white gaussian noise channel , ihara s example is the only nontrivial channel with known closed - form feedback capacity .",
    "hence theorem  1 provides the first feedback capacity result on stationary colored gaussian channels .",
    "moreover , as will be discussed in section  4 , a simple linear signalling scheme similar to the schalkwijk - kailath scheme achieves the feedback capacity .",
    "this result links the cover - pombra formulation of the feedback capacity with the schalkwijk - kailath scheme and its generalizations to stationary colored channels , and provides new hope for the optimality of the achievable rate for the ar(1 ) channel obtained by butman  @xcite .",
    "in this section , we digress a little to review a few characteristics of first - order moving average gaussian processes .",
    "first , we give three alternative characterizations of gaussian ma(1 ) processes .",
    "as defined in the previous section , the gaussian ma(1 ) noise process @xmath60 with parameter @xmath104 can be characterized as @xmath105 where the innovations @xmath106 are @xmath107    we reinterpret the above definition in ( [ ma1 noise2 ] ) by regarding the noise process @xmath3 as the output of the linear time - invariant filter with transfer function @xmath108 which is driven by the white innovation process @xmath109 .",
    "thus we alternatively characterize the gaussian ma(1 ) noise process @xmath3 with parameter @xmath104 and unit innovation through its power spectral density @xmath110 given by @xmath111    we can further identify the power spectral density @xmath110 with the infinite toeplitz covariance matrix of a gaussian process .",
    "thus , we can define @xmath3 as @xmath112 for each finite horizon @xmath30 where @xmath113 is tri - diagonal with @xmath114 , \\intertext{or equivalently , } [ k_z]_{i , j } = \\left\\ { \\begin{array}{ll } 1 + \\alpha^2,&\\quad |i - j| = 0,\\\\ \\alpha,&\\quad |i - j| = 1,\\\\ 0 , & \\quad |i - j| \\ge 2 .",
    "\\end{array } \\right.\\end{gathered}\\ ] ] note that this covariance matrix @xmath113 is consistent with our initial definition of the ma(1 ) process given in ( [ ma1 noise2 ] ) .",
    "thus all three definitions of the ma(1 ) process given above are equivalent . as we will see in the next section , the special structure of the ma(1 ) process , especially the tri - diagonality of the covariance matrix , makes the maximization in ( [ fb cap 2 ] ) easier than the generic case .",
    "we will need the entropy rate of the ma(1 ) gaussian process later in our discussion . as shown by kolmogorov ( see  ( * ? ? ? * ; * ? ? ?",
    "* section 11.6 ) ) , the entropy rate of a stationary gaussian process with power spectral density @xmath115 can be expressed as @xmath116 we can calculate the above integral with the power spectral density @xmath110 in ( [ ma1 spectrum ] ) by jensen s formula  ( * ? ? ?",
    "* theorem 15.18 ) @xmath117 \\log |\\alpha|,&\\quad |\\alpha| > 1 , \\end{array } \\right.\\ ] ] and obtain the entropy rate of the ma(1 ) gaussian process ( [ ma1 noise2 ] ) as @xmath118 & = \\left\\ { \\begin{array}{ll } \\frac{1}{2 } \\log ( 2\\pi e),&\\quad |\\alpha| \\le 1,\\\\[1em ] \\frac{1}{2 } \\log ( 2\\pi e \\alpha^2),&\\quad |\\alpha| >",
    "1 . \\end{array } \\right .",
    "\\label{ent rate}\\end{aligned}\\ ] ] ( one can alternatively deal with the determinant of @xmath119 directly by a simple recursion .",
    "for example , we can show that @xmath120 for @xmath121 . ) for a more general discussion of the entropy rate of stationary gaussian processes , refer to ( * ? ? ?",
    "* ; * ? ? ?",
    "* chapter 2 ) .",
    "we finish our digression by noting a certain reciprocal relationship between the gaussian ma(1 ) process with parameter @xmath104 and the gaussian ar(1 ) process with parameter @xmath122 .",
    "we can define the gaussian ar(1 ) process @xmath60 with parameter @xmath123 @xmath89 as @xmath124 where the innovations @xmath125 are @xmath126 and @xmath127 is independent of @xmath128 .",
    "equivalently , we can define the above process as the output of the linear time - invariant filter with transfer function @xmath129 where @xmath130 is the transfer function ( [ ma1 filter ] ) of the ma(1 ) process with parameter @xmath104 . this reciprocity is indeed reflected in the striking similarity between the fourth - order polynomial ( [ ma1 cap ] ) for the capacity of the gaussian ma(1 ) noise channel and the fourth - order polynomial ( [ ar1 cap ] ) for the best known achievable rate of the gaussian ar(1 ) noise channel .",
    "we will first transform the optimization problem @xmath131 to a series of ( asymptotically ) equivalent forms .",
    "then we solve the problem by imposing individual power constraints @xmath132 on each input signal .",
    "subsequently we optimize over @xmath133 under the average power constraint @xmath134 then using lemma  [ lemma asymp ] , we will prove that the uniform power allocation @xmath135 is asymptotically optimal .",
    "this leads to a closed - form solution given in theorem  1 .",
    "1em _ step 1 .",
    "transformations into equivalent optimization problems .",
    "_    recall that we wish to solve the optimization problem : @xmath136 over all nonnegative definite @xmath93 and strictly lower triangular @xmath137 satisfying @xmath138 we approximate the covariance matrix @xmath113 of the given ma(1 ) noise process with parameter @xmath104 by another covariance matrix @xmath139 .",
    "define @xmath140 where the lower - triangular toeplitz matrix @xmath141 is given by @xmath142 \\alpha & 1 & 0 & \\cdots&0\\\\ 0 & \\alpha & 1 & \\ddots & \\vdots\\\\[3pt ] \\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\[3pt ] 0 & \\cdots & 0 & \\alpha & 1 \\end{array } \\right].\\end{aligned}\\ ] ] this matrix @xmath143 is a covariance matrix of the gaussian process @xmath144 defined by @xmath145 where @xmath128 is the white gaussian process with unit variance .",
    "it is easy to check that @xmath146 ( i.e. , @xmath147 is nonnegative definite ) and that the difference between @xmath113 and @xmath139 is given by @xmath148_{i , j } = \\left\\ { \\begin{array}{ll } \\alpha^2,&\\quad i = j=1,\\\\ 0,&\\quad\\text{otherwise}. \\end{array}\\right.\\end{aligned}\\ ] ] it is intuitively clear that there is no asymptotic difference in capacity between the channel with the original noise covariance @xmath113 and the channel with noise covariance @xmath139 .",
    "we will prove this claim more rigorously in the appendix . throughout",
    "we will assume that the noise covariance matrix of the given channel is @xmath139 , which is equivalent to the statement that the time - zero noise innovation @xmath149 is revealed to both the transmitter and the receiver .",
    "now by identifying @xmath150 for some lower - triangular @xmath151 and identifying @xmath152 for some strictly lower - triangular @xmath153 , we transform the optimization problem ( [ opt 1 ] ) into @xmath154 \\text{subject to}&\\quad \\tr({f}_v { f}_v^t + { f}_z { f}_z^t ) \\le np \\end{array}\\end{aligned}\\ ] ] with new variables @xmath155 .",
    "we shall use @xmath156-dimensional row vectors @xmath157 and @xmath158 , @xmath159 , to denote the @xmath77-th row of @xmath160 $ ] and @xmath161 $ ] , respectively .",
    "there is an obvious identification between the time-@xmath77 input signal @xmath162 and the vector @xmath157 , @xmath163 for we can regard @xmath157 as a point in the hilbert space with the innovations of @xmath49 and @xmath27 as a basis .",
    "we can similarly identify @xmath164 with @xmath158 and identify @xmath165 with @xmath166 .",
    "we also introduce new variables @xmath167 representing the power constraint for each input @xmath157 . now the optimization problem in ( [ opt 2 ] ) becomes the following equivalent form : @xmath168 \\text{subject to}&\\quad \\|f_i\\|^2 \\le p_i , \\quad i=1,\\ldots , n,\\\\[6pt ] &",
    "\\quad \\sum_{i=1}^n p_i \\le np . \\end{array}\\end{aligned}\\ ] ] here @xmath169 denotes the euclidean norm of a @xmath156-dimensional vector .",
    "note that the variables @xmath170 should satisfy @xmath171 where @xmath172    1em _ step 2 .",
    "optimization under the individual power constraint for each signal .",
    "_    we solve the optimization problem ( [ opt 4 ] ) in @xmath173 after fixing @xmath132 .",
    "this step is mostly algebraic , but we can easily give a geometric interpretation .",
    "we need some notation first .",
    "we define an @xmath30-by-@xmath156 matrix @xmath174 : = \\left [ \\begin{array}{c } f_1 + h_1\\\\ \\vdots\\\\ f_n + h_n \\end{array } \\right ] = f + h,\\end{aligned}\\ ] ] and we define the @xmath30-by-@xmath156 matrix @xmath175 by @xmath176 : = [ \\,0_{n\\times n}~i\\,],\\end{aligned}\\ ] ] where @xmath177 is identity .",
    "we also define an @xmath30-by-@xmath156 matrix @xmath178 : = \\left [ \\begin{array}{c } h_1 - e_1\\\\ \\vdots\\\\ h_n - e_n \\end{array } \\right ] = h - e.\\end{aligned}\\ ] ] we can interpret the row vector @xmath179 as the noise innovation @xmath11 and the row vector @xmath180 as @xmath181 .    we will use the notation @xmath182 to denote the @xmath82-by-@xmath156 submatrix of @xmath183 which consists of the first @xmath82 rows of @xmath183 , that is , @xmath184.\\end{aligned}\\ ] ] we will use the similar notation for the @xmath82-by-@xmath156 submatrices of @xmath185 and @xmath186 .",
    "we now introduce a sequence of @xmath156-by-@xmath156 matrices @xmath187 as @xmath188 observe that @xmath189 is of full rank and thus that @xmath190 always exists .",
    "we can view @xmath191 as a map of a @xmath156-dimensional row vector ( acting from the right ) to its component orthogonal to the subspace spanned by the rows @xmath192 of @xmath189 .",
    "( or @xmath193 maps a generic random variable @xmath194 to @xmath195 . )",
    "it is easy to verify that @xmath196    finally we define the intermediate objective functions of the maximization ( [ opt 4 ] ) as @xmath197    we will show that if @xmath198 maximizes @xmath199 , then @xmath200 maximizes @xmath201 for some @xmath202 satisfying @xmath203 thus the maximization for @xmath204 can be solved in a greedy fashion by sequentially maximizing @xmath205 through @xmath206 .",
    "furthermore , we will obtain the recursive relationship @xmath207    we need the following result to proceed to the actual maximization .",
    "suppose @xmath208 and @xmath209 .",
    "suppose @xmath189 and @xmath193 defined as above .",
    "let @xmath210 be an arbitrary subspace of @xmath211 such that @xmath210 is not contained in the span of @xmath192 .",
    "then , for any @xmath212 , @xmath213 furthermore , if @xmath214 , the maximum is attained by @xmath215    when @xmath216 , that is , @xmath217 , the maximum of @xmath218 is attained by any vector @xmath219 @xmath220 orthogonal to @xmath221 , and we trivially have @xmath222    when @xmath223 , we have @xmath224 where the first inequality follows from the fact that @xmath225 is nonnegative definite .",
    "it is easy to check that we have equality if @xmath226 is given by ( [ opt v ] ) .",
    "c@c    &     + ( a ) the case @xmath227 . &",
    "( b ) the case @xmath228 .",
    "we observe that , for @xmath229 , @xmath230 \\left [ \\begin{array}{c } s_{k-1}\\notag\\\\ s_k \\end{array } \\right]^t \\right)\\notag\\\\ & = \\det \\left [ \\begin{array}{cc } s_{k-1}s_{k-1}^t & s_{k-1 } s_k^t\\notag\\\\ s_k s_{k-1}^t & s_k s_k^t \\end{array } \\right]\\notag\\\\ & = \\det ( s_{k-1 } s_{k-1}^t ) \\cdot s_k ( i - s_{k-1}^t ( s_{k-1 } s_{k-1}^t)^{-1 } s_{k-1 } ) s_k^t \\notag\\\\ & = \\det ( s_{k-1 } s_{k-1}^t ) \\cdot s_k \\,\\pi_{k-1 } s_k^t \\notag\\\\ & = \\det ( s_{k-1 } s_{k-1}^t ) \\cdot ( f_k + g_k + e_k ) \\,\\pi_{k-1 } ( f_k + g_k + e_k)^t \\notag\\\\ \\label{eq orth } & = \\det ( s_{k-1 } s_{k-1}^t ) \\cdot \\left [ 1 + ( f_k + g_k ) \\,\\pi_{k-1 } ( f_k + g_k)^t \\right]\\end{aligned}\\ ] ] where ( [ eq orth ] ) follows since @xmath231 @xmath232 and @xmath233 now fix @xmath234 . since @xmath235 is not contained in @xmath236 and @xmath237 , we have from the above lemma and ( [ eq orth ] ) that @xmath238 if @xmath239 , the maximum of is attained by @xmath240 in the special case @xmath102 , that is , when the noise is white , we trivially have @xmath241 which immediately implies that @xmath242 which , in turn , combined with the concavity of the logarithm , implies that @xmath243    we continue our discussion throughout this step under the assumption @xmath239 .",
    "until this point we have not used the special structure of the ma(1 ) noise process .",
    "now we rely heavily on it .",
    "we trivially have @xmath244 following ( [ max det ] ) , we have , for @xmath245 @xmath246.\\end{aligned}\\ ] ] we wish to show that both terms in ( [ two max ] ) are individually maximized by the same optimizer @xmath247 for @xmath248 .",
    "once we establish , the desired recursion formula for @xmath249 follows immediately from the definition of @xmath249 and .",
    "we shall prove by induction .",
    "first note that @xmath250 also recall that @xmath251 and @xmath252 for @xmath253 we trivially have @xmath254 which establishes .",
    "further , from and , we can check that @xmath255\\\\ & = j_1 +   \\log \\left(1 + \\left(\\sqrt{p_2 } + |\\alpha| \\sqrt{1 - \\frac { 1}{e^{j_1 } } } \\right)^2\\right).\\end{aligned}\\ ] ]    now suppose holds for @xmath256 .",
    "for @xmath257 we observe that @xmath258 s_{k-1 } \\end{array } \\right]^t \\left [ \\begin{array}{cc } s_{k-2}s_{k-2}^t & s_{k-2}s_{k-1}^t\\\\[3pt ] s_{k-1}s_{k-2}^t & s_{k-1}s_{k-1}^t \\end{array } \\right]^{-1 }   \\left [ \\begin{array}{c } s_{k-2}\\\\[3pt ] s_{k-1 } \\end{array } \\right]\\\\ & = i - s_{k-2}^t \\left(s_{k-2 } s_{k-2}^t\\right)^{-1 } s_{k-2 } -   \\pi_{k-2}\\,{s}_{k-1}^t \\left({s}_{k-1}\\,\\pi_{k-2}\\,{s}_{k-1}^t\\right)^{-1}\\!{s}_{k-1}\\,\\pi_{k-2}\\\\ & = \\pi_{k-2 } ( i - \\pi_{k-2}\\,{s}_{k-1}^t   \\left({s}_{k-1}\\,\\pi_{k-2}\\,{s}_{k-1}^t\\right)^{-1}\\!{s}_{k-1}\\,\\pi_{k-2 } ) \\pi_{k-2}.\\end{aligned}\\ ] ] now from , ( [ e and s ] ) , and ( [ e and pi ] ) , we have @xmath259 it follows from  ( [ maximizer ] ) and ( [ g pi ] ) that , for fixed @xmath260 , both @xmath261 and @xmath262 have the same maximizer @xmath263 plugging this back to , for fixed @xmath260 , we have @xmath264 while @xmath265 but from the induction hypothesis , @xmath266 and @xmath267 have the same maximizer @xmath268 .",
    "thus @xmath269 and @xmath270 have the same maximizer @xmath271 .",
    "therefore , we have established for @xmath272 and hence for all @xmath273 from and , we easily get the desired recursion formula as @xmath274    1em _ step 3 .",
    "optimal power allocation over time .",
    "_    in the previous step , we solved the optimization problem ( [ opt 4 ] ) under a fixed power allocation @xmath132 .",
    "thanks to the special structure of the ma(1 ) noise process , this brute force optimization was tractable via backward dynamic programming . here",
    "we optimize the power allocation @xmath167 under the constraint @xmath275 ,    as we saw earlier , when @xmath102 , we can use the concavity of the logarithm to show that , for all @xmath30 , @xmath276 with @xmath277 when @xmath239 , it is not tractable to optimize @xmath167 for @xmath204 in ( [ recursion1 ] )  ( [ recursion3 ] ) to get a closed - form solution of @xmath38 for finite @xmath30 .",
    "the following lemma , however , enables us to figure out the asymptotically optimal power allocation and to obtain a closed - form solution for @xmath278 .",
    "[ lemma asymp ] let @xmath279 such that the following conditions hold :    1 .",
    "@xmath280 is continuous , concave in @xmath281 , and strictly concave in @xmath282 for all @xmath283 ; 2 .",
    "@xmath280 is increasing in @xmath282 and @xmath284 , respectively ; and 3 .   for each @xmath283",
    ", there is a unique solution @xmath285 to the equation @xmath286    for some fixed @xmath287 , let @xmath288 be any infinite sequence of nonnegative numbers satisfying @xmath289 let @xmath290 be defined recursively as @xmath291 then @xmath292 where @xmath293 is the unique solution to @xmath294 .",
    "furthermore , if @xmath295 @xmath296 then the corresponding @xmath297 converges to @xmath298 .",
    "fix @xmath31 . from the concavity and monotonicity of @xmath299 , for @xmath30 sufficiently large , @xmath300 taking @xmath301 on both sides and using the continuity of @xmath299 , we have @xmath302 since @xmath303 is arbitrary and @xmath299 is continuous , we have @xmath304 but from uniqueness of @xmath298 and strict concavity of @xmath299 in @xmath282 , we have @xmath305 thus @xmath306    it remains to show that we can actually attain @xmath298 by choosing @xmath307 , @xmath308 let @xmath309 from the monotonicity of @xmath310 and ( [ psi ] ) , we have @xmath311 thus the sequence",
    "@xmath312 has a limit , which we denote as @xmath313 .",
    "but from the continuity of @xmath310 , we must have @xmath314 thus @xmath315 .",
    "we continue our main discussion .",
    "define @xmath316 , @xmath317 and @xmath318 then @xmath319 now that @xmath320 is strictly concave and strictly increasing , @xmath321 is concave ( strictly concave in @xmath282 alone for each @xmath283 ) and elementwise strictly increasing , and @xmath322 is strictly concave , we can conclude that @xmath299 is concave in @xmath281 and strictly concave in @xmath282 for all @xmath283 . since for any @xmath283 , @xmath323 and @xmath324 as @xmath282 tends to infinity , the uniqueness of the root of @xmath325 is trivial from the continuity of @xmath299 .",
    "for an arbitrary infinite sequence @xmath288 satisfying @xmath326 we define @xmath291 note that @xmath327 now from lemma  2 , we have @xmath328 where @xmath298 is the unique solution to @xmath329 is arbitrary , we conclude that @xmath330 where the supremum ( in fact , maximum ) is over all infinite sequences @xmath331 satisfying the asymptotic average power constraint  ( [ asymp power ] ) .    finally , we prove that @xmath332 .",
    "more specifically , we will show that @xmath333 the only subtlety here is how to justify the interchange of the order of limit and supremum in ( [ lim sup ] ) and ( [ sup lim ] ) .",
    "it is easy to verify that @xmath334 for it is always advantageous to choose for each @xmath30 a finite sequence @xmath167 with @xmath275 for each @xmath30 rather than fixing a single infinite sequence @xmath331 with @xmath335 for all @xmath77 .",
    "( recall that the supremum on the right side is achieved by the uniform power allocation . )    to prove the other direction of inequality , we fix @xmath336 and choose @xmath30 and @xmath337 such that @xmath338 now we construct an infinite sequence @xmath288 by concatenating @xmath337 repeatedly , that is , @xmath339 for all @xmath340 and @xmath341 obviously , this choice of @xmath331 satisfies the power constraint ( [ asymp power ] ) .",
    "as before , let @xmath342 @xmath308 by induction , it is easy to see that @xmath343 for all @xmath344 for @xmath345 , holds trivially .",
    "suppose holds for @xmath346 then from the monotonicity of @xmath280 in @xmath282 , we have @xmath347 for all @xmath348 .",
    "thus , holds for all @xmath82 .",
    "therefore @xmath349 which , combined with ( [ j_n q ] ) , implies that @xmath350 since @xmath303 is arbitrary , we have the desired inequality .",
    "thus @xmath351",
    "we conclude this section by characterizing the capacity @xmath352 in an alternative form .",
    "recall that @xmath298 is the unique solution to @xmath353 or equivalently , @xmath354 .",
    "it is easy to verify that @xmath355 is the unique positive solution to @xmath356 this establishes the feedback capacity @xmath58 of the additive gaussian noise channel with the noise covariance @xmath357 , which is , in turn , the feedback capacity of the first - order moving average additive gaussian noise channel with parameter @xmath104 , as is argued at the end of step 1 and proved in the appendix .",
    "this completes the proof of theorem 1 .",
    "the derived asymptotically optimal feedback input signal sequence , or equivalently , the ( sequence of ) matrices @xmath358 has two prominent properties .",
    "first , the optimal @xmath359 for the @xmath30-block can be found sequentially , built on the optimal @xmath360 for the @xmath361-block .",
    "although this property may sound quite natural , it is not true in general for other channel models . later in this section",
    ", we will see an ma(2 ) channel counterexample . as a corollary to this sequentiality property",
    ", the optimal @xmath93 has rank one , which agrees with the previous result by ordentlich  @xcite .",
    "secondly , the current input signal @xmath95 is orthogonal to the past output signals @xmath362 . in the notation of section 3",
    ", we have @xmath363 this orthogonality property is indeed a necessary condition for the optimal @xmath364 for any ( possibly nonstationary nonergodic ) noise covariance matrix @xmath113  @xcite .",
    "it should be pointed out that the recursion formula  can be also derived from the orthogonality property and the optimality of rank - one @xmath93 .",
    "we explore the possibility of extending the current proof technique to a more general class of noise processes .",
    "the immediate answer is negative .",
    "we comment on two simple cases : ma(2 ) and ar(1 ) .",
    "consider the following ma(2 ) noise process which is essentially two interleaved ma(1 ) processes : @xmath365 it is easy to see that this channel has the same capacity as the ma(1 ) channel with parameter @xmath104 , which can be attained by signalling separately for each interleaved ma(1 ) channel .",
    "this suggests that the sequentiality property does not hold for this example .",
    "indeed , if we sequentially optimize the @xmath30-block capacity , we achieve the rate @xmath366 , where @xmath8 is the unique positive root of the sixth order polynomial @xmath367 it is not difficult to see that this rate is strictly less than the feedback capacity of the interleaved ma(1 ) channel unless @xmath368 . a similar argument can prove that butman s conjecture on the ar(@xmath82 ) capacity  ( * ? ? ?",
    "* ; * ? ? ?",
    "* abstract ) is not true in general for @xmath369 .",
    "in contrast to ma(1 ) channels , we are missing two basic ingredients for ar(1 ) channels  the optimality of rank - one @xmath93 and the asymptotic optimality of the uniform power allocation . under these two conditions ,",
    "both of which are yet to be justified , it is known  @xcite that the optimal achievable rate is given by @xmath370 where @xmath8 is the unique positive root of the fourth order polynomial @xmath371 there is , however , a major difficulty in establishing the above two conditions by the two - stage optimization strategy we used in the previous section , namely , first maximizing @xmath173 and then @xmath167 . for certain values of individual signal power constraints @xmath167 , the optimal @xmath173 does not satisfy the sequentiality , resulting in @xmath93 with rank higher than one . hence , a greedy maximization of @xmath372 does not establish the recursion formula for the ar(1 ) @xmath30-block capacity that corresponds to our ( [ recursion1 ] )  ( [ recursion3 ] ) : @xmath373 ( see @xcite for the derivation of the above recursion formula . ) even under the assumption that the optimal @xmath93 for the ar(1 ) channel has rank one , it has been unclear whether the uniform power allocation over time is asymptotically optimal .",
    "nonetheless , using a technique similar to the one deployed in lemma  2 , we can prove the optimality of the uniform power allocation , resolving a question raised by butman  @xcite and tiernan  @xcite among others . since the proof is a little technical in nature",
    ", we defer it to the appendix .",
    "finally we show that the feedback capacity of the ma(1 ) channel can be achieved by using a simple stationary filter of the noise innovation process .",
    "before we proceed , we point out that the optimal input process @xmath2 we obtained in the previous section is asymptotically stationary .",
    "this observation is not hard to prove through the well - developed theory on the asymptotic behavior of recursive estimators  ( * ? ? ?",
    "* ; * ? ? ?",
    "* chapter 14 ) .    at the beginning",
    ", we send code functions i.i.d .  according to @xmath374 for some @xmath375 , and",
    "transmit one of them . ]",
    "@xmath376 for subsequent transmissions , we transmit the filtered version of the noise innovation process up to the time @xmath377 : @xmath378 in other words , we use a first - order regressive filter with transfer function given by @xmath379 here @xmath380 with @xmath8 being the same unique positive root of the fourth - order polynomial ( [ ma1 cap ] ) in theorem 1 .",
    "the scaling factor @xmath381 is chosen to satisfy the power constraint as @xmath382 where @xmath383 this input process and the ma(1 ) noise process @xmath384 yield the output process given by @xmath385 y_k   & = \\beta\\ , x_{k-1 } + ( \\alpha + \\sigma ) u_{k-1 } + u_k,\\\\ & = \\beta\\ ; y_{k-1 } -\\alpha\\beta\\,u_{k-2 } + ( \\alpha - \\beta + \\sigma)u_{k-1},\\qquad k=2,3,\\ldots,\\end{aligned}\\ ] ] which is asymptotically stationary with power spectral density @xmath386 the `` asymptotic stationarity '' here should not bother us since @xmath387 is stationary for @xmath388 and @xmath389 is uniformly bounded in @xmath30 ; hence the entropy rate of the process @xmath390 is determined by @xmath391 . thus from ( [ ent rate ] ) in section",
    "[ sec - noise ] , the entropy rate of the output process @xmath387 is given by @xmath392 hence we attain the feedback capacity @xmath58 .",
    "furthermore , it can be shown that the mean - square error of @xmath393 given the observations @xmath394 decays exponentially with rate @xmath395 .",
    "in other words , @xmath396    note that the optimal filter has an interesting feature . in the light of",
    ", we can think of the output process @xmath387 as the filtered version of the noise innovation process @xmath397 through the monic filter @xmath398 as the entropy rate formula  , or more fundamentally , jensen s formula   shows , the entropy rate of @xmath387 is totally determined by all zeros of the filter outside the unit circle , which , for our case , is @xmath399 .",
    "hence , we can interpret the feedback capacity problem as the problem of relocating the zero of the original noise filter @xmath400 to the outside of the unit circle and making the modulus of that zero as large as possible by adding a causal filter @xmath130 using the power latexmath:[$(2\\pi)^{-1 } \\int    filter is given by . under this interpretation ,",
    "the initial input @xmath393 is merely a perturbation which guarantees that the output process is not causally invertible from the innovation process and hence that the entropy rate is fully determined by the spectral density of the stationary part .",
    "( without @xmath393 , the entropy rate of @xmath387 is exactly same as the entropy rate of @xmath402 )    from a classical viewpoint , we can interpret the signal @xmath95 as the adjustment of the receiver s estimate of the message - bearing signal @xmath393 after observing @xmath403 .",
    "we can further check that following signalling schemes are equivalent ( and thus optimal ) up to scaling : @xmath404 & \\propto\\quad x_j - \\hat{x}_j(y^{k-1})\\qquad\\qquad ( j < k)\\\\[.25em ] & \\propto\\quad u_{k-1 } - \\hat{u}_{k-1}(y^{k-1})\\\\[.25em ] & \\propto\\quad \\hat{z}_k(y^{k-1},x^{k-1 } ) - \\hat{z}_k(y^{k-1}).\\end{aligned}\\ ] ] the connection to the schalkwijk - kailath coding scheme is now apparent . recall that there is a simple linear relationship  ( * ? ? ?",
    "* ; * ? ? ?",
    "* section 3.4 )  ( * ? ? ?",
    "* ; * ? ? ?",
    "* section 4.5 ) between the minimum mean square error estimate ( in other words , the minimum variance biased estimate ) for the gaussian input @xmath393 and the maximum likelihood estimate ( or equivalently , the minimum variance unbiased estimate ) for an arbitrary real input @xmath79 .",
    "thus we can easily transform the above coding scheme based on the asymptotic equipartition property  @xcite to a variant of the schalkwijk - kailath linear coding scheme based on the maximum likelihood nearest neighborhood decoding of uniformly spaced @xmath80 points . more specifically",
    ", we send as @xmath393 one of @xmath80 possible signals , say , @xmath405 , where @xmath406 .",
    "subsequent transmissions follow ( [ stat trans ] ) .",
    "the receiver forms the maximum likelihood estimate @xmath407 and finds the nearest signal point to @xmath87 in @xmath408 .",
    "the analysis of the error for this coding scheme follows schalkwijk  @xcite and butman  @xcite . from  ( [ error decay ] ) and the standard result on the relationship between the minimum variance unbiased and biased estimation errors , the maximum likelihood estimation error @xmath409 is , conditioned on @xmath79 , gaussian with mean @xmath79 and variance exponentially decaying with rate @xmath410 .",
    "thus , the nearest neighbor decoding error , ignoring lower order terms , is given by @xmath411 \\doteq \\erfc\\big(\\sqrt{\\frac{3}{2\\sigma_\\theta^2 } } 2^{n(c_\\text{fb } - r)}\\big ) , \\intertext{where } \\erfc(x ) = \\frac{2}{\\sqrt{\\pi } } \\int_{x}^\\infty \\exp(-t^2 ) dt,\\end{gathered}\\ ] ] and @xmath412 is the variance of input signal @xmath79 chosen uniformly over @xmath408 . as far as @xmath413 , the decoding error decays doubly exponentially in @xmath30 .",
    "note that this coding scheme uses only the second moments of the noise process .",
    "this implies that the rate @xmath58 is achievable for the additive noise channel with any non - gaussian noise process with the same covariance matrix .",
    "* asymptotic equivalence of @xmath113 and @xmath357 for feedback capacity *    recall that @xmath414 and @xmath415 . to stress the dependence of the capacity on the power constraint and the noise covariance",
    ", we use the notation @xmath416 for @xmath30-block feedback capacity of the channel with @xmath30-block noise covariance matrix @xmath417 under the power constraint @xmath418 with a little abuse of notation , we similarly use @xmath419 for feedback capacity of the channel with infinite noise covariance matrix naturally extended from @xmath417 under the power constraint @xmath420 .",
    "suppose @xmath421 maximizes @xmath422 and @xmath423 maximizes @xmath424 .",
    "since @xmath425 , we have @xmath426 which shows that @xmath427 is a feasible ( not necessarily optimal ) solution to @xmath428 . on the other hand",
    ", we have @xmath429 so that @xmath430 where ( [ data proc ] ) follows from ( [ noise covs ] ) , divisibility of the gaussian distribution , and the data processing inequality  ( * ? ? ?",
    "* ; * ? ? ?",
    "* section 2.8 ) ; and follows from the optimality of @xmath423 for @xmath428 and the feasibility of @xmath427 for @xmath428 . by letting @xmath30 tend to infinity",
    ", we obtain @xmath431    for the other direction of inequality , we first consider the case @xmath432 .",
    "fix @xmath30 and define the conditional covariance matrix @xmath433 , @xmath434 of @xmath27 conditioned on @xmath435 past values as @xmath436 it is easy to see that under this notation , the ( elementwise ) limit of covariance matrices @xmath433 exists and @xmath437 by sending a length-@xmath435 training sequence over the channel with the noise covariance matrix @xmath113 , i.e. , by transmitting @xmath438 and then estimating the noise process at the receiver using @xmath439 , we can achieve the rate @xmath440 over @xmath441 transmissions .",
    "hence , we have @xmath442 for all @xmath420 . by carefully increasing both @xmath30 and @xmath435 , we will derive the desired inequality .",
    "consider using @xmath423 , which is optimal for the channel with noise covariance matrix @xmath357 , for the channel with noise covariance @xmath433 .",
    "since @xmath443 , the resulting power usage can be greater than @xmath444 .",
    "however , we have @xmath445 now observe that @xmath433 and @xmath446 differ only at the @xmath447 entry .",
    "furthermore , the convergence of @xmath448 to @xmath449 is exponentially fast in @xmath435 ( uniformly in @xmath30 ) .",
    "hence , we can bound the amount of additional power usage as @xmath450 where @xmath451 is a constant independent of @xmath30 and @xmath435 . combining above observations , we have the following chain of inequalities for all @xmath30 and @xmath435 : @xmath452\\\\ \\nonumber & \\ge \\frac{1}{2 } \\left   [ \\log \\det \\left(k_v^ { * * } + ( i + b^ { * * } ) k'_z ( i + b^{**})^t \\right ) -   \\log\\det k_z^{(m ) } \\right]\\\\ \\label{final inequality } & = n c_{n,\\text{fb}}(k'_z , p ) + \\frac{1}{2 } \\left [ \\log \\det k'_z -   \\log\\det k_z^{(m ) } \\right].\\end{aligned}\\ ] ] finally we let @xmath30 and @xmath435 grow to infinity such that @xmath453 the inequality certainly implies that @xmath454 for every @xmath455 .",
    "the desired inequality follows from the continuity of the @xmath456 in @xmath420 .",
    "for the case @xmath121 , we can perturb the noise process using a negligible amount of power and proceeds similarly as above . indeed ,",
    "if we perturb the original covariance matrices @xmath357 and @xmath113 into the perturbed covariance matrices @xmath457 and @xmath458 that correspond to the ma(1 ) process with parameter @xmath459 , we have @xmath460 where follows because we can transform the channel @xmath457 into @xmath357 using very small power , follows from the result for @xmath461 we obtained above , and follows since we can perturb the channel @xmath462 into @xmath458 by adding some extra white noise . since @xmath463 as @xmath464 , @xmath465 and @xmath456 is continuous in @xmath420 , we have @xmath466 this completes the proof of the asymptotic equivalence of @xmath113 and @xmath357 .",
    "1em * optimality of uniform power allocation for the schalkwijk - kailath - butman coding scheme for the ar(1 ) gaussian feedback channel . *",
    "recall that for the ar(1 ) gaussian feedback channel , the best @xmath30-block achievable rate @xmath467 of the schalkwijk - kailath - butman coding scheme , or equivalently , the best achievable rate over all @xmath93 with rank one , is given by @xmath468 where @xmath469 we wish to show that @xmath470 where @xmath8 is the unique positive root of the fourth order polynomial @xmath471    define @xmath472 and @xmath473 it is easy to check the followings :    1 .",
    "@xmath474 is increasing and concave in @xmath475 ; 2 .   for each @xmath476",
    ", @xmath474 is a decreasing contraction of @xmath282 in the sense that @xmath477 for all @xmath478 and @xmath479 ; and consequently , 3 .   for each @xmath283",
    ", there is a unique solution @xmath480 to the equation @xmath325 such that @xmath481 for all @xmath482 and @xmath483 for all @xmath484 .    for an arbitrary infinite sequence @xmath485 with @xmath486 and @xmath487",
    "we define @xmath488 then we can rewrite the recursion formula  as @xmath489 and we have @xmath490    now we show that @xmath491 where @xmath293 is the unique solution to the equation @xmath492 .",
    "indeed , @xmath493 where the first inequality follows from the aforementioned property ( ii ) and the second inequality follows from the property ( i ) and jensen s inequality . by taking limits on both sides , we get from continuity of @xmath474 in @xmath475 @xmath494 which",
    ", from the property ( iii ) , implies that @xmath495 . we can also check that letting @xmath307 for all @xmath496 attains @xmath497 from the property ( ii ) and the principle of contraction mappings  ( * ? ? ?",
    "* ; * ? ? ?",
    "* section 14 ) .",
    "( see figure  [ ar figure ] below and the detailed analysis in ( * ? ? ?",
    "* ; * ? ? ?",
    "* section 5 ) . )",
    "thus , we conclude that the supremum of @xmath498 over all infinite power sequences @xmath331 satisfying the power constraint is achieved by the uniform power allocation . from simple change of variable @xmath499",
    ", we can easily verify @xmath500 where @xmath501 is the unique positive solution to .",
    "[ ar figure ]    as in the ma(1 ) case before , it remains to justify the interchange of the order of limit and supremum in @xmath502 obviously we have @xmath503 for the other direction of inequality , fix @xmath30 and take @xmath504 that achieves @xmath505 .",
    "we construct the infinite sequence @xmath288 by concatenating @xmath506 repeatedly , that is , @xmath507 @xmath508 and @xmath509 for all @xmath510 now we can easily verify that @xmath511 ( taking @xmath509 resets the dependence on the past . ) by taking limits on both sides , we get @xmath512 this completes the proof of the asymptotic optimality of the uniform power allocation .",
    "the author is pleased to express his gratitude to tom cover for his invaluable insights and guidance throughout this work .",
    "he thanks styrmir sigurjnsson and erik ordentlich for enlightening discussions , and sina zahedi for his numerical optimization program , which was especially useful in the initial phase of this study .",
    "he is also grateful to anonymous reviewers for their careful reading of the paper , which resulted in many improvements of the manuscript .",
    "j. c. tiernan and j. p. m. schalkwijk , `` an upper bound to the capacity of the band - limited gaussian autoregressive channel with noiseless feedback , '' _ ieee trans .",
    "theory , _ vol .",
    "it-20 , pp .",
    "311316 , may 1974 ."
  ],
  "abstract_text": [
    "<S> the feedback capacity of the stationary gaussian additive noise channel has been open , except for the case where the noise is white . here </S>",
    "<S> we find the feedback capacity of the stationary first - order moving average additive gaussian noise channel in closed form . </S>",
    "<S> specifically , the channel is given by @xmath0 @xmath1 where the input @xmath2 satisfies a power constraint and the noise @xmath3 is a first - order moving average gaussian process defined by @xmath4 with white gaussian innovations @xmath5 </S>",
    "<S> @xmath6    we show that the feedback capacity of this channel is @xmath7 where @xmath8 is the unique positive root of the equation @xmath9 and @xmath10 is the ratio of the average input power per transmission to the variance of the noise innovation @xmath11 . </S>",
    "<S> the optimal coding scheme parallels the simple linear signalling scheme by schalkwijk and kailath for the additive white gaussian noise channel  the transmitter sends a real - valued information - bearing signal at the beginning of communication and subsequently refines the receiver s error by processing the feedback noise signal through a linear stationary first - order autoregressive filter . </S>",
    "<S> the resulting error probability of the maximum likelihood decoding decays doubly - exponentially in the duration of the communication . </S>",
    "<S> this feedback capacity of the first - order moving average gaussian channel is very similar in form to the best known achievable rate for the first - order _ autoregressive _ gaussian noise channel studied by butman , wolfowitz , and tiernan , although the optimality of the latter is yet to be established .    _ index terms_additive gaussian noise channels , capacity , feedback , feedback capacity , first - order moving average , gaussian feedback capacity , linear signalling . </S>"
  ]
}