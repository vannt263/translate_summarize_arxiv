{
  "article_text": [
    "we start with a few background facts about kernel - based methods .",
    "details can be retrieved from the monographs @xcite and the surveys @xcite .",
    "let @xmath3 be a nonempty set , and let @xmath4 be a positive definite and symmetric kernel on @xmath5 .",
    "associated with @xmath6 there is a unique _ native space _",
    "@xmath7 , that is a separable hilbert space of functions @xmath8 where k is the reproducing kernel .",
    "this means that @xmath9 is the riesz representer of the evaluation functional @xmath10 , i.e. , @xmath11 where we use the notation @xmath12 , without subscript , to denote here and in the following the inner product of @xmath7 .",
    "also the converse holds : any hilbert space on @xmath5 where the evaluation functionals @xmath10 are continuous for any @xmath13 is the native space of a unique kernel .    one way to construct",
    "the native space is as follows .",
    "first one considers the space @xmath14 and then equips it with the positive definite and symmetric bilinear form @xmath15 the native space @xmath7 then is the closure of @xmath16 with respect to the inner product defined by this form",
    ".    given a finite set @xmath17 of distinct points , the interpolant @xmath18 of a function @xmath19 on @xmath20 is uniquely defined , since the _ kernel matrix _",
    "@xmath21_{i , j=1}^n\\ ] ] is positive definite , the kernel being positive definite . letting @xmath22 be the subspace spanned by the kernel translates @xmath23 , @xmath24 , @xmath25 , the interpolant @xmath18 is the projection of @xmath26 into @xmath27 with respect to @xmath28 .    the usual way to study",
    "the approximation error is to consider the _ power function _",
    "@xmath29 of @xmath20 at @xmath30 , that is the @xmath7-norm of the pointwise error functional @xmath31 at @xmath30 , but we will consider also other measurements of the interpolation error , and other projectors .",
    "we make the additional assumptions that @xmath5 is a compact set in @xmath32 and the kernel is continuous on @xmath33 .",
    "this ensures that the native space has a continuous embedding into @xmath34 .",
    "indeed , using the reproducing property we have @xmath35 where the integral of the kernel is finite .",
    "this allows to define a compact and self - adjoint integral operator @xmath36 which will be crucial for our goals . for @xmath37",
    "we define @xmath38 it can be shown that the range @xmath39 of @xmath40 is dense in @xmath7 , and @xmath41    the operator @xmath40 can be defined using any positive and finite measure @xmath42 with full support on @xmath5 ( see @xcite ) and the same properties still hold , but we will concentrate here on the lebesgue measure .    the following theorem",
    "( see e.g. ( * ? ? ?",
    "5 ) ) applies to our situation , and provides a way to represent the kernel as an _ expansion _ ( or _ hilbert - schmidt _ or _ mercer _ ) kernel ( see e.g. @xcite ) .",
    "[ mercer ] if @xmath6 is a continuous and positive definite kernel on a compact set @xmath5 , the operator @xmath40 has a countable set of positive eigenvalues @xmath43 and eigenfunctions @xmath44 with @xmath45 .",
    "the eigenfunctions are orthonormal in @xmath34 and orthogonal in @xmath7 with @xmath46 .",
    "moreover , the kernel can be decomposed as @xmath47 where the sum is absolutely and uniformly convergent .",
    "from now on we will call @xmath48 the _ eigenbasis _ , and use the notation @xmath49 .",
    "we recall that it is also possible to go the other way round and define a positive definite and continuous kernel starting from a given sequence of functions @xmath50 and weights @xmath51 , provided some mild conditions of summability and linear independence .",
    "see @xcite for a detailed discussion about this construction , and note that eigenfunction expansions play a central role in the rbf - qr technique dealt with in various papers @xcite recently .",
    "furthermore , eigenexpansion techniques are a central tool when working with kernels on spheres and riemannian manifolds @xcite .    in this paper we shall study the eigenbasis in detail and compare the eigenspaces to other @xmath2-dimensional subspaces of @xmath7 .",
    "general finite dimensional subspaces and their associated @xmath34- and @xmath7- projectors are treated in section [ sec : pro ] .",
    "in particular , the approximation error is bounded in terms of generalized power functions that turn out to be very useful for the rest of the paper .",
    "the determination of error - optimal @xmath2-dimensional subspaces is the core of section [ sec : minerrsub ] , and is treated there by @xmath2-widths , proving that eigenspaces are optimal under various circumstances .",
    "in addition to the case of kolmogorov @xmath2-width as treated in @xcite , we prove that eigenspaces minimize the @xmath34 norm of the power function .",
    "in section [ sec : restriction ] we move towards numerical calculation of eigenbases by focusing on ( possibly finite - dimensional ) closed subspaces . in particular , we want to use subspaces @xmath27 spanned by kernel translates @xmath52 , @xmath24 , @xmath53 for point sets @xmath20 to calculate approximations to the eigenbasis . by means of power functions",
    "we can bound the error committed by working with finite dimensional subspaces .",
    "section [ sec : asympeig ] focuses on the decay of eigenvalues .",
    "we recall the fact that increased smoothness of the kernel leads to faster decay of eigenvalues .",
    "we prove that using point - based subspaces @xmath27 we can approximate the first @xmath2 eigenvalues with an error connected to the decay of @xmath54 .",
    "section [ sec : alg ] describes the numerical algorithms used for the examples in section [ sec : num ] . in particular",
    ", we use a greedy method for selecting sets @xmath20 of @xmath2 points out of @xmath55 given points such that eigenvalue calculations in @xmath27 are stable and efficient .",
    "finally , section [ sec : num ] shows that our algorithm allows to approximate the eigenvalues for sobolev spaces in a way that recovers the true decay rates , and by results of section [ sec : asympeig ] we have bounds on the committed error . for brownian bridge kernels",
    "the eigenvalues are known , and our algorithm approximates them very satisfactorily .",
    "as mentioned in the introduction , the interpolation problem in @xmath7 is well defined , and the interpolation operator is a @xmath7-projector into the spaces generated by translates of the kernel .",
    "but we want to look also at fully general subspaces @xmath56 of @xmath7 , generated by any set of @xmath2 basis functions , and at other linear approximation processes defined on such spaces , e.g. approximations in the @xmath34 norm .",
    "for instance , we consider the two projectors @xmath57 where the @xmath58 and @xmath59 are @xmath34- and @xmath7- orthonormal basis functions of @xmath56 , respectively .",
    "the first projector is defined on all of @xmath34 and can be analyzed on all intermediate spaces .",
    "we want to see how these projectors are connected .",
    "the two projectors do not coincide in general , but there is a special case . for the sake of clarity we present here the proof of the following lemma , even if it relies on a result that is proven in section [ sec : restriction ] .",
    "[ thevn ] if the projectors coincide on @xmath7 for an @xmath2-dimensional space @xmath56 , then @xmath56 is spanned by @xmath2 eigenfunctions .",
    "the converse also holds .",
    "we start with the converse . for each fixed @xmath60 , thanks to , we have @xmath61 then @xmath62 assume now that the projectors coincide on @xmath7 . we can choose a basis @xmath63 of @xmath56 which is @xmath34-orthonormal and @xmath7-orthogonal ( see lemma [ doubleorth ] ) , with @xmath64 .",
    "since @xmath65 for any @xmath19 , necessarily @xmath66 and in particular for @xmath67 , @xmath13 . consequently , @xmath63 and @xmath68 are eigenfunctions / eigenvalues of @xmath40 .",
    "we are now interested in an error analysis of the approximation by functions from these general subspaces , and we want to allow both of the above projectors .",
    "[ defgenpow ] for a normed linear space @xmath69 of functions on @xmath5 and a linear operator @xmath70 on @xmath69 such that all the functionals @xmath71 are continuous for some norm @xmath72 , the _ generalized power function _ in @xmath13 is the norm of the error functional at @xmath30 , i.e. , @xmath73    the definition fits our situation , because we are free to take @xmath74 with @xmath75 , the normed linear space @xmath69 being @xmath7 .    in the following , when no confusion is possible , we will use the simplified notation @xmath76 or just @xmath77 to denote the power function of @xmath78 with respect to @xmath79 .    to look at the relation between generalized power functions , subspaces and bases we start with the following lemma .",
    "[ lemsumex ] if a separable hilbert space @xmath69 of functions on @xmath5 has continuous point evaluation , then each @xmath69-orthonormal basis @xmath63 satisfies @xmath80 conversely , the above condition ensures that all point evaluation functionals are continuous .",
    "the formula @xmath81 holds in the sense of limits in @xmath69 .",
    "if point evaluations are continuous , we can write @xmath82 in the sense of limits in @xmath83 . since the sequence @xmath84 is in @xmath85 and can be an arbitrary element of that space , the sequence @xmath86 must be in @xmath85 , because the above expression is a continuous linear functional on @xmath85 .    for the converse , observe",
    "that for any @xmath87 , @xmath13 and @xmath88 the term @xmath89 is bounded above by @xmath90 , which is finite for any @xmath13 .",
    "hence , for any @xmath13 , @xmath91 is uniformly bounded for @xmath92 .",
    "[ lemprojpow ] for projectors @xmath93 within separable hilbert spaces @xmath69 of functions on some domain @xmath5 onto finite - dimensional subspaces @xmath56 generated by @xmath69-orthonormal functions @xmath94 that are completed , we have @xmath95 provided that all point evaluation functionals are continuous .",
    "the pointwise error at @xmath30 is @xmath96 and , thanks to the previous lemma , we can safely bound its norm as @xmath97 with equality if @xmath98 .",
    "this framework includes also the usual power function .",
    "[ lempointproj ] let @xmath20 be a set of @xmath2 points in a compact domain @xmath5 , and let @xmath27 be spanned by the @xmath20-translates of @xmath6 .",
    "then the above notion of @xmath99 coincides with the standard notion of the interpolatory power function wrt .",
    "@xmath20 .",
    "the proof follows from the fact that the interpolation operator coincides with the projector @xmath100 and the power function is in both cases defined as the norm of the pointwise error functional .",
    "in this section we present the main results of this paper .",
    "our goal is to analyze the behavior of the approximation error , considered from different points of view , depending on the @xmath2-dimensional subspace @xmath56 . roughly speaking",
    ", we will see that it is possible to exactly characterize the @xmath2-dimensional subspaces of minimal error , both considering the @xmath34 norm of the error and the the @xmath34 norm of the pointwise error .    the way this problem",
    "is addressed in approximation theory is the study of _ widths _ ( see e.g. the comprehensive monograph @xcite , and in particular chapter 4 for the theory in hilbert spaces ) .",
    "we will concentrate first on the @xmath2-width of kolmogorov .",
    "the kolmogorov @xmath2-width @xmath101 of a subset @xmath102 in an hilbert space @xmath69 is defined as @xmath103 it measures how @xmath2-dimensional subspaces of @xmath69 can approximate a given subset @xmath102 .",
    "if the infimum is attained by a subspace , this is called an _ optimal subspace_. the interest is in characterizing optimal subspaces and to compute or estimate the asymptotic behavior of the width , usually letting @xmath102 to be the unit ball @xmath104 of @xmath69 .    the first result that introduces and studies @xmath2-widths for native spaces was presented in @xcite .",
    "the authors consider the @xmath2-width @xmath105 , simply @xmath106 in the following , and prove that @xmath107 and the unique optimal subspace is @xmath108 .",
    "this result is the first that exactly highlights the importance of analyzing the expansion of the operator @xmath40 to better understand the process of approximation in @xmath7 . in the following",
    "we will try to deepen this connection .",
    "our main concern will be to replace the @xmath34 projector @xmath109 by the @xmath7 projector @xmath78 , while still keeping the @xmath34 norm to measure the error .",
    "the @xmath7 projector is closer to the standard interpolation projector , and it differs from the @xmath34 projector unless the space @xmath56 is an eigenspace , see lemma [ thevn ] .",
    "we consider the @xmath34 norm of the error functional in @xmath7 for the projection @xmath78 into a subspace @xmath110 , i.e. , @xmath111 and we look for the subspace which minimizes this quantity . in other words , following the definition of the kolmogorov @xmath2-width , we can define @xmath112    we recall in the next theorem that @xmath113 is equivalent to @xmath106 , i.e. , the best approximation in @xmath34 of @xmath114 with respect to @xmath115 can be achieved using @xmath7 itself and the projector @xmath78 .",
    "the result can be found in @xcite .",
    "[ thekolwidth ] for any @xmath116 we have @xmath117 and the unique optimal subspace is @xmath108 .",
    "since @xmath118 and since @xmath119 is the best approximation from @xmath56 of @xmath19 wrt .",
    "@xmath115 , we have @xmath120 on the other hand , since @xmath121 on @xmath7 ( lemma [ thevn ] ) , @xmath122 since @xmath108 is optimal for @xmath106 .",
    "hence @xmath123 .",
    "we now move to another way of studying the approximation error . instead of directly considering the @xmath34 norm of approximants ,",
    "we first take the norm of the pointwise error of the @xmath124 projector and then minimize its @xmath34 norm over @xmath5 .",
    "this means to find a subspace which minimizes the @xmath34 norm @xmath125 of the power function @xmath77 among all @xmath2-dimensional subspaces @xmath110 . using the definition of the generalized power function",
    ", we can rephrase the problem in the fashion of the previous results by defining @xmath126    in the following theorem , mimicking ( * ? ? ?",
    "* theorem 1 ) , we prove that , also in this case , the optimal @xmath2-dimensional subset is @xmath108 , and @xmath127 can be expressed in terms of the eigenvalues .",
    "[ l2minpow ] for any @xmath116 we have @xmath128 and the unique optimal subspace is @xmath108 .    for a subset @xmath56",
    "we can consider a @xmath7-orthonormal basis @xmath129 and complete it to an orthonormal basis @xmath130 of @xmath7 .",
    "we can move from the eigenbasis to this basis using a matrix @xmath131 as @xmath132 where @xmath133 .",
    "hence , the power function of @xmath56 is @xmath134 and , defining @xmath135 , we can compute its norm as @xmath136 now we need to prove that @xmath137 .",
    "let @xmath138 .",
    "we split the cumulative sum over the @xmath139 into integer ranges @xmath140 then @xmath141 can be infinite , but @xmath142 is finite , and since @xmath143 we get stepwise @xmath144 for @xmath145 , using @xmath146 . since the sequence of the eigenvalues is non negative and non increasing , this implies @xmath147    if we take @xmath148 and @xmath149 as its basis , the matrix @xmath102 in is the infinite identity matrix .",
    "thus equality holds in the last inequality .",
    "the previous results motivate the interest for the knowledge and study of the eigenbasis . but from a practical point of view there is some limitation : the eigenbasis can not be computed in general , and it can not be used for truly scattered data approximation , since there exists at least a set @xmath150 such that the collocation matrix of @xmath108 on @xmath20 is singular ( by the mairhuber - curtis theorem , see e.g. ( * ? ? ?",
    "* theorem 2.3 ) ) .    to overcome this problem we consider instead subspaces of @xmath7 of the form @xmath151 , where @xmath152 is a possibly large but finite set of points in @xmath5 .",
    "the basic idea is to replace @xmath7 by @xmath153 in order to get a good numerical approximation to the true eigenbasis with respect to @xmath7 .    to this end , we repeat the constructions of the previous section for a finite - dimensional native space , i.e. , the problem of finding , for @xmath154 , an @xmath2-dimensional subset @xmath56 which minimizes the error , in some norm , among all the subspaces of @xmath153 of dimension @xmath2 , and that can now be exactly computed .",
    "one could expect that the optimal subset for this restricted problem is the projection of @xmath108 into @xmath27 .",
    "in fact , as we will see , this is not the case , but the optimal subspace will still approximate the true eigenspaces in a near - optimal sense .    the analysis of such point - based subspaces can be carried out by looking at general closed subspaces of the native space . it can be proven ( see ( * ? ? ?",
    "1 ) ) that , if @xmath155 is a closed subspace of @xmath7 , it is the native space on @xmath5 of the kernel @xmath156 , with inner product given by the restriction of the one of @xmath7 . the restricted operator @xmath157 defined as @xmath158 maps @xmath34 into @xmath155 .",
    "then theorem [ mercer ] applied to this operator gives the eigenbasis for @xmath155 and @xmath159 on @xmath5 and the corresponding eigenvalues , which will be denoted as @xmath160 , @xmath161 .",
    "they can be finitely or infinitely many , depending on the dimension of @xmath155 .",
    "we will use the notation @xmath162 if @xmath163 .",
    "this immediately proves the following lemma that was already used in section [ sec : pro ] .",
    "[ doubleorth ] any closed subspace @xmath155 of the native space has a unique basis which is @xmath7-orthogonal and @xmath34-orthonormal .",
    "uniqueness is understood here like stating uniqueness of the eigenvalue expansion of the integral operator defined by the kernel , i.e. , the eigenspaces are unique .    before analyzing the relation between the approximation in @xmath155 and in @xmath7",
    "we establish a connection between the corresponding kernels and power functions .    if @xmath164 is closed , @xmath165 moreover , if @xmath166 are closed , the power functions are related as @xmath167    the power function is the norm of the error functional . for @xmath19 , @xmath168 , and @xmath13",
    "we have @xmath169 with equality if @xmath26 is the normalized difference of the kernels .",
    "this proves , and the relation between the power functions easily follows .",
    "since @xmath155 is a native space itself , the results of the previous section hold also for @xmath155 .",
    "we can then define in @xmath155 the analogous notions of @xmath106 , @xmath113 and @xmath127 , and by theorems [ thekolwidth ] , [ l2minpow ] we know that they are all minimized by @xmath170 , with values @xmath171 , @xmath171 , and @xmath172 , respectively .",
    "these results deal with the best approximation of the unit ball @xmath173 , but allow also to face the problem of the constrained optimization in the case of @xmath127 , i.e. , the minimization of the error of approximation of @xmath114 using only subspaces of @xmath155 .",
    "indeed , thanks to lemma [ powandkers ] , we know that for any @xmath174 and for any @xmath13 , the squared power functions of @xmath7 and of @xmath155 differ by an additive constant .",
    "this means that the minimality of @xmath170 does not change if we consider the standard power function on @xmath7 .",
    "moreover , @xmath175    this proves the following corollary of theorem [ l2minpow ] .",
    "[ l2minvm ] let @xmath164 be a closed subspace of @xmath7 , and let @xmath176 .",
    "for any @xmath2-dimensional subspace @xmath177 we have @xmath178 and @xmath170 is the unique optimal subspace .",
    "in particular @xmath179    this corollary has two consequences .",
    "at one hand , if we want to have a small power function , we need to choose a subspace @xmath155 which provides a good approximation of the true eigenvalues . on the other hand , when dealing with certain point based subspaces , we can control the decay of the power function depending on the number of points we are using , and this bound will provide a bound also on the convergence of the discrete eigenvalues to the true one . the last fact will be discussed in more detail in section [ sec : asympeig ] .",
    "we remark that there is a relation between @xmath108 and @xmath170 : as mentioned before , the optimal subspace @xmath170 is not the projection of @xmath108 into @xmath155 , but is near to be its optimal approximation from @xmath155 . to see this ,",
    "observe that the operator @xmath159 is the projection of @xmath40 into @xmath155 .",
    "in fact , given @xmath180 , we have for any @xmath37 @xmath181 this means that the couples @xmath182 are precisely the bubnov - galerkin approximations ( see e.g. ( * ? ? ?",
    "18.4 ) ) of the solutions @xmath183 of the eigenvalue problem for the restricted operator @xmath184 ( which is still a compact , positive and self - adjoint operator ) .",
    "we can then use the well known estimates on the convergence of the bubnov - galerkin method to express the distance between @xmath108 and @xmath170 .",
    "the following proposition collects convergence results which follow from ( * ? ? ?",
    "* th . 18.5 , th .",
    "18.6 ) .",
    "[ prop : conv ] let @xmath185 be a sequence of closed subspaces which become dense in @xmath7 . for @xmath186 we have    a.   @xmath187 , b.   let @xmath188 be the multiplicity of @xmath189 and + @xmath190 . for @xmath2",
    "sufficiently large , @xmath191 and there exists @xmath192 s.t .",
    "@xmath193    equation proves in particular that @xmath170 is an asymptotically optimal approximation of @xmath108 .",
    "indeed , under the assumptions of the last proposition , we have @xmath194 with @xmath195 as @xmath196 .    to conclude this section we point out that , in addition to the point based sets , there is another remarkable way to produce closed subspaces of the native space .",
    "namely , if @xmath197 is any lipschitz subdomain of @xmath5 , @xmath198 is a closed subset of @xmath7 .",
    "this implies that the eigenvalues are decreasing with respect to the inclusion of the base domain ( as can by proven also by the min / max characterization of the eigenvalues ) .",
    "we established a relation between the approximation error and the eigenvalues of @xmath40 .",
    "this allows to use the well known bounds on the approximation error to give corresponding bounds on the decay of the eigenvalues .",
    "these results were already presented in @xcite , but we include them here for completeness and add some extensions .",
    "we consider a set of asymptotically uniformly distributed points @xmath20 , such that the fill distance @xmath199 behaves like @xmath200 where @xmath201 is independent of @xmath2 .",
    "if the kernel is translational invariant and fourier transformable on @xmath32 and @xmath5 is bounded and with a smooth enough boundary , there are standard error estimates for the error between @xmath19 and its interpolant @xmath18 on the points @xmath20 ( see @xcite ) .    for kernels @xmath202 with finite smoothness , we have that @xmath203 for @xmath204 , and @xmath205 while for infinitely smooth kernels we have @xmath206 both bounds are in fact bounds on the @xmath207 norm of the power function . if instead one considers directly the @xmath34 error , for kernels with finite smoothness the estimate can be improved as follows :",
    "@xmath208    this immediately leads to the following theorem .",
    "[ theeigdecay ] under the above assumptions on @xmath6 and @xmath5 , the eigenvalues decay at least like @xmath209 for a kernel with smoothness @xmath210 , and at least like @xmath211 for kernels with unlimited smoothness .",
    "the constants @xmath212 are independent of @xmath2 , but dependent on @xmath6 , @xmath5 , and the space dimension .",
    "it is important to notice that the asymptotics of the eigenvalues is known for the kernels of limited smoothness , and on @xmath32 .",
    "if the kernel is of order @xmath210 , its native space on @xmath32 is norm equivalent to the sobolev space @xmath213 . in these spaces",
    "the @xmath2-width , and hence the eigenvalues , decay like @xmath214 ( see @xcite ) .",
    "this means that in sobolev spaces one can recover ( asymptotically ) the best order of approximation using kernel spaces .    this can be done also with point based spaces .",
    "the following statement follows from corollary [ l2minvm ] , applying the same ideas as before .",
    "observe that , in this case , we need to consider the bound .",
    "[ theeigapprox ] under the above assumptions on @xmath6 and @xmath5 , we have @xmath215 for a kernel with smoothness @xmath210 , and at least like @xmath216 for kernels with unlimited smoothness . the constants @xmath212 are independent of @xmath2 , but dependent on @xmath6 , @xmath5 , and the space dimension .",
    "this corollary and the previous theorem proves that , using point based sets with properly chosen points , one can achieve at the same time a fast decay of the true eigenvalues and a fast convergence of the discrete ones .    both results in this section raise some questions about the converse implication . from theorem [ theeigdecay ]",
    "we know that the smoothness of the kernel guarantees a fast decay of the eigenvalues .",
    "but we can also start from a given expansion to construct a kernel .",
    "is it possible to conclude smoothness of the kernel from fast decay of the eigenvalues ?",
    "corollary [ theeigapprox ] tells us that uniformly distributed point based sets provide a good approximation of the eigenvalues .",
    "we will see in section [ sec : alg ] and [ sec : num ] that one can numerically construct point based sets whose eigenvalues are close to the true ones .",
    "is it possible to prove that these sets are necessarily asymptotically uniformly distributed ?",
    "if we consider a closed subspace @xmath217 spanned by translates of the kernel on a set @xmath218 of @xmath55 points in @xmath5 , it is possible to explicitly construct @xmath219 , @xmath220 .",
    "the number @xmath55 of points should be large enough to simulate @xmath34 inner products by discrete formulas .",
    "the first method we present aims at a direct construction of the eigenspaces and gives some insight on the structure of the basis , but it is numerically not convenient since it involves the computation of two gramians . for stability and computability reasons , we shall then focus on lower - dimensional subspaces of @xmath153 .",
    "there are various ways to construct these , and we present two .",
    "we will see in section [ sec : num ] that these approximation are very effective .    from now on we will use the subscript @xmath55 in place of @xmath153 to simplify the notation , keeping in mind that there is an underlying set of points @xmath221 and the corresponding subspace @xmath153 .",
    "we use at first the fact that the eigenbasis is the unique set of @xmath55 functions in @xmath153 which is orthonormal in @xmath34 and orthogonal in @xmath7 , where uniqueness is understood in the sense of uniqueness of the eigendecomposition of the integral operator .",
    "given any couple of inner products @xmath222 and @xmath223 on @xmath153 , it is always possible to build a basis @xmath224 of @xmath153 which is @xmath225-ortho_normal _ and @xmath226-ortho_gonal _ with norms @xmath227 .",
    "let @xmath228_{i , j=1}^n,\\\\ b = & [ ( k(\\cdot , x_i),k(\\cdot , x_j))_b]_{i , j=1}^n\\end{aligned}\\ ] ] be the gramians with respect to the two inner products of the standard basis @xmath52 , @xmath24 , @xmath229 of @xmath6 translates .",
    "following the notation of @xcite , to construct the basis we need to construct an invertible matrix @xmath230 of change of basis to express this new basis with respect to the standard basis . to have the right orthogonality , we need @xmath231 and @xmath232 , where @xmath233 is the diagonal matrix having on the diagonal the @xmath226-norms of the new basis .",
    "this means to simultaneously diagonalize the two gramians , and since they are symmetric and positive definite this is always possible , e.g. in the following way :    * @xmath234 be a cholesky decomposition , * define @xmath235 ( which is symmetric and positive definite ) , * let @xmath236 be a svd decomposition , * define @xmath237 .",
    "observe that , for practical use , it is more convenient to swap the role of @xmath102 and @xmath238 . in this way",
    "we construct the basis @xmath239 , which is @xmath7-orthonormal hence more suitable for approximation purposes , and , moreover , we obtain directly the eigenvalues of order @xmath55 as @xmath240 .    in both ways",
    "we are just computing a generalized diagonalization of the pencil @xmath241 , up to a proper scaling of the diagonals . in our case",
    "@xmath102 is the usual kernel matrix , while @xmath242 .",
    "thus , provided we know @xmath238 , we can explicitly construct the basis .",
    "this can be done , for a general kernel , using a large set of points to approximate the @xmath34 inner product .",
    "numerical experiments ( see section [ sec : num ] ) suggest that this construction of the eigenspaces is highly unstable also in simple cases , since it requires to solve an high dimensional matrix eigenvalue problem .",
    "another way to face the problem consists of greedy procedures as considered next .      instead of directly constructing the subspace @xmath243 via @xmath244 matrix eigenvalue problems as described before",
    ", we can first select @xmath2 points in @xmath218 such that working on these points is more stable than working with the full original matrix , and then solve the problem in @xmath27 .",
    "the selection of the set @xmath20 is performed with a greedy construction of the newton basis ( see @xcite ) .",
    "first we show how to construct the eigenspaces via the newton basis .",
    "assume that @xmath94 is the newton basis for @xmath27",
    ". then @xmath245 and if @xmath246 is an eigenvalue with eigenfunction @xmath247 then @xmath248 implies @xmath249 thus the coefficients of the eigenbasis with respect to the newton basis are the eigenvectors of the @xmath34 gramian of the newton basis .",
    "experimentally , the newton basis is nearly @xmath34 orthogonal .",
    "thus the above procedure should have a nice gramian matrix , provided that the @xmath34 inner products that are near zero can be calculated without loss of accuracy .    to select the points we use two similar greedy strategies , based on maximization of @xmath207 and @xmath34 norms of the power function .",
    "the first point is chosen as @xmath250 denoting by @xmath251 the already chosen points , the @xmath252-th point is selected as @xmath253 the point sets selected by the two strategies are different , but we will see in the next section that they provide similar results in terms of approximation of the eigenspaces .",
    "in these spaces the asymptotic behavior of the kolmogorov width , hence of the eigenvalues , is known as recalled in section [ sec : asympeig ] .",
    "we assume here that the same bounds hold in a bounded domain ( the unit disk ) , and we want to compare it with the discrete eigevalues of point based sets , which can be computed with one of the methods of the previous section .    to this aim , we start from a grid of equally spaced points in @xmath256 ^ 2 $ ] restricted to the unit disk , so that the number of points inside the disk is @xmath257 .",
    "we use this grid both for point selection and to approximate the @xmath34 inner products as weighted pointwise products , i.e. , @xmath258 we select @xmath259 points by the greedy @xmath207 maximization of the power function in the unit disk .",
    "the results are shown in figure [ fig : matorder ] . as expected , for any order under consideration there",
    "exist a positive constant @xmath201 such that the discrete eigenvalues decay with the same rate of the sobolev best approximation .",
    "moreover , we expect that the discrete eigenvalues converge to the true ones with a rate that can also be controlled by @xmath210 . indeed , according to corollary [ theeigapprox ]",
    ", we have @xmath260 to verify this , we instead look at the decay of @xmath261 since we can exactly compute the first term .",
    "indeed , since the kernels are radial , we have @xmath262 results are presented in figure [ fig : matdiff ] . from the experiments it seems that we can improve the convergence speed somewhat , and in fact obtain a rate of order @xmath263 instead of @xmath264 .",
    "this may be another instance of the `` gap of @xmath265 '' already observed in @xcite . sometimes , observed convergence rates are by @xmath265 better than proven ones .",
    "w.  pogorzelski . .",
    "translated from the polish by jacques j. schorr - con , a. kacner and z. olesiak . international series of monographs in pure and applied mathematics , vol .",
    "pergamon press , oxford , 1966 .",
    "r.  schaback .",
    "native hilbert spaces for radial basis functions i. in m.  buhmann , d.  h. mache , m.  felten , and m.  mller , editors , _ new developments in approximation theory _ ,",
    "number 132 in international series of numerical mathematics , pages 255282 .",
    "birkhuser verlag , 1999 .",
    "r.  schaback and h.  wendland .",
    "approximation by positive definite kernels . in m.",
    "buhmann and d.  mache , editors , _ advanced problems in constructive approximation _ ,",
    "volume 142 of _ international series in numerical mathematics _ ,",
    "pages 203221 , 2002 ."
  ],
  "abstract_text": [
    "<S> kernel - based methods in numerical analysis have the advantage of yielding optimal recovery processes in the  native  hilbert space @xmath0 in which they are reproducing . continuous kernels on compact domains have an expansion into eigenfunctions that are both @xmath1-orthonormal and orthogonal in @xmath0 ( mercer expansion ) . </S>",
    "<S> this paper examines the corresponding eigenspaces and proves that they have optimality properties among all other subspaces of @xmath0 . </S>",
    "<S> these results have strong connections to @xmath2-widths in approximation theory , and they establish that errors of optimal approximations are closely related to the decay of the eigenvalues .    </S>",
    "<S> though the eigenspaces and eigenvalues are not readily available , they can be well approximated using the standard @xmath2-dimensional subspaces spanned by translates of the kernel with respect to @xmath2 nodes or centers . </S>",
    "<S> we give error bounds for the numerical approximation of the eigensystem via such subspaces . </S>",
    "<S> a series of examples shows that our numerical technique via a greedy point selection strategy allows to calculate the eigensystems with good accuracy .    * _ keywords : _ * mercer kernels , radial basis functions , eigenfunctions , eigenvalues , @xmath2-widths , optimal subspaces , greedy methods + * _ 2000 msc : _ * 41axx , 41a46 , 41a58 , 42cxx , 42c15 , 42a82 , 45p05 , 46e22 , 47a70 , 65d05 , 65f15 , 65r10 , 65t99 </S>"
  ]
}