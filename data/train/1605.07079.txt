{
  "article_text": [
    "the performance of many machine learning algorithms hinges on certain hyperparameters . for example , the prediction error of non - linear support vector machines depends on regularization and kernel hyperparameters @xmath0 and @xmath1 ; and modern neural networks are sensitive to a wide range of hyperparameters , including learning rates , momentum terms , number of units per layer , dropout rates , weight decay , etc .",
    "the poor scaling of nave methods like grid search with dimensionality has driven interest in more sophisticated hyperparameter optimization methods over the past years  @xcite .",
    "_ bayesian optimization _ has emerged as an efficient framework , achieving impressive successes .",
    "for example , in several studies , it found better instantiations of convolutional network hyperparameters than domain experts , repeatedly improving the top score on the cifar-10  @xcite benchmark without data augmentation  @xcite .    in the traditional setting of bayesian hyperparameter optimization ,",
    "the loss of a machine learning algorithm with hyperparameters @xmath2 is treated as the `` black - box '' problem of finding @xmath3 , where the only mode of interaction with the objective @xmath4 is to evaluate it for inputs @xmath2 .",
    "if individual evaluations of @xmath4 on the entire dataset require days or weeks , only very few evaluations are possible , limiting the quality of the best found value .",
    "human experts instead often study performance on subsets of the data first , to become familiar with its characteristics before gradually increasing the subset size  @xcite .",
    "this approach can still outperform contemporary bayesian optimization methods .",
    "motivated by the experts strategy , here we leverage dataset size as an additional degree of freedom enriching the representation of the optimization problem .",
    "we treat the size of a randomly subsampled dataset @xmath5 as an additional input to the blackbox function , and allow the optimizer to actively choose it at each function evaluation .",
    "this allows bayesian optimization to mimic and improve upon human experts when exploring the hyperparameter space . in the end , @xmath5 is not a hyperparameter itself , but the goal remains a good performance on the full dataset , i.e.  @xmath6 .",
    "hyperparameter optimization for large datasets has been explored by other authors before .",
    "our approach is similar to multi - task bayesian optimization by @xcite , where knowledge is transferred between a finite number of correlated tasks .",
    "if these tasks represent manually - chosen subset - sizes , this method also tries to find the best configuration for the full dataset by evaluating smaller , cheaper subsets .",
    "however , the discrete nature of tasks in that approach requires evaluations on the entire dataset to learn the necessary correlations .",
    "instead , our approach exploits the regularity of performance across dataset size , enabling generalization to the full dataset without evaluating it directly .",
    "other approaches for hyperparameter optimization on large datasets include work by @xcite , who estimated a configuration s performance on a large dataset by evaluating several training runs on small , random subsets of fixed , manually - chosen sizes .",
    "@xcite showed that , in practical applications , small subsets can suffice to estimate a configuration s quality , and proposed a cross - validation scheme that sequentially tests a fixed set of configurations on a growing subset of the data , discarding poorly - performing configurations early .    in parallel work",
    ", @xcite proposed a multi - arm bandit strategy , called hyperband , which dynamically allocates more and more resources to randomly sampled configurations based on their performance on subsets of the data .",
    "hyperband assures that only well - performing configurations are trained on the full dataset while discarding bad ones early . despite its simplicity , in their experiments",
    "the method was able to outperform well - established bayesian optimization algorithms .",
    "in [ sec : bo ] , we review bayesian optimization , in particular the entropy search algorithm and the related method of multi - task bayesian optimization . in [ sec : fabolas ] , we introduce our new bayesian optimization method for hyperparameter optimization on large datasets . in each iteration , chooses the configuration @xmath7 and dataset size @xmath5 predicted to yield most information about the loss - minimizing configuration on the full dataset per _ unit time spent_. in [ sec : experiments ] , a broad range of experiments with support vector machines and various deep neural networks show often identifies good hyperparameter settings 10 to 100 times faster than state - of - the - art bayesian optimization methods acting on the full dataset as well as hyperband .",
    "given a black - box function @xmath8 , bayesian optimization aims to find an input @xmath9 that globally minimizes @xmath4 .",
    "it requires a prior @xmath10 over the function and an acquisition function @xmath11 quantifying the _ utility _ of an evaluation at any @xmath12 . with these ingredients",
    ", the following three steps are iterated @xcite : ( 1 )  find the most promising by numerical optimization ; ( 2 )  evaluate the expensive and often noisy function @xmath13 and add the resulting data point @xmath14 to the set of observations @xmath15 ; and ( 3 )  update @xmath16 and @xmath17 .",
    "typically , evaluations of the acquisition function @xmath18 are cheap compared to evaluations of @xmath4 such that the optimization effort is negligible .",
    "gaussian processes ( gp ) are a prominent choice for @xmath10 , thanks to their descriptive power and analytic tractability ( e.g. * ? ? ?",
    "formally , a gp is a collection of random variables , such that every finite subset of them follows a multivariate normal distribution .",
    "a gp is identified by a mean function @xmath19 ( often set to @xmath20 ) , and a positive definite covariance function ( kernel ) @xmath21 .",
    "given observations @xmath22 with joint gaussian likelihood @xmath23 , the posterior @xmath24 follows another gp , with mean and covariance functions of tractable , analytic form .",
    "the covariance function determines how observations influence the prediction . for the hyperparameters we wish to optimize , we adopt the matrn @xmath25 kernel @xcite , in its automatic relevance determination form @xcite .",
    "this stationary , twice - differentiable model constitutes a relatively standard choice in the bayesian optimization literature .",
    "in contrast to the gaussian kernel popular elsewhere , it makes less restrictive smoothness assumptions , which can be helpful in the optimization setting @xcite : @xmath26 here , @xmath27 and @xmath28 are free parameters",
    " hyperparameters of the gp surrogate model  and @xmath29 is the mahalanobis distance . for the dataset size dependent performance and cost",
    ", we construct a custom kernel in [ sec : kernels ] .",
    "an additional hyperparameter of the gp model is a overall noise covariance needed to handle noisy observations . for clarity",
    ": these gp hyperparameters are _ internal _ hyperparameters of the bayesian optimizer , as opposed to those of the target machine learning algorithm to be tuned .",
    "section [ sec : implementation ] shows how we handle them .",
    "the role of the acquisition function is to trade off exploration vs.  exploitation .",
    "popular choices include expected improvement ( ei ) @xcite , upper confidence bound ( ucb )  @xcite , entropy search ( es )  @xcite , and predictive entropy search ( pes )  @xcite . in our experiments , we will use ei and es",
    ".    we found ei to perform robustly in most applications , providing a solid baseline ; it is defined as @xmath30\\ , .",
    "\\ ] ] where @xmath31 is the best function value known ( also called the _ incumbent _ ) . this expected drop over the best known value is high for points predicted to have small mean and/or large variance .",
    "es is a more recent acquisition function that selects evaluation points based on the predicted _ information gain _ about the optimum , rather than aiming to evaluate near the optimum . at the heart of es",
    "lies the probability distribution @xmath32 , the belief about the function s minimum given the prior on @xmath4 and observations @xmath33 .",
    "information gain _ at @xmath12",
    "is then measured by the expected kullback - leibler divergence ( relative entropy ) between @xmath34 and the uniform distribution @xmath35 , with expectations taken over the measurement @xmath36 to be obtained at @xmath12 : @xmath37 .",
    "\\end{split}\\ ] ] the primary numerical challenge in this framework is the computation of @xmath34 and the integral above . due to the intractability",
    ", several approximations have to be made .",
    "we refer to @xcite for details , as well as to the supplemental material ( section a ) , where we also provide pseudocode for our implementation . despite the conceptual and computational complexity of es ,",
    "it offers a well - defined concept for information gained from function evaluations , which can be meaningfully traded off against other quantities , such as the evaluations cost .",
    "pes refers to the same acquisition function , but uses different approximations to compute it . in section [ sec : implementation ] we describe why , for our application , es was the more direct choice .",
    "the _ multi - task _ bayesian optimization ( mtbo ) method of @xcite refers to a general framework for optimizing in the presents of different , but correlated tasks .",
    "given a set of such tasks @xmath38 , the objective function @xmath39 corresponds to evaluating a given @xmath2 on one of the tasks @xmath40 .",
    "the relation between points in @xmath41 is modeled via a gp using a product kernel : @xmath42 the kernel @xmath43 is represented implicitly by the cholesky decomposition of @xmath44 whose entries are sampled via mcmc together with the other hyperparameters of the gp . by considering the distribution over the optimum on the target task @xmath45 , @xmath46 , and computing any information w.r.t .  it ,",
    "@xcite use the information gain per unit cost as their acquisition function and @xmath47 .",
    "they stated this to work better in practice , but we did not find evidence for this in our experiments and thus , for consistency , use the variant presented here throughout . ] : @xmath48\\,,\\end{aligned}\\ ] ] where @xmath49 .",
    "the expectation represents the information gain on the target task averaged over the possible outcomes of @xmath50 based on the current model .",
    "if the cost @xmath51 of a configuration @xmath12 on task @xmath52 is not known a priori it can be modelled the same way as the objective function .",
    "this model supports machine learning hyperparameter optimization for large datasets by using discrete dataset sizes as tasks .",
    "@xcite indeed studied this approach for the special case of @xmath53 , representing a small and a large dataset ; this will be a baseline in our experiments .",
    "here , we introduce our new approach for fast bayesian optimization on large data sets ( ) . while traditional bayesian hyperparameter optimizers model the loss of machine learning algorithms on a given dataset as a blackbox function @xmath4 to be minimized , models loss and computational cost _ across dataset size _ and uses these models to carry out bayesian optimization with an extra degree of freedom .",
    "the blackbox function @xmath54 now takes another input representing the data subset size ; we will use relative sizes @xmath55 $ ] , with @xmath56 representing the entire dataset . while the eventual goal is to minimize the loss @xmath57 for the entire dataset , evaluating @xmath4 for smaller",
    "@xmath58 is usually cheaper , and the function values obtained correlate across @xmath58 .",
    "unfortunately , this correlation structure is initially unknown , so the challenge is to design a strategy that trades off the cost of function evaluations against the benefit of learning about the scaling behavior of @xmath4 and , ultimately , about which configurations work best on the full dataset . following the nomenclature of @xcite , we call @xmath59 $ ] an _ environmental variable _ that can be changed freely _ during _ optimization , but that is set to @xmath56 ( i.e. , the entire dataset size ) , at evaluation time .    we propose a principled rule for the automatic selection of the next @xmath60 pair to evaluate . in a nutshell , where standard bayesian optimization would always run configurations on the full dataset",
    ", we use es to reason about , how much can be learned about performance on the full dataset from an evaluation at any @xmath58 . in doing so , automatically determines the amount of data necessary to ( usefully ) extrapolate to the full dataset .                    for an initial intuition on how performance changes with dataset size",
    ", we evaluated a grid of @xmath61 configurations of a support vector machine ( svm ) on subsets of the mnist dataset  @xcite ; mnist has @xmath62 data points and we evaluated relative subset sizes @xmath63 .",
    "figure [ fig : grid ] visualizes the validation error of these configurations on @xmath64 , @xmath65 , @xmath66 , and @xmath67 .",
    "evidently , just @xmath68 of the dataset is quite representative and sufficient to locate a reasonable configuration . additionally , there are no deceiving local optima on smaller subsets . based on these observations ,",
    "we expect that relatively small fractions of the dataset yield representative performances and therefore vary our relative size parameter @xmath58 on a logarithmic scale .      to transfer the insights from this illustrative example into a formal model for the loss and cost across subset sizes , we extend the gp model by an additional input dimension , namely @xmath69 $ ] .",
    "this allows the surrogate to extrapolate to the full data set at @xmath56 without necessarily evaluating there .",
    "we chose a factorized kernel , consisting of the standard stationary kernel over hyperparameters , multiplied with a finite - rank ( `` degenerate '' ) covariance function in @xmath58 : @xmath70 since any choice of the basis function @xmath71 yields a positive semi - definite covariance function , this provides a flexible language for prior knowledge relating to @xmath58 .",
    "we use the same form of kernel to model the loss @xmath4 and cost @xmath72 , respectively , but with different basis functions @xmath73 and @xmath74 .",
    "the loss of a machine learning algorithms usually decreases with more training data .",
    "we incorporate this behavior by choosing @xmath75 to enforce monotonic predictions with an extremum at @xmath56 .",
    "this kernel choice is equivalent to bayesian linear regression with these basis functions and gaussian priors on the weights .",
    "to model computational cost @xmath72 , we note that the complexity usually grows with relative dataset size @xmath58 .",
    "to fit polynomial complexity @xmath76 for arbitrary @xmath77 and simultaneously enforce positive predictions , we model the log - cost and use @xmath78 . as above , this amounts to bayesian linear regression with shown basis functions .    in the supplemental material ( section b ) , we visualize scaling of loss and cost with @xmath58 for the svm example above and show that our kernels indeed fit them well .",
    "we also evaluate the possibility of modelling the heteroscedastic noise introduced by subsampling the data ( supplementary material , section c )",
    ".      starts with an initial design , described in more detail in section [ sec : initial_design ] .",
    "afterwards , at the beginning of each iteration it fits gps for loss and computational cost across dataset sizes @xmath58 using the kernel from eq .",
    "[ eq : our_kernel ] . then , capturing the distribution of the optimum for @xmath56 using @xmath79 , it selects the maximizer of the following acquisition function to trade off information gain versus cost : @xmath80.\\end{aligned}\\ ] ] algorithm [ alg : fabolas ] shows pseudocode for .",
    "we also provide an open - source implementation at https://github.com/automl/robo .",
    "initialize data @xmath81 using an initial design.[line : initial_design ] fit gp models for @xmath82 and @xmath83 on data @xmath84[line : fit_models ] choose @xmath85 by maximizing the acquisition function in equation [ eq : fabolas_acq].[line : opt_acq ] evaluate @xmath86 , also measuring cost @xmath87 , and augment the data : @xmath88 choose incumbent @xmath89 based on the predicted loss at @xmath56 of all @xmath90.[line : incumbent ]    our proposed acquisition function resembles the one used by mtbo ( eq .  [ eq : mtbo_acq ] ) , with two differences : first , mtbo s discrete tasks @xmath52 are replaced by a continuous dataset size @xmath58 ( allowing to learn correlations without evaluations at @xmath56 , and to choose the appropriate subset size automatically ) .",
    "second , the prediction of computational cost is augmented by the overhead of the bayesian optimization method .",
    "this inclusion of the reasoning overhead is important to appropriately reflect the information gain per unit time spent : it does not matter whether the time is spent with a function evaluation or with reasoning about which evaluation to perform . in practice , due to cubic scaling in the number of data points of gps and the computational complexity of approximating @xmath91 , the additional overhead of is within the order of minutes , such that differences in computational cost in the order of seconds become negligible in comparison .    being an anytime algorithm , keeps track of its incumbent at each time step . to select a configuration that performs well on the full dataset ,",
    "it predicts the loss of all evaluated configurations at @xmath56 using the gp model and picks the minimizer .",
    "we found this to work more robustly than globally minimizing the posterior mean , or similar approaches .",
    "it is common in bayesian optimization to start with an initial design of points chosen at random or from a latin hypercube design to allow for reasonable gp models as starting points . to fully leverage the speedups we can obtain from evaluating small datasets , we bias this selection towards points with small ( cheap ) datasets in order to improve the prediction for dependencies on @xmath58 : we draw @xmath21 random points in @xmath92 ( @xmath93 in our experiments ) and evaluate them on different subsets of the data ( for instance on the support vector machine experiments we used @xmath94 ) .",
    "this provides information on scaling behavior , and , assuming that costs increase linearly or superlinearly with @xmath58 , these @xmath21 function evaluations cost less than @xmath95 function evaluations on the full dataset .",
    "this is important as the cost of the initial design , of course , counts towards  runtime .",
    "the presentation of above omits some details that impact the performance of our method .",
    "as it has become standard in bayesian optimization @xcite , we use markov - chain monte carlo ( mcmc ) integration to marginalize over the gps hyperparameters ( we use the emcee package @xcite ) . to accelerate the optimization",
    ", we use hyper - priors to emphasize meaningful values for the parameters , chiefly adopting the choices of the spearmint toolbox @xcite : a uniform prior between @xmath96 $ ] for all length scales @xmath28 in log space , a lognormal prior ( @xmath97 , @xmath98 ) for the covariance amplitude @xmath27 , and a horseshoe prior with length scale of @xmath99 for the noise variance @xmath100 .",
    "we used the original formulation of es by @xcite rather than the recent reformulation of pes by @xcite .",
    "the main reason for this is that the latter prohibits non - stationary kernels due to its use of bochner s theorem for a spectral approximation .",
    "pes could in principle be extended to work for our particular choice of kernels ( using an eigen - expansion , from which we could sample features ) ; since this would complicate making modifications to our kernel , we leave it as an avenue for future work , but note that in any case it may only further improve our method . to maximize the acquisition function we used the blackbox optimizer direct @xcite and cmaes @xcite .",
    "for our empirical evaluation of , we compared it to standard bayesian optimization ( using ei and es as acquisition functions ) , mtbo , and hyperband . for each method",
    ", we tracked wall clock time ( counting both optimization overhead and the cost of function evaluations , including the initial design ) , storing the incumbent returned after every iteration . in an offline validation step , we then trained models with all incumbents on the full dataset and measured their test error .",
    "we plot these test errors throughout .",
    "is an exception : here , we trained networks with the incumbents on the full training set ( 50000 data points , augmented to 100000 as in the original code ) and then measured and plotted performance on the validation set . ] to obtain error bars , we performed 10 independent runs of each method with different seeds ( except on the grid experiment , where we could afford 30 runs per method ) and plot medians , along with @xmath101 and @xmath102 percentiles for all experiments .",
    "details on the hyperparameter ranges used in every experiment are given in the supplemental material ( section d ) .",
    "we implemented hyperband following @xcite using the recommended setting for the parameter @xmath103 that controls the intermediate subset sizes . for each experiment , we adjusted the budget allocated to each hyperband iteration to allow the same minimum dataset size as for : 10 times the number of classes for the support vector machine benchmarks and the maximum batch size for the neural network benchmarks .",
    "we also followed the prescribed incumbent estimation after each iteration as the configuration with the best performance on the full dataset size .",
    "first , we considered a benchmark allowing the comparison of the various bayesian optimization methods on ground truth : our svm grid on mnist ( described in section [ sec : fabolas ] ) , for which we had performed all function evaluations beforehand , measuring loss and cost 10 times for each configuration @xmath12 and subset size @xmath58 to account for performance variations .",
    "( in this case , we computed each method s wall clock time in each iteration as its summed optimization overheads so far , plus the summed costs for the function values it queried so far . )",
    "mtbo requires choosing the number of data points in its auxiliary task .",
    "figure [ fig : svm_on_grid ] ( middle ) evaluates mtbo variants with a single auxiliary task with a relative size of @xmath66 , @xmath104 , and @xmath105 , respectively . with auxiliary tasks at either @xmath106 or @xmath104 ,",
    "mtbo improved quickly , but converged more slowly to the optimum ; we believe small correlations between the tasks cause this .",
    "figure [ fig : svm_on_grid ] ( right ) shows the dataset sizes chosen by the different algorithms during the optimization ; all methods slowly increased the average subset size used over time .",
    "an auxiliary task with @xmath107 worked best and we used this for mtbo in the remaining experiments .    at first glance , one might expect many tasks ( e.g. , with a task for each @xmath108 ) to work best , but quite the opposite is true . in preliminary experiments , we evaluated mtbo with up to 3 auxiliary tasks ( @xmath107 , @xmath104 , and @xmath105 ) , but found performance to strongly degrade with a growing number of tasks .",
    "we suspect that the @xmath109 kernel parameters that have to be learned for the discrete task kernel for @xmath110 tasks are the main reason .",
    "if the mcmc sampling is too short , the correlations are not appropriately reflected , especially in early iterations , and an adjusted sampling creates a large computational overhead that dominates wall - clock time .",
    "we therefore obtained best performance with only one auxiliary task .",
    "figure [ fig : svm_on_grid ] ( left ) shows results using ei , es , random search , mtbo and on this svm benchmark .",
    "ei and es perform equally well and find the best configuration ( which yields an error of @xmath111 , or @xmath112 ) after around @xmath113 seconds , roughly five times faster than random search .",
    "mtbo achieves good performance faster , requiring only around @xmath114 seconds to find the global optimum .",
    "is roughly another order of magnitude faster than mtbo in finding good configurations , and finds the global optimum at the same time .                  for a more realistic scenario , we optimized the same svm hyperparameters without a grid constraint on mnist and two other prominent uci datasets ( gathered from openml @xcite ) , vehicle registration  @xcite and forest cover types  @xcite with more than 50000 data points , now also comparing to hyperband .",
    "training svms on these datasets can take several hours , and figure [ fig : svm_openml ] shows that found good configurations for them between 10 and 1000 times faster than the other methods .",
    "hyperband required a relatively long time until it recommended its first hyperparameter setting , but this first recommendation was already very good , making hyperband substantially faster to find good settings than standard bayesian optimization running on the full dataset . however , typically returned configurations with the same quality another order of magnitude faster .",
    "convolutional neural networks ( cnns ) have shown superior performance on a variety of computer vision and speech recognition benchmarks , but finding good hyperparameter settings remains challenging , and almost no theoretical guarantees exist .",
    "tuning cnns for modern , large datasets is often infeasible via standard bayesian optimization ; in fact , this motivated the development of .",
    "we experimented with hyperparameter optimization for cnns on two well - established object recognition datasets , namely cifar10  @xcite and svhn  @xcite .",
    "we used the same setup for both datasets ( a cnn with three convolutional layers , with batch normalization  @xcite in each layer , optimized using adam  @xcite ) .",
    "we considered a total of five hyperparameters : the initial learning rate , the batch size and the number of units in each layer .",
    "for cifar10 , we used 40000 images for training , 10000 to estimate validation error , and the standard 10000 hold - out images to estimate the final test performance of incumbents .",
    "for svhn , we used 6000 of the 73257 training images to estimate validation error , the rest for training , and the standard 26032 images for testing .",
    "the results in figure [ fig : cnn ] show that  compared to the svm tasks speedup was smaller because cnns scale linearly in the number of datapoints .",
    "nevertheless , it found good configurations about 10 times faster than vanilla bayesian optimization .",
    "for the same reason of linear scaling , hyperband was substantially slower than vanilla bayesian optimization to make a recommendation , but it did find good hyperparameter settings when given enough time .          in the final experiment , we evaluated the performance of our method further on a more expensive benchmark , optimizing the validation performance of a deep residual network on the cifar10 dataset , using the original architecture from @xcite .",
    "as hyperparameters we exposed the learning rate , @xmath115 regularization , momentum and the factor by which the learning rate is multiplied after @xmath116 and @xmath117 epochs .",
    "figure [ fig : res_nets ] shows that found configurations with reasonable performance roughly 10 times faster than es and mtbo .",
    "note that due to limited computational capacities , we were unable to run hyperband on this benchmark : a single iteration took longer than a day , making it prohibitively expensive .",
    "( also note that by that time all other methods had already found good hyperparameter settings . )",
    "we want to emphasize that the runtime could be improved by adapting hyperband s parameters to the benchmark , but we decided to keep all methods parameters fixed throughout the experiments to also show their robustness .",
    "we presented , a new bayesian optimization method based on entropy search that mimics human experts in evaluating algorithms on subsets of the data to quickly gather information about good hyperparameter settings .",
    "extends the standard way of modelling the objective function by treating the dataset size as an additional continuous input variable .",
    "this allows the incorporation of strong prior information .",
    "it models the time it takes to evaluate a configuration and aims to evaluate points that yield  per time spent  the most information about the globally best hyperparameters for the full dataset .",
    "in various hyperparameter optimization experiments using support vector machines and deep neural networks , often found good configurations 10 to 100 times faster than the related approach of multi - task bayesian optimization , hyperband and standard bayesian optimization .",
    "our open - source code is available at https://github.com/automl/robo , along with scripts for reproducing our experiments .    in future work",
    ", we plan to expand our algorithm to model other environmental variables , such as the resolution size of images , the number of classes , and the number of epochs , and we expect this to yield additional speedups . since our method reduces the cost of individual function evaluations but requires more of these cheaper evaluations , we expect the cubic complexity of gaussian processes to become the limiting factor in many practical applications .",
    "we therefore plan to extend this work to other model classes , such as bayesian neural networks  @xcite , which may lower the computational overhead while having similar predictive quality ."
  ],
  "abstract_text": [
    "<S> bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms , such as support vector machines or deep neural networks . despite its success , for large datasets , training and validating </S>",
    "<S> a single configuration often takes hours , days , or even weeks , which limits the achievable performance . to accelerate hyperparameter optimization </S>",
    "<S> , we propose a generative model for the validation error as a function of training set size , which is learned during the optimization process and allows exploration of preliminary configurations on small subsets , by extrapolating to the full dataset . </S>",
    "<S> we construct a bayesian optimization procedure , dubbed , which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost . </S>",
    "<S> experiments optimizing support vector machines and deep neural networks show that often finds high - quality solutions 10 to 100 times faster than other state - of - the - art bayesian optimization methods or the recently proposed bandit strategy hyperband . </S>"
  ]
}