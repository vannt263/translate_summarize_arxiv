{
  "article_text": [
    "name entity disambiguation  @xcite ( also known as name entity resolution ) is an important problem , which has numerous applications in information retrieval  @xcite , digital forensic  @xcite , and social network analysis  @xcite . in information retrieval domain ,",
    "name disambiguation is important for sanitizing search results of ambiguous queries .",
    "for example , an online search query for `` michael jordan '' may retrieve pages of former us basketball player , the pages of uc berkeley machine learning professor , and the pages of other persons having that name , and name disambiguation is needed to organize those pages in homogeneous groups . in digital forensic , resolving name ambiguity is essential before inserting a person s profile in a law enforcement database ; failing to do so may cause severe distress to many innocents who are namesakes of a known criminal .",
    "evidently , name disambiguation plagues the digital library science domain the most . in this domain ,",
    "a key task is to record academic publications in digital repositories and it is often the case that the publications of multiple scholars sharing a name are recorded erroneously under a unique profile in some repositories .",
    "for example , in google scholar ( gs ) , which is one of the largest digital libraries for scholarly publications from various disciplines , there are more than @xmath1 distinct persons named `` wei wang '' , all of whose publications are listed under the same entity . severe cases of name ambiguity like this arise in digital library because the first name of an author is typically written in an abbreviated form in the citation of many scientific articles .",
    "unresolved name ambiguity in digital library over- or under - estimates a scholar s citation related impact metrics .    due to its importance ,",
    "the task of name entity disambiguation has allured data mining and database researchers , and over the years , they have proposed several methods for solving this problem  @xcite .",
    "the proposed methodologies differ in their learning approaches ( supervised  @xcite or unsupervised  @xcite  @xcite ) , machine learning methodologies ( support vector machines  @xcite , markov random field  @xcite , graph clustering  @xcite , etc ) and the data sources that they use ( internal data or external data , such as wikipedia  @xcite ) .",
    "many of the proposed methods are specific to resolving name ambiguity in digital library , and the major contribution in these works is effective feature engineering involving co - authors , publication venues , and research keywords . however , a key limitation of most of the existing methods for name disambiguation is that they operate in a batch mode , where all records to be resolved are initially accessible to the learning algorithm and a learning model is trained using features extracted from these records .",
    "hence , they fail to resolve emerging name ambiguities caused from the evolution of digital data , or they fail to utilize emerging evidences suggestive of merging of name entities which are separated in the existing state .",
    "re - running a batch learning to catch up with the data evolution is not practical due to the enormity of the computation on a large digital repository .",
    "so , it is more practical to perform name entity disambiguation task in an incremental fashion by considering the streaming nature of records .",
    "we call this _ online name entity disambiguation _ , which is the focus of this work .",
    "perhaps , the most prudent among the existing name disambiguation methodologies is supervised classification where for a given name reference   a classification model is trained which classifies each of the records into a given number of entities ( sharing that name ) .",
    "however , such a method is unable to identify records belonging to emerging name entities , who do not have any record in the training data .",
    "for example , in digital library , every now and then , papers are authored by a new scholar who is a namesake of some of the existing scholars ; as an example , consider the name reference  lei wang \" in arnetminer   bibliographic data repository . in 1999 , there were a handful of authors sharing that name , but with each passing year this number has been growing and in 2010 there existed more than 100 authors that have that same name ( see figure  [ fig : intro ] ) . a supervised classification model for the name reference `` lei wang '' , with a fixed number of classes will never be able to disambiguate the papers of new authors correctly , as these models have no provision for inclusion of new classes instantly .",
    "majority of the existing works on name entity disambiguation consider batch learning solution , however , very recently online name disambiguation is gaining some traction with a few works  @xcite .",
    "however , all of these methods propose a threshold - based approach for identifying emerging classes with a heuristically chosen threshold leading to unpredictable performance .",
    "real - life online digital library platforms , such as researchgate  , confirm the authorship of ambiguous records with the authors themselves before including them in their profiles .",
    "however , such a solution may not be very effective as it relies on a manual verification process which is tedious , and also introduces significant indexing delays for new records .    in this work",
    ", we solve the online name disambiguation task by a principled approach , namely bayesian non - exhaustive classification framework .",
    "we use non - exhaustive learning  @xcite a recent development in machine learning , which considers the scenario that training data may miss some classes ; it enables our proposed method to disambiguate records belonging to not only the existing entities , but also the emerging ones .",
    "specifically , given a non - exhaustive training dataset , we use a dirichlet process prior to model both known and emerging classes , where each class distribution is modeled by a normal distribution .",
    "we use a common normal @xmath0 inverse wishart ( niw ) prior to model the mean vectors and covariance matrices of all class distributions .",
    "the hyperparameters of the niw prior are estimated using data from known classes facilitating information sharing between known and emerging classes . for a future record ,",
    "the proposed approach computes class conditional probabilities by considering the possibility that the record may also originate from a new class .",
    "based on these probabilities each record is assigned to one of the existing classes , or to an emerging class that has no previous records in the training set .",
    "we update the set of classes every time a new class is created by the online model and then use a new classification model to classify subsequent records .",
    "the proposed framework paves the way for simultaneous online classification and novel class discovery .",
    "our contributions can be summarized as follows :    1 .",
    "we study online name disambiguation problem in a non - exhaustive streaming setting and propose a self - adjusting bayesian model that is capable of performing classification , and class discovery , at the same time . to the best of our knowledge ,",
    "our work is the first one to adapt bayesian non - exhaustive learning for online name disambiguation task .",
    "we propose a one sweep gibbs sampler to perform online non - exhaustive classification in order to efficiently evaluate the class assignment of an online record .",
    "we demonstrate the utility of our proposed approach on bibliographic datasets and present experimental results , which demonstrate the superiority of our proposed method over the state - of - the - art methodologies for online name disambiguation .",
    "there exist a large number of works on name entity disambiguation . in terms of methodologies , supervised  @xcite , unsupervised  @xcite , and probabilistic relational models  @xcite , are considered . in",
    "a supervised setting , a distinct entity can be considered as a class , and the objective is to classify each record to one of the classes .",
    "han et al .",
    "@xcite propose supervised name disambiguation methodologies by utilizing naive bayes and svm for name entity disambiguation task . for unsupervised name disambiguation ,",
    "the records are partitioned into several clusters with the goal of obtaining a partition where each cluster contains records from a unique entity .",
    "for example ,  @xcite proposes one of the earliest unsupervised name disambiguation methods for bibliographical data , which is based on @xmath2-way spectral clustering .",
    "specifically , they compute gram matrix representing similarities between different citations and apply @xmath2-way spectral clustering algorithm on the gram matrix in order to obtain the desired clusters of the citations . recently , probabilistic relational models , especially graphical models have also been considered for the name entity disambiguation task .",
    "for instance ,  @xcite proposes to use markov random fields to address name entity disambiguation challenge in a unified probabilistic framework .",
    "another work is presented in  @xcite which uses pairwise factor graph model for this task .",
    "@xcite present approaches for the name disambiguation task on anonymized graphs and they only leverage graph topological features due to the privacy concerns . in addition ,  @xcite solves name entity disambiguation problem as the privacy - preserving classification task such that the anonymized dataset satisfies non - disclosure requirements , at the same time it achieves high disambiguation performance .",
    "most of existing methods above tackle disambiguation task in a batch setting , where all records to be resolved are initially available to the algorithm , which makes them unsuitable for disambiguating a future record . in recent years , a few works have considered online name entity disambiguation  @xcite , which perform name disambiguation incrementally without retraining the system every time a new record is received .",
    "khabsa et al .",
    "@xcite use an online variant of dbscan , a well - known density - based clustering to cluster new records incrementally as they are being added . since",
    ", dbscan does not take the number of clusters as input , technically speaking , this method is able to adapt to the non - exhaustive scenario simply by assigning a new record in a new cluster , as needed .",
    "however , dbscan is quite susceptible to the choice of parameter values , and depending on the parameter value , a record belonging to an emerging class can simply be labeled as an outlier instance .",
    "@xcite proposes a two stage framework for online name disambiguation .",
    "the first stage performs batch name disambiguation to disambiguate all the records no later than a given time threshold using hierarchical agglomerative clustering .",
    "the second stage performs incremental name disambiguation to determine the class membership of a newly added record .",
    "however , the method uses a heuristic threshold to decide on the cluster assignments of new records which makes this approach very susceptible to the choice of threshold parameter .",
    "@xcite introduces an association rule based approach for detecting unseen authors in test set .",
    "however , the major drawback of proposed solution is that it can only identify records of emerging authors in a binary setting but fails to further distinguish among them .",
    "@xcite presents the expectation maximization based approach for the name disambiguation in twitter streaming data .",
    "in addition ,  @xcite proposes a threshold based method to discover emerging entities with ambiguous names in the domain of knowledge base .",
    "our proposed solution utilizes non - exhaustive learning  a rather recent development in machine learning .",
    "@xcite are some of the existing works related to non - exhaustive learning .",
    "akova et al .",
    "@xcite propose a bayesian approach for detecting emerging classes based on posterior probabilities .",
    "however , the decision function for identifying emerging classes uses a heuristic threshold and does not consider a prior model over class parameters ; hence the emerging class detection procedure of this model is purely data - driven .",
    "miller et al .",
    "@xcite present a mixture model using expectation maximization ( em ) for online class discovery .",
    "@xcite proposes a sequential importance sampling based online inference approach for emerging class discovery and the work is motivated by a bio - detection application .",
    "[ sec : ps ]    for a given name reference @xmath3 , assume @xmath4 is a stream of records associated with @xmath3 .",
    "the subscript @xmath5 represents the identifier of the last record in the stream and the value of this identifier increases as new records are observed in the stream .",
    "each record @xmath6 can be represented by a @xmath7-dimensional vector which is the feature representation of the record in a metric space . in real - life ,",
    "the name reference @xmath3 is associated with multiple persons ( say @xmath8 ) all sharing the same name , @xmath3 .",
    "the task of name entity disambiguation is to partition @xmath4 into @xmath8 disjoint sets such that each partition contains records of a unique person entity .",
    "when @xmath8 is fixed and known a priori , name entity disambiguation can be solved as a @xmath8-class classification task using supervised learning methodologies . however , for many domains the number of classes ( @xmath8 ) is not known , rather with new records being inserted in the stream , @xmath4 , the number of distinct person entities associated with @xmath3 may increase .",
    "the objective of online name entity disambiguation is to learn a model that assigns each incoming record into an appropriate partition containing records of a unique person entity .",
    "online name entity disambiguation is marred by several challenges , which we discuss below :    first , for a given record stream @xmath9 , the record @xmath10 is classified with the records leading up to @xmath11 , i.e. @xmath12 is our training data for this classification task .",
    "however , the record @xmath10 may belong to a new person entity ( having name @xmath3 ) with no previous records in @xmath12 .",
    "this happens because for online setting , the number of real - life name entities in @xmath4 is not fixed , rather it increases over the time .",
    "a traditional @xmath8-class supervised classification model which is trained with records of known entities mis - classifies the new emerging record with certainty , leading to an ill - defined classification problem .",
    "so , for online name entity disambiguation , a learning model is needed which works in non - exhaustive setting , where instances of some classes are not at all available in the training data . in existing works",
    ", this challenge is resolved using clustering framework where a new cluster is introduced for the emerging record of a new person entity , but this solution is not robust because small changes in clustering parameters make widely varying clustering outcomes .",
    "the second challenge is that online name entity disambiguation , more often , leads to a severely imbalanced classification task .",
    "this is due to the fact that in most of the real - life name entity disambiguation problems , the size of the true partitions of the record set @xmath4 follows a power - law distribution . in other words",
    ", there are a few persons ( dominant entities ) with the name reference @xmath3 to whom the majority of the records belong",
    ". only a few records ( typically one or two ) belong to each of the remaining entities ( with name reference @xmath3 ) .",
    "typically , the persons whose records appear at earlier time are dominant entities , which makes identifying novel entity an even more challenging task .    the third challenge in online name entity disambiguation is related to online learning scenario , where the incoming record is not merely a test instance of typical supervised learning .",
    "rather , the learning algorithm requires to detect whether the incoming record belongs to a novel entity , and if so , the algorithm must adapt itself and configure model to identify future records of this novel entity .",
    "overall , this requires a self - adjusting model that updates the number of classes to accurately classify incoming records of both new and existing entities .",
    "the final challenge in our list is related to temporal ordering of the records . in traditional classification",
    ", records do not have any temporal connotation , so an arbitrary train / test split is permitted .",
    "but , for online setting the model must respect time order of the records , i.e. , a future record can not be used for building a training model that classifies as older record .",
    "our proposed model overcomes all the above challenges by using a principled approach .",
    "as we have mentioned earlier , name entity disambiguation is a severe issue in digital library domain . in many other domains , solving name disambiguation is easier as the method may have access to personalized attributes of an entity , such as institution affiliation , and email address .",
    "but , in digital library , the reference of a paper only includes paper title , author name , publication venue , and year of publication , which are not sufficient for disambiguation of most of the name references . besides ,",
    "in many citations the first name of the authors are often replaced by initials , which worsen the disambiguation issue . as a result ,",
    "nearly , all the online bibliographic repositories , including dblp , google scholar , arnetminer , and pubmed , suffer from this issue .",
    "nevertheless , these repositories provide timely update of the publication data along with their chronological orders , so they provide an ideal for evaluating the effectiveness of an online name entity disambiguation method .",
    "in this work , we use bibliographic data as a case study for online name entity disambiguation . for each name reference @xmath3",
    ", we build a distinct classification model .",
    "the record stream @xmath4 for the name reference @xmath3 is the chronologically ordered stream of scholarly publications where @xmath3 is one of the authors . to build a feature vector for a paper in @xmath4",
    "we extract features from its author - list , keywords from its paper title , and paper venue ( journal / conference ) .",
    "we provide more details of feature construction in the following subsection .",
    "[ sec : preprocessing ]    for a given name reference @xmath3 , say we have a record stream containing @xmath5 papers for which the name reference @xmath3 is in the author - list .",
    "we represent each paper with a @xmath7 dimensional feature vector .",
    "then we define a data matrix @xmath13 for @xmath3 , in which each row corresponds to a record ( paper ) and each column represents a feature   to denote both the record stream and record data matrix . ] .",
    "in addition , each paper has a class label @xmath14 representing the @xmath15-th distinct person entity under name reference @xmath3 , who has authored this paper .",
    "our goal is to learn a model to partition papers with name reference @xmath3 in an online setting .    for a paper , we construct its feature vector ( a row of matrix @xmath4 ) using author information , paper title , and publication venue as attributes .",
    "these features are well - known for name disambiguation in digital library . for author information",
    ", we first aggregate the author - list of all papers into authors , then define a binary feature representation for each author , indicating his presence or absence in the author - list of that paper . for constructing keyword based feature , we first filter a set of predefined stop words from the paper titles and use the remaining words as a feature . for a given paper ,",
    "we use a binary value based on presence or absence of that word in the title of that paper .",
    "publication venues are converted to binary feature in the same way . for keywords , instead of using binary value ,",
    "we have also considered tf - idf value , but it does not provide noticeable performance improvement in any of our datasets . a possible reason for that could be we pre - process the data matrix with dimensionality reduction , which is able to capture hidden features directly from a binary data matrix .",
    "dimensionality reduction step reduces the dimensionality of data matrix @xmath13 .",
    "this step is important because the matrix @xmath4 is severely sparse with many zero entries .",
    "for dimensionality reduction we design an incremental variant of non - negative matrix factorization ( innmf ) , which maps @xmath4 into a low dimensional space denoted as @xmath16 , where @xmath17 is the number of hidden dimensions   to denote both the record stream and record data matrix after performing dimensionality reduction on @xmath4 . ] .",
    "specifically , we first perform non - negative matrix factorization ( nnmf )  @xcite in the batch mode on the set of training records initially available .",
    "then in the online mode , we express each sequentially observed record by a linear combination of the basis vectors generated from initial set of training records , where the coefficients of the basis vectors serve as our latent feature values . in order to learn the coefficients , we solve a constrained quadratic programming problem by minimizing a least square loss function under the constraint that each coefficient is non - negative .",
    "the goal of using innmf is to discover low dimensional latent features for each sequentially observed record in an effort to better fit our proposed normal @xmath0 normal @xmath0 inverse wishart ( nniw ) data model .",
    "after pre - processing , low dimensional data matrix @xmath18 is a collection of time - stamped @xmath5 record streams with which @xmath3 is associated , namely @xmath19 , where @xmath20 is a @xmath17 dimensional row vector generated from innmf to represent the @xmath15-th record in the given temporal record stream , and all of the @xmath5 records in @xmath18 are sorted temporally , namely @xmath21 , where @xmath22 represents the time for record @xmath23 .    by using an incessant stream of records , we formalize our online name disambiguation task as follows : given a time - stamped partition @xmath24 , we consider two types of records , namely record samples initially available in the training set with known class membership information , and record samples sequentially observed online with no verified class membership information .",
    "formally we treat the collection of time - stamped record streams @xmath25 as the set of training samples initially available , where @xmath26 and @xmath5 is the total number of samples in @xmath18 .",
    "as we can see , all of the records in @xmath18 occur no later than the given time - stamped threshold @xmath24 .",
    "@xmath27 is class indicator vector with @xmath28 , where @xmath8 is the number of distinct classes in the training set . in order to differentiate records in the training set from those observed online",
    ", we use @xmath29 to represent @xmath15-th record sequentially observed online .",
    "here we define @xmath30 to be the set of @xmath31 record samples sequentially available online after time threshold @xmath24 , i.e. @xmath32 . as new ambiguous authors emerge with the incoming records , the set of classes may expand . here",
    "we denote @xmath33 to be their corresponding unknown class information with @xmath34 , where @xmath35 is the number of new classes associated with these @xmath31 records observed online .    given an arbitrary online record @xmath36 at a certain time point , our proposed bayesian non - exhaustive classification model computes a probability to decide whether we should assign @xmath37 to one of the existing classes , or to a new class not yet observed in any of the historical records .",
    "in this section we discuss bayesian non - exhaustive name entity disambiguation methodologies .",
    "the methodologies discussed in this section are domain neutral and can be applied to any domain , once an appropriately constructed feature matrix is obtained .",
    "[ sec : dp ]    we model the set of record streams @xmath38 by using a set of latent parameters @xmath39 . each @xmath40 is drawn independently and identically ( iid ) by a dirichlet process  @xcite ( dp ) , while each record @xmath20 is distributed according to an unknown distribution @xmath41 parametrized by @xmath40 .",
    "mathematically ,    @xmath42    where @xmath43 is a random discrete probability measure defined by a base distribution @xmath44 along with the precision parameter @xmath45 .",
    "it is a common practice to use stick breaking construction  @xcite to represent samples drawn from a dirichlet process as below :    @xmath46    as shown in equation  [ eq : sbc ] , in order to simulate the process of stick breaking construction , imagine we have a stick of length @xmath47 to represent total probability .",
    "we first generate each point @xmath48 from base distribution @xmath44 , which originates from our proposed nniw data model ( details in section  [ sec : datamodel ] )",
    ". then we sample a random variable @xmath49 from beta(1 , @xmath50 ) distribution .",
    "after that we break off a fraction @xmath49 of the remaining stick as the weight of parameter @xmath48 , denoted as @xmath51 .",
    "in this way it allows us to represent random discrete probability measure @xmath43 as a probability mass function in terms of infinitely many @xmath52 and their corresponding weights @xmath53 yielding @xmath54 , where @xmath55 is the point mass of @xmath48 .",
    "consider that at a certain time point , we have a set of @xmath5 training records for name reference @xmath3 , denoted by @xmath56 where each record is assigned to a latent parameter from the set @xmath39 .",
    "a future online record @xmath37 is assigned to @xmath57 with a probability @xmath58    the conditional prior distribution in equation  [ eq : condp ] can be interpreted by a mixture of two distributions . specifically , given the future record @xmath37 with parameter @xmath57 , @xmath37 belongs to the base distribution @xmath44 with probability @xmath59 and it originates from random discrete probability measure @xmath43 generated from dp with probability @xmath60 . since @xmath43 is a discrete distribution , each record in a sequence of @xmath5 records generated from @xmath43 may not belong to a distinct @xmath61 .",
    "if we assume that there are @xmath8 distinct values of @xmath62 associated with the first @xmath5 records , then we can re - write equation  [ eq : condp ] as below :    @xmath63    where in the last expression of equation  [ eq : condp2 ] , @xmath64 represents each distinct @xmath62 , and @xmath65 denotes the number of times @xmath64 appears among the sequence of @xmath5 records .",
    "furthermore , each @xmath64 is associated with a unique class @xmath66 whose corresponding record @xmath67 is generated according to @xmath68 .",
    "thus after a sequence of @xmath5 records are observed , @xmath69 , the class membership of the future record @xmath37 , is equal to @xmath66 with probability @xmath70 , and it is equal to @xmath71 with probability @xmath72 , where @xmath71 is an emerging class which has not been observed in the sequence of @xmath5 training records .",
    "next we incorporate the data model into the dirichlet process prior and use the conditional posterior to determine whether @xmath37 should be assigned to one of the existing classes or to an emerging class .",
    "more specifically , we are interested to evaluate the following distribution :    @xmath73    as we can observe from equation  [ eq:5 ] , @xmath37 either belongs to a new class @xmath71 with the probability proportional to @xmath74 , or to a existing class @xmath66 with the probability proportional to @xmath75 .    due to the fact that @xmath64 is not known for online record sequence , here we replace @xmath76 with the conditional predictive distribution @xmath77 , where @xmath78 represents the subset of records with class label @xmath66 .",
    "thus for the online record @xmath37 , given the class membership information of all of @xmath5 sequence of records processed before @xmath37 , the following decision making function @xmath79 decides whether @xmath37 belongs to an emerging class or to one of the existing ones .    @xmath80    where @xmath81 .",
    "[ sec : oi ]    as shown in figure  [ fig : temporal ] , since we only have access to the class membership information @xmath82 for the streaming records initially available in the training set @xmath18 , which occur no later than the given time - stamped threshold @xmath24 , and the class membership @xmath83 of online sequentially observed record stream @xmath84 is not verified , thus it is impractical to evaluate the decision function shown in equation  [ eq : df1 ] directly . instead in order to respect the temporal order of each record , we utilize one sweep gibbs sampler for online classification to efficiently evaluate the probability of an online record belonging to an emerging class or to one of the existing ones .",
    "if @xmath85 is the predicted class label of @xmath15-th online observed record @xmath86 , the conditional distributions of the class indicator variable @xmath87 can be easily obtained via one sweep gibbs sampler .",
    "specifically , we are interested to sample from the following conditional distribution :    @xmath88    where @xmath89 , and @xmath90 is the number of new classes among the first @xmath91 online records .",
    "more clearly , given @xmath18 and @xmath82 , following equation  [ eq:2 ] , the mechanism of using one sweep gibbs sampling to process the first @xmath15 online sequentially observed records @xmath92 is presented as below :    @xmath93      the one - sweep gibbs sampling based online classification technique shown in equation  [ eq:2 ] requires the evaluation of the conditional predictive distribution @xmath94 and marginal distribution @xmath95 .",
    "fortunately a closed - form solution for both distributions does exist for the normal @xmath0 normal @xmath0 inverse wishart ( nniw ) model .",
    "we assume that the collected streaming records @xmath96 generated by the innmf step has the property of unimodality as each record is expressed by a linear combination of the basis vectors and the coefficients of the basis vectors are used as our features .",
    "thus we use a normally distributed data model , which can model class distributions fairly well .",
    "we present our data model under the bayesian non - exhaustive classification framework as below :",
    "@xmath97    where @xmath98 is the prior mean and @xmath99 is a scaling constant that controls the deviation of the class conditional mean vectors from the prior mean .",
    "the smaller @xmath99 is , the larger the between class scattering will be .",
    "the parameter @xmath100 is a positive definite matrix that encodes our prior belief about the expected @xmath101 .",
    "the parameter @xmath102 is a scalar that is negatively correlated with the degrees of freedom . in other words",
    "the larger @xmath102 is , the less @xmath101 will deviate from @xmath100 and vice versa .",
    "in addition , the base distribution @xmath44 in our proposed dirichlet process prior model originates from the joint conjugate distribution @xmath103 , which is defined as below :    @xmath104    here we define the sample mean of these @xmath5 streaming records to be @xmath105 , and sample covariance matrix @xmath106 . as sample mean @xmath107 and sample covariance matrix @xmath108 are sufficient statistics for the multivariate normally distributed data , in order to compute the conditional predictive distribution @xmath109",
    ", we can replace it with @xmath110 , where @xmath111 and @xmath112 are sample mean and sample covariance matrix for class @xmath66 .",
    "the mathematical derivation of @xmath110 is available in  @xcite and the result is in the form of a multivariate student - t distribution as is shown below :",
    "@xmath113    where @xmath114 is a @xmath115 mean vector , @xmath116 is a @xmath117 scale matrix , and @xmath118 is the degree of freedom of the obtained multivariate student - t distribution .",
    "besides , we can set @xmath119 in @xmath120 in order to evaluate the marginal distribution @xmath95 and it becomes another multivariate student - t distribution . therefore we could use equation  [ eq:10 ] to evaluate both conditional predictive distribution @xmath121 and marginal distribution @xmath122 for one sweep gibbs sampler shown in equation  [ eq:2 ] .",
    "in this section , we discuss our experimental results which validate the performance of our proposed bayesian non - exhaustive classification method for online name entity disambiguation on real - life data .",
    "this results also demonstrate the superiority of our proposed method against current state - of - the - art in online name entity disambiguation .",
    "we use arnetminer s name entity disambiguation dataset   for our experiment .",
    "this dataset contains author records of @xmath123 highly ambiguous name references selected from arnetminer database .",
    "the name references are shown in table  [ tab : dataset ] . in this table , for each name reference we also show the number of records , the number of binary attributes ( explained in section  [ sec : preprocessing ] ) , and the number of distinct authors associated with that name reference .",
    "for each of the 15 name references in the above dataset , we train a separate model for the disambiguation task .",
    "to simulate streaming data , we sort the records ( papers ) in temporal order and make train - test partition . specifically , we put the publication records of most recent @xmath124 years into the test set and the papers from earlier years in the training set .",
    "then we measure the performance of our proposed model by varying the value of @xmath124 . for a given train - test partition ,",
    "we first train the model using the training set initially available , then we process the records in the test set one - by - one in order to simulate streaming data in the online setting . for evaluation",
    "metric , we use macro - f1 measure , which is average of f1-measure of each class . the range of macro - f1 measure is between @xmath125 and @xmath47 , and a higher value indicates better disambiguation performance .",
    "our proposed method has the following tunable parameters , which we tune by using the training data .",
    "the first among those is the latent dimensionality @xmath17 for innmf ( section  [ sec : preprocessing ] ) .",
    "we consider different values between @xmath126 and @xmath127 and set the value that achieves the best macro - f1 measure on the training set by cross validation .",
    "the second parameter is @xmath50 in the dirichlet process prior model ( section  [ sec : dp ] ) , which controls the probability of assigning an incoming record to a new class entity .",
    "it plays a critical role in the number of generated classes in the name disambiguation process . in this work , in order to estimate the parameter @xmath50 , we first encode our prior belief about the odds of encountering a new class by a prior probability value @xmath128 , which can be set by measuring the probability of emerging records in a temporal sub - partition of the training data .",
    "given this prior , we estimate @xmath50 by empirical bayes through collecting a large number of samples from a chinese restaurant process ( crp )  @xcite for varying values of @xmath50 and then choosing the value that minimizes the difference between the empirical and true value of @xmath128 .",
    "our final set of parameters are the prior distribution of niw model : @xmath129 ( see section  [ sec : datamodel ] ) .",
    "we estimate these offline by using data records in the training set .",
    "specifically , we use the mean of the training set to estimate @xmath98 , and set @xmath100 as the pooled covariance @xmath130 as suggested in  @xcite . here , @xmath130 is defined as below :    @xmath131    where @xmath17 is the number of latent dimension from nnmf step , @xmath65 and @xmath112 are the number of samples and sample covariance matrix with respect to class @xmath66 in the training set . besides , we coarsely tune @xmath102 and @xmath99 with three values each , namely @xmath132 and @xmath133 and select the parameter combination with best disambiguation performance by cross validation on the training set .    in order to illustrate the merit of our proposed approach , we compare our model with the following benchmark techniques . among these the first two are existing state - of - the - art online name entity disambiguation methods , and the latter two are baselines that we have designed .    1 .   * qian s method  @xcite * given the collection of training records initially available , for a new record , qian s method computes class conditional probabilities for existing classes .",
    "this approach assumes that all the attributes are independent and the procedure of probability computation is based on the occurrence count of each attribute in all records of each class . then the computed probability is compared with a pre - defined threshold value to determine whether the newly added record should be assigned to an existing class , or to a new class not yet included in the previous data .",
    "* khabsa s method  @xcite * given the collection of training records initially available this approach first computes the @xmath134-neighborhood density for each online sequentially observed record .",
    "the @xmath134-neighborhood density of a new record is considered as the set of records within @xmath134 euclidean distance from that record .",
    "then if the neighborhood is sparse , the new record is assigned to a new class . otherwise , it is classified into the existing class that contains the most records in the @xmath134-neighborhood of the new record .",
    "* bernounaive - hac : * in this baseline , we first model the data with a multivariate bernoulli distribution ( features are binary , so bernoulli distribution is used ) and train a naive bayes classifier .",
    "this classifier returns class conditional probabilities for each record in the test set which we use as meta features in a hierarchical agglomerative clustering ( hac ) framework .",
    "* nnmf - svm - hac : * we perform nnmf on our binary feature matrix and use the coefficients returned by nnmf to train a linear svm . class",
    "conditional probabilities for each test record are used as meta features in a hierarchical agglomerative clustering ( hac ) framework the same way described above .",
    "for all the competing methods , we use identical set of features ( before dimensionality reduction ) .",
    "we vary the probability threshold value of qian s method and @xmath134 value of khabsa s method by cross validation on the training dataset . and select the ones that obtain the best disambiguation performance in terms of macro - f1 score . for bernounaive - hac and nnmf - svm - hac methods , during the hierarchical agglomerative clustering step , we tune the number of clusters in training set by cross validation in order to get the best disambiguation result .    for both data processing and model implementation , we implement our own code in python and use numpy  , scipy  , cvxopt   and scikit - learn   libraries for linear algebra , optimization and machine learning operations .",
    "we run all the experiments on a 2.1 ghz machine with 4 gb memory running linux operating system .",
    "we vary the train / test split to observe the performance comparison between our proposed method and other competing methods for different experimental settings . in our first setting ,",
    "records from the latest two years are used in the test split , and the remaining records are used in the train split . in two other settings , records from the",
    "latest three and latest four years are used in test split , respectively .",
    "table  [ tab : result1 ] , table  [ tab : result2 ] , and table  [ tab : result3 ] show the experimental results for these three settings . in all these three tables , the rows correspond to the fifteen name references .",
    "the last five columns show the performance of entity disambiguation of the corresponding name reference using macro - f1 score .",
    "since one sweep gibbs sampler in our proposed model is a randomized method , for each name reference we execute the method @xmath127 times and report the average macro - f1 score . for our method",
    ", we also show the standard deviation in the parenthesis  . for better visual comparison ,",
    "we highlight the best macro - f1 score of each name reference with boldface font .",
    "the `` # train records '' and `` # test records '' columns in these tables represent the number of training and test records ; `` # emerging records '' is the number of records in test set with their corresponding classes not represented in the initial training set , and `` # emerging classes '' denotes the number of emerging classes not represented in the training set .",
    "for all rows , as we compare the values in these columns across table  [ tab : result1 ] , table  [ tab : result2 ] , and table  [ tab : result3 ] , the number of training records decreases , the number of test records , emerging records , and emerging classes increase .",
    "it makes the disambiguation task in the first setting ( 2 years test split ) the easiest , and the third setting ( 4 years of test split ) the hardest .",
    "this is reflected in the macro - f1 values of all the name references across these three tables . for example ,",
    "for the first name reference , kai zhang , macro - f1 score of our method across these three tables are 0.683 , 0.602 , and 0.523 respectively .",
    "this performance reduction is caused by the increasing number of emerging classes ; 8 in table  [ tab : result1 ] , 10 in table  [ tab : result2 ] , and 17 in table  [ tab : result3 ] .",
    "another reason is decreasing number of training instances ; 42 in table  [ tab : result1 ] , 27 in table  [ tab : result2 ] , and 12 in table  [ tab : result3 ] .",
    "as can be seen in these tables , our name disambiguation dataset contains a large number of emerging records in the test data , all of these records will be misclassified with certainty by any traditional exhaustive name disambiguation methods .",
    "this is our main motivation for designing a non - exhaustive classification framework for online name entity disambiguation task .",
    "now we compare our method with the four competing methods .",
    "we observe that our proposed bayesian non - exhaustive classification method performs the best for 11 , 11 , and 12 name references ( out of 15 ) in table  [ tab : result1 ] , table  [ tab : result2 ] , and table  [ tab : result3 ] , respectively .",
    "besides , the margin of performance difference between our method and the second best method is large , typically between 0.05 and 0.20 in terms of macro - f1 score . for an example , consider the name entity lei wang in table  [ tab : result2 ] , even though it has 67 emerging records with @xmath135 emerging classes , our method achieves 0.723 macro - f1 score for this name reference ; whereas the second best method achieves only 0.560which is smaller by 0.163 .",
    "the relatively good performance of the proposed method may be due to our non - exhaustive learning methodologies .",
    "it also suggests that the base distribution used by the proposed dirichlet process prior model whose parameters are estimated using data from known classes can be generalized for the class distributions of unknown classes as well .",
    "in contrast , among all the competing methods , qian s method and khabsa s method perform the worst as they fail to incorporate prior information about class distribution into the models and the results are very sensitive to the selections of threshold parameters . on the other hand",
    "both bernounaive - hac and nnmf - svm - hac operate in an off - line framework .",
    "although for some name references f1 scores obtained by these techniques are higher than our proposed method , there is a clear trend favoring our proposed method over these methods  latter can not explicitly identify streaming records of new ambiguous classes in an online setting .    in table",
    "[ tab : result4 ] , using the data records of most recent @xmath136 years as test set , we present the results of automatic estimation of the number of distinct entities in test set .",
    "as shown in table  [ tab : result4 ] , # actual authors is the ground truth number of real - life persons among the records in the test set , and # predicted authors is the value predicted by our proposed method .",
    "we can see that the estimated numbers are close to the actual numbers for most name references .",
    "for example , for the name reference of  bo liu \" , our predicted result is exactly the same as the actual one .",
    "overall these results demonstrate that our proposed framework offers a robust approach to accurately estimate the number of actual real - life persons , especially when the records appear in a streaming fashion .",
    "we also investigate the contribution of each of the defined features ( coauthor , keyword , venue ) for the task of online name disambiguation .",
    "specifically , we first rank the individual features by their performance in terms of macro - f1 score , then add the features one by one in the order of their disambiguation power .",
    "in particular , we first use author - list , followed by keywords , and publication venue . in each step , we evaluate the performance of our proposed online name disambiguation method using the most recent two years publication records as test set .",
    "figure  [ fig : feature - analysis ] shows the macro - f1 value of our method with different feature combinations . as we can see from this figure , after adding each feature group we observe improvements for most of the name references .",
    "similar patterns are observed when we use different number of publication records as test set .",
    "a very desirable feature of our proposed bayesian non - exhaustive classification model is its running time .",
    "for example , using the most recent two years records as test set , on the name reference  kai zhang \" containing @xmath137 papers with @xmath138 latent dimensionality , it takes around @xmath139 seconds on average to assign the test papers to different real - life authors for one - sweep gibbs sampler . for the name reference",
    " lei wang \" with @xmath140 papers using same number of latent dimensionality , it takes around @xmath141 seconds on average under the same setting .",
    "this suggests only a linear increase in computational time with respect to the number of records .",
    "however in addition to number of records , the computational time depends on other factors , such as the latent dimensionality and the number of classes generated , which in turn depends on the values of the hyperparameters used in the data model .",
    "to conclude , in this paper we propose a bayesian non - exhaustive classification framework for online name entity disambiguation .",
    "given sequentially observed record streams , our method classifies the incoming records into existing classes , as well as emerging classes by using one sweep gibbs sampler for learning posterior probability of a dirichlet process mixture model .",
    "our experimental results on bibliographic datasets show that the proposed method significantly outperforms the existing state - of - the - arts for disambiguating authors name in online setting .",
    "this research is sponsored by both mohammad al hasan s nsf career award ( iis-1149851 ) and murat dundar s nsf career award ( iis-1252648 ) .",
    "the contents are solely the responsibility of the authors and do not necessarily represent the official view of nsf ."
  ],
  "abstract_text": [
    "<S> the name entity disambiguation task aims to partition the records of multiple real - life persons so that each partition contains records pertaining to a unique person . </S>",
    "<S> most of the existing solutions for this task operate in a batch mode , where all records to be disambiguated are initially available to the algorithm . however , more realistic settings require that the name disambiguation task be performed in an online fashion , in addition to , being able to identify records of new ambiguous entities having no preexisting records . in this work , </S>",
    "<S> we propose a bayesian non - exhaustive classification framework for solving online name disambiguation task . </S>",
    "<S> our proposed method uses a dirichlet process prior with a normal @xmath0 normal @xmath0 inverse wishart data model which enables identification of new ambiguous entities who have no records in the training data . for online classification , we use one sweep gibbs sampler which is very efficient and effective . as a case study we consider bibliographic data in a temporal stream format and disambiguate authors by partitioning their papers into homogeneous groups . </S>",
    "<S> our experimental results demonstrate that the proposed method is better than existing methods for performing online name disambiguation task .    </S>",
    "<S> = 10000 = 10000 </S>"
  ]
}