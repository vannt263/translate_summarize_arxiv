{
  "article_text": [
    "in their seminal work , alon , matias and szegedy @xcite presented celebrated sketching techniques and showed that @xmath0-wise independence is sufficient to obtain good approximations of the second frequency moment .",
    "indyk and mcgregor @xcite make use of this technique in their work introduce the problem of measuring independence in the streaming model .",
    "there they give efficient algorithms for approximating pairwise independence for the @xmath14 and @xmath7 norms . in their model ,",
    "a stream of pairs @xmath3 ^ 2 $ ] arrive , giving a joint distribution @xmath4 , and the notion of approximating pairwise independence corresponds to approximating the distance between the joint distribution and the product of the marginal distributions for the pairs .",
    "indyk and mcgregor state , as an explicit open question in their paper , the problem of whether one can estimate @xmath8-wise independence on @xmath8-tuples for any @xmath15 .",
    "in particular , indyk and mcgregor show that , for the @xmath7 norm , they can make use of the product of @xmath0-wise independent functions on @xmath1 $ ] in the sketching method of alon , matias , and szegedy .",
    "we extend their approach to show that on the product domain @xmath1^k$ ] , the sketching method of alon , matias , and szegedy works when using the product of @xmath8 copies of @xmath0-wise independent functions on @xmath1 $ ] .",
    "the cost is that the memory requirements of our approach grow exponentially with @xmath8 , proportionally to @xmath13 .",
    "measuring independence and @xmath8-wise independence is a fundamental problem with many applications ( see e.g. , lehmann @xcite ) .",
    "recently , this problem was also addressed in other models by , among others , alon , andoni , kaufman , matulef , rubinfeld and xie @xcite ; batu , fortnow , fischer , kumar , rubinfeld and white @xcite ; goldreich and ron @xcite ; batu , kumar and rubinfeld @xcite ; alon , goldreich and mansour @xcite ; and rubinfeld and servedio @xcite .",
    "traditional non - parametric methods of testing independence over empirical data usually require space complexity that is polynomial to either the support size or input size .",
    "the scale of contemporary data sets often prohibits such space complexity .",
    "it is therefore natural to ask whether we will be able to design algorithms to test for independence in streaming model .",
    "interestingly , this specific problem appears not to have been introduced until the work of indyk and mcgregor . while arguably results for the @xmath14 norm would be stronger than for the @xmath7 norm in this setting , the problem for @xmath7 norms is interesting in its own right .",
    "the problem for the @xmath14 norm has been recently resolved by braverman and ostrovsky in @xcite .",
    "they gave an @xmath16-approximation algorithm that makes a single pass over a data stream and uses polylogarithmic memory .",
    "in this paper we generalize the `` sketching of sketches '' result of indyk and mcgregor .",
    "our specific theoretical contributions can be summarized as follows :     _ * main theorem . *",
    "_    let @xmath17 be a vector with entries @xmath18 for @xmath19^k$ ] .",
    "let @xmath20 \\rightarrow \\{-1 , 1\\}$ ] be independent copies of 4-wise independent hash functions ; that is , @xmath21 are @xmath0-wise independent hash functions for each @xmath22 $ ] , and @xmath23 are mutually independent .",
    "define @xmath24 , and the sketch @xmath25^k } \\vec v_{\\mathbf p } h(p)$ ] .",
    "we prove that the sketch @xmath6 can be used to give an efficient approximation for @xmath26 ; our result is stated formally in theorem [ tm : main1 ] .",
    "note that @xmath27 is not @xmath0-wise independent .     as a corollary ,",
    "the main application of our main theorem is to extend the result of indyk and mcgregor @xcite to detect the dependency of @xmath8 random variables in streaming model .    for every @xmath28 and @xmath29",
    ", there exists a randomized algorithm that computes , given a sequence @xmath30 of @xmath8-tuples , in one pass and using @xmath31 memory bits , a number @xmath6 so that the probability @xmath6 deviates from the @xmath7 distance between product and joint distribution by more than a factor of @xmath32 is at most @xmath33 .",
    "this paper is merge from @xcite , where the same result was obtained with different proofs .",
    "the proof of @xcite generalizes the geometric approach of indyk and mcgregor @xcite with new geometric observations .",
    "the proofs of @xcite are more combinatorial in nature .",
    "these papers offer new insights , but due to the space limitation , we focus on the proof from @xcite in this paper .",
    "original papers are available on line and are recommended to the interested reader .",
    "we provide the general underlying model . here",
    "we mostly follow the notation of @xcite .",
    "let @xmath34 be a stream of size @xmath12 with elements @xmath30 , where @xmath35^k$ ] .",
    "( when we have a sequence of elements that are themselves vectors , we denote the sequence number by a subscript and the vector entry by a superscript when both are needed . )",
    "the stream @xmath34 defines an _ empirical _ distribution over @xmath1^k$ ] as follows : the frequency @xmath36 of an element @xmath37^k$ ] is defined as the number of times it appears in @xmath34 , and the empirical distribution is @xmath38 = \\frac{f(\\omega)}{m } \\quad \\mbox{for any $ \\omega \\in [ n]^k$.}\\ ] ]    since @xmath39 is a vector of size @xmath8 , we may also view the streaming data as defining a joint distribution over the random variables @xmath40 corresponding to the values in each dimension .",
    "( in the case of @xmath2 , we write the random variables as @xmath5 and @xmath6 rather than @xmath41 and @xmath42 . )",
    "there is a natural way of defining marginal distribution for the random variable @xmath43 : for @xmath44 $ ] , let @xmath45 be the number of times @xmath46 appears in the @xmath47th coordinate of an element of @xmath34 , or @xmath48 the empirical marginal distribution @xmath49 $ ] for the @xmath47th coordinate is defined as @xmath50 = \\frac{f_i(\\omega_i)}{m } \\quad \\mbox{for any $ \\omega_i \\in [ n]$.}\\ ] ]    next let @xmath51 be the vector in @xmath52^k}$ ] with @xmath53 - \\prod_{1 \\leq i\\leq k}\\pr_i[\\omega_i]$ ] for all @xmath37^k$ ] .",
    "our goal is to approximate the value @xmath54^k}\\left|\\pr[\\omega ] - \\prod_{1 \\leq i \\leq k}\\mathrm{pr}_i[\\omega_i ] \\right|^2\\right)^{\\frac 1 2}.\\ ] ] this represent the @xmath7 norm between the tensor of the marginal distributions and the joint distribution , which we would expect to be close to zero in the case where the @xmath43 were truly independent .",
    "finally , our algorithms will assume the availability of 4-wise independent hash functions .",
    "for more on 4-wise independence , including efficient implementations , see @xcite . for the purposes of this paper ,",
    "the following simple definition will suffice .    _",
    "( 4-wise independence ) _ a family of hash functions @xmath55 with domain @xmath1 $ ] and range @xmath56 is _",
    "4-wise independent _ if for any distinct values",
    "@xmath57 $ ] and any @xmath58 , the following equality holds , @xmath59 = 1/16.\\ ] ]    in @xcite , the family of 4-wise independent hash functions @xmath55 is called 4-wise independent random vectors . for consistencies within our paper",
    ", we will always view the object @xmath55 as a hash function family .",
    "we begin by reviewing the approximation algorithm and associated proof for the @xmath7 norm given in @xcite . reviewing this result",
    "will allow us to provide the necessary notation and frame the setting for our extension to general @xmath8 .",
    "moreover , in our proof , we find that a constant in lemma 3.1 from @xcite that we subsequently generalize appears incorrect .",
    "( because of this , our proof is slightly different and more detailed than the original . )",
    "although the error is minor in the context of their paper ( it only affects the constant factor in the order notation ) , it becomes more important when considering the proper generalization to larger @xmath8 , and hence it is useful to correct here .    in the case @xmath60 , we assume that the sequence @xmath61 arrives an item by an item .",
    "each @xmath62 ( for @xmath63 ) is an element in @xmath1 ^ 2 $ ] .",
    "the random variables @xmath5 and @xmath6 over @xmath1 $ ] can be expressed as follows : @xmath64 & = & \\pr[x = i , y = j ] & = & |\\{\\ell : ( a^1_\\ell , a^2_\\ell ) = ( i , j)\\}| / m \\\\ \\pr_1[i ] & = & \\pr[x = i ] & = & |\\{\\ell : ( a^1_\\ell , a^2_\\ell ) = ( i , \\cdot)\\}| / m \\\\ \\pr_2[j ] & = & \\pr[y = j ] & = & |\\{\\ell : ( a^1_\\ell , a^2_\\ell ) = ( \\cdot , j)\\}| / m. \\end{array}\\right.\\ ] ] we simplify the notation and use @xmath65 $ ] , @xmath66 $ ] , @xmath67 $ ] . and @xmath68\\pr[y = j]$ ] .",
    "indyk and mcgregor s algorithm proceeds in a similar fashion to the streaming algorithm presented in @xcite . specifically let @xmath69 and @xmath70 .",
    "the algorithm computes @xmath71 random variables @xmath72 and outputs their median .",
    "the output is the algorithm s estimate on the norm of @xmath73 defined in equation  [ eqn : alpha ] .",
    "each @xmath74 is the average of @xmath75 random variables @xmath76 : @xmath77 , where @xmath76 are independent , identically distributed random variables .",
    "each of the variables @xmath78 can be computed from the algorithmic routine shown in figure  [ fig : algx ] .",
    "independently generate 4-wise independent random functions @xmath79 from @xmath1 $ ] to @xmath56 .",
    "@xmath80 @xmath12 let the @xmath81th item @xmath82 @xmath83 , @xmath84 , @xmath85 .",
    "return @xmath86 .    by the end of the process @xmath87",
    ", we have @xmath88}h_1(i)h_2(j)r_{i , j}$ ] , @xmath89}h_1(i)p_i$ ] , and @xmath90}h_2(i)q_i$ ] . also ,",
    "when a vector is in @xmath91 , its indices can be represented by @xmath92 ^ 2 $ ] . in what follows",
    ", we will use a bold letter to represent the index of a high dimensional vector , e.g. , @xmath93 .",
    "the following lemma shows that the expectation of @xmath6 is @xmath94 and the variance of @xmath6 is at most @xmath95)^2 $ ] because @xmath96 \\leq 9 \\mathrm e[y]^2 $ ] .",
    "[ lem : varx](@xcite ) let @xmath79 be two independent instances of 4-wise independent hash functions from @xmath1 $ ] to @xmath56 .",
    "let @xmath97 and @xmath98 .",
    "let us define @xmath99 ^ 2}h(\\mathbf i)v_{\\mathbf i}\\right)^2 $ ] .",
    "then @xmath100 = \\sum_{\\mathbf i \\in [ n]^2}\\vec v_{\\mathbf i}^2 $ ] and @xmath96 \\leq 9(\\mathrm e[y])^2 $ ] , which implies @xmath101 \\leq 8 e^2[y]$ ] .",
    "we have @xmath100 = \\mathrm e[(\\sum_{\\mathbf i}h(\\mathbf i)\\vec v_{\\mathbf i})^2 ] = \\sum_{\\mathbf i}\\vec v^2_{\\mathbf i } \\mathrm e[h^2(\\mathbf i ) ] + \\sum_{\\mathbf i \\neq \\mathbf j}\\vec v_{\\mathbf i}\\vec v_{\\mathbf j}\\mathrm e[h(\\mathbf i)h(\\mathbf j)]$ ] . for all @xmath102 ^ 2 $ ] , we know @xmath103 . on the other hand , @xmath104 . the probability that @xmath105 is @xmath106 = \\pr[h_1(i_1)h_1(j_1)h_2(i_2)h_2(j_2 ) = 1 ] = 1/16 + \\binom{4}{2}1/16 + 1/16 = 1/2 $ ] .",
    "the last equality holds is because @xmath107 is equivalent to saying either all these variables are 1 , or exactly two of these variables are -1 , or all these variables are -1 .",
    "therefore , @xmath108 = 0 $ ] .",
    "consequently , @xmath100 = \\sum_{\\mathrm i \\in [ n]^2}(\\vec v_{\\mathrm i})^2 $ ] .",
    "now we bound the variance . recall that @xmath101 = \\mathrm e[y^2 ] - \\mathrm e[y]^2 $ ] , we bound @xmath109",
    "= \\sum_{\\mathbf i , \\mathbf j , \\mathbf k , \\mathbf l \\in [ n]^2 } \\mathrm e[h(\\mathbf i)h(\\mathbf j)h(\\mathbf k)h(\\mathbf l)]\\vec v_{\\mathbf i}\\vec v_{\\mathbf j}\\vec v_{\\mathbf k}\\vec v_{\\mathbf l } \\leq   \\sum_{\\mathbf i , \\mathbf j , \\mathbf k , \\mathbf l \\in [ n]^2 } \\left|\\mathrm e[h(\\mathbf i)h(\\mathbf j)h(\\mathbf k)h(\\mathbf l)]\\right| \\cdot |\\vec v_{\\mathbf i}\\vec v_{\\mathbf j}\\vec v_{\\mathbf k}\\vec v_{\\mathbf l}|.\\ ] ]    also @xmath110\\right| \\in \\{0 , 1\\}$ ] .",
    "the quantity @xmath111 \\neq 0 $ ] if and only if the following relation holds , @xmath112 : \\left ( ( i_s = j_s ) \\wedge ( k_s = l_s ) \\right ) \\vee \\left ( ( i_s = k_s ) \\wedge ( j_s = l_s ) \\right ) \\vee \\left ( ( i_s = l_s ) \\wedge ( k_s = j_s ) \\right).\\ ] ] denote the set of 4-tuples @xmath113 that satisfy the above relation by @xmath114",
    ". we may also view each 4-tuple as an ordered set that consists of 4 points in @xmath1 ^ 2 $ ] .",
    "consider the unique smallest axes - parallel rectangle in @xmath1 ^ 2 $ ] that contains a given 4-tuple in @xmath114 ( i.e. contains the four ordered points ) .",
    "note this could either be a ( degenerate ) line segment or a ( non - degenerate ) rectangle , as we discuss below .",
    "let @xmath115 be the function that maps an element @xmath116 to the smallest rectangle @xmath117 defined by @xmath118 .",
    "since a rectangle can be uniquely determined by its diagonals , we may write @xmath119 , where @xmath120 $ ] , @xmath121 $ ] and the corresponding rectangle is understood to be the one with diagonal @xmath122 .",
    "also , the inverse function @xmath123 represents the pre - images of @xmath124 in @xmath114 .",
    "@xmath124 is degenerate if either @xmath125 or @xmath126 , in which case the rectangle ( and its diagonals ) correspond to the segment itself , or @xmath125 and @xmath126 , and the rectangle is just a single point .",
    "let @xmath127 , @xmath128 , @xmath129 , and @xmath130 .",
    "the tuple is in @xmath114 and its corresponding bounding rectangle is a non - degenerate rectangle .",
    "the function @xmath131 .",
    "let @xmath132 and @xmath133 . the tuple is also in @xmath114 and minimal bounding rectangle formed by these points is an interval @xmath134 . the function @xmath135 .",
    "to start we consider the non - degenerate cases .",
    "fix any @xmath124 with @xmath136 and @xmath137 .",
    "there are in total @xmath138 tuples @xmath139 in @xmath114 with @xmath140 .",
    "twenty - four of these tuples correspond to the setting where none of @xmath141 are equal , as there are twenty - four permutations of the assignment of the labels @xmath141 to the four points .",
    "( this corresponds to the first example ) . in this case",
    "the four points form a rectangle , and we have @xmath142 .",
    "intuitively , in these cases , we assign the `` weight '' of the tuple to the diagonals .",
    "the remaining twelve tuples in @xmath123 correspond to intervals .",
    "( this corresponds to the second example . ) in this case two of @xmath141 correspond to one endpoint of the interval , and the other two labels correspond to the other endpoint .",
    "hence we have either @xmath143 or @xmath144 , and there are six tuples for each case .",
    "therefore for any @xmath145 $ ] and @xmath146 $ ] we have : @xmath147    the analysis is similar for the degenerate cases , where the constant 18 in the bound above is now quite loose . when exactly one of @xmath148 or @xmath126 holds , the size of @xmath123 is @xmath149 , and the resulting intervals correspond to vertical or horizontal lines .",
    "when both @xmath148 and @xmath126 , then @xmath150",
    ". in sum , we have following the same analysis as for the non - degenerate cases , we find @xmath151 @xmath152 @xmath153 @xmath154 ^ 2 \\\\",
    "\\mathbf j \\in [ n]^2}}(\\vec v_{\\mathbf i}\\vec v_{\\mathbf j})^2 = 9 \\mathrm e^2[y].\\ ] ]    finally , we have @xmath155 ^ 2 } \\left|\\mathrm",
    "e[h(\\mathbf i)h(\\mathbf j)h(\\mathbf k)h(\\mathbf l)]\\right| \\cdot |\\vec v_{\\mathbf i}\\vec v_{\\mathbf j}\\vec v_{\\mathbf k}\\vec v_{\\mathbf l}| \\leq \\sum_{\\mathbf i , \\mathbf j , \\mathbf k , \\mathbf l \\in \\mathcal d } |\\vec v_{\\mathbf i}\\vec v_{\\mathbf j}\\vec v_{\\mathbf k}\\vec v_{\\mathbf l}| \\leq 9 \\mathrm e^2[y]$ ] and @xmath101 \\leq 8\\mathrm e[y]^2 $ ] .",
    "we emphasize the geometric interpretation of the above proof as follows .",
    "the goal is to bound the variance by a constant times @xmath156 = \\sum_{\\scriptsize{}\\substack{\\mathbf i,\\mathbf j \\in [ n]^2 } } ( \\vec v_{\\mathbf i}v_{\\mathbf j})^2 $ ] , where the index set is the set of all possible lines in plane @xmath1 ^ 2 $ ] ( each line appears twice ) .",
    "we first show that @xmath101 \\leq \\sum_{\\mathbf i , \\mathbf j , \\mathbf k , \\mathbf l \\in \\mathcal d } |\\vec v_{\\mathbf i}\\vec v_{\\mathbf j}\\vec v_{\\mathbf k}\\vec v_{\\mathbf l}|$ ] , where the 4-tuple index set corresponds to a set of rectangles in a natural way .",
    "the main idea of @xcite is to use inequalities of the form @xmath157 to assign the `` weight '' of each @xmath0-tuple to the diagonals of the corresponding rectangle .",
    "the above analysis shows that @xmath158 copies of all lines are sufficient to accommodate all 4-tuples . while similar inequalities could also assign the weight of a @xmath0-tuple to the vertical or horizontal edges of the corresponding rectangle , using vertical or horizontal edges is problematic .",
    "the reason is that there are @xmath159 @xmath0-tuples but only @xmath160 vertical or horizontal edges , so some lines would receive @xmath161 weight , requiring @xmath161 copies .",
    "this problem is already noted in @xcite .    our bound here is @xmath96",
    "\\leq 9\\mathrm e^2[y]$ ] , while in @xcite the bound obtained is @xmath96 \\leq 3\\mathrm e^2[y]$ ] .",
    "there appears to have been an error in the derivation in @xcite ; some intuition comes from the following example .",
    "we note that @xmath162 is at least @xmath163 .",
    "( this counts the number of non - degenerate @xmath0-tuples . )",
    "now if we set @xmath164 for all @xmath165 , we have @xmath96 \\geq |\\mathcal d| = 9n^4 - 9n^2 \\sim \\mathrm 9 \\mathrm e^2(d)$ ] , which suggests @xmath166 > 3 \\mathrm e^2[d]$ ] .",
    "again , we emphasize this discrepancy is of little importance to @xcite ; the point there is that the variance is bounded by a constant factor times the square of the expectation .",
    "it is here , where we are generalizing to @xmath167 , that the exact constant factor is of some importance .    given the bounds on the expectation and variance for the @xmath168 , standard techniques yield a bound on the performance of our algorithm .",
    "[ thm : case2 ] for every @xmath28 and @xmath29 , there exists a randomized algorithm that computes , given a sequence @xmath169 , in one pass and using @xmath170 memory bits , a number @xmath171 so that the probability @xmath171 deviates from @xmath94 by more than @xmath172 is at most @xmath33 .",
    "recall the algorithm described in the beginning of section [ sec : kis2 ] : let @xmath69 and @xmath173 .",
    "we first computes @xmath71 random variables @xmath72 and outputs their median @xmath171 , where each @xmath74 is the average of @xmath75 random variables @xmath76 : @xmath77 and @xmath76 are independent , identically distributed random variables computed by figure  [ fig : algx ] . by chebyshev s inequality ,",
    "we know that for any fixed @xmath47 , @xmath174 \\leq \\frac{\\mathrm{var}(y_i)}{\\epsilon^2 \\|\\vec v\\|^2 } = \\frac{(1/s_1)\\mathrm{var}[y]}{\\epsilon^2\\|\\vec v\\|^2 } = \\frac{(9\\epsilon^2/72 ) \\|\\vec v\\|^2}{\\epsilon^2 \\|\\vec v\\|^2 } = \\frac 1 8.\\ ] ] finally , by standard chernoff bound arguments ( see for example chapter 4 of @xcite ) , the probability that more than @xmath175 of the variables @xmath74 deviate by more than @xmath176 from @xmath177 is at most @xmath33 . in case",
    "this does not happen , the median @xmath171 supplies a good estimate to the required quantity @xmath177 as needed .",
    "now let us move to the general case where @xmath167 . recall that @xmath179 is a vector in @xmath180 that maintains certain statistics of a data stream ,",
    "and we are interested in estimating its @xmath7 norm @xmath181 .",
    "there is a natural generalization for indyk and mcgregor s method for @xmath2 to construct an estimator for @xmath181 : let @xmath20 \\rightarrow \\{-1 , 1\\}$ ] be independent copies of 4-wise independent hash functions ( namely , @xmath21 are @xmath0-wise independent hash functions for each @xmath22 $ ] , and @xmath23 are mutually independent . ) .",
    "let @xmath24 .",
    "the estimator @xmath6 is defined as @xmath182^k}\\vec v_{{{\\mathbf p } } } h({{\\mathbf p } } ) \\right)^2 $ ] .",
    "our goal is to show that @xmath100 = \\|\\vec v\\|^2 $ ] and @xmath101 $ ] is reasonably small so that a streaming algorithm maintaining multiple independent instances of estimator @xmath6 will be able to output an approximately correct estimation of @xmath181 with high probability .",
    "notice that when @xmath181 represents the @xmath7 distance between the joint distribution and the tensors of the marginal distributions , the estimator can be computed efficiently in a streaming model similarly to as in figure  [ fig : algx ] .",
    "we stress that our result is applicable to a broader class of @xmath7-norm estimation problems , as long as the vector @xmath179 to be estimated has a corresponding efficiently computable estimator @xmath6 in an appropriate streaming model .",
    "formally , we shall prove the following main lemma in the next subsection .",
    "[ lem : main_lemma ] let @xmath179 be a vector in @xmath180 , and @xmath20 \\rightarrow \\{-1 , 1\\}$ ] be independent copies of 4-wise independent hash functions .",
    "define @xmath24 , and @xmath182^k}\\vec v_{{{\\mathbf p } } } h({{\\mathbf p } } ) \\right)^2 $ ] .",
    "we have @xmath183 = ||{{\\vec{v } } } ||$ ] and @xmath184 \\leq 3^k { { \\mathrm{e } } } [ y]^2 $ ] .",
    "we remark that the bound on the variance in the above lemma is tight .",
    "one can verify that when the vector @xmath179 is a uniform vector ( i.e. , all entries of @xmath179 are the same ) , the variance of @xmath6 is @xmath185 ^ 2)$ ] . with the above lemma",
    ", the following main theorem mentioned in the introduction immediately follows by a standard argument presented in the proof of theorem [ thm : case2 ] in the previous section .",
    "[ tm : main1 ] let @xmath51 be a vector in @xmath186^k}$ ] that maintains an arbitrary statistics in a data stream of size @xmath12 , in which every item is from @xmath1^k$ ] .",
    "let @xmath187 be real numbers .",
    "if there exists an algorithm that maintains an instance of @xmath6 using @xmath188 memory bits , then there exists an algorithm @xmath189 such that :    1 .",
    "with probability @xmath190 the algorithm @xmath189 outputs a value between @xmath191 $ ] and 2 .",
    "the space complexity of @xmath189 is @xmath192 .    as discussed above",
    ", an immediate corollary is the existence of a one - pass space efficient streaming algorithm to detect the dependency of @xmath8 random variables in @xmath7-norm :    for every @xmath28 and @xmath29 , there exists a randomized algorithm that computes , given a sequence @xmath30 of @xmath8-tuples , in one pass and using @xmath31 memory bits , a number @xmath6 so that the probability @xmath6 deviates from the square of the @xmath7 distance between product and joint distribution by more than a factor of @xmath32 is at most @xmath33 .",
    "this section is devoted to prove lemma [ lem : main_lemma ] , where the main challenge is to bound the variance of @xmath6 . the geometric approach of indyk and mcgregor @xcite presented in section [ sec : kis2 ] for the case of @xmath60",
    "can be extended to analyze the general case .",
    "however , we remark that the generalization requires new ideas .",
    "in particular , instead of performing `` local analysis '' that maps each rectangle to its diagonals , a more complex `` global analysis '' is needed in higher dimensions to achieve the desired bounds .",
    "the alternative proof we present here utilizes similar ideas , but relies on a more combinatorial rather than geometric approach .    for the expectation of @xmath6",
    ", we have @xmath193 & = & \\mathrm e\\left[\\sum_{{{\\mathbf p } } , { { \\mathbf q } } \\in [ n]^k}{{\\vec{v } } } _ { { { \\mathbf p } } } \\cdot { { \\vec{v } } } _ { { { \\mathbf q } } } \\cdot h({{\\mathbf p } } ) \\cdot h({{\\mathbf q } } ) \\right ] \\\\ & = & \\sum_{{{\\mathbf p } } \\in [ n]^k}\\vec v_{{{\\mathbf p } } } ^2 \\cdot \\mathrm e\\left[h({{\\mathbf p } } ) ^2\\right ] + \\sum_{{{\\mathbf p } } \\neq { { \\mathbf q } } \\in [ n]^k}{{\\vec{v } } } _ { { { \\mathbf p } } } \\cdot { { \\vec{v } } } _ { { { \\mathbf q } } } \\cdot \\mathrm e\\left[h({{\\mathbf p } } ) h({{\\mathbf q } } ) \\right ] \\\\ & = & \\sum_{{{\\mathbf p } } \\in [ n]^k}\\vec v_{{{\\mathbf p } } } ^2 = ||{{\\vec{v } } } ||^2,\\end{aligned}\\ ] ] where the last equality follows by @xmath194 , and @xmath195 = 0 $ ] for @xmath196 .",
    "now , let us start to prove @xmath184 \\leq 3^k \\mathrm e[y]^2 $ ] . by definition ,",
    "@xmath184 = \\mathrm e [ ( y- { { \\mathrm{e } } } [ y])^2]$ ] , so we need to understand the following random variable : @xmath197 = \\sum_{{{\\mathbf p } } \\neq { { \\mathbf q } } \\in [ n]^k } h({{\\mathbf p } } ) h({{\\mathbf q } } ) { { \\vec{v } } } _ { { { \\mathbf p } } } { { \\vec{v } } } _ { { { \\mathbf q } } } .\\ ] ] the random variable @xmath198 is a sum of terms indexed by pairs @xmath199^k \\times [ n]^k$ ] with @xmath196 . at a very high level ,",
    "our analysis consists of two steps .",
    "in the first step , we group the terms in @xmath198 properly and simplify the summation in each group . in the second step ,",
    "we expand the square of the sum in @xmath184 = \\mathrm e[err^2]$ ] according to the groups and apply cauchy - schwartz inequality three times to bound the variance .    we shall now gradually introduce the necessary notation for grouping the terms in @xmath198 and simplifying the summation .",
    "we remind the reader that vectors over the reals ( e.g. , @xmath200 ) are denoted by @xmath201 , and vectors over @xmath1 $ ] are denoted by @xmath202 and referred as _",
    "index vectors_. we use @xmath203 $ ] to denote a subset of @xmath204 $ ] , and let @xmath205 \\backslash s$ ] .",
    "we use @xmath206 to denote the _ hamming distance _ of index vectors @xmath207^k$ ] , i.e. , the number of coordinates where @xmath208 and @xmath209 are different .",
    "[ def : projection ] _",
    "( projection and inverse projection ) _ let @xmath210^k$ ] be an index vector and @xmath211 $ ] a subset .",
    "we define the _ projection of @xmath212 to @xmath34 _ , denoted by @xmath213^{|s|}$ ] , to be the vector @xmath212 restricted to the coordinates in @xmath34 . also , let @xmath214^{|s|}$ ] and @xmath215^{k - |s|}$ ] be index vectors .",
    "we define the _ inverse projection of @xmath216 and @xmath217 with respect to @xmath34 _ , denoted by @xmath218^k$ ] , as the index vector @xmath210^k$ ] such that @xmath219 and @xmath220 .",
    "we next define _ pair groups _ and use the definition to group the terms in @xmath198 .    [ def : pairs projection ] _",
    "( pair group ) _ let @xmath211 $ ] be a subset of size @xmath221 .",
    "let @xmath222^t$ ] be a pair of index vectors with @xmath223 ( i.e. , all coordinates of @xmath212 and @xmath224 are distinct . ) .",
    "the _ pair group _",
    "@xmath225 is the set of pairs @xmath199^k \\times [ n]^k$ ] such that ( i ) on coordinate @xmath34 , @xmath226 and @xmath227 , and ( ii ) on coordinate @xmath228 , @xmath208 and @xmath209 are the same , i.e. , @xmath229 .",
    "namely , @xmath230^k \\times [ n]^k : \\big({{\\mathbf c } } = \\phi_s({{\\mathbf p } } ) \\big ) \\wedge \\big({{\\mathbf d } } = \\phi_s({{\\mathbf q } } ) \\big ) \\wedge \\big(\\phi_{{{\\bar{s } } } } ( { { \\mathbf p } } ) = \\phi_{{{\\bar{s } } } } ( { { \\mathbf q } } ) \\big)\\right\\}.\\ ] ]    to give some intuition for the above definitions , we note that for every @xmath214^{|{{\\bar{s } } } |}$ ] , there is a unique pair @xmath231 with @xmath232 , and so @xmath233 . on the other hand , for every pair @xmath199^k \\times [ n]^k$ ] with @xmath196 ,",
    "there is a unique non - emtpy @xmath211 $ ] such that @xmath208 and @xmath209 are distinct on exactly coordinates in @xmath34 .",
    "therefore , @xmath234 belongs to exactly one pair group @xmath235 .",
    "it follows that we can partition the summation in @xmath198 according to the pair groups : @xmath236 \\\\s \\neq \\emptyset}}\\ \\ \\ \\ \\",
    "\\sum_{\\substack{{{\\mathbf c } } , { { \\mathbf d } } \\in [ n]^{|s|},\\\\ \\mathrm{ham}({{\\mathbf c } } , { { \\mathbf d } } ) = |s| } } \\ \\ \\ \\ \\",
    "\\sum_{\\substack{({{\\mathbf p } } , { { \\mathbf q } } ) \\in \\\\ \\sigma_s({{\\mathbf c } } , { { \\mathbf d } } ) } } h({{\\mathbf p } } ) h({{\\mathbf q } } ) { { \\vec{v } } } _ { { { \\mathbf p } } } { { \\vec{v } } } _ { { { \\mathbf q } } } .\\ ] ]    we next observe that for any pair @xmath231 , since @xmath208 and @xmath209 agree on coordinates in @xmath228 , the value of the product @xmath237 depends only on @xmath34 , @xmath212 and @xmath224 .",
    "more precisely , @xmath238 } h_i(p_i ) h_i(q_i ) = \\left ( \\prod_{i\\in s } h_i(p_i ) h_i(q_i ) \\right ) \\cdot \\left ( \\prod_{i\\in { { \\bar{s } } } } h_i(p_i)^2 \\right ) = \\prod_{i\\in s } h_i(p_i ) h_i(q_i),\\ ] ] which depends only on @xmath34 , @xmath212 and @xmath224 since @xmath226 and @xmath227 .",
    "this motivates the definition of _ projected hashing_.    [ def : projectedhashing ] _",
    "( projected hashing ) _ let @xmath239 be a subset of @xmath204 $ ] , where @xmath240 .",
    "let @xmath210^t$ ] .",
    "we define the _ projected hashing _ @xmath241 .",
    "we can now translate the random variable @xmath198 as follows : @xmath242 \\\\s \\neq \\emptyset}}\\ \\ \\ \\ \\",
    "\\sum_{\\substack{{{\\mathbf c } } , { { \\mathbf d } } \\in [ n]^{|s|},\\\\ \\mathrm{ham}({{\\mathbf c } } , { { \\mathbf d } } ) = |s| } } \\left ( h_s({{\\mathbf c } } ) h_s({{\\mathbf d } } )   \\sum_{\\substack{({{\\mathbf p } } , { { \\mathbf q } } ) \\in \\\\ \\sigma_s({{\\mathbf c } } , { { \\mathbf d } } ) } } { { \\vec{v } } } _ { { { \\mathbf p } } } { { \\vec{v } } } _ { { { \\mathbf q } } } \\right).\\ ] ]    fix a pair group @xmath235 , we next consider the sum @xmath243 . recall that for every @xmath214^{|{{\\bar{s } } } |}$ ] , there is a unique pair @xmath231 with @xmath232 .",
    "the sum can be viewed as the inner product of two vectors of dimension @xmath244 with entries indexed by @xmath214^{|{{\\bar{s } } } |}$ ] . to formalize this observation",
    ", we introduce the definition of _ hyper - projection _ as follows .",
    "[ def : hyperproj ] _",
    "( hyper - projection ) _ let @xmath200 , @xmath211 $ ] , and @xmath245^{|s|}$ ] .",
    "the _ hyper - projection _ @xmath246 of @xmath179 ( with respect to @xmath34 and @xmath212 ) is a vector @xmath247 in @xmath186^{k - |s|}}$ ] such that @xmath248 for all @xmath249^{k - |s|}$ ] .",
    "using the above definition , we continue to rewrite the @xmath198 as @xmath250 \\\\s \\neq \\emptyset}}\\ \\ \\ \\ \\   \\sum_{\\substack{{{\\mathbf c } } , { { \\mathbf d } } \\in [ n]^{|s|},\\\\ \\mathrm{ham}({{\\mathbf c } } , { { \\mathbf d } } ) = |s| } }   h_s({{\\mathbf c } } ) h_s({{\\mathbf d } } ) \\cdot \\langle \\upsilon_{s,{{\\mathbf c } } } ( { { \\vec{v } } } ) ,    \\upsilon_{s,{{\\mathbf d } } } ( { { \\vec{v } } } ) \\rangle .\\ ] ]    finally , we consider the product @xmath251 again and introduce the following definition to further simplify the @xmath198 .",
    "[ def : similar pairs ] _ ( similarity and dominance ) _ let @xmath252 be a positive integer .    * two pairs of index vectors @xmath253^t \\times [ n]^t$ ] and @xmath254^t \\times [ n]^t$ ] are _ similar _ if for all @xmath255 $ ] , the two sets @xmath256 and @xmath257 are equal .",
    "we denote this as @xmath258 .",
    "* let @xmath212 and @xmath249^t$ ] be two index vectors .",
    "we say @xmath212 _ is dominated by _",
    "@xmath224 if @xmath259 for all @xmath260 $ ] .",
    "we denote this as @xmath261 .",
    "note that @xmath262 .",
    "now , note that if @xmath258 , then @xmath263 since the value of the product @xmath251 depends on the values @xmath256 only as a set .",
    "it is also not hard to see that @xmath264 is an equivalence relation , and for every equivalent class @xmath265 $ ] , there is a unique @xmath266 $ ] with @xmath261 .",
    "therefore , we can further rewrite the @xmath198 as @xmath267 \\\\s \\neq \\emptyset}}\\   \\sum_{{{\\mathbf c } } \\prec { { \\mathbf d } } \\in [ n]^{|s| } }    h_s({{\\mathbf c } } ) h_s({{\\mathbf d } } ) \\cdot \\left ( \\sum_{({{\\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } \\langle \\upsilon_{s,{{\\mathbf a } } } ( { { \\vec{v } } } ) ,    \\upsilon_{s,{{\\mathbf b } } } ( { { \\vec{v } } } ) \\rangle \\right).\\ ] ]    we are ready to bound the term @xmath268 $ ] by expanding the square of the sum according to equation ( [ eq : final_err ] ) .",
    "we first show in lemma [ lem : vanish ] below that all the cross terms in the following expansion vanish .",
    "@xmath269 = \\sum_{\\substack{s , s ' \\subseteq [ k ] \\\\s , s ' \\neq \\emptyset}}\\   \\sum_{\\substack{{{\\mathbf c } } \\prec { { \\mathbf d } } \\in [ n]^{|s| } \\\\   { { \\mathbf c } } ' \\prec { { \\mathbf d } } ' \\in [ n]^{|s| ' } } } { { \\mathrm{e } } } [ h_s({{\\mathbf c } } ) h_s({{\\mathbf d } } ) h_{s'}({{\\mathbf c } } ' ) h_{s'}({{\\mathbf d } } ' ) ]   \\cdot $ $ $ $ \\left[\\left(\\sum_{({{\\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } \\langle \\upsilon_{s,{{\\mathbf a } } } ( { { \\vec{v } } } ) ,    \\upsilon_{s,{{\\mathbf b } } } ( { { \\vec{v } } } ) \\rangle \\right ) \\left(\\sum_{({{\\mathbf a } } ' , { { \\mathbf b } } ' ) \\sim ( { { \\mathbf c } } ' , { { \\mathbf d } } ' ) } \\langle \\upsilon_{s',{{\\mathbf a } } ' } ( { { \\vec{v } } } ) ,    \\upsilon_{s',{{\\mathbf b } } ' } ( { { \\vec{v } } } ) \\rangle \\right ) \\right].\\ ] ]    [ lem : vanish ] let @xmath34 and @xmath270 be subsets of @xmath204 $ ] , and @xmath271^{|s|}$ ] and @xmath272^{|s'|}$ ] index vectors .",
    "we have @xmath273 \\in \\{0,1\\}$ ] .",
    "furthermore , we have    @xmath274   = 1 $ ] iff @xmath275 .",
    "recall that @xmath276 are independent copies of @xmath0-wise independent uniform random variables over @xmath277 .",
    "namely , for every @xmath22 $ ] , @xmath278 are @xmath0-wise independent , and @xmath23 are mutually independent .",
    "observe that for every @xmath22 $ ] , there are at most @xmath0 terms out of @xmath278 appearing in the product @xmath279 .",
    "it follows that all distinct terms appearing in @xmath279 are mutually independent uniform random variable over @xmath56 .",
    "therefore , the expectation is either 0 , if there is some @xmath280 that appears an odd number of times , or 1 , if all @xmath280 appear an even number of times . by inspection",
    ", the latter case happens if and only if @xmath275 .    by the above lemma , equation ( [ eq : expand_var ] )",
    "is simplified to @xmath281 = \\sum_{\\substack{s \\subseteq [ k]\\\\ s \\neq \\emptyset}}\\   \\sum_{{{\\mathbf c } } \\prec { { \\mathbf d } } \\in [ n]^{|s| } }   \\left ( \\sum_{({{\\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } \\langle \\upsilon_{s,{{\\mathbf a } } } ( { { \\vec{v } } } ) ,    \\upsilon_{s,{{\\mathbf b } } } ( { { \\vec{v } } } ) \\rangle\\right)^2.\\ ] ]    we next apply the cauchy - schwartz inequality three times to bound the above formula . consider a subset @xmath211 $ ] and a pair @xmath271^{|s|}$ ] .",
    "note that there are precisely @xmath282 pairs @xmath283 such that @xmath284 .",
    "thus , by the cauchy - schwartz inequality : @xmath285^{|s| } \\\\({{\\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } } \\langle\\upsilon_{s,{{\\mathbf a } } } ( { { \\vec{v } } } ) , \\upsilon_{s,{{\\mathbf b } } } ( \\vec v)\\rangle\\right)^2 & \\leq & 2^{|s| } \\sum_{\\substack{({{\\mathbf a } } , { { \\mathbf b } } ) \\in [ n]^{|s|}\\\\ ( { { \\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } } ( \\langle\\upsilon_{s,{{\\mathbf a } } } , \\upsilon_{s,{{\\mathbf b } } } \\rangle)^2 \\\\   & \\leq & 2^{|s|}\\sum_{\\substack{({{\\mathbf a } } , { { \\mathbf b } } ) \\in [ n]^{|s|}\\\\ ( { { \\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } } \\langle\\upsilon_{s,{{\\mathbf a } } } ( { { \\vec{v } } } ) , \\upsilon_{s,{{\\mathbf a } } } ( { { \\vec{v } } } ) \\rangle\\cdot\\langle\\upsilon_{s,{{\\mathbf b } } } , \\upsilon_{s , { { \\mathbf b } } } ( { { \\vec{v } } } ) \\rangle.\\end{aligned}\\ ] ] notice that in the second inequality , we applied cauchy - schwartz in a component - wise manner .",
    "next , for a subset @xmath211 $ ] , we can apply the cauchy - schwartz inequality a third time ( from the third line to the fourth line ) as follows : @xmath286^{|s| } } \\ \\ \\ \\ \\",
    "\\left(\\sum_{\\substack{({{\\mathbf a } } , { { \\mathbf b } } ) \\in [ n]^{|s|}\\\\ ( { { \\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } } \\langle\\upsilon_{s,{{\\mathbf a } } } ( \\vec v ) , \\upsilon_{s,{{\\mathbf b } } } ( \\vec v)\\rangle\\right)^2 \\\\ & \\leq & 2^{|s|}\\sum_{{{\\mathbf c } } \\prec { { \\mathbf d } } \\in [ n]^{|s| } } \\ \\ \\ \\sum_{\\substack{({{\\mathbf a } } , { { \\mathbf b } } ) \\in [ n]^{|s|}\\\\ ( { { \\mathbf a } } , { { \\mathbf b } } ) \\sim ( { { \\mathbf c } } , { { \\mathbf d } } ) } } \\langle\\upsilon_{s,{{\\mathbf a } } } ( \\vec v ) , \\upsilon_{s,{{\\mathbf a } } } ( \\vec v)\\rangle\\cdot\\langle\\upsilon_{s,{{\\mathbf b } } } ( \\vec v ) , \\upsilon_{s , { { \\mathbf b } } } ( \\vec v)\\rangle \\\\ & = & 2^{|s|}\\sum_{\\substack{{{\\mathbf c } } , { { \\mathbf d } } \\in [ n]^{|s|}\\\\ \\mathrm{ham}({{\\mathbf c } } , { { \\mathbf d } } ) = |s| } } \\langle\\upsilon_{s,{{\\mathbf c } } } ( \\vec v ) , \\upsilon_{s,{{\\mathbf c } } } ( \\vec v)\\rangle\\cdot\\langle\\upsilon_{s , { { \\mathbf d } } } ( \\vec v ) , \\upsilon_{s , { { \\mathbf d } } } ( \\vec v)\\rangle \\\\ & \\leq & 2^{|s|}\\sum_{{{\\mathbf c } } , { { \\mathbf d } } \\in [ n]^{|s| } } \\langle\\upsilon_{s,{{\\mathbf c } } } ( \\vec v ) , \\upsilon_{s,{{\\mathbf c } } } ( \\vec v)\\rangle\\cdot\\langle\\upsilon_{s , { { \\mathbf d } } } ( \\vec v ) , \\upsilon_{s , { { \\mathbf d } } } ( \\vec v)\\rangle \\\\ & = & 2^{|s|}\\left(\\sum_{{{\\mathbf c } } \\in [ n]^{|s| } } \\langle\\upsilon_{s,{{\\mathbf c } } } ( \\vec v ) , \\upsilon_{s,{{\\mathbf c } } } ( \\vec v)\\rangle\\right)^2.\\end{aligned}\\ ] ]    finally , we note that by definition , we have @xmath287^{|s| } } \\langle\\upsilon_{s,{{\\mathbf c } } } ( \\vec v ) , \\upsilon_{s,{{\\mathbf c } } } ( \\vec v)\\rangle = ||{{\\vec{v } } } ||^2 $ ] , which equals to @xmath100 $ ] .",
    "it follows that the variance in equation ( [ eq : simplified_var ] ) can be bounded by @xmath288 \\leq \\sum_{s \\subseteq [ k ] , s \\neq \\emptyset } 2^{|s| } \\cdot \\mathrm e[y]^2 = \\mathrm e[y]^2 \\sum_{i=1}^k { k \\choose i}2^i = ( 3^k-1)\\mathrm e[y]^2,\\ ] ] which finishes the proof of lemma [ lem : main_lemma ] .",
    "there remain several open questions left in this space",
    ". lower bounds , particularly bounds that depend non - trivially on the dimension @xmath8 , would be useful .",
    "there may still be room for better algorithms for testing @xmath8-wise independence in this manner using the @xmath7 norm .",
    "a natural generalization would be to find a particularly efficient algorithm for testing @xmath8-out - of-@xmath289-wise independence ( other than handling each set of @xmath8 variable separately ) . more generally , a question given in @xcite , to identify random variables whose correlation exceeds some threshold according to some measure , remains widely open ."
  ],
  "abstract_text": [
    "<S> in their seminal work , alon , matias , and szegedy introduced several sketching techniques , including showing that @xmath0-wise independence is sufficient to obtain good approximations of the second frequency moment . in this work , </S>",
    "<S> we show that their sketching technique can be extended to product domains @xmath1^k$ ] by using the product of @xmath0-wise independent functions on @xmath1 $ ] . </S>",
    "<S> our work extends that of indyk and mcgregor , who showed the result for @xmath2 . </S>",
    "<S> their primary motivation was the problem of identifying correlations in data streams . </S>",
    "<S> in their model , a stream of pairs @xmath3 ^ 2 $ ] arrive , giving a joint distribution @xmath4 , and they find approximation algorithms for how close the joint distribution is to the product of the marginal distributions under various metrics , which naturally corresponds to how close @xmath5 and @xmath6 are to being independent . by using our technique , </S>",
    "<S> we obtain a new result for the problem of approximating the @xmath7 distance between the joint distribution and the product of the marginal distributions for @xmath8-ary vectors , instead of just pairs , in a single pass . </S>",
    "<S> our analysis gives a randomized algorithm that is a @xmath9 approximation ( with probability @xmath10 ) that requires space logarithmic in @xmath11 and @xmath12 and proportional to @xmath13 .    </S>",
    "<S> vladimir braverman    kai - min chung    zhenming liu    michael mitzenmacher    rafail ostrovsky </S>"
  ]
}