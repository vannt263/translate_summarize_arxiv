{
  "article_text": [
    "sparse representation encodes a signal as a sparse linear combination of redundant basis vectors . with its inspirational roots in human vision system @xcite , @xcite ,",
    "this technique has been successfully employed in image restoration @xcite , @xcite , @xcite , compressive sensing @xcite , @xcite and morphological component analysis  @xcite .",
    "more recently , sparse representation based approaches have also shown promising results in face recognition and gender classification @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , texture and handwritten digit classification @xcite , @xcite , @xcite , @xcite , natural image and object classification @xcite , @xcite , @xcite and human action recognition @xcite , @xcite , @xcite , @xcite .",
    "the success of these approaches comes from the fact that a sample from a class can generally be well represented as a sparse linear combination of the other samples from the same class , in a lower dimensional manifold  @xcite . for classification ,",
    "a discriminative sparse representation approach first encodes the test instance over a dictionary , i.e. a redundant set of basis vectors , known as atoms .",
    "therefore , an effective dictionary is critical for the performance of such approaches .",
    "it is possible to use an off - the - shelf basis ( e.g. fast fourier transform  @xcite or wavelets  @xcite ) as a generic dictionary to represent data from different domains / classes .",
    "however , research in the last decade (  @xcite , @xcite , @xcite,@xcite , @xcite , @xcite , @xcite , @xcite ) has provided strong evidence in favor of learning dictionaries using the domain / class - specific training data , especially for classification and recognition tasks  @xcite where class label information of the training data can be exploited in the supervised learning of a dictionary .",
    "whereas unsupervised dictionary learning approaches ( e.g. k - svd  @xcite , method of optimal directions @xcite ) aim at learning faithful signal representations , supervised sparse representation additionally strives for making the dictionaries discriminative .",
    "for instance , in sparse representation based classification ( src ) scheme , wright et al .",
    "@xcite constructed a discriminative dictionary by directly using the training data as the dictionary atoms . with each atom associated to a particular class ,",
    "the query is assigned the label of the class whose associated atoms maximally contribute to the sparse representation of the query .",
    "impressive results have been achieved for recognition and classification using src , however , the computational complexity of this technique becomes prohibitive for large training data .",
    "this has motivated considerable research on learning discriminative dictionaries that would allow sparse representation based classification with much lower computational cost . in order to learn a discriminative dictionary , existing approaches either force subsets of the dictionary atoms to represent data from only specific classes @xcite , @xcite , @xcite or they associate the complete dictionary to all the classes and constrain their sparse coefficient to be discriminative @xcite , @xcite , @xcite .",
    "a third category of techniques learns exclusive sets of class specific and common dictionary atoms to separate the common and particular features of the data from different classes @xcite , @xcite .",
    "establishing association between the dictionary atoms and the corresponding class labels is a key step of existing methods .",
    "however , adaptively building this association is still an open research problem  @xcite .",
    "moreover , the strategy of assigning different number of dictionary atoms to different classes and adjusting the overall size of the dictionary become critical for the classification accuracy of the existing approaches , as no principled approach is generally provided to predetermine these parameters .",
    "0.99      +    0.99     in this work , we propose a solution to this problem by approaching the sparse representation based classification from a non - parametric bayesian perspective .",
    "we propose a bayesian sparse representation technique that infers a discriminative dictionary using a beta process  @xcite .",
    "our approach adaptively builds the association between the dictionary atoms and the class labels such that this association signifies the probability of selection of the dictionary atoms in the expansion of class - specific data .",
    "furthermore , the non - parametric character of the approach allows it to automatically infer the correct size of the dictionary .",
    "the scheme employed by our approach is shown in fig .",
    "[ fig : schema ] .",
    "we perform bayesian inference over a model proposed for discriminative sparse representation of the training data .",
    "the inference process learns distributions over the dictionary atoms and sets of bernoulli distributions associating the dictionary atoms to the labels of the data .",
    "the bernoulli distributions govern the support of the final sparse codes and are later utilized in learning a multi - class linear classifier .",
    "the final dictionary is learned by sampling the distributions over the dictionary atoms and the corresponding sparse codes are computed by element - wise product of the support and the inferred weights of the codes . the computed dictionary and",
    "the sparse codes also represent the training data faithfully .",
    "a query is classified in our approach by first sparsely encoding it over the inferred dictionary and then classifying its sparse code with the learned classifier . in this work",
    ", we learn the classifier and the dictionary using the same hierarchical bayesian model .",
    "this allows us to exploit the aforementioned bernoulli distributions in the accurate estimate of the classifier .",
    "we present the proposed bayesian model along its inference equations for gibbs sampling .",
    "our approach has been tested on two face - databases  @xcite ,  @xcite , an object - database  @xcite , an action - database  @xcite and a scene - database  @xcite .",
    "the classification results are compared with the state - of - the - art discriminative sparse representation approaches .",
    "the proposed approach not only outperforms these approaches in terms of accuracy , its computational efficiency for the classification stage is also comparable to the most efficient existing approaches .",
    "this paper is organized as follows .",
    "we review the related work in section [ sec : rw ] of the paper . in section",
    "[ sec : pfnb ] , we formulate the problem and briefly explain the relevant concepts that clarify the rationale behind our approach .",
    "the proposed approach is presented in section [ sec : pa ] , which includes details of the proposed model , the gibbs sampling process , the classification scheme and the initialization of the proposed approach .",
    "experimental results are reported in section  [ sec : e ] and a discussion on the parameter settings is provided in section  [ sec : d ] .",
    "we draw conclusions in section [ sec : conc ] .",
    "there are three main categories of the approaches that learn discriminative sparse representation . in the first category ,",
    "the learned dictionary atoms have direct correspondence to the labels of the classes @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite .",
    "yang et al .",
    "@xcite proposed an src like framework for face recognition , where the atoms of the dictionary are learned from the training data instead of directly using the training data as the dictionary . in order to learn a dictionary that is simultaneously discriminative and reconstructive , mairal et al .",
    "@xcite used a discriminative penalty term in the k - svd model @xcite , achieving state - of - the - art results on texture segmentation .",
    "sprechmann and sapiro  @xcite also proposed to learn dictionaries and sparse codes for clustering . in @xcite , castrodad and sapiro computed class - specific dictionaries for actions .",
    "the dictionary atoms and their sparse coefficients also exploited the non - negativity of the signals in their approach .",
    "active basis models are learned from the training images of each class and applied to object detection and recognition in @xcite .",
    "ramirez et al .",
    "@xcite have used an incoherence promoting term for the dictionary atoms in their learning model .",
    "encouraging incoherence among the class - specific sub - dictionaries allowed them to represent samples from the same class better than the samples from the other classes .",
    "wang et al .",
    "@xcite have proposed to learn class - specific dictionaries for modeling individual actions for action recognition .",
    "their model incorporated a similarity constrained term and a dictionary incoherence term for classification .",
    "the above mentioned methods mainly associate a dictionary atom directly to a single class .",
    "therefore , a query is generally assigned the label of the class whose associated atoms result in the minimum representational error for the query .",
    "the classification stages of the approaches under this category often require the computation of representations of the query over many sub - dictionaries .    in the second category of the approaches for discriminative sparse representation ,",
    "a single dictionary is shared by all the classes , however the representation coefficients are forced to be discriminative ( @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) .",
    "jiang et al.@xcite proposed a dictionary learning model that encourages the sparse representation coefficients of the same class to be similar .",
    "this is done by adding a discriminative sparse - code error constraint to a unified objective function that already contains reconstruction error and classification error constraints .",
    "a similar approach is taken by rodriguez and sapiro @xcite where the authors solve for a simultaneous sparse approximation problem @xcite while learning the coefficients .",
    "it is common to learn dictionaries jointly with a classifier .",
    "pham and venkatesh @xcite and mairal et al .",
    "@xcite proposed to train linear classifiers along the joint dictionaries learned for all the classes .",
    "zhang and li @xcite enhanced the k - svd algorithm  @xcite to learn a linear classifier along the dictionary . a task",
    "driven dictionary learning framework has also been proposed @xcite . under this framework ,",
    "different risk functions of the representation coefficients are minimized for different tasks . broadly speaking , the above mentioned approaches",
    "aim at learning a single dictionary together with a classifier .",
    "the query is classified by directly feeding its sparse codes over the learned single dictionary to the classifier .",
    "thus , in comparison to the approaches in the first category , the classification stage of these approaches is computationally more efficient . in terms of learning a single dictionary for the complete training data and the classification stage",
    ", the proposed approach also falls under this category of discriminative sparse representation techniques .",
    "the third category takes a hybrid approach for learning the discriminative sparse representation . in these approaches ,",
    "the dictionaries are designed to have a set of shared atoms in addition to class - specific atoms .",
    "deng et al .",
    "@xcite extended the src algorithm by appending an intra - class face variation dictionary to the training data .",
    "this extension achieves promising results in face recognition with a single training sample per class .",
    "zhou and fan @xcite employ a fisher - like regularizer on the representation coefficients while learning a hybrid dictionary .",
    "wang and kong @xcite learned a hybrid dictionary to separate the common and particular features of the data .",
    "their approach additionally encouraged the class - specific dictionaries to be incoherent during the optimization process .",
    "shen et al .",
    "@xcite proposed to learn a multi - level dictionary for hierarchical visual categorization . to some extent , it is possible to reduce the size of the dictionary using the hybrid approach , which also results in reducing the classification time in comparison to the approaches that fall under the first category .",
    "however , it is often non - trivial to decide on how to balance between the shared and the class - specific parts of the hybrid dictionary @xcite , @xcite .",
    "let @xmath0 \\in \\mathbb r^{m\\times n}$ ] be the training data comprising @xmath1 instances from @xmath2 classes , wherein @xmath3 represents the data from the @xmath4 class and @xmath5 .",
    "the columns of @xmath6 are indexed in @xmath7 .",
    "we denote a dictionary by @xmath8 with atoms @xmath9 , where @xmath10 and @xmath11 represents the cardinality of the set .",
    "let @xmath12 be the sparse code matrix of the data , such that @xmath13 .",
    "we can write @xmath14 $ ] , where @xmath15 is the sub - matrix related to the @xmath16 class .",
    "the @xmath17 column of @xmath18 is denoted as @xmath19 . to learn a sparse representation of the data",
    ", we can solve the following optimization problem : @xmath20 where @xmath21 is a predefined constant , @xmath22 computes the frobenius norm and @xmath23 denotes the @xmath24-norm of a vector .",
    "generally , @xmath25 is chosen to be @xmath26 or @xmath27 for sparsity  @xcite .",
    "the non - convex optimization problem of eq .",
    "( @xmath28 ) can be iteratively solved by fixing one parameter and solving a convex optimization problem for the other parameter in each iteration .",
    "the solution to eq .",
    "( [ eq : gen ] ) , factors the training data @xmath29 into two complementary matrices , namely the dictionary and the sparse codes , without considering the class label information of the training data .",
    "nevertheless , we can still exploit this factorization in classification tasks by using the sparse codes of the data as features  @xcite , for which , a classifier can be obtained as @xmath30 where @xmath31 contains the model parameters of the classifier , @xmath32 is the loss function , @xmath33 is the label of the @xmath17 training instance @xmath34 and @xmath35 is the regularization parameter .",
    "it is usually suboptimal to perform classification based on sparse codes learned by an unsupervised technique . considering this , existing approaches @xcite , @xcite , @xcite",
    ", @xcite proposed to jointly optimize a classifier with the dictionary while learning the sparse representation .",
    "one intended ramification of this approach is that the label information also gets induced into the dictionary .",
    "this happens when the information is utilized in computing the sparse codes of the data , which in turn , are used for computing the dictionary atoms , while solving eq .",
    "( [ eq : gen ] ) .",
    "this results in improving the discriminative abilities of the learned dictionary .",
    "jiang et al .",
    "@xcite built further on this concept and encouraged explicit correspondence between the dictionary atoms and the class - labels .",
    "more precisely , the following optimization problem is solved by the label - consistent k - svd ( lc - ksvd2 ) algorithm  @xcite : @xmath36 where @xmath37 and @xmath38 are the regularization parameters , the binary matrix @xmath39 contains the class label information training instance , the @xmath17 column of @xmath40 has @xmath27 appearing only at the index corresponding to the class label .",
    "] , @xmath41 is the transformation between the sparse codes and the _ discriminative sparse codes _ @xmath42 . here",
    ", for the @xmath17 training instance , the @xmath17 column of the fixed binary matrix @xmath43 has @xmath27 appearing at the @xmath44 index only if the @xmath44 dictionary atom has the same class label as the training instance .",
    "thus , the discriminative sparse codes form a pre - defined relationship between the dictionary atoms and the class labels .",
    "this brings improvement to the discriminative abilities of the dictionary learned by solving eq .",
    "( [ eq : lcksvd ] ) .",
    "it is worth noting that in label - consistent k - svd algorithm  @xcite , the relationship between class - specific subsets of dictionary atoms and class labels is pre - defined .",
    "however , regularization allows flexibility in this association during optimization .",
    "we also note that using @xmath45 in eq .",
    "( [ eq : lcksvd ] ) reduces the optimization problem to the one solved by discriminative k - svd ( d - ksvd ) algorithm  @xcite .",
    "successful results are achievable using the above mentioned techniques for recognition and classification . however , like any discriminative sparse representation approach , these results are obtainable only after careful optimization of the algorithm parameters , including the dictionary size . in fig .  [",
    "fig : mok ] , we illustrate the behavior of recognition accuracy under varying dictionary sizes for @xcite and @xcite for two face databases .    0.39     0.39     paisley and carin  @xcite developed a _ beta process _ for non - parametric factor analysis , which was later used by zhou et al .",
    "@xcite in successful image restoration and compressive sensing . exploiting the non - parametric bayesian framework , a beta process can automatically infer the factor / dictionary size from the training data . with the base measure @xmath46 and parameters @xmath47 and @xmath48 , a beta process is denoted by bp@xmath49 .",
    "a draw from this process , i.e. @xmath50 , can be represented as @xmath51 with this a valid measure as @xmath52 . in the above equation ,",
    "@xmath53 is @xmath27 when @xmath54 and @xmath26 otherwise .",
    "therefore , @xmath55 can be represented as a set of @xmath56 probabilities , each having an associated vector @xmath9 , drawn @xmath57 from the base measure @xmath46 .",
    "using @xmath55 , we can draw a binary vector @xmath58 , such that the @xmath44 component of this vector is drawn @xmath59 . by independently drawing @xmath1 such vectors",
    ", we may construct a matrix @xmath60 , where @xmath61 is the @xmath17 column of this matrix . using the above mentioned beta process",
    ", it is possible to factorize @xmath62 as follows : @xmath63 where , @xmath64 has @xmath9 as its columns and @xmath65 is the error matrix . in eq .",
    "( [ eq : basic ] ) , the number of non - zero components in a column of @xmath66 is a random number drawn from poisson@xmath67 @xcite .",
    "thus , sparsity can be imposed on the representation with the help of parameters @xmath68 and @xmath69 .",
    "the components of the @xmath44 row of @xmath66 are independent draws from bernoulli@xmath70 .",
    "let @xmath71 be a vector with @xmath72 , as its @xmath44 element .",
    "this vector governs the probability of selection of the columns of @xmath73 in the expansion of the data .",
    "existence of this physically meaningful latent vector in the beta process based matrix factorization plays a central role in the proposed approach for discriminative dictionary learning .",
    "we propose a discriminative bayesian dictionary learning approach for classification . for the @xmath16 class",
    ", the proposed approach draws @xmath74 binary vectors @xmath75 , @xmath76 using a beta process . for each class",
    ", the vectors are sampled using separate draws with the same base .",
    "that is , the matrix factorization is governed by a set of @xmath2 probability vectors @xmath77 , instead of a single vector , however the inferred dictionary is shared by all the classes .",
    "an element of the aforementioned set , i.e. @xmath78 , controls the probability of selection of the dictionary atoms for a single class data .",
    "this promotes the discriminative abilities of the inferred dictionary .",
    "let @xmath79 denote the sparse code of the @xmath17 training instance of the @xmath16 class , i.e. @xmath80 , over a dictionary @xmath8 .",
    "mathematically , @xmath81 , where @xmath82 denotes the modeling error .",
    "we can directly use the beta process discussed in section [ sec : pfnb ] for computing the desired sparse code and the dictionary .",
    "however , the model employed by the beta process is restrictive , as it only allows the code to be binary . to overcome this restriction ,",
    "let @xmath83 , where @xmath84 denotes the hadamard / element - wise product , @xmath75 is the binary vector and @xmath85 is a weight vector .",
    "we place a standard normal prior @xmath86 on the @xmath44 component of the weight vector @xmath87 , where @xmath88 denotes the precision of the distribution . in here , as in the following text , we use the subscript ` @xmath89 ' to distinguish the parameters of the prior distributions .",
    "the prior distribution over the @xmath44 component of the binary vector is @xmath90 .",
    "we draw the atoms of the dictionary from a multivariate gaussian base , i.e. @xmath91 , where @xmath92 is the mean vector and @xmath93 is the precision matrix for the @xmath44 atom of the dictionary .",
    "we model the error as zero mean gaussian in @xmath94 .",
    "thus , we arrive at the following representation model : @xmath95    notice , in the above model a conjugate beta prior is placed over the parameter of the bernoulli distribution , as mentioned in section [ sec : pfnb ] .",
    "hence , a latent probability vector @xmath96 ( with @xmath97 as its components ) is associated with the dictionary atoms for the representation of the data from the @xmath16 class .",
    "the common dictionary @xmath98 is inferred from @xmath2 such vectors . in the above model",
    ", this fact is notationally expressed by showing the dictionary atoms being sampled from a common set of @xmath56 distributions , while distinguishing the class - specific variables in the other notations with a superscript ` @xmath99 ' .",
    "we assume the same statistics for the modeling error over the complete training data .",
    "we further place non - informative gamma hyper - priors over the precision parameters of the normal distributions .",
    "that is , @xmath100 and @xmath101 , where @xmath102 and @xmath103 are the parameters of the respective gamma distributions . here",
    ", we allow the error to have an isotropic precision , i.e. @xmath104 , where @xmath105 denotes the identity matrix in @xmath106 . the graphical representation of the complete model is shown in fig .",
    "[ fig : gr ] .",
    "gibbs sampling is used to perform bayesian inference over the proposed model .",
    "starting with the dictionary , below we derive analytical expressions for the posterior distributions over the model parameters for the gibbs sampler .",
    "the inference process performs sampling over these posterior distributions .",
    "the expressions are derived assuming zero mean gaussian prior over the dictionary atoms , with isotropic precision .",
    "that is , @xmath107 and @xmath108 .",
    "this simplification leads to faster sampling , without significantly affecting the accuracy of the approach .",
    "the sampling process samples the atoms of the dictionary one - by - one from their respective posterior distributions .",
    "this process is analogous to the atom - by - atom dictionary update step of k - svd  @xcite , however the sparse codes remain fixed during our dictionary update .    *",
    "sampling @xmath9 : * for our model , we can write the following about the posterior distribution over a dictionary atom : @xmath109 here , we intentionally dropped the superscript ` @xmath99 ' as the dictionary is updated using the complete training data .",
    "let @xmath110 denote the contribution of the dictionary atom @xmath9 to the @xmath17 training instance @xmath111 : @xmath112 using eq .",
    "( [ eq : atomic ] ) , we can re - write the aforementioned proportionality as @xmath113 considering the above expression , the posterior distribution over a dictionary atom can be written as @xmath114 where , @xmath115    * sampling @xmath116 : * once the dictionary atoms have been sampled , we sample @xmath116 , @xmath117 , @xmath118 . using the contribution of the @xmath44 dictionary atom ,",
    "the posterior probability distribution over @xmath116 can be expressed as @xmath119 here we are concerned with the @xmath16 class only , therefore @xmath120 is computed with the @xmath16 class data in eq .",
    "( [ eq : atomic ] ) . with the prior probability of @xmath121 given by @xmath122 ,",
    "we can write the following about its posterior probability : @xmath123 it can be shown that the right hand side of the above proportionality can be written as : @xmath124 where , @xmath125 and @xmath126 . furthermore , since the prior probability of @xmath127 is given by @xmath128 , we can write the following about its posterior probability : @xmath129 thus , @xmath116 can be sampled from the following normalized bernoulli distribution : @xmath130 by inserting the value of @xmath131 and simplifying , we finally arrive at the following expression for sampling @xmath116 : @xmath132    * sampling @xmath87 : * we can write the following about the posterior distribution over @xmath87 : @xmath133 again , notice that we are concerned with the @xmath16 class data only . in light of the above expression ,",
    "@xmath87 can be sampled from the following posterior distribution : @xmath134 where , @xmath135    * sampling @xmath97 : * based on our model , we can also write the posterior probability distribution over @xmath97 as @xmath136 using the conjugacy between the distributions , it can be easily shown that the @xmath44 component of @xmath96 must be drawn from the following posterior distribution during the sampling process : @xmath137    * sampling @xmath138 : * in our model , the components of the weight vectors are drawn from a standard normal distribution . for a given weight vector , common priors are assumed over the precision parameters of these distributions .",
    "this allows us to express the likelihood function for @xmath138 in terms of standard multivariate gaussian with isotropic precision .",
    "thus , we can write the posterior over @xmath138 as the following : @xmath139 using the conjugacy between the gaussian and gamma distributions , it can be shown that @xmath138 must be sampled as follows : @xmath140    * sampling @xmath141 : * we can write the posterior over @xmath141 as @xmath142 similar to @xmath138 , we can arrive at the following for sampling @xmath141 during the inferencing process : @xmath143    as a result of bayesian inference over the model , we obtain sets of posterior distributions over the model parameters .",
    "we are particularly interested in two of them .",
    "namely , the set of distributions over the dictionary atoms @xmath144 , and the set of probability distributions characterized by the vectors @xmath145 .",
    "momentarily , we defer the discussion on the latter .",
    "the former is used to compute the desired dictionary @xmath98 .",
    "this is done by drawing multiple samples from the elements of @xmath146 and estimating the corresponding dictionary atoms as respective means of the samples .",
    "indeed , the mean parameters of the elements of @xmath146 can also be chosen as the desired dictionary atoms . however , we prefer the former approach for robustness .    the proposed model and the sampling process also results in inferring the correct size of the desired dictionary .",
    "we present the following lemmas in this regard :    [ lem:1 ] for @xmath52 , @xmath147 = \\frac{a_o}{b_o},\\forall c$ ] , where @xmath148 .     +   according to the proposed model",
    ", the covariance of a data vector from the @xmath16 class can be given by : @xmath149 =   \\frac{a_o k } { a_o + b_o(k-1 ) }   \\frac { \\boldsymbol\\lambda_{k_o}^{-1 } } { \\lambda_{s_o}^c } + \\boldsymbol\\lambda_{\\epsilon_o}^{-1 } \\label{eq : cov}\\end{aligned}\\ ] ] in eq .",
    "( [ eq : cov ] ) , fraction @xmath150 appears due to the presence of @xmath151 in the model and the equation simplifies to @xmath152   = k \\frac { \\boldsymbol\\lambda_{k_o}^{-1 } } { \\lambda_{s_o}^c } + \\boldsymbol\\lambda_{\\epsilon_o}^{-1}$ ] when we neglect @xmath153 . here",
    ", @xmath154 signifies the number of dictionary atoms required to represent the data vector .",
    "notice in the equation , that as @xmath52 , we observe @xmath155 \\rightarrow \\frac{a_o}{b_o } \\frac{\\boldsymbol\\lambda_{k_o}^{-1}}{\\lambda_{s_o}^c } + \\boldsymbol\\lambda^{-1}_{\\epsilon_o}$ ] .",
    "thus , in the limit @xmath52 , @xmath156 corresponds to the expected number of non - zero components in @xmath153 , given by @xmath157 $ ] , where @xmath148 .",
    "[ lem:2 ] once @xmath158 in a given iteration of the sampling process , @xmath159 \\approx 0 $ ] for the later iterations .     + according to eq .",
    "( [ eq : berni ] ) , @xmath160 , @xmath127 when @xmath161 .",
    "once this happens , the posterior distribution over @xmath162 becomes @xmath163 , where @xmath164 and @xmath165 ( see eq .",
    "[ eq : beta ] ) .",
    "thus , the expected value of @xmath97 for the later iterations can be written as @xmath166 = \\frac{\\hat a } { \\hat a + \\hat b } = \\frac{a_o } { a_o + b_o(k-1 ) + k |\\mathcal i_c|}$ ] .",
    "with @xmath167 we can see that @xmath159 \\approx 0 $ ] .",
    "in the gibbs sampling process , we start with @xmath168 in our implementation and let @xmath169 . considering lemma  [ lem:1 ] , the values of @xmath68 and @xmath69 are set to ensure that the resulting representation is sparse .",
    "we drop the @xmath44 dictionary atom during the sampling process if @xmath170 , for all the classes simultaneously .",
    "according to lemma  [ lem:2 ] , dropping such an atom does not bring significant changes to the final representation .",
    "thus , by removing the redundant dictionary atoms in each sampling iteration , we finally arrive at the correct size of the desired dictionary , i.e. @xmath56 .",
    "as mentioned above , with bayesian inference over the proposed model we also infer a set of probability vectors @xmath77 .",
    "each element of this set , i.e. @xmath78 , further characterizes a set of probability distributions @xmath171 . here , @xmath172 is jointly followed by all the @xmath44 components of the sparse codes for the @xmath16 class .",
    "if the @xmath44 dictionary atom is commonly used in representing the @xmath16 class training data , we must expect a high value of @xmath97 , and @xmath173 otherwise .",
    "in other words , for an arranged dictionary , components of @xmath96 having large values should generally cluster well if the learned dictionary is discriminative .",
    "furthermore , these clusters must appear at different locations in the inferred vectors for different classes .",
    "such clusterings would demonstrate the discriminative character of the inferred dictionary . fig .",
    "[ fig : probs ] verifies this character for the dictionaries inferred under the proposed model .",
    "each row of the figure plots six different probability vectors ( i.e. @xmath96 ) for different training datasets .",
    "a clear clustering of the high value components of the vectors is visible in each plot .",
    "detailed experiments are presented in section  [ sec : e ] .",
    "let @xmath174 be a query signal .",
    "we follow the common methodology @xcite , @xcite for classification that first encodes @xmath175 over the inferred dictionary such that @xmath176 , and then computes @xmath177 , where @xmath178 contains model parameters of a multi - class linear classifier .",
    "the query is assigned the class label corresponding to the largest component of @xmath179 .",
    "the main difference between the classification approach of this work and that of the existing techniques is in the learning process of @xmath180 .",
    "whereas discrimination is induced in @xmath98 by the joint optimization of @xmath180 and @xmath98 in the existing techniques ( see eq .",
    "[ eq : lcksvd ] ) , this is already achieved in the inference process of the proposed approach .",
    "thus , it is possible to optimize a classifier separately from the dictionary learning process without affecting the discriminative abilities of the learned dictionary .",
    "let @xmath181 be a binary vector with the only @xmath27 appearing at the @xmath16 index , indicating the class of the training instance @xmath182 .",
    "let @xmath183 be the binary index matrix formed by such vectors for the complete training data @xmath29 .",
    "we aim at computing @xmath180 such that @xmath184 , where @xmath185 denotes the modeling error and @xmath186 is the coefficient matrix .",
    "notice that , we can directly use the model in eq .",
    "( [ eq : mod ] ) to compute @xmath180 . for that , we can write @xmath187 , where @xmath188 is a column of @xmath189 .",
    "thus , we infer @xmath180 under the bayesian framework using the model proposed in eq .",
    "( [ eq : mod ] ) . while learning this matrix , we perform gibbs sampling such that the probability vectors @xmath190 are kept constant to those finally inferred by the dictionary learning stage . that is , wherever required , the value of @xmath97 is directly used from @xmath190 instead of inferring a new value during the sampling process .    the reason for using the same @xmath190 vectors for inferring @xmath180 and @xmath98 is straightforward .",
    "since we first sparse code the query over the learned discriminative dictionary , we expect the underlying support of the learned codes to follow some @xmath96 closely . thus , @xmath180 can be expected to classify the learned codes better if the discriminative information regarding their support is encoded in it .",
    "notice that , unlike the existing approaches ( e.g. @xcite , @xcite ) the coupling between @xmath180 and @xmath98 is kept probabilistic in our approach .",
    "we do not assume that the exact values of the sparse codes of the query would match to those of the training sample ( and hence @xmath180 and @xmath98 should be trained jointly ) , rather , our assumption is that samples from the same class are more likely explainable using similar basis .",
    "therefore , coupling between @xmath180 and @xmath98 is kept in terms of probabilistic selection of their columns .",
    "our view point also makes orthogonal matching pursuit ( omp )  @xcite a natural choice for sparse coding the query over the dictionary .",
    "this greedy pursuit algorithm efficiently searches for the right basis to represent the data .",
    "therefore , we used omp in sparse coding the query over the learned dictionary .      for inferring the dictionary ,",
    "we need to first initialize @xmath98 , @xmath153 , @xmath191 and @xmath97 .",
    "we initialize @xmath98 by randomly selecting the training instances with replacement .",
    "we sparsely encode @xmath182 over the initial dictionary using omp  @xcite .",
    "the sparse codes are considered as the initial @xmath191 , whereas their support forms the initial vector @xmath153 .",
    "computing the initial @xmath191 and @xmath153 with other methods , such as regularized least squares , is equally effective .",
    "we set @xmath192 for the initialization .",
    "notice , this means that all the dictionary atoms initially have equal chances of getting selected in the expansion of a training instance from any class .",
    "the values of @xmath193 finally inferred by the dictionary learning process serve as the initial values of these parameters for learning the classifier .",
    "similarly , the vectors @xmath153 and @xmath191 computed by the dictionary learning stage are used for initializing the corresponding vectors for the classifier .",
    "we initialize @xmath180 using the ridge regression model @xcite with the @xmath194-norm regularizer and quadratic loss : @xmath195 where @xmath35 is the regularization constant .",
    "the computation is done over the complete training data , therefore the superscript ` @xmath99 ' is dropped in the above equation .",
    "we have evaluated the proposed approach on two face data sets : the extended yaleb  @xcite and the ar database  @xcite , a data set for object categories : caltech-101  @xcite , a data set for scene categorization : fifteen scene categories  @xcite , and an action data set : ucf sports actions  @xcite .",
    "these data sets are commonly used in the literature for evaluation of sparse representation based classification techniques .",
    "we compare the performance of the proposed approach with src  @xcite , the two variants of label - consistent k - svd  @xcite ( i.e. lc - ksvd1 , lc - ksvd2 ) , the discriminative k - svd algorithm ( d - ksvd )  @xcite , the fisher discrimination dictionary learning algorithm ( fddl )  @xcite and the dictionary learning based on separating the commonalities and the particularities of the data ( dl - copar )  @xcite . in our comparisons , we also include results of unsupervised sparse representation based classification that uses k - svd  @xcite as the dictionary learning technique and separately computes a multi - class linear classifier using eq .",
    "( [ eq : w ] ) .    for all of the above mentioned methods , except src and d - ksvd , we acquired the public codes from the original authors . to implement src , we used the lasso  @xcite solver of the spams toolbox @xcite .",
    "for d - ksvd , we used the public code provided by jiang et al .",
    "@xcite for lc - ksvd2 algorithm and solved eq .",
    "( [ eq : lcksvd ] ) with @xmath45 .",
    "the experiments are performed on an intel core i7 - 2600 cpu at 3.4 ghz with 8 gb ram .",
    "we performed our own experiments using the above mentioned methods and the proposed approach using the same data .",
    "the parameters of the existing approaches were carefully optimized following the guidelines of the original works .",
    "we mention the used parameter values and , where it exists , we note the difference between our values and those used in the original works . in our experiments",
    ", these differences were made to favor the existing approaches .",
    "results of the approaches other than those mentioned above , are taken directly from the literature , where the same experimental protocol has been followed .",
    "for the proposed approach , the used parameter values were as follows . in all experiments , we chose @xmath196 for initialization , whereas @xmath102 and @xmath103 were all set to @xmath197 .",
    "we selected @xmath198 , whereas @xmath199 and @xmath200 were set to @xmath27 and @xmath201 , respectively .",
    "furthermore , @xmath202 was set to @xmath203 for all the datasets except for fifteen scene categories  @xcite , where we used @xmath204 . in each experiment ,",
    "the bayesian inference was performed with 35 gibbs sampling iterations .",
    "we defer further discussion on the selection of the parameter values to section  [ sec : d ] .",
    "extended yaleb @xcite contains 2,414 frontal face images of 38 different people , each having about 64 samples .",
    "the images are acquired under varying illumination conditions and the subjects have different facial expressions .",
    "this makes the database fairly challenging , see fig  [ fig : yaleface ] for examples . in our experiments , we used the random face feature descriptor @xcite , where a cropped @xmath205 pixels image was projected onto a 504-dimensional vector .",
    "for this , the projection matrix was generated from random samples of standard normal distributions . following the common settings for this database , we chose one half of the images for training and the remaining samples were used for testing .",
    "we performed ten experiments by randomly selecting the samples for training and testing .",
    "based on these experiments , the mean recognition accuracies of different approaches are reported in table [ tab : eyaleb ] .",
    "the results for locality - constrained linear coding ( llc ) @xcite is directly taken from @xcite , where the accuracy is computed using 70 local bases .",
    "similar to jiang et al .",
    "@xcite , the sparsity threshold for k - svd , lc - ksvd1 , lc - ksvd2 and d - ksvd was set to 30 in our experiments .",
    "larger values of this parameter were found to be ineffective as they mainly resulted in slowing the algorithms without improving the recognition accuracy .",
    "furthermore , as in @xcite , we used @xmath206 for lc - ksvd1 and lc - ksvd2 , whereas @xmath38 was set to @xmath207 for lc - ksvd2 and d - ksvd in eq .",
    "( [ eq : lcksvd ] ) . keeping these parameter values",
    "fixed , we optimized for the number of dictionary atoms for each algorithm .",
    "this resulted in selecting 600 atoms for lc - ksvd2 , d - ksvd and k - svd , whereas 500 atoms consistently resulted in the best performance of lc - ksvd1 .",
    "this value is set to 570 in @xcite for all of the four methods . in all techniques that learn dictionaries",
    ", we used the complete training data in the learning process . therefore , all training samples were used as dictionary atoms for src .",
    "following @xcite , we set the residual error tolerance to 0.05 for src .",
    "smaller values of this parameter also resulted in very similar accuracies .",
    "for fddl , we followed @xcite for the optimized parameter settings .",
    "these settings are the same as those reported for ar database in the original work .",
    "we refer the reader to the original work for the list of the parameters and their exact values .",
    "the results reported in the table are obtained by the global classifier ( gc ) of fddl , which showed better performance than the local classifier ( lc ) .",
    "for the parameter settings of dl - copar we followed the original work @xcite .",
    "we fixed 15 atoms for each class and a set of 5 atoms was chosen to learn commonalities of the classes .",
    "the reported results are achieved by lc , that performed better than gc in our experiments .    0.3      +    0.3     .recognition accuracy with random - face features on the extended yaleb database @xcite .",
    "the computed average time is for classification of a single instance . [",
    "cols=\"<,^,^\",options=\"header \" , ]     [ tab : ucf ]    this database comprises video sequences that are collected from different broadcast sports channels ( e.g. espn and bbc ) @xcite .",
    "the videos contain 10 categories of sports actions that include : kicking , golfing , diving , horse riding , skateboarding , running , swinging , swinging highbar , lifting and walking .",
    "examples from this dataset are shown in fig .",
    "[ fig : ucf ] . under the common evaluation protocol we performed fivefold cross validation over the dataset , where four folds are used in training and the remaining one is used for testing .",
    "results , computed as the average of the five experiments , are summarized in table [ tab : ucf ] . for d - ksvd , lc - ksvd1 and lc - ksvd2 we followed @xcite for the parameter settings . again , the value of @xmath197 ( along with similar small values ) resulted in the best accuracies for src .    in the table , the results for some specific action recognition methods",
    "are also included , for instance , qui et al .",
    "@xcite and action back feature with svm  @xcite .",
    "these results are taken directly from @xcite along the results of dlsi  @xcite , dl - copar @xcite and fddl  @xcite .",
    "following @xcite , we also performed leave - one - out cross validation on this database for the proposed approach .",
    "our approach achieves @xmath208 accuracy under this protocol , which is @xmath209 better than the state - of - the - art results claimed in @xcite .",
    "in our experiments , we chose the values of @xmath210 and @xmath69 in light of the theoretical results presented in section  [ sec : i ] . by setting @xmath211",
    "we make sure that @xmath154 is very large",
    ". the results mainly remain insensitive to other similar large values of this parameter .",
    "the chosen values of @xmath68 and @xmath69 ensure that @xmath212 .",
    "we used large values for @xmath202 in our experiments as this parameter represents the precision of the white noise distribution in the samples .",
    "the datasets used in our experiments are mainly clean in terms of white noise .",
    "therefore , we achieved the best performance with @xmath213 . in the case of noisy data",
    ", this parameter value can be adjusted accordingly .",
    "for ucf sports action dataset @xmath204 gave the best results because less number of training samples were available per class .",
    "it should be noted that the value of @xmath214 increases as a result of bayesian inference with the availability of more clean training samples .",
    "therefore , we adjusted the precision parameter of the prior distribution to a larger value for ucf dataset . among the other parameters , @xmath215 to @xmath103",
    "were fixed to @xmath197 .",
    "similar small non - negative values can also be used without affecting the results .",
    "this fact can be easily verified by noticing the large values of the other variables involved in equations ( [ eq:12 ] ) and ( [ eq:13 ] ) , where these parameters are used . with the above mentioned parameter settings and the initialization procedure presented in section  [ sec : init ] , the gibbs sampling process converges quickly to the desired distributions and the correct number of dictionary atoms , i.e. @xmath56 . in fig .",
    "[ fig : atoms ] , we plot the value of @xmath56 as a function of gibbs sampling iterations during dictionary training . each plot represent a complete training process for one dataset .",
    "it can be easily seen that the first 10 iterations of the gibbs sampling process were enough to infer the correct size of the dictionary .",
    "however , it should be mentioned that this fast convergence also owes to the initialization process adopted in this work . in our experiments , while sparse coding a test instance over the learned dictionary , we consistently used the sparsity threshold of @xmath216 for all the datasets except for the ucf @xcite , for which this parameter was set to @xmath217 because of the smaller dictionary resulting from less training samples . in all the experiments",
    ", these values were also kept the same for k - svd , lc - ksvd1 , lc - ksvd2 and d - ksvd for fair comparisons .",
    "we proposed a non - parametric bayesian approach for learning discriminative dictionaries for sparse representation of data .",
    "the proposed approach employs a beta process to infer a discriminative dictionary and sets of bernoulli distributions associating the dictionary atoms to the class labels of the training data .",
    "the said association is adaptively built during bayesian inference and it signifies the selection probabilities of dictionary atoms in the expansion of class - specific data .",
    "the inference process also results in computing the correct size of the dictionary . for learning the discriminative dictionary , we presented a hierarchical bayesian model and the corresponding inference equations for gibbs sampling .",
    "the proposed model is also exploited in learning a linear classifier that finally classifies the sparse codes of a test instance that are learned using the inferred discriminative dictionary .",
    "the proposed approach is evaluated for classification using five different databases of human face , human action , scene category and object images .",
    "comparisons with state - of - the - art discriminative sparse representation approaches show that the proposed bayesian approach consistently outperforms these approaches and has computational efficiency close to the most efficient approach .",
    "whereas its effectiveness in terms of accuracy and computation is experimentally proven in this work , there are also other key advantages that make our bayesian approach to discriminative sparse representation much more appealing than the existing optimization based approaches .",
    "firstly , the bayesian framework allows us to learn an ensemble of discriminative dictionaries in the form of probability distributions instead of the point estimates that are learned by the optimization based approaches .",
    "secondly , it provides a principled approach to estimate the required dictionary size and we can associate the dictionary atoms and the class labels in a physically meaningful manner .",
    "thirdly , the bayesian framework makes our approach inherently an online technique .",
    "furthermore , the bayesian framework also provides an opportunity of using domain / class - specific prior knowledge in our approach in a principled manner .",
    "this can prove beneficial in many applications .",
    "for instance , while classifying the spectral signatures of minerals on pixel and sub - pixel level in remote - sensing hyperspectral images , the relative smoothness of spectral signatures  @xcite can be incorporated in the inferred discriminative bases . for this purpose ,",
    "gaussian processes  @xcite can be used as a base measure for the beta process . adapting the proposed approach for remote - sensing",
    "hyperspectral image classification is also our future research direction .",
    "this research was supported by arc grant dp110102399 .",
    "a.  georghiades , p.  belhumeur and d.  kriegman , _ from few to many : illumination cone models for face recognition under variable lighting and pose _ , ieee trans .",
    "pattern analysis and machine intelligence , vol .",
    "23 , no . 6 , pp .",
    "643 - 660 , june 2001 .",
    "l.  feifei , r.  fergus and p.  perona , _ learning generative visual models from few training samples : an incremental bayesian approach tested on 101 object categories _ , proc .",
    "ieee conf .",
    "computer vision and pattern recognition workshop generative model based vision , 2004 .",
    "z.  jiang , z.  lin and l.s .",
    "davis , _ label consistent k - svd : learning a discriminative dictionary for recognition _ , ieee trans . pattern analysis and machine intelligence , vol.35 ,",
    "no.11 , pp.2651,2664 , nov . 2013 .    m.  yang , l.  zhang , x.  feng and d.  zhang , _ sparse representation based fisher discrimination dictionary learning for image classification _ , international journal of computer vision , vol.109 , no 3 , pp.209 - 232 , sept . 2014 .",
    "d. wang and s.  kong , _ a classification - oriented dictionary learning model : explicitly learning the particularity and commonality across categories _ , pattern recognition , vol.47 , no.2 , pp.885 - 898 , feb 2014 .",
    "i. ramirez , p. sprechmann and g. sapiro , _ classification and clustering via dictionary learning with structured incoherence and shared features _",
    ", in proc .",
    "ieee conf .",
    "computer vision and pattern recognition , 2010 .",
    "j. bobin , j. l. starck , j. m. fadili , y. moudden and d. l. donoho , _ morphological component analysis : an adaptive thresholding strategy _ , ieee transactions on image processing , vol 16 , no",
    "2675 - 2681 , 2006 .",
    "m. yang and l. zhang , _ gabor feature based sparse representation for face recognition with gabor occlusion dictionary _ , in proc .",
    "computer vision?eccv 2010 , pp .",
    "448 - 461 .",
    "springer berlin heidelberg , 2010 .",
    "h. zhang , a. berg , m. maire and j. malik , _ svm - knn : discriminative nearest neighbor classification for visual category recognition _ , in ieee conf . on computer vision and pattern recognition ,",
    "2126 - 2136 , 2006 .",
    "s. lazebnik , c. schmid , and j. ponce , _ beyond bags of features : spatial pyramid matching for recognizing natural scene categories _ , in proc .",
    "ieee conf . on computer vision and pattern recognition , pp .",
    "2169 - 2178 , 2007 .",
    "m. zhou , h. chen , l. ren , g. sapiro , l. carin , and j. w. paisley , _ non - parametric bayesian dictionary learning for sparse image representations _ , in advances in neural information processing systems , pp .",
    "2295 - 2303 , 2009 .",
    "w. deng , j. hu and j. guo , _ extended src : undersampled face recognition via intraclass variant dictionary _",
    ", ieee transactions on pattern analysis and machine intelligence , vol .",
    "34 , no . 9 ,",
    "pp.1864 - 1870 .",
    "2012 .",
    "l. shen , s. wang , g. sun , s. jiang and q. huang , _ multi - level discriminative dictionary learning towards hierarchical visual categorization _",
    ", in proc .",
    "ieee conference on computer vision and pattern recognition , pp .",
    "383 - 390 , 2013 .",
    "m. elad , _ sparse and redundant representation : from theory to applications in signal and image processing _ , springer , 2010 .",
    "m. beal , _ variational algorithms for approximate bayesian inference _ , doctoral dissertation , gatsby computational neuroscience unit , university college london .",
    "naveed akhtar received be degree with distinction in avionics from the college of aeronautical engineering , national university of sciences and technology ( nust ) , pakistan , in 2007 and m.sc .",
    "degree with distinction in autonomous systems from hochschule bonn - rhein - sieg ( hbrs ) , sankt augustin , germany , in 2012 .",
    "he is currently working toward the ph.d .",
    "degree at the university of western australia ( uwa ) under the supervision of dr .",
    "faisal shafait and prof .",
    "ajmal mian .",
    "he was a recipient of competitive scholarship for higher studies by higher education commission , pakistan in 2009 and currently he is a recipient of sirf scholarship at uwa .",
    "he has served as a research assistant at research institute for microwaves and millimeter - waves studies , nust , pakistan , from 2007 to 2009 and as a research associate at the department of computer science at hbrs , germany in 2012 .",
    "his current research is focused on sparse representation based image analysis .",
    "faisal shafait is working as an assistant professor in the computer science and software engineering department at the university of western australia .",
    "formerly , he was a senior researcher at the german research center for artificial intelligence ( dfki ) , germany and a visiting researcher at google , california .",
    "he received his ph.d . in computer engineering with the highest distinction from kaiserslautern university of technology , germany in 2008 .",
    "his research interests include machine learning and pattern recognition with a special emphasis on applications in document image analysis .",
    "he has co - authored over 80 publications in international peer - reviewed conferences and journals in this area .",
    "he is an editorial board member of the international journal on document analysis and recognition ( ijdar ) , and a program committee member of leading document analysis conferences including icdar , das , and icfhr .",
    "he is also serving on the leadership board of iapr?s technical committee on computational forensics ( tc-6 ) .",
    "ajmal mian received the be degree in avionics from the college of aeronautical engineering , nadirshaw edulji dinshaw ( ned ) university , pakistan , in 1993 , the ms degree in information security from the national university of sciences and technology , pakistan , in 2003 , and the phd degree in computer science with distinction from the university of western australia in 2006 .",
    "he received the australasian distinguished doctoral dissertation award from computing research and education association of australia ( core ) in 2007 .",
    "he received two prestigious fellowships , the australian postdoctoral fellowship in 2008 and the australian research fellowship in 2011 .",
    "he was named the west australian early career scientist of the year 2012 .",
    "he has secured four national competitive research grants and is currently a research professor in the school of computer science and software engineering , the university of western australia .",
    "his research interests include computer vision , pattern recognition , machine learning , multimodal biometrics , and hyperspectral image analysis ."
  ],
  "abstract_text": [
    "<S> we propose a bayesian approach to learn discriminative dictionaries for sparse representation of data . </S>",
    "<S> the proposed approach infers probability distributions over the atoms of a discriminative dictionary using a beta process </S>",
    "<S> . it also computes sets of bernoulli distributions that associate class labels to the learned dictionary atoms . </S>",
    "<S> this association signifies the selection probabilities of the dictionary atoms in the expansion of class - specific data . </S>",
    "<S> furthermore , the non - parametric character of the proposed approach allows it to infer the correct size of the dictionary . </S>",
    "<S> we exploit the aforementioned bernoulli distributions in separately learning a linear classifier . </S>",
    "<S> the classifier uses the same hierarchical bayesian model as the dictionary , which we present along the analytical inference solution for gibbs sampling . for classification , </S>",
    "<S> a test instance is first sparsely encoded over the learned dictionary and the codes are fed to the classifier . </S>",
    "<S> we performed experiments for face and action recognition ; and object and scene - category classification using five public datasets and compared the results with state - of - the - art discriminative sparse representation approaches . </S>",
    "<S> experiments show that the proposed bayesian approach consistently outperforms the existing approaches .    </S>",
    "<S> bayesian sparse representation , discriminative dictionary learning , supervised learning , classification . </S>"
  ]
}