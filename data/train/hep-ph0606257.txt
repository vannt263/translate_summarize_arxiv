{
  "article_text": [
    "particle identification is very important in the physics of high energy collisions , and especially , of relativistic heavy - ion collisions .",
    "for example , the identification of multi - strange baryons with high efficiency and high signal - background ratio is essential for the study of elliptic flow .",
    "the popular method used in the identification of multi - strange baryons is based on topological reconstruction . in this method",
    "the signals are extracted from a large amount of combinatoric background by cutting on certain parameters .",
    "this method is reliable , but the reconstruction efficiency is low .",
    "it is about @xmath0  @xmath1 in central and 7%  25% in peripheral collisions for @xmath2  @xcite and even much lower for @xmath3 .",
    "in addition , to optimize the cuts in a multi - dimensional space by trial and error can be very tedious .",
    "therefore , a method for raising the reconstruction efficiency of the identification of this kind of particles is highly sought .",
    "an alternative method , the artificial neural network has been introduced into high energy physics in 1988  @xcite and has been widely used in particle classification such as quark- and gluon- jets separation  @xcite@xcite , photon hadron discrimination  @xcite , top quark and higgs search  @xcite@xcite@xcite .",
    "most of the applications proved that neural network method is superior to traditional cut method or statistical likelihood method .",
    "the success of neural network method is mainly due to its nonlinear property , which enables it to explore many hypotheses simultaneously and consider the correlations between all variables . nonetheless , there are also disadvantages when implementing this method , the final result is more or less influenced by the design of the architecture and the initialization of the weight matrices .",
    "the effects of these factors are hard to follow and there is no universal instruction to help choosing the best parameters .",
    "boosting is a kind of adaptive reweighting and combining approach that combines several weak learners into a strong one .",
    "it can be applied to unstable classifiers such as decision trees and neural networks .",
    "ref  @xcite claims that boosted decision tree performs better than artificial neural network in the neutrino - oscillation search in miniboone experiment .",
    "naturally , one will ask the question `` how about boosted neural networks ? ''",
    "although plenty of studies on uci machine learning database show that the performance of boosted neural network is better than that of single neural network and boosted decision tree  @xcite@xcite@xcite , so far we have not seen the application of boosted neural network in high energy physics .    in this article , we will first discuss briefly the reason why the efficiency of topological reconstruction method in the identification of multi - strange baryons is low . then in section 3 a brief introduction to neural network and boosting technique will be given . in section 4",
    "we will show how boosting works with neural network in the case of a two - dimensional toy model , where the boundary of signal and background is irregular but not overlapping . in section 5",
    "we will apply boosted neural network to monte carlo quark- and gluon- jets classification , where the two data sets are overlapping . in the last section",
    "we will discuss the performance of boosted neural network at two different boundary cases  with and without overlapping and a possible method for raising the efficiency of multi - strange baryon identification is proposed .",
    "ccccc particle & decay mode & fraction ( % ) & c@xmath4 ( cm ) & mass ( mev/@xmath5 + @xmath6 & p@xmath7 & 63.9@xmath80.5 & 7.89 & 1115.684@xmath80.006 + @xmath9 & @xmath10 & 99.887@xmath80.035 & 4.91 & 1321.32@xmath80.13 + @xmath11 & @xmath12 & 67.8@xmath80.7 & 2.46 & 1672.45@xmath80.29 +    traditionally , the strange particles with two - body decay , @xmath9 , @xmath11 and @xmath13 , are detected through their decay topology .",
    "the properties of these decays are summarized in table  [ decaytable ]  @xcite .",
    "let us take @xmath9 search as an example .",
    "the primary decay channel @xmath14 has a 99.9@xmath15 branching ratio .",
    "the daughter particle @xmath13 further decays into @xmath16 with a 63.9@xmath15 branching ratio .",
    "@xmath9 s are found by tracing the decay topology backwards .",
    "first , a neutral decay vertex is found by identifying the crossing points of positive and negative particles tracks .",
    "kinematic information about the tracks are used to determine the trajectory of the parent neutral particle .",
    "the neutral particle is then intersected with other negative tracks to obtain candidate @xmath9 decay vertices . a schematic diagram of a @xmath9 decay is given in figure 1 .",
    "schematic representation of a @xmath9 decay with distance of closest approach ( dca ) parameters .",
    ", scaledwidth=50.0% ]    in each au - au collision event at rhic energy ( @xmath17 gev ) up to several thousand particles are produced .",
    "the finite momentum resolution of the tpc causes the primary tracks to not point back exactly to the primary vertex . as a result",
    ", these tracks may randomly cross with other primary tracks and form fake secondary vertices .",
    "indeed , in the quagmire of particle tracks , the vertices can be quite easily misidentified , leading to a large combinatoric background . to reduce this background ,",
    "basic cuts are applied during the event reconstruction chain .    to determine",
    "if two tracks are originated from the same vertex , a cut is placed on their distance of closest approach ( dca ) .",
    "this cut reduces the random background by a large amount , but is insufficient to guarantee a good identification of the parent particle .",
    "other cuts are necessary because of the following reasons :    lll track selection criteria & loose cut & tight cut + v0 vertex cuts : & & + proton tpc hits & @xmath1815 & @xmath1820 + pion tpc hits & @xmath1815 & @xmath1817 + pion and proton pid ( for @xmath19 gev/@xmath20 ) & @xmath21&@xmath21 + track proton dca to primary vertex(cm ) & @xmath180.5 & @xmath180.5 + ( for @xmath19 gev/@xmath20 ) & & + track pion dca to primary vertex ( cm ) & @xmath182.0 & @xmath182.0 + dca between v0 daughters ( cm ) & @xmath220.7&@xmath220.65 + v0 dca to primary vertex ( cm ) & @xmath23 and @xmath220.7 & @xmath24 and @xmath25 + v0 decay length from primary vertex ( @xmath26)(cm)&@xmath185.0&@xmath185.0 + @xmath2 vertex cuts : & & + v0 mass window ( mev/@xmath27)&@xmath28&@xmath29 + bachelor @xmath7 tpc hits & @xmath1810&@xmath1817 + bachelor @xmath7 dca to primary vertex ( cm)&@xmath180&@xmath180.5 + bachelor pid ( for @xmath19 gev/@xmath20 ) & @xmath21&@xmath21 + bachelor @xmath30 ( gev/@xmath20)&@xmath31&@xmath31 + dca between @xmath2 daughters ( cm)&@xmath220.7&@xmath220.65 + angle between @xmath2 s momentum and decay vertex vector&@xmath32&@xmath33 + @xmath2 dca to primary vertex ( cm)&@xmath221.0&@xmath220.5 + @xmath2 decay length from primary vertex ( @xmath34)(cm)&@xmath35 and @xmath36&@xmath37 and @xmath36 +    * due to high density of tracks near the primary vertex , it is easy to form many fake track crossings .",
    "this leads to a larger combinatoric background as one gets closer to the primary vertex .",
    "the decay distance distribution has an exponential fall - off to zero , so cuts used on these distances for the candidate @xmath9 and daughter @xmath13 are greater than 2 cm and 5 cm , respectively .",
    "the decay distances are measured from the primary vertex . * the candidate parents , candidate @xmath9 s , point back to the primary vertex since they are produced at this vertex . *",
    "the daughter tracks do not point back to the primary vertex to ensure they are not primary tracks .",
    "* a cut on the calculated mass of the daughter neutral particle is added to increase the likelihood that the parent particle did indeed decay into a @xmath13 ( @xmath38 ) plus a charged track .",
    "typical cuts used for @xmath9 are listed in table  [ cuts ] . for track pairs",
    "passed all the cuts the invariant mass for a decay vertex is calculated by the kinematic information of its daughters : @xmath39 where subscripts 1 and 2 represent the two daughter particles from a decay vertex .",
    "this equation is used twice since there are two decay steps associated with a @xmath9 particle . for @xmath13 reconstruction one of them",
    "is @xmath40 and the other is @xmath7 . for @xmath9 reconstruction one of them is @xmath13 and the other is bachelor @xmath7 .",
    "@xmath41 is the parent momentum with @xmath42 and @xmath43 representing the daughters momentum .",
    "with all the topological cuts used , clear signal peak is observed in invariant mass distribution .",
    "a tighter cut will make the peak becomes more significant .",
    "however , the extremely evil cut results in a high signal - background ratio but at the same time it reduces the signal yield greatly , causing the reconstruction efficiency to be very low .",
    "the neural network approach is nothing but functional fitting to data . in classification cases ,",
    "one wants to construct a mapping @xmath44 between a set of observable quantities @xmath45 ( @xmath46 ) and category variable @xmath47 by fitting @xmath44 to a set of @xmath48 known `` training '' samples @xmath49 .",
    "once the parameters in @xmath44 are fixed , one then uses this parametrization to interpolate and find the category of `` test '' samples not included in the `` training '' set .",
    "obviously , the performance of the network on the test set estimates the generalization ability of the fitting . in the present work we use the multilayer perceptron program developed in root version 4.00/04  @xcite .",
    "the function @xmath44 is an expansion of sigmoidal function in a feed forward network structure since there is a theorem  @xcite saying that a linear combination of sigmoids can approximate any continuous function .",
    "a typical three - layer neural network is sketched in figure  [ nnstructure ] .",
    "it consists of an input layer , a hidden layer and an output layer , with various number of nodes ( also called neurons ) in each layer . in the following",
    ", we will use the notation [ @xmath50-@xmath51-@xmath52 to denote a neural network with @xmath50 input nodes , @xmath51 hideen nodes and @xmath53 output nodes .",
    "there are weights connecting the nodes from any two adjacent layers and each node in the hidden and the output layers has a threshold .",
    "the output of the @xmath54th node in the output layer ( @xmath55 ) is @xmath56 where @xmath57 are the input parameters , @xmath58 are weights between the @xmath59th node in the input layer and the @xmath60th node in the hidden layer , @xmath61 are thresholds of each node in the hidden layer , @xmath62 are weights between the @xmath60th node in the hidden layer and the @xmath54th node in the output layer , @xmath63 are thresholds of each node in the output layer .",
    "@xmath64 is the sigmoid transfer function .",
    "the goal of adjusting the parameters , or training the neural network , is to minimize the fitting error .",
    "the mean square error @xmath65 averaged over the training samples is defined as @xmath66 where @xmath67 is the output of the @xmath54th node of the neural network , @xmath68 is the training target , @xmath48 is the number of samples in the training set . in binary case ,",
    "the output layer has only one node , @xmath69 , with @xmath70 for background and @xmath71 for signal .",
    "there are several algorithms for error minimization and weight updating , which are implemented in root as options .",
    "the initial weights are random numbers in the range @xmath72 .",
    "the distribution of signal and background from the two - dimensional toy model , where crosses are signals and circles are backgrounds.,title=\"fig:\",width=192 ] 0.5 cm   the distribution of signal and background from the two - dimensional toy model , where crosses are signals and circles are backgrounds.,title=\"fig:\",width=192 ]    boosting is a technique to construct a committee of weak learners that lowers the error rate in classification .",
    "it is first developped by schapire  @xcite and the theoretical study followed shows that given a significant number of weak learners , the boosting algorithm can decrease the error rate on the training set and convert the ensemble of weak learners to a strong learner whose error rate on the ensemble is arbitrarily low  @xcite@xcite . in the binary classification case ,",
    "one only needs to construct weak learners with error rate be slightly better than random guessing ( 0.5 ) .",
    "there are a number of variations on basic boosting .",
    "the most popular one , adaboost , allows the designer to continue adding weak learners until the desired low training error has been achieved . in adaboost",
    "each training sample receives a distribution @xmath73 that determines its probability of being selected in a training set for individual component classfier .",
    "the distribution @xmath73 is determined in the following way  if a training sample is accurately classified , then its chance of being used again in a subsequent component classifier is reduced ; conversely , if the pattern is not accurately classified , then its chance of being used again is raised .",
    "thus adaboost `` focuses '' the component classifier on more informative or more `` difficult '' samples .",
    "the adaboost procedure for neural network in binary case is as follows :    * input : * sequence of @xmath48 examples @xmath74 with labels @xmath75 , here @xmath76 .",
    "* init : * @xmath77 for all @xmath40 .    * repeat ( @xmath78 ) : *    * train neural network with respect to distribution @xmath79 and obtain hypothesis @xmath80 , * calculate the weighted error of @xmath81 @xmath82 and abort loop if @xmath83 , * set @xmath84 , * update distribution @xmath79 @xmath85 where @xmath86 is a normalizing constant .    * output : * final hypothesis : @xmath87 .    in binary case",
    "the final hypothesis can be restated as @xmath88",
    "in high energy physics , the separation of signal from background is a typical binary case .",
    "assume a data set with @xmath89 signals and @xmath90 backgrounds .",
    "neural network is applied to it and the result can be denoted by the following quantities :    * @xmath91 the number of signals correctly classified * @xmath92 the number of signals incorrectly classified * @xmath93 the number of backgrounds correctly classified * @xmath94 the number of backgrounds incorrectly classified    the classification ability of the neural network can be judged by the error rate , classification efficiency and signal - background ratio , with the definitions : @xmath95 according to the above definition , even though boosting is capable in decreasing the error rate of the training set , this does not imply that the classification efficiency and signal - background ratio could be increased . in physics , what is most sought for is high efficiency and high signal - background ratio .    in this section",
    "we construct a two - dimensional toy model to show how the boosting algorithm works with neural network for the case of non - overlapping boundary between signal and background , figure  [ toydis ] . in the figure , crosses are signals and circles are backgrounds . the boundary between them is irregular but does not overlap .    to train the neural network we require similar signal- and background- sample - density in the phase space of the training set .",
    "thus 1000 signals and 4000 backgrounds are used to form the training set and another set with the same amount of samples are used as test set .",
    "the inputs of the network are the coordinates of the points in x - y plane .",
    "firstly , we choose a three - layer network [ 2 - 10 - 1 ] with two input nodes , ten hidden nodes and one output node .",
    "all the other network parameters take the default values from root . in figure",
    "[ toy ] are shown the error rate , the classification efficiency and the signal - background ratio of the classification for both training and test sets during being boosted one hundred rounds .",
    "we see that with respect to the boosting round , the error rates of the training and test sets decreas sharply while the classification efficiency increases at the first few rounds then slightly oscillates afterwards .",
    "the signal - background ratio also increases . obviously , a boosted neural network can distinguish signal from background better than single neural network with the same network parameters .     the error rate , classification efficiency and signal - background ratio of the classification by boosted neural network for both training and test sets under the network architecture 2 - 10 - 1.,title=\"fig:\",width=172 ]   the error rate , classification efficiency and signal - background ratio of the classification by boosted neural network for both training and test sets under the network architecture 2 - 10 - 1.,title=\"fig:\",width=172 ]   the error rate , classification efficiency and signal - background ratio of the classification by boosted neural network for both training and test sets under the network architecture 2 - 10 - 1.,title=\"fig:\",width=172 ]    @*10l architecture&&&w1&w2&w3&w4&w5&w6&w7 + & & error rate ( % ) & 5.94&4.93&4.46&4.30&5.90&4.40&5.96 + & single&efficiency ( % ) & 89.1&78.6&92.9&93.7&89.6&93.3&89.0 + [ 0pt ]  2 - 5 - 1 & & s - b ratio&4.74&3.42&6.11&6.16&4.69&6.10&4.73 + & & error rate ( % ) & 1.04&0.60&0.52&0.94&0.94&0.94&0.98 + & boosted&efficiency ( % ) & 96.9&97.4&99.4&96.5&98.4&97.4&97.2 + & & s - b ratio&46.1&34.8&49.7&80.4&31.7&46.4&46.3 + & & error rate ( % ) & 2.94&4.10&4.68&3.26&2.66&3.08&3.06 + & single&efficiency ( % ) & 94.6&92.4&89.7&92.6&94.2&93.6&94.5 + [ 0pt ]  2 - 10 - 1 & & s - b ratio&10.2&7.16&6.85&10.4&12.6&10.4&9.64 + & & error rate ( % ) & 0.88&1.10&0.94&1.04&1.02&0.98&0.88 + & boosted&efficiency ( % ) & 97.3&96.7&97.0&97.4&98.2&97.5&97.5 + & & s - b ratio&57.3&44.0&57.1&37.5&29.8&40.6&51.3 + & & error rate ( % ) & 2.32&1.48&1.6&2.38&1.46&1.54&2.38 + & single&efficiency ( % ) & 93.0&95.6&95.6&92.4&95.8&95.6&94.4 + [ 0pt ]  2 - 5 - 4 - 1 & & s - b ratio&20.2&31.9&26.6&21.5&30.9&28.9&15.0 + & & error rate ( % ) & 0.88&0.64&0.94&0.60&0.88&0.76&0.70",
    "+ & boosted&efficiency ( % ) & 97.3&98.6&97.0&98.6&97.5&97.6&98.8 + & & s - b ratio&57.2&54.7&57.1&61.6&51.3&75.1&43.0 +    one of the disadvantages of neural network is that its performance depends on network architecture and initial weight matrices . to study the dependency ,",
    "we vary the hidden nodes of the network and use different weight matrices .",
    "the behavior of the networks in terms of error rate , classification efficiency and signal - background ratio are listed in table  [ toytable ] .",
    "it can be seen from the table that , in average , more complicated single network architecture , , gives out lower error rate , higher efficiency and higher signal - background ratio than the other two architectures , and that for different initial weight matrices the performance of the network varies even for the same architecture . after boosting , not only the error rate decreases but also the classification efficiency and signal - background ratio increase in comparison to those of the single neural network .",
    "in addition , the dependency of the network performance on different network architectures and initial weight matrices vanishes . therefore , boosted simple neural network with arbitrary initial weight matrices has comparable ability as a single neural network with a complicated structure and fine - tuned parameters .",
    "the monte carlo jetset7.4 is used to generate @xmath96 collision events at 91.2 gev . the quark- and gluon- jet samples are obtained through the following procedure : ( 1 ) force events into 3-jet ones both at parton and at hadron levels by using the durham jet algorithm  @xcite .",
    "( 2 ) select the planar 3-jet events by requiring the sum of the three angles between two adjacent jets to be greater than @xmath97 at hadron level ( this condition is automatically satisfied at parton level ) .",
    "( 3 ) apply the angular cut method  @xcite to the hadronic 3-jet event , _",
    "i.e. _ the three angles between two adjacent jets are ordered and the jet opposite to the largest angle is supposed to be a gluon jet and the jet opposite to the smallest angle is the more energetic quark jet .",
    "we require the difference between the largest angle and the middle one to be greater than an angular cut @xmath98 and the more energetic quark jet is rejected in an event .",
    "( 4 ) match the hadronic quark- and gluon- jets with the corresponding parton level jets .",
    "four variables are chosen to describe a jet , the multiplicity @xmath99 inside jet , the transverse momentum @xmath30 of jet , the included angle @xmath100 opposite to the jet and the jet energy @xmath101 .",
    "usually , the quark and gluon jet samples are hard to be distinguished because of the large overlapping of these two sets . using the above procedure",
    ", the quark- and gluon- jet samples are well selected from the generated raw samples and the overlapping in some phase space is decreased , so that we can study how the network works and compare it with the simple cut method .    in figure  [ qgboundary ]",
    "is shown the mixing region of quark- and gluon- jets in @xmath101 and @xmath100 space .",
    "compared with the simple two - dimensional toy model , the boundary is unclear and the two sets overlap each other strongly . if we apply a @xmath100 cut , setting @xmath102 to be gluon jet and @xmath103 to be quark jet , the efficiency for both quark- and gluon- jets are greater than @xmath104 and the error rate is @xmath105     the error rate of the classification of quark and gluon jet samples by boosted neural network both for training set and for test set under network architecture 4 - 5 - 1.,title=\"fig:\",width=211 ] 0.5 cm   the error rate of the classification of quark and gluon jet samples by boosted neural network both for training set and for test set under network architecture 4 - 5 - 1.,title=\"fig:\",width=211 ]    next , we take 2500 quark jets and 2500 gluon jets as training set and another 5000 as test set . from the above two - dimensional toy model",
    "we know that boosting different network architectures will result in similar performance , so we choose a very simple network architecture [ 4 - 5 - 1 ] to save the computer time and ensure network generalization ability .    in figure  [ qger ] are plotted the error rates of the network in the training and test sets versus the boosting round .",
    "it can be seen from the figure that the error rates increase in the first boosting round then decrease afterwards . for the training set the error rate decreases to zero while for the test set , the error rate decreases slightly and oscillates , keeping higher than that of the first round . to check the results we tried a more complicated network architecture [ 4 - 50 - 1 ] to do the boosting .",
    "the similar trends are obtained .",
    "the comparison of single neural network , boosted network and simple cut method with respect to error rate , efficiency and signal - background ratio are shown in table  [ qgtable ] .",
    "it can be seen that , although the boosted neural network does not work well for the present case , the single neural network still presents lower error rate , higher efficiency and higher signal - background ratio in comparison with the simple cut method .",
    "@*11l & & & & + & & & + & & w1&w2&w3&w1&w2&w3&w1&w2&w3 + & single&1.42&1.38&1.50&98.80&98.80&98.64&60.2&63.3&60.2 + 4 - 5 - 1&1st round&2.22&2.46&2.12&97.96&98.32&98.00&40.8&30.3&43.8 + & boosted&1.82&1.60&1.50&98.08&98.32&98.28&57.0&64.7&76.8 + & single&1.36&1.34&1.38&98.80&98.80&98.72&65.0&66.8&66.7 + 4 - 50 - 1&1st round&2.72&2.70&2.58&97.68&97.36&97.84&31.3&35.3&32.6 + & boosted&1.68&1.72&1.66&98.28&98.04&98.28&59.9&66.2&61.4 + simple&-0.2 cm cut & & 4.05 & & & 97.65 & & & 18.0 & +",
    "in this paper we apply the boosting technique to artificial neural network .",
    "a two - dimensional toy model is constructed to show how boosted neural network works when the boundary between signal and background is complicated but does not overlap .",
    "then the boosted neural network is applied to monte carlo quark- and gluon- jet samples of collision , where the two samples strongly mix with each other .    in both cases",
    "the boosting technique drives the error rate of the training set to zero , while the error rates of the test sets behave differently . in the case of two - dimensional toy model with non - overlapping boundary ,",
    "the error rate of the test set also decreases dramatically but in the case of quark- and gluon- jet samples with strong overlap , the error rate of the test set increases at the first boosting round then decreases slightly .",
    "we also tried some other event samples , for example , a tighter or looser angular cut mentioned in section 5 or cuts on the network input parameter , the included angle @xmath100 , are applied to quark and gluon jets to make the classification task easier or harder .",
    "we found that once there are mixing between quark - jet set and gluon - jet set , the performance of boosted neural networks is similar as that shown above .",
    "so we conclude that boosting technique does not improve the performance of single neural networks in the case of overlapping samples like the quark and gluon jets .",
    "this is easy to understand since boosting technique is always applied to unstable classifiers , while we see that from table  [ qgtable ] the outcomes of single neural network are rather stable at different network architectures and initial weight matrices .",
    "there is no space for boosting technique to improve the behavior of single neural networks in this case . to summarize , artificial neural network , in general , results in lower error rate than simple cut method .",
    "boosted neural network avoids the disadvantage of single neural network and is more stable and easier to implement .",
    "it will lower the error rate , increase the efficiency and signal - background ratio of classification in case the boundaries between signal and background are complicated but separable which could not be easily classified by simple cut method . while for the case with mixed signal and background , the boosted neural network does not help improving the classification .",
    "lll + pre - processing : & + proton tpc hits & + pion tpc hits & + pion and proton pid & + bachelor @xmath7 tpc hits & + bachelor pid & + bachelor @xmath30 ( gev/@xmath20 ) & + first generation cuts : & + track proton dca to primary vertex(cm ) & + track pion dca to primary vertex ( cm ) & + dca between v0 daughters ( cm ) & + bachelor @xmath7 dca to primary vertex ( cm ) & + second generation cuts : & + v0 dca to primary vertex ( cm ) & + v0 decay length from primary vertex ( @xmath26)(cm ) & + v0 mass window ( mev/@xmath27 ) & + dca between @xmath2 daughters ( cm ) & + third generation cuts : & & + angle between @xmath2 s momentum and decay vertex vector & + @xmath2 dca to primary vertex ( cm ) & + @xmath2 decay length from primary vertex ( @xmath34)(cm ) & +    let us turn to discuss the possible application of the ( boosted ) neural network in raising the efficiency of multi - strange baryon identification .",
    "the usual way of topological reconstruction of multi - strange baryons can be divided into 3 steps , table  [ cutstable ] . after a pre - processing selection on the track quality ,",
    "the first step is to make cuts on the 4 dca s ( distances of closest approach ) of pion and proton tracks . in this way",
    "a 4-dimensional polyhedron @xmath106 in the 4-dimensional parameter space is isolated .",
    "the region inside @xmath106 is taken as the candidates of v0 particles ( @xmath107 ) .",
    "in the second step , the v0 dca to the primary vertex , v0 decay length from the primary vertex and the v0 mass window , together with the dca between @xmath2 daughter are taken as 4 parameters to form another 4-dimensional parameter space .",
    "cutting on the parameters isolates out another 4-dimensional polyhedron @xmath108 in this space .",
    "the region inside @xmath108 is taken as the candidates of @xmath2 .",
    "the third step is to take the kinematic parameters of reconstructed @xmath2  the angle between @xmath2 s momentum and the decay vertex vector , the @xmath2 dca to the primary vertex and the @xmath2 decay length from the primary vertex to form a 3-dimensional parameter space .",
    "cutting on the parameters isolates out a 3-dimensional cube @xmath109 .",
    "the region inside @xmath109 is taken as the finally reconstructed @xmath2 .",
    "it should be noticed that although the region inside @xmath106-@xmath108-@xmath109 is dominated by signals , there is still a large amount of signals lying outside of @xmath106-@xmath108-@xmath109 . in the traditional method the region outside of @xmath106-@xmath108-@xmath109",
    "is thrown away together with the signals lying in it , resulting in a very low reconstruction efficiency .",
    "therefore , the key to raise the reconstruction efficiency is to find some way to pick up the signals lying outside of @xmath106-@xmath108-@xmath109 .",
    "thus , a possible way of raising the efficiency of multi - strange baryon identification is suggested as the following .",
    "firstly , use a set of reasonable cuts to isolate polyhedrons @xmath106 , @xmath108 and @xmath109 in the three multi - dimensional parameter spaces .",
    "then use boosted neural networks inside @xmath106 , @xmath108 and @xmath109 to further increase the signal - background ratio in these regions , and at the same time utilize specially designed single neural networks to pick up signals ( together with some backgrounds ) from the region outside of @xmath106-@xmath108-@xmath109 . in this way , a good particle identification with reasonable signal - background ratio and higher reconstruction efficiency might be obtained .",
    "this work is supported in part by the national science foundation of china under project 10375025 and by the cultivation fund of the key scientific and technical innovation project , ministry of education of china no cfkstip-704035 ."
  ],
  "abstract_text": [
    "<S> the possible application of boosted neural network to particle classification in high energy physics is discussed . a two - dimensional toy model , where the boundary between signal and background is irregular but not overlapping , is constructed to show how boosting technique works with neural network . </S>",
    "<S> it is found that boosted neural network not only decreases the error rate of classification significantly but also increases the efficiency and signal - background ratio . </S>",
    "<S> besides , boosted neural network can avoid the disadvantage aspects of single neural network design . </S>",
    "<S> the boosted neural network is also applied to the classification of quark- and gluon- jet samples from monte carlo collisions , where the two samples show significant overlapping . </S>",
    "<S> the performance of boosting technique for the two different boundary cases  with and without overlapping is discussed .    </S>",
    "<S> _ keywords _ : neural network boosting particle classification error rate efficiency </S>"
  ]
}