{
  "article_text": [
    "markov chain monte carlo ( mcmc ) methods are fundamental tools for sampling highly complex distributions .",
    "they are of paramount importance in bayesian inference as posterior distributions are generally difficult to characterize analytically ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) . when the posterior distribution is based on a massive sample of size @xmath0 , posterior sampling can be computationally prohibitive since for some widely - used samplers at least @xmath1 operations are needed to draw one mcmc sample .",
    "additional issues include memory and storage bottlenecks where datasets are too large to be stored on one computer .",
    "a common solution relies on parallelizing the computation task , i.e. dividing the load among a number of parallel _ workers _ , where a worker can be a processing unit , a computer , etc .",
    "given the abundant availability of processing units , such strategies can be extremely efficient as long as there is no need for frequent communication between workers .",
    "some have discussed parallel mcmc methods @xcite such that each worker runs on the full dataset",
    ". however , these methods do not resolve memory overload , and also face difficulties in assessing the number of burn - in iterations for each processor .",
    "a truly parallel approach is to divide the dataset into smaller groups and run parallel mcmc methods on each subset using different workers .",
    "such techniques benefit from not demanding space on each computer to store the full dataset .",
    "generally , one needs to avoid frequent communication between workers , as it is time consuming . in a typical divide and conquer strategy the data is partitioned into non - overlapping sub - sets , called _ shards _ , and each shard is analyzed by a different worker . for such strategies",
    "some essential mcmc - related questions are : 1 ) how to define the sub - posterior distributions for each shard , and 2 ) how to combine the mcmc samples obtained from each sub - posterior so that we can recover the same information that would have been obtained by sampling the full posterior distribution .",
    "existing communication - free parallel methods proposed by @xcite , @xcite and @xcite have in common the fact that the product of the unnormalized sub - posteriors is equal to the unnormalized full posterior distribution , but differ in the strategies used to combine the samples .",
    "specifically , @xcite approximate each sub - posterior using kernel density estimators , while @xcite use the weierstrass transformation .",
    "the popular consensus monte carlo ( cmc ) method @xcite relies on a weighted averaging approach to combine sub - posterior samples .",
    "the cmc relies on theoretical derivations that guarantee its validity when the full - data posterior and all sub - posteriors are gaussian or mixtures of gaussian .",
    "we introduce a new communication - free parallel method , the _ likelihood inflating sampling algorithm ( lisa ) _",
    ", that also relies on independent and parallel processing of the shards by different workers to sample the sub - posterior distributions .",
    "the latter are defined differently than in the competing approaches described above . in this paper , we develop techniques to combine the sub - posterior draws obtained for lisa in the case of bayesian additive regression trees ( bart ) @xcite and compare the performance of our method with cmc",
    ".    sections [ motiv ] and [ newmethod ] contain a brief review of the cmc algorithm and the detailed description of lisa , respectively .",
    "section [ motex ] illustrates the potential difference brought by lisa over cmc in a simple bernoulli example , and discusses the application of lisa to linear regression models .",
    "the main application is introduced in section [ bart_sec ] where the bart model is described , and we show the comparison between lisa and cmc when applied to this important model .",
    "we end the paper with a discussion of ideas for future work .",
    "the appendix contains theoretical derivations and description of the steps used when running bart .",
    "in this paper we assume that of interest is to generate samples from @xmath2 , the posterior distribution @xmath3 given the iid sample @xmath4 of size @xmath0 .",
    "the assumption is that @xmath0 is large enough to prohibit running a standard mcmc algorithm in which draws from @xmath5 are obtained on a single computer .",
    "we use the notation @xmath6 , where @xmath7 is the likelihood function corresponding to the observed data @xmath8 and @xmath9 is the prior . major issues with mcmc posterior sampling for big data can be triggered because a ) the data sample is too large to be stored on a single computer , or b ) each chain update is too costly , e.g. if @xmath5 is sampled via a metropolis - hastings type of algorithm each update requires @xmath0 likelihood calculations .    in order to reduce the computational costs , the cmc method of @xcite partitions the sample into @xmath10 batches ( i.e. @xmath11 ) and uses the workers independently and in parallel to sample each sub - posterior .",
    "more precisely , the @xmath12-th worker ( @xmath13 ) will generate samples from the @xmath12-th sub - posterior distribution defined as : @xmath14 note that the prior for each batch is considered to be @xmath15^{1/k}$ ] such that @xmath16 and thus the overall full - data unnormalized posterior distribution which we denote as @xmath17 is equal to the product of unnormalized sub - posterior distributions , i.e. @xmath18 when the full posterior is gaussian , the weighted averages of the sub - samples from all batches can be used as full - data posterior draws . that is , assuming @xmath19 are @xmath20 sub - samples from the @xmath21th worker then the @xmath22-th approximate full posterior draw will be : @xmath23 where the weights @xmath24 are optimal for gaussian models with @xmath25 . in the next section",
    "we introduce an alternative method to define the sub - posteriors in each batch .",
    "lisa is an alternative to cmc that also benefits from the random partition of the dataset followed by independently processing each batch on a different worker . assuming that the data have been divided into @xmath10 batches of approximately equal size @xmath26",
    ", we define the sub - posterior distributions for each machine by adjusting the likelihood function without making changes to the prior .",
    "thus the @xmath12-th sub - posterior distribution will be : @xmath27}^{k } { p(\\theta)}.\\ ] ] since the data are assumed to be iid , inflating the likelihood function @xmath10-times is intuitive because the sub - posterior from each batch of data will be a closer representation of the whole data posterior .",
    "we thus expect that sub - posteriors sampled by each worker will be closer to the full posterior and thus improve the statistical efficiency of the bayesian inference .",
    "we indeed prove in a theorem below that under mild conditions , lisa s sub - posterior distributions are asymptotically closer to the full posterior than those produced by the cmc - type approach .    the taylor s series expansion for a log - posterior density @xmath28 around its posterior mode @xmath29 yields the approximation @xmath30 where @xmath31 .",
    "exponentiating both sides will result in @xmath32\\ ] ] which shows asymptotic normality , i.e. @xmath33 as @xmath34 where @xmath35 .",
    "let @xmath36 and @xmath37 denote lisa s sub - posterior mode and negative second derivative of its log sub - posterior at @xmath38 for batch @xmath12 , respectively ( similarly @xmath39 and @xmath40 for cmc ) .",
    "then consider the assumptions ,    1 .",
    "there exists @xmath41 such that @xmath42 and @xmath43 @xmath44 and all @xmath45 ( i.e. sub - posterior distributions of lisa have the same mean and variance across all batches for a large enough @xmath26 ) .",
    "2 .   there exists @xmath46 such that @xmath47 and @xmath48 @xmath44 and all @xmath49 ( i.e. sub - posterior distributions of cmc have the same mean and variance across all batches for a large enough @xmath26 ) .",
    "assume that * a1 * and * a2 * hold and @xmath50 where @xmath51 .",
    "then @xmath52 where @xmath53 and @xmath54 .",
    "[ thm1 ]    see appendix .",
    "theorem 1 states that lisa s individual sub - posterior distributions are asymptotically similar to the full posterior distribution , while cmc s are over - dispersed by a factor of @xmath10 .",
    "hence , we expect that lisa s batch - specific sub - posteriors will be better approximations of the full posterior than the ones generated using the cmc s design .    thus , one straightforward strategy for combining lisa s sub - samples is to uniformly sub - sample from the samples produced by each worker .",
    "however , we will see that this aggregation scheme for lisa can be exact in some cases , e.g. for a bernoulli model with balanced batch samples , while in others it may require modifications to improve its performance .    in the next section",
    "we will illustrate lisa in some simple examples and compare its performance to the full - data posterior sampling as well as cmc .",
    "in this section we examine some simple examples where theoretical derivations can be carried out in detail .",
    "we emphasize the difference between lisa and cmc .",
    "consider @xmath55 to be n i.i.d .",
    "bernoulli random variables with parameter @xmath3 .",
    "hence , we consider a prior @xmath56 . assuming that we know little about the size of @xmath3 we set @xmath57 which corresponds to a @xmath58 prior .",
    "the resulting full - data posterior @xmath17 is beta@xmath59 where @xmath60 is the total number of ones .",
    "suppose we divide the data into @xmath10 batches with @xmath61 number of ones in batch @xmath12 , such that @xmath62 @xmath63 , i.e. the number of 1 s are divided equally between batches",
    ". then the @xmath64 sub - posterior based on batch - data of size @xmath65 for each method will be :    * * cmc : * @xmath66 * * lisa : * @xmath67 which implies @xmath68    in this simple case any one of lisa s sub - posterior distributions is equal to the full posterior distribution if the batches are balanced , i.e. the number of 1 s are equally split across all batches .",
    "thus , lisa s sub - samples from any batch will represent correctly the full posterior . on the other hand ,",
    "the draws from the cmc sub - posterior distributions will need to be recombined to obtain a representative sample from the true full posterior @xmath17 .",
    "however , when the number of ones is unequally distributed among the batches it is not clear the winner between cmc and lisa as both require a careful weighting of each batch sub - posterior samples .",
    "in the remaining part of this paper , we will mainly focus on the performance of lisa when it is applied to the bayesian additive regression trees ( bart ) model .",
    "interestingly , we discover that using a minor modification inspired by running lisa on the simpler bayesian linear regression model we can approximate the full posterior .",
    "the idea behind the modification is described in the next section .",
    "consider a standard linear regression model @xmath69 where @xmath70 , @xmath71 and @xmath72 with @xmath73 .",
    "we consider the conjugate priors @xmath74 hence the full posterior distribution is @xmath75 with conditional posterior distributions @xmath76 where @xmath77 and @xmath78 .",
    "\\end{cases } \\label{full2}\\ ] ] using gibbs sampling one can draw from the posterior of @xmath79 and @xmath80 using the conditional distributions described in and . now assume the dataset has been randomly divided into @xmath10 batches , with data in the @xmath12th batch denoted by @xmath81 . to simplify the notation",
    "we assume that the sample size in each batch is @xmath82 .",
    "thus , lisa s conditional sub - posterior distribution of @xmath79 and @xmath80 from batch @xmath12 will be : @xmath83 where , if we set @xmath84 and @xmath85 , we have @xmath86    and    @xmath87 \\\\ & = k \\bigg ( { b_0}^ { * } + \\frac{1}{2}~\\bigg [ ( y^{(j)}-x_j\\beta)^{t}(y^{(j)}-x_j\\beta ) + ( \\beta-\\mu_o)^{t}{\\omega_0}^{*}(\\beta-\\mu_0 ) \\bigg ] \\bigg )     \\end{cases } \\label{batch2}\\ ] ]    for the remaining part of the section we assume that the inferential focus is on prediction so we centre the discussion on the posterior samples for @xmath79 .",
    "when comparing to , we find similarities between lisa s sub - posterior distributions and the original posterior distributions of each batch - data ( which we call `` batchsinglemachine '' ) .",
    "let @xmath88 and @xmath89 denote the parameters for the conditional distribution of @xmath79 in batchsinglemachine . hence comparing to lisa",
    ", we have @xmath90 implying @xmath91 as it is clear from , lisa has smaller residuals compared to batchsinglemachine",
    ". we will show in the following theorem that with minor changes to the residuals in lisa , we can achieve exact posterior samples of @xmath79 by taking weighted averages of the sub - samples",
    ". for simplicity , denote @xmath92 and @xmath93 for @xmath94 .",
    "suppose that lisa is applied to the linear regression model .",
    "suppose that each sub - posterior update for @xmath95 is multiplied by @xmath10 .",
    "set weight @xmath96 .",
    "then , if @xmath97 has pdf @xmath98 for any @xmath99 , then the weighted average @xmath100 has pdf @xmath101 .",
    "[ thm2 ]    since in this model , lisa s sub - posterior distributions of @xmath79 are normally distributed , taking weighted averages of sub - samples will also result in a normal distribution @xmath102 , with mean and variance @xmath103^{-1 } \\\\ & = \\left [ \\frac{k}{\\sigma^2}({x}^{t}x + k{\\omega_0}^ { * } ) \\right]^{-1}.\\end{aligned}\\ ] ] thus @xmath104 which implies @xmath105 \\\\ & = { \\sigma^*}\\left [ \\frac{k}{\\sigma^2}({x}^{t }   \\vec y_n + { \\omega_0}\\mu_0 ) \\right ] \\\\ & = \\frac{\\sigma^2}{k}{\\omega_n}^{-1 } \\frac{k}{\\sigma^2}({x}^{t } \\vec y_n + { \\omega_0}\\mu_0 ) = \\mu_n.\\end{aligned}\\ ] ] thus , lisa s weighted samples are distributed @xmath106 . comparing this to the full posterior distribution , @xmath107",
    ", we see that they are almost identical except that lisa has smaller variance by a factor of @xmath10 ( similar to its difference with batchsinglemachine ) .",
    "hence , the adjustment of @xmath80 into @xmath108 , will restore the variance to the variance of the sub - posterior to that of the full posterior distribution .    in the next section",
    ", we will examine lisa s performance on a more complex model , the bayesian additive regression trees ( bart ) .",
    "we will see similarities between applying lisa to bart and the bayesian linear regression model .",
    "hence theorem [ thm2 ] will play an important role in lisa s implementation for bart .",
    "consider the nonparametric regression model : @xmath109 where @xmath110 is a @xmath111-dimensional vector of inputs and @xmath112 is approximated by a sum of @xmath113 regression trees:@xmath114 where @xmath115 denotes a binary tree consisting of a set of interior node decision rules and a set of terminal nodes .",
    "@xmath116 is the set of parameter values associated with the @xmath117 terminal nodes of @xmath115 .",
    "in addition , @xmath118 is the function that maps each @xmath119 to a @xmath120 . thus the regression model is approximated by a sum - of - trees model @xmath121 let @xmath122 denote the vector of model parameters .",
    "below , we have briefly described the prior specifications stated in @xcite and @xcite . +",
    "* prior specifications : *    * prior independence and symmetry : @xmath123 p(\\sigma)\\ ] ] + where @xmath124 . * recommended number of trees : m=200 @xcite and m=50 @xcite * tree prior @xmath125 , is characterised by three aspects : 1 .",
    "the probability that a node at depth @xmath126 is non - terminal , which is assumed to have the form @xmath127 , where @xmath128   and @xmath129 .",
    "( recommended values are @xmath130 and @xmath131 ) 2 .   the distribution on the splitting variable assignments at each interior node which is recommended to have a uniform distribution .",
    "the distribution on the splitting rule assignment in each interior node , conditional on the splitting variable which is also recommended to have a uniform distribution . *",
    "the conditional prior for @xmath132 is @xmath133 such that : @xmath134 with @xmath135 recommended . *",
    "the prior for @xmath95 is @xmath136 where @xmath137 is recommended and @xmath138 is chosen such that @xmath139 with recommended @xmath140 and sample variance @xmath141 .",
    "hence the posterior distribution will have the form : @xmath142 \\bigg\\}}_\\text{prior}.\\end{gathered}\\ ] ] gibbs sampling is used to sample from this posterior distribution .",
    "the algorithm iterates between the following steps :    * @xmath143 + where @xmath144 and @xmath145 $ ] . + * @xmath146 which is the same as drawing from the conditional @xmath147 where @xmath148 denotes all trees except the @xmath12-th tree , and residual @xmath149 is defined as : @xmath150 the sampling of @xmath151 is performed in two steps : 1 .",
    "@xmath152 and 2 .",
    "+ step 2 involves sampling from each component of @xmath154 using @xmath155 where @xmath156 denotes the average residual ( computed without tree @xmath12 ) at terminal node @xmath157 with total number of observations @xmath158 .",
    "the conditional density of @xmath115 in step 1 can be expressed as : @xmath159    the metropolis - hastings ( mh ) algorithm is then applied to draw @xmath115 from ( [ eq:1 ] ) with four different proposal moves on trees :    * * grow : * growing a terminal node ( with probability 0.25 ) ; * * prune : * pruning a pair of terminal nodes ( with probability 0.25 ) ; * * change : * changing a non - terminal rule ( with probability 0.4 ) ( * ? ? ?",
    "* change rules only for parent nodes with terminal children ) ; * * swap : * swapping a rule between parent and child ( with probability 0.1 ) ( this proposal move was removed by * ? ? ?",
    "detailed derivations involving the metropolis - hastings acceptance ratios are described in the appendix .",
    "two existing packages in r , `` bayestree '' and `` bartmachine '' , can be used to run bart on any dataset , but as the sample size increases , these packages tend to run slower . in these situations",
    "we expect methods such as lisa or cmc to become useful , and for a fair illustration of the advantages gained we have used our own r implementation of bart and applied the same structure to implement lisa and cmc algorithm for bart .",
    "the metropolis - hastings acceptance ratios for lisa and cmc are also reported in the appendix .    as discussed by @xcite , the approximation to the posterior produced by the cmc algorithm",
    "can be poor .",
    "thus , for comparison reasons , we applied both lisa and cmc to bart using a simulated dataset ( described further ) with @xmath160 batches .",
    "given theorem 1 , since lisa s sub - posterior distributions are asymptotically equivalent to the full posterior distribution , we examined its performance by uniformly taking sub - samples from all its batches as an approximation to full posterior samples .",
    "we will see further that lisa with uniform weights produces higher prediction accuracy compared to cmc . however , they both perform poorly in approximating the posterior samples as they generate larger trees and under - estimate @xmath80 , which results in over - dispersed posterior distributions .",
    "the following sub - section discusses a modified version of lisa for bart which will have significant improvement in performance .",
    "the under estimation of @xmath95 when applying lisa to bart is similar to the problem encountered when using lisa for the linear regression model discussed in section [ linear ] .",
    "this is not a coincidence since bart is also a linear regression model , albeit one where the set of independent variables is determined through a highly sophisticated process",
    ". we will show below that when applying a similar variance adjustment to the one stated in theorem 2 , the modified lisa ( modlisa ) for bart will exhibit superior computational and statistical efficiency compared to either lisa or cmc .",
    "just like in the regression model we `` correct '' the sampling algorithm by adjusting the residual variance .",
    "we start with the conditional distribution of tree @xmath12 from expression which takes the form @xmath161 note that only the conditional distribution of the residuals , @xmath162 is affected by the modifications brought by lisa .",
    "note also that the metropolis - hastings acceptance ratios for tree proposals consists of three parts : the transition ratio , the likelihood ratio and the tree structure ratio . the likelihood ratio is constructed from the conditional distributions of residuals which is affected by lisa .",
    "consider the likelihood ratio for grow proposal in lisa ( full details are presented in the appendix ) @xmath163",
    "\\bigg\\ } \\label{eq : modif}\\end{gathered}\\ ] ] where @xmath164 is the total number of observations from batch - data that end up in terminal node @xmath165 .",
    "the newly grown tree , @xmath166 , splits terminal node @xmath165 into two terminal nodes ( children ) @xmath167 and @xmath168 , which will also divide @xmath164 to @xmath169 and @xmath170 which are the corresponding number of observations in each new terminal node . by factoring out @xmath10 in ,",
    "we can rewrite it as @xmath171   \\bigg\\}. \\label{eq : batchsingle}\\end{gathered}\\ ] ] expression in lisa is equivalent to batchsinglemachine except for the smaller variance considered for the conditional distribution of residuals ( @xmath172 ) , while in batchsinglemachine each residual has conditional distribution given as @xmath173 .",
    "hence , to preserve consistency and achieve similar variance for residuals as in batchsinglemachine , we will also need to modify lisa for bart by changing @xmath174 when updating _ trees _ and then taking a weighted average combination of sub - samples ( similar to bayesian linear regression  theorem 2 ) . note that in modlisa , we do nt apply any changes in updating @xmath80 , i.e. we keep the same conditional distribution as in lisa : @xmath175 where @xmath176 and @xmath177 $ ] .",
    "this is obviously different from the conditional distribution of @xmath80 in batchsinglemachine where there is no @xmath10 .",
    "all our numerical experiments show that , despite this difference , modlisa will still generate accurate predictions and the modification corrects the bias in the posterior draws of @xmath80 and properly calibrates the size of the trees .",
    "we have simulated data of size @xmath178 from friedman s test function @xcite @xmath179 where the covariates @xmath180 are simulated independently from a @xmath58 and @xmath181 with @xmath182 .",
    "note that five of the ten covariates are unrelated to the response variable .",
    "we have also generated test data containing 5000 cases .",
    "applying bart to this simulated dataset will generate posterior draws of @xmath183 which equivalently produces posterior draws for @xmath184 using the approximation @xmath185 for each @xmath180 .",
    "since in this simulated data the true @xmath112 is known , one can compute the root mean squared error ( rmse ) using average posterior draws of @xmath186 for each @xmath119 ( i.e. @xmath187 ) , as an estimate to measure its performance , i.e. rmse @xmath188 .",
    "it is known that singlemachine bart may mix poorly when it is run on an extremely large dataset with small residual variance .",
    "however since the data simulated is of reasonable size and @xmath189 is not very small the singlemachine bart is expected to be a good benchmark for comparison ( see discussion in * ? ? ?",
    "we have implemented modlisa , lisa , and cmc for bart with @xmath160 batches on the simulated data for 5000 iterations with a total of 1000 posterior draws .",
    "table [ modlisa ] shows results from all methods including the singlemachine which runs bart on the full dataset using only one machine .",
    "results are averaged over three different realizations of train and test data , and are reporting the train and test rmse for each method , along with average post burn - in @xmath80 estimates and tree sizes .",
    "in addition , table [ modlisa ] also includes the average train and test coverage of 95% prediction intervals , i.e. the proportion of 1000 newly simulated @xmath190 at a given train or test @xmath119 that is covered by its corresponding 95% prediction interval .",
    "as it is seen from table [ modlisa ] , lisa does a terrible job at estimating @xmath80 , its estimate being orders of magnitude smaller than the one produced by cmc .",
    "and although lisa has better prediction performance compared to cmc , we also observe much higher coverage probabilities for cmc . on the other hand ,",
    "cmc and lisa both generate larger trees compared to singlemachine , with cmc generating trees that are ten times larger than lisa s .",
    "overall , it is clear that neither cmc nor lisa exhibit desirable properties for bart .",
    "the story changes with modlisa with weighted average which dominates both cmc and lisa across all performance indicators since it has the lowest rmse , the highest coverage , the lowest tree sizes , and less biased @xmath80 estimates and produces results that are the closest to the ones produced by singlemachine .",
    "crrrrrrrc + method & & & & & & +   + @xmath191 & 2.73 & 2.94 & 45.71 % & 47.83 % & 1.91 & 602 + @xmath192 & 1.18 & 1.19 & 1.54 % & 1.54 % & 0.001 & 55 + @xmath193 & 0.57 & 0.59 & 92.93 % & 92.91 % & 7.97 & 7 + @xmath194 & 0.55 & 0.56 & 94.67 % & 94.65 % & 9.04 & 7 +    crrrc + method & & & +   + @xmath191 & 21% & 0.03% & 34% + @xmath195 & 1.8% & 0.5% & 1.6% + @xmath196 & 20% & 26% & 19% + @xmath197 & 9% & 10% & 6% +    the size of trees produced by each method is in sync with the average acceptance rates of each tree proposal move shown in table [ acc ] .",
    "it is noticeable the difference between cmc and lisa s average acceptance rates between growing a tree and pruning one .",
    "on the other hand , modlisa has overall larger acceptance rates with the smallest relative absolute difference between growing and pruning probabilities compared to lisa and cmc ( @xmath198% for modlisa , @xmath199% for cmc , and @xmath200% for lisa ) and is closest to singlemachine ( @xmath201% ) .",
    "overall , modlisa induced a significant reduction in tree sizes by preserving a balance between growing and pruning trees which also improves exploring the posterior distribution .",
    ".4   obtained via modlisa , lisa , and cmc with @xmath160 , to the one produced by singlemachine bart for two different pairs of train and test data.,title=\"fig : \" ]    .4   obtained via modlisa , lisa , and cmc with @xmath160 , to the one produced by singlemachine bart for two different pairs of train and test data.,title=\"fig : \" ]    .4   obtained via modlisa , lisa , and cmc with @xmath160 , to the one produced by singlemachine bart for two different pairs of train and test data.,title=\"fig : \" ]    .4   obtained via modlisa , lisa , and cmc with @xmath160 , to the one produced by singlemachine bart for two different pairs of train and test data.,title=\"fig : \" ]      in order to investigate the closeness of posterior samples in each method to the singlemachine bart , we have plotted in figure [ fig:4obsall ] the empirical distribution functions of @xmath186 generated from each algorithm for two pairs of observations in the train and test dataset .",
    "one can see that the empirical distribution functions in lisa and cmc do nt match the ones from singlemachine ( in both train and test data ) , and look over - dispersed as they cover a larger range of @xmath119 values within @xmath202 . however , the empirical distribution functions in modlisa weighted average look much closer to singlemachine with a similar dispersion but a slight shift in location .    .45   produced by singlemachine .",
    "grey areas represent the @xmath203 credible intervals constructed based on 100 replicates .",
    ", title=\"fig : \" ]    .45   produced by singlemachine .",
    "grey areas represent the @xmath203 credible intervals constructed based on 100 replicates .",
    ", title=\"fig : \" ]    .45   produced by singlemachine .",
    "grey areas represent the @xmath203 credible intervals constructed based on 100 replicates .",
    ", title=\"fig : \" ]    in order to assess the performance of the sampling procedures considered , we use the cramr - von mises distance to assess the difference between empirical distribution functions .",
    "this distance is defined to be @xmath204 where in our case we assume @xmath205 to be the empirical distribution function generated from posterior samples in singlemachine bart and @xmath206 is similarly computed for the alternative method that is considered for comparison .    using a set of @xmath207 equispaced points",
    ", we compute the average squared difference between the single machine and all other alternative methods for each observation in the dataset . to illustrate , for lisa we estimate @xmath208 using @xmath209 .",
    "figure [ fig : mmodlisa ] is comparing the fitted polynomial trends of @xmath210 ( in each method ) versus mean predicted @xmath186 in singlemachine with their corresponding @xmath203 credible regions ( for both train and test data ) .",
    "clearly in lisa and modlisa , there are small variations around the trends with no significant changes in values of @xmath210 among different mean predicted @xmath186 , which specifies consistency within different train or test observations . in addition , the gap between trends from train and test data indicate that the average distance between lisa / modlisa and singlemachine s distributions are smaller for test data compared to train data . furthermore , there are still small variations seen around cmc s trends , but with slight changes in values of @xmath210 among different mean predicted @xmath186 , especially for the test dataset which indicates inconsistency within different observations .",
    "0.5   in singlemachine ( for both train and test data).,title=\"fig : \" ]    0.5   in singlemachine ( for both train and test data).,title=\"fig : \" ]    to emphasize the difference in performance between modlisa and its competitors , figure [ fig : allmodlisa ] shows all the fitted polynomial trends without their credible regions for the train and test data .",
    "one can see that there is a large gap between @xmath211 values in modlisa weighted average and other alternative methods ( for both train and test data ) , with modlisa having the lowest value .",
    "thus the weighted average of samples produced by modlisa yields the closest results to singlemachine .",
    "this can also be justified by comparing average @xmath211 over all train observations for each trend which is calculated to be @xmath212 for modlisa that is significantly smaller than @xmath213 , @xmath214 for cmc , and lisa , respectively .",
    "similarly , the average @xmath211 over test data are @xmath215 , @xmath216 , and @xmath217 for modlisa , cmc , and lisa respectively , which again the smallest value is seen in modlisa .",
    "hence we conclude that modlisa weighted average generate closest to true posterior draws of bart and has the best performance among its alternative methods .",
    "at last we compare run time per iteration for each method so we can draw some conclusions regarding the overall efficiency .",
    "the main goal of methods such as lisa and cmc was to reduce run times regarding big data applications .",
    "here we have compared average run times per iteration ( from one processor ) for each method using our implementation of bart .",
    "crrc + method & & +   + @xmath191 & 11.99 & 31% + @xmath195 & 5.04 & 71% + @xmath196 & 1.81 & 90% + @xmath197 & 17.28 &  +    as it is seen in table [ speed ] , modlisa , lisa and cmc with @xmath160 are all faster compared to singlemachine since they are influenced by the smaller subsets of data used . however , since lisa and cmc generate much larger trees , they become slower compared to modlisa which is the fastest method .",
    "we have also reported the speed - up percentages with respect to singlemachine , which is defined to be @xmath218 where @xmath219 is the average time per iteration in each method . clearly , cmc has the least speed - up ( @xmath220 ) while modlisa has the highest ( @xmath221 ) , which counts as the most computational efficient method .        to see how the number of training data ( @xmath0 ) can effect the posterior accuracy ,",
    "we have examined the performance of all methods when @xmath0 is increased to 60,000 while we keep the same number of batches @xmath160 .",
    "tables [ 60kdata ] shows the results of 1000 posterior samples generated from fitting the bart model to the training set with additional 5000 data considered as test cases .",
    "crrrrrrrc + method & & & & & & +   + @xmath191 & 2.85 & 5.56 & 25.74 % & 17.28 % & 0.48 & 983 + @xmath192 & 1.17 & 1.19 & 0.84 % & 0.84 % & 0.0003 & 125 + @xmath193 & 0.41 & 0.42 & 94.54 % & 94.53 % & 8.82 & 7 + @xmath194 & 0.41 & 0.41 & 94.83 % & 94.84 % & 9.04 & 11 +    comparing table [ 60kdata ] to [ modlisa ] , it is not surprising that the train and test rmses in modlisa , lisa , and singlemachine decrease as @xmath0 increases . while lisa and cmc estimates for @xmath80 get worse , modlisa generates more accurate estimates of @xmath80 with a larger @xmath0 .",
    "trees have consistent size in modlisa , but tend to grow larger in cmc and lisa ( as @xmath0 rises ) .",
    "empirical coverages decrease in cmc and lisa , while they increase in modlisa and singlemachine for larger training data ( specifically , modlisa competes with singlemachine for larger @xmath0 ) .",
    "overall , as @xmath0 increases , modlisa seems to be a more reliable method as it has a better performance compared to all other alternatives .      to examine the effect of @xmath10 on posterior accuracy ,",
    "we have generated 1000 posterior draws from fitting the 20,000 dataset to each method with @xmath222 ( except singlemachine ) .",
    "the results are shown in table [ k10 ] .",
    "crrrrrrrc + method & & & & & & +   + @xmath191 & 2.92 & 3.18 & 31.08 % & 29.83 % & 0.73 & 951 + @xmath192 & 1.70 & 1.78 & 1.44 % & 1.43 % & 0.001 & 131 + @xmath193 & 0.46 & 0.47 & 94.30 % & 94.29 % & 8.69 & 7 + @xmath194 & 0.55 & 0.56 & 94.67 % & 94.65 % & 9.04 & 7 +    comparing table [ modlisa ] with @xmath160 to table [ k10 ] , we see that as @xmath10 decreases , the performance of lisa and cmc drops while modlisa generates stronger results , which is what we intuitively expect as each batch has more information when @xmath10 is smaller",
    ". we also note the promising advantage that modlisa exhibits over the singlemachine when @xmath222 in terms of rmse .",
    "consistency in performance of modlisa can also be seen when the underlying model is changed .",
    "for instance , we also considered a sample of size 20,000 using @xmath223 where @xmath224 is a four - dimensional input vector that is simulated independently from a @xmath58 and @xmath181 with @xmath225 .",
    "additional 5000 data have also been simulated as test cases .",
    "similarly , by fitting this newly simulated dataset to each method with @xmath160 , we have generated 1000 posterior samples with results averaged across three different realizations of data shown in table [ f2 ] .",
    "crrrrrrrc + method & & & & & & +   + @xmath191 & 0.89 & 0.76 & 49.74 % & 52.97 % & 0.21 & 614 + @xmath192 & 0.32 & 0.33 & 1.50 % & 1.49 % & 0.0001 & 57 + @xmath193 & 0.11 & 0.11 & 93.07 % & 93.18 % & 0.88 & 7 + @xmath194 & 0.14 & 0.14 & 94.82 % & 94.81 % & 1.00 & 7 +    again modlisa has the best performance among its alternatives , and its performance is closest to singlemachine .",
    "this confirms the previous simulation results and we conclude that modlisa is a more reliable method for bart models with large datasets .    in the next section",
    "we will apply modlisa weighted average bart to a big real data .",
    "the american community survey ( acs ) is a growing survey from the us census bureau and the public use microdata sample ( pums ) is a sample of responses to acs which consists of various variables related to people and housing units ( see * ? ? ?",
    "considering the person - level data from pums 2013 , we would like to predict a person s total income based on variables such as sex , age , education , class of worker , living state , and citizenship status .",
    "we have collected information related to people who are employed and have total income of at least $ 5000 with education level of either bachelor s degree , master s degree , or a phd which resulted in @xmath226 observations .",
    "we randomly divided the dataset into approximately @xmath227 training and @xmath228 testing sets , with @xmath229 batches considered for splitting the training data to apply modlisa .",
    "computations were performed on the gpc supercomputer at the scinet hpc consortium ( @xcite ) using 100 cores , each running on @xmath230 observations .",
    "considering the logarithm of total income for each person as the response variable , we have ran modlisa with weighted average and singlemachine bart on this dataset for 1500 iterations ( since singlemachine is very slow ) and discarded the first 1000 draws which resulted in 500 posterior samples .",
    "table [ acs ] is showing the results of test rmse as well as average post burn - in @xmath80 estimates and tree sizes .",
    "crrrrrc + method & & & & +   + @xmath193 & 0.71 & 0.488 & 7 & 90% + @xmath197 & 0.70 & 0.485 & 23 &",
    " +    as seen in table [ acs ] , test rmse in modlisa is similar to the one from singlemachine which indicates high prediction accuracy .",
    "however , modlisa shows significant advantage as it has @xmath221 speed - up with respect to singlemachine .",
    "crrrc + method & & & +   + @xmath196 & 10 % & 11 % & 14 % + @xmath197 & 8% & 7% & 7% +    this can be justified from table [ accmodlisa_acs ] that shows overall higher acceptance rates for each proposal in modlisa compared to singlemachine .",
    "the 90% speedup is in this case important as it takes more than a day to simulate from the posterior using singlemachine .",
    "the result indicate the potential of this method for reducing computational costs while producing accurate predictions .",
    "the challenge of using mcmc algorithms to sample posterior distributions obtained from a massive sample of observations is a serious one .    in this paper",
    ", we introduced a new method based on the idea of randomly dividing the data into batches and drawing samples from each of the resulting sub - posteriors independently and parallel on different machines .",
    "we propose a novel way to define the sub - posteriors and we develop a strategy to combine the samples produced by each batch analysis for the important class of bayesian additive regression trees models . for this model , the proposed methodology performs very well and shows reduction in computation time that are as high as 90% .    in future work",
    "we would like to find a procedure for combining the sub - posterior samples that will make lisa easy to adapt to a wide variety of models .",
    "we also hope that our paper will stimulate the research into this type of divide - and - conquer approaches for big data mcmc and will expand the research on how to construct the batch - specific sub - posteriors along with novel strategies of combining or weighting the samples obtained from each batch analysis .",
    "we thank the editor , grace yi , the guest editor , richard lockhart , and three anonymous referees for helpful suggestions that have greatly improved the paper .",
    "this work has been supported by nserc of canada grants to rvc and jsr .",
    "chris loken , daniel gruner , leslie groer , richard peltier , neil bunn , michael craig , teresa henriques , jillian dempsey , ching - hsing yu , joseph chen , et  al .",
    "scinet : lessons learned from building a power - efficient top-20 system and data centre . in _ journal of physics : conference series _ ,",
    "volume 256 , page 012026 .",
    "iop publishing , 2010 ."
  ],
  "abstract_text": [
    "<S> markov chain monte carlo ( mcmc ) sampling from a posterior distribution corresponding to a massive data set can be computationally prohibitive since producing one sample requires a number of operations that is linear in the data size . in this paper </S>",
    "<S> , we introduce a new communication - free parallel method , the _ likelihood inflating sampling algorithm ( lisa ) _ , that significantly reduces computational costs by randomly splitting the dataset into smaller subsets and running mcmc methods _ independently _ in parallel on each subset using different processors . </S>",
    "<S> each processor will be used to run an mcmc chain that samples sub - posterior distributions which are defined using an  inflated \" likelihood function . </S>",
    "<S> we develop a strategy for combining the draws from different sub - posteriors to study the full posterior of the bayesian additive regression trees ( bart ) model . </S>",
    "<S> the performance of the method is tested using both simulated and real data .    * _ keywords : _ * bayesian additive regression trees ( bart ) , bayesian inference , big data , consensus monte carlo , markov chain monte carlo ( mcmc ) . </S>"
  ]
}