{
  "article_text": [
    "there is a pressing need for accurate estimates of galaxy photometric redshifts ( photo  z s ) as demonstrated by the increasing number of papers on this topic and especially by recent attempts to objectively compare methods ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "the need for photo - z s will only increase as larger and deeper surveys such as pan - starrs@xcite , lsst@xcite and euclid @xcite come on  line in the coming decade .",
    "the photometric  only surveys ( pan - starrs , lsst ) will have relatively small numbers of follow - up spectroscopic redshifts and will rely upon either template - fitting methods such as bayesian photo - z s @xcite le phare @xcite , or training - set methods such as those discussed herein . the euclid mission may include a slitless spectrograph offering far more training  set galaxies .    a diverse set of regression techniques using training  set methods",
    "have been applied to the problem of estimating photometric redshifts in the past 10 years .",
    "these include artificial neural networks @xcite , decision trees @xcite , gaussian process regression @xcite , support vector machines @xcite , ensemble modeling @xcite , random forests @xcite , and kd  trees @xcite to name but a few .    on the other hand , even though self  organizing maps ( soms ) have been used extensively in a number of other scientific fields ( the paper that opened the field , @xcite , currently has over 2000 citations ) they have been used sparingly thus far in astronomy ( e.g. * ? ? ? * ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and only this year in estimating photometric redshifts @xcite .    in this work we attempt to use soms to estimate photometric redshifts for several sloan digital sky survey ( sdss , *",
    "* ) derived catalogs of different galaxy types , including quasars along with the phat0 data set of @xcite . in section [ sec : data ] we describe the input data sets used , in section [ sec : methods ] we give an overview of soms , and some conclusions in section [ sec : conclusion ] .",
    "three different data sets derived from the sdss data release seven ( dr7 , * ? ? ?",
    "* ) were used .",
    "they include the main galaxy sample ( mgs , * ? ? ?",
    "* ) the luminous red galaxy sample ( lrg , * ? ? ?",
    "* ) , and the quasar sample ( qso , * ? ? ?",
    "data from the galaxy zoo @xcite data release 1 @xcite survey results were used to segregate galaxies as spiral or elliptical in the case of the mgs and lrg samples .",
    "details of how this was done are given in @xcite .",
    "dereddened magnitudes ( u , g , r , i , z ) were used as inputs in all scenarios .",
    "the same sdss photometric and redshift quality flags on the input variables were used as in @xcite .",
    "in addition we used the simulation ",
    "based phat0 data set ( see * ? ? ?",
    "* ) which was constructed to to test a variety of different photo  z estimation methods .",
    "the phat0 data set consists of 5 sdss like filters ( u , g , r , i , z ) used on megacam at cfht @xcite with an additional 6 input filters ( y , j , h , k , spitzer irac [ 3.6 ] , spitzer irac [ 4.5 ] ) giving a total of 11 filters spanning a range of 4000 to 50,000 .",
    "this large range should help to avoid color ",
    "redshift degeneracies that can occur if ultraviolet or infrared bandpasses are not used @xcite .",
    "the phat0 synthetic photometry was created from the le phare photo - z code @xcite .",
    "initially le phare creates noise free data , but given the desire to test more real  world conditions we utilized the phat0 data with added noise .",
    "a parametric form was used for the signal  to  noise as a function of magnitude where it acts as an exponential at fainter magnitudes and a power ",
    "law a brighter ones .",
    "the magnitude cut between these two regimes is filter dependent and is given in table 2 of @xcite .",
    "the larger of two catalogs was used herein ( as suggested for training  set methods ) that contains @xmath3 170,000 objects .    since we use a training ",
    "set method our original data sets are split into training=89% , testing=10% and validation=1% .",
    "validation was only used in the artificial neural network algorithm discussed in the next section .",
    "the full size of each input data set are listed in parentheses in column 1 of table [ tbl-1 ] .",
    "several methods in use for calculating photometric redshifts were compared with the som results : the artificial neural network code of @xcite ( annz ) , the gaussian process regression code of @xcite ( gpr ) , as well as simple linear and quadratic regression .",
    "the latter is comparable to that of the polynomial fits used by @xcite .",
    "both the annz and gpr codes are freely downloadable .",
    "details on the annz and gpr algorithms can be found in their respective citations above .",
    "llcccc mgs & gpr & 0.02087 & 0.02072 & 0.02096&0.11629 + ( 455803)&annz & 0.02044 &  &  & 0.14482 +  & som & 0.02339 &  &  & 0.1689 +  & linear & 0.02742 & 0.02729 & 0.02758 & 0.35986 +  & quadratic & 0.02494 & 0.02412 & 0.02762 & 0.29184 + lrg & gpr & 0.02278 & 0.02256 & 0.02309 & 0.41898 + ( 143221 ) & annz & 0.02138 &  &  & 0.41176 +  & som & 0.02689 &  &  & 0.64292 +  & linear & 0.02896 & 0.02896 & 0.02897 & 0.71516 +  & quadratic & 0.02382 & 0.02376 & 0.02402 & 0.45510 + mgs  ell & gpr & 0.01455 & 0.01434 & 0.01473 & 0.06591 + ( 45521 ) & annz & 0.01442 &  &  & 0.06591 +  & som & 0.02044 &  &  & 0.10984 +  & linear & 0.01745 & 0.01731 & 0.01756 & 0.19772 +  & quadratic & 0.01612 & 0.01609 & 0.01629 & 0.10984 + mgs  sp & gpr & 0.02078 & 0.02061 & 0.02093 & 0.13305 + ( 120266 ) & annz & 0.01991 &  &  & 0.05821 +  & som & 0.02426 &  &  & 0.04158 +  & linear & 0.02539 & 0.02529 & 0.02555 & 0.28272 +",
    " & quadratic & 0.02326 & 0.02296 & 0.02607 & 0.20788 + lrg  sp & gpr & 0.01416 & 0.01397 & 0.01436 & 0.00000 + ( 13708 ) & annz & 0.01516 &  &  & 0.00000 +  & som & 0.01848 &  &  & 0.07299 +  & linear & 0.01635 & 0.01627 & 0.01649 & 0.07299 +  & quadratic & 0.01469 & 0.01462 & 0.01477 & 0.00000 + lrg  ell & gpr & 0.01186 & 0.01162 & 0.01224 & 0.00000 + ( 27378 ) & annz & 0.01298 &  &  & 0.10961 +  & som & 0.01568 &  &  & 0.00000 +  & linear & 0.01362 & 0.01361 & 0.01364 & 0.10961 +  & quadratic & 0.01263 & 0.01254 & 0.01274 & 0.07307 + qso & gpr & 0.37342 & 0.03967 & 0.37626 & 50.96627 + ( 56923 ) & annz & 0.65802 &  &  & 88.54533 +  & som & 0.41821 &  &  & 54.23401 +  & linear & 0.57061 & 0.57010 & 0.57102 & 84.64512 +  & quadratic & 0.53972 & 0.53679 & 0.54539 & 81.27196 + phat0 & gpr & 0.01487 & 0.01436 & 0.01532 & 0.03539 + ( 169520 ) & ann & 0.01805 &  &  & 0.05309 +  & som & 0.02236 &  &  & 0.37754 +  & linear & 0.08703 & 0.08702 & 0.08704 & 19.34875 +  & quadratic & 0.02436 & 0.02433 & 0.02438 & 0.19467 +    the main purpose of self ",
    "organized mapping is the ability of soms to transform a feature vector of arbitrary dimension drawn from the given feature space of photometric inputs ( e.g. , the sdss u , g , r , i , z magnitudes ) into simplified 1 or 2dimensional discrete maps .",
    "the method was originally developed by @xcite to organize information in a logical manner .",
    "this type of machine learning utilizes an unsupervised learning scheme of vector quantization , known as competitive learning in the field of neural information processing .",
    "it is useful for analyzing complex data with a ",
    "priori unknown relationships that are visualized by the self - organization process @xcite .",
    "a som is structured in two layers : an input layer and a kohonen layer ( figure [ fig1 ] ) .",
    "for example , the kohonen layer could represent a structure with a single 2dimensional map ( lattice ) consisting of neurons arranged in rows and columns .",
    "each neuron of this discrete lattice is fixed and is fully connected with all source neurons in the input layer . for the given task of estimating photometric redshifts , a 5dimensional feature vector of the u , g , r , i , z magnitudes is defined .",
    "one feature vector ( u , g , r , i , z ) is presented to 5 input layer neurons .",
    "this typically activates ( stimulates ) one neuron in the kohonen layer .",
    "learning occurs during the self  organizing procedure as feature vectors drawn from a training data set are presented to the input layer of the som network ( figure [ fig1]a ) .",
    "these feature vectors are also referred to as input vectors .",
    "neurons of the kohonen layer compete to see which neuron will be activated by the weight vectors that connect the input neurons and kohonen neurons . in other words , the weight vectors identify which input vector can represented by a single kohonen neuron .",
    "hence , they are used to determine only one activated neuron in the kohonen layer after the winner  takes  all principle ( figure [ fig1]b ) .",
    "the som is considered as trained after learning , at which time the weights of the neurons have stored the inter  relations of all 5dimensional u , g , r , i , z feature vectors .",
    "then , known spectroscopic redshift values for all input vectors of a test data set that are represented by a single kohonen neuron are averaged ( fig.[fig1]b ) .",
    "the redshift mean value represents all 5d u , g , r , i , z vectors that are similar to the weight vector of the activated kohonen neuron .",
    "the more kohonen neurons there are the more precisely each input vector can be represented by a weight vector . however , the total number of kohenen neurons are optimized for each data set ( see figure [ fig2 ] ) . a practical overview about the learning / training process is described by @xcite and in much greater detail by @xcite .",
    "after training , the u , g , r , i , z input vectors of a test data set are presented to a trained som . at the end of a classification step ,",
    "every kohonen neuron approximates an input vector whereby similar / dissimilar input data were represented by neighboring / distant neurons .",
    "one neuron could even classify several input vectors , if these input vectors were very similar compared to the other given input vectors .",
    "results from the photometric redshift approximations are then compared to known spectroscopic redshift data .",
    "regression performance is estimated based on the root mean square error ( rmse ) of the predicted photometric redshifts and the known spectroscopic redshifts ( using @xmath0z = z@xmath1z@xmath2 ) .",
    "to reiterate , during the training phase , each kohonen neuron identifies a certain number of galaxies that are characterized by similar u , g , r , i , z intensities .",
    "photometric redshift data were then averaged for these intensity values .",
    "the som approximates the input feature space and maps it into an output space .",
    "the output space shows the som approximation as a 2-d map @xcite .",
    "best results can be obtained with an optimization scheme such that the rmse of the test data set is minimal as illustrated in figure [ fig2 ] .",
    "accuracy ( e.g. rmse ) depends on the size of the kohonen map .",
    "the number of neurons in the kohonen map can be considered a regularization parameter ( @xmath4 ) as shown in figure [ fig2 ] .",
    "figure [ fig2 ] shows that rmse is high when the number of kohonen neurons is too small ( @xmath52000 ) or too large ( @xmath610000 ) and hence that the 5dimensional u , g , r , i , z  input space is underfit or overfit .",
    "theoretically , a global minimum of the rmse  curve might exist .",
    "however , the input feature space for the given photometric redshift problem shows a very rough rmse  curve ( figure [ fig2 ] ) with at least 2 local minima . in this case",
    "it is clear that sdss redshift estimation tends to have several local minima , which makes is important to chose the right optimization method to determine the som network size . on the other hand ,",
    "the smoother the rmse ",
    "curve is the better gradient methods can be utilized .",
    "evolution strategies or genetic programming could be applied to rougher curves with many local minima .",
    "this in turn can make it cumbersome to find fast back  propagation artificial neural network ( ann ) structures , especially when data sets are small .",
    "another advantage of soms in comparison to anns is that there is no need to optimize the structure of soms ( e.g. , number of hidden layers ) , since it is based on unsupervised learning .",
    "only the size of the kohonen map needs to be optimized for each data set .",
    "soms also allow non  experts to visualize the redshift estimates in relation to the multi  dimensional input space .",
    "this eliminates the often criticized  black box \" problem of anns . as mentioned previously , soms approximate the input feature space while anns typically separate them into sub",
    " regions .",
    "finally , soms are known to be powerful when very small data sets are available for training ( see , * ? ? ?",
    "input neurons and @xmath7 kohonen neurons .",
    "the som visualizes the structure of the @xmath8dimensional input space . in this case",
    ", the som illuminates a certain redshift@xmath9error within the kohonen map and as a function of the input space . ]     for the lrg  ell data set ( see table [ tbl-1 ] ) .",
    "different classifications will result from different choices of the @xmath4 value .",
    "the regularization value is defined by the number of kohonen neurons , which is maximum with respect to the training data set .",
    "the convex curve has a two local minima at @xmath4=4100 and @xmath4=5100 .",
    "the roughness of this rmse cost function shows that traditional gradient based optimization strategies , e.g. deterministic annealing , might result in sub  optimal solutions .",
    "other methods , such as , genetic programming might find the global minimum much faster . ]    [ cols=\"^,^ \" , ]",
    "soms offer a competitive choice in terms of low rmse , algorithm comprehension ( also see @xcite ) and percentage of outliers .",
    "the final results are presented in table [ tbl-1 ] and plots for the lrg ",
    "ell data set for the som , annz and gpr methods are shown in figure [ fig3 ]    as mentioned previously , obtaining the global minimum is important and , not surprisingly , can affect the results .",
    "figure [ fig2 ] shows the two local minima ( @xmath4=4100 and 5100 ) listed for the lrg  ell ( luminous red galaxies classified as ellipticals by galaxyzoo ) data set in table [ tbl-1 ] .",
    "clearly there are a number of other @xmath4values and the rmse will be greatly affected by the choice as seen on the y  axis of figure [ fig2 ] for a given @xmath4value .",
    "given these facts , the roughness of the rmse cost function in figure [ fig2 ] shows that traditional gradient based optimization strategies , e.g , deterministic annealing , might yield sub  optimal solutions .",
    "other methods , such as , genetic programming might find the ",
    "global \" minimum much faster , if a global minima exists with respect to the uncertainties of the rmse .    during completion of this manuscript another paper using soms for classification and photometric estimation",
    "was released @xcite .",
    "our work differs in that we mostly focus on a wider variety of low ",
    "redshift samples drawn from the sdss , while @xcite focuses more on the higher redshift samples akin to those used in @xcite .",
    "we have shown that soms are a powerful tool for estimating photometric redshifts and that with additional work they are sure to be useful in future surveys with limited numbers of follow  up spectroscopic redshifts .",
    "funding for the sdss has been provided by the alfred p. sloan foundation , the participating institutions , the national aeronautics and space administration , the national science foundation , the u.s .",
    "department of energy , the japanese monbukagakusho , and the max planck society .",
    "the sdss web site is http://www.sdss.org/.    the sdss is managed by the astrophysical research consortium for the participating institutions .",
    "the participating institutions are the university of chicago , fermilab , the institute for advanced study , the japan participation group , the johns hopkins university , los alamos national laboratory , the max  planck  institute for astronomy , the max  planck ",
    "institute for astrophysics , new mexico state university , university of pittsburgh , princeton university , the united states naval observatory , and the university of washington .",
    "gppert , j. & rosenstiel , w. 1993 , `` self - organizing maps vs. backpropagation : an experimental study '' , proc . of workshop on design methodologies for microelectronis and signal processing , pp . 153162 , giwice , poland ."
  ],
  "abstract_text": [
    "<S> we present an unsupervised machine learning approach that can be employed for estimating photometric redshifts . the proposed method is based on a vector quantization approach called self  organizing mapping ( som ) . </S>",
    "<S> a variety of photometrically derived input values were utilized from the sloan digital sky survey s main galaxy sample , luminous red galaxy , and quasar samples along with the phat0 data set from the photo - z accuracy testing project . </S>",
    "<S> regression results obtained with this new approach were evaluated in terms of root mean square error ( rmse ) to estimate the accuracy of the photometric redshift estimates . </S>",
    "<S> the results demonstrate competitive rmse and outlier percentages when compared with several other popular approaches such as artificial neural networks and gaussian process regression . </S>",
    "<S> som rmse  results ( using @xmath0z = z@xmath1z@xmath2 ) for the main galaxy sample are 0.023 , for the luminous red galaxy sample 0.027 , quasars are 0.418 , and phat0 synthetic data are 0.022 . </S>",
    "<S> the results demonstrate that there are non  unique solutions for estimating som rmses . </S>",
    "<S> further research is needed in order to find more robust estimation techniques using soms , but the results herein are a positive indication of their capabilities when compared with other well - known methods . </S>"
  ]
}