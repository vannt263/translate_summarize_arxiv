{
  "article_text": [
    "compressed sensing ( cs ) is a relatively new area of signal processing that has recently received a large amount of attention .",
    "the main idea is that many real - world signals ( e.g. , those sparse in some transform domain ) can be reconstructed from a relatively small number of linear dot - product measurements .",
    "its roots lie in the areas of statistics and signal processing @xcite , but it is also very much related to previous work in computer science @xcite and applied mathematics @xcite .",
    "cs is also very closely related to error correcting codes , and can be seen as source coding using linear codes over real numbers @xcite .    in this paper , we analyze the performance of low - density parity - check ( ldpc ) codes with verification decoding @xcite as applied to cs . the resulting decoding algorithm is almost identical to that of sudocodes @xcite , but a more suitable code ensemble is chosen and a more precise analysis is presented .",
    "since most of the interesting applications of cs require very sparse ( or compressible ) signals , the natural mapping to coding implies high - rate codes .",
    "one new characteristic of this analysis , which is also interesting from a coding perspective , is that the performance estimates hold uniformly as the code rate approaches one .",
    "this allows us to explore the sparsity ( or rate ) regime that makes sense for compressed sensing .",
    "an important implication of this work is that our randomized reconstruction system allows linear - time reconstruction of strictly - sparse signals with a constant oversampling ratio .",
    "in contrast , all previous reconstruction methods with moderate reconstruction complexity have an oversampling ratio which grows logarithmically with the signal dimension .",
    "ldpc codes are linear codes introduced by gallager in 1962 @xcite and re - discovered by mackay in 1995 @xcite .",
    "binary ldpc codes are now known to be capacity approaching on various channels when the block length tends to infinity .",
    "they can be represented by a tanner graph , where the @xmath4-th variable node is connected to the @xmath5-th check node if the entry on the @xmath4-th column and @xmath5-th row of its parity - check matrix is non - zero .",
    "ldpc codes can be decoded by an iterative _ message - passing _ ( mp ) algorithm which passes messages between the variable nodes and check nodes iteratively .",
    "if the messages passed along the edges are probabilities , then the algorithm is also called _ belief propagation _ ( bp ) decoding .",
    "the performance of the mp algorithm can be evaluated using density evolution ( de ) @xcite and stopping set analysis @xcite @xcite .",
    "each method provides a decoding threshold for the code ensemble .",
    "section [ sec : background ] provides background information on coding and cs .",
    "section [ sec : main - results ] summarizes the main results . in section [ sec : de - asymptotic - analysis ] , proofs and details are given for the main results based on de . while in section",
    "[ sec : stopping - set - asymptitic ] , proofs and details are provided for the main results based on stopping - set analysis . section [ sec : sparsecsit ] discusses a simple information - theoretic bound on the number of measurements required for reconstruction . section [ sec : simulation - results ] presents simulation results comparing the algorithms discussed in this paper with a range of other algorithms .",
    "finally , some conclusions are discussed in section [ sec : conclusion ] .",
    "[ author s note : the equations in this paper were originally typeset for two - column presentation , but we have submitted it in one - column format for easier reading",
    ". please accept our apologies for some of the rough looking equations . ]",
    "the sparse graph representation of ldpc codes allows encoding and decoding algorithms to be implemented with linear complexity in the code length @xmath6 . since ldpc codes are usually defined over the finite field @xmath7 instead of the real numbers , we need to modify the encoding / decoding algorithm to deal with signals over real numbers .",
    "each entry in the parity - check matrix is either 0 or a real number drawn from a continuous distribution .",
    "the parity - check matrix @xmath8 will be full - rank with high probability ( w.h.p . ) and is used as the measurement matrix in the cs system ( e.g. , the signal vector @xmath9 is observed as @xmath10 ) .",
    "the process of generating the observation symbols can also be seen in a bipartite tanner graph representation .",
    "1 shows the encoder structure .",
    "each non - zero entry in @xmath11 is the edge - weight of its corresponding edge in this graph .",
    "therefore , the observation process associated with a degree @xmath12 check node is as follows :    1 .",
    "encoding : the observation symbol is the weighted sum ( using the edge weights ) of the @xmath12 neighboring signal components .    in this work , we only consider strictly sparse signals and we use two decoders based on verification which were proposed and analyzed in @xcite .",
    "the second algorithm was also proposed independently for cs in @xcite .",
    "the decoding process uses the following rules :    1 .",
    "if a measurement is zero , then all the neighboring variable nodes are verified as zero .",
    "if a check node is of degree one , verify the variable node with the value of the measurement .",
    "[ enhanced verification ] if two check nodes overlap in a single variable node and have the same measurement value , then verify that variable node to the value of the measurement .",
    "4 .   remove all verified variable nodes and the edges attached to them by subtracting out the verified values from the measurements .",
    "repeat steps 1 - 4 until decoding succeeds or makes no further progress .",
    "note the first algorithm follows steps 1 , 2 , 4 and 5 .",
    "the second algorithm follows steps from 1 to 5 .",
    "these two algorithms correspond to the first and second algorithms in @xcite and are referred to as lm1 and node - based lm2 ( lm2-nb ) in this paper . note that ldpc codes with regular check degree and poisson symbol degree with lm2-nb decoding is identical to the sudocodes introduced in @xcite . in @xcite , the lm2-nb algorithm which is an enhanced version of message - based lm2 ( lm2-mb )",
    "is analyzed precisely .",
    "in general , the scheme described above does not guarantee that all verified symbols are actually correct .",
    "the event that a symbol is verified but incorrect is called false verification ( fv ) . in order to guarantee there are no fvs",
    ", one can add a constraint on the signal such that the weighted sum , of any subset of a check node s non - zero neighbors , does not equal to zero @xcite @xcite .",
    "another approach ( e.g. , taken in this paper ) is to consider random signals with continuous distributions so that fv occurs with probability zero .",
    "finally , if the measured signal is assumed to be non - negative , then fv is impossible for the lm1 and lm2-nb decoding algorithms .",
    "verification decoding was originally introduced and analyzed for the @xmath1-sc .",
    "it is based on the observation that , over large alphabets , the probability that `` two independent random numbers are equal '' is quite small .",
    "this leads to the _ verification assumption _ that any two matching values ( during decoding ) are generated by the same set of non - zero coefficients .",
    "the primary connection between cs , codes over real numbers , and verification decoding lies in the fact that :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ the verification assumption applies equally well to both large discrete alphabets and the real numbers .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _      based on the bipartite graph structure , ldpc codes can be decoded efficiently using iterative mp algorithms .",
    "the average performance of mp decoding algorithms can be analyzed with density evolution ( de ) @xcite or extrinsic information transfer ( exit ) charts @xcite .",
    "the concentration theorem @xcite shows that random realizations of decoding are close to the average behavior w.h.p . as the block length goes to infinity .",
    "de analysis provides a threshold below which decoding ( or reconstruction ) succeeds w.h.p .. the decoding threshold can be improved by optimizing the edge degree distribution ( d.d . )",
    "pair @xmath13 and @xmath14 .",
    "decoding can also be analyzed using combinatorial methods such as stopping set analysis @xcite and @xcite .",
    "stopping set analysis gives a threshold below which _ all _ error patterns can be recovered with certainty under the assumption of no fv .",
    "note that de and stopping set analysis lead to different thresholds in general .",
    "since stopping set analysis implies uniform recovery of all the error patterns , instead of just most of them , the threshold given by stopping set analysis is always lower than the one given by de .",
    "for example , de analysis of @xmath15 regular codes on the bec shows that erasure patterns with size less than @xmath16 of the code length can be corrected w.h.p .",
    "@xcite . on the other hand , the result from stopping set analysis guarantees that most codes correct all erasure patterns with size less than @xmath17 of the code length when @xmath18 .    likewise , in cs systems",
    ", there are two standard measures of reconstruction : _ uniform _ reconstruction and _ randomized _ ( or _ non - uniform _ ) reconstruction .",
    "a cs system achieves randomized reconstruction if _ most _ randomly chosen measurement matrices recover _ most _ of the signals in the signal set . while a cs system achieves uniform reconstruction if a measurement matrix and the decoder recover _ all _ the signals in the signal set with certainty .",
    "another criterion , which is between uniform reconstruction and randomized reconstruction , is what we call _ uniform - in - probability _ reconstruction .",
    "a cs system achieves uniform - in - probability reconstruction if , for any signal in the signal set , _ most _ randomly chosen measurement matrices achieve successful decoding .",
    "since de and the concentration theorem lead to w.h.p .",
    "statements for mp decoding over all signals and graphs , it is natural to adopt a de analysis to evaluate the performance of randomized reconstruction cs systems based on ldpc codes . for uniform reconstruction ,",
    "a stopping set analysis of the mp decoder is the natural choice .",
    "while this works for the bec , the possibility of fv prevents this type of strong statement for verification decoding .",
    "if the non - zero entries of @xmath11 are chosen randomly from a continuous distribution , however , then the probability of fv is zero for all signals .",
    "therefore , one can use stopping set analysis to analyze mp decoding of ldpc code ensembles and show that the ldpc codes with mp decoding achieves uniform - in - probability reconstruction in the cs system .",
    "the reader is cautioned that these results are somewhat brittle , however , because they rely on exact calculation and measurement of real numbers .",
    "understanding cs systems requires one to consider how the system parameters ( e.g. , the number of measurements and the sparsity of the signal ) scale in the regime where the signal is both high - dimensional and extremely sparse . to compare results ,",
    "we focus on the _ oversampling ratio _ ( i.e. , the number of measurements divided by the number of non - zero elements in the signal ) required for reconstruction .",
    "this leads us to a scaling law approach to the standard de and stopping set analysis .      in cs ,",
    "optimal decoding ( in terms of oversampling ratio ) requires a combinatorial search that is known to be np - hard @xcite .",
    "practical reconstruction algorithms tend to either be based on linear programming ( e.g. , basis pursuit ( bp ) @xcite ) or low - complexity iterative algorithms ( e.g. , orthogonal matching pursuit ( omp ) @xcite ) .",
    "a wide range of algorithms allow one to trade - off the oversampling ratio for reconstruction complexity .",
    "in @xcite , ldpc codes are used in the cs system and the algorithm is essentially identical to the verification based decoding proposed in @xcite .",
    "the scaling law analysis shows the oversampling ratio for ldpc codes based cs system can be quite good .",
    "encoding / decoding complexity is also a consideration .",
    "ldpc codes have sparse bipartite graph representation so that encoding and decoding algorithms with complexity linearly with the code length can be developed .",
    "there are several existing mp decoding algorithms for ldpc codes over non - binary fields . in @xcite and @xcite ,",
    "an analysis is introduced to find provably capacity - achieving codes for erasure channels under mp decoding .",
    "metzner presents a modified majority - logic decoder in @xcite that is similar to verification decoding .",
    "davey and mackay develop and analyze a symbol - level mp decoder over small finite fields @xcite .",
    "two verification decoding algorithms for large discrete alphabets are proposed by luby and mitzenmacher in @xcite and called as lm1 and lm2 in this paper .",
    "( two capacity - achieving algorithms are presented by shokrollahi and wang in @xcite and are denoted as sw1 and sw2 in this paper . )",
    "the list - message - passing ( lmp ) algorithm @xcite provides a smooth trade - off between the performance and complexity of sw1 and sw2 .",
    "these algorithms are summarized in @xcite .",
    "one can get a rough idea of the performance of these algorithms by comparing their performance for the standard @xmath15-regular ldpc code .",
    "a standard performance measure is the noise threshold ( or sparsity threshold for cs ) below which decoding succeeds with high probability .",
    "the threshold of the lm1 algorithm in this case is @xmath19 .",
    "this means that a long random @xmath15-regular ldpc code will correct a @xmath1-sc error pattern with high probability as long as the error rate is less than @xmath19 .",
    "likewise , it means that using the same code for lm1 reconstruction of a strictly - sparse signal with succeed w.h.p .",
    "as long as the sparsity rate ( i.e. , fraction of non - zero elements ) of the signal vector is less than @xmath19 .",
    "the lm2-mb algorithm improves this threshold to @xmath20 and the lm2-nb algorithm further improves this threshold to @xmath21 @xcite .    likewise , the stopping - set analysis of the lm1 algorithm in section v shows that a @xmath15-regular code exists where lm1 succeeds ( ignoring fv ) for all error ( or sparsity ) patterns whose fraction of non - zero entries is less than @xmath22 . in comparison , the bec stopping - set threshold of the @xmath15 code is @xmath23 for erasure patterns .",
    "however , both of these thresholds can be increased significantly ( for the same code rate ) by increasing the variable node degree .",
    "in fact , the @xmath24-regular ldpc code gives the best ( both lm1 and bec ) stopping - set thresholds and they are ( respectively ) @xmath25 and @xmath26 .",
    "finally , if the signal is non - negative , then fv is not possible during lm1 decoding and therefore @xmath25 is a lower bound on the true lm1 rate-@xmath27 threshold for uniform reconstruction .",
    "[ fig : bounds ] shows the decoding / recovery thresholds for bec stopping set analysis , lm1 stopping set analysis and lm1 de analysis .     thresholds vs 1-@xmath28 , where @xmath28 is the code rate , for lm1 stopping set / de analysis and the bec stopping set analysis . ]    note that if the signal coefficients are non - negative , the threshold of lm1 given by stopping set analysis is comparable to the strong bound given in ( * ? ? ?",
    "1(a ) ) , and the threshold of lm1 given by de analysis is comparable to the weak bound given in ( * ? ? ?",
    "1(b ) ) .",
    "since the scaling law analysis becomes somewhat tedious when complicated algorithms are applied , we consider only the @xmath0-regular code ensemble and the relatively simple algorithms lm1 and lm2-mb .",
    "the rather surprising result is that even with regular codes and simple decoding algorithms , the scaling law implies that ldpc codes with verification decoding perform very well in cs systems with strictly - sparse signals .      there are some significant differences between coding theory and cs .",
    "one of them is the signal model .",
    "the first difference is that coding theory typically uses discrete alphabets ( see @xcite for one exception to this ) while cs deals with signals over the real numbers .",
    "fortunately , some codes designed for large discrete alphabets ( e.g. , the @xmath1-ary symmetric channel ) can be adapted to the real numbers . by exploring the connection and the analogy between real field and finite field with large @xmath1 , the cs system can be seen as an essentially a syndrome - based source coding system @xcite . using the parity - check matrix of a non - binary ldpc code as the measurement matrix",
    ", the mp decoding algorithm can be used as the reconstruction algorithm .",
    "the second difference in the signal model is that cs usually models the sparse signal @xmath29 as a random vector in an @xmath30-dimensional unit weak @xmath31 ball which is denoted as @xmath32 .",
    "note that the weak @xmath31 ball of radius @xmath33 consists of vectors @xmath34 whose decreasing rearrangement , denoted as @xmath35 , satisfies @xmath36 with @xmath37 .",
    "the constraint @xmath38 defines the approximate sparsity property of the signal . as @xmath39 approaches zero ,",
    "the decreasing rearrangement of the @xmath34 coefficients must decay faster . for decoding ,",
    "the constants @xmath39 and @xmath33 provide a partial ordering of the set @xmath40 and allow one to regularize the ill - conditioned decoding problem .    in information theory and coding theory ,",
    "the most commonly used signal model is a probabilistic model ; the signal is treated as a random variable and the pdf is used to describe the signal .",
    "the sparsity of the signal can also be captured in the probabilistic model .",
    "for example , we can use a weighted sum of a dirac delta function at zero and a very wide uniform distribution to model a strictly sparse signal .      in coding theory ,",
    "the code rate depends on the application and the interesting rate regime varies from close to zero to almost one . in cs systems ,",
    "the signal is sparse in some domain and becomes increasingly sparse as the dimension increases .",
    "intuitively , this means we can use codes with very little redundancy or very high code rate to represent the signal .",
    "so the interesting rate regime for cs systems is the high - rate regime .",
    "we consider the relationship between the system parameters and how they scale as the rate goes to one .",
    "the answer lies in the scaling law analysis developed in this paper .",
    "the main results of this paper are listed as follows .",
    "the details follow in section [ sec : de - asymptotic - analysis ] and section [ sec : stopping - set - asymptitic ] .",
    "note that all results hold for randomly - chosen regular ldpc codes with variable - degree @xmath5 and check - degree @xmath41 . for a given @xmath5 , we can increase @xmath41 and observe how the decoding threshold scales .",
    "this provides a scaling law for the threshold and leads to conditions for successful reconstruction and the converse .",
    "one consequence of this is that randomized reconstruction can be achieved , for cs of strictly - sparse signals , when the number of measurements scales linearly with the sparsity of the signal .",
    "\\(i ) [ de - bec ] for the bec , there is a @xmath42 such that : a check - regular ldpc codes with average variable node degree @xmath43 and check - degree @xmath41 can recover a @xmath44 fraction of erasures ( w.h.p .",
    "as @xmath18 ) when @xmath45 .",
    "the constant @xmath46 ( independent of @xmath41 ) is essentially the fraction of the optimal @xmath47 achieved as @xmath18 and the rate goes to one .",
    "conversely , if the erasure probability @xmath48 , then decoding fails ( w.h.p . as @xmath18 ) for all @xmath41 .",
    "\\(ii ) [ ss - bec ] for any @xmath49 , there is a @xmath42 such that : for all @xmath50 , a @xmath0-regular ldpc code with @xmath51 can recover all erasures ( w.h.p . as @xmath18 ) of size @xmath52 .",
    "\\(iii ) [ de-@xmath1-sc - lm1 ] for the @xmath1-sc , when one chooses a code randomly from the @xmath0 regular ensemble with @xmath43 and uses lm1 as decoding algorithm , then there is a @xmath53 such that one can recover almost all error patterns of size @xmath54 for @xmath55 ( w.h.p . as @xmath18 ) for all @xmath56 .",
    "conversely , when @xmath57 there is a @xmath58 such that the decoder fails ( w.h.p . as @xmath18 ) for all @xmath59 .",
    "\\(iv ) [ de-@xmath1-sc - lm2-mb ] for the @xmath1-sc ,",
    "when one chooses a code randomly from the @xmath0 regular ensemble with @xmath51 and uses lm2-mb as decoding algorithm , then there is a @xmath53 such that one can recover almost all error patterns of size @xmath54 for @xmath60 ( w.h.p . as @xmath18 ) .",
    "the constant @xmath46 ( independent of @xmath41 ) is essentially the fraction of the optimal @xmath47 achieved as the rate goes to one .",
    "conversely , there is a @xmath58 such that the decoder fails ( w.h.p . as @xmath18 ) when @xmath61 for all @xmath59 .",
    "\\(v ) [ ss-@xmath1-sc - lm1 ] for any @xmath49 , there is a @xmath42 such that : for all @xmath45 , a @xmath0-regular ldpc code with @xmath51 using lm1 decoding can recover ( w.h.p as @xmath18 ) all @xmath1-sc error patterns of size @xmath62 if no false verifications occur .",
    "note that the constants @xmath63 , @xmath64 , @xmath65 and @xmath46 in ( i ) , ( ii ) , ( iii ) , ( iv ) and ( v ) are different , but for the simplicity of expression , we use the same notation .    the rest of the paper is organized as follows . in section",
    "[ sec : de - asymptotic - analysis ] , we derive the high - rate scaling based on de analysis .",
    "we first show the high - rate scaling analysis for the bec under mp decoding .",
    "then , the analysis of high - rate scaling for the @xmath1-sc with lm1 and lm2-mb decoding algorithms is shown . in section",
    "[ sec : stopping - set - asymptitic ] , we derive the stopping set analysis for the @xmath1-sc with lm1 decoding algorithm and the high - rate scaling .",
    "the simulation results are shown in section [ sec : simulation - results ] .",
    "de analysis provides an explicit recursion which connects the distributions of messages passed from variable nodes to check nodes at two consecutive iterations of mp algorithms . in the case of bec",
    ", this task has been accomplished in @xcite and @xcite .",
    "it has been shown that the expected fraction of erasure messages which are passed in the @xmath4-th iteration , called @xmath66 , evolves as @xmath67 where @xmath68 is the erasure probability of the channel .",
    "for general channels , the recursion may be much more complicated because one has to track the general distributions which can not be represented by a single parameter @xcite .    to illustrate the scaling law",
    ", we start by analyzing the bec case using de .",
    "although this is not applicable to cs , it motivates the scaling law analysis for the @xmath1-sc which is related to cs .",
    "the scaling law of ldpc codes of check - regular ensemble over the bec is shown by the following theorem .",
    "[ thm : bec - alpha - threshold]consider a sequence of check - regular ldpc codes with fixed variable node degree distribution @xmath13 and increasing check degree @xmath41 .",
    "let @xmath69 be the average symbol degree and @xmath70 , which is called @xmath71-threshold , be the largest @xmath71 such that @xmath72 for @xmath73 $ ] . for the erasure probability @xmath74 , the iterative decoding of a randomly chosen length-@xmath6 code from this ensemble fails ( w.h.p as @xmath18 ) for all @xmath41 if @xmath75 .",
    "conversely , if @xmath76 , then there exists a @xmath42 such that iterative decoding succeeds ( w.h.p as @xmath18 ) for all @xmath50 .",
    "[ lem : monotonictoexp ] for all @xmath77 and @xmath78 , the sequence @xmath79 is strictly increasing in @xmath41 and @xmath80    [ proof of lemma [ lem : monotonictoexp ] ] we restrict our attention to @xmath81 because the proof is simplified in this case and the continuation does not require @xmath82 .",
    "we show that @xmath83 is strictly increasing with @xmath41 by considering the power series expansion of @xmath84 , which converges if @xmath78 .",
    "this gives @xmath85 and keeping only the first term shows that @xmath86 . since all the terms are negative and decreasing with @xmath41 , we see that @xmath83 is strictly increasing with @xmath41 . since @xmath83 is convex in @xmath34 for @xmath78 , the lower bound @xmath87 follows from tangent lower bound at @xmath88 .",
    "[ proof of theorem [ thm : bec - alpha - threshold ] ] using the change of variable , @xmath89 , the de recursion can be scaled to get @xmath90 by lemma [ lem : monotonictoexp ] , @xmath91 increases monotonically ( for @xmath92 ) to @xmath93 , and we see that @xmath94 decreases monotonically to @xmath95 . if @xmath75 , then ( by the definition of @xmath70 ) @xmath96 for some @xmath97 $ ] . since @xmath98",
    ", the recursion @xmath99 will not converge to zero ( from @xmath100 ) and iterative decoding will fail for all @xmath41 w.h.p . as @xmath18 .",
    "if @xmath76 , then @xmath101 for @xmath97 $ ] . since @xmath102 , we can choose @xmath42 to be the first @xmath41 such that @xmath103 for @xmath97 $ ] .",
    "in this case , the recursion @xmath99 will converge to zero ( from @xmath100 ) for all @xmath50 and iterative decoding will succeed w.h.p . as @xmath18 .",
    "the following proposition determines a few @xmath71-thresholds explicitly .",
    "[ cor : bec - alpha - thr]for @xmath0 regular ldpc codes , the @xmath71-threshold is given by @xmath70 with @xmath104 , @xmath105 , and @xmath106 .",
    "see appendix [ sec : appendix1 ] .",
    "for example , if @xmath107 and @xmath108 , then numerical results show that @xmath109 suffices so that de converges for all @xmath110 when @xmath111 .",
    "note that in the proof of theorem [ thm : bec - alpha - threshold ] , we make use of the standard concentration theorem @xcite for mp decoding . for example , the `` w.h.p . ''",
    "statements rely on the concentration theorem",
    ". however , the concentration theorem for the bec de analysis proved in @xcite holds only for fixed @xmath41 ( i.e. , @xmath41 is independent of @xmath6 ) . in many cs applications ,",
    "the desired number of measurements is sublinear in the block length ( i.e. , @xmath112 with @xmath113 ) . in this section ,",
    "we discuss how the proof of the concentration theorem can be modified to handle this case .",
    "the concentration theorem @xcite shows the performance of a randomly chosen code concentrates around its expected value so that@xmath114 where @xmath115 is the r.v . which is equal to the number of variable - to - check erasure messages in the @xmath116-th iteration , @xmath117/n_{e}$ ] is the expected erasure rate ,",
    "@xmath118 is the number of edges in the graph , and @xmath119 is a constant which depends on the degree distribution . let @xmath120 be the number of edges in the depth-@xmath121 directed neighborhood of the @xmath4-th edge , then the constant @xmath119 can be chosen to be @xmath122 , where @xmath123 is a constant independent of @xmath6 and @xmath116 . for regular ensembles ,",
    "the number @xmath120 is independent of @xmath4 and this implies @xmath124 concentration occurs as @xmath18 for fixed @xmath5 , @xmath41 and @xmath116 .    in this section ,",
    "we consider the case where the check - node degree increases with @xmath6 .",
    "in this case , the concentration theorem is not informative if @xmath41 grows faster than @xmath125 .",
    "but , the following theorem shows that the concentration theorem can be modified to handle the scaling law .",
    "[ thm : tm2]for check - degree @xmath112 and erasure rate @xmath126 , the performance ( in terms of the fraction of erasure messages ) of a code randomly chosen from the @xmath0-regular ensemble over bec concentrates around its expected value exponentially in @xmath127 where @xmath116 is the number of decoding iterations .",
    "mathematically , there exist constants @xmath128 and @xmath129 such that @xmath130 for any @xmath131 and @xmath132 .",
    "see the appendix [ sec : appendixtm2 ] .        for the simplicity of our analysis",
    ", we only consider @xmath0-regular code ensemble and the lm1 decoding algorithm @xcite for the @xmath1-sc with error probability @xmath68 .",
    "the de recursion for lm1 is ( from @xcite)@xmath133^{k-1}\\right)^{j-1}\\!\\!\\!\\!\\!\\!\\!\\!\\!,\\label{eq : de_lm1}\\ ] ] where @xmath66 is the fraction of unverified messages in the @xmath4-th iteration .",
    "our analysis of the scaling law relies on the following lemma .",
    "[ lem : let - the - functions]let the functions @xmath134 and @xmath135 be defined by @xmath136^{k}\\biggr)^{j-1}\\\\ \\vspace{-10mm}\\end{gathered}\\ ] ] and    @xmath137^{k}\\right)^{j-1}\\!\\!\\!\\!\\!\\!,\\ ] ] where @xmath138 , @xmath139 $ ] , and @xmath140 . for @xmath73 $ ] and @xmath141 , these functions satisfy ( i ) @xmath142 , ( ii ) @xmath143 is monotonically decreasing with @xmath41 for @xmath141 , and ( iii ) @xmath144 .",
    "see the appendix [ sec : appendix2 ] .",
    "[ thm : thm2 ] consider a sequence of @xmath0-regular ldpc codes with fixed symbol degree @xmath43 and increasing check degree @xmath41 .",
    "let @xmath46 be the largest @xmath71 such that @xmath145 for @xmath73 $ ] .",
    "if the sparsity of the signal is @xmath54 for @xmath146 and @xmath147 , then there exists a @xmath64 such that by randomly choosing a length-@xmath6 code from the @xmath0 regular ldpc code ensemble , lm1 reconstruction succeeds ( w.h.p as @xmath18 ) for all @xmath148 .",
    "conversely , if @xmath149 then there exists a @xmath65 such that lm1 reconstruction fails ( w.h.p as @xmath18 ) for all @xmath59 .",
    "scaling ( [ eq : de_lm1 ] ) using the change of variables @xmath146 and @xmath150 gives @xmath151 .",
    "the function @xmath143 also allows us to define the upper bound @xmath152 where @xmath153 implies @xmath154 .",
    "since @xmath155 increases monotonically to @xmath93 , we see that @xmath156 decreases monotonically to @xmath157 . if @xmath76 , then @xmath158 for all @xmath97 $ ] . since @xmath159 , we can choose @xmath53 to be the first @xmath41 such that @xmath160 for all @xmath97 $ ] . in this case , the recursion @xmath151 will converge to zero ( from @xmath100 ) for all @xmath148 and iterative decoding will succeed w.h.p . as @xmath18 .",
    "if @xmath75 , then ( by the definition of @xmath70 ) @xmath161 for some @xmath97 $ ] . since @xmath162",
    ", there exists a @xmath65 such that , for all @xmath163 , the recursion @xmath151 will not converge to zero ( from @xmath100 ) and iterative decoding will fail w.h.p . as @xmath18 .",
    "if a randomly chosen code from the @xmath0 regular ensemble is applied to a cs system with lm1 reconstruction , then randomized reconstruction succeeds ( w.h.p as @xmath18 ) when the sparsity is @xmath54 with @xmath55 .",
    "this requires @xmath164 measurements with an oversampling ratio of @xmath165 .",
    "the following lemma shows how to calculate the constants in front of the scaling laws .",
    "[ lem : lm1_alpha]@xmath166 , @xmath167 @xmath168 and @xmath169 .",
    "see appendix [ sec : appendix3 ] .    for regular ldpc codes and lm1 reconstruction , choosing @xmath170 gives a uniform lower bound on the oversampling ratio ( as @xmath171 ) of @xmath172 .",
    "the minimum oversampling ratio @xmath173 and we choose @xmath170 , taking the logarithm of both sides shows that @xmath174      for the second algorithm in @xcite , the de recursion for the fraction @xmath66 of unverified messages in the @xmath4-th iteration is @xmath175 like the analysis of lm1 , we first introduce a lemma to bound the scaled de equation .    [ lem : lm2_bound]the functions @xmath176 and @xmath177 are defined as    @xmath178 where @xmath179 , i.e. , @xmath180 , and    @xmath181    for @xmath73 $ ] and @xmath182 , these functions satisfy ( i ) @xmath183 , ( ii ) @xmath184 where @xmath185 and ( iii ) @xmath177 is a monotonically decreasing function of @xmath41 .",
    "see the appendix [ sec : appendix4 ] .",
    "[ thm : consider - a - sequence]consider a sequence of @xmath0-regular ldpc codes with variable node degree @xmath51 .",
    "let @xmath70 be the largest @xmath71 such that @xmath186 for @xmath73 $ ] .",
    "if the sparsity of the signal is @xmath54 with @xmath187 and @xmath76 , then there exists a @xmath64 such that lm2-mb reconstruction succeeds ( w.h.p as @xmath18 ) for all @xmath148 .",
    "conversely , if @xmath75 then there exists a @xmath65 such that lm2-mb decoding fails ( w.h.p as @xmath18 ) for all @xmath59 .",
    "the lm2-mb de recursion is given by ( [ eq : lm2de ] ) .",
    "using the change of variables @xmath188 and @xmath126 , the scaled de equation can be written as @xmath189 taking the limit as @xmath190 gives @xmath191 .",
    "if @xmath76 , then the definition of @xmath46 implies that @xmath158 for @xmath97 $ ] . since @xmath159 ( by lemma [ lem : lm2_bound ] ) , we can choose @xmath53 to be the first @xmath41 such that @xmath160 for @xmath97 $ ] . in this case",
    ", the recursion @xmath151 will converge to zero ( from @xmath100 ) for all @xmath148 and iterative decoding will succeed w.h.p . as @xmath18 .",
    "if @xmath75 , then ( by the definition of @xmath70 ) @xmath161 for some @xmath97 $ ] . in this case",
    ", there is a @xmath65 and @xmath192 such that @xmath193 for all @xmath59 , and the recursion @xmath151 does not converge to zero ( from @xmath100 ) and iterative decoding will fail w.h.p . as @xmath18 .    for @xmath194 ,",
    "the quantity @xmath195 is undefined because @xmath196 .",
    "this implies that @xmath197 regular ldpc codes do not obey this scaling law for lm2-mb decoding .",
    "if a randomly chosen code from the @xmath0 regular ensemble is applied to a cs system with lm2-mb reconstruction , then randomized reconstruction succeeds ( w.h.p as @xmath18 ) when the sparsity is @xmath54 with @xmath60 .",
    "this requires @xmath198 measurements and an oversampling ratio of @xmath199 .",
    "for @xmath200 regular ldpc codes , the @xmath71-threshold of lm2-mb is given by @xmath70 and can be calculated numerically to get @xmath201 @xmath202 and @xmath203 .",
    "the interesting part of this result is that the number of measurements needed for randomized _ reconstruction with lm2-mb scales linearly with the sparsity _ of the signal .",
    "all previous reconstruction methods with reasonable complexity require a super - linear number of measurements .",
    "note that the proof of theorem [ thm : thm2 ] also depends on the standard concentration theorem in @xcite . like the bec case",
    ", the result becomes non -",
    "informative if @xmath41 grows faster than @xmath125 .",
    "therefore , we conjecture ( i.e. , give without proof ) the following concentration result for lm1 decoding for a randomly chosen code and @xmath1-sc error pattern . for @xmath112 and @xmath146 , the fraction of unverified messages after @xmath116 iterations concentrates around its expected value @xmath204 .",
    "mathematically , there exist constants @xmath205 and @xmath129 such that @xmath206 for any @xmath131 and @xmath132 .    to show the concentration theorem , the idea is similar with the one used in the bec case .",
    "we first reduce the graph to a smaller one by removing the verified edges in the first iteration , then prove the concentration for the residual graph .",
    "the probability that an edge is not removed in the first iteration is @xmath207 so the average number of edges which are not removed in the first iteration is @xmath208 .",
    "note that the average number of check nodes which are not removed is @xmath209 .",
    "therefore , the normalized ( by the number edges connected to the remaining check nodes ) probability of an edge to remain is @xmath210 if we substitute @xmath211 for @xmath68 in ( [ eq : eps ] ) and use a taylor expansion around @xmath212 we have @xmath213 .",
    "so the probability that a check node has degree @xmath214 after removal is @xmath215 which converges to poisson distribution with mean @xmath216 as @xmath217 following the approach used for the bec case shows that the concentration theorem also holds for lm1 on the @xmath1-sc .",
    "de analysis provides the threshold below which the _ randomized _ ( or _ non - uniform _ ) recovery is guaranteed , in the following sense : the signal and the measurement matrix are both chosen randomly , and w.h.p .",
    "the reconstruction algorithm gives the correct answer .",
    "if the reconstruction algorithm is guaranteed to succeed for all signals of sufficient sparsity , this is called _ uniform _ recovery . on the other hand , if reconstruction algorithm is uniform over all support sets of sufficient sparsity , but succeeds w.h.p . over the amplitudes of the non - zero elements ( i.e.",
    ", has a small but non - zero failure probability based on amplitudes ) , then the reconstruction is called _ uniform - in - probability _ recovery .    according to the analysis in section [ sec : de - asymptotic - analysis ] , we know that the number of measurements needed for randomized recovery by using lm2-mb scales linearly with the sparsity of the signal .",
    "still , the reconstruction algorithm may fail due to the support set ( e.g. , it reaches a stopping set ) or due to the non - zero amplitudes of the signal ( e.g. , a false verification occurs ) .    in this section",
    ", we will analyze the performance of mp decoding algorithms with uniform - in - probability recovery in the high - rate regime .",
    "this follows from a stopping set analysis of the decoding algorithms .",
    "a stopping set is defined as an erasure pattern ( or internal decoder state ) from which the decoding algorithm makes no further progress . following the definition in @xcite ,",
    "we let @xmath218 be the tanner graph of a code where @xmath219 is the set of variable nodes , @xmath220 is the set of check nodes and @xmath221 is the set of edges between @xmath219 and @xmath222 a subset @xmath223 is a bec stopping set if no check node is connected to @xmath224 via a single edge . the scaling law below uses the average stopping - set enumerator for ldpc codes as a starting point .",
    "the _ average stopping set distribution _",
    "@xmath225 is defined as the average ( over the ensemble ) number of stopping sets with size @xmath226 in a randomly chosen @xmath0 regular code with @xmath6 variable nodes .",
    "the _ normalized stopping set distribution _",
    "@xmath227 is defined as @xmath228 the _ critical stopping ratio _",
    "@xmath229 is defined as @xmath230 intuitively , when the normalized size of the stopping set is greater than or equal to @xmath231 the average number of stopping sets grows exponentially with @xmath232 when the normalized size of the stopping set is less than @xmath231 the average number of stopping sets decays exponentially with @xmath6 .",
    "in fact , there exist codes with no stopping sets of normalized size less than @xmath229 .",
    "therefore , the quantity @xmath229 can also be thought of as a _ deterministic decoding threshold .",
    "_    the normalized average stopping set distribution @xmath227 for @xmath0 regular ensembles on the bec is given by @xcite @xmath233 where @xmath234 is the entropy of a binary distribution and the bound holds for any @xmath235 .",
    "the optimal value @xmath236 is the unique positive solution of @xmath237 this gives the following theorem .    for any @xmath49 ,",
    "there is a @xmath42 such that , for all @xmath45 , a randomly chosen @xmath0 regular ldpc code ( @xmath238 ) will ( w.h.p . as @xmath18 ) correct all erasure patterns of size less than @xmath239 .",
    "[ sketch of proof ] since there is no explicit solution for @xmath236 , we use a 2nd order expansion of the lhs of ( [ eq : bec_ss_x0 ] ) around @xmath88 and solve for @xmath34 .",
    "this gives @xmath240 .",
    "since @xmath241 holds for all @xmath242 , we have@xmath243 next we expand the rhs of ( [ eq : scaling_bec_gamma ] ) around @xmath244 and neglect the high order terms ; solving for @xmath71 gives an upper bound on the critical stopping ratio@xmath245 it can be shown that this bound on @xmath229 is tight as @xmath190 .",
    "this means that , for any @xmath49 , there is a @xmath63 such that @xmath246 for all @xmath247 .",
    "therefore , the critical stopping ratio @xmath229 scales like @xmath248 as @xmath190 .",
    "although the threshold is strictly increasing with @xmath5 , this ignores the fact that the code rate is decreasing with @xmath5 .",
    "however , if one optimizes the oversampling ratio instead , then the choice of @xmath249 is nearly optimal . moreover , it leads to the simple formula @xmath250 and an oversampling ratio which grows logarithmically with @xmath41 .      a stopping set for lm1",
    "is defined by considering a decoder where @xmath251 are disjoint subsets of @xmath219 corresponding to verified , correct , and incorrect variable nodes .",
    "decoding progresses if and only if ( i ) a check node has all but one edge attached to @xmath252 or ( ii ) a check node has all edges attached to @xmath253 .",
    "otherwise , the pattern is a stopping set . in the stopping set analysis for @xmath1-sc",
    ", we can define @xmath254 as the _ average number of stopping sets _ with @xmath255 correctly received variable nodes and @xmath256 incorrectly received variable nodes where @xmath6 is the code length .    the average number of stopping sets @xmath254 can be computed by counting the number of ways , @xmath257 , that @xmath123 correct variable nodes , @xmath258 incorrect variables nodes , and @xmath259 verified variable nodes can be connected to @xmath260 check nodes to form a stopping set .",
    "the number @xmath257 can be computed using the generating function for one check , @xmath261 which enumerates the number of edge connection patterns ( `` 1 '' counts verified edges , `` @xmath34 '' counts correct edges , and `` @xmath192 '' counts incorrect edges ) that prevent decoder progress .",
    "generalizing the approach of @xcite gives @xmath262 where @xmath263    for this work , we are mainly interested in largest @xmath119 for which @xmath254 goes to zero as @xmath18 . since the growth ( or decay )",
    "rate of @xmath254 is exponential in @xmath6 , this leads us to consider the _ normalized average stopping set distribution _ @xmath264 which is defined as @xmath265 likewise , the _ critical stopping ratio _",
    "@xmath266 is defined as @xmath267:w_{j , k}(\\beta)>0\\}\\ ] ] where @xmath268}\\gamma_{j , k}(\\alpha,\\beta).\\ ] ] note that @xmath269 describes the asymptotic growth rate of the number of stopping sets with number of incorrectly received nodes @xmath270 the number of stopping sets with size less than @xmath271 decays exponentially with @xmath6 and the ones with size larger than @xmath271 grows exponentially with @xmath232    the normalized average stopping set distribution @xmath264 for lm1 can be bounded by    @xmath272 where the tightest bound is given by choosing @xmath273 to be the unique positive solution of @xmath274 and@xmath275    starting from ( [ eq : average_number_ss ] ) and using stirling s formula , it can be verified easily that @xmath276 where @xmath234 is the entropy of a ternary distribution .",
    "using a chernoff - type bound for @xmath257 ( i.e. , @xmath277 for all @xmath278 ) , we define @xmath279 minimizing the bound over @xmath280 gives @xmath281 where ( @xmath282 is the unique positive solution of ( [ eq : x0 ] ) and ( [ eq : y0 ] ) .",
    "one can also show that the bound is exponentially tight in @xmath6 .      in cs literature , we are only interested in the scenario that @xmath119 is small .",
    "it means we need to perform stopping set analysis in the high - rate regime or to the signal vectors with sparse support . for the convenience of analysis",
    ", we only derive the analysis for @xmath0 regular codes though it can be generalized to irregular codes @xcite . in our analysis ,",
    "the variable node degree @xmath5 is fixed and the check node degree @xmath41 is increasing . by calculating the scaling law of @xmath269",
    ", we find the uniform - in - probability recovery decoding threshold @xmath266 which tells us the relationship between the minimum number of measurements needed for uniform - in - probability recovery and the sparsity of the signal .",
    "the following theorem shows the scaling law of lm1 for the @xmath1-sc .",
    "numerical evaluation @xmath283 and theoretical bound @xmath284    [ thm : lm1ss]there is a code from @xmath0 regular ldpc code ensemble and a constant @xmath63 such that for the @xmath1-sc , all error patterns of size @xmath54 for @xmath285 can be recovered by lm1 ( w.h.p . as @xmath18 ) for @xmath45 where @xmath286 is the unique positive root on @xmath287 of the following implicit function @xmath288 where @xmath289 .",
    "[ lem : stp_lm1_1 ] consider sequences of @xmath290 given by ( [ eq : x0 ] ) and ( [ eq : y0 ] ) which satisfy @xmath291 as @xmath41 goes to infinity . in this case , the implied @xmath292 , @xmath293 , and @xmath294 all tend to zero .",
    "see the appendix [ sec : appendix5 ] .",
    "[ lem : for - the  sc]for the @xmath1-sc with lm1 decoding and @xmath238 , the average number of stopping sets with size sublinear in @xmath6 goes to zero as @xmath18 .",
    "more precisely , for each @xmath295 there exists a @xmath296 such that @xmath297    see the appendix [ sec : proof - of - lemma7 ] .",
    "[ proof of theorem [ thm : lm1ss]]the main idea of the proof is to start from ( [ eq : gamma ] ) and find a scaling law for @xmath269 as @xmath41 grows .",
    "since @xmath269 is the exponent of the average number of stopping sets and the resulting scaling function @xmath298 is negative in the range @xmath299 , almost all codes have no stopping sets of size @xmath54 with @xmath300 . because finding the limiting function of the scaled @xmath269 is mathematically difficult , we first find an upper bound on @xmath269 and then analyze the limiting function of this upper bound .",
    "before we make any assumptions on the structure of @xmath34 and @xmath301 we note that picking any @xmath34 and @xmath192 gives an upper bound of @xmath302 to make the bound tight , we should pick good values for @xmath34 and @xmath303 for example , the ( @xmath280 ) which leads to the tightest bound is the positive solution of ( [ eq : x0 ] ) and ( [ eq : y0 ] ) . since we are free to choose the variables @xmath34 and @xmath192 as we like , we assume @xmath34 and @xmath192 decay with the same rate and are both equal to @xmath304 so that the taylor expansions of ( [ eq : x0 ] ) and ( [ eq : y0 ] ) converge .    applying taylor expansion for small @xmath280 to",
    "( [ eq : x0 ] ) and ( [ eq : y0 ] ) , we have@xmath305 solving these equations for @xmath34 and @xmath192 gives the approximations@xmath306 next , we choose @xmath307 for @xmath308 , which requires that @xmath309 . and",
    "this leads to the scaling of @xmath280 .",
    "the scaling of @xmath280 also implies that @xmath309 .",
    "so we see that , although there exist stopping sets with @xmath310 , they do not occur in the scaling regime we consider . ]",
    "applying these substitutions to ( [ eq : gamma ] ) gives @xmath311 which equals@xmath312 plugging @xmath313 into this equation for @xmath314 gives @xmath315    scaling the rhs of ( [ eq : gamma_3 ] ) by @xmath316 gives the limiting function @xmath317 next , we maximize the scaled upper bound of @xmath264 over @xmath71 by maximizing @xmath318 over @xmath287 . the resulting function @xmath319 is a scaled upper bound on @xmath269 as @xmath41 goes to infinity .",
    "taking the derivative w.r.t .",
    "@xmath287 , setting it to zero , and solving for @xmath12 gives the unique solution @xmath320 since the second derivative @xmath321 is negative , so we have found a maximum .",
    "moreover , @xmath298 is given implicitly by ( [ eq : v(c , d ) ] ) and ( [ eq : d_fun ] ) .",
    "the only positive root of @xmath298 is denoted @xmath286 and is a constant independent of @xmath322 fig .",
    "[ fig : v(d ) ] shows the curves given by numerical evaluation of the scaled @xmath269 , which is given by @xmath323 and the limiting function @xmath324    the proof is not yet complete , however , because we have not yet considered stopping sets whose sizes are sublinear in @xmath6 .",
    "to handle these , we use lemma [ lem : for - the  sc ] , which shows that the average number of stopping sets with size sublinear in @xmath6 also goes to zero .    in a cs system with strictly sparse signals and lm1 reconstruction",
    ", we have uniform - in - probability reconstruction ( w.h.p . as @xmath18 ) of all signals with sparsity at most @xmath54 where @xmath285 .",
    "this requires @xmath164 measurements and an oversampling rate of @xmath325 .",
    "if the signal has all non - negative components , then the verification based algorithm will have no fv because the neighbors of a check node will sum to zero only if these neighbors are exactly zero .",
    "therefore , the above analysis implies uniform recovery of non - negative signals that are sufficiently sparse .",
    "at first glance , the results in this paper seem to be at odds with existing lower bounds on the number of measurements required for cs . in this section",
    ", we explore the fundamental conditions for linear scaling using sparse measurements from an information theoretic point of view .",
    "let @xmath41 and @xmath5 be check and symbol degrees ; let @xmath6 be the number of symbol nodes and @xmath326 be the number of check symbol nodes .",
    "the random signal vector @xmath327 has i.i.d .",
    "components drawn from @xmath328 and the random measurement vector is @xmath329 .",
    "the number of non - zero elements in the signal is controlled by assuming that the average number of non - zero symbol nodes attached to a check node is given by @xmath330 .",
    "this allows us to write @xmath331 , where @xmath332 is the random variable associated with a non - zero signal element . since @xmath333 , the condition @xmath113 implies @xmath190 and that the number of non - zero symbol nodes attached to a check node becomes poisson with mean @xmath330 .",
    "therefore , the amount of information provided by the measurements is given by    @xmath334    since @xmath335 is the average fraction of non - zero symbol nodes , the entropy of the signal vector can be written as@xmath336 this implies that @xmath337 since a necessary condition for reconstruction is @xmath338 , we therefore find that @xmath339 is required for reconstruction .",
    "this implies , that for any cs algorithm to work , either @xmath340 has to be infinite or @xmath5 has to grow at least logarithmically with @xmath232 this does not conflict with the analysis of lm2-mb in randomized reconstruction because , for signals over real numbers or unbounded alphabets , the entropy @xmath340 can be infinite .",
    "simulation results for zero - one sparse signals of length 256 with 128 measurements . ]    in this section , we provide the simulation results of lm1 , lm2-mb and lm2-nb reconstruction algorithms and compare these results with other reconstruction algorithms .",
    "we consider two types of strictly sparse signals .",
    "the first type is the zero - one sparse signal where the entries of the signal vector are either 0 or @xmath341 .",
    "the second type is the gaussian sparse case where the entries of the signal are either 0 or a gaussian random variable with zero mean and unit variance .",
    "we choose the signal length @xmath342 and number of measurements @xmath343    we compare different recovery algorithms such as linear - programming ( lp ) @xcite , subspace pursuit ( sp ) @xcite , regularized orthogonal matching pursuit ( romp ) @xcite , reweighted @xmath344 minimization ( rwlp-@xmath345 ) @xcite , lm1 , lm2-mb and lm2-nb .",
    "the measurement matrices for lm1 , lm2-mb and lm2-nb are generated randomly from the @xmath15 , ( 4,8 ) and ( 5,10 ) ensembles without double edges and 4-cycles .",
    "we also pick the non - zero entries in the measurement matrices to be i.i.d .",
    "gaussian random variables . in all other algorithms ,",
    "the measurement matrices are i.i.d .",
    "gaussian random matrices with zero mean and unit variance .",
    "each point is obtained by simulating 100 blocks .",
    "[ fig : sim_zero - one ] shows the simulation results for the zero - one sparse signal and fig .",
    "[ fig : sim_gaussian ] shows the results for gaussian sparse signal . from the results we can see lm2-mb and",
    "lm2-nb perform favorably when compared to other algorithms .",
    "simulation results for gaussian sparse signals of length 256 with 128 measurements . ]",
    "simulation of high rate scaling of @xmath346 and @xmath347 ensembles for block length @xmath348 . ]",
    "another interesting observation is that lm1 , lm2-mb and lm2-nb are not sensitive to the magnitudes of the non - zero coefficients .",
    "they perform almost the same for zero - one sparse signal and gaussian sparse signal .",
    "this is due to the verification - based nature of the decoding algorithm .",
    "the other advantage of lm1 , lm2-mb and lm2-nb is the computational complexity is linear for both the measuring process ( i.e. , encoding ) and the reconstruction process ( i.e. , decoding )",
    ".    we also find the maximum sparsity @xmath349 for perfect reconstruction when we use parity - check matrices from @xmath346 and @xmath347 ensembles ( with different @xmath41 ) as the measurement matrices when @xmath6 is large and try to see how @xmath349 scales with the code rate . in the simulation",
    ", we fix @xmath350 , try different @xmath41 s ( or @xmath351 s ) and use lm2-mb as the decoding algorithm . fig .",
    "[ fig : highratescaling ] shows the how @xmath349 scales with @xmath351 in high rate regime .",
    "we also show the theoretical scaling in fig .",
    "[ fig : highratescaling ] which is @xmath352 with @xmath353 and @xmath354 .",
    "notice that the simulation and the theoretical results match very well .",
    "the simulation results for @xmath15 , @xmath355 and @xmath356 ensembles are shown in fig .",
    "[ fig : sim_zero - one_345 ] and fig .",
    "[ fig : sim_gaussian_345 ] .",
    "the results show that for short block length and rate a half , using measurement matrix from ensemble with higher vn / cn degree leads to worse performance .",
    "this seems to conflict the results shown in fig .",
    "[ fig : highratescaling ] .",
    "actually , in the scaling law analysis , we consider rates close to 1 and large block - length which is not satisfied in the simulation of fig .",
    "[ fig : sim_zero - one_345 ] and fig .",
    "[ fig : sim_gaussian_345 ] .",
    "we analyze message - passing decoding algorithms for ldpc codes in the high - rate regime .",
    "the results can be applied to compressed sensing systems with strictly - sparse signals . a high - rate analysis based on de",
    "is used to derive the scaling law for randomized reconstruction cs systems and stopping set analysis is used to analyze uniform - in - probability / uniform reconstruction .",
    "the scaling law analysis gives the surprising result that ldpc codes , together with the lm2-mb algorithm , allow randomized reconstruction when the number of measurements scales linearly with the sparsity of the signal .",
    "simulation results and comparisons with a number of other cs reconstruction algorithms are also provided .",
    "simulation results for zero - one spikes of length 256 with 128 measurements by using @xmath15 , @xmath355 and @xmath356 ensembles . ]",
    "starting with the convergence condition @xmath357 for @xmath73 $ ] , we first solve for @xmath70 to get@xmath358 next , we substitute @xmath359 and simplify to get@xmath360 for @xmath238 , this function is unbounded as @xmath361 or @xmath362 , so the minimum must occur at an interior critical point @xmath363 . choosing @xmath364 and setting the derivative w.r.t .",
    "@xmath192 to zero gives @xmath365 canceling terms and simplifying the numerator gives @xmath366 , which can be rewritten as @xmath367 .",
    "ignoring @xmath368 , this implies that @xmath363 is given by the unique intersection of @xmath369 and @xmath370 for @xmath371 . that intersection point can be written in closed form using the non - principal real branch of the lambert w - function @xcite , @xmath372 , and is given by , for @xmath140 , @xmath373 using this , the @xmath71-threshold for @xmath5-regular ensembles is given by @xmath374 . for @xmath194 , the minimum occurs as @xmath375 and the limit gives @xmath376 .",
    "simulation results for gaussian spikes of length 256 with 128 measurements by using @xmath15 , @xmath355 and @xmath356 ensembles . ]",
    "before proving the theorem , we first prove the following lemma .    [",
    "lem : converge]let @xmath377 be a poisson random variable with mean @xmath330 . the tail probability can be bounded by@xmath378",
    "the moment generating function of poisson distribution is @xmath379 the chernoff bound shows@xmath380 for @xmath381 setting the derivative of the rhs over @xmath226 to zero to minimize the rhs ( since the second derivative is positive ) , we have the rhs of ( [ eq : chernoff_poisson ] ) .",
    "note that the rhs of ( [ eq : chernoff_poisson ] ) decays faster than @xmath382 for any constant @xmath383    [ proof of theorem 2]to show the concentration for our case , we modify the proof as following . in the first iteration , we reduce the graph to a smaller one by removing the edges which are not erased .",
    "let @xmath126 be the channel erasure probability and @xmath112 , the number of check nodes concentrates around @xmath384 and the number of variable nodes concentrates around @xmath385 .",
    "the probability that a check node has degree @xmath214 after removal is @xmath386 which converges to poisson distribution as @xmath217 so the edge degree distribution of check node is @xmath387 , the average check node degree is @xmath388 , and the edge degree distribution of the variable node is @xmath389    by the similar edge - revealing argument @xcite , we can model the graph - uncovering process as a martingale and apply the azuma s inequality , we can bound the probability that the performance of a particular code deviates the cycle - free case by more than @xmath390 , @xmath391 where @xmath392 is an upper - bound of the difference of the numbers of erasure messages between the graphs obtained by revealing the @xmath393-th and the @xmath4-th edge .",
    "@xmath394 is the number of edges in the _ residual graph _ which concentrates around @xmath395 @xmath396 is the fraction of erasures of the residual graph given by de .",
    "it is easy to see @xmath397 where @xmath204 is the de result of the original graph . for simplicity",
    ", we can pick @xmath392 to be @xmath398 where @xmath120 is the number of edges in the depth-@xmath121 directed neighborhood of the @xmath4-th edge .",
    "note that @xmath392 is identically distributed but not independent .",
    "so the law of large numbers does not apply in general .    since the check node degree is an i.i.d .",
    "random variable . by applying the union bound to lemma [ lem :",
    "converge ] , the probability that every check node in the residual graph has degree no more than @xmath399 can be bounded as follows @xmath400 for sufficiently large @xmath401 , where @xmath402    for a @xmath403 regular ensemble , @xmath404 and @xmath405 so we have @xmath406 given every check node has degree no more than @xmath407 where @xmath408 and @xmath409 are constants independent of @xmath401 . note that the event that @xmath410 can be caused by two reasons .",
    "the first one is the rare events that captured by azuma s inequality .",
    "the second reason is that some check nodes have very large degrees such that the rhs of ( [ eq : martingale_bnd ] ) has positive exponent .",
    "equation ( [ eq : azuma ] ) shows the probability that @xmath410 which is caused by the first reasons and ( [ eq : cn_large_deg ] ) shows the probability that @xmath410 which is caused by the second reason .",
    "note that the event @xmath410 can be caused by both of the first and the second reasons . by the union bound , @xmath411 now we set @xmath412 to balance the probability that azuma s inequality fails and the probability that any check node has too large degree .",
    "this gives @xmath413 so ( [ eq : union_bnd ] ) can be rewritten as @xmath414 where @xmath415 is a constant independent of @xmath232",
    "all statements are implied to hold for all @xmath141 , all @xmath416 $ ] , and all @xmath417 $ ] . since @xmath418 is concave for @xmath419",
    ", the tangent upper bound at @xmath88 shows that @xmath420 .",
    "this implies that@xmath421 since @xmath422 , we can use ( [ eq : lm1de_lemma_eqn1 ] ) to upper bound @xmath134 with    @xmath423^{k}\\biggr)^{j-1}\\\\ \\le\\frac{\\alpha}{\\overline{\\alpha}_{j}}\\biggl(1-\\biggl[1-\\frac{\\overline{\\alpha}_{j}^{j-1}x^{j-1}}{k}-\\frac{\\overline{\\alpha}_{j}x}{k^{j/(j-1)}}\\biggr]^{k}\\biggr)^{j-1}.\\\\ \\vspace{-10mm}\\end{gathered}\\ ] ]    this completes the proof of ( i ) .    the fact that @xmath135 is monotonically decreasing follows from lemma [ lem : monotonictoexp ] .",
    "this completes the proof of ( ii ) .",
    "lemma [ lem : monotonictoexp ] also shows that the limit of @xmath135 is @xmath424 this proves the first part of ( iii ) .",
    "next , we will show that @xmath425 first , we show that@xmath426 in light of the the upper bound ( [ eq : lm1de_lemma_eqn1 ] ) , the limit is clearly upper bounded by @xmath427 . using the lower bound in lemma [ lem : monotonictoexp ] , we see that@xmath428 this implies that @xmath429 together with@xmath430 we see that the limit ( [ eq : lm1de_limit1 ] ) holds .",
    "to calculate the limit of @xmath176 , we can use the fact that @xmath431 whenever @xmath432 exists . using this ,",
    "we see that @xmath433 can be rewritten aswhere the last step follows from ( [ eq : lm1de_limit1 ] ) .",
    "recall that @xmath46 is defined as the largest @xmath71 s.t .",
    "@xmath434 for @xmath73.$ ] so @xmath46 can be written as @xmath435}h_{j}(x)\\label{eq : alpha_bar_lm1}\\ ] ] where @xmath436 notice that @xmath437 is a monotonically increasing function of @xmath34 when @xmath438 so we have @xmath439 when @xmath440 @xmath437 goes to infinity when @xmath34 goes to either 0 or 1 , so the infimum is achieved at an interior point @xmath441 . by taking derivative of @xmath34 and setting it to zero , @xmath441 is the solution of @xmath442 so @xmath443 by solving this numerically , we find that @xmath444 @xmath445 and @xmath446 substituting @xmath441 into ( [ eq : alpha_bar_lm1 ] ) , we have @xmath167 @xmath168 and @xmath169 .",
    "let us define the function @xmath447 with    @xmath448    to prove ( i ) , we will show @xmath449 to see that @xmath450 we must simply observe that@xmath451 this can be seen by working from the inner expression outwards and using the facts that @xmath452 and @xmath453 .",
    "each step gives a result that is bounded between 0 and 1 .    to show @xmath454 we first change variables to @xmath455 where @xmath456",
    "this allows @xmath177 to be written as a function of @xmath457 with@xmath458 taking the derivative of @xmath459 with respect to @xmath457 gives@xmath460 which is negative for @xmath238 .",
    "so @xmath459 is a monotonically decreasing function of @xmath461 using the inequality @xmath462 we find that @xmath463 .",
    "next , we will prove ( ii ) by showing the limits of @xmath176 and @xmath177 are the same .",
    "first , we take the the term by term limit of @xmath177 to see that    @xmath464    next , we use the fact that @xmath465 to see that@xmath466 from this , we find that the term by term limit of @xmath176 is also equal to ( [ eq : lm2_gkbar_limit ] ) .    to prove ( iii ) , we recall that , using the change of variables @xmath455 , @xmath459 is a monotonically decreasing function of @xmath457 .",
    "moreover , @xmath459 does not depend on @xmath41 and @xmath455 is a monotonically increasing function of @xmath41 ( e.g. , see lemma [ lem : monotonictoexp ] ) .",
    "so @xmath177 is a monotonically decreasing function of @xmath322      consider whether the sequences @xmath292 and @xmath293 converge to zero or not .",
    "clearly , there are only 4 possible cases .",
    "if @xmath467 and @xmath468 , the limit@xmath469 contradicts @xmath470    if @xmath471 and @xmath472 , the limit @xmath473 contradicts @xmath291 .",
    "if @xmath471 and @xmath468 , the limit satisfies @xmath474 and this contradicts @xmath291 .",
    "since all stopping sets with size sublinear in @xmath6 shrink to the zero point on the scaled curve , we must treat sublinear stopping sets separately .",
    "the proof proceeds by considering separately stopping sets of size @xmath475 and size @xmath476 for very small @xmath68 .",
    "the number of correct and incorrect variable nodes in a stopping set is denoted , respectively , @xmath123 and @xmath258 ( i.e. , @xmath477 and @xmath478 ) .    using ( [ eq : average_number_ss ] ) and lemma [ lem : bounds_on_multinomial ] ,",
    "we can bound @xmath254 with @xmath479 the coefficient @xmath257 can be bounded using a chernoff - type bound and this gives@xmath480 for arbitrary @xmath81 and @xmath481 .",
    "choosing @xmath482 and @xmath483 gives the bound@xmath484 where @xmath220 is a constant independent of @xmath6 .",
    "applying ( [ eq : bnd_a ] ) to the @xmath254 bound shows that @xmath485@xmath486 where @xmath487 and @xmath51 .",
    "now , we can use this to show that@xmath488 since a stopping set can not have a check node that attaches to only verified and correct edges , a simple counting argument shows that @xmath489 if @xmath490 .",
    "therefore , the above condition can be simplified to@xmath491 starting from ( [ eq : enjk_simple_chernoff ] ) , we note that @xmath492 and @xmath493 implies that @xmath494 for large enough @xmath6 .",
    "therefore , we find that the double sum in ( [ eq : double_sum ] ) is upper bounded by@xmath495 for large enough @xmath6 .",
    "since the exponent @xmath496 of @xmath6 is negative as long as @xmath497 and @xmath238 , we also find that the limit of the double sum in ( [ eq : double_sum ] ) goes to zero as @xmath6 goes to infinity for any @xmath498 .",
    "now , we consider stopping sets of size greater than @xmath499 but less than @xmath500 . combining ( [ eq : gamma ] ) and lemma [ lem : bounds_on_multinomial ] shows that @xmath501 notice that ( [ eq : v(c , d)_small_beta ] ) is an accurate upper bound on @xmath264 for small enough @xmath119 and its maximum over @xmath71 is given parametrically by ( [ eq : v(d ) ] ) .",
    "moreover , @xmath298 is strictly decreasing at @xmath502 , and this implies that @xmath264 is strictly decreasing in @xmath119 at @xmath503 for all valid @xmath71 .",
    "therefore , there is a @xmath296 and @xmath504 such that@xmath505 for all @xmath506 . from this , we conclude that @xmath507 which implies that @xmath508 where @xmath509 can be made arbitrarily large by increasing @xmath510 .",
    "choosing @xmath511 so that @xmath512 shows that@xmath513 this completes the proof .",
    "[ lem : bounds_on_multinomial ] the ratio @xmath514 can be bounded with@xmath515",
    "e.  j. cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inform . theory _",
    "52 , no .  2 ,",
    "489509 , 2006 .",
    "a.  c. gilbert , m.  j. strauss , j.  a. tropp , and r.  vershynin , `` one sketch for all : fast algorithms for compressed sensing , '' in _ in proceedings of the acm symposium on the theory of computing ( stoc 2007 ) _ , 2007 .",
    "w.  dai and o.  milenkovic , `` weighted superimposed codes and constrained integer compressed sensing , '' 2008 , submitted to _",
    "ieee trans . on inform .",
    "theory _ also available in arxiv preprint cs.it/0806.2682v1 .            c.  di , d.  proietti , e.  telatar , t.  j. richardson , and r.  urbanke , `` finite - length analysis of low - density parity - check codes on the binary erasure channel , '' _ ieee trans .",
    "inform . theory _",
    "48 , no .  6 , pp . 15701579 , june 2002",
    ".        f.  zhang and h.  d. pfister , `` list - message passing achieves capacity on the @xmath1-ary symmetric channel for large @xmath1 , '' 2008 , submitted to",
    "_ ieee trans . on inform .",
    "theory _ also available in arxiv preprint cs.it/0806.3243 .",
    "n.  deanna and v.  roman , `` uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit , '' _ foundations of computational mathematics _ , vol .  9 , no .  3 , pp .",
    "317334 , june 2009 .",
    "r.  chartrand and w.  yin , `` iteratively reweighted algorithms for compressive sensing , '' in _ acoustics , speech and signal processing , 2008 .",
    "icassp 2008 .",
    "ieee international conference on _ , 2008 , pp ."
  ],
  "abstract_text": [
    "<S> this paper considers the performance of @xmath0-regular low - density parity - check ( ldpc ) codes with message - passing ( mp ) decoding algorithms in the high - rate regime . </S>",
    "<S> in particular , we derive the high - rate scaling law for mp decoding of ldpc codes on the binary erasure channel ( bec ) and the @xmath1-ary symmetric channel ( @xmath1-sc ) . for the bec , the density evolution ( de ) threshold of iterative decoding scales like @xmath2 and the critical stopping ratio scales like @xmath3 . for the @xmath1-sc , the de threshold of verification decoding depends on the details of the decoder and scales like @xmath2 for one decoder .    </S>",
    "<S> using the fact that coding over large finite alphabets is very similar to coding over the real numbers , the analysis of verification decoding is also extended to the the compressed sensing ( cs ) of strictly - sparse signals . </S>",
    "<S> a de based approach is used to analyze the cs systems with randomized - reconstruction guarantees . </S>",
    "<S> this leads to the result that strictly - sparse signals can be reconstructed efficiently with high - probability using a constant oversampling ratio ( i.e. , when the number of measurements scales linearly with the sparsity of the signal ) . </S>",
    "<S> a stopping - set based approach is also used to get stronger ( e.g. , uniform - in - probability ) reconstruction guarantees .    </S>",
    "<S> ldpc codes , verification decoding , compressed sensing , stopping sets , q - ary symmetric channel </S>"
  ]
}