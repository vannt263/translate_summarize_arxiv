{
  "article_text": [
    "there is significant value in the ability to associate natural language descriptions with images .",
    "describing the contents of images is useful for automated image captioning and conversely , the ability to retrieve images based on natural language queries has immediate image search applications .",
    "in particular , in this work we are interested in training a model on a set of images and their associated natural language descriptions such that we can later rank a fixed set of withheld sentences given an image query , and vice versa .",
    "this task is challenging because it requires detailed understanding of the content of images , sentences and their inter - modal correspondence .",
    "consider an example sentence query , such as _  a dog with a tennis ball is swimming in murky water \" _",
    "( figure [ fig : pull ] ) . in order to successfully retrieve a corresponding image",
    ", we must accurately identify all entities , attributes and relationships present in the sentence and ground them appropriately to a complex visual scene .",
    "our primary contribution is in formulating a structured , max - margin objective for a deep neural network that learns to embed both visual and language data into a common , multimodal space . unlike previous work that embeds images and sentences , our model breaks down and embeds fragments of images ( objects ) and fragments of sentences ( dependency tree relations @xcite ) in a common embedding space and explicitly reasons about their latent , inter - modal correspondences .",
    "reasoning on the level of fragments allows us to impose a new fragment - level loss function that complements a traditional sentence - image ranking loss .",
    "extensive empirical evaluation validates our approach .",
    "in particular , we report dramatic improvements over state of the art methods on image - sentence retrieval tasks on pascal1k @xcite , flickr8k",
    "@xcite and flickr30k @xcite datasets . we plan to make our code publicly available .",
    "* image annotation and image search .",
    "* there is a growing body of work that associates images and sentences .",
    "some approaches focus on describing the contents of images , formulated either as a task of mapping images to a fixed set of sentences written by people @xcite , or as a task of automatically generating novel captions @xcite .",
    "more closely related to our approach are methods that naturally allow bi - drectional mapping between the two modalities .",
    "socher and fei - fei @xcite and hodosh et al . @xcite",
    "use kernel canonical correlation analysis to align images and sentences , but their method is not easily scalable since it relies on computing kernels quadratic in number of images and sentences .",
    "farhadi et al .",
    "@xcite learn a common meaning space , but their method is limited to representing both images and sentences with a single triplet of ( object , action , scene ) .",
    "zitnick et al .",
    "@xcite use a conditional random field to reason about complex relationships of cartoon scenes and their natural language descriptions .",
    "* multimodal representation learning . *",
    "our approach falls into a general category of learning from multi - modal data .",
    "several probabilistic models for representing joint multimodal probability distributions over images and sentences have been developed , using deep boltzmann machines @xcite , log - bilinear models @xcite , and topic models @xcite .",
    "ngiam et al .",
    "@xcite described an autoencoder that learns audio - video representations through a shared bottleneck layer .",
    "more closely related to our task and approach is work of frome et al .",
    "@xcite , who introduced a visual semantic embedding model that learns to map images and words to a common semantic embedding with a ranking cost . adopting a similar approach , socher et al .",
    "@xcite described a dependency tree recursive neural network that puts entire sentences into correspondence with visual data . however , these methods reason about the image only on global level , using a single , fixed - sized representation from a top layer of a convolutional neural network as a description for the entire image , whereas our model reasons explicitly about objects that make up a complex scene .    * neural representations for images and natural language . *",
    "our model is a neural network that is connected to image pixels on one side and raw 1-of - k word representations on the other .",
    "there have been multiple approaches for learning neural representations in these data domains . in computer vision ,",
    "convolutional neural networks ( cnns ) @xcite have recently been shown to learn powerful image representations that support state of the art image classification @xcite and object detection @xcite . in language domain ,",
    "several neural network models have been proposed to learn word / n - gram representations @xcite , sentence representations @xcite and paragraph / document representations @xcite .",
    "* overview of learning and inference . *",
    "our task is to retrieve relevant images given a sentence query , and conversely , relevant sentences given an image query .",
    "we will train our model on a training set of @xmath0 images and @xmath0 corresponding sentences that describe their content ( figure [ fig : teaser ] ) .",
    "given this set of correspondences , we train the weights of a neural network to output a high score when a compatible image - sentence pair is fed through the network , and low score otherwise . once the training is complete , all training data is discarded and the network is evaluated on a withheld set of testing images and sentences .",
    "the evaluation will score all image - sentence pairs , sort images / sentences in order of decreasing score and record the location of a ground truth result in the list .",
    "* fragment embeddings . *",
    "our core insight is that images are complex structures that are made up of multiple interacting entities that the sentences make explicit references to .",
    "we capture this intuition directly in our model by breaking down both images and sentences into fragments and reason about their ( latent ) alignment .",
    "in particular , we propose to detect objects as image fragments and use sentence dependency tree relations @xcite as sentence fragments ( figure [ fig : teaser ] )",
    ".    * fragment - level objective . * in previous related work @xcite ,",
    "neural networks embed images and sentences into a common space and the parameters are trained such that true image - sentence pairs have an inner product ( interpreted as a score ) higher than false image - sentence pairs by a margin . in our approach , we instead embed the image and sentence fragments , and compute the image - sentence score as a fixed function of the scores of their fragments .",
    "thus , in addition to the ranking loss seen in previous work ( we refer to this as the global ranking objective ) , we will add a second , stronger fragment alignment objective .",
    "we will show that these objectives provide complementary information to the network .",
    "we first describe the neural networks that compute the image and sentence fragment embeddings .",
    "then we discuss the objective function , which is composed of the two aforementioned objectives .    ) . *",
    "right : * dependency tree relations in the sentence are embedded ( section [ sec : sent ] ) . our model interprets inner products ( shown as boxes ) between fragments as a similarity score . the alignment ( shaded boxes ) is latent and inferred by our model ( section [ sec : fao ] ) .",
    "the image - sentence similarity is computed as a fixed function of the pairwise fragment scores . ]",
    "[ sec : sent ]    we would like to extract and represent the set of visually identifiable entities described in a sentence .",
    "for instance , using the example in figure [ fig : teaser ] , we would like to identify the entities ( dog , child ) and characterise their attributes ( black , young ) and their pairwise interactions ( chasing ) .",
    "inspired by previous work @xcite we observe that a dependency tree of a sentence provides a rich set of typed relationships that can serve this purpose more effectively than individual words or bigrams .",
    "we discard the tree structure in favor of a simpler model and interpret each relation ( edge ) as an individual sentence fragment ( figure [ fig : teaser ] , right shows 5 example dependency relations ) .",
    "thus , we represent every word using 1-of - k encoding vector @xmath1 using a dictionary of 400,000 words and map every dependency triplet @xmath2 into the embedding space as follows :    @xmath3 + b_{r } \\right).\\ ] ]    here , @xmath4 is a @xmath5 matrix that encodes a 1-of - k vector into a @xmath6-dimensional word vector representation ( we use @xmath6 = 200 ) .",
    "we fix @xmath4 to weights obtained through an unsupervised objective described in huang et al .",
    "@xcite . note that every relation @xmath7 has its own set of weights @xmath8 and biases @xmath9 .",
    "we fix the element - wise nonlinearity @xmath10 to be the rectified linear unit which computes @xmath11 .",
    "the dimensionality of @xmath12 is cross - validated .",
    "[ sec : img ]    similar to sentences , we wish to extract and describe the set of entities that images are composed of . inspired by prior work @xcite , as a modeling assumption we observe that the subject of most sentence descriptions are attributes of objects and their context in a scene .",
    "this naturally motivates the use of objects and the global context as the fragments of an image .",
    "in particular , we follow girshick et al . @xcite and detect objects in every image with a region convolutional neural network ( rcnn ) .",
    "the cnn is pre - trained on imagenet @xcite and finetuned on the 200 classes of the imagenet detection challenge @xcite .",
    "we use the top 19 detected locations and the entire image as the image fragments and compute the embedding vectors based on the pixels @xmath13 inside each bounding box as follows :    @xmath14,\\ ] ]    where @xmath15 takes the image inside a given bounding box and returns the 4096-dimensional activations of the fully connected layer immediately before the classifier .",
    "it might be possible to initialize some of the weights in @xmath16 with the parameters in the cnn classifier layer , but we choose to discard these weights after the initial object detection step for simplicity .",
    "the cnn architecture is identical to the one described in girhsick et al .",
    "it contains approximately 60 million parameters @xmath17 and closely resembles the architecture of krizhevsky et al @xcite .",
    "[ sec : embed ]    we are now ready to formulate the objective function . recall that we are given a training set of @xmath0 images and corresponding sentences . in the previous sections we described parameterized functions that map every sentence and image to a set of fragment vectors @xmath18 and @xmath19 , respectively . as shown in figure [ fig : teaser ]",
    ", our model interprets the inner product @xmath20 between an image fragment @xmath21 and a sentence fragment @xmath22 as a similarity score .",
    "the similarity score for any image - sentence pair will in turn be computed as a fixed function of their pairwise fragment scores .",
    "intuitively , multiple matching fragments will give rise to a high image - sentence score .",
    "we are motivated by two criteria in designing the objective function .",
    "first , the image - sentence similarities should be consistent with the ground truth correspondences .",
    "that is , corresponding image - sentence pairs should have a higher score than all other image - sentence pairs .",
    "this will be enforced by the * global ranking objective*. second , we introduce a * fragment alignment objective * that learns the appearance of all sentence fragments ( such as  black dog \" ) in the visual domain . our full objective is the weighted sum of these two objectives and a regularization term :    @xmath23    where @xmath24 is a shorthand for parameters of our neural network ( @xmath25 ) and @xmath26 and @xmath27 are hyperparameters that we cross - validate .",
    "we now describe both objectives in more detail .",
    "[ sec : fao ] the fragment alignment objective encodes the intuition that if a sentence contains a fragment ( e.g.blue ball \" , figure [ fig : img ] ) , at least one of the boxes in the corresponding image should have a high score with this fragment , while all the other boxes in all the other images that have no mention of  blue ball \" should have a low score .",
    "this assumption can be violated in multiple ways : a triplet may not refer to anything visually identifiable in the image .",
    "the box that the triplet refers to may not be detected by the rcnn .",
    "lastly , other images may contain the described visual concept but its mention may omitted in the associated sentence descriptions .",
    "nonetheless , the assumption is still satisfied in many cases and can be used to formulate a cost function .",
    "consider an ( incomplete ) fragment alignment objective that assumes a dense alignment between every corresponding image and sentence fragments :    @xmath28    here , the sum is over all pairs of image and sentence fragments in the training set .",
    "the quantity @xmath20 can be interpreted as the alignment score of visual fragment @xmath21 and sentence fragment @xmath22 . in this incomplete objective , we define @xmath29 as @xmath30 if fragments @xmath21 and @xmath22 occur together in a corresponding image - sentence pair , and @xmath31 otherwise .",
    "the constants @xmath32 normalize the objective with respect to the number of positive and negative @xmath29 .",
    "intuitively , @xmath33 encourages scores in red regions of figure [ fig : img ] to be less than -1 and scores along the block diagonal ( green and yellow ) to be more than + 1 .",
    "* multiple instance learning extension .",
    "* the problem with the objective @xmath33 is that it assumes dense alignment between all pairs of fragments in every corresponding image - sentence pair .",
    "however , this is hardly ever the case .",
    "for example , in figure [ fig : img ] , the  boy playing \" triplet refers to only one of the three detections .",
    "we now describe a multiple instance learning ( mil ) @xcite extension of the objective @xmath34 that attempts to infer the latent alignment between fragments in corresponding image - sentence pairs . concretely , for every triplet we put image fragments in the associated image into a positive bag , while image fragments in every other image become negative examples .",
    "our precise formulation is inspired by the _",
    "@xcite , which is a simple and natural extension of a support vector machine to a multiple instance learning setting . instead of treating the @xmath29 as constants",
    ", we minimize over them by wrapping equation [ eq : fobj ] as follows :    @xmath35    here , we define @xmath36 to be the set of image fragments in the positive bag for sentence fragment @xmath37 . @xmath38 and @xmath39 return the index of the image and sentence ( an element of @xmath40 ) that the fragments @xmath21 and @xmath22 belong to . note that the inequality simply states that at least one of the @xmath29 should be positive for every sentence fragment @xmath37 ( i.e. at least one green box in every column in figure [ fig : img ] ) .",
    "this objective can not be solved efficiently @xcite but a commonly used heuristic is to set @xmath41 . if the constraint is not satisfied for any positive bag ( i.e. all scores were below zero ) , the highest - scoring item in the positive bag is set to have a positive label .",
    "recall that the global ranking objective ensures that the computed image - sentence similarities are consistent with the ground truth annotation .",
    "first , we define the image - sentence alignmnet score to be the average thresholded score of their pairwise fragment scores :    @xmath42    here , @xmath43 is the set of image fragments in image @xmath44 and @xmath45 is the set of sentence fragments in sentence @xmath46 . both @xmath47 range from @xmath48 .",
    "we truncate scores at zero because in the _ mi - svm _ objective , scores greater than 0 are considered correct alignments and scores less than 0 are considered to be incorrect alignments ( i.e. false members of a positive bag ) . in practice",
    ", we found that it was helpful to add a smoothing term @xmath49 , since short sentences can otherwise have an advantage ( we found that @xmath50 works well on validation experiments ) .",
    "the global ranking objective then becomes :    @xmath51.\\ ] ]    here , @xmath52 is a hyperparameter that we cross - validate .",
    "the objective stipulates that the score for true image - sentence pairs @xmath53 should be higher than @xmath54 or @xmath55 for any @xmath56 by at least a margin of @xmath52 .",
    "this concludes our discussion of the objective function .",
    "we note that our entire model ( including the cnn ) and objective functions are made up exclusively of dot products and thresholding at zero .",
    "we use stochastic gradient descent ( sgd ) with mini - batches of 100 , momentum of 0.9 and make 15 epochs through the training data .",
    "the learning rate is cross - validated and annealed by a fraction of @xmath57 for the last two epochs . since both multiple instance learning and cnn finetuning benefit from a good initialization ,",
    "we run the first 10 epochs with the fragment alignment objective @xmath34 and cnn weights @xmath17 fixed .",
    "after 10 epochs , we switch to the full mil objective @xmath58 and begin finetuning the cnn .",
    "the word embedding matrix @xmath4 is kept fixed due to overfitting concerns .",
    "our implementation runs at approximately 1 second per batch on a standard cpu workstation .",
    "* datasets . *",
    "we evaluate our image - sentence retrieval performance on pascal1k @xcite , flickr8k",
    "@xcite and flickr30k @xcite datasets . the datasets contain 1,000 , 8,000 and 30,000 images respectively and each image is annotated using amazon mechanical turk with 5 independent sentences . + * sentence data preprocessing .",
    "* we did not explicitly filter , spellcheck or normalize any of the sentences for simplicity .",
    "we use the stanford corenlp parser to compute the dependency trees for every sentence .",
    "since there are many possible relations ( as many as hundreds ) , due to overfitting concerns and practical considerations we remove all relation types that occur less than 1% of the time in each dataset . in practice , this reduces the number of relations from 136 to 16 in pascal1k , 170 to 17 in flickr8k , and from 212 to 21 in flickr30k .",
    "additionally , all words that are not found in our dictionary of 400,000 words @xcite are discarded . + * image data preprocessing .",
    "* we use the caffe @xcite implementation of the imagenet detection rcnn model @xcite to detect objects in all images . on our machine with a tesla k40 gpu , the rcnn processes one image in approximately 25 seconds .",
    "we discard the predictions for 200 imagenet detection classes and only keep the 4096-d activations of the fully connect layer immediately before the classifier at all of the top 19 detected locations and from the entire image . + * evaluation protocol for bidirectional retrieval . * for pascal1k",
    "we follow socher et al .",
    "@xcite and use 800 images for training , 100 for validation and 100 for testing . for flickr datasets we use 1,000 images for validation , 1,000 for testing and the rest for training ( consistent with @xcite ) .",
    "we compute the dense image - sentence similarity @xmath54 between every image - sentence pair in the test set and rank images and sentences in order of decreasing score . for both image annotation and image search ,",
    "we report the median rank of the closest ground truth result in the list , as well as recall@k which computes the fraction of times the correct result was found among the top k items . when comparing to hodosh et al .",
    "@xcite we closely follow their evaluation protocol and only keep a subset of @xmath0 sentences out of total @xmath59 ( we use the first sentence out of every group of 5 ) .",
    "* sdt - rnn . * socher et al .",
    "@xcite embed a fullframe cnn representation with the sentence representation from a semantic dependency tree recursive neural network ( sdt - rnn ) .",
    "their loss matches our global ranking objective .",
    "we requested the source code of socher et al .",
    "@xcite and substituted the flickr8k and flick30k datasets .",
    "to better understand the benefits of using our detection cnn and for a more fair comparison we also train their method with our cnn features .",
    "since we have multiple objects per image , we average representations from all objects with detection confidence above a ( cross - validated ) threshold .",
    "we refer to the exact method of socher et al .",
    "@xcite with a single fullframe cnn as  socher et al \" , and to their method with our combined cnn features as  sdt - rnn \" .",
    "we were not able to evaluate sdt - rnn on flickr30k because training times on order of multiple days prevent us from satisfyingly cross - validating their hyperparameters .    *",
    "the devise @xcite source code is not publicly available but their approach is a special case of our method with the following modifications : we use the average ( l2-normalized ) word vectors as a sentence fragment , the average cnn activation of all objects above a detection threshold ( as discussed in case of sdt - rnn ) as an image fragment and only use the global ranking objective .",
    "l|cccc|cccc + & & + * model * & * r@1 * & * r@5 * & * r@10 * & * mean * _ r & * r@1 * & * r@5 * & * r@10 * & * mean * _ r + random ranking & 4.0 & 9.0 & 12.0 & 71.0 & 1.6 & 5.2 & 10.6 & 50.0 + socher et al .",
    "@xcite & 23.0 & 45.0 & 63.0 & 16.9 & 16.4 & 46.6 & 65.6 & 12.5 + kcca .",
    "@xcite & 21.0 & 47.0 & 61.0 & 18.0 & 16.4 & 41.4 & 58.0 & 15.9 + devise @xcite & 17.0 & 57.0 & 68.0 & 11.9 & 21.6 & 54.6 & 72.4 & 9.5 + sdt - rnn @xcite & 25.0 & 56.0 & 70.0 & 13.4 & * 25.4 * & * 65.2 * & * 84.4 * & * 7.0 * + our model & * 39.0 * & * 68.0 * & * 79.0 * & * 10.5 * & 23.6 & * 65.2 * & 79.8 & 7.6 + _",
    "_    l|cccc|cccc + & & + * model * & * r@1 * & * r@5 * & * r@10 * & * med * _ r & * r@1 * & * r@5 * & * r@10 * & * med * _ r + random ranking & 0.1 & 0.6 & 1.1 & 631 & 0.1 & 0.5 & 1.0 & 500 + socher et al .",
    "@xcite & 4.5 & 18.0 & 28.6 & 32 & 6.1 & 18.5 & 29.0 & 29 + devise @xcite & 4.8 & 16.5 & 27.3 & 28 & 5.9 & 20.1 & 29.6 & 29 + sdt - rnn @xcite & 6.0 & 22.7 & 34.0 & 23 & 6.6 & 21.6 & 31.7 & 25 + fragment alignment objective & 7.2 & 21.9 & 31.8 & 25 & 5.9 & 20.0 & 30.3 & 26 + global ranking objective & 5.8 & 21.8 & 34.8 & 20 & 7.5 & 23.4 & 35.0 & 21 + ( @xmath60 ) fragment + global & 12.5 & 29.4 & 43.8 & 14 & 8.6 & 26.7 & 38.7 & 17 + @xmath60 @xmath61 images : fullframe only & 5.9 & 19.2 & 27.3 & 34 & 5.2 & 17.6 & 26.5 & 32 + @xmath60 @xmath61 sentences : bow & 9.1 & 25.9 & 40.7 & 17 & 6.9 & 22.4 & 34.0 & 23 + @xmath60 @xmath61 sentences : bigrams & 8.7 & 28.5 & 41.0 & 16 & 8.5 & 25.2 & 37.0 & 20 + our model ( @xmath60 + mil ) & * 12.6 * & * 32.9 * & * 44.0 * & * 14 * & * 9.7 * & * 29.6 * & * 42.5 * & * 15 * + hodosh et al .",
    "@xcite & 8.3 & 21.6 & 30.3 & 34 & 7.6 & 20.7 & 30.1 & 38 + * our model ( @xmath60 + mil ) & * 9.3 * & * 24.9 * & * 37.4 * & * 21 * & * 8.8 * & * 27.9 * & * 41.3 * & * 17 * + _ _      the quantitative results for pascal1k , flickr8k , and flickr30k are in tables [ fig : p1 ] , [ fig : f8 ] , and [ fig : f30 ] respectively .    *",
    "our model outperforms previous methods . *",
    "our full method consistently and significantly outperforms previous methods on flickr8k ( table [ fig : f8 ] ) and flickr30k ( table [ fig : f30 ] ) datasets .",
    "on pascal1k ( table [ fig : p1 ] ) the sdt - rnn appears to be competitive on image search . + * fragment and global objectives are complementary . *",
    "as seen in tables [ fig : f8 ] and [ fig : f30 ] , both objectives perform well but there is a noticeable improvement when the two are combined , suggesting that the objectives bring complementary information to the cost function .",
    "note that the global objective consistently performs slightly better , possibly because it directly minimizes the evaluation criterion ( ranking cost ) , while the fragment alignment objective only does so indirectly . + * extracting object representations is important . * using only the global scene - level cnn representation as a single fragment for every image leads to a consistent drop in performance , suggesting that a single fullframe cnn alone is inadequate in effectively representing the images .",
    "( table [ fig : f8 ] ) + * dependency tree relations outperform bow / bigram representations .",
    "* we compare to a simpler bag of words ( bow ) baseline to understand the contribution of dependency relations . in bow baseline",
    "we iterate over words instead of dependency triplets when creating bags of sentence fragments ( set @xmath62 in equation[eq : s ] ) . as can be seen in the table [ fig : f8 ] , this leads to a consistent drop in performance .",
    "this drop could be attributed to a difference between using one word or two words at a time , so we also compare to a bigram baseline where the words @xmath63 in equation [ eq : s ] refer to consecutive words in a sentence , not nodes that share an edge in the dependency tree .",
    "again , we observe a consistent performance drop , which suggests that the dependency relations provide useful structure that the neural network takes advantage of . + * finetuning the cnn helps on flickr30k . *",
    "our end - to - end neural network approach allows us to backpropagate gradients all the way down to raw data ( pixels or 1-of - k word encodings ) .",
    "in particular , we observed additional improvements on the flickr30k dataset ( table [ fig : f30 ] ) when we finetune the cnn .",
    "we were not able to improve the validation performance on pascal1k and flickr8k datasets and suspect that there is an insufficient amount of training data .",
    "l|cccc|cccc + & & + * model * & * r@1 * & * r@5 * & * r@10 * & * med * _ r & * r@1 * & * r@5 * & * r@10 * & * med * _ r + random ranking & 0.1 & 0.6 & 1.1 & 631 & 0.1 & 0.5 & 1.0 & 500 + devise @xcite & 4.5 & 18.1 & 29.2 & 26 & 6.7 & 21.9 & 32.7 & 25 + fragment alignment objective & 11 & 28.7 & 39.3 & 18 & 7.6 & 23.8 & 34.5 & 22 + global ranking objective & 11.5 & 33.2 & 44.9 & 14 & 8.8 & 27.6 & 38.4 & 17 + ( @xmath60 ) fragment + global & 12.0 & 37.1 & 50.0 & 10 & 9.9 & 30.5 & 43.2 & 14 + our model ( @xmath60 + mil ) & 14.2 & 37.7 & 51.3 & 10 & 10.2 & 30.8 & 44.2 & 14 + our model + finetune cnn & * 16.4 * & * 40.2 * & * 54.7 * & * 8 * & * 10.3 * & * 31.4 * & * 44.5 * & * 13 * + _ _          * interpretable predictions . *",
    "we show some example sentence retrieval results in figure [ fig : f8r ] . the alignment in our model is explicitly inferred on the fragment level , which allows us to interpret the scores between images and sentences . for instance , in the last image it is apparent that the model retrieved the top sentence because it erroneously associated a mention of a blue person to the blue flag on the bottom right of the image .",
    "+ * fragment alignment objective trains attribute detectors . *",
    "the detection cnn is trained to predict one of 200 imagenet detection classes , so it is not clear if the representation is powerful enough to support learning of more complex attributes of the objects or generalize to novel classes . to see whether our model successfully learns to predict sentence triplets",
    ", we fix a triplet vector and search for the highest scoring boxes in the test set .",
    "qualitative results shown in figure [ fig : tsearch ] suggest that the model is indeed capable of generalizing to more fine - grained subcategories ( such as  black dog \" ,  soccer ball \" ) and to out of sample classes such as  rocky terrain \" and  jacket \"",
    ". + * limitations . *",
    "our method has multiple limitations and failure cases .",
    "first , from a modeling perspective , sentences are only modeled as bags of relations .",
    "therefore , relations that belong to the same noun phrase can sometimes align to different objects .",
    "additionally , people frequently use phrases such as  three children playing \" but the model is incapable of counting .",
    "moreover , the non - maximum suppression in the rcnn can sometimes detect , for example , multiple people inside one person .",
    "since the model does not take into account any spatial information associated with the detections , it is hard for it to tell the difference between two distinct people or spurious detections of one person . on the language side ,",
    "there are many dependency relations that do nt have a natural grounding in an image ( for example ,  each other \" ,  four people \" , etc . ) .",
    "compound relations and attributes can also become separated .",
    "for instance  black and white dog \" is parsed as two relations ( conj , black , white ) and ( amod , white , dog ) . while we have shown that the relations give rise to more powerful representations than words or bigrams , a more careful treatment of sentence fragments might be necessary for further progress .",
    "we addressed the problem of bidirectional retrieval of images and sentences .",
    "our neural network model learns a multi - modal embedding space for fragments of images and sentences and reasons about their latent , inter - modal alignment .",
    "reasoning on a finer level of image and sentence fragments allowed us to formulate a new fragment alignment objective that complements a more traditional global ranking objective term .",
    "we have shown that our model significantly improves the retrieval performance on image sentence retrieval tasks compared to previous work .",
    "our model also produces interpretable predictions . in future work",
    "we hope to extend the model to support counting , reasoning about spatial positions of objects , and move beyond bags of fragments ."
  ],
  "abstract_text": [
    "<S> we introduce a model for bidirectional retrieval of images and sentences through a multi - modal embedding of visual and natural language data . </S>",
    "<S> unlike previous models that directly map images or sentences into a common embedding space , our model works on a finer level and embeds fragments of images ( objects ) and fragments of sentences ( typed dependency tree relations ) into a common space . </S>",
    "<S> in addition to a ranking objective seen in previous work , this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities . </S>",
    "<S> extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image - sentence retrieval tasks . </S>",
    "<S> additionally , our model provides interpretable predictions since the inferred inter - modal fragment alignment is explicit . </S>"
  ]
}