{
  "article_text": [
    "large - scale blind surveys in astronomy provide a wealth of information about the universe .",
    "they are the best means to gain knowledge in an unbiased way about populations of sources , particularly if the only limitation is the flux limit imposed by observational constraints . for determining the nature of the local universe ,",
    "neutral hydrogen ( hi ) surveys provide a view of the bulk of the baryonic mass , whilst also giving insights into the dark matter content .",
    "the hiparkes all - sky survey ( hipass , * ? ? ?",
    "* ) is the best example of a truly large - scale blind hisurvey , yielding catalogues of 4315 sources over all the sky south of @xmath0 @xcite and a further 1002 sources between @xmath1 @xcite , plus catalogues of the 1000 brightest galaxies @xcite and high - velocity clouds @xcite .",
    "these results provide a great deal of information on the hiproperties of galaxies , the himass function , the local velocity fields , and the cosmic himass density .",
    "blind surveys require a good method of detecting objects , to build up a reliable catalogue that is as complete as possible .",
    "such detection techniques must cope with the presence of noise that provides spurious sources and hides real but faint ones .",
    "the hipasssurvey featured a source detection procedure that required a large amount of human input ",
    "each source was verified several times by eye .",
    "as one increases the size of the spectral cubes , in terms of spatial size and the number of channels , this approach quickly becomes unfeasible , and a reliable automated source finding algorithm becomes essential to extract the required science catalogues from surveys .",
    "an example of an instrument driving such development is the australian square kilometre array pathfinder ( askap , * ? ? ?",
    "this will be a survey telescope , built in the radio - quiet ska candidate site of western australia , that will be able to do , amongst other things , hisurveys over large areas of sky and a wide redshift range ( due to its 30 square degree field of view and 300mhz bandwidth ) .",
    "ten `` survey science projects '' ( ssps ) have been identified as the major projects to consume at least 75% of askap s initial five years , and two of these are extragalactic hisurveys : the `` widefield askap l - band legacy all - sky blind survey '' ( wallaby , * ? ? ?",
    "* ) , an all - sky survey to @xmath2 that is one of two top - ranked projects ( along with the `` evolutionary map of the universe '' ( emu , * ? ? ?",
    "* ) all - sky continuum survey ) , and `` deep investigations into neutral gas origins '' ( dingo , * ? ? ?",
    "* ) , a deeper survey over a smaller area to @xmath3 .",
    "( note that these are not the only ssps with source finding requirements , they are simply the two surveys for extragalactic hiemission . ) the large data rates that will be produced by askap will necessitate largely automated processing , and so a robust , reliable source finder will be essential if the surveys are to meet their science goals .",
    "the anticipated demands of askap , combined with the realisation that there was no publicly - available generic 3d source - finder suitable for such work , prompted the development of the duchampsource - finding software package .",
    "this is a stand - alone piece of software , not closely associated with any particular instrument or survey , but aims to be a general - purpose 3d source finder .",
    "the algorithms , however , are being evaluated by the askap development team and the survey science teams for inclusion in the processing pipelines for askap .",
    "duchamphas been available for some years now , and has been used with a variety of data from single - dish @xcite and interferometer @xcite telescopes , as well as an aid to visualisation techniques @xcite .",
    "this paper aims to demonstrate the utility of duchampby presenting some of its key development aspects : the source detection techniques , statistical calculations and noise - suppression techniques , as well as memory management and software design .",
    "it also describes some of the testing techniques that have been used in its evaluation .",
    "all examples of duchampprocessing use version 1.1.13 of the software , the most recent version at time of writing .",
    "the philosophy i have used in developing duchamphas been to make things as friendly as possible for the user .",
    "this has resulted in straightforward user input and a range of graphical and text - based outputs to enable the user to accurately judge the quality of their detections .    the aim of duchampis to provide the user with information about the location of the sources of interest in the data .",
    "it does not make any prior assumptions about the nature of those sources ( e.g.  assuming they are gaussian - shaped ) , nor does it perform any profile fitting or analysis beyond direct measurements of the provided data .",
    "the aim is to provide the user with the _ location _ of the interesting sources . to that end",
    ", mask cubes can be written to highlight the detected pixels , allowing the user to do their own post - processing .",
    "duchamphas been designed for use primarily with spectral - line cubes , and this colours the thinking in much of the design .",
    "spectral - line cubes have two spatial dimensions and one spectral dimension representing frequency , wavelength or velocity , and this distinction is reflected in a number of areas , such as reference to `` channel maps '' , being 2d spatial slices at a particular spectral value . despite this",
    ", duchampis readily applicable to two - dimensional , and even one - dimensional data .",
    "the other main assumption made by duchampis that the sources are sparsely populated throughout a cube that is dominated by noise .",
    "if this assumption does not hold ( for example , in the case of observations of diffuse hifrom the milky way ) , processing will still be able to be done , but any statistics duchampcalculates will be biased by the presence of signal .",
    "the noise calculations duchampemploys are detailed in sec .",
    "[ sec - threshold ] .",
    "duchampis open - source ( gpl licensed ) software , written in c++ , and is publicly available for download from the duchampweb site .",
    "there , one can find the source code , an extensive user s guide and links to required packages .",
    "these are : cfitsio@xcite , to handle operations on fits files ; wcslib(described in * ? ? ?",
    "* ) , to correctly convert between pixel and world coordinates ; and pgplottjp / pgplot/ ] , to produce the graphical output .",
    "duchampshould run in any unix / linux - based environment , and is being maintained to ensure this continues to be the case . to this end",
    ", feedback is encouraged should any problems arise when using it .",
    "this section summarises the different steps undertaken by duchampin finding sources in an image .",
    "certain elements of the processing will be explained in detail in later sections .",
    "[ fig - flow ] indicates the key processing steps that duchampwill perform on an image .",
    "duchamprequires two inputs .",
    "the first is an image or cube , in fits format .",
    "the image merely needs to conform to the fits standard @xcite and have an image extension .",
    "fits files produced by common reduction packages , such as miriad and casa , should be suitably compatible .",
    "the second input is a set of parameters that govern the execution .",
    "these parameters allow a large degree of flexibility both in choosing different processing algorithms and in fine tuning many aspects of the searching and output .    following the reading of the image file , which can be the entire image or a specified subsection ( that is , the user can nominate pixel ranges for each dimension of the image ) ,",
    "a user - selectable set of pre - processing can be done prior to searching .",
    "this can be simple inversion of the image ( to search for negative features ) , a basic spectral bandpass subtraction to remove a continuum ripple or some other large - scale spectral variation , or either smoothing or multi - resolution wavelet reconstruction to enhance faint sources .",
    "these latter two approaches are described in sec .",
    "[ sec - noisesuppression ] .",
    "the threshold is then set according to the input parameters as either a flux or signal - to - noise threshold - sec .",
    "[ sec - threshold ] provides details of how the threshold is determined .",
    "sources are then extracted from the image , and merged to form objects . these are then parameterised - if relevant , the inversion and bandpass subtraction are reversed prior to parametrisation .",
    "the detection , merging and parametrisation steps are described in sec .",
    "[ sec - detection ] .",
    "the final object catalogue is then ready , and can be returned in a variety of formats , including ascii text and votable / xml .",
    "appendix  [ app - example ] gives an example of the ascii output file .",
    "duchampalso provides a range of graphical output to assist the user in understanding the nature of the detected sources .",
    "these include spatial maps showing the location and size of each source , and individual plots for each source showing the both the spectrum and the 0th - moment map .",
    "additional outputs include fits images containing the mask indicating the location of detected sources , or the smoothed or reconstructed image ( these fits images will have their header information formatted in the same way as the input image ) .",
    "these allow post - processing to extract individual sources , or fit particular functions to detected objects as dictated by the particular science of interest to the user .",
    "to keep the memory usage manageable for the largest range of input data possible , duchampimplements specialised techniques for storing the location of detected objects . a naive method",
    "could be to store each single pixel , but this results in a lot of redundant information being stored in memory .",
    "to reduce the storage requirements , the run - length encoding method is used for storing the spatial information . in this fashion , an object in 2d is stored as a series of `` runs '' , encoded by a row number ( the @xmath4-value ) , the starting column ( the minimum @xmath5-value ) and the run length ( @xmath6 : the number of contiguous pixels in that row connected to the starting pixel ) .",
    "a single set of @xmath7 values is called a `` scan '' .",
    "a two - dimensional image is therefore made up of a set of scans .",
    "an example can be seen in fig .",
    "[ fig - objexample ] .",
    "note that the object shown has fourteen pixels , and so would require 28 integers to record the positions of all pixels .",
    "the run - length encoding uses just 18 integers to record the same information .",
    "the longer the runs are in each scan , the greater the saving of storage over the naive method .    a 3d object is stored as a set of channel maps , with a channel map being a 2d plane with constant @xmath8-value .",
    "each channel map is itself a set of scans showing the @xmath9 position of the pixels .",
    "the fundamental step in source finding is the application of a threshold , as this determines whether a given voxel is part of a source ( and therefore of some interest ) , or part of the background .",
    "duchampuses a single threshold for the entire cube , leading to an output catalogue with a uniform selection criterion .",
    "this approach lends itself best to data that has uniform noise , but this need not be the case : duchamphas features that are able to remove some background structure ( such as large - scale spectral baseline features ) . the ability to vary the threshold in response to the local noise would allow deeper searching where the sensitivity warrants , and such features are part of development under way for the askap source - finding pipeline .    the threshold for duchampcan",
    "be given as a flux value directly , or it can be expressed as a signal - to - noise level ( a so - called `` @xmath10 '' threshold ) . in this case , the value of @xmath11 needs to be measured from the image statistics .",
    "either the entire set of image pixels are used , or a subsection can be specified ( via the parameter file input ) that specifies which pixels to measure the statistics from .",
    "this latter option allows the user to , for instance , measure the noise level in a region of the cube known to be free of sources ( for example , a frequency or velocity range in a spectral cube in which no sources are expected ) .",
    "it may be , however , that no such region can be found , and that the threshold has to be based on noise statistics estimated from data containing sources ( i.e.  voxels that are not pure noise ) .",
    "duchampprovides mechanisms to achieve this , either by using robust methods to estimate the noise , or by setting the threshold in an alternative way to the `` @xmath10 '' approach .",
    "these methods are detailed in the following sections .",
    "when searching for small objects in a noisy background detection thresholds are often set at some multiple of the standard deviation of the image background .",
    "however , direct measurements of this value can be biased by the presence of bright pixels , yielding overestimates .",
    "furthermore , these bright pixels are often in the sources of interest or some additional artefact ( such as interference ) , rather than the background , and so should not contribute to the background calculation .",
    "it can be preferable , then , to calculate the noise properties via robust methods .",
    "duchampmakes use of the median absolute deviation from the median ( henceforth madfm ) as a proxy for the standard deviation .",
    "although computationally expensive ( it requires the data to be at least partially sorted twice ) , it is very robust against bright pixels .",
    "it is best used in the situation where the data in question is dominated by a large number of noise pixels , with a relatively small number of brighter pixels present .",
    "it should be noted that the standard error of the median is 1.253 times that of the mean , and so can be more susceptible to fluctuations of the sampling , but with well - behaved noise this is acceptable .",
    "note that the assumption duchampmakes about the nature of the data , that there are relatively isolated sources embedded in data dominated by noise , is important here .",
    "the median and madfm will still be biased by the presence of bright non - noise pixels , as what will be interpreted as the centre of the distribution will be displaced from the centre of the noise pixel distribution .",
    "the bias , however , only depends on the number of non - noise pixels compared to the total size of the array under consideration , and not on the actual values of the bright pixels .    to compute the madfm , @xmath12 ,",
    "one first calculates the median value of the array , and then the median value of the absolute difference between the pixel values and the median : @xmath13 for a pure gaussian distribution of values , the madfm does not give the same value as the standard deviation , but can be scaled easily to determine what the standard deviation of that distribution is .",
    "this scaling can be calculated analytically .",
    "the normal , or gaussian , distribution for mean @xmath14 and standard deviation @xmath11 has a density function @xmath15 the median , @xmath16 , is the middle of the distribution , defined for a continuous distribution by @xmath17 from symmetry ( since @xmath18 ) , we quickly see that for the continuous normal distribution , @xmath19 .",
    "we consider the case henceforth of @xmath20 , without loss of generality .    to find the madfm , @xmath12",
    ", we find the distribution of the absolute deviation from the median , and then find the median of that distribution .",
    "this distribution is given by @xmath21 so , the median absolute deviation from the median , @xmath12 , is given by @xmath22 if we use the identity @xmath23 we find that @xmath24 hence , @xmath25 which gives @xmath26 . thus , to estimate @xmath11 for a normally distributed data set , one can calculate @xmath12 , then divide by 0.6744888 ( or multiply by 1.4826042 ) to obtain the correct estimator .",
    "this conversion can be compared to solutions quoted elsewhere .",
    "for instance , @xcite use the madfm to estimate the noise in a cube , then quote a `` cube noise '' parameter defined by @xmath27 . clearly",
    "this `` cube noise '' should not be interpreted as the standard deviation of the noise ( despite being called rms@xmath28 in the hicat catalogue ) - it is in fact only 82% of the standard deviation as calculated above .",
    "these robust techniques can be used in duchampto provide accurate estimates of the noise background . when thresholds are defined according to a multiple of the standard deviation , the madfm is always converted to an equivalent standard deviation using the conversion factor obtained above .",
    "users are cautioned , however , that even these robust techniques can be biased by non - noise signal . in the case that the data being examined is purely noise plus some positive signal ( or artefacts ) , then the median will not fall in the middle of the noise distribution , but slightly to the positive side , and similarly for the madfm . for typical data , however , where the real signals are sparsely populated and outnumbered by noise voxels this bias will be small , and certainly smaller than the bias arising from normal statistics ( mean and standard deviation )",
    ".    it may also be the case that the noise is not precisely gaussian .",
    "for instance , @xcite consider wsrt interferometer images and find an excess number of negative pixels over that expected from an ideal gaussian distribution .",
    "these are ascribed to interferometry artefacts in the image , which are expected to be symmetric about zero flux due to the purely relative nature of interferometric data when auto - correlations are not included . in practice , if one knows , or can estimate the scaling between madfm and @xmath11 , then it is straightforward to scale the requested threshold such that the appropriate @xmath10 level is applied .",
    "these points highlight the need for users to have some understanding of the nature of the data being searched .",
    "although one can estimate the standard deviation more accurately with the robust techniques , the problem of false discoveries will still be present .",
    "a technique has been presented @xcite that attempts to control the false discovery rate ( fdr ) in the presence of noise .",
    "this fixes the false discovery rate ( given by the number of false detections divided by the total number of detections ) to a certain value @xmath29 ( e.g.  @xmath30 implies 5% of detections",
    "are false positives ) . in practice ,",
    "an @xmath29 value is chosen , and the ensemble average fdr ( i.e.  @xmath31 ) when the method is used will be less than @xmath29 .",
    "one calculates @xmath32  the probability , assuming the null hypothesis is true , of obtaining a test statistic as extreme as the pixel value ( the observed test statistic )  for each pixel ( in duchamp , normal statistics are assumed ) , and sorts them in increasing order .",
    "one then calculates @xmath33 where @xmath34 and then rejects all hypotheses whose @xmath32-values are less than or equal to @xmath35 .",
    "( so a @xmath36 will be rejected even if @xmath37 . )",
    "note that `` reject hypothesis '' here means `` accept the pixel as an object pixel '' ( i.e.  we are rejecting the null hypothesis that the pixel belongs to the background ) .",
    "the @xmath38 value here is a normalisation constant that depends on the correlated nature of the pixel values . if all the pixels are uncorrelated , then @xmath39 . if @xmath40 pixels are correlated , then their tests will be dependent on each other , and so @xmath41 .",
    "@xcite consider real radio data , where the pixels are correlated over the beam .",
    "for the case of three - dimensional spectral data , the beam size would be multiplied by the number of correlated channels .",
    "this number can be specified via the input parameter set , and so a user can set it according to their knowledge of the cube being searched .",
    "this can be important if the cube is known to have been smoothed prior to searching , or if the telescope backend results in some correlation between neighbouring channels .",
    "the theory behind the fdr method implies a direct connection between the choice of @xmath29 and the fraction of detections that will be false positives .",
    "these detections , however , are individual pixels , which undergo a process of merging and rejection , and so the fraction of the final list of detected objects that are false positives will often be much smaller than @xmath29 .",
    "the problem of identifying sources in a two - dimensional image , by means of a single threshold , has well defined solutions .",
    "@xcite presented an algorithm for locating objects in an image with a single raster - scan pass through the image .",
    "such algorithms are possible because of the well - nested nature of objects in two dimensions : on a given row of an image , if object a lies between two portions of object b , then _ all _ of object a lies between those two portions .",
    "the problem of 3-dimensional source extraction is , in general , a more complex one .",
    "the extra dimension means that the well - nested property no longer applies , and so distinct objects can be intertwined while still remaining distinct .",
    "this makes it hard for a simple raster - scanning ( or equivalent ) algorithm to be applied .",
    "the approach duchampuses has two aspects : multiple lower - dimensional searches , followed by merging of the detected sources to produce complete three - dimensional sources .",
    "the simplest search duchampcan do is to treat each spatial pixel as a distinct spectrum , search with a simple 1d search routine , and combine the objects afterwards . the alternative ( actually the default )",
    "is to treat each frequency channel as a two - dimensional image , and search it using the algorithm of @xcite .",
    "the algorithm performs a raster scan of the image , processing one horizontal line at a time and builds up a list of objects by keeping track of which objects in each row are connected ( in an 8-fold manner ) to objects on the previous row .",
    "objects from different images are then combined afterwards ( see sec .  [ sec - merge ] ) .",
    "duchamphas been developed for three - dimensional data , and even though we are using two- or one - dimensional searching , we know that the objects we are interested in will be three dimensional ( that is , extended in all three directions )",
    ". ideally one would want to make use of this fact to help identify sources . a good way to do",
    "this is to use the smoothing or wavelet reconstruction approaches introduced in sec .",
    "[ sec - noisesuppression ] . generally , smoothing in a direction perpendicular to the direction of the searching will respond well to 3d structures .",
    "for instance , if a 3d source is smoothed spectrally , then its brightness in each 2d channel map will be enhanced relative to the background due to the flux in neighbouring channels .",
    "searching in 2d will then be more sensitive than a search without the prior smoothing .",
    "a single detection threshold is used at this point , effectively treating each voxel in a binary manner .",
    "once a source list has been established ( see sec .  [ sec - merge ] ) , the objects can be `` grown '' to a secondary threshold if required .",
    "this enables the detection of all faint features provided they have at least one ( or some number  the minimum size of a detection is a user - selectable parameter ) voxel above the primary threshold .      to form the final catalogue , objects found in individual channel maps or spectra need to be merged .",
    "merging takes place if a pair of objects are judged to be close according to criteria determined by the user .",
    "either the objects must be adjacent , meaning there is at least one pair of voxels ( one voxel from each object ) that are adjacent in one direction , or they must have a separation less than a given number of voxels ( which can differ between spatial and spectral separations ) .",
    "the merging is done in two stages .",
    "when a single search of a 2d plane or a 1d spectrum is complete , each detected object ( stored as a set of scans ) is added to a master list of detections .",
    "it is first compared to all those present in the list , and if it is close to one ( according to the criteria defined by the user ) the two objects are merged .",
    "no other comparisons are made at this point ( to save time ) .",
    "if it is close to none , it is added to the end of the list .",
    "once all the searching has been completed , the list needs to be examined for any further pairs of close objects .",
    "although an initial comparison has been done , there will be unexamined pairs present .",
    "for instance , consider objects a and b , which are not close , and a new object c that is close to both a & b. when c is added to the list , it is first compared to a , found to be close , and combined .",
    "the list then has the combined ac object and b , which are both close .",
    "a second comparison needs to be made to combine these two .",
    "this secondary merging stage looks at successive pairs of objects from the list .",
    "if they are close , the second object is added to the first and removed from the list .",
    "this ensures that all possible combinations are found .",
    "the way the comparison is done is to first look at the pixel ranges ( minimum & maximum in each dimension ) to see if they overlap or lie suitably close according to the above criteria .",
    "this is a quick way of ignoring objects that should not be merged without doing a detailed examination",
    ". if they do , then the individual scans ( refer sec .",
    "[ sec - scan ] ) from common channels in each object are examined until a pair is found that satisfy the criteria for closeness .",
    "note that this can be done directly on scans , without needing to look at individual pixels .",
    "if growing of sources is requested , this is done after the merging step .",
    "a second merge is then performed to ensure that no duplicate detections are present , due to sources growing over each other .",
    "once the unique set of sources has been found , duchampmeasures a basic set of parameters for them .",
    "the location of the source is measured in three ways : the peak position , or the voxel containing the maximum flux value ; the `` average '' position , being the average of all pixel values in each dimension @xmath42 where @xmath40 is the number of pixels ; and the `` centroid '' position , being the flux - weighted average of all pixels in each dimension @xmath43 where @xmath44 is the total flux of all detected pixels .    all positions and spectral locations",
    "are given in both pixel and world coordinates , making use of the wcslib library ( described in * ? ? ?",
    "the sizes of the sources are reported : in the spatial direction , this is just the extent of the detected source ( although not every pixel within this extent is necessarily detected  if the closeness criteria permit it , there can be voxels below the threshold within this range ) .    in the spectral direction the parameters w50 and w20 are provided , being the width at 50% and 20% of the peak flux .",
    "these are measured on the integrated spectrum ( i.e.  the spectra of all detected spatial pixels summed together ) , and are defined by starting at the outer spectral extent of the object ( the highest and lowest spectral values ) and moving in or out until the required flux threshold is reached .",
    "the widths are then just the difference between the two values obtained .",
    "the full spectral width of the detection ( wvel ) is also reported  this is analogous to the spatial extent , being the extent of the detected source in the spectral direction .",
    "the flux of the source is given by the peak flux and both the `` total flux '' , defined above as @xmath45 , and the `` integrated flux '' @xmath46 , or the total flux integrated over the spectral extent and corrected for the beam : @xmath47 where @xmath48 is the area of a beam of major and minor axes @xmath29 and @xmath49 ( in pixels ) , and @xmath50 is the spectral width of each voxel .",
    "these parameters are intended as a basic set , as duchamphas not been optimised for any particular survey .",
    "those wanting more detailed or science - specific parameters should make use of the mask cubes ( as discussed in sec .  [ sec - philosophy ] ) to extract the locations of detected objects and do their own processing from there .",
    "note that these parameters are measured using just the detected pixels , since duchampdoes no fitting of spectral or spatial profiles .",
    "this means that some parameters , integrated flux for instance , will be biased by not including voxels that fall below the detection ( or secondary ) threshold .",
    "the exceptions to this are w50 and w20 , as these are calculated using the full integrated spectrum , including channels not necessarily forming part of the detection .",
    "the presence of noise ultimately limits the ability to detect objects .",
    "this occurs in two ways : reducing the completeness of the final catalogue by making faint sources appear to have a flux erroneously below the detection threshold ; and reducing the catalogue reliability by generating false detections through detection of spurious noise peaks .",
    "duchampimplements several methods to minimise the effect of the noise and improve the reliability , which are discussed here .",
    "one option is to smooth the data , either spectrally ( via a hanning filter ) , or spatially ( via convolution with a gaussian kernel ) . when the size of the filter / kernel is chosen appropriately",
    ", these approaches can be very effective at enhancing structures of a particular scale , and suppressing the finer - scale noise fluctuations .",
    "they are limited , however , to enhancing just the scale determined by the filter size .",
    "this becomes an _ a priori _ choice made by the user , which may be appropriate if searching for a particular type of signal ( for instance , galaxies of a particular velocity width ) , but can bias the results of a truly blind search by suppressing other scales .    to examine how the choice of scale influences the outcome , fig .",
    "[ fig - smoothexample ] shows the effect of smoothing on differently - sized sources ( just in one dimension , for ease of representation ) .",
    "the model spectrum , shown in ( a ) and in ( b ) ( the latter with noise added ) , contains five sources .",
    "the flux scale is chosen so that the noise has a standard deviation of 1 unit .",
    "the three central sources # 2 , # 3 & # 4 each have the same peak flux of 2 with different full width at half maximum ( fwhm ) values : 5 , 15 and 45 channels respectively , while # 1 and # 5 have the same integrated flux as # 3 , but the same width as # 2 and # 4 respectively",
    ". panel ( b ) shows this spectrum with noise added , and with a @xmath51 threshold superimposed .",
    "the value of @xmath11 is that measured from the spectrum itself , and so is a little larger than 1 due to the presence of positive sources(in this example it is 1.11 ) . below the spectrum",
    "we indicate with bars where we detect sources above this threshold .",
    "only the first two sources are detected  the second perhaps due to a boost from the noise .",
    "panel ( c ) shows the spectrum after smoothing with a hanning filter of the same width as the first two sources .",
    "this keeps the first two sources visible , and makes enough structure visible in sources # 3 and # 4 for them to be detected , although note that # 4 is fragmented .",
    "when smoothing with the width of source # 2 ( panel ( d ) ) , all five sources are able to be recovered , with source # 4 now forming a single detection . by the time we smooth with the width of sources # 4 and # 5 ( panel ( e ) )",
    ", the weakness of # 2 results in it not being detected - we are starting to smooth over structure on its scale ( # 1 is still detected , but its peak strength is greatly reduced ) .            clearly , smoothing with a given filter imposes a choice of scales for which the search is sensitive , but this may result in information on other scales ( either finer or broader ) being lost . an alternative approach is to use a multi - resolution wavelet transform that works over a range of scales . duchampuses the _  trous _ , or `` with holes '' , transform @xcite to sample the structure on scales at logarithmically - spaced intervals . the algorithm used in the _  trous_transform is as follows :    1 .   initialise the reconstructed array to zero everywhere .",
    "discretely convolve the input array with the chosen filter function .",
    "define the wavelet coefficients by taking the difference between the convolved array and the input array ( input @xmath52 convolved ) .",
    "4 .   threshold the wavelet coefficients array : only keep those values above a designated threshold , and add them to the reconstructed array .",
    "double the separation between elements of the filter function , creating the holes in the filter coverage alluded to by the transform name .",
    "repeat from step 2 , using the convolved array instead of the input array .",
    "continue until the required maximum number of scales is reached .",
    "this is defined by the largest scale where the filter function size is not greater than the array size .",
    "add the final smoothed ( i.e.  convolved ) array to the reconstructed array .",
    "this provides the `` dc offset '' , as each of the wavelet coefficient arrays will have zero mean .",
    "the doubling of the filter scale means that a range of scales from the finest to the broadest are sampled in creating the reconstructed array .",
    " trous_transform is a so - called `` redundant '' transform .",
    "this means that the sum of all wavelet arrays , plus the final smoothed array , results in the input array .",
    "symbolically , denoting the input and output arrays by @xmath53 and @xmath54 , and the smoothed and wavelet arrays at scale @xmath55 by @xmath56 and @xmath57 , this can be written @xmath58    applying a threshold to the wavelet coefficients has the effect of only keeping those parts of the input array where there is significant signal at some scale . if the threshold is chosen well , the random noise present in astronomical images can be suppressed quite effectively .",
    "the threshold applied is a constant multiple ( provided by the user ) of the noise level in the wavelet array .",
    "the reconstruction may be done in one , two or three dimensions .",
    "the three - dimensional case reconstructs the full array at once , by defining a filter function that has the same profile in each of the three dimensions .",
    "a single largest scale is used , limited by the smallest array dimension . in the one - dimensional case ,",
    "the 1d spectrum for each spatial pixel is reconstructed independently , while the two - dimensional case sees each 2d channel map reconstructed independently .",
    "examples of reconstructions with different dimensionality are discussed in sec .",
    "[ sec - reconexample ] below .",
    "it was shown in sec .",
    "[ sec - smooth ] and fig .",
    "[ fig - smoothexample ] how smoothing reduces the measured noise in a spectrum , by making pixels more and more correlated with their neighbours . the implication for the wavelet reconstruction of this is to reduce the measured noise level at each scale , affecting how the threshold is applied . since we need to know each scale s noise , to apply a fixed signal - to - noise threshold , we need to calculate it for each wavelet array . rather than doing so directly , we scale the noise from that measured in the input array to take into account the increasing correlation of the pixels .",
    "to do this , for a given filter function , we transform a spectrum of zero flux save for a single unit pixel in the centre . by summing in quadrature the pixel values at each wavelet scale",
    ", we can find the scaling factor for the standard deviation .",
    "this procedure is explained in detail in appendix  [ app - scaling ] .",
    "these calculations depend on the pixels in the original array being independent",
    ". if they are not ( for instance , the beam or point spread function of a telescope will result in partially correlated neighbouring pixels , or neighbouring channels may be partly correlated by the instrument or by some processing prior to duchampanalysis ) , taking the sum in quadrature is not the appropriate course of action . in this case , the only way to obtain the correct scaling factors would be through simulations that properly account for the correlations in the pixels .              in fig .",
    "[ fig - reconexamplescales ] , the step - by - step detail of a reconstruction is shown , highlighting what happens to the smoothed and wavelet spectra at each scale .",
    "the left hand side of the figure shows the spectrum smoothed at successively larger scales  the effects described in sec .",
    "[ sec - smooth ] are apparent here too .",
    "an 8th smoothed spectrum , the `` dc offset '' described above , is also computed , although , for simplicity , is not plotted ( it is a fairly flat line and has no matching right hand side column ) .    the right hand side of fig .",
    "[ fig - reconexamplescales ] shows how the wavelet spectrum changes from scale to scale .",
    "these spectra show where the strongest signal ( either positive or negative ) can be found for a given scale - note how source # 1 is strongest between scales 3 and 5 , # 3 strongest at scales 5 and 6 , and # 4 at 6 and 7 .",
    "also shown on the wavelet spectra are the thresholds used in the reconstruction .",
    "these are set at @xmath59 , where @xmath11 for each spectrum is scaled in the manner described above from that measured in the input spectrum . only data",
    "outside the dotted lines are added together ( along with the dc offset ) to create the reconstructed spectrum .",
    "the effectiveness of the wavelet reconstruction are evident in , fig .",
    "[ fig - reconexampleresults ] , where the results of reconstructing the model spectrum used in fig .",
    "[ fig - smoothexample ] are shown .",
    "three wavelet threshold levels are used : 2 , 3 ( as used in fig .",
    "[ fig - reconexamplescales ] ) and 4 @xmath11 .",
    "clearly , as the threshold is increased , more of the fine - scale noise is removed .",
    "note , however , that the sources , particularly the fainter , narrower ones , get affected as well , so that the @xmath60 reconstruction starts to lose sources # 2 and # 3 .    in setting the threshold",
    ", the wavelet reconstruction requires a different approach to the smoothing case . a signal to noise threshold",
    "is determined based on the statistics measured from the residual spectrum .",
    "if the wavelet reconstruction worked perfectly , this would contain only the noise , and so would give the exact measurement of the image noise .",
    "but the threshold is applied to the reconstructed spectrum , which , ideally , would hold just the sources of interest . since there is ( ideally ) no noise present",
    ", the threshold can be set much lower than one would otherwise consider .",
    "the example in fig .",
    "[ fig - reconexampleresults ] shows a @xmath61 threshold applied to all cases . when applied to the raw spectrum ,",
    "a vast number of clearly spurious sources is returned , but the reconstructions remove the bulk of these spurious sources .",
    "this can be taken to extremes  a @xmath62 detection threshold even allows recovery of source # 5 for all cases , with only 4 and 1 additional ( spurious ) sources for the @xmath51 and @xmath60 reconstructions respectively .    since the residuals have had structures that are not noise removed , estimating the noise properties from them provide much better estimates .",
    "the values for the @xmath61 threshold in each of the spectra in fig .",
    "[ fig - reconexampleresults ] are 1.25 for the input spectrum , compared to 0.92 , 1.03 and 1.06 for the @xmath63 , @xmath51 and @xmath60 results respectively ( these values include the mean  the robust estimates for the standard deviation of the noise are 1.11 for the input , and 0.89 , 0.99 and 1.03 for the respective reconstructions ) .    in fig .",
    "[ fig - reconexampledim ] , we show the effect of changing the dimensionality of the reconstruction . for this",
    ", we use real data from the hipasssurvey @xcite .",
    "shown is a single spectrum , taken from the hipasscube # 201 at @xmath64 , @xmath65 .",
    "this position is chosen to highlight two galaxies at velocities of 861 km  s@xmath66 ( ugca 120 ) and 2319 km  s@xmath66 ( ngc 2196 ) which have quite different spectral profiles - this highlights how the reconstruction recovers structure on different scales .",
    "three reconstructions are performed , using the one- , two- and three - dimensional algorithms .",
    "clearly , all three approaches recover both galaxies , while removing a large proportion of the noise .",
    "there are differences , however .",
    "there is a larger degree of channel - to - channel noise remaining in the 2d reconstruction , and it appears that more of the input spectrum is included in the reconstructed spectrum ( see around @xmath67 km  s@xmath66 and in the range of the broader galaxy ) .",
    "this appears to be largely due to the way the reconstruction has been done .",
    "the 2d reconstruction works on each channel map separately , so each channel in the extracted spectrum has been calculated independently , depending not on its neighbouring channels but the structure in the channel map .",
    "the channel maps are also where the largest pixel - to - pixel correlations exist ( that is , within a beam ) , which allows beam - sized noise fluctuations to rise above the reconstruction threshold more frequently than in the 1d case .    the 3d case ,",
    "while still containing more noise than the 1d ( for the same reason - it is also seeing the spatially - correlated beam noise ) , does seem to give a better - looking reconstruction of the shape of both the galaxies and the spectral baseline ( the sharp negative features next to the galaxies in the 1d spectrum are less well - defined in the 3d spectrum ) .",
    "the 3d reconstruction uses the full range of information for a given pixel , so that structure in all three directions contributes to the information about its reconstructed value .",
    "the aim of this section is to provide an indication of the relative performance of the different algorithms of duchamp .",
    "this is done through simple tests using synthetic data purely to provide indicative performance .",
    "synthetic cubes were used for this purpose , rather than real survey data from , say , hipass , as this prevents systematic effects that limit the completeness of the survey from affecting the interpretation of the completeness of the source - finder .",
    "a complete set of tests that is relevant for a particular survey would use simulations and/or real data that better match the noise and source characteristics expected for that survey , and so is beyond the scope of this paper .",
    "the synthetic data used for these tests were cubes of dimensions @xmath68 voxels , with a grid of sources of varying brightnesses .",
    "we use 16 sources , ranging in peak brightness from 1.5 to 9 units , increasing in increments of 0.5 .",
    "each source is spatially unresolved , so that it lies in a single pixel , and has a spectral shape of a gaussian with a 5 channel fwhm .",
    "the locations of the sources are chosen so that there is at most one source in a given channel map or spectrum .",
    "the cube then has noise added to each voxel , sampled from a normal distribution with a standard deviation of 1 flux unit ( meaning the flux units in the cube are essentially units of @xmath11 ) , and is then convolved with a gaussian beam that has a 3-pixel fwhm .",
    "this cube is then searched for sources .",
    "this process is repeated 100 times for different realisations of the noise , to provide an idea of the average completeness and reliability of the algorithms .",
    "we also make a reference cube without noise , but with the same beam .",
    "this provides the `` truth '' result - what would be found in the ideal case of no noise .",
    "this reference cube is searched both to a depth of 0.01 ( providing the full extent of sources ) , and to the same depth as searches in the noisy cubes .",
    "the cubes produced in this manner are then able to be searched using different duchampparameter sets .",
    "each parameter set will correspond to a different mode of operation : basic searching , smoothing or wavelet reconstruction , with different requirements on the minimum number of spatial pixels & spectral channels , as well as different smoothing widths or reconstruction thresholds .",
    "each parameter set is then applied with a range of input signal - to - noise thresholds : 3 , 4 , 5 , 6 , 7 and 8 .",
    "a given signal - to - noise threshold will provide different effective flux thresholds when different pre - processing is done : note that the different pre - processing will result in different effective flux thresholds for a given signal - to - noise : in the smoothing case , the noise is measured from the smoothed cube , where it is lower due to the correlated pixels , and the threshold is applied to the same cube ; while in the reconstruction case the noise is measured from the rejected pixels ( which ideally will indeed be noise ) , and the threshold is applied to the reconstructed pixels .",
    "searching each of the 100 cubes , and then cross - matching with the truth result , allows us to find the average number of times each source is detected , subject to the noise . in this way",
    ", a completeness curve can be constructed for a given search threshold , showing the fraction of sources detected as a function of peak source brightness .",
    "we can also determine the reliability for a given search threshold , being the fraction of detected sources that correspond to a true source .",
    "the reliability measure here is implicitly dependent on the image size . in the case of these simulations ,",
    "a larger image size would result in more noise detections with no additional true sources , leading to a decrease in reliability .",
    "the key aspect to look at when examining the results of the tests is the different dependence of the reliability on the detection threshold from test to test .      ,",
    "@xmath69 and @xmath70 , with no restriction on the size of objects .",
    "the dashed lines show searches at @xmath51 , @xmath60 and @xmath69 , rejecting sources that span only one channel . the reliability ( @xmath71 , the fraction of detected sources that are real ) are indicated for each search type . ]    , but for searches with different types of pre - processing : ( a ) smoothing with different widths : 3 , 11 and 21 pixels ( b ) 1d wavelet reconstruction with different reconstruction thresholds : @xmath63 , @xmath51 and @xmath60 . in each plot , each pre - processing type has its own line style , while each detection threshold has its own symbol , allowing comparison of the same detection threshold used with different pre - processing . ]    in fig .",
    "[ fig - comp - basic ] , we compare different searches that use no pre - processing , but only differing degrees of rejection by applying minimum numbers of channels and spatial pixels .",
    "it is apparent that applying even a small degree of post - finding rejection ( rejecting sources that span only a single channel ) , the reliability greatly improves , from 4% for a @xmath60 search to 95% .",
    "the completeness , however , does reduce : the completeness level for the same signal - to - noise threshold increases by about 1 unit .",
    "this is due to single - channel matches that align with the true sources - many of these are probably chance coincidences of noise peaks ( which are more likely to be single - channel sources ) , and so applying this rejection will have improved the quality of the resultant catalogue .",
    "furthermore , the decrease in the number of spurious sources means that deeper searches can be done .",
    "the deepest one can go with less than 1 source in 10 spurious is a @xmath70 search when accepting all sources , but by rejecting single - channel sources one can use a @xmath60 search and get at least 1 flux unit deeper for the same completeness .",
    "[ fig - comp - preproc ] compares a few different types of preprocessing , together with a basic search rejecting single - channel sources . both spectral",
    "smoothing and one - dimensional wavelet reconstruction are considered , each with a range of the critical parameter ( smoothing width or reconstruction threshold ) .",
    "[ fig - comp - preproc](a ) shows the spectral smoothing results , using widths of 3 , 11 and 21 channels . these widths span scales below and above the spectral widths of the sources ( gaussians with fwhm of 5 pixels ) .",
    "the 11-pixel case does the best in terms of completeness , as it is most closely matched to the shape of the sources .",
    "[ fig - comp - preproc](b ) shows the results from wavelet reconstruction case .",
    "these are slightly more complicated  as was seen in sec .",
    "[ sec - reconexample ] , increasing the reconstruction threshold allows you to search to deeper signal - to - noise thresholds , and increases the reliability at the expense of completeness for a search at a given threshold .",
    "the symbols in fig .  [ fig - comp - preproc](b ) relate to the detection threshold , while the lines relate to the reconstruction threshold .    for the particular simulation under consideration , where the sources have a single spectral size ,",
    "the smoothing approach does slightly better than the wavelet , although the difference in the completeness level is only about half a flux unit .",
    "these tests indicate that getting complete below about @xmath60 is very hard , even with ideally behaved noise .",
    "the other consideration in choosing a processing method is the time required .",
    "this is particularly the case in a survey , where a large number of cubes may have to be searched , or in the case where a number of different search types are required .",
    "while the utility of providing execution times is debatable , given the range of capabilities that are available to researchers , being able to compare execution times for the same type of search with different processing options can at least provide some idea of the relative processing load .",
    "to this end , duchampwas run using a macbook pro ( 2.66ghz , 8 mb ram ) on the hipasscube # 201 ( of size @xmath72 voxels , or 53  mb ) , with detection thresholds of both @xmath73  jy  beam@xmath66 ( no sources will be found , so that the time taken is dominated by preprocessing ) , and 35  mjy  beam@xmath66 ( or @xmath74 , chosen so that the time taken will include that required to process sources ) . the basic searches , with no pre - processing done , took less than a second for the high - threshold search , but between 1 and 3  min for the low - threshold case  the numbers of sources detected ranged from 3000 ( rejecting sources with less than 3 channels and 2 spatial pixels ) to 42000 ( keeping all sources ) .",
    "when smoothing , the raw time for the spectral smoothing was only a few seconds , with a small dependence on the width of the smoothing filter .",
    "and because the number of spurious sources is markedly decreased ( the final catalogues ranged from 17 to 174 sources , depending on the width of the smoothing ) , searching with the low threshold did not add much more than a second to the time .",
    "the spatial smoothing was more computationally intensive , taking about 4 minutes to complete the high - threshold search .",
    "the wavelet reconstruction time primarily depended on the dimensionality of the reconstruction , with the 1d taking 20  s , the 2d taking 30 - 40  s and the 3d taking 2 - 4  min .",
    "the spread in times for a given dimensionality was caused by the different reconstruction thresholds , with lower thresholds taking longer ( since more pixels are above the threshold and so need to be added to the final spectrum ) . in all cases",
    "the reconstruction time dominated the total time for the low - threshold search , since the number of sources found was again smaller than the basic searches .",
    "in response to the growing challenges of accurate and reliable source extraction in large 3d spectral - line datasets , i have developed duchamp , a stand - alone source - finding package .",
    "since the noise is the limiting factor in source extraction , duchampuses robust methods to accurately estimate the noise level , and provides pre - processing options to prevent the cataloguing of as many noise - generated spurious sources as possible .",
    "duchamphas been designed for use with spectral - line surveys in general , and hisurveys in particular , but can readily be applied to other types of data .",
    "the core functionality of duchampis already forms the basis for the prototype source finder for the askap processing pipeline , and is being evaluated , along with other source - finding approaches , by the askap survey science teams for inclusion in the final pipeline software .",
    "duchampis maintained independently , however , and is provided as a separate , open - source software package .",
    "thanks are due to the many people who have provided assistance and advice during the development and testing of duchamp , particularly ivy wong , tobias westmeier , mary putman , cormac purcell , attila popping , tara murphy , enno middelberg , korinne mcdonnell , philip lah , russell jurek , simon guest , luca cortese , david barnes and robbie auld . additionally , emil lenc and the referee anita richards both provided valuable comments that improved the paper .    duchampmakes use of the pgplottjp / pgplot/ ] , cfitsio and wcslib software packages .",
    "the bulk of this work was conducted as part of a csiro emerging science postdoctoral fellowship , and duchampcontinues to be maintained both as a standalone package and as part of the software development effort for the askap telescope .",
    "this work was supported by the nci national facility at the anu .",
    "d.  g. , et  al .",
    ", 2001 , mnras , 322 , 486    m. , greisen e. , 2002 , a&a , 395 , 1077    deboer d. , et  al . , 2009 , proceedings of the ieee , 97 , 1507    c.  j. , barnes d.  g. , hassan a.  h. , 2010 , proceedings of the sixth ieee international conference on e - science workshops ( brisbane , australia , december 2010 )    a.  h. , fluke c.  j. , barnes d.  g. , 2011 , in i.  n.  evans , a.  accomazzi , d.  j.  mink , & a.  h.  rots ed .",
    ", astronomical society of the pacific conference series vol .",
    "442 of astronomical society of the pacific conference series , distributed gpu volume rendering of askap spectral data cubes .",
    "pp 207210    a. , miller c. , connolly a. , genovese c. , nichol r. , wasserman l. , 2002 , aj , 123 , 1086    w. , putman m.  e. , heitsch f. , stanimirovi s. , peek j.  e.  g. , clark s.  e. , 2011 , aj , 141 , 57    m. , sato m. , gill j.  a. , fleenor m.  c. , brick a. , 2008 , mnras , 390 , 289    b. , staveley - smith l. , , 2009 , wallaby : the widefield askap l - band legacy all - sky blind survey , askap survey science proposal    b.  s. , et  al . , 2004 ,",
    "aj , 128 , 16    p. , et  al . , 2009 , mnras , 399 , 1447    lutz r. , 1980 , the computer journal , 23 , 262    k.  e. , wardle m. , vaughan a.  e. , 2008 , mnras , 390 , 49    m. , 2009 , in panoramic radio astronomy : wide - field 1 - 2 ghz research on galaxy evolution exploring the hi universe with askap    m. , et  al .",
    ", 2004 , mnras , 350 , 1195    c. , genovese c. , nichol r. , wasserman l. , connolly a. , reichart d. , hopkins a. , schneider j. , moore a. , 2001 , aj , 122 , 3492    r.  p. , et  al .",
    ", 2011 , pasa , 28 , 215    w. , 1999 , in d.  m.  mehringer , r.  l.  plante , & d.  a.  roberts ed .",
    ", astronomical data analysis software and systems viii vol .",
    "172 of astronomical society of the pacific conference series , cfitsio , v2.0 : a new full - featured data interface .",
    "p.  487",
    "w.  d. , chiappetti l. , page c.  g. , shaw r.  a. , stobie e. , 2010 , a&a , 524 , a42",
    "a. , braun r. , 2011 , a&a , 528 , a28    c.  r. , et  al .",
    ", 2012 , mnras , in press    m.  e. , et  al . , 2002 ,",
    "aj , 123 , 873    m.  e. , peek j.  e.  g. , muratov a. , gnedin o.  y. , hsu w. , douglas k.  a. , heiles c. , stanimirovic s. , korpela e.  j. , gibson s.  j. , 2009 , apj , 703 , 1486    j .- l . , murtagh f. , 1994 , a&a , 288 , 342    starck j .- l . ,",
    "murtagh f. , 2002 , `` astronomical image and data analysis '' .",
    "springer    o.  i. , 2007 , phd thesis , university of melbourne    o.  i. , et  al .",
    ", 2006 , mnras , 371 , 1855",
    "llll & @xmath75 spline & triangle & haar + & @xmath76 & @xmath77 & @xmath78 +   + 1 & 0.723489806 & 0.612372436 & 0.707106781 + 2 & 0.285450405 & 0.330718914 & 0.5 + 3 & 0.177947535 & 0.211947812 & 0.353553391 + 4 & 0.122223156 & 0.145740298 & 0.25 + 5 & 0.0858113122 & 0.102310944 & 0.176776695 + 6 & 0.0605703043 & 0.0722128185 & 0.125 + 7 & 0.0428107206 & 0.0510388224 & 0.0883883476 + 8 & 0.0302684024 & 0.0360857673 & 0.0625 + 9 & 0.0214024008 & 0.0255157615 & 0.0441941738 + 10 & 0.0151336781 & 0.0180422389 & 0.03125 + 11 & 0.0107011079 & 0.0127577667 & 0.0220970869 + 12 & 0.00756682272 & 0.00902109930 & 0.015625 + 13 & 0.00535055108 & 0.00637887978 & 0.0110485435 +   + 1 & 0.890796310 & 0.800390530 & 0.866025404 + 2 & 0.200663851 & 0.272878894 & 0.433012702 + 3 & 0.0855075048 & 0.119779282 & 0.216506351 + 4 & 0.0412474444 & 0.0577664785 & 0.108253175 + 5 & 0.0204249666 & 0.0286163283 & 0.0541265877 + 6 & 0.0101897592 & 0.0142747506 & 0.0270632939 + 7 & 0.00509204670 & 0.00713319703 & 0.0135316469 + 8 & 0.00254566946 & 0.00356607618 & 0.00676582347 + 9 & 0.00127279050 & 0.00178297280 & 0.00338291173 + 10 & 0.000636389722 & 0.000891478237 & 0.00169145587 + 11 & 0.000318194170 & 0.000445738098 & 0.000845727933 +   + 1 & 0.956543592 & 0.895954449 & 0.935414347 + 2 & 0.120336499 & 0.192033014 & 0.330718914 + 3 & 0.0349500154 & 0.0576484078 & 0.116926793 + 4 & 0.0118164242 & 0.0194912393 & 0.0413398642 + 5 & 0.00413233507 & 0.00681278387 & 0.0146158492 + 6 & 0.00145703714 & 0.00240175885 & 0.00516748303 + 7 & 0.000514791120 & 0.000848538128 & 0.00182698115 +    as discussed in section  [ sec - atrous ] , the _",
    " trous_algorithm requires that the thresholds applied at each scale are equivalent .",
    "the threshold is defined as a multiple of the standard deviation of the original spectrum , scaled appropriately for each wavelet scale .    however , since the wavelet arrays are produced by convolving the input array by an increasingly large filter , the pixels in the coefficient arrays become increasingly correlated as the scale of the filter increases . this results in the measured standard deviation from a given coefficient array decreasing with increasing scale . to calculate this",
    ", we need to take into account how many other pixels each pixel in the convolved array depends on .    to demonstrate , suppose we have a 1-d array with @xmath40 pixel values given by @xmath79 , and we convolve it with the b@xmath80-spline filter , defined by the set of coefficients @xmath81",
    ". the flux of the @xmath55th pixel in the convolved array will be @xmath82 and the flux of the corresponding pixel in the wavelet array will be @xmath83 now , assuming each pixel has the same standard deviation @xmath84 , we can work out the standard deviation for the wavelet array : @xmath85 thus , the first scale wavelet coefficient array will have a standard deviation of 72.3% of the input array .",
    "this procedure can be followed to calculate the necessary values for all scales , dimensions and filters used by duchamp .",
    "calculating these values is clearly a critical step in performing the reconstruction .",
    "the method used by @xcite was to simulate data sets with gaussian noise , take the wavelet transform , and measure the value of @xmath11 for each scale .",
    "we take a different approach , by calculating the scaling factors directly from the filter coefficients by taking the wavelet transform of an array made up of a 1 in the central pixel and 0s everywhere else .",
    "the scaling value is then derived by taking the square root of the sum ( in quadrature ) of all the wavelet coefficient values at each scale .",
    "we give the scaling factors for the three filters available to duchampin table  [ tab - scaling ] .",
    "these values are hard - coded into duchamp , so no on - the - fly calculation of them is necessary .",
    "memory limitations prevent us from calculating factors for large scales , particularly for the three - dimensional case ( hence the smaller table ) .",
    "to calculate factors for higher scales than those available , we divide the previous scale s factor by either @xmath86 , @xmath87 , or @xmath88 for 1d , 2d and 3d respectively .",
    "[ fig - examplecat ] shows an example of the ascii output table produced by duchamp .",
    "this table comes from the hipass cube # 201 , processed with 1d wavelet reconstruction ( with a @xmath51 reconstruction threshold ) and searched to a @xmath60 threshold ( to provide a small catalogue for demonstration purposes ) .    .... -------------------------------------------------------------------------------------------   obj #            name      x      y      z            ra           dec       vel      w_ra     w_dec                                                                     [ km / s ] [ arcmin ] [ arcmin ] -------------------------------------------------------------------------------------------      1 j060921 - 215712   59.4 140.5 114.6   06:09:21.89 -21:57:12.54   225.613     64.57     43.20      2 j060229 - 254657   83.4   83.3 118.3   06:02:29.09 -25:46:57.48   274.754     40.05     27.94      3 j060604 - 272027   71.4   59.8 121.3   06:06:04.62 -27:20:27.38   314.287     72.41     51.54      4 j061118 - 213700   52.5 145.4 162.5   06:11:18.70 -21:37:00.27   860.075     32.39     23.49      5 j060033 - 285911   89.7   35.2 202.1   06:00:33.77 -28:59:11.64 1386.318     27.91     28.12      6 j061706 - 272350   34.7   58.3 227.8   06:17:06.48 -27:23:50.26 1728.877     24.89     27.30      7 j055848 - 252527   95.8   88.6 231.8   05:58:48.61 -25:25:27.83 1782.146     31.86     24.19      8 j061522 - 263426   40.2   70.8 232.6   06:15:22.77 -26:34:26.29 1792.992     16.53     19.59      9 j060053 - 214235   88.9 144.3 232.7   06:00:53.49 -21:42:35.96 1794.159     35.95     28.16     10 j060443 - 260638   75.8   78.3 233.4   06:04:43.20 -26:06:38.58 1803.162     24.13     23.87     11 j060107 - 234004   88.0 115.0 235.5   06:01:07.33 -23:40:04.33 1831.452     35.95     36.09     12 j061213 - 214901   49.4 142.3 271.3   06:12:13.72 -21:49:01.21 2309.807     28.48     27.52     13 j061617 - 213333   35.2 145.9 298.4   06:16:17.30 -21:33:33.92 2673.464     20.21      7.47     14 j060838 - 223918   62.0 130.0 724.9   06:08:38.82 -22:39:18.06 8510.966      4.05      3.96     15 j055212 - 291706 117.0   30.5 727.0   05:52:12.72 -29:17:06.49 8539.914     15.50     24.34    -------------------------------------------------------------------------------------------      w_50     w_20    w_vel      f_int      f_tot     f_peak s / nmax   x1   x2   y1   y2   z1   z2   npix    [ km / s ]   [ km / s ]   [ km / s ] [ jy km / s ] [ jy / beam ] [ jy / beam ]                                 [ pix ] -------------------------------------------------------------------------------------------    26.003   64.416   52.851     13.932     15.486      0.213   19.65   52   67 135 145 114 118    211    24.620   59.902   66.084      3.544      3.938      0.118   10.90   78   87   80   86 117 122     67    31.622   46.235 105.758     13.622     15.133      0.150   13.83   65   82   53   65 118 126    248    87.170 110.181 106.141     30.126     33.345      0.410   37.94   49   56 142 147 159 167    266   178.136 196.548 173.083     17.993     19.846      0.173   15.98   87   93   32   38 196 209    295   307.095 327.361 293.519      7.198      7.921      0.093    8.58   32   37   55   61 215 237    146   237.471 274.205 240.237     10.753     11.830      0.115   10.67   92   99   86   91 221 239    215    70.682   90.968   53.400      1.671      1.838      0.068    6.32   39   42   69   73 231 235     35   103.364 227.932 226.900     21.574     23.731      0.166   15.30   85   93 141 147 222 239    336   215.186 232.314 213.600     17.760     19.536      0.155   14.28   73   78   76   81 225 241    305   201.838 254.227 267.072     60.536     66.574      0.297   27.44   84   92 111 119 226 246    742   409.395 530.057 549.514     13.523     14.825      0.101    9.31   46   52 140 146 257 298    270   142.639 249.637 214.911      2.879      3.148      0.127   11.74   33   37 145 146 294 310     41   141.561 249.361 125.554      0.497",
    "0.523      0.081    7.51   62   62 130 130 720 729      8   221.143 304.807 362.794     26.690     28.089      0.479   44.26 116 119   28   33 714 740    135    -----------------------------------------------------------------   flag   x_av   y_av   z_av x_cent y_cent z_cent x_peak y_peak z_peak                                                                    -----------------------------------------------------------------      -   59.7 140.5 114.7    59.4   140.5   114.6      59     140     114      -   83.4   83.2 118.3    83.4    83.3   118.3      84      83     118      -   71.5   59.8 121.3    71.4    59.8   121.3      72      61     121      e   52.5 145.1 162.5    52.5   145.4   162.5      53     146     164      -   89.7   35.2 202.2    89.7    35.2   202.1      90      36     197      -   34.7   58.3 227.8    34.7    58.3   227.8      35      58     236      -   95.8   88.6 231.5    95.8    88.6   231.8      96      89     237      -   40.1   70.8 232.6    40.2    70.8   232.6      40      71     231      e   88.8 144.2 232.3    88.9   144.3   232.7      89     144     233      -   75.9   78.3 233.4    75.8    78.3   233.4      76      78     240      -   88.0 114.9 235.7    88.0   115.0   235.5      88     115     231      -   49.4 142.3 271.8    49.4   142.3   271.3      50     142     259      e   35.3 145.8 298.7    35.2   145.9   298.4      34     146     297      -   62.0 130.0 724.8    62.0   130.0   724.9      62     130     723      - 117.0   30.3 726.7   117.0    30.5   727.0     117      30     733 ...."
  ],
  "abstract_text": [
    "<S> this paper describes the duchampsource finder , a piece of software designed to find and describe sources in 3-dimensional , spectral - line data cubes . </S>",
    "<S> duchamphas been developed with hi(neutral hydrogen ) observations in mind , but is widely applicable to many types of astronomical images . </S>",
    "<S> it features efficient source detection and handling methods , noise suppression via smoothing or multi - resolution wavelet reconstruction , and a range of graphical and text - based outputs to allow the user to understand the detections . </S>",
    "<S> this paper details some of the key algorithms used , and illustrates the effectiveness of the finder on different data sets .    </S>",
    "<S> [ firstpage ]    methods : data analysis  techniques : image processing  surveys </S>"
  ]
}