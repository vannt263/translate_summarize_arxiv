{
  "article_text": [
    "research in deep learning using neural networks has increased significantly over the last years .",
    "this occurred due to the ability of deep neural networks to achieve higher performance when compared to other methods on problems with a large amount of data @xcite and advances in computing power , such as the use of graphic processing units ( gpus ) @xcite .    despite the advances obtained by using gpus for training deep neural networks ,",
    "this step still can take a lot of time , which affects negatively both research and industry as new methods take longer to be tested and deployed .",
    "some researches have focused on speeding up deep neural networks in general , including proposals based on hardware , such as using limited numerical precision @xcite , which could increase the number of computing units on the hardware , and software , such as using fourier transform to compute a convolution @xcite .",
    "these and other methods optimized for computation of neural networks on gpus lead to the development of domain - specific libraries , such as cudnn @xcite . in this paper",
    ", we focus on existing research interested in decreasing the training time , as these approaches are closer to the proposed method . however , we highlight that these improvements are not mutually exclusive and can be used together .",
    "@xcite proposed a mixture of data and model parallelism over gpus in a single machine based on the type of the layer , exploiting their particularities for increased speed .",
    "essentially , the convolutional layers exploit data parallelism , since they are the most computing intensive part of the neural network , and the fully - connected layers exploit model parallelism , since they have most of the parameters and may not fit in a single gpu .",
    "this leads to a significant speedup in comparison to other existing methods for training convolutional neural networks over a gpu cluster .",
    "distbelief @xcite is another framework to speedup the training of large neural network by exploiting parallelism , but it focuses on clusters of computers .",
    "the data parallelism is exploited by dividing the data set in shards that are fed to different groups of computers , where each group replicates the full model and are synchronized through a parameter server .",
    "for the model parallelism , a locally connected neural network is used in order to reduce the communication among machines that jointly represent one replica of the model .",
    "since the model is locally connected , only the edges that cross the machine boundary need to be transmitted .",
    "this framework is extended in @xcite to use gpus as computing units .",
    "these methods of model parallelization to handle large neural networks require communication between the multiple computing units , which usually is slower than the computation and characterizes an overhead in the learning process .",
    "other methods of parallelization , such as computing the branches of a model like googlenet @xcite in parallel , also require communication among the units .",
    "these communications are proportional to the size of the network , hence presenting less overhead for smaller models .",
    "moreover , even if a single gpu can hold the full model and the model is dense and occupies all the computational resources , all computations of a layer may not be performed in parallel , leading to an increase in test and training time .",
    "for instance , vggnet @xcite is a very dense model and may be able to keep the gpu completely busy . but consider the two fully - connected layers with 4096 in this network .",
    "if the gpu has 4096 cores and we consider only the time for multiplications , a total of 4096 gpu cycles will be required per sample , while only 1024 cycles are required for a model with half as many neurons . so smaller versions of dense models are beneficial even if we discard the associated overheads .",
    "therefore , using smaller models can speedup the task , but smaller models may not be able to achieve the same performance attainable by larger models . however , the learning process does not have to adjust the full model in all iterations .",
    "pre - training methods exploit this characteristic by providing adjusted , instead of fully random , initial conditions for the training to continue with the full model .",
    "a well known example of this is unsupervised pre - training @xcite , where the network is greedly trained layer - wise to reconstruct its input . since only the parameters of the current layer have to be learned , this pre - training requires less memory and processing time to fit the model .",
    "after the initial iterations of pre - training , the full model is built stacking the layers on top of each other and the training continues , performing a fine - tuning of the parameters found .",
    "however , two important limitations of unsupervised pre - training are that it requires a generative model of the data to be known , which can be different from the desired task , and that the layers must be learnt sequentially .    in this paper",
    ", we introduce a new method of partitioning the network to perform a pre - training .",
    "instead of partitioning layer - wise , the proposed method partitions the neural network in smaller neural networks whose tasks are equivalent to the original training task , avoiding the need to create substitute tasks such as generative costs , for pre - training . moreover , the subtasks are independent of each other , which allows them to be learnt in parallel without communication .",
    "another advantage of the proposed method is that , since the task is kept the same , the learning algorithm used to adjust the parameters can also be the same for all stages .",
    "therefore , it can be viewed as a higher level method and is compatible with existing training strategies .",
    "after the proposed pre - training is complete , the obtained smaller neural networks are merged and used as initial condition for the original neural network .",
    "the new method is also straightforward to implement and decreases the number of parameters of the subtasks quadratically in the number of subtasks created , thus being characterized as a highly scalable approach .",
    "we perform two experiments , one with mnist and another with cifar10 data sets , to show that the training time required is indeed reduced while the generalization capability may not be affected by the new method .",
    "furthermore , the experiments also show that the method can be used to speedup the training even when the subtasks are performed sequentially , that larger models have higher speedups , and that the sub - models learn different representations for the data , which is advantageous for the performance when merging these sub - models .",
    "recently , an approach called net2net @xcite , which is similar to the one proposed in this paper , was made published .",
    "both approaches were developed in parallel by different research groups and focus on speeding - up the training of neural networks while maintaining the same performance level achievable when training the full model from a random initialization .",
    "however , net2net requires the existence of a pre - trained neural network and focuses on training a larger model faster than from a random initialization by using the existing network as initial condition , while our method does not require any pre - trained network , as the pre - training is part of the method .",
    "therefore , the speed - up analysis we perform for our method includes both the pre - training and the regular training , while the analysis presented by @xcite does not include the training time of the teacher network .",
    "in fact , it is possible that the total training time of the teacher network plus the expanded network is higher than the expanded network directly . we compare our method to net2net in the cifar10 experiment .",
    "this paper is organized as follows .",
    "section  [ sec : partition ] provides the motivation , description and main advantages of the method proposed .",
    "section  [ sec : experiment ] describes the experiments performed to test the new method and discusses the obtained results .",
    "finally , section  [ sec : conclusion ] provides a summary of the findings and future research directions .",
    "this section is divided in three parts .",
    "section  [ sec : partition : motivation ] further clarifies the problem which is being solved and how the solution is related to existing methods in the literature .",
    "section  [ sec : partition : method ] describes the method itself , and section  [ sec : partition : analysis ] analyzes the possible benefits achievable by the method , being some of them confirmed in the experiments performed .",
    "large neural networks are able to achieve better performance than smaller ones , but are considerably more expensive to learn and use .",
    "they may require special methods for training if they do not fit in a single computing unit @xcite and can be used after trained to provide guidance to improve smaller networks @xcite .",
    "therefore , even if a large neural network will not be used for the desired task due to its high computational cost , training large networks is still important for guiding the improvement of smaller ones .",
    "another common solution to improve performance achievable by smaller neural networks is through ensembles @xcite , where predictions of multiple neural networks are combined to provide more accurate outputs .",
    "it is important to highlight that ensembles only work because the multiple models provide diverse predictions for the same data @xcite .",
    "therefore , the improvement in performance relies on the distinct behavior of the predictors .    since the performance achieved by a neural network may depend on its initialization ,",
    "there has been a search for good initialization methods @xcite .",
    "nonetheless , neural networks seem to be able to achieve good and diverse local minima or saddle points @xcite , so they can easily be used as components of ensembles to improve performance . in the experiment with the mnist data set",
    ", we will show that the sub - models in fact achieve diverse performance even when they are small , which indicates that they learn non - redundant features and provides variety in the features provided as initial condition for the merged model .",
    "but if we view an ensemble of trained neural networks as a single large neural network with a constraint of no connection between layers of different base networks , it raises the question of whether it is possible to use smaller networks to properly initialize larger ones . in the initial learning phase ,",
    "isolated sets of parameters are going to be adjusted independently , thus avoiding the necessity of using methods devoted to train the full network with all adjustable parameters .",
    "consider a neural network described by a directed graph from the input to the output that performs some computation as information flows through the graph .",
    "the case of undirected neural networks will be discussed later , and it will be shown that they do not modify the algorithm very much .",
    "the proposed partitioning method first divides the neurons in the large neural network in disjoint sets , except for neurons in the inputs and outputs layers , which must be present in all sets . in this partition",
    ", one filter of a convolutional layer corresponds to an atomic unit , since all the computed activations share the same parameters , and any layer that has internal parameters or whose activation depends on the individual input values instead of their aggregate , such as normalization layers @xcite , must be replaced by multiple similar , parallel layers .    for each set of neurons , the only sources and sinks in the vertex - induced subgraph must be the input and output neurons of the full neural network , respectively .",
    "therefore , the following holds : 1 ) each subset defines a complete flow of information from the input to the output ; 2 ) each vertex - induced subgraph defines a valid smaller neural network ; 3 ) the original cost function can be learnt on each subgraph ; 4 ) every neuron of the full neural network is allocated to exactly one of the smaller networks , except for the input and output neurons ; and 5 ) every parameter is allocated to at most one smaller network , except for the parameters of the output neurons .",
    "\\(x ) @xmath0 ;    ( n111 ) [ above right=0.45 cm and 1.5 cm of x ] ; ( n112 ) [ above right=0.3 cm and 1.5 cm of x ] ; ( l11 ) ;    ( n121 ) [ below right=0.45 cm and 1.5 cm of x ] ; ( n122 ) [ below right=0.3 cm and 1.5 cm of x ] ; ( l12 ) ;    ( n211 ) [ right=2 cm of n111 ] ; ( n212 ) [ right=2 cm of n112 ] ; ( l21 ) ;    ( n221 ) [ right=2 cm of n121 ] ; ( n222 ) [ right=2 cm of n122 ] ; ( l22 ) ;    \\(y ) [ below right=0.3 cm and 1.5 cm of n212 ] @xmath1 ;    \\(x ) edge [ left ] node [ above left=0 cm and -0.3 cm ] @xmath2 ( l11 ) ; ( x ) edge [ left ] node [ below left=0 cm and -0.3 cm ] @xmath3 ( l12 ) ; ( l11 ) edge node [ above ] @xmath4 ( l21 ) ; ( l12 ) edge node [ below ] @xmath5 ( l22 ) ; ( l11 ) edge[dashed ] node [ below right=-0.1 cm and 0.4 cm ] @xmath6 ( l22 ) ; ( l12 ) edge[dashed ] node [ below left=-0.1 cm and 0.4 cm ] @xmath7 ( l21 ) ; ( l21 ) edge [ right ] node [ above right=0 cm and -0.3 cm ] @xmath8 ( y ) ; ( l22 ) edge [ right ] node [ below right=0 cm and -0.3 cm ] @xmath9 ( y ) ;    ( x1p ) [ above right=0.2 cm and 1.5 cm of y ] @xmath0 ; ( n111p ) [ above right=-0.14 cm and 1.5 cm of x1p ] ; ( n112p ) [ below right=-0.14 cm and 1.5 cm of x1p ] ; ( l11p ) ; ( n211p ) [ right=1.5 cm of n111p ] ; ( n212p ) [ right=1.5 cm of n112p ] ; ( l21p ) ; ( y1p ) [ below right=-0.13 cm and 1.5 cm of n211p ] @xmath1 ; ( x1p ) edge node [ above ] @xmath10 ( l11p ) ; ( l11p ) edge node [ above ] @xmath4 ( l21p ) ; ( l21p ) edge node [ above ] @xmath11 ( y1p ) ;    ( x2p ) [ below right=0.4 cm and 1.5 cm of y ] @xmath0 ; ( n121p ) [ above right=-0.14 cm and 1.5 cm of x2p ] ; ( n122p ) [ below right=-0.14 cm and 1.5 cm of x2p ] ; ( l12p ) ; ( n221p ) [ right=1.5 cm of n121p ] ; ( n222p ) [ right=1.5 cm of n122p ] ; ( l22p ) ; ( y2p ) [ below right=-0.13 cm and 1.5 cm of n221p ] @xmath1 ; ( x2p ) edge node [ above ] @xmath12 ( l12p ) ; ( l12p ) edge node [ above ] @xmath5 ( l22p ) ; ( l22p ) edge node [ above ] @xmath13 ( y2p ) ;    as an example , consider the neural network on the left of figure  [ fig : split ] , which will be separated into two sub - models .",
    "the neurons are grouped in the sets @xmath14 and @xmath15 , which satisfy the conditions imposed before . when defining the vertex - induced subgraphs , which are shown on the right of figure  [ fig : split ] , the connections between @xmath16 and @xmath17 and between @xmath18 and @xmath19 are not allocated to any subgraph .",
    "the extension for larger number of sub - models is straightforward .",
    "since the subgraphs are neural networks by themselves and have the same input and output as the original neural network , the next step in the method is to train these smaller models independently in the original task .",
    "moreover , since there is no communication among these networks , they can be trained in parallel .",
    "once done , almost all the weights and biases can be copied from the sub - models to the original model directly , except for the output parameters , while setting to zero all parameters associated with edges that were not trained .",
    "as each sub - model learns to predict the output by themselves , simply copying the weights would directly sum the contributions of each sub - model , besides each one providing different values for the biases . while the sum of contributions would not be a problem for classification problems , since the softmax is scale - independent ,",
    "it is a problem for regressions tasks .",
    "therefore , to normalize the contributions , each weight to the output is divided by the number k of subtasks created and the output bias is given by the mean of the learnt biases of all k sub - models , that is , @xmath20 where @xmath21 are the output biases learned by each sub - model .",
    "this change on the parameters is equivalent to computing the mean of the sub - models on linear outputs or the geometric mean on softmax outputs , which are the common methods to compute an ensemble prediction .",
    "therefore , the full network has a behavior similar to the ensemble of the sub - models right after merging but it is able to learn better parameters as the training continues , as it is composed of a full network and has more connections than would be present in the ensemble .",
    "an alternative to changing the weights to the output layer , which would be necessary in the case of undirected neural networks , since changing the weights towards the output affect the activation of the hidden units from the input , is to sum the biases and change the input / output activation function . instead of scaling the weights and biases for the output layer , we just have to scale the input of the functions that describe how the data is generated , as higher activations will occur .",
    "since the internal parts of the neural network were trained independently , they can be connected as described before .",
    "once the parameters of the full model are adjusted based on those from the sub - models , the fine - tunning can occur by training the full pre - trained model .      from the description of the partitioning method",
    ", it is clear that each sub - model of the network will have less parameters to train .",
    "after the partition , there are three possibilities for changing the number of parameters : 1 ) keep the same number as the original network , which happens to the output biases ; 2 ) reduce linearly , which happens to all other biases and input and output weights , since the number of neurons reduces linearly and the input and output are copied ; and 3 ) reduce quadratically , which happens to the weights between internal neurons .",
    "since most of the parameters in deep neural networks are concentrated in connections between internal layers , we can expect an almost quadratic reduction in the number of parameters in relation to the number of partitions .",
    "this indicates that even a distributed , large neural network can quickly become small enough to fit in a single machine , which completely eliminates the communication cost between nodes during the initial training phase .",
    "moreover , it is possible to further partition a network that is already small enough to fit a single machine , which reduces the number of operations required for the training and may further speedup the process .",
    "this not only decreases the communication overhead but can also reduce the number of machines required during the initial phase of the training .",
    "consider , for instance , a large neural network that requires four machines connected to be represented . due to the space overhead for loading values from other computers , it might be possible that partitioning the network in two sub - models , each with about a quarter the original size , allows one of the reduced models to fit in a single machine .",
    "so , besides not requiring communication between the original machines to train the reduced neural networks , only two of the four original machines are required for this training step , one for each sub - model of the full model , supposing that these sub - models are going to be trained in parallel .",
    "furthermore , we conjecture that the pre - training should not affect the final performance of the neural network if the partitions have different initial conditions and are flexible enough . as discussed in section  [ sec : partition : motivation ] , the improved performance achieved by an ensemble relies on each of its models producing diverse predictions .",
    "moreover , these diverse predictions may be achieved by distinct representations of the data in the internal nodes of the neural network .    in this case , after merging the sub - models , new lower - level features for the data are available to each neuron that was learned in one of the sub - models , which allows them to exploit these new features to provide better outputs . since the new connections have zero value , the network does not have to revert a bad initial value for the parameters and can directly take advantage of the promising representations .",
    "moreover , since the representations were also learned from the data , they should provide useful features and the performance should not be affected by the smaller parametrization during the pre - training .",
    "a geforce gtx 760 was used in these experiments and they were conducted on neural networks small enough to fit the memory of the gpu , including the parameters , intermediary values and data batch , but large enough to prevent all the layer - wise processing to be done completely in parallel .",
    "this avoids overheads due to communication between nodes that may be specific to the method used to perform model parallelism , while allowing the proposed method to show its improvements .",
    "we present two experiments to evaluate the method proposed in this paper .",
    "the first uses a small network to classify digits on mnist @xcite and focuses on analysing the effects of the number of partitions created on the training time and performance .",
    "the second uses a larger network to classify the images on cifar10 @xcite and considers different number of epochs for the pre - training .      for this task",
    ", we used a neural network similar to lenet @xcite , which was composed of three layers , all with relu activation .",
    "the first two layers are convolutions with 20 and 50 filters , respectively , of size 5x5 , both followed by max - pooling of size 2x2 , while the last layer is fully connected and composed of 500 hidden units with dropout probability of 0.5 .",
    "the learning was performed by gradient descent with learning rate of 0.1 and momentum of 0.9 , which were selected using the validation set to provide the best performance for the full model without pre - training , and minibatches of 500 samples .",
    "when the parameters of the sub - models are copied to the full neural network , the accumulated momenta of each training task are also copied to the respective sub - set of the full network and adjusted like the associated parameters .",
    "hence , the accumulated momenta for the weights and biases for the output layer are computed using eq .   and the other accumulated momenta are copied directly .",
    "the neural network was partitioned in 2 , 5 and 10 sub - models with the same number of neurons in each layer , besides the baseline of 1 sub - model , which is the standard neural network model .",
    "each sub - model was trained for 100 epochs on the full data set , followed by another 100 epochs on the full model with the merged parameters .",
    "the performance of each sub - model was averaged to obtain the results during pre - training , as they are considered independent models and do not belong to an ensemble .",
    "we also compared the performance of merging the sub - models before any pre - training , which is equivalent to setting the initial values for the inter - model weights to zero .",
    "however , since this approach achieved slightly worse results than the baseline , the results are not shown .",
    "tests with different numbers of sub - models used the same initialization for the weights , avoiding fluctuation of the results due to random number generation .",
    "a total of 50 runs with different initialization were performed and the averages of the runs are reported .",
    ".comparison between networks with different numbers of sub - models against the baseline given by k = 1 .",
    "the speedup is measured over one iteration of one sub - model . [ cols=\"^,^,^\",options=\"header \" , ]     table  [ tab : results : cifar ] shows the results for each one of the models considered .",
    "note that duplicating the normalization layers and dividing inputs among them , named `` duplicated '' in the table , decreases the training time because there are less inputs being considered at each normalization unit , reducing the time required to compute the normalized data .    for merging epoch up to 400",
    ", there is no noticeable trend in the performance of the classifier , with fluctuations occurring due to the stochastic optimization . on the other hand",
    ", the new pre - training method reduced the training time significantly even when serial training of the partitions is considered , with higher improvements assuming parallel training .",
    "since the training schedule performs 520 iterations , the higher error for the last two merges can be explained by the optimization algorithm not having enough time to adjust the new weights created and initially set to zero by the merge .    each training iteration over",
    "the merged model takes about 3.3 seconds while iterations over one partition takes 1 second , characterizing a speedup of 3.3 times .",
    "when compared against the speedup of 2.1 times obtained on the mnist network , which was about 5 times smaller , this provides evidence for the conjecture in section  [ sec : experiment : mnist ] that larger networks benefit more from the pre - training .",
    "therefore , even larger neural networks should have higher improvements on their training time , since more computations are saved and less computing power is idle during the training of the sub - models .",
    "table  [ tab : results : cifar ] also show the results obtained by applying net2widernet @xcite to one of the sub - models to double the size of each layer , so that the expanded network has the same topology as the network after the partition , but with different parameters . instead of the random expansion presented by @xcite",
    ", we duplicated each layer completely as the new size is multiple of the original one , and we added noise to the duplicated parameters to break symmetry , as described by @xcite . since only one model is being trained , only the second ( higher ) speedup applies . from the results obtained when limiting the number of iterations of the base and expanded models ,",
    "it is clear that net2widernet presents a smaller speedup than our partitioning method for the same level of error .",
    "namely , net2widernet is able to perform only 100 iterations on the base model , which corresponds to a speedup of 1.14 times , before the mean error gets higher than 1190 , while our method is able to perform 400 iterations on the two sub - models before achieving the same error level , which corresponds to a speedup of 1.45 times if we consider that the models are trained serially .",
    "we must highlight that , despite these results , net2net may be faster when training a new model from an existing , pre - trained one , specially if the expansion is not as large as the one pursued here , and allows changes to the model that would require a training from scratch using our method , like increasing the number of neurons in a single layer , since it is not possible to create a complete sub - model from input to output to append to the existing network .",
    "therefore , a combination of our partitioning method , to quickly get to a large base model , and net2net methods , to experiment with changes in this base model , may be the best choice and should be investigated in the future .",
    "in this paper , we introduced a method for pre - training a neural network by partitioning it into smaller neural networks and training them on the original learning task .",
    "the size of the sub - models reduces almost quadratically with the number of sub - models created , which allows larger neural networks to save more computational resources during pre - training .    by design ,",
    "the method decreases the training time by creating training subtasks that can be parallelized and may reduce the communication overhead present in model parallelism .",
    "it may also decrease the number of computing units required during the pre - training due to the quadratic reduction on the number of parameters being learned .",
    "two experiments , on mnist and on cifar10 , with neural networks that fit in a single gpu , confirmed that the training time of the full model can be decreased without affecting the performance .",
    "the experiments also show that the proposed method may be able to improve training speed even if the training of the sub - models is performed serially and that larger models may experience higher speedups , with a speedup on the pre - training iterations of 2.1 for a 3-layer model with 430k parameters and a speedup of 3.3 for a 4-layer model with 2.1 m parameters when creating 2 sub - models .",
    "since the proposed method relies on the different sub - models finding diverse representations for the data that can be exploited once they are merged , it is plausible to worry about the size of the sub - models created , as smaller models have less flexibility and might learn similar functions .",
    "however , the mnist experiment shows that indeed the sub - models learn diverse representations for the data , with the pairwise diversity being higher when we increase the number of sub - models .",
    "future research should experiment with deep neural networks using both data and model parallelism to evaluate the gains obtained by reducing the models .",
    "the time spent training the neural network can be decomposed mainly in three parts , all of which may be affected by the proposed method : 1 ) the time to compute the gradients and adjust the parameters , which we showed that can achieve large improvement ; 2 ) the time to communicate updates between units with different parts of the data , which is proportional to the number of parameters and should get an almost quadratic speedup ; and 3 ) the time to communicate activations between computers when using model parallelism , which can be decreased or even avoided by using smaller models . these reductions in training time and the possibility of reducing the number of computers required during pre - training , as discussed in section  [ sec : partition : analysis ] , make the proposed method appealing to handle large models .",
    "another direction is the evaluation of a hierarchical training method , in which the neural network is decomposed recursively and the pre - training is performed at each level , such that the proposed method characterizes one level of recursion .",
    "this could provide additional benefits to very large models by allowing smaller sub - models to be trained without merging all of them at the same time ."
  ],
  "abstract_text": [
    "<S> this paper presents a new method for pre - training neural networks that can decrease the total training time for a neural network while maintaining the final performance , which motivates its use on deep neural networks . by partitioning the training task in multiple training subtasks with sub - models , which can be performed independently and in parallel </S>",
    "<S> , it is shown that the size of the sub - models reduces almost quadratically with the number of subtasks created , quickly scaling down the sub - models used for the pre - training . </S>",
    "<S> the sub - models are then merged to provide a pre - trained initial set of weights for the original model . </S>",
    "<S> the proposed method is independent of the other aspects of the training , such as architecture of the neural network , training method , and objective , making it compatible with a wide range of existing approaches . </S>",
    "<S> the speedup without loss of performance is validated experimentally on mnist and on cifar10 data sets , also showing that even performing the subtasks sequentially can decrease the training time . </S>",
    "<S> moreover , we show that larger models may present higher speedups and conjecture about the benefits of the method in distributed learning systems . </S>"
  ]
}