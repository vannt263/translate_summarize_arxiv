{
  "article_text": [
    "convolutional neural networks ( cnns ) show state - of - the - art performance on many problems in computer vision , natural language processing and other fields  @xcite . at the same time , cnns require millions of floating point operations to process an image and therefore real - time applications need powerful cpu or gpu devices .",
    "moreover , these networks contain millions of trainable parameters and consume hundreds of megabytes of storage and memory bandwidth  @xcite .",
    "thus , cnns are forced to use ram instead of solely relying on the processor cache  orders of magnitude more energy efficient memory device  @xcite  which increases the energy consumption even more .",
    "these reasons restrain the spread of cnns on mobile devices .    to address the storage and memory requirements of neural networks ,  @xcite used tensor decomposition techniques to compress fully - connected layers .",
    "they represented the parameters of the layers in the tensor train format  @xcite and learned the network from scratch in this representation .",
    "this approach provided enough compression to move the storage bottleneck of vgg-16  @xcite from the fully - connected layers to convolutional layers . for a more detailed literature overview ,",
    "see sec .",
    "[ sec : related - works ] .    in this paper",
    ", we propose a tensor factorization based method to compress convolutional layers .",
    "our contributions are :    * we experimentally show that applying the tensor train decomposition  the compression technique used in  @xcite  directly to the tensor of a convolution yields poor results ( see sec .",
    "[ sec : experiments ] ) .",
    "we explain this behavior and propose a way to reshape the 4-dimensional kernel of a convolution into a multidimensional tensor to fully utilize the compression power of the tensor train decomposition ( see sec .",
    "[ sec : tt - conv ] ) .",
    "* we experimentally show that the proposed approach allows compressing a network that consists only of convolutions up to @xmath2 times with @xmath3 accuracy decrease ( sec .",
    "[ sec : experiments ] ) .",
    "* we combine the proposed approach with the fully - connected layers compression of  @xcite . compressing both convolutional and fully - connected layers of a network yields @xmath4 network compression with @xmath5 accuracy drop ,",
    "see sec .",
    "[ sec : experiments ] .",
    "a convolutional network is a type of feed - forward architecture that transforms an input image to the final class scores using a sequence of layers .",
    "the main building block of such networks is a convolutional layer , that transforms the @xmath6-dimensional input tensor @xmath7 into the output tensor @xmath8 by _ convolving _ @xmath9 with the kernel tensor @xmath10 : @xmath11    to improve the computational performance , many deep learning frameworks reduce the convolution   to a matrix - by - matrix multiplication  @xcite ( see fig .",
    "[ fig : conv - to - mat ] ) .",
    "we exploit this matrix formulation to motivate a particular way of applying the tensor train format to the convolutional kernel ( see sec .",
    "[ sec : tt - conv ] ) . in the rest of this section",
    ", we introduce the notation needed to reformulate convolution   as a matrix - by - matrix multiplication  @xmath12 .    ,",
    "scaledwidth=70.0% ]    for convenience , we denote @xmath13 and @xmath14 .",
    "let us reshape the output tensor @xmath15 into a matrix @xmath16 of size @xmath17 in the following way @xmath18    let us introduce a matrix @xmath19 of size @xmath20 , the @xmath21-th row of which corresponds to the @xmath22 patch of the input tensor that is used to compute the @xmath21-th row of the matrix @xmath16 @xmath23 where @xmath24 , @xmath25 , @xmath26 . finally , we reshape the kernel tensor @xmath27 into a matrix @xmath28 of size @xmath29 @xmath30 using the matrices defined above",
    ", we can rewrite the convolution definition   as @xmath12 .",
    "note that the compression approach presented in the rest of the paper works with other types of convolutions , such as convolutions with padding , stride larger than @xmath31 , or rectangular filters .",
    "but for clarity , we illustrate the proposed idea on the basic convolution  .",
    "the tt - decomposition ( or tt - representation ) of a tensor @xmath32 is the set of matrices @xmath33",
    "\\in \\mathbb{r}^{r_{k-1}\\times r_{k}},$ ] where @xmath34 and @xmath35 , such that each of the tensor elements can be represented as @xmath36{\\mathbold{g}}_2[j_2]\\ldots{\\mathbold{g}}_d[j_d].\\ ] ] the elements of the collection @xmath37 are called _ tt - ranks_. the collections of matrices @xmath38\\}_{j_k = 1}^{n_k}\\}_{k = 1}^d$ ] are called _ tt - cores _ @xcite .",
    "the tt - format requires @xmath39 parameters to represent a tensor @xmath32 which has @xmath40 elements .",
    "the tt - ranks @xmath41 control the trade - off between the number of parameters versus the accuracy of the representation : the smaller the tt - ranks , the more memory efficient the tt - format is .",
    "an attractive property of the tt - format is the ability to efficiently perform basic linear algebra operations on tensors by working on the tt - cores of the tt - format , i.e. without materializing the tensor itself  @xcite .    for a matrix  a @xmath42-dimensional tensor  the tt - decomposition coincides with the matrix low - rank decomposition",
    "to represent a matrix more compactly than in the low - rank format , the matrix tt - format is defined in a special way .",
    "let us consider a matrix @xmath43 of size @xmath44 where @xmath45 , and reshape it into a tensor @xmath46 of size @xmath47 by defining bijective mappings @xmath48 and @xmath49 .",
    "the mapping @xmath50 maps row index @xmath51 into a @xmath52-dimensional vector index , where @xmath21-th dimension @xmath53 varies from @xmath31 to @xmath54 .",
    "the bijection @xmath55 maps column index @xmath56 into a @xmath52-dimensional vector index , where @xmath21-th dimension @xmath57 varies from @xmath31 to @xmath58 .",
    "thus , using these mappings , we can form the tensor @xmath46 , whose @xmath21-th dimension is indexed by the compound index @xmath59 and consider its tt - representation : @xmath60\\ldots{\\mathbold{g}}_d[(\\mu_d(\\ell),\\nu_d(t))].\\ ] ]",
    "in this section , we propose two ways to represent a convolutional kernel @xmath27 in the tt - format .",
    "one way is to apply the tt - decomposition to the tensor @xmath27 directly . to see the drawbacks of this approach ,",
    "consider a @xmath61 convolution , which is a small fully - connected layer applied to the channels of the input image in each pixel location .",
    "the kernel of such convolution is essentially a @xmath42-dimensional array , and the tt - decomposition of @xmath42-dimensional arrays coincides with the matrix low - rank format .",
    "but for fully - connected layers , the matrix tt - format proved to be more efficient than the matrix low - rank format @xcite .",
    "thus , we seek for a decomposition that would coincide with the matrix tt - format on @xmath61 convolutions .    taking into account that a convolutional layer can be formulated as a matrix - by - matrix multiplication ( see sec .  [",
    "sec : conv ] ) , we reshape the 4-dimensional kernel tensor into a matrix @xmath28 of size @xmath62 , where @xmath63 .",
    "then we apply the matrix tt - format ( see sec .",
    "[ sec : tt - format ] ) to the matrix @xmath28 , i.e. reshape it into a tensor @xmath64 and convert it into the tt - format . to reshape the matrix @xmath28 into a tensor , we assume that its dimensions factorize : @xmath65 and @xmath66 .",
    "this assumption us not restrictive since we can always add some dummy channels filled with zeros to increase the values of @xmath67 and @xmath68 .",
    "then we can define a @xmath69-dimensional tensor , where @xmath21-th dimension has the length @xmath70 for @xmath71 and @xmath72 for @xmath73 .",
    "thus we obtain the following representation of the matrix @xmath28 @xmath74{\\mathbold{g}}_1[c_1,s_1 ] \\ldots { \\mathbold{g}}_d[c_d , s_d],\\end{gathered}\\ ] ] where @xmath75 and @xmath76 .",
    "to simplify the notation , we index the @xmath77-th core @xmath78 with @xmath79 and @xmath80 : @xmath81 = \\widetilde{{\\mathbold{g}}}_0[\\ell(y-1)+x,1],$ ] where @xmath82 finally , substituting @xmath78 into  , we obtain the following decomposition of the convolution kernel @xmath27 @xmath83{\\mathbold{g}}_1[c_1,s_1] ... {\\mathbold{g}}_d[c_d , s_d].\\ ] ]    to summarize our pipeline starting from an input tensor @xmath9 ( an image ) : the tt - convolutional layer firstly reshapes the input tensor into a @xmath84-dimensional tensor @xmath85 of size @xmath86 ; then , the layer transforms the input tensor @xmath85 into the output tensor @xmath87 of size @xmath88 in the following way @xmath89 { \\mathbold{g}}_1[c_1,s_1]\\ldots { \\mathbold{g}}_d[c_d , s_d].\\end{aligned}\\ ] ]    note that for a @xmath90 convolution ( @xmath91 ) , the @xmath79 and @xmath80 indices vanish from the decomposition  . the convolutional kernel collapses into a matrix  @xmath92 and the decomposition   for this matrix coincides with the tensor train format for the fully - connected layer proposed in  @xcite .    to train a network with tt - conv layers , we treat the elements of the tt - cores as the parameters of the layer and apply stochastic gradient descent with momentum to them",
    "to compute the necessary gradients we use automatic differentiation implemented in tensorflow  @xcite .",
    "fully - connected layers of neural networks are traditionally considered as the memory bottleneck and numerous works focused on compressing these layers  @xcite .",
    "however , several state - of - the - art neural networks are either bottlenecked by convolutional layers  @xcite , or their fully - connected layers can be compressed to move the bottleneck to the convolutional layers  @xcite .",
    "this leads to a number of works focusing on compressing and speeding up the convolutional layers  @xcite .",
    "one approach to compressing a convolutional layer is based on either pruning less important weights from the convolutional kernel , or restricting possible variation of the weights ( quantization ) , or both  @xcite .",
    "our approach is compatible with the quantization technique : one can quantize the elements of the tt cores of the decomposition .",
    "some works also add huffman coding on top of other compression techniques  @xcite , which is also compatible with the proposed method .",
    "another approach is to use tensor or matrix decompositions .",
    "cp - decomposition  @xcite and kronecker product factorization  @xcite allow to speed up the inference time of convolutions and compress the network as a side effect .",
    "we evaluated the compressing strength of the proposed approach on cifar-10 dataset  @xcite , which has @xmath93 train images and @xmath94 test images .",
    "in all the experiments , we used stochastic gradient descent with momentum with coefficient  @xmath95 , trained for  @xmath96 epochs starting from the learning rate of  @xmath97 and decreased it  @xmath98 after each  @xmath99 epochs . to make the experiments reproducible",
    ", we released the codebase .",
    "we used two architectures as references : the first one is dominated by the convolutions ( they occupy @xmath100 parameters of the network ) , and the second one is dominated by the fully - connected layers ( they occupy @xmath101 parameters of the network ) .",
    "[ [ convolutional - network . ] ] convolutional network .",
    "+ + + + + + + + + + + + + + + + + + + + + +    the first network has the following architecture : conv ( @xmath102 output channels ) ; bn ; relu ; conv ( @xmath102 output channels ) ; bn ; relu ; max - pool ( @xmath103 with stride @xmath42 ) ; conv ( @xmath104 output channels ) ; bn ; relu ; conv ( @xmath104 output channels ) ; bn ; relu ; max - pool ( @xmath103 with stride @xmath42 ) ; conv ( @xmath104 output channels ) ; bn ; relu ; conv ( @xmath104 output channels ) ; avg - pool ( @xmath105 ) ; fc ( @xmath106 ) , where bn stands for batch normalization @xcite and all convolutional filters are of size @xmath107 . to compress the network we replace each convolutional layer excluding the first one ( it contains less than @xmath5 of the network parameters ) with the tt - conv layer  ( see sec .",
    "[ sec : tt - conv ] ) . for training ,",
    "we initialize the tt - cores of the tt - conv layers with random noise and train the whole network from scratch .",
    "we compare the proposed tt - convolution against the naive approach  directly applying the tt - decomposition to the @xmath108-dimensional convolutional kernel ( see sec.[sec : tt - conv ] ) .",
    "we report that on the @xmath109 compression level the proposed approach ( @xmath110 loss of accuracy ) outperforms the naive baseline ( @xmath111 loss of accuracy ) , for details see tbl .",
    "[ tbl : compression]a .    .45    .compressing the second baseline ( ` conv - fc ' ) , which is dominated by fully - connected layers . ` conv - tt - fc ' : only the fully - connected part of the network is compressed ; ` tt - conv - tt - fc ' : fully - connected and convolutional parts are compressed . [ cols=\"<,<,<\",options=\"header \" , ]     [ [ network - with - convolutions - and - fully - connected - layers . ] ] network with convolutions and fully - connected layers .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the second reference network was obtained from the first one by replacing the average pooling with two fully - connected layers of size @xmath112 and @xmath113 .    to compress the second network",
    ", we replace all layers excluding the first and the last one ( they occupy less than @xmath5 of parameters ) with tt - conv and tt - fc  @xcite layers . to speed up the convergence ,",
    "we trained the network in two stages : first we replaced only the convolutional layers with tt - conv layers and trained the network ; then we replaced the fully - connected layers with randomly initialized tt - fc layers and fine - tuned the whole model .    to compare against  @xcite we include the results of compressing only fully - connected layers ( tbl .",
    "[ tbl : compression]b ) .",
    "initially , the fully - connected part was the memory bottleneck and it was more fruitful to compress it while leaving the convolutions untouched : we obtained @xmath114 network compression with @xmath115 accuracy drop by compressing only fully - connected layers , and @xmath116 compression with @xmath117 accuracy drop by compressing both fully - connected and convolutional layers .",
    "but after the first gains , the bottleneck moved to the convolutional part and the fully - connected layers compression capped at about @xmath118 network compression . at this point , by additionally factorizing the convolutions we raised the network compression up to @xmath0 while losing @xmath1 of accuracy  ( tbl .",
    "[ tbl : compression]b ) .",
    "in this paper , we proposed a tensor decomposition approach to compressing convolutional layers of a neural network . by combing this convolutional approach with the work  @xcite for fully - connected layers , we compressed a convolutional network @xmath0 times .",
    "these results make a step towards the era of embedding compressed models into smartphones to allow them constantly look and listen to its surroundings ."
  ],
  "abstract_text": [
    "<S> convolutional neural networks excel in image recognition tasks , but this comes at the cost of high computational and memory complexity . to tackle this problem ,  </S>",
    "<S> @xcite  developed a tensor factorization framework to compress fully - connected layers . in this paper , we focus on compressing convolutional layers . we show that while the direct application of the tensor framework  @xcite to the 4-dimensional kernel of convolution does compress the layer , we can do better . we reshape the convolutional kernel into a tensor of higher order and factorize it . </S>",
    "<S> we combine the proposed approach with the previous work to compress both convolutional and fully - connected layers of a network and achieve @xmath0 network compression rate with @xmath1 accuracy drop on the cifar-10 dataset . </S>"
  ]
}