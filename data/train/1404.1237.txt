{
  "article_text": [
    "sources naturally arise in wireless sensor networks , where sensors may acquire over time several readings of the same natural quantity , _",
    "e.g. _ , temperature , in different points of the same environment .",
    "such data must be transmitted to a fusion center for further processing . however , since radio access is the most energy  consuming operation in a wireless sensor network , data transmission among sensors needs to be minimized in order to maximize sensors battery life .",
    "this calls for lossy compression techniques to find a cost  constrained representation in order to exploit data redundancies . in particular , following the example above , sensor readings may vary slowly over time , and hence consecutive readings have similar values , because of the slow variation of the underlying physical phenomenon .",
    "moreover , inter ",
    "sensor correlations also exist , as the sensors may be located in the same environment , in which the temperature is rather uniform , leading to compressibility of each single signal and of the set of signals as an ensemble .",
    "the question hence arises of how to exploit such correlations in a distributed way , _",
    "i.e. , _ without communication among the sensors , and employing a low  complexity signal representation in order to minimize energy consumption .    in this framework ,",
    "compressed sensing ( cs ) @xcite has emerged in past years as an efficient technique for sensing a signal with fewer coefficients than dictated by classic shannon / nyquist theory .",
    "the hypothesis underlying this approach is that the signal to be sensed must have a sparse  or at least compressible  representation in a convenient basis . in cs , sensing is performed by taking a number of linear projections of the signal onto pseudorandom sequences . therefore , the acquisition presents appealing properties . for example , it has low encoding complexity , since no sorting of the sparse signal coefficients is required .",
    "moreover , the choice of the sensing matrix is blind to the source distribution .    using cs as signal representation requires to cast the representation / coding problem in a rate  distortion ( rd ) framework , particularly regarding the rate necessary to encode the measurements . for single sources , this problem has been addressed by several authors . in @xcite , a rd analysis of cs reconstruction from quantized measurements",
    "was performed , when the observed signal is sparse . instead ,",
    "@xcite considered the rd behavior of strictly sparse or compressible memoryless sources in their own domain . @xcite",
    "considered the cost of encoding the random measurements for single sources .",
    "more precisely , rd analysis was performed and it was shown that adaptive encoding , taking into account the source distribution , outperforms scalar quantization of random measurements at the cost of higher computational complexity . however , in the distributed context , adaptive encoding may loose the inter - correlation between the sources since it is adapted to the distribution of each single source or even the realization of each source .",
    "on the other hand , the distributed case is more sophisticated .",
    "not only one needs to encode a source , but also to design a scheme capable of exploiting the correlation among different sources .",
    "therefore , distributed cs ( dcs ) was proposed in @xcite and further analyzed in @xcite . in those papers , an architecture for separate acquisition and joint reconstruction was defined , along with three different joint sparsity models ( which where merged into a single formulation in the latter paper ) . for each model",
    ", necessary conditions were posed on the number of measurement to be taken on each source to ensure perfect reconstruction .",
    "an analogy between dcs and slepian ",
    "wolf distributed source coding was depicted , in terms of the necessary conditions about the number of measurements , depending on the sparsity degree of sources , and the necessary conditions on encoding rate , depending on conditional and joint entropy , typical of slepian  wolf theory .",
    "moreover , it was shown that a distributed system based on cs could save up to 30% of measurements with respect to separate cs encoding / decoding of each source . on the other hand , @xcite extended cs in the acquisition part , but , like cs @xcite , was mainly concerned with the performance of perfect reconstruction , and did not consider the representation / coding problem , which is one of the main issues of a practical scheme and a critical aspect of cs .    in @xcite , we proposed a dcs scheme that takes into account the encoding cost , exploits both inter- and intra  correlations , and has low complexity .",
    "the main idea was to exploit the knowledge of side information ( si ) not only as a way to reduce the encoding rate , but also in order to improve the reconstruction quality , as is common in the wyner ",
    "ziv context @xcite .",
    "the proposed architecture applies the same gaussian random matrix to information and si sources , then quantizes and encodes the measurements with a sw source code .    in this paper , we study analytically the best achievable rd performance of any single  source and distributed cs scheme , under the constraint of high  rate quantization , providing simulation results that perfectly match the theoretical analysis . in particular",
    ", we provide the following contributions .",
    "first , we derive the asymptotic ( in the rate and in the number of measurements ) distribution of the measurement vector .",
    "even if the analysis is asymptotic , we show that the convergence to a gaussian distribution occurs with parameter values of practical interest .",
    "moreover , we provide an analytical expression of the rate gain obtained exploiting inter ",
    "source correlation at the decoder .",
    "second , we provide a closed  form expression of the average reconstruction error using the oracle receiver , improving the results existing in literature , consisting only in bounds hardly comparable to the results of numerical simulations @xcite .",
    "the proof relies on recent results on random matrix theory @xcite .",
    "third , we provide a closed  form expression of the rate gain due to joint reconstruction from the measurements of multiple sources .",
    "we compare the results obtained by theory both with the ideal oracle receiver and with a practical algorithm @xcite , showing that the penalty with respect to the ideal receiver is due to the lack of knowledge of the sparsity support in the reconstruction algorithm . despite this penalty ,",
    "the theoretically  derived rate gain matches that obtained applying distributed source coding followed by joint reconstruction to a practical reconstruction scheme .",
    "with respect to @xcite , we use information theoretic tools to provide an analytical characterization of the performance of cs and dcs , for a given number of measurements and set of system parameters .",
    "this paper is organized as follows .",
    "some background information about source coding with side information at the decoder and cs is given in section  [ sec : background ] .",
    "novel analytical results are presented in sections  [ sec : rd_cs ] and [ sec : rd_dcs ] .",
    "these results are validated via numerical simulations that are presented in section  [ sec : num_res ] . finally , concluding remarks are given in section  [ sec : conclusions ] .",
    "we denote ( column- ) vectors and matrices by lowercase and uppercase boldface characters , respectively . the @xmath0-th element of a matrix @xmath1 is @xmath2 .",
    "the @xmath3-th row of matrix @xmath1 is @xmath4 .",
    "the @xmath5-th element of column vector @xmath6 is @xmath7 .",
    "the transpose of a matrix @xmath1 is @xmath8 .",
    "the notation @xmath9 denotes the number of nonzero elements of vector @xmath6 .",
    "the notation @xmath10 denotes the @xmath11-norm of the vector @xmath6 and is defined as @xmath12  .",
    "the notation @xmath13 denotes the euclidean norm of the vector @xmath6 and is defined as @xmath14  .",
    "the notation @xmath15 means that the random variable @xmath16 is gaussian distributed , its mean is @xmath17 , and its variance is @xmath18 .",
    "additional notation will be defined throughout the paper where appropriate .",
    "source coding with si at the decoder refers to the problem of compressing a source @xmath19 when another source @xmath20 , correlated to @xmath19 , is available at the decoder only .",
    "it is a special case of distributed source coding , where the two sources have to be compressed without any cooperation at the encoder .    for lossless compression , if @xmath19 is compressed without knowledge of @xmath20 at its conditional entropy , _",
    "i.e. _ , @xmath21 , it can be recovered with vanishing error rate exploiting @xmath20 as si .",
    "this represents the asymmetric setup , where source @xmath20 is compressed in a lossless way ( @xmath22 ) or otherwise known at the decoder .",
    "therefore , the lack of si at the encoder does not incur any compression loss with respect to joint encoding , as the total rate required by dsc is equal to @xmath23 .",
    "the result holds for i.i.d .",
    "finite sources @xmath19 and @xmath20 @xcite but also for ergodic discrete sources @xcite , or when @xmath19 is i.i.d .",
    "finite , @xmath20 is i.i.d .",
    "continuous and is available at the decoder ( * ? ? ?",
    "* proposition  19 ) .    for lossy compression of i.i.d sources",
    ", @xcite shows that the lack of si at the encoder incurs a loss except for some distributions ( gaussian sources , or more generally gaussian correlation noise ) .",
    "interestingly , @xcite shows that uniform scalar quantization followed by lossless compression incurs a suboptimality of @xmath24  db , in the high  rate regime .",
    "therefore , practical solutions ( see for example @xcite ) compress and decompress the data relying on an _ inner lossless distributed codec _",
    ", usually referred to as slepian  wolf code ( swc ) , and an",
    "_ outer quantization  plus ",
    "reconstruction filter_.      in the standard cs framework , introduced in @xcite , a signal @xmath25 which has a sparse representation in some basis @xmath26 , _",
    "i.e. _ : @xmath27 can be recovered by a smaller vector of linear measurements @xmath28 , @xmath29 and @xmath30 , where @xmath31 is the _ sensing matrix_. the optimum solution , requiring at least @xmath32 measurements , would be @xmath33 since the @xmath34 norm minimization is an np - hard problem , one can resort to a linear programming reconstruction by minimizing the @xmath11 norm @xmath35 and @xmath36 , provided that @xmath37 @xcite .    when the measurements are noisy , i.e. when @xmath38 , where @xmath39 is the vector representing additive noise such that @xmath40 , @xmath11 minimization with inequality constraints is used for reconstruction : @xmath41 and @xmath36 , known as basis pursuit denoising , provided that @xmath37 and that each submatrix consisting of @xmath42 columns of @xmath43 is ( almost ) distance preserving ( * ? ? ? * definition 1.3 ) .",
    "the latter condition is the _ restricted isometry property _ ( rip ) .",
    "formally , the matrix @xmath43 satisfies the rip of order @xmath42 if @xmath44 $ ] such that , for any @xmath45 with @xmath46 : @xmath47 where @xmath48 is the rip constant of order @xmath42 .",
    "it has been shown in @xcite that when @xmath49 is an i.i.d .",
    "random matrix drawn from any subgaussian distribution and @xmath50 is an orthogonal matrix , @xmath43 satisfies the rip with overwhelming probability .",
    "in this section , we derive the best achievable performance in the rd sense over all cs schemes , under the constraint of high  rate quantization .",
    "the novel result about the distribution of the measurements derived in theorem  [ theorem : y iid gaussian ] allows to write a closed  form expression of the rd functions of the measurement vectors . in theorem",
    "[ th : rd reconstruction non distributed ] , we derive a novel closed  form expression of the average reconstruction error of the oracle receiver , which will use the results from theorem  [ theorem : y iid gaussian ] to present the rd functions of the reconstruction .",
    "[ def : sparse ] _ ( sparse vector ) . _",
    "the vector @xmath51 is said to be @xmath52-_sparse _ if @xmath53 is sparse in the domain defined by the orthogonal matrix @xmath26 , namely : @xmath54 , with @xmath55 , and if the nonzero components of @xmath56 are modeled as i.i.d . centered random variables with variance @xmath57 .",
    "@xmath58  is independent of @xmath45 .",
    "the sparse @xmath53 vector is observed through a smaller vector of gaussian measurements defined as    [ def : measurement ] _",
    "( gaussian measurement ) . _ the vector @xmath59 is called the @xmath60-_gaussian measurement _ of @xmath61 , if @xmath62 , where the sensing matrix @xmath63 , with @xmath64 , is a random matrix where the variance @xmath65 of the elements of @xmath66  depends on @xmath67 . here",
    ", we wanted to keep @xmath65 independent of system parameters . ] with i.i.d .",
    "entries drawn from @xmath68 with @xmath69 .",
    "we denote as @xmath70 the quantized version of @xmath59 . to analyze the rd tradeoff",
    ", we consider the large system regime defined below .",
    "[ def : asymptotic mode cs ] _ ( large system regime , overmeasuring and sparsity rates ) .",
    "_ let @xmath53 be @xmath52-sparse .",
    "let @xmath59 be the @xmath60-gaussian measurement of @xmath53 .",
    "the system is said to be in the _ large system regime _",
    "if @xmath71 goes to infinity , @xmath42 and @xmath67 are functions of @xmath71 and tend to infinity as @xmath71 does , under the constraint that the rates @xmath72 and @xmath73 converge to constants called _ sparsity rate _",
    "( @xmath74 ) and _ overmeasuring rate _ ( @xmath17 ) i.e. : @xmath75    the sparsity rate is a property of the signal . instead , the overmeasuring rate is the ratio of the number of measurements to the number of non zero components and is therefore a property of the system @xcite .      the _ information rd function _ of an i.i.d .",
    "source @xmath19 defines the minimum amount of information per source symbol @xmath76 needed to describe the source under the distortion constraint @xmath77 . for an i.i.d .",
    "gaussian source @xmath78 , choosing as distortion metric the squared error between the source and its representation on @xmath76 bits per symbol , the distortion satisfies @xmath79 interestingly , the _ operational rd function _ of the same gaussian source , with uniform scalar quantizer and entropy coding satisifies , in the high  rate regime : @xmath80 where @xmath81 stands for _ entropy  constrained scalar quantizer_. this leads to a @xmath24  db gap between the information and the operational rd curves .",
    "can be easily extended to other types of quantization adapting the factor @xmath82 to the specific quantization scheme .",
    "[ theorem : y iid gaussian ] _",
    "( cs : asymptotic distribution of gaussian measurements and measurement rd function ) .",
    "_ let @xmath53 be @xmath52-sparse .",
    "let @xmath59 be the @xmath60-gaussian measurement of @xmath53 , s.t .",
    "@xmath30 . consider the large system regime with finite sparsity rate @xmath83 and finite overmeasuring rate @xmath84 .",
    "+ the gaussian measurement converges in distribution to an i.i.d .",
    "gaussian , centered random sequence with variance @xmath85 therefore , the information rd function satisifies @xmath86 where @xmath76 is the encoding rate per measurement sample , and the entropy  constrained scalar quantizer achieves a distortion @xmath87 that satisfies @xmath88    * sketch of proof .",
    "* the distribution of the gaussian matrix @xmath49 is invariant under orthogonal transformation .",
    "thus , we obtain @xmath89 , where @xmath90 is an i.i.d .",
    "gaussian matrix with variance @xmath65 .",
    "then , we consider a finite length subvector @xmath91 of @xmath59 . from the multidimensional central limit theorem ( clt )",
    "* theorem 7.18 ) , @xmath91 converges to a gaussian centered vector with independent components .",
    "then , as @xmath92 , the sequence of gaussian measurements converges to an i.i.d .",
    "gaussian sequence .",
    "see appendix  [ annex : lemma y iid gaussian ] for the complete proof .",
    "@xmath93    theorem  [ theorem : y iid gaussian ] generalizes ( * ? ?",
    "* theorem 2 ) , which derives the marginal distribution of the measurements , when the observed signal is directly sparse .",
    "instead , we derive the _ joint _ distribution of the measurements and consider _ transformed _ sparse signals .",
    "we stress the fact that , even if the rd curves for measurement vectors do not have any `` practical '' direct use , they are required to derive the rd curves for the reconstruction of the sources , which can be found later in this section .",
    "we now evaluate the performance of cs reconstruction with quantized measurements .",
    "the performance depends on the amount of noise affecting the measurements .",
    "in particular , the distortion @xmath94 is upper bounded by the noise variance up to a scaling factor @xcite .",
    "@xmath95 where the constant @xmath96 depends on the realization of the measurement matrix , since it is a function of the rip constant . since we consider the average and noise , as for example in @xcite . ] performance , we need to consider the worst case @xmath96 and this upper bound will be very loose ( * ? ? ?",
    "* theorem 1.9 ) .    here , we consider the _ oracle _ estimator , which is the estimator knowing exactly the sparsity support @xmath97 of the signal @xmath53 . for the oracle estimator , upper and lower bounds depending on the rip constant can be found , for example in @xcite when the noise affecting the measurements is white and in @xcite when the noise is correlated .",
    "unlike @xcite , in this paper the average performance of the oracle , depending on system parameters only , is derived exactly .",
    "as we will show in the following sections , the characterization of the ideal oracle estimator allows to derive the reconstruction rd functions with results holding also when non ideal estimators are used .",
    "[ th : rd reconstruction non distributed ] _ ( cs : reconstruction rd functions ) .",
    "_ let @xmath53 be @xmath52-sparse .",
    "let @xmath59 be the @xmath60-gaussian measurement of @xmath53 , s.t .",
    "@xmath98 . consider the large system regime with finite sparsity rate @xmath83 and finite overmeasuring rate @xmath84 .",
    "@xmath76 denotes the encoding rate per measurement sample .",
    "+ assume reconstruction by the oracle estimator , when the support @xmath99 of @xmath53 is available at the receiver .",
    "the operational rd function of any cs reconstruction algorithm is lower bounded by that of the oracle estimator that satisfies @xmath100 similarly , the entropy - constrained rd function satisfies in the high - rate regime @xmath101    * sketch of proof . *",
    "we use a novel result about the expected value of a matrix following a generalized inverse wishart distribution ( * ? ? ?",
    "* theorem 2.1 ) .",
    "this result can be applied to the distortion of the oracle estimator for finite ",
    "length signals , depending on the expected value of the pseudo inverse of wishart matrix @xcite .",
    "the key consequence is that the distortion of the oracle only depends on the variance of the quantization noise and not on its covariance matrix .",
    "therefore , our result holds even if the noise is correlated ( for instance if vector quantization is used ) .",
    "hence , this result applies to any quantization algorithm .",
    "this result improves those in ( * ? ? ?",
    "* theorem 4.1 ) and @xcite , where upper and lower bounds depending on the rip constant of the sensing matrix are given , and it also generalizes @xcite , where a lower bound is derived whereas we derive the exact average performance .",
    "see appendix  [ annex : th : rd reconstruction non distributed ] for the complete proof .",
    "@xmath93    it must be noticed that the condition @xmath102 is not restrictive since in all cases of practical interest , @xmath103 .",
    "in this section , we derive the best achievable performance in the rd sense over all dcs schemes , under the constraint of high  rate quantization . note that @xcite ( see fig .  [ fig : bd_mq ] ) is one instance of such a scheme .",
    "novel results about the distribution of the measurements in the distributed case are presented in theorem  [ prop : rd_dcs ] .",
    "hence , theorem  [ th : rd reconstruction ] , will combine the results of theorem  [ prop : rd_dcs ] and previous section to derive the rd functions of the reconstruction in the distributed case .",
    "[ def : jsm1 ] _ ( correlated sparse vectors ) .",
    "_ + @xmath104 vectors @xmath105 are said to be @xmath106 -_sparse _ if : + * _ i ) _ * each vector @xmath107 is the sum of a _ common _ component @xmath108 shared by all signals and an _ innovation _ component @xmath109 , which is unique to each signal @xmath110 . + * _ ii ) _ * both @xmath108 and @xmath109 are sparse in the same domain defined by the orthogonal matrix @xmath26 , namely : @xmath111 and @xmath112 , with @xmath113 , @xmath114 and @xmath115 . + * _",
    "iii ) _ * the global sparsity of @xmath110 is @xmath116 , with @xmath117 .",
    "+ * _ iv ) _ * the nonzero components of @xmath118 and @xmath119 are i.i.d . centered random variables with variance @xmath120 and @xmath121 , respectively .",
    "the correlation between the sources is modeled through a _",
    "common _ component and their difference through an individual _ innovation _ component .",
    "this is a good fit for signals acquired by a group of sensors monitoring the same physical event in different spacial positions , where local factors can affect the innovation component of a more global behavior taken into account by this common component .",
    "note that the joint sparsity model-1 ( jsm-1 ) @xcite and the ensemble sparsity model ( esm ) in @xcite are deterministic models .",
    "instead , the sparse model ( def .",
    "[ def : jsm1 ] ) is probabilistic , since we look for the performance averaged over all possible realizations of the sources .",
    "focusing without loss of generality on the case @xmath122 , we assume that @xmath123 and @xmath124 are @xmath125 @xmath126-sparse .",
    "@xmath123 is the source to be compressed whereas @xmath124 serves as si . @xmath127 and @xmath128 are the @xmath60-gaussian measurements of @xmath123 and @xmath124 , and @xmath129 is the quantized version of @xmath130 .",
    "the large system regime becomes in the distributed case :    [ def : asymptotic model ] _ ( large system regime , sparsity andovermeasuring rates , and overlaps ) . _",
    "+ let the @xmath104 vectors @xmath131 @xmath132 be @xmath106 -sparse . for each @xmath133 ,",
    "let @xmath134 be the @xmath60-_gaussian measurement _ of @xmath110 .",
    "the system is said to be in the _ large system regime _ if : + * _ i ) _ * @xmath71 goes to infinity .",
    "+ * _ ii ) _ * the other dimensions @xmath135 are functions of @xmath136 and tend to infinity as @xmath71 does .",
    "+ * _ iii ) _ * the following rate converges to a constant called _",
    "sparsity rate _ as @xmath71 goes to infinity : @xmath137 * _ iv ) _ * the following rate converges to a constant called _ overmeasuring rate _ as @xmath71 goes to infinity : @xmath138 * _ v ) _ * all following rates converge to constants called _ overlaps _ of the common and innovation components as @xmath71 goes to infinity : @xmath139 note that @xmath140 .",
    "the information rd function can also be derived for a pair @xmath141 of i.i.d .",
    "jointly gaussian distributed random variables with covariance matrix @xmath142 interestingly , when the si is available at both encoder and decoder or at the decoder only , the information rd function is the same : @xmath143 where @xmath144 is the _ rate gain _",
    ", measuring the amount of rate we save by using the side information @xmath20 to decode @xmath19 .",
    "this result holds for optimal vector quantizer @xcite but also for scalar uniform quantizers ( * ? ? ?",
    "* theorem 8 and corollary 9 ) by replacing @xmath145 in by the entropy constrained distortion function @xmath146 , defined in .    to derive the rd curves for the reconstruction of the sources , we first generalize theorem  [ theorem : y iid gaussian ] and derive the asymptotic distribution of pairs of measurements .    [ prop : rd_dcs ] _ ( distributed cs : asymptotic distribution of the pair of gaussian measurements and measurement rd functions ) .",
    "_ let @xmath123 and @xmath124 be @xmath147 @xmath148-sparse .",
    "@xmath124 serves as si for @xmath123 and is available at the decoder , only .",
    "let @xmath127 and @xmath128 be the @xmath60-gaussian measurements of @xmath123 and @xmath124 .",
    "let @xmath149 be the pair of random processes associated to the random vectors @xmath150 . in the large system regime",
    ", @xmath149 converges to an i.i.d .",
    "gaussian sequence with covariance matrix @xmath151 \\label{eq : var_y1}\\\\   \\rho_{12 } & = \\left [ \\big(1 + \\frac { \\omega_{\\mathsf{i},1 } } { \\omega_{\\mathsf{c},1}}\\frac{{\\ensuremath{\\sigma^2}}_{\\theta_{\\mathsf{i},1}}}{{\\ensuremath{\\sigma^2}}_{\\theta_\\mathsf{c}}}\\big ) \\big(1 + \\frac { \\omega_{\\mathsf{i},2 } } { \\omega_{\\mathsf{c},2}}\\frac{{\\ensuremath{\\sigma^2}}_{\\theta_{\\mathsf{i},2}}}{{\\ensuremath{\\sigma^2}}_{\\theta_\\mathsf{c}}}\\big ) \\right]^{-\\frac{1}{2}}. \\label{eq : rho_y1y2 } \\end{aligned}\\ ] ]    let @xmath76 be the encoding rate per measurement sample . when the si is not used , the information rd function satisifies @xmath152 and the entropy  constrained scalar quantizer achieves a distortion @xmath87 that satisfies @xmath153    when the measurement @xmath128 of the si is used at the decoder , the information rd function satisifies @xmath154 while the entropy  constrained scalar quantizer achieves a distortion @xmath155 that satisfies @xmath156 where @xmath157    therefore , in the large system regime ( and in the high  rate regime for entropy constrained scalar quantizer ) , the measurement @xmath128 of the si helps reducing the rate by @xmath158 bits per measurement sample : @xmath159    * sketch of proof .",
    "* we consider a vector of finite length @xmath160 , which contains the first @xmath3 components of @xmath127 followed by the first @xmath3 components of @xmath128 .",
    "the vector can be seen as a sum of three components , where each component converges to a gaussian vector from the multidimensional clt ( * ? ? ?",
    "* theorem 7.18 ) .",
    "finally , we obtain that @xmath149 converges to an i.i.d .",
    "gaussian process .",
    "therefore , classical rd results for i.i.d .",
    "gaussian sources apply . see appendix  [ annex : prop : rd_dcs ] for the complete proof.@xmath93    theorem  [ prop : rd_dcs ] first states that the measurements of two sparse vectors converge to an i.i.d .",
    "gaussian process in the large system regime .",
    "then , lossy compression of the measurements is considered and the information and entropy constrained rate distortion functions are derived .",
    "it is shown that if one measurement vector is used as side information at the decoder , some rate can be saved , depending on the sparse source characteristics , only ( see and ) .",
    "we now derive the rd functions after reconstruction of the dcs scheme .",
    "[ th : rd reconstruction ] _ ( distributed cs : reconstruction rd functions ) .",
    "_ let @xmath123 and @xmath124 be @xmath161 , @xmath162,@xmath163,@xmath164,@xmath165,@xmath71 , @xmath166,@xmath167,@xmath168,@xmath169-sparse .",
    "@xmath124 serves as si for @xmath123 and is available at the decoder , only .",
    "let @xmath127 and @xmath128 be the @xmath60-gaussian measurements of @xmath123 and @xmath124 , s.t .",
    "let @xmath76 be the encoding rate per measurement sample .",
    "the distortion of the source @xmath123 is denoted as @xmath171 when the si is not available at the receiver , @xmath172 when the measurements of the si are available at the swc decoder ( @xmath173 stands for _ independent reconstruction _ ) , and @xmath174 when the si is used not only to reduce the encoding rate but also to improve the reconstruction fidelity ( @xmath175 stands for _ joint reconstruction _ ) .",
    "+ then , when independent reconstruction is performed , the rd functions for @xmath123 satisfy , in the large system regime : @xmath176 therefore , in the large system regime , the operational rd functions satisfy @xmath177 where @xmath158 is defined in . in the large system regime and in the high  rate regime , the entropy constrained rd functions satisfy : @xmath178    when joint reconstruction is performed , the rd functions for @xmath123 satisfy : @xmath179 where , in the large system regime , @xmath180 and in the high rate regime @xmath181 finally , @xmath182 \\label{eq : rate gain ir_jr}\\end{aligned}\\ ] ] and where @xmath158 has been defined in",
    ". therefore , when the si is available at the decoder , it helps reducing the rate by @xmath183 bits per measurement sample .",
    "* sketch of proof . * an oracle is considered in order to derive lower bounds .",
    "more precisely , it is assumed that the sparsity support of @xmath123 is known if independent reconstruction is performed and that also the support of the common component @xmath184 is known if joint reconstruction is performed .",
    "the exact distortion of the oracles are derived , from which a closed  form expression of the rate gains are given . see the complete proof in appendix  [ annex : th : rd reconstruction ] .",
    "@xmath93    as one would expect , when there is no innovation component ( @xmath185 ) , the distortion of the oracle is zero and the rate gain @xmath186 is largest ( tends to infinity ) . on the contrary ,",
    "when there is no common component ( @xmath187 ) , @xmath186 tends to zero .",
    "moreover , even if the si could be exploited also to enhance the quality of the dequantization of the unknown source , the gain due to joint dequantization becomes negligible in the high  rate region @xcite . for this reason we neglected joint dequantization in this analysis",
    "in this section , we validate by simulations the results obtained in the previous sections , comparing numerical results to the derived equations . in particular , first we validate the rd functions of the measurement vector , both in the single  source and in the distributed case , hence validating at the same time the rate gain @xmath158 .",
    "then , we validate the rd functions of independent and joint reconstruction ( hence the rate gain @xmath188 ) .",
    "finally , we compare the results obtained using the oracle estimator with the results obtained using practical reconstruction algorithms , showing that the rate gains hold also in a practical scenario . for each test",
    ", @xmath58  is the dct matrix , each non zero component of @xmath56 is drawn from a normal distribution , and @xmath189 .",
    "first , we test the validity of the rd functions derived for the measurements . fig .",
    "[ fig : rd_y_m128_espo-1 ] plots the quantization distortion of @xmath127 , _",
    "i.e. _ , @xmath190}}$ ] versus the rate @xmath76 , measured in bits per measurement sample ( bpms ) .",
    "the distortion has been averaged over @xmath191 trials , and for each trial different realizations of the sources , the sensing matrix and the noise have been drawn .",
    "[ fig : rd_y_m128_espo-1 ] shows two subfigures corresponding to different sets of signal and system parameters ( signal length @xmath71 , sparsity of the common component @xmath192 and of the innovation component @xmath193 , variance of the nonzero components @xmath166 and @xmath194 , respectively , and length of the measurement vector @xmath67 ) .",
    "each subfigure shows two families of curves , corresponding to the cases in which @xmath128 is ( respectively , is not ) used as si .",
    "each family is composed by 3 curves . @xmath195",
    "the curve labeled as ",
    "standing for _ high rate _",
    " is the asymptote of the operational rd curves ( or ) .",
    "@xmath196 the curve labeled as corresponds to the distortion of a synthetic correlated gaussian source pair with covariance matrix as in , where @xmath197 is defined in and @xmath198 in , and quantized with a uniform scalar quantizer .",
    "the rate is computed as the _ symbol _ entropy of the samples @xmath199 ( the conditional _ symbol _ entropy of @xmath199 given @xmath200 ) , quantized with a uniform scalar quantizer .",
    "entropy and conditional entropy have been evaluated computing the number of symbol occurrences over vectors of length @xmath201 . @xmath202",
    "the curves labeled as are the simulated rd for a measurement vector pair obtained generating @xmath123 and @xmath124 according to def .",
    "[ def : jsm1 ] , measuring them with the same @xmath66  to obtain @xmath127 and @xmath128 and quantizing @xmath127 and @xmath128 with a uniform scalar quantizer .",
    "first , we notice that the equation perfectly matches the simulated curves when @xmath203 , showing that the high  rate regime occurs for relative small values of @xmath76 .",
    "then , it can be noticed that curves perfectly overlap the ones , validating both equations and and showing that the convergence to the gaussian case occurs for low values of @xmath71 , @xmath67 , @xmath204 , @xmath193 .",
    "+    after validating the rd functions derived for the measurements , we test the rd functions for the oracle reconstruction . fig .",
    "[ fig : rd_xrec_m128_espo-1 ] depicts the performance of the complete dcs scheme , in terms of reconstruction error , _",
    "i.e. _ , @xmath205}}$ ] versus the rate per measurement sample @xmath76 .",
    "the figure shows two subfigures corresponding to different sets of signal and system parameters ( @xmath71 , @xmath192 , @xmath193 , @xmath166 , @xmath194 , @xmath67 ) .",
    "each subfigure shows three pairwise comparisons .",
    "first , it compares the rhs of equation (  standing for _ independent reconstruction high rate _ ) with the oracle reconstruction distortion from @xmath199 vs. the _ symbol _ entropy of the samples @xmath199 (  standing for _ independent reconstruction simulated _ ) , obtaining a match for @xmath203 .",
    "second , it compares the rhs of equation ( ) with the oracle reconstruction distortion from @xmath199 vs. the conditional _ symbol _ entropy of @xmath199 given @xmath200 ( ) , obtaining a match for @xmath206 and validating once more the evaluation of the rate gain @xmath158 due to the swc .",
    "third , it compares the rhs of equation (  standing for _ joint reconstruction high rate _ ) with the ideal ( knowing the sparsity support of the common component ) oracle joint reconstruction distortion from @xmath199 and @xmath128 vs. the conditional _ symbol _ entropy of @xmath199 given @xmath200 (  standing for _ joint reconstruction simulated _ ) , obtaining a match for @xmath207 , validating the expression of the rate gain due to joint reconstruction given in .",
    "+    finally , we report in fig .",
    "[ fig : rd_bpdn_m128_espo-1 ] the performance of practical reconstruction algorithms solving the optimization problem .",
    "the curve labelled as reports the rd function of the independent reconstruction .",
    "the curve labelled as reports the rd function of the joint reconstruction when the sparsity support of the common component is known at the decoder .",
    "the curve labelled as , instead , shows the rd performance of a joint reconstruction scheme in which the sparsity support of the common component is not known _ a priori _ , but is estimated from the measurements @xmath127 and @xmath128 using the jr algorithm 1 in @xcite ( intersect jr ) .",
    "the principle behind the intersect jr algorithm is that the sparsity support of the common component is obtained as the intersection between the estimated sparsity supports of the information source and the si .",
    "comparing with the oracle performance curve , it can be noticed that , apart from a penalty due to the missing knowledge of the sparsity support , the slope of the rd curve is the same as in the ideal oracle case",
    ". moreover , fig .",
    "[ fig : rd_bpdn_m128_espo-1 ] shows that the practical jr algorithm 1 in @xcite performs very close to ideal jr algorithm . note that a fully practical scheme will use a real swc encoder / decoder .",
    "in @xcite we showed that a system implementing a real swc encoder / decoder performs very close to the lower bound represented by the entropy and conditional entropy .",
    "finally , in fig .",
    "[ fig : rd_bpdn_m128_espo-1 ] it can be seen that @xmath208  bpms , which roughly corresponds to the sum of @xmath209  bpms in figs .",
    "[ fig : meas_1 ] and [ fig : oracle_1 ] and @xmath210  bpms in fig .",
    "[ fig : oracle_1 ] , proving that the rate gains derived for the oracle receiver hold in a practical scenario , as well . finally , fig .",
    "[ fig : rd_bpdn_m128_espo-1 ] plots the performance of the @xmath74-weighted @xmath11-norm minimization algorithm ( * ? ? ?",
    "* ( 12 ) ) , which for a fair comparison we optimized to take into account that the measurements of the si are perfectly known whereas the measurements of the unknown source are subject to quantization noise .",
    "it can be noticed that the performance is slightly worse with respect to the intersect jr algorithm . plus",
    ", the complexity increase is significant since the @xmath74-weighted @xmath11-norm minimization algorithm needs the solution of a problem of size @xmath211 , which means a compexity increase of 8 times since the complexity of basis pursuit minimization algorithms is cubic with the size of the problem .    , @xmath212 , @xmath189 , @xmath213 , @xmath214 and @xmath215,scaledwidth=50.0% ]",
    "we have studied the best achievable performance in the rd sense over all single  source and dcs schemes , under the constraint of high  rate quantization . closed form expressions of the rd curves",
    "have been derived in the asymptotic regime , and simulations have shown that the asymptote is reached for relatively small number of measurements ( @xmath216 ) and small rate ( @xmath203 bits / measurement sample ) .",
    "the rd curve computation is based on the convergence of the measurement vector to the multidimensional standard normal distribution .",
    "this generalizes ( * ? ? ?",
    "* theorem 2 ) that derives the marginal distribution of the measurement samples when the observed signal is directly sparse . we have then derived a closed  form expression of the highest rate gain achieved exploiting all levels of source correlations at the receiver , along with a closed  form expression of the average performance of the oracle receiver , using novel results on random wishart matrix theory .",
    "simulations showed that the scheme proposed in @xcite almost achieves this best rate gain , and that the only penalty is due to the missing knowledge of the sparsity support as in single  source compressed sensing .",
    "the gaussian random matrix @xmath49 is orthogonal invariant ( * ? ? ?",
    "* section 4.3 ) , ( * ? ? ?",
    "* ex . 2.4 ) . therefore , @xmath217 is a random matrix with i.i.d .",
    "entries drawn from @xmath68 . without loss of generality , we assume that the @xmath42 non zeros of @xmath56 are placed at the beginning of @xmath56 . to show that the infinite length measurement random variables are asymptotically independent and gaussian , we consider a finite length vector @xmath91 that contains the @xmath3 first components of @xmath59 , and show that , for each @xmath3 , @xmath91 converges to a gaussian vector with diagonal covariance matrix , as @xmath71 grows .",
    "let @xmath218 and @xmath219denote the random vectors in @xmath220 associated to the gaussian measurement vector @xmath91 and to the @xmath221-th column of the matrix @xmath90 , restricted to its @xmath3 first rows .",
    "let @xmath222 denote the random variable in @xmath223 associated to the @xmath221-th component of the vector @xmath56 .",
    "we have @xmath224 by definition , @xmath225 and @xmath222 are independent , and the sequences @xmath226 and @xmath227 are i.i.d . centered with covariance matrix @xmath228 and variance @xmath229 , respectively . therefore",
    ", each vector @xmath230 is centered with covariance matrix @xmath231 , where @xmath232 is the @xmath233 identity matrix . from the multidimensional central limit theorem ( * ? ? ?",
    "* theorem 7.18 ) , @xmath20 converges ( in distribution ) to a multidimensional gaussian distribution with mean vector @xmath234 and covariance matrix @xmath235 .",
    "thus , the entries of the vector are independent . letting @xmath3 grow to @xmath236",
    ", we have that the sequence of gaussian measurements converges to a gaussian i.i.d . sequence with mean @xmath237 and variance @xmath238 .",
    "@xmath93      we derive a lower bound on the achievable distortion by assuming that the sparsity support @xmath99 of @xmath53 is known at the decoder .",
    "let @xmath239 be the submatrix of @xmath240 obtained by keeping the columns of @xmath43 indexed by @xmath99 , and let @xmath241 denote the complementary set of indexes .",
    "the optimal reconstruction is then obtained by using the pseudo  inverse of @xmath239 , denoted by @xmath242 : @xmath243 and @xmath244 , where @xmath245 is the quantized version of @xmath59 i.e. @xmath246 , where @xmath247 is the quantization noise of variance @xmath248 .",
    "note that in the product by @xmath249 is a consequence of definition  [ def : measurement ] .",
    "@xmath250 } } & = { \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left\\| \\widehat{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{\\theta}}}}}}-{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{\\theta}}}}}}\\right\\|_{2}}}^2\\right ] } } =   { \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left\\| \\widehat{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{\\theta}}}}}}_{\\omega}-{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{\\theta}}}}}}_{\\omega}\\right\\|_{2}}}^2\\right ] } }   \\label{eq : rc 1}\\\\ & = m { \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{\\left\\|   { \\ensuremath{{\\ensuremath{\\bm{\\mathrm{u}}}}^\\dagger_{\\omega}}}{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{e}}}}}}\\right\\|_{2}}}^2\\right ] } } \\label{eq : rc 2}\\\\ & = m { \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{e}}}}}}^t { \\ensuremath{\\mathbb{e}\\left[({\\ensuremath{{\\ensuremath{\\bm{\\mathrm{u}}}}_{\\omega}}}{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{u}}}}_{\\omega}}}^t)^\\dagger\\right ] } } { \\ensuremath{{\\ensuremath{\\bm{\\mathrm{e}}}}}}\\right ] } }    \\label{eq : rc 3}\\end{aligned}\\ ] ] the first equality in follows from the orthogonality of the matrix @xmath50 , whereas the second follows from the assumption that @xmath99 is the true support of @xmath56 . derives from the definition of the pseudo - inverse , and from @xmath251 .",
    "then , if @xmath102 , @xmath250 } } & = m { \\ensuremath{\\mathbb{e}\\left[{\\ensuremath{{\\ensuremath{\\bm{\\mathrm{e}}}}}}^t \\frac{k}{m ( m - k-1 ) } \\frac{1}{\\sigma^2_\\phi } { \\ensuremath{{\\ensuremath{\\bm{\\mathrm{i}}}}}}\\   { \\ensuremath{{\\ensuremath{\\bm{\\mathrm{e}}}}}}\\right ] } } \\label{eq : rc 4}\\\\ & = \\frac{m k}{m - k-1 }   \\frac{\\sigma_e^2}{\\sigma^2_\\phi } \\label{eq : rc 5 } \\\\",
    "\\sigma^2_{\\hat x } & = \\frac{k}{n } \\frac{m}{m - k-1 } \\frac{\\sigma_e^2}{\\sigma^2_\\phi}\\label{eq : rc 6}\\end{aligned}\\ ] ] where @xmath252 stands for the distortion of the oracle estimator at finite length @xmath71",
    ". comes from the fact that , since @xmath253 , @xmath254 is rank deficient and follows a singular @xmath67-variate wishart distribution with @xmath42 degrees of freedom and scale matrix @xmath255 @xcite .",
    "its pseudo - inverse follows a generalized inverse wishart distribution , whose distribution is given in @xcite and mean in ( * ? ? ?",
    "* theorem 2.1 ) , under the assumption that @xmath102 .",
    "note that the distortion of the oracle only depends on the variance of the quantization noise and not on its covariance matrix .",
    "therefore , our result holds even if the noise is correlated ( for instance if vector quantization is used ) . as a consequence",
    ", we can apply our result to any quantization algorithm .",
    "therefore as @xmath256 , if @xmath257 @xmath258 where the first equality is obtained by taking the limit @xmath256 of and the second equality follows from theorem  [ theorem : y iid gaussian ] . substituting the entropy - constrained rd function of the measurements in leads to the entropy - constrained reconstruction rd function .@xmath93",
    "we first consider non - overlapping sparse components , i.e. @xmath259 , @xmath260 and @xmath261 have non overlapping sparsity supports . from theorem  [",
    "theorem : y iid gaussian ] , @xmath217 is a random matrix with i.i.d .",
    "entries drawn from @xmath68 . without loss of generality , we assume that all non zeros of @xmath56 are placed at the beginning of @xmath56 , with first the @xmath163 common , then the @xmath262 and the @xmath162 individual components .",
    "we build the finite length @xmath160 vector @xmath263 that contains the @xmath3 first components of @xmath127 concatenated with the @xmath3 first components of @xmath128 , and let @xmath71 go to infinity . let @xmath218 denote the random vector in @xmath264 associated to the gaussian measurement vector @xmath263 and let @xmath225 denote the random vector in @xmath265 associated to the @xmath221-th column of the matrix @xmath266 , restricted to its @xmath3 first rows .",
    "let @xmath222 denote the random variable in @xmath223 associated to the @xmath221-th component of the vector @xmath56 . by definition of the sparse vectors and their measurements , we have @xmath267 where @xmath234 stands for the all zero vector of length @xmath3 and @xmath268 , @xmath269and @xmath270  are defined as @xmath271    each vector @xmath230 is centered with covariance matrix @xmath272 where @xmath232 is the @xmath233 identity matrix .    from the multidimensional clt ( * ?",
    "* theorem 7.18 ) , @xmath268  converges ( in distribution ) to a multidimensional gaussian distribution with mean vector @xmath234 and covariance matrix @xmath273 in the large system regime .",
    "similarly , in the large system regime , @xmath269  and @xmath270  converge ( in distribution ) to a multidimensional gaussian distribution with mean vector @xmath234 and covariance matrix @xmath274 , @xmath275 respectively , where @xmath234 stands for the all zero @xmath233 matrix to denote either an @xmath3 length vector or an all zero @xmath233 matrix . ] .",
    "moreover , @xmath268 , @xmath269  and @xmath270  are independent , since the elements of each sum are all different in .",
    "therefore , @xmath218 converges to a gaussian centered vector with covariance matrix @xmath276 since @xmath277 . in the case of overlapping supports ,",
    "each entry of the vector @xmath56 can either contain the contribution of only one component ( common , or any innovation ) , or any combination of at least two components .",
    "therefore , the support of the non zero components of @xmath56 and therefore the random vector @xmath218 , can be decomposed into 7 terms , that correspond to the number of subsets of a set of 3 elements ( excluding the empty set ) .",
    "note that each term is a sum of i.i.d .",
    "random vectors such that the multidimensional clt still applies and holds .",
    "letting @xmath3 grow to @xmath236 , we have that , in the large system regime , @xmath149 converges to an i.i.d .",
    "gaussian sequence with covariance matrix @xmath278\\ \\text{and } \\nonumber\\\\ \\rho_{12 } & = \\left [ \\big(1 + \\frac { \\omega_{\\mathsf{i},1 } } { \\omega_{\\mathsf{c},1}}\\frac{{\\ensuremath{\\sigma^2}}_{\\theta_{\\mathsf{i},1}}}{{\\ensuremath{\\sigma^2}}_{\\theta_\\mathsf{c}}}\\big ) \\big(1 + \\frac { \\omega_{\\mathsf{i},2 } } { \\omega_{\\mathsf{c},2}}\\frac{{\\ensuremath{\\sigma^2}}_{\\theta_{\\mathsf{i},2}}}{{\\ensuremath{\\sigma^2}}_{\\theta_\\mathsf{c}}}\\big ) \\right]^{-\\frac{1}{2}}~ ,   \\nonumber\\end{aligned}\\ ] ] since @xmath277 .",
    "therefore , the rd functions are given in , , and .",
    "@xmath93      let us first consider independent reconstruction .",
    "this means that the measurements of the si are used at the swc decoder only and not to improve the quality of the dequantization or the reconstruction stages .",
    "we derive a lower bound on the achievable distortion by assuming that the sparsity support @xmath279 of @xmath123 is known at the decoder .",
    "this receiver is called the oracle and leads to a variance of estimation @xmath280 : @xmath281 where the derivation is similar to the non distributed case ( see appendix  [ annex : th : rd reconstruction non distributed ] ) .",
    "@xmath282 is the quantization noise variance , i.e. @xmath283 if the @xmath128 is used as side information at the swc decoder and @xmath284 otherwise .",
    "this leads to and .",
    "then , in the large system regime , the measurements are gaussian and the rd functions of the measurements have a closed form expression , which is used to derive , , , and .",
    "finally , note that the gap between the oracle based lower bound and the true rd functions , in the ir case , is only due to the performance of the algorithm chosen to reconstruct @xmath123 from @xmath127 .",
    "the performance of a deterministic cs reconstruction algorithm depends only on the density @xmath285 , which in the present gaussian case is determined by the mse @xmath284 . in the ir case ,",
    "@xmath284 depends on the presence of the quantization / dequantization step , since the source coding / decoding step is considered a zero  error stage .",
    "hence , the presence or absence of a ( distributed ) source encoding and decoding stage does not alter @xmath284 since it does not alter the output of the dequantizer .",
    "conditioning on @xmath286 yields a rate shift @xmath287 that will thus be directly reflected in the cs reconstruction .",
    "hence , it can be written that @xmath288 and @xmath289 for some @xmath290 depending on the specific reconstruction algorithm . this yields .",
    "as for the joint reconstruction , the si is used to reduce the sparsity level of the unknown signal , and hence the performance of the reconstruction . more precisely , to give a lower bound we assume that the receiver perfectly knows the common component @xmath108 and @xmath291 of the innovation component @xmath292 .",
    "hence , it will use the former to estimate the measurements of the common component @xmath293 and then subtract them from @xmath127 . in this way , the vector to be reconstructed is the innovation component @xmath292 only , which is sparser than @xmath123 ( given the same @xmath71 and @xmath67 ) .",
    "then , it will use the latter information to apply the oracle , leading to a variance of estimation @xmath280 : @xmath294 where @xmath282 is the measurement distortion , when @xmath128 is used as side information at the receiver .",
    "this leads to ( [ eq : perf_oracle_dsc_jr]-[eq : perf_oracle_dsc_jr gaussian ec ] ) . @xmath93",
    "the authors would like to thank the associate editor and the anonymous reviewers for their valuable suggestions that helped to improve the quality of the final version of this paper .",
    "a.  fletcher , s.  rangan , and v.  goyal , `` on the rate - distortion performance of compressed sensing , '' in _ ieee int .",
    "conf . on acoustics , speech and signal processing ( icassp )",
    "_ , vol .",
    "3.1em plus 0.5em minus 0.4em ieee , 2007 .",
    "m.  duarte , m.  wakin , d.  baron , s.  sarvotham , and r.  baraniuk , `` measurement bounds for sparse signal ensembles via graphical models , '' _ ieee transactions on information theory _",
    "59 , no .  7 , pp . 42804289 , 2013 .",
    "z.  liu , s.  cheng , a.  d. liveris , and z.  xiong , `` slepian - wolf coded nested lattice quantization for wyner - ziv coding : high - rate performance analysis and code design , '' _ ieee transactions on information theory _ , vol .",
    "52 , no .",
    "43584379 , 2006 .",
    "m.  a. davenport , j.  n. laska , j.  r. treichler , and r.  g. baraniuk , `` the pros and cons of compressive sensing for wideband signal acquisition : noise folding vs. dynamic range , '' _ corr _ , vol .",
    "abs/1104.4842 , 2011 .            d.  rebollo - monedero , s.  rane , a.  aaron , and b.  girod , `` high - rate quantization and transform coding with side information at the decoder , '' _ signal process .",
    "_ , vol .",
    "86 , no .  11 , pp . 31603179 , nov .",
    "2006 .",
    "j.  a. daz - garca and r.  gutirrez - jimez , `` distribution of the generalised inverse of a random matrix and its applications , '' _ journal of statistical planning and inference _ , vol .",
    "136 , no .  1 ,",
    "183192 , 2006 .",
    "a.  m. tulino and s.  verd , `` random matrix theory and wireless communications , '' _ foundations and trends in commun .",
    "inf . theory _",
    ", vol .  1 , no .  1 ,",
    "1182 , jun . 2004 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1516/0100000001    giulio coluccia giulio coluccia received the bachelor in science ( cum laude ) in 2003 and the master in science ( cum laude ) in 2005 in telecommunications engineering , both from the politecnico di torino , torino , italy .",
    "he was a ph.d .",
    "student within the electronics department of the politecnico di torino , torino , italy , under the supervision of prof .",
    "giorgio taricco . during his ph.d",
    ". program , he visited the telecommunications research center vienna ( ftw . ) , vienna , austria .",
    "he received the ph.d .",
    "degree in electronic and communications engineering in 2009 .",
    "his research activity included mimo communications and space - time detection .    currently , he is a post doctoral researcher within the image processing lab at politecnico di torino , leaded by prof .",
    "enrico magli .",
    "his resarch is focused on compressed sensing , with particular interest in its application to image processing , multidimensional signals and to distributed source coding and wireless sensor netowrks .",
    "he is involved in the 5-year project entitled `` crisp - towards compressive information processing systems '' funded by the european union .",
    "aline roumy aline roumy received the engineering degree from ecole nationale superieure de lelectronique et de ses applications ( ensea ) , cergy , france in 1996 , the master degree in 1997 and the ph.d . degree in 2000 from the university of cergy - pontoise , france . during 2000 - 2001 , she was a research associate at princeton university , princeton , nj . in 2001 , she joined inria , rennes , france as a research scientist . she has held visiting positions at eurecom and berkeley university .      she has been a technical program committee member and session chair at several international conferences , including isit , icassp , eusipco .",
    "she is currently serving as a member of the french national university council ( cnu 61 ) .",
    "she received the 2011 `` francesco carassa '' best paper award .",
    "enrico magli enrico magli received the ph.d .",
    "degree in electrical engineering in 2001 , from politecnico di torino , italy .",
    "he is currently an associate professor at the same university , where he leads the image processing lab .",
    "his research interests are in the field of multimedia signal processing and networking , compressive sensing , distributed source coding , image and video security , and compression of satellite images .",
    "he is an associate editor of the ieee transactions on circuits and systems for video technology , and of the ieee transactions on multimedia .",
    "he is a member of the multimedia signal processing technical committee of the ieee signal processing society , and of the multimedia systems and applications and the visual signal processing and communications technical committees of the ieee circuits and systems society .",
    "he has been co - editor of jpeg 2000 part 11 - wireless jpeg 2000 .",
    "he is general co - chair of ieee mmsp 2013 , and has been tpc co - chair of icme 2012 , vcip 2012 , mmsp 2011 and imap 2007 .",
    "he has published about 40 papers in refereed international journals , 3 book chapters , and over 100 conference papers .",
    "he is a co - recipient of the ieee geoscience and remote sensing society 2011 transactions prize paper award , and has received the 2010 best reviewer award of ieee journal of selected topics in applied earth observation and remote sensing .",
    "he has received a 2010 best associate editor award of ieee transactions on circuits and systems for video technology ."
  ],
  "abstract_text": [
    "<S> we consider correlated and distributed sources without cooperation at the encoder . for these sources </S>",
    "<S> , we derive the best achievable performance in the rate - distortion sense of any distributed compressed sensing scheme , under the constraint of high  rate quantization . </S>",
    "<S> moreover , under this model we derive a closed  form expression of the rate gain achieved by taking into account the correlation of the sources at the receiver and a closed  form expression of the average performance of the oracle receiver for independent and joint reconstruction . </S>",
    "<S> finally , we show experimentally that the exploitation of the correlation between the sources performs close to optimal and that the only penalty is due to the missing knowledge of the sparsity support as in ( non distributed ) compressed sensing . even if the derivation is performed in the large system regime , where signal and system parameters tend to infinity , numerical results show that the equations match simulations for parameter values of practical interest .    </S>",
    "<S> coluccia : operational rate  distortion performance of single  source and distributed compressed sensing    compressed sensing , rate  </S>",
    "<S> distortion function , distributed source coding , slepian  wolf coding . </S>"
  ]
}