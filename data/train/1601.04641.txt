{
  "article_text": [
    "b.  s. , v.  s. , a.  b. , and a.  b. acknowledge support by the ministry of education , science , and technological development of the republic of serbia under projects on171017 , iii43007 , on171009 , on174027 and ibec , and by daad - german academic and exchange service under project ibec .",
    "m. acknowledges support by the science and engineering research board , department of science and technology , government of india under project no .",
    "s.  k.  a. acknowledges support by the cnpq of brazil under project 303280/2014 - 0 , and by the fapesp of brazil under project 2012/00451 - 0 .",
    "numerical simulations were run on the paradox supercomputing facility at the scientific computing laboratory of the institute of physics belgrade , supported in part by the ministry of education , science , and technological development of the republic of serbia under project on171017 .",
    "d. vudragovi , i. vidanovi , a. bala , p. muruganandam , s.  k. adhikari , c programs for solving the time - dependent gross - pitaevskii equation in a fully anisotropic trap , comput .",
    ". commun . *",
    "183 * ( 2012 ) 2021 .",
    "r.  k. kumar and p. muruganandam , j. phys .",
    "b : at . mol .",
    "* 45 * ( 2012 ) 215301 ; + l.",
    "e. young - s . and s.  k. adhikari , phys . rev .",
    "a * 86 * ( 2012 ) 063611 ; + s.  k. adhikari , j. phys .",
    "b : at . mol .",
    "opt . phys .",
    "* 45 * ( 2012 ) 235303 ; + i. vidanovi , n.  j. van druten , and m. haque , new j. phys .",
    "* 15 * ( 2013 ) 035008 ; + s. balasubramanian , r. ramaswamy , and a.  i. nicolin , rom .",
    "* 65 * ( 2013 ) 820 ; + l.  e. young - s . and s.  k. adhikari , phys . rev . a * 87 * ( 2013 ) 013618 ; + h. al - jibbouri , i. vidanovic , a. balaz , and a. pelster , j. phys .",
    "b : at . mol .",
    "opt . phys .",
    "* 46 * ( 2013 ) 065303 ; + x. antoine , w. bao , and c. besse , comput .",
    "* 184 * ( 2013 ) 2621 ; + b. nikoli , a. bala , and a. pelster , phys .",
    "a * 88 * ( 2013 ) 013624 ; + h. al - jibbouri and a. pelster , phys .",
    "a * 88 * ( 2013 ) 033621 ; + s.  k. adhikari , phys .",
    "rev . a * 88 * ( 2013 ) 043603 ; + j.  b. sudharsan , r. radha , and p. muruganandam , j. phys .",
    "b : at . mol .",
    "* 46 * ( 2013 ) 155302 ; + r.  r. sakhel , a.  r. sakhel , and h.  b. ghassib , j. low temp",
    "* 173 * ( 2013 ) 177 ; + e.  j.  m. madarassy and v.  t. toth , comput .",
    "* 184 * ( 2013 ) 1339 ; + r.  k. kumar , p. muruganandam , and b.  a. malomed , j. phys .",
    "b : at . mol .",
    "* 46 * ( 2013 ) 175302 ; + w. bao , q. tang , and z. xu , j. comput",
    "* 235 * ( 2013 ) 423 ; + a.  i. nicolin , proc .",
    "ser . a - math",
    "* 14 * ( 2013 ) 35 ; + r.  m. caplan , comput .",
    "* 184 * ( 2013 ) 1250 ; + s.  k. adhikari , j. phys .",
    "b : at . mol .",
    "* 46 * ( 2013 ) 115301 ; +  .",
    "marojevi , e. gkl , and c. lmmerzahl , comput .",
    "* 184 * ( 2013 ) 1920 ; + x. antoine and r. duboscq , comput .",
    "* 185 * ( 2014 ) 2969 ; + s.  k. adhikari and l.  e. young - s , j. phys .",
    "b : at . mol .",
    "* 47 * ( 2014 ) 015302 ; + k. manikandan , p. muruganandam , m. senthilvelan , and m. lakshmanan , phys .",
    "e * 90 * ( 2014 ) 062905 ; + s.  k. adhikari , phys .",
    "a * 90 * ( 2014 ) 055601 ; + a. bala , r. paun , a. i. nicolin , s. balasubramanian , and r. ramaswamy , phys . rev .",
    "a * 89 * ( 2014 ) 023609 ; + s.  k. adhikari , phys .",
    "rev . a * 89 * ( 2014 ) 013630 ; + j. luo , commun .",
    "nonlinear sci .",
    "* 19 * ( 2014 ) 3591 ; + s.  k. adhikari , phys .",
    "rev . a * 89 * ( 2014 ) 043609 ; + k .- t .",
    "xi , j. li , and d .-",
    "shi , physica b * 436 * ( 2014 ) 149 ; + m.  c. raportaru , j. jovanovski , b. jakimovski , d. jakimovski , and a. mishev , rom . j. phys . * 59 * ( 2014 ) 677 ; + s. gautam and s.  k. adhikari , phys .",
    "a * 90 * ( 2014 ) 043619 ; + a.  i. nicolin , a. bala , j. b. sudharsan , and r. radha , rom .",
    "* 59 * ( 2014 ) 204 ; + k. sakkaravarthi , t. kanna , m. vijayajayanthi , and m. lakshmanan , phys .",
    "e * 90 * ( 2014 ) 052912 ; + s.  k. adhikari , j. phys .",
    "* 47 * ( 2014 ) 225304 ; + r.",
    "k. kumar and p. muruganandam , numerical studies on vortices in rotating dipolar bose - einstein condensates , proceedings of the 22nd international laser physics workshop , j. phys .",
    "conf . ser .",
    "* 497 * ( 2014 ) 012036 ; + a.  i. nicolin and i. rata , density waves in dipolar bose - einstein condensates by means of symbolic computations , high - performance computing infrastructure for south east europe s research communities : results of the hp - see user forum 2012 , in springer series : modeling and optimization in science and technologies * 2 * ( 2014 ) 15 ; + s.  k. adhikari , phys . rev . a * 89 * ( 2014 ) 043615 ; + r.  k. kumar and p. muruganandam , eur .",
    "j. d * 68 * ( 2014 ) 289 ; + j.  b. sudharsan , r. radha , h. fabrelli , a. gammal , and b.",
    "a. malomed , phys .",
    "a * 92 * ( 2015 ) 053601 ; + s.  k. adhikari , j. phys .",
    "b : at . mol .",
    "opt . phys .",
    "* 48 * ( 2015 ) 165303 ; + f.",
    "i. moxley iii , t. byrnes , b. ma , y. yan , and w. dai , j. comput .",
    "* 282 * ( 2015 ) 303 ; + s.  k. adhikari , phys .",
    "e * 92 * ( 2015 ) 042926 ; + r.  r. sakhel , a.  r. sakhel , and h.  b. ghassib , physica b * 478 * ( 2015 ) 68 ; + s. gautam and s.  k. adhikari , phys .",
    "rev . a * 92 * ( 2015 ) 023616 ; + d. novoa , d. tommasini , and j.  a. nvoa - lpez , phys .",
    "e * 91 * ( 2015 ) 012904 ; + s. gautam and s.  k. adhikari , laser phys .",
    "* 12 * ( 2015 ) 045501 ; + k .- t .",
    "xi , j. li , and d .-",
    "shi , physica b * 459 * ( 2015 ) 6 ; + r.  k. kumar , l.  e. young - s . , d. vudragovi , a. bala , p. muruganandam , and s.  k. adhikari , comput .",
    ". commun . * 195 * ( 2015 ) 117 ; + s. gautam and s.  k. adhikari , phys .",
    "rev . a * 91 * ( 2015 ) 013624 ; + a. i. nicolin , m.  c. raportaru , and a. bala , rom .",
    "* 67 * ( 2015 ) 143 ; + s. gautam and s.  k. adhikari , phys . rev . a * 91 * ( 2015 ) 063617 ; + e.  j.  m. madarassy and v.  t. toth , phys .",
    "d * 91 * ( 2015 ) 044041 ."
  ],
  "abstract_text": [
    "<S> we present hybrid openmp / mpi ( open multi - processing / message passing interface ) parallelized versions of earlier published c programs ( d. vudragovi et al . ( 2012 ) @xcite ) for calculating both stationary and non - stationary solutions of the time - dependent gross - pitaevskii ( gp ) equation in three spatial dimensions . </S>",
    "<S> the gp equation describes the properties of dilute bose - einstein condensates at ultra - cold temperatures . </S>",
    "<S> hybrid versions of programs use the same algorithms as the c ones , involving real- and imaginary - time propagation based on a split - step crank - nicolson method , but consider only a fully - anisotropic three - dimensional gp equation , where algorithmic complexity for large grid sizes necessitates parallelization in order to reduce execution time and/or memory requirements per node . since </S>",
    "<S> distributed memory approach is required to address the latter , we combine mpi programing paradigm with existing openmp codes , thus creating fully flexible parallelism within a combined distributed / shared memory model , suitable for different modern computer architectures . </S>",
    "<S> the two presented c / openmp / mpi programs for real- and imaginary - time propagation are optimized and accompanied by a customizable makefile . </S>",
    "<S> we present typical scalability results for the provided openmp / mpi codes and demonstrate almost linear speedup until inter - process communication time starts to dominate over calculation time per iteration . </S>",
    "<S> such a scalability study is necessary for large grid sizes in order to determine optimal number of mpi nodes and openmp threads per node .    </S>",
    "<S> bose - einstein condensate ; gross - pitaevskii equation ; split - step crank - nicolson scheme ; real- and imaginary - time propagation ; c program ; mpi ; openmp ; partial differential equation    02.60.lj ; 02.60.jh ; 02.60.cb ; 03.75.-b    * new version program summary *     + _ program title : _ gp - scl - hyb package , consisting of : ( i ) imagtime3d - hyb , ( ii ) realtime3d - hyb . </S>",
    "<S> + _ catalogue identifier : _ </S>",
    "<S> aedu_v3_0 + _ program summary url : _ </S>",
    "<S> http://cpc.cs.qub.ac.uk/summaries/aedu_v3_0.html + _ program obtainable from : _ cpc program library , queen s university of belfast , n. ireland . </S>",
    "<S> + _ licensing provisions : _ apache license 2.0 + _ no . of lines in distributed program , including test data , etc . : _ 26397 . </S>",
    "<S> + _ no . of bytes in distributed program , including test data , etc . : _ 161195 . </S>",
    "<S> + _ distribution format : _ </S>",
    "<S> tar.gz . </S>",
    "<S> + _ programming language : _ c / openmp / mpi . </S>",
    "<S> + _ computer : _ any modern computer with c language , openmp- and mpi - capable compiler installed . </S>",
    "<S> + _ operating system : _ linux , unix , mac os x , windows . </S>",
    "<S> + _ ram : _ total memory required to run programs with the supplied input files , distributed over the used mpi nodes : ( i ) 310 mb , ( ii ) 400 mb . </S>",
    "<S> larger grid sizes require more memory , which scales with nx*ny*nz . </S>",
    "<S> + _ number of processors used : _ no limit , from one to all available cpu cores can used on all mpi nodes . </S>",
    "<S> + _ number of nodes used : _ no limit on the number of mpi nodes that can be used . </S>",
    "<S> depending on the grid size of the physical problem and communication overheads , optimal number of mpi nodes and threads per node can be determined by a scalability study for a given hardware platform . </S>",
    "<S> + _ classification : _ 2.9 , 4.3 , 4.12 . + _ catalogue identifier of previous version : _ aedu_v2_0 . </S>",
    "<S> + _ journal reference of previous version : _ comput . </S>",
    "<S> phys . </S>",
    "<S> commun . 183 ( 2012 ) 2021 . </S>",
    "<S> + _ does the new version supersede the previous version ? : _ no .     </S>",
    "<S> + _ nature of problem : _ these programs are designed to solve the time - dependent gross - pitaevskii ( gp ) nonlinear partial differential equation in three spatial dimensions in a fully anisotropic trap using a hybrid openmp / mpi parallelization approach . the gp equation describes the properties of a dilute trapped bose - einstein condensate .     </S>",
    "<S> + _ solution method : _ the time - dependent gp equation is solved by the split - step crank - nicolson method using discretization in space and time </S>",
    "<S> . the discretized equation is then solved by propagation , in either imaginary or real time , over small time steps . </S>",
    "<S> the method yields solutions of stationary and/or non - stationary problems .     </S>",
    "<S> + _ reasons for the new version : _ previous c @xcite and fortran @xcite programs are widely used within the ultracold atoms and nonlinear optics communities , as well as in various other fields @xcite . </S>",
    "<S> this new version represents extension of the two previously openmp - parallelized programs ( imagtime3d - th and realtime3d - th ) for propagation in imaginary and real time in three spatial dimensions to a hybrid , fully distributed openmp / mpi programs ( imagtime3d - hyb and realtime3d - hyb ) . </S>",
    "<S> hybrid extensions of previous openmp codes enable interested researchers to numerically study bose - einstein condensates in much greater detail ( i.e. , with much finer resolution ) than with openmp codes . in openmp ( threaded ) versions of programs , numbers of discretization points in x , y , and z </S>",
    "<S> directions are bound by the total amount of available memory on a single computing node where the code is being executed . </S>",
    "<S> new , hybrid versions of programs are not limited in this way , as large numbers of grid points in each spatial direction can be evenly distributed among the nodes of a cluster , effectively distributing required memory over many mpi nodes . </S>",
    "<S> this is the first reason for development of hybrid versions of 3d codes . </S>",
    "<S> the second reason for new versions is speedup in the execution of numerical simulations that can be gained by using multiple computing nodes with openmp / mpi codes .     </S>",
    "<S> + _ summary of revisions : _ two c / openmp programs in three spatial dimensions from previous version @xcite of the codes ( imagtime3d - th and realtime3d - th ) are transofrmed and rewritten into a hybrid openmp / mpi programs and named imagtime3d - hyb and realtime3d - hyb . </S>",
    "<S> the overall structure of two programs is identical . </S>",
    "<S> the directory structure of the gp - scl - hyb package is extended compared to the previous version and now contains a folder scripts , where examples of scripts that can be used to run the programs on a typical mpi cluster are given . </S>",
    "<S> the corresponding readme.txt file contains more details . </S>",
    "<S> we have also included a makefile with tested and verified settings for most popular mpi compliers , including openmpi ( open message passing interface ) @xcite and mpich ( message passing interface chameleon ) @xcite .    </S>",
    "<S> transformation from pure openmp to a hybrid openmp / mpi approach has required that the array containing condensate wavefunction is distributed among mpi nodes of a computer cluster . </S>",
    "<S> several data distribution models have been considered for this purpose , including block distribution and block cyclic distribution of data in a 2d matrix . </S>",
    "<S> finally , we decided to distribute the wavefunction values across different nodes so that each node contains only one slice of the x - dimension data , while containing the complete corresponding y- and z - dimension data , as illustrated in fig .  </S>",
    "<S> [ fig1 ] . </S>",
    "<S> this allows central functions of our numerical algorithm , calcluy , calcuz , and calcnu to be executed purely in parallel on different mpi nodes of a cluster , without any overhead or communication , as nodes contain all the information for y- and z - dimension data in the given x - sub - domain . </S>",
    "<S> however , the problem arises when functions calclux , calcrms , and calcmuen need to be executed , as they also operate on the whole x - dimension data . </S>",
    "<S> thus , the need for additional communication arises during the execution of the function calcrms , while in the case of fuctions calclux and calcmuen also the transposition of data between x- and y - dimensions is necessary , while data in z dimension have to stay contiguous . </S>",
    "<S> transposition provides nodes with all the necessary x - dimension data to execute functions calclux and calcmuen . </S>",
    "<S> however , this needs to be done in each iteration of numerical algorithm , thus necessarily increasing communication overhead of the simulation .             </S>",
    "<S> transposition algorithms that were considered where the ones that account for greatest common divisor ( gcd ) between number of nodes in columns ( designated by n ) and rows ( designated by m ) of a cluster configured as 2d mash of nodes @xcite . </S>",
    "<S> two of such algorithms have been tested and tried for implementation : the case when gcd = 1 and the case when gcd > 1 . </S>",
    "<S> the trivial situation n = m = 1 is already covered by the previous , purely openmp programs , and therefore , without any loss of generality , we have considered only configurations with number of nodes in x - dimension satisfying n > 1 . only the former algorithm ( gcd = 1 ) was found to be sound in case where data matrix is not a 2d , but a 3d structure . </S>",
    "<S> latter case was found to be too demanding implementation - wise , since mpi functions and data - types are bound to certain limitations . </S>",
    "<S> therefore , the algorithm with m = 1 nodes in y - dimension was implemented , as depicted by the wavefunction data structure in fig .  </S>",
    "<S> [ fig1 ] .             </S>",
    "<S> implementation of the algorithm relies on a sliced distribution of data among the nodes , as explained in fig .  </S>",
    "<S> [ fig2 ] . </S>",
    "<S> this successfully solves the problem of large ram consumption of 3d codes , which arises even for moderate grid sizes . </S>",
    "<S> however , it does not solve the question of data transposition between the nodes . in order to implement the most effective ( gcd = 1 ) transposition algorithm according to ref .  </S>",
    "<S> @xcite , we had to carry out block distribution of data within one data slice contained on a single node . </S>",
    "<S> this block distribution of data was done implicitly , i.e. , data on one node have been put in a single 1d array ( psi ) of contiguous memory , in which z - dimension has stride 1 , y - dimension has stride nz , and x - dimension has stride ny*nz . </S>",
    "<S> this is different from previous implementation of the programs , where the wavefunction was represented by an explicit 3d array . </S>",
    "<S> this change was also introduced in order to more easily form user mpi datatypes , which allow for implicit block distribution of data , and represent 3d blocks of data within 1d data array . </S>",
    "<S> these blocks are then swapped between nodes , effectively performing the transposition in x - y and y - x directions .    together with transposition of blocks between the nodes </S>",
    "<S> , the block data also have to be redistributed . to illustrate how this works , </S>",
    "<S> let us consider example shown in fig .  </S>",
    "<S> [ fig1](a ) , where one data block has size ( nx / gisze)*(ny / gsize)*nz . </S>",
    "<S> it represents one 3d data block , swapped between two nodes of a cluster ( through one non - blocking mpi_isend and one mpi_ireceive operation ) , containing ( nx / gsize)*(ny / gsize ) 1d rods of contiguous nz data . </S>",
    "<S> these rods themselves need to be transposed within the transposed block as well . </S>",
    "<S> this means that two levels of transpositions need to be performed . at a single block level </S>",
    "<S> , rods have to be transposed ( as indicated in upper left corner of fig .  </S>",
    "<S> [ fig1](a ) for sending index type and in fig .  </S>",
    "<S> [ fig1](b ) for receiving index type ) . </S>",
    "<S> second level is transposition of blocks between different nodes , which is depicted by blue arrows connecting different blocks in fig .  </S>",
    "<S> [ fig1 ] .    </S>",
    "<S> the above described transposition is applied whenever needed in the functions calclux and calcmuen , which require calculations to be done on the whole range of data in x - dimension . when performing renormalization of the wavefunction or calculation of its norm , root - mean - square radius , chemical potential , and energy , collective operations mpi_gather and mpi_bcast are also used .          </S>",
    "<S> figures  [ fig3 ] and [ fig4 ] show the scalability results obtained for hybrid versions of programs for small and large grid sizes as a function of number of mpi nodes used . </S>",
    "<S> the baseline for calculation of speedups in the execution time for small grid sizes are previous , purely openmp programs , while for large grid sizes , which can not fit onto a single node , the baseline are hybrid programs with minimal configuration runs on 8 nodes . </S>",
    "<S> the figures also show efficacies , defined as percentages of measured speedups compared to the ideal ones . </S>",
    "<S> we see that an excellent scalability ( larger than 80% compared to the ideal one ) can be obtained for up to 32 nodes . </S>",
    "<S> the tests have been performed on a cluster with nodes containing 2 x 8-core sandy bridge xeon 2.6 ghz processors with 32 gb of ram and infiniband qdr ( quad data rate , 40 gbps ) interconnect . </S>",
    "<S> we stress that the scalability depends greatly on the ratio between the calculation and communication time per iteration , and has to be studied for a particular type of processors and interconnect technology .     </S>",
    "<S> + _ additional comments : _ this package consists of 2 programs , see program title above . both are hybrid , threaded and distributed ( openmp / mpi parallelized ) . for the particular purpose of each program , see descriptions below .     + _ running time : _ all running times given in descriptions below refer to programs compiled with openmpi / gcc compiler and executed on 8 to 32 nodes with 2 x 8-core sandy bridge xeon 2.6 ghz processors with 32 gb of ram and infiniband qdr interconnect . with the supplied input files for small grid sizes , running wallclock times of several minutes </S>",
    "<S> are required on 8 to 10 mpi nodes .     </S>",
    "<S> + _ special features : _ ( 1 ) since the condensate wavefunction data are distributed among the mpi nodes , when writing wavefunction output files each mpi process saves its data into a separate file , to avoid i / o issues . concatenating </S>",
    "<S> the corresponding files from all mpi processes will created the complete wavefunction file . </S>",
    "<S> ( 2 ) due to a known bug in openmpi up to version 1.8.4 , allocation of memory for indexed datatype on a single node for large grids ( such as 800x640x480 ) may fail . </S>",
    "<S> the fix for this bug is already in 3c489ea branch and is fixed in openmpi as of version 1.8.5 .     </S>",
    "<S> + program summary ( i )     + _ program title : _ </S>",
    "<S> imagtime3d - hyb . </S>",
    "<S> + _ title of electronic files : _ imagtime3d - hyb.c , imagtime3d - hyb.h . </S>",
    "<S> + _ computer : _ any modern computer with c language , openmp- and mpi - capable compiler installed . + _ ram memory requirements : _ 300 mbytes of ram for a small grid size 240x200x160 , and scales with nx*ny*nz . </S>",
    "<S> this is total amount of memory needed , and is distributed over mpi nodes used for execution . </S>",
    "<S> + _ programming language used : _ c / openmp / mpi . </S>",
    "<S> + _ typical running time : _ few minutes with the supplied input files for a small grid size 240x200x160 on 8 nodes . </S>",
    "<S> up to one hour for a large grid size 1920x1600x1280 on 32 nodes ( 1000 iterations ) . </S>",
    "<S> + _ nature of physical problem : _ this program is designed to solve the time - dependent gp nonlinear partial differential equation in three space dimensions with an anisotropic trap . </S>",
    "<S> the gp equation describes the properties of a dilute trapped bose - einstein condensate . </S>",
    "<S> + _ method of solution : _ the time - dependent gp equation is solved by the split - step crank - nicolson method by discretizing in space and time . the discretized equation is then solved by propagation in imaginary time over small time steps . </S>",
    "<S> the method yields solutions of stationary problems . </S>",
    "<S> +   + program summary ( ii )     + _ program title : _ </S>",
    "<S> realtime3d - hyb . </S>",
    "<S> + _ title of electronic files : _ realtime3d - hyb.c , realtime3d - hyb.h . </S>",
    "<S> + _ computer : _ any modern computer with c language , openmp- and mpi - capable compiler installed . + _ ram memory requirements : _ 410 mbytes of ram for a small grid size 200x160x120 , and scales with nx*ny*nz . </S>",
    "<S> this is total amount of memory needed , and is distributed over mpi nodes used for execution . </S>",
    "<S> + _ programming language used : _ c / openmp / mpi . </S>",
    "<S> + _ typical running time : _ 10 - 15 minutes with the supplied input files for a small grid size 200x160x120 on 10 nodes . </S>",
    "<S> up to one hour for a large grid size 1600x1280x960 on 32 nodes ( 1000 iterations ) . </S>",
    "<S> + _ nature of physical problem : _ this program is designed to solve the time - dependent gp nonlinear partial differential equation in three space dimensions with an anisotropic trap . </S>",
    "<S> the gp equation describes the properties of a dilute trapped bose - einstein condensate . </S>",
    "<S> + _ method of solution : _ the time - dependent gp equation is solved by the split - step crank - nicolson method by discretizing in space and time . the discretized equation is then solved by propagation in real time over small time steps . </S>",
    "<S> the method yields solutions of stationary and non - stationary problems . </S>"
  ]
}