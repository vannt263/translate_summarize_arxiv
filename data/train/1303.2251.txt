{
  "article_text": [
    "compressive sensing ( cs ) @xcite is a recently proposed concept that enables sampling below nyquist rate , without ( or with little ) sacrificing reconstruction quality . based on exploiting the signal sparsity in typical domains ,",
    "cs methods can be used in the sensing devices , such as mr imaging @xcite and ad conversion @xcite , where the devices have a high cost of acquiring each additional sample or a high requirement on time .",
    "therefore , as the sparsity level is often not known a priori , it can be very challenging to use cs in practical sensing hardware .",
    "sequential compressive sensing @xcite can effectively deal with the above problems .",
    "sequential cs considers a scenario where the observations can be obtained in sequence , and computations with observations are performed to decide whether these samples are enough .",
    "consequently , it is allowed to recover the signal either exactly or to a given tolerance from the smallest possible number of observations .",
    "there have been several recovery algorithms for sequential cs .",
    "asif @xcite solved the problem by homotopy method .",
    "garrigues @xcite discussed the lasso problem with sequential observations .",
    "this work extends a recent proposed zero - point attracting projection ( zap ) algorithm @xcite to the scenario of sequential cs .",
    "zap employs an approximate @xmath0 norm as the sparsity constraint and updates in the solution space . comparing with the existing algorithms , it needs fewer measurements and lower computation complexity .",
    "therefore the new algorithm can provide a much more appropriate solution for practical sensing devices , which is validated by numerical simulations .",
    "suppose @xmath1 is an unknown sparse signal , which is @xmath2-length but has only @xmath3 nonzero entries , where @xmath4 .",
    "in their ice - breaking contributions , candes et al suggested to measure @xmath1 with under - determined observations , i.e. @xmath5 , where @xmath6 consists of random entries and has much fewer rows than columns .",
    "they also proved that @xmath0 norm or @xmath7 norm constraint optimization can successfully recover the unknown signal with overwhelming probability , @xmath8 there are many methods proposed to solve ( [ problem ] ) , of which concerned in this work is zap .",
    "zap iteratively searches the sparsest result in solution space .",
    "the recursion starts from the least square optimal solution , @xmath9 , where @xmath10 denotes the pseudo - inverse matrix of @xmath11 . in the @xmath12th iteration ,",
    "the solution is first updated along the negative gradient direction of a sparse penalty , @xmath13 where @xmath14 denotes a sparse constraint function and @xmath15 denotes the step size . in the reference , an approximate @xmath0 norm is employed and the corresponding @xmath16th entry of @xmath17 is @xmath18 where @xmath19 is a controlling parameter and it is readily recognized that the penalty tends to @xmath0 norm as @xmath19 approaches to infinity . then @xmath20 is projected back to the solution space to satisfy the observation constraint , @xmath21 where @xmath22 is defined as projection matrix and @xmath23 .",
    "equation ( [ za ] ) appears that an attractor locates at the zero - point is pulling the iterative solution to be sparser , as explains the first part of the algorithm s name .",
    "the last part comes from ( [ projection ] ) , which means that @xmath20 is projected back to the solution space .      imagining a scenario that the samples are measured in realtime . at time",
    "@xmath24 , an @xmath24-length measurement vector @xmath25 is collected and utilized to solve the sparsest solution by ( [ problem ] ) .",
    "if the available measurements are not enough to recover the original sparse signal , a new sample @xmath26 is generated at time @xmath27 , where @xmath28 denotes the sampling weight vector .",
    "thus the problem becomes solving @xmath29 , where @xmath30,\\qquad { \\bf a}_{m+1}=\\left[{\\bf a}_m \\atop { \\bf a}_{m+1}^{\\rm t}\\right].\\ ] ] obviously , it is a waste of resources if the recovery algorithm is re - initialized without the utilization of earlier estimate , i.e. the available result at time @xmath24 .",
    "consequently , the basic aim of sequential compressive signal reconstruction is to find an effective method of refining @xmath31 based on the information of @xmath32 .",
    "for conciseness , the detailed iteration procedure of online zap is provided in tab.[tab1 ] .",
    "it can be seen that online zap has two recursions .",
    "the inner iteration is to update the solution by zap with the given measurements .",
    "the outer iteration is for sequential input . in order to improve the performance ,",
    "several techniques are used in online zap and they are discussed in the following subsections .",
    ".the procedure of online zap [ cols= \" < \" , ]      zap works in an iterative way to produce a sparse solution via recursion in the solution space . in the online scenario , the previous estimate can be used to initialize the incoming iteration , i.e. @xmath33 , where @xmath34 denotes the maximum iteration number at time @xmath24 .",
    "the pseudo - inverse matrix @xmath35 plays an important role in the recursion of ( [ projection ] ) . considering the high computational cost of matrix inverse operation , @xmath35 is generally prepared before iterations .",
    "however , in the online scenario , @xmath36 becomes time - dependent and @xmath37 need to be recalculated in each time instant . in order to reduce the complexity ,",
    "the pseudo - inverse matrix is updated iteratively .",
    "define @xmath38 , which is already available after time @xmath24 .",
    "consequently , as the new sample is arriving , using basic algebra one has the recursion @xmath39\\left[{\\bf a}_m^{\\rm t } \\ ; { \\bf a}_{m+1}\\right]\\right]^{-1 }      = \\left[\\begin{matrix}{\\bf\\gamma}_{m}^{-1 } & { \\boldsymbol\\alpha}_m \\\\ { \\boldsymbol\\alpha}_m^{\\rm t } & \\beta_m\\end{matrix}\\right]^{-1}\\nonumber\\\\      & = \\left[\\begin{matrix}{\\bf\\gamma}_m+\\theta_m{\\bf\\gamma}_m     { \\boldsymbol\\alpha}_m{\\boldsymbol\\alpha}_m^{\\rm t}{\\bf\\gamma}_m & -\\theta_m{\\bf\\gamma}_m{\\boldsymbol\\alpha}_m\\\\     -\\theta_m{\\boldsymbol\\alpha}_m^{\\rm t}{\\bf\\gamma}_m & \\theta_m\\end{matrix}\\right],\\label{updategamma}\\end{aligned}\\ ] ] where @xmath40      as the step size in gradient descent iterations , the parameter @xmath15 controls a tradeoff between the speed of convergence and the accuracy of the solution . in order to improve the performances of the proposed algorithm ,",
    "the idea of variable step size is taken into consideration .",
    "the control scheme is rather direct : @xmath15 is initialized to be a large value after new sample arrived , and reduced by a factor as long as the iteration is convergent .",
    "the reduction is repeated several times until @xmath15 is sufficiently small .",
    "since the algorithm has two recursions , we employ @xmath41 and @xmath42 to denote the decreasing speed of outer and inner iteration , respectively . in addition",
    ", @xmath15 is no longer decreased when the step size is rather small .",
    "there are two kinds of recursions requiring stop rules in the online zap algorithm .",
    "firstly , after the @xmath24th sample arrived , @xmath43 iterates with @xmath12 to produce the best estimate based on the @xmath24 measurements .",
    "the inner iteration should stop after the algorithm reaches steady state , which means the sparsity penalty starts increasing .",
    "consequently , the inner recursion stops ( a ) when the number of reductions of @xmath15 reaches one-@xmath44th of its initial value or ( b ) when the number of iterations reaches the bound @xmath45 .    secondly , as soon as the sparse signal is successfully reconstructed , the following samples are no longer necessary and the sensing procedure stops .",
    "therefore , the outer recursion stops when the estimate error is below a particular value @xmath46 .",
    "computer simulation is presented in this section to verify the performance of the proposed algorithm compared with typical sequential cs reconstruction algorithm for solving bpdn problem @xcite , whose matlab code can be downloaded from the website @xcite . in the following experiment ,",
    "the entries of each row of @xmath6 are independently generated from normal distribution .",
    "the locations of @xmath3 nonzero coefficients of sparse signal @xmath1 are randomly chosen with uniform distribution @xmath47 $ ] .",
    "the corresponding nonzero coefficients are gaussian with mean zero and unit variance .",
    "the system parameters are @xmath48 and @xmath49 .",
    "the number of measurements @xmath50 increases form @xmath51 to @xmath52 .",
    "the parameters for bpdn are set as the recommended values by the author .",
    "the parameters for online zap are @xmath53 , @xmath54 , @xmath55 , @xmath56 , @xmath57 , @xmath58 .",
    "the simulation is repeated ten times , then mean square derivation ( msd ) between the original signal and reconstruction signal as well as the average running time calculated .",
    "figure [ varm ] shows msd curve according to @xmath50 . as can be seen ,",
    "the performance of zap is better than that of bpdn .",
    "when the sparse signal is recovered successfully , the number of measurements bpdn needs is larger than @xmath59 , while the number zap algorithm needs is less than 80 .",
    "figure [ time ] demonstrates the cpu running time as @xmath50 increases .",
    "again , zap has the better performance .",
    "the cpu time of bpdn is twice than that of zap for successful recovery ( according to fig.[varm ] , here @xmath50 is chosen as @xmath59 for comparison ) .",
    "we have introduced in this paper a new online signal reconstruction algorithm for sequential compressive sensing .",
    "the proposed algorithm extends zap to sequential scenario . and",
    "in order to improve the performance , some methods , including the warm start and variable step size , are adopted .",
    "the final experiment indicates that the proposed algorithm needs less measurements and less cpu time than the reference algorithm .",
    "d.  l.  donoho , `` compressed sensing , '' _ ieee trans . on information theory _ , 52(4 ) , pp.1289 - 1306 , april 2006 .",
    "e.  candes , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "inform . theory _ , 52:489 - 509 , 2006 . m.  lustig , d.  donoho , and j.  pauly , `` sparse mri : the application of compressed sensing for rapid mr imaging , '' _ magnetic resonance in medicine _ , 58(6 ) pp .",
    "1182 - 1195 , december 2007 . m.  mishali , y.  c.  eldar , and j.  a.  tropp , `` efficient sampling of sparse wideband analog signals , '' _ ccit report # 705 _ , october 2008 .",
    "d.  malioutov , s.  sanghavi , and a.  willsky , `` compressed sensing with sequential observations , '' _ icassp _ , pp .",
    "3357 - 3360 , april 2008 .",
    "j. jin , y. gu , and s. mei , `` a stochastic gradient approach on compressive sensing signal reconstruction based on adaptive filtering framework , '' _ ieee journal of selected topics in signal processing _ , 4(2 ) , pp.409 - 420 , 2010 ."
  ],
  "abstract_text": [
    "<S> sequential compressive sensing , which may be widely used in sensing devices , is a popular topic of recent research . </S>",
    "<S> this paper proposes an online recovery algorithm for sparse approximation of sequential compressive sensing . </S>",
    "<S> several techniques including warm start , fast iteration , and variable step size are adopted in the proposed algorithm to improve its online performance . </S>",
    "<S> finally , numerical simulations demonstrate its better performance than the relative art .    </S>",
    "<S> * keywords : * compressive sensing , sparse signal recovery , sequential , online algorithm , zero - point attracting projection </S>"
  ]
}