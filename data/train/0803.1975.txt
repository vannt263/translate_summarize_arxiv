{
  "article_text": [
    "compression of matrices over fields of characteristic 2 is naturally made via the binary representation of machine integers @xcite .",
    "the fflas / ffpack project has demonstrated the need of a wrapping of cache - aware routines for efficient small finite field linear algebra @xcite .",
    "therefore , a conversion between a modular representation of prime fields of any ( small ) characteristic and e.g. floating points can be performed via the homomorphism to the integers @xcite . in @xcite",
    "it is proposed to transform polynomial over a prime field into a @xmath0-adic representation where @xmath0 is an integer than the field characteristic .",
    "we call this transformation dqtfor discrete q - adic transform . with some care , in particular on the size of @xmath0 ,",
    "it is possible to map the polynomial operations into the floating point arithmetic realization of this @xmath0-adic representation and convert back using an inverse dqt .",
    "efficient matrix computations over very small finite fields of characteristic other than two are required e.g. to study strongly regular graphs @xcite , in order to prove / disprove and help in the comprehension of the conjectures of @xcite .    in this note",
    "we propose to use this fast polynomial arithmetic within machine words to compute dot products .",
    "we show in section [ sec : dp ] how to recover a dot product of size @xmath1 can be recovered from the single coefficient of degree @xmath2 of a polynomial product .",
    "whenever the prime modulus is small enough this enables to compute several accumulations of binary products in a single machine operation",
    ". then we propose in section [ sec : mixed ] an alternative matrix multiplication using multiplication of a compressed word by a single residue .",
    "the latter requires also a simultaneous modular reduction , called redq in @xcite . in general , the prime field , the size of matrices and the available mantissa are given .",
    "this gives some constraints on the possible choices of @xmath0 and @xmath2 . in both cases",
    "anyway , we show that these compression techniques represent a speed - up factor of up to the number @xmath1 of residues stored in the compressed format .",
    "suppose that @xmath3 and @xmath4 are two polynomials in @xmath5 $ ] .",
    "one can perform the dot product @xmath6 by extracting the coefficient of degree @xmath2 of @xmath7 .",
    "the idea here , as in @xcite , is to replace @xmath8 by an integer @xmath0 , usually a power of 2 in order to speed up conversions .",
    "thus the vectors of residues @xmath9 $ ] and @xmath10 $ ] are stored respectively as @xmath11 and the _ reverse _ @xmath12 .",
    "now for matrix multiplication @xmath13 one wishes to convert a whole row of the left @xmath14 matrix @xmath15 and a whole column of the right @xmath16 matrix @xmath17 .",
    "thus @xmath15 is transformed into a @xmath18 ` compressedrowmatrix ` , @xmath19 and @xmath17 is transformed into a @xmath20 ` compressedrowmatrix ` , @xmath21 .",
    "therefore the matrix multiply @xmath22 can gain _ a factor of @xmath1 _ over the multiplication of @xmath13 , for classical multiplication as shown on the @xmath23 example below where the matrix product @xmath24 is performed via integer multiplications .",
    "the precise gain will be given in table [ tab : gains ] .",
    "the result matrix @xmath26 is @xmath27 .",
    "thus in order to compare similar computations one has either to consider multiplication of compressed matrices which is then the procedure @xmath28 or to consider multiplication of normal matrices via compression and thus the procedure @xmath29      note that the last column of @xmath19 and the last row of @xmath17 might not have @xmath1 elements if @xmath30 .",
    "thus one has to artificially append some zeroes to the converted values .",
    "on @xmath31 this means just do nothing . on the reversed @xmath32 this means multiplying by @xmath0 several times .      for the results to be correct",
    "the inner dot product must not exceed @xmath0 . with a positive modular representation mod @xmath33 ( i.e. integers from @xmath34 to @xmath35 ) ,",
    "this means that @xmath36 .",
    "moreover , we would like to use delayed reductions on the intermediate results and thus accumulate the @xmath37 before any modular reduction .",
    "it is thus possible to perform matrix multiplications of with common dimension @xmath38 as long as : @xmath39      if the product @xmath37 is performed with floating point arithmetic we just need that the coefficient of degree @xmath2 remains fully in the mantissa @xmath40 .",
    "write @xmath41 , the latter means that @xmath42 , _ and @xmath42 only _ , must remain lower that @xmath43 .",
    "it could then be exactly recovered by multiplication of @xmath44 by the correctly precomputed and rounded inverse of @xmath45 and floored , as shown e.g. in ( * ? ? ?",
    "* lemma 2 ) .        .... element & init ( element & rem , const double dp ) const {      double r = dp ;          //",
    "multiply by the inverse of q^d with correct rounding          r * = _ inverseqto_d ;           // now we just need the part less than q=2^t          unsigned long rl ( static_cast < unsigned long>(r ) ) ;          rl & = _ qminusone ;          // and we finally perform a single modular reduction           rl % = _ modulus ;          return rem = static_cast < element>(rl ) ; } ....    note that one can avoid the multiplication by the inverse of @xmath0 when @xmath49 : by adding @xmath50 to the final result one is guaranteed that the @xmath51 high bits represent exactly the @xmath1 high coefficients .",
    "on the one hand , the floating point multiplication can be replaced by an addition . on the other hand , this doubles the size of the dot product and thus reduces by a factor of @xmath52{2}$ ] the largest possible dot product size @xmath38 .",
    "the difference there will mainly be on the number and on the kind of modular reductions .",
    "since the @xmath55 reduction is faster than @xmath56 classical reductions , see @xcite , and since @xmath57 and @xmath58 are roughly the same operations , the best algorithm would then be one of the left , right or full compression .",
    "for example , with algorithm ( [ eq : compc ] ) on matrices of sizes @xmath59 it took @xmath60 seconds to perform the matrix multiplication modulo @xmath61 and @xmath62 seconds to convert the resulting @xmath63 matrix .",
    "this is less than @xmath64% . for @xmath65 matrices",
    "it takes less than @xmath66 seconds to perform the multiplication and roughly @xmath67 seconds for the conversions .",
    "there , the conversions count for @xmath68 .",
    "the full compression algorithm seems the better candidate for the locality and fast matrix multiplication reasons above ; howbeit the compression factor is an integer , depending on the flooring of either @xmath69 or @xmath70 .",
    "thus there are matrix dimensions for which the compression factor of e.g. the right compression will be larger than the square of the compression factor of the full compression .",
    "there the right compression will have some advantage over the full compression ."
  ],
  "abstract_text": [
    "<S> we propose to store several integers modulo a small prime into a single machine word . </S>",
    "<S> modular addition is performed by addition and possibly subtraction of a word containing several times the modulo . </S>",
    "<S> modular multiplication is not directly accessible but modular dot product can be performed by an integer multiplication by the reverse integer . </S>",
    "<S> modular multiplication by a word containing a single residue is a also possible . </S>",
    "<S> therefore matrix multiplication can be performed on such a compressed storage . </S>",
    "<S> we here give bounds on the sizes of primes and matrices for which such a compression is possible . </S>",
    "<S> we also explicit the details of the required compressed arithmetic routines . </S>"
  ]
}