{
  "article_text": [
    "consider the problem of regret minimization in non - stochastic multi - armed bandits , as defined in the classic paper of @xcite .",
    "this sequential decision - making problem can be formalized as a repeated game between a _ learner _ and an _ environment _ ( sometimes called the _ adversary _ ) . in each round @xmath2 ,",
    "the two players interact as follows : the learner picks an _ arm _",
    "( also called an _ action _ ) @xmath3 = { \\left\\{1,2,\\dots , k\\right\\}}$ ] and the environment selects a loss function @xmath4 { \\rightarrow}[0,1]$ ] , where the loss associated with arm @xmath5 $ ] is denoted as @xmath6 .",
    "subsequently , the learner incurs and observes the loss @xmath7 . based solely on these observations ,",
    "the goal of the learner is to choose its actions so as to accumulate as little loss as possible during the course of the game . as traditional in the online learning literature @xcite",
    ", we measure the performance of the learner in terms of the _ regret _ defined as @xmath8}\\sum_{t=1}^t { \\ell}_{t , i}.\\ ] ] we say that the environment is _ oblivious _ if it selects the sequence of loss vectors irrespective of the past actions taken by the learner , and _ adaptive _ ( or _ non - oblivious _ ) if it is allowed to choose @xmath9 as a function of the past actions @xmath10 .",
    "an equivalent formulation of the multi - armed bandit game uses the concept of _ rewards _ ( also called _ gains _ or _ payoffs _ ) instead of losses : in this version , the adversary chooses the sequence of _ reward functions _",
    "@xmath11 with @xmath12 denoting the reward given to the learner for choosing action @xmath13 in round @xmath14 . in this game",
    ", the learner aims at maximizing its total rewards .",
    "we will refer to the above two formulations as the _ loss game _ and the _ reward game _",
    ", respectively .",
    "our goal in this paper is to construct algorithms for the learner that guarantee that the regret grows sublinearly .",
    "since it is well known that no deterministic learning algorithm can achieve this goal @xcite , we are interested in _ randomized _ algorithms .",
    "accordingly , the regret @xmath15 then becomes a random variable that we need to bound in some probabilistic sense .",
    "most of the existing literature on non - stochastic bandits is concerned with bounding the _ pseudo - regret _ ( or _ weak regret _ ) defined as @xmath16}{\\mathbb{e}\\left[\\sum_{t=1}^t { \\ell}_{t , i_t } - \\sum_{t=1}^t { \\ell}_{t , i}\\right]},\\ ] ] where the expectation integrates over the randomness injected by the learner .",
    "proving bounds on the actual regret that hold with high probability is considered to be a significantly harder task that can be achieved by serious changes made to the learning algorithms and much more complicated analyses .",
    "one particular common belief is that in order to guarantee high - confidence performance guarantees , the learner can not avoid repeatedly sampling arms from a uniform distribution , typically @xmath17 times @xcite .",
    "it is easy to see that such _",
    "explicit exploration _ can impact the empirical performance of learning algorithms in a very negative way if there are many arms with high losses : even if the base learning algorithm quickly learns to focus on good arms , explicit exploration still forces the regret to grow at a steady rate . as a result , algorithms with high - probability performance guarantees tend to perform poorly even in very simple problems @xcite .    in the current paper",
    ", we propose an algorithm that guarantees strong regret bounds that hold with high probability without the explicit exploration component .",
    "one component that we preserve from the classical recipe for such algorithms is the _ biased estimation of losses _ , although our bias is of a much more delicate nature , and arguably more elegant than previous approaches . in particular , we adopt the _ implicit exploration _",
    "( ix ) strategy first proposed by @xcite for the problem of online learning with side - observations . as we show in the current paper , this simple loss - estimation strategy allows proving high - probability bounds for a range of non - stochastic bandit problems including bandits with expert advice , tracking the best arm and bandits with side - observations .",
    "our proofs are arguably cleaner and less involved than previous ones , and very elementary in the sense that they do not rely on advanced results from probability theory like freedman s inequality @xcite .",
    "the resulting bounds are tighter than all previously known bounds and hold simultaneously for all confidence levels , unlike most previously known bounds @xcite .",
    "for the first time in the literature , we also provide high - probability bounds for anytime algorithms that do not require prior knowledge of the time horizon @xmath1 .",
    "a minor conceptual improvement in our analysis is a direct treatment of the loss game , as opposed to previous analyses that focused on the reward game , making our treatment more coherent with other state - of - the - art results in the online learning literature .",
    "the rest of the paper is organized as follows . in section  [ sec : est ] , we review the known techniques for proving high - probability regret bounds for non - stochastic bandits and describe our implicit exploration strategy in precise terms .",
    "section  [ sec : apps ] states our main result concerning the concentration of the ix loss estimates and shows applications of this result to several problem settings .",
    "finally , we conduct a set of simple experiments to illustrate the benefits of implicit exploration over previous techniques in section  [ sec : exp ] .",
    "most principled learning algorithms for the non - stochastic bandit problem are constructed by using a standard online learning algorithm such as the exponentially weighted forecaster ( @xcite ) or follow the perturbed leader ( @xcite ) as a black box , with the true ( unobserved ) losses replaced by some appropriate estimates .",
    "one of the key challenges is constructing reliable estimates of the losses @xmath6 for all @xmath5 $ ] based on the single observation @xmath7 .",
    "following @xcite , this is traditionally achieved by using importance - weighted loss / reward estimates of the form @xmath18 where @xmath19}$ ] is the probability that the learner picks action @xmath13 in round @xmath14 , conditioned on the observation history @xmath20 of the learner up to the beginning of round @xmath14 .",
    "it is easy to show that these estimates are unbiased for all @xmath13 with @xmath21 in the sense that @xmath22 for all such @xmath13 .    for concreteness ,",
    "consider the exp3algorithm of @xcite as described in ( * ? ? ?",
    "* section  3 ) . in every round @xmath14",
    ", this algorithm uses the loss estimates defined in equation   to compute the _ weights _",
    "@xmath23 for all @xmath13 and some positive parameter @xmath24 that is often called the _ learning rate_. having computed these weights , exp3draws arm @xmath25 with probability proportional to @xmath26 . relying on the unbiasedness of the estimates   and an optimized setting of @xmath24",
    ", one can prove that exp3enjoys a pseudo - regret bound of @xmath27 .",
    "however , the fluctuations of the loss estimates around the true losses are too large to permit bounding the true regret with high probability . to keep these fluctuations under control , @xcite propose to use the _ biased reward - estimates _",
    "@xmath28 with an appropriately chosen @xmath29 . given these estimates",
    ", the exp3.palgorithm of @xcite computes the weights @xmath30 for all arms @xmath13 and then samples @xmath31 according to the distribution @xmath32 where @xmath33 $ ] is the exploration parameter .",
    "the argument for this _ explicit exploration _ is that it helps to keep the range ( and thus the variance ) of the above reward estimates bounded , thus enabling the use of ( more or less ) standard concentration results . in particular ,",
    "the key element in the analysis of exp3.p@xcite is showing that the inequality @xmath34 holds simultaneously for all @xmath13 with probability at least @xmath35 .",
    "in other words , this shows that the cumulative estimates @xmath36 are upper confidence bounds for the true rewards @xmath37 .    in the current paper",
    ", we propose to use the loss estimates defined as @xmath38 for all @xmath13 and an appropriately chosen @xmath39 , and then use the resulting estimates in an exponential - weights algorithm scheme without any explicit exploration .",
    "loss estimates of this form were first used by @xcite  following them , we refer to this technique as _ implicit exploration _ , or , in short , ix . in what follows , we argue that that ix as defined above achieves a similar variance - reducing effect as the one achieved by the combination of explicit exploration and the biased reward estimates of equation  . in particular , we show that the ix estimates   constitute a lower confidence bound for the true losses which allows proving high - probability bounds for a number of variants of the multi - armed bandit problem .",
    "in this section , we present a concentration result concerning the ix loss estimates of equation  , and apply this result to prove high - probability performance guarantees for a number of non - stochastic bandit problems .",
    "the following lemma states our concentration result in its most general form :    [ lem : fixbound ] let @xmath40 be a _",
    "fixed _ non - increasing sequence with @xmath41 and let @xmath42 be nonnegative @xmath20-measurable random variables satisfying @xmath43 for all @xmath14 and @xmath13 .",
    "then , with probability at least @xmath35 , @xmath44    a particularly important special case of the above lemma is the following :    [ cor : allbound ] let @xmath45 for all @xmath14 .",
    "with probability at least @xmath35 , @xmath46 simultaneously holds for all @xmath5 $ ] .",
    "this corollary follows from applying lemma  [ lem : fixbound ] to the functions @xmath47 for all @xmath48 and applying the union bound .",
    "the full proof of lemma  [ lem : fixbound ] is presented in the appendix . for didactic purposes ,",
    "we now present a direct proof for corollary  [ cor : allbound ] , which is essentially a simpler version of lemma  [ lem : fixbound ] .    for convenience , we will use the notation @xmath49 .",
    "first , observe that @xmath50 where the first step follows from @xmath51 $ ] and last one from the elementary inequality @xmath52 that holds for all @xmath53 .",
    "using the above inequality , we get that @xmath54 } \\le & { \\mathbb{e}\\left[\\left.1+\\beta { { \\widehat}{\\ell}}_{t , i}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\le 1+\\beta { \\ell}_{t , i } \\le \\exp{\\left(\\beta { \\ell}_{t , i}\\right ) } ,   \\end{split}\\ ] ] where the second and third steps are obtained by using @xmath55 } \\le { \\ell}_{t , i}$ ] that holds by definition of @xmath56 , and the inequality @xmath57 that holds for all @xmath58 . as a result ,",
    "the process @xmath59 is a supermartingale with respect to @xmath60 : @xmath61 } \\le z_{t-1}$ ] .",
    "observe that , since @xmath62 , this implies @xmath63 } \\le { \\mathbb{e}\\left[z_{t-1}\\right ] } \\le \\ldots \\le 1 $ ] , and thus by markov s inequality , @xmath64 } & \\le { \\mathbb{e}\\left[\\exp{\\left(\\beta   \\sum_{t=1}^t{\\bigl({{\\widetilde}{\\ell}}_{t , i }   - { \\ell}_{t , i}\\bigr)}\\right)}\\right ] } \\cdot \\exp(-\\beta \\varepsilon ) \\le",
    "\\exp(-\\beta \\varepsilon ) \\end{split}\\ ] ] holds for any @xmath65 .",
    "the statement of the lemma follows from solving @xmath66 for @xmath67 and using the union bound over all arms @xmath13 .    in what follows , we put lemma  [ lem : fixbound ] to use and prove improved high - probability performance guarantees for several well - studied variants of the non - stochastic bandit problem ,",
    "namely , the multi - armed bandit problem with expert advice , tracking the best arm for multi - armed bandits , and bandits with side - observations . the general form of lemma  [ lem : fixbound ] will allow us to prove high - probability bounds for anytime algorithms that can operate without prior knowledge of @xmath1 . for clarity , we will only provide such bounds for the standard multi - armed bandit setting ; extending the derivations to other settings is left as an easy exercise . for all algorithms ,",
    "we prove bounds that scale linearly with @xmath68 and hold simultaneously for all levels @xmath69 .",
    "note that this dependence can be improved to @xmath70 for a fixed confidence level @xmath69 , if the algorithm can use this @xmath69 to tune its parameters .",
    "this is the way that table  [ tab : results ] presents our new bounds side - by - side with the best previously known ones .",
    ".our results compared to the best previously known results in the four settings considered in sections  [ sec : exp3][sec : exp3ix ] .",
    "see the respective sections for references and notation . [",
    "cols=\"<,^,^ \" , ]      r0.5    * parameters : * @xmath71 , @xmath72 . +",
    "* initialization : * @xmath73 .",
    "+ * for @xmath74 , * repeat * *    1 .",
    "2 .   draw @xmath76 .",
    "3 .   observe loss @xmath7 .",
    "@xmath77 for all @xmath5 $ ] .",
    "@xmath78 for all @xmath79 $ ] .    in this section ,",
    "we propose a variant of the exp3algorithm of @xcite that uses the ix loss estimates : exp3-ix .",
    "the algorithm in its most general form uses two nonincreasing sequences of nonnegative parameters : @xmath80 and @xmath81 . in every round , exp3-ixchooses action @xmath82 with probability proportional to @xmath83 without mixing any explicit exploration term into the distribution . a fixed - parameter version of exp3-ixis presented as algorithm  [ alg : expix ] .    our theorem below states a high - probability bound on the regret of exp3-ix",
    "notably , our bound exhibits the best known constant factor of @xmath84 in the leading term , improving on the factor of @xmath85 due to @xcite .",
    "the best known leading constant for the pseudo - regret bound of exp3is @xmath86 , also proved in @xcite .",
    "[ thm : main ] fix an arbitrary @xmath87 . with @xmath88 for all @xmath14 , exp3-ix guarantees @xmath89 with probability at least @xmath35 . furthermore",
    ", setting @xmath90 for all @xmath14 , the bound becomes @xmath91    let us fix an arbitrary @xmath92 . following the standard analysis of exp3 in the loss game and nonincreasing learning rates @xcite , we can obtain the bound @xmath93 for any @xmath48 .",
    "now observe that @xmath94 similarly , @xmath95 holds by the boundedness of the losses .",
    "thus , we get that @xmath96 holds with probability at least @xmath97 , where the last line follows from an application of lemma  [ lem : fixbound ] with @xmath98 for all @xmath99 and taking the union bound . by taking @xmath100 and @xmath101 , and using the boundedness of the losses ,",
    "we obtain @xmath102 the statements of the theorem then follow immediately , noting that @xmath103 .",
    "we now turn to the setting of multi - armed bandits with expert advice , as defined in @xcite , and later revisited by @xcite and @xcite . in this setting",
    ", we assume that in every round @xmath2 , the learner observes a set of @xmath104 probability distributions @xmath105^k$ ] over the @xmath106 arms , such that @xmath107 for all @xmath108 $ ] .",
    "we assume that the sequences @xmath109 are measurable with respect to @xmath110 .",
    "the @xmath111@xmath112of these vectors represent the probabilistic advice of the corresponding @xmath111@xmath112  _ expert_. the goal of the learner in this setting is to pick a sequence of arms so as to minimize the regret against the best expert : @xmath113 } \\sum_{t=1}^t \\sum_{i=1}^k { \\xi}_{t , i}(n){\\ell}_{t , i } { \\rightarrow}\\min.\\ ] ] to tackle this problem , we propose a modification of the exp4algorithm of @xcite that uses the ix loss estimates  , and also drops the explicit exploration component of the original algorithm . specifically , exp4-ixuses the loss estimates defined in equation   to compute the weights @xmath114 for every expert @xmath108 $ ] , and then draw arm @xmath13 with probability @xmath115 .",
    "we now state the performance guarantee of exp4-ix .",
    "our bound improves the best known leading constant of @xmath116 due to @xcite to @xmath84 and is a factor of @xmath117 worse than the best known constant in the pseudo - regret bound for exp4@xcite .",
    "the proof of the theorem is presented in the appendix .",
    "[ thm : experts ] fix an arbitrary @xmath87 and set @xmath118 for all @xmath14 .",
    "then , with probability at least @xmath35 , the regret of exp4-ixsatisfies @xmath119      in this section , we consider the problem of competing with sequences of actions .",
    "similarly to @xcite , we consider the class of sequences that switch at most @xmath120 times between actions .",
    "we measure the performance of the learner in this setting in terms of the regret against the best sequence from this class @xmath121^t$ ] , defined as @xmath122 similarly to @xcite , we now propose to adapt the fixed share algorithm of @xcite to our setting . our algorithm , called exp3-six , updates a set of weights @xmath123 over the arms in a recursive fashion . in the first round , exp3-sixsets @xmath124 for all @xmath13 . in the following rounds , the weights are updated for every arm @xmath13 as @xmath125 in round @xmath14 , the algorithm draws arm @xmath82 with probability @xmath126 .",
    "below , we give the performance guarantees of exp3-six .",
    "note that our leading factor of @xmath84 again improves over the best previously known leading factor of @xmath127 , shown by @xcite .",
    "the proof of the theorem is given in the appendix .",
    "[ thm : tracking ] fix an arbitrary @xmath87 and set @xmath128 and @xmath129 , where @xmath130 . then , with probability at least @xmath35 , the regret of exp3-six satisfies @xmath131      let us now turn to the problem of online learning in bandit problems in the presence of side observations , as defined by @xcite and later elaborated by @xcite . in this setting ,",
    "the learner and the environment interact exactly as in the multi - armed bandit problem , the main difference being that in every round , the learner observes the losses of some arms other than its actually chosen arm @xmath31 .",
    "the structure of the side observations is described by the directed graph @xmath132 : nodes of @xmath132 correspond to individual arms , and the presence of arc @xmath133 implies that the learner will observe @xmath134 upon selecting @xmath82 .",
    "implicit exploration and exp3-ixwas first proposed by @xcite for this precise setting . to describe this variant ,",
    "let us introduce the notations @xmath135 and @xmath136}$ ] .",
    "then , the ix loss estimates in this setting are defined for all @xmath99 as @xmath137 . with these estimates at hand , exp3-ixdraws arm @xmath31 from the exponentially weighted distribution defined in equation  .",
    "the following theorem provides the regret bound concerning this algorithm .",
    "[ thm : sideobs ] fix an arbitrary @xmath87 .",
    "assume that @xmath138 and set @xmath139 , where @xmath140 is the _ independence number _ of @xmath132 .",
    "with probability at least @xmath35 , exp3-ixguarantees @xmath141    the proof of the theorem is given in the appendix . while the proof of this statement is significantly more involved than the other proofs presented in this paper , it provides a fundamentally new result .",
    "in particular , our bound is in terms of the _ independence number _ @xmath140 and thus matches the minimax regret bound proved by @xcite for this setting up to logarithmic factors .",
    "in contrast , the only high - probability regret bound for this setting due to @xcite scales with the size @xmath142 of the maximal acyclic subgraph of @xmath132 , which can be much larger than @xmath140 in general ( i.e. , @xmath142 may be @xmath143 for some graphs @xcite ) .",
    "we conduct a simple experiment to demonstrate the robustness of exp3-ixas compared to exp3and its superior performance as compared to exp3.p .",
    "our setting is a 10-arm bandit problem where all losses are independent draws of bernoulli random variables .",
    "the mean losses of arms 1 through 8 are @xmath144 and the mean loss of arm 9 is @xmath145 for all rounds @xmath2 .",
    "the mean losses of arm 10 are changing over time : for rounds @xmath146 , the mean is @xmath147 , and @xmath148 afterwards .",
    "this choice ensures that up to at least round @xmath149 , arm 9 is clearly better than other arms . in the second half of the game , arm 10",
    "starts to outperform arm 9 and eventually becomes the leader .",
    ".,title=\"fig : \" ] .,title=\"fig : \" ]    we have evaluated the performance of exp3 , exp3.pand exp3-ixin the above setting with @xmath150 and @xmath151 . for fairness of comparison , we evaluate all three algorithms for a wide range of parameters . in particular , for all three algorithms , we set a base learning rate @xmath24 according to the best known theoretical results ( * ? ? ? * theorems  3.1 and  3.3 ) and varied the multiplier of the respective base parameters between @xmath152 and @xmath153 .",
    "other parameters are set as @xmath154 for exp3-ixand @xmath155 for exp3.p .",
    "we studied the regret up to two interesting rounds in the game : up to @xmath149 , where the losses are i.i.d .",
    ", and up to @xmath1 where the algorithms have to notice the shift in the loss distributions .",
    "figure  [ fig : exp ] shows the empirical means and standard deviations over 50 runs of the regrets of the three algorithms as a function of the multipliers .",
    "the results clearly show that exp3-ixlargely improves on the empirical performance of exp3.pand is also much more robust in the non - stochastic regime than vanilla exp3 .",
    "in this paper , we have shown that , contrary to popular belief , explicit exploration is not necessary to achieve high - probability regret bounds for non - stochastic bandit problems .",
    "interestingly , however , we have observed in several of our experiments that our ix - based algorithms still draw every arm roughly @xmath156 times , even though this is not explicitly enforced by the algorithm .",
    "this suggests a need for a more complete study of the role of exploration , to find out whether pulling every single arm @xmath0 times is necessary for achieving near - optimal guarantees .",
    "one can argue that tuning the ix parameter that we introduce may actually be just as difficult in practice as tuning the parameters of exp3.p .",
    "however , every aspect of our analysis suggests that @xmath157 is the most natural choice for these parameters , and thus this is the choice that we recommend .",
    "one limitation of our current analysis is that it only permits deterministic learning - rate and ix parameters ( see the conditions of lemma  [ lem : fixbound ] ) . that is",
    ", proving adaptive regret bounds in the vein of @xcite that hold with high probability is still an open challenge .",
    "another interesting question for future study is whether the implicit exploration approach can help in advancing the state of the art in the more general setting of linear bandits .",
    "all known algorithms for this setting rely on explicit exploration techniques , and the strength of the obtained results depend crucially on the choice of the exploration distribution ( see @xcite for recent advances ) .",
    "interestingly , ix has a natural extension to the linear bandit problem . to see this ,",
    "consider the vector @xmath158 and the matrix @xmath159}$ ] .",
    "then , the ix loss estimates can be written as @xmath160 .",
    "whether or not this estimate is the right choice for linear bandits remains to be seen .",
    "finally , we note that our estimates   are certainly not the only ones that allow avoiding explicit exploration .",
    "in fact , the careful reader might deduce from the proof of lemma  [ lem : fixbound ] that the same concentration bound can be shown to hold for the alternative loss estimates @xmath161 and @xmath162 .",
    "actually , a variant of the latter estimate was used previously for proving high - probability regret bounds in the reward game by @xcite  however , their proof still relied on explicit exploration .",
    "it is not hard to verify that all the results we presented in this paper ( except theorem  [ thm : sideobs ] ) can be shown to hold for the above two estimates , too .",
    "[ [ acknowledgments ] ] acknowledgments + + + + + + + + + + + + + + +    this work was supported by inria , the french ministry of higher education and research , and by fui project herms .",
    "the author wishes to thank haipeng luo for catching a bug in an earlier version of the paper , and the anonymous reviewers for their helpful suggestions .",
    "fix any @xmath14 . for convenience , we will use the notation @xmath163 .",
    "first , observe that for any @xmath13 , @xmath164 where the first step follows from @xmath51 $ ] and last one from the elementary inequality @xmath52 that holds for all @xmath53 .",
    "define the notations @xmath165 and @xmath166 . using the above inequality",
    ", we get that @xmath167 } \\le & { \\mathbb{e}\\left[\\left.\\exp{\\left(\\sum_{i=1}^k \\frac{\\alpha_{t , i}}{\\beta_t } \\cdot   \\log{\\left(1+\\beta_t { { \\widehat}{\\ell}}_{t , i}\\right)}\\right)}\\right|{\\mathcal{f}}_{t-1}\\right ] }   \\\\ \\le & { \\mathbb{e}\\left[\\left.\\prod_{i=1}^k { \\left(1 + \\alpha_{t , i } { { \\widehat}{\\ell}}_{t , i}\\right)}\\right|{\\mathcal{f}}_{t-1}\\right ] } = { \\mathbb{e}\\left[\\left.{1 + \\sum_{i=1}^k \\alpha_{t , i } { { \\widehat}{\\ell}}_{t , i}}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\\\ \\le & { 1 + \\sum_{i=1}^k \\alpha_{t , i } { \\ell}_{t , i } } \\le \\exp{\\left(\\sum_{i=1}^k \\alpha_{t , i } { \\ell}_{t , i}\\right ) } = \\exp{\\left ( \\lambda_{t}\\right ) } ,   \\end{split}\\ ] ] where the second line follows from noting that @xmath168 , using the inequality @xmath169 that holds for all @xmath170 and @xmath171 $ ] and the identity @xmath172 that follows from the fact that @xmath173 holds whenever @xmath174 . the last line is obtained by using @xmath55 } \\le { \\ell}_{t , i}$ ] that holds by definition of @xmath56 , and the inequality @xmath57 that holds for all @xmath58 .    as a result ,",
    "the process @xmath175 is a supermartingale with respect to @xmath60 : @xmath61 } \\le z_{t-1}$ ] .",
    "observe that , since @xmath62 , this implies @xmath63 } \\le { \\mathbb{e}\\left[z_{t-1}\\right ] } \\le \\ldots \\le 1 $ ] , and thus by markov s inequality , @xmath176 } & \\le { \\mathbb{e}\\left[\\exp{\\left ( \\sum_{t=1}^t{\\bigl({\\widetilde}{\\lambda}_{t }   - \\lambda_{t}\\bigr)}\\right)}\\right ] } \\cdot \\exp(-\\varepsilon ) \\le \\exp(-\\varepsilon ) \\end{split}\\ ] ] holds for any @xmath65 .",
    "the statement of the lemma follows from solving @xmath177 for @xmath67 .",
    "fix an arbitrary @xmath178 . for ease of notation ,",
    "let us define @xmath179 . by standard arguments ( along the lines of @xcite )",
    ", we can obtain @xmath180 for any fixed @xmath181 $ ] .",
    "the last term on the right - hand side can be bounded as @xmath182 where the first step uses jensen s inequality and the last uses @xmath183 .",
    "now , we can apply lemma  [ lem : fixbound ] and the union bound to show that @xmath184 holds simultaneously for all experts with probability at least @xmath185 , and in particular for the best expert , too . putting this observation together with the above bound and equation",
    ", we get that @xmath186 holds with probability at least @xmath187 , where the last line follows from lemma  [ lem : fixbound ] and the union bound . the proof is concluded by taking @xmath101 and plugging in the choices of @xmath188 and @xmath24 .      the proof of the theorem builds on the techniques of @xcite and @xcite .",
    "let us fix an arbitrary @xmath92 and denote the best sequence from @xmath189 by @xmath190 .",
    "then , a straightforward modification of theorem  2 of @xcite yields the bound that holds for all @xmath191 . ] @xmath192 to proceed , let us apply lemma  [ lem : fixbound ] to obtain that @xmath193 simultaneously holds for all sequences @xmath194 . by standard arguments ( see , e.g. , the proof of theorem 22 in @xcite ) , one can show that @xmath195 .",
    "now , combining the above with equation   and @xmath196 , we get that @xmath197 holds with probability at least @xmath187 . where the last line follows from lemma  [ lem : fixbound ] and the union bound .",
    "then , after observing that the losses are bounded in @xmath198 $ ] and choosing @xmath101 , we get that @xmath199 holds with probability at least @xmath35 . the only remaining piece required for proving",
    "the theorem is showing that @xmath200 which follows from the proof of corollary  1 in @xcite , and then substituting the choice of @xmath24 and @xmath188 .",
    "before we dive into the proof , we note that lemma  [ lem : fixbound ] does _ not _ hold for the loss estimates used by this variant of exp3-ixdue to a subtle technical issue .",
    "precisely , in this case @xmath201 prevents us from directly applying lemma  [ lem : fixbound ] .",
    "however , corollary  [ cor : allbound ] can still be proven exactly the same way as done in section  [ sec : apps ]",
    ". the only effect of this change is that the term @xmath202 is replaced by @xmath203 .    turning to the actual proof ,",
    "let us fix an arbitrary @xmath92 and introduce the notation @xmath204 by the standard exp3-analysis , we have @xmath205 now observe that @xmath206 holds with probability at least @xmath185 by an application of corollary  [ cor : allbound ] for all @xmath13 and taking a union bound .",
    "furthermore , we have @xmath207 by the hoeffding  azuma inequality ,",
    "we have @xmath208 with probability at least @xmath185 . after putting the above inequalities together and applying lemma  [ lem : fixbound ] ,",
    "we obtain the bound @xmath209 that holds with probability at least @xmath210 by the union bound . to bound the last term on the right hand side , observe that @xmath211 is a martingale - difference sequence for all @xmath5 $ ] with @xmath212 and conditional variance @xmath213 } \\\\",
    "\\le & { \\mathbb{e}\\left[\\left.{\\left(\\sum_{i=1}^k o_{t , i } \\frac{p_{t , i}}{o_{t , i } + \\gamma}\\right)}^2\\right|{\\mathcal{f}}_{t-1}\\right ] } \\quad\\qquad\\qquad\\qquad\\mbox{(since $ { \\mathbb{e}\\left[\\left.o_{t , i}\\right|{\\mathcal{f}}_{t-1}\\right ] } = o_{t , i}$ ) } \\\\ = & { \\mathbb{e}\\left[\\left.\\sum_{i=1}^k \\sum_{j=1}^k o_{t , i}o_{t , j } \\frac{p_{t , i}}{o_{t , i } + \\gamma } \\cdot\\frac{p_{t , j}}{o_{t , j } + \\gamma}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\\\ \\le & { \\mathbb{e}\\left[\\left.\\sum_{i=1}^k \\sum_{j=1}^k o_{t , i } \\frac{p_{t , i}}{o_{t , i } + \\gamma } \\cdot\\frac{p_{t , j}}{o_{t , j } + \\gamma}\\right|{\\mathcal{f}}_{t-1}\\right ] } \\quad\\qquad\\mbox{(since $ o_{t , j\\le 1}$ ) } \\\\ = & \\sum_{i=1}^k \\sum_{j=1}^k \\frac{p_{t , i}o_{t , i}}{o_{t , i } + \\gamma } \\cdot\\frac{p_{t , j}}{o_{t , j } + \\gamma } \\le \\sum_{i=1}^k p_{t , i } \\sum_{j=1}^k \\frac{p_{t , j}}{o_{t , j } + \\gamma } = q_t . \\end{split}\\ ] ] thus , an application of freedman s inequality ( see , e.g. , theorem  1 of @xcite ) , we can thus obtain the bound @xmath214 that holds with probability at least @xmath185 for all @xmath215 . combining this result with the previous bounds and using the union bound , we arrive at the bound @xmath216 that holds with probability at least @xmath217 .",
    "now notice that when setting @xmath222 and @xmath223 , we have @xmath224 and the above bound becomes @xmath225 the proof is concluded by observing that the last term is bounded by the third one if @xmath138 ."
  ],
  "abstract_text": [
    "<S> this work addresses the problem of regret minimization in non - stochastic multi - armed bandit problems , focusing on performance guarantees that hold with high probability . </S>",
    "<S> such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard , more intuitive algorithms that come only with guarantees that hold on expectation . </S>",
    "<S> one of these modifications is forcing the learner to sample arms from the uniform distribution at least @xmath0 times over @xmath1 rounds , which can adversely affect performance if many of the arms are suboptimal . while it is widely conjectured that this property is essential for proving high - probability regret bounds , we show in this paper that it is possible to achieve such strong results without this undesirable exploration component . </S>",
    "<S> our result relies on a simple and intuitive loss - estimation strategy called _ implicit exploration _ </S>",
    "<S> ( ix ) that allows a remarkably clean analysis . to demonstrate the flexibility of our technique , we derive several improved high - probability bounds for various extensions of the standard multi - armed bandit framework . </S>",
    "<S> finally , we conduct a simple experiment that illustrates the robustness of our implicit exploration technique . </S>"
  ]
}