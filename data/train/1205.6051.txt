{
  "article_text": [
    "consider the following discrete variational form , depending on a parameter @xmath0 : find @xmath1 in a hilbert space @xmath2 such that @xmath3 , @xmath4 , where @xmath5 is a bilinear form and @xmath6 a linear form . in a many queries context , a quantity of interest of the solution @xmath7 has to be computed for many values of @xmath0 . in this note ,",
    "the following assumptions are made for simplicity , but the conclusions are general : ( i ) the variational formulation is coercive , ( ii ) @xmath5 has a so - called affine dependence on the parameter @xmath8 so that @xmath9 , ( iii ) the quantity of interest is the solution itself : @xmath10",
    ".    reduced basis ( rb ) strategies consist in replacing @xmath11 by an easily computable surrogate @xmath12 , that is made precise below ( see @xcite ) .",
    "we denote @xmath13 the solution of @xmath12 , @xmath14 the size of the matrix involved in the resolution of @xmath11 , and @xmath15 the size of the matrix involved in the resolution of @xmath12 .",
    "the rb method consists in two steps : ( i ) an offline stage , where a basis , whose vectors are solutions of @xmath11 for well - chosen values of the parameter @xmath8 , is constructed using , e.g. , a greedy algorithm on the parameter . during this stage , @xmath15 problems of size @xmath14 are solved , and some quantities related to the solutions are stored .",
    "( ii ) an online stage , where the precomputed quantities are used to solve @xmath12 for many values of @xmath8 . in this stage ,",
    "an a posteriori error estimator @xmath16 is also computed to check the quality of the approximation .",
    "this is called certification .",
    "the a posteriori error estimator verifies @xmath17 , where @xmath18 is the coercivity constant of @xmath5 ( or a lower bound of it ) and @xmath19 is the unique affine application from @xmath2 to @xmath2 such that @xmath20 , @xmath21 . in this note",
    ", we consider different ways to compute the same quantity @xmath16 .",
    "we distinguish between formulae to compute @xmath16 by adding an index to @xmath16 .",
    "thus , @xmath22 is the first formula for the estimator , directly given by the definition .",
    "since @xmath23 requires the computation of a size @xmath14 scalar product , this formula is not compatible with the constraint that the computations in the online stage should be of complexity independent of @xmath14 .",
    "+ suppose that a reduced basis of size @xmath15 has been computed in the offline stage , namely a family @xmath24 , where each of the @xmath14-dimensional vectors @xmath25 is the solutions of @xmath26 . the reduced problem is a galerkin procedure on the @xmath25 basis :",
    "find @xmath27 such that @xmath28 , @xmath29 .",
    "writing @xmath30 , the reduced linear system is @xmath31 , where @xmath32 , @xmath33 and @xmath34 . using the affine parameter dependance ,",
    "the matrix @xmath35 is built and solved in complexity independent of @xmath14 , provided that the quantities @xmath36 and @xmath37 have been precomputed in the offline stage .",
    "consider the riesz isomorphism @xmath38 from @xmath39 to @xmath2 such that @xmath40 , @xmath41 , @xmath42 .",
    "the operator @xmath19 inherits the affine dependance of @xmath5 in @xmath8 since , @xmath41 , @xmath43 where @xmath44 and @xmath45 , @xmath46 , are elements of @xmath39 .",
    "the a posteriori error estimator is then written in the following compact form : @xmath47 where @xmath48 , @xmath49 , @xmath50 , @xmath51 ( with @xmath52 and @xmath38 re - indexing respectively",
    "@xmath53 and @xmath54 , @xmath55 , @xmath56 ) and @xmath57 .",
    "provided that @xmath58 , @xmath59 , and @xmath60 ( which are independent of @xmath8 ) have been precomputed in the offline stage , @xmath61 is computed in complexity independent of @xmath14 .",
    "this is what is typically used in rb implementations .",
    "canuto , tonn and urban @xcite identified that the evaluation of @xmath61 suffers in practice from a loss of accuracy , which they attributed to the square root in [ eq : est2 ] .",
    "herein , we show more precisely that this loss of accuracy comes from round - off errors . indeed , when substracting two real numbers within floating point arithmetics , the number of lost significant digits equals the number of common decimals between the two reals . for simplicity",
    ", we neglect the round - off errors introduced when solving @xmath11 and @xmath12 , so that the vectors of the reduced basis @xmath25 and the reduced solutions @xmath13 are considered free of round - off errors . therefore , we only consider round - off errors in the evaluation of @xmath23 and @xmath61 due to the summations .",
    "we define the machine precision @xmath62 by the maximal floating point representation relative error of real numbers : @xmath63 . under these hypotheses , the smallest possible values that can be practically computed for @xmath23 and @xmath61 using floating point arithmetics with machine precison @xmath62",
    "is bounded below by respectively @xmath64 and @xmath65 .",
    "this is supported numerically ( see section [ sec : application ] ) . +",
    "this observation is of paramount importance since the certification of a rb procedure can not be better than these values . in a successful rb procedure ,",
    "the value of the estimator gets smaller as the size @xmath15 of the reduced basis increases .",
    "enriching the basis with a new vector improves the quality of the approximation introduced by the method . as a result",
    ", there exists @xmath66 such that , @xmath67 , @xmath68 , @xmath69 . if @xmath70 , @xmath61 is no longer suitable for computing the a posteriori error estimator .",
    "we notice that increasing the machine precision from @xmath62 to @xmath71 enables the accuracy of @xmath61 to reach the one of @xmath23 .",
    "thus , the use of quadruple precision is a first solution , checked numerically in section [ sec : application ] .",
    "this is however not pratical since current computer architectures are optimized for double precision .",
    "another solution is to develop an alternative algorithm for the evaluation of the estimator that still achieves the machine precision .",
    "this is the purpose of the next section .",
    "consider that a reduced basis of size @xmath15 has been constructed .",
    "let us denote @xmath72 . for a given @xmath8 and @xmath27",
    ", we define @xmath73 as the vector with components @xmath74 , with @xmath75 . using the symmetry of the matrix @xmath76 , we can write the right - hand side of [ eq : est2 ] as a linear form in @xmath77 : @xmath78 , where @xmath79 is independent of @xmath8 and @xmath80 is the p - th component of @xmath77 .    during the offline stage ,",
    "we take @xmath81 values , possibly random , @xmath82 , @xmath83 of the parameter @xmath8 . then",
    ", we compute the vectors @xmath84 and , using the accurate formula @xmath85 for the estimator , the quantities @xmath86 . finally , we define @xmath87 as the matrix whose columns are formed by the vectors @xmath84 and we assume that @xmath88 is invertible , which was the case in our simulations .    in the online stage ,",
    "suppose that we want to evaluate the estimator for the parameter value @xmath8 .",
    "we compute the vector @xmath77 and solve the linear system @xmath89 , for @xmath90 .",
    "we then have @xmath91 and @xmath92 this yields the new formula for computing the estimator , @xmath93 quite importantly , we notice that the additional cost is such that the quantity @xmath94 is still computed in complexity independent of @xmath14 . + the quantity @xmath95 is a sum of terms , whose first one is fixed and equals @xmath96 . on the contrary , @xmath97 is a sum whose terms @xmath98 are updated each time a vector is added to the reduced basis . since @xmath98 is computed for @xmath99 , @xmath100 and @xmath98 , ( or , at least , @xmath101 and @xmath102 , which are the important quantities in a greedy selection and online certification ) are of the same orders of magnitude .",
    "consider the equation @xmath103 on @xmath1040,1[$ ] with @xmath105 , where @xmath106 is the parameter .",
    "the analytic solution is @xmath107 .",
    "the lax - milgram theory is valid , the coercivity constant is @xmath108 in the @xmath109-norm .",
    "the estimator is given by @xmath1100,1[\\right)}$ ] .",
    "lagrange p@xmath111 finite elements are used , with uniform mesh cells of length 0.005 .",
    "the rb method is carried - out until a reduced basis of size @xmath112 is constructed .    , @xmath113 algorithms for the estimator and error curves with respect to the parameter @xmath8 ; right : @xmath85 , @xmath114 ( double and quadruple precision ) curves.,title=\"fig:\",width=302 ] , @xmath113 algorithms for the estimator and error curves with respect to the parameter @xmath8 ; right : @xmath85 , @xmath114 ( double and quadruple precision ) curves.,title=\"fig:\",width=302 ]    on the left part of figure [ fig : plot ] , @xmath113 yields the same curve as the accurate but expensive @xmath85 algorithm .",
    "notice that the values of the a posteriori error estimators are very close to the values of the error .",
    "this means that the efficiency of the estimator is very close to @xmath108 . on the right part of figure",
    "[ fig : plot ] , we see that @xmath114 yields a flat curve for the estimator , meaning that all information relative to the error is lost .",
    "as expected , the use of quadruple precision enables @xmath114 to recover the accuracy levels of @xmath85 . in this example ,",
    "@xmath115 and @xmath116 , which sould be respectively compared to the numerical values of @xmath114 in quadruple precision and @xmath85 ( @xmath117 and @xmath118 ) and @xmath114 ( @xmath119 ) .",
    "to sum up , we have developed a procedure where the accuracy of the online evaluation is limited by the accuracy of the evaluation of quantities precomputed during the offline stage , where heavy but accurate algorithms are allowed . in the online stage , instead of a linear combination of @xmath81 terms , we have to solve a linear system of size @xmath81 , before doing a linear combination of the same size .",
    "we have increased the accuracy of the estimator , with a procedure of complexity independent of the size @xmath14 of the initial problem .",
    "when the size of the reduced basis increases , we observe that the condition number of the matrix @xmath88 increases as well . finally , we notice that oversampling strategies consisting in defining a least squares problem to compute @xmath120 such that @xmath88 is rectangular with more than @xmath81 columns improve the quality of our results when the rb is close to convergence .",
    "experiments on a more complicated problem ( external acoustics solved by integral equations where the criterion is on the far field approximation of the diffracted acoustic potential ) lead to similar conclusions , which will be discussed in more detail elsewhere .",
    "this work was supported by eads innovation works .",
    "the author thanks nolwenn balin , jrme robert , jayant sen gupta , guillaume sylvand ( eads innovation works ) , and alexandre ern , tony lelivre ( cermics ) for fruitful discussions ."
  ],
  "abstract_text": [
    "<S> in the reduced basis method , the evaluation of the a posteriori estimator can become very sensitive to round - off errors . in this note </S>",
    "<S> , the origin of the loss of accuracy is revealed , and a solution to this problem is proposed and illustrated on a simple example . </S>"
  ]
}