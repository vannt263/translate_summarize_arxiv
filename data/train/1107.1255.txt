{
  "article_text": [
    "in the 40 years that @xmath0-body simulations have been used in cosmology research , visualization has been the most indispensable tool .",
    "physical processes have often been identified first and studied via images of simulations .",
    "a few examples are : formation of filamentary structures in the large - scale distribution of matter @xcite , growth of feedback bubbles around quasars @xcite ; cold flows of gas forming galaxies @xcite , and the evolution of ionization fronts during the re - ionization epoch @xcite .",
    "the size of current and upcoming peta - scale simulation datasets can make such visual exploration to discover new physics technically challenging .",
    "here we present techniques that can be used to display images at full resolution of datasets of hundreds of billions of particles in size .",
    "several implementations of visualization software for cosmological simulations already exist .",
    "irfit @xcite is a general purpose visualization suite that can deal with mesh based scalar , vector and tensor data , as well as particle based datasets as points .",
    "yt @xcite is an analysis toolkit for mesh based simulations that also supports imaging .",
    "splash @xcite is a visualization suite specialized for simulations that use smoothed particle hydrodynamics(sph ) techniques . aside from the cpu based approaches mentioned above ,",
    "@xcite implemented a gpu based interactive visualization tool for sph simulations .",
    "the millennium i & ii simulations @xcite have been used to test an interactive scalable rendering system developed by @xcite . both splash and the millennium visualizer support high quality visualization of sph data sets , while irfit treats sph data as discrete points .",
    "continuing improvements in computing technology and algorithms are allowing sph cosmological simulations to be run with ever increasing numbers of particles .",
    "runs are now possible on scales which allow rare objects , such as quasars to form in a large simulation volume with uniform high resolution ( see section 2.1 ; and @xcite )",
    ". being able to scan through a vast volume and seek out the tiny regions of space where most of the activity is occurring , while still keeping the large - scale structure in context necessitates special visualization capabilities .",
    "these should be able to show the largest scale information but at the same be interactively zoomable .",
    "however , as the size of the datasets quickly exceeds the capability of moderately sized in - house computer clusters , it becomes difficult to perform any interactive visualizations .",
    "for example , a single snapshot of the massiveblack simulation ( section 2.1 ) consists of 8192 files and is over 3 tb in size .",
    "even when a required large scale high resolution image has been rendered , actually exploring the data requires special tools .",
    "the gigapan collaboration has essentially solved this problem in the context of viewing large images , with the gigapan viewer enabling anyone connected to the internet to zoom into and explore in real time images which would take hours to transfer in totality .",
    "the viewing technology has been primarily used to access large photographic panoramas , but is easily applicable to simulated datasets . a recent enhancement to deal with the time dimension , in the form of gigapixel frame interactive movies ( gigapan time machine ) will turn out to give particularly novel and exciting results when applied to simulation visualization .    in this work",
    "we combine an off - line imaging technique together with gigapan technology to implement an interactively accessible visual probe of large cosmological simulations .",
    "while gigapan is an independent project ( uploading and access to the gigapan website is publicly available ) , we release our toolkit for the off - line visualization as gaepsi , a software package aimed specifically at gadget @xcite sph simulations .",
    "the layout of our paper is as follows . in section 2",
    "we give a brief overview of the physical processes modeled in gadget , as well as describing two p - gadget simulations which we have visualized . in section 3",
    "we give details of the spatial domain remapping we employ to convert cubical simulation volumes into image slices . in section 4",
    ", we describe the process of rasterizing an sph density field , and in section 5 the image rendering and layer compositing . in section 6",
    "we address the parallelism of our techniques and give measures of performance . in section 7",
    "we briefly describe the gigapan and gigapan time machine viewers and present examples screenshots from two visualizations ( which are both accessible on the gigapan websites ) .",
    "adaptive mesh refinement(amr , e.g. , @xcite ) and smoothed particle hydrodynamics ( sph , @xcite ) are the two most used schemes for carrying out cosmological simulations . in this work",
    "we focus on the visualization of the baryonic matter in sph simulations run with p - gadget @xcite .",
    "gadget is an sph implementation , and p - gadget is a version which has been developed specifically for petascale computing resources .",
    "it simultaneously follows the self - gravitation of a collision - less n - body system ( dark matter ) and gas dynamics ( baryonic matter ) , as well as the formation of stars and super - massive black holes .",
    "dark matter particles and gas particle positions and initial characteristics are set up in a comoving cube , and black hole and star particles are created according to sub - grid modeling @xcite gas particles carry hydrodynamical properties , such as temperature , star formation rate , and neutral fraction .",
    "although our attention in this paper is limited to imaging properties the of gas , stars and black holes in gadget simulations , similar techniques could be used to visualize the dark matter content .",
    "also , the software we provide should be easily adaptable to the data formats of other sph codes ( e.g. gasoline , @xcite )      the massiveblack simulation is the state - of - art sph simulation of a @xmath1 universe @xcite .",
    "p - gadget was used to evolve @xmath2 particles in a volume of side length @xmath3{h^{-1}mpc}$ ] with a gravitational force resolution of @xmath4{h^{-1}kpc}$ ] .",
    "one snapshot of the simulation occupies 3 tera - bytes of disk space , and the simulation has been run so far to redshift @xmath5 , creating a dataset of order @xmath6 tb .",
    "the fine resolution and large volume of the simulation permits one to usefully create extremely large images .",
    "the simulation was run on the high performance computing facility , kraken , at the national institute for computational sciences in full capability mode with 98,000 cpus .",
    "[ [ e5 ] ] e5 ~~    to make a smooth animation of the evolution of the universe typically requires hundreds frames directly taken as snapshots of the simulation .",
    "the scale of the massiveblack run is too large for this purpose , so we ran a much smaller simulation ( e5 ) with @xmath7 particles in a @xmath8{h^{-1}mpc}$ ] comoving box .",
    "the model was again a @xmath1 cosmology , and one snapshot was output per 10 million years , resulting in 1367 snapshots .",
    "this simulation ran on @xmath9 cores of the warp cluster in the mcwilliams center for cosmology at cmu .",
    "spatial domain remapping can be used to transform the periodic cubic domain of a cosmological simulation to a patch whose shape is similar to the domain of a sky survey , while making sure that the local structures in the simulation are preserved @xcite .",
    "another application is making a thin slice that includes the entire volume of the simulation .",
    "our example will focus on the latter case .    a gadget cosmological simulation",
    "is usually defined in the periodic domain of a cube . as a result , if we let @xmath10 be any position dependent property of the simulation , then @xmath11 where @xmath12 are integers .",
    "the structure corresponds to a simple cubic lattice with lattice constant @xmath13 , the simulation box side - length . a bijective mapping from the cubic unit cell to a remapped domain corresponds to a choice of the primitive cell .",
    "figure [ fig : transformationprimitive ] illustrates the situation in 2 dimensions .    .",
    "the cubical unit cell is shown using solid lines . the new primitive cell , generated by @xmath14 $ ] is shown with dash - dotted lines .",
    "the transformed domain is shown in gray . ]    whilst the original remapping algorithm by @xcite results in the correct transformations being applied , it has two drawbacks : ( i ) the orthogonalization is invoked explicitly and ( ii ) the hit - testing for calculation of the shifting ( see below ) is against non - aligned cuboids .",
    "the second problem especially undermines the performance of the program . in this work we present a faster algorithm based on similar ideas , but which features a qr decomposition ( which is widely available as a library routine ) , and hit - testing against an aabb ( axis aligned bounding box ) .",
    "first , the transformation of the primitive cell is given by a uni - modular integer matrix , @xmath15 where @xmath16 are integers and the determinant of the matrix @xmath17 .",
    "it is straight - forward to obtain such matrices via enumeration .",
    "@xcite the @xmath18 decomposition of @xmath19 is @xmath20 where @xmath21 is an orthonormal matrix and @xmath22 is an upper - right triangular matrix .",
    "it is immediately apparent that ( i ) application of @xmath21 yields rotation of the basis from the simulation domain to the transformed domain , the column vectors in matrix @xmath23 being the lattice vectors in the transformed domain ; ( ii ) the diagonal elements of @xmath22 are the dimensions of the remapped domain . for imaging it",
    "is desired that the thickness along the line of sight is significantly shorter than the extension in the other dimensions , thus we require @xmath24 . note that if a domain that is much longer in the line of sight direction is desired , for example to calculate long range correlations or to make a sky map of a whole simulation in projection , the choice should be @xmath25 .",
    "next , for each sample position @xmath26 , we solve the indefinite equation of integer cell number triads @xmath27 ,    & = q^tx+aq^ti ,    where @xmath28 is the box size , @xmath29 is the transformed sample position satisfying @xmath30 . in practice , the domain of @xmath29 is enlarged by a small number @xmath31 to address numerical errors .",
    "multiplying by @xmath21 on the left and re - organizing the terms , we find @xmath32 notice that @xmath33 is the transformed sample position expressed in the original coordinate system , and is bounded by its aabb box .",
    "if we let @xmath34 $ ] , where @xmath35 and @xmath36 are integers , and notice @xmath37 , the resulting bounds of @xmath38 are given by @xmath39.\\ ] ] we then enumerate the range to find @xmath29 .",
    "when the remapping method is applied to the sph particle positions , the transformations of the particles that are close to the edges give inexact results .",
    "the situation is similar to the boundary error in the original domain when the periodic boundary condition is not properly considered .",
    "figure [ fig : distortion ] illustrates the situation by showing all images of the particles that contribute to the imaging domain .",
    "we note that for the purpose of imaging , by choosing a @xmath40(the thickness in the thinner dimension ) much larger than the typical smoothing length of the sph particle , the errors are largely constrained to lie near the edge .",
    "these issues are part of general complications related to the use of a simulation slice for visualization .",
    "for example in an animation of the distribution of matter in a slice it is possible for objects to appear and disappear in the middle of the slice as they pass through it .",
    "these limitations should be borne in mind , and we leave 3d visualization techniques for future work .    the transformations used for the massiveblack and e5 simulations are listed in table [ tab : transformations ] .    .",
    "four images of a particle intersecting the boundary are shown . the top - right image is contained in the transformed domain , but the other three are not .",
    "the contribution of the two bottom images is lost . by requiring the size of the transformed domain to be much larger than typical sph smoothing lengths ,",
    "most particles do not intersect a boundary of the domain and the error is contained near the edges . ]",
    ".transformations[tab : transformations ] [ cols=\"^,^,^\",options=\"header \" , ]",
    "in a simulation , many field variables are of interest in visualization .",
    "* scalar fields : density @xmath41 , temperature @xmath42 , neutral fraction @xmath43 , star formation rate @xmath44 ; * vector fields : velocity , gravitational force .    in an sph simulation , a field variable as a function of spatial position is given by the interpolation of the particle properties .",
    "rasterization converts the interpolated continuous field into raster pixels on a uniform grid .",
    "the kernel function of a particle at position @xmath45 with smoothing length @xmath46 is defined as @xmath47 following the usual prescriptions ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) , the interpolated density field is taken as@xmath48 where @xmath49 , @xmath50 , @xmath51 are the mass , position , and smoothing length of the @xmath52th particle , respectively .",
    "the interpolation of a field variable , denoted by @xmath53 , is given by @xmath54 where @xmath55 is the corresponding field property carried by the @xmath52th particle .",
    "note that the density field can be seen as a special case of the general formula .",
    "two types of pixel - wise mean for a field are calculated , the    1 .",
    "volume - weighted mean of the density field , +    & = _",
    "im_i_pd^3 w(-_i , h_i ) + & = _",
    "im_i(p ) , + where @xmath56 is the mass overlapping of the @xmath52th particle and the pixel , and the 2 .",
    "mass - weighted mean of a field ( @xmath53),[multiblock footnote omitted ] @xmath57    to obtain a line of sight projection along the third axis , the pixels are chosen to extend along the third dimension , resulting a two dimensional final raster image .",
    "the calculation of the overlapping @xmath58 in this circumstance is two dimensional .",
    "both formulas require frequent calculation of the overlap between the kernel function and the pixels .",
    "an effective way to calculate the overlap is via a lookup table that is pre - calculated and hard coded in the program .",
    "three levels of approximation are used in the calculation of the contribution of a particle to a pixel :    1 .",
    "when a particle is much smaller than a pixel , the particle contributes to the pixel as a whole .",
    "no interpolation and lookup occurs .",
    "2 .   when a particle and pixel are of similar size of ( up to a few pixels in size ) ,",
    "the contribution to each of the pixels are calculated by interpolating between the overlapping areas read from a lookup table .",
    "3 .   when a particle is much larger than a pixel , the contribution to a pixel taken to be the center kernel value times the area of a pixel .    note that level 1 and 3 are significantly faster than level 2 as they do not require interpolations .",
    "the rasterization of the @xmath5 snapshot of massiveblack was run on the sgi uv blacklight supercomputer at the pittsburgh supercomputing center .",
    "blacklight is a shared memory machine equipped with a large memory for holding the image and a fairly large number of cores enabling parallelism , making it the most favorable machine for the rasterization .",
    "the rasterization of the e5 simulation was run on local cmu machine warp .",
    "the pixel dimensions of the raster images are also listed in table [ tab : transformations ] .",
    "the pixel scales have been chosen to be around the gravitational softening length of @xmath59{h^{-1}kpc}$ ] in these simulations in order to preserve as much information in the image as possible .",
    "the rasterized sph images are color - mapped into rgba ( red , green , blue and opacity ) layers .",
    "two modes of color - mapping are implemented , the simple mode and the intensity mode .    in the simple mode , the color of a pixel is directly obtained by looking up the normalized pixel value in a given color table . to address the large ( several orders of magnitude ) variation of the fields ,",
    "the logarithm of the pixel value is used in place of the pixel value itself .    in the intensity mode",
    ", the color of a pixel is determined in the same way as done in the simple mode .",
    "however , the opacity is reduced by a factor @xmath60 that is determined by the logarithm of the total mass of the sph fluid contained within the pixel . to be more specific , @xmath61^{\\gamma } & \\text{otherwise},\\end{cases}\\ ] ] where @xmath28 and @xmath62 are the underexposure and overexposure parameters : any pixel that has a mass below @xmath63 is completely transparent , and any pixel that has a mass above @xmath64 is completely opaque .",
    "the rgba layers are stacked one on top of another to composite the final image .",
    "the compositing assumes an opaque black background .",
    "the formula to composite an opaque bottom layer @xmath65 with an overlay layer @xmath42 into the composite layer @xmath66 is @xcite @xmath67 where @xmath66 , @xmath65 and @xmath42 stand for the rgb pixel color triplets of the corresponding layer and @xmath68 is the opacity value of the pixel in the overlay layer @xmath42 .",
    "for example , if the background is red and the overlay color is green , with @xmath69 , the composite color is a @xmath70-dimmed yellow .",
    "point - like ( non sph ) particles are rendered differently .",
    "star particles are rendered as colored points , while black hole particles are rendered using circles , with the radius proportional to the logarithm of the mass . in our example images ,",
    "the massiveblack simulation visualization used a fast rasterizer that does not support anti - aliasing , whilst the frames of e5 are rendered using matplotlib @xcite that does anti - aliasing .",
    "the choice of the colors in the color map has to be made carefully to avoid confusing different quantities .",
    "we choose a color gradient which spans black , red , yellow and blue for the color map of the normalized gas density field .",
    "this color map is shown in figure [ fig : colormap - of - gas ] .",
    "composited above the gas density field is the mass weighted average of the star formation rate field , shown in dark blue , and with completely transparency where the field vanishes .",
    "additionally , we choose solid white pixels for the star particles .",
    "blackholes are shown as green circles . in the e5 animation frames ,",
    "the normalization of the gas density color map has been fixed so that the maximum and minimum values correspond to the extreme values of density in the last snapshot .    .",
    "the colors span a darkened red through yellow to blue.,width=192 ]",
    "the large simulations we are interested in visualizing have been run on large supercomputer facilities . in order to image them with sufficient resolution to be truly useful",
    ", the creation of images from the raw simulation data also needs significant computing resources . in this section",
    "we outline our algorithms for doing this and give measures of performance .",
    "we have implemented two types of parallelism , which we shall refer to as `` tiny '' and `` huge '' , to make best use of shared memory architectures and distributed memory architectures , respectively .",
    "the tiny parallelism is implemented with openmp and takes advantage of the case when the image can be held within the memory of a single computing node .",
    "the parallelism is achieved by distributing the particles in batches to the threads within one computing node .",
    "the raster pixels are then color - mapped in serial , as is the drawing of the point - like particles .",
    "the tiny mode is especially useful for interactively probing smaller simulations .    the huge version of parallelism",
    "is implemented using the message passing interface ( mpi ) libraries and is used when the image is larger than a single computing node or the computing resources within one node are insufficient to finish the rasterization in a timely manner .",
    "the imaging domain is divided into horizontal stripes , each of which is hosted by a computing node . when the snapshot is read into memory , only the particles that contribute to the pixels in a domain",
    "are scattered to the hosting node of the domain . due to the growth of cosmic structure",
    "as we move to lower redshifts , some of the stripes inevitably have many more particles than others , introducing load imbalance .",
    "we define the load imbalance penalty @xmath71 as the ratio between the maximum and the average of the number of particles in a stripe .",
    "the computing nodes with fewer particles tend to finish sooner than those with more .",
    "the color - mapping and the drawing of point - like particles are also performed in parallel in the huge version of parallelism .",
    "the time spent in domain remapping scales linearly with the total number of particles @xmath0,@xmath72 the time spent in color - mapping scales linearly with the total number of pixels @xmath73,@xmath74 both processes consume a very small fraction of the total computing cycles .",
    "the rasterization consumes a much larger part of the computing resources and it is useful to analyze it in more detail . if we let @xmath75 be the number of pixels overlapping a particle , then @xmath76 , where @xmath77 is a constant related to the simulation .",
    "now we let @xmath78 be the time it takes to rasterize one particle , as a function of the number of pixels overlapping the particle . from the 3 levels of detail in the rasterization algorithm ( section 4 )",
    ", we have @xmath79 with @xmath80 . the effective pixel filling rate @xmath22",
    "is defined as the total number of image pixels rasterized per unit time,@xmath81^{-1}=\\bar{n}k[t(\\bar{n})]^{-1}\\\\ = \\begin{cases } \\bar{n}kc_{1}^{-1 } , & \\bar{n}\\ll1\\\\ kc_{2}^{-1 } & \\bar{n}\\sim1\\\\ kc_{3}^{-1 } , & \\bar{n}\\gg1\\end{cases}.\\end{gathered}\\ ] ]    .",
    "we show the number of pixels fixed as a function of resolution ( pixel scale ) .",
    "the rate peaks at @xmath82 at the high resolution limit and approaches @xmath83 as the resolution worsens .",
    "the @xmath84 domain is not explored.,width=259 ]    the rasterization time to taken to create images from a single snapshot of the massiveblack simulation ( at redshift 4.75 ) at various resolutions is presented in table [ tab : rasterizationtime ] and figure [ fig : rasterizationrate ] .",
    "> p0.25in > p0.35in > p0.25in > p0.25in > p0.25in > p0.15in > p0.25 in pixels & res + ( @xmath85 ) & @xmath75 & cpus & wall - time + ( @xmath86 ) & @xmath71 & rate + ( @xmath87 ) + 5.6 g & 58.4 & 80 & 128 & & 1.45 & 11.3 + 22.5 g & 29.2 & 330 & 256 & 3.17 & 1.66 & 13.4 + 90 g & 14.6 & 1300 & 512 & 3.35 & 1.57 & 24.0 + 90 g & 14.6 & 1300 & 512 & 3.06 & 1.39 & 23.3 + 360 g & 7.3 & 5300 & 512 & 7.65 & 1.39 & 37.2 + 1160 g & 4.2 & 16000 & 1344 & 10.1 & 1.56 & 37.2 +    the rasterization of the images were carried out on blacklight at psc .",
    "it is interesting to note that for the largest images , the disk i / o wall - time , limited by the i / o bandwidth of the machine , overwhelms the total computing wall - time .",
    "the performance of the i / o subsystem shall be an important factor in the selection of machines for data visualization at this scale .",
    "once large images or animation frames have been created , viewing them presents a separate problem .",
    "we use the gigapan technology for this , which enables someone with a web browser and internet connection to access the simulation at high resolution . in this section",
    "we give a brief overview of the use of the gigapan viewer for exploring large static images , as well as the recently developed gigapan time machine viewer for gigapixel animations .",
    "individual gigapixel - scale images are generally too large to be shared in easy ways ; they are too large to attach to emails , and may take minutes or longer to transfer in their entirety over typical internet connections .",
    "gigapan addresses the problem of sharing and interactive viewing of large single images by streaming in real - time the portions of images actually needed by the viewer of the image , based on the viewers current area of focus inside the image . to support this real - time streaming ,",
    "the image is divided up and rendered into small tiles of multiple resolutions .",
    "the viewer pulls in only the tiles needed for a given view .",
    "many mapping programs ( e.g. , google maps ) use the same technique .",
    "we have uploaded an example terapixel image of the redshift @xmath5 snapshot of the massiveblack simulation to the gigapan website , which is run as a publicly accessible resource for sharing and viewing large images and movies .",
    "the dimension of the image is @xmath88 , and the finished image uncompressed occupies @xmath89{tb}$ ] of storage space .",
    "the compressed hierarchical data storage in gigapan format is about 15% of the size , or @xmath90{tb}$ ] .",
    "there is no fundamental limits to size , provided the data can be stored on the disk .",
    "it is possible to create directly the compressed tiles of a gigapan , bypassing the uncompressed image as an intermediate step , and thus reducing the requirement on memory and disk storage .",
    "we leave this for future work .    on the viewer side ,",
    "static gigapan works well at different bandwidths ; the interface remains responsive independent of bandwidth , but the imagery resolves more slowly as the bandwidth is reduced .",
    "is a recommended bandwidth for exploring with a @xmath91 window , but the system works well even when the bandwidth is lower .",
    "an illustration of the screen output is shown in figure [ fig : gigapanview ] .",
    "the reader is encouraged to visit the website to explore the image .      in order to make animations ,",
    "one starts with the rendered images of each individual snapshot in time .",
    "these can be gigapixel in scale or more . in our example , using the e5 simulation ( section 2.2 ) we have 1367 images each with 0.75 gigapixels .",
    "one approach to showing gigapixel imagery over time would be to modify the single - image gigapan viewer to animate by switching between individual gigapan tile - sets . however , this approach is expensive in bandwidth and cpu , leading to sluggish updates when moving through time .",
    "to solve this problem , we created a gigapixel video streaming and viewing system called gigapan time machine @xcite , which allows the user to fluidly explore gigapixel - scale videos across both space and time .",
    "we solve the bandwidth and cpu problems using an approach similar to that used for individual gigapan images : we divide the gigapixel - scale video spatially into many smaller videos .",
    "different video tiles contain different locations of the overall video stream , at different levels of detail .",
    "only the area currently being viewed on a client computer need be streamed to the client and decoded . as the user translates and zooms through different areas in the video , the viewer scales and translates the currently streaming video , and over time the viewer requests from the server different video tiles which more closely match the current viewing area .",
    "the viewing system is implemented in javascript+html , and takes advantage of recent browser s ability to display and control videos through the new html5 video tag .",
    "the architecture of gigapan time machine allows the content of all video tiles to be precomputed on the server ; clients request these precomputed resources without additional cpu load on the server .",
    "this allows scaling to many simultaneously viewing clients , and allows standard caching protocols in the browser and within the network itself to improve the overall efficiency of the system .",
    "the minimum bandwidth requirement to view videos without stalling depends on the size of the viewer , the frame rate , and the compression ratios .",
    "the individual videos in `` evolution of the universe '' ( the e5 simulation , see below for link ) are currently encoded at with relatively low compression .",
    "the large video tiles require a continuous bandwidth of , and a burst bandwidth of .",
    "we have uploaded an example animation of the e5 simulation , showing its evolution over the interval between redshift @xmath92 and @xmath93 with 1367 frames equally spaced in time by . again",
    ", the reader is encouraged to visit the website to explore the image .",
    "we have presented a framework for generating and viewing large images and movies of the formation of structure in cosmological sph simulations .",
    "this framework has been designed specifically to tackle the problems that occur with the largest datasets . in the generation of images , it includes parallel rasterization ( for either shared and distributed memory ) and adaptive pixel filling which leads to a well behaved filling rate at high resolution . for viewing images ,",
    "the gigapan viewers use hierarchical caching and cloud based storage to make even the largest of these datasets fully explorable at high resolution by anyone with an internet connection .",
    "we make our image making toolkit publicly available , and the gigapan web resources are likewise publicly accessible .",
    "this work was supported by nsf awards oci-0749212 , ast-1009781 , and the moore foundation .",
    "the following computer resources were used in this research : , , .",
    "development of this work has made extensive use of the bruce and astrid mcwilliams escience video facility at carnegie mellon university ."
  ],
  "abstract_text": [
    "<S> the increasing size of cosmological simulations has led to the need for new visualization techniques . </S>",
    "<S> we focus on smoothed particle hydrodynamical ( sph ) simulations run with the gadget code and describe methods for visually accessing the entire simulation at full resolution . </S>",
    "<S> the simulation snapshots are rastered and processed on supercomputers into images that are ready to be accessed through a web interface ( gigapan ) . </S>",
    "<S> this allows any scientist with a web - browser to interactively explore simulation datasets in both in spatial and temporal dimensions , datasets which in their native format can be hundreds of terabytes in size or more . </S>",
    "<S> we present two examples , the first a static terapixel image of the massiveblack simulation , a p - gadget sph simulation with 65 billion particles , and the second an interactively zoomable animation of a different simulation with more than one thousand frames , each a gigapixel in size . </S>",
    "<S> both are available for public access through the gigapan web interface . </S>",
    "<S> we also make our imaging software publicly available . </S>"
  ]
}