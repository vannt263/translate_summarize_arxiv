{
  "article_text": [
    "in medical research , _ dynamic treatment regimes _ are increasingly used to choose effective treatments for individual patients with long - term patient care .",
    "a  dynamic treatment regime ( or similarly , policy ) is a  set of decision rules for how the treatment should be chosen at each decision time - point , depending on both the patient s medical history up to the current time - point and the previous treatments .",
    "note that although the same set of decision rules is applied to all patients , the choice of treatment at a  given time - point may differ , depending on the patient s medical state .",
    "moreover , the patient s treatment plan is not known at the beginning of a  dynamic regime , since it may depend on subsequent time - varying variables that may be influenced by earlier treatments and response to treatment .",
    "an optimal treatment regime is a  set of treatment choices that maximizes the mean response of some clinical outcome at the end of the final time interval [ see , e.g. , @xcite , @xcite , @xcite ] .",
    "we consider the problem of finding treatment regimes that lead to longer survival times , where the number of treatments is flexible and where the data are subject to censoring .",
    "this type of framework is natural for cancer applications , where the initiation of the next line of therapy depends on the disease progression and thus the number of treatments is flexible .",
    "in addition , data are subject to censoring since patients can drop out during the trial .",
    "for example , in advanced nonsmall cell lung cancer ( nsclc ) , patients receive one to three treatment lines .",
    "the timing of the second and third lines of treatment is determined by the disease progression and by the ability of patients to tolerate therapy [ @xcite , @xcite ] .",
    "we focus on mean survival time restricted to a  specific interval , since in a  limited - time study , censoring prevents reliable estimation of the unrestricted mean survival time [ see discussion in @xcite , @xcite , @xcite ; see also @xcite in the context of sequential decision problems and see @xcite for an alternative approach ] .",
    "finding an optimal policy for survival data poses many statistical challenges .",
    "we enumerate four .",
    "first , one needs to incorporate information accrued over time into the decision rule .",
    "second , one needs to avoid treatments which appear optimal in the short term but may lead to poor final outcome in the long run .",
    "third , the data are subject to censoring since some of the patients may be lost to follow - up and the final outcome of those who reached the end of the study alive is unknown .",
    "fourth , the number of decision points ( i.e. , treatments ) and the timing of these decision points can be different for different patients .",
    "this follows since the number of treatments and duration between treatments may depend on the medical condition of the patient .",
    "in addition , in the case of a  patient s death , treatment is stopped .",
    "the first two challenges are shared with general multistage decision optimization [ @xcite , @xcite ] .",
    "the latter two arise naturally in the context of optimizing survival time , but are applicable to other scenarios as well . developing valid methodology for estimating dynamic treatment regimes in this flexible timing setup is crucial for applications in cancer and in other diseases where such structure is the norm and appropriate existing methods are unavailable .",
    "one of the primary tools used in developing dynamic treatment regimes is q - learning [ @xcite , @xcite , @xcite , @xcite ] .",
    "q - learning [ @xcite , @xcite ] , which is reviewed in section  [ secqlearning ] , is a  reinforcement learning algorithm . since we do not assume that the problem is markovian , we present a  version of q - learning that uses backward recursion . the backward recursion used by q - learning addresses the first two challenges posed above : it enables both accrual of information and incorporation of long - term treatment effects",
    "when the number of stages is flexible , and censoring is introduced , it is not clear how to implement backward recursion . indeed , finding the optimal treatment at the last stage is not well defined , since the number of stages is patient - dependent . also , it is not clear how to utilize the information regarding censored patients .    in this paper",
    "we present a  novel q - learning algorithm that takes into account the censored nature of the observations using inverse - probability - of - censoring weighting [ see @xcite ; see also @xcite , @xcite in the context of sequential decision problems ] .",
    "we provide finite sample bounds on the generalization error of the policy learned by the algorithm , that is , bounds on the average difference in expected survival time between the optimal dynamic treatment regime and the dynamic treatment regime obtained by the proposed q - learning algorithm .",
    "we also present a  simulation study of a  sequential - multiple - assignment randomized trial ( smart ) [ see @xcite and references therein ] with flexible number of stages depending on disease progression and failure event timing .",
    "we demonstrate that the censored - q - learning algorithm proposed here can find treatment strategies tailored to each patient which are better than any fixed - treatment sequence .",
    "we also demonstrate the result from ignoring censored observations .",
    "one general contribution of the paper is the development of a  methodology for solving backward recursion when the number and timing of stages are flexible .",
    "as mentioned previously , this is crucial for applications but has not been addressed previously . in section  [ secauxiliary ]",
    "we present an auxiliary multistage decision problem that has a  fixed number of stages .",
    "since the number of stages is fixed for the auxiliary problem , backward recursion can be used in order to estimate the decision policy .",
    "we then show how to translate the original problem to the auxiliary one and obtain the surprising conclusion that results obtained for the auxiliary problem can be translated into results regarding the original problem with flexible number and timing of stages .",
    "an additional contribution of the paper is the universal consistency proof for the algorithm performance .",
    "universal consistency of an algorithm means that for every distribution function on the sample space , the expected loss of the function learned by the algorithm converges in probability to the infimum of the expected loss , where the infimum is taken over all the measurable functions [ see , e.g. , @xcite ] . in section  [ sectheoretical ]",
    "we prove that when the optimal q - functions belong to the corresponding approximation spaces considered by the algorithm , the algorithm is universally consistent .",
    "the proof presented here is algorithm - specific , but the tools used in the proof are widely applicable for universal consistency proofs when the data are subject to censoring [ see , e.g. , @xcite ] . while other learning algorithms were suggested for survival data [ see , e.g. , @xcite , @xcite , @xcite ; see also @xcite in the context of a  multistage decision problem ] , we are not aware of any other universal consistency proof for survival data .",
    "the paper is organized as follows . in section  [ secqlearning ]",
    "we review the q - learning algorithm and discuss the challenges for adapting the q - learning methodology for a  framework with flexible number of stages and censored data .",
    "we also review existing methods for finding optimal policies .",
    "definitions and notation are presented in section  [ secnotation ] .",
    "the auxiliary problem is presented in section  [ secauxiliary ] .",
    "the censored - q - learning algorithm is presented in section  [ secalgo ] .",
    "the main theoretical results are presented in section  [ sectheoretical ] . in section  [ secsimulation ]",
    "we present a  multistage - randomized - trial simulation study . concluding remarks appear in section  [ secsummary ] .",
    "supplementary proofs are provided in the . a  description of and link to the code and data sets used in section  [ secsimulation ] appear in the supplementary material [ @xcite ] .",
    "reinforcement learning is a  methodology for solving multistage decision problems .",
    "it involves recording sequences of actions , statistically estimating the relationship between these actions and their consequences and then choosing a  policy ( i.e. , a  set of decision rules ) that approximates the most desirable consequence based on the statistical estimation .",
    "a  detailed introduction to reinforcement learning can be found in @xcite . in the medical context of long - term patient care ,",
    "the reinforcement learning setting can be described as follows . for each patient",
    ", the stages correspond to clinical decision points in the course of the patient s treatment . at these decision points , actions ( e.g. , treatments )",
    "are chosen , and the state of the patient is recorded . as a  consequence of a  patient s treatment , the patient receives a  ( random ) numerical reward .    more formally , consider a  multistage decision problem with @xmath0 decision points .",
    "let @xmath1 be the ( random ) state of the patient at stage @xmath2 and let @xmath3 be the vector of all states up to and including stage  @xmath4 .",
    "similarly , let @xmath5 be the action chosen in stage  @xmath4 , and let @xmath6 be the vector of all actions up to and including stage  @xmath4 .",
    "we use the corresponding lower case to denote a  realization of these random variables and random vectors .",
    "let the random reward be denoted @xmath7 , where @xmath8 is an ( unknown ) time - dependent deterministic function of all states up to stage @xmath9 and all past actions up to stage @xmath4 .",
    "a  trajectory is defined as a  realization of @xmath10 .",
    "note that we do not assume that the problem is markovian . in the medical context example , a  trajectory is a  record of all the patient covariates at the different decision points , the treatments that were given , and the medical outcome in numerical terms .",
    "we define a  policy , or similarly , a  dynamic treatment regime , to be a  set of decision rules .",
    "more formally , define a  policy @xmath11 to be a  sequence of deterministic decision rules , @xmath12 , where for every pair @xmath13 , the output of the  @xmath4th decision rule , @xmath14 , is an action .",
    "our goal is to find a  policy that maximizes the expected sum of rewards .",
    "the bellman equation [ @xcite ] characterizes the optimal policy @xmath15 as one that satisfies the following recursive relation : @xmath16,\\ ] ] where the value function @xmath17\\ ] ] is the expected cumulative sum of rewards from stage @xmath9 to stage @xmath0 , where the history up to stage @xmath9 is given by @xmath18 , and when using the optimal policy @xmath15 thereafter .    finding a  policy that leads to a  high expected cumulative reward is the main goal of reinforcement learning .",
    "naively , one could learn the transition distribution functions and the reward function using the observed trajectories , and then solve the bellman equation recursively .",
    "however , this approach is inefficient both computationally and memory - wise .",
    "in the following section , we introduce the q - learning algorithm , which requires less memory and less computation .",
    "q - learning [ @xcite ] is an algorithm for solving reinforcement learning problems .",
    "it is claimed by sutton and barto to be one of the most important breakthroughs in reinforcement learning [ @xcite , section 6.5 ] .",
    "q - learning uses backward recursion to compute the bellman equation without the need to know the full dynamics of the process .",
    "more formally , we define the optimal time - dependent q - function @xmath19 .\\ ] ] note that @xmath20 , and thus @xmath21 .\\ ] ]    in order to estimate the optimal policy , one first estimates the q - functions backward through time @xmath22 and obtains a  sequence of estimators @xmath23 .",
    "the estimated policy is given by @xmath24    in the next section we discuss the difficulties in applying the q - learning methodology when trajectories are subject to censoring and the number of stages is flexible .",
    "as discussed in the , our goal is to develop a  q - learning algorithm that can handle a  flexible number of stages and that takes into account the censored nature of the observations .",
    "we face two main challenges .",
    "first , recall that the estimation of the q - functions in ( [ eqqfunction ] ) is done recursively , starting from the last stage backward .",
    "thus , when the number of stages is flexible , it is not clear how to perform the base step of the recursion .",
    "second , due to censoring , some of the trajectories may be incomplete . incorporating the data of a  censored trajectory",
    "is problematic : even when the number of stages is fixed , the known number of stages for a  censored trajectory may be less than the number of stages in the multistage problem . moreover , the reward is not known for the stage at which censoring occurs .",
    "finding optimal policies or optimal treatment regimes has been discussed extensively in other work .",
    "we discuss shortly some additional work that is related to the approach taken here .",
    "however , we are not aware of any other existing approaches that address simultaneously both censoring and flexible number of stages .",
    "the approach closest to our proposal is the censored - q - learning algorithm of @xcite .",
    "zhao et al .",
    "considered a  q - learning algorithm for censored data based on support vector regression adjusted for censoring with fixed number of stages .",
    "a  simulation study was performed to demonstrate the algorithm s performance ; however , the theoretical properties of this algorithm were not evaluated .",
    "a  general approach for finding optimal policies that uses backward recursion was studied by @xcite and @xcite in the semiparametric context , and by @xcite in the nonparametric context .",
    "these works do not treat flexible number of stages or censoring , and can not be applied to the framework considered here without some adjustments .",
    "another approach for finding optimal policies was studied by @xcite [ see also @xcite , @xcite ] .",
    "orellana et al .",
    "considered dynamic regime marginal structural mean models [ @xcite ] . in this approach , for each regime , one considers all trajectories that comply to the regime up to some point .",
    "the trajectories are then censored at the first time - point at which they do not comply to the regime .",
    "the contribution of the noncompliant trajectories is redistributed among compliant trajectories that have the same covariate and treatment history , using the inverse - probability - of - censoring weighting .",
    "advantages and disadvantages of this approach compared to the backward recursion approach mentioned above are discussed in @xcite , section 5 .",
    "we note that it is assumed in their approach that the length of each stage is fixed , an assumption we do not require .",
    "this general issue is also related to the analysis of two - stage randomized trials involving right - censored data studied in a  series of papers including @xcite , @xcite , @xcite , @xcite .",
    "the authors use inverse - probability - of - censoring to correct for censoring . see also @xcite that considers analysis of two - stage randomized trials with interval censoring",
    ". however , the main focus of these works is in finding the best regime from a  finite number of optional regimes , as opposed to the individualized - treatment policies addressed in our proposal .",
    "in this section we present definitions and notation which will be used in the paper .",
    "let @xmath0 be the maximal number of decision time - points for a  given multistage time - dependent decision problem .",
    "note that the number of stages for different observations can be different . for each @xmath25 ,",
    "the state @xmath1 is the pair @xmath26 , where @xmath27 is either a  vector of covariates describing the state of the patient at beginning of stage  @xmath4 or @xmath28 .",
    "@xmath28 indicates that a  failure event happened during the  @xmath4th stage which has therefore reached a  terminal state .",
    "@xmath29  is the length of the interval between decision time - points @xmath30 and  @xmath4 , where we denote @xmath31 .",
    "although in the usual q - learning context @xmath32 is the sum of rewards up to and including stage  @xmath4 , in our context it is more useful to think of this sum as the total survival time up to and including stage  @xmath4 .",
    "let @xmath5 be an action chosen at decision time  @xmath4 , where @xmath5 takes its values in a  finite discrete space @xmath33 .",
    "the model assumes that observations are subject to censoring .",
    "let @xmath34 be a  censoring variable and let @xmath35 be its survival function .",
    "we assume that censoring is independent of both covariates and failure time .",
    "we assume that @xmath34 takes its values in the segment @xmath36 $ ] where @xmath37 and that @xmath38 .",
    "let @xmath39 be an indicator with @xmath40 if no censoring event happened before the ( @xmath9)th decision time - point .",
    "note that @xmath41 .",
    "[ remsurvival ] note that for a  censoring variable , we define the survival function @xmath42 as @xmath43 rather than the usual @xmath44 .",
    "this is because given a  failure time @xmath45 , we are interested in the probability @xmath46 . however , to avoid complications that are not of interest to the main results of this paper , we assume that the probability of simultaneous failure and censoring is zero [ see , e.g. , @xcite ] .",
    "the inclusion of failure times in the model affects the trajectory structure .",
    "usually , a  trajectory is defined as a  @xmath47-length sequence @xmath48 , @xmath49 . however , in our context , if a  failure event occurs before decision time - point @xmath0 , the trajectory will not be of full length .",
    "denote by @xmath50 the ( random ) number of stages for the individual ( @xmath51 ) . due to the censoring , the trajectories themselves are not necessarily fully observed .",
    "assume that a  censoring event occurred during stage  @xmath4 .",
    "note that this means that @xmath52 while @xmath53 and that @xmath54 . in this case",
    "the observed trajectories have the following structure : @xmath55 and @xmath34 is also observed .",
    "we now discuss the distribution of the observed trajectories .",
    "assume that  @xmath56 trajectories are sampled at random according to a  fixed distribution denoted by @xmath57 .",
    "the distribution @xmath57 is composed of the unknown distribution of each @xmath1 conditional on @xmath58 ( denoted by @xmath59 ) and an exploration policy that generates the actions .",
    "denote the exploration policy by @xmath60 where the probability that action @xmath61 is taken given history @xmath62 is @xmath63 .",
    "we assume that @xmath64 for every action @xmath65 and for each possible value @xmath66 , where @xmath67 is a  constant .",
    "the likelihood ( under @xmath57 ) of the trajectory @xmath68 is @xmath69 we denote expectations with respect to the distribution @xmath57 by @xmath70 .",
    "the survival function with respect to the distribution @xmath57 is denoted by @xmath71 .",
    "we assume that @xmath72 , that is , that there is a  positive probability that the survival time is greater than @xmath73 .",
    "we define policy @xmath11 to be a  sequence of deterministic decision rules , @xmath74 , where for every nonterminating pair @xmath66 , the output of the  @xmath4th decision rule , @xmath75 , is an action .",
    "let the distribution @xmath76 denote the distribution of a  trajectory for which the policy @xmath11 is used to generate the actions .",
    "the likelihood ( under @xmath76 ) of the trajectory , @xmath68 is=-1 @xmath77=0    our goal is to find a  policy that maximizes the expected rewards .",
    "since with probability 1 @xmath78 , the maximum observed survival time is less than or equal to @xmath73 .",
    "thus we try to maximize the truncated - by-@xmath73 expected survival time .",
    "formally , we look for a  policy @xmath79 that approximates the maximum over all deterministic policies of the following expectation : @xmath80,\\ ] ] where @xmath81 is the expectation with respect to @xmath76 and @xmath82 .",
    "in this section we construct an auxiliary q - learning model for our original problem .",
    "the modified trajectories of the construction are of fixed length @xmath0 , and the modified sum of rewards is less than or equal to  @xmath73 .",
    "we then show how results obtained for the auxiliary problem can be translated into results regarding the original problem .    for the auxiliary problem , we complete all trajectories to full length in the following way .",
    "assume that a  failure time occurred at stage @xmath83 .",
    "in that case the trajectory up to @xmath84 is already defined .",
    "write @xmath85 for @xmath86 and @xmath87 for @xmath88 . for all @xmath89 set @xmath90 and for all @xmath91 draw @xmath92 uniformly from @xmath33 .",
    "we also modify trajectories with overall survival time greater than @xmath73 in the following way .",
    "assume that  @xmath4 is the first index for which .",
    "for all @xmath93 , write @xmath85 and @xmath87 .",
    "write @xmath94 and assign @xmath95 and thus the modified state @xmath96 . if @xmath83 , then for all @xmath89 set @xmath90 and for all @xmath91 draw  @xmath97 uniformly from @xmath33 .",
    "the modified trajectory is given by the sequence @xmath98 .",
    "note that trajectories with fewer than @xmath99 entries and for which @xmath100 are modified twice .",
    "the @xmath56 modified trajectories are distributed according to the fixed distribution @xmath101 which can be obtained from @xmath57 .",
    "this distribution is composed of the unknown distribution of each @xmath102 conditional on @xmath103 , denoted by @xmath104 , and exploration policy @xmath105 .",
    "the conditional distribution  @xmath106 equals @xmath107 , and for @xmath108 , @xmath109\\\\[-8pt ] & & \\qquad= \\cases { \\displaystyle f_t((z_t',r_t')|{\\mathbf{s}}_{t-1}',{\\mathbf{a}}_{t-1 } ' ) , & \\quad $ \\displaystyle z_{t-1}'\\neq\\varnothing , \\sum_{i=1}^t r_i'<\\tau$,\\cr \\displaystyle\\int_{g_{z_t'}}f_t((z_{t}',r_{t})|{\\mathbf{s}}_{t-1}',{\\mathbf{a}}_{t-1 } ' ) \\,\\mathrm{d}r_t , & \\quad $ \\displaystyle z_{t-1}'\\neq\\varnothing , \\sum_{i=1}^t r_i'=\\tau$,\\cr { { 1}_{{\\mathbf{s}}_{t}'=(\\varnothing,0 ) } } , & \\quad $ z_{t-1}'= \\varnothing$,}\\nonumber\\end{aligned}\\ ] ] where @xmath110 and @xmath111 is @xmath112 if @xmath113 is true and is @xmath114 otherwise .",
    "the exploration policy @xmath105 agrees with @xmath115 on every pair @xmath116 for which @xmath117 and draws @xmath118 uniformly from @xmath33 whenever @xmath28 .",
    "the likelihood ( under @xmath101 ) of the modified trajectory , @xmath119 , is @xmath120 denote expectations with respect to the distribution @xmath101 by @xmath121 .",
    "let @xmath11 be a  policy for the original problem .",
    "we define a  version of the policy  @xmath122 for the auxiliary problem in the following way . for any state @xmath123 for which @xmath124 , the same action is chosen . for any state @xmath123 for which @xmath125 , a  fixed action @xmath126 is chosen ; w.o.l.g .",
    ", let @xmath127 be chosen .",
    "for the auxiliary problem , we say that two policies @xmath128 and @xmath129 are equivalent if @xmath130 for every @xmath123 for which @xmath124 .",
    "we denote both the original policy and any modified version of it by @xmath11 whenever it is clear from the context which policy is considered .",
    "similarly , we omit the prime from states and actions in the auxiliary problem whenever there is no reason for confusion .",
    "let @xmath131 be the distribution in the auxiliary problem where actions are chosen according to @xmath11 .",
    "the likelihood under @xmath131 of the trajectory @xmath132 is @xmath133 denote expectations with respect to the distribution @xmath131 by @xmath134 .",
    "we now define the value functions and the q - functions for policies in the auxiliary model . for any auxiliary policy @xmath11 define its corresponding value function  @xmath135 .",
    "given an initial state @xmath136 , @xmath137 is the expected truncated - by-@xmath73 survival time when the initial state is @xmath136 and the actions are chosen according to the policy @xmath11 .",
    "formally @xmath138 $ ] where the truncation takes place since the expectation is taken with respect to the distribution of the modified trajectories .",
    "the stage-@xmath4 value function for the auxiliary policy @xmath11 , @xmath139 , is the expected ( truncated ) remaining survival time from the  @xmath4th decision time - point , given the trajectory @xmath140 , and when following the policy @xmath11 thereafter .",
    "note that given @xmath141 , the survival time up to the beginning of stage  @xmath4 is known , and thus truncation ensures that the overall survival time is less than or equal to @xmath73 .",
    "formally @xmath142 $ ] .    the stage-@xmath4 q - function for the auxiliary policy @xmath11 is the expected remaining ( truncated ) survival time , given that the state is @xmath140 , that @xmath143 is chosen at stage  @xmath4 , and that @xmath11 is followed thereafter .",
    "formally , @xmath144 .\\ ] ] the optimal value function @xmath145 and the optimal q - function @xmath146 are defined by ( [ eqvaluefunction ] ) and ( [ eqqfunction ] ) , respectively .",
    "the following lemma relates the values of the value function @xmath135 in the auxiliary problem to the expected truncated - by-@xmath73 survival time for a  policy  @xmath11 in the original problem .",
    "[ lemequalityoriginalaux ] let @xmath147 be the collection of all policies in the original problem .",
    "then for all @xmath148 , the following equalities hold true : @xmath149,\\\\ \\label{eqequalityoriginalaux2 } v^*(s_o)&= & \\max_{\\pi\\in\\pi}e_{{0},\\pi}\\biggl [ \\biggl ( \\sum_{t=1}^{\\overline{t}}r_t\\biggr)\\wedge\\tau \\big|s_1=s_o\\biggr],\\end{aligned}\\ ] ] where @xmath135 and @xmath150 are value functions in the auxiliary problem .",
    "@xmath151we start by decomposing the expectations depending on both  the terminal stage and whether the sum of rewards is greater than or equal to  @xmath73 .",
    "define @xmath152 g_t&=&\\biggl\\{\\{s_o , a_1,\\ldots , s_{k+1}\\}\\dvtx t=\\min\\biggl\\{j\\dvtx\\sum_{i=1}^{j}r_i\\geq\\tau\\biggr\\}\\mbox { , and } k = t \\mbox { or } z_{k+1}=\\varnothing\\biggr\\},\\\\[-3pt ] f_t'&=&\\bigl\\ { ( { \\mathbf{s}}_{t+1}',{\\mathbf{a}}_t')\\dvtx ( { \\mathbf{s}}_{t+1}',{\\mathbf{a}}_t')\\in f_t , \\{a_{t+1}',\\ldots , s_{t+1}\\}=\\{a_o,(\\varnothing,0),\\ldots,(\\varnothing , 0)\\}\\bigr\\},\\\\[-3pt ] g_t'&=&\\biggl\\ { ( { \\mathbf{s}}_{t+1}',{\\mathbf{a}}_t ' ) : ( { \\mathbf{s}}_t',{\\mathbf{a}}_t ' ) \\mbox { is   a~ beginning of   sequence in } g_t,\\\\[-3pt ] & & \\hspace*{7.5pt } \\{s_{t+1}',a_{t+1}',\\ldots , s_{t+1}\\}=\\biggl\\{\\biggl(\\varnothing,\\tau-\\sum _ { j=1}^{t-1}r_j\\biggr ) , a_o,\\ldots,(\\varnothing,0)\\biggr\\ } \\biggr\\}.\\end{aligned}\\ ] ] denote @xmath153 & & \\qquad = f_1(s_1)\\bigl[{{1}_{\\pi(s_1)=a_1}}\\bigr ] \\prod_{j=2}^{t-1 } \\bigl(f_{j}(s_j|{\\mathbf{s}}_{j-1},{\\mathbf{a}}_{j-1 } ) { { 1}_{\\pi_j({\\mathbf{s}}_{j},{\\mathbf{a}}_{j-1})=a_j}}\\bigr)\\\\[-3pt ] & & \\qquad\\quad{}\\times f_{t}(s_{t}|{\\mathbf{s}}_{t-1},{\\mathbf{a}}_{t-1})\\end{aligned}\\ ] ] and similarly @xmath154 .    note that @xmath155 & = & \\sum_{t=1}^t\\int_{f_t}\\biggl(\\sum_{i=1}^{t}r_i\\biggr){\\mathbf{f}}_{t+1,\\pi } ( { \\mathbf{s}}_{t+1},{\\mathbf{a}}_t)\\,\\mathrm{d}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t ) \\nonumber\\\\[-9.5pt]\\\\[-9.5pt ] & & { } + \\tau\\sum_{t=1}^t p_{{0},\\pi}(g_t ) \\nonumber\\end{aligned}\\ ] ] and @xmath156\\\\[-9.5pt ] & & { } + \\tau\\sum_{t=1}^t p_{\\pi}(g_t ' ) .\\nonumber\\vadjust{\\goodbreak}\\end{aligned}\\ ] ] note that @xmath157 where the first equality follows from ( [ eqfprime ] ) and the second follows since there is a  one - to - one correspondence between trajectories in @xmath158 and @xmath159 , and by construction , for each such trajectory in @xmath159 we have @xmath160 and @xmath161\\prod _ { j = t+2}^{t}\\bigl(f_{j}'(s_j|{\\mathbf{s}}_{j-1},{\\mathbf{a}}_{j-1 } ) { { 1}_{\\pi_j({\\mathbf{s}}_{j},{\\mathbf{a}}_{j-1})=a_o}}\\bigr)f_{t+1}(s_{t+1}|{\\mathbf{s}}_{t},{\\mathbf{a}}_{t})=1.\\ ] ]    similarly , we show that @xmath162 .",
    "denote by @xmath163 the set of all sequences @xmath164 which are the beginning part of some trajectory in @xmath165 .",
    "note that @xmath166\\nonumber\\\\ & & \\hspace*{14.2pt}{}\\times\\int_{\\{s_{t+1}\\dvtx\\sum_{i=1}^t r_i\\geq\\tau\\}}f_{t+1}(s_{t+1}|{\\mathbf{s}}_{t},{\\mathbf{a}}_{t } ) \\,\\mathrm{d}({\\mathbf{s}}_{t+1})\\,\\mathrm{d}({\\mathbf{s}}_{t},{\\mathbf{a}}_t)\\nonumber\\\\ & = & \\int_{\\hat{g}_t}{\\mathbf{f}}_{t}'({\\mathbf{s}}_{t},{\\mathbf{a}}_{t-1})\\bigl[{{1}_{\\pi_t({\\mathbf{s}}_{t},{\\mathbf{a}}_{t-1})=a_t}}\\bigr]\\\\ & & \\hspace*{14.2pt}{}\\times\\int_{\\{s_{t+1}\\dvtx\\sum_{i=1}^t r_i= \\tau\\}}f_{t+1}'(s_{t+1}|{\\mathbf{s}}_{t},{\\mathbf{a}}_{t } ) \\,\\mathrm{d}({\\mathbf{s}}_{t+1})\\,\\mathrm{d}({\\mathbf{s}}_{t},{\\mathbf{a}}_t)\\nonumber\\\\ & = & \\int_{g_t'}{\\mathbf{f}}_{t+1}'({\\mathbf{s}}_{t+1},{\\mathbf{a}}_{t})\\,\\mathrm{d}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_{t})=p_{\\pi}(g_t ' ) , \\nonumber\\end{aligned}\\ ] ] where the second equality follows from ( [ eqfprime ] ) and the third equality follows from the construction of @xmath167 .",
    "the first assertion of the lemma , namely , ( [ eqequalityoriginalaux1 ] ) , follows by substituting the right - hand side of the equalities ( [ eqfs ] ) and ( [ eqgs ] ) in ( [ eqetruev ] ) for each  @xmath4 and comparing to ( [ eqev ] ) .",
    "the second assertion , ( [ eqequalityoriginalaux2 ] ) , is proven by maximizing both sides of ( [ eqequalityoriginalaux1 ] ) over all policies .",
    "note that the maximization is taken over two different sets since each policy in the original problem has an equivalent class of policies in the auxiliary problem .",
    "however , since @xmath135 is the same for all policies in the same equivalence class , the result follows .",
    "we now present the proposed censored - q - learning algorithm .",
    "as discussed before , we are looking for a  policy @xmath79 that approximates the maximum over all deterministic policies of the following expectation : @xmath80 .\\ ] ]    we find this policy in three steps .",
    "first , we map our problem to the corresponding auxiliary problem .",
    "then we approximate the functions @xmath168 using backward recursion based on ( [ eqqfunction ] ) and obtain the functions @xmath169 .",
    "finally , we define @xmath79 by maximizing @xmath170 over all possible actions  @xmath143 .",
    "let @xmath171 be the approximation spaces for the @xmath172-functions .",
    "we assume that @xmath173 whenever @xmath174 .",
    "in other words , if a  failure occurred before the  @xmath4th time - point , @xmath175 equals zero .",
    "note that by ( [ eqqfunction ] ) , the optimal  @xmath4-stage @xmath172-function @xmath176 equals the conditional expectation of @xmath177 given @xmath178 .",
    "thus @xmath179 .\\ ] ]    ideally , we could compute the functions @xmath180 using backward recursion in the following way : @xmath181,\\ ] ] where @xmath182 is the empirical expectation .",
    "the problem is that @xmath183 may be censored and thus unknown .",
    "note that @xmath184=p(c\\geq\\sum_{i=1}^t r_i)=s_c(\\sum_{i=1}^t r_i)$ ] and thus @xmath185=1\\ ] ] since @xmath186 includes the information regarding @xmath187 and @xmath34 is independent of the covariates and actions .",
    "thus , for every function @xmath188 , @xmath189\\nonumber\\\\ & & \\qquad = e\\biggl[\\bigl(r_t+ \\max_{a_{t+1 } } q_{t+1}^ * ( { \\mathbf{s}}_{t+1},({\\mathbf{a}}_t , a_{t+1}))-q_t({\\mathbf{s}}_{t},{\\mathbf{a}}_t ) \\bigr)^2\\nonumber\\\\ & & \\hspace*{132pt}{}\\times e\\biggl[\\frac{\\delta_t}{s_c(\\sum_{i=1}^t r_i)}\\big| { \\mathbf{s}}_{t},{\\mathbf{a}}_t , r_t\\biggr]\\biggr]\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad = e\\biggl[e\\biggl[\\bigl(r_t+ \\max_{a_{t+1 } } q_{t+1}^ * ( { \\mathbf{s}}_{t+1},({\\mathbf{a}}_t , a_{t+1}))-q_t({\\mathbf{s}}_{t},{\\mathbf{a}}_t ) \\bigr)^2 \\nonumber\\\\ & & \\hspace*{160pt}{}\\times\\frac{\\delta_t}{s_c(\\sum_{i=1}^t r_i)}\\big| { \\mathbf{s}}_{t},{\\mathbf{a}}_t , r_t\\biggr]\\biggr]\\nonumber\\\\ & & \\qquad = e\\biggl[\\bigl(r_t+ \\max_{a_{t+1 } } q_{t+1}^ * ( { \\mathbf{s}}_{t+1},({\\mathbf{a}}_t , a_{t+1}))-q_t({\\mathbf{s}}_{t},{\\mathbf{a}}_t ) \\bigr)^2 \\frac{\\delta_t}{s_c(\\sum_{i=1}^t r_i)}\\biggr].\\nonumber\\end{aligned}\\ ] ]    since @xmath190 is the minimizer of the first expression in the above sequence of equalities , it also minimizes the last expression .",
    "thus , we suggest to choose  @xmath180 recursively as follows : @xmath191\\\\[-8pt ] & & \\hspace*{202pt}{}\\times\\frac{\\delta_t}{\\hat{s}_c(\\sum_{i=1}^t r_i)}\\biggr ] , \\nonumber\\end{aligned}\\ ] ] where we define @xmath192 , and @xmath193 is the kaplan  meier estimator of the survival function of the censoring variable @xmath194 .",
    "note that by remark  [ remsurvival ] , the kaplan  meier estimator at @xmath45 needs to estimate @xmath46 rather than @xmath195 .",
    "this can be done by taking a  right continuous version of the kaplan ",
    "meier estimator that interchanges the roles of failure and censoring events for estimation [ see @xcite ] .    we define the policies @xmath196 using the approximated q - functions @xmath197 as follows : @xmath198",
    "let @xmath171 be the approximation spaces for the minimization problems ( [ eqminimizationproblem ] ) .",
    "note that we do _ not _ assume that the problem is markovian , but , instead , we assume that each @xmath175 is a  function of all the history up to and including stage  @xmath4 . hence the spaces @xmath199 can be different over  @xmath4 .",
    "we assume that the absolute values of the functions in the spaces @xmath200 are bounded by some constant @xmath201 .",
    "moreover , we need to bound the complexity of the spaces @xmath200 .",
    "we choose to use uniform entropy as the complexity measure [ see @xcite ] .",
    "this enables us to obtain exponential bounds on the difference between the true and empirical expectation of the loss function that involves a  random component , namely , the kaplan  meier estimator , as in ( [ eqminimizationproblem ] ) ( see lemma  [ lementropyforkq ] ) .",
    "this is different from @xcite who uses the covering number as a  measure of complexity [ @xcite , page 148 ] for the squared error loss function .    for every @xmath202 and measure @xmath101",
    ", we denote the covering number of @xmath203 by @xmath204 , where @xmath205 is the minimal number of closed @xmath206-balls of radius @xmath207 required to cover @xmath203 . the uniform covering number of @xmath203",
    "is defined as @xmath208 where the supremum is taken over all finitely discrete probability measures @xmath101 on @xmath203 .",
    "the log of the uniform covering number is called the uniform entropy [ @xcite , page 84 ] .",
    "we assume the following uniform entropy bound for the spaces @xmath209 : @xmath210 for all @xmath211 and some constants @xmath212 and @xmath213 , where the supremum is taken over all finitely discrete probability measures , and @xmath201 is the uniform bound defined above .    in the following , we prove a  finite sample bound on the difference between the expected truncated survival times of an optimal policy and the policy  @xmath79 obtained by the algorithm . as a  corollary",
    "we obtain that the difference converges to zero under certain conditions .",
    "the proof of the theorem consists of the following steps .",
    "first we use lemma  [ lemequalityoriginalaux ] to map the original problem to the corresponding auxiliary one .",
    "second , for the auxiliary problem , we adapt arguments given in @xcite to bound the difference between the expected value of the learned policy and the expected value of the optimal policy using error terms that involve expectations of both the learned and optimal q - functions .",
    "third , we bound these error terms by decomposing them to terms that arise due to the difference between the empirical and true expectation , terms that arise due to the differences between the estimated and true censoring distribution , and terms related to the empirical difference between the estimated and optimal q - function .",
    "fourth , and finally , we obtain a  finite sample bound which depends on the complexity of the spaces @xmath209 , the deviation of the kaplan ",
    "meier estimator from the censoring distribution , and the size of the empirical errors in  ( [ eqminimizationproblem ] ) .",
    "[ thmmain ] let @xmath171 be the approximation spaces for the q - functions .",
    "assume that the uniform entropy bound ( [ equniformentropy ] ) holds .",
    "assume that @xmath56 trajectories are sampled according to @xmath57 .",
    "let @xmath79 be defined by ( [ eqpihat ] ) .",
    "then for any @xmath214 , we have with probability at least @xmath215 , over the random sample of trajectories , @xmath216 -e_{{0},\\hat{\\pi}}\\biggl[\\biggl ( \\sum_{t=1}^{\\overline { t}}r_t\\biggr)\\wedge\\tau\\biggr]\\nonumber\\\\ & & \\qquad\\leq16{\\varepsilon}+ \\sum_{t=1}^t l^{t/2 } \\sum_{j = t}^{t}\\biggl(2l^{j}4^{j - t}{\\mathbb{e}}_n \\biggl [ \\frac{\\delta _ t}{\\hat { s}_c(\\sum_{i=1}^t r_i)}\\\\ & & \\hspace*{170pt}{}\\times\\bigl(f(\\hat{q}_t,\\hat{q}_{t+1})-f(q_t^*,\\hat{q}_{t+1 } ) \\bigr ) \\biggr]_+\\biggr)^{1/2}\\nonumber \\ ] ] for all @xmath56 that satisfies @xmath217 where @xmath218 and where @xmath219 , @xmath220 is the constant that appears in bitouz , laurent and massart [ ( @xcite ) , equation ( 1 ) ] , @xmath221 , @xmath222 and @xmath223 are the constants that appear in lemma  [ lementropyforkq ] , and for some @xmath224 small enough such that .",
    "before we begin the proof of theorem  [ thmmain ] , we note that the bound ( [ eqmainbound ] ) can not be used in practice to perform structural risk minimization [ see , e.g. , @xcite ] for two reasons .",
    "first , the bound itself is too loose [ see also @xcite , theorem 1 , remark 4 ] .",
    "second , the constants , such as @xmath221 and @xmath222 , are not given , and are model - dependent .",
    "interestingly , a  bound on @xmath220 was established recently by @xcite . however ,",
    "this bound is large and simulations suggest that it is not tight .",
    "the bound ( [ eqmainbound ] ) can , however , be used to derive asymptotic rates [ @xcite , chapter 6 ] . moreover",
    ", when the functions @xmath190 are in @xmath199 , we obtain universal consistency , as stated in the following corollary :    [ cormain ] assume that the conditions of theorem  [ thmmain ] hold .",
    "assume also that for every  @xmath4 , @xmath225 .",
    "then @xmath226 -e_{{0},\\hat{\\pi}}\\biggl[\\biggl ( \\sum_{t=1}^{\\overline { t}}r_t\\biggr)\\wedge\\tau\\biggr]{\\stackrel{\\mathit{a.s.}}{\\rightarrow}}0 .\\ ] ]    note that for every  @xmath4 , @xmath180 is the minimizer of @xmath227 .\\vadjust{\\goodbreak}\\ ] ] hence , the second expression in the right - hand side of ( [ eqmainbound ] ) equals zero , and the result follows .",
    "proof of theorem  [ thmmain ] by lemma  [ lemequalityoriginalaux ] , @xmath226 -e_{{0},\\hat{\\pi}}\\biggl[\\biggl ( \\sum_{t=1}^{\\overline { t}}r_t\\biggr)\\wedge\\tau\\biggr]= e[v^*(s_1)-v_{\\hat{\\pi}}(s_1 ) ] , \\ ] ] where the expectation on the right - hand side of the equality is with respect to the modified distribution @xmath101 .    by lemma 2 of @xcite and remark 2 that follows , for every state @xmath228 , @xmath229 } .\\ ] ] applying jensen s inequality , we obtain @xmath230\\leq\\sum_{t=1}^t 2l^{t/2 } \\sqrt{e\\bigl[\\bigl(\\hat{q}_t({\\mathbf{s}}_t,{\\mathbf{a}}_t)-q^*_t({\\mathbf{s}}_t,{\\mathbf{a}}_t ) \\bigr)^2\\bigr]}.\\ ] ]    we wish to obtain a  bound on the expression @xmath231 $ ] using the expressions @xmath232 , where @xmath233\\ ] ] for any pair of function @xmath175 and @xmath234 . to obtain this",
    "bound we follow the line of arguments that leads to the bound in equation ( 13 ) in the proof of theorem 1 of @xcite .",
    "the bound ( [ eqresulterrqterrqtstar ] ) obtained here is tighter since only the special case of @xmath190 in the second @xmath235 function is considered . to simplify the following expressions , we write @xmath175 instead of @xmath236 whenever no confusion could occur .    for each  @xmath4 , @xmath237-e[(q_{t}^*)^2]\\nonumber\\\\ & & \\qquad\\quad { } + 2e\\bigl[\\bigl(r_t+\\max _ { a_{t+1 } }",
    "\\hat { q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\bigr)(q_{t}^*-\\hat { q}_{t})\\bigr]\\nonumber\\\\ & & \\qquad = e[\\hat{q}_{t}^2]-e[(q_{t}^*)^2]\\nonumber\\\\ & & \\qquad\\quad{}+2e\\bigl[(q_{t}^*-\\hat{q}_{t})e\\bigl[\\bigl(r_t-\\max _ { a_{t+1}}q_{t+1}^*({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\bigr)\\big|{\\mathbf{s}}_{t},{\\mathbf{a}}_t\\bigr]\\bigr]\\nonumber\\\\ & & \\qquad\\quad { } + 2e\\bigl[\\bigl(\\max _ { a_{t+1}}q_{t+1}^*({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\nonumber\\\\ & & \\hspace*{68pt } { } -\\max _ { a_{t+1 } } \\hat{q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\bigr)(q_{t}^*-\\hat { q}_{t})\\bigr]\\\\ & & \\qquad = e[\\hat{q}_{t}^2]-e[(q_{t}^*)^2]+2e[(q_{t}^*)^2]-2e[\\hat{q}_{t } q_{t}^*]\\nonumber\\\\ & & \\qquad\\quad { } + 2e\\bigl[\\bigl(\\max _ { a_{t+1}}q_{t+1}^*({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\nonumber\\\\ & & \\hspace*{68pt}{}-\\max _ { a_{t+1 } } \\hat{q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\bigr)(q_{t}^*-\\hat { q}_{t})\\bigr]\\nonumber\\\\ & & \\qquad = e[(\\hat{q}_{t}-q_{t}^*)^2\\nonumber]\\\\ & & \\qquad\\quad{}+2e\\bigl[\\bigl(\\max _ { a_{t+1}}q_{t+1}^ * ( { \\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\nonumber\\\\ & & \\hspace*{68pt } { } -\\max _ { a_{t+1 } } \\hat{q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\bigr)(q_{t}^*-\\hat{q}_{t})\\bigr ] , \\nonumber\\end{aligned}\\ ] ] where the second to the last equality follows since @xmath238.\\ ] ]    using the cauchy ",
    "schwarz inequality for the second expression of ( [ eqerrqterrqtstar ] ) , we obtain @xmath239 \\\\ & & \\qquad\\quad { } - 2e\\bigl[\\bigl(\\max _ { a_{t+1}}q_{t+1}^*({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1 } ) -\\max _ { a_{t+1 } } \\hat{q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\bigr)^2\\bigr]^{1/2}\\\\ & & \\qquad\\quad\\hspace*{10pt}{}\\times e[(q_{t}^*-\\hat{q}_{t})^2]^{1/2 } .\\end{aligned}\\ ] ] note that @xmath240\\nonumber\\\\ & & \\qquad\\leq e\\bigl[\\max _ { a_{t+1}}\\bigl(q_{t+1}^*({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})-\\hat{q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})\\bigr)^2\\bigr]\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad\\leq e\\biggl[l\\sum_{a\\in{\\mathcal{a}}}\\bigl(q_{t+1}^*({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a)-\\hat{q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a)\\bigr)^2 p_t(a|{\\mathbf{s}}_{t+1},{\\mathbf{a}}_t)\\biggr]\\nonumber\\\\ & & \\qquad= l e\\bigl[\\bigl(q_{t+1}^*({\\mathbf{s}}_{t+1},{\\mathbf{a}}_{t+1})-\\hat{q}_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_{t+1})\\bigr)^2\\bigr ] , \\nonumber\\end{aligned}\\ ] ] where the first inequality follows since @xmath241 and where @xmath242 is the constant that appears in the definition of the exploration policy @xmath115 ( see section  [ secnotation ] ) .    using inequality ( [ eqmaxtoexpectation ] ) and the fact that @xmath243 , we obtain @xmath244 - e[4l(q_{t+1}^*-\\hat{q}_{t+1})^2]^{1/2}e[(q_{t}^*-\\hat { q}_{t})^2]^{1/2}\\\\ & & \\qquad\\geq \\tfrac{1}{2}e[(\\hat{q}_{t}-q_{t}^*)^2]-2le[(q_{t+1}^*-\\hat { q}_{t+1})^2 ] .\\end{aligned}\\ ] ] hence @xmath245 \\leq2 \\bigl(\\mathit{err}_{\\hat{q}_{t+1}}(\\hat{q}_{t})-\\mathit{err}_{\\hat { q}_{t+1}}(q_{t}^*)\\bigr)+4le[(q_{t+1}^*-\\hat{q}_{t+1})^2 ] .\\ ] ] using the fact that @xmath246 , we obtain @xmath247 \\leq2 \\sum_{j = t}^{t}(4l)^{j - t } \\bigl(\\mathit{err}_{\\hat{q}_{j+1}}(\\hat{q}_j)-\\mathit{err}_{\\hat { q}_{j+1}}(q_j^*)\\bigr ) .\\ ] ]    we are now ready to bound the expressions @xmath248 . for any @xmath249 , @xmath250 , and censoring survival function @xmath251\\mapsto[k_{\\min},1]$ ] , where @xmath252 , define @xmath253 , \\nonumber\\\\[-8pt]\\\\[-8pt ] & & { \\mathcal{e}}_n(q_t , q_{t+1},k)\\nonumber\\\\ & & \\qquad= { \\mathbb{e}}_n \\biggl[\\frac{\\delta_t } { k ( \\sum_{i=1}^t r_i ) } \\bigl(r_t + \\max _ { a_{t+1 } } q_{t+1}({\\mathbf{s}}_{t+1},{\\mathbf{a}}_t , a_{t+1})-q_t \\bigr)^2\\biggr ] .\\nonumber\\end{aligned}\\ ] ]    note that similarly to ( [ eqicpw ] ) we have @xmath254 , where  @xmath255 is the censoring survival function .    using this notation , we have @xmath256 where @xmath257 is the kaplan ",
    "meier estimator of @xmath255 , and @xmath258 .",
    "hence @xmath259\\\\[-9pt ] & & \\qquad\\quad { } + { 2\\sup_{\\{q_t , q_{t+1},k\\}}}| { \\mathcal{e}}(q_t , q_{t+1},k)-{\\mathcal{e}}_n(q_t , q_{t+1},k)|\\nonumber\\\\ & & \\qquad\\quad { } + \\bigl({\\mathcal{e}}_n(\\hat{q}_t,\\hat{q}_{t+1},\\hat{s}_c)-{\\mathcal{e}}_n(q_t^*,\\hat { q}_{t+1},\\hat { s}_c)\\bigr)_+ .\\nonumber\\end{aligned}\\ ] ] combining ( [ eqresulterrqterrqtstar ] ) and ( [ eqresult2errqterrqtstar ] ) , and substituting in ( [ eqvstarminusvhat ] ) , we have @xmath260\\nonumber\\hspace*{-25pt}\\\\ & & \\quad \\leq2\\sum_{t=1}^t l^{t/2 } \\sum_{j = t}^{t}\\sqrt{2(4l)^{j - t } \\bigl(\\mathit{err}_{q_{t+1}}(q_t)-\\mathit{err}_{q_{t+1}}(q_t^*)\\bigr)}\\nonumber\\hspace*{-25pt}\\\\ \\label{eqneedboundforkm } & & \\quad\\leq8(4l)^{(t+1)/2}\\sqrt{{\\max_t \\sup_{\\{q_t , q_{t+1}\\}}}|{\\mathcal{e}}(q_t , q_{t+1},s_c)-{\\mathcal{e}}(q_t , q_{t+1},\\hat{s}_c)|}\\hspace*{-25pt } \\\\",
    "\\label{eqneedboundforempirical } & & \\qquad { }   + 8(4l)^{(t+1)/2}\\sqrt{{\\max_t \\sup_{\\{q_t , q_{t+1},k\\}}}| { \\mathcal{e}}(q_t , q_{t+1},k)-{\\mathcal{e}}_n(q_t , q_{t+1},k)|}\\hspace*{-25pt } \\\\ & & \\qquad { } + 2\\sum_{t=1}^t l^{t/2 } \\sum_{j = t}^{t}2^{j - t}l^{j/2}\\sqrt{2 \\bigl({\\mathcal{e}}_n(\\hat{q}_t,\\hat { q}_{t+1},\\hat { s}_c)-{\\mathcal{e}}_n(q_t^*,\\hat{q}_{t+1},\\hat{s}_c)\\bigr)_+}\\nonumber , \\hspace*{-25pt}\\end{aligned}\\ ] ] where we used the fact that @xmath261 for @xmath262 and the fact that @xmath263 .    in the following ,",
    "we replace the bounds in ( [ eqneedboundforkm ] ) and ( [ eqneedboundforempirical ] ) with exponential bounds .",
    "we start with ( [ eqneedboundforkm ] ) .",
    "note that @xmath264 for all @xmath265 .",
    "hence , @xmath266\\ ] ] and thus @xmath267 where the first equality follows from the fact that @xmath268    using a  dvoretzky  kiefer ",
    "wolfowitz - type inequality for the kaplan ",
    "meier estimator [ @xcite , theorem 2 ] , we have @xmath269\\\\[-8pt ] & & \\qquad<\\tfrac{5}{2}\\exp\\bigl\\{-2n(1-g_{\\min } ) ^2({\\varepsilon}')^2+c_o\\sqrt{n}(1-g_{\\min}){\\varepsilon}'\\bigr\\ } , \\nonumber\\end{aligned}\\ ] ] where @xmath220 is some universal constant and @xmath270 is a  lower bound on the survival function at @xmath73 ( see section  [ secnotation ] ) .",
    "write @xmath271 , and thus @xmath272 .",
    "note that @xmath273 iff @xmath274 .",
    "applying the inequality ( [ eqdkw ] ) to the right - hand side of ( [ eqboundforkm ] ) and substituting for  @xmath207 , we obtain @xmath275\\\\[-8pt ] & & \\hspace*{37pt}\\qquad { } + c_o\\sqrt{n}(1-g_{\\min})m_1^{-1}k_{\\min}^2{\\varepsilon}^2(4l)^{-(t+1)}\\bigr\\ } \\nonumber\\hspace*{-25pt}\\\\ & & \\quad\\equiv\\frac{5t}{2 } \\exp\\bigl\\{-nc_1{\\varepsilon}^4+\\sqrt{n}c_2{\\varepsilon}^2\\bigr\\ } , \\nonumber\\hspace*{-25pt}\\end{aligned}\\ ] ] where @xmath276 and @xmath277 .",
    "we now find an exponential bound for ( [ eqneedboundforempirical ] ) .",
    "we follow the same line of arguments , replacing the dvoretzky ",
    "wolfowitz - type inequality used in the previous proof with the uniform entropy bound .",
    "recall that by assumption , the uniform entropy bound ( [ equniformentropy ] ) holds for the spaces @xmath199 and thus also for the spaces @xmath278 .",
    "hence , by lemma  [ lementropyforkq ] , and ( [ eqsumgreaterthanmax ] ) , for @xmath279 and for all @xmath280 , we have @xmath281 where @xmath282 , @xmath283 and @xmath284 .",
    "take @xmath56 large enough such that the right - hand sides of ( [ eqfinalboundforkm ] ) and ( [ eqfinalboundempirical ] ) are less than @xmath285 and substitute in ( [ eqneedboundforkm ] ) and ( [ eqneedboundforempirical ] ) , respectively , and the result of the theorem follows .",
    "we simulate a  randomized clinical trial with flexible number of stages to examine the performance of the proposed censored - q - learning algorithm .",
    "we compare the estimated individualized treatment policy to various possible fixed treatments .",
    "we also compare the given expected survival times of different censoring levels . finally , we test the effect of ignoring the censoring .",
    "this section is organized as follows .",
    "we first describe the setting of the simulated clinical trial ( section  [ secsimulatedclincaltrial ] ) .",
    "we then describe the implementation of the simulation ( section  [ secimplementation ] ) .",
    "the simulation results appear in section  [ secresults ] .",
    "we consider the following hypothetical cancer trial .",
    "the duration of the trial is @xmath286 years .",
    "the state of each patient at each time - point @xmath287 $ ] includes the tumor size [ @xmath288 , and the wellness [ @xmath289 . the time - point @xmath290 such that @xmath291 is considered the failure time .",
    "we define the critical tumor size to be @xmath112 . at time @xmath292 such that @xmath293 , we begin a  treatment .",
    "we call the duration @xmath294 $ ] the @xmath295th stage . note that different patients may have different numbers of stages .    at each time - point @xmath292",
    ", we consider two optional treatments : a  more aggressive treatment ( @xmath113 ) , and a  less aggressive treatment ( @xmath296 ) .",
    "the immediate effects of treatment @xmath113 are @xmath297\\\\[-8pt ] t(u_i^+|a)&=&t(u_i)/(10 w(u_i ) ) , \\nonumber\\end{aligned}\\ ] ] that is , the wellness at time @xmath292 after treatment @xmath113 [ denoted by @xmath298 decreases by @xmath299 wellness units .",
    "the tumor size at time @xmath292 after treatment  @xmath113 [ denoted by @xmath300 decreases by a  factor of @xmath301 which reflects a  greater decrease of tumor size for a  larger wellness value .",
    "similarly , the immediate effects of the less aggressive treatment @xmath296 are @xmath302\\\\[-8pt ] t(u_i^+|b)&= & t(u_i)/(4 w(u_i ) ) , \\nonumber\\end{aligned}\\ ] ] which , in comparison to the treatment @xmath113 , has lower effect on the tumor size but also lower decrease of wellness .",
    "the wellness and tumor size at time @xmath303 follow the dynamics @xmath304\\\\[-8pt ] t(u)&= & t(u_i^+)+ 4 t(u_i^+)(u - u_i)/3 .\\nonumber\\end{aligned}\\ ] ] the stage that begins at time - point @xmath292 ends when either @xmath305 for some @xmath306 or when a  failure event occurs or at the end of the trial when @xmath307 . during this stage",
    ", we model the survival function of the patient as an exponential distribution with mean @xmath308 .",
    "the trajectories are constructed as follows .",
    "we assume that patients are recruited to the trial when their tumor size reaches the critical size , that is , for all patients @xmath309 , and hence @xmath310 is the beginning of the first stage .",
    "the wellness at the beginning of the first stage , @xmath311 , is uniformly distributed on the segment @xmath312 $ ] . with equal probability",
    ", a  treatment @xmath313 is chosen .",
    "if no failure event occurs during the first stage , the first stage ends when either @xmath314 for some @xmath315 or at the end of the trial .",
    "if the first stage ends before the end of the trial , then with equal probability another treatment @xmath316 is chosen .",
    "the trial continues in the same way until either a  failure time occurs or the trial ends .",
    "we note that the actual number of stages for each patient is a  random function of the initial state and the treatments chosen during the trial . due to the choices of model parameters ,",
    "the number of stages in the above dynamics is at least one and not more than three .    for each trajectory ,",
    "a  censoring variable @xmath34 is uniformly drawn from the segment @xmath317 $ ] for some constant @xmath318 , where the choice of the constant @xmath319 determines the expected percentage of censoring .",
    "when an event is censored , the trajectory ( i.e. , the states and treatments ) up to the point of censoring and the censoring time are given .",
    "the q - learning algorithm presented in section  [ secalgo ] was implemented in the matlab environment . for the implementation we used the spider library for matlab .",
    "the matlab code , as well as the data sets , are available online [ see @xcite ] .    the algorithm is implemented as follows . the input for the algorithm is a  set of trajectories obtained according to the dynamics described in section  [ secsimulatedclincaltrial ] .",
    "first , the kaplan ",
    "meier estimator for the survival function of the censoring variable is computed from the given trajectories .",
    "then , we set @xmath320 and compute @xmath321 , @xmath322 backwardly , as the minimizer of ( [ eqminimizationproblem ] ) over all the functions @xmath323 which are linear in the first variable . the policy  @xmath79 is computed from the functions @xmath324 using ( [ eqpihat ] ) .",
    "we tested the policy @xmath325 by constructing @xmath326 new trajectories , in which the choice of treatment at each stage is according to @xmath79 .",
    "one thousand initial wellness values were drawn uniformly from the segment @xmath312 $ ] .",
    "for each wellness value , a  treatment was chosen from the set @xmath327 , according to the policy  @xmath328 .",
    "the immediate effect of the treatment was computed according to .",
    "a  failure time was drawn from the exponential distribution with mean as described in the previous section ; denote this time by @xmath107 .",
    "the time that the tumor reached the critical size was computed according to the dynamics ( [ eqdynamcis ] ) , and we denote this time by @xmath329 . if both @xmath107 and @xmath329 are greater than @xmath286 ( the end of the trial ) , then the trajectory was ended after the first stage and the survival time for this patient was given as @xmath286 . otherwise , if @xmath330 , the trajectory was ended after the first stage and the survival time for this patient was given as @xmath107 .",
    "if @xmath331 , then at time  @xmath329 , a  second treatment is chosen according to the policy @xmath332 .",
    "the computation of the remainder of the trajectory is done similarly .",
    "the expected value of the policy @xmath79 is estimated by the mean of the survival times of all @xmath326 patients .",
    "we compared the results of the algorithm to all fixed treatment sequences @xmath333 , where @xmath334 .",
    "the expected values of the fixed treatment sequences were computed explicitly .",
    "we also compared the results to that of the optimal policy , which was also computed explicitly .",
    "first , we would like to examine the influence of the sample size and censoring percentage on the algorithm s performance .",
    "we simulated data sets of trajectories of sizes @xmath335 . for each set of trajectories we considered four levels of censoring : no censoring , @xmath336 censoring , @xmath337 censoring , and @xmath338 censoring .",
    "higher levels of ( uniform ) censoring were not considered since this requires drawing the censoring variable from a  segment @xmath317 $ ] for @xmath339 , which is in contrast to the assumption on the censoring variable ( see the beginning of section  [ secnotation ] ) .",
    "a  policy @xmath79 was computed for each combination of data set size and censoring percentage .",
    "the policy  @xmath79 was evaluated on a  data set of size @xmath326 , as described in section  [ secimplementation ] .",
    "we repeated the simulation @xmath340 times for each combination of data set size and censoring percentage .",
    "the mean values of the estimated mean survival time are presented in figure  [ figtestnum ] . a  comparison between the different fixed policies , policies obtained by the algorithm for different censoring levels , and the optimal policy appears in figure  [ figbar ] .",
    "as can be seen from both figures , the individualized treatment policies obtained by the algorithm are better than any fixed policy .",
    "moreover , as the number of observed trajectories increases , the expected survival time increases , for all censoring percentages .    censoring , @xmath337 censoring and @xmath338 censoring , respectively .",
    "the expected survival time was computed as the mean of @xmath340 repetitions of the simulation .",
    "the black straight line , blue dashed straight line , and the dot - dashed red straight line correspond to the expected survival times of the optimal policy , the best fixed treatment policy , and the average of the fixed treatment policies , respectively . ]     indicates the policy that chooses @xmath341 at the @xmath295th stage .",
    "the four dark gray bars represent the expected survival times for policy @xmath79 obtained by the algorithm with no censoring , @xmath336  censoring , @xmath337 censoring and @xmath338 censoring .",
    "the white bar is the expected value of the optimal policy .",
    "the values of the fixed treatments and the optimal policy were computed analytically while the values of @xmath79 are the means of @xmath340 repetitions of the simulation on @xmath342 trajectories . ]     censoring , @xmath337 censoring and @xmath338 censoring .",
    "each boxplot is based on @xmath340 repetitions of the simulation for each given data set size and censoring percentage . ]",
    "trajectory set .",
    "the left panel presents both the optimal q - function ( solid red curve ) and the estimated q - function ( dashed blue curve ) for different wellness levels and when treatment @xmath113 is chosen .",
    "similarly , the middle panel shows both q - functions when treatment @xmath296 is chosen .",
    "the right panel shows the optimal value function ( solid red curve ) and the estimated value function ( dashed blue curve ) . ]",
    "we also examined the influence of the sample size and censoring percentage on the distribution of estimated expected survival time .",
    "we simulated data sets of sizes @xmath343 and we considered the four levels of censoring as before .",
    "as can be seen from figure  [ figtestnumlong ] , the variance decreases when the sample size becomes larger . also , the variance is smaller for smaller percentage of censoring , although the difference is modest .",
    ", when no failure event occurs during the trial .",
    "the policy @xmath79 was estimated from @xmath344 trajectories .",
    "the results were computed using a  size @xmath345 testing set . ]",
    "note that the maximum expected survival times obtained by the algorithm are a  little bit above @xmath346 months ( see both figures  [ figtestnum ] and  [ figbar ] ) , while the value of the optimal policy is @xmath347 .",
    "the difference follows from the fact that the q - functions estimated by the algorithm are linear while the optimal q - function is not ( see figure  [ figqfunction ] ) .",
    "it is worth mentioning that even in the class of linear functions on which the optimization is done there are q - functions that yield higher values .",
    "this fact is often referred to as the `` mismatch '' that follows from the fact that optimization of the value function is not performed explicitly , but rather through optimization of the q - functions [ see @xcite , @xcite , for more details ] .",
    "figure  [ figtestnumofactions ] shows the number of treatments that were needed for patients that followed the policy @xmath79 and did not have a  failure event during the trial .",
    "as can be seen from this figure , patients with high initial wellness need only one treatment . on the other hand ,",
    "patients with very low initial wellness value need three treatments .    finally , we checked the effect of ignoring the censoring on the expected survival time .",
    "we considered two ways of ignoring the censoring .",
    "first , we consider an algorithm that ignores the weights in the minimization problem  ( [ eqminimizationproblem ] ) .",
    "this is equivalent to deleting the last stage from each trajectory that was censored .",
    "we also consider an algorithm that deletes all censored trajectories . in the example presented in figures  [ figtestnum][figtestnumofactions ] , where uniform censoring takes place",
    ", there is a  relatively moderate difference between the expected survival time for the proposed algorithm and the other two algorithms that ignore censoring .",
    "however , when the censoring variable follows the exponential distribution ( leaving fewer observations with longer survival times ) , the bias from ignoring the censored trajectories is substantial , as can be seen in figure  [ figcensoring ] .",
    "we studied a  framework for multistage decision problems with flexible number of stages in which the rewards are survival times and are subject to censoring .",
    "we proposed a  novel q - learning algorithm adjusted for censoring .",
    "we derived the generalization error properties of the algorithm and demonstrated the algorithm performance using simulations .     censoring on average .",
    "the expected survival time was computed as the mean of @xmath340 repetitions of the simulation . ]    the work as presented is applicable to real - world multistage decision problems with censoring .",
    "however , two main issues should be noted .",
    "first , we assumed that censoring is independent of observed trajectories .",
    "it would be useful to relax this assumption and allow censoring to depend on the covariates .",
    "developing an algorithm that works under this relaxed assumption is a  challenge .",
    "second , we have used the inverse - probability - of - censoring weighting to correct the bias induced by censoring .",
    "when the percentage of censored trajectories is large , the algorithm may be inefficient . finding a  more efficient algorithm is also an open question .",
    "the main goal of this section is to provide an exponential bound on the difference between the empirical expectation @xmath348 and the true expectation @xmath349 as a  function of the uniform entropy of the class of functions [ see  ( [ eqslashe ] ) ] .",
    "this result appears in lemma  [ lementropyforkq ] .",
    "similar results for glivenko ",
    "cantelli classes , donsker classes and bounded uniform entropy integral ( buei ) classes can be found in @xcite and @xcite .",
    "[ lemcoveringnumberforclasses ] let @xmath350 be @xmath351 sets of functions .",
    "assume that for every @xmath352 , @xmath353 .",
    "let @xmath354 satisfy @xmath355 for every @xmath356 , where @xmath357 .",
    "let @xmath101 be a  finitely discrete probability measure .",
    "define @xmath358 .",
    "then @xmath359    the proof is similar to the proof of @xcite , lemma  9.13 .",
    "let @xmath360 satisfy @xmath361 for @xmath362 .",
    "note that @xmath363 which implies ( [ eqcoveringnumberforclasses ] ) .",
    "the following two corollaries are a  direct result of lemma  [ lemcoveringnumberforclasses ] :    [ corinverse ] let @xmath364 is monotone decreasing @xmath365\\mapsto[k_{\\min},1]\\}$ ] .",
    "define @xmath366 .",
    "let @xmath101 be a  finitely discrete probability measure .",
    "then @xmath367    note that inequality ( [ eqphisproperty ] ) holds for @xmath368 and @xmath369 , and the results follow from lemma  [ lemcoveringnumberforclasses ] .",
    "[ cormax ] let @xmath370 .",
    "define @xmath371 .",
    "let @xmath101 be a  finitely discrete probability measure .",
    "then @xmath372    since @xmath373 , inequality ( [ eqphisproperty ] ) holds for @xmath374 .",
    "the results now follow from lemma  [ lemcoveringnumberforclasses ] .",
    "we also need the following lemma and its corollary :    [ lemmultipilcationentropy ] let @xmath375 and @xmath376 be two function classes uniformly bounded in absolute value by @xmath377 and @xmath378 , respectively .",
    "define @xmath379 .",
    "then @xmath380    let @xmath381 where @xmath382 .",
    "note that @xmath383 the result follows .",
    "[ corsquare ] let @xmath384 be a  function class uniformly bounded in absolute value by @xmath201 .",
    ". then @xmath386    apply lemma  [ lemmultipilcationentropy ] with @xmath387 .",
    "we use the previous results to prove the following lemma :    [ lementropyforkq ] let @xmath388\\mapsto[k_{\\min},1]\\ } , \\\\ & & \\mathcal{r}=\\biggl\\{\\frac{1}{k(t)}\\bigl(r+\\max _ { a}q_{t+1}(x , a)-q_{t}(x , a)\\bigr)^2\\dvtx r\\in[0,\\tau],\\\\ & & \\hspace*{123pt}{}q_t\\in{\\mathcal{q}}_t , { \\mathcal{q}}_{t+1}\\in{\\mathcal{q}}_{t+1},k\\in\\mathcal{k}\\biggr\\ } , \\end{aligned}\\ ] ] where @xmath389 and @xmath390 .",
    "assume that the uniform entropy bound for each of the spaces @xmath199 ( [ equniformentropy ] ) holds .",
    "then :    [ lementropyforkq1 ] there are constants @xmath391 and @xmath392 such that @xmath393 , where @xmath279 .",
    "[ lementropyforkq2 ] for every @xmath280 and @xmath394 , @xmath395 where @xmath396 , the constants @xmath221 and @xmath222 depend only on  @xmath391 , @xmath392 and @xmath397 , and where @xmath398 is outer probability .",
    "let @xmath279 .",
    "note that uniform entropy bound ( [ equniformentropy ] ) for the spaces @xmath199 holds also for @xmath392 .",
    "note that by corollary  [ cormax ] , @xmath399 , @xmath400 . since @xmath401",
    ", we can apply lemma  [ lemcoveringnumberforclasses ] to the class @xmath402,q_t\\in{\\mathcal{q}}_t , q_{t+1}\\in{\\mathcal{q}}_{t+1}\\bigr\\}\\ ] ] with @xmath403 and @xmath404 to obtain @xmath405 , @xmath406 , where we used the fact that the segment @xmath36 $ ] can be covered by no more than @xmath407 balls of radius @xmath207 and that @xmath408 . by corollary  [ corsquare ]",
    ", we have @xmath409 or , equivalently , @xmath410 where @xmath219 is a  uniform bound for @xmath411 , and @xmath412 .    by kosorok [ ( @xcite ) , lemma 9.11 ] , @xmath413 for some universal constant @xmath414 which is independent of the choice of probability measure @xmath101 . by corollary  [ corinverse ] , @xmath415    applying lemma  [ lemmultipilcationentropy ] to @xmath416",
    ", we obtain @xmath417 since this inequality holds for every finitely discrete probability measure @xmath101 , assertion  [ lementropyforkq1 ] is proved .",
    "the second assertion follows from @xcite , theorem 2.14.10 .",
    "the authors are grateful to the anonymous reviewers and the associate editor for their helpful suggestions and comments ."
  ],
  "abstract_text": [
    "<S> we develop methodology for a  multistage decision problem with flexible number of stages in which the rewards are survival times that are subject to censoring . </S>",
    "<S> we present a  novel q - learning algorithm that is adjusted for censored data and allows a  flexible number of stages . </S>",
    "<S> we provide finite sample bounds on the generalization error of the policy learned by the algorithm , and show that when the optimal q - function belongs to the approximation space , the expected survival time for policies obtained by the algorithm converges to that of the optimal policy . </S>",
    "<S> we simulate a  multistage clinical trial with flexible number of stages and apply the proposed censored - q - learning algorithm to find individualized treatment regimens . the methodology presented in this paper </S>",
    "<S> has implications in the design of personalized medicine trials in cancer and in other life - threatening diseases .    .    </S>"
  ]
}