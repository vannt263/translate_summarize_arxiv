{
  "article_text": [
    "given the class information of training data , metric learning methods for dimensionality reduction and data visualization essentially learn a linear or nonlinear transformation from a high - dimensional input feature space to a low - dimensional embedding space , aiming at increasing the similarity between pairwise data points from the same class while decreasing the similarity between pairwise data points from different classes in the embedding space .",
    "these methods in combination with knn have been widely used in many applications including computer vision , information retrieval , and bioinformatics .",
    "recent surveys on metric learning can be found in @xcite .",
    "however , most of these approaches , including the popular maximally collapsing metric learning ( mcml )  @xcite , neighborhood component analysis ( nca )  @xcite , and large - margin nearest neighbor ( lmnn )  @xcite , need to model neighborhood structures by comparing pairwise training data points either for learning parameters or for constructing target neighborhoods in the input feature space , which results in quadratic computational complexity requiring careful tuning and heuristics to get approximate solutions in practice and thus limits the methods scalability .",
    "moreover , during testing , knn is often employed to compare each test data point against all training data points in the input feature or embedding space , which is also expensive in terms of both computational cost and resources required .",
    "in addition , a lot of previous methods , e.g. , mcml , on one extreme , focus on learning a mahalanobis metric that is equivalent to learning a linear feature transformation matrix and thus incapable of achieving the goal of collapsing classes . on the other extreme , nonlinear metric learning methods based on deep neural networks such as dt - mcml and dt - nca  @xcite are powerful but very hard to learn and require complicated procedures such as tuning network architectures and tuning many hyperparameters . for data embedding and visualization purposes , most users are reluctant to go through these complicated procedures , which explains why dt - mcml and dt - nca were not widely used although they are much more powerful than simpler mcml , nca , and lmnn .    to address the aforementioned issues of previous metric learning methods for dimensionality reduction and data visualization , in this paper , we present an exemplar - centered supervised shallow parametric data embedding model based on a maximally collapsing metric learning objective and student @xmath0-distributions . our model learns a shallow high - order parametric embedding function that is as powerful as a deep neural network but much easier to learn .",
    "moreover , during training , our model avoids pairwise training data comparisons and compares training data only with some jointly learned exemplars or precomputed exemplars from supervised k - means centers , resulting in an objective function with linear computational complexity with respect to the size of training set .",
    "in addition , during testing , our model only compares each test data point against a very small number of exemplars . as a result",
    ", our model in combination with knn accelerates knn using high - dimensional input features by hundreds of times owing to the benefits of both dimensionality reduction and sample size reduction , and achieves much better performance .",
    "even surprisingly , in terms of both accuracy and testing speed , our shallow model based on pre - computed exemplars significantly outperforms state - of - the - art deep embedding method dt - mcml .",
    "we also empirically observe that , using a very small number of randomly sampled exemplars from training data , our model can also achieve competitive classification performance .",
    "we call our proposed model exemplar - centered high order parametric embedding ( en - hope ) .",
    "our contributions in this paper are summarized as follows : ( 1 ) we propose a salable metric learning strategy for data embedding with an objective function of linear computational complexity , avoiding pairwise training data comparisons ; ( 2 ) our method compares test data only with a small number of exemplars and gains speedup of knn by hundreds of times ; ( 3 ) our approach learns a simple shallow high - order parametric embedding function , beating state - of - the - art embedding models on several benchmark datasets in term of both speed and accuracy .",
    "metric learning methods and their applications have been comprehensively surveyed in  @xcite . among them ,",
    "our proposed method en - hope is closely related to the ones that can be used for dimensionality reduction and data visualization , including mcml  @xcite , nca  @xcite , lmnn  @xcite , nonlinear lmnn  @xcite , and their deep learning extensions such as dt - mcml  @xcite , dt - nca  @xcite , and dnet - knn  @xcite .",
    "en - hope is also related to neighborhood - modeling dimensionality reduction methods such as lpp  @xcite , t - sne  @xcite , its parametric implementation sne - encoder  @xcite and deep parametric implementation pt - sne  @xcite .",
    "the objective functions of all these related methods have at least quadratic computational complexity with respect to the size of training set due to pairwise training data comparisons required for either loss evaluations or target neighborhood constructions .",
    "en - hope is closely related to a recent sample compression method called stochastic neighbor compression ( snc )  @xcite for accelerating knn classification in a high - dimensional input feature space .",
    "snc learns a set of high - dimensional exemplars by optimizing a modified objective function of nca .",
    "en - hope differs from snc in several aspects : first , their objective functions are different ; second , en - hope learns a nonlinear metric based on a shallow model for dimensionality reduction and data visualization , but snc does not have such capabilities ; third , en - hope does not necessarily learn exemplars , instead , which can be precomputed .",
    "we will compare en - hope to snc in the experiments to evaluate the compression ability of en - hope , however , the focus of en - hope is for data embedding and visualization but not for sample compression in a high - dimensional space .",
    "en - hope learns a shallow parametric embedding function by considering high - order feature interactions .",
    "high - order feature interactions have been studied for learning boltzmann machines , autoencoders , structured outputs , feature selections , and biological sequence classification  @xcite .",
    "to the best of our knowledge , our work here is the first successful one to model input high - order feature interactions for supervised data embedding and exemplar learning .",
    "in this section , we introduce mcml and dt - mcml at first .",
    "then we describe our shallow parametric embedding function based on high - order feature interactions .",
    "finally , we present our scalable model en - hope .      given a set of data points @xmath1 , where @xmath2 is the input feature vector , @xmath3 is the class label of a labeled data point , and @xmath4 is the total number of classes .",
    "mcml learns a mahalanobis distance metric to collapse all data points in the same class to a single point and push data points from different classes infinitely farther apart . learning a mahalanobis distance metric",
    "can be thought of as learning a linear feature transformation @xmath5 from the high - dimensional input feature space to a low - dimensional latent embedding space , where @xmath6 , and @xmath7 . for data visualization",
    ", we often set @xmath8 .",
    "mcml assumes , @xmath9 , the probability of each data point @xmath10 chooses every other data point @xmath11 as its nearest neighbor in the latent embedding space follows a gaussian distribution , @xmath12 and @xmath13    to maximally collapse classes , mcml minimizes the sum of the kullback - leibler divergence between the conditional probabilities @xmath9 computed in the embedding space and the `` ground - truth '' probabilities @xmath14 calculated based on the class labels of training data . specifically , @xmath15 iff @xmath16 and @xmath17 iff @xmath18 .",
    "formally , the objective function of the mcml is as follows : @xmath19\\log q_{j|i } + const,\\ ] ] where @xmath20 $ ] is an indicator function .",
    "however , learning a mahalanobis metric requires solving a positive semidefinite programming problem , which is computationally prohibitive and prevents mcml from scaling to a fairly big dataset .",
    "moreover , a linear feature transformation is very constrained and makes it impossible for mcml to achieve its goal of collapsing classes .",
    "dt - mcml extends mcml in two aspects : ( 1 ) it learns a powerful deep neural network to parameterize the feature transformation function @xmath21 ; ( 2 ) it uses a symmetric heavy - tailed @xmath0-distribution to compute @xmath9 for supervised embedding due to its capabilities of reducing overfitting , creating tight clusters , increasing class separation , and easing gradient optimization . formally , this stochastic neighborhood metric first centers a @xmath0-distribution over @xmath22 , and then computes the density of @xmath23 under the distribution as follows .",
    "@xmath24 although dt - mcml based on a deep neural network has a powerful nonlinear feature transformation , parameter learning is hard and requires complicated procedures such as tuning network architectures and tuning many hyperparameters .",
    "most users who are only interested in data embedding and visualization are reluctant to go through these complicated procedures . here",
    "we propose to use high - order feature interactions , which often capture structural knowledge of input data , to learn a shallow parametric embedding model instead of a deep model .",
    "the shallow model is much easier to train and does not have many hyperparameters . in the following",
    ", the shallow high - order parametric embedding function will be presented .",
    "we expand each input feature vector @xmath25 to have an additional component of @xmath26 for absorbing bias terms , that is , @xmath27 $ ] , where @xmath28 .",
    "the @xmath29-order feature interaction is the product of all possible @xmath29 features @xmath30 where , @xmath31 , and @xmath32 .",
    "ideally , we want to use each @xmath29-order feature interaction as a coordinate and then learn a linear transformation to map all these high - order feature interactions to a low - dimensional embedding space .",
    "however , it s very expensive to enumerate all possible @xmath29-order feature interactions .",
    "for example , if @xmath33 , we must deal with a @xmath34-dimensional vector of high - order features .",
    "we approximate a sigmoid - transformed high - order feature mapping @xmath21 by constrained tensor factorization as follows ( derivations omitted due to space constraint ) , @xmath35 where @xmath36 is a bias term , @xmath37 is a factorization matrix , @xmath38 is the @xmath39-th column of @xmath40 , @xmath41 and @xmath42 are projection matrices , @xmath43 is the @xmath44-th component of @xmath45 , @xmath46 is the number of factors , @xmath47 is the number of high - order hidden units , and @xmath48 .",
    "because the last component of @xmath49 is 1 for absorbing bias terms , the full polynomial expansion of @xmath50 essentially captures all orders of input feature interactions up to order @xmath29 .",
    "empirically , we find that @xmath51 works best for all datasets we have and set @xmath51 for all our experiments .",
    "the hyperparameters @xmath46 and @xmath47 are set by users .",
    "combining equation  [ obj ] , equation  [ eqn : symmq ] and the feature transformation function in equation  [ shopemap ] leads to a method called high order parametric embedding ( hope ) .",
    "as mcml and dt - mcml , the objective function of hope involves comparing pairwise training data and thus has quadratic computational complexity with respect to the sample size .",
    "the parameters of hope are learned by conjugate gradient descent .",
    "building upon hope for data embedding and visualization described earlier , we present two related approaches to implement en - hope , resulting in an objective function with linear computational complexity with respect to the size of training set .",
    "the underlying intuition is that , instead of comparing pairwise training data points , we compare training data only with a small number of exemplars in the training set to achieve the goal of collapsing classes , collapsing all training data to the points defined by exemplars . in the first approach ,",
    "we simply precompute the exemplars by supervised k - means and only update the parameters of the embedding function during training . in the second approach ,",
    "we simultaneously learn exemplars and embedding parameters during training . during testing",
    ", fast knn classification can be efficiently performed in the embedding space against a small number of exemplars especially when the dataset is huge .",
    "given the same dataset @xmath52 with formal descriptions as introduced in section  [ sec : sup ] , we aim to obtain @xmath53 exemplars from the whole dataset with their designated class labels uniformly sampled from the training set to account for data label distributions , where @xmath53 is a user - specified free parameter and @xmath54 .",
    "we denote these exemplars by @xmath55 . in the first approach",
    ", we perform k - means on the training data to identify the same number of exemplars as in the sampling step for each class . then we minimize the following objective function to learn high - order embedding parameters @xmath56 while keeping the exemplars @xmath57 fixed , @xmath58\\log q_{j|i } + const\\end{aligned}\\ ] ] where @xmath10 indexes training data points , @xmath11 indexes exemplars , @xmath56 denotes the high - order embedding parameters @xmath59 in equation  [ shopemap ] , @xmath14 is calculated in the same way as in the previous description , but @xmath9 is calculated with respect to exemplars , @xmath60 where @xmath61 denotes the high - order embedding function as described in equation  [ shopemap ] .",
    "note that unlike the probability distribution in equation  [ eqn : symmq ] , @xmath9 here is computed only using the pairwise distances between training data points and exemplars .",
    "this small modification has significant benefits .",
    "because @xmath54 , compared to the quadratic computational complexity with respect to @xmath62 of equation  [ obj ] , the objective function in equation  [ exobj ] has a linear computational complexity with respect to @xmath62 . in the second approach",
    ", we jointly learn the high - order embedding parameters @xmath56 and the exemplars @xmath57 by optimizing the objective function in equation  [ exobj ] .",
    "the derivative of the above objective function with respect to exemplar @xmath63 is as follows , @xmath64 in both approaches to implementing en - hope , all the model parameters are learned using conjugate gradient descent .",
    "we call the first approach en - hope ( k - means exemplars ) and the second approach en - hope ( learned exemplars ) .",
    "[ tab : accuracy : mnist ]    .error rates by knn on the 2-dimensional representations produced by hope and en - hope and other methods on top of vgg features of mnist data .",
    "the knn error rate in the original 512-dimensional space generated by vgg is 0.62 , which is comparable to the knn performance on the 2-dimensional representations produced by hope and en - hope . [",
    "cols=\"^,^\",options=\"header \" , ]     en - hope speeds up computational efficiency of fast information retrieval such as knn classification used in the above experiments by hundreds of times . table  [ tab : speedup ] shows the experimentally observed computational speedup of en - hope over standard knn on our desktop with intel xeon 2.60ghz cpu and 48 gb memory on different datasets .",
    "the test error rates by knn in high - dimensional feature space are much worse than the ones produced by en - hope even in a much lower feature dimension , i.e. , the two - dimensional latent space . in detail , on our desktop , for classifying 10000 mnist test data , standard knn takes 124.97 seconds , but our method en - hope with 20 learned exemplars only takes 0.24 seconds including the time for computing the two - dimensional embedding of test data .",
    "in other words , our method en - hope has 463 times speedup over standard knn along with much better classification performance .",
    "this computational speedup will be more pronounced on massive datasets .",
    "stochastic neighbor compression ( snc )  @xcite is a leading sample compression method in high - dimensional input feature space .",
    "in contrast , snc can only achieve up to 136 times speedup over knn with comparable performance on mnist with at least 600 learned exemplars  @xcite .",
    "that is , it only achieves a compression ratio as high as 30 times of that of en - hope .",
    "part of the reason here is that it is not designed for data embedding and visualization and thus unable to compress dataset from the aspect of dimensionality reduction .",
    "this assumption is further verified by the following experimental observations . when using 20 learned exemplars in the high - dimensional input feature space , snc produced test error rates of 6.31% on mnist and 17.50% on usps , which are much higher than those of en - hope .",
    "also , if we pre - project data to two - dimensional space by other methods such as pca or lmnn and then run snc , the results of snc should be much worse than the ones in the high - dimensional input feature space . although the focus of en - hope is not for sample compression but for data embedding and visualization by collapsing classes , when we embed mnist data to a 10-dimensional latent space using en - hope with 20 exemplars , we can further reduce the test error rate from @xmath65% to @xmath66% .",
    "we also further evaluate the performance of our shallow model en - hope with 20 learned exemplars against deep method dt - mcml on the mnist data .",
    "when compared to dt - mcml , en - hope achieves 316 times speedup for classifying mnist test data in 2d owing to its proposed exemplar learning functionality .",
    "it is also worth mentioning that , although both methods have the overhead of computing the 2d embedding of test data , en - hope has 2 times speedup over dt - mcml on this burden owing to its shallow architecture .",
    "in this paper , we present an exemplar - centered supervised shallow parametric data embedding model en - hope by collapsing classes for data visualization and fast knn classification . owing to the benefit of a small number of precomputed or learned exemplars ,",
    "en - hope avoids pairwise training data comparisons and only has linear computational cost for both training and testing .",
    "experimental results demonstrate that en - hope accelerates knn classification by hundreds of times , outperforms state - of - the - art supervised embedding methods , and effectively collapses classes for impressive two - dimensional data visualizations in terms of both classification performance and visual effects .    in the future",
    ", we aim to extend our method to an unsupervised learning setting to increase the scalability of traditional t - sne , for which we just need to compute the pairwise probability @xmath67 using high - dimensional feature vectors instead of class labels and optimize exemplars accordingly .",
    "jacob goldberger , sam  t. roweis , geoffrey  e. hinton , and ruslan salakhutdinov .",
    "neighbourhood components analysis . in _ proceedings of advances in neural information processing systems 19 _ ,",
    "pages 513520 . 2004 .",
    "dor kedem , stephen tyree , fei sha , gert  r. lanckriet , and kilian  q weinberger .",
    "non - linear metric learning . in _ proceedings of advances in neural information processing systems 25 _ ,",
    "pages 25732581 .",
    "2012 .",
    "martin  renqiang min , laurens van  der maaten , zineng yuan , anthony  j. bonner , and zhaolei zhang . deep supervised t - distributed embedding . in",
    "_ proceedings of the 27th international conference on machine learning _ , pages 791798 , 2010 .",
    "martin  renqiang min , xia ning , chao cheng , and mark gerstein .",
    "interpretable sparse high - order boltzmann machines . in _ proceedings of the seventeenth international conference on artificial intelligence and statistics _ , pages 614622 , 2014 .",
    "marcaurelio ranzato , alex krizhevsky , and geoffrey  e. hinton .",
    "factored 3-way restricted boltzmann machines for modeling natural images . in _ proceedings of the thirteenth international conference on artificial intelligence and statistics , chia laguna resort , sardinia , italy , may 13 - 15 , 2010 _ , pages 621628 , 2010 .",
    "laurens van  der maaten .",
    "learning a parametric embedding by preserving local structure . in _ proceedings of the 12th international conference on artificial intelligence and statistics _ ,",
    "pages 384391 , 2009 ."
  ],
  "abstract_text": [
    "<S> metric learning methods for dimensionality reduction in combination with k - nearest neighbors ( knn ) have been extensively deployed in many classification , data embedding , and information retrieval applications . </S>",
    "<S> however , most of these approaches involve pairwise training data comparisons , and thus have quadratic computational complexity with respect to the size of training set , preventing them from scaling to fairly big datasets . </S>",
    "<S> moreover , during testing , comparing test data against all the training data points is also expensive in terms of both computational cost and resources required . </S>",
    "<S> furthermore , previous metrics are either too constrained or too expressive to be well learned . to effectively solve these issues , we present an exemplar - centered supervised shallow parametric data embedding model , using a maximally collapsing metric learning ( mcml ) objective . </S>",
    "<S> our strategy learns a shallow high - order parametric embedding function and compares training / test data only with learned or precomputed exemplars , resulting in a cost function with linear computational complexity for both training and testing . </S>",
    "<S> we also empirically demonstrate , using several benchmark datasets , that for classification in two - dimensional embedding space , our approach not only gains speedup of knn by hundreds of times , but also outperforms state - of - the - art supervised embedding approaches . </S>"
  ]
}