{
  "article_text": [
    "in 1971 , viterbi @xcite published a nowadays classical upper bound on the bit error probability @xmath9 for viterbi decoding , when convolutional codes are used to communicate over the binary symmetric channel ( bsc ) .",
    "this bound was derived from the extended path weight enumerators , obtained using a signal flow chart technique for convolutional encoders .",
    "later , van de meeberg @xcite used a very clever observation to tighten viterbi s bound for large signal - to - noise ratios ( snrs ) .",
    "the challenging problem of deriving an expression for the _ exact _ ( decoding ) bit error probability was first addressed by morrissey in 1970 @xcite for a suboptimal feedback decoding algorithm .",
    "he obtained the same expression for the exact bit error probability for the rate @xmath0 , memory @xmath1 ( @xmath2-state ) convolutional encoder with generator matrix @xmath10 that best et al .",
    "@xcite obtained for viterbi decoding .",
    "their method is based on considering a markov chain of the so - called metric states of the viterbi decoder ; an approach due to burnashev and cohn @xcite .",
    "an extension of this method to the rate @xmath0 memory @xmath4 ( @xmath5-state ) convolutional encoder with generator matrix @xmath11 was published by lentmaier et al .",
    "@xcite .",
    "in this paper we use a different and more general approach to derive a closed form expression for the exact ( decoding ) bit error probability for viterbi decoding of convolutional encoders , when communicating over the bsc as well as the quantized additive white gaussian noise ( awgn ) channel .",
    "our new method allows the calculation of the exact bit error probability for more complex encoders in a wider range of code rates than the methods of @xcite and @xcite . by considering a random tie - breaking strategy",
    ", we average the information weights over the channel noise sequence and the sequence of random decisions based on coin - flippings ( where the coin may have more than two sides depending on the code rate ) .",
    "unlike the backward recursion in @xcite and @xcite , the bit error probability averaged over time is obtained by deriving and solving a recurrent matrix equation for the average information weights at the current and previous states of a trellis section when the maximum - likelihood branches are decided by the viterbi decoder at the current step .    to illustrate our method",
    ", we use a rate @xmath7 systematic convolutional @xmath2-state encoder whose minimal realization is given in observer canonical form , since this encoder is both general and simple .    in section  [ sec : recurrent_equation ] , the problem of computing the exact bit error probability is reformulated via the average information weights .",
    "a recurrent matrix equation for these average information weights is derived in section  [ sec : inf_weights ] and solved in section  [ sec : solving_the_recurrent_equation ] . in section  [ sec : examples ] , we give additional examples of rate @xmath0 and @xmath7 encoders of various memories .",
    "furthermore , we analyze a rate @xmath0 @xmath5-state encoder used to communicate over the quantized additive white gaussian noise ( awgn ) channel and show an interesting result that would be difficult to obtain without being able to calculate the exact bit error probability .    before proceeding",
    ", we would like to emphasize that the bit error probability is an encoder property , neither a generator matrix property nor a convolutional code property .",
    "assume that the all - zero sequence is transmitted over a bsc with crossover probability @xmath12 and let @xmath13 denote the weight of the information sequence corresponding to the code sequence decided by the viterbi decoder at state @xmath14 and time instant @xmath15 . if the initial values @xmath16 are known , then the random process @xmath13 ,",
    "@xmath17 , is a function of the random sequence of the received @xmath18-tuples @xmath19 , @xmath20 , and the coin - flippings used to resolve ties .",
    "our goal is to determine the mathematical expectation of the random variable @xmath13 over this ensemble , since for rate @xmath21 minimal convolutional encoders the bit error probability can be computed as the limit    rcl [ eq : viterbi_bit_error_probability ] p _ & = & _ t    assuming that this limit exists .",
    "_ if we consider nonminimal encoders , all states equivalent to the all - zero state have to be also taken into account .",
    "we consider encoder realizations in both controller and observer canonical form and denote the encoder states by @xmath14 , @xmath22 , where is the set of all possible encoder states .    during the decoding step at time instant @xmath23",
    "the viterbi algorithm computes the cumulative viterbi branch metric vector @xmath24 for the time instant @xmath23 using the vector @xmath25 and the received @xmath18-tuple @xmath26 .",
    "it is convenient to normalize the metrics such that the cumulative metrics at every all - zero state will be zero , that is , we subtract the value @xmath27 from @xmath28 and introduce the normalized cumulative branch metric vector    rcl @xmath29_t & = & ( _ t(1 ) _",
    "t(2 )  _ t(-1 ) ) + & = & ( _ t(1)-_t(0 ) _",
    "t(2)-_t(0 )  _ t(-1)-_t(0 ) )    for example , for a @xmath2-state encoder we obtain the scalar    rcl _ t & = & _ t(1 )    while for a @xmath5-state encoder we have the vector    rcl @xmath29_t & = & ( _ t(1 ) _",
    "t(2 ) _ t(3 ) )    the elements of the random vector @xmath30 belong to a set whose cardinality @xmath31 depends on the channel model , encoder structure , and the tie - breaking rule .",
    "enumerating the vectors @xmath30 by numbers @xmath32 which are random variables taking on @xmath31 different integer values @xmath33 , the sequence of numbers @xmath32 forms an @xmath31-state markov chain @xmath34 with transition probability matrix @xmath35 , where @xmath36    a minimal encoder for the generator matrix given in equation . ]",
    "+    let @xmath37 be the vector of information weights at time instant @xmath15 that depends both on the @xmath38 encoder states @xmath39 and on the @xmath31 normalized cumulative metrics @xmath40 ; that is , @xmath37 is expressed as the following vector with @xmath41 entries    rclccccr @xmath42_t & = & ( & @xmath42_t(=0 ) & @xmath42_t(=1 ) &  & @xmath42_t(=-1 ) )    where    rclcccr @xmath42_t ( ) & = & ( & w_t(^(0 ) , ) & w_t(^(1 ) , ) &  & w_t(^(m-1 ) , ) )    then can be rewritten as    rcl p _ & = & _ t = _ t   + & = & _ t = _ t   + & = & _ t",
    "( @xmath43_1,m @xmath44_1,m  @xmath44_1,m)^ [ eq : viterbi_bit_error_probability_w ]    where @xmath45 and @xmath46 denote the all - one and the all - zero row vectors of length @xmath31 , respectively , @xmath47 represents the length @xmath48 vector of the average information weights , while the length @xmath31 vector of average information weights at the state @xmath14 is given by @xmath49 .",
    "note that the mathematical expectations in are computed over the sequences of channel noises and coin - flipping decisions .    to illustrate the introduced notations",
    ", we use the rate @xmath7 memory @xmath1 , overall constraint length @xmath50 , minimal encoder with systematic generator matrix @xmath51 it has a @xmath2-state realization in observer canonical form as shown in fig .",
    "[ fig : ocf_realization ] .",
    "illustration of the @xmath52-state markov chain formed by the sequences of normalized cumulative metric states @xmath40 . ]    assuming that the normalized cumulative metric state is @xmath53 , we obtain the eight trellis sections given in fig .",
    "[ fig : eight_trellis_sections ] .",
    "these trellis sections yield the normalized cumulative metric states @xmath54 . using @xmath55 and @xmath56",
    ", we obtain @xmath8 additional trellis sections and two additional normalized cumulative metric states @xmath57 . from the metrics @xmath58 and @xmath59",
    ", we get another @xmath8 trellis sections but those will not yield any new metrics . thus , in total we have @xmath60 normalized cumulative metric states @xmath61 . together with the eight different received triples , @xmath62 , @xmath63 , @xmath64 , @xmath65 , @xmath66 , @xmath67 , @xmath68 , @xmath69 , they correspond to in total @xmath70 different trellis sections .",
    "the bold branches in fig .  [ fig : eight_trellis_sections ] correspond to the branches decided by the viterbi decoder at time instant @xmath23 .",
    "when we have more than one branch with maximum normalized cumulative metric entering the same state , we have a tie which we , in our analysis , resolve by fair coin - flipping .    hence , the normalized cumulative metric @xmath71 is a @xmath52-state markov chain with transition probability matrix @xmath35 , @xmath72 .",
    "from the four trellis sections , ( a ) , ( b ) , ( g ) , and ( h ) , in fig .  [ fig : eight_trellis_sections ] we obtain    rcl _ 0(-1 ) & = & ( @xmath73_t = 000 ) + ( @xmath73_t = 001 ) + & & + ( @xmath73_t = 110 ) + ( @xmath73_t = 111 ) + & = & q^3 + pq^2 + p^2 q + p^3 = p^2 + q^2    while the four trellis sections , ( c ) , ( d ) , ( e ) , and ( f ) , yield @xmath74 where @xmath75 .    similarly , we can obtain the remaining transition probabilities from the @xmath76 trellis sections not included in fig .",
    "[ fig : eight_trellis_sections ] .",
    "their transition probability matrix follows as    c =    whose metric state markov chain is shown in fig .",
    "[ fig : state_transition_diagram ] .",
    "let @xmath77 denote the probabilities of the @xmath31 different normalized cumulative metric values of @xmath71 , that is , @xmath78 .",
    "their stationary distribution is denoted @xmath79 and is determined as the solution of , for example , the first @xmath80 equations of    rcl [ steady - state ] @xmath81 _ & = & @xmath81 _    and    rcl _ i=0^m-1 p_^(i ) & = & 1    for the @xmath2-state convolutional encoder with generator matrix we obtain    c @xmath81_^ = + (    ccccccccccccccc 1 & + & 7p & - & 28p^2 & + & 66p^3 & - & 100p^4 & + & 96p^5 & - & 56p^6 & + & 16p^7 + & - & 3p & + & 16p^2 & - & 46p^3 & + & 80p^4 & - & 88p^5 & + & 56p^6 & - & 16p^7 + & - & 3p & + & 10p^2 & - & 20p^3 & + & 20p^4 & - & 8p^5 + & & & - & 6p^2 & + & 26p^3 & - & 60p^4 & + & 80p^5 & - & 56p^6 & - & 16p^7 + & & & - & 2p^2 & - & 6p^3 & + & 40p^4 & - & 72p^5 & + & 56p^6 & - & 16p^7    )    in order to compute the exact bit error probability according to , it is necessary to determine @xmath82 . in the next section",
    "we will derive a recurrent matrix equation for the average information weights and illustrate how to obtain its components using as an example the rate @xmath7 memory @xmath1 minimal encoder determined by .",
    "the vector @xmath83 describes the dynamics of the information weights when we proceed along the trellis and satisfies the recurrent equation    c [ eq : recurrent_equation ] \\ {    rcl @xmath84_t+1 & = & @xmath84_t a + @xmath85_t b + @xmath85_t+1 & = & @xmath85_t    .    where @xmath86 and @xmath87 are @xmath88 nonnegative matrices",
    ", @xmath89 is an @xmath88 stochastic matrix , and @xmath90 .",
    "both matrices consist of @xmath91 submatrices @xmath92 and @xmath93 of size @xmath94 , respectively , where the former satisfy @xmath95 since we consider only encoders for which every encoder state is reachable with probability @xmath96 .",
    "the matrix a represents the linear part of the affine transformation of the information weights while the matrix @xmath87 describes their increments .",
    "the submatrices @xmath92 and @xmath93 describe the updating of the average information weights if the transition from state @xmath97 to state @xmath98 exists ; and are zero otherwise .",
    "moreover , the vector @xmath99 of length @xmath41 is the concatenation of @xmath38 stochastic vectors @xmath77 , and hence the @xmath88 matrix  @xmath89 follows as    rcl = (    cccc & 0 &  & 0 + 0 & &  & 0 + & & & + 0 & 0 &  &    )    for simplicity , we choose the initial value of the vector of the information weights to be    rcl [ eq : initial_value_w ] @xmath84_0 & = & @xmath44    continuing the previous example , we will illustrate how the @xmath100 matrices @xmath86 and @xmath87 can be obtained directly from all @xmath70 trellis sections . for example , the eight trellis sections in fig .",
    "[ fig : eight_trellis_sections ] determine all transitions from @xmath53 to either @xmath101 or @xmath102 .    to be more specific , consider all transitions from @xmath103 and @xmath53 to @xmath104 and @xmath101 , as shown in fig .",
    "[ fig : eight_trellis_sections](a ) , ( b ) , ( g ) , and ( h ) . only fig .",
    "[ fig : eight_trellis_sections](a ) and ( g ) have transitions decided by the viterbi algorithm , which are @xmath105 in fig .",
    "[ fig : eight_trellis_sections](a ) and @xmath106 in fig .",
    "[ fig : eight_trellis_sections](g ) , and thus the entry @xmath103 , @xmath53 , @xmath104 , @xmath101 in matrix @xmath86 follows as @xmath107 and in matrix @xmath87 as    rcl + & = & 0 + 2p^2q = 2p^2q    where @xmath108 denotes the number of information @xmath96s corresponding to @xmath109 . since we use coin - flipping to resolve ties , we obtain that the entry @xmath103 , @xmath53 , @xmath104 , @xmath102 ( fig .",
    "[ fig : eight_trellis_sections](c ) and ( d ) ) in matrix @xmath86 is    rcl +   + & = & pq^2 + pq^2 + pq^2 + pq^2 = 2pq^2    and in matrix @xmath87    rcl +   + & = & 0 + 2 pq^2 + 0 + 2 pq^2 = 2pq^2    similarly the entry @xmath110 , @xmath53 , @xmath104 , @xmath101 ( fig .",
    "[ fig : eight_trellis_sections](b ) and ( h ) ) in matrix @xmath86 is @xmath111 and in matrix @xmath87 @xmath112 finally , the entry @xmath110 , @xmath113 , @xmath104 , @xmath102 ( fig .",
    "[ fig : eight_trellis_sections](e ) and ( f ) ) in matrix @xmath86 is given by @xmath114 and in matrix @xmath87 by @xmath115    the trellis sections in fig .",
    "[ fig : eight_trellis_sections ] determine also the entries for the transitions @xmath103 , @xmath53 , @xmath116 , @xmath101 and @xmath103 , @xmath53 , @xmath116 , @xmath102 as well as the transitions @xmath117 , @xmath53 , @xmath116 , @xmath101 and @xmath110 , @xmath53 , @xmath116 , @xmath102 .",
    "the remaining transitions with @xmath53 are never decided by the viterbi algorithm , and hence the corresponding entries are zero .",
    "the eight trellis sections in fig .",
    "[ fig : eight_trellis_sections ] yield @xmath118 entries in the matrices @xmath86 and @xmath87 , while the @xmath76 trellis sections not shown in fig .",
    "[ fig : eight_trellis_sections ] yield the remaining @xmath119 entries . for the convolutional encoder shown in fig .  [",
    "fig : ocf_realization ] we obtain @xmath120 where    c    a_00 =    c    a_01 =    c    a_10 =    c    a_11 =    and @xmath121 where    c    b_00 =    c    b_01 =    c    b_10 =    c    b_11 =",
    "consider the second equation in .",
    "it follows from that we are only interested in the asymptotic values , and hence letting @xmath15 tend to infinity yields    rcl [ eq : b_infinity ] @xmath85 _ & = & @xmath85 _    where @xmath122 can be chosen as    rcl [ inf_case ] @xmath85 _ & = & ( @xmath81 _ @xmath81 _  @xmath81 _ )    to obtain the last equality , we took into account that @xmath89 is a block - diagonal matrix whose diagonal elements are given by the transition probability matrix @xmath123 which satisfies .",
    "based on these observations , can be simplified to    rcl [ eq : simplified_recurrent_equation ] @xmath84_t + 1 & = & @xmath84_t a + @xmath85 _ b    by iterating the recurrent equation and using the initial value , the vector of the information weights at time instant @xmath23 is given by    rcl @xmath84_t+1 & = & @xmath85 _ b a^t + @xmath85 _ b a^t-1 + + @xmath85 _ b [ eq : w_t_plus_1 ]    taking its limit , it follows that    rcl _ t & = & _ t = _ t _",
    "@xmath85 _ b a^t - j + & = & @xmath85 _ b a^/b [ eq : final_limit ]    where @xmath124 denotes the limit of the sequence @xmath125 when @xmath15 tends to infinity and we used the fact that , if a sequence converges to a finite limit , then it is cesro - summable to the same limit .    from it follows that    rcl [ eq : left_eigenvector ] @xmath126 _ & = & ( @xmath81 _ @xmath81 _  @xmath81 _ )    satisfies    rcl @xmath126 _ a & = & @xmath126 _    and hence @xmath127 is a left eigenvector with eigenvalue @xmath128 . due to the nonnegativity of @xmath86 , @xmath128 is a maximal eigenvalue of @xmath86 ( corollary 8.1.30 @xcite )",
    ". let @xmath129 denote the right eigenvector corresponding to the eigenvalue @xmath128 normalized such that @xmath130 . if we remove the allzero rows and corresponding columns from the matrix @xmath86 we obtain an irreducible matrix which has a unique maximal eigenvalue @xmath128 ( lemma 8.4.3  @xcite ) .",
    "hence , it follows ( lemma 8.2.7 , statement ( i )  @xcite ) that    rcl [ eq : multiplication_eigenvectors ] a^ = @xmath126 _ @xmath126 _    combining , , and yields    rcl [ eq : combination ] _ t & = & @xmath85 _ b @xmath126 _",
    "( @xmath81 _ @xmath81 _  @xmath81 _ ) / b    following , by summing up the first @xmath31 components of the vector @xmath131 on the right side of , we obtain the closed form expression for the exact bit error probability as    rcl [ eq : final ] p _ & = & @xmath85 _ b @xmath126 _ / b    to summarize , the exact bit error probability @xmath9 for viterbi decoding of a rate @xmath21 minimal convolutional encoder , when communicating over the bsc , is calculated as follows :    * construct the set of metric states and find the stationary probability distribution @xmath132 .",
    "* determine the matrices @xmath86 and @xmath87 as in section  [ sec : recurrent_equation ] and compute the right eigenvector @xmath129 normalized according to @xmath133 . * calculate the exact bit error probability @xmath134 using .",
    "for the encoder shown in fig .",
    "[ fig : ocf_realization ] we obtain    rcl p _ & = & ( 4p - 2p^2 + 67p^3 - 320p^4 + 818p^5 - 936p^6 - 884p^7 .",
    "+ & & + 5592p^8 - 11232p^9 + 13680p^10 - 11008p^11 + & & .+ 5760p^12 - 1792p^13 + 256p^14 ) / ( 2 - 5p + 41p^2 .",
    "+ & & - 128p^3 + 360p^4 - 892p^5 + 1600p^6 - 1904p^7 + & & .+ 1440p^8 - 640p^9 + 128p^10 ) + & = & 2p + 4p^2 + p^3 - p^4 - p^5 + p^6 + & & - p^7 - p^8 + p^9 + & & + p^10 -    if we instead realize the _",
    "minimal _ generator matrix in controller canonical form , we obtain a _ nonminimal _ ( @xmath5-state ) encoder with @xmath135 normalized cumulative metric state ; _ cf .",
    "_ ,  the remark after .",
    "its exact bit error probability is slightly worse than that of its minimal realization in observer canonical form .",
    "exact bit error probability for the rate @xmath0 minimal encoders of memory @xmath1 ( @xmath2-state ) @xmath3 , memory @xmath4 ( @xmath5-state ) @xmath6 , memory @xmath136 ( @xmath137-state ) @xmath138 , and memory @xmath139 ( @xmath8-state ) @xmath140 . ]",
    "first we consider some rate @xmath0 , memory @xmath141 , and @xmath5 convolutional encoders ; that is , encoders with @xmath142 , and @xmath8 states , realized in controller canonical form . in fig .",
    "[ fig : graph ] we plot the exact bit error probability for those four convolutional encoders .",
    "if we draw all @xmath118 trellis sections for the rate @xmath0 , memory @xmath1 ( @xmath2-state ) convolutional encoder with generator matrix @xmath10 realized in controller canonical form , we obtain the normalized cumulative metric states @xmath143 .",
    "its metric state markov chain yields the stationary probability distribution @xmath144 based on these @xmath118 trellis sections , the @xmath100 matrices @xmath86 and @xmath87 are constructed as @xmath145 and @xmath146 where @xmath147 denotes the @xmath148 all - zero matrix .",
    "the normalized right eigenvector of @xmath86 is    rcl [ eq : right_eigenvector_memory_1 ] @xmath126 _ & = & (    c 0 + 0 + 0 + 0 + 0 + 0 +   +   +   + 1    )    finally , inserting , , and into yields the following expression for the exact bit error probability    rcl p _ & = & + & = & 7p^2 - 8p^3 - 31p^4 + 64p^5 + 86p^6 - p^7 + & & - p^8 + p^9 - p^10 -    which coincides with the exact bit error probability formula given in @xcite .    for the rate @xmath0 , memory @xmath4 ( @xmath5-state ) convolutional encoder with generator matrix @xmath11 realized in controller canonical form , we obtain , for example , the four trellis sections for @xmath149 shown in fig .  [ fig : four_trellis_sections_m2 ] . the corresponding metric states at times @xmath23 are @xmath150 and @xmath151 .",
    "completing the set of trellis sections yields in total @xmath152 different normalized cumulative metric states , and hence the @xmath153 matrices @xmath86 and @xmath87 have the following block structure + @xmath154 and @xmath155    following the method for calculating the exact bit error probability described in section  [ sec : solving_the_recurrent_equation ] we obtain    rcl p _",
    "& = & 44p^3 + p^4 - p^5 - p^6 + & & - p^7 + p^8 + & & + p^9 - p^10 - [ eq : ebep_m2 ]    which coincides with the previously obtained result by lentmaier et al .",
    "@xcite .    for the rate @xmath0",
    ", memory @xmath136 ( @xmath137-state ) convolutional encoder with generator matrix @xmath156 realized in controller canonical form we have @xmath157 normalized cumulative metric states and the @xmath86 and @xmath87 matrices are of size @xmath158 .",
    "since the complexity of the symbolic derivations increases greatly , we can only obtain a numerical solution of , as shown in fig .",
    "[ fig : graph ] .    for the rate @xmath0",
    ", memory @xmath139 ( @xmath8-state ) convolutional encoder with generator matrix @xmath159 realized in controller canonical form , we have as many as @xmath160 normalized cumulative metric states .",
    "thus , the matrices @xmath86 and @xmath87 are of size @xmath161 . the corresponding numerical solution of is plotted in fig .",
    "[ fig : graph ] .",
    "the obvious next step is to try a rate @xmath0 , memory @xmath162 ( @xmath76-state ) convolutional encoder .",
    "we tried the generator matrix @xmath163 realized in controller canonical form but were only able to show that the number of cumulative normalized metric states @xmath31 exceeds @xmath164 .",
    "exact bit error probability for the rate @xmath0 memory @xmath4 minimal encoders with @xmath165 , @xmath166 , and @xmath167 . ]",
    "consider the generator matrix @xmath168 and its equivalent systematic generator matrices @xmath166 and @xmath167 .",
    "when realized in controller canonical form , all three realizations have @xmath152 normalized cumulative metric states . the exact bit error probability for @xmath169 is given by . for @xmath170 and @xmath171",
    "we obtain    rcl p _ & = & p^3 + p^4 - p^5 - p^6 + & & + p^7 + p^8 + & & + p^9 + p^10 - [ eq : ebep_m2_5_7 ]    and    rcl p _ & = & p^3 + p^4 - p^5 - p^6 + & & + p^7 + p^8 + & & + p^9 - p^10 - [ eq : ebep_m2_7_5 ]    respectively .",
    "the corresponding numerical results are illustrated in fig .",
    "[ fig : equivalent_m2 ] .    exact bit error probability for the rate @xmath7 , overall constraint length @xmath172 , and @xmath5 ( @xmath5-state , @xmath137-state , and @xmath8-state , respectively ) minimal encoders whose",
    "generator matrices are given in table  [ tab : encodingmatrices ] . ]",
    "the exact bit error probabilities for the rate @xmath7 @xmath5-state , @xmath137-state , and @xmath8-state generator matrices , given in table  [ tab : encodingmatrices ] and realized in controller canonical form , are plotted in fig .",
    "[ fig : ebep_r23 ] .",
    "c|c|c|c @xmath173 & # states & @xmath174 & @xmath31 + & & & + & & + & & + & & & + & & + & & + & & & + & & + & & +    as an example , the @xmath5-state encoder has the exact bit error probability    rcl p _ & = & p^2 + p^3 - p^4 - p^5 + & & + p^6 + p^7 + & & - p^8 - p^9 + & & + p^10 +    if we replace the bsc with the quantized additive white gaussian noise ( awgn ) channel , the calculation of the exact bit error probability follows the same method as described in section  [ sec : solving_the_recurrent_equation ] , but the computational complexity increases dramatically as illustrated by the following example .    exact bit error probability for the rate @xmath0 , memory @xmath4 ( @xmath5-state ) encoder with @xmath6 used to communicate over an awgn channel with different quantization levels . ]",
    "examples of uniform and massey quantizations for an awgn channel with snr = @xmath175 . ]",
    "consider the generator matrix @xmath11 used to communicate over a quantized awgn channel .",
    "we use different quantization methods , namely , uniform quantization @xcite and massey quantization @xcite ; see fig .",
    "[ fig : quant_levels ] .",
    "the uniform intervals were determined by optimizing the cut - off rate @xmath176 .",
    "the massey quantization thresholds @xmath177 between intervals were also determined by optimizing @xmath176 , but allowing for nonuniform intervals .",
    "the realization in controller canonical form yields that , for all signal to noise ratios ( snrs ) , @xmath178 , and uniform quantization with @xmath179 , @xmath137 , and @xmath180 levels , the number of the normalized cumulative metric states is @xmath181 , @xmath182 , and @xmath183 , respectively .",
    "however , for the massey quantization the number of normalized cumulative metric states varies with both the number of levels and the snr .",
    "moreover , these numbers are much higher .",
    "for example , considering the interval between @xmath184 and @xmath185 with @xmath137 quantization levels , we have @xmath186 for @xmath187 , while for @xmath188 we obtain @xmath189 .",
    "the exact bit error probability for this @xmath5-state encoder is plotted for all different quantizations in fig .  [ fig : ebep_quant ] , ordered from worst ( top ) to best ( bottom ) as    [ cols= \" > , < , < \" , ]     all differences are very small , and hence it is hard to distinguish all the curves .",
    "it is interesting to notice that using @xmath179 instead of @xmath137 uniform quantization levels yields a better bit error probability .",
    "however , this is not surprising since the presence of a quantization bin around zero typically improves the quantization performance .",
    "moreover , the number of cumulative normalized metric states for @xmath179 quantization levels is only about one half of that for @xmath137 quantization levels .",
    "notice that such a subtle comparison of channel output quantizers has only become possible due to the closed form expression for the exact bit error probability .",
    "we have derived a closed form expression for the exact bit error probability for viterbi decoding of convolutional codes using a recurrent matrix equation . in particular , the described method is feasible to evaluate the performance of encoders with as many as @xmath8 states when communicating over the bsc . by applying our new approach to a @xmath5-state encoder used to communicate over the quantized awgn channel ,",
    "the expression for the exact error probability for viterbi decoding is also derived . in particular",
    ", it is shown that the proposed technique can be used to select the optimal encoder implementation as well as the optimal channel output quantizer based on comparing their corresponding exact bit decoding error probability .",
    "m.  r. best , m.  v. burnashev , y.  levy , a.  rabinovich , p.  c. fishburn , a.  r. calderbank , and d.  j. costello , jr .",
    ", `` on a technique to calculate the exact performance of a convolutional code , '' _ ieee trans .",
    "inf . theory _ ,",
    "41 , no .  2 ,",
    "441447 , mar .",
    "1995 .",
    "m.  lentmaier , d.  v. truhachev , and k.  s. zigangirov , `` analytic expressions for the bit error probabilities of rate-1/2 memory 2 convolutional encoders , '' _ ieee trans .",
    "inf . theory _ ,",
    "50 , no .  6 , pp . 13031311 , jun .",
    "2004 .",
    "irina e. bocharova was born in leningrad , u.s.s.r .",
    ", on july 31 , 1955 .",
    "she received the diploma in electrical engineering in 1978 from the leningrad electro - technical institute and the ph.d .",
    "degree in technical sciences in 1986 from the leningrad institute of aerospace instrumentation .    during 1986 - 2007",
    ", she has been a senior researcher , an assistant professor , and then associate professor at the leningrad institute of aerospace instrumentation ( now state university of aerospace instrumentation , st .- petersburg , russia ) . since 2007",
    "she has been an associate professor at the state university of information technologies , mechanics and optics .",
    "her research interests include convolutional codes , communication systems , source coding and its applications to speech , audio and image coding .",
    "she has published more than 50 papers in journals and proceedings of international conferences , and seven u.s .",
    "patents in speech , audio and video coding .",
    "she has authored the textbook _",
    "compression for multimedia _ ( cambridge university press , 2010 ) .",
    "florian hug ( s08 ) was born in memmingen , germany , on may 21 , 1983 .",
    "he received the dipl .- ing .",
    "degree from the faculty of electrical engineering , university of ulm , ulm , germany , in 2008 .",
    "since then , he has been with the department of electrical and information technology , lund university , lund , sweden , where he is working towards the ph.d .",
    "degree in information theory .",
    "his research interests covers the field of information and coding theory .",
    "currently , he is focusing on codes over graphs .",
    "rolf johannesson ( m72 , f98 , lf12 ) was born in hssleholm , sweden , on july 3 , 1946 .",
    "he received the m.s . and ph.d .",
    "degrees in 1970 and 1975 , respectively , both from lund university , lund , sweden , and the degree of professor , _ honoris causa _",
    ", from the institute for problems of information transmission , russian academy of sciences , moscow , in 2000 .",
    "since 1976 , he has been a faculty member with lund university where he has the chair of information theory . from 1976 to 2003 , he was department head and during 1988 - 1993 dean of the school of electrical engineering and computer sciences . during 1990 - 1995 ,",
    "he served as a member of the swedish research council for engineering sciences .",
    "his scientific interests include information theory , error - correcting codes , and cryptography .",
    "in addition to papers in the area of convolutional codes and cryptography , he has authored two textbooks on switching theory and digital design ( both in swedish ) and one on information theory ( in both swedish and german ) and coauthored _ fundamentals of convolutional coding _",
    "( new york : ieee press , 1999 ) , and _ understanding information transmission _ ( hoboken , nj : ieee press / wiley - interscience , 2005 ) .",
    "professor johannesson has been an associate editor for the _ international journal of electronics and communications_. during 1983 - 1995 he co - chaired seven russian - swedish workshops , which were the chief interactions between russian and western scientists in information theory and coding during the final years of the cold war .",
    "he became an elected member of the royal swedish academy of engineering sciences in 2006 .",
    "boris d. kudryashov was born in leningrad , u.s.s.r . , on july 9 , 1952 .",
    "he received the diploma in electrical engineering in 1974 and the ph.d in technical sciences degree in 1978 both from the leningrad institute of aerospace instrumentation , and the doctor of science degree in 2005 from institute of problems of information transmission , moscow .    in 1978 , he became an assistant professor and then associate professor and professor in the leningrad institute of aerospace instrumentation ( now the state university on aerospace instrumentation , st .- petersburg , russia ) . since november 2007 , he has been a professor with the state university on information technologies , mechanics and optics , st .- petersburg , russia .",
    "his research interests include coding theory , information theory and applications to speech , audio and image coding .",
    "he has authored a textbook on information theory ( in russian ) and has more than 70 papers published in journals and proceedings of international conferences , 15 u.s . patents and published patent applications in image , speech and audio coding ."
  ],
  "abstract_text": [
    "<S> in 1995 , best et al . published a formula for the exact bit error probability for viterbi decoding of the rate @xmath0 , memory @xmath1 ( @xmath2-state ) convolutional encoder with generator matrix @xmath3 when used to communicate over the binary symmetric channel . </S>",
    "<S> their formula was later extended to the rate @xmath0 , memory @xmath4 ( @xmath5-state ) convolutional encoder with generator matrix @xmath6 by lentmaier et al .    in this paper , a different approach to derive the exact bit error probability is described . </S>",
    "<S> a general recurrent matrix equation , connecting the average information weight at the current and previous states of a trellis section of the viterbi decoder , is derived and solved . </S>",
    "<S> the general solution of this matrix equation yields a closed form expression for the exact bit error probability . as special cases , the expressions obtained by best et al . for the @xmath2-state encoder and by lentmaier et al . for a @xmath5-state encoder </S>",
    "<S> are obtained . the closed form expression derived in this paper </S>",
    "<S> is evaluated for various realizations of encoders , including rate @xmath0 and @xmath7 encoders , of as many as @xmath8 states .    </S>",
    "<S> moreover , it is shown that it is straightforward to extend the approach to communication over the quantized additive white gaussian noise channel .    </S>",
    "<S> additive white gaussian noise channel , binary symmetric channel , bit error probability , convolutional code , convolutional encoder , exact bit error probability , viterbi decoding </S>"
  ]
}