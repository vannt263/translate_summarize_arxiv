{
  "article_text": [
    "these proceedings discuss high energy physics ( hep ) usage of support vector machines ( svms )  @xcite , and in particular recent improvements to the functionality of the tmva  @xcite implementation .",
    "these improvements are deployed in the root git repository and from version 6.08 .",
    "having discussed this machine learning ( ml ) algorithm we proceed to raise the issue of generalisation of hyper - parameters ( hps ) ; and in particular the use of hold - out and cross - validation ( cv ) .",
    "we also discuss the issues of understanding variance on the classification and performing a hypothesis test to address the issue of identifying if the hps of a multivariate algorithm ( mva ) are generalised , having used some method to promote hp generalisation .",
    "svms were developed in the 1960 s in a hard margin form .",
    "these were based on rosenblatt s perceptron that allows one to trivially separate distinct types of data using a linear decision boundary , given by @xmath1  @xcite . here",
    "@xmath2 is a set of weights defining the orientation of the separating hyper - plane , @xmath3 is the bias ( offset from the origin ) and @xmath4 is an event .",
    "subsequent developments expanded the power of svms in two significant ways ; firstly to allow for misclassification of events and thereby extending the algorithm to a broader set of problems , including those encountered in a typical hep analysis .",
    "this extension requires introduction of an overall cost parameter @xmath5 for event mis - classification as well as slack parameters @xmath6 for the mis - classification of any given event ( or support vector , sv ) .",
    "here @xmath5 is a hp that alters the selection of the hyper - plane separating classes of event .",
    "this type of svm is referred to as a soft margin svm ( see fig  [ fig : softmargin ] ) .",
    "the second extension was to replace the use of scalar products between two svs , @xmath7 , with inner products in terms of kernel functions ( kfs ) of svs , @xmath8 .",
    "these functions introduce additional hps ( kf parameters is also referred to as a kf parameter . ] ) that need to be determined in order for one to separate classes of event .",
    "the use of kfs allows us to attempt to linearly separate types of event in a dual hyperspace of features @xmath9 mapped implicitly from the input feature space @xmath10 .",
    "it is not necessary to understand the mapping @xmath11 . for this mapping to exist",
    "a kf needs to be symmetric and satisfy mercer s condition  @xcite . for such kernels",
    "the algorithm can be used without detailed knowledge of the transformation ; this is referred to as the _ kernel trick _ in the literature .",
    "while the kernel trick allows us to use complicated kfs it does obscure understanding of how the separating hyper - plane is determined .",
    "this plane is determined for a 1-norm svm by optimising the dual space lagrangian @xmath12 to determine the lagrange multipliers , @xmath13 $ ] , in order to maximise the margin .",
    "the lagrange multipliers are determined using sequential minimal optimisation ( smo ) and the constraint equation @xmath14 . here",
    "the @xmath15 are example labels given by @xmath16 .",
    "they denote which side of the decision boundary an event lies on .",
    "the functional margin of an example @xmath17 is given by @xmath18 where positive values are obtained for correctly classified examples .",
    "the @xmath19 and svs determine hyper - plane parameters as @xmath2 and @xmath3 are given by @xmath20    svms are widely used outside of physics ; for example in handwriting recognition , speech recognition , bioinformatics , risk analysis , finance , electronics , etc .",
    "many of these problems involve distinguishing between features that are largely not overlapping ; for example the classification of handwriting as illustrated by the mnist classification example in tensorflow  @xcite .",
    "practitioners typically apply different kfs to a given problem in order to empirically validate that a given kf provides good performance for that problem . in doing",
    "so one implicitly relies on the kernel trick to work without trying to understand details of the problem in the dual feature space .",
    "the most commonly used kfs in hep examples are scalar products and the radial basis function ( rbf ) where performance has been reported to be comparable with that for other ml algorithms ; neural networks ( nns ) and boosted decision trees ( bdts ) .",
    "in contrast to other fields there has not been a broad uptake of svms within the hep community .",
    "soft margin svm showing the ( solid ) maximal margin hyper - plane and ( dashed ) functional margins for two categories of events ( dots and squares ) in two dimensions . ]    several reports of svms use in hep have been made : including jet flavour tagging and muon identification  @xcite , top physics  @xcite , background suppression for @xmath21 signatures  @xcite , and susy searches  @xcite at lep , tevatron and lhc experiments .",
    "@xcite discusses a custom implementation of a svm , and where specified the other references rely on the libsvm implementation  @xcite .",
    "a review of svm use in hep as of 2008 can be found in ref .",
    "we have applied svms to higgs physics  @xcite , using the kaggle higgs boson ml challenge data sample  @xcite .",
    "the examples discussed below use the tmva svm algorithm implementation that uses smo for lagrange multiplier optimisation and minuit  @xcite for hyper - parameter optimisation .",
    "kfs used in previous hep examples are scalar products and the rbf function .",
    "the latter is @xmath22 where @xmath23 is a hp to be optimised .",
    "having just a single hp to determine ( in addition to @xmath5 ) has the advantage of being a relatively cheap computing task .",
    "the disadvantage of the rbf function is that it assumes that the norm of the sv is sufficient to distinguish between events of different categories .",
    "thus information may be lost in the computation of the kf for some problems .",
    "computer science literature discusses a number of other kfs , some of which we have added to tmva .",
    "these include multi - gaussian ( mg ) and polynomial ( poly ) functions @xmath24 respectively .",
    "the mg kf has @xmath25 hps , @xmath26 , to overcome potential loss of information that the rbf may result in .",
    "the hps of the poly kf are @xmath27 and @xmath28 .",
    "the sum and product of kfs are also valid kfs and these four functions are now available in tmva .",
    "logarithmic grid searches have been used ( via libsvm and svm - hint ) in order to determine hps for rbf kfs .",
    "this is computationally inexpensive way to determine hps for a kf with a low dimensionality , especially if it is of the form of an rbf or mg kernel .",
    "the error in selecting optimal hps is governed by the granularity of the lattice used in the grid search .",
    "as soon as one moves to a high dimensionality of kf hps this optimisation paradigm becomes expensive as the grid search scales with @xmath25 . to address this issue for tmva the davidon - fletcher - powell based minuit optimisation algorithm  @xcite has been adopted for hp optimisation ( hpo )  .",
    "hpo using this approach is more computationally expensive than the grid search approach for the 2d rbf optimisation problem however in general this provides a more accurate solution and is not specifically attuned to rbf and mg kernel functions , unlike the logarithmic grid search .",
    "anecdotal remarks related to the use of svms note that these tend to result in a more generalised solution than other methods for a given sample size of training data .",
    "this is plausible and can be seen trivially for a hard margin svm following the observation that the maximal margin hyper - plane is defined only by the svs ; hence only those , generally few , events on the decision boundary edges define the hyper - plane .",
    "it is not trivial to translate this observation to a soft margin svm , however in general a parallel situation will arise , where now it is the set of svs with non - zero @xmath6 that determine the definition of the decision boundary .",
    "the svs with large values of @xmath6 dominate the determination this boundary . in practice",
    "we observe better generalisation performance for svms with small training example sets compared with decision trees and neural networks in the comparisons that we have performed .",
    "while it is possible to optimise the hps for a svm , the issue of parameter generalisation remains one to be addressed as this is not guaranteed .",
    "generalisation bounds exist for svms and methods used to promote generalised solutions are discussed further in section  [ sec : generalisation ] .",
    "as mentioned above the hard margin svm requires determination of the handful of svs that fall on the functional margin to determine the hps and hence decision boundary . extending this to a soft margin svm problem like those encountered with hep background fighting problem the number of svs routinely increases to encompass all training data .",
    "this means that the @xmath29 gramm matrix to be computed scales from a low dimensionality to a high dimensionality and becomes computationally expensive relative to training many other algorithms .",
    "this can be seen trivially by the computing time required to fit a svm , vs that to train a nn or bdt using tmva for a given data sample .",
    "many of the svs for a hep soft margin problem have slack parameters that are small and therefore they do not significantly contribute to the determination of the maximal margin hyper - plane .",
    "the low @xmath6 svs are largely superfluous to the problem and a similar performance is attainable by presenting the svm with a redacted data set that focuses on the svs with @xmath6 . in itself",
    "this observation is of little use as one needs to perform the smo optimisation of the lagrangian in order to determine the @xmath6 s and in doing so therefore fix the gramm matrix dimensionality accordingly .",
    "data redaction or sparsification can be achieved by noting that the svs with low value @xmath6 s are far from the decision boundary .",
    "hence if a - priori it is possible to identify regions far from the decision boundary then one can decrease compute cost by removing those regions from training at the expense of introducing an approximation to the definition of the maximal margin hyper - plane .",
    "obtaining a set of hyper - parameters for an mva that can provide predictive performance when applied to any unseen data sample consistent with the hyper - parameter optimisation is desirable . in hep",
    "it is typical that generalisation is promoted using the hold - out technique ( see below ) .",
    "hep problems include both regression and classification variants . for classification",
    "we need to compare classification rates ( or confusion matrices ) and for regression problems we need to compare histograms to test for over - training . in practice a @xmath30 by eye or kolomogrov - smirnov ( ks ) test is performed on training and test histograms for the mva output . in the case of tmva",
    "the latter uses a binned implementation in the th1 class that is known to be incorrect  @xcite .",
    "furthermore the correctly calculated ( un - binned ) ks test statistic compares the similarity of the bulk of a distribution which is not the right comparison to make in general .",
    "this can be seen clearly for examples of searches for new particles in tails of distributions that corresponds to a large proportion of results in hep at this time .",
    "the issue of similarity metrics is discussed elsewhere  @xcite and it is evident from that review that no single metric provides a general solution to this issue .",
    "we now consider three issues : firstly enhancing generalised behaviour at the hpo stage of algorithm tuning ; secondly to determining if the set of hps corresponds to generalised performance ; thirdly to determine which mva provides the best performance for a problem .",
    "regarding the first point we focus on the hold - out ( section  [ sec : generalisation : holdout ] ) and k - fold cv ( section  [ sec : generalisation : cv ] ) methods that are generally applicable to mvas .",
    "note that nns use regularisation  @xcite and deep networks utilise dropout  @xcite and maxout  @xcite techniques .",
    "the second issue revolves around estimation of bias and variance for the ml algorithm and performing a hypothesis test to reject the null hypothesis that the hps are generalised ( section  [ sec : generalisation : other ] ) in order to choose an mva model .",
    "the third issue can be dealt with in a number of ways and in ref .",
    "@xcite we use a likelihood ratio test of a proxy of the observable being measured in order to illustrate this issue .",
    "the `` standard '' method currently in use within hep for promoting generalised hp determination for supervised learning algorithms is the hold - out cv method  @xcite .",
    "this involves dividing control ( monte carlo simulated or real ) data samples into two ; taking one sample used for hpo ( the training sample ) and reserving the second sample to evaluate the performance of the hps ( the test sample ) . some similarity test of the classification performance or binned distributions of the algorithm score",
    "can then be used to gauge if the hps are generalised ; however it is evident that this crucial step is sometimes overlooked . when this last step is not overlooked",
    "then often an incorrect binned ks test is performed as noted above  .",
    "having performed the hold - out validation of hps and assuming that the mva passes a subsequent over - training check ; the mva is now ready to apply to unseen data ( i.e. the physics analysis ) . note that to apply an mva to data used to determine",
    "hps will result in a biased outcome and should be avoided .",
    "the hold - out method suffers from the fact that half of the available training examples are used in order to determine the optimal hps .",
    "this in turn can lead to a large variance in performance of the ml algorithm .",
    "one can improve upon hold - out by increasing the sample size used for hp training at the expense of the sample for validation .",
    "this introduces the issue of how to split the data into training and test samples as the optimal hps obtained for a given training will depend on the data sampled from the available training examples as those examples reserved for validation are not used in hp determination .",
    "the @xmath31-fold cv algorithm divides the data sample into @xmath31 distinct samples ; and trains @xmath31 sets of hps using a training sample that reserves the @xmath32 sub - sample ( or fold ) for testing , the remaining @xmath33 folds being used for training  @xcite .",
    "this algorithm trains all permutations of leaving one fold out  .",
    "the fraction of training examples used for each permutation is @xmath34 .",
    "this results in a reduced variance for @xmath35 compared with the hold - out approach as the @xmath36 limit corresponds to that method .",
    "the the extreme limit where @xmath37 ( 1 ) events are ( is ) reserved for testing is the leave @xmath37-out ( one - out ) cv method , for example see  @xcite and references therein . to avoid biases the training examples are independent of the test sample used to check hp generalisation prior to applying an mva to unseen data .",
    "having trained @xmath31 sets of hps the temptation is to select the best performing set according to some figure of merit ( fom ) , e.g. the receiver operating characteristic , roc , integral at some pre - determined working point .",
    "this is a biased choice for the same reason that selecting the worst case scenario is also biased .",
    "it may lead to significant variation in mva performance from one unseen data sample to the next .",
    "the aggregate performance of the ensemble of trainings corresponds to a more robust selection of hps than the extremes , and the spread in performance obtained for the different trainings contains information about algorithm variance .",
    "using the aggregate ensures that the error rate corresponds to the average error rate of the @xmath31 folds ; i.e. @xmath38",
    ". a suitable number of folds @xmath31 used for a particular problem needs to be determined on a case by case basis .",
    "we have introduced @xmath31-fold cv to tmva .",
    "there are two relevant foms to optimise an algorithm on in particle physics , one is the precision of a measurement and the other is significance of a limit obtained from a search .",
    "however often these are not used as the metric when selecting mvas as a considerable effort may be required to optimise an algorithm using one of these foms . here",
    "we illustrate the situation encountered when optimising an mva using significance as the appropriate fom .",
    "we apply a stratified jackknife approach with 50 strata and construct samples of 2000 training and 2000 validation examples obtained using the madgraph monte carlo simulation for a new particle search .",
    "training and test samples are statistically independent and each training sample is used to train a boosted decision tree ( bdt ) to distinguish between one signal and one background class of events . to force overfitting",
    "we train bdts with a depth between 2 and 10 ( labeled as samples 0 through 8 , respectively ) .",
    "figure  [ fig : jackknife ] shows black ( red ) points representing the average significance obtained for the training ( test ) samples .",
    "the band illustrated for the black points is an estimate of the standard deviation applied to the test sample .",
    "the difference between red and black points indicates the level of hp overtraining as tree depth increases .    illustrating overtraining with increasing tree depth for the example in the text . ]",
    "if the end result of a measurement is not used to construct the fom to check for generalisation then we can consider performing a hypothesis test on the mva output ; either confusion matrices ( for classification ) or sets of histograms ( for regression ) .",
    "the null and alternate hypotheses , @xmath39 and @xmath40 , respectively are @xmath39 : the mva is generalised and @xmath40 : the mva is not generalised for all classes of event . for a classification problem with @xmath41 species of event the fraction of ( in)correctly classified species of events is indicated in the @xmath42 confusion matrix that describe the binomial classification of event type . the parent distribution is estimated using the average of the train and test confusion matrices .",
    "it straight forward to construct a hypothesis test that can be used to assert if @xmath39 is generalised at a given confidence level @xmath43 . for more than 2 species of event",
    "this approach can be extended to the corresponding multinomial problem . for a regression analysis , instead of a confusion matrix",
    "we have a pair of histograms ( one for test and one for train samples ) for each species of event .",
    "we can construct an estimator of the underlying distributions to perform a hypothesis test in analogy with a classification problem , where now we use a multinomial estimator for the fraction of events expected in each bin .      to illustrate the ml issues discussed above we now turn to a concrete hep example , the search for @xmath44 at the lhc .",
    "this decay has one @xmath45 decaying leptonically and the other decaying hadronically , and the final state is not fully reconstructed due to the undetected neutrinos .",
    "this is not a fully optimised study , but rather an illustration of potential using a limited set of input features from the kaggle data set .",
    "we use the following input features : the estimated higgs mass using the missing mass calculator mmc , transverse mass of the missing energy and lepton @xmath46 , the @xmath47 separation between the hadronically and leptonically reconstructed @xmath45 s @xmath48 , ratio of transverse momenta for the leptonic and hadronic @xmath45 s @xmath49 , @xmath50 centrality , and missing transverse energy @xmath51 .",
    "distributions of input features for the signal and background are shown in figure  [ fig : example : features ] .",
    "this highlights that the data are overlapping and the signal features are always found in regions of space that are also occupied by background in @xmath10 .",
    "these high level derived features are input to svms with the rbf , mg , and poly kfs and to a bdt for performance comparison .",
    "all trainings use the hold - out approach and the corresponding roc curves are shown in figure  [ fig : example : roccomparison ] .",
    "one can see that the svms are able to give similar performance to the bdts in this example ; a result consistent with the conclusions of previous studies with svm in hep .",
    "we also note that there is some variation in performance depending on choice of kf for this problem with the mg kf performing the worst .",
    "distributions of input features for signal and background for the @xmath44 . ]",
    "roc curves for svms and a bdt as discussed in the text using ( left ) hold - out and ( right ) with 5-fold cv .",
    "the best and average svms determined in the cv process are shown to illustrate the variation between these two.,title=\"fig : \" ] roc curves for svms and a bdt as discussed in the text using ( left ) hold - out and ( right ) with 5-fold cv .",
    "the best and average svms determined in the cv process are shown to illustrate the variation between these two.,title=\"fig : \" ]    we proceed to explore application of @xmath31-fold cv to this problem . the number of folds to use needs to be determined anecdotal for a given problem . here",
    "we illustrate the effect of cv on roc integral in comparison with the hold - out technique for a 5-fold cv ( see fig .",
    "[ fig : example : roccomparison ] ) .",
    "the best and average roc curves are found to be similar ; and while it is tempting to take the training corresponding to the best roc integral to derive the nominal hp set , that is not guaranteed to provide a typical performance for the algorithm when presented with unseen data .",
    "the average solution corresponds to a more robust hp choice for application of the algorithm to unseen data . as mentioned above the cv method",
    "is not specific to svms and can be applied to other mvas such as nns and bdts .",
    "tmva has been extended to include a cv tool based on this work .",
    "having promoted generalisation using cv one still has to address the question of hp fine tuning as these methods do not guarantee that the optimised hp set determined correspond to a generalised result .",
    "likewise when using more than one mva , the roc curve is not sufficient to determine which mva leads to the best result . for that one has to apply the mva to an analysis and follow through to the end result , or some proxy thereof ; for example some fom relating to significance such as the approximate mean significance used in ref .",
    "we have discussed the use of svms in the context of hep . here",
    "we focus on the example of searching for the @xmath44 mode at the lhc in the context of functionality available in tmva .",
    "we also discuss the issue of generalisation , highlighting some pitfalls in current practice as well as issues to be considered when determining hps for a given ml algorithm .",
    "while cv is an improvement over the hold - out method , this does not guarantee generalisation .",
    "thus one should establish if an mva is generalised having previously attempted to improve the robustness of hp determination via cv or some other method .",
    "the issues of how to illustrate overtraining by using jackknife event selection and by hypothesis testing have been discussed .",
    "9 c. cortes and v. vapnik , machine learning * 20 * issue 3 ( 1995 ) .",
    "a. hoecker et al . , pos acat * 040 * ( 2007 ) 040 .",
    "f. rosenblatt , psychol .",
    "rev . * 65 * , 386 - 408 ( 1958 ) .",
    "j. mercer , philo .",
    "london , a * 209*:415 - 446 ( 1909 ) .",
    "y. lecun et al . , proceedings of the ieee , 86(11):2278 - 2324 , november 1998 . also see http://yann.lecun.com/exdb/mnist/ and https://www.tensorflow.org .",
    "p.  vannerem , k.  r.  muller , b.  scholkopf , a.  smola and s.  soldner - rembold , hep - ex/9905027 .",
    "a.  vaiciulis , nucl .",
    "instrum .",
    "a * 502 * ( 2003 ) 492 .",
    "b.  e.  whitehouse , fermilab - thesis-2010 - 61 .",
    "f.  sforza and v.  lippi , nucl .",
    "instrum .",
    "meth .  a * 722 * ( 2013 ) 11 .",
    "m.  .  sahin , d.  krcker and i.  a.  melzer - pellmann , nucl .",
    "instrum .",
    "a * 838 * ( 2016 ) 137 .",
    "chang and c .- j .",
    "lin , libsvm : a library for support vector machines , acm transactions on intelligent systems and technology , * 2 * 2011 27:127:27 .",
    "a.  vossen , arxiv:0803.2345 [ physics.data-an ] . c. adam - bourdarios , g. cowan , c. germain - renaud , i. guyon , b. kgl and d. rousseau , j. phys .",
    "conf . ser . *",
    "664 * ( 2015 ) no.7 , 072015 .",
    "see also url : http://higgsml.lal.in2p3.fr/documentation/ ( 2014 ) .",
    "a.  bethani , a.  j.  bevan , j.  hays and t.  j.  stevenson , arxiv:1610.09932 [ physics.data-an ] .",
    "f.  james and m.  winkler ,  minuit user s guide ; f.  james , cern - d-506 , cern - d506 .",
    "see the th1::kolmogorovtest function implementation https://root.cern.ch/doc/master/classth1.html s .- h .",
    "cha , int .",
    "j. math . models and methods in applied sciences , * 1 * 300 - 307 ( 2007 ) ; f.  c.  porter , arxiv:0804.0380 [ physics.data-an ] .",
    "for example see c.  m.  bishop , neural networks for pattern recognition , clarendon press ( 1996 ) . g. e. hinton et al . corr , abs/1207.0580 ( 2012 ) . i. j. goodfellow , d. warde - farley , m. mirza , a. courvill , y. bengio , jmlr wcp 28 ( 3 ) : 1319 - 1327 ( 2013 ) .",
    "j. hays , a. bevan , t. j. stevenson , proceedings of ichep 2016 ( to appear in pos ) .",
    "l.  devroye and t.  j.  wagner , ieee transaction in information theory , * 25 * ( 5):601604 ( 1979 ) . m.  stone , j. roy .",
    "b , * 36 * 111147 ( 1974 )",
    ". s.  geisser , j. amer .",
    ", * 70*:320328 ( 1975 ) .",
    "s. arlot , stat .",
    "surveys * 4 * ( 2010 ) 4079 ."
  ],
  "abstract_text": [
    "<S> we review the concept of support vector machines ( svms ) and discuss examples of their use in a number of scenarios . </S>",
    "<S> several svm implementations have been used in hep and we exemplify this algorithm using the toolkit for multivariate analysis ( tmva ) implementation . </S>",
    "<S> we discuss examples relevant to hep including background suppression for @xmath0 at the lhc with several different kernel functions . </S>",
    "<S> performance benchmarking leads to the issue of generalisation of hyper - parameter selection . </S>",
    "<S> the avoidance of fine tuning ( over training or over fitting ) in mva hyper - parameter optimisation , i.e. the ability to ensure generalised performance of an mva that is independent of the training , validation and test samples , is of utmost importance . </S>",
    "<S> we discuss this issue and compare and contrast performance of hold - out and k - fold cross - validation . </S>",
    "<S> we have extended the svm functionality and introduced tools to facilitate cross validation in tmva and present results based on these improvements . </S>"
  ]
}