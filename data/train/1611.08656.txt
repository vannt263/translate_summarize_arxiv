{
  "article_text": [
    "recurrent neural networks ( rnns )  @xcite have been shown to perform well in many sequence modeling tasks@xcite . in rnns ,",
    "the gated memory cells like long short - term memory ( lstm )  @xcite and gated recurrent unit ( gru )  @xcite are widely used .",
    "the attention mechanism has been applied on rnn models .",
    "neural turing machine ( ntm )  @xcite is one of the examples .",
    "the idea of attention mechanism is to let the model automatically find the related part of information from memory ( usually represented as a vector sequence ) , and use the information to obtain the results .",
    "attention mechanism shows promising results on many tasks including machine translation@xcite , caption generation@xcite and question answering  @xcite .",
    "language modeling has been recognized as an important task in human language processing .",
    "the statistical models such as n - gram language model  @xcite were widely used to solve this task .",
    "recently , rnns are introduced in language modeling  @xcite and have shown great improvement compared to the traditional counterpart . however , since the rnns have fixed size of memory , their memory can not store all the information about the words have seen in the sentence , and thus the useful long - term information may be ignored when predicting the next words . by making the rnn models have the ability to review the information obtained in every previous time step , the attention mechanism improves rnn model .",
    "the examples of integrating attention mechanism and lstm - based rnn model for language modeling are long short - term memory - network ( lstmn )  @xcite and recurrent memory network ( rmn )  @xcite .",
    "lstmn uses an expandable hidden memory to explicitly store every past memory segments , making use of all the previous values in the memory to compute every update and generate the results .",
    "rmn uses the hidden memory of lstm to generate the attention weights , and then uses the attention weights and another trainable memory to generate the outputs . both lstmn and rmn",
    "are shown to outperform original lstm on language modeling .    in this paper",
    ", we propose attention - based memory selection recurrent network ( amsrn ) , a novel rnn architecture that applies the attention mechanism on lstm . in amsrn",
    ", the attention mechanism extracts the relevant information from the lstm memory states in all the previous time steps for predicting the next word .",
    "the information in different dimensions of lstm memory states has different degrees of involvement in attention weight generation and relevant information extraction .",
    "the degree of the involvement for each dimension is different for each time step .",
    "the memory selection mechanism is automatically learned from data . in this paper",
    ", we mainly make the following contributions :    1 .",
    "we investigate different ways of integrating lstm and attention mechanism .",
    "the experimental results show that the attention mechanism helps the lstm language model on three different corpora including english and chinese .",
    "2 .   different from lstmn which makes some modification on the way lstm updates the memory , in amsrn the attention mechanism",
    "is stacked on original lstm , so the architecture of the original lstm has remained .",
    "therefore , the lstm part in amsrn can be initialized by a typical lstm language model .",
    "3 .   in rmn",
    ", there are two sets of memory , one for computing the attention weights and the other for extracting the information . on the other hand , the proposed model learns to determine which memory dimensions should be involved more in computing the attention weights and which should be considered more when extracting the information , and the role of each dimension can be different at different time steps . from this point of view",
    ", rmn can be considered as a special case of the proposed model .",
    "the experimental results show that the proposed model has more stable performance across different corpora than rmn .",
    "we investigate to use entropy as regularizer for attention weights .",
    "finally , we make a visualization analysis of how the attention mechanism helps language modeling .",
    "in the figure represents elementwise multiplication of two vectors .",
    "the notation @xmath0 represents inner product . ]",
    "the overall structure of the proposed attention - based memory selection recurrent network ( amsrn ) is shown in fig .",
    "[ fig : overview ] .",
    "amsrn consists of two major parts : the typical lstm described in section  [ subsec : lstm ] and the attention mechanism module stacking on lstm in section  [ subsec : att ] .",
    "the lstm reads through the input word sequence , and stores the hidden layer outputs generated at each time step .",
    "the attention mechanism module takes the stored information as input , and generates a vector relevant to the prediction of the next words .",
    "then the relevant vector and the current lstm hidden state are used to generate the distributions for the next words . in the attention mechanism module ,",
    "the memory selection is applied on the lstm memory to determine which dimensions of lstm hidden states should be involved in computing the attention weights and extracting relevant information , which will be described in section  [ subsec : selection ] .",
    "finally , the attention weights can be regularized by their entropy in section  [ subsec : reg ] .",
    "the input of the lstm is a sequence of words represented by 1-of - n encoding , @xmath1 , at the bottom of fig .",
    "[ fig : overview ] . at each time",
    "step @xmath2 , the hidden layer output of the lstm is a @xmath3-dimensional vector @xmath4 , where @xmath3 is the number of memory cells in lstm , and @xmath4 would be stored for further use .",
    "therefore , at the time step @xmath2 ( when the model has read the first @xmath2 words in the sentence in question ) , the information stored is @xmath5 , @xmath6 , \\label{eq : memory}\\ ] ] where @xmath7 is the initial state of the lstm .",
    "@xmath5 in ( [ eq : memory ] ) is a @xmath8 matrix , which grows as @xmath2 increases",
    ". the attention module will extract the information from @xmath5 .      in the attention mechanism ,",
    "the memory selection module generates two @xmath3-dimensional vectors , @xmath9 and @xmath10 , from the current lstm state @xmath4 .",
    "@xmath9 and @xmath10 are used to select the stored information . here all the elements in @xmath9 and @xmath10 are between @xmath11 and @xmath12 . how to generate @xmath9 and @xmath10",
    "will be described in the next subsection .",
    "then the current hidden state @xmath13 and the two memory selection vectors , @xmath9 and @xmath10 , are used to extract the relevant information , represented as a @xmath3-dimensional vector @xmath14 , from @xmath15 in ( [ eq : memory ] ) .",
    "the model first generates a @xmath3-dimensional vector @xmath16 from the current hidden state @xmath4 as the ` key ' for attention weight generation , @xmath17 where the @xmath18 matrix @xmath19 and @xmath3-dimensional vector @xmath20 are network parameters to be learned .",
    "then the inner product similarity @xmath21 between the key @xmath22 and each @xmath23 in @xmath24 $ ] is computed .",
    "@xmath25 where @xmath26 denotes the elementwise multiplication , and @xmath0 denotes the inner product . by multiplying each element in @xmath27 by the corresponding element in @xmath9 ( that is , @xmath28 in ( [ eq : dot ] ) )",
    ", the model determines the degree of each dimension of @xmath23 involved in computing the similarity ( for example , the dimension multiplied by @xmath11 would be totally ignored in generating the attention weights ) .",
    "the similarity @xmath21 is further normalized by softmax normalization to obtain the attention weights @xmath29 , @xmath30 to generate the relevant vector @xmath31 , each @xmath27 is selected by @xmath10 to obtain @xmath32 , @xmath33 in which the degree each dimension of @xmath23 is involved in extracting the relevant vector @xmath31 is determined .",
    "finally , @xmath31 is generated by the weighted sum of @xmath32 according to @xmath29 , @xmath34 the attention vector @xmath31 and the hidden state @xmath13 predicts the distribution of the next word @xmath35 , @xmath36 where @xmath37 , @xmath38 and @xmath39 are network parameters to be learned , and @xmath40 is the softmax activation function .",
    "the cost @xmath41 to be minimized by optimizing the network parameters is the cross - entropy between the word distribution @xmath35 and the reference distribution for all the words in the training set .      in this paper",
    ", we investigate three different ways to obtain @xmath9 used in ( [ eq : dot ] ) and @xmath10 in ( [ eq : extract ] ) :    1 .",
    "@xmath9 and @xmath10 are generated independently .",
    "the current state of lstm , @xmath13 , is passed into two different fully connected layers with sigmoid activation function to generate @xmath9 and @xmath10 as below , @xmath42 where the @xmath43 , @xmath44 , @xmath45 and @xmath46 denote the weights and the biases of the fully connected layer .",
    "the two vectors @xmath9 and @xmath10 are forced to be the same .",
    "@xmath9 is generated by the same way as the first approach , and the model simply sets @xmath47 .",
    "the only difference between the third and the second approaches is that here we set @xmath48 , where @xmath49 is a @xmath3-dimensional vector with all ones , and ` @xmath50 ' here represents elementwise subtraction .",
    "the inspiration of the third approach is that in the attention models the memory used to generate attention distribution and the memory used to generate the final attention vector can be different  @xcite .",
    "therefore , by constraining the sum of the two weights , @xmath9 and @xmath10 , it simulates the situation that there are two different sets of memory for attention weights and information extraction respectively .      when training model , a regularization term is usually used to prevent overfitting .",
    "for example , the two - norm of the model parameters are widely used as a regularizer . here",
    "we investigate to use the entropy of the attention weights as the regularization term  @xcite .",
    "the purpose of using entropy as regularizer is because only part of the information in the previous steps is relevant to the prediction of the next word .",
    "therefore , the attention weights that extract useful information from the previous time steps are sparse . the entropy regularizer to keep the attention weights sparse",
    "is designed as below .",
    "@xmath51 where @xmath52 is a sentence in the training corpus , and @xmath53 is the length of @xmath52 .",
    "@xmath54 in ( [ eq : reg ] ) denotes the attention weight of @xmath23 at the time step @xmath2 when reading sentence @xmath52 , and @xmath55 is the entropy of the attention weights obtained at the time step @xmath2 . with the regularization term , the network is learned to minimize @xmath56 , where @xmath41 has been mentioned in subsection  [ subsec : att ] and @xmath57 is determined by a validation set .",
    "we tested the proposed model on two english data sets and one chinese data set .",
    "the first data set we used is the penn treebank corpus  @xcite , which is a widely used data set to evaluate the effectiveness of a language model .",
    "it contains about 40k training sentences , 3k validation sentences and 4k testing sentences .",
    "the other english data set we used is from the switchboard corpus@xcite .",
    "switchboard is a telephone speech corpus which collect two - sided telephone conversations among speakers in the united states .",
    "we used about 945k sentences for training , 10k for validation and about 5.2k for testing . for chinese , we used chinese gigaword data set@xcite to evaluate the model .",
    "chinese gigaword data set consists of around 25k chinese news articles . after parsing , there are 531k sentences for training , 165k for validation and about 260k for testing .",
    "table 1 summaries the statistics of the three data sets we used in the following experiments .",
    "the perplexities ( ppls ) on the testing data sets are used to evaluate different methods .",
    ".the statistics of the three data sets we used in the following experiments . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]      the experimental results of different models are shown in table 3 .",
    "columns ( 1 ) , ( 2 ) and ( 3 ) are the results on penn treebank corpus , switchboard corpus and chinese gigaword data set , respectively .",
    "experiments were done step by step .",
    "first , a typical lstm language model was trained , and ppls of the lstm model on the testing sets are in row ( a ) .",
    "then in row ( b ) the attention module was added on top of lstm but without memory selection ( or all the elements in @xmath9 and @xmath10 are one ) and entropy regularizer .",
    "it is found that attention mechanism was helpful on both penn treebank and switchboard ( rows ( b ) v.s .",
    "( a ) on columns ( 1 ) and ( 2 ) ) , but it does not improve the lstm on chinese gigaword ( rows ( b ) v.s .",
    "( a ) on columns ( 3 ) ) . in row",
    "( c ) , we show the results of wit memory selection based on the second approach in subsection  [ subsec : selection ] .",
    "we found that memory selection is essential for attention mechanism here . with memory selection ,",
    "attention - based model outperformed lstm on all the three corpora ( rows ( c ) v.s .",
    "then the entropy regularization for the attention weights was applied on the attention - based model with memory selection .",
    "the results are in row ( d ) .",
    "the results of entropy regularization are mixed .",
    "it improved the performance on penn treebank , but degrades the performance on the rest two corpora ( rows ( d ) v.s .",
    "the experimental results suggest that the assumption of sparse attention weights is probably not very accurate .",
    "we further compare the proposed model with another two attention - based language model , recurrent memory network ( rmn )  @xcite and recurrent - memory - recurrent ( rmr )  @xcite . comparing lstm with the two attention - based model in the literature",
    ", the conclusion is also mixed .",
    "rmn and rmr outperformed drastically the two english corpora ( rows ( e ) , ( f ) v.s .",
    "( a ) on columns ( 1 ) and ( 2 ) ) , but contrary conclusion is obtained on the chinese corpus ( rows ( e ) , ( f ) v.s .",
    "( a ) on column ( 3 ) ) .",
    "this is probably rmn and rmr have only be verified on english , german , and italian , and there are some special techniques on chinese that should be specially considered .",
    "the proposed approach consistently improves lstm , and better than rmn and rmr on chinese , but worse than them on english corpora .",
    "the proposed model seems to be more robust across different corpora , but the improvements are limited .",
    "( a ) lstm & & & + ( b ) lstm+att & & & + ( c ) lstm+att+select & & & + ( d ) lstm+att+select+entropy & & & + ( e ) rmn  @xcite & & & + ( f ) rmr  @xcite & & & +   +   +   +      to illustrate how attention mechanism works , we visualize the attention weights in some sentences .",
    "we first compute the perplexities of each sentence in gigaword ( chinese ) and penn treebank ( english ) data sets , then select the sentences which improved the most by the proposed model ( row ( c ) in table 3 ) compared with the lstm baselines .",
    "we chose ten sentences from gigaword ( chinese ) and penn treebank data sets , and visualize and analysis the attention weights .",
    "four examples are shown in fig .",
    "[ fig : vis ] . in fig .",
    "[ fig : vis ] , the arrows point to the words to be predicted , and we highlight the words whose attention weights are higher than a threshold when predicting the words with arrows .",
    "we found that a word will have large attention under one of the following four conditions :    1 .",
    "trigger ( example ( a ) in fig .",
    "[ fig : vis ] : when the information is repeated , the model attends to the part where the same information is mentioned before . 2 .",
    "causal relationship ( example ( b ) ) : if @xmath58 is the cause of @xmath59 , when prediction the words related to @xmath59 , the model will attend to the words related to @xmath58 .",
    "phrases ( example ( c ) ) : when predicting a word in the later part of a phrase , the model will attend on the former part of the same phrase .",
    "grammar ( example ( d ) ) : some grammar rules are considered by the attention - based model .",
    "for example , to predict the word are , the model attends on a plural noun .",
    "in this paper , we propose attention - based memory selection recurrent network ( amsrn ) for language modeling and investigate the integration of attention mechanism and lstm .",
    "the results were verified on two english corpora and a chinese corpus .",
    "the results show that amsrn consistently outperformed lstm - based language model , and memory selection is essential for attention mechanism .",
    "we further visualize how the attention mechanism works in language modeling .",
    "some questions unresolved in this paper will be studied in the future , for example , the influence of the language characteristics to the attention - based model ."
  ],
  "abstract_text": [
    "<S> recurrent neural networks ( rnns ) have achieved great success in language modeling . however , since the rnns have fixed size of memory , their memory can not store all the information about the words it have seen before in the sentence , and thus the useful long - term information may be ignored when predicting the next words . in this paper , we propose attention - based memory selection recurrent network ( amsrn ) , in which the model can review the information stored in the memory at each previous time step and select the relevant information to help generate the outputs . in amsrn , </S>",
    "<S> the attention mechanism finds the time steps storing the relevant information in the memory , and memory selection determines which dimensions of the memory are involved in computing the attention weights and from which the information is extracted . in the experiments , amsrn outperformed long short - term memory ( lstm ) based language models on both english and chinese corpora . </S>",
    "<S> moreover , we investigate using entropy as a regularizer for attention weights and visualize how the attention mechanism helps language modeling .    language modeling , recurrent network , attention model </S>"
  ]
}