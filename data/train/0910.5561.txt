{
  "article_text": [
    "finding causal structures that generated the statistical dependences among observed variables has attracted increasing interest in machine learning .",
    "although there is in principle no method for reliably identifying causal structures if no randomized studies are feasible , the seminal work of spirtes et al .",
    "@xcite and pearl @xcite made it clear that under reasonable assumptions it is possible to derive causal information from purely observational data .",
    "the formal language of the conventional approaches is a graphical model , where the random variables are the nodes of a directed acyclic graph ( dag ) and an arrow from variable @xmath3 to @xmath4 indicates that there is a direct causal influence from @xmath3 to @xmath4 .",
    "the definition of `` direct causal effect '' from @xmath3 to @xmath4 refers to a hypothetical intervention where all variables in the model except from @xmath3 and @xmath4 are adjusted to fixed values and one observes whether the distribution of @xmath4 changes while @xmath3 is adjusted to different values .",
    "as clarified in detail in @xcite , the change of the distribution of @xmath4 in such an intervention can be derived from the joint distribution of all relevant variables after the causal dag is given .    the essential postulate that connects statistics to causality is the so - called causal markov condition stating that every variable is conditionally independent of its non - effects , given its direct causes @xcite .",
    "if the joint distribution of @xmath5 has a density @xmath6 with respect to some product measure @xmath7 ( which we assume throughout the paper ) , the latter factorizes @xcite into @xmath8 where @xmath9 is the set of all values of the parents of @xmath10 with respect to the true causal graph .",
    "the conditional densities @xmath11 will be called _ markov kernels_.",
    "they represent the mechanism that generate the statistical dependences .",
    "a large class of known causal inference algorithms ( like , for instance , pc , ic , fci , see @xcite ) are based on the causal faithfulness principle which reads : among all graphs that render the joint distribution markovian , prefer those structures that allow _ only _ the observed conditional dependences . in other words ,",
    "faithfulness is based on the assumption that all the observed independences are due to the causal structure rather than being a result of specific adjustments of parameters .",
    "one of the main limitations of this type of independence - based causal inference is that there are typically a large number of dags that induce the same set of independences",
    ". rules for the selection of hypotheses within these _ markov equivalence classes _ are therefore desirable .",
    "before we describe our method , we briefly sketch some methods from the literature . @xcite",
    "have observed that linear causal relationships between non - gaussian distributed random variables induce joint measures which require non - linear cause - effect relations for the wrong causal directions .",
    "their causal inference principle  @xcite for linear non - gaussian acyclic models ( short : lingam ) , is based on independent component analysis .",
    "it selects causal hypotheses for which almost linear cause - effect relations are sufficient whenever such hypotheses are possible for a given distribution .",
    "@xcite generalized this idea to the case where every variable is a possibly ( non - linear ) function of its direct causes up to some additive noise term that is independent of the causes ( see also @xcite and @xcite ) . under this assumption ,",
    "different causal structures induce , in the generic case , different classes of joint distributions even if the causal graphs belong to the same equivalence class .",
    "@xcite generalized the model class to the case where every function is additionally subjected to non - linear distortion compared to the models of @xcite .",
    "however , all these algorithms only work for real - valued variables and the generalization to discrete variables is not straightforward .",
    "here we describe a method ( first proposed in our conference paper @xcite ) that can deal with combinations of discrete and continuous variables , it even benefits from such a combination .",
    "more precisely , it requires that at least one of the variables is discrete or attains only values in a proper subset of @xmath0 .",
    "we define a parametric family of conditionals that induce different families of joint distributions for different causal directions .",
    "the underlying idea is related to an observation of @xcite stating that the same joint distribution of combinations of discrete and continuous variables may have descriptions in terms of simple markov kernels for one dag but require more complex ones for other dags .",
    "in contrast to @xcite , we define families of markov kernels that are derived from a unique principle , regardless of whether the variables are discrete or continuous .",
    "to describe our idea , assume that @xmath3 is a binary variable and @xmath4 real - valued and that we observe the joint distribution shown in fig .",
    "[ bimodal ] : let @xmath12 be a bimodal mixture of two gaussians such that both @xmath13 and @xmath14 are gaussians with the same width but different mean . then it is natural to assume that @xmath3 is the cause and @xmath4 the effect because changing the value of @xmath3 then would simply shift the mean of @xmath4 .    left : joint density @xmath15 of a real - valued variable @xmath3 and a binary variable @xmath3 suggesting a model @xmath16 , because the influence of @xmath3 on @xmath4 then only consists of shifting the mean of the gaussian .",
    "the causal hypothesis @xmath17 is less likely : only specific choices of @xmath18 would separate the bimodal gaussian mixture @xmath12 ( right ) into two separate modes . it requires an even more specific conditional @xmath18 to make the components gaussian.,title=\"fig:\"]left : joint density @xmath15 of a real - valued variable @xmath3 and a binary variable @xmath3 suggesting a model @xmath16 , because the influence of @xmath3 on @xmath4 then only consists of shifting the mean of the gaussian .",
    "the causal hypothesis @xmath17 is less likely : only specific choices of @xmath18 would separate the bimodal gaussian mixture @xmath12 ( right ) into two separate modes .",
    "it requires an even more specific conditional @xmath18 to make the components gaussian.,title=\"fig : \" ]    for the converse model @xmath19 , bimodality of @xmath4 remains unexplained .",
    "moreover , it seems unlikely , that conditioning on the effect @xmath3 separates the two modes of @xmath12 even though @xmath3 is not causally responsible for the bimodality .    to show that there are also joint distributions where @xmath17 is more natural , assume that @xmath12 is gaussian and the supports of @xmath13 and @xmath14 are @xmath20 $ ] and @xmath21 , respectively , as shown in fig .  [ threshold ] .",
    "joint density @xmath15 of a real - valued random variable @xmath4 and a binary variable @xmath3 .",
    "the marginal distribution @xmath12 is gaussian .",
    "the causal hypothesis @xmath17 is plausible : the conditional @xmath18 corresponds to setting @xmath22 for all @xmath23 above a certain threshold .",
    "we reject the converse hypothesis @xmath16 because @xmath24 and @xmath25 share algorithmic information : given @xmath24 , only _",
    "specific _ choices of @xmath25 reproduce the gaussian @xmath12 , whereas generic choices of @xmath25 would yield `` odd '' densities of the type on the right.,title=\"fig:\"]joint density @xmath15 of a real - valued random variable @xmath4 and a binary variable @xmath3 .",
    "the marginal distribution @xmath12 is gaussian .",
    "the causal hypothesis @xmath17 is plausible : the conditional @xmath18 corresponds to setting @xmath22 for all @xmath23 above a certain threshold .",
    "we reject the converse hypothesis @xmath16 because @xmath24 and @xmath25 share algorithmic information : given @xmath24 , only _",
    "specific _ choices of @xmath25 reproduce the gaussian @xmath12 , whereas generic choices of @xmath25 would yield `` odd '' densities of the type on the right.,title=\"fig : \" ]    one can easily think of a causal mechanism whose output @xmath26 is @xmath27 for all inputs @xmath23 above a certain threshold @xmath28 , and @xmath29 otherwise .",
    "assuming @xmath16 , we would require a mechanism that generates outputs @xmath23 from inputs @xmath26 according to @xmath24",
    ". given this mechanism , there is only one distribution @xmath25 of inputs for which @xmath12 is gaussian .",
    "hence , the generation of the observed joint distribution requires mutual adjustments of parameters for this causal model .",
    "in section  [ just ] we will describe more formal arguments that support this way of reasoning .",
    "it is based on ideas in @xcite and @xcite to draw causal conclusions not only from _ statistical _ dependences . instead , also _",
    "algorithmic _ information can indicate causal directions .",
    "here we define a parametric family of markov kernels @xmath11 that describe a simple way how @xmath10 is influenced by its parents @xmath30 . without loss of generality , we will only consider complete acyclic graphs , i.e. , the parents of @xmath10 are given by @xmath31 ( the general case is implicitly included by setting the corresponding parameters to zero ) .",
    "the domains @xmath32 of @xmath10 are subsets of @xmath33 with integer hausdorff dimension @xcite @xmath34 , i.e. , we exclude fractal subsets . in sections  [ ident ] and [ expe ] we will consider , for instance , intervals in @xmath35 , circles in @xmath36 , and countable subsets of @xmath35 .",
    "we define @xmath37 with vector - valued parameters @xmath38 and matrix - valued parameters @xmath39 .",
    "the log - partition functions @xmath40 are given by @xmath41 where the reference measure @xmath42 is given by the product of the hausdorff measures @xcite of the corresponding dimensions @xmath34 , and only parameters are allowed that yield normalizable densities .",
    "the term `` hausdorff measure '' only formalizes the natural intuition of a volume of sufficiently well - behaved subsets of @xmath0 : for a circle , for instance , it is given by the arc length , for countable subsets it is just the counting measure .    for every reordering @xmath43 of variables ,",
    "the second order conditionals define a family of joint distributions @xmath44 .",
    "the key observation on which our method relies is that @xmath44 and @xmath45 need not to coincide if some of the variables @xmath10 have domains @xmath32 that are _ proper _ subsets of @xmath0 ( if , for instance , all @xmath10 can attain all values in @xmath35 , then @xmath46 is the set of all non - degenerate @xmath47-variate gaussians for all @xmath43 and we can not give preference to any causal ordering ) .",
    "our inference rule reads : if there are causal orders @xmath43 for which the observed density @xmath48 is in @xmath44 , prefer them to orderings @xmath49 for which @xmath50 .",
    "to apply this idea to finite data where @xmath48 is not available , we prefer the orderings @xmath43 for which the kullback - leibler distance between the empirical distribution and @xmath48 is minimized , i.e. , the likelihood of the data is maximized .    in section  [ expe ] ,",
    "we discuss experiments with just two variables @xmath51 .",
    "we have several cases where @xmath3 is binary and @xmath4 real - valued , and one example where @xmath3 is two - dimensional and attains values on a circle and @xmath4 is real - valued .",
    "this shows that also causal structures containing only continuous variables can be dealt with by our method when some of the domains are restricted .",
    "we now describe the algorithm .    *",
    "second order model causal inference *    1 .   given an @xmath52 matrix of observations @xmath53 .",
    "2 .   let @xmath5 be an ordering @xmath43 of the variables .",
    "3 .   in the @xmath54th step ,",
    "compute @xmath55 by minimizing the conditional inverse log - likelihood @xmath56 with @xmath57 to compute the partition function numerically , we discretize and bound the domain to a finite set of points . 4 .",
    "compute the corresponding joint density @xmath58 and its total log - likelihood @xmath59 5 .   select the causal orderings for which @xmath60 is minimal .",
    "this can be a unique ordering or a set of orderings because not all orderings induce different families of joint distributions and because values @xmath60 and @xmath61 are considered equal if their difference is below a certain threshold .",
    "for a preliminary justification of the approach , we recall that conditionals of this kind occur from maximizing the conditional shannon entropy @xmath62 subject to @xmath63 and subject to the given first and second moments @xcite , for more details see also @xcite : @xmath64 where @xmath65 denotes the expected value of a variable @xmath66 . for multi - dimensional @xmath10 , the @xmath67 are vectors and the @xmath68 are matrices .",
    "bilinear constraints are the simplest constraints for which the entropy maximization yields interactions between the variables @xmath10 ( apart from this , linear constraints would not yield normalizable densities for unbounded domains ) . in this sense , second order models generate the simplest non - trivial family of conditional densities within a hierarchy of exponential models @xcite that are given by entropy maximization subject to higher order moments .",
    "@xcite provides a thermodynamic justification of second order models .",
    "the paper describes models of interacting physical systems , where the joint distribution is given by first maximizing the entropy of the _ cause_-system and then the conditional entropy of the _ effect_-system , given the distribution of the cause .",
    "both entropy maximizations are subject to energy constraints .",
    "if we assume that the physical energy is a polynomial of second order in the relevant observables ( which is not unusual in physics ) , we obtain exactly the second order models introduced here .",
    "here we describe examples that show how the restriction of the domains to proper subsets of @xmath35 can make the models identifiable . a case with vector - valued variables",
    "has already been described in @xcite , where we have considered the causal relation between the day in the year and the average temperature of the day .",
    "the former takes values on a circle in @xmath36 , the latter is real - valued .",
    "second order models from day to temperature induce seasonal oscillations of the average temperature according to a sine function , which was closer to the truth than the second order model from temperature to day in the year .",
    "however , in the following examples we will restrict the attention to one - dimensional variables .",
    "[ ident ]      a simple case where cause and effect is identifiable in our model class is already given by the motivating example with a binary variable @xmath3 and a variable @xmath4 that can attain all values in @xmath35 .      using both equations  ( [ so ] ) , we obtain @xmath69 with parameters @xmath70",
    ". both distributions @xmath71 for @xmath72 are obviously gaussians with equal width and different mean , i.e. , @xmath12 is a mixture of two gaussians ( see fig .",
    "[ bimodal ] , left ) .",
    "we obtain @xmath73 with parameters @xmath74 , where we have used @xmath75 a typical joint distribution for the model @xmath17 is shown in fig .",
    "[ tanh ] .",
    "joint density @xmath15 for binary @xmath3 and real - valued @xmath4 induced by a second order model @xmath17 .",
    "here we have chosen a relatively steep sigmoid function for @xmath76 , which leads to a steep decrease at the right of the left mode and the left of the right mode . an infinitely steep sigmoid function yields sharp thresholding as in fig .",
    "[ threshold ] . ]    since mixtures of two different gaussians can never yield a gaussian as marginal distribution @xmath12 , the only joint distribution that is contained in the model classes for both directions is a product distribution of a gaussian @xmath12 and an arbitrary binary distribution @xmath25 .",
    "this shows that the models are identifiable except for the trivial case of independence .",
    "furthermore , eqs .",
    "( [ gaussmixpara ] ) and ( [ sigmoidpara ] ) show that our method is indeed consistent with the intuitive arguments we gave for the examples in the introduction : the gaussian mixture in fig .",
    "[ bimodal ] is a second order model for @xmath16 and the example with thresholding @xmath23 ( fig .",
    "[ threshold ] ) can be approximated by second order models for @xmath19 via the limit @xmath77 in eq .",
    "( [ sigmoidpara ] ) .",
    "we first simplify equation  ( [ so ] ) for the case that all variables @xmath5 are binary .",
    "writing @xmath78 for the ancestors of @xmath79 , we obtain @xmath80 using eq .",
    "( [ exptanh ] ) yields @xmath81 with @xmath82    the joint distributions induced by these conditionals do not coincide for all causal orders provided that @xmath83 . to show this , we first observe that second order models can approximate the causal relation between the inputs and the output of an @xmath84-bit or gate .",
    "then we show that the conditional probability for one input , given the other @xmath85 inputs and the output is significantly more complex than a second order model since it requires polynomials of degree @xmath85 as argument of the @xmath86-function ( which corresponds to polynomials of degree @xmath87 in the exponent in the same way as second order models lead to linear arguments of @xmath86 ) .",
    "the or gate with input @xmath88 and output @xmath89 is described by @xmath90 introducing a sequence of second order conditionals by @xmath91 we have @xmath92 and thus they approximate the or - gate .",
    "let the inputs @xmath88 be sampled from the uniform distribution over @xmath93 .",
    "we then have @xmath94 and @xmath95 note that the event @xmath96 and @xmath97 for some @xmath98 does not occur and the corresponding conditional probabilities need not to be specified .",
    "we now show that the joint distribution can not be approximated by second order models if @xmath89 is not the last node . for symmetry reasons , it is sufficient to show that @xmath99 has no second order model approximation .",
    "if such an approximation existed , we would have @xmath100 where @xmath101 is a sequence of _ linear _ functions in @xmath102 , see eq .",
    "( [ tanhref ] ) .",
    "we prove that eq .",
    "( [ polyappr ] ) can indeed be satisfied with @xmath101 of polynomials of order @xmath85 , but not for any sequence of polynomials of lower order ( which shows that these non - causal conditional can be rather complex ) .",
    "introducing @xmath103 eq .  ( [ condb ] ) is equivalent to @xmath104 if the space of polynomials of degree @xmath105 or lower contained such a sequence @xmath106 , completeness of finite dimensional real vector spaces implies that it also contained @xmath107 which is given by @xmath108 however , @xmath109 which is a polynomial of degree @xmath85 .",
    "hence , @xmath106 ( and also @xmath101 ) consists at least of polynomials of order @xmath85 . to see that this bound is tight , set @xmath110 and observe that it satisfies eq .",
    "( [ equivcondb ] ) and @xmath111 and thus the corresponding conditionals satisfy asymptotically eqs.([condb ] ) and ( [ condb2 ] ) .    by inverting logical values",
    ", the same proof applies to and gates . since and and or gates are reasonable models for many causal relations in real - life ,",
    "it is remarkable that the corresponding non - causal conditionals of the generated joint distribution already require exponential models of high order .",
    "successful experiments with artificial and real - world data with four binary variables are briefly sketched in @xcite .",
    "since second order models provide a simple class of non - trivial conditional densities , occam s razor seems to strongly support the principle of preferring the direction that admits such a model .",
    "however , occam s razor can not justify why we should try to find simple expressions for the causal conditional @xmath1 instead of simple models for non - causal conditionals like @xmath112 . here",
    "we present a justification that is based on recent algorithmic information theory based approaches to causal inference .",
    "@xcite proposed to prefer those dags as _ causal _ hypotheses for which the shortest description of the joint density @xmath6 is given by separate descriptions of causal conditionals @xmath11 in eq .",
    "( [ fac ] ) .",
    "we will refer to this as the principle of independent conditionals ( ic ) . here , the description length is measured in terms of algorithmic information @xcite , sometimes also called `` kolmogorov complexity '' . even though it is hard to give a precise meaning to this principle",
    ", it provides the leading motivation for our theory .    to show this , we reconsider one of the examples from the introduction .",
    "we have argued that the distribution in fig .",
    "[ threshold ] is unlikely to be generated by the causal structure @xmath16 because the observed distribution @xmath25 is special among all possible @xmath113 since it is the only distribution that yields a gaussian marginal @xmath12 after feeding it into the conditional @xmath24 .",
    "hence , after knowing @xmath24 , the input distribution @xmath25 is simply described by `` the unique input that renders @xmath12 gaussian '' .",
    "thus , a description for @xmath15 that contains separate descriptions of @xmath24 and @xmath25 would contain redundant information and the ic principle would be fail .",
    "we propose a slightly modified version of ic that will be more convenient to use because it refers to algorithmic dependences between _",
    "un_conditional distributions :    @xmath114 + [ modpost ] if the joint density @xmath15 is generated by the causal structure @xmath16 then the following condition must hold :    let @xmath113 be a hypothetical input density that has been chosen without knowing @xmath15 .",
    "define @xmath115 .",
    "then @xmath25 and @xmath116 are algorithmically independent .",
    "the idea is that @xmath116 only contains algorithmic information about @xmath24 and @xmath113 .",
    "the object @xmath24 has been chosen independently of @xmath25 `` by nature '' , as in @xcite , and @xmath113 has been chosen independently of @xmath24 by assumption .    due to the lack of a precise meaning of the concept of `` algorithmic information of probability densities '' , as it would be required by @xcite and our modified postulate , we will describe arguments that avoid such concepts but still rely on the above intuition .",
    "we therefore rephrase the probability - free approach to causal inference developed by @xcite .",
    "the idea is that causal inference in real life often does not rely on statistical dependences .",
    "instead , similarities between single objects indicate causal relations . observing , for instance ,",
    "that two carpets contain the same patterns makes us believe that designers have copied from each other ( provided that the patterns are complex and not common ) .",
    "@xcite develop a general framework for inferring causal graphs that connect individual objects based upon _",
    "algorithmic _ dependences . here , two objects are called algorithmically independent if their shortest joint description is given by the concatenations of their separate descriptions .",
    "it is assumed that every such description is a binary string @xmath117 formalizing all relevant properties of an observation .",
    "then the kolmogorov complexity @xmath118 of @xmath117 is defined by the length of the shortest program that generates the output @xmath117 and then stops .",
    "conditional kolmogorov complexity @xmath119 is defined as the length of the shortest program that computes @xmath117 from the input @xmath120 .",
    "if @xmath121 denotes the shortest compression of @xmath120 , @xmath122 can be smaller than @xmath119 because there is no algorithmic way to obtain the shortest compression ( the difference between @xmath119 and @xmath122 can at most be logarithmic in the length of @xmath120 @xcite ) .",
    "the strings @xmath117 and @xmath120 are conditionally independent , given @xmath123 if @xmath124 as in the statistical setting , unconditional depedences indicate causal links between two objects @xmath117 and @xmath120 : if @xmath125 the descriptions can be better compressed jointly than independently and we postulate a causal connection .",
    "the following terminology @xcite will be crucial :    @xmath114 + for any two binary strings @xmath126 , the difference @xmath127 is called the _ algorithmic mutual _ information between @xmath117 and @xmath120 . as usual in algorithmic information theory , the symbol @xmath128 denotes equality up to a constant that is independent of the strings @xmath126 , but does depend on the turing machine @xmath129 refers to .    to also infer causal directions",
    "we have postulated a causal markov condition stating conditional independence of every object from its non - effects , given its causes . we will here state an equivalent version ( see theorem  3 in @xcite ) :    @xmath114 + [ algc ] let @xmath130 be a dag with the binary strings @xmath131 as nodes .",
    "if every @xmath132 is the description of an object or an observation in real world and @xmath130 formalizes the causal relation between them , then the following condition must hold .",
    "for any three sets @xmath133 we have @xmath134 in the sense of eq .",
    "( [ alin ] ) , whenever @xmath135 d - separates @xmath136 and @xmath137 ( for the notion of d - separation see , e.g. , @xcite ) .",
    "here we have slightly overloaded notation and identified the set of strings with their concatenation .",
    "moreover , @xmath138 denotes the shortest compression of @xmath135 .",
    "in particular , we have @xmath139 whenever @xmath136 and @xmath137 are d - separated ( by the empty set ) . here",
    ", the threshold for counting dependences as significant is up to the decision of the researcher and not provided by the theory .",
    "based on the above framework and inspired by the ic - principle , @xcite describe the following approach to distinguishing between @xmath16 and @xmath17 for two random variables @xmath51 after observing the samples @xmath140 .",
    "one considers the causal structure among @xmath141 individual objects instead of a dag with the two variables as nodes .",
    "these objects are : the @xmath26-values , the @xmath23-values , the source @xmath136 emitting @xmath26-values according to @xmath25 and a machine emitting @xmath23-values according to @xmath24 .",
    "the causal dag connecting the objects is shown in fig .",
    "[ res ] , left .",
    "one may wonder why there are no arrows from the @xmath26-values to @xmath142 even though @xmath142 gets them as inputs .",
    "the reason is that the object @xmath142 is not changed by the @xmath79 , i.e. , the conditional @xmath24 remains constant .",
    "left : causal structure obtained by resolving the statistical sample generated by the causal structure @xmath16 into single observations .",
    "right : modified structure where the input comes from a different source @xmath143 that samples according to a different distribution @xmath144 .",
    "note that @xmath145 and @xmath146 must be algorithmically independent because there is no unblocked path between these two sets of nodes.,title=\"fig:\"]left : causal structure obtained by resolving the statistical sample generated by the causal structure @xmath16 into single observations .",
    "right : modified structure where the input comes from a different source @xmath143 that samples according to a different distribution @xmath144 .",
    "note that @xmath145 and @xmath146 must be algorithmically independent because there is no unblocked path between these two sets of nodes.,title=\"fig : \" ]    it has been pointed out @xcite that the dag in fig .",
    "[ res ] , left , already imposes the algorithmic independence relation @xmath147 and describes examples where this is violated after exchanging the role of @xmath3 and @xmath4 .",
    "this is an observable implication of the algorithmic independence of the unobservable objects @xmath136 and @xmath142 . the relevant information about @xmath136 and @xmath142 is given by @xmath25 and @xmath24 , respectively , hence condition ( [ dsep ] ) is closely linked to lemeire s and dirkx s postulate .",
    "@xcite discusses toy examples for which the destinction between @xmath16 and @xmath17 is possible using condition  ( [ dsep ] ) .    for our purposes",
    ", it will be more convenient to work with a slightly different condition that can be seen as a finite - sample counterpart of postulate  [ modpost ] .",
    "to this end , we consider fig .",
    "[ res ] , right .",
    "let @xmath148 be the sample of @xmath26-values from source @xmath136 ( here @xmath149 ) .",
    "@xmath150 denote the @xmath26-values from source @xmath143 and @xmath151 the corresponding @xmath23-values .",
    "the d - separation criterion yields the unconditional relation @xmath152 of course , we do not assume that we have the option to really change the input distribution from @xmath25 to @xmath113 ( i.e. , replacing the source @xmath136 with @xmath143 ) , otherwise we could directly test whether @xmath3 causes @xmath4 by observing whether such an intervention also changes @xmath12",
    ". our way of reasoning will be indirect : given that the true causal structure is @xmath153 , we could simulate the effect of the intervention by choosing a subsample of @xmath26-values that is distributed according to @xmath113 and know that the corresponding pairs @xmath154 are distributed according to @xmath155 .",
    "we now describe the violation of condition  ( [ undsep ] ) for the example in fig .",
    "[ threshold ] .",
    "the true model @xmath17 involves the parameters @xmath156 , @xmath157 , and @xmath158 ( mean and standard deviation of the gaussian and threshold of @xmath23-values for which @xmath22 ) .",
    "we denote the corresponding density therefore by @xmath159 .",
    "now we consider the _ non - causal _",
    "conditional @xmath160 .",
    "one checks easily that different triples @xmath161 indeed induce different @xmath24 . on the other hand",
    ", there is a unique input probability @xmath162 such that the marginal @xmath163 is gaussian : @xmath164 the fact that @xmath165 that does not involve any free parameters would already be in contradiction with lemeire s and dirkx s postulate if @xmath16 were true because @xmath165 only has constant description length and the parameters can be described with arbitrary accuracy . for sufficiently large accuracy , the description length of @xmath166 thus exceeds the length of @xmath165 and eq .",
    "( [ fmu ] ) provides a shorter description for @xmath166 than its explicit binary representation .    according to our finite - sample point of view",
    ", the parameters @xmath167 must only be described up to an accuracy that corresponds to the error made when estimating them from a finite sample : observing @xmath168 , i.e. , an ensemble of @xmath26-values drawn from @xmath25 , we can estimate @xmath166 up to a certain accuracy .",
    "similarly , we estimate @xmath169 after observing @xmath170 up to a certain accuracy .",
    "then , the estimator for @xmath7 and the estimator of the other parameters will approximately satisfy the functional relation ( [ fmu ] ) .",
    "this shows that @xmath171 , on the one hand , and @xmath172 on the other hand , share at least that amount of algorithmic information shared by the estimators because the latter have only processed the information contained in the observations .    for general second order models ,",
    "the argument reads as follows .",
    "for @xmath17 we have parameter vectors @xmath173 and @xmath174 for @xmath12 and @xmath18 , respectively .",
    "the joint distribution is determined by @xmath175 .",
    "factorizing @xmath176 into the non - causal conditionals @xmath25 and @xmath24 leads to families @xmath177 and @xmath178 , where @xmath179 only is an element of the family @xmath180 if @xmath181 and @xmath182 satisfy a certain functional relation and thus share algorithmic information . to translate this into algorithmic dependences between ( real and hypothetical ) observations , we feed @xmath178 with a modified input distribution @xmath183 and observe that the generated @xmath154-pairs still share algorithmic information with those @xmath26-values that were sampled from the original input distribution because @xmath182 can be estimated from the new @xmath154 pairs and @xmath181 from the original @xmath26-values . if the parameters @xmath184 and @xmath181 are algorithmically independent , the sources @xmath136 and @xmath143 in fig .",
    "[ res ] , right , are independent and the causal hypothesis @xmath16 implies independence of @xmath168 and @xmath185 .",
    "this way of reasoning can further be generalized as follows : assume that @xmath15 can be described by @xmath179 where @xmath177 and @xmath178 are some families of densities for which the map from @xmath186 to the corresponding probabilities is a computable function .",
    "then @xmath16 can be rejected whenever one of the parameter is determined by the other one via a computable function @xmath165 provided that the kolmogorov complexity of both parameters is infinite ( which can , of course , never be proved ) .",
    "the accuracy of estimating @xmath187 depends on the statistical distinguishability of the ( conditional ) densities from those for slightly modified @xmath188 .",
    "therefore , fisher information of parametric families plays a crucial role in the following quantitative result :    @xmath114 + [ mainth ] let @xmath177 with @xmath189 and @xmath178 with @xmath190 be computable families of continuously differentiable ( conditional ) densities . define the fisher information matrix for @xmath177 by @xmath191 where @xmath192 defines the hausdorff measure corresponding to @xmath3 . define the conditional fisher information matrix for @xmath178 with respect to the reference input distribution @xmath177 by @xmath193    let @xmath194 be drawn from @xmath177 and @xmath195@xmath196@xmath197 from @xmath198 where @xmath199 and @xmath200 are non - singular and @xmath181 and @xmath182 are generic in the sense that a description up to an error @xmath201 ( in vector norm ) requires @xmath202 or @xmath203 bits , respectively .",
    "assume , moreover , that @xmath181 and @xmath182 are related as follows . if @xmath204 , let @xmath205 for some continuously differentiable function @xmath165 with @xmath206 . for @xmath207 ,",
    "let @xmath208 for some continuously differentiable @xmath209 with @xmath210 .",
    "then the algorithmic mutual information between the @xmath26-values sampled from the original distribution and the @xmath154-pairs generated by the modified input distribution satisfies asymptotically almost surely @xmath211 for every @xmath212 .",
    "note that the requirement of `` generic '' parameter values ( in the sense we used the term ) can be met by a model where `` nature chooses '' them according to some prior density .",
    "since the statement is only an asymptotic one , the theorem holds regardless of the prior .",
    "proof of theorem  [ mainth ] : assume first that @xmath204 .",
    "we define an estimator @xmath213 for @xmath181 by minimizing @xmath214 hence , @xmath215 with probability converging to @xmath27 for @xmath216 if @xmath217 denotes the smallest eigenvalue of @xmath199 .",
    "this is because @xmath218 is asymptotically a @xmath219-dimensional gaussian with concentration matrix @xmath199 .",
    "the standard deviation of the gaussian is maximal for the direction corresponding to @xmath217 and is then given by @xmath220 .",
    "we construct an estimator @xmath221 by minimizing the inverse loglikelihood @xmath222 since @xmath200 is non - singular , @xmath178 is a strict minimum of the expected loglikelihood . as for the unconditional distributions above",
    ", @xmath223 is asymptotically gaussian and the probability for @xmath224 tends to @xmath27 if @xmath225 denotes the smallest eigenvalue of @xmath200 .",
    "denoting the operator norm of the jacobi matrix @xmath226 by @xmath227 , we obtain @xmath228 where the last inequality holds asymptotically almost surely for any @xmath229 .    due to the error bounds ( [ fhat ] ) and ( [ etahat ] ) we have @xmath230 asymptotically with probability @xmath231 for any desired @xmath232 .",
    "since @xmath182 is a generic value , the amount of information required to specify it up to an accuracy @xmath233 grows asymptotically with @xmath234 ( up to some negligible constant ) . on the other hand , @xmath221 and @xmath235 share at least this amount of information because they also coincide up to an accuracy @xmath233 .",
    "hence , @xmath236 asymptotically , @xmath234 grows with @xmath237 .",
    "hence the mutual information between @xmath221 and @xmath235 is asymptotically larger than @xmath238 bits for every @xmath212 .",
    "hence we have @xmath239 the first inequality follows because @xmath240 whenever @xmath241 ( cf .",
    "theorem ii.7 in @xcite ) .",
    "here @xmath242 because @xmath235 is computed from the @xmath243 observed @xmath26-values by the above estimation procedure and the application of @xmath165 .",
    "likewise , @xmath221 is derived from the observed @xmath154-pairs .    the case for @xmath207",
    "is shown similarly .",
    "we estimate @xmath182 and @xmath181 and show that they share algorithmic information because @xmath181 is a simple function of @xmath182 .",
    "@xmath244    now we present our main theorem stating that second order models between one binary and one real - valued variables induce joint distributions whose _ non - causal _ marginals and conditionals are algorithmically dependent in the sense of theorem  [ mainth ] :    @xmath114 + [ justso ] let @xmath3 be a binary variable and @xmath4 real - valued and the density of @xmath15 be given by a second order model from @xmath4 to @xmath3 for some generic values of the parameters @xmath74 in eq .",
    "( [ sigmoidpara ] ) , left and right .",
    "then the causal hypothesis @xmath153 contradicts the algorithmic markov condition .",
    "this is because the @xmath26-values sampled from @xmath25 contain algorithmic information about the @xmath154-pairs obtained after changing the `` input '' distribution @xmath25 ( see fig  [ res ] , right ) and keeping @xmath24 .    likewise , if @xmath15 admits a second order model from @xmath3 to @xmath4 with generic values @xmath70 ( see eq .",
    "( [ gaussmixpara ] ) , then @xmath17 must be rejected .",
    "the amount of the shared algorithmic information grows at least logarithmically in the sample size .",
    "the remainder of this section is devoted to the proof of theorem  [ justso ] and a lemma that is required for this purpose . to show that the conditions of theorem  [ mainth ] are met , we determine the parameter vectors @xmath187 of the non - causal conditionals ,",
    "show that they satisfy a functional relation and that the fisher information matrices are nonsingular . to prove the latter statement",
    ", we will use the following result :    [ central ] let @xmath245 for all @xmath246 be a differentiable family of continuous positive definite densities on a probability space @xmath247 with respect to the reference measure @xmath7 .",
    "assume there are @xmath219 points @xmath248 @xmath249 , @xmath196 @xmath250 such that the matrix @xmath251 defined by @xmath252 or the matrix @xmath253 is non - singular .",
    "then the fisher information matrix @xmath199 is non - singular .",
    "proof : the fisher information matrix can be rewritten as @xmath254 hence , @xmath255 it thus is the weighted integral over all rank one matrices @xmath256 at the same time , it can also be written as a weighted integral over all @xmath257 note that for any vector - valued continuous function @xmath258 and strictly positive scalar function @xmath259 , the image of the matrix @xmath260 is given by the span of all @xmath261 .",
    "@xmath199 thus is the span over all @xmath262 and , at the same time , the span over all @xmath263 .",
    "@xmath244    we are now able to prove the main theorem :    proof ( of theorem  [ justso ] ) : first consider the case where @xmath15 has a second order model from @xmath4 to @xmath3 . to apply theorem  [ mainth ] we have to show that @xmath264 is non - singular .",
    "we can use lemma  [ central ] even though it is not explicitly stated for _ conditional _ densities because we can apply the latter to the joint density @xmath265 for @xmath266 and fixed @xmath181 . then @xmath267 and @xmath268 i.e. , it is sufficient to check whether the gradients of the _ conditional _ or its logarithm span a @xmath269-dimensional space .",
    "we have @xmath270 where we have used eq .",
    "( [ exptanh ] ) .",
    "this yields @xmath271 introducing the parameter vector @xmath272 we obtain @xmath273 where the input distribution @xmath25 still is formally parameterized by @xmath182 and will be written in terms of one relevant parameter @xmath181 below . in the appendix we provide @xmath274 points @xmath275 and a value @xmath276",
    "for which the vectors @xmath277 are linearly independent .",
    "hence @xmath278 is non - singular for @xmath279 and all @xmath181 .",
    "all entries of @xmath264 are analytical functions in every component of @xmath182 because they are uniformly converging integrals over analytical functions .",
    "hence , regularity of @xmath278 for one @xmath182 already shows regularity for _ generic _ @xmath182 .",
    "now we parameterize @xmath25 by an one - dimensional parameter @xmath280 where @xmath281 is given by the integral in eq .",
    "( [ x=1 ] ) .",
    "this defines the family of densities @xmath282 via @xmath283 hence @xmath199 is one - dimensional .",
    "it is clearly non - singular for generic @xmath181 because @xmath284 using @xmath210 , theorem  [ mainth ] shows that the @xmath26 values sampled from @xmath177 share algorithmic information with the @xmath154-pairs sampled from @xmath285 .",
    "now consider the case that there is a second order model from @xmath3 to @xmath4 .",
    "hence @xmath286 with the parameter vector @xmath287 .",
    "note that we now apply theorem  [ mainth ] with exchanging the role of @xmath3 and @xmath4 . to show that @xmath199 is non - singular we compute @xmath288 and find points @xmath275 and a value @xmath181 such that the corresponding gradients are linearly independent ( see appendix  [ kram ] ) .",
    "hence @xmath289 is nonsingular due to lemma  [ central ] . as above",
    ", this also holds for generic @xmath181 .    for the conditional density of @xmath3 given @xmath4 ,",
    "only a function of @xmath181 is relevant ( as above ) but we start by writing it first in terms of @xmath181 and reduce the parameter space later to the relevant part : @xmath290^{-1}\\,.\\end{aligned}\\ ] ] introducing @xmath291 and @xmath292 the conditional is of the form @xmath293 we define @xmath294 and check that @xmath278 is non - singular . for doing so , we compute @xmath295 and find values @xmath279 and @xmath296 such that the gradients are linearly independent ( appendix ) . hence @xmath278 is non - singular for one @xmath279 and all @xmath181 and thus also for generic pairs @xmath297 . the function @xmath209 is given by @xmath298 with @xmath299 and @xmath158 as in eqs .",
    "( [ al ] ) and ( [ bet ] ) , which satisfies @xmath210 .",
    "this shows that the @xmath23-values sampled from @xmath300 share algorithmic information with the @xmath154-pairs sampled from @xmath301 by theorem  [ mainth ] .",
    "we conducted 8 experiments with real - world data for which the causal structure is known . in all cases we had pairs of variables where one is the cause and one the effect .",
    "even though there may also be hidden common causes , prior knowledge strongly suggests that a significant part of the dependences are due to an arrow from one variable to the other .",
    "the selection of datasets was based on the following criteria : we have chosen several examples where one variable is binary and the other one is either continuous or discrete with a wide range , because this is the case where identifiability becomes most obvious ( see subsection  [ bico ] ) . to demonstrate that we have identifiability for various types of value sets we have also included an example with a variable of angular - type and example with positive variables .",
    "the restriction to positive values , however , only leads to significantly different distributions for different causal directions if there is enough probability close to the boundary . otherwise , the second order models yield almost bivariate gaussians and the direction is not identifiable .",
    "most examples of the data base `` cause effect pairs '' in the nips 2008 causality competition @xcite are of this type , except for the examples with `` altitude '' .",
    "our algorithm constructs the domains by binning the observed values into intervals of equal length instead of asking for the range as additional input .",
    "if the differences of the loglikelihoods are too small , our algorithm will not decide for either of the causal directions .",
    "we have set the treshold to @xmath302 the choice of this threshold , however , is the result of our limited number of experiments .",
    "our theory in section  [ just ] only states the following : if the true distribution _ perfectly _ coincides with a second order model in one direction but not the other , the latter one has to be rejected because this causal structure would require unlikely adjustments . for the case where the distribution is only close to a second order model",
    "it is hard to analyze how close it should be to justify our causal conclusion .",
    "the answer to this question is left to the future .",
    "experiment no .",
    "1 considers the altitude and average temperature of 675 locations in germany @xcite .",
    "the statistical dependence between both variables is very obvious from scatter plots and one observes an almost linear decrease of the temperature with increasing altitude . the fact that a significant part of the points are close to altitude @xmath29 ( i.e. , the minimal value ) is important for identifiability of the causal direction because the restriction of the domain to positive values can only be relevant in this case .",
    "experiment no .",
    "2 studies the relation between altitude and precipitation of 4748 locations in germany @xcite . here",
    "both variables are positive - valued , which also leads to different models in the two directions .",
    "in experiment no .  3 , we were given the daily temperature averages of 9162 consecutive days between 1979 and 2004 in furtwangen , germany @xcite .",
    "the seasonal cycle leads to a strong statistical dependence between the variable day in the year ( represented as a point on the unit circle @xmath303 ) and temperature , where the former should be considered as the cause since it describes the position of the earth on its orbit around the sun .",
    "our experiments no .  4 and no .",
    "5 consider two datasets from the same psychological experiment on human categorization .",
    "the subjects are shown artificially generated faces that interpolate between male and female faces @xcite .",
    "the interpolation correponds to switching a parameter between @xmath27 and @xmath304 ( in integer steps ) .",
    "the subjects are asked to decide whether the face is male ( answer=0 ) or female ( answer=1 ) .",
    "the experimentalist has chosen parameter values according to a uniform distribution on @xmath305 .",
    "4 studies the relation between parameter and answer .",
    "since the experimentalist chose uniform distribution over @xmath305 and the dependence of the probability for answer@xmath306 is close to a sigmoid function , the empirical distribution is here very close to the second order model corresponding to the correct causal structure parameter @xmath307 answer .",
    "our experiment no .",
    "5 studies the relation between the response time and the parameter values .",
    "since the response time is minimal for both extremes in the parameter values , we have strongly non - linear interactions that can not be captured by second - order models .",
    "it is therefore not surprising that there is no decision in this case .",
    "experiments no .  6 and no .",
    "7 consider census data from 35.326 persons in the usa @xcite . in no .",
    "6 , the relation between age and marital status is studied .",
    "the latter takes the two values @xmath29 for never married and @xmath27 for married , divorced , or widowed .",
    "7 considers the relation between gender and income .",
    "here we assume that the gender is almost randomized by nature and there we thus expect no confounding to any observable variable .",
    "experiment no.8 considers the concentration of proline in wine from two different cultivars .",
    "we assume that the binary variable cultivar is the cause , even though one can not exclude that the proline level ( if relevant for the taste ) directly influenced the decision of the cultivar to choose this sort of wine .",
    "the results are shown in the below table .",
    "the ground truth is always that variable 1 influences variable 2 , i.e. , we have one wrong result and no decision in two cases .    [",
    "cols=\"<,<,<,<,<,<,<\",options=\"header \" , ]",
    "in section  [ just ] we have shown for a special case that the model @xmath16 must be rejected if there is a second order model from @xmath4 to @xmath3 because it required specific mutual adjustments of @xmath25 and @xmath24 to admit such a model .",
    "we have already mentioned that this is the same idea as rejecting unfaithful distributions .",
    "indeed , @xcite argued that the markov kernels in unfaithful distributions share algorithmic information .",
    "hence algorithmic information theory provides a unifying framework for independence - based approaches and those that impose constraints on the shapes of conditional densities .",
    "the following example makes this link even closer because it shows that in some situations the same constraints on a joint distribution may appear as _ independence _ constraints from one point of view and as constraints on the _ shape _ of conditionals from an other perspective .",
    "consider the causal chain @xmath308 where every @xmath10 is a vector of dimension @xmath219 .",
    "structures of this kind occur , for instance , if @xmath10 represents the state of some system at time @xmath120 and the dynamics is generated by a first order markov process . due to the causal markov condition the joint distribution factorizes into @xmath309 but no constraints",
    "are imposed on the conditionals @xmath310 .",
    "assume now we consider each component @xmath311 of layer @xmath54 as a variable in its own right and thus obtain a causal structure between @xmath312 variables .",
    "assuming that no component @xmath311 is influenced by components of the same layer , @xmath310 must be of the form @xmath313 moreover , if we assume that every @xmath314 is only influenced by some of the variables in the previous layer , the conditional further simplifies into @xmath315 where @xmath316 denote the values of @xmath317 , i.e. , the parents of @xmath314 ( fig .",
    "[ layers ] ) .    two layers in the causal chain .",
    "if the components are only influenced by horizontally adjacent ones from the layer above , the markov condition further simplifies the forward time conditional . ]",
    "hence , the fine - structure of the causal graph imposes constraints on @xmath310 that are not imposed by the coarse - grained structure .",
    "assume we are given data from the above time series , but it is not known whether the true causal structure reads @xmath318 or the one in ( [ timeorder ] ) . when resolving the vectors in their components , we have to reject the latter hypothesis because the independences @xmath319 would violate faithfulness . from the coarse - grained perspective , we can only reject the causal hypothesis by imposing an appropriate simplicity principle of conditionals .",
    "then we conclude that ( [ timeorder ] ) is more likely to be true because the markov kernels are simpler because they satisfy eq .",
    "( [ sim ] ) .",
    "finding further useful simplicity constraints has to be left to the future .",
    "we have : @xmath320 hence @xmath321 intuitively , it is quite evident that the functions @xmath322 are linearly independent for generic @xmath182 because @xmath323 contains linear terms in @xmath23 , @xmath324 is a polynomial of degree two in @xmath23 , @xmath325 contains @xmath23 and an expression with an exponential function in the denominator , @xmath325 contains only the exponential expression in the denominator .",
    "we can thus find points @xmath275 such that the row vectors @xmath326 are linearly independent . instead of proving this directly ( which would involve derivatives of the logarithms of marginals )",
    ", we use the following indirect argument : choose @xmath327 values @xmath328 and consider the rows @xmath329 \\quad j=1,\\dots,4\\,.\\ ] ] consider the projection of @xmath330 onto the quotient space @xmath331 and represent the images of the vectors ( [ 5dim ] ) by @xmath332 \\quad j=1,\\dots,4\\,.\\ ] ] check that these @xmath274 vectors are linearly independent ( which can fortunately be done without computing the derivatives of @xmath333 ) , hence the row vectors ( [ 5dim ] ) are independent , too .",
    "we can thus select @xmath274 values @xmath275 from @xmath334 such that the rows @xmath335 are independent .",
    "we have numerically checked this for @xmath336 .",
    "the coefficients of @xmath288 read : @xmath337 the vectors are linearly independent for the points @xmath338 for @xmath339 with @xmath340 .      introducing the function @xmath341 with @xmath342 we have @xmath343 and thus @xmath344 with @xmath345",
    "since the functions @xmath346 and @xmath347 are linearly independent for generic @xmath348 , we can obviously find values @xmath349 such that @xmath350 and @xmath351 are linearly independent .",
    "thanks to bastian steudel and jonas peters for several comments on an earlier version .",
    "y.  kano and s.  shimizu .",
    "causal inference using nonnormality . in _ proceedings of the international symposium on science of modeling , the 30th anniversary of the information criterion _ , pages 261270 , tokyo , japan , 2003 .",
    "s.  shimizu , a.  hyvrinen , y.  kano , and p.  o. hoyer .",
    "discovery of non - gaussian linear causal models using ica . in _ proceedings of the 21st conference on uncertainty in artificial intelligence _ , pages 526533 , edinburgh , uk , 2005 .",
    "p.  hoyer , d.  janzing , j.  mooij , j.  peters , and b  schlkopf .",
    "nonlinear causal discovery with additive noise models . in d.",
    "koller , d.  schuurmans , y.  bengio , and l.  bottou , editors , _ proceedings of the conference neural information processing systems ( nips ) 2008 _ , vancouver , canada , 2009 . mit press .",
    "http://books.nips.cc/papers/files/nips21/nips2008_0266.pdf .",
    "j.  mooij , d.  janzing , j.  peters , and b.  schlkopf .",
    "regression by dependence minimization and its application to causal inference . in _ proceedings of the international conference on machine learning , montreal _ , 2009 . to appear .",
    "d.  janzing , j.  peters , j.  mooij , and b.  schlkopf .",
    "identifying latent confounders using additive noise models . in _ proceedings of the conference uncertainty in artificial intelligence _ , montreal , 2009 .",
    "k.  zhang and a.  hyvrinen .",
    "distinguishing cause and effect using non - linear acyclic models . in _ to appear in : proceedings of the nips 2008 workshop `` causality : objectives and assessment '' _ , 2009 .",
    "/ coa08_zhang_hyvarinen_dcfeu/.                      x.  sun and d.  janzing . .",
    "in _ proceedings of the european symposium on artificial neural networks 2007 _ , pages 441446 , bruges , belgium , 2007 .",
    "a.  kolmogorov .",
    "three approaches to the quantitative definition of information .",
    ", 1(1):17 , 1965 ."
  ],
  "abstract_text": [
    "<S> we propose a method to infer causal structures containing both discrete and continuous variables . the idea is to select causal hypotheses for which the conditional density of every variable , given its causes , </S>",
    "<S> becomes smooth . </S>",
    "<S> we define a family of smooth densities and conditional densities by second order exponential models , i.e. , by maximizing conditional entropy subject to first and second statistical moments . </S>",
    "<S> if some of the variables take only values in proper subsets of @xmath0 , these conditionals can induce different families of joint distributions even for markov - equivalent graphs .    </S>",
    "<S> we consider the case of one binary and one real - valued variable where the method can distinguish between cause and effect . </S>",
    "<S> using this example , we describe that sometimes a causal hypothesis must be rejected because @xmath1 and @xmath2 share algorithmic information ( which is untypical if they are chosen independently ) . this way , our method is in the same spirit as faithfulness - based causal inference because it also rejects non - generic mutual adjustments among dag - parameters . </S>"
  ]
}