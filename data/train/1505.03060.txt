{
  "article_text": [
    "in recent years the concept of big data has attracted the general interest since valuable information can be extracted from the analysis of large volumes of data . even if there is not a reference size , big data generally refers to those data sets whose size requires hardware and software tools beyond the common machines capabilities .",
    "a crucial aspect of big data computing is concurrency , since data must be processed in parallel by distributing the computation across multiple machines . to facilitate this process",
    ", specific application frameworks are becoming widespread as they autonomously manage the parallelism and distribution of the operations , requiring the user to only define the processing of small amounts of data . in this work",
    "we focus on a couple of algorithmic schemes that lie under the major big data application frameworks , that is _ mapreduce _ and the _ bulk synchronous parallel _ model .",
    "mapreduce is a model inspired by _",
    "map _ and _ reduce _ functions from the functional programming , through which many real world tasks can be expressed ( e.g. , sorting and searching tasks @xcite ) .",
    "the bulk synchronous parallel ( bsp ) model has recently gained great interest because of its ability to process graphs of enormous size , such as social or location graphs , by deeply exploiting the work distribution on several machines ( @xcite ) .",
    "these two algorithmic schemes are abstract enough to be implemented on any platform ; there are indeed several implementations based on the most popular languages .",
    "we just mention here the major frameworks : google s mapreduce  @xcite , apache hadoop and apache spark for mapreduce , and google s pregel  @xcite and apache giraph for bsp .",
    "this work aims at studying how specific implementations of mapreduce and bsp take advantage of the platform on which they are implemented .",
    "in particular , we address the crucial core of the programming paradigm , that is the concurrency model that shapes the platform design .",
    "we then focus on the _ actor model _ and the _ shared memory model _",
    ", in order to assess how they affect the development of big data applications and their performance .",
    "to concretely instantiate the shared memory model , we consider the x10 platform , which is specifically designed for scale - out distributed programming with a partitioned global address space . as for the actor model , we consider the scala language and the akka cluster library .",
    "both platforms allow the development of applications that run on the java virtual machine ( jvm ) , which thus provides a common basis to compare the applications and to bring out the differences that specifically depend on each model .",
    "even if established big data application frameworks are available both for x10 and scala technologies ( e.g. , @xcite and @xcite ) , we implement from scratch the mapreduce and bsp frameworks in a lightweight but effective way , so to better value and compare the distinctive features of the two underlying programming models .",
    "for the same reason it is also necessary to identify some tractable problem whose significance can be appreciated ( @xmath0 ) at the abstract level , when comparing concurrency models , ( @xmath1 ) at the structural level , when comparing the programming styles , and ( @xmath2 ) at the experimental level , when comparing performances .",
    "we then apply the mapreduce scheme to the sorting of a one - dimensional distributed array spanning over a set of nodes , and we instantiate bsp on the exploration of a distributed sparse graph . both problems are implemented twice : as an x10 application and as an akka application . in the mapreduce projects we focus on remote parallelism , that is we let each node just host sequential computation , while in the bsp projects we study both local and remote parallelization opportunities .    both the code style comparison and the experimental results attest that x10 excels with _ inter - node parallelism _ , distinctive of mapreduce , while akka shines on _ intra - node concurrency _",
    "better scaling to the higher local concurrency degree required by bsp .",
    "[ [ structure - of - the - paper ] ] structure of the paper + + + + + + + + + + + + + + + + + + + + + +    in section  [ sec : platforms ] we overview the main characteristics of the two platforms under examination . in section  [ sec : algomr - bsp ]",
    "we illustrate the mapreduce and bsp algorithmic schemes together with their instantiation on the problems of sorting a distributed array and exploring a sparse graph . in section  [ sec : implementations ] we report on the implementation of the two problems both in x10 and akka , going into a comparison of the resulting code styles and drawing some insight about the impact of the different concurrency models . in section  [ sec",
    ": experiments ] we provide the details of the experimental comparison and we put forward our assessment of the impact of the two concurrency models in the big data analytics scenario .",
    "finally , we draw our conclusions in section  [ sec : conclusions ] pointing to future research directions .",
    "x10@xcite is an open - source object - oriented programming language designed for productive high performance programming of parallel and multi - core computer systems .",
    "it is designed around a partitioned global address space ( pgas ) : the computation is spread over a set of _ places _ , each of which holds data and hosts asynchronous",
    "_ activities _ that operate on local data .",
    "objects residing in one place may contain references ( ` globalref`s ) to objects residing in other places .",
    "however , x10 enforces a strong locality property : an object s mutable state can not be accessed through a remote reference to that object .",
    "therefore the language provides the at(p)s construct that permits the current activity to change its place of execution to p , execute s at place p and return , leaving behind activities that may have been spawned during the execution of s. the values of variables used in s but defined outside s are serialized , transmitted to p , and de - serialized to reconstruct a binding environment in which s is executed .",
    "activities represent lightweight threads in x10 and can be created and started locally with the async s statement .",
    "dually , the finish s statement executes s and waits for the termination of every activity spawned by s either locally or at a remote place .",
    "given the underlying shared - memory model , atomic execution is provided by the ` atomic s ` construct , which guarantees that a single local activity at a time can enter its atomic block .",
    "finally , x10 supports global data - structures spanning multiple places .",
    "for instance , as described in section  [ sec : implementations ] , we will take advantage of the ` distarray ` object , which represents a global reference to a distributed array and provides useful methods that simplify the centralized control of its distributed elements .",
    "as far as akka is concerned , in this work we draw on the combination of the scala language and akka s actor implementation included in its standard library .",
    "actors are objects that encapsulate state and behavior ; they communicate exclusively by asynchronously exchanging messages which are placed into the recipient s mailbox .",
    "the environment takes care of the actor mailbox and guarantees that a single messages is processed at a time , hence as long as only the actor has access to its local variables , the absence of race conditions is enforced by the model . as for distribution",
    ", we rely on the akka remote and akka cluster libraries , which support referential transparency in remote actors communications , that is the developer can use the actor references ( ` actorref ` ) to send messages without having to worry about whether the recipient resides on the same node of the sender or not .",
    "there are two main types of actor references : local actor references and remote actor references .",
    "the remote references contain the information necessary to identify the actors on different nodes .",
    "the actor references may be exchanged between actors through messages or can be calculated since an actor can be identified in the system by combining the _ node address _ with the _ actor path _ in the node .",
    "finally , we mention that in order to monitor and manage the cluster status , akka cluster relies on the gossip protocol to support the elastic and resilient nature of the system .",
    "this component exchanges a bunch of implicit messages that are not directly controlled by the user , which however only cause a small overhead in actor communications .",
    "in the mapreduce algorithmic scheme , a distributed computation is specified as a ( possibly repeated ) sequence of _ map _ , _ shuffle _ and _ reduce _ steps , parametrized around a pair of functions supplied by the user . the first function , ` mapper ` , processes a ( key , value ) pair to generate an intermediate ( key , value ) pair , and the second function , ` reducer ` , merges all the values associated with the same intermediate key to output a final ( key , value ) pair .",
    "more precisely , the algorithm corresponds to the following sequence of steps , depicted in figure  [ fig : mapreducediagram ] :    * _ initialization _ step : data are loaded in the initial ( key , value ) pairs , in order to process them in the map step . *",
    "_ map _ step : each node applies the ` mapper ` function to the local ( key , value ) pairs obtaining intermediate pairs . * _ shuffle _ step : each node redistributes intermediate pairs produced by the ` mapper ` function , such that all values belonging to one key are located on the same node . *",
    "_ reduce _ step : each node now processes each group of pairs , per key , in parallel , using the ` reducer ` function . * _ sink _ step : it processes the reduce output to apply some final operation like storing the results .    as for the distribution",
    ", each node performs the _ init _ step independently , in parallel to the others .",
    "subsequently each node performs the _ map _ step on data produced by the local _ init _ step ( dashed line boxes ) . in the same way ,",
    "data processed by a _ reduce _ step",
    "are then processed by a _",
    "sink _ step on the same node ( dotted line boxes ) . as for the _ shuffle _ step ,",
    "the user defines a ` partition ` function to determine the target node depending on the output key of the _ map _ step .",
    "figure  [ fig : mapreducediagram ] shows that in each node the _ map _ and _ reduce _ steps can be locally parallelized by applying the ` mapper ` and ` reducer ` functions to different portions of local data .",
    "indeed , mapreduce can take advantage of _ locality _ of data , by processing them on or near the storage assets in order to reduce the distance over which they must be transmitted .",
    "therefore , at every step , the nodes only work on their own data portion , eliminating data races , and local computation proceeds independently of the remote one .",
    "hence the change in the number of nodes does not involve the computation , but only affects performance .",
    "while this process can often appear inefficient compared to algorithms that are more sequential , or designed to run on a single machine , mapreduce can be applied to significantly larger data - sets than `` commodity '' servers can handle ( a large server farm can use mapreduce to sort a petabyte of data in only a few hours@xcite ) . however ,",
    "when designing a mapreduce algorithm , the author needs to choose a good trade - off@xcite between the computation costs and the communication costs of the shuffle step .",
    "in particular , the partition function and the amount of data transferred after the map step can have a large impact on the performance ; indeed , in the big data scenario communication cost often dominates the computation cost .",
    "we now apply the mapreduce algorithmic scheme to the sorting of a one - dimensional distributed array spanning over a set of nodes .",
    "we assume that the cluster has a fixed number of @xmath3 nodes , that are ordered from @xmath4 to @xmath5 .",
    "we let the value of the input array to be randomly generated and we let the array to be equally distributed over the nodes so that the indexes of the array slice allocated on any node @xmath6 are greater than the indexes of the slice allocated on the node @xmath7 , as depicted in figure  [ fig : distributedarray ] .",
    "we also require the output array to be sorted , to be equally distributed over the nodes , and the elements at node @xmath6 must be greater than those at node @xmath7 , for any @xmath8 .",
    "before the mapreduce execution starts , there is a little pre - elaboration where the global maximum and global minimum of the whole array are retrieved , so to compute the @xmath9 of the values in the array .",
    "we now define the behavior of every step .",
    "we denote by ` k1 ` and ` v1 ` respectively the key and the value in input for the _ map _ step ; ` ( k2,v2 ) ` is the output pair of _ map _ and the input of the _ reduce _ step ; ` ( k3,v3 ) ` is the output of the _ reduce _ step .",
    "[ [ initialization - step ] ] initialization step + + + + + + + + + + + + + + + + + + +    a ` source ` function is performed once on each node to define the ( key , value ) pairs to be passed to the _ map _ step .",
    "more precisely , for every element @xmath0 of the local array slice , it defines the key ` k1`@xmath10 as the global position in the array and the value ` v1`@xmath10 as the essential data needed for comparing @xmath0 with other elements . in this case using `` essential data '' instead of the whole element is convenient because ` v1`@xmath10 might be too heavy to be transferred from a node to another during the intermediate steps . storing the original position in",
    "the key is essential to finally transfer the whole element once its destination has been found .",
    "[ [ map - step ] ] map step + + + + + + + +    this step applies the ` mapper ` function to each ( local ) element @xmath0 . in particular",
    ", the function computes a destination node for the element @xmath0 , storing it in the output key .",
    "let _ min _",
    "be the global minimum ; we split the range of values into @xmath3 intervals , each of size @xmath11 , and we intend to use each node @xmath6 to host values in the interval @xmath12\\ ] ] then the ` mapper ` function takes in input the pair ` ( k1`@xmath10`,v1`@xmath10 ` ) ` and produces the output pair ` ( k2`@xmath10`,v2`@xmath10 ` ) ` where ` k2`@xmath13 for some @xmath14 such that ` v1`@xmath15 , and ` v2`@xmath16`(k1`@xmath10`,v1`@xmath10 ` ) ` .",
    "[ [ shuffle - step ] ] shuffle step + + + + + + + + + + + +    given the mapper function above , the partition function used in the shuffle step is the identity function .",
    "this means that each pair ` ( k2``,v2 ) ` is sent at node ` k2 ` .",
    "therefore , since the nodes are ordered , at the end of this step we have that for any pair of nodes @xmath6 and @xmath14 , with @xmath17 , all the elements located in @xmath14 are greater than all the elements located in @xmath6 . on the other hand the array slices hosted at different nodes are not sorted and in general have different sizes .",
    "[ [ reduce - step ] ] reduce step + + + + + + + + + + +    as a consequence of the previous steps , at this stage all the pairs hosted at each node share the same key ` k2 ` .",
    "hence the ` reducer ` function is performed once for every different node to sort the local elements with the best algorithm . at the end the local slice of the array is sorted and no elements will be inserted anymore .",
    "more precisely , the output of ` reducer ` at node ` j ` is the pair ` ( k3`@xmath18`,v3`@xmath18 ` ) ` , where ` v3`@xmath18 is the sorted slice and ` k3`@xmath19 its first element .",
    "[ [ sink - step ] ] sink step + + + + + + + + +    as observed before , the array slices sorted at different nodes have different sizes . the final ` sink ` step , performed once on each node , is then necessary to equally redistribute the sorted array over the nodes .",
    "all the sorted sub - arrays will be sent to a specific node , which will take care of the elements redistribution .",
    "notice that the ` sink ` step is logically simple , but involves a huge data transfer between nodes and it is unavoidable .      compared to mapreduce",
    ", bsp is best suited for the analysis of complex , very - large - scale dynamic graphs ( billions of nodes , billions of edges ) .",
    "bsp programs consist of a sequence of iterations , called _ phases _ , and a set of logical units that communicate by means of message passing . in order to avoid confusion with cluster nodes and actors",
    ", we refer to bsp logical units as _ agents_. each phase @xmath20 is made of the following steps , depicted in figure [ fig : bspdiagram ] where dashed boxes correspond to cluster nodes :    1 .",
    "_ local computation _ :",
    "parallel agents compute a user - defined function , using their own local memory .",
    "the function specifies the behavior of a single agent @xmath21 in the phase @xmath20 : it can read messages sent to @xmath21 in phase @xmath22 , send messages to other agents that will be received at phase @xmath23 , and modify the state of @xmath21 .",
    "communication _ : messages are exchanged between all agents . 3 .   _ synchronization _ : each node waits for the others to complete their message transfer at phase @xmath20 , before starting phase @xmath23 .",
    "in addition , each phase is characterized by a set of initially _ active _ agents .",
    "these agents execute local computation , then deactivate themselves , and activate other agents by sending a message to them .",
    "job _ that executes in a bsp machine is defined by a set of agents , a subset of starting active agents , and a stop criterion .",
    "a bsp machine stops when there are no more active agents or when the stop criterion is reached even if there are yet active agents .",
    "figure [ fig : bspdiagram ] shows how each agent runs independently and in parallel to the others , both local and remote , but each agent waits for the others at the barrier synchronization , before starting the next phase .",
    "an important part of the analysis of a bsp algorithm rests on quantifying the synchronization and communication that are needed . on the other hand ,",
    "the bsp agents can be programmed by only focusing on their local memory without directly referring to remote entities .",
    "we now describe how the bsp algorithmic scheme applies to the exploration of a sparse graph .",
    "we consider a random directed graph generated with up to 3 edges for every vertex .",
    "we assume that each node of the cluster hosts @xmath24 vertexes , and we let a vertex correspond to a bsp - agent . every edge can point to a vertex in the same node or to a vertex in a different node , but we assume that the whole graph is _ simple _ , that is there are no loops and no more than one edge with the same source and destination .",
    "each vertex has a ` parent ` field and an inbox listing the received messages .",
    "the local computation step , executed at every phase by every active vertex , is so defined : the vertex checks the value of its ` parent ` field , if it has a non - null value than it deactivates itself , otherwise ( @xmath0 ) it sets its ` parent ` field with the content of the first received message , ( @xmath1 ) than it sends a message to every target vertex of any out - coming edge , ( @xmath2 ) and finally deactivates itself .",
    "every vertex that receives a message will be active in the next phase . at the beginning a start vertex is marked as active so to begin the computation . in this problem",
    "there is no stop criterion , so the execution stops when there are no more active agents .",
    "the ` parent ` field and the barrier synchronization between each phase guarantee that each vertex is visited only once , hence the output is a tree . on the other hand , the resulting tree does not contain the vertexes that have not been visited since they were not reachable from the start vertex .",
    "in this section we comparatively discuss our implementation of the distributed array sorting and the sparse graph exploration problems , both in x10 and scala+akka cluster .",
    "first of all , we observe that even if established big data application frameworks are available for both technologies ( e.g. , mr3 for x10  @xcite and spark for scala  @xcite ) , we decided to implement from scratch the mapreduce and bsp frameworks in a lightweight but effective way , so to better value and compare the distinctive features of the two underlying programming models .",
    "we assumed for simplicity that the cluster has a fixed number of nodes , and if a node fails we let the whole computation fail . although akka is elastic and resilient by nature , and also x10 supports a resilient mode  @xcite , we think that resiliency is orthogonal w.r.t . our goals , and we postpone the comparison of a resilient version of our applications to future work . we also observe that to get best performance with x10 one should use native x10 , which compiles to c++ .",
    "however we are not interested in absolute performances , we aim instead at drawing some insight about the impact of the concurrency model on the application performances .",
    "hence we rely on managed x10 , which compiles to java , so to better compare with the scala+akka technology .",
    "a first important remark is that , besides the different concurrency models , the two programming platforms under examination significantly differ in their structure . as a consequence , even the programming styles that the developer must adopt , substantially differ .",
    "we illustrate below these differences , postponing to section  [ sec : experiments ] the experimental comparison .",
    "[ [ project - structure ] ] project structure + + + + + + + + + + + + + + + + +    each one of the four programs is divided in two distinct parts : ( @xmath0 ) the _ engine _ , consisting of the modules that implement the algorithmic scheme ( mapreduce or bsp ) parametrized w.r.t .",
    "the job to be executed , and ( @xmath1 ) the _ job _ instance ( distributed sorting , graph exploration ) , to be submitted to the engine .",
    "this modular structure entails genericity and reusability , and allows a more effective comparison between the two frameworks .",
    "the projects based on akka are formed by a larger number of _ modules _ than those in x10 .",
    "this because akka entails a more fragmented structure , in which each actor embodies a specific functionality , i.e. a certain role in the project .",
    "there are actors representing worker nodes , distinguished actors acting as control nodes , and a number of actors implementing the aggregator pattern to coordinate interactions .",
    "instead , x10 _ views the cluster at an higher abstraction level _ than akka , being able to describe in the same _ module _",
    "( i.e. block of code ) more computation steps which are usually separated in the most popular programming languages .",
    "this is especially possible with the language constructs distinctive of x10 : ` at ` , ` async ` , and ` finish ` . in particular ,",
    "the ` at ` construct considerably abstracts and simplifies the distribution of the computation on different nodes .",
    "this allows to have a _ direct view of the distribution _ to be obtained , which promotes code readability .",
    "moreover , the joint use of these three constructs is very effective in retrieving a value from all nodes or , in general , in the centralized management of the global computation ( e.g. , in the main classes of the mapreduce and bsp engines ) .",
    "[ [ main - control - flow - and - program - launch ] ] main control flow and program launch + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the difference in the structure of the two projects is reflected on the main control flow of the programs . in x10",
    "the main control flow is rendered as a control loop that asynchronously spawns code to be executed in every place , while in akka the control flow is spread on several modules taking advantage of the aggregator pattern .",
    "even the procedure to launch the program execution reflects the different nature of the two platforms .",
    "starting an x10 execution environment is not so different from starting any other mono - machine program .",
    "basically it requires that an ssh connection is available between all the machines , then a start - up script , provided by the framework , activates all the nodes .",
    "the script basically requires only two parameters : the number of _ places _ to be activated , and an ip address list of the available machines .",
    "if the number of places is greater than the number of ip addresses , then more than one place will be activated on some machines .",
    "this start mode allows to easily execute the program in batch mode , which will be required by our tests , as we shall see in the next section .",
    "akka cluster has instead a _ dynamic and decentralized view of the cluster _",
    ", which makes the startup more complex . there is more than one way to boot the whole system , however , in order to start the tests in batch mode , a script has been developed to have a centralized boot of the system .",
    "the script requires an `` ` ip : port ` '' list in order to know where to place the _ akka nodes _ , and an ssh connection available between all the machines .",
    "the script connects to the machines and boots up every single node .",
    "[ [ data - structures ] ] data structures + + + + + + + + + + + + + + +    being distributed by nature , x10 has some built - in useful distributed data structures . in this case , ` distarray ` represents a generic multidimensional array distributed over some places , and provides a centralized control of the array .",
    "for instance , the class has the method ` localindices ` that returns a subset of array s indexes accessible from the place where it has been called ; remind that all the other indexes can not be accessed from that place . to illustrate",
    ", the following code retrieves the maximum value of the whole distributed array . in the second line , the ` finish ` keyword waits for the termination of all the asynchronous activities into the inner block . in the third line ,",
    "an asynchronous activity is started in every place .",
    "then , in each place , the maximum is retrieved among the local values and is returned to the asynchronous call . at last , when the ` finish ` is satisfied , the ` maxreducer ` is evaluated and the maximum value is stored to the ` max ` value .",
    ".... val max =        finish(reducible.maxreducer[long](long.min_value ) ) {         for(p in place.places ( ) ) at(p ) async {            var localmax : long = long.min_value ;            for ( i in originarray.localindices ( ) )              if ( originarray(i ) > localmax )                  localmax = originarray(i ) ;            offer localmax ;         }   } ; ....    this piece of code also clearly shows how the control flow can be managed in x10 within few lines .",
    "akka design states that every node is independent and there must not be a single - point of failure , therefore there is no distributed data structure spanning over the nodes , but it is required to independently manage individual slices on every node . in the case of the distributed array ,",
    "we let ` distarraynodeactor ` hold , in every node , a slice of the distributed array and manage all the operations on it . as to retrieve the global maximum / minimum of the whole array , a ` minmaxaggregator ` actor is instantiated , so that it asks to the worker nodes the maximum / minimum of their local slice .",
    "then the actor collects the responses and extracts the global maximum / minimum .    as for the distributed graph processed by bsp",
    ", each node holds its local vertexes in an array of size @xmath24 , and each vertex holds in turn an array of three remote references to other vertexes , that represent the edges of the graph .",
    "[ [ the - cost - of - communication ] ] the cost of communication + + + + + + + + + + + + + + + + + + + + + + + + +    in section  [ subsec : bsp ] we have seen that the second step of the bsp algorithm involves message transfer between all agents .",
    "this communication step is accomplished in x10 by storing the list of received messages directly into the ( possibly remote ) recipient . since x10 is a language with a shared memory model , and the insertion of messages in this list is allowed to all the active agents at the current phase , it is necessary to control the insertion of messages in the list with an ` atomic ` block .",
    "although the ` atomic ` block is an heavy construct ( @xcite ) , in this case its use is appropriate , given the enormous number of possible senders .",
    "other solutions , such as a memory slot for each possible incoming message , are inapplicable .",
    "we observe that the shuffle and sink steps of mapreduce do not suffer from the same problem since in their case the possibility of predicting the target destination of the involved communications allows to reserve a reasonable amount of memory slots to be filled without resorting to ` atomic ` .    in the akka - based implementation",
    ", the bsp - inbox of an agent is implemented with an actor , ` messagereceiver ` , to which is delegated the function of receiving messages .",
    "the code of ` messagereceiver ` is reported below : collects in a single block all messages received in a given phase ( 5th line ) to be returned to the agent in the _ next _ phase ( 8th line ) :    .... class messagereceiver[s ] extends actor {    val inc = array(arraybuffer.empty[any ] , arraybuffer.empty[any ] )    def receive = {      case message(phase , x ) = > inc(phase%2 ) + = x        sender ( ) ! msgreceived        case getinbox(phase ) = >        sender ( ) !",
    "inc(phase%2).toarray        inc(phase%2).clear ( )   } } ....    as a final remark about akka s communication model , we observe that akka does not impose constraints on the size of the messages exchanged between the actors , that is , the messages within the same actor system can have any size .",
    "however , the connections between the cluster nodes are based on akka remote that uses the udp protocol , which in turn does not support arbitrary large datagrams .",
    "therefore , large messages such as those exchanged in the sink step of the mapreduce projects , have to be chunked , and then re - compacted , by the programmer .",
    "[ [ local - and - remote - parallelism ] ] local and remote parallelism + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in the implementations of mapreduce we decided to focus on remote parallelism , hence each node just hosts sequential computation .",
    "more precisely , all the local executions of the ` mapper ` functions ( one for each local array element ) run sequentially , and the ` reducer ` function is performed once for each node .",
    "each step of the algorithm is then executed in parallel at the level of cluster nodes .    on the other hand ,",
    "in the bsp implementations we studied both local and remote parallelization opportunities .",
    "in particular , observe that the local computation involved in the first step of each bsp phase can be parallelized ( see figure  [ fig : bspdiagram ] ) .",
    "indeed , in our akka implementation , each agent executes in parallel ; remind that we let @xmath24 agents run on each node . on the other hand ,",
    "when @xmath24 grows , x10 does not properly scale - up : as we shall see in the next section , x10 executions with a large number of agents per node ( @xmath25 ) give rise to a runtime warning saying that they are running too many threads , and execution does not terminate .",
    "we will more precisely discuss this issue in section  [ sec : experiments ] , we just observe here that the problem comes form the usage of the atomic construct , whose implementation blocks both the lightweight activity , i.e. , the logical task , and the underlying worker thread ( @xcite ) .",
    "we thus developed two x10 versions of bsp : x10p , where local agents are executed in parallel , and x10s , where agents in the same node are executed sequentially . anyway , notice that the atomic block described above is necessary even in the case where the local agents are sequentially run , since local agent runs in parallel to agents in other places .",
    "we conclude this section by summarizing the comments above into a more general picture .",
    "our detailed description of the four applications illustrates the very different programming styles involved by x10 and scala .",
    "such a difference reflects the fact the design principles of these languages significantly diverge , despite being both very expressive .",
    "as far as x10 is concerned , even if functional constructs are available and the ` async ` construct greatly simplifies asynchronous execution , the underlying shared - memory model and the centralized control of the distributed computation bring about an imperative programming style with critical accesses to shared memory . in this sense x10",
    "is predisposed to a more synchronous programming , where concurrency involve ( large ) sets of activities running _ in parallel _ with few coordination requests .",
    "accordingly , we will see in the next section that x10 shines on the mapreduce application , which essentially consists of sequences of steps with _",
    "inter - node parallelism_.    in scala , sticking to the actor model is a choice , since shared variables are still available .",
    "for instance , using the _ futures _ requires the operations scheduled at the future completion not to modify the internal state of the actor so to prevent interferences .",
    "however , the functional flavour of the language promotes the definition of well - defined behaviours to be delegated and composed by means of actors and futures .",
    "such a disaggregation suits a dynamic and decentralized view of the cluster , which fosters the distributed execution of asynchronous operations boosting the parallelism .",
    "accordingly , we will see that akka s asynchronous programming style shines on the scale - up , that is in presence of _ intra - node concurrency _",
    ", where a large number of _ concurrent _ activities can be effectively coordinated without the constraints and the limitations of synchronization constructs .",
    "in this section we provide the details of our experimental comparison , and we discuss the execution performances of the projects under consideration .    to develop the four projects we used the following tools : x10 language ( v. 2.5.1 ) and x10 development tool ide ( v. 2.5.1 ) for x10 projects , and scala language ( v. 2.11.4 ) , akka - actor ( v. 2.3.8 ) , akka - remote ( v. 2.3.8 ) , akka - cluster ( v. 2.3.8 ) , intellij idea ide ( v. 14.0.2 ) and scala build tool ( v. 0.3.7 ) for akka projects . to perform the tests we used the cluster of the department of mathematics of the university of padova . the table below lists the machines we used , each one with the following minimum requirements : 8 cores , 16 gb ram , infiniband connection , and jvm 1.6 ( openjdk ) .",
    "@xmath26    the access to the cluster is regulated by the torque resource manager queuing system , which defines the framework for the aggregate use . to use the cluster",
    "it is then necessary to submit to the queuing system an executable that specifies the resource requirements in terms of the number of required machines and the number of required cores for each machine ; then torque starts the program execution by assigning an ip address to each required core , depending on the current load of the system .",
    "when more than one core per machine has been requested , the same ip address will be assigned multiple times .    as anticipated in section  [ sec : implementations ] , the x10 platform provides a script which , given the list of ip addresses , launches the executable on each machine and autonomously establishes the connections between the nodes ( i.e. x10 places ) .",
    "akka instead requires a manual script that computes the various `` ` ip : port ` '' combinations and starts each node separately .",
    "this works well , given the relatively small size we achieved ; for larger clusters ( 200 , 1000 , or more nodes ) , we would have to load the nodes interactively , and adapt the configuration of the gossip protocol .    in the case of x10 ,",
    "a place then corresponds to a jvm , and similarly , an akka node corresponds to a single jvm .",
    "[ [ description - of - the - experiments ] ] description of the experiments + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given the random generation of the problems and the intrinsic nondeterminism , the reported results reflect the average time of ten executions for each instance . the small variability revealed by the execution times of the same instance is an indication of the reliability of the obtained results .    to measure the performance of programs , we first considered varying the problem size , then varying the number of nodes in the cluster . from now on ,",
    "when we consider the number of nodes in the cluster , we will use the notation @xmath27x@xmath28 , where @xmath27 is the number of physical machines used , and @xmath28 is the number of nodes on that machine .",
    "this to highlight that part of the communication between nodes can take place within the same machine and does not pass through a real network , which is clearly slower .",
    "it is important to observe that , besides the cluster size , in the performance analysis it is crucial to consider the single machine load .",
    "for instance , a cluster 8x4 is very different from a cluster 4x8 : they have the same number of nodes , but with a different distribution between the machines , leading to different performances ( as illustrated by the plots in the following subsection ) .",
    "more precisely , since most of the machines have eight cores , a machine hosting eight nodes , such as those in the 4x8 cluster , is somehow `` saturated '' . instead",
    ", when just four nodes run on a eight - core machine , the corresponding four jvms enjoy more powerful resources .",
    "this key observation highlights the fact that in a performance comparison it is not enough to consider just the number of nodes in the cluster , but also the available resources guaranteed by the machines hosting the nodes .",
    "( 0,0 )  coordinate ( x axis mid ) ( 5,0 ) ; ( 0,0 )  coordinate ( y axis mid ) ( 0,5 ) ;    ( 1,1pt )  ( 1,-3pt ) node[anchor = north ] @xmath29 ; ( 2,1pt )  ( 2,-3pt ) node[anchor = north ] @xmath30 ; ( 3,1pt )  ( 3,-3pt ) node[anchor = north ] @xmath31 ; ( 4,1pt )  ( 4,-3pt ) node[anchor = north ] @xmath32 ; ( 5,2pt )  ( 5,-2pt ) ; in 0,0.5, ... ,5 ( 1pt , ) ",
    "( -3pt , ) node[anchor = east ] ;    at ( x axis mid ) array size ; at ( y axis mid ) time [ s ] ;    plot[mark= * ] file mr_arrayvar_akka_10x3.data ; plot[mark = triangle * ] file mr_arrayvar_x10_10x3.data ;    ( 0,0 ) ",
    "plot[mark = triangle * ] ( 0.2,0 )  ( 0.4,0 ) node[right , black]x10 ;    ( 0,0 )  plot[mark= * ] ( 0.2,0 )  ( 0.4,0 ) node[right , black]akka ;    [ [ variation - of - the - array - size ] ] variation of the array size + + + + + + + + + + + + + + + + + + + + + + + + + + +    figure [ fig : mrarrayvar ] shows the execution times of akka and x10 on the mapreduce implementation in a cluster where the number of nodes is fixed ( 10x3 ) , while the size of the array varies from @xmath29 to @xmath32 integers .",
    "what you notice is that akka is a bit slower than x10 in all the considered cases .",
    "remind that in our implementation the local computation is sequential , hence the program is locally synchronous .",
    "we then think that the difference in the performances comes from akka s overhead due to asynchronous message exchange .",
    "( 0,0 )  coordinate ( x axis",
    "mid ) ( 11,0 ) ; ( 0,0 )  coordinate ( y axis mid ) ( 0,8 ) ;    ( 1,1pt ) ",
    "( 1,-3pt ) node[anchor = north ] 4x4 ; ( 2,1pt )  ( 2,-3pt ) node[anchor = north ] 5x4 ; ( 3,1pt )  ( 3,-3pt ) node[anchor = north ] 6x4 ; ( 4,1pt )  ( 4,-3pt ) node[anchor = north ] 7x4 ; ( 5,1pt )  ( 5,-3pt ) node[anchor = north ] 8x4 ;    ( 6,1pt ) ",
    "( 6,-3pt ) node[anchor = north ] 4x8 ; ( 7,1pt ) ",
    "( 7,-3pt ) node[anchor = north ] 5x8 ; ( 8,1pt )  ( 8,-3pt ) node[anchor = north ] 6x8 ; ( 9,1pt )  ( 9,-3pt ) node[anchor = north ] 7x8 ; ( 10,1pt )  ( 10,-3pt ) node[anchor = north ] 8x8 ; ( 11,2pt )  ( 11,-2pt ) ; in 0,1, ... ,8 ( 1pt , )  ( -3pt , ) node[anchor = east ] ;    at ( x axis mid ) cluster size ; at ( y axis mid ) time [ s ] ;    plot[mark= * ] file mr_clustervar_akka_1000000_4cores.data ; plot[mark= * ] file mr_clustervar_akka_1000000_8cores.data ;    plot[mark = triangle * ] file mr_clustervar_x10_1000000_4cores.data ; plot[mark = triangle * ] file mr_clustervar_x10_1000000_8cores.data ;    ( 0,0 ) ",
    "plot[mark = triangle * ] ( 0.35,0 )  ( 0.7,0 ) node[right , black]x10 ;    ( 0,0 )  plot[mark= * ] ( 0.35,0 ) ",
    "( 0.7,0 ) node[right , black]akka ;    [ [ variation - of - the - cluster - size ] ] variation of the cluster size + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the plot in figure [ fig : mr - exp ] shows the comparison made by keeping constant the array size ( set to @xmath32 elements ) , while varying the number of nodes in the cluster .",
    "also in this case akka is a bit slower , in a uniform manner , compared to x10 .",
    "notice in particular the central gap between the execution times in a 8x4 and a 4x8 cluster : both programs have better performances on `` non - saturated '' machines .",
    "more generally , what emerges is that both languages very well succeed in managing the change in the number of nodes , but the divergent directions of the segments might suggest that x10 scales - out better than akka for larger clusters . however , the considerations made above about the saturation of the machines indicate that no proper scale - out considerations can be done without further tests .",
    "for instance , the divergence of akka might as well be explained again by the overhead of the asynchronous management of messages , aggravated by the machine saturation .",
    "c    ( 0,0 )  coordinate ( x axis mid ) ( 7,0 ) ; ( 0,0 )  coordinate ( y axis mid ) ( 0,80 ) ;    ( 1,1pt )  ( 1,-3pt ) node[anchor = north ] @xmath33 ; ( 2,1pt ) ",
    "( 2,-3pt ) node[anchor = north ] @xmath34 ; ( 3,1pt ) ",
    "( 3,-3pt ) node[anchor = north ] @xmath35 ; ( 4,1pt ) ",
    "( 4,-3pt ) node[anchor = north ] @xmath36 ; ( 5,1pt ) ",
    "( 5,-3pt ) node[anchor = north ] @xmath37 ; ( 6,1pt )  ( 6,-3pt ) node[anchor = north ] @xmath38 ; ( 7,2pt ) ",
    "( 7,-2pt ) ;    in 0,10, ... ,80 ( 1pt , ) ",
    "( -3pt , ) node[anchor = east ] ;    at ( x axis mid ) agents per node ; at ( y axis mid ) time [ s ] ;    plot[mark= * ] file bsp_agentvar_akka_8x4.data ; plot[mark = triangle * ] file bsp_agentvar_x10p_8x4.data ; plot[mark = square * ] file bsp_agentvar_x10s_8x4.data ;    ( 5,21.271 ) ",
    "plot[mark = triagle * , mark options = fill = white ] ( 5,21.271 ) node[above]x ;    ( 0,0 ) ",
    "plot[mark = square * ] ( 0.25,0 )  ( 0.5,0 ) node[right , black]x10s ;    ( 0,0 ) ",
    "plot[mark = triangle * ] ( 0.25,0 )  ( 0.5,0 ) node[right , black]x10p ;    ( 0,0 )  plot[mark= * ] ( 0.25,0 )  ( 0.5,0 )",
    "node[right , black]akka ;     +   +    [ cols=\"^ \" , ]     in order to fully test akka s scalability , we also tested the bsp program by assuming a graph with @xmath30 agents per node and varying the cluster size .",
    "as illustrated by the execution times reported in the table in figure  [ fig : bsp - exp ] , akka s implementation scales with the size of the cluster and the corresponding increase of the number of total agents .",
    "this then means that it scales well with the proliferation of messages exchanged in the system .",
    "the table also shows that the execution time increase of x10s is sharper , on the other hand , no x10p execution terminated .",
    "in the light of what emerged from the performance comparison , we have that x10 excels with _ inter - node parallelism _ , while akka shines on _ intra - node concurrency_. it is important to remind here that fair performance tuning x10 applications requires different strategies ( see  @xcite ) , such that avoiding the ` atomic ` construct and relying on native x10 which compiles to c++ .",
    "nevertheless , the main explanation of these results essentially comes from the underlying programming models , as observed in section  [ sec : implementations ] , which are indeed the main target of our investigation .",
    "x10 s centralized control appears to be best suited to inter - node parallelism , hence to algorithms , like mapreduce , characterized by sequences of independent steps with rare and predictable synchronizations that minimize the need to protect the access to shared memory .",
    "instead , akka makes the most of his asynchronous model in presence of a high degree of concurrent activities , that can be efficiently coordinated with no need of blocking mechanisms .",
    "let s consider this point more in depth : observe that both akka and x10 distinguish logical tasks ( respectively the actors in akka and the activities in x10 ) from the worker threads , so to allow the execution of many tasks with a few threads .",
    "however , in the two programming models such a distinction has a very different impact .",
    "indeed , consider how the x10 runtime realizes the decoupling between tasks and worker threads : if a thread is about to block because of a pending synchronization such as an ` atomic ` block , before suspending it starts a new worker thread with the aim of preserving the parallelism .",
    "however the system has a limit ( set by default to @xmath39 threads ) , after which the user is warned with the already seen _",
    "`` too many threads '' _ message . instead in the asynchronous model of actors the tasks never block , hence worker threads can be mainly reused making the best of the initial pool of threads .",
    "moreover , @xcite illustrates how the functional model provides opportunities to further improve the decoupling between logical tasks and worker threads by means of the continuation - passing style . anyway , despite the non - blocking implementation strategies , synchronous primitives like ` atomic ` , are also _ logical blockers _ that establish a conceptual limit to the scale - up : if a shared variable undergoes many synchronized accesses , any task that needs to access must wait his turn , so the global computation hardly advance .",
    "this is indeed what happens in the inbox of the bsp - agents , where a huge number of senders is ready to deliver a message but few tend to do it .",
    "in this paper we compared concrete implementations of the mapreduce and bsp algorithmic schemes using the x10 and akka cluster platforms .",
    "rather than addressing performance issues , we aimed at testing the actor model and the shared memory model at work on a big data analytics scenario .",
    "the experimental tests assess the expected conceptual scale - up limit entailed by the blocking constructs required to safely access shared memory .",
    "moreover , both the code style comparison and the experimental results , attest that the centralized and imperative flavour of x10 stands out in the mapreduce implementation , while akka s actors foster the distributed execution of asynchronous tasks better scaling to the higher concurrency degree required by bsp .    as for the scale - out",
    ", we showed that both platforms can properly handle executions that involve up to 64 nodes .",
    "however , a more faithful analysis requires additional tests with a larger cluster and carefully tuning the resources available in each machine ; this kind of analysis is the subject of future work .",
    "we also plan to extend our investigation to assess the impact of the programming models on fault tolerant executions .",
    "resiliency is indeed a critical issue of big data applications and both x10 and akka provide support for resilient executions . in this case",
    "however it is necessary to suitably structure the problems in order to evaluate the fault tolerance potential of the two platforms .",
    "p.  charles , c.  grothoff , v.  saraswat , c.  donawa , a.  kielstra , k.  ebcioglu , c.  von praun , and v.  sarkar .",
    "x10 : an object - oriented approach to non - uniform cluster computing . in _ proc . of oopsla05",
    "_ , pages 519538 , 2005 .",
    "d.  cunningham , d.  grove , b.herta , a.iyengar , k.  kawachiya , h.  murata , v.  saraswat , m.  takeuchi , and o.tardieu .",
    "resilient x10 : efficient failure - aware programming . in _ proc . of ppopp 14 _ , pages 67 - 80 , 2014 ."
  ],
  "abstract_text": [
    "<S> this work aims at analyzing how two different concurrency models , namely the shared memory model and the actor model , can influence the development of applications that manage huge masses of data , distinctive of big data applications . </S>",
    "<S> the paper compares the two models by analyzing a couple of concrete projects based on the mapreduce and bulk synchronous parallel algorithmic schemes . </S>",
    "<S> both projects are doubly implemented on two concrete platforms : akka cluster and managed x10 . </S>",
    "<S> the result is both a conceptual comparison of models in the big data analytics scenario , and an experimental analysis based on concrete executions on a cluster platform . </S>"
  ]
}