{
  "article_text": [
    "markov chain monte carlo ( mcmc ) methods are a remarkably robust way to sample from complex probability distributions .",
    "metropolis - hastings ( mh ) sampling @xcite stands out as an important benchmark .",
    "an appealing feature of mh sampling is the simple physical picture which underlies the general method . roughly speaking",
    ", the idea is that the thermal fluctuations of a particle moving in an energy landscape provides a conceptually elegant way to sample from a target distribution .",
    "but there are also potential drawbacks to mcmc methods . for example",
    ", the speed of convergence to the correct posterior is often unknown since a sampler can become trapped in a metastable equilibrium for a long period of time .",
    "once a sampler becomes trapped , a large free energy barrier can obstruct an accurate determination of the distribution . from this perspective",
    "it is therefore natural to ask whether further inspiration from physics can lead to new examples of samplers .",
    "now , although the physics of point particles underlies much of our modern understanding of natural phenomena , it has proven fruitful to consider objects such as strings and branes with finite extent in @xmath1 spatial dimensions ( a string being a case of a @xmath2-brane ) .",
    "one of the main features of branes is that the number of spatial dimensions strongly affects how a localized perturbation propagates across its worldvolume .",
    "viewing a brane as a collective of point particles that interact with one another ( see fig .",
    "[ parstringbrane ] ) , this suggests applications to questions in statistical inference @xcite .    motivated by these physical considerations , our aim in this work will be to study generalizations of the mh algorithm for such extended objects .",
    "for an ensemble of @xmath3 parallel mh samplers of a distribution @xmath4 , we can alternatively view this as a single particle sampling from @xmath3 variables @xmath5 with density : @xmath6 where the proposal kernel is simply : @xmath7 to realize mcmc with strings and branes , we keep the same target @xmath8 , but we change the proposal kernel by interpreting the index @xmath9 on @xmath10 as specifying the location of a statistical agent in a network . depending on the connectivity of this network ,",
    "an agent may interact with several neighboring agents @xmath11 , so we introduce a proposal kernel : @xmath12    in the above , the connectivity of the extended object specifies its overall topology .",
    "for example , in the case of a string , i.e. , a one - dimensional extended object , the neighbors of @xmath13 are @xmath14 , @xmath13 , and @xmath15 .",
    "[ parstringbrane ] depicts the time evolution of parallel mh samplers compared with the suburban sampler .",
    "however , there are potentially many consistent ways to connect together the inferences of statistical agents . from the perspective of physics , this amounts to a notion of distance / proximity between nearest neighbors in a brane .",
    "a physically well - motivated way to eliminate this arbitrary feature is to allow the notion of proximity itself to _ fluctuate _ , and for the brane to split and join .",
    "we view mcmc with extended strings and branes as a novel class of ensemble samplers . by correlating the inferences of nearest neighbors",
    ", we can expect there to be some impact on performance .",
    "for example , the degree of connectivity impacts the mixing rate for obtaining independent samples .",
    "another important feature is that because we are dealing with an extended object , different statistical agents may become localized in different high density regions .",
    "provided the connectivity with neighbors is sufficiently low , coupling these agents then has the potential to provide a more accurate global characterization of a target distribution .",
    "conversely , connecting too many agents together may cause the entire collective to suffer from groupthink  in the sense of @xcite .",
    "in particular , we shall present evidence that the optimal connectivity for a network of agents on a grid arranged as a hypercubic lattice with some percolation ( i.e. , we allow for broken links ) occurs at a critical effective dimension : @xmath16 where @xmath17 is the average number of neighbors .",
    "to summarize : with too few friends one drifts into oblivion , but with too many friends one becomes a boring conformist .    turning our discussion around",
    ", one can view this paper as providing a concrete way to study the physics of branes with a strongly fluctuating worldvolume , that is , the non - perturbative regime of string theory .",
    "the appendices provide some additional details ( see also @xcite ) .",
    "the suburban code is available at ` https://gitlab.com/suburban/suburban ` .",
    "one of the main ideas we shall develop in this paper is mcmc methods for extended objects . in an mcmc",
    "algorithm we produce a sequence of timesteps  @xmath18 which can be viewed as the motion of a point particle exploring a target space @xmath19 . more formally , this sequence of points defines the worldline  for a particle , and consequently a map : @xmath20 for an extended object with @xmath21 spatial directions , we get a map from a worldvolume  to the target : @xmath22 the special cases @xmath23 and @xmath24 respectively denote a point particle and string .",
    "the general physical intuition is that minus the log of the target distribution @xmath25 defines a potential energy , and minus the log of the markov chain transition probability @xmath26 is a kinetic energy .",
    "the key point is that statistical field theory in @xmath27 euclidean dimensions strongly depends on the number of dimensions .",
    "for example ,",
    "the two - point function for a free gaussian field @xmath28 with @xmath29 is : @xmath30 where in the case of @xmath31 , the two - point function is @xmath32 . for @xmath33 ,",
    "a random field explores its surroundings at large @xmath9 , but the overall variance decreases as @xmath34 . for @xmath35 , however , `` groupthink '' sets in and the ensemble less quickly explores its surroundings .",
    "this suggests a special role for stringlike objects @xcite .    in a theory of quantum gravity",
    "( such as string theory ) it is also physically natural to let the proximity of nearest neighbors fluctuate .",
    "so , we introduce an ensemble of random graphs @xmath36 . for example , for a @xmath21-dimensional toroidal hypercubic lattice ,",
    "introduce @xmath37 lattice sites along a spatial direction so that @xmath38 is the total number of agents . for a hypercubic lattice in @xmath21 dimensions ,",
    "we define the ensemble of random graphs for a brane @xmath39 as one in which we have a random shuffling of the agents , and in which a given link in a @xmath21-dimensional hypercubic lattice is active with probability @xmath40 .",
    "we can also consider more general ensembles of adjacency matrices .",
    "for example , the erds - renyi ensemble @xmath41 has an edge between any two nodes with probability @xmath40 .",
    "we also introduce the notion of an effective dimension which depends on the average number of neighbors : @xmath42 which need not be an integer .",
    "we now present the suburban algorithm . for ease of exposition , we shall present the case of a 1d target . the generalization to a @xmath43-dimensional target is straightforward , and we can take mh within a gibbs sampler , or a sampler with joint variables in which all @xmath43 dimensions update simultaneously .    to avoid overloading the notation",
    ", we shall write @xmath44 for the current state of the grid . instead of directly sampling from @xmath4 , we introduce multiple copies of the target and sample from the joint distribution @xmath45 using mh sampling with proposal kernel @xmath46 , where @xmath47 denotes the adjacency matrix .",
    "if the system is in a state @xmath48 , with adjacency matrix @xmath49 , we pick a new state according to the mh  update rule with a proposal kernel which depends on both these inputs .",
    "the mh acceptance probability is : @xmath50 this leads us to algorithm [ alg : suburban ] .",
    "randomly initialize @xmath51 and @xmath52 * for * @xmath53 * do *  @xmath54 sample from @xmath55  accept with probability @xmath56  * if * accept = true * then *  @xmath57  * else *  @xmath58  @xmath59 draw from @xmath36 @xmath60    some of these steps can be parallelized whilst retaining detailed balance . for example we could pick a coloring of a graph and then perform an update for all nodes of a particular color whilst holding fixed the rest .",
    "we can also stochastically evolve the adjacency matrices .",
    "now , having collected a sequence of values @xmath60 , we can interpret this as @xmath61 samples of the original distribution @xmath4 .",
    "as standard for mcmc  methods , we can then calculate quantities of interest such as the mean : @xmath62 as well as higher order moments .",
    "let us discuss the reason we expect our sampler to converge to the correct posterior distribution .",
    "first note that although we are modifying the proposal kernel at each time step ( i.e. , by introducing a different adjacency matrix @xmath63 ) , this modification is independent of the current state of the system .",
    "so , it can not impact the eventual posterior distribution we obtain .",
    "second , we observe that since we are just performing a specific kind of mh  sampling routine for the distribution @xmath8 , we expect to converge to the correct posterior distribution .",
    "but , since the variables @xmath5 are all independent , this is tantamount to having also sampled multiple times from @xmath4 .",
    "the caveat is that we need the sampler to actually wander around during its random walk ; @xmath33 is typically necessary to prevent `` groupthink . ''      to accommodate a flexible framework for prototyping , we have implemented the suburban algorithm in the probabilistic programming language ` dimple ` @xcite .    for practical purposes we take a fairly large burn - in cut , discarding the first @xmath64 of samples from a run .",
    "we always perform gibbs sampling over the @xmath3 agents .",
    "for mh within gibbs sampling over a @xmath43-dimensional target , we thus get a gibbs schedule with @xmath65 updates for each time step . for a joint sampler ,",
    "the gibbs schedule consists of just @xmath3 updates .    the specific choice of @xmath66 for eqn .",
    "( [ stringkernel ] ) is motivated by having a free gaussian field on a fluctuating graph topology : @xmath67 with : @xmath68 for @xmath69 a neighbor of @xmath9 on the graph defined by the adjacency matrix .",
    "additionally , we set the hyperparameters for the kernel as : @xmath70 that is , we take an adaptive value for @xmath71 specified by the number of nearest neighbors joined to @xmath10 .",
    "this condition leads to a well - behaved continuum limit on a fully connected hypercubic lattice .",
    "in most cases , we consider mh within gibbs sampling , though we also consider the case where joint variables are sampled , that is , pure mh . rather than perform error analysis within a single long mcmc run",
    ", we opt to take multiple independent trials of each mcmc run in which we vary the hyperparameters of the sampler such as the overall topology and average degree of connectivity of the sampler . though this leads to less efficient statistical estimators , it has the virtue of allowing us to easily compare the performance of different algorithms , i.e. , as we vary the continuous and discrete hyperparameters of the suburban algorithm .",
    "we take @xmath72 to compare different grid topologies .",
    "we have also compared performance with parallel slice ( within gibbs ) samplers @xcite to ensure that our performance is comparable to other benchmarks .    to gauge accuracy ,",
    "we collect the inferred mean and covariance matrix .",
    "we then compute the distance to the true values : @xmath73    we also collect performance metrics from the mcmc  runs such as the rejection rate . a typical rule of thumb",
    "is that for targets with no large free energy barriers , a rejection rate of somewhere between @xmath74 is acceptable ( see e.g. , @xcite ) .",
    "we also collect the integrated auto - correlation time for the energy  of the distribution : @xmath75 by collecting the values @xmath76 . for @xmath77 ,",
    "we evaluate : @xmath78{c}\\frac{1}{n}\\underset{t=1}{\\overset{n - k}{{\\displaystyle\\sum } } } \\left (   v^{(t)}-\\overline{v}\\right )   \\left (   v^{(t+k)}-\\overline{v}\\right )   \\text { \\ \\ \\ \\ } k\\geq0\\\\ \\frac{1}{n}\\underset{t=1}{\\overset{n+k}{{\\displaystyle\\sum } } } \\left (   v^{(t)}-\\overline{v}\\right )   \\left (   v^{(t - k)}-\\overline{v}\\right )   \\text { \\ \\ \\ \\ } k<0 \\end{array } \\right\\ }   , \\ ] ] and then extract the integrated auto - correlation time : @xmath79 we also refer to this as the decay timeas it reflects how quickly the chain mixes .",
    "for this observable we include all samples ( no burn - in ) .    to extract numerical estimates we perform @xmath80 independent trials with random initialization for each agent on @xmath81^{d}$ ] .",
    "we present all plots with a @xmath82-sigma level standard error around the mean value from these trials . in practice",
    ", we typically find acceptable error bars for @xmath83 and @xmath84 trials .",
    "perhaps the single most important feature of the suburban algorithm is that it correlates the inferences drawn by nearest neighbors on a grid .",
    "quite strikingly , we find that the effective dimension rather than the overall topology of the grid plays the dominant role in the performance of the algorithm .",
    "we illustrate this point with a class of target distribution examples which we refer to as symmetric mixtures.for a fixed choice of @xmath43 the number of target space dimensions , we introduce a mixture model consisting of @xmath85 equal weight components , each of which is a normal distribution with means and covariance matrices : @xmath86 where @xmath87 , @xmath88 is a kronecker delta and @xmath89 is the @xmath90 identity matrix .",
    "we find qualitatively similar behavior for @xmath91 and @xmath92 , so we give the plots for the @xmath91 runs with @xmath93 and @xmath94 . in this case , we have four equally weighted components of our mixture model , and there is a free energy barrier separating these centers .    as a first class of tests , we consider sampling with different topology grids for @xmath95 timesteps , with each grid consisting of @xmath96 agents . for each choice of hyperparameter , we perform @xmath83 trials .",
    "we have scanned the value of @xmath97 in steps of factors of @xmath98 , and find that performance is better around @xmath99 , so we focus on this case . in all cases , we find that the values of the observables @xmath100 and @xmath101 are comparable and small , indicating reasonable convergence .    there is , however , a marked difference in the mixing rate as we vary the split / join probability for the ensemble . in fig .",
    "[ twodcubetaus ] we display the values of @xmath102 as a function of the effective dimension dictated by the split / join rate for a given grid topology .",
    "quite striking is the universal behavior of the samplers as a function of the effective dimension near @xmath103 , i.e. , for connectivity similar to that of a string . near @xmath104 , i.e. , for parallel mh  samplers , we also see much slower mixing rates .",
    "once we go beyond @xmath105 , the overall performance of the sampler suffers .",
    "twodcubetaus.pdf    because of this universal behavior , we shall primarily focus on representative behavior  as obtained from a @xmath106 grid topology . in fig .",
    "[ timetwodcubemets ] we show various performance metrics as a function of the total number of samples . by inspection ,",
    "stringlike samplers tend to fare the best .",
    "timetwodcubemets.pdf      as another example , we consider mixture models in which there is a landscape of local maxima and minima .",
    "a priori , a compromise will need to be struck between wandering freelyand moving more slowly around individual components of the mixture model .",
    "we use a variant on the same random mixture model considered in @xcite , focussing on the case of @xmath107 gaussian mixtures with relative weights randomly drawn from the uniform distribution on @xmath108 $ ] . compared with @xcite ,",
    "we take the parameters ` stdmu ` @xmath109 , ` stdsig ` @xmath110 .",
    "we do this primarily to achieve convergence for the samplers in a reasonable amount of time .",
    "the different mixture models are obtained by setting the random seed in the code of @xcite to different values ( see fig . [ rand40plot ] ) .",
    "rand40plot.pdf    by design , we have chosen our domain for the random variables so that the brane tension @xmath111 should give a roughly comparable class of length scales for the target distribution .",
    "since the overall topology of the grid does not appear to affect the qualitative behavior of the sampler , we have also focussed on the case of a @xmath106 grid topology with shuffling and percolation . for each choice of hyperparameter , we perform @xmath83 trials .",
    "rand40b0p01mets.pdf    the random seed @xmath112 model gives representative behavior .",
    "the stringlike sampler is faster and more accurate than the parallel mh sampler , while a @xmath113 sampler suffers from `` groupthink , '' settling in an incorrect metastable configuration .",
    "it is also of interest to consider distributions concentrated on a lower - dimensional subspace such as the two - dimensional banana distribution : @xmath114 this distribution is often used as a performance test of various optimization algorithms .",
    "we focus on a suburban sampler with joint variables , taking different grid topologies for the statistical agents and then perform a sweep over different values of the hyperparemeters @xmath97 and @xmath115 , performing @xmath83 trials for each case .",
    "time2dbananamets.pdf    we present the representative case of a @xmath106 grid , and further specialize to the tuned case of @xmath99 . fig .",
    "[ time2dbananamets ] shows that parallel samplers ( @xmath116 ) and collectives with groupthink @xmath117 both fare worse than a stringlike sampler .",
    "the extended nature of the suburban sampler also suggests that for target distributions with various disconnected deep pockets ,  different pieces of the ensemble can wander over to different regions .",
    "we consider a mixture model with two gaussian components : @xmath118 with : @xmath119 where we vary @xmath120 and hold fixed @xmath121 .",
    "we take @xmath122 samples with @xmath96 agents on a @xmath106 grid with @xmath111 , performing @xmath84 independent trials .",
    "since we use mh within gibbs , we do not find much decrease in performance in comparing the @xmath91 and @xmath92 free energy barrier tests .",
    "[ 2dfreebarrierplotmet ] shows that parallel samplers fare worse than the extended objects .",
    "the @xmath123 runs are sometimes more accurate , but mix slower than for @xmath124 . after thinning samples",
    ", the former runs will be less accurate .",
    "jjh thanks d. krohn for collaboration at an early stage .",
    "we thank j.a .",
    "barandes , c. barber , c. freer , m. freytsis , j.j .",
    "heckman sr .",
    ", a. murugan , p. oreto , r. yang , and j. yedidia for helpful discussions .",
    "the work of jjh is supported by nsf career grant phy-1452037 .",
    "jjh also acknowledges support from the bahnson fund as well as the r.  j. reynolds industries , inc .",
    "junior faculty development award at unc chapel hill .",
    "99 n.  metropolis , a.  w. rosenbluth , m.  n. rosenbluth , a.  h. teller , and e.  teller , `` equation of state calculations by fast computing machines , '' _ j. chem . phys . _",
    "* 21 * ( 1953 ) 10871092 .",
    "w.  k. hastings , `` monte carlo sampling methods using markov chains and their applications , '' _ biometrika _ * 57 * no .  1 ,",
    "( 1970 ) 97109 .",
    "j.  j. heckman , `` statistical inference and string theory , '' _ int .",
    "* a30 * no .  26 , ( 2015 ) 1550160 , arxiv:1305.3621 [ hep - th ] . r.  h. swendsen and j .- s",
    ". wang , `` replica monte carlo simulation of spin - glasses , '' _ phys .",
    "_ * 57 * no .",
    "21 , ( 1986 ) 26072609 .",
    "c.  j. geyer , `` markov chain monte carlo maximum likelihood , '' in _ computing science and statistics : proceedings of the 23rd symposium on the interface _ ,",
    "e.  m. keramidas , ed . , pp .",
    "interface foundation , 1991 .",
    "w.  r. gilks , g.  o. roberts , and e.  i. george , `` adaptive direction sampling , '' _ journal of the royal statistical society .",
    "series d ( the statistician ) _ * 43 * no .  1 , ( 1997 ) 179189 .",
    "d.  j. earl and m.  w. deem , `` parallel tempering : theory , applications , and new perspectives , '' _ phys .",
    "phys . _ * 7 * ( 2005 ) 39103916 .",
    "r.  m. neal , `` mcmc using ensembles of states for problems with fast and slow variables such as gaussian process regression , '' arxiv:1101.0387 [ stat ] .",
    "j.  goodman and j.  weare , `` ensemble samplers with affine invariance , '' _ comm . in appl .",
    "math . and comp .",
    "sci . _ * 5 * no .  1 , ( 2010 ) 6580 .",
    "j.  j.  heckman , j.  g.  bernstein , and b.  vigoda , `` mcmc with strings and branes : the suburban algorithm ( extended version ) , '' arxiv:1605.05334 [ physics.comp-ph ] .",
    "s.  hershey , j.  bernstein , b.  bradley , a.  schweitzer , n.  stein , t.  weber , and b.  vigoda , `` accelerating inference : towards a full language , compiler and hardware stack , '' arxiv:1212.2991 [ cs.se ] .",
    "r.  a. neal , `` slice sampling , '' _ the ann . of stat . _",
    "* 31 * no .  3 , ( 2003 ) 705767 .",
    "a.  gelman , w.  r. gilks , and g.  o. roberts , `` weak convergence and optimal scaling of random walk metropolis algorithms , '' _ ann .",
    "* 7 * no .  1 , ( 1997 ) 110120 .",
    "y.  chen , m.  welling , and a.  j. smola , `` super - samples from kernel herding , '' arxiv:1203.3472 [ cs.lg ] .",
    "m.  e. peskin and d.  v. schroeder , _ an introduction to quantum field theory_. , reading , usa , 1995 .",
    "a.  wipf , `` statistical approach to quantum field theory , '' _ lect .",
    "notes phys . _",
    "* 864 * , ( 2013 ) .",
    "v.  balasubramanian , `` statistical inference , occam s razor and statistical mechanics on the space of probability distributions , '' _ neural comp . _ * 9(2 ) * ( 1997 ) 349368 , arxiv : cond - mat/9601030 .",
    "p.  di  francesco , p.  mathieu , and d.  senechal , _ conformal field theory_. , new york , usa , 1997 .",
    "in this appendix we give a path integral formulation for mcmc with extended objects . for additional background on path integrals in statistical field theory , see @xcite . in what follows",
    ", we denote the random variable as @xmath125 with outcome @xmath126 on a target space @xmath19 with measure @xmath127 .",
    "we consider sampling from a probability density @xmath4 . in accord with physical intuition",
    ", we view @xmath128 as a potential energy .    in general ,",
    "our aim is to discover the structure of @xmath4 by using some sampling algorithm to produce a sequence of values @xmath129 .",
    "a quantity of interest is the expected value of @xmath4 with respect to a given probability distribution of paths .",
    "this helps in telling us the relative speed of convergence and the mixing rate . to study this",
    ", it is helpful to evaluate the expectation value of the quantity:@xmath130 with respect to a given path generated by our sampler .    in more general terms",
    ", the reason to be interested in this expectation value comes from the statistical mechanical interpretation of statistical inference @xcite : there is a competition between staying in high likelihood regions ( minimizing the potential ) , and exploring more of the distribution ( maximizing entropy ) .",
    "the tradeoff between the two is neatly captured by the path integral formalism : it tells us about a particle moving in a potential @xmath131 , and subject to a thermal background , as specified by the choice of probability measure over possible paths .",
    "indeed , we will view this probability measure as defining a `` kinetic energy '' in the sense that at each time step , we apply a random kick to the trajectory of the particle , as dictated by its contact with a thermal reservoir .    along these lines",
    ", if we have an mcmc  sampler with transition probabilities @xmath132 , marginalizing over the intermediate values yields the expected value of line ( [ productpot ] ) : @xmath133 \\left ( \\underset{i=0}{\\overset{n-1}{{\\displaystyle\\prod } } } t(x^{(i)}\\rightarrow x^{(i+1)})e^{-v(x^{(i+1)})}\\right)\\ ] ] where we have introduced the measure factor @xmath134 = dx^{(1 ) } ... dx^{(n)}$ ] .",
    "we would like to interpret @xmath131 as the potential energy and @xmath135 as a kinetic energy : @xmath136    we now observe that our expectation value has the form of a well - known object in physics : @xmath137 e^{- \\underset{t}{\\sum } l^{(e)}[x^{(t)}]},\\ ] ] a path integral ! here",
    "the euclidean signature lagrangian is : @xmath138=k+v.\\text { } \\ ] ] since we shall also be taking the number of timesteps to be very large , we make the riemann sum approximation and introduce the rescaled lagrangian density:@xmath139 so that we can write our process as : @xmath140 e^{-\\int dt\\mathcal{l}^{(e)}[x(t ) ] }   , \\ ] ] where by abuse of notation , we use the same variable @xmath141 to reference both the discretized timestep as well as its continuum counterpart .    to give further justification for this terminology , consider now the specific case of the metropolis - hastings algorithm . in this case",
    ", we have a proposal kernel @xmath142 , and acceptance probability:@xmath143 the total transmission probability is then given by a sum of two terms .",
    "one is given by @xmath144 , i.e. , we accept the new sample .",
    "we also sometimes reject the sample , i.e. , we keep the same value as before:@xmath145 where @xmath146 is the dirac delta function , and we have introduced an averaged rejection rate : @xmath147    to gain further insight , we now approximate the mixture model @xmath148 by a normal distribution @xmath149 such that @xmath150 . hence , , matching the first and second moments to @xmath151 requires @xmath152 , with @xmath153 the average acceptance rate . ]",
    "@xmath154\\simeq\\alpha_{\\text{eff}}\\left (   x^{(t+1)}-x^{(t)}\\right ) ^{2}+v(x^{(t)})+ ... ,\\ ] ] where here , the ...  denotes additional correction terms which are typically suppressed by powers of @xmath155",
    ".    our plan will be to assume a kinetic term with quadratic time derivatives , but a general potential .",
    "the overall strength of the kinetic term will depend on details such as the average acceptance rate . as the acceptance rate decreases , @xmath156 increases and the sampled values all concentrate together .",
    "we now turn to the generalization of the above concepts for strings and branes , i.e. , extended objects .",
    "introduce @xmath3 copies of the original distribution , and consider the related joint distribution:@xmath157 if we keep the proposal kernel unchanged , we can simply describe the evolution of @xmath3 independent point particles exploring an enlarged target space:@xmath158 if we also view the individual statistical agents on the worldvolume as indistinguishable , we can also consider quotienting by the symmetric group on @xmath3 letters , @xmath159:@xmath160    of course , we are also free to consider a more general proposal kernel in which we correlate these values . viewed in this way , an extended object is a single point particle , but on an enlarged target space .",
    "the precise way in which we correlate entries across a grid will in turn dictate the type of extended object .    indeed , much of the path integral formalism carries over unchanged .",
    "the only difference is that now , we must also keep track of the spatial extent of our object .",
    "so , we again introduce a potential energy @xmath161 and a kinetic energy @xmath162 : @xmath136 and a euclidean signature lagrangian density : @xmath163=k+v,\\ ] ] where here , @xmath164 indexes locations on the extended object , and the subscript @xmath47 makes implicit reference to the adjacency on the graph . in a similar notation , the expected value is now : @xmath165\\text { } e^{-\\underset{t}{\\sum}\\underset{\\sigma}{\\sum } l^{(e)}[x(t,\\sigma_{a})]}.\\ ] ] since we shall also be taking the number of time steps and agents to be large , we again make the riemann sum approximation : @xmath166",
    "so that : @xmath167 e^ { -\\int dtd\\sigma_{a}\\text { } \\mathcal{l}^{(e)}[x(t,\\sigma_{a } ) ] } , \\ ] ] in the obvious notation .",
    "so far , we have held fixed a particular adjacency matrix .",
    "this is somewhat arbitrary , and physical considerations suggest a natural generalization where we sum over a statistical ensemble of choices .",
    "one can loosely refer to this splitting and joining of connectivity as incorporating gravity  into the dynamics of the extended object , because it can change the notion of which statistical agents are nearest neighbors .",
    "along these lines , we incorporate an ensemble @xmath36 of possible adjacency matrices , with some prescribed probability to draw a given adjacency matrix .",
    "the topology of an extended object dictates a choice of statistical ensemble @xmath36 .",
    "since we evolve forward in discretized time steps , we can in principle have a sequence of such matrices @xmath168 , one for each timestep . for each draw of an adjacency matrix , the notion of nearest neighbor will change , which we denote by writing @xmath169 , that is , we make implicit reference to the connectivity of nearest neighbors . marginalizing over the choice of adjacency matrix , we get:@xmath170[da]\\text { } e^{-\\underset{t}{\\sum}\\underset{\\sigma } { \\sum}l^{(e)}[x(t,\\sigma_{a(t)})]},\\ ] ] where now the integral involves summing over multiple ensembles : the spatial and temporal values with measure factor @xmath171 , as well as the choice of a random matrix from the ensemble @xmath172 ( one such integral for each timestep ) . at a very general level , one can view the adjacency matrix as adding additional auxiliary random variables to the process .",
    "so in this sense , it is simply part of the definition of the proposal kernel .",
    "following some of the general considerations outlined in reference @xcite , we now discuss the extent to which the extended nature of such objects plays a role in statistical inference and in particular mcmc .",
    "to keep our discussion from becoming overly general , we specialize to the case of a hypercubic lattice of agents in @xmath21 spatial dimensions arranged on a torus , and we denote a location on the grid by a @xmath21-component vector @xmath9 .",
    "we can allow for the possibility of a fluctuating worldvolume by making the crude substitution @xmath173 .",
    "consider the gaussian proposal kernel of line ( [ colonelklink ] ) . in a large lattice ,",
    "we approximate the finite differences in one of the @xmath21 spatial directions by derivatives of continuous functions . expanding in this limit ,",
    "various cross - terms cancel and we get for the proposal kernel : @xmath174 where @xmath175 denotes a finite difference in the @xmath176 spatial component of the @xmath21-dimensional lattice .    just as in the case of the point particle , the transition rate defines a kinetic energy quadratic in derivatives ( to leading order ) , with an effective strength dictated by the overall acceptance rate .",
    "one of the things we would most like to understand is the extent to which an extended object can explore the hills and valleys of @xmath161 .",
    "we perform a perturbative analysis , at first viewing @xmath161 as a small correction to the lagrangian . starting from some fixed position @xmath177 ,",
    "consider the expansion of @xmath161 around this point:@xmath178 each of the derivatives of @xmath131 reveals another characteristic feature length of @xmath131 .",
    "these feature lengths are specified by the values of the moments for the distribution @xmath4 .    when @xmath179 , there is a well - known behavior of correlation functions which is given by eqn .",
    "( [ twopoint ] ) . in @xmath27 dimensions exhibits the requisite power law behavior .",
    "] there is thus a rather sharp change in the inferential powers of an extended object above and below @xmath180 .    to understand the impact of a non - trivial potential , we introduce the notion of a `` scaling dimension '' for @xmath181 and its derivatives .",
    "this is a well - known notion , see @xcite for a review .",
    "just as we assign a notion of proximity in space and time to agents on a grid , we can also ask how rescaling all distances on the grid via : @xmath182 impacts the structure of our continuum theory lagrangian .",
    "the key point is that provided @xmath183 and @xmath3 have been taken sufficiently large , or alternatively we take @xmath184 sufficiently large , we do not expect there to be any impact on the physical interpretation .",
    "unpacking this statement naturally leads us to the notion of a scaling dimension for @xmath185 itself . observe that rescaling the number of samples and number of agents in line ( [ rescaler ] ) can be interpreted equivalently as holding fixed @xmath183 and @xmath3 , but rescaling @xmath141 and @xmath9 : @xmath186 now , for our kinetic term to remain invariant , we need to _ also _ rescale @xmath181 : @xmath187 the exponent @xmath188 is often referred to as the `` scaling dimension '' for @xmath126 obtained from `` naive dimensional analysis '' or nda .",
    "it is `` naive '' in the sense that when the potential @xmath189 and we have strong coupling , the notion of a scaling dimension may only emerge at sufficiently long distance scales .",
    "note that because we are uniformly rescaling the spatial and temporal pieces of the grid , we get the same answer for the scaling dimension if we consider spatial derivatives along the grid .",
    "this assumption can also be relaxed in more general physical systems .",
    "to illustrate , invariance of the free field action requires : @xmath190 we can also consider the behavior of a perturbation of the form @xmath191",
    ". applying our nda analysis prescription , we see that under a rescaling , the contribution such a term makes to the action is : @xmath192 so terms of the form @xmath193 for @xmath194 die off as we take @xmath195 , i.e. , @xmath196 .",
    "additionally , we see that when @xmath33 , we can in principle expect more general contributions of the form @xmath197 . for additional discussion on the interpretation of such contributions ,",
    "see reference @xcite .",
    "consider next possible perturbations to the potential energy .",
    "each successive interaction term in the potential is of the form @xmath198 , with scaling dimension @xmath199 .",
    "so , for @xmath200 , all higher order terms can impact the long distance behavior of the correlation functions , while for @xmath201 , the most relevant term is bounded above , and the global structure of the potential will be missed .",
    "rand40b0p01tails.pdf    time2dbananatails.pdf",
    "in figs . [ rand40b0p01tails ] and [ time2dbananatails ] we display some tests of how well a sampler collects `` rare events , '' i.e. , tail statistics . after taking a burn - in cut with @xmath202 remaining samples ,",
    "we compute the number of counts in the @xmath203 region , the @xmath204 region , the @xmath205 region , and events which fall outside the @xmath206 region . for each such region , we compute the difference between the inferred and true counts and return the fraction : @xmath207    landrand40slicevsmh.pdf    2dfreebarrierslicevssub.pdf",
    "figs . [ landrand40slicevsmh ] and [ 2dfreebarrierplotmetnew ]",
    "compare the performance of a suburban sampler with @xmath106 grid topology ( with @xmath208 and @xmath111 ) with parallel slice within gibbs sampling .",
    "we use the default implementation in ` dimple ` so that for a 1d target distribution the initial size of the @xmath126-axis width is an interval of length one containing @xmath177 , and the maximum number of doublings is @xmath98 .",
    "a direct comparison with suburban is subtle because in slice sampling the halting of the `` stepping out '' and `` stepping in '' loops is not fixed ahead of time . in practice",
    "we find that for a fixed number of samples , slice typically makes several more queries to the target distribution compared with suburban , roughly a factor of @xmath209 . for large free energy barriers ,",
    "it is also sometimes helpful to enlarge the initialization width from @xmath2 to @xmath210 ( see fig . [ 2dfreebarrierplotmetnew ] ) ."
  ],
  "abstract_text": [
    "<S> motivated by the physics of strings and branes , we introduce a general suite of markov chain monte carlo ( mcmc ) `` suburban samplers '' ( i.e. , spread out metropolis ) . </S>",
    "<S> the suburban algorithm involves an ensemble of statistical agents connected together by a random network . </S>",
    "<S> performance of the collective in reaching a fast and accurate inference depends primarily on the average number of nearest neighbor connections . increasing the average number of neighbors above zero initially leads to an increase in performance , </S>",
    "<S> though there is a critical connectivity with effective dimension @xmath0 , above which `` groupthink '' takes over , and the performance of the sampler declines . </S>"
  ]
}