{
  "article_text": [
    "* is there a measure of complexity that is widely applicable and can distinguish moderately complex from very complex systems ?",
    "parallel depth , the topic of this paper , is a candidate for such a measure .",
    "it is formulated in the probabilistic setting of statistical physics and is defined as the length of the parallel computation needed to construct a typical state of a system .",
    "the connection between parallel depth and intuitive notions of complexity is predicated on the assumption that no system can become complex without carrying out , explicitly or implicitly , a long computation .",
    "parallel depth is a complexity measure related to computational complexity and is essentially different from complexity measures related to entropy and information . *",
    "an explorer from a parallel universe arrives in the solar system .",
    "she makes some preliminary measurements and observes that the mass , entropy and entropy production of the system are concentrated in one large , hot body at its center .",
    "the other much smaller objects in the system pale by comparison .",
    "she reports back to home base that she will focus her research on the large central body and that her study will soon be completed of what appears to be a typical system on this scale and at this time epoch in this universe .",
    "`` have you done a complexity measurement ? '' asks the team leader .",
    "`` no , i did nt think we would find anything interesting . ''",
    "`` let s do it . ''",
    "after some painstaking observations , in - depth theoretical analysis and extensive simulations , she excitedly reports her findings : the estimated complexity of this system is extremely high and it is all concentrated on the surface of one of the small , dense objects orbiting the central object .    what did she measure ?",
    "is there a quantity that distinguishes the complexity of the biosphere from everything else in the solar system ?",
    "could such a quantity be universally defined rather than formulated using concepts from specific disciplines such as biology ?",
    "the purpose of this paper is to describe one such quantity , illustrate some of its features , contrast it to some other complexity measures and argue that it does indeed formalize some of the salient intuitions about complexity .",
    "this quantity is called ` parallel , '  @xcite or simply ` ' when there is no ambiguity .",
    "is related to computational complexity .",
    "it is quite general in the sense that it exists for any systems described within the probabilistic framework of statistical physics .",
    "is the parallel time complexity of sampling system states or histories using an efficient , randomized parallel algorithm .",
    "it is a formal measure of the irreducible amount of history required to create system states from simple initial conditions and can only become large for systems with embedded computation .",
    "the notion of parallel  presented here was motivated by and is closely related to _",
    "logical depth _ introduced by bennett  @xcite .",
    "both parallel  and logical depth measure the number of computational steps that are needed to generate a state .",
    "the key difference is that the unit of measurement for parallel  is a _",
    "parallel _ computational step whereas logical depth is defined in terms of turing time , which is a _ sequential _ computational step .",
    "it turns out that parallel and sequential time emphasize rather different computational resources and have rather different properties when applied as measures of natural complexity .",
    "although parallel  is closely related to logical depth , it is not closely related to thermodynamic depth  @xcite , which is an entropy - based measure rather than a computation - based measure .",
    "` complexity ' has many scientific meanings .",
    "some of these meanings apply to specialized domains while others are rather general . for a categorized list of proposed definitions",
    "see @xcite . among the general definitions many",
    "are related to entropy and information .",
    "examples include excess entropy , effective complexity and thermodynamic depth  @xcite .",
    "many systems in statistical physics have been studied from the perspective of  @xcite .",
    "these systems include pattern formation models  @xcite , network growth models  @xcite , dynamical systems  @xcite and spin models  @xcite .",
    "these studies reveal that long range correlations and power law distributions , often considered to be markers of complex systems , do not necessarily imply substantial . on the other hand , some systems in statistical physics such as diffusion limited aggregation",
    "do appear to have considerable .",
    "diffusion limited aggregation , discussed below in sec .  [ sec : comp ] , has both long range correlations and embedded computation in the sense that its dynamics is sufficiently rich that it can be used to evaluate arbitrary logical expressions  @xcite .    in sec .",
    "[ sec : frame ] we define  and discuss its general properties .",
    "section [ sec : exen ] contrasts it to several other complexity measures via some examples .",
    "the paper concludes in sec .",
    "[ sec : conclusions ] with a discussion .",
    "is defined within the general framework of statistical physics and the theory of computation . in statistical physics systems are described in terms of probability distributions over their states .",
    "states are broadly defined here .",
    "a state may be a snapshot of the system at a single instant in time or it may be a sequence of snapshots constituting a time history .",
    "states are descriptions of a system , not the system itself , and must exist within some theoretical framework . for example , in classical mechanics , states are defined by the positions and momenta of the particles while in ecology , states might consist of numbers of individuals of each species .",
    "as these examples suggest , coarse - graining is often involved in specifying states . in chemistry",
    ", nuclei can often be described as point - like particles ignoring the hierarchy of internal degrees of freedom within the nucleus  the nucleons , quarks and gluons , strings ... in simple physical systems , a separation of time scales and energy scales often provides a rigorous justification for coarse graining .",
    "we can ignore the nuclear degrees of freedom because the energy required to excite them is much higher than molecular energy scales in chemistry so these degrees of freedom remain in their ground states .",
    "for more complex systems it is an art to choose an appropriately small set of degrees of freedom that adequately capture the observed behavior of a system .",
    "the self - organization of complex systems often suggests a natural coarse - graining .",
    "a probabilistic description is central to our formulation .",
    "the origin of randomness differs in different situations . in many cases ,",
    "it is enough to point to the loss of information in the coarse - graining .",
    "the degrees of freedom that have been ignored have some influence on the behavior of the system that causes it to deviate from strictly deterministic behavior .",
    "if the coarse - graining is done well , the effects of the ignored degrees of freedom can be treated as random perturbations . for open systems ,",
    "noise from outside the system may be a source of randomness . in any case ,  is defined for probability distributions over the states of a system .",
    "is defined for whole systems that are isolated or interacting simply with their environment .",
    "for example , the earth system is an example of a complex system that interacts with the rest of the universe in a simple way , primarily via the exchange of blackbody radiation .",
    "is not straightforwardly defined for strongly interacting components of a system even though , intuitively , these might be more or less complex . for example ,",
    "consider an agent - based model of an economic system with many complex interacting agents exchanging goods and money .",
    "it would make sense to measure the  of the entire economic system but it is not clear how to assign  meaningfully to the individual agents .",
    "in addition to statistical physics , the formulation of  requires the theory of ( classical digital ) computation .",
    "thus , the states of the system must be expressed using a finite number of bits and the probability distributions over these states should be samplable to good approximation in finite time using a digital computer .",
    "our essential task is to find a computational procedure , i.e.  a randomized algorithm , that samples the states of the system with the correct probability . by sampling , we mean that on each run of the algorithm a description of the system is produced with the correct probability .",
    "for example , markov chain monte carlo methods such as the metropolis - hastings algorithm are designed to sample specific probability distributions .",
    "the randomized algorithm makes use of random numbers and we assume that the computational device has free access to a supply of random numbers . finally , we assume that we have available a massively parallel computer , as described in more detail below .",
    "we minimize the parallel time to sample the states on this idealized parallel computer and define  as this parallel time .    _",
    "is defined as running time of the most efficient parallel algorithm for sampling the states of the system .",
    "_ alternatively , it is the minimum number of parallel logical steps needed to transform random numbers into typical system states .    describing a system in terms of the computation that simulates its states allows us to make connections to other fundamental quantities related to many proposed complexity measures . to measure",
    "we minimize parallel time .",
    "suppose instead one minimizes the amount of randomness needed to sample the distribution of states .",
    "the average number of random bits required to sample system states , using the program that minimizes this number , is roughly the entropy of the system .",
    "the intuition for this relationship is that the shannon entropy of an ensemble is well - approximated by the average algorithmic randomness of the states in the ensemble , see for example  @xcite .",
    "it is important to note that randomness and running time can not typically be simultaneously optimized ; a program with a short running time may not use randomness efficiently and vice versa . finally , if one minimizes the size of the program needed to convert random bits into typical system states , the program size is related to the complexity measure called _ effective complexity _  @xcite .",
    "since  is defined in terms of time on a parallel computer we need to briefly survey the theory of parallel computation  @xcite .",
    "the strategies for parallel computing can be quite different than for the more familiar sequential computing .",
    "a simple example is adding a list of @xmath0 numbers . on a single processor machine with random access memory",
    "the obvious method is to use a ` for loop . '",
    "a partial sum is initialized to zero and on each iteration of the loop a new summand is retrieved from memory and added to the current partial sum .",
    "the answer is produced after o@xmath1 steps , assuming each addition requires o@xmath2 steps . an idealized machine with a single processor that can carry out simple operations such as addition and multiplication , and read and write to any cell in an arbitrarily large memory all in a single time step",
    "is known as a random access machine or ram .",
    "the parallel programming method to add numbers and carry out many other computations is ` divide and conquer . '",
    "the @xmath0 numbers are divided into @xmath3 pairs and @xmath3 processors simultaneously add these pairs yielding @xmath3 partial sums on the first parallel step .",
    "on subsequent parallel steps the remaining partial sums are pairwise added .",
    "since the number of partial sums decreases by half after each parallel step , the answer is produced in o@xmath4 parallel time using @xmath0 processors .",
    "the same amount of computational work is needed in the parallel computation but it is distributed in a way that achieves an exponential speedup in running time relative to the single processor calculation .    the idealized parallel machine based on the ram is the parallel random access machine or pram .",
    "the pram has many processors equivalent to the single processor of the ram",
    ". each processor can carry out a simple computation and read and write to a global random access memory in a single time step .",
    "the processors in a pram run synchronously and all processors run the same program but may branch through the program differently and read and write to different memory cells depending on their i d s .",
    "conflicts may arise if two processors simultaneously attempt to read from or write to the same memory location on the same step .",
    "the way in which these conflicts are resolved yields more and less powerful variants of the pram .",
    "because of the assumption of unit time communication with the global memory , neither the ram nor pram can be scaled up indefinitely since they must ultimately violate either speed of light constraints for fixed component size or materials constraints on miniaturization .",
    "nonetheless , the pram is useful for understanding strategies for parallel algorithms that can be adapted for more realistic machines .",
    "since our purpose is to understand to what extent the simulation of the system can be broken into independent tasks we can ignore communication time .    the theory of parallel computational complexity is concerned with the limits to the efficiency with which problems can be solved in parallel on a pram . for an introduction to the field see ,",
    "for example  @xcite .",
    "it is formulated as a scaling theory and the basic question that is asked is how do computational resources scale with the problem size .",
    "the primary resources are parallel time and hardware .",
    "interesting results follow if the parallel time is minimized under the constraint that number of processors is restricted to grow as a power of the problem size . under this reasonable constraint , which is the assumption that is adopted here ,",
    "some problems , such as addition , are efficiently parallelizable and others are apparently not .",
    "addition , for example , requires a number of processors that grows linearly in the problem and can then be solved in logarithmic time .",
    "computational complexity theory defines broad classes of problems .",
    "the first such complexity class is * p * ; the class of problems that can be solved on a pram in polynomial time with polynomially many processors .",
    "addition of @xmath0 numbers is in * p *  but",
    "it is also within the subclass * nc *  of problems that can be solved in polylogarithmic time , i.e.  a power of the logarithm of the problem size , using polynomially many processors . problems in * nc *  are considered to be efficiently parallelizable .",
    "while it is obvious that @xmath5 , it is not clear whether the inclusion is strict .",
    "the question is whether there exist problems in * p *  that can not be solved in polylogarithmic time in parallel .",
    "such problems would be ` inherently sequential . '",
    "this question has not been settled but the consensus is that there exist problems in * p *  that are , in fact , inherently sequential .",
    "candidates for such problems are the set of * p*-complete problems .",
    "the reader is referred to  @xcite for a discussion of the theory of * p*-completeness and a compendium of * p*-complete problems . the definition of * p*-completeness and the question of whether @xmath6 closely parallels the definition of * np*-completeness and the question of whether @xmath7 .",
    "it is strongly believed that both inclusions @xmath5 and @xmath8 are , in fact , strict .",
    "suffice it to say that no polylog time parallel algorithm has been found for any * p*-complete problem .    the class of * p*-complete is defined via ` reductions '  @xcite .",
    "roughly speaking , a problem @xmath9 reduces to a problem @xmath10 if an algorithm for @xmath10 can be used as a subroutine to solve @xmath9 with only polylog additional parallel time needed .",
    "a problem is * p*-complete if all other problems in * p *  can be reduced to it .",
    "thus , if a polylog time parallel algorithm were to be found for one * p*-complete problem , then efficient parallel algorithms would exist for all problems in * p *  and the class * p *  would collapse to * nc*. it is interesting to note , however , that there exist random ensembles of * p*-complete problems and their solutions that can be sampled efficiently in parallel  @xcite illustrating the point that it is sometimes easier to simultaneously generate a problem and its solution than to first generate the problem and then solve it .",
    "the definition of parallel time sketched in the foregoing was presented in the context of a specific model of parallel computation , the pram , and thus it does not appear to be a satisfactory basis for a general definition of natural complexity .",
    "however , computational complexity theory is not tied to a specific model of computation .",
    "the broad complexity classes * nc * , * p * , and * np *   can be equivalently defined in terms of prams , turing machines or boolean circuits . for boolean circuits ,",
    "the resource that is equivalent to parallel time is circuit depth , motivating the choice of the term ` ' for the complexity measure discussed here .",
    "circuit depth is the length of the longest path from inputs to outputs in a boolean circuit .",
    "finally , the theory of descriptive complexity  @xcite shows that computational complexity theory can be formulated in terms of the logical description of computational problems without reference to computing machines at all .",
    "the quantity corresponding to parallel time in a first - order logical description of a problem is the number of quantifier alternations .",
    "the robustness of computational complexity theory makes it a suitable candidate for a general and fundamental definition of physical complexity .",
    "parallel computational complexity theory and the complexity classes defined above refer to the computational difficulty of solving problems whereas  is defined in terms of the difficulty of sampling distributions",
    ". we can illustrate this distinction in the context of an important model in statistical physics , diffusion limited aggregation ( dla ) .",
    "the dla model defines a probability distribution on patterns in two ( or more ) dimensions according to the following growth law .",
    "initially the aggregate consists of a single particle fixed at the origin of space .",
    "a second particle is initialized some distance from the origin and performs a random walk .",
    "if during its random motion it touches the initial particle it sticks to it and the aggregate now consists of two particles .",
    "if it walks too far from the origin , it is removed from the system and a new particle started .",
    "the aggregate grows when a diffusing particle touches the existing aggregate and sticks to the particle it touches .",
    "the aggregate grows , adding one particle at a time , until it consists of @xmath11 particles . a typical aggregate with @xmath12 particles is shown in fig .",
    "[ fig : dla ] . as can be seen in the figure",
    ", dla produces visually complex , fractal structures .",
    "it is important to observe that the dla growth rule is sequential in the sense that only one walking particle is in the system at a time .",
    "if many particles walk at the same time the ensemble of aggregates is very different .",
    "thus one might guess that dla is inherently sequential in the computational sense that it will require o(@xmath11 ) parallel steps to create an aggregate of @xmath11 particles and that the  of dla is o(@xmath11 ) .",
    "we can think of the process of creating the aggregate as converting random numbers first into random walk trajectories and then , using the growth rules , into the aggregate itself .",
    "thus the growth rules define a sampling algorithm for dla .",
    "we can also think of this algorithm as solving a problem .",
    "the problem is converting an input , the random numbers controlling the random walk trajectories , into an output , the aggregate .",
    "we can ask how hard that problem is to solve in parallel .",
    "it turns out that our intuition about the sequential nature of the process is confirmed in the sense that we can show that the dla problem is * p*-complete  @xcite .",
    "thus there is almost certainly no polylog time parallel algorithm for solving the problem of converting random numbers into random walks and then into aggregates of the type shown in fig .",
    "[ fig : dla ] .",
    "particles.,scaledwidth=45.0% ]    the * p*-completeness result is nice and strongly suggests that dla has considerable .",
    "unfortunately , there is a catch .",
    "is defined as the complexity of sampling dla using the _ most efficient _ parallel algorithm .",
    "the defining growth law is only one way to create dla aggregates .",
    "other methods are known and one of these has also been shown to be associated with a * p*-complete problem  @xcite . however , to prove that dla has more than polylog  we would need to consider _ all _ possible sampling problems for dla .",
    "it would be nice to have a well - developed theory of the complexity of sampling but unfortunately this does not yet exist .",
    "thus , we do not have tools available to set rigorous lower bounds on . on the other hand",
    ", it is straightforward to set an upper bound on  by actually demonstrating a parallel sampling algorithm and determining its running time .",
    "the defining growth law for dla shows that its  is no worse than linear in @xmath11 . in @xcite",
    "we improve this result slightly .    because it is defined in terms of parallel computation ,",
    "is _ maximal _",
    "maximality is a generalization of the property of intensivity in statistical mechanics .",
    "properties such as temperature , pressure and chemical potential in statistical mechanics are _ intensive _ because they are independent of system size for homogeneous equilibrium systems .",
    "such systems can be decomposed , to good approximation , into independent subsystems each of which has the same value of the intensive quantities .",
    "now consider a system that is not homogeneous but can still be decomposed into independent subsystems .",
    "if the subsystems have different s then the  of the whole is the maximum of the s of the subsystems .",
    "this property follows trivially from the independence of the subsystems and the fact that  is running time on a parallel computer .",
    "we can assign a separate set of processors to each subsystem and the  of the whole system is then determined by when the last set of processors has finished . note that if the system is homogeneous so that the subsystems are independent and identically distributed then  is also intensive .",
    "two interesting and well - studied complexity measures are excess entropy and statistical complexity @xcite .",
    "the domain to which these measures most naturally apply is to stationary time series , although generalizations to other domains are possible . for a stationary time",
    "series we may define @xmath13 as the entropy of a block of the series of length @xmath14 , @xmath15 where @xmath16 is the probability of time series @xmath17 of length @xmath14 where each element of the series is chosen from a set of symbols and @xmath18 is the expectation with respect to @xmath19 . the entropy rate @xmath20 is defined as @xmath21 this limit must converge from above and the sum of the deviations from the limit is the excess entropy . specifically , let @xmath22 and let @xmath23 .",
    "then the excess entropy @xmath24 is given by @xmath25    the statistical complexity  @xcite requires knowing the causal states of the system as defined in computational mechanics .",
    "causal states are sets of past states with statistically indistinguishable futures .",
    "the statistical complexity is the entropy of the collection of causal states .",
    "it can be shown that the excess entropy is a lower bound for the statistical complexity .    a time series that has no memory ,",
    "e.g.  a series of independent coin tosses , has no excess entropy and no statistical complexity .",
    "let @xmath26 be @xmath27 with probability @xmath28 and @xmath29 with probability @xmath28 .",
    "the block entropies are given by @xmath30 for all @xmath31 as required by the additive property of entropy so @xmath32 and the excess entropy vanishes .",
    "since the past is uncorrelated with the future , there is only one causal state consisting of all possible pasts so the statistical complexity also vanishes .",
    "since each @xmath26 is independent , one processor can be assigned to each @xmath26 .",
    "this processor simply reads a random bit and assigns it to @xmath26 .",
    "the entire series is thus created in a o(1 ) parallel steps independent of the length of the series .",
    "the depth of this random series is o(1 ) .",
    "next consider a coin toss with memory .",
    "the set of symbols is again @xmath33 .",
    "let @xmath34 with probability @xmath35 and @xmath36 with probability @xmath37 .",
    "it is straightforward to compute the block entropies to be @xmath38 $ ] and from this expression we find that the excess entropy is @xmath39.\\ ] ] although @xmath24 varies with @xmath35 , the statistical complexity is independent of @xmath35 and given by @xmath40 .",
    "the two equally likely causal states are distinguished by the value of @xmath41 since the statistical properties of the future , @xmath42 , is completely determined by @xmath41 .",
    "as advertised , the statistical complexity is here greater than the excess entropy .    to estimate the  of the coin toss with memory we seek an efficient parallel algorithm for simulating it .",
    "here is one of several possible methods .",
    "first , in parallel construct a biased sequence of bits @xmath43 where each @xmath44 is independently chosen to be @xmath45 with probability @xmath35 and @xmath29 with probability @xmath37 .",
    "the @xmath46 implement the flipping of successive coins with probability @xmath35 according to relation @xmath47 . since",
    "the @xmath46 are independent , they can be constructed in parallel time o(1 ) .",
    "next choose @xmath41 randomly .",
    "finally , let @xmath48 .",
    "as in the case of addition , the product can be carried out in logarithmic parallel time using divide and conquer .",
    "thus the  of the coin toss with memory is o(@xmath49 ) .",
    "it is interesting to note that the excess entropy of the coin toss with memory is computed directly from the probability distribution of blocks but estimating the  requires additional understanding of how one might simulate the time series .",
    "we chose one method and determined the parallel time for implementing it but did not prove that it was the best method .",
    "we have an upper bound rather than an exact answer .",
    "this open - endedness is a feature of  that we will return to in the next section .    ,",
    "excess entropy and statistical complexity are all small for the coin toss with memory even though the correlation time ( average length of blocks of like bits ) is @xmath50 and diverges as @xmath35 approaches one .",
    "for the coin toss with memory , there is a finite excess entropy that approaches @xmath40 as @xmath35 approaches either zero or one . of course , for @xmath51 or 1 , the time series is simple and deterministic . for @xmath52",
    "all bits are the same and for @xmath51 the bits alternate . the  is a constant for these deterministic time series .",
    "an interesting example of a stochastic time series with excess entropy that diverges as a power law is described in the paper in this volume by debowski  @xcite in the context of linguistics and called the santa fe process .",
    "the santa fe process produces a series of pairs @xmath53 for @xmath54 .",
    "the @xmath55 are independent , identically distributed positive integers chosen from a power law density , @xmath56 with @xmath57 and @xmath58 the appropriate normalization .",
    "the @xmath59 are not independent but are associated with @xmath55 according to @xmath60 . for each positive integer @xmath61",
    ", @xmath62 is an independent coin toss ( @xmath63 with probability @xmath28 and @xmath64 with probability @xmath28 ) .",
    "of course , we only need to know @xmath62 for the values of @xmath61 that appear in the list @xmath65 . as @xmath14 increases the number of distinct values of @xmath66 increases as @xmath67 .    for the santa fe process",
    ", the excess entropy diverges as a power of time  @xcite , @xmath68 the santa fe process has excess entropy that diverges as a power law because , as the sequence is made longer , information encoded in the set @xmath69 is slowly revealed .",
    "it is important here that the probability density for @xmath66 decays slowly so that the amount of memory ( number of distinct @xmath62 bits in the sequence ) diverges as a power law in the length of the sequence .",
    "although the excess entropy and statistical complexity of the santa fe process both diverge as a power law , the  is logarithmic .",
    "here is a way to sample the santa fe process of length @xmath14 quickly in parallel .",
    "first the sequence @xmath70 is constructed .",
    "since the @xmath55 s are independent random variables chosen from a power law distribution , the construction can be done independently with one group of processor assigned to each @xmath71 , @xmath72 .",
    "this step requires o(@xmath49 ) parallel time because the integers that need to be manipulated to generate the random @xmath55 grows as a power of @xmath14 .",
    "another group of processors generates a sequence of random bits @xmath73 .",
    "this can be done independently for each @xmath71 in o(1 ) parallel time .",
    "this sequence is not correct however because of possible repeated values of the @xmath55 .",
    "we eliminate dupicates and implicitly determine the independent random bits @xmath62 in the following two parallel steps .",
    "we use an auxiliary variable @xmath44 that is initialized to zero for all @xmath71 , @xmath74 . for all pairs @xmath75 in parallel compare @xmath55 and @xmath76 . if @xmath77 then let @xmath78 and write @xmath79 .",
    "( note that many processor may simultaneously attempt to write to @xmath80 but they will all write 1 .",
    "we have implicitly assumed the crcw common pram model where many processors may simultaneously write to the same memory element if they all agree . )",
    "this step requires o(1 ) parallel time using @xmath81 processors . after this step is complete @xmath82 if and only if either @xmath55 is distinct or @xmath71 is the least index among the set of duplicates of @xmath55 .",
    "one more parallel step is required to correctly determine the set @xmath83 .",
    "for all pairs @xmath75 in parallel if @xmath77 and if @xmath82 then @xmath84 else if @xmath85 then @xmath86 .",
    "this step also requires o(1 ) parallel time using @xmath81 processors .",
    "although we did not explicitly construct @xmath69 , after this step is complete we have for each @xmath61 for which it is defined , @xmath87 such that @xmath88 is the least index @xmath71 for which @xmath89 .",
    "since this @xmath90 was not overwritten in the above step , it is an independent random bit , as required .",
    "in addition , for all @xmath91 such that @xmath92 then @xmath93 as required .    we have demonstrated a parallel algorithm whose running time is o ( @xmath14 ) using @xmath81 processors .",
    "thus the  of the santa fe process is no greater than logarithmic in @xmath14 .",
    "the santa fe process is an example where the  is much less than the excess entropy .",
    "the santa fe process has lots of memory and long range correlations but almost no embedded computation .",
    "the contrast between  and excess entropy for the santa fe process highlights a key difference between  and entropy - based measures of complexity .",
    "entropy - based measures are sensitive to stored information and how it is distributed in a system but not whether it has been processed by a long computation .",
    "is sensitive to embedded computation and can only be large for systems that carry out computationally complex information processing .",
    "are there examples of bit sequences with  that increases as a power of the length of the sequence ?",
    "a candidate from statistical physics is the equilibrium one - dimensional ising model with long range interactions .",
    "the one - dimensional ising model with nearest neighbor interactions is equivalent to the coin toss with memory",
    ". it does not have a finite temperature phase transition and has little complexity .",
    "on the other hand , ising models with long range interactions have a finite temperature phase transition  @xcite and long range order . the only known ways to sample the states of this system are to use markov chain monte carlo methods such as the metropolis algorithm or cluster algorithms of the swendsen - wang variety  @xcite .",
    "it is believed that these algorithms , even in parallel , require polynomial running time to converge to equilibrium at critical points .",
    "the excess entropy of one - dimensional ising models with long range interactions also diverges as power law in the system length @xmath14  @xcite though apparently for quite different reasons .",
    "we saw in the previous section that  is maximal .",
    "excess entropy , by contrast , is _ sub - extensive_. in statistical mechanics , an extensive property is one that grows linearly in the number of degrees for freedom for homogeneous systems with short range correlations .",
    "a sub - extensive property grows more slowly than the number of degrees of freedom .",
    "the sub - extensivity of the excess entropy follows simply from its definition since excess entropy is that part of the entropy that remains after the entropy rate , or linearly growing part , is subtracted off .",
    "the maximal property of parallel  has interesting consequences for a system with one small but very complex component weakly interacting with a much larger environment .",
    "the maximal property insures that the  of the whole system is dominated by the complex subsystem .",
    "extensive or sub - extensive measures of complexity both suffer from the drawback that the complex subsystem will make a small contribution to the complexity of the whole .",
    "more formally , consider a system consisting of @xmath94 independent subsystems of which @xmath95 are not very complex as measured by a complexity measure while one is much more complex .",
    "let s assign a complexity @xmath96 to each of the @xmath95 less complex subsystems and @xmath97 to the single much more complex subsystem such that @xmath98 .",
    "logical depth , excess entropy and many other measures are additive for independent subsystems so that the total complexity @xmath99 would be given by @xmath100 .",
    "if @xmath95 is sufficiently large then @xmath101 and the complex system makes an insignificant contribution to the complexity of the whole .",
    "this problem is not alleviated by constructing an intensive measure by dividing by @xmath94 .",
    "on the other hand , for a maximal measure @xmath102 , independent of @xmath95 and the very complex subsystem is clearly identified in its much larger environment .",
    "the story of the explorer from a parallel universe visiting the solar system illustrates the advantage of using a complexity measure with the maximal property .",
    "suppose the explorer used an additive complexity measure instead of one that has the maximal property .",
    "for the sake of argument , let s also assume that the conditions of the previous paragraph apply : the sun can be well approximated as a large number of nearly independent subsystems such that the earth s complexity is much greater than each subsystem but much less than the sum over all the subsystems comprising the sun . the complexity of the solar system would then be dominated by the sun and would not be significantly different than the complexity of a lifeless planetary system with a similar central star .",
    "the additive complexity measure would have failed to alert the explorer to the existence of the earth s biosphere .",
    "this argument suggest that good measures of complexity should have the maximal property and should not be additive .",
    "excess entropy , statistical complexity and parallel  all assign the highest complexity to systems that lie between completely ordered and completely random .",
    "all assign greater complexity to systems with long range correlations and are able to distinguish simple , long range correlations ( e.g.  the coin toss with memory aka the nearest - neighbor one - dimensional ising model ) from more complex correlations ( e.g.  the long range one - dimensional ising model ) . for some examples such as the santa fe process , the long range correlations are complex as measured by excess entropy and statistical complexity but simple as measured by .",
    "on the other hand , there are presumably time series with  that grows linearly whereas the excess entropy is , by definition , sublinear .",
    "has been defined as the length of the shortest path on a parallel computer from random numbers to typical states ( or histories ) of a system . , like entropy",
    ", is defined for any system described within the general , probabilistic framework of statistical physics and both are functions of the probability distribution of system states .",
    "reflects some of our intuitions about complexity . like other reasonable measures of complexity , it is small for systems that are either very ordered or very disordered and tends to be largest at transitions between order and disorder .",
    "systems with considerable  are capable of computation . unlike other complexity measures ,  is maximal , that is , it is dominated by a complex subsystem even if it is part of a much larger but less complex environment .",
    "emergence and complexity are closely related concepts .",
    "emergence is both a signature of complexity and a means for reducing it .",
    "the relation between emergence and complexity is apparent even in simple examples taken from elementary physics .",
    "newton s laws , if applied blindly , equate  with physical time . in the absence of any other knowledge , we simply have to integrate newton s laws without taking shortcuts .",
    "the simulation will take a time that is proportional to the physical time . as we develop theoretical understanding we can take shortcuts .",
    "consider the simple pendulum .",
    "once we discover the exact solution for the simple pendulum we can avoid explicitly integrating the equations of motion to sample typical trajectories .",
    "like the alternating series 1010101 ... , the  of simple pendulum trajectories is small .",
    "pendulum trajectories are not complex because they are ordered .",
    "equilibrium statistical mechanics provides a second example of the interplay of emergence and complexity .",
    "consider a classical gas confined for a long time in a closed , isolated container .",
    "again , in the absence of other knowledge , we would have to integrate the equations of motion for as long as the gas is in the container and  appears to grow in proportion to physical time .",
    "however , once we discover that such a system rapidly reaches thermodynamic equilibrium , we see that its  is much less than its age .",
    "it is not necessary to do a full integration of the many - body equations of motion .",
    "instead , we can do a relatively short molecular dynamics or monte carlo simulation to sample an equilibrium state , independent of how long the gas is in the container . equilibrium states ( at least away from critical points ) are not complex because they are disordered in the sense that they have maximum entropy given the constraints on the system . in both of these examples ,",
    "new properties emerge . for the pendulum ,",
    "periodic motion emerges and for the gas , thermodynamic equilibrium emerges . in both cases , knowing the emergent behavior reduces the estimate of  relative to a blind application of the fundamental dynamics .",
    "our estimate of  is thus contingent on our understanding of a system .",
    "this phenomenon is dramatically illustrated by the scientific history of simulations of the ising model . in 1987 , after more than three decades of numerical studies , a new algorithm was developed by swendsen and wang  @xcite that significantly accelerates the simulation of critical states of the ising model  @xcite .",
    "the swendsen - wang algorithm can be efficiently parallelized and thus provides a much better upper bound on the  of ising critical states .",
    "it is interesting that the improved swendsen - wang algorithm is able to accelerate the simulation of the ising model because it identifies and acts on transient large scale structures that are present at the critical point .",
    "older algorithms , such as the metropolis algorithm  @xcite , flip a single spin at a time and therefore do not `` know '' about the relevant critical fluctuations .",
    "these algorithms correctly sample the critical state but they do so blindly and inefficiently . on the one hand ,",
    "large scale critical fluctuations are the signature of the complexity of the critical state . on the other hand ,",
    "understanding the dynamics of the critical fluctuations allows us to show that the  of the critical state is less than we previously believed though it is still the most complex point in the phase diagram of the ising model .",
    "the ising model again illustrates the general phenomenon of ` emergence ' and its connection to complexity .",
    "single spin flip dynamics can be viewed as the microscopic or ` fundamental ' dynamics of the ising model .",
    "single - spin flip dynamics is correct and very general but inefficient at critical points because here large scale structure ` emerges ' and has its own slow dynamics .",
    "identifying and understanding the dynamics of the emergent structures permits more efficient sampling using the swendsen - wang algorithm .",
    "the ising model example also reveals a serious objection with using  as a complexity measure .",
    "it is not computable and can only be bounded from above .",
    "as we understand a system better we may find an algorithm that simulates the system more efficiently than was previously believed to be possible .",
    "we would then see that the  of the system is smaller than we previously thought .",
    "is the non - computability of  a feature or a bug ?",
    "i conjecture that it is a necessary feature of any general measure that has a hope of distinguishing differences in complexity among very complex systems .",
    "scientific progress is open - ended and presumably not computable .",
    "measures of the complexity of the objects of scientific study should have the same property .",
    "this observation should not be a cause for despair , many systems are already well - understood and major advances in understanding them are not likely . for such systems we can have good estimates of .",
    "the above remarks take us full circle .",
    "the original intent was to construct an intuitively plausible , fundamental general measure of complexity not tied to specific disciplines . in one sense",
    "this has been accomplished since  is defined in the very general setting of statistical physics and computational complexity theory and has at least some of the properties we expect of a complexity measure .",
    "on the other hand , to actually estimate  requires discipline - specific knowledge .",
    "the estimate is provisional and subject to revision as understanding of the system advances .",
    "unlike , there is a simple expression for entropy given a probability distribution though actually computing the entropy may be difficult .",
    "entropy - based complexity measures such as excess entropy are therefore likely to be blind to many features of very complex systems",
    ". they may detect various correlations and memory but can not distinguish whether the information embodied in these features arose from computationally simple or computationally complex processes .    what are complexity measures good for ? what more do we learn by measuring complexity that we would nt otherwise know ?",
    "this question is especially acute for parallel  and logical depth since these quantities can not be measured without first developing considerable understanding of the system .",
    "a complexity measure by itself is not very useful or interesting . instead , complexity measures should play a central role in a theory of complex systems .",
    "such a theory should organize and classify systems within its purview .",
    "it should enhance our understanding of specific systems and help make predictions or formulate hypotheses .",
    "it should allow us to go beyond what we might learn by studying systems in isolation .",
    "the next step in developing the field and demonstrating the utility of   and other complexity measures is to make non - trivial deductive statements relating these measures to other characteristic properties of complex systems .",
    "this work was supported in part by nsf grant dmr-0907235 .",
    "i thank the santa fe institute for supporting visits during which much of this paper was written , and for sponsoring the january 2011 workshop , `` randomness , structure , and causality : measures of complexity from theory to applications , '' which was the inspiration for this paper .",
    "i thank cris moore for useful discussions and suggesting improvements to the parallel algorithms .            c.  h. bennett .",
    "how to define complexity in physics , and why . in w.",
    "h. zurek , editor , _ complexity , entropy and the physics of information _",
    ", page 137 .",
    "sfi studies in the sciences of complexity , vol .  7 , addison - wesley , 1990 ."
  ],
  "abstract_text": [
    "<S> _ _  is a complexity measure for natural systems of the kind studied in statistical physics and is defined in terms of computational complexity .   quantifies the length of the shortest parallel computation required to construct a typical system state or history starting from simple initial conditions . </S>",
    "<S> the properties of  are discussed and it is compared to other complexity measures .  can only be large for systems with embedded computation . </S>"
  ]
}