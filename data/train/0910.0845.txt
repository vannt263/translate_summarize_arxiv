{
  "article_text": [
    "let @xmath0 , @xmath1 , be iid random vectors from a @xmath2-variate , continuous distribution function @xmath3 with multivariate extreme - value copula @xmath4 : for @xmath5^p \\setminus \\{(1 , \\ldots , 1)\\}$ ] , denoting the margins of @xmath3 by @xmath6 , @xmath7 the function @xmath8 , whose domain is @xmath9^p : w_1 + \\cdots + w_p = 1 \\}$ ] , is called the pickands dependence function of @xmath4 , after @xcite .",
    "multivariate extreme - value copulas arise as the limits of copulas of vectors of component - wise maxima of independent random samples @xcite . as a consequence ,",
    "they coincide with the class of copulas of multivariate extreme - value or max - stable distributions .",
    "therefore , they provide models for dependence between extreme values that allow extrapolation beyond the support of the sample .",
    "it is then of interest to estimate the pickands dependence function @xmath8 .",
    "a necessary condition for @xmath4 in to be a copula is that @xmath8 is convex and satisfies @xmath10 for all @xmath11 ; in the bivariate case , this is also sufficient . in general",
    ", @xmath8 should admit an integral representation in terms of a spectral measure .",
    "some other properties of pickands dependence functions are studied in @xcite and @xcite .",
    "the upshot of all this is that the class of pickands dependence functions is infinite - dimensional .",
    "this warrants the use of nonparametric methods .",
    "whereas most papers hitherto concentrated on the bivariate case , a nonparametric estimator for general multivariate pickands dependence functions was introduced in @xcite .",
    "this estimator is in fact a multivariate generalization of the one by capra",
    " fougres  genest @xcite .",
    "the estimator was shown to be uniformly consistent and pointwise asymptotically normal .",
    "however , the joint asymptotic distribution of the estimator as a random function on @xmath12 was not provided .",
    "moreover , implementation of the estimator requires the choice of @xmath2 weight functions @xmath13 on @xmath12 , the issue of their optimal selection being left unresolved .    using a simplified representation of the above - mentioned estimator",
    ", we are able to uncover its asymptotic distribution in the space @xmath14 of continuous , real - valued functions on @xmath15 .",
    "moreover , we give explicit expressions for the weight functions @xmath13 that minimize the pointwise asymptotic variance of the estimator .",
    "these optimal weight functions depend on the unknown distribution .",
    "we show that the cfg - estimator with estimated variance - minimizing weight functions can be implemented as the intercept estimator in a certain linear regression model via ordinary least squares .",
    "the ols - estimator is data - adaptive in the sense that the asymptotic distribution is the same as if the optimal weight functions were used . in a simulation study ,",
    "the gain in efficiency is shown to be quite sizeable .",
    "as in @xcite , the setting here is that of a random sample from a distribution whose margins are known and whose copula is an extreme - value copula . it would be worthwhile to extend this to the case of unknown margins @xcite and the case that the copula of @xmath3 is merely in the domain of attraction of an extreme - value copula @xcite .",
    "the outline of our paper is as follows .",
    "the cfg - estimator is introduced in the next section , including its simplified representation and asymptotic distribution .",
    "the variance - minimizing weight functions are computed in section  [ s : ols ] together with an adaptive estimator based on ordinary least squares in a linear regression framework .",
    "section  [ s : simul ] reports on a simulation study .",
    "the proofs of the results in sections  [ s : cfg ] and  [ s : ols ] are deferred to appendices  a and  b , respectively .",
    "let @xmath0 , @xmath1 , be iid random vectors from a @xmath2-variate , continuous distribution function @xmath3 with multivariate extreme - value copula @xmath4 and pickands dependence function @xmath8 as in .",
    "let @xmath6 be the marginal distribution functions of @xmath3 .",
    "put @xmath16 where @xmath17 for @xmath1 and @xmath18 .",
    "the marginal distributions of the random variables @xmath19 are standard exponential .",
    "the random vectors @xmath20 are iid with common joint survivor function @xmath21 for @xmath22 , where @xmath23 . put @xmath24 with ` @xmath25 ' denoting minimum and with the obvious convention for division by zero ; in particular , @xmath26 for the @xmath2 standard unit vectors @xmath27 in @xmath28 . for @xmath11 and @xmath29 , we have @xmath30 hence the random variables @xmath31 constitute an independent random sample from the exponential distribution with mean @xmath32 .",
    "it follows that the distribution of @xmath33 is gumbel with location parameter @xmath34 , whence @xmath35 = \\log a({\\boldsymbol{w } } ) + \\gamma,\\ ] ] the euler  mascheroni constant @xmath36 being the mean of the standard gumbel distribution .",
    "this suggests the naive estimator @xmath37 the naive estimator is itself not a valid pickands dependence function .",
    "for instance , it does not verify the vertex constraints @xmath38 for all @xmath18 .",
    "a simple way to at least remedy this defect is by putting @xmath39 where @xmath40 are continuous functions verifying @xmath41 for all @xmath42 .",
    "continuity of the functions @xmath13 is assumed merely to ensure that the resulting estimator is a continuous function of @xmath43 as well .",
    "the superscript ` cfg ' refers to the bivarate estimator by capra  fougres ",
    "genest in @xcite , generalized to the multivariate case in @xcite . actually , the original definition in @xcite is @xmath44 where , with @xmath19 as in , @xmath45 moreover , in , the weight functions @xmath13 are supposed to be nonnegative and to satisfy the additional constraint @xmath46 however , if holds , then actually the two estimators coincide , that is , @xmath47 the proof of is essentially the same as the one in @xcite for the bivariate case , the key being that the integrals in   can be solved : @xmath48 + \\log(1 - w_j ) - \\log \\ { ( 1-w_j ) \\wedge z_{ij}({\\boldsymbol{w } } ) \\ } \\\\    = \\log y_{ij } - \\log \\xi_i({\\boldsymbol{w}}).\\end{gathered}\\ ] ] in our representation , however , there is no reason whatsoever to restrict the weight functions to satisfy  .",
    "the asymptotics of the naive estimator and the cfg - estimator follow from standard empirical process theory as presented for instance in @xcite and @xcite .",
    "let @xmath14 denote the banach space of continuous functions from @xmath12 into @xmath49 equipped with the supremum norm .",
    "convergence in distribution is denoted by the arrow ` @xmath50 ' .",
    "[ p : naive ] let @xmath0 , @xmath1 , be iid random variables from a @xmath2-variate , continuous distribution function @xmath3 with multivariate extreme - value copula @xmath4 and pickands dependence function @xmath8 .",
    "the naive estimator @xmath51 in satisfies @xmath52 and in @xmath14 , @xmath53 where @xmath54 is a centered gaussian process with covariance function @xmath55 with @xmath56 as in .",
    "[ t : cfg ] if , in addition to the assumptions in proposition  [ p : naive ] , the functions @xmath40 are continuous , then @xmath57 and in @xmath14 , @xmath58 where @xmath59 is a centered gaussian process defined by @xmath60 with @xmath54 as in proposition  [ p : naive ] .    [ r : cov ] the covariance function can be expressed in terms of @xmath8 as follows .",
    "an application of the identity @xmath61 for @xmath62 yields , by fubini s theorem , @xmath63 \\frac{ds}{s } \\ , \\frac{dt}{t}.\\end{gathered}\\ ] ] where @xmath64 and @xmath23 . replacing @xmath8 by any estimator of it",
    "results in an estimator of the covariance function .",
    "however , a more practical way to estimate this function is by the sample covariance of the pairs @xmath65 ; see also ( the proof of ) theorem  [ t : ols ] .",
    "a further enhancement to the cfg - estimator is to replace it by the convex minorant of the function @xmath66 , \\qquad { \\boldsymbol{w } } \\in { { \\delta_p}},\\ ] ] as in @xcite and @xcite for the bivariate case .",
    "although the resulting estimator would be a convex function respecting the bounds @xmath10 , in case @xmath67 this would still not guarantee it to be a genuine pickands dependence function .",
    "still other ways to impose ( some of ) the shape restrictions are spline smoothing under constraints @xcite , orthogonal projection @xcite , or bayesian nonparametrics @xcite .",
    "a different way to exploit the exponentiality of the random variables @xmath68 in would be via the pickands estimator @xmath69 as in @xcite . to impose the vertex constraints @xmath38 , the techniques of @xcite or @xcite can be used , see @xcite . in the bivariate case",
    "however , it is known that the resulting estimators are outperformed by the cfg - estimator @xmath70 @xcite .",
    "this is confirmed in the simulation study in ( * ? ? ?",
    "* section  3 ) , as well as by our own simulations in section  [ s : simul ] .",
    "for this reason , we restrict attention here to the family of cfg - estimators .",
    "the question remains which weight functions @xmath13 to choose in the cfg - estimator . in @xcite ,",
    "the choice @xmath71 was recommended as a pragmatic one .",
    "the option of using variance - minimizing functions @xmath13 was mentioned but not carried out . by casting the estimation problem in a linear regression framework",
    ", we will obtain an estimator with the same asymptotic performance as the cfg - estimator with those optimal weights . in this section",
    ", we define the estimator and prove its consistency and asymptotic normality , both in the functional sense . in the next section , the gain in efficiency",
    "is assessed by means of simulations .    in view of theorem  [ t : cfg ] , for each @xmath11 we have @xmath72 where @xmath73 is a zero - mean normal random variable .",
    "we will look for those @xmath74 that minimise the variance of @xmath73 .",
    "let @xmath54 be the gaussian process on @xmath14 in proposition  [ p : naive ] .",
    "for ease of notation , put @xmath75 the symbol `` @xmath76 '' denoting matrix transposition . then @xmath77    + { \\boldsymbol{\\lambda}}({\\boldsymbol{w}})^\\top \\ , e [ { \\boldsymbol{\\zeta}}({\\boldsymbol{e } } ) \\ , { \\boldsymbol{\\zeta}}({\\boldsymbol{e}})^\\top ] \\ , { \\boldsymbol{\\lambda}}({\\boldsymbol{w}}).\\end{aligned}\\ ] ] note that @xmath78\\ ] ] is the covariance matrix of @xmath79 .",
    "provided this matrix is non - singular , @xmath80 attains a unique global minimum for @xmath81 equal to @xmath82.\\ ] ] with this choice of the weight functions , the variance of @xmath83 is equal to @xmath84 \\",
    ", \\sigma^{-1 } \\ , e [ { \\boldsymbol{\\zeta}}({\\boldsymbol{e } } ) \\ , \\zeta({\\boldsymbol{w}})].\\ ] ] this variance is minimal over all possible choices of weight functions @xmath13 .    the optimal weight functions @xmath85 in depend on the unknown pickands dependence function @xmath8 .",
    "fortunately , replacing these weight functions by uniformly consistent estimators @xmath86 is just as good asymptotically . for such estimated weight functions ,",
    "define the adaptive cfg - estimator by @xmath87    [ p : cfgad ] assume that , in addition to the assumptions in proposition  [ p : naive ] , the matrix @xmath88 in is non - singular and @xmath86 are random elements in @xmath14 such that , for every @xmath18 , @xmath89 with @xmath85 as in .",
    "then the adaptive cfg - estimator in satisfies @xmath90 and in @xmath14 , @xmath91 where @xmath92 is the zero - mean gaussian process defined in .",
    "finally we propose a particularly convenient way to implement the adaptive cfg - estimator in . for @xmath11 ,",
    "let @xmath93 be the minimizer in @xmath94 of @xmath95 in words , @xmath96 is the ordinary least - squares ( ols ) estimator of the vector of regression coefficients in a linear regression of the dependent variable @xmath97 upon the explanatory variables @xmath98 , @xmath18 .",
    "define the ols - estimator of @xmath8 via the estimated intercept by @xmath99 since the residuals @xmath100 verify @xmath101 , we have @xmath102 that is , the ols - estimator is equal to the adaptive cfg - estimator with estimated weights @xmath103 .",
    "the variance of the ( logarithm of the ) ols - estimator can be estimated by the sample variance of the residuals , properly corrected for the loss in number of degrees of freedom , @xmath104    [ t : ols ] assume that , in addition to the assumptions in proposition  [ p : naive ] , the matrix @xmath88 in is non - singular . then , with probability tending to one , the minimizer @xmath96 of is uniquely defined and for @xmath18 , @xmath105 as a consequence , the ols - estimator in is uniformly consistent , @xmath106 and in @xmath14 , @xmath107 where @xmath92 is the zero - mean gaussian process defined in . in addition , the variance estimator in satisfies @xmath108    in the bivariate case , the assumption that the covariance matrix @xmath88 in is non - singular is equivalent to the assumption that the copula @xmath4 is not the comonotone copula @xcite . we conjecture that in the general multivariate case , a necessary and sufficient condition for @xmath88 to be non - singular is that none of the bivariate margins of @xmath4 is equal to the comonotone copula .",
    "in order to investigate the finite - sample properties of the estimators discussed in the previous sections , we generated pseudo - random samples from trivariate extreme - value copulas of logistic type as presented in @xcite : @xmath109 for @xmath110 ^ 3 $ ] . to facilitate comparisons , we opted for the same parameter values as chosen in @xcite : a symmetric case , @xmath111 , and an asymmetric one , @xmath112 .",
    "for each case @xmath113 samples were generated of size @xmath114 using the simulation algorithms in @xcite and implemented in the r - package @xcite .",
    "four estimators were compared : the cfg - estimator @xmath70 with weight functions @xmath71 ( as recommended in * ? ? ?",
    "* ) , the ols - estimator @xmath115 in , and the enhanced versions of the original pickands estimator due to @xcite and @xcite as presented in @xcite . to visualize the performances of the estimators",
    ", we plotted their biases and mean squared errors along the line @xmath116 ; see figures  [ f : sym ] and  [ f : asym ] for the symmetric and asymmetric logistic dependence functions respectively .    in accordance to the theory",
    ", the ols - estimator is in virtually all cases considered more efficient than the cfg - estimator .",
    "moreover , our simulations confirm the findings in @xcite that the cfg - estimator is typically more efficient than the ones of deheuvels and hall  tajvidi .",
    "note that the finite - sample bias of the ols - estimator is somewhat larger than for the other estimators .",
    "however , thanks to its minimum - variance property it ends up as an overall winner in terms of mean squared error .    [ cols=\"^,^ \" , ]",
    "for @xmath11 , define @xmath117 by @xmath118 we can write @xmath119 consider the function class @xmath120 . we will show that @xmath121 is @xmath122-donsker and therefore also @xmath122-glivenko  cantelli , where @xmath122 denotes the common probability distribution on @xmath123 of the random vectors @xmath124 .",
    "according to theorem  2.6.8 in @xcite and the proof thereof , we need to verify that @xmath121 is a pointwise separable vapnik  cervonenkis - class ( vc - class ) that admits an envelope function with a finite second moment under @xmath122 .",
    "pointwise separability follows from the fact that the map @xmath125 is continuous in @xmath11 for each @xmath126 .",
    "the vc - property can be established by repeated applications of lemmas  2.6.15 and  2.6.18 , items  ( i ) and  ( viii ) , in @xcite .",
    "finally , the readily established bound @xmath127 yields an envelope function of @xmath121 all of whose moments are finite under @xmath122 . observe that the distribution of @xmath128 is exponential with mean equal to @xmath129 $ ] .",
    "from the fact that @xmath121 is @xmath122-glivenko  cantelli it follows that @xmath130 \\biggr|    \\to 0 , \\qquad n \\to \\infty , \\qquad \\text{almost surely.}\\end{gathered}\\ ] ] ( here , we dropped a subscript @xmath131 for convenience . ) continuity of the map @xmath132 yields uniform consistency as in",
    ".    moreover , the @xmath122-donsker property entails @xmath133 in the space @xmath134 of bounded functions from @xmath12 into @xmath49 equipped with the topology of uniform convergence , where we identified @xmath121 with @xmath12 .",
    "the process @xmath54 is zero - mean gaussian with covariance function given in  .",
    "the sample paths of the limit process @xmath54 are continuous with respect to the standard deviation ( semi-)metric @xmath135 on @xmath12 defined by @xmath136^{1/2 } , \\qquad { \\boldsymbol{v } } , { \\boldsymbol{w } } \\in { { \\delta_p}}.\\ ] ] if @xmath137 in @xmath12 according to the euclidean metric , then by continuity of @xmath138 in @xmath43 and by uniform integrability , also @xmath139 .",
    "( uniform integrability is checked by using the bound in . )",
    "it follows that the trajectories of @xmath54 are also continuous with respect to the euclidean metric on @xmath12 , that is , @xmath54 actually takes its values in @xmath14 .",
    "as the trajectories of the left - hand side in are continuous too , the convergence in takes place not only @xmath134 but also in @xmath14 .",
    "the convergence in   follows from the hadamard - differentiability of the map @xmath140 and the functional delta - method ( * ? ? ?",
    "* section  3.9 ) .",
    "uniform consistency of @xmath70 in follows from uniform consistency of @xmath51 in and the fact that the functions @xmath13 are continuous , hence bounded .    to show , define @xmath141 by @xmath142 for @xmath143 and @xmath11 .",
    "the operator @xmath144 is linear and bounded .",
    "we have @xmath145 .",
    "moreover , as @xmath38 for all @xmath18 , also @xmath146 .",
    "we find @xmath147 the weak convergence in follows from the functional delta - method ( * ? ? ?",
    "* section  3.9 ) .",
    "the representation @xmath148 coincides with .",
    "if the optimal weight functions @xmath85 were known , we could consider the optimal cfg - estimator @xmath149 by theorem  [ t : cfg ] , the optimal cfg - estimator is uniformly consistent and is asymptotically normal in the sense of with @xmath150 .",
    "now @xmath151 by uniform consistency of @xmath86 and asymptotic normality of @xmath152 , we obtain , as @xmath153 , @xmath154 as a consequence , the adaptive cfg - estimator is uniformly consistent and asymptotically normal .    in analogy to the linear regression framework , define the @xmath155 matrix @xmath156 and the @xmath157 vector @xmath158 ( no confusion should arise between this @xmath159 and the random vectors @xmath124 in . )",
    "provided the matrix @xmath160 is non - singular , the ols - estimator @xmath96 is given by @xmath161 recall the functions @xmath162 in .",
    "for @xmath163 , define @xmath164 by @xmath165 by and by example  2.10.23 in @xcite , the function class @xmath166 is @xmath122-donsker and thus @xmath122-glivenko  cantelli , where @xmath122 is the common distribution on @xmath123 of the random vectors @xmath124 .",
    "it follows that , almost surely as @xmath153 , @xmath167    \\end{pmatrix } \\biggr| \\to 0,\\end{gathered}\\ ] ] as @xmath88 is non - singular , we have @xmath168 while @xmath169 is with probability tending to one a non - singular matrix too .",
    "we find , almost surely and uniformly in @xmath11 , @xmath170    \\end{pmatrix }     =     \\begin{pmatrix }    \\log a({\\boldsymbol{w } } ) \\\\    { \\boldsymbol{\\lambda}}{^{\\mathrm{opt}}}({\\boldsymbol{w } } )    \\end{pmatrix } ,    \\qquad n \\to \\infty.\\end{gathered}\\ ] ] equation   follows .",
    "proposition  [ p : cfgad ] and equation then yield equations and .    finally , for the estimation of the variance , note that it does not matter asymptotically if we divide by @xmath171 or by @xmath172 .",
    "elementary calculations yield @xmath173 the glivenko ",
    "cantelli property yields , almost surely and uniformly in @xmath11 , @xmath174    = { \\operatorname{var}}\\zeta({\\boldsymbol{w } } ) + \\bigl ( \\log a({\\boldsymbol{w } } ) \\bigr)^2 , \\qquad n \\to \\infty.\\end{gathered}\\ ] ] in combination with and , we obtain that @xmath175 converges almost surely and uniformly in @xmath11 to @xmath176    \\end{pmatrix}^\\top       \\begin{pmatrix }    1 & 0 \\\\    0 & \\sigma^{-1 }    \\end{pmatrix }    \\begin{pmatrix }    \\log a({\\boldsymbol{w } } ) \\\\",
    "e [ { \\boldsymbol{\\zeta}}({\\boldsymbol{e } } ) \\zeta({\\boldsymbol{w } } ) ]    \\end{pmatrix } \\\\    = { \\operatorname{var}}\\zeta({\\boldsymbol{w } } ) - e [ { \\boldsymbol{\\zeta}}({\\boldsymbol{e}})^\\top \\zeta({\\boldsymbol{w } } ) ] \\ , \\sigma^{-1 } \\ , e [ { \\boldsymbol{\\zeta}}({\\boldsymbol{e } } ) \\zeta({\\boldsymbol{w}})],\\end{gathered}\\ ] ] which by is equal to @xmath177 ."
  ],
  "abstract_text": [
    "<S> inference on an extreme - value copula usually proceeds via its pickands dependence function , which is a convex function on the unit simplex satisfying certain inequality constraints . in the setting of an iid random sample from a multivariate distribution with known margins and unknown extreme - value copula , an extension of the capra </S>",
    "<S>  fougres  </S>",
    "<S> genest estimator was introduced by d.  zhang , m.  t.  wells and l.  peng [ journal of multivariate analysis 99 ( 2008 ) 577588 ] . </S>",
    "<S> the joint asymptotic distribution of the estimator as a random function on the simplex was not provided . </S>",
    "<S> moreover , implementation of the estimator requires the choice of a number of weight functions on the simplex , the issue of their optimal selection being left unresolved .    </S>",
    "<S> a new , simplified representation of the cfg - estimator combined with standard empirical process theory provides the means to uncover its asymptotic distribution in the space of continuous , real - valued functions on the simplex . </S>",
    "<S> moreover , the ordinary least - squares estimator of the intercept in a certain linear regression model provides an adaptive version of the cfg - estimator whose asymptotic behavior is the same as if the variance - minimizing weight functions were used . as illustrated in a simulation study , the gain in efficiency </S>",
    "<S> can be quite sizeable .    empirical process , linear regression , minimum - variance estimator , multivariate extreme - value distribution , ordinary least squares , pickands dependence function , unit simplex    60f17 , 62g32 , 62h20 </S>"
  ]
}