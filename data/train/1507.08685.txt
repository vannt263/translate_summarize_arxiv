{
  "article_text": [
    "the stochastic block model is the simplest statistical model for networks with a community ( or cluster ) structure . as such ,",
    "it has attracted considerable amount of work across statistics , machine learning , and theoretical computer science @xcite .",
    "a random graph @xmath1 from this model has its vertex set @xmath2 partitioned into @xmath3 groups , which are assigned @xmath3 distinct labels .",
    "the probability of edge @xmath4 being present depends on the group labels of vertices @xmath5 and @xmath6 .    in the context of social network analysis",
    ", groups correspond to social communities @xcite . for other data - mining applications , they represent latent attributes of the nodes @xcite . in all of these cases ,",
    "we are interested in inferring the vertex labels from a single realization of the graph .",
    "in this paper we develop an information - theoretic viewpoint on the stochastic block model .",
    "namely , we develop an explicit ( ` single - letter ' ) expression for the per - vertex conditional entropy of the vertex labels given the graph .",
    "equivalently , we compute the asymptotic per - vertex mutual information between the graph and the vertex labels .",
    "our results hold asymptotically for large networks under suitable conditions on the model parameters .",
    "the asymptotic mutual information is of independent interest , but is also intimately related to estimation - theoretic quantities .    for the sake of simplicity",
    ", we will focus on the symmetric two group model .",
    "namely , we assume the vertex set @xmath7\\equiv \\{1,2,\\dots , n\\}$ ] to be partitioned into two sets @xmath8 , with @xmath9 independently across vertices @xmath5 .",
    "in particular , the size of each group @xmath10 concentrates tightly around its expectation @xmath11 .",
    "conditional on the edge labels , edges are independent with @xmath12 throughout we will denote by @xmath13 the set of vertex labels @xmath14 , and we will be interested in the conditional entropy @xmath15 or equivalently the mutual information @xmath16 in the limit @xmath17",
    ". we will write @xmath18 ( or @xmath19 ) to imply that the graph @xmath0 is distributed according to the stochastic block model with @xmath20 vertices and parameters @xmath21 .",
    "since we are interested in the large @xmath20 behavior , two preliminary remarks are in order :    1 .   _",
    "_ we obviously have , and entropies will be measured in nats . ]",
    "it is therefore natural to study the per - vertex entropy @xmath23 . + as we will see ,",
    "depending on the model parameters , this will take any value between @xmath24 and @xmath25 .",
    "2 .   _ scaling . _",
    "the reconstruction problem becomes easier when @xmath26 and @xmath27 are well separated , and more difficult when they are closer to each other .",
    "for instance , in an early contribution , dyer and frieze @xcite proved that the labels can be reconstructed exactly modulo an overall flip if @xmath28 are distinct and independent of @xmath20 .",
    "this in particular implies @xmath29 in this limit ( in fact , it implies @xmath30 ) . in this regime ,",
    "the ` signal ' is so strong that the conditional entropy is trivial .",
    "indeed , recent work @xcite show that this can also happen with @xmath26 and @xmath27 vanishing , and characterizes the sequences @xmath31 for which this happens .",
    "( see section [ sec : related ] for an account of related work . )",
    "+ let @xmath32 be the average edge probability .",
    "it turns out that the relevant ` signal - to - noise ratio ' ( snr ) is given by the following parameter : @xmath33 indeed , we will see that @xmath23 of order @xmath34 , and has a strictly positive limit when @xmath35 is of order one .",
    "this is also the regime in which the fraction of incorrectly labeled vertices has a limit that is strictly between @xmath24 and @xmath34 .",
    "as mentioned above , our main result provides a single - letter characterization for the per - vertex mutual information .",
    "this is given in terms of an _ effective gaussian scalar channel_. namely , define the gaussian channel @xmath36 where @xmath37 independent subscript ] of @xmath38 .",
    "we denote by @xmath39 and @xmath40 the corresponding minimum mean square error and mutual information : @xmath41 in the present case , these quantities can be written explicitly as gaussian integrals of elementary functions : @xmath42    we are now in position to state our main result .",
    "[ thm : main ] for any @xmath43 , let @xmath44 be the largest non - negative solution of the equation : @xmath45 we refer to @xmath46 as to the _ effective signal - to - noise ratio_. further , define @xmath47 by : @xmath48 let the graph @xmath49 and vertex labels @xmath50 be distributed according to the stochastic block model with @xmath20 vertices and parameters @xmath51 ( i.e. @xmath52 ) and define @xmath53",
    ".    assume that , as @xmath17 , @xmath54 @xmath55 and @xmath56 @xmath57 .",
    "then , @xmath58    a few remarks are in order .    of course",
    ", we could have stated our result in terms of conditional entropy . namely @xmath59",
    "notice that our assumptions require @xmath57 _ at any , arbitrarily slow , rate_. in words , this corresponds to the graph average degree diverging at any , arbitrarily slow , rate .",
    "recently ( see section [ sec : related ] for a discussion of this literature ) , there has been considerable interest in the case of bounded average degree , namely @xmath60 with @xmath61 bounded .",
    "our proof gives an explicit error bound in terms of problem parameters even when @xmath62 is of order one .",
    "hence we are able to characterize the asymptotic mutual information for large - but - bounded average degree up to an offset that vanishes with the average degree .",
    "explicitly , we prove that : @xmath63 for some absolute constant @xmath64 .    our main result and its proof has implications on the minimum error that can be achieved in estimating the labels @xmath50 from the graph @xmath49 . for reasons that will become clear below , a natural metric is given by the matrix minimum mean square error @xmath65 ( occasionally , we will also use the notation @xmath66 for @xmath67 . ) using the exchangeability of the indices @xmath68 , this can also be rewritten as @xmath69 ^ 2\\big\\}\\\\ & = \\e\\big\\{\\big[x_1x_2-\\e\\{x_1x_2|\\bg\\}\\big]^2\\big\\}\\label{eq : mmse_2vert}\\\\ & = \\min_{\\hx_{12 } : \\cg_n\\to \\reals } \\e\\big\\{\\big[x_1x_2-\\hx_{12}(\\bg)\\big]^2\\big\\}\\ , .",
    "\\label{eq : mmse_2vert_opt } \\ ] ] ( here @xmath70 denotes the set of graphs with vertex set @xmath71 $ ] . ) in words , @xmath67 is the minimum error incurred in estimating the relative sign of the labels of two given ( distinct ) vertices .",
    "equivalently , we can assume that vertex @xmath34 has label @xmath72 .",
    "then @xmath67 is the minimum mean square error incurred in estimating the label of any other vertex , say vertex @xmath73 .",
    "namely , by symmetry , we have ( see section [ sec : estimation ] ) @xmath74 ^ 2\\big|x_1=+1\\big\\}\\label{eq : alternativemmse1}\\\\ & =   \\min_{\\hx_{2|1 } : \\cg_n\\to\\reals } \\e\\big\\{\\big[x_2-\\hx_{2|1}(\\bg)\\big]^2|x_1=+1\\big\\}\\ , .",
    "\\label{eq : alternativemmse2 } \\ ] ] in particular @xmath75 $ ] , with @xmath76 corresponding to random guessing .",
    "[ thm : mainestimation ] under the assumptions of theorem [ thm : main ] ( in particular assuming @xmath77 as @xmath17 ) , the following limit holds for the matrix minimum mean square error @xmath78 further , this implies @xmath79 for @xmath80 and @xmath81 for @xmath82 .    for further discussion of this result and its generalizations",
    ", we refer to section [ sec : estimation ] .",
    "in particular , corollary [ coro : metrics ] establishes that @xmath83 is a phase transition for other estimation metrics as well , in particular for overlap and vector mean square error .    as theorem [ thm : main ] ,",
    "also the last theorem holds under the mild condition that the average degree @xmath84 diverges _ at any , arbitrarily slow rate_. this should be contrasted with the phase transition of naive spectral methods .",
    "it is well understood that the community structure can be estimated by the principal eigenvector of the centered adjacency matrix @xmath85 .",
    "( we denote by @xmath49 the graph as well as its adjacency matrix . )",
    "this approach is successful fro @xmath82 but requires average degree @xmath86 for @xmath87 a constant @xcite .",
    "our proof of theorem [ thm : main ] and theorem [ thm : mainestimation ] involves the analysis of a gaussian observation model , whereby the rank one matrix @xmath88 is corrupted by additive gaussian noise , according to @xmath89 .",
    "in particular , we prove a single letter characterization of the asymptotic mutual information per dimension in this model @xmath90 , cf .",
    "theorem [ thm : singlelettergauss ] below .",
    "the resulting asymptotic value is proved to coincide with the asymptotic value in the stochastic block model , as established in theorem [ thm : main ] .",
    "in other words , the per - dimension mutual information turns out to be _",
    "universal _ across multiple noise models .      in section [ sec : related ] we review the literature on this problem .",
    "we then discuss the connection with estimation in section [ sec : estimation ] .",
    "this section also demonstrates how to evaluate the asymptotic formula in theorem [ thm : main ] .",
    "section [ sec : strategy ] describes the proof strategy . as an intermediate step ,",
    "we introduce a gaussian observation model which is of independent interest .",
    "the proof of theorem [ thm : main ] is reduced to two main propositions :    * proposition [ prop : gausseq ] establishes that within the regime defined in theorem [ thm : main] the stochastic block model is asymptotically equivalent to the gaussian observation model ( see section [ sec : strategy ] for a formal definition ) .",
    "this statement ( with explicit error bounds ) is proved in section [ sec : gausseq ] through a careful application of the lindeberg method .",
    "* proposition [ prop : singleletter ] develops a single - letter characterization of the asymptotic per - vertex mutual information of the gaussian observation model .",
    "the proof of this fact is presented in section [ sec : singleletterproof ] and builds on two steps .",
    "we first prove an asymptotic upper bound on the matrix minimum mean square error @xmath67 using an approximate message passing ( amp ) algorithm .",
    "we then use an area theorem to prove that this upper bound is tight .",
    "finally , section [ sec : proofestimation ] contains the proof of theorem [ thm : mainestimation ] .",
    "several technical details are deferred to the appendices .",
    "the set of first @xmath20 integers is denoted by @xmath71 = \\{1,2,\\dots , n\\}$ ] .",
    "when possible , we will follow the convention of denoting random variables by upper - case letters ( e.g. @xmath91 ) , and their values by lower case letters ( e.g. @xmath92 ) .",
    "we use boldface for vectors and matrices , e.g. @xmath50 for a random vector and @xmath93 for a deterministic vector .",
    "the graph @xmath49 will be identified with its adjacency matrix .",
    "namely , with a slight abuse of notation , we will use @xmath49 both to denote a graph @xmath94 , e)$ ] ( with @xmath7 $ ] the vertex set , and @xmath95 the edge set , i.e. a set of unordered pairs of vertices ) , and its adjacency matrix .",
    "this is a symmetric zero - one matrix @xmath96 with entries @xmath97 throughout we assume @xmath98 by convention .",
    "we write @xmath99 to mean that @xmath100 for a universal constant @xmath64 .",
    "we denote by @xmath64 a generic ( large ) constant that is independent of problem parameters , whose value can change from line to line .",
    "we say that an event holds _ with high probability _ if it holds with probability converging to one as @xmath17 .",
    "we denote the @xmath101 norm of a vector @xmath93 by @xmath102 and the frobenius norm of a matrix @xmath103 by @xmath104 .",
    "the ordinary scalar product of vectors @xmath105 is denoted as @xmath106 .",
    "unless stated otherwise , logarithms will be taken in the natural basis , and entropies measured in nats .",
    "the stochastic block model was first introduced within the social science literature in @xcite . around the same time , it was studied within theoretical computer science @xcite , under the name of ` planted partition model . '",
    "a large part of the literature has focused on the problem of _ exact recovery _ of the community ( cluster ) structure .",
    "a long series of papers @xcite , establishes sufficient conditions on the gap between @xmath26 and @xmath27 that guarantee exact recovery of the vertex labels with high probability .",
    "a sharp threshold for exact recovery was obtained in @xcite , showing that for @xmath107 , @xmath108 , @xmath109 , exact recovery is solvable ( and efficiently so ) if and only if @xmath110 .",
    "efficient algorithms for this problem were also developed in @xcite .",
    "for the sbm with arbitrarily many communities , necessary and sufficient conditions for exact recovery were recently obtained in @xcite .",
    "the resulting sharp threshold is efficiently achievable and is stated in terms of a ch - divergence .",
    "a parallel line of work studied the _ detection _ problem . in this case , the estimated community structure is only required to be asymptotically positively correlated with the ground truth . for this requirement , two independent groups @xcite proved that detection is solvable ( and so efficiently ) if and only if @xmath111 , when @xmath112 , @xmath113 .",
    "this settles a conjecture made in @xcite and improves on earlier work @xcite .",
    "results for detection with more than two communities were recently obtained in @xcite .",
    "a variant of community detection with a single hidden community in a sparse graph was studied in @xcite .    in a sense , the present paper bridges detection and exact recovery , by characterizing the minimum estimation error when this is non - zero , but for @xmath82 smaller than for random guessing .",
    "an information - theoretic view of the sbm was first introduced in @xcite .",
    "there it was shown that in the regime of @xmath112 , @xmath113 , and @xmath114 ( i.e. , disassortative communities ) , the normalized mutual information @xmath115 admits a limit as @xmath17 .",
    "this result is obtained by showing that the condition entropy @xmath15 is sub - additive in @xmath20 , using an interpolation method for planted models . while the result of @xcite holds for arbitrary @xmath116 ( possibly small ) and extend to a broad family of planted models ,",
    "the existence of the limit in the assortative case @xmath117 is left open .",
    "further , sub - additivity methods do not provide any insight as to the limit value .    for the partial recovery of the communities",
    ", it was shown in @xcite that the communities can be recovered up to a vanishing fraction of the nodes if and only if @xmath118 diverges .",
    "this is generalized in @xcite to the case of more than two communities . in these regimes",
    ", the normalized mutual information @xmath115 ( as studied in this paper ) tends to @xmath25 nats . for the constant degree regime , it was shown in @xcite that when @xmath119 is sufficiently large , the fraction of nodes that can be recovered is determined by the broadcasting problem on tree @xcite .",
    "namely , consider the reconstruction problem whereby a bit is broadcast on a galton - watson tree with poisson(@xmath120 ) offspring and with binary symmetric channels of bias @xmath121 on each branch .",
    "then the probability of recovering the bit correctly from the leaves at large depth gives the fraction of nodes that can be correctly labeled in the sbm .    in terms of proof techniques , our arguments are closest to @xcite .",
    "we use the well - known lindeberg strategy to reduce computation of mutual information in the sbm to mutual information of the gaussian observation model .",
    "we then compute the latter mutual information by developing sharp algorithmic upper bounds , which are then shown to be asymptotically tight via an area theorem .",
    "the lindeberg strategy builds from @xcite while the area theorem argument also appeared in @xcite .",
    "we expect these techniques to be more broadly applicable to compute quantities like normalized mutual information or conditional entropy in a variety of models .",
    "let us finally mentioned that the result obtained in this paper are likely to extend to more general sbms , with multiple communities , to the censored block model studied in @xcite , the labeled block model @xcite , and other variants of block models . in particular , it would be interesting to understand which estimation - theoretic quantities appear for these models , and whether a general result stands behind the case of this paper .",
    "while this paper was in preparation , lesieur , krzakala and zdborov @xcite studied estimation of low - rank matrices observed through noisy memoryless channels .",
    "they conjectured that the resulting minimal estimation error is universal across a variety of channel models .",
    "our proof ( see section [ sec : strategy ] below ) establishes universality across two such models : the gaussian and the binary output channels .",
    "we expect that similar techniques can be useful to prove universality for other models as well .",
    "in this section we discuss how to evaluate the asymptotic formulae in theorem [ thm : main ] and theorem [ thm : mainestimation ] .",
    "we then discuss the consequences of our results for various estimation metrics .    before passing to these topics",
    ", we will derive a simple upper bound on the per - vertex mutual information , which will be a useful comparison for our results .",
    "it is instructive to start with an elementary upper bound on @xmath16 .",
    "[ lemma : elementary ] assume @xmath26 , @xmath27 satisfy the assumptions of theorem [ thm : main ] ( in particular @xmath54 @xmath55 and @xmath56 @xmath122 ) .",
    "then @xmath123    we have @xmath124 where @xmath125 follows since @xmath126 are conditionally independent given @xmath50 and @xmath127 because @xmath128 only depends on @xmath50 through the product @xmath129 ( notice that there is no comma but product in @xmath130 .    from our model , it is easy to check that @xmath131 the claim follows by substituting @xmath132 , @xmath133 and by taylor expansion for all @xmath20 large enough . ] .      ) .",
    "the ` effective signal - to - noise ratio ' @xmath46 is given by the intersection of the curve @xmath134 , and the line @xmath135.,title=\"fig : \" ] ( -90,120)@xmath136    . the dashed lines are simple upper bounds : @xmath137 ( cf",
    ". lemma [ lemma : elementary ] ) and @xmath138 .",
    "right frame : asymptotic estimation error under different metrics ( see section [ sec : est ] ) . note the phase transition at @xmath83 in both frames.,title=\"fig : \" ] .",
    "the dashed lines are simple upper bounds : @xmath137 ( cf .",
    "lemma [ lemma : elementary ] ) and @xmath138 .",
    "right frame : asymptotic estimation error under different metrics ( see section [ sec : est ] ) . note the phase transition at @xmath83 in both frames.,title=\"fig : \" ]    our asymptotic expression for the mutual information , cf .",
    "theorem [ thm : main ] , and for the estimation error , cf .",
    "theorem [ thm : mainestimation ] , depends on the solution of eq .",
    "( [ eq : mainequation ] ) which we copy here for the reader s convenience : @xmath139 here we defined @xmath140 the effective signal - to - noise ratio @xmath46 that enters theorem [ thm : main ] and theorem [ thm : mainestimation ] is the largest non - negative solution of eq .",
    "( [ eq : mainequation ] ) .",
    "this equation is illustrated in figure [ fig : formulaevaluation ] .",
    "it is immediate to show from the definition ( [ eq : gdef ] ) that @xmath141 is continuous on @xmath142 with @xmath143 , and @xmath144 .",
    "this in particular implies that @xmath145 is always a solution of eq .",
    "( [ eq : mainequation ] ) .",
    "further , since @xmath39 is monotone decreasing in the signal - to - noise ratio @xmath146 , @xmath147 is monotone increasing . as shown in the proof of remark [ rem : unique ] ( see appendix [ sec : proofunique ] ) , @xmath141 is also strictly concave on @xmath142 .",
    "this implies that eq .",
    "( [ eq : mainequation ] ) as at most one solution in @xmath148 , and a strictly positive solution only exists if @xmath149 .",
    "we summarize these remarks below , and refer to figure [ fig : informationestimation ] for an illustration .",
    "[ lemma : fixedpoint ] the effective snr , and the asymptotic expression for the per - vertex mutual information in theorem [ thm : main ] have the following properties :    * for @xmath80 , we have @xmath150 and @xmath151 . * for @xmath80",
    ", we have @xmath152 strictly with @xmath153 as @xmath154 . +",
    "further , @xmath155 strictly with @xmath156 as @xmath157 .",
    "all of the claims follow immediately form the previous remarks , and simple calculus , except the claim @xmath158 for @xmath82 .",
    "this is direct consequence of the variational characterization established below .",
    "we next give an alternative ( variational ) characterization of the asymptotic formula which is useful for proving bounds .",
    "under the assumptions and definitions of theorem [ thm : main ] , we have @xmath159    the function @xmath160 is differentiable on @xmath142 with @xmath161 as @xmath157 .",
    "hence , the @xmath162 is achieved at a point where the first derivative vanishes ( or , eventually , at @xmath24 ) . using the i - mmse relation @xcite , we get @xmath163 hence the minimizer is a solution of eq .",
    "( [ eq : mainequation ] ) .",
    "as shown above , for @xmath80 , the only solution is @xmath150 , which therefore yields @xmath164 as claimed .",
    "for @xmath82 , eq .  ( [ eq : mainequation ] )",
    "admits the two solutions : @xmath24 and @xmath165 .",
    "however , by expanding eq .",
    "( [ eq : derpsi ] ) for small @xmath146 , we obtain @xmath166 and hence @xmath167 is a local maximum , which implies the claim for @xmath82 as well .",
    "we conclude by noting that eq .",
    "( [ eq : mainequation ] ) can be solved numerically rather efficiently .",
    "the simplest method consists is by iteration .",
    "namely , we initialize @xmath168 and then iterate @xmath169 . this approach was used for figure [ fig : informationestimation ] .",
    "theorem [ thm : mainestimation ] establishes that a phase transition takes place at @xmath83 for the matrix minimum mean square error @xmath170 defined in eq .",
    "( [ eq : mmsedef ] ) . throughout this section",
    ", we will omit the subscript @xmath20 to denote the @xmath17 limit ( for instance , we write @xmath171 ) .",
    "figure [ fig : informationestimation ] reports the asymptotic prediction for @xmath172 stated in theorem [ thm : mainestimation ] , and evaluated as discussed above .",
    "the error decreases rapidly to @xmath24 for @xmath82 .    in this section",
    "we discuss two other estimation metrics . in both cases",
    "we define these metrics by optimizing a suitable risk over a class of estimators : it is understood that randomized estimators are admitted as well .    *",
    "the first metric is the _ vector minimum mean square error _ :",
    "@xmath173 note the minimization over the sign @xmath174 : this is necessary because the vertex labels can be estimated only up to an overall flip .",
    "of course @xmath175 $ ] , since it is always possible to achieve vector mean square error equal to one by returning @xmath176 . *",
    "the second metric is the _ overlap _ : @xmath177 again @xmath178 $ ] ( but now large overlap corresponds to good estimation ) . indeed by returning @xmath179 uniformly at random , we obtain @xmath180 .",
    "+ note that the main difference between overlap and vector minimum mean square error is that in the latter case we consider estimators @xmath181 taking arbitrary real values , while in the former we assume estimators @xmath182 taking binary values .    in order to clarify the relation between various metrics ,",
    "we begin by proving the alternative characterization of the matrix minimum mean square error in eqs .",
    "( [ eq : alternativemmse1 ] ) , ( [ eq : alternativemmse2 ] ) .    letting @xmath67 be defined as per eq .  ( [ eq : mmsedef ] ) , we have @xmath183 ^ 2\\big|x_1=+1\\big\\ } \\label{eq : alternativemmse1_bis}\\\\ & =   \\min_{\\hx_{2|1 } : \\cg_n\\to\\reals } \\e\\big\\{\\big[x_2-\\hx_{2|1}(\\bg)\\big]^2|x_1=+1\\big\\}\\ , .",
    "\\label{eq : alternativemmse2_bis } \\ ] ]    first note that eq .",
    "( [ eq : alternativemmse2_bis ] ) follows immediately from eq .",
    "( [ eq : alternativemmse1_bis ] ) since conditional expectation minimizes the mean square error ( the conditioning only changes the prior on @xmath50 ) .",
    "in order to prove eq .",
    "( [ eq : alternativemmse1_bis ] ) , we start from eq .",
    "( [ eq : mmse_2vert ] ) .",
    "since the prior distribution on @xmath184 is uniform , we have @xmath185 where in the second line we used the fact that , conditional to @xmath49 , @xmath50 is distributed as @xmath186 .",
    "continuing from eq .",
    "( [ eq : mmse_2vert ] ) , we get @xmath187 ^ 2\\big\\}\\\\ & = \\frac{1}{2}\\ , \\e\\big\\{\\big[x_1x_2-\\e\\{x_2|x_1=+1,\\bg\\}\\big]^2\\big|x_1 = + 1\\big\\}\\nonumber\\\\ & \\phantom{aaaa}+   \\frac{1}{2}\\ , \\e\\big\\{\\big[x_1x_2-\\e\\{x_2|x_1=+1,\\bg\\}\\big]^2\\big|x_1 = -1\\big\\}\\\\ & =   \\e\\big\\{\\big[x_2-\\e\\{x_2|x_1=+1,\\bg\\}\\big]^2\\big|x_1 = + 1\\big\\}\\ , , \\ ]",
    "] which proves the claim .",
    "the next lemma clarifies the relationship between matrix and vector minimum mean square error .",
    "its proof is deferred to appendix [ sec : vectorvsmatrix ] .",
    "[ lemma : vectorvsmatrix ] with the above definitions , we have @xmath188    finally , a lemma that relates overlap and vector minimum mean square error , whose proof can be found in appendix [ sec : overlapvsvector ] .",
    "[ lemma : overlapvsvector ] with the above definitions , we have @xmath189    as an immediate corollary of these lemmas ( together with theorem [ thm : mainestimation ] and lemma [ lemma : fixedpoint ] ) , we obtain that @xmath83 is the critical point for other estimation metrics as well .",
    "[ coro : metrics ] the vector minimum mean square error and the overlap exhibit a _ phase transition at @xmath83_. namely , under the assumptions of theorem [ thm : main ] ( in particular , @xmath55 and @xmath57 ) , we have    * if @xmath80 , then estimation can not be performed asymptotically better than without any information : @xmath190 * if @xmath82 , then estimation can be performed better than without any information , even in the limit @xmath17 : @xmath191",
    "in this section we describe the main elements used in the proof of theorem [ thm : main ] :    * we describe a gaussian observation model which has asymptotically the same mutual information as the sbm introduced above . *",
    "we state an asymptotic characterization of the mutual information of this gaussian model . *",
    "we describe an approximate message passing ( amp ) estimation algorithm that plays a key role in the last characterization .",
    "we then use these technical results ( proved in later sections ) to prove theorem [ thm : main ] in section [ sec : prooftheoremmain ] .",
    "we recall that @xmath192 .",
    "define the gap @xmath193 .",
    "we will assume for the proofs that @xmath194 ( i.e. the assortative model ) but the results also hold for @xmath195 in an analogous fashion .",
    "the edges @xmath126 are conditionally independent given the vertex labels @xmath50 , with distribution : @xmath196 as a first step , we compare the sbm with an alternate gaussian observation model defined as follows .",
    "let @xmath197 be a gaussian random symmetric matrix generated with independent entries @xmath198 and @xmath199 , independent of @xmath50 .",
    "consider the noisy observations @xmath200 defined by @xmath201 note that this model matches the first two moments of the original model .",
    "more precisely , if we define the rescaled adjacency matrix @xmath202 , then @xmath203 and @xmath204 .",
    "our first proposition proves that the mutual information between the vertex labels @xmath50 and the observations agrees to leading order across the two models .    [ prop : gausseq ] assume that , as @xmath17 , @xmath54 @xmath55 and @xmath56 @xmath57 .",
    "then there is a constant @xmath64 independent of @xmath20 such that @xmath205    the proof of this result is presented in section [ sec : gausseq ] .",
    "the next step consists in analyzing the gaussian model ( [ eq : gaussobs ] ) , which is of independent interest .",
    "it turns out to be convenient to embed this in a more general model whereby , in addition to the observations @xmath103 , we are also given observations of @xmath50 through a binary erasure channel with erasure probability @xmath206 , @xmath207 .",
    "we will denote by @xmath208 the output of this channel , where we set @xmath209 every time the symbol is erased .",
    "formally we have @xmath210 where @xmath211 are independent random variables , independent of @xmath50 , @xmath49 . in the special case @xmath212 , all of these observations are trivial , and we recover the original model .",
    "the reason for introducing the additional observations @xmath213 is the following .",
    "the graph @xmath49 has the same distribution conditional on @xmath50 or @xmath186 , hence it is impossible to recover the sign of @xmath50 . as we will see , the extra observations @xmath213 allow to break this trivial symmetry and we will recover the required results by continuity in @xmath214 as the extra information vanishes .    indeed , our next result establishes a single letter characterization of @xmath215 in terms of a recalibrated _ scalar _ observation problem .",
    "namely , we define the following observation model for @xmath216 a rademacher random variable : @xmath217 here @xmath218 , @xmath219 , @xmath220 , are mutually independent .",
    "we denote by @xmath221 , the minimum mean squared error of estimating @xmath218 from @xmath222 , @xmath223 , conditional on @xmath224 .",
    "recall the definitions ( [ eq : infodef ] ) , ( [ eq : mmsedef ] ) of @xmath40 , @xmath39 , and the expressions ( [ eq : infoformula ] ) , ( [ eq : mmseformula ] ) .",
    "a simple calculation yields @xmath225    [ prop : singleletter ] for any @xmath43 , @xmath226 $ ] , let @xmath227 be the largest non - negative solution of the equation : @xmath228 further , define @xmath229 by : @xmath230 then , we have @xmath231    using continuity in @xmath214 , the last result implies directly a limit result for the mutual information under the gaussian model , which we single out since it is of independent interest .",
    "[ thm : singlelettergauss ] for any @xmath43 , let @xmath46 be the largest non - negative solution of the equation : @xmath232 further , define @xmath47 by : @xmath233 then , we have @xmath234      to analyze the gaussian model eq .",
    "we introduce an approximate message passing ( amp ) algorithm that computes estimates @xmath235 at time @xmath236 , which are functions of the observations @xmath237 .",
    "this construction follows the general scheme of amp algorithms developed in @xcite .",
    "given a sequence of functions @xmath238 , we set @xmath239 and compute @xmath240 above ( and in the sequel ) we extend the function @xmath241 to vectors by applying it component - wise , i.e. @xmath242 .    the amp iteration above proceeds analogously to the usual power iteration to compute principal eigenvectors , but has an additional memory term @xmath243 .",
    "this additional term changes the behavior of the iterates in an important way : unlike the usual power iteration , there is an explicit distributional characterization of the iterates @xmath244 in the limit of large dimension .",
    "namely , for each time @xmath236 we will show that , approximately @xmath245 is a scaled version of the truth @xmath246 observed through gaussian noise of a certain variance .",
    "we define the following two - parameters recursion , with initialization @xmath247 , which will be referred to as _ state evolution _ :",
    "@xmath248 where expectation is with respect to the independent random variables @xmath249 , @xmath38 and @xmath250 , setting @xmath251 .",
    "the following lemma makes this distributional characterization precise .",
    "it follows from the more general result of @xcite and we provide a proof in appendix [ app : addproofs ] .    [ lem : stateevollem ] let @xmath252 be a sequence of functions such that @xmath253 are lipschitz continuous in their first argument ( where @xmath254 denotes the derivative of @xmath241 with respect to the first argument ) .",
    "let @xmath255 be a test function such that @xmath256 for all @xmath257 .",
    "then the following limit holds almost surely for @xmath258 random variables distributed as above @xmath259    although the above holds for a relatively broad class of functions @xmath241 , we are interested in the amp algorithm for specific functions @xmath241 . specifically , we following sequence of functions @xmath260 it is easy to see that @xmath241 satisfy the requirement of lemma [ lem : stateevollem ]",
    ". we will refer to this version of amp as _ bayes - optimal amp_.    note that the definition ( [ eq : optfunc ] ) depends itself on @xmath261 and @xmath262 defined through eqs .",
    "( [ eq : stateevol1 ] ) , ( [ eq : stateevol1 ] ) .",
    "this recursive definition is perfectly well defined and yields @xmath263 using the fact that @xmath264 is the minimum mean square error estimator , we obtain @xmath265 where @xmath266 is given explicitly by eq .  ( [ eq : mmseformula ] ) .    in other words",
    ", the state evolution recursion reduces to a simple one - dimensional recursion that we can write in terms of the variable @xmath267 .",
    "we obtain @xmath268    our proof strategy uses the amp algorithm to construct estimates that _ bound from above _ the minimum error of estimating @xmath50 from observations @xmath269 .",
    "however , in the limit of a large number of iterations , we show that the gap between this upper bound and the minimum estimation error vanishes via an area theorem .",
    "more explicitly , we develop an upper bound on the matrix mean square error first introduced in eq .",
    "( [ eq : mmsedef ] ) .",
    "we generalize this in the obvious way to the gaussian observation model : @xmath270 ( note that we adopt here a slightly different normalization with respect to eq .",
    "( [ eq : mmsedef ] ) .",
    "this change is immaterial in the large @xmath20 limit . )",
    "we then use amp to construct the sequence of estimators @xmath271 , indexed by @xmath272 , where @xmath273 is defined as in eq .. the matrix mean squared error of this estimators will be denoted by @xmath274    we also define the limits @xmath275 in the course of the proof , we will also see that these limits are well - defined , using the state evolution lemma [ lem : stateevollem ]      the proof is almost immediate given propositions [ prop : gausseq ] and [ prop : singleletter ] .",
    "firstly , note that , for any @xmath226 $ ] , @xmath276 since , by proposition [ prop : singleletter ] @xmath277 has a well - defined limit as @xmath17 , and @xmath278 is arbitrary , we have that : @xmath279 it is immediate to check that @xmath229 is continuous in @xmath280 , @xmath281 and @xmath282 as defined in theorem [ thm : main ] . furthermore , as @xmath283 , the unique positive solution @xmath227 of eq .",
    "( [ eq : epsfixedpoint ] ) converges to @xmath46 , the largest non - negative solution to of eq .",
    "( [ eq : mainequation ] ) , which we copy here for the readers convenience : @xmath284 this follows from the smoothness and concavity of the function @xmath136 ( see lemma [ rem : unique ] ) .",
    "it follows that @xmath285 and therefore @xmath286 this proves theorem [ thm : singlelettergauss ] .",
    "theorem [ thm : main ] follows by applying proposition [ prop : gausseq ] .",
    "given a collection @xmath287 of random variables defined on the same probability space as @xmath50 , and a non - negative real number @xmath288 , we define the following hamiltonian and log - partition function associated with it : @xmath289    [ lem : gaussentro ] we have the identity : @xmath290    by definition : @xmath291 since the two distributions @xmath292 and @xmath293 are absolutely continuous with respect to each other , we can write the above simply in terms of the ratio of ( lebesgue ) densities , and we obtain : @xmath294 we modify the final term as follows : @xmath295 substituting this in eq .",
    "we have @xmath296 as required .",
    "[ lem : sbmentro ] define the ( random ) hamiltonian @xmath297 by : @xmath298 then we have that : @xmath299    this follows directly from the definition of mutual information : @xmath300 as in lemma [ lem : gaussentro ] we can write this in terms of densities as : @xmath301 substituting this in the mutual information formula eq .",
    "yields the lemma .",
    "define the random variables @xmath302 as follows : @xmath303    the following lemma shows that , to compute @xmath16 it suffices to compute the log - partition function with respect to the approximating hamiltonian .",
    "assume that , as @xmath17 , @xmath54 @xmath55 and @xmath56 @xmath57 .",
    "then , we have @xmath304 [ lem : sbmapproxrel ]    we concentrate on the log - partition function for the hamiltonian @xmath305 . first , using the fact that @xmath306 when @xmath307 : @xmath308 now when @xmath309 , for small enough @xmath310 , we have by taylor expansion the following approximation for @xmath311 $ ] : @xmath312 which implies , by triangle inequality : @xmath313 where @xmath314    we first simplify the rhs in eq .. recalling the definition of @xmath315 : @xmath316 this implies that : @xmath317 where @xmath318 satisfies eq .. we now use the following remark , which is a simple application of bernstein inequality ( the proof is deferred to appendix [ app : addproofs ] ) .    [ rem : berns ] there exists a constant @xmath64 such that for every @xmath20 large enough : @xmath319    using this remark , the error bound eq . and eq . in lemma",
    "[ lem : sbmentro ] yields @xmath320 substituting @xmath321 gives the lemma .",
    "we now control the deviations that occur when replacing the variables @xmath315 with gaussian variables @xmath322 .",
    "[ lem : approx ] assume that , as @xmath17 , @xmath54 @xmath55 and @xmath56 @xmath57 . then we have : @xmath323    this proof follows the lindeberg strategy @xcite",
    ". we will show that : @xmath324 ( with the @xmath325 term uniform in @xmath50 ) .",
    "the claim then follows by taking expectations on both sides .",
    "note that , by construction : @xmath326 and @xmath327 we now derive the following estimates : @xmath328 here @xmath329 is the @xmath3-fold derivative of @xmath330 in the entry @xmath331 of the matrix @xmath332 . to write explicitly the derivatives @xmath333",
    "we introduce some notation . for a function @xmath334",
    ", we write @xmath335 to denote its expectation with respect to the measure defined by the hamiltonian @xmath336 . explicitly : @xmath337 then the partial derivatives above can be expressed as @xmath338 however since @xmath339 , we obtain : @xmath340 applying theorem 2 of @xcite ( stated below as theorem [ thm : lindegen ] ) gives : @xmath341 further , we have : @xmath342 here @xmath343 denotes the derivative with respect to the variable @xmath288 .",
    "thus , @xmath344 combining eqs . , gives eq .",
    ", and the lemma follows by taking expectations on either side .",
    "we state below the lindeberg generalization theorem for convenience :    [ thm : lindegen ] suppose we are given two collections of random variables @xmath345}$ ] , @xmath346}$ ] with independent components and a function @xmath347 .",
    "let @xmath348 and @xmath349 .",
    "then : @xmath350    with these in hand , we can now prove proposition [ prop : gausseq ] .",
    "the proposition follows simply by combining the formulae for @xmath351 in lemmas [ lem : gaussentro ] , [ lem : sbmapproxrel ] with the approximation guarantee of lemma [ lem : approx ] .      throughout this section",
    "we will write @xmath352 whenever we want to emphasize the dependence of the law of @xmath103 on the signal to noise parameter @xmath288 .",
    "the proof of proposition [ prop : singleletter ] follows essentially from a few preliminary lemmas .",
    "we begin with some properties of the fixed point equation ( [ eq : epsfixedpoint ] ) .",
    "the proof of this lemma can be found in appendix [ sec : proofunique ] .",
    "[ rem : unique ] for any @xmath353 $ ] , the following properties hold for the function @xmath354 :    1 .",
    "it is continuous , monotone increasing and concave in @xmath355 .",
    "it satisfies the following limit behaviors @xmath356 & = 1\\ , .",
    "\\end{aligned}\\ ] ]    as a consequence we have the following for all @xmath357 $ ] :    1 .   a non - negative solution @xmath358 of eq .",
    "( [ eq : epsfixedpoint ] ) exists and is unique for all @xmath278 .",
    "2 .   for any @xmath278",
    ", the function @xmath359 is differentiable in @xmath288 .",
    "3 .   let @xmath360 be defined recursively by eq .",
    "( [ eq : simplstateevol3 ] ) , with initialization @xmath361 . then @xmath362 .",
    "we then compute the value of @xmath363 at @xmath364 and @xmath365 .",
    "[ lem : limpoints ] for any @xmath278 : @xmath366    recall the definition of @xmath39 , cf .",
    "( [ eq : mmsedef ] ) .",
    "upper bounding @xmath39 by the minimum error obtained by linear estimator yields , for any @xmath367 , @xmath368 .",
    "substituting these bounds in eq .",
    "( [ eq : epsfixedpoint ] ) , we obtain @xmath369                                 = \\lambda-(1-\\eps)+o(\\lambda^{-1})\\ , , \\ ] ] where the last expansion holds as @xmath154 .",
    "let us now consider the limit @xmath370 , cf .",
    "claim ( [ eq : psilamblda0 ] ) . considering eq .",
    "( [ eq : psieps ] ) , and using @xmath371 , we have @xmath372 .",
    "further from the definition ( [ eq : infodef ] ) it follows ) . ]",
    "that @xmath373 thus yielding eq .",
    "( [ eq : psilamblda0 ] ) .",
    "consider next the @xmath154 limit of eq .",
    "( [ eq : psilambldainfty ] ) . in this limit",
    "( [ eq : boundgamma ] ) implies @xmath374 , where @xmath375 .",
    "hence @xmath376 ( this follows again from the definition of @xmath40 ) .",
    "further @xmath377 substituting in eq .",
    "( [ eq : psieps ] ) we obtain the desired claim .    the next lemma characterizes the limiting matrix mean squared error of the amp estimates .",
    "[ lem : fixedpt ] let @xmath360 be defined recursively by eq .",
    "( [ eq : simplstateevol3 ] ) with initialization @xmath378 , and recall that @xmath358 denotes the unique non - negative solution of eq ..    then the following limits hold for the amp mean square error @xmath379    note that eq .",
    "( [ eq : mseamp_tinfty ] ) follows from eq .",
    "( [ eq : mseamp_t ] ) using lemma [ rem : unique ] , point @xmath380 .",
    "we will therefore focus on proving eq .",
    "( [ eq : mseamp_t ] ) .",
    "first notice that : @xmath381 since @xmath382 , the first term evaluates to 1 .",
    "we use lemma [ lem : stateevollem ] to deal with the final two terms . consider the last term @xmath383 . using lemma [ lem : stateevollem ] with the @xmath384 we have ,",
    "almost surely @xmath385 note also that @xmath386 and @xmath387 are bounded by 1 , hence so is @xmath388 .",
    "it follows from the bounded convergence theorem that @xmath389 in a similar manner , we have that @xmath390 , whence the thesis follows .",
    "[ lem : psiintegral ] for every @xmath391 and @xmath392 : @xmath393    by differentiating eq .",
    "( [ eq : psieps ] ) we obtain ( recall @xmath394 ) : @xmath395 it follows from the uniqueness and differentiability of @xmath358 ( cf .",
    "lemma [ rem : unique ] ) that @xmath396 is differentiable for any fixed @xmath278 , with derivative @xmath397 the lemma follows from the fundamental theorem of calculus using lemma [ lem : limpoints ] for @xmath398 , and lemma [ lem : fixedpt ] , cf .",
    "( [ eq : mseamp_tinfty ] ) .",
    "we are now in a position to prove proposition [ prop : singleletter ] .",
    "we start from a simple remark , proved in appendix    [ rem : limitentro ] we have @xmath399 further the asymptotic mutual information satisfies @xmath400    we defer the proof of these facts to appendix [ sec : limentro ] .    applying the ( conditional )",
    "i - mmse identity of @xcite we have @xmath401 we therefore have @xmath402 \\\\ & \\stackrel{(b)}{= } \\lim_{\\lambda\\to\\infty}\\liminf_{n\\to\\infty } \\frac{1}{4 } \\int_{0}^{\\lambda}\\mmse(\\lambda',\\eps , n ) \\ , \\d\\lambda'\\\\ & \\stackrel{(c)}{\\le } \\lim_{\\lambda\\to\\infty}\\lim_{t\\to \\infty}\\limsup_{n\\to\\infty } \\frac{1}{4 } \\int_{0}^{\\lambda}\\mseamp(t;\\lambda',\\eps , n ) \\",
    ", \\d\\lambda'\\\\ & \\stackrel{(d)}{= } \\lim_{\\lambda\\to\\infty}\\lim_{t\\to \\infty } \\frac{1}{4 } \\int_{0}^{\\lambda}\\mseamp(t;\\lambda',\\eps ) \\ , \\d\\lambda ' \\ ] ] where @xmath125 follows from remark [ rem : limitentro ] , @xmath127 from eq .",
    "( [ eq : ixx ] ) and ( [ eq : immse ] ) , @xmath403 from ( [ eq : immse2 ] ) , and @xmath380 from bounded convergence . continuing from the previous chain",
    "we get @xmath404\\\\ & \\stackrel{(g)}{= } ( 1-\\eps)\\log 2\\ , , \\ ] ] where @xmath405 follows from lemma [ lem : fixedpt ] , @xmath406 from lemma [ lem : psiintegral ] , and @xmath407 from lemma [ lem : limpoints ] .",
    "we therefore have a chain of equalities , whence the inequality @xmath403 must hold with equality . since @xmath408 for any @xmath288 , this implies @xmath409 for almost every @xmath288 .",
    "the conclusion follows for every @xmath288 by the monotonicity of @xmath410 , and the continuity of @xmath411 .    using again remark [ rem : limitentro ] , and the last display",
    ", we get that the following limit exists @xmath412 where we used lemma [ lem : psiintegral ] in the last step .",
    "this concludes the proof .",
    "in this section we recall a general formula to compute the derivative of the conditional entropy @xmath15 with respect to noise parameters .",
    "the formula was proved in @xcite and ( * ? ? ?",
    "* lemma 2 ) .",
    "we restate it in the present context and present a self - contained proof for the reader s convenience .",
    "we consider the following setting . for @xmath20 an integer , denote by @xmath413 the set of unordered pairs in @xmath71 $ ] ( in particular @xmath414 ) .",
    "we will use @xmath415 to denote elements of @xmath413 . for for each @xmath416",
    "we are given a one - parameter family of discrete noisy channels indexed by @xmath417 ( with @xmath418 a non - empty interval ) , with input alphabet @xmath419 and finite output alphabet @xmath420 .",
    "concretely , for any @xmath421 , we have a transition probability @xmath422 which is differentiable in @xmath423 .",
    "we shall omit the subscript @xmath423 since it will be clear from the context .",
    "we then consider @xmath424 a random vector in @xmath425 , and @xmath426 a set of observations in @xmath427 that are conditionally independent given @xmath50 .",
    "further @xmath428 is the noisy observation of @xmath429 through the channel @xmath430 . in formulae ,",
    "the joint probability density function of @xmath50 and @xmath103 is @xmath431 this obviously include the two - groups stochastic block model as a special case , if we take @xmath432 to be the uniform distribution over @xmath425 , and output alphabet @xmath433 . in that case",
    "@xmath434 is just the adjacency matrix of the graph .    in the following we write @xmath435 for the set of observations excluded @xmath421 , and @xmath436 for @xmath437 .",
    "[ lemma : differentiation ] with the above notation , we have : @xmath438\\big\\}\\ , .",
    "\\label{eq : bigderivative } \\ ] ]    fix @xmath439 . by linearity of differentiation ,",
    "it is sufficient to prove the claim when only @xmath440 depends on @xmath423 .",
    "writing @xmath441 by chain rule in two alternative ways we get @xmath442 where in the last identity we used the conditional independence of @xmath443 from @xmath50 , @xmath444 , given @xmath445 . differentiating with respect to @xmath423 , and using the fact that @xmath446 is independent of @xmath440 , we get @xmath447",
    "consider the first term . singling out the dependence of @xmath448 on @xmath449 we get @xmath450 in the second line we used the fact that the distribution of @xmath445 is independent of @xmath423 , and the normalization condition @xmath451 .",
    "we follow the same steps for the second term ( [ eq : derivativefirst ] ) : @xmath452\\log \\big[\\sum_{x'_e}p_{x_e , y_e|\\by_{-e}}(x'_e , y_e|\\by_{-e})\\big]\\big\\}\\\\ & \\phantom{aa}=-\\frac{\\d\\phantom{a}}{\\d\\theta}\\sum_{y_e } \\e\\big\\{\\big[\\sum_{x_e}p_e(y_e|x_e)p_{x_e|\\by_{-e}}(x_e|\\by_{-e})\\big]\\log \\big[\\sum_{x'_e}p_e(y_e|x'_e)p_{x_e|\\by_{-e}}(x'_e|\\by_{-e})\\big]\\big\\}\\\\ & \\phantom{aa}=-\\sum_{x_e , y_e}\\frac{\\d p_e(y_e|x_e)}{\\d\\theta } \\e\\big\\{p_{x_e|\\by_{-e}}(x_e|\\by_{-e})\\log \\big[\\sum_{x'_e}p_e(y_e|x'_e)p_{x_e|\\by_{-e}}(x'_e|\\by_{-e})\\big]\\big\\}\\ , .\\label{eq : derivative3 } \\ ] ] taking the difference of eq .",
    "( [ eq : derivative2 ] ) and eq .",
    "( [ eq : derivative3 ] ) we obtain the desired formula .",
    "we next apply the general differentiation lemma [ lemma : differentiation ] to the stochastic block model .",
    "as mentioned above , this fits the framework in the previous section , by setting @xmath103 be the adjacency matrix of the graph @xmath49 , and taking @xmath453 to be the uniform distribution over @xmath425 . for the sake of convenience",
    ", we will encode this as @xmath454 .",
    "in other words @xmath455 and @xmath456 ( respectively @xmath457 ) encodes the fact that edge @xmath421 is present ( respectively , absent ) . we then have the following channel model for all @xmath439 : @xmath458 we parametrize these probability kernels by a common parameter @xmath459 by letting @xmath460 we will be eventually interested in setting @xmath461 to make contact with the setting of theorem [ thm : mainestimation ] .    [ lemma : differentiationsbm ]",
    "let @xmath16 be the mutual information of the two - groups stochastic block models with parameters @xmath462 and @xmath463 given by eq .",
    "( [ eq : pnqnparam ] ) . then there exists a numerical constant @xmath64 such that the following happens .    for any @xmath464 there exists @xmath465 such that , if @xmath466 then for all @xmath467 $ ] , @xmath468    we let @xmath469 and apply lemma [ lemma : differentiation ] .",
    "simple calculus yields @xmath470 from eq .",
    "( [ eq : bigderivative ] ) , letting @xmath471 , @xmath472\\big\\}\\nonumber\\\\ & -\\frac{1}{2}\\sqrt{\\frac{\\op_n(1-\\op_n)}{n\\theta}}\\sum_{e\\in\\pair}\\sum_{x_e , y_e}x_ey_e p_{x_e}(x_e)\\log p_e(y_e|x_e)\\\\ = & \\frac{1}{2}\\sqrt{\\frac{\\op_n(1-\\op_n)}{n\\theta}}\\sum_{e\\in\\pair } \\e\\left\\{\\hx_e(\\by_{-e})\\log \\left[\\frac{\\sum_{x'_e}p_e(+1|x'_e )      p_{x_e|\\by_{-e}}(x'_e|\\by_{-e})}{\\sum_{x'_e}p_e(-1|x'_e )      p_{x_e|\\by_{-e}}(x'_e|\\by_{-e})}\\right]\\right\\}\\nonumber\\\\ & -\\frac{1}{4}\\sqrt{\\frac{\\op_n(1-\\op_n)}{n\\theta}}\\binom{n}{2}\\log\\big\\{\\frac{p_e(+|+)p_e(-|-)}{p_e(+|-)p_e(-|+)}\\big\\}\\ , .",
    "\\label{eq : dh } \\ ] ] notice that , letting @xmath473 , @xmath474 since have @xmath475}\\to 0 $ ] , and @xmath476 , we obtain the following bounds by taylor expansion @xmath477-b_0-",
    "\\frac{\\delta_n}{\\op_n(1-\\op_n)}\\,\\hx_e(\\by_{-e})\\right|\\le c\\ ,    \\frac{\\theta}{\\op_n(1-\\op_n)n}\\ , , \\\\",
    "\\left|\\log\\big[\\frac{p_e(+|+)p_e(-|-)}{p_e(+|-)p_e(-|+)}\\big]- \\frac{2\\delta_n}{\\op_n(1-\\op_n)}\\right|\\le c\\ ,    \\frac{\\theta}{\\op_n(1-\\op_n)n}\\ , , \\ ] ] where @xmath478 and @xmath64 will denote a numerical constant that will change from line to line in the following .",
    "such bounds hold for all @xmath467 $ ] provided @xmath466 .    substituting these bounds in eq .",
    "( [ eq : dh ] ) and using @xmath479 , after some manipulations , we get @xmath480 we now define ( with a slight overloading of notation ) @xmath481 , and relate @xmath482 to the overall conditional expectation @xmath483 . by bayes formula",
    "we have @xmath484 rewriting this identity in terms of @xmath483 , @xmath485 , we obtain @xmath486 using the definition of @xmath487 , we obtain @xmath488 this in particular implies @xmath489}$ ] . from eq .",
    "( [ eq : hxe ] ) we therefore get ( recalling @xmath490 ) @xmath491 substituting this in eq .",
    "( [ eq : dhfinal ] ) , we get @xmath492 finally we rewrite the sum over @xmath439 explicitly as sum over @xmath493 and recall that @xmath494 to get @xmath495 since @xmath103 is equivalent to @xmath103 ( up to a change of variables ) and @xmath496 , with @xmath497 is independent of @xmath423 , this is equivalent to our claim ( recall the definition of @xmath498 , eq .",
    "( [ eq : mmse_2vert ] ) ) .      from lemma [ lemma : differentiationsbm ] and theorem [ thm : main ] , we obtain , for any @xmath499 , @xmath500 from lemma [ lem : fixedpt ] and [ lem : psiintegral ] @xmath501",
    "y.d . and a.m. were partially supported by nsf grants ccf-1319979 and dms-1106627 and the afosr grant fa9550 - 13 - 1 - 0036 .",
    "part of this work was done while the authors were visiting simons institute for the theory of computing , uc berkeley .",
    "let us begin with the upper bound on @xmath502 . by using @xmath503 in eq .",
    "( [ eq : vmsedef ] ) , we get @xmath504 where the equality on the second line follows because @xmath505 is distributed as @xmath506 .",
    "the last inequality yields the desired upper bound @xmath507 .    in order to prove the lower bound on @xmath502 assume , for the sake of simplicity , that the infimum in the definition ( [ eq : vmsedef ] ) is achieved at a certain estimator @xmath508 .",
    "if this is not the case , the argument below can be easily adapted by letting @xmath509 be an estimator that achieves error within @xmath214 of the infimum .    under this assumption , we have ,",
    "from ( [ eq : vmsedef ] ) , @xmath510 where the last identity follows since the minimum over @xmath511 is achieved at @xmath512 .",
    "consider next the matrix minimum mean square error .",
    "let @xmath513}$ ] an optimal estimator with respect to @xmath502 , and define @xmath514 using eq .",
    "( [ eq : mmsedef ] ) and the optimality of posterior mean , we obtain @xmath515}\\e\\big\\{\\big[x_ix_j-\\hx_{ij}(\\bg)\\big]^2\\big\\}\\\\ & = \\frac{1}{n^2}\\e\\big\\{\\big\\|\\bx\\bx^{\\st}-\\beta(\\bg)\\hbx(g)\\hbx^{\\st}(\\bg)\\big\\|_f^2\\big\\}\\\\ & = \\e\\big\\{1-    \\frac{2\\beta(\\bg)}{n^2}\\,\\<\\bx,\\hbx(\\bg)\\>^2+\\frac{\\beta(\\bg)^2}{n^2}\\,\\|\\hbx(\\bg)\\|_2 ^ 4\\big\\}\\\\ & = 1    -\\e\\left(\\frac{\\<\\bx,\\hbx(\\bg)\\>^2}{n\\|\\hbx(\\bg)\\|_2 ^ 2}\\right)^2\\ , .",
    "\\label{eq : boundvmse2 } \\ ] ] the desired lower bound in eq .",
    "( [ eq : vmsebounds ] ) follows by comparing eqs .",
    "( [ eq : boundvmse1 ] ) and ( [ eq : boundvmse2 ] ) .      we shall assume , for the sake of simplicity , that the infimum in the definition of @xmath502 , see eq .",
    "( [ eq : vmsedef ] ) is achieved for a given estimator @xmath181 .",
    "if this is not the case , the proof below is easily adapted by considering an approximately optimal estimator .",
    "we then define @xmath516 by letting @xmath517 notice that @xmath518 . also by the proof in previous section ,",
    "( [ eq : boundvmse1 ] ) , we have @xmath519 and therefore ( since @xmath520 ) @xmath521 next consider the definition of overlap ( [ eq : overlapdef ] ) .",
    "consider the randomized estimator @xmath522 defined by letting @xmath523}$ ] with @xmath524 independently across @xmath525 $ ] .",
    "( formally , @xmath526 with @xmath527 a probability space , but we prefer to avoid unnecessary technicalities . )",
    "we then have , by central limit theorem @xmath528 with the @xmath529 uniform in @xmath530 .",
    "this yields the desired lower bound since , by dominated convergence , @xmath531",
    "we prove the claim for @xmath532 ; the other claim follows from an identical argument .",
    "since @xmath533 , we have by triangle inequality , that @xmath534 . applying bernstein inequality to the sum @xmath535 of random variables bounded by 1 : @xmath536 setting @xmath537 for large enough @xmath64 yields the required result .",
    "let us start from point @xmath125 , .",
    "since @xmath538 , it is sufficient to prove this claim for @xmath539 where , for the rest of the proof , we keep @xmath540 .",
    "we start by noting that , for all @xmath541 , @xmath542 this identity can be proved using the fact that @xmath543 .",
    "indeed this yields @xmath544 where the first and last equalities follow by symmetry .    differentiating with respect to @xmath146 ( which can be justified by dominated convergence ) : @xmath545 now applying stein s lemma ( or gaussian integration by parts ) : @xmath546 using the trigonometric identity @xmath547 , the shorthand @xmath548 and identity above : @xmath549 now , let @xmath550 , whereby we have @xmath551    note now that @xmath552 satisfies @xmath54 @xmath552 is even with @xmath553 , @xmath56 @xmath552 is continuously differentiable and @xmath554 @xmath552 and @xmath555 are bounded . consider the function @xmath556 , where @xmath557 .",
    "we have the identities : @xmath558 hence , to prove that @xmath559 is concave on @xmath281 , it suffices to show that @xmath560 , @xmath561 are non - positive for @xmath562 . by properties @xmath56 and @xmath554 above we can differentiate @xmath563 with respect to @xmath564 and interchange differentiation and expectation .      computing the derivative with respect to @xmath569 yields @xmath570 where the last line follows from the fact that @xmath571 is odd and @xmath572 is even in @xmath573 . consequently @xmath574 since @xmath575 and @xmath576 for @xmath577 , the integrand is negative and we obtain the desired result .",
    "for any random variable @xmath578 we have @xmath579 since @xmath580 ( given @xmath88 there are exactly @xmath73 possible choices for @xmath50 ) , this implies @xmath581 the claim ( [ eq : ixx ] ) follows by applying the last inequality once to @xmath582 and once to @xmath583 and taking the difference .      for the second claim",
    ", we prove that @xmath586 where @xmath587 as @xmath588 , whence the claim follows since @xmath497 .",
    "we claim that we can construct an estimator @xmath589 and a function @xmath590 with @xmath591 , such that , defining @xmath592 then we have @xmath593 to prove this claim , it is sufficient to consider @xmath594 where @xmath595 is the principal eigenvector of @xmath103",
    ". then @xcite implies that , for @xmath596 , almost surely , @xmath597 hence the above claim holds , for instance , with @xmath598 .",
    "then expanding @xmath599 with the chain rule ( whereby @xmath600 ) , we get : @xmath601 since @xmath95 is a function of @xmath602 , @xmath603 .",
    "furthermore @xmath604 since @xmath95 is binary .",
    "hence : @xmath605 when @xmath606 , @xmath50 differs from @xmath607 in at most @xmath608 positions , whence @xmath609 .",
    "when @xmath610 , we trivially have @xmath611 . consequently : @xmath612 the second claim then follows by dividing with @xmath20 and letting @xmath17 on the right hand side .",
    "by definition , we have : @xmath613 define a related sequence @xmath614 as follows : @xmath615 } f'_t(s_i^t + \\mu_t x_i ,    x(\\eps)_i)\\ , , \\\\",
    "\\bs^0 & = \\bx^0 + \\mu_0\\bx\\ , . \\ ] ] here @xmath261 is defined via the state evolution recursion : @xmath616 we call a function @xmath617 is pseudo - lipschitz if , for all @xmath618 @xmath619 where @xmath620 is a constant . in the rest of the proof , we will use @xmath620 to denote a constant that may depend on @xmath236 and but not on @xmath20 , and can change from line to line .",
    "we are now ready to prove lemma [ lem : stateevollem ] . since the iteration for @xmath621 is in the form of @xcite , we have for any pseudo - lipschitz function @xmath622 :",
    "@xmath623 letting @xmath624 , this implies that , almost surely : @xmath625 it then suffices to show that , for any pseudo - lipschitz function @xmath626 , almost surely : @xmath627 = 0\\ , .",
    "\\ ] ]    we instead prove the following claims that include the above . for any @xmath236 fixed , almost surely : @xmath628 & = 0,\\label{eq : induc0}\\\\     \\lim_{n\\to \\infty}\\frac{1}{n}\\,\\|\\bdelta^t\\|_2 ^ 2   & = 0 , \\label{eq : induc1}\\\\    \\limsup_{n\\to\\infty } \\frac{1}{n}\\,\\|\\bs^t +    \\mu_t \\bx\\|_2 ^ 2 & < \\infty\\ , , \\label{eq : induc2 } \\ ] ] where we let @xmath629 .",
    "we can prove this claim by induction on @xmath236 .",
    "the base case of @xmath630 is trivial for all three claims : @xmath631 and @xmath632 is satisfied by our initial condition @xmath633 , @xmath634 .",
    "now , assuming the claim holds for @xmath635 we prove the claim for @xmath236 .    by the pseudo - lipschitz property and triangle inequality ,",
    "we have , for some @xmath620 : @xmath636 consequently : @xmath637}\\right\\rvert } & \\le    \\frac{l}{n } \\sum_{i=1}^n\\left ( { \\left\\lvert{\\delta^t_i}\\right\\rvert } +   { \\left\\lvert{s^t_i + \\mu_t x_i}\\right\\rvert } { \\left\\lvert{\\delta^t_i}\\right\\rvert } + { \\left\\lvert{\\delta^t_i}\\right\\rvert}^2\\right)\\\\ & \\le \\frac{l}{n } \\big(\\|\\bdelta^t\\|_2 ^ 2 + \\sqrt{n}\\|\\bdelta^t\\|_2 + \\|\\bdelta^t\\|_2\\|\\bs^t+\\mu_t\\bx\\|_2\\big)\\ , . \\ ] ] hence the induction claim eq . at @xmath236",
    "follows from claims eq .",
    "and eq . at @xmath236",
    "now , with the standard inequality @xmath643 : @xmath644 using the fact that @xmath645 are lipschitz : @xmath646 by the induction hypothesis , ( specifically @xmath647 at @xmath648 , wherein it is immediate to check that @xmath649 is pseudo - lipschitz by the boundedness of @xmath650 ) : @xmath651 thus the first term in eq . vanishes .",
    "for the second term to vanish , using the induction hypothesis for @xmath652 , it suffices that almost surely : @xmath653 this follows from standard eigenvalue bounds for wigner random matrices @xcite . for the third term in eq . to vanish , we have by @xcite that : @xmath654 hence it suffices that @xmath655 a.s . , for which we expand their definitions to get : @xmath656.\\end{aligned}\\ ] ] by assumption , @xmath657 is lipschitz and we can apply the induction hypothesis with @xmath658 to obtain that the limit vanishes .",
    "indeed , by a similar argument @xmath659 is bounded asymptotically in @xmath20 , and so is @xmath660 .",
    "along with the induction hypothesis for @xmath661 this implies that the fourth term in eq .",
    "asymptotically vanishes .",
    "this establishes the induction claim eq ..",
    "e.  abbe , a.  s. bandeira , a.  bracher , and a.  singer , _ decoding binary node labels from censored edge measurements : phase transition and efficient recovery _ , ieee transactions on network science and engineering * 1 * ( 2014 ) , no .  1 .",
    "e.  abbe , a.s .",
    "bandeira , a.  bracher , and a.  singer , _ linear inverse problems on erds - rnyi graphs : information - theoretic limits and efficient recovery _",
    ", information theory ( isit ) , 2014 ieee international symposium on , june 2014 , pp .",
    "12511255 .",
    "mireille capitaine , catherine donati - martin , and delphine fral , _ the largest eigenvalues of finite rank deformation of large wigner matrices : convergence and nonuniversality of the fluctuations _ , the annals of probability * 37 * ( 2009 ) , no .  1 , 147 .                                                          cyril masson , andrea montanari , thomas  j richardson , and rdiger urbanke , _ the generalized area theorem and some of its consequences _ , information theory , ieee transactions on * 55 * ( 2009 ) , no .  11 , 47934821 .",
    "andrea montanari and david tse , _ analysis of belief propagation for non - linear problems : the example of cdma ( or : how to prove tanaka s formula ) _ , information theory workshop , 2006 .",
    "itw06 punta del este .",
    "ieee , ieee , 2006 , pp ."
  ],
  "abstract_text": [
    "<S> we develop an information - theoretic view of the stochastic block model , a popular statistical model for the large - scale structure of complex networks . </S>",
    "<S> a graph @xmath0 from such a model is generated by first assigning vertex labels at random from a finite alphabet , and then connecting vertices with edge probabilities depending on the labels of the endpoints . in the case of the symmetric two - group model </S>",
    "<S> , we establish an explicit ` single - letter ' characterization of the per - vertex mutual information between the vertex labels and the graph .    the explicit expression of the mutual information is intimately related to estimation - theoretic quantities , and in particular reveals a phase transition at the critical point for community detection . below the critical point </S>",
    "<S> the per - vertex mutual information is asymptotically the same as if edges were independent . </S>",
    "<S> correspondingly , no algorithm can estimate the partition better than random guessing . </S>",
    "<S> conversely , above the threshold , the per - vertex mutual information is strictly smaller than the independent - edges upper bound . in this regime </S>",
    "<S> there exists a procedure that estimates the vertex labels better than random guessing . </S>"
  ]
}