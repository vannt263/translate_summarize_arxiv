{
  "article_text": [
    "the problem of devising global models of nuclidic ( atomic ) masses has a long history , going back to the early work of bohr , von weizscker and bethe based on the liquid drop model ( see refs .",
    "@xcite for reviews ) .",
    "the principal objectives are ( i ) a fundamental understanding of the physics of the mass surface and ( ii ) the prediction of the masses of `` new '' nuclides far from stability , both in the superheavy region and in the regions approaching the proton and neutron drip lines .",
    "the actual predictions for the masses are of great current interest in connection with present and future experimental studies of nuclei far from stability , conducted at heavy - ion and radioactive ion - beam facilities @xcite .",
    "the results are also useful for such astrophysical problems as nucleosynthesis and supernova explosions .",
    "the spectrum of models of the atomic mass table ranges from those with high theoretical input that take explicit account of known physical principles in terms of a relatively small number of fitting parameters , to models that are shaped mostly by the data and very little by theory and thus have a correspondingly larger number of adjustable parameters .",
    "epitomizing models of the former class are the macroscopic / microscopic models of mller , nix , and coworkers @xcite , and the semi - microscopic models of pearson , tondeur , and coworkers @xcite .",
    "the models of mller et al .",
    "appeal to the macroscopic descriptions provided by the liquid drop and droplet models , solve a one - body schrdinger equation to incorporate single - particle degrees of freedom , and include pairing through semi - microscopic calculation .",
    "a prominent version , which sets the standard for state - of - the - art theory - based models , is the finite - range droplet model ( frdm ) detailed in ref .",
    "the models of pearson , tondeur , and coworkers are based on the hartree - fock method , with pairing correlations described by either a bcs or bogolyubov treatment .",
    "the current version , namely the hfb2 model @xcite , features the bogoliubov approach and an improved skyrme force .    in this work we use neural networks to develop global nuclear mass models which are situated far toward the other end of the spectrum , where one ( in the ideal ) seeks to determine the degree to which the entire mass table is determined by the existing experimental data , and only the data . during the last decade ,",
    "artificial neural networks have been utilized to construct predictive statistical models in a variety of scientific problems ranging from astronomy to experimental high - energy physics to protein structure @xcite . in a typical application ,",
    "a multilayer feedforward neural network is trained with back - propagation or some other supervised training algorithms @xcite so as to create a `` predictive '' statistical model of a certain input - output mapping , which may in general be physical or mathematical in character .",
    "information contained in a set of learning examples of the input - output association is embedded in the weights of the connections between the layered units .",
    "this information may ( or may not ) be sufficient to allow the trained network to make reliable predictions for examples outside the learning set . at any rate ,",
    "the network is taught to generalize ( well or poorly ) , based on what it has learned from the set of examples . in the more mundane language of function approximation , the neural - network model provides a means for interpolation or extrapolation .",
    "nuclear physics offers especially rich territory for `` data mining '' with neural nets . on the one hand , a huge collection of high - quality experimental data is available for diverse properties of more than 2000 nuclides . on the other , quantitative calculation of some properties of some classes of nuclei",
    "presents difficult challenges even for the best _ ab initio _ quantum - mechanical theories and phenomenological macroscopic / microscopic models . to date",
    ", global neural - network models have been developed for the stability / instability dichotomy , for the atomic - mass table , for neutron separation energies , for spins and parities , for decay branching probabilities of nuclear ground states , and for @xmath0 decay half - lives @xcite .    in the work to be described here ,",
    "neural nets have been trained to predict the nuclear mass excess or `` defect '' @xmath1 , continuing the program established in refs .",
    "@xcite . in sec .  2",
    ", we outline the training methodology that has been applied and specify the data sets used in the modeling process .",
    "the results are presented and discussed in sec .",
    "3 . finally ,",
    "sec .  4 states the general conclusions of the current study and views the prospects for further successes and further improvements in statistical prediction of atomic masses .",
    "our immediate tasks are to specify ( i ) the structure and unit - dynamics of the networks that will be developed to model the mass data and ( ii ) the algorithm for their training . we must also specify ( iii ) the data sets to be utilized together with ( iv ) the schemes for encoding and decoding input and output data .      a multilayer feedforward architecture is adopted , with various numbers of hidden layers and distributions of units among layers .",
    "the gross architecture of a given net is summarized in the notation ( @xmath2@xmath3@xmath4 ... @xmath5@xmath6)[@xmath7 , where @xmath8 is the total number of weight / bias parameters and @xmath2 , @xmath9 , and @xmath6 are integers that indicate , respectively , the numbers of neuron - like units in the input layer , the @xmath10th intermediate ( or `` hidden '' ) layer , and the output layer . unless otherwise indicated , each unit in a layer is connected to all units to the next layer .",
    "the connection from unit @xmath11 to unit @xmath12 is characterized by a real - number weight @xmath13 with initial value positioned at random in the range @xmath14 $ ] .    when a pattern @xmath15 is impressed on the input interface , the activities of the input units are set in accordance to the coding scheme assumed ( see section 2.4 . ) .",
    "each unit in a hidden layer or in the output layer receives a stimulus @xmath16 , where the @xmath17 are the activities of the units in the immediately preceding layer .",
    "the activity of generic unit @xmath11 in the hidden or output layers is in general a nonlinear function of its stimulus , @xmath18 . in our work , the activation function @xmath19 is taken to have the logistic form , @xmath20^{-1}$ ] .",
    "the system response may be decoded from the activities of the units of the output layer also in accordance to the coding scheme assumed .",
    "the dynamics is particularly simple : the states of all units within a given layer are updated in parallel and the layers are updated successively , proceeding from input to output .",
    "several training algorithms exist that seek to minimize the cost function with respect to the network weights .",
    "for the cost function we make the traditional choice of the sum of squared errors calculated over the learning set , or more specifically @xmath21 where @xmath22 and @xmath23 denote , respectively , the target and actual activities of unit @xmath10 of the output layer for input pattern ( or example ) @xmath24 . in global modeling of the atomic mass table ,",
    "the root - mean - square error  has been widely adopted as the key figure of merit , and we shall use its value , calculated over various data sets , to assess the performance of neural - network models . when evaluated over the learning set ,",
    "this quantity evidently coincides with @xmath25 , where @xmath26 is the number of training examples .",
    "the most familiar training algorithm is standard back - propagation @xcite ( hereafter often denoted sb ) , according to which the weight update rule to be implemented upon presentation of pattern @xmath24 is @xmath27 where @xmath28 is the learning rate @xmath29 , @xmath30 is the momentum parameter @xmath31 , and @xmath32 is the pattern impressed on the input interface one training step earlier . the second term on the right - hand side of eq .",
    "( [ standardupdate ] ) , called the momentum term , serves to damp out the wild oscillations in weight space that might otherwise occur during the gradient - descent minimization process that underlies the back - propagation algorithm . in our implementation of the sb algorithm ,",
    "the learning rate and momentum parameters remain constant at @xmath33 and @xmath34 respectively .",
    "our artificial neural networks are trained with a modified version of the sb algorithm @xcite that we have found empirically to be advantageous in the mass - modeling problem . in this new algorithm ,",
    "to be denoted mb , the weight update prescription corresponding to eq .",
    "( [ standardupdate ] ) reads @xmath35 the momentum term being modified through the quantity @xmath36 in the latter expression , @xmath37 is the number of the current epoch , with @xmath38 .",
    "an epoch consists of @xmath26 pattern presentations , the patterns being chosen at random from the set of @xmath26 training examples .",
    "many epochs of training are required to achieve acceptable performance in the problem at hand .",
    "the variable @xmath39 entering eq .",
    "( [ mbupdate ] ) is initialized as follows : for the first pattern to be presented in epoch @xmath40 , it is set equal to zero . at the beginning of each new epoch ( @xmath41 )",
    ", it is taken equal to the value reached by @xmath42 at the end of the immediately preceding epoch .",
    "the replacement of @xmath43 by @xmath44 in the update rule for the generic weight @xmath13 allows earlier patterns of the current epoch to have more influence on the training than is the case for standard back - propagation . by the time @xmath37 becomes large , @xmath44 is effectively zero .",
    "it can be shown , after rather lengthy algebra , that if a plateau region of the cost surface has been reached ( i.e. @xmath45 remains almost constant ) and @xmath37 is relatively large , then eq .",
    "( [ mbupdate ] ) converges to @xmath46 thus achieving an effective learning rate twice that of the sb algorithm ( cf .",
    "@xcite ) .        in training neural networks , there is always the question of when to stop the process .",
    "if the network is trained for too short a period , the training data will not be adequately fitted . on the other hand ,",
    "if the training period is continued for too long , generalization ( i.e. prediction ) will suffer , since the `` overtrained '' network will be specialized to the idiosyncrasies of the particular learning set that has been supplied .",
    "thus some reasonable compromise must be struck between the _ desiderata _ of a good fit and good prediction . in our computer experiments ,",
    "we have adopted the following criterion .",
    "a given training run consists of a relatively large number of epochs , specified beforehand . during such a run , we monitor not only the cost function for the patterns in the learning set , but also for a separate _ validation set _ of nuclei whose masses are known .",
    "the `` trained '' network model resulting from a given run is taken as that network with the set of connection weights producing the smallest value of the cost function on the validation set , over the full course of the run .",
    "while the members of the validation set are _ not _ used in the weight updates of the mb ( or sb ) training rule , they clearly do affect the choice of model .",
    "therefore , accuracy on the validation set can not strictly be regarded as a measure of predictive performance , although in practice it may still provide a useful indicator of this aspect of the model . to obtain a clean measure of predictive performance , still a third set of examples",
    "is needed : a _ test ( prediction ) set _ that is never referred to during the training process .",
    "another general problem that one faces in training neural networks is that of the optimal architecture in terms of the numbers of layers and the numbers of units in each layer . as in most neural - network applications ,",
    "we have simply followed a `` trial - and - error '' approach to this problem ; certainly no claim can be made that a full optimization has been achieved .",
    "numerical experiments have shown that performance can be improved by modifying the learning rate @xmath28 and momentum parameter @xmath30 during training . in applying the mb algorithm , @xmath28 and @xmath30",
    "are assigned starting values 0.5 and 0.9 , respectively , and the validation error is calculated every five epochs .",
    "if this error decreases for two or more consecutive evaluations , the learning rate @xmath28 ( with @xmath47 is increased by 0.02 ; otherwise it is decreased by 0.005 .",
    "the momentum parameter @xmath30 is usually set to @xmath48 when @xmath37 becomes relatively large .",
    "comparative studies of the mass - modeling problem ( to be summarized in table 1 of section 3 ) demonstrate that this training procedure , in conjunction with the mb update rule ( [ mbupdate])([sterm ] ) , generally yields better results than does the sb algorithm .",
    "the evolution of the learning error under application of the mb algorithm is illustrated by the sample shown in fig .",
    "we emphasize that this algorithm departs from gradient descent , allowing the network to escape from local minima .",
    "in exploring the prospects for statistical modeling of nuclear mass excesses , we have primarily employed a database o+n made up of ( i ) @xmath49 `` old '' ( o ) experimental masses which the 1981 mller - nix theoretical model @xcite was designed to reproduce , together with ( ii ) @xmath50 `` new '' ( n ) experimental masses , measured subsequently for nuclei that lie mostly beyond the edges of the 1981 data collection when viewed in the @xmath51 plane .",
    "as discussed in ref .",
    "@xcite , the o and n data sets were selected as part of a strategy for quantifying the extrapolation capability ( `` extrapability '' ) of global mass models ",
    "i.e. , their ability to predict the atomic masses for nuclides far from stability .",
    "these sets have also been used in evaluating the predictive performance of neural network models of the atomic mass function , with set o providing the fitting data ( learning set ) and set n the target data for prediction ( test set ) ( e.g. , see ref .",
    "@xcite ) .    to further characterize the interpolation / extrapolation capability of our models ,",
    "we have also employed two data sets of @xmath52 ( m1 ) and @xmath50 ( m2 ) nuclei and their masses , chosen _",
    "randomly _ from the union of the o and n sets , after excluding 20 nuclides with poorly measured masses .",
    "together , these @xmath53 cases form the database fitted by the frdm parametrization of ref .",
    "@xcite , i.e. , by the best of the mid-90 s theoretical models developed by the los alamos - berkeley group .",
    "we also make use of another set of @xmath54 nuclei ( denoted nb ) that lie outside the o and n databases , the experimental masses of these examples being drawn from the nubase evaluation of nuclear and decay properties @xcite .",
    "the locations of the nuclei of the o , n , and , nb sets in the @xmath51 plane are shown in fig .  2 .",
    "we have considered several input coding schemes designed to facilitate learning of quantal properties ( pairing , shell structure ) that depend on the integral nature of @xmath55 and @xmath56 ( see refs .",
    "the scheme that achieves this aim most efficiently while keeping the number of weights to a minimum is one that implements analog ( floating - point ) coding of @xmath55 and @xmath56 in terms of the inputs of only two dedicated analog input neurons , which , however , are aided by two further binary ( `` on - off '' ) input units that encode the parity ( even or odd ) of @xmath55 and @xmath56 @xcite .",
    "the analog input units scale the @xmath55 and @xmath56 values to the interval [ 0,1 ] in such a manner that the stimuli received by the logistic units in hidden and output layers remain within their best dynamical range . a less efficient scheme , introduced in ref .",
    "@xcite , utilizes banks of on - off units to represent @xmath55 and @xmath56 as binary integers .",
    "the mass excess computed by the network is represented by the activity of a single analog output unit .",
    "for the same reason as for the input units representing @xmath55 and @xmath56 , the target mass excess values @xmath1 are also scaled to the interval [ 0,1 ] .",
    "several prescriptions have been tried for scaling the @xmath55 , @xmath56 , and @xmath1 variables , two of which are represented in the results reported here .",
    "extensive comparative studies of the mb and sb training algorithms were based on the p1 prescription , which admits the ranges @xmath57 $ ] , @xmath58 $ ] , and @xmath59 $ ] for the variables @xmath55 , @xmath56 , and @xmath1 , respectively . in later work",
    "we adopted the p2 prescription , which allows for the extended respective ranges @xmath60 $ ] , @xmath61 $ ] , and @xmath62 $ ] , thereby providing ample room for new nuclei far from the stable valley .",
    "the latter scaling recipe usually gives better results .",
    "we first present results of a comparative study of the quality of models generated with the modified back - propagation training algorithm mb and with the standard back - propagation routine sb .",
    "seven pairs of models were constructed , one member of each pair being trained with mb and the other with sb , and both members of the pair being started from the same choice among seven different sets of random initial weights .",
    "all of these models have architecture ( @xmath63@xmath64@xmath64@xmath64@xmath65)[281 ] and employ the p1 scaling recipe . in all seven cases , the values of the error measures  attained by the mb algorithm for the learning , validation , and test sets are consistently smaller than the corresponding values achieved with sb . a similar pattern is expected to hold for the p2 scaling prescription .",
    ".performance comparison of standard back - propagation ( sb ) and modified back - propagation ( mb ) training algorithms in the task of mass modeling .",
    "results for the rms error  on learning , validation , and test sets are given for seven pairs of models with network architecture ( @xmath63@xmath64@xmath64@xmath64@xmath65)[281 ] .",
    "the members of each model pair belong to the same choice among seven initial sets of random weights .",
    "all models have been trained for a total of 20,000 epochs using the p1 scaling recipe defined in section 2.4 . [",
    "cols=\"^,^,^,^,^\",options=\"header \" , ]     the generalization ability ( extrapability ) of models belonging to the first group , whether statistical or theoretical , was assessed by treating the n data set as a test set . in this way , results obtained by the neural - network approach could be directly compared with the results @xcite of the extrapability study carried out by the los alamos - berkeley group for the frdm approach ( and especially with the rms error values shown in the first row of table 2 ) .",
    "the first of the network models selected from earlier studies is of the five - layer architecture ( @xmath66@xmath64@xmath64@xmath64@xmath65)[421 ] ; it was constructed by gernoth et  al .",
    "@xcite using standard back - propagation , with binary encoding of @xmath55 and @xmath56 and `` redundant '' analog encoding of the atomic mass number @xmath67 and the neutron excess @xmath51 .",
    "the three - layer network ( @xmath63@xmath68@xmath65)[245 ] is due to kalman , who adopted analog coding of @xmath55 and @xmath56 and auxiliary parity units for these variables . in training this model ,",
    "the input patterns were pre - processed by singular - value decomposition and the cost function minimized by a powell - update conjugate - gradient algorithm ( for additional details , see refs .",
    "@xcite ) .    the network model labeled with one asterisk was one of those created mainly for evaluating the modified training procedure mb .",
    "it has the five - layer architecture ( @xmath63@xmath64@xmath64@xmath64@xmath65)[281 ] and employs parity - aided input coding and the scaling recipe p1 .",
    "utilization of the nb data for validation gives this model an _ a priori _ advantage over the earlier network models , for which only the learning set was involved in the training process .",
    "this advantage is clearly realized in practice .",
    "also included in the first group is a set of results @xcite obtained recently with a support vector machine @xcite .",
    "the network models appearing in the second group employ the scaling recipe p2 and were trained with the mixed data sets m1 and m2 described in section 2.3 .",
    "thus , the training and validation examples include , in this case , members from both the o and n data sets .",
    "the intent was to develop statistical models that can be compared more directly with the most refined frdm model of ref .",
    "@xcite , recalling that the parameters of this model were fit to the 1654 examples of the m1+m2 database .",
    "the network model marked with two asterisks has the same five - layer architecture and number of weight parameters as the ( * ) network . in the network model marked with three asterisks , we chose to introduce connections from the analog input units to all units of all the hidden layers , to avoid or reduce the degradation of information as it propagates toward the output unit .",
    "however , this innovation comes at the expense of increasing the number of weight parameters from the @xmath69 of the ( * * ) network to @xmath70 .",
    "the resulting system is the best neural - network model of the mass table yet achieved , based on the accuracy of the estimated masses of the nb set of nuclides , taken as the test set .",
    "the corresponding rms error is 0.95 mev , which is to be compared with the figure 0.70 mev obtained in the frdm evaluation .",
    "also included in the third group are the corresponding rms errors for the hfb2 model @xcite .",
    "the parameters of this model have been adjusted to an extended data set of @xmath71 nuclei , which however includes the @xmath54 nuclei of the nb set .",
    "top panel : deviations from experiment ( in mev ) of mass - excesses values predicted by the neural - network model ( @xmath63@xmath64@xmath64@xmath64@xmath65)@xmath72 for the nubase ( nb ) nuclei identified in fig .  2 .",
    "the plot represents a projection of the mass surface onto a plane of constant @xmath55 and thus shows dependence on neutron number @xmath56 .",
    "bottom panel : same for the frdm evaluation @xcite . ]",
    "further information on the performance of the ( * * * ) network is furnished in fig .",
    "here we compare the deviations from experimental data of the mass - excess values generated by the net and by the frdm evaluation , for the nb nuclei .",
    "the extrapolation capability of the ( * * * ) network model is better illustrated in fig .  4 , which shows these deviations as a function of the number of neutrons away from the @xmath73 stability line .",
    "top panel : deviations from experiment ( in mev ) of mass - excesses values predicted by the neural - network model ( @xmath63@xmath64@xmath64@xmath64@xmath65)@xmath72 for the nubase ( nb ) nuclei identified in fig .  2 , as a function of the number of neutrons away from the line of @xmath73 stability .",
    "bottom panel : same for the frdm evaluation @xcite . ]    in spite of its residual shortcomings , the current generation of neural - network models of the mass table represents a significant step toward extrapability levels comparable with those reached by the best traditional global models rooted in quantum theory . the ultimate test of any class of global mass models is the accuracy that can be realized in the prediction of masses of nuclear species prior to measurement .",
    "a text data file containing the mass - excess values predicted by the ( * * * ) network for 7709 nuclides ( @xmath74 , @xmath75 , @xmath76 ) is available for downloading from _ http://www.cc.uoa.gr/@xmath77sathanas/mass_excess _ ( see file `` massfiles.txt '' for details ) .",
    "the present investigation is a continuation and an elaboration of a research thrust @xcite that seeks to develop accurate global models of nuclear properties with demonstrable predictive power , within the arena of statistical methods based on multilayer neural networks .",
    "our particular concern has been the central problem of modeling the systematics of nuclear mass excesses .",
    "we have introduced a modified back - propagation algorithm ( mb ) along with new prescriptions for encoding and decoding input and output patterns .",
    "the study has been based mainly on data sets selected by mller and nix @xcite for the purpose of testing the extrapolation capabilities of global models of atomic masses . as seen in table 2 , our best network models of the nuclear mass excess @xmath1 display substantially improved performance relative to earlier attempts that use neural networks to predict masses far from the valley of @xmath0 stability . a strong impetus for further improvement of this approach comes from the production of new nuclei at radioactive beam facilities and heavy - ion colliders , as well as by the needs of supernova modeling and state - of - the - art theories of nucleosynthesis .",
    "in closing , we would like to emphasize the conceptual and structural differences between    * statistical models of the atomic - mass function constructed with the aid of learning rules operating purely on the experimental data , without any overt imposition of physical principles and theory , and * the familiar theory - based phenomenological models , constrained by the data .",
    "although the latter models become more elaborate as the standards of description increase , they are nevertheless relatively compact , having relatively few adjustable parameters , with transparent physical meaning . by contrast , the neural - network methodology is a more abstract kind of `` engine '' that generates a statistical representation of the experimental data having many parameters .",
    "while this representation may have strong predictive power , its parameters are ordinarily ( though not always ) opaque to physical interpretation . in view of these fundamental differences",
    ", the two approaches should more fruitfully be viewed as complementary , rather than in competition .",
    "we are currently exploring and implementing a number of refinements of neural - network approaches to the mass problem .",
    "these include the introduction of diverse pruning and network construction schemes and the application of other more powerful training ( optimization ) procedures .",
    "additionally , we have made some initial attempts to construct an informative statistical model of the _ differences _ between the experimental mass - excess values @xmath78 and the theoretical values @xmath79 given by the frdm model of mller et  al .",
    "this study is being pursued with the hope of revealing subtle regularities of nuclear structure not yet embodied in the best microscopic / phenomenological models of atomic - mass systematics . to date , the results have not been illuminating in terms of the emergence of systematic trends  a tentative finding which , if sustained , could imply that the residual physical corrections to the theoretical model are small but numerous , and of fluctuating size and sign .",
    "also under investigation is the potential of support vector machines @xcite for systematic development of near - optimal statistical models of atomic masses and other nuclear properties .",
    "the results reported in table 2 are suggestive of the power of this approach .",
    "after completing the training of the ( * * * ) network , the ame@xmath80 atomic mass evaluation @xcite was published .",
    "this compilation made available precision mass measurements for nuclei farther off the stability line , while providing corrected mass - excess values for nuclei already used in our study .",
    "the next generation of neural - network models will be trained using the ame@xmath80 data .",
    "already , however , we can further appraise the extrapability performance of the ( * * * ) network , the best neural - network model of the mass table yet achieved , by making use of @xmath81 new nuclei included in the ame@xmath80 evaluation , which extend beyond the edges of the @xmath53-nuclide set m1+m2 as viewed in the @xmath51 plane .",
    "a text data file containing the predicted mass - excess values for these @xmath81 additional nuclides is available for downloading from _",
    "mass_excess _ ( see file `` massfiles.txt '' for details ) .",
    "the resulting value of  for these nuclei is 1.03 mev , which is to be compared with the figures 0.58 mev and 0.67 mev obtained in the frdm and hfb2 evaluations .",
    "when comparing these results , it should be kept in mind that the parameters of the hbf2 model have been adjusted by making use of an extended data set of @xmath71 nuclei , which includes @xmath82 of the @xmath81 nuclides .      00",
    "p.  mller and j.  r.  nix , j.  phys .",
    "g 20 ( 1994 ) 1681 . c.  borcea and g.  audi , rom .",
    "j.  phys .",
    "38 ( 1993 ) 455 .",
    "g.  audi , o.  bersillon , j.  blachot , and a.  h.  wapstra , nucl .",
    "a624 ( 1997 ) 1 .",
    "m.  uno , riken review 26 ( 2000 ) 38 .",
    "j.  m.  pearson , hyperfine interactions 132 ( 2001 ) 59 .",
    "d.  lunney , j.  m.  pearson and , c.  thibault , rev .",
    "phys . 75 ( 2003 ) 1021 .",
    "opportunities in nuclear science : `` a long range plan in the next decade '' ( doe / nsf , 2002 ) ; `` nuclear physics in europe : highlights and opportunities '' ( nupecc , 1997 ) .",
    "p.  mller and j.  r.  nix , at .",
    "data  nucl .",
    "data tables 26 ( 1981 ) 165 .",
    "p.  mller and j.  r.  nix , at .",
    "data nucl .",
    "data tables 39 ( 1988 ) 213 ; p.  mller , w.  d.  myers , w.  j.  swiatecki , and j.  treiner , ibid .",
    "39 ( 1988 ) 225 .",
    "p.  mller , j.  r.  nix , w.  d.  myers , and w.  j.  swiatecki , at .",
    "data nucl .",
    "data tables 59 ( 1995 ) 185 .",
    "y.  aboussir , j.  m.  pearson , a.  k.  dutta and f.  tondeur , at .",
    "data nucl .",
    "data tables 61 ( 1995 ) 127 .",
    "s.  goriely , f.  tondeur and j.  m.  pearson , at .",
    "data nucl .",
    "data tables 77 ( 2001 ) 311 .",
    "( a ) m.  samyn , s.  goriely , p .- h .",
    "heenen , j.  m.  pearson and f.  tondeur , nucl .",
    "a700 ( 2002 ) 142 ; ( b ) s.  goriely , m.  samyn , p .- h .",
    "heenen , j.  m.  pearson and f.  tondeur , phys .",
    "c66 ( 2002 ) 024326 .",
    "v.  cherkassky , j.  h.  friedman , and w.  wechsler , eds . , from statistics to neural networks .",
    "theory and pattern recognition applications ( springer - verlag , berlin , 1994 ) .",
    "j.  w.  clark , t.  lindenau , and m.  l.  ristig , eds . ,",
    "scientific applications of neural nets ( springer - verlag , berlin , 1999 ) .",
    "j.  hertz , a.  krogh , and r.  g.  palmer , introduction to the theory of neural computation ( addison - wesley , redwood city , ca , 1991 ) .",
    "j.  w.  clark , phys .  med .",
    "36 ( 1992 ) 1259 .",
    "s.  haykin , neural networks : a comprehensive foundation ( mcmillan , new york , 1998 ) . c.  bishop , neural networks for pattern recognition ( clarendon , oxford , 1995 ) .",
    "s.  gazula , j.  w.  clark , and h.  bohr , nucl .",
    "a540 ( 1992 ) 1 .",
    "k.  a.  gernoth , j.  w.  clark , j.  s.  prater , and h.  bohr , phys .",
    "b300 ( 1993 ) 1 .",
    "k.  a.  gernoth and j.  w.  clark , neural networks 8 ( 1995 ) 291 .",
    "k.  a.  gernoth and j.  w.  clark , comp .",
    "88 ( 1995 ) 1",
    ". j.  w.  clark , in scientific applications of neural nets , j.  w.  clark , t.  lindenau , and m.  l.  ristig , eds .",
    "( springer - verlag , berlin , 1999 ) , p.  1 .",
    "k.  a.  gernoth , in scientific applications of neural nets , j.  w.  clark , t.  lindenau , and m.  l.  ristig , eds .",
    "( springer - verlag , berlin , 1999 ) , p.  139 .",
    "s.  athanassopoulos , msc thesis , university of athens , 1998 .",
    "e.  mavrommatis , a.  dakos , k.  a.  gernoth , and j.  w.  clark , condensed matter theories , vol .  13 , j.  da  providencia and f.  b.  malik , eds .",
    "( nova science publishers , commack , ny , 1998 ) p.  423",
    "e.  mavrommatis , s.  athanassopoulos , k.  a.  gernoth , and j.   w.  clark , condensed matter theories , vol .  15 , g.  s.  anagnostatos et al .",
    "( nova science publishers , commack , n.y . , 2000 )",
    "j.  w.  clark , e.  mavrommatis , s.  athanassopoulos , a.  dakos and k.  a.  gernoth in proceedings of the conference on `` fission dynamics of atomic clusters and nuclei '' , d.  m.  brink et al .",
    "( world scientific , singapore ) pp .  76 - 85 b.  l.  kalman , private communication",
    ". h. li , j. w. clark , s. athanassopoulos , e. mavrommatis , and k. a. gernoth , to be published .",
    "v. vapnik , the nature of statistical learning theory ( springer - verlag , heidelberg , 1995 ) . n. cristianini and john shawe - taylor , an introduction to support vector machines and other kernel - based learning methods ( cambridge university press , cambridge , 2002 ) .",
    "a.  h.  wapstra , g.  audi , and c.  thibault , nucl .",
    "a729 ( 2003 ) 337 ."
  ],
  "abstract_text": [
    "<S> new global statistical models of nuclidic ( atomic ) masses based on multilayered feedforward networks are developed . </S>",
    "<S> one goal of such studies is to determine how well the existing data , and only the data , determines the mapping from the proton and neutron numbers to the mass of the nuclear ground state . </S>",
    "<S> another is to provide reliable predictive models that can be used to forecast mass values away from the valley of stability . </S>",
    "<S> our study focuses mainly on the former goal and achieves substantial improvement over previous neural - network models of the mass table by using improved schemes for coding and training . </S>",
    "<S> the results suggest that with further development this approach may provide a valuable complement to conventional global models .    </S>",
    "<S> ,    binding energies and masses ; statistical modeling ; neural networks 07.05.mh , 21.10.dr , 07.05.tp </S>"
  ]
}