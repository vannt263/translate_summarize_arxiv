{
  "article_text": [
    "first order methods have recently been widely applied to solve convex optimization problems in a variety of areas including machine learning and signal processing . in particular ,",
    "proximal gradient algorithms ( a.k.a .",
    "forward - backward splitting algorithms ) and their accelerated variants have received considerable attention ( see @xcite and references therein ) .",
    "these algorithms are easy to implement and suitable for solving high dimensional problems thanks to the low memory requirement of each iteration .",
    "moreover , they are particularly suitable for composite optimization , that is when a convex objective function is the sum of a smooth and a non - smooth component .",
    "this class of optimization problems arises naturally in regularization schemes where one component is a data fitting term and the other a regularizer , see for example @xcite .",
    "interestingly , proximal splitting algorithms separate the contribution of each component at every iteration : the proximal operator defined by the non smooth term is applied to a gradient descent step for the smooth term . in practice",
    "it is often relevant to consider situations where the latter operation can not be perfomed _",
    "exactly_. for example the case where the proximal operator is known only up - to an error have been considered in @xcite .",
    "in this paper we are interested in the complementary situation where it is the gradient of the smooth term to be know up - to an error .",
    "more precisely , we consider the case where only stochastic estimates of the gradient are available and develop stochastic versions of proximal splitting methods .",
    "this latter situation is particularly relevant in statistical learning , where we have to minimize an expected objective function from random samples . in this context ,",
    "iterative algorithms , where only one gradient estimate is used in each step , are often referred to as online learning algorithms .",
    "more generally , the situation where only stochastic gradient estimates are available is important in stochastic optimization , where iterative algorithms can be seen as a form of stochastic approximation .",
    "finally , stochastic gradient approaches are considered in the incremental optimization of an objective function which is the sum of many terms , e.g. the empirical risk in machine learning @xcite , see section [ ss : amm ] for a detailed discussion . in the next section",
    "we describe our contribution in the context of the state of the art .",
    "the study of stochastic approximation methods originates in the classical work of @xcite , and assumes the objective function to be smooth and strongly convex ; the related literature is vast ( see e.g. @xcite and references therein ) .",
    "an improvement of the original stochastic approximation method , based on averaging of the trajectories and larger step - sizes , is proposed by @xcite and @xcite .",
    "more recently , one can recognize two main approaches to solve general nonsmooth convex stochastic optimization .",
    "the first one uses different versions of mirror descent stochastic approximation , based on projected subgradient averaging techniques @xcite .",
    "similar methods have been extensively studied also in the machine learning community in the context of online learning , where the proof of convergence of the average of the iterates is often based on regret analysis and , the so called , online - to - batch conversion @xcite .",
    "the second line of research is based on stochastic variants of accelerated proximal gradient descent @xcite .",
    "the algorithm we consider is also a stochastic extension of proximal gradient descent , but corresponds to its basic version with no acceleration . indeed , as discussed below , a main question we consider is if accelerated methods yield any advantage in the stochastic case .",
    "the fobos algorithm in @xcite is the closest approach to the one we consider , the main two differences being 1 ) we consider an additional relaxation step which may lead to accelerations , and especially 2 ) we do not consider averaging of the iterates . this latter point is important , since averaging can have a detrimental effect .",
    "indeed , non - smooth problems often arise in applications where sparsity of the solution is of interest , and it is easy to see that averaging prevent the solution to be sparse @xcite . moreover , as noted in @xcite and @xcite , averaging can have a negative impact on the convergence rate in the strongly convex case . indeed , in this paper we improve the error bound in @xcite in this latter case .    our study is developed in an infinite dimensional setting , where we focus on almost sure convergence of the iterates and non asymptotic bounds on their expectation . considering iterates convergence is standard in optimization theory and",
    "often considered in machine learning when sparsity based learning is studied @xcite .",
    "the theoretical analysis in the paper is divided in two parts . in the first",
    ", we study convergence in expectation in the strongly convex case , generalizing the results in ( * ? ? ?",
    "* section 3 ) to the nonsmooth case .",
    "we provide a non - asymptotic analysis of stochastic proximal gradient descent where the bounds depend explicitly on the parameters of the problem .",
    "interestingly , we obtain , in the strongly convex case , the same @xmath0 error bound that can be obtained from the optimal rate of convergence for function values as achieved by accelerated methods , see e.g. @xcite .",
    "this result ( confirmed by numerical simulations ) suggests that , unlike in the deterministic setting , in stochastic optimization acceleration does not have an impact on the rate of convergence . in the second part",
    ", we establish almost sure convergence .",
    "our results generalize to the composite case the analysis of the stochastic projected subgradient algorithm in a hilbert space @xcite ( see also @xcite ) .",
    "our analysis is based on a novel extension of the analysis of proximal methods with exact gradient , based on considering random quasi - fejr sequences @xcite .",
    "this approach allows to consider assumptions on the stochastic estimates of the gradients which are more general than those considered in previous work , and does not require boundedness of the iterates .",
    "we note that a recent technical report @xcite also analyzes a stochastic proximal gradient method ( without the relaxation step ) and its accelerated variant .",
    "almost sure convergence of the iterates ( without averaging ) is proved under uniqueness of the minimizer , but under assumptions different from ours : continuity of the objective function thus excluding constrained smooth optimization and boundedness the iterates .",
    "convergence rates for the iterates without averaging are derived , but only for the accelerated method .",
    "finally , we note that convergence of the iterates of stochastic proximal gradient has been recently obtained from the analysis of convergence of stochastic fixed point algorithms presented in the recent preprint @xcite .",
    "however , this latter results is derived from summability assumptions on the errors of the stochastic estimates which are usually not satisfied in the machine learning setting .",
    "the paper is organized as follows . in section [ sec : psae ] we introduce composite optimization and the stochastic proximal gradient algorithm , along with some relevant special cases . in section [ s : alconv ] , we study convergence in expectation and almost surely that we prove in section [ sec : proofs ] .",
    "section [ s : exp ] describes some numerical tests comparing the stochastic projected gradient algorithm with state of the art stochastic first order methods .",
    "the proofs of auxiliary results are found in appendix [ s : bg ] .    [",
    "[ notation - and - basic - definitions ] ] * notation and basic definitions * + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    throughout , @xmath1 is a probability space , @xmath2 , and @xmath3 is a real separable hilbert space .",
    "we use the notation @xmath4 and @xmath5 for the scalar product and the associated norm in @xmath3 . the symbols",
    "@xmath6 and @xmath7 denote , respectively , weak and strong convergence . the class of lower semicontinuous convex functions @xmath8-\\infty,+\\infty\\right]$ ] such that @xmath9 , is denoted by @xmath10 .",
    "the proximity operator of @xmath11 is @xmath12 throughout this paper , we assume implicitly that the closed - form expressions of the proximity operators to be available .",
    "we refer to @xcite for the closed - form expression of a wide class of functions , see @xcite for examples in machine learning . given a random variable @xmath13 , we denote by @xmath14 $ ] its expected value , and by @xmath15 the @xmath16-field generated by @xmath13 . the conditional expectation of @xmath13 given a @xmath16-algebra @xmath17 is denoted by @xmath18 $ ] .",
    "the conditional expectation of @xmath13 given @xmath19 is denoted by @xmath20 $ ] .",
    "a filtration of @xmath21 is an increasing sequence @xmath22 of sub-@xmath16-algebras of @xmath21 .",
    "a @xmath3-valued random process is a sequence of random variables @xmath23 taking values in @xmath3 . the shorthand notation ` a.s . '",
    "stands for ` almost sure ' .",
    "in this section , we introduce the composite convex optimization problem , the stochastic proximal method we study , and discuss some special cases of the framework we consider .",
    "composite optimization problems are defined as the problem of minimizing the sum of a smooth convex function and a possibly nonsmooth convex function . here",
    "we assume that the latter is proximable , that is the proximity operator is available in closed form or can be easily computed .",
    "[ prob2 ] let @xmath24 , let @xmath250,\\infty[$ ] , and let @xmath26 be convex and differentiable , with a @xmath27-lipschitz continuous gradient .",
    "the problem is to @xmath28 under the assumption that the set of solutions to is non - empty .    as mentioned in the introduction , problems with this composite structure",
    "has been recently extensively studied in convex optimization . in particular , the class of splitting methods , which decouple the contribution of the smooth term and the nonsmooth one , received a lot of attention @xcite . within the class of splitting methods , in this paper we study the following stochastic proximal gradient ( spg ) algorithm .    [ a : main ] let @xmath29 be a strictly positive sequence , let @xmath30 be a sequence in @xmath31 $ ] , and let @xmath32 be a @xmath3-valued random process such that @xmath33 @xmath34<+\\infty$ ] .",
    "fix @xmath35 a @xmath3-valued integrable vector with @xmath36 < + \\infty$ ] and set @xmath37 \\end{array}\\ ] ]    algorithm [ a : main ] is a stochastic version of the proximal forward - backward splitting @xcite , where we replace the exact gradient by a stochastic element . more specifically ,",
    "if , for every @xmath38 , @xmath39 , our algorithm reduces to the one in @xcite .",
    "a stochastic proximal forward - backward splitting ( fobos ) was firstly proposed in @xcite for minimizing the sum of two functions where one of them is proximable , and the other is convex and subdifferentiable .",
    "algorithm [ a : main ] generalizes the fobos algorithm , by including a relaxation step , while assuming the first component in to be smooth .",
    "as it is the standard , to ensure convergence of the proposed algorithm , we need additional conditions on the random process @xmath40 as well as on the sequence of step - sizes @xmath29 .",
    "the following conditions will be considered for the filtration @xmath22 with @xmath41 .    1 .",
    "[ e : csss1 ] for every @xmath38 , @xmath42 = \\nabla { l}(w_n)$ ] .",
    "2 .   [ e : csss2 ] for every @xmath38 , there exist @xmath430,+\\infty\\right[$ ] and @xmath440,+\\infty\\right[$ ] such that @xmath45   \\leq \\sigma^2(1 + \\alpha_n\\|\\nabla { l}(w_n)\\|^2 ) \\ ] ] 3 .",
    "there exists @xmath460,+\\infty\\right[$ ] such that @xmath47 4 .",
    "for any solution @xmath48 of the problem , set @xmath49 .",
    "assume that @xmath50    condition ( a1 ) means that , at each iteration @xmath51 , @xmath52 is an unbiased estimate of the gradient of the smooth term .",
    "condition ( a2 ) has been considered in @xcite .",
    "it is weaker than typical conditions used in the analysis of stochastic ( sub)gradient algorithms , namely boundedness of the sequence @xmath53)_{n\\in{\\ensuremath{\\mathbb n}}^*}$ ] ( see @xcite ) or even boundedness of @xmath54 ( see @xcite ) .",
    "we note that this last requirement on the entire space is not compatible with the assumption of strong convexity , because the gradient is necessarily not uniformly bounded , therefore the use of the more general condition ( a2 ) is needed in this case .    conditions such as ( a3 ) and ( a4 ) are , respectively , widely used in the deterministic setting and in stochastic optimization .",
    "assumption ( a3 ) is more restrictive that the one usually assumed in the deterministic setting , that is @xmath55 .",
    "we also note that when @xmath30 is bounded away from zero , and @xmath56 is bounded , ( a4 ) implies ( a3 ) for @xmath51 large enough .",
    "the condition @xmath57 in assumption @xmath58 is satisfied if @xmath59 is summable .",
    "moreover , if @xmath60 , it reduces to @xmath61 , since in this case @xmath62 for every solution @xmath63 . finally , in our case , the step - size is required to converge to zero , while it is typically bounded away from zero in the study of deterministic proximal forward - backward splitting algorithm @xcite .",
    "problem [ prob2 ] covers a wide class of deterministic as well as stochastic convex optimization problems , especially from machine learning and signal processing , see e.g. @xcite and references therein .",
    "the simplest case is when @xmath64 is identically equal to 0 , so that problem [ prob2 ] reduces to the classic problem of finding a minimizer of a convex differentiable function from unbiased estimates of its gradients . in the case when @xmath64 is the indicator function of a nonempty , convex , closed set @xmath65 , i.e. @xmath66 then problem reduces to a constrained minimization problem of the form @xmath67 which is well studied in the literature , as mentioned in the introduction .",
    "below , we discuss in more detail some special cases of interest .",
    "( * minimization of an expectation*).[ex : expmin ] let @xmath68 be a random vector with probability distribution @xmath69 supported on @xmath70 and @xmath71 .",
    "stochastic gradient descent methods are usually studied in the case where @xmath3 is an euclidean space and @xmath72 = \\int_{{\\ensuremath{\\boldsymbol{e } } } } f(w,\\xi)dp(\\xi),\\ ] ] under the assumption that @xmath73 @xmath74 is a convex differentiable function with lipschitz continuous gradient @xcite .",
    "let @xmath75 be independent copies of the random vector @xmath68 .",
    "assume that there is an oracle that , for each @xmath76 , returns a vector @xmath77 such that @xmath78 $ ] . by setting @xmath79 and @xmath80 , then ( a2 ) holds .",
    "this latter assertion follows from standard properties of conditional expectation , see e.g. ( * ? ? ?",
    "* example 5.1.5 ) .",
    "( * minimization of a sum of functions*)[ex : sumoffunctions ] let @xmath24 , let @xmath81 be a strictly positive integer",
    ". for every @xmath82 , let @xmath83 be convex and differentiable , such that @xmath84 has a @xmath27-lipschitz continuous gradient , for some @xmath850,+\\infty\\right[$ ] .",
    "the problem is to @xmath86 this problem is a special case of problem [ e : prob2 ] with @xmath87 , and is especially of interest when @xmath81 is very large and we know the exact gradient of each component @xmath88 .",
    "the stochastic estimate of the gradient of @xmath89 is then defined as @xmath90 where @xmath91 is a random process of independent random variables uniformly distributed on @xmath92 , see @xcite .",
    "clearly @xmath93 holds .",
    "assumption @xmath94 specializes in this case to @xmath95 if the latter is satisfied , then spg algorithm can be applied with a suitable choice of the stepsize .    finally , in the next section",
    ", we discuss how the above setting specializes to the context of machine learning .",
    "consider two measurable spaces @xmath96 and @xmath97 and assume there is a probability measure @xmath98 on @xmath99 .",
    "the measure @xmath98 is fixed but known only through a training set @xmath100 of samples i.i.d with respect to @xmath98 .",
    "consider a loss function @xmath101 and a hypothesis space @xmath3 of functions from @xmath96 to @xmath97 , e.g. a reproducing kernel hilbert space .",
    "a key problem in this context is ( regularized ) empirical risk minimization , @xmath102 the above problem can be seen as an approximation of the problem , @xmath103 the analysis , in this paper , can be adapted to the machine learning setting in two different ways .",
    "the first , following example [ ex : expmin ] , is to apply the spg algorithm to directly solve the regularized _ expected _ loss minimization problem .",
    "the second , following example [ ex : sumoffunctions ] , is to apply the spg algorithm to solve the regularized _ empirical _ risk minimization problem .    in either one of the above two problems , the first term is differentiable",
    "if the loss functions is differentiable with respect to its second argument , examples being the squared or the logistic loss . for these latter loss functions , and more generally for loss functions which are twice differentiable in their second argument , it easy to see that the lipschitz continuity of the gradient is satisfied if the maximum eigenvalue of the hessian is bounded .",
    "the second term @xmath104 can be seen as a regularizer / penalty encoding some prior information about the learning problem .",
    "examples of convex , non - differentiable penalties include sparsity inducing penalties such as the @xmath105 norm , as well as more complex structured sparsity penalties @xcite .",
    "stronger convexity properties can be obtained considering an _ elastic net penalty _ @xcite , that is adding a small strongly convex term to the sparsity inducing penalty .",
    "clearly , the latter term would not be necessary if the risk in problem [ e : learn1 ] ( or the empirical risk in ) is strongly convex",
    ". however , this latter requirement depends on the probability measure @xmath98 and is typically not satisfied when considering high ( possibly infinite ) dimensional settings .",
    "in this section , we state and discuss the main results of the paper .",
    "we derive convergence rates of the proximal gradient algorithm ( with relaxation ) for stochastic minimization .",
    "the section is divided in two parts . in the first one ,",
    "section [ sec : exp ] , we focus on convergence in expectation . in the second one ,",
    "section [ sec : as ] , we study almost sure convergence of the sequence of iterates . in both cases ,",
    "additional convexity conditions on the objective function are required to derive convergence results .",
    "the proofs are deferred to section [ sec : proofs ] .",
    "in this section , we denote by @xmath48 a solution of problem [ e : prob2 ] and provide an explicit non - asymptotic bound on @xmath106 $ ] .",
    "this result generalizes to the nonsmooth case the bound obtained in ( * ? ? ?",
    "* theorem 1 ) for stochastic gradient descent .",
    "the following assumption is considered throughout this section .",
    "[ a : strcon ] the function @xmath89 is @xmath107-strongly convex and @xmath64 is @xmath108-strongly convex , for some @xmath109 and @xmath110 , with @xmath111 .",
    "note that , we do not assume both @xmath89 and @xmath64 to be strongly convex , indeed the constants @xmath107 and @xmath108 can be zero , but require that only one of the two is .",
    "this implies that @xmath48 is the unique solution of problem [ e : prob2 ] .    in the statement of the following theorem",
    ", we will use the family of functions @xmath112 defined by setting , for every @xmath113 , @xmath1140,{\\ensuremath{+\\infty}}\\right[\\to { \\ensuremath{\\mathbb r}}\\colon t\\mapsto   \\begin{cases } ( t^{c}-1)/c & \\text{if $ c \\not=0$};\\\\   \\log t & \\text{if $ c = 0$}. \\end{cases}\\ ] ] this family of functions arises in lemma [ l : ocs ] in the appendix and are useful to bound the sum of the stepsizes .",
    "[ t:1 ] assume that conditions @xmath115 , @xmath116 and assumption [ a : strcon ] are satisfied .",
    "suppose that there exist @xmath1170,+\\infty\\right[$ ] and @xmath1180,+\\infty\\right[$ ] such that @xmath119 let @xmath1200,+\\infty[$ ] and let @xmath1210,1\\right]$ ] .",
    "suppose that , for every @xmath122 , @xmath123 .",
    "set @xmath124 let @xmath125 be the smallest integer such that @xmath126 , and @xmath127 then , by setting @xmath128,\\ ] ] we have , for every @xmath129 , @xmath1300,1\\right[$,}\\\\   s_{n_0}\\big(\\dfrac{n_0}{n+1}\\big)^{c}+   \\dfrac{2^c\\tau c^2 } { ( n+1)^{c}}\\varphi_{c-1}(n ) & \\text{if $ \\theta = 1$. } \\end{cases}\\ ] ]    in theorem [ t:1 ] , the dependence on the strong convexity constants is hidden in the constant @xmath131 . taking into account , we can write more explicitly the asymptotic behavior of the sequence @xmath132 .",
    "[ cor:1 ] under the same assumptions and with the same notation of theorem [ t:1 ] , the following holds @xmath133=\\begin{cases}o(n^{-\\theta } ) & \\text{if $ \\theta\\in\\,]0,1[$},\\\\   o(n^{-c})+o(n^{-1 } ) & \\text{if $ \\theta=1$}. \\end{cases}\\ ] ] thus , if @xmath134 and @xmath135 is chosen such that @xmath136 , then @xmath137 = o(n^{-1})$ ] .",
    "in particular , if @xmath138 , @xmath139 for every @xmath38 , and @xmath140 , then @xmath141 , @xmath142 , and @xmath133\\leq \\dfrac{n_{0}^2{\\ensuremath{\\mathbb{e}}}[\\|w_{n_0}-\\overline{w}\\|^2 ] } { ( n+1)^2}+   \\dfrac{8\\sigma^2(1+\\overline{\\alpha } \\|\\nabla { l}(\\overline{w})\\|)(1+\\nu)^4}{\\underline{\\lambda}^2\\left(\\mu \\epsilon+\\nu\\right)^2}\\ ] ]    theorem [ t:1 ] is the extension to the nonsmooth case of ( * ? ? ? * theorem 1 ) , in particular , when @xmath143 , we obtain the same bounds .",
    "note however that the assumptions on the stochastic approximations of the gradient of the smooth part are different .",
    "in particular , we replace the boundedness condition at the solution and the lipschitz continuity assumption on @xmath40 with assumption ( a2 ) .",
    "as can be seen from corollary [ cor:1 ] , the fastest asymptotic rate corresponds to @xmath138 and it is the same obtained in the smooth case in ( * ? ? ?",
    "* theorem 2 ) . note that this rate depends on the asymptotic behavior of the step - size , but also on the constant @xmath131 , which in turns depends on @xmath135 .",
    "as pointed out in @xcite , see also in @xcite , this choice is critical , because too small choices of @xmath135 affect the convergence rates , and too big choices influence significantly the value of the constants in the first term of . in particular , as can be readily seen in corollary [ cor:1 ] , the choice is determined by the strong convexity constants . moreover , the dependence on the strong convexity constant shown in corollary [ cor:1 ] is of the same type of the one obtained in the regret minimization framework by @xcite .",
    "there are other stochastic first order methods achieving the same rate of convergence for the iterates in the strongly convex case , see e.g. @xcite . indeed , the rate we obtain is the rate that can be obtained by the optimal ( in the sense of @xcite ) convergence rate on the function values . among the mentioned methods those in @xcite",
    "belong to the class of accelerated proximal gradient methods .",
    "our result shows that , in the strongly convex case , the rate of convergence of the iterates is the same in the accelerated and non accelerated case .",
    "in addition , if sparsity is the main interest , we highlight that many of the algorithms discussed above ( e.g. @xcite ) involve some form of averaging or linear combination which prevent sparsity of the iterates , as it is discussed in @xcite .",
    "our result shows that in this case averaging is not needed , since the iterates themselves are convergent .",
    "we next compare in some detail our results with those obtained for the fobos algorithm in @xcite and to the stochastic proximal gradient in @xcite .",
    "there are a few difference in the settings considered .",
    "in particular , convergence of the average of the iterates with respect to the function values is considered in @xcite assuming uniform boundedness of the iterations and the subdifferentials .",
    "the space @xmath3 is assumed to be finite dimensional , though the analysis might be extended to infinite dimensional spaces . finally , the optimal stepsize in @xcite depends explicitly on the radius of the ball containing the iterates , which in general might not be available .",
    "our convergence results consider convergence of the iterates ( with no averaging ) and hold in an infinite dimensional setting , without boundedness assumptions .",
    "the non asymptotic rate @xmath144 which we obtain for the iterates improves the @xmath145 rate derived from ( * ? ? ?",
    "* corollary 10 ) for the average of the iterates .",
    "however , it should be noted that convergence of the objective values is studied in @xcite also for the non strongly convex case .",
    "spg ( without relaxation ) has been recently studied in @xcite .",
    "also in this case the authors assume a priori boundedness of the iterates and prove convergence of the averaged sequence .",
    "theorem [ t:1 ] is also comparable with deterministic stochastic proximal forward - backward algorithm with errors @xcite . on the one hand",
    ", we allow the errors to satisfy assumption ( a2 ) , while in the deterministic case the errors in the computation of the gradient should decrease to zero sufficiently fast . on the other hand",
    ", we require asymptotically vanishing ( and smaller , according to ( a3 ) ) step - sizes , while , in the deterministic case , the step - size is bounded from below . finally , if @xmath146 is continuous , in the setting of theorem [ t:1 ] , it holds @xmath147 .",
    "moreover , if @xmath146 is lipschitz continuous , then @xmath148 and if @xmath146 is differentiable with lipschitz continuous gradient , @xmath149 .      in this section",
    ", we focus on almost sure convergence of spg algorithm .",
    "this kind of convergence of the iterates is the one traditionally studied in the stochastic optimization literature .",
    "depending on the convexity properties of the function @xmath89 , we get two different convergence properties .",
    "the first theorem requires uniform convexity of @xmath89 at the solution .",
    "[ t:2 ] suppose that the conditions @xmath115 , @xmath116 , and @xmath58 are satisfied .",
    "let @xmath150 be a sequence generated by algorithm [ a : main ] and assume that @xmath89 is uniformly convex at @xmath48 . then @xmath151 a.s .    if we relax the strong convexity assumption , we can still prove weak convergence of a subsequence in the strictly convex case , provided an additional regularity assumption holds .",
    "[ t:3 ] suppose that the conditions @xmath115 , @xmath116 , and @xmath58 are satisfied .",
    "let @xmath150 be a sequence generated by algorithm [ a : main ] .",
    "assume that @xmath89 is strictly convex , and let @xmath48 be the unique solution of problem [ e : prob2 ] . if @xmath152 is weakly continuous , then there exists a subsequence @xmath153 such that @xmath154 a.s .    with respect to the previous section , here",
    "we make the additional assumption @xmath58 on the summability of the sequence of step - sizes multiplied by the relaxation parameters . for stochastic gradient algorithm without relaxation ,",
    "i.e , @xmath143 and , for every @xmath38 @xmath155 , assumption ( a4 ) coincides with the classical step - size condition @xmath156 and @xmath157 which guarantees a sufficient but not too fast decrease of the step - size ( see e.g. @xcite ) .",
    "assumption @xmath94 has been considered in the context of stochastic gradient descent in @xcite .",
    "note that under such a condition , the variance of the stochastic approximation is allowed to grow with @xmath158 .    as mentioned in the introduction ,",
    "the study of almost sure convergence is classical .",
    "an analysis of a stochastic projected subgradient algorithm in an infinite dimensional hilbert space can be found in @xcite .",
    "theorem [ t:2 ] can be seen as an extension of ( * ? ? ?",
    "* theorem 3.1 ) , where the case where @xmath64 is an indicator function is considered .",
    "our approach is based on random quasi - fejr sequences , and on probabilistic quasi martingale techniques @xcite .",
    "[ r:3 ] if @xmath89 is assumed to be only strictly convex and its gradient is not weakly continuous , theorem [ t:3 ] does not ensure weak convergence of any subsequence of @xmath159 .",
    "however , if the sequence of function values @xmath160 converges to the minimum of @xmath161 , then @xmath162 a.s .",
    "this happens ( see @xcite ) when @xmath163 for some closed subspace @xmath164 of @xmath3 , or when @xmath165 for some non - empty closed convex @xmath65 of @xmath3 , and there exists a bounded function @xmath166 such that @xmath167 \\leq h(\\|\\nabla { l}(w_n)\\|).$ ]    the proof of remark [ r:3 ] can be found in the next section .",
    "we start by recalling the firmly non - expansiveness of the proximity operator and the baillon - haddad theorem ( see ( * ? ? ? * theorem 18.15 ) ) .",
    "* lemma 2.4 ) [ l : firm ] let @xmath168 .",
    "then the proximity of @xmath64 is firmly non - expansive , i.e. , @xmath169    ( * ? ? ?",
    "* definition 4.4 ) [ d : coco ] let @xmath170 , and let @xmath1710,+\\infty\\right[}}$ ]",
    ". then @xmath172 is @xmath173-cocoercive if @xmath174    let @xmath175 be a convex differentiable function with @xmath27 lipschitz gradient .",
    "then , @xmath152 is @xmath176-cocoercive .",
    "we next state the following lemma ; see also ( * ? ? ?",
    "* lemma 5 , chapter 2.2 ) and @xcite .",
    "we will use the family of functions @xmath177 defined in . for completeness , the proof is given in the appendix .",
    "[ l : ocs ] let @xmath1780,1\\right]$ ] , and let @xmath131 and @xmath179 be in @xmath1800,+\\infty[$ ] , let @xmath181 be a strictly positive sequence defined by @xmath182 .",
    "let @xmath183 be such that @xmath184 let @xmath125 be the smallest integer such that @xmath185 and set @xmath186 .",
    "then , for every @xmath129 , @xmath1870,1\\right[$,}\\\\   s_{n_0}\\big(\\frac{n_0}{n+1}\\big)^{c}+   \\frac{\\tau c^2}{(n+1)^{c}}(1 + \\frac{1}{n_0})^{c}\\varphi_{c-1}(n ) & \\text{if $ \\alpha = 1$. } \\end{cases}\\ ] ]    we start with a technical result , giving some bounds that will be repeatedly used .",
    "[ p : bb ] consider the setting of the spg algorithm and let @xmath48 be a solution of problem [ e : prob2 ] .",
    "suppose that conditions ( a1 ) , ( a2 ) , and ( a3 ) are satisfied .",
    "then the following hold :    1 .",
    "[ e : est1 t ] @xmath188 2 .",
    "[ p : bbii ] set @xmath189 then , for every @xmath190 @xmath191 3 .",
    "[ p : bbiii ] for every @xmath190 @xmath192 \\leq & \\left({\\ensuremath{\\mathbb{e}}}\\left[\\|w_n-\\overline{w}\\|^2\\right]-2\\gamma_n\\left(1-\\gamma_n\\beta(1 + 2\\sigma^2\\alpha_n)\\right)\\cdot \\right.\\\\ & \\cdot{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}},{\\nabla{l}(w_n)-\\nabla { l}(\\overline{w } ) } \\right\\rangle } ] + 2\\gamma_n^2\\sigma^2 ( 1+\\alpha_n\\|\\nabla{l}(\\overline{w})\\|^2)\\bigg).\\end{aligned}\\ ] ]    [ e : est1 t ] : follows from convexity of @xmath193 .",
    "[ p : bbii ] : we have @xmath194 moreover , since @xmath195 is firmly non - expansive by lemma [ l : firm ] @xmath196 and the statement follows .",
    "[ p : bbiii ] : note that , for every @xmath38 , we have that @xmath197 and @xmath52 are measurable with respect to @xmath198 since they are @xmath199 measurable and by definition @xmath200 .",
    "the same holds for @xmath201 , for it is the difference of two measurable functions .",
    "we next show by induction that @xmath33 @xmath202 is integrable .",
    "first , @xmath203 is integrable by assumption",
    ". then , assume by inductive hypothesis that @xmath202 is integrable .",
    "then so is @xmath204 , for @xmath52 is square integrable by assumption .",
    "moreover , @xmath205 , because @xmath206 is nonexpansive .",
    "therefore @xmath207 is integrable and hence so is @xmath208 .",
    "this implies that @xmath209<+\\infty$ ] and @xmath210<+\\infty$ ] .",
    "therefore , using assumption ( a1 ) , we obtain @xmath211   & = { \\ensuremath{\\mathbb{e}}}[{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}},{\\mathrm{g}_n - \\nabla{l}(\\overline{w } ) } \\right\\rangle}|\\mathcal{a}_n ] \\notag\\\\ & = { \\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}},{{\\ensuremath{\\mathbb{e } } } [ \\mathrm{g}_n - \\nabla{l}(\\overline{w})|\\mathcal{a}_n ] } \\right\\rangle}]\\notag\\\\ & = { \\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}},{\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) } \\right\\rangle}].\\end{aligned}\\ ] ] moreover , using the assumption ( a2 ) , we have @xmath212 \\leq 2 { \\ensuremath{\\mathbb{e}}}[\\|\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) \\|^2 ] + 2 { \\ensuremath{\\mathbb{e}}}[\\|\\mathrm{g}_n - \\nabla{l}(w_n)\\|^2]\\notag\\\\ & \\leq   2 { \\ensuremath{\\mathbb{e}}}[\\|\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) \\|^2 ] + 2\\sigma^2(1 + \\alpha_n{\\ensuremath{\\mathbb{e}}}[\\|\\nabla{l}(w_n)\\|^2])\\notag\\\\ & \\leq ( 2 + 4\\sigma^2\\alpha_n){\\ensuremath{\\mathbb{e}}}[\\|\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) \\|^2 ] + 2\\sigma^2(1 + 2\\alpha_n\\|\\nabla{l}(\\overline{w})\\|^2)\\notag\\\\ & \\leq    { ( 2 + 4\\sigma^2\\alpha_n)}{\\beta}{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}},{\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) } \\right\\rangle } ]   + 2\\sigma^2(1 + 2\\alpha_n)\\|\\nabla{l}(\\overline{w})\\|^2 ,   \\end{aligned}\\ ] ] where the last inequality follows from the fact that @xmath213 is cocoercive since it is lipschitz - continuous ( by the baillon - haddad theorem ) .",
    "the statement then follows from , , and .",
    "we are now ready to prove theorem [ t:1 ] .    since @xmath111 , then @xmath214 is strongly convex .",
    "hence , problem has a unique minimizer @xmath48 . since @xmath215 is @xmath216-strongly convex , by ( * ? ? ?",
    "* proposition 23.11 ) @xmath206 is @xmath217-cocoercive , and then @xmath218 next , proceeding as in the proof of proposition [ p : bb ] , we get an inequality analogue to , that is @xmath219 \\leq & \\frac{1}{(1+\\gamma_n\\nu)^2 } \\left({\\ensuremath{\\mathbb{e}}}\\left[\\|w_n-\\overline{w}\\|^2\\right]-2\\gamma_n\\left(1-\\gamma_n\\beta(1 + 2\\sigma^2\\alpha_n)\\right)\\cdot \\right.\\\\   \\label{eq : ymenw } & \\cdot{\\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}},{\\nabla{l}(w_n)-\\nabla { l}(\\overline{w } ) } \\right\\rangle } ] + 2\\gamma_n^2\\sigma^2 ( 1+\\alpha_n\\|\\nabla{l}(\\overline{w})\\|^2)\\bigg).\\end{aligned}\\ ] ] since @xmath89 is strongly convex of parameter @xmath107 , it holds @xmath220 .",
    "therefore , from , using the @xmath107-strong convexity of @xmath89 and ( a3 ) , we get @xmath221 & \\leq    \\frac{1}{(1+\\gamma_n\\nu)^2}\\bigg((1 - 2\\gamma_n\\mu\\epsilon){\\ensuremath{\\mathbb{e}}}\\left[\\|w_n-\\overline{w}\\|^2\\right ] + 2\\sigma^2\\chi_n^2\\bigg).\\end{aligned}\\ ] ] hence , by definition of @xmath222 , @xmath223    & \\leq \\bigg(1-\\frac{\\lambda_n\\gamma_n(2\\nu +   \\gamma_{n}\\nu^2 + 2\\mu\\epsilon)}{(1+\\gamma_n\\nu)^2}\\bigg){\\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2]+    \\frac { 2\\sigma^2\\chi^{2}_n}{(1+\\gamma_n\\nu)^2}.\\end{aligned}\\ ] ] let @xmath224 and fix @xmath225 .",
    "since @xmath226 , we have @xmath227 where we set @xmath228 . on the other hand",
    ", @xmath229 then , putting together , , and , we get @xmath230    \\leq ( 1-\\eta_n){\\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2]+ \\tau\\eta_n^2 $ ] , with @xmath231 and @xmath232 . finally , follows from lemma [ l : ocs ] .    in order to prove theorem [ t:2 ] , we start by giving the definition of deterministic and random quasi - fejr sequences .",
    "we denote by @xmath233 the set of summable sequences in @xmath234 .",
    "@xcite let @xmath235 be a non - empty subset of @xmath3 and let @xmath236 be a sequence in @xmath237 .",
    "then ,    * a sequence @xmath159 in @xmath3 is deterministic quasi - fejr monotone with respect to the target set @xmath235 if @xmath238 * a sequence of random vectors @xmath159 in @xmath3 is stochastic quasi - fejr monotone with respect to the target set @xmath235 if @xmath36 < + \\infty$ ] and @xmath239 \\leq \\|w_n - w\\|^2 + \\varepsilon_n.\\ ] ]    the following result has been stated in @xcite without a proof .",
    "for the sake of completeness , a proof is given in the appendix .",
    "* lemma 2.3 ) [ p : fejer ] let @xmath235 be a non - empty closed subset of @xmath3 , let @xmath240 .",
    "let @xmath150 be a sequence of random vectors in @xmath3 such that @xmath36 < + \\infty$ ] , and let @xmath241 .",
    "assume that @xmath242   \\leq \\|w_n - w\\|^2 + \\varepsilon_n   \\quad \\text{a.s}.\\ ] ] then the following hold .    1 .",
    "[ p : fejeri ] let @xmath243 .",
    "then , @xmath244)_{n\\in{\\ensuremath{\\mathbb n}}^{*}}$ ] converges to some @xmath245 and @xmath246 converges a.s . to an integrable random vector @xmath247 .",
    "[ p : fejerii ] @xmath150 is bounded a.s .",
    "[ p : fejeriii ] the set of weak cluster points of @xmath150 is non - empty a.s .",
    "we next collect some convergence results that will be useful in the proof of the main theorem [ t:2 ] .",
    "[ p:1 ] suppose that ( a1 ) , ( a2 ) , ( a3 ) , and ( a4 ) are satisfied .",
    "let @xmath150 be a sequence generated by algorithm [ a : main ] .",
    "then , for any solution @xmath48 of the problem , the following hold :    1 .",
    "[ p:1i ] the sequence @xmath248)_{n\\in{\\ensuremath{\\mathbb n}}^{*}}$ ] converges to a finite value .",
    "[ t:2i ] the sequence @xmath249 converges a.s to some integrable random variable @xmath250 .",
    "[ p:1iii ] @xmath251 < { \\ensuremath{+\\infty}}$ ] .",
    "consequently , @xmath252 = 0 \\quad \\text{and } \\quad   \\varliminf_{n\\to\\infty}{\\ensuremath{\\mathbb{e}}}[\\|\\nabla{l}(w_n)-\\nabla{l}(\\overline{w})\\|^2 ] = 0.\\ ] ] 4 .",
    "[ p:1ii ] @xmath253 < { \\ensuremath{+\\infty}}$ ] and @xmath254 < + \\infty$ ] .    by proposition  [ p : bb][e",
    ": est1t]-[p : bbiii ] , and by condition ( a3 ) , we get @xmath255   \\leq ( 1-\\lambda_n ) { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ] + \\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|y_n-\\overline{w}\\|^2 ] \\notag\\\\ & \\leq { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ]   - 2\\varepsilon \\gamma_n\\lambda_n { \\ensuremath{\\mathbb{e}}}[{\\left\\langle{w_n-\\overline{w}},{\\nabla{l}(w_n)-\\nabla{l}(\\overline{w } ) } \\right\\rangle } ]    + 2\\sigma^2\\chi^{2}_n -\\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|u_n\\|^2]\\notag\\\\ & \\leq { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ]   + 2\\sigma^2\\chi^{2}_n,\\end{aligned}\\ ] ] where the last inequality follows by the monotonicity of @xmath152 .",
    "[ p:1i ] : since the sequence @xmath256 is summable by assumption ( a4 ) , we derive from that @xmath257)_{n\\in{\\ensuremath{\\mathbb n}}^{*}}$ ] converges to a finite value .",
    "[ t:2i ] : we estimate the conditional expectation with respect to @xmath199 of each term in the right hand side of .",
    "since @xmath197 is @xmath199-measurable , we have @xmath258 = \\|w_n-\\overline{w}\\|^2.\\ ] ] using assumption ( a1 ) , @xmath259   & = { \\left\\langle{w_n-\\overline{w}},{{\\ensuremath{\\mathbb{e } } } [ \\mathrm{g}_n - \\nabla{l}(\\overline{w})| \\mathcal{a}_n } \\right\\rangle}\\notag\\\\ & = { \\left\\langle{w_n-\\overline{w}},{\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) } \\right\\rangle}. \\end{aligned}\\ ] ] next , note that @xmath260 is @xmath199-measurable by ( a1 ) , and therefore by @xmath94 , we get @xmath261   \\leq 2 { \\ensuremath{\\mathbb{e}}}[\\|\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) \\|^2| \\mathcal{a}_n ] + 2 { \\ensuremath{\\mathbb{e}}}[\\|\\mathrm{g}_n - \\nabla{l}(w_n)\\|^2 |\\mathcal{a}_n   ] \\notag\\\\ & \\leq   2\\|\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) \\|^2   + 2\\sigma^2(1+\\alpha_n\\|\\nabla{l}(w_n)\\|^2)\\notag\\\\ & \\leq   2\\|\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) \\|^2 + 2\\sigma^2(1 + 2\\alpha_n   \\|\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) \\|^2 + 2\\alpha_n\\|\\nabla{l}(\\overline{w})\\|^2 ) \\notag\\\\ & \\leq { ( 2 + 4\\sigma^2\\alpha_n ) } { \\beta } { \\left\\langle{w_n-\\overline{w}},{\\nabla{l}(w_n ) - \\nabla{l}(\\overline{w } ) } \\right\\rangle }   + 2\\sigma^2(1 + 2\\alpha_n\\|\\nabla{l}(\\overline{w})\\|^2 ) ,   \\end{aligned}\\ ] ] where the last inequality follows from the cocoercivity of @xmath213 . taking the conditional expectation with respect to @xmath199 , and invoking , , , and",
    ", we obtain , @xmath262   \\leq ( 1-\\lambda_n ) \\|w_n-\\overline{w}\\|^2   + \\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|y_n-\\overline{w}\\|^2|\\mathcal{a}_n ] \\notag\\\\ & \\leq \\|w_n-\\overline{w}\\|^2 - 2\\gamma_n\\lambda_n(1-{\\beta\\gamma_n(1 + 2\\sigma^2\\alpha_n)}){\\left\\langle{\\nabla{l}(w_n)-\\nabla{l}(\\overline{w } ) } , { w_n-\\overline{w } } \\right\\rangle } \\notag\\\\ & \\quad + 2\\sigma^2\\chi^{2}_n-\\lambda_n{\\ensuremath{\\mathbb{e } } } [ \\|u_n\\|^2|\\mathcal{a}_n]\\notag\\\\ & \\leq \\|w_n-\\overline{w}\\|^2 - 2\\varepsilon \\gamma_n\\lambda_n { \\left\\langle{\\nabla{l}(w_n)-\\nabla{l}(\\overline{w } ) } , { w_n-\\overline{w } } \\right\\rangle } +   2\\sigma^2\\chi^{2}_n-\\lambda_n{\\ensuremath{\\mathbb{e } } } [ \\|u_n\\|^2|\\mathcal{a}_n]\\notag\\\\ & \\leq \\|w_n-\\overline{w}\\|^2 + 2\\sigma^2\\chi^{2}_n\\,.\\end{aligned}\\ ] ] hence , @xmath150 is a random quasi - fejr sequence with respect to the nonempty closed and convex set @xmath263 .",
    "taking into account that @xmath36<+\\infty$ ] by assumption , it follows from proposition [ p : fejer][p : fejeri ] that @xmath249 converges a.s to some integrable random variable @xmath250 .",
    "[ p:1iii ] : we derive from that @xmath264 < + \\infty.\\ ] ] since @xmath265 , we obtain @xmath266 = 0 \\quad \\rightarrow   \\varliminf_{n\\to\\infty}{\\ensuremath{\\mathbb{e}}}[\\|\\nabla{l}(w_n)-\\nabla{l}(\\overline{w } ) \\|^2 ] = 0,\\ ] ] using again the cocoercivity of @xmath152 .",
    "[ p:1ii ] we directly get from that @xmath267 .",
    "since @xmath213 is lipschitz - continuous , and @xmath268)_{n\\in{\\ensuremath{\\mathbb n}}^*}$ ] is convergent by [ p:1i ] , there exists @xmath2690,+\\infty[$ ] such that @xmath270 \\leq \\beta   { \\ensuremath{\\mathbb{e}}}[\\|w_n-\\overline{w}\\|^2 ] \\leq m < + \\infty.\\end{aligned}\\ ] ] hence , we derive from and that @xmath271 < + \\infty.\\ ] ] now , recalling the definition of @xmath272 in , using and , we obtain @xmath273 \\leq 2\\sum_{n\\in{\\ensuremath{\\mathbb n } } } \\lambda_n { \\ensuremath{\\mathbb{e}}}[\\|u_n\\|^2 ] + 2\\sum_{n\\in{\\ensuremath{\\mathbb n}}^ { * } } \\lambda_n\\gamma^{2}_n   { \\ensuremath{\\mathbb{e}}}[\\|\\mathrm{g}_n-\\nabla{l}(\\overline{w})\\|^2 ] < + \\infty.\\ ] ]    since @xmath89 is uniformly convex at @xmath48 , there exists @xmath274 increasing and vanishing only at @xmath275 such that @xmath276 therefore , we derive from proposition [ p:1 ] [ p:1iii ] that @xmath277   < \\infty,$ ] and hence @xmath278 since @xmath279 is not summable , we have @xmath280 a.s .",
    "consequently , there exists a subsequence @xmath281 such that @xmath282 a.s , which implies that @xmath283 a.s . in view of proposition [ p:1][t:2i ] , we get @xmath284 a.s .    by proposition [ p:1][p:1i ]",
    ", @xmath285 converges to an integrable random variable , hence it is uniformly bounded .",
    "moreover , @xmath286= 0 $ ] , and hence there exists a subsequence @xmath281 such that @xmath287= 0.$ ] thus , there exists a subsequence @xmath288 of @xmath281 such that @xmath289 let @xmath290 be a weak cluster point of @xmath291 , then there exists a subsequence @xmath292 such that for almost all @xmath293 , @xmath294 . since @xmath213 is weakly continuous , for almost all @xmath293 , @xmath295 .",
    "therefore , for almost every @xmath293 , by , @xmath296 , and hence @xmath297 since @xmath89 is strictly convex , @xmath152 is strictly monotone , we obtain @xmath298 .",
    "this shows that @xmath299 a.s .",
    "let @xmath300 be a weak cluster point of @xmath150 , i.e. , there exists a subsequence @xmath301 such that @xmath302 a.s . since @xmath303 is convex and lower semicontinous , it is weakly lower semicontinous ,",
    "hence @xmath304 which shows that @xmath305 a.s .",
    "we therefore conclude that @xmath150 converges weakly to an optimal solution a.s .",
    "in this section we first present numerical experiments aimed at studying the computational performance of the spg algorithm ( see algorithm [ a : main ] ) , with respect to the step - size , the strong convexity constant , and the noise level .",
    "then we compare the proposed method with other state - of - the - art stochastic first order methods : an accelerated stochastic proximal gradient method , called sage ( * ? ? ?",
    "* theorem 2 ) and the fobos algorithm @xcite .      in order to study the behavior of the spg algorithm with respect to the relevant parameters of the optimization problem , we focus on a toy example , where the exact solution is known . more specifically ,",
    "we consider the following minimization problem on the real line : @xmath306 it is clear that @xmath307 is @xmath107-strongly convex function with @xmath308 and the optimal value @xmath309 .",
    "we consider a stochastic perturbation of the exact gradient of the function @xmath310 of the form @xmath311 where @xmath312 is a realization of a gaussian random variable with @xmath275 mean and @xmath313 variance .",
    "we apply spg one hundred times for 100 independent realizations of the random process @xmath314 to problem with @xmath33 @xmath155 and @xmath315 for some constant @xmath316 .",
    "we evaluate the average performance of spg over the first 100 iterations for different values of the strong convexity parameter @xmath107 , and several values of @xmath16 and @xmath65 , and by measuring @xmath317 .",
    "the results are displayed in figure [ fig : orbit1pcabc ] . as can be seen by visual inspection ,",
    "the convergence is faster when @xmath107 is bigger and when the noise variance is smaller .",
    "moreover , the constant @xmath65 in the step - size heavily influence the convergence behavior .",
    "the latter is a well - known phenomenon in the context of stochastic optimization @xcite ,    [ cols=\"^,^,^ \" , ]      as a last experiment , we focus on the problem of recovering an ideal signal @xmath48 from a noisy observation of the form @xmath318 where @xmath319 and @xmath320 is a gaussian kernel .",
    ", width=302 ]    to find an approximation of the ideal signal , we solve the following variational problem @xmath321 an approximation @xmath48 of the exact solution is found by running the forward - backward splitting method in @xcite for @xmath322 iterations .",
    "then , we run spg , sage , and fobos with the same initialization , for @xmath323 iterations using at the @xmath51-th iteration a stochastic gradient of the form @xmath324 where @xmath325 and @xmath326 .",
    "fobos is run with @xmath327 .",
    "this is not the theoretically optimal choice , but gave better results in practice . in spg we set @xmath155 and @xmath328 .",
    "convergence of @xmath329 for the three algorithms is presented in figure  [ fig : orbit1pc1 ] . in this case sage",
    "is the fastest , and spg shows slightly worse convergence .",
    "fobos is again slower .",
    ", width=302 ]    finally , we address the problem of the iterations sparsity .",
    "we generate the data according to the model in , starting from an original signal with @xmath330 zero components . in figure",
    "[ fig : orbit1pc1a ] we display the number of zero components of the iterates .",
    "as it can be readily seen by visual inspection , after few iteratons both sage ans spg generate sparse iterations . on this example",
    "this does not hold for the fobos algorithm , for which the sparsity of the iterates is a decreasing function of the number of iterations .",
    "the number of zero components of the last iterate of spg , sage , and fobos is 937 , 937 , and 438 , respectively .     with the same initial point @xmath275 for spg and sage .",
    "[ fig : orbit1pc1a ] , width=302 ]",
    "in this paper we proposed and studied a stochastic approach to the problem of minimizing a strongly convex non - smooth function .",
    "in particular , we have considered the case of composite minimization ( the objective function is the sum of a smooth and a convex term ) and proposed a stochastic extension of proximal splitting methods which have become widely popular in a deterministic setting .",
    "these latter approaches are based on recursively computing the gradient of the smooth term and then applying the proximity operator defined by the convex term .",
    "the starting point of the paper is considering the case where only stochastic estimates of the gradient are available .",
    "this latter situation is relevant in online approaches to learning and more generally in stochastic and incremental optimization .",
    "the main contributions of this paper are to provide convergence rates in expectation and to establish almost sure convergence of the proposed stochastic proximal gradient method .",
    "a further contribution regards our proving techniques , which differ from many previous approaches based on regret analysis and online - to batch conversion .",
    "indeed , our approach is based on extending convex optimization techniques from the deterministic to the stochastic setting .",
    "such extensions are interesting in their own right and could lead to further applications .",
    "an outcome of our analysis is that , unlike in the deterministic case , in the stochastic case acceleration does not yield any improvement in the rates .",
    "the analysis in the paper suggests a few venues for future work .",
    "a main one is the relaxation of the strong convexity assumption , considering in particular objective functions which are only convex ; steps in this direction have been taken in @xcite .",
    "deriving high probability , rather than expectation bounds , would also be interesting .",
    "this material is based upon work supported by the center for brains , minds and machines ( cbmm ) , funded by nsf stc award ccf-1231216 .",
    "l. r. acknowledges the financial support of the italian ministry of education , university and research firb project rbfr12m3ac .",
    "s. v. is member of the gruppo nazionale per lanalisi matematica , la probabilit e le loro applicazioni ( gnampa ) of the istituto nazionale di alta matematica ( indam ) .",
    "here we prove the statements about random quasi - fejr sequences and an upper bound on a numerical sequence satisfying the assumptions of lemma [ l : ics ] .",
    "[ p : fejeri ] : since the sequence @xmath236 is summable and @xmath332 $ ] is finite , we derive from that @xmath333)_{n\\in{\\ensuremath{\\mathbb n}}^*}$ ] is a real positive quasi - fejr sequence , and therefore it converges to some @xmath334 by ( * ? ? ?",
    "* lemma 3.1 ) . set @xmath335 then , it follows from that @xmath336 & =   { \\ensuremath{\\mathbb{e}}}[\\|w_{n+1}-w\\|^2   |\\mathcal{a}_n ] + \\sum_{k = n+1}^{\\infty}\\varepsilon_n\\notag\\\\ & \\leq \\|w_n - w\\|^2 +   \\sum_{k = n}^{\\infty}\\varepsilon_n\\notag\\\\ & = r_n.\\end{aligned}\\ ] ] therefore @xmath337 is a ( real ) supermartingale .",
    "since @xmath338 < + \\infty$ ] by , @xmath339 converges a.s to an integrable random variable ( * ? ? ?",
    "* theorem 9.4 ) , that we denote by @xmath340 .",
    "note that , for every @xmath341 , @xmath38 , @xmath342 : @xmath343 where @xmath344 is defined by .",
    "since all terms in are positive for @xmath225 , by applying the recursion @xmath345 times we have @xmath346 let us estimate the first term in the right hand side of . since @xmath347 for every @xmath348 , from , we derive @xmath349 s_{n_0 } \\exp\\big(\\frac{c}{1-\\alpha}(n_{0}^{1-\\alpha } - ( n+1)^{1-\\alpha } ) \\big ) & \\text{if $ 0 < \\alpha < 1$. }     \\end{cases}\\end{aligned}\\ ] ] to estimate the second term in the right hand side of , let us first consider the case @xmath350 , let @xmath351 such that @xmath352 .",
    "we have @xmath353 hence , combining and , for @xmath1780,1\\right[$ ] we get @xmath354 we next estimate the second term in the right hand side of in the case @xmath355 .",
    "we have @xmath356 therefore , for @xmath355 , we obtain , @xmath357 which completes the proof .                              p.  l. combettes .",
    "quasi - fejrian analysis of some optimization algorithms . in",
    "_ inherently parallel algorithms in feasibility and optimization and their applications ( haifa , 2000 ) _ , volume  8 of _ stud .",
    "comput . math .",
    "_ , pages 115152 .",
    "north - holland , amsterdam , 2001 .",
    "l. combettes and jean - christophe pesquet .",
    "proximal splitting methods in signal processing . in _",
    "fixed - point algorithms for inverse problems in science and engineering _ , volume  49 of _ springer optim .",
    "_ , pages 185212 .",
    "springer , new york , 2011 .",
    "s.  mosci , l.  rosasco , m.  santoro , a.  verri , and s.  villa . solving structured sparsity regularization with proximal methods . in _",
    "machine learning and knowledge discovery in databases european conference , ecml pkdd 2010 _ , pages 418433 , barcelona , spain , 2010 .",
    "springer .",
    "o.  shamir and t.  zhang .",
    "stochastic gradient descent for non - smooth optimization : convergence results and optimal averaging schemes . in _ proceedings of the 30th international conference on machine learning _"
  ],
  "abstract_text": [
    "<S> we prove novel convergence results for a stochastic proximal gradient algorithm suitable for solving a large class of convex optimization problems , where a convex objective function is given by the sum of a smooth and a possibly non - smooth component . </S>",
    "<S> we consider the iterates convergence and derive @xmath0 non asymptotic bounds in expectation in the strongly convex case , as well as almost sure convergence results under weaker assumptions . </S>",
    "<S> our approach allows to avoid averaging and weaken boundedness assumptions which are often considered in theoretical studies and might not be satisfied in practice .    </S>",
    "<S> proximal methods , forward - backward splitting algorithm , stochastic optimization , online learning algorithms . </S>"
  ]
}