{
  "article_text": [
    "human have the remarkable ability of selective visual attention  @xcite .",
    "cognitive science explains this as the  biased competition theory ``  @xcite that human visual cortex is enhanced by top - down guidance during feedback loops .",
    "the feedback signals suppress non - relevant stimuli present in the visual field , helping human searching for ' ' goals \" . with visual attention ,",
    "both human recognition and detection performances increase significantly , especially in images with cluttered background  @xcite .    inspired by human attention , the recurrent visual attention model ( ram )",
    "is proposed for image recognition  @xcite .",
    "ram is a deep recurrent neural architecture with iterative attention selection mechanism , that mimics the human visual system to suppress non - relevant image regions and extract discriminative features in a complicated environment .",
    "this significantly improves the recognition accuracy  @xcite , especially for fine - grained object recognition  @xcite .",
    "ram also allows the network to process a high resolution image with only limited computational resources . by iteratively attending to different sub - regions ( with a fixed resolution ) , ram could efficiently process images with various resolutions and aspect ratios in a constant computational time  @xcite .    besides attention",
    ", human also tend to dynamically allocate different computational time when processing different images  @xcite .",
    "the length of the processing time often depends on the task and the content of the input images ( e.g.  background clutter , occlusion , object scale ) .",
    "for example , during the recognition of a fine - grained bird category , if the bird appears in a large proportion with clean background ( figure  [ fig : splash]a ) , human can immediately recognize the image without hesitation .",
    "however , when the bird is under camouflage ( figure  [ fig : splash]b ) or hiding in the scene with background clutter and pose variation ( figure  [ fig : splash]c ) , people may spend much more time on locating the bird and extracting discriminative parts to produce a confident prediction .",
    "c|cc   & + ( a ) easy & +   +    inspired by this , we propose an extension to ram named as dynamic time recurrent attention model ( dt - ram ) , by adding an extra binary ( continue / stop ) action at every time step . during each step",
    ", dt - ram will not only update the next attention , but produce a decision whether stop the computation and output the classification score .",
    "the model is a simple extension to ram , but can be viewed as a first step towards dynamic model during inference  @xcite , where the model structure can vary based on each input instance .",
    "this could bring dt - ram more flexibility and reduce redundant computation to further save computation , especially when the input examples are  easy \" to recognize .",
    "although dt - ram is an end - to - end recurrent neural architecture , we find it hard to directly train the model parameters from scratch , particularly for challenging tasks like fine - grained recognition .",
    "when the total number of steps increases , the delayed reward issue becomes more severe and the variance of gradients becomes larger .",
    "this makes policy gradient training algorithms such as reinforce  @xcite harder to optimize .",
    "we address this problem with curriculum learning  @xcite . during the training of ram",
    ", we gradually increase the training difficulty by gradually increasing the total number of time steps .",
    "we then initialize the parameters in dt - ram with the pre - trained ram and fine - tune it with reinforce .",
    "this strategy helps the model to converge to a better local optimum than training from scratch .",
    "we also find intermediate supervision is crucial to the performance , particularly when training longer sequences .",
    "we demonstrate the effectiveness of our model on public benchmark datasets including mnist  @xcite as well as two fine - grained datasets , cub-200 - 2011  @xcite and stanford cars  @xcite .",
    "we also conduct an extensive study to understand how dynamic time works in these datasets .",
    "experimental results suggest that dt - ram can achieve state - of - the - art performance on fine - grained image recognition .",
    "compared to ram , the model also uses less average computational time , better fitting devices with computational limitations .",
    "visual attention is a long - standing topic in computer vision  @xcite . with the recent success of deep neural networks  @xcite , mnih",
    "@xcite develop the recurrent visual attention model ( ram ) for image recognition , where the attention is modeled with neural networks to capture local regions in the image .",
    "ba  @xcite follow the same framework and apply ram to recognize multiple objects in images .",
    "sermanet  @xcite further extend ram to fine - grained image recognition , since fine - grained problems usually require the comparison between local parts . besides fine - grained recognition , attention models also work for various machine learning problems including machine translation  @xcite , image captioning  @xcite , image question answering  @xcite and video activity recognition  @xcite .",
    "based on the differentiable property of attention models , most of the existing work can be divided into two groups : soft attention and hard attention  @xcite .",
    "the soft attention models define attention as a set of continuous variables representing the relative importance of spatial or temporal cues .",
    "the model is differentiable hence can be trained with backpropogation .",
    "the hard attention models define attention as actions and model the whole problem as a partially observed markov decision process ( pomdp )  @xcite .",
    "such models are usually nondifferentiable to the reward function hence use policy gradient such as reinforce  @xcite to optimize the model parameters .",
    "our model belongs to the hard attention since its stopping action is discrete .",
    "the visual attention models can be also viewed as a special type of feedback neural networks  @xcite .",
    "a feedback neural network is a special recurrent architecture that uses previously computed high level features to back refine low level features .",
    "it uses both top - down and bottom - up information to compute the intermediate layers . besides attention models ,",
    "feedback neural networks also have other variants .",
    "for example , carreira  @xcite performs human pose estimation with iterative error feedback .",
    "newell  @xcite build a stacked hourglass network for human pose estimation .",
    "hu and ramanan  @xcite show that network feedbacks can help better locating human face landmarks .",
    "all these models demonstrate top - down information could potentially improve the model discriminative ability  @xcite",
    ". however , these models either fix the number of recurrent steps or use simple rules to decide early stopping .      graves  @xcite recently introduce _ adaptive computational time _ in recurrent neural networks .",
    "the model augments the network with a _ sigmoidal halting unit _ at each time step , whose activation determines the probability whether the computation should stop .",
    "figurnov  @xcite extend  @xcite to spatially adaptive computational time for residual networks .",
    "their approach is similar but define the _ halting units _ over spatial positions .",
    "neumann  @xcite extend the similar idea to temporally dependent reasoning .",
    "they achieve a small performance benefit on top of a similar model without an adaptive component .",
    "jernite  @xcite learn a scheduler to determine what portion of the hidden state to compute based on the current hidden and input vectors .",
    "all these models can vary the computation time during inference , but the stopping policy is based on the cumulative probability of _ halting units _ , which can be viewed as a fixed policy .",
    "as far as we know , odena  @xcite is the first attempt that learn to change model behavior at test time with reinforcement learning .",
    "their model adaptively constructs computational graphs from sub - modules on a per - input basis .",
    "however , they only verify on small dataset such as mnist  @xcite and cifar-10  @xcite .",
    "ba  @xcite augment ram with the `` end - of - sequence '' symbol to deal with variable number of objects in an image , which inspires our work on dt - ram .",
    "however , they still fix the number of attentions for each target .",
    "there is also a lack of diagnostic experiments on understanding how `` end - of - sequence '' symbol affects the dynamics . in this work",
    ", we conduct extensive experimental comparisons on larger scale natural images from fine - grained recognition .",
    "fine - grained image recognition has been extensively studied in recent years  @xcite . based on the research focus",
    ", fine - grained recognition approaches can be divided into representation learning , part alignment models or emphasis on data .",
    "the first group attempts to build implicitly powerful feature representations such as bilinear pooling or compact bilinear pooling  @xcite , which turn to be very effective for fine - grained problems .",
    "the second group attempts to localize discriminative parts to effectively deal with large intra - class variation as well as subtle inter - class variation  @xcite .",
    "the third group studies the importance of the scale of training data  @xcite .",
    "they achieve significantly better performance on multiple fine - grained dataset by using an extra large set of training images .    with the fast development of deep models such as bilinear cnn  @xcite and spatial transformer networks  @xcite ,",
    "it is unclear whether attention models are still effective for fine - grained recognition . in this paper , we show that the visual attention model , if trained carefully , can still achieve comparable performance as state - of - the - art methods .",
    "the difference between a dynamic structure model and a fixed structure model is that during inference the model structure @xmath0 depends on both the input @xmath1 and parameter @xmath2 .",
    "given an input @xmath1 , the probability of choosing a computational structure @xmath0 is @xmath3 .",
    "when the model space of @xmath0 is defined , this probability can be modeled with a neural network . during training , with a given model structure ,",
    "the loss is @xmath4 . hence the overall expected loss for an input @xmath1 is @xmath5 = \\sum_{\\mathcal{s } } p(\\mathcal{s}|x , \\theta ) l_\\mathcal{s}(x , \\theta ) \\label{eq : loss}\\ ] ] the gradient of @xmath6 with respect to parameter @xmath2 is : @xmath7\\end{aligned}\\ ] ] the first term in the above expectation is the same as reinforce algorithm  @xcite , it makes the structure leading to smaller loss more probable .",
    "the second term is the standard gradient for neural nets with a fixed structure .    during experiments ,",
    "it is difficult to directly compute the gradient of the @xmath6 over @xmath2 because it requires to evaluate exponentially many possible structures during training .",
    "hence to train the model , we first sample a set of structures , then approximate the gradient with monte carlo simulation  @xcite : @xmath8      , the model could output more confident predictions @xmath9 . ]",
    "the recurrent attention model is formulated as a partially observed markov decision process ( pomdp ) . at each time step",
    ", the model works as an agent that executes an action based on the observation and receives a reward .",
    "the agent actively control how to act , and it may affect the state of the environment . in ram , the action corresponds to the location of the attention region .",
    "the observation is a local ( partially observed ) region cropped from the image .",
    "the reward measures the quality of the prediction using all the cropped regions and can be delayed .",
    "the target of learning is to find the optimal decision policy to generate attentions from observations that maximizes the expected cumulative reward across all time steps .",
    "more formally , ram defines the input image as @xmath1 and the total number of attentions as @xmath10 . at each time step @xmath11",
    ", the model crops a local region @xmath12 around location @xmath13 which is computed from the previous time step .",
    "it then updates the internal state @xmath14 with a recurrent neural network @xmath15 which is parameterized by @xmath16 .",
    "the model then computes two branches .",
    "one is the location network @xmath17 which models the attention policy , parameterized by @xmath18 .",
    "the other is the classification network @xmath19 which computes the classification score , parameterized by @xmath20 . during inference ,",
    "it samples the attention location based on the policy @xmath21 .",
    "figure  [ fig : ram ] illustrates the inference procedure .",
    "when the dynamic structure comes to ram , we simply augment it with an additional set of actions @xmath22 that decides when it will stop taking further attention and output results .",
    "@xmath23 is a binary variable with 0 representing  continue \" and 1 representing  stop \" .",
    "its sampling policy is modeled via a stopping network @xmath24 . during inference , we sample both the attention @xmath25 and stopping @xmath26 with each policy independently .",
    "@xmath27 figure  [ fig : dt - ram ] shows how the model works .",
    "compared to figure  [ fig : ram ] , the change is simply by adding @xmath26 to each time step .",
    "figure  [ fig : dt - ram - illustration ] illustrates how dt - ram adapts its model structure and computational time to different input images for image recognition .",
    "when the input image is  easy \" to recognize ( figure  [ fig : dt - ram - illustration ] left ) , we expect dt - ram stop at the first few steps .",
    "when the input image is  hard \" ( figure  [ fig : dt - ram - illustration ] right ) , we expect the model learn to continue searching for informative regions .",
    "is added to each time step .",
    "@xmath28 represents `` continue '' ( green solid circle ) and @xmath29 represents `` stop '' ( red solid circle ) . ]",
    "given a set of training images with ground truth labels @xmath30 , we jointly optimize the model parameters by computing the following gradient : @xmath31 where @xmath32 are the parameters of the recurrent network , the attention network , the stopping network and the classification network respectively .    compared to equation  [ eq : train ] , equation  [ eq : loss - dt - ram ] is an approximation where we use a negative of reward function @xmath33 to replace the loss of a given structure @xmath34 in the first term .",
    "this training loss is similar to  @xcite .",
    "although the loss in equation  [ eq : train ] can be optimized directly , using @xmath33 can reduce the variance in the estimator  @xcite .",
    "@xmath35 is the sampling policy for structure @xmath0 .",
    "@xmath36 is the cumulative discounted reward over @xmath37 time steps for the @xmath38-th training example .",
    "the discount factor @xmath39 controls the trade - off between making correct classification and taking more attentions .",
    "@xmath40 is the reward at @xmath41-th step . during experiments , we use a delayed reward .",
    "we set @xmath42 if @xmath43 and @xmath44 only if @xmath45 .",
    "* intermediate supervision : * unlike original ram  @xcite , dt - ram has intermediate supervision for the classification network at every time step , since its underlying dynamic structure could require the model to output classification scores at any time step .",
    "the loss of @xmath46 is the average cross - entropy classification loss over @xmath47 training samples and @xmath37 time steps .",
    "note that @xmath10 depends on @xmath38 , indicating that each instance may have different stopping times . during experiments ,",
    "we find intermediate supervision is also effective for the baseline ram .",
    "* curriculum learning : * during experiments , we adopt a gradual training approach for the sake of accuracy .",
    "first , we start with a base convolutional network ( e.g.  residual networks  @xcite ) pre - trained on imagenet  @xcite .",
    "we then fine - tune the base network on the fine - grained dataset .",
    "this gives us a very high baseline .",
    "second , we train the ram model by gradually increase the total number of time steps .",
    "finally , we initialize dt - ram with the trained ram and further fine - tune the whole network with reinforce algorithm .",
    "we conduct experiments on three popular benchmark datasets : mnist  @xcite , cub-200 - 2011  @xcite and stanford cars  @xcite .",
    "table  [ tab : dataset ] summarizes the details of each dataset .",
    "mnist contains 70,000 images with 10 digital numbers .",
    "this is the dataset where the original visual attention model tests its performance .",
    "however , images in mnist dataset are often too simple to generate conclusions to natural images .",
    "therefore , we also compare on two challenging fine - grained recognition dataset . cub-200 - 2011  @xcite consists of 11,778 images with 200 bird categories .",
    "stanford cars  @xcite includes 16,185 images of 196 car classes .",
    "both datasets contain a bounding box annotation in each image .",
    "cub-200 - 2011 also contains part annotation , which we do not use in our algorithm .",
    "most of the images in these two datasets have cluttered background , hence visual attention could be effective for them .",
    "all models are trained and tested without ground truth bounding box annotations .",
    ".statistics of the three dataset .",
    "cub-200 - 2011 and stanford cars are both benchmark datasets in fine - grained recognition . [ cols=\"<,^,^,^,^\",options=\"header \" , ]     * qualitative results : * we visualize the qualitative results of dt - ram on cub-200 - 2011 and stanford cars testing set in figure  [ fig : visualization ] and figure  [ fig : visualization_car ] respectively . from step 1 to step 6 , we observe a gradual increase of background clutter and recognition difficulty , matching our hypothesis of using dynamic computation time for different types of images .",
    "in this work we present a simple but novel method for learning to dynamically adjust computational time during inference with reinforcement learning . we apply it on the recurrent visual attention model and show its effectiveness for fine - grained recognition .",
    "we believe that such methods will be important for developing dynamic reasoning in deep learning and computer vision",
    ". future work on developing more sophisticated dynamic models for reasoning and apply it to more complex tasks such as visual question answering will be conducted .",
    "richard  s sutton , david  a mcallester , satinder  p singh , yishay mansour , et  al .",
    "policy gradient methods for reinforcement learning with function approximation . in _ nips _ , volume  99 , pages 10571063 , 1999 .",
    "jonathan krause , michael stark , jia deng , and li  fei - fei .",
    "3d object representations for fine - grained categorization . in _ proceedings of the ieee international conference on computer vision workshops _ , pages 554561 , 2013 .",
    "christian szegedy , wei liu , yangqing jia , pierre sermanet , scott reed , dragomir anguelov , dumitru erhan , vincent vanhoucke , and andrew rabinovich .",
    "going deeper with convolutions . in _ proceedings of the ieee conference on computer vision and pattern recognition _ ,",
    "pages 19 , 2015 .",
    "kaiming he , xiangyu zhang , shaoqing ren , and jian sun .",
    "deep residual learning for image recognition . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 770778 , 2016 .",
    "kelvin xu , jimmy ba , ryan kiros , kyunghyun cho , aaron  c courville , ruslan salakhutdinov , richard  s zemel , and yoshua bengio .",
    "show , attend and tell : neural image caption generation with visual attention . in _",
    "icml _ , volume  14 , pages 7781 , 2015 .",
    "huijuan xu and kate saenko .",
    "ask , attend and answer : exploring question - guided spatial attention for visual question answering . in _",
    "european conference on computer vision _ , pages 451466 .",
    "springer , 2016 .",
    "zichao yang , xiaodong he , jianfeng gao , li  deng , and alex smola .",
    "stacked attention networks for image question answering . in",
    "_ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 2129 , 2016 .",
    "serena yeung , olga russakovsky , greg mori , and li  fei - fei .",
    "end - to - end learning of action detection from frame glimpses in videos . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 26782687 , 2016 .",
    "marijn  f stollenga , jonathan masci , faustino gomez , and jrgen schmidhuber .",
    "deep networks with internal selective attention through feedback connections . in _ advances in neural information processing systems _ , pages 35453553 , 2014",
    "chunshui cao , xianming liu , yi  yang , yinan yu , jiang wang , zilei wang , yongzhen huang , liang wang , chang huang , wei xu , et  al . look and think twice : capturing top - down visual attention with feedback convolutional neural networks . in _ proceedings of the ieee international conference on computer vision _",
    ", pages 29562964 , 2015 .",
    "qian wang , jiaxing zhang , sen song , and zheng zhang .",
    "attentional neural network : feature selection using cognitive feedback . in _ advances in neural information processing systems _ ,",
    "pages 20332041 , 2014 .",
    "joao carreira , pulkit agrawal , katerina fragkiadaki , and jitendra malik .",
    "human pose estimation with iterative error feedback . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 47334742 , 2016 .",
    "peiyun hu and deva ramanan .",
    "bottom - up and top - down reasoning with hierarchical rectified gaussians . in _ proceedings of the ieee conference on computer vision and pattern recognition _",
    ", pages 56005609 , 2016 .",
    "thomas berg , jiongxin liu , seung woo  lee , michelle  l alexander , david  w jacobs , and peter  n belhumeur .",
    "birdsnap : large - scale fine - grained visual categorization of birds . in",
    "_ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 20112018 , 2014 .",
    "yin cui , feng zhou , yuanqing lin , and serge belongie .",
    "fine - grained categorization and dataset bootstrapping using deep metric learning with humans in the loop . in",
    "_ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 11531162 , 2016 .",
    "shaoli huang , zhe xu , dacheng tao , and ya  zhang . part - stacked cnn for fine - grained visual categorization . in _ proceedings of the ieee conference on computer vision and pattern recognition _ ,",
    "pages 11731182 , 2016 .",
    "jonathan krause , hailin jin , jianchao yang , and li  fei - fei .",
    "fine - grained recognition without part annotations . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 55465555 , 2015 .",
    "aditya khosla , nityananda jayadevaprakash , bangpeng yao , and fei - fei li .",
    "novel dataset for fine - grained image categorization : stanford dogs . in _ proc .",
    "cvpr workshop on fine - grained visual categorization ( fgvc ) _ , volume  2 , 2011 .",
    "maria - elena nilsback and andrew zisserman .",
    "automated flower classification over a large number of classes . in _ computer vision , graphics & image processing , 2008 .",
    "sixth indian conference on _ , pages 722729 .",
    "ieee , 2008 .",
    "tsung - yu lin , aruni roychowdhury , and subhransu maji .",
    "bilinear cnn models for fine - grained visual recognition . in _ proceedings of the ieee international conference on computer vision _ , pages 14491457 , 2015 .",
    "thomas berg and peter belhumeur .",
    "poof : part - based one - vs .-",
    "one features for fine - grained categorization , face verification , and attribute estimation . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 955962 , 2013 .",
    "efstratios gavves , basura fernando , cees  gm snoek , arnold  wm smeulders , and tinne tuytelaars .",
    "fine - grained categorization by alignments . in _ proceedings of the ieee international conference on computer vision _ ,",
    "pages 17131720 , 2013 .",
    "jonathan krause , benjamin sapp , andrew howard , howard zhou , alexander toshev , tom duerig , james philbin , and li  fei - fei",
    ". the unreasonable effectiveness of noisy data for fine - grained recognition . in _",
    "european conference on computer vision _ , pages 301320 .",
    "springer , 2016 .",
    "jia deng , wei dong , richard socher , li - jia li , kai li , and li  fei - fei .",
    "imagenet : a large - scale hierarchical image database . in _ computer vision and pattern recognition , 2009 .",
    "cvpr 2009 .",
    "ieee conference on _ , pages 248255 .",
    "ieee , 2009 .",
    "marcel simon and erik rodner .",
    "neural activation constellations : unsupervised part model discovery with convolutional networks . in _ proceedings of the ieee international conference on computer vision _ , pages 11431151 , 2015 .    yuning chai , victor lempitsky , and andrew zisserman .",
    "symbiotic segmentation and part localization for fine - grained categorization . in _ proceedings of the ieee international conference on computer vision _ ,",
    "pages 321328 , 2013 .",
    "ross girshick , jeff donahue , trevor darrell , and jitendra malik .",
    "rich feature hierarchies for accurate object detection and semantic segmentation . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 580587 , 2014 .    yaming wang , jonghyun choi , vlad morariu , and larry  s davis . mining discriminative triplets of patches for fine - grained classification . in _ proceedings of the ieee conference on computer vision and pattern recognition",
    "_ , pages 11631172 , 2016 ."
  ],
  "abstract_text": [
    "<S> we propose a dynamic computational time model to accelerate the average processing time for recurrent visual attention ( ram ) . rather than attention with a fixed number of steps for each input image , the model learns to decide when to stop on the fly . to achieve this , we add an additional continue / stop action per time step to ram and use reinforcement learning to learn both the optimal attention policy and stopping policy . </S>",
    "<S> the modification is simple but could dramatically save the average computational time while keeping the same recognition performance as ram . </S>",
    "<S> experimental results on cub-200 - 2011 and stanford cars dataset demonstrate the dynamic computational model can work effectively for fine - grained image recognition.the source code of this paper can be obtained from https://github.com/baidu-research/dt-ram    = 1 </S>"
  ]
}