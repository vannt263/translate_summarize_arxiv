{
  "article_text": [
    "the search for periodicities in time or spatial dependent signals is a topic of the uttermost relevance in many fields of research , from geology ( stratigraphy , seismology , etc . ;",
    "( brescia et al . 1996 ) ) to astronomy ( barone et al . 1994 ) where it finds wide application in the study of light curves of variable stars , agn s , etc .",
    "the more sensitive instrumentation and observational techniques become , the more frequently we find variable signals in time domain that previously were believed to be constant .",
    "research for possible periodicities in the signal curves is then a natural consequence , when not an important issue .",
    "one of the most relevant problems concerning the techniques of periodic signal analysis is the way in which data are collected : many time series are collected at unevenly sampling rate .",
    "we have two types of problems related either to unknown fundamental period of the data , or their unknown multiple periodicities .",
    "typical cases , for instance in astronomy , are the determination of the fundamental period of eclipsing binaries both of light and radial velocity curves , or the multiple periodicities determination of ligth curves of pulsating stars .",
    "the difficulty arising from unevenly spaced data is rather obvious and many attempts have been made to solve the problem in a more or less satisfactory way . in this paper",
    "we will propose a new way to approach the problem using neural networks , that seems to work satisfactory well and seems to overcome most of the problems encountered when dealing with unevenly sampled data .",
    "in what follows , we assume @xmath0 to be a physical variable measured at discrete times @xmath1 .",
    "@xmath2 can be written as the sum of the signal @xmath3 and random errors @xmath4 : @xmath5 .",
    "the problem we are dealing with is how to estimate fundamental frequencies which may be present in the signal @xmath6 ( deeming 1975 ; kay 1988 ; marple 1987 ) .",
    "if @xmath0 is measured at uniform time steps ( even sampling ) ( horne & baliunas 1986 ; scargle 1982 ) there are a lot of tools to effectively solve the problem which are based on fourier analysis ( kay 1988 ; marple 1987 ; oppennheim & schafer 1965 ) .",
    "these methods , however , are usually unreliable for unevenly sampled data . for instance",
    ", the typical approach of resampling the data into an evenly sampled sequence , through interpolation , introduces a strong amplification of the noise which affects the effectiveness of all fourier based techniques which are strongly dependent on the noise level ( horowitz 1974 ) .",
    "there are other techniques used in specific areas ( ferraz - mello 1981 ; lomb 1976 ) : however , none of them faces directly the problem , so that they are not truly reliable .",
    "the most used tool for periodicity analysis of evenly or unevenly sampled signals is the periodogram ( lomb 1976 ; scargle 1982 ) ; therefore we will refer to it to evaluate our system .",
    "the periodogram ( p ) , is an estimator of the signal energy in the frequency domain ( deeming 1975 ; kay 1988 ; marple 1987 ; oppennheim & schafer 1965 ) .",
    "it has been extensively applied to pulsating star light curves , unevenly spaced , but there are difficulties in its use , specially concerning with aliasing effects .",
    "this tool is a variation of the classical p. it was introduced by j.d .",
    "scargle ( scargle 1982 ) for these reasons : 1 ) data from instrumental sampling are often not equally spaced ; 2 ) due to p inconsistency ( kay 1988 ; marple 1987 ; oppennheim & schafer 1965 ) , we must introduce a selection criterion for signal components .",
    "in fact , in the case of even sampling , the classical p has a simple statistic distribution : it is exponentially distributed for gaussian noise . in the uneven sampling case the distribution becomes very complex .",
    "however , scargle s p has the same distribution of the even case ( scargle 1982 ) .",
    "its definition is :    @xmath7 ^ 2 } { \\sum_{n=0}^{n-1 } \\cos^2 2\\pi f(t_n-\\tau ) } + \\nonumber \\\\ & & \\frac{[\\sum_{n=0}^{n-1 } x(n)\\sin 2\\pi f(t_n-\\tau)]^2}{\\sum_{n=0}^{n-1 } \\sin^2 2\\pi f(t_n-\\tau)}\\end{aligned}\\ ] ]    where @xmath8    and @xmath9 is a shift variable on the time axis , @xmath10 is the frequency , @xmath11 is the observation series .",
    "this tool is another variation of the classical p and is similar to the scargle s p. it was introduced by lomb ( lomb 1976 ) and we used the _ numerical recipes in c _ release ( numerical recipes in c 1988 - 1992 ) .",
    "let us suppose to have @xmath12 points @xmath13 and to compute mean and variance :    @xmath14    therefore , the normalised lomb s p ( power spectra as function of an angular frequency @xmath15 ) is defined as follows    @xmath16^{2}}{\\sum_{n=0}^{n-1 } \\cos^{2}\\omega ( t_{n}-\\tau ) } \\right ] + \\nonumber \\\\   & & + \\frac{1}{2\\sigma^{2 } } \\left [ \\frac{[\\sum_{n=0}^{n-1}\\left ( x(n)- \\bar{x}\\right ) \\sin \\omega ( t_{n}-\\tau ) ] ^{2}}{\\sum_{n=0}^{n-1}\\sin ^{2}\\omega   ( t_{n}-\\tau ) } \\right ]   \\label{plomb}\\end{aligned}\\ ] ]    where @xmath9 is defined by the equation @xmath17    and @xmath9 is an offset , @xmath18 is the frequency , @xmath19 is the observation series .",
    "the horizontal lines in the figures 19 , 22 , 25 , 27 , 32 and 34 correspond to the practical significance levels , as indicated in ( numerical recipes in c 1988 - 1992 ) .",
    "frequency estimation of narrow band signals in gaussian noise is a problem related to many fields ( kay 1988 ; marple 1987 ) .",
    "since the classical methods of fourier analysis suffer from statistic and resolution problems , then newer techniques based on the analysis of the signal autocorrelation matrix eigenvectors were introduced ( kay 1988 ; marple 1987 ) .",
    "let us assume to have a signal with p sinusoidal components ( narrow band ) .",
    "the p sinusoids are modelled as a stationary ergodic signal , and this is possible only if the phases are assumed to be indipendent random variables uniformly distributed in @xmath20 ( kay 1988 ; marple 1987 ) . to estimate the frequencies we exploit the properties of the signal autocorrelation matrix ( a.m. ) ( kay 1988 ; marple 1987 ) .",
    "the a.m. is the sum of the signal and the noise matrices ; the p principal eigenvectors of the signal matrix allow the estimate of frequencies ; the p principal eigenvectors of the signal matrix are the same of the total matrix .",
    "principal component analysis ( pca ) is a widely used technique in data analysis .",
    "mathematically , it is defined as follows : let @xmath21 be the covariance matrix of l - dimensional zero mean input data vectors @xmath22 .",
    "i_-th principal component of @xmath22 is defined as @xmath23 , where @xmath24 is the normalized eigenvector of * c * corresponding to the _",
    "i_-th largest eigenvalue @xmath25 .",
    "the subspace spanned by the principal eigenvectors @xmath26 is called the pca subspace ( of dimensionality m ) ( oja et al . 1991",
    "; oja et al .",
    "pca s can be neurally realized in various ways ( baldi & hornik 1989 ; jutten & herault 1991 ; oja 1982 ; oja et al .",
    "1991 ; plumbley 1993 ; sanger 1989 ) .",
    "the pca neural network used by us is a one layer feedforward neural network which is able to extract the principal components of the stream of input vectors .",
    "typically , hebbian type learning rules are used , based on the one unit learning algorithm originally proposed by oja ( oja 1982 ) .",
    "many different versions and extensions of this basic algorithm have been proposed during the recent years ; see ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ; oja et al .",
    "1996 ; sanger 1989 ) .",
    "the structure of the pca neural network can be summarised as follows ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ; oja et al .",
    "1996 ; sanger 1989 ) : there is one input layer , and one forward layer of neurons totally connected to the inputs ; during the learning phase there are feedback links among neurons , that classify the network structure as either hierarchical or symmetric .",
    "after the learning phase the network becomes purely feedforward .",
    "the hierarchical case leads to the well known gha algorithm ( karhunen & joutsensalo 1995 ; sanger 1989 ) ; in the symmetric case we have the oja s subspace network ( oja 1982 ) .",
    "pca neural algorithms can be derived from optimisation problems , such as variance maximization and representation error minimisation ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ) so obtaining nonlinear algorithms ( and relative neural networks ) .",
    "these neural networks have the same architecture of the linear ones : either hierarchical or symmetric .",
    "these learning algorithms can be further classified in : robust pca algorithms and nonlinear pca algorithms .",
    "we define robust a pca algorithm when the objective function grows less than quadratically ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ) .",
    "the nonlinear learning function appears at selected places only . in nonlinear pca algorithms",
    "all the outputs of the neurons are nonlinear function of the responses .      in the robust generalization of variance maximisation",
    ", the objective function @xmath27 is assumed to be a valid cost function ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ) , such as @xmath28 e @xmath29 .",
    "this leads to the algorithm : @xmath30    in the hierarchical case we have @xmath31 . in the symmetric case @xmath32",
    ", the error vector @xmath33 becomes the same @xmath34 for all the neurons , and equation([eq34 ] ) can be compactly written as : @xmath35    where @xmath36 is the instantaneous vector of neuron responses .",
    "the learning function @xmath37 , derivative of @xmath10 , is applied separately to each component of the argument vector .",
    "the robust generalisation of the representation error problem ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ) , with @xmath38 , leads to the stochastic gradient algorithm : @xmath39    this algorithm can be again considered in both hierarchical and symmetric cases . in the symmetric case @xmath32",
    ", the error vector is the same @xmath40 for all the weights @xmath41 . in the hierarchical case @xmath31 , equation([eq37 ] ) gives the robust counterparts of principal eigenvectors @xmath24 .",
    "the first update term @xmath42 in eq.([eq37 ] ) is proportional to the same vector @xmath43 for all weights @xmath44 .",
    "furthermore , we can assume that the error vector @xmath34 should be relatively small after the initial convergence .",
    "hence , we can neglet the first term in equation([eq37 ] ) and this leads to : @xmath45      let us consider now the nonlinear extensions of pca algorithms .",
    "we can obtain them in a heuristic way by requiring all neuron outputs to be always nonlinear in the equation([eq34 ] ) ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ) .",
    "this leads to :    @xmath46",
    "independent component analysis ( ica ) is a useful extension of pca that was developed in context with source or signal separation applications ( oja et al .",
    "1996 ) : instead of requiring that the coefficients of a linear expansion of data vectors are uncorrelated , in ica they must be mutually independent or as independent as possible .",
    "this implies that second order moments are not sufficient , but higher order statistics are needed in determining ica .",
    "this provides a more meaningful representation of data than pca . in current ica methods based on pca neural networks ,",
    "the following data model is usually assumed .",
    "the @xmath47-dimensional @xmath48-th data vector @xmath49 is of the form ( oja et al .",
    "1996 ) :    @xmath50    where in the m - vector @xmath51^{t}$ ] , @xmath52 denotes the @xmath53-th independent component ( source signal ) at time @xmath48 , @xmath54 $ ] is a @xmath55 _ mixing matrix _ whose columns @xmath56 are the basis vectors of ica , and @xmath57 denotes noise .",
    "the source separation problem is now to find an @xmath58 separating matrix @xmath59 so that the @xmath60-vector @xmath61 is an estimate @xmath62 of the original independent source signal ( oja et al .",
    "1996 ) .",
    "whitening is a linear transformation @xmath63 such that , given a matrix @xmath64 , we have @xmath65 where @xmath66 is a diagonal matrix with positive elements ( kay 1988 ; marple 1987 ) .",
    "several separation algorithms utilise the fact that if the data vectors @xmath49 are first pre - processed by whitening them ( i.e. @xmath67 with @xmath68 denoting the expectation ) , then the separating matrix @xmath59 becomes orthogonal ( @xmath69 see ( oja et al . 1996 ) ) .",
    "approximating contrast functions which are maximised for a separating matrix have been introduced because the involved probability densities are unknown ( oja et al . 1996 ) .",
    "it can be shown that , for prewhitened input vectors , the simpler contrast function given by the sum of kurtoses is maximised by a separating matrix @xmath59 ( oja et al .",
    "1996 ) .",
    "however , we found that in our experiments the whitening was not as good as we expected , because the estimated frequencies calculated for prewhitened signals with the neural estimator ( n.e . ) were not too much accurate .",
    "in fact we can pre - elaborate the signal , whitening it , and then we can apply the n.e . .",
    "otherwise we can apply the whitening and separate the signal in independent components with the nonlinear neural algorithm of equation([eq310 ] ) and then apply the n.e . to each of these components and",
    "estimate the single frequencies separately .",
    "the first method gives comparable or worse results than n.e . without whitening .",
    "the second one gives worse results and is very expensive .",
    "when we used the whitening in our n.e .",
    "the results were worse and more time consuming than the ones obtained using the standard n.e .",
    "( i.e. without whitening the signal ) .",
    "experimental results are given in the following sections .",
    "for these reasons whitening is not a suitable technique to improve our n.e .  .",
    "the process for periodicity analysis can be divided in the following steps :    * - preprocessing *    we first interpolate the data with a simple linear fitting and then calculate and subtract the average pattern to obtain zero mean process ( karhunen & joutsensalo 1994 ; karhunen & joutsensalo 1995 ) .",
    "* - neural computing *    the fundamental learning parameters are :    * 1 ) * the initial weight matrix ;    * 2 ) * the number of neurons , that is the number of principal eigenvectors that we need , equal to twice the number of signal periodicities ( for real signals ) ;    * 3 ) * @xmath70 , i.e. the threshold parameter for convergence ;    * 4 ) * @xmath71 , the nonlinear learning function parameter ;    * 5 ) * @xmath72 , that is the learning rate .",
    "we initialise the weight matrix @xmath73 assigning the classical small random values .",
    "otherwise we can use the first patterns of the signal as the columns of the matrix : experimental results show that the latter technique speeds up the convergence of our neural estimator ( n.e . ) .",
    "however , it can not be used with anomalously shaped signals , such as stratigraphic geological signals .",
    "experimental results show that @xmath71 can be fixed to : @xmath74 , @xmath75 , @xmath76 , @xmath77 , even if for symmetric networks a smaller value of @xmath71 is preferable for convergence reasons .",
    "moreover , the learning rate @xmath72 can be decreased during the learning phase , but we fixed it between @xmath78 and @xmath79 in our experiments .",
    "we use a simple criterion to decide if the neural network has reached the convergence : we calculate the distance between the weight matrix at step @xmath80 , @xmath81 , and the matrix at the previous step @xmath82 , and if this distance is less than a fixed error threshold ( @xmath83 ) we stop the learning process .",
    "we finally have the following general algorithm in which step 4 is one of the neural learning algorithms seen above in section [ section3 ] :    * * step 1 * initialise the weight vectors @xmath84 with small random values , or with orthonormalised signal patterns .",
    "initialise the learning threshold @xmath70 , the learning rate @xmath72 .",
    "reset pattern counter @xmath85 . *",
    "* step 2 * input the k - th pattern @xmath86 $ ] where @xmath12 is the number of input components . * * step 3 * calculate the output for each neuron @xmath87 . * * step 4 * modify the weights @xmath88 . * * step 5 * calculate @xmath89 * * step 6 * convergence test : if @xmath90 then goto * step 8*. * * step 7 * @xmath91 .",
    "goto * step 2*. * * step 8 * end .",
    "* - frequency estimator *    we exploit the frequency estimator",
    "_ multiple signal classificator _",
    "( music ) .",
    "it takes as input the weight matrix columns after the learning .",
    "the estimated signal frequencies are obtained as the peak locations of the following function ( kay 1988 ; marple 1987 ) : @xmath92    where @xmath93 is the @xmath94th weight vector after learning , and @xmath95 is the pure sinusoidal vector : @xmath96^h $ ] .",
    "when @xmath10 is the frequency of the @xmath94th sinusoidal component , @xmath97 , we have @xmath98 and @xmath99 . in practice",
    "we have a peak near and in correspondence of the component frequency .",
    "estimates are related to the highest peaks .",
    "in this section we show the relation between the music estimator and the cramer - rao bound following the notation and the conditions proposed by stoica and nehorai in their paper ( stoica and nehorai 1990 ) .",
    "the problem under consideration is to determine the parameters of the following model :    @xmath100    where @xmath101 are the vectors of the observed data , @xmath102 are the unknown vectors and @xmath103 is the added noise ; the matrix @xmath104 and the vector @xmath105 are given by @xmath106 ; \\qquad \\qquad { \\bf \\theta } = \\left [ \\omega _ { 1} ... \\omega _ { n}\\right ]   \\label{eq*.2}\\ ] ]    where @xmath107 varies with the applications .",
    "our aim is to estimate the unknown parameters of @xmath108 .",
    "the dimension @xmath109 of @xmath110 is supposed to be known a priori and the estimate of the parameters of @xmath110 is easy once @xmath108 is known .",
    "now , we reformulate music to follow the above notation . the music estimate is given by the position of the @xmath109 smallest values of the following function : @xmath111 { \\bf a}\\left ( \\omega \\right ) \\label{eq*.3}\\ ] ]    from equation([eq*.3 ] ) we can define the estimation error of a given parameter .",
    "@xmath112 has ( for big @xmath12 ) an asintotic gaussian distribution , with @xmath113 mean and with the following covariance matrix : @xmath114    where @xmath115 is the real part of @xmath0 , where @xmath116 d   \\label{eq*.5}\\ ] ]    and where @xmath117 is implicitly defined by : @xmath118    where @xmath119 is the covariance matrix of @xmath120 .",
    "the elements of the diagonal of the matrix @xmath121 are the variances of the estimation error . on the other hand , the cramer - rao lower bound of the covariance matrix of every estimator of @xmath108 , for large @xmath12 ,",
    "is given by : @xmath122 \\right\\}^{-1}.   \\label{eq*.7}\\ ] ]    therefore the statistical efficiency can be defined with the condition that p is diagonal as : @xmath123 _ { ii}\\geq \\left [ c_{cr}\\right ] _ { ii }   \\label{eq*.8}\\ ] ]    where the equality is reached when @xmath124 increases if and only if    @xmath125    for @xmath119 non - diagonal , @xmath126 _ { ii}>\\left [ c_{cr}\\right]_{ii}$ ] .",
    "to adapt the model used in the spectral analysis @xmath127    where @xmath60 is the total number of samples , to equation([eq*.3 ] ) we make the following transformations , after fixing an integer @xmath128 : @xmath129   \\nonumber \\\\",
    "{ \\bf a}\\left ( \\omega \\right ) & = & \\left [ 1\\quad e^{j\\omega } \\quad e^{j\\omega \\left ( m-1\\right ) } \\right ]   \\label{eq*.23 } \\\\ { \\bf x}(t ) & = & \\left [ a_{1}e^{j\\omega _ { 1}t}\\quad ... \\quad a_{n}e^{j\\omega _ { n}t}\\right ] \\qquad t=1, ... ,m - m+1   \\nonumber\\end{aligned}\\ ] ]    in this way our model satisfies the conditions of ( stoica and nehorai 1990 ) .",
    "moreover , equations([eq*.23 ] ) depend on the choice of @xmath124 which influences the minimum error variance .      in this subsection",
    "we compare the n.e .",
    "method with the cramer - rao lower bound , by varying the frequencies distance , the parameters @xmath60 and @xmath124 and the noise variance .    from the experiments it derives that , fixed @xmath60 and @xmath124 , by varying the noise ( white gaussian ) variance , the n.e .",
    "estimate is more accurate for small values of the noise variance as shown in figures 1 - 3 . for @xmath130 small",
    ", the noise variance is far from the bound . by increasing @xmath124",
    "the estimate improves , but there is a sensitivity to the noise ( figures 4 - 6 ) . by varying @xmath60 , there is a sensitivity of the estimator to the number of points and to @xmath124 ( figures 7 - 8 ) .",
    "in fact , if we have a quite large number of points we reach the bound as illustrated in figures 9 - 10 .     and @xmath131 .",
    "+ crb ( down ) ; standard deviation of n.e .",
    "( up ) with @xmath132 , @xmath133 and @xmath134.,width=302,height=264 ]    therefore , the n.e .",
    "estimate depends on both the increase of @xmath124 and the number of points in the input sequence . increasing the number of points ,",
    "we improve the estimate and the error approximates the cramer - rao bound . on the other hand , for noise variances very small , the estimate reaches a very good performance .",
    "finally , we see that in all the experiments shown in the figures we reach the bound with a good approximation , and we can conclude that the n.e .",
    "method is statistically efficient .",
    "in this section we show the performance of the neural based estimator system using artificial and real data .",
    "the artificial data are generated following the literature ( kay 1988 ; marple 1987 ) and are noisy sinusoidal mixtures .",
    "these are used to select the neural models for the next phases and to compare the n.e . with p s , by using montecarlo methods to generate samples .",
    "real data , instead , come from astrophysics : in fact , real signals are light curves of cepheids and a light curve in the johnson s system .    in the sections [ realistic data ] and [ real data ] , we use an extension of music to directly include unevenly sampled data without using the interpolation step of the previous algorithm in the following way :    @xmath135    where @xmath136 is the frequency number ,",
    "@xmath93 is the @xmath94th weight vector of the pca neural network after the learning , and @xmath137 is the sinusoidal vector : @xmath138^{h}$ ] where @xmath139 are the first @xmath47 components of the temporal coordinates of the uneven signal .    furthermore , to optimise the performance of the pca neural networks , we stop the learning process when @xmath140 , so avoiding overfitting problems .      in this section",
    "we use synthetic data to select the neural networks used in the next experiments . in this case , the uneven sampling is obtained by randomly deleting a fixed number of points from the synthetic sinusoid - mixtures : this is a widely used technique in the specialised literature ( horne & baliunas 1986 ) .",
    "the experiments are organised in this way .",
    "first of all , we use synthetic unevenly sampled signals to compare the different neural algorithms in the neural estimator ( n.e . ) with the scargle s",
    "p.    for this type of experiments ,",
    "we realise a statistical test using five synthetic signals .",
    "each one is composed by the sum of five sinusoids of randomly chosen frequencies in @xmath141 $ ] and randomly chosen phases in @xmath142 $ ] ( kay 1988 ; karhunen & joutsensalo 1994 ; marple 1987 ) , added to white random noise of fixed variance .",
    "we take @xmath143 samples of each signal and randomly discard @xmath144 of them ( @xmath145 points ) , getting an uneven sampling ( horne & baliunas 1986 ) . in this way we have several degree of randomness : frequencies , phases , noise , deleted points .",
    "after this , we interpolate the signal and evaluate the p and the n.e .",
    "system with the following neural algorithms : robust algorithm in equation([eq34 ] ) in the hierarchical and symmetric case ; nonlinear algorithm in equation([eq310 ] ) in the hierarchical and symmetric case .",
    "each of these is used with two nonlinear learning functions : @xmath146 and @xmath147 .",
    "therefore we have eight different neural algorithms to evaluate .",
    "we chose these algorithms after we made several experiments involving all the neural algorithms presented in section [ section3 ] , with several learning functions , and we verified that the behaviour of the algorithms and learning functions cited above was the same or better than the others .",
    "so we restricted the range of algorithms to better show the most relevant features of the test .",
    "we evaluated the average differences between target and estimated frequencies .",
    "this was repeated for the five signals and then for each algorithm we made the average evaluation of the single results over the five signals .",
    "the less this averages were , the greatest the accuracy was .",
    "we also calculated the average of the number of epochs and cpu time for convergence .",
    "we compare this with the behaviour of p.    common signals parameters are : number of frequencies @xmath148 , variance noise @xmath149 , number of sampled points @xmath150 , number of deleted points @xmath151 .",
    "signal 1 : frequencies=@xmath152    signal 2 : frequencies=@xmath153    signal 3 : frequencies=@xmath154    signal 4 : frequencies=@xmath155    signal 5 : frequencies=@xmath156    neural parameters : @xmath157 ; @xmath158 ; @xmath159 ; number of points in each pattern @xmath160 ( these are used for almost all the neural algorithms ; however , for a few of them a little variation of some parameters is required to achieve convergence ) .",
    "scargle parameters : @xmath161 , @xmath162 .",
    "results are shown in table  [ table61 ] :    [ cols=\"^,^,^,^,^,^,^,^,^ \" , ]      the first real signal is related to the cepheid su cygni ( fernie 1979 ) .",
    "the sequence was obtained with the photometric tecnique ubvri and the sampling made from june to december 1977 .",
    "the light curve is composed by 21 samples in the v band , and a period of @xmath163 , as shown in figure 17 . in this case , the parameters of the n.e .",
    "are : @xmath164 , @xmath165 , @xmath166 , @xmath167 .",
    "the estimate frequency interval is @xmath168   $ ] .",
    "the estimated frequency is @xmath169 ( 1/jd ) in agreement with the lomb s p , but without showing any spurious peak ( see figures 18 and 19 ) .    the second real signal is related to the cepheid u aql ( moffet and barnes 1980 ) .",
    "the sequence was obtained with the photometric tecnique bvri and the sampling made from april 1977 to december 1979 .",
    "the light curve is composed by 39 samples in the v band , and a period of @xmath170 , as shown in figure 20 . in this case , the parameters of the n.e .",
    "are : @xmath171 , @xmath165 , @xmath172 , @xmath173 .",
    "the estimate frequency interval is @xmath174 $ ] .",
    "the estimated frequency is @xmath175 ( 1/jd ) in agreement with the lomb s p , but without showing any spurious peak ( see figures 21 and 22 ) .    the third real signal is related to the cepheid x cygni ( moffet and barnes 1980 ) .",
    "the sequence was obtained with the photometric technique bvri and the sampling made from april 1977 to december 1979 .",
    "the light curve is composed by 120 samples in the v band , and a period of @xmath176 , as shown in figure 23 . in this case ,",
    "the parameters of the n.e .",
    "are : @xmath177 , @xmath165 , @xmath172 , @xmath173 .",
    "the estimate frequency interval is @xmath178 $ ] .",
    "the estimated frequency is @xmath179 ( 1/jd ) in agreement with the lomb s p , but without showing any spurious peak ( see figures 24 and 25 ) .    the fourth real signal is related to the cepheid t mon ( moffet and barnes 1980 ) . the sequence was obtained with the photometric technique bvri and the sampling made from april 1977 to december 1979 .",
    "the light curve is composed by @xmath180 samples in the v band , and a period of @xmath181 , as shown in figure 26 . in this case , the parameters of the n.e .",
    "are : @xmath164 , @xmath165 , @xmath172 , @xmath173 .",
    "the estimate frequency interval is @xmath178 $ ] .",
    "the estimated frequency is @xmath182 ( 1/jd ) ( see figure 28 ) .",
    "the lomb s p does not work in this case because there many peaks , and at least two greater than the threshold of the most accurate confidence interval ( see figure 27 ) .",
    "the fifth real signal we used for the test phase is a light curve in the johnson s system ( binnendijk 1960 ) for the eclipsing binary u peg ( see figure 29 and 30 ) .",
    "this system was observed photoelectrically in the effective wavelengths 5300 a and 4420 a with the 28-inch reflecting telescope of the flower and cook observatory during october and november , 1958 .",
    "we made several experiments with the n.e . , and we elicited a dependence of the frequency estimate on the variation of the number of elements for input pattern .",
    "the optimal experimental parameters for the n.e .",
    "are : @xmath183 , @xmath184 ; @xmath173 . the period found by the n.e .",
    "is expressed in @xmath185 and is not in agreement with results cited in literature ( binnendijk 1960 ) , ( rigterink 1972 ) , ( zhai et al . 1984),(lu 1985 ) and ( zhai et al . 1988 ) .",
    "the fundamental frequency is @xmath186 @xmath187 ( see figure 31 ) instead of @xmath188 @xmath187 .",
    "we obtain a frequency double of the observed one .",
    "lomb s p has some high peaks as in the previous experiments and the estimated frequency is always the double of the observed one ( see figure 32 ) .",
    "we have realised and experimented a new method for spectral analysis for unevenly sampled signals based on three phases : preprocessing , extraction of principal eigenvectors and estimate of signal frequencies .",
    "this is done , respectively , by input normalization , nonlinear pca neural networks , and the multiple signal classificator algorithm .",
    "first of all , we have shown that neural networks are a valid tool for spectral analysis .",
    "however , above all , what is really important is that neural networks , as realised in our neural estimator system , represent a new tool to face and solve a problem tied with data acquisition in many scientific fields : the unevenly sampling scheme .",
    "experimental results have shown the validity of our method as an alternative to periodogram , and in general to classical spectral analysis , mainly in presence of few input data , few a priori information and high error probability . moreover , for unevenly sampled data ,",
    "our system offers great advantages with respect to p. first of all , it allows us to use a simple and direct way to solve the problem as shown in all the experiments with synthetic and cepheid s real signals .",
    "secondly , it is insensitive to the frequency interval : for example , if we expand our interval in the su cygni light curve , while our system finds the correct frequency , the lomb s p finds many spurious frequencies , some of them greater than the confidence threshold , as shown in figures 33 and 34 .",
    "furthermore , when we have a multifrequency signal , we can use our system also if we do not know the frequency number .",
    "in fact , we can detect one frequency at each time and continue the processing after the cancellation of the detected periodicity by iir filtering .",
    "a point worth of noting is the failure to find the right frequency in the case of eclipsing binary for both our method and lomb s p. taking account of the morphology of eclipsing light curve with two minima , this fact can not be of concern because in practical cases the important thing is to have a first guess of the orbital frequency",
    ". further refinement will be possible through a wise planning of observations .",
    "in any case we have under study this point to try to find a method to solve the problem .",
    "the authors would like to thank dr .",
    "m. rasile for the experiments related to the model selection and an unknown referee for his comments who helped the authors to improve the paper .",
    "oja e. , karhunen j. , wang l. , vigario r. , 1996 , principal and independent components in neural networks - recent developments . in marinaro m. ,",
    "tagliaferri r. ( eds ) , wirn vietri 95,world scientific pu . , singapore , p. 16",
    "- 35      plumbley m. , 1993 , a hebbian / anti hebbian network which optimizes information capacity by orthonormalizing the principal subspace . in j. taylor",
    "al ( eds ) proc .",
    "int . conf . on artificial neural networks ,"
  ],
  "abstract_text": [
    "<S> periodicity analysis of unevenly collected data is a relevant issue in several scientific fields . in astrophysics , for example </S>",
    "<S> , we have to find the fundamental period of light or radial velocity curves which are unevenly sampled observations of stars . </S>",
    "<S> classical spectral analysis methods are unsatisfactory to solve the problem . in this paper </S>",
    "<S> we present a neural network based estimator system which performs well the frequency extraction in unevenly sampled signals . </S>",
    "<S> it uses an unsupervised hebbian nonlinear neural algorithm to extract , from the interpolated signal , the principal components which , in turn , are used by the music frequency estimator algorithm to extract the frequencies . </S>",
    "<S> the neural network is tolerant to noise and works well also with few points in the sequence . </S>",
    "<S> we benchmark the system on synthetic and real signals with the periodogram and with the cramer - rao lower bound . </S>"
  ]
}