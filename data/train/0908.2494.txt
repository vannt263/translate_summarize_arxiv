{
  "article_text": [
    "as new content mushrooms at a brisk pace , finding relevant information is increasingly a challenge . consequently , recommendation systems are commonly being used to assist users : amazon recommends books , netflix recommends movies , linkedin recommends professional contacts , google recommends webpages for a given query , etc .",
    "such recommendation systems exploit various aspects to make suggestions : popularity amongst peers , similarity of content , available user - item ratings , etc .",
    "this paper is about collaborative filtering using the rating matrix : we are interested in making recommendations using only available ratings given by users to the items they have experienced . in a practical system , such a rating based collaborative filter is typically complemented by content - based analysis specific to the data .",
    "there is vast literature on recommendation systems and collaborative filtering ; see for example the special issue @xcite and the survey paper @xcite .",
    "given the massive datasets and the lack of good statistical model of user behavior , the dominant stream of work has been to propose methods and demonstrate their scalability on real data sets .",
    "however , recently the netflix prize @xcite has popularized the problem to other research communities and several researchers have started exploring _ provably _ good methods .",
    "this paper falls in the latter category : we deal with fundamental limits of collaborative filters . in the remainder of this section , we first discuss related models and results , and then outline our model and results .",
    "the netflix data consists of rating matrix where the rows correspond to movies and the columns correspond to users .",
    "only a small fraction of the entries are known and the goal is to estimate the missing entries , that is , this is a matrix completion problem .",
    "several algorithms have been proposed and tested on this data set ; see for example @xcite .",
    "mathematically , without any further restriction , this is an ill - posed problem .",
    "motivated by this , some authors have recently considered the matrix completion problem under the restriction of low - rank matrices .",
    "( this problem also arises in other contexts such as location estimation in sensor networks . )",
    "this problem has attracted much attention , and in the past year a number of results have been reported . in @xcite , using nuclear norm minimization proposed in @xcite , an upper bound on the number of samples needed for recovery asymptotically is derived in terms of the size and rank of the matrix . in @xcite , a lower bound is established on the number of samples needed by any algorithm .",
    "the order of this lower bound is shown to be achievable in @xcite . in @xcite ,",
    "the problem of matrix recovery from linear measurements ( of which sampling is a special case ) is considered and a new algorithm is proposed . in @xcite , the problem of matrix completion under bounded noise is considered .",
    "a semi - definite programming based algorithm is proposed and shown to have recovery error proportional to the noise magnitude .    in this paper , we take an alternative channel coding viewpoint of the problem .",
    "our results differ from the above works in several aspects outlined below .",
    "* we consider finite alphabet for the ratings and a different model for the rating matrix based on row and column clusters .",
    "* we consider noisy user behavior , and our goal is not to complete the missing entries , but to estimate an underlying  block constant \" matrix ( in the limit as the matrix size grows ) . *",
    "since we consider a finite alphabet , even in the presence of noise , error free recovery is asymptotically feasible .",
    "hence , unlike @xcite , which considers real - valued matrices , we do not allow any distortion .",
    "we next outline our model and results .",
    "we consider a finite alphabet for the ratings . in this section",
    ", we briefly outline our model and results without any mathematical details ; the details can be found in subsequent sections .    to motivate our model , consider an ideal situation where every user rates every item without any noise . in this ideal scenario , it is reasonable to expect that similar users rate similar items by the same value .",
    "we therefore assume that the users ( items ) are clustered into groups of similar users ( items , respectively ) .",
    "the rating matrix in this ideal situation ( say @xmath3 with size @xmath1 ) is then a block constant matrix ( where the blocks correspond to cartesian product of row and column clusters ) .",
    "the observations are obtained from @xmath3 by passing its entries through a discrete memoryless channel ( dmc ) consisting of an erasure channel modeling missing data and a noisy dmc representing noisy user behavior . moreover , the row and column clusters are _ unknown_. the goal is to make recommendations by estimating @xmath3 based on the observations .",
    "the performance metric we use is the probability of block error : we make an error if any of the entries in the estimate is erroneous .",
    "our goal is to identify conditions under which error free recovery is possible in the limit as the matrix size grows large .",
    "thus we view the recommendation system problem as a channel coding problem .",
    "the cluster sizes in our model represent the resolution : the larger the cluster , the smaller are the degrees of freedom ( or rate of the channel code ) .",
    "if the channel is more noisy and the erasures are high , then we can only support a small number of codewords .",
    "the challenge is to find the exact order . for our model , we show that if the largest cluster size ( defined precisely in section [ sect : bin ] ) is smaller than @xmath0 , where @xmath4 is a constant dependent on the channel parameters , then for any estimator the probability of error approaches one . on the other hand , if the smallest cluster size ( defined precisely in section [ sect : bin ] ) is larger than @xmath2 , where @xmath5 is a constant dependent on the channel parameters , then we give a polynomial time algorithm that has diminishing probability of error .",
    "thus we identify the order of the threshold exactly . in the case of uniform cluster size , the constants @xmath4 and @xmath5 are identical and thus in this special case , even the constant is identified precisely . moreover , for the special case of binary ratings and uniform cluster size , the algorithm used to show the achievability part does nor depend on the cluster size , erasure parameter , and needs knowledge of a worst case parameter for the noisy part of the channel .",
    "these results are obtained by averaging over @xmath3 ( as per the probability law specified in section [ sect : model ] ) .",
    "the achievability part of our result is shown by first clustering the rows and columns , and then estimating the matrix entries assuming that the clustering is correct .",
    "the clustering is done by computing a normalized hamming metric for every pair of rows and comparing with a threshold to determine if the rows are in same cluster or not .",
    "the converse is proved by considering the case when the clusters are known exactly .",
    "our results for the average case show that the threshold is determined by the problem of estimating entries , and relatively , clustering is an easier task ( see figure [ fig : bounds ] for an illustration ) .    ) , the asymptotic cluster size threshold from theorem [ thm : bin_main ] , and an upper bound on the clustering error ( theorem [ thm : bin_c ] ) for the case @xmath6 , erasure probability @xmath7 , and binary symmetric channel with error @xmath8 .",
    "the threshold in the clustering algorithm is chosen to be @xmath9 . ]      the precise model for @xmath3 and the observations is stated in section [ sect : model ] .",
    "the case of uniform cluster size and binary ratings leads to sharper bounds and results .",
    "hence results for this case are given in section [ sect : bin ] .",
    "the case of general alphabets and non - uniform cluster sizes is considered in section [ sect : general ] .",
    "the conclusion is given in section [ sect : con ] , while all the proofs are collected together in section [ sect : proofs ] .",
    "all the logarithms are to the natural base unless specified otherwise .",
    "@xmath10 denotes the kl divergence ( @xcite ) between probability mass functions @xmath11 and @xmath12 .",
    "by @xmath13 we mean that for @xmath14 large enough , @xmath15 .",
    "by @xmath16 we denote the indicator variable , which is 1 if @xmath17 is true and 0 otherwise .",
    "the main elements of our model are a block constant ensemble of rating matrices ( whose blocks of constancy are not known ) and an observation matrix obtained from the underlying rating matrix via a noisy channel and erasures .",
    "the noise in the observations represents the inherent noise in user - item ratings as well as the error in our model .",
    "the erasures denote missing entries . to be more precise ,",
    "suppose @xmath3 is the unknown @xmath1 rating matrix with entries from a finite alphabet , where @xmath14 is the number of buyers and @xmath18 is the number of items .",
    "let @xmath19 and @xmath20 be partitions of @xmath21 $ ] and @xmath22 $ ] respectively .",
    "we call the sets @xmath23 clusters and we call @xmath24 s ( @xmath25 s ) the row ( column ) clusters .",
    "we denote the corresponding row and column cluster sizes by @xmath26 and @xmath27 , and the number of row clusters and the number of column clusters by @xmath28 and @xmath29 respectively .",
    "thus @xmath30 , @xmath31 .",
    "we state our results under two sets of conditions - the set of conditions a1)-a4 ) and b1)-b3 ) below .",
    "conditions a1)-a4 ) are a special case of conditions b1)-b3 ) .",
    "the results under a1)-a4 ) are sharper and illustrate the important concepts more easily . hence they are stated separately .",
    "we begin by stating and discussing a1)-a4 ) first and then we state b1)-b3 ) .",
    "( a few additional conditions needed in the results are stated at appropriate places . ) + * conditions a1)-a4 ) : * the conditions a1)-a4 ) below correspond to binary rating matrix with equal size clusters and uniform probability of sampling entries .    1 .   the entries of @xmath3 are from @xmath32 .",
    "the row ( column ) clusters are of equal size : @xmath33 , @xmath34 for all @xmath35 .",
    "@xmath3 is constant over the cluster @xmath23 and the entries are i.i.d .",
    "bernoulli(1/2 ) across the clusters . 4 .",
    "the observed data @xmath36 ( @xmath37 denotes erasure ) is obtained by passing the entries of @xmath3 through the cascade of a binary symmetric channel ( bsc ) with probability of error @xmath38 and an erasure channel with erasure probability @xmath39 .",
    "the cluster sizes are representative of the _ resolution _ of @xmath3 - large cluster sizes correspond to a coarse structure with fewer degrees of freedom in choosing @xmath3 , while small cluster size corresponds to a fine structure",
    ". condition a2 ) suggests that we can think of the cluster size @xmath40 as representative of the resolution of @xmath3 and it plays a central role in our results .",
    "if we think of all permissible @xmath3 as a channel code , then a higher @xmath40 corresponds to a smaller rate code .",
    "however , in order to interpret @xmath40 precisely , we also need to take into account condition a3 ) . when the entries of the cluster are filled with i.i.d .",
    "bernoulli(1/2 ) random variables as per a3 ) , it is likely that rows in two clusters turn out to be the same , and hence these two row clusters can be merged to form a single bigger cluster .",
    "the following lemma shows that if the number of clusters is @xmath41 , then this happens with small probability and hence we should think of @xmath40 as _ the _ representative cluster size .",
    "[ lem : mn ] if @xmath42 , @xmath43 , then @xmath44 and a similar result holds for the column clusters .",
    "each row is uniformly distributed over @xmath45 possibilities and rows in different clusters are independent .",
    "hence the probability that any given pair of rows is same is @xmath46 .",
    "since there are @xmath47 pairs , we then have @xmath48 since @xmath49 , we have @xmath50 hence if @xmath51 for some @xmath52 , then @xmath53       ' '' ''    condition a3 ) also implies that in any row or column , for large matrices , roughly the number of 0s and 1s is same .",
    "this essentially implies that the opinions are diverse for any user or item .",
    "while this may seem unrealistic ( and can indeed be fixed ) , we prefer the bernoulli(1/2 ) model for the following reason : under this assumption no recommendations can be extracted from any row or column alone and thus collaborative filtering is necessary .",
    "such a model is desirable for evaluation of collaborative filtering schemes .",
    "moreover , one can pre - process data so that rows and columns with fraction of 1s far from 1/2 are removed ( because they are relatively easy to recommend ) and then assumption a3 ) is reasonable .",
    "we note that in condition a3 ) , we only specify the probability law of @xmath3 given the clusters ; the clusters are deterministic , even though they are unknown .",
    "the bsc in a4 ) models the inherent noise in user - item ratings as well as modeling error , while the erasure channel models the missing data .",
    "+ * conditions b1)-b3 ) : * these conditions are more general allowing any finite alphabet and non - uniform cluster sizes .    1",
    ".   the entries of @xmath3 are from a finite alphabet @xmath54 .",
    "@xmath3 is constant over the cluster @xmath23 and the entries across the clusters are i.i.d .  with a uniform distribution over @xmath54 .",
    "the observed data @xmath55 ( @xmath37 denotes erasure ) is obtained from @xmath3 as follows 1 .",
    "the entries of @xmath3 are passed through a dmc with probability law @xmath56 and output alphabet @xmath54 , resulting in @xmath57 .",
    "the entries @xmath58 are then passed through an erasure channel with erasure probability @xmath39 .",
    "in this section , we state our results under conditions a1)-a4 ) . the main result of this section appears in section [",
    "sect : bin_main ] .",
    "it is obtained by studying two quantities : probability of error when the clustering is _ known _ ( section [ sect : bin_knownc ] ) and probability error in clustering for a specific algorithm ( section [ sect : bin_c ] ) .",
    "our main result stated below identifies a threshold on the cluster size above which error free recovery is asymptotically feasible but below which error free recovery is not possible .",
    "[ thm : bin_main ] suppose conditions a1)-a4 ) are true and the clusters are _",
    "unknown_. let @xmath59 .",
    "suppose that @xmath60 and @xmath61 $ ] , @xmath62 .    1 .",
    "* converse : * if @xmath63 then @xmath64 for _ any _ estimator .",
    "* achievability : * if @xmath65 , @xmath66 , @xmath67 , @xmath68 and @xmath69 then @xmath70 for the following polynomial time estimator : * cluster rows and columns using the algorithm of section [ sect : bin_c ] using the threshold @xmath71 ( which does not depend on @xmath72 ) . * employ majority decoding in a cluster ( as in section [ sect : bin_knownc ] ) assuming the clustering to be correct .    the proof is given in section [ sect : proof_bin_main ] .       ' '' ''    the result identifies @xmath73 as the cluster size threshold . the first part states that if the cluster size is too small , then any estimator makes an error with high probability .",
    "the second part states that if the cluster size is large enough , then diminishing probability of error can be achieved with a polynomial time estimator , which does not need knowledge of @xmath72 and needs only knowledge of a worst case bound on @xmath38 .",
    "the result is reminiscent of the channel coding theorem in the context of our model .",
    "the proof of part 1 ) of theorem [ thm : bin_main ] relies on lower bounding @xmath74 by considering the case of known clustering ( see theorem [ thm : bin_knownc ] in section [ sect : bin_knownc ] ) .",
    "the proof of part 2 ) of theorem [ thm : bin_main ] relies on showing that for the average case , the probability of error in clustering is much smaller than the probability of error in filling values when the clusters are known ( see theorem [ thm : bin_c ] in section [ sect : bin_c ] ) .",
    "we illustrate this in figure [ fig : bounds ] by plotting various bounds : for @xmath6 , @xmath75 ranging from 10 to 150 , @xmath8 and @xmath7 , we plot    * upper and lower bounds for probability of error when clustering is known ( from theorem [ thm : bin_knownc ] ) , * upper bound on probability of clustering error ( from theorem [ thm : bin_c ] ) , * and the asymptotic threshold @xmath73 ( from theorem [ thm : bin_main ] ) .",
    "it is seen that around the asymptotic threshold , the probability of clustering error is dominated by the probability of error in filling values under known clustering .      in this section ,",
    "we consider the case when the clusters are known . under this assumption",
    ", the decoder only has to estimate the value in a cluster , and the minimum probability of error estimator under a3 ) is just a majority decoder .",
    "the analysis of this decoder is elementary and we state a stronger result for a fixed @xmath3 with possibly unequal cluster sizes .",
    "let @xmath76 where @xmath77 and @xmath78 are the row and column cluster sizes in @xmath3 .",
    "[ thm : bin_knownc ] suppose conditions a1 ) , a3 ) are true and in addition assume that the clusters are known .",
    "let @xmath79 then the probability of error in filling in values satisfies @xmath80 suppose we are given a sequence of rating matrices of increasing size , that is , @xmath81",
    ". then the following are true .    1 .   if @xmath82 then @xmath83 .",
    "2 .   if @xmath84 then @xmath85 .",
    "the proof is given in section [ sect : proof_bin_knownc ] .       ' '' ''    we note that when all the clusters are of the same size ( which happens with high probability as per lemma [ lem : mn ] ) , then the above result states that there is a sharp threshold : if the cluster size is smaller than @xmath73 , then exact recovery is not possible , but if it is larger , then we can make probability of error as small as we wish .",
    "+ * example : * for @xmath6 , @xmath86 , @xmath7 , @xmath8 , this threshold corresponds to clusters of size about @xmath87 .",
    "we plot the lower and upper bounds for @xmath88 from theorem [ thm : bin_knownc ] and the threshold in figure [ fig : bounds ] . + * remark : * a finer analysis reveals that we can refine part 2 ) of theorem [ thm : bin_knownc ] ( and hence also part 1 ) of theorem [ thm : bin_main ] ) by letting @xmath89 approach zero as @xmath90 .",
    "the result holds as long as @xmath91 .      to get an upper bound on the probability of error @xmath74 , in this section we analyze a specific collaborative filter",
    ": we first cluster the rows and columns using the algorithm described below and then we fill in values using the majority decoder assuming that the clustering is correct .",
    "the majority decoder has already been analyzed in section [ sect : bin_knownc ] and for proving part 2 of theorem [ thm : bin_main ] , we only need to analyze the probability of error in clustering .",
    "* clustering algorithm : * we cluster rows and columns separately . for rows",
    "@xmath92 , the normalized hamming distance over commonly sampled entries is @xmath93 where @xmath94 is the number of commonly sampled positions in rows @xmath35 and @xmath95 , given by @xmath96 let @xmath97 be equal to 1 if rows @xmath35 , @xmath95 belong to the same cluster and let it be 0 otherwise .",
    "the algorithm gives an estimate : @xmath98 where @xmath99 is a treshold whose choice will be discussed later .",
    "a similar algorithm is used to cluster columns .",
    "we are interested in the probability that we make an error in row clustering averaged over the probability law on the rating matrices defined as @xmath100 we note that this is a conservative definition of clustering error .",
    "as seen in lemma [ lem : mn ] , there is a small chance that rows in different clusters may be the same resulting in the merging of two clusters into a larger one .",
    "the above definition of error does not account for this and declares more errors .",
    "we use this conservative definition of clustering error to simplify analysis .",
    "[ thm : bin_c ] suppose conditions a1)-a4 ) are true .",
    "let @xmath101 , @xmath102 be constants and let @xmath103 be the smaller root of the quadratic equation @xmath104 where @xmath105 .",
    "suppose the threshold @xmath106 .",
    "let @xmath107 then for the above clustering algorithm , @xmath108 where @xmath109 and @xmath110 for a positive constant @xmath111 .",
    "the proof is given in section [ sect : proof_bin_c ] .       ' '' ''    the proof uses the union bound and considers pairwise errors .",
    "the pairwise errors consists of two cases : error when the pair of rows is in the same cluster and error when they are in different clusters .",
    "the probability of the first kind of error is exponentially decaying in @xmath14 .",
    "the probability of the second kind of error is upper bounded by the minimum of @xmath112 and @xmath113 : while @xmath112 is tight for finite @xmath14 and large @xmath114 , the bound @xmath113 is useful for establishing asymptotic results ( like theorem [ thm : bin_main ] ) for _ all _ @xmath114 .",
    "for example , in figure [ fig : bounds ] , the upper bound on clustering error is dominated by @xmath112 , while the proof of part 2 ) of theorem [ thm : bin_main ] uses @xmath113 .",
    "we note that both @xmath112 and @xmath113 have terms that decay exponentially in @xmath14 as well as @xmath29 .",
    "the terms decaying exponentially in @xmath29 are related to lemma [ lem : mn ] and the conservative definition of clustering error as discussed before the statement of theorem [ thm : bin_c ] .",
    "these terms are the origin of the @xmath115 condition in part 2 ) of theorem [ thm : bin_main ] and can perhaps be avoided with more sophisticated analysis ; however , we prefer to work with this condition since as per lemma [ lem : mn ] , the condition @xmath115 is anyway needed for interpreting @xmath116 as the representative cluster size .",
    "in this section , we consider a general finite alphabet @xmath54 and non - uniform cluster sizes .",
    "we work with assumptions b1)-b3 ) described in the section [ sect : model ] and generalize the results in section [ sect : bin ] .",
    "to state our results , we first introduce some notation . for @xmath117 , define @xmath118 if @xmath119 , @xmath120 are i.i.d .",
    "uniform on @xmath54 and we pass them through the dmc @xmath121 to get outputs @xmath122 , @xmath123 , then @xmath124 the following useful lemma sheds light on the relationship between @xmath125 and @xmath126 .",
    "[ lem : dineq ] for any dmc , @xmath127 , with equality iff @xmath128 @xmath129 .    the proof is given in section [ proof : lem : dineq ] .       ' '' ''    we next state our main result for general finite alphabet and non - uniform cluster size .",
    "[ thm : dmc_main ] suppose conditions b1)-b3 ) are true and the clusters are _ unknown_. then there exist constants @xmath130 , @xmath131 such that    1 .   *",
    "converse : * if @xmath132 then @xmath64 for _ any _ estimator .",
    "* achievability : * suppose that there exist some @xmath133 such that @xmath134 and @xmath135 .",
    "( by lemma [ lem : dineq ] , this ensures that @xmath136 . ) if @xmath137 , @xmath138 , @xmath67 , @xmath68 and @xmath139 then @xmath70 for the following polynomial time estimator : * cluster rows and columns using the algorithm of section [ sect : bin_c ] using the threshold @xmath140 ( which does not depend on @xmath141 ) . *",
    "employ maximum likelihood decoding in a cluster assuming the clustering is correct .",
    "the proof is similar to theorem [ thm : bin_main ] ; we now use theorems [ thm : dmc_knownc ] and [ thm : gen_clus ] in place of theorems [ thm : bin_knownc ] and [ thm : bin_c ] respectively .       ' '' ''    the above result again identifies @xmath142 as the exact order of the cluster size threshold for asymptotic recovery .",
    "similar to the binary alphabet and uniform cluster size case in section [ sect : bin ] , the constants @xmath143 , @xmath144 arise from the case when the clusters are known ( see theorem [ thm : dmc_knownc ] below ) .",
    "the gap between the constants @xmath145 can be made arbitrarily small : the proof of theorem [ thm : dmc_knownc ] identifies a constant @xmath4 ( see equation ) such that for any @xmath52 , @xmath146 is a valid choice in theorem [ thm : dmc_main ] .",
    "we next consider the case when the clusters are known and extend theorem [ thm : bin_knownc ] .",
    "[ thm : dmc_knownc ] suppose conditions b1)-b3 ) are true and in addition assume that the clusters are _ known_. also let @xmath147 where @xmath148 are as defined above .",
    "then for a sequence of rating matrices of increasing size @xmath81 , the following are true .    1 .",
    "if @xmath149 then @xmath83 .",
    "2 .   if @xmath150 then @xmath85 .",
    "the proof is given in section [ sect : proof : thm : dmc_knownc ] .       ' '' ''    finally , we study the performance of the clustering algorithm and extend theorem [ thm : bin_c ] .",
    "[ thm : gen_clus ] suppose conditions b1)-b3 ) are true and in addition suppose that there exist some @xmath133 such that @xmath134 and @xmath135 .",
    "( by lemma [ lem : dineq ] , this ensures that @xmath136 . ) if we choose the threshold @xmath140 , then @xmath151 for some positive constants @xmath152 , @xmath153 .",
    "consequently , if @xmath137 , then @xmath154 as @xmath155 .",
    "the proof is given in section [ sect : proof : thm : gen_clus ] .       ' '' ''",
    "we take a channel coding perspective of collaborative filtering and identify the threshold on cluster size for perfect reconstruction of the underlying rating matrix .",
    "the result is similar in flavor to some recent results in completion of real - valued matrices .",
    "the advantage of our model is that the proofs are relatively simple relying on chernoff bounds and noisy user behavior can be easily handled .    in the typical applications of recommendation systems ,",
    "there is a lack of good models .",
    "we believe that our model has two characteristics that make it suitable for analytical comparison of various methods : a ) in our model the user opinions are diverse and no single user / item reveals much information about itself , that is , collaborative filtering is necessary ; b ) as we have shown , the model is analytically tractable .",
    "there are several directions where this model may turn out to be useful : analysis of bit error probability instead of block error probability , analysis of local popularity based mechanisms , etc .",
    "when @xmath156 are known , under our model all feasible rating matrices are equally likely .",
    "hence the ml decoder gives the minimum probability of error and so we have @xmath157 .",
    "$ ] to prove part 1 ) , we lower bound @xmath158 $ ] .",
    "let @xmath159 be the event that @xmath160 .",
    "proceeding as in lemma [ lem : mn ] , we have for @xmath42 , @xmath161 , @xmath43 , @xmath162 hence @xmath163 .",
    "now , @xmath164",
    "\\geq   e[{p_{e|{\\cal a},{\\cal b}}}({{\\mathbf x } } ) ; t^c].\\ ] ] but on the event @xmath165 , @xmath166 and hence we get @xmath167 \\geq ( 1-{\\text{pr}}(t)){p_{e|{\\cal a},{\\cal b}}}({{\\mathbf x}}).\\ ] ] but from part 1 ) of theorem [ thm : bin_knownc ] , @xmath85 for @xmath168 this proves part 1 ) .",
    "next we prove part 2 ) .",
    "let @xmath169 denote the event that the clustering is identified correctly .",
    "we note that the probability of error in estimating @xmath3 averaged over the probability law on the block constant matrices satisfies @xmath170 \\\\ & \\leq e\\left[{p_{e|{\\cal a},{\\cal b}}}({{\\mathbf x}})\\right ]   + \\left({\\bar{p}_{e , rc}}+ \\bar{p}_{e , cc}\\right ) \\end{split}\\ ] ] where @xmath171 is the probability of error in column clustering .",
    "the desired result follows from part 2 ) of theorem [ thm : bin_knownc ] and theorem [ thm : bin_c ] .",
    "suppose in cluster @xmath23 we have @xmath172 non erased samples .",
    "then the probability of correct decision in this cluster is given by @xmath173 averaging over the number of non erased samples , the probability of correct decision in cluster @xmath23 is given by @xmath174 since the erasure and bsc are memoryless @xmath175 equations , , and specify the probability of error .",
    "* upper bound : * the desired upper bound is obtained by deriving a lower bound on @xmath176 .",
    "first we note that from , @xmath177 but for @xmath178 and @xmath179 , @xmath180 . substituting this in the previous equation",
    ", we have @xmath181 from equations and , we have @xmath182 and so from , @xmath183 we note that for @xmath184 $ ] , @xmath185 .",
    "hence @xmath186 where the first inequality holds for @xmath187 .",
    "this is true since @xmath188 .",
    "the upper bound follows by noting that @xmath189    * lower bound : * the lower bound on @xmath88 is obtained from an upper bound on @xmath176 . from , @xmath190 if @xmath172 is even , we have @xmath191 for @xmath172 odd , @xmath192 and so @xmath193 from and , we have for all @xmath172 , @xmath194 now from , @xmath195 using this bound on @xmath196 in , we have @xmath197 where in we have used @xmath198 .",
    "this completes the proof of .",
    "* asymptotics : * now consider a sequence of rating matrices of increasing size .",
    "the upper bound on error in is a decreasing function of @xmath199 .",
    "hence if @xmath149then @xmath200 now suppose @xmath201 the lower bound on error is a decreasing function of @xmath202 , and hence substituting the above upper bound on @xmath202 , we have @xmath203 where @xmath204 are some positive constants .",
    "hence @xmath85 as @xmath81 .",
    "recall that @xmath94 is the number of commonly sampled positions in rows @xmath35 and @xmath95 , given by @xmath96 from the chernoff bound ( * ? ? ? * theorem 1 ) , we have @xmath205 to get a handle on the probability of error , we first analyze it conditioned on the erasure sequence and @xmath3 .",
    "let @xmath206 denote the erasure matrix : @xmath207_{m \\times n } \\in \\{0,1\\}^{m \\times n}.\\ ] ]",
    "* rows in same cluster : * consider rows @xmath92 of @xmath3 and suppose @xmath208 , i.e.  @xmath92 are in the same cluster .",
    "we wish to evaluate the probability of error @xmath209 . in this case",
    ", the random variable @xmath210 is given by @xmath211 for any column @xmath212 such that @xmath213 , the indicator @xmath214 has mean @xmath215 .",
    "hence , the above summation has @xmath94",
    "i.i.d  bernoulli random variables of mean @xmath11 .",
    "an application of chernoff bound ( * ? ? ?",
    "* theorem 1 ) yields @xmath216 the bound is independent of @xmath3 .",
    "we only need to take the average of with respect to @xmath206 . using ,",
    "we have @xmath217    * rows in different clusters : * next consider the case @xmath218 , i.e.  rows @xmath35 and @xmath95 are in different clusters .",
    "we wish to evaluate @xmath219 . for @xmath218 and fixed @xmath206 , @xmath3 , the random variable @xmath210 is given by @xmath220 note that for any column @xmath212 such that @xmath213 , the indicator @xmath214 has mean      define @xmath225 as the number of columns @xmath212 such that @xmath213 and @xmath224 .",
    "then from , we observe that the first sum in has @xmath226",
    "i.i.d  bernoulli random variables of mean @xmath11 and the second sum has @xmath225",
    "i.i.d  bernoulli random variables of mean @xmath12 , all the random variables being independent . using the chernoff bound ,",
    "we may then write @xmath227 by substituting @xmath228 , we can rewrite the above bound as @xmath229 we are free to choose @xmath230 in the above bound .",
    "we choose @xmath231 such that the bound is optimized for the average case @xmath232 . for this case ,",
    "the bound in reduces to @xmath233 the value of @xmath231 that minimizes this bound can be checked to be the smaller root of the quadratic given by .",
    "now , from , we have @xmath236 now , since @xmath237 , we have @xmath238 first note that the function @xmath239 for @xmath240 has derivative @xmath241 since @xmath242 , @xmath243 and so @xmath244 . hence @xmath245 . now",
    "if @xmath246 and @xmath247 , then @xmath248 combining this with and , we have + @xmath249 since @xmath250 , where @xmath251 is binomial@xmath252 , we have @xmath253 = { { \\mathsf e}}\\left[\\exp(\\lambda n_0 x ) \\right]=\\left(\\frac{1+\\exp(\\lambda n_0)}{2}\\right)^t.\\ ] ] now taking expectation with respect to @xmath3 in , we have @xmath254 it remains to show that @xmath255 this result follows from of theorem [ thm : gen_clus ] for the general case , which is proved in section [ sect : proof : thm : gen_clus ] .",
    "for simplicity let @xmath256 denote the @xmath172 samples in block @xmath257 .",
    "let @xmath258 , @xmath259 be the transition law of the channel for input @xmath260 and let @xmath261 denote the empirical probability mass function ( pmf ) of @xmath256 .",
    "let @xmath262 be the error event when the @xmath263th block has @xmath172 samples .",
    "for simplicity let @xmath264 denote the set of types with denominator @xmath172 @xcite and define the set of pmfs : @xmath265    * upper bound : * then @xmath266 where in the second step we have used the union bound and in the last step we have used ( * ? ? ? * theorem 11.1.4 , pp .",
    "let @xmath267 then for @xmath268 small , for @xmath269 , we have @xmath270 while for @xmath271 we can bound this probability by 1 .",
    "hence we have from , @xmath272 & \\geq e\\left[1(s\\geq s_0)\\left(1-\\frac{\\exp\\left(-(c_1-\\delta ) s\\right)}{|{{\\mathsf{a}}}|}\\right)\\right ] \\nonumber \\\\   & \\quad = e\\left[1-\\frac{\\exp\\left(-(c_1-\\delta ) s\\right)}{|{{\\mathsf{a}}}|}\\right ] - e\\left[1(s < s_0)\\left(1-\\frac{\\exp\\left(-(c_1-\\delta ) s\\right)}{|{{\\mathsf{a}}}|}\\right)\\right ] \\nonumber \\\\ & \\geq e\\left[1-\\frac{\\exp\\left(-(c_1-\\delta ) s\\right)}{|{{\\mathsf{a}}}|}\\right ] - e\\left[1(s < s_0)\\right ] \\nonumber \\\\ & \\quad = e\\left[1-\\frac{\\exp\\left(-(c_1-\\delta ) s\\right)}{|{{\\mathsf{a}}}|}\\right ] - { \\text{pr}}(s < s_0 ) .",
    "\\label{eq : dmc_eijslb}\\end{aligned}\\ ] ] but for large enough @xmath273 using the chernoff bound ( * ? ? ?",
    "* theorem 1 ) , @xmath274 as @xmath275 , @xmath276 . hence given any @xmath277 , for large enough @xmath273",
    ", we have @xmath278 hence , from and , @xmath279 where we have used the fact that @xmath172 is binomial@xmath280 and so the binomial expansion .",
    "note that @xmath281 , and hence we can choose @xmath282 so that @xmath283 .",
    "hence we have @xmath284    using , we then have @xmath285 where in the last step we have used @xmath286 for @xmath184 $ ] .",
    "note that for large enough @xmath273 , we have @xmath287 . but using @xmath288 we have , @xmath289 the rhs in is a decreasing function of @xmath199 .",
    "hence if @xmath149then @xmath290    * lower bound : * next we give a lower bound on @xmath291 . if for each @xmath260 we consider some @xmath292 , then we get @xmath293 where again we have used ( * ? ? ?",
    "* theorem 11.1.4 , pp .",
    "354 ) in the third step .",
    "since we are free to choose @xmath294 , we choose it such that @xmath295 then we see that @xmath296 hence for @xmath268 , for @xmath297 , @xmath298 and for smaller @xmath172 we use the trivial bound that the probability is non - negative .",
    "hence we have from , @xmath299 where in we have used the chernoff bound ( * ? ? ?",
    "* theorem 1 ) , in we have used the fact that @xmath300 monotonically and in the last step we have used @xmath301 .",
    "further , from , we have @xmath302 and hence @xmath303 using , we then have @xmath304 where to obtain we have used @xmath198 . now since @xmath305 we have @xmath306 the rhs above is a decreasing function of @xmath202 , and hence if @xmath307 we have @xmath308 and hence @xmath85 as @xmath81 .",
    "we recall @xmath309 adding and subtracting the terms corresponding to @xmath310 , we have , @xmath311 now , @xmath312 is the sum of all entries of the transition probability matrix , and hence is equal to @xmath313 .",
    "so we have @xmath314 similarly @xmath315 adding and subtracting the terms coresponding to @xmath310 , we have , @xmath316 in the last step we have used @xmath317 for the first term . from and , we have @xmath318 from the cauchy - schwarz inequality , @xmath319 with equality iff @xmath128 for all @xmath320",
    ". the result then follows .",
    "[ lem:1 ] let @xmath321 be i.i.d  with mean @xmath11 such that @xmath322 $ ] .",
    "let @xmath323 and @xmath18 be positive integers such that @xmath324 .",
    "let @xmath325 then the following hold for sufficiently large @xmath14 .",
    "_ proof of lemma [ lem:1 ] : _ and are direct applications of the chernoff bound ( * ? ? ?",
    "* theorem 2 ) .",
    "( this particular form is also known as hoeffding s inequality . ) to prove , first assume that @xmath326 .",
    "then @xmath331 now , from the cauchy - schwarz inequality , we have @xmath332 this gives with @xmath333 , @xmath334 for sufficiently large @xmath14 .            * rows in same cluster : * first consider case when @xmath208 , i.e.  @xmath92 are in the same cluster .",
    "we wish to evaluate the probability of error @xmath209 .",
    "define @xmath338 as the number of columns @xmath212 such that @xmath213 and @xmath339 .",
    "clearly , @xmath340 note that for such @xmath212 , the indicator @xmath341 has mean @xmath342 .",
    "hence , for @xmath208 and a fixed @xmath206 , the random variable @xmath210 is given by @xmath343 the above summation has @xmath338",
    "i.i.d  bernoulli random variables of mean @xmath342 , for each @xmath344 , all the random variables being independent .",
    "hence the charcterstic function of @xmath210 ( for @xmath208 , fixed @xmath206 and @xmath3 ) is given by @xmath345 using the chernoff bound , we have @xmath346 by using the inequality @xmath347 , we obtain @xmath348 where @xmath349 using @xmath350 we obtain @xmath351 for tractability , we further simplify this bound . to do",
    "so we note that for @xmath352 and @xmath353 , the function @xmath354 is increasing .",
    "this can be seen by noting that @xmath355 since @xmath356 and @xmath357 , we have @xmath358 .",
    "hence @xmath359 in the interval @xmath352 .",
    "now for @xmath360 , @xmath361 , and so @xmath362 using this in , for @xmath363 , we have @xmath364 taking expectation over @xmath3 , we obtain @xmath365 \\label{eq : case1 - 3 } \\\\ & = : t_1 + t_2 . \\nonumber\\end{aligned}\\ ] ] we next bound @xmath366 and @xmath367 .    for @xmath368 $ ] ,",
    "let @xmath369 denote the number of commonly sampled positions for rows @xmath35 and @xmath95 in the @xmath370 column cluster , i.e. @xmath371 note that @xmath372 and @xmath373 where @xmath374 is the rating vector of user @xmath35 in the @xmath370 column cluster . from and the above equation , @xmath375 where the random variable @xmath376 takes the value @xmath342 with probability @xmath377 , for each @xmath344 .",
    "the mean of @xmath378 is @xmath379 .",
    "further , @xmath378 s are i.i.d .  from , lemma [ lem:1 ]",
    "can be applied to @xmath380 , @xmath378 , @xmath381 .",
    "using of lemma [ lem:1 ] , we have @xmath382 for some positive constant @xmath383 .",
    "similarly using of lemma [ lem:1 ] , we have @xmath384 for some positive constant @xmath385 . from",
    ", we then have @xmath386 for some positive constant @xmath260 and for sufficiently large @xmath14 . using @xmath387",
    ", we can loosen the bound to @xmath388 taking expectation over @xmath206 , for @xmath389 and suitable positive constants @xmath204 and @xmath152 , we have , @xmath390 where in we have used . is obtained by a similar argument as used to obtain using cauchy schwarz inequality .",
    "* rows in different clusters : * next consider the case @xmath218 , i.e.  rows @xmath35 and @xmath95 are in different clusters .",
    "the bounding technique is similar to the case when @xmath208 .",
    "we wish to evaluate @xmath219 .",
    "let @xmath391 be the number of columns @xmath212 such that @xmath392 and @xmath393 .",
    "then for a @xmath218 and fixed @xmath206 , @xmath3 , the random variable @xmath210 is given by @xmath394 the above summation has @xmath391",
    "i.i.d  bernoulli random variables of mean @xmath395 , for each @xmath396 , all the random variables being independent . using the chernoff bound ,",
    "we may then write @xmath397 by using the inequality @xmath347 we obtain @xmath398 where @xmath399 using @xmath400 , we obtain @xmath401    but @xmath402 , and so @xmath403 where @xmath172 is defined as @xmath404 so for @xmath405 , we have @xmath406 . but for any @xmath407 , the function @xmath408 is a decreasing function on @xmath409 $ ] .",
    "so we have the following @xmath362 using this in , for @xmath410 , we have @xmath411 taking expectation over @xmath3 , we obtain @xmath412 .",
    "\\label{eq : case2 - 3}\\end{aligned}\\ ] ] then we follow the same line of arguments as in the case when @xmath208 . note that now @xmath413 where the random variable @xmath414 takes the value @xmath395 with probability @xmath415 .",
    "the mean of @xmath378 is @xmath416 .",
    "further , @xmath378 s are i.i.d  applying lemma [ lem:1 ] and as in the case of @xmath208 , we again have @xmath417 since there are at most @xmath418 pairs of rows , the result follows by the union bound .",
    "g.  adomavicius , a.  tuzhilin , `` toward the next generation of recommender systems : a survey of the state - of - the - art and possible extensions , '' _ ieee tran .",
    "knowledge and data engineering _ , vol .",
    "17 , no .  6 , pp .  734 - 749 , june 2005"
  ],
  "abstract_text": [
    "<S> we consider the problem of collaborative filtering from a channel coding perspective . </S>",
    "<S> we model the underlying rating matrix as a finite alphabet matrix with block constant structure . </S>",
    "<S> the observations are obtained from this underlying matrix through a discrete memoryless channel with a noisy part representing noisy user behavior and an erasure part representing missing data . moreover , the clusters over which the underlying matrix is constant are _ </S>",
    "<S> unknown_. we establish a sharp threshold result for this model : if the largest cluster size is smaller than @xmath0 ( where the rating matrix is of size @xmath1 ) , then the underlying matrix can not be recovered with any estimator , but if the smallest cluster size is larger than @xmath2 , then we show a polynomial time estimator with diminishing probability of error . in the case of uniform cluster size , not only the order of the threshold , but also the constant is identified . </S>"
  ]
}