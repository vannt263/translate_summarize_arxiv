{
  "article_text": [
    "attributed graphs are used to represent data as diverse as images , shapes , molecules and protein structures .",
    "the statistical analysis of a dataset of patterns represented by graphical structures is a challenging problem and is closely related to tasks such as density estimation , mixture modelling , classification and clustering .",
    "there have been some efforts to develop probabilistic models for attributed graphs in the context of pattern recognition .",
    "wong et al . ,",
    "@xcite propose the concept of a random graph which takes into account structural and contextual probabilities .",
    "an instantiation ( outcome ) of a random graph is an attributed graph , which enables the characterization of an ensemble of outcome graphs with a probability distribution .",
    "sole - ribalta et al . ,",
    "@xcite generalize the idea of random graphs to structure described random graphs ( sdrg ) , with node and edge value distributions .",
    "algorithms have been proposed where random attributed graph models are used for classification in the maximum likelihood framework .",
    "this framework has also been adopted by seong et al .",
    ", @xcite to develop an incremental clustering algorithm for attributed graphs , and by sengupta et al . , @xcite to efficiently organize large structural modelbases for quick retrieval .",
    "there are two features of such a definition that are quite noteworthy- ( i ) both the structural and contextual probabilities are considered , which are estimated with suitable independence assumptions and ( ii ) the ability to deal with a wide variety of attribute values .",
    "the present contribution aims to develop a theory for probabilistic modeling of attributed graphs similar to generative models for feature vectors and demonstrate its utility to classify graph patterns .",
    "we propose random graph models as prototypes for a set of graphs with continuous node and edge attribute vectors and estimate its parameters . instead of using the random graph models to classify the patterns in terms of maximum likelihood",
    ", we use the likelihood values as features for classification by subsequent discriminative classifiers such as support vector machines .",
    "a random attributed graph ( simply referred to as random graphs ) is a graph whose nodes and edges are finite probability distributions .",
    "each outcome of a random graph is a labeled graph along with a morphism of the labeled graph into the random graph .",
    "the morphism specifies for each vertex ( or edge ) of the outcome graph which vertex ( or edge ) of the random graph generated it .",
    "the probability space of random graphs should be such that , the outcomes are attributed graphs with specified morphism relation and is complete . the definitions in this section follow @xcite closely .    technically , the random graph @xmath0 script .",
    "] is defined to be such that :    \\1 .",
    "each vertex @xmath1 and edge @xmath2 is a finite probability distribution    \\2 .",
    "@xmath3 , @xmath4    \\3 .",
    "the space of joint distribution of all random nodes and random edges is complete    condition 2 ensures that an edge can occur in an outcome only if both its ends ( terminal nodes , given by @xmath5 ) occur .",
    "completedness means that the space is indeed a ( standard ) probability space .",
    "consider the probability space of the joint distribution .",
    "this space is the probability space of attributed graphs and every outcome is an attributed graph .",
    "let @xmath6 be an outcome graph .",
    "a morphism @xmath7 and @xmath8 specifies the structural mapping between the random graph and its outcome .",
    "thus , an outcome of a random graph is specified by the tuple @xmath9 , where @xmath10 .",
    "it is to be noted that the mappings @xmath11 and @xmath12 are into and the inverse mappings @xmath13 and @xmath14 are such that some elements could be mapped to @xmath15 , i.e. @xmath16 if no morphism exists .",
    "the probability of an outcome graph is then the probability of its joint outcome described by the following    @xmath17    where @xmath18 is the node attribute function that assigns attribute for every node and @xmath19 denotes the particular node attribute value .",
    "@xmath20 are the corresponding edge attribute function and values respectively .",
    "we make the following assumptions to make the definition computationally feasible- node occurences are independent , and edge occurences depend only on the nodes that the edge is incident to .",
    "then , we can simplify eq.([eqn1 ] ) to    @xmath21    where @xmath22 denotes a probability that the node @xmath23 occurs and @xmath24 is the probability that @xmath23 does not occur .",
    "similar notation has been adopted for the edges as well .",
    "we note that formula in eq .",
    "[ eqn2 ] decomposes the probability of an attributed graph instance as the product of probability of nodes / edges of generating random graphs that occur in the outcome , `` not occurence '' probability of nodes / edges that are absent in the outcome , and the probability of the occuring nodes / edges to assume their respective attribute values .",
    "figure  [ ex1 ] illustrates the above definitions with an example .",
    "the estimation of structural parameters of a random graph given a dataset follows from maximizing the likelihood : the node and edge occurence probabilities of random graphs are set to those values which maximize the likelihood of the dataset being generated from the random graph .",
    "the cost function is    @xmath25    where @xmath26 is the likelihood that random graph @xmath27 generates the graph @xmath28 .",
    "we now consider the case where the node and edge attributes are given by feature vectors . in order to simplify the analytical treatment , we assume the attribute vectors to be generated by gaussian distributions whose means and covariances are to be determined .    initially , we maximize the cost function with respect to the node and edge occurence probabilities @xmath22 and @xmath29 . as the node occurences are modelled by independent bernoulli distributions ,",
    "the maximum likelihood estimate is the fraction of its occurences in the dataset    @xmath30    where @xmath31 is the number of occurences of node @xmath23 in the sample set .",
    "similar estimates hold good for the edges except that edge occurence probabilities are normalized by their respective node probabilities ( accounting for the fact that the edges can not occur if any of their end nodes do not occur ) .",
    "we now consider the problem of estimating means and covariances of node and edge attribute distributions .",
    "it is possible to derive gradient descent update rule for the mean and covariance matrices .",
    "the vanilla gradient descent where the means and covariances are updated in the direction of the gradient ( as it is assumed to the steepest direction ) is not ideal as it ignores the geometry of the underlying probability space .",
    "therefore , we use natural gradient descent to estimate the mean and covariance online @xcite .",
    "natural gradient descent is a modification of the gradient descent procedure which takes into account the geometry of the manifold by incorporating a corrective term given by the riemannian metric tensor .",
    "the equations for updating the means and covariances in the direction of the natural gradient are given by    @xmath32    @xmath33    where , @xmath34 are the riemannian metric tensors in the space of means and covariances respectively",
    ". the riemannian metric tensor in the space of mean vectors @xmath35 is given by    @xmath36    hence , the online update equation for the gaussian means is given as below    @xmath37    the metric tensor in the space of covariance matrices is defined as    @xmath38    which after some simplification turns out to be    @xmath39    the online covariance estimation is given by a first order update rule as    @xmath40",
    "prototype based classification schemes are widespread in the domain of attributed graphs @xcite .",
    "the key idea is to embed the graphs into a vector space in the following manner . given a set of graphs @xmath41 , we synthesize a set of prototype graphs @xmath42 such that every graph @xmath43 is embedded in @xmath44 as    @xmath45    where @xmath46 is a dissimilarity measure between the graphs and prototypes .",
    "the choice of prototypes influences the distance measure and hence the dissimilarity space . to illustrate , when the prototype graphs are chosen to be set median or means or cluster centres ,",
    "it is clear as to how the distance is calculated .",
    "however , what is a suitable distance measure when we choose random graphs as prototypes ?",
    "the key lies in defining the kullback - leibler divergence between the probability density of random prototype graph @xmath27 and the true ( hidden ) probability distribution @xmath47 @xcite    @xmath48    the unknown probability distribution @xmath47 is represented by @xmath49 , where @xmath50 is the dirac delta function at every data sample @xmath51 . seperating the ln term into @xmath52 and noting    @xmath53    which is the log - likelihood that the random graph @xmath27 generates the outcome @xmath51",
    "hence , likelihood ( or more precisely its logarithm ) could be used as a feature for classification naturally in the dissimilarity / distance representation framework .",
    "we also note here that a feature space embedding of graphs defined by likelihood values corresponds to the framework of jaakkola et .",
    "al . , @xcite who propose to use kernels derived from generative models .",
    "we thus summarize the scheme as below .",
    "given a dataset of graphs representing patterns belonging to different classes , sythesize first random attributed graphs acting as a model / prototype for each class . the largest graph ( i.e. the graph with maximum number of nodes ) is initialized as prototype classwise .",
    "we then present every graph in the training set , align them with the corresponding prototype and update the node and edge occurence ( structural ) probabilities .",
    "the means and covariances are also updated according to the formulae in eq .",
    "( 8) , ( 11 ) . once the parameters of the random prototype graphs are determined , we embed the dataset into a feature space by calculating the log - likelihood between every graph in the dataset and every element in the prototype set .",
    "we point out the following notable features of this scheme : ( 1 ) more than one prototypes could be used for every class especially for datasets with diverse graphs in the same class .",
    "however , in our analysis and experiments , we consider just one random prototype per class in view of computational complexity of graph matching ; ( 2 ) the size of the prototypes are bound by the size of the largest graph in the dataset ( 3 ) the number of graph matching operations during the parameter estimation stage is @xmath54 , the size of the training set ; once the prototype random graphs are sythesized , the training set ( with @xmath54 samples ) and the test set ( with @xmath55 samples ) have to be embedded in the likelihood space .",
    "this needs another @xmath56 graph matching operations .",
    "_ matching attributed graphs_- the problem of aligning random graphs with each of the sample graph and the likelihood calculation involve attributed graph matching .",
    "we adopt again the graduated assignment algorithm @xcite with a suitable compatibility function for this purpose .",
    "this algorithm minimizes a cost function as a function of match matrix over all possible matchings by an iterative procedure which estimates match matrix at every step and normalizing it .",
    "the matching quality is influenced by node compatibilites which measure how similar the nodes are structurally and attribute - value wise . in determining the morphism between random attributed graphs and outcome graphs ,",
    "the compatibility function is set to the likelihood of the node being structurally present in the outcome , thus in effect finding the morphism which is most probable .",
    "_ classification procedure_- once the random graphs have been synthesized classwise , the dataset was embedded in to a feature space by calculating the log - likelihood of graphs beng generated by the prototype random graphs . in the feature space ,",
    "various classifiers were learned on the training set and validated ( by performance on the validation set or by cross - validation on the training set ) .",
    "the classifier exhibiting best validation performance was used to classify the test data .",
    "extensive experimentations indicated that support vector machines with polynomial / gaussian kernels yielded the best performance .",
    "all classification experiments were done using pyml software @xcite .",
    "we first analyzed the performance of this algorithm on sythetic datasets .",
    "we consider a dataset consisting of 200 graphs in training and test set belonging to two classes .",
    "the dataset is generated by considering distortions of two base graphs classwise at different levels viz .",
    "node and edge attributes are generated according to a normal distribution .",
    "the noise according to the specified distortion level is added which modifies node and edge occurences and also their respective attributes .",
    "the nodes are then randomly permuted .",
    "the dataset is then divided uniformly into training and test sets .",
    "the classification scheme described in this chapter is referred to as @xmath58 ( random attributed graph model + likelihood as a feature ) . the standard @xmath59 nearest neighbour algorithm ( @xmath60 ) in the graph domain",
    "is chosen as the benchmark classifier .",
    "the classifiers are evaluated on the basis of the area under the roc curve ( @xmath61 ) @xcite , blue for @xmath62 and red for @xmath60 ( figure  [ roccurves ] ) .",
    "the classification rates of @xmath60 compared with the proposed algorithm is shown in table 1 . as is seen , for low values of distortion , @xmath58 family of classifiers give near ideal performance . for higher noise levels ,",
    "the algorithm does achieve higher robustness to noise compared to @xmath60 .",
    ".classification rates ( @xmath63 ) on the synthetic datasets [ cols=\"<,<,<,<,<\",options=\"header \" , ]     the following observations are made- the results compare well for the fingerprint dataset overall , and for the letter ( high ) dataset compares well with sk + svm and is superior to sdrg ; although k - nn yields good results overall , it faces the computationally challenging task of choosing k. for sk + svm and le + svm , the task of choosing effective prototype set and calculating the graph - edit distance between the dataset and prototype set is expensive as well and offers no analytical insight .",
    "the approach presented here is fast as it involves estimating the parameters of random graph model analytically and needs far less graph matching operations corresponding to generating only one class prototype model .",
    "the prototypes also give a good summary of node and edge occurence probabilities in the dataset and probability distributions of their attributes . embedding the prototypes in the space spanned by likelihood values",
    "offers statistically significant improvement with almost no significant loss of speed as there fast packages for svm s and other classification algorithms .",
    "this work builds upon the notion of random graph models with applications in structural pattern recognition with the following contributions- with independence assumptions a random attributed graph is represented as a joint random variable in its node and edge occurences and of their respective attribute values , an analytical method to estimate the different probability distributions of a random graph model as a prototype given an ensemble of attributed graphs is presented using a maximum likelihood procedure , the utility of the random graph as a prototype is shown by using the likelihood of an outcome graph as a feature for classification .",
    "the proposed approach is suited to contexts involving large number of graph data samples , as determination of random prototype graph is a density estimation problem .",
    "it is robust to noise and faster on account of lesser number of graph matching operations that need to be performed in contrast to other approaches .",
    "there are several possible extensions to this approach- first , a method to derive a class of probabilistic clustering and classification algorithms is being currently investigated .",
    "this means that the random prototype graph is learned from the dataset in a procedure akin to a standard quantization type scheme .",
    "second , is there a way to tie the classifiers in the feature space directly with the learning of prototypes ? to elaborate , it is important to investigate the link between type / family of classifiers on the feature space ( due to likelihood ) with how the random prototypes are estimated / learned .",
    "this would help to integrate probabilistic learning in the domain of graphs with discriminative methods for classification in the subsequent likelihood space .",
    "lastly , the foundations of the random graph definitions needs to be explored- although node and edge independence is useful in that it allows an easy analytical estimation of model parameters , it is too strong an assumption .",
    "is there a way to model dependencies of nodes and edges and their attributes ( node / edge co - occurences ) ?",
    "such a model would help enormously in probabilistic sub - structure analysis methods and also give possibly superior classification and clustering algorithms ."
  ],
  "abstract_text": [
    "<S> this contribution proposes a new approach towards developing a class of probabilistic methods for classifying attributed graphs . </S>",
    "<S> the key concept is _ random attributed graph _ , which is defined as an attributed graph whose nodes and edges are annotated by random variables . </S>",
    "<S> every node / edge has two random processes associated with it- occurence probability and the probability distribution over the attribute values . </S>",
    "<S> these are estimated within the maximum likelihood framework . </S>",
    "<S> the likelihood of a random attributed graph to generate an outcome graph is used as a feature for classification . </S>",
    "<S> the proposed approach is fast and robust to noise . </S>"
  ]
}