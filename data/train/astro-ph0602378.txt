{
  "article_text": [
    "the quest for a cosmological standard model is being driven by an increasing amount of high quality observations .",
    "an important and natural question concerns the number of fundamental parameters of the underlying physical model .",
    "how many numbers are necessary to characterize the universe ? or in other words , how complex is the universe ?",
    "generally the term `` complexity '' is employed in a rather loose fashion : a more complex model is one with a larger number of parameters that can be adjusted over a large range to fit the model to the observations . in this paper",
    "we will try to measure the effective number of model parameters that a given set of data can support . because of the connection to the data",
    ", we will call this the _ effective complexity _ or _",
    "bayesian complexity_. our main purpose is to present a statistically sound quantity that embodies in a quantitative way the above notion of complexity when a model is compared to the observations .",
    "bayesian model comparison makes use of an occam s razor argument to rank models in term of their quality of fit _ and _ economy in the number of free parameters . a model with more free parameters",
    "will naturally fit the observations better , but it will also be penalized for the wasted parameter space that the larger number of parameters implies .",
    "several studies have made use of bayesian model comparison to assess the viability of different models in the cosmological context @xcite . in this work",
    "we show that bayesian complexity is an ideal complement to model selection in that it allows to identify the number of effective parameters supported by the data .",
    "we start by introducing our notation and the fundamentals of bayesian statistics and model comparison .",
    "we then present the bayesian complexity and illustrate its use in the context of a toy model in section  [ sec : linmod ] . in section [ sec : cosmo ]",
    "we apply it to observations of cosmic microwave background anisotropies and we conclude in section [ sec : conclusions ] .",
    "we first briefly review the basic ingredients of bayesian statistics and some relevant aspects of information theory .",
    "this serves both to introduce our notation and to remind the reader of the main points .",
    "we will use a fairly compact notation where possible and refer the reader to _ e.g. _",
    "@xcite for the exact mathematical definitions .",
    "specifically , for an outcome @xmath1 of a random variable @xmath2 we will write @xmath3 for the probability distribution function ( pdf ) , ie the probability that @xmath2 takes a certain value @xmath1 . in the case of a multi - dimensional parameter space",
    "we will write @xmath3 as a short form of the joint probability over all components of @xmath1 , @xmath4 . the conditional probability of @xmath1 given @xmath5 is written @xmath6 .    the starting point of our analysis is _ bayes theorem _ : p(x|y , i ) = .",
    "[ eq : bayes ] here , the quantity @xmath7 represents a collection of all external hypotheses and our model assumptions .",
    "given the data @xmath8 and a model @xmath9 with @xmath10 free parameters @xmath11 , statistical inference deals with the task of determining a posterior pdf for the free parameters of the model , @xmath12 .",
    "the latter is computed via bayes theorem as [ eq : bayes2 ] p(|d , ) = . on the right  hand side of this equation , @xmath13 is the probability of obtaining the observed data given the parameter value @xmath11 .",
    "since the observed data are a fixed quantity , we interpret @xmath13 as a function of @xmath11 and we call it the _ likelihood _ , @xmath14 .",
    "the _ prior pdf _ @xmath15 embodies our state of knowledge about the values of the parameters of the model before we see the data .",
    "there are two conceptually different approaches to the definition of the prior : the first takes it to be the outcome of previous observations ( _ i.e. _ , the posterior of a previous experiment becomes the prior for the next ) , and is useful when updating ones knowledge about the values of the parameters from subsequent observations .",
    "for the scope of this paper , it is more appropriate to interpret the prior as the available parameter space under the model , which then is collapsed to the posterior after arrival of the data .",
    "thus the prior constitutes an integral part of our model specification . in order to avoid cluttering the notation ,",
    "we will write @xmath16 , with the model dependence understood whenever no confusion is likely to arise .",
    "the expression in the denominator of is a normalization constant and can be computed by integrating over the parameters , [ eq : evidence ] p(d|)=d ( ) ( ) .",
    "this corresponds to the average of the likelihood function under the prior and it is the fundamental quantity for model comparison . the quantity @xmath17 is called _ marginal likelihood _",
    "( because the model parameters have been marginalized ) , or in recent papers in the cosmological context , the _ evidence_. in the following we shall refer to it as to the _ likelihood for the model_. the posterior probability of the model is then , using bayes theorem again , p(|d ) = , where @xmath18 is the prior for the model .",
    "the quantity in the denominator on the right  hand side is just a normalization factor depending on the data alone , which we can ignore . when comparing two models , @xmath19 versus @xmath20 , one introduces the _ bayes factor _ @xmath21 , defined as the ratio of the model likelihoods = b_12 in other words , the prior odds of the models are updated by the data through the bayes factor . if we do not have any special reason to prefer one model over the other before we see the data , then @xmath22 , and the posterior odds reduce to the bayes factor",
    "alternatively , one can interpret the bayes factor as the factor by which our relative belief in the two model is modified once we have seen the data .",
    "the usefulness of a bayesian model selection approach based on the model likelihood is that it tells us whether the increased `` complexity '' of a model with more parameters is justified by the data .",
    "but the number of free parameters in a model is only the simplest possible notion of complexity , which we call _ input complexity _ and denote by @xmath23",
    ". a much more powerful definition of model complexity was given by spiegelhalter et al @xcite , who introduced the _",
    "bayesian complexity _ , which measures the number of model parameters that the data can constrain : [ eq : c_b_rewritten ] _ b = -2(d_kl(p , ) - ) .",
    "on the right  hand side , @xmath24 is the kullback - leibler ( kl ) divergence between the posterior and the prior , representing the information gain obtained when upgrading the prior to the posterior via bayes theorem : [ eq : def_kl ] d_kl(p , ) p(|d ) d the kl divergence measures the relative entropy between the two distributions ( e.g.  @xcite ) ) , but it does have the useful properties that @xmath25 ( gibbs inequality ) , with equality only for @xmath26 , and that it is invariant under general transformations of the parameter space . ] . in eq .",
    ", @xmath27 is a point ",
    "estimate for the kl divergence .",
    "if all parameters are well measured , then the posterior pdf will collapse into a small region around @xmath28 , and thus the kl divergence will approximately be given by @xmath29 . by taking the difference",
    ", we compare the effective information gain to the maximum information gain we can expect under the model , @xmath27 .",
    "the factor of 2 is chosen so that @xmath30 for highly informative data , as shown below . as a point estimator for @xmath28",
    "we employ the posterior mean , denoted by an overbar .",
    "other choices are possible , and we discuss this further below",
    ".    we can use eq .   and bayes theorem to rewrite as [ eq_def_c_b ] _ b = -2 p(|d , ) ( ) + 2 ( ) , defining an effective @xmath31 through the likelihood as @xmath32 ( any constant factors drop out of the difference of the logarithms in eq .  ) we can write the _ effective number of parameters as _ [ eq : complex_as_chi ] _ b = - ^2 ( ) , where the mean is taken over the posterior pdf .",
    "this quantity can be computed fairly easily from a markov chain monte carlo ( mcmc ) run , which is nowadays widely used to perform the parameter inference step of the analysis .",
    "we thus see that the effective number of parameters of the model is not an absolute quantity , but rather _ a measure of the constraining power of the data as compared to the predictivity of the model _ , _ i.e. _  the prior .",
    "hence @xmath33 depends both on the data at hand and on the prior available parameter space .",
    "in fact , it is clear that the very notion of `` well measured parameter '' is not absolute , but it depends on what our expectations are , _ i.e. _  on the prior .",
    "for example , consider a measurement of @xmath34 , the total energy density of the universe , expressed in units of the critical density .",
    "the current posterior uncertainty around @xmath35 is about @xmath36 . whether this means that we have `` measured '' the universe to be flat ( _ i.e. _ @xmath35 ) or not depends on the prediction of the model we consider .",
    "if we take a generic prior in the range @xmath37 , then we conclude that current data have measured the universe to be flat with moderate odds ( a precise analysis gives odds of @xmath38 in favor of the flat model , see @xcite ) . on the contrary , in the framework of _",
    "e.g. _  landscape theories , the prior range of the model is much narrower , say @xmath39 , and therefore current posterior knowledge is insufficient to deem the parameter measured .",
    "is conceptually related to the @xmath40 approach used for example in @xcite to analyze how many dark energy parameters are measured by the data . in that work",
    "the decrease of the @xmath31 value within the confidence regions encompassing 68% and 95% of the posterior was measured and compared against the theoretical decrease of a multivariate gaussian distribution with a given number of degrees of freedom ( see e.g. chapter 15.6 of @xcite ) .",
    "this in turn allowed to deduce the effective number of degrees of freedom of the @xmath31 .",
    "we now turn to an explicit demonstration of the power and usefulness of coupling the model likelihood with the bayesian model complexity in model selection questions , by analyzing in some detail linear toy models .",
    "before applying the bayesian complexity and model likelihood to current cosmological data , we compute them explicitly for a linear model and we illustrate their use in a toy example involving fitting a polynomial of unknown degree .",
    "we show below how the bayesian complexity tells us how many parameters the data could in principle constrain given the prior expectations under the model .",
    "let us consider the following _ linear model _",
    "y = f + where the dependent variable @xmath5 is a @xmath8-dimensional vector of observations , @xmath11 is a vector of dimension @xmath10 of unknown regression coefficients and @xmath41 is a @xmath42 matrix of known constants that specify the relation between the input variables @xmath11 and the dependent variables @xmath5   fitted with a linear model @xmath43 , the matrix @xmath41 is given by the basis functions @xmath44 evaluated at the locations @xmath45 of the observations , @xmath46 . ] .",
    "furthermore , @xmath47 is a @xmath8-dimensional vector of random variables with zero mean ( the _ noise _ ) .",
    "if we assume that @xmath47 follows a multivariate gaussian distribution with uncorrelated covariance matrix @xmath48 , then the likelihood function takes the form p(y | ) = , [ eq : data_like ] where we have defined @xmath49 and @xmath50 .",
    "this can be cast in the form p(y | ) = _ 0 , [ eq : like ] with the likelihood fisher matrix @xmath51 given by l a^t a and a normalization constant _ 0 . here",
    "@xmath52 denotes the parameter value that maximizes the likelihood , given by _ 0 = l^-1a^t b. as a shortcut , we will write [ eq : defchisq ] ^2 ( ) -2 p(y| ) = ^2(_0 ) + ( -_0)^t l ( -_0 ) , where @xmath53 .      in this section",
    "we gain some intuitive feeling about the functional dependence of the model likelihood and complexity on the prior and posterior for the simple case of linear models outlined above .",
    "the results are then applied in section [ sec : toymodel ] to an explicit toy model , showing the model likelihood and complexity in action .",
    "assuming as a prior pdf a multinormal gaussian distribution with zero mean and fisher information matrix @xmath54 ( we remind the reader that the fisher information matrix is the inverse of the covariance matrix ) , _ i.e. _ [ eq : gaussprior ] ( ) = , where @xmath55 denotes the determinant of the matrix @xmath54 , the model likelihood and model complexity of the linear model above are given by eqs .   and , respectively ( see appendix [ ap : computed ] ) .",
    "let us now consider the explicit illustration of a model with @xmath10 parameters , @xmath56 and @xmath57 .",
    "without losing generality , we can always choose the units so that the prior fisher matrix is the unity matrix , _",
    "i.e. _ p = _ n. this choice of units is natural since it is the prior that sets the scale of the problem . the likelihood fisher matrix being a symmetric and positive matrix , it is characterized by @xmath58 real numbers , which we choose to be its eigenvalues @xmath59 , @xmath60 and the elements of the orthogonal matrix @xmath61 that diagonalizes it ( corresponding to rotation angles ) .",
    "here the @xmath62 represent the standard deviations of the likelihood covariance matrix along its eigendirections @xmath63 , expressed in units of the prior width .",
    "with @xmath64 we have that the likelihood fisher matrix is given by @xmath65 and thus eq .",
    "gives [ eq : complex2 ]    _ b & = _ 0 - = + & = _ 0 - = _ i=1^n .",
    "the complexity only depends on how well we have measured the eigendirections of the likelihood function _ with respect to the prior_. every well  measured direction , ( _ i.e. _ , one for which @xmath66 ) counts for one parameter in @xmath33 , while directions whose posterior is dominated by the prior , @xmath67 , do not count towards the effective complexity .",
    "this automatically takes into account strong degeneracies among parameters .",
    "notice also that once an eigendirection is well measured ( _ i.e. _  in the limit @xmath66 ) , then the prior width does not matter anymore .",
    "in contrast , the model likelihood is given by ( assuming for simplicity that the mean of the likelihood corresponds to the prior mean , _",
    "@xmath68 ) p(d | ) = _ 0 _ i=1^n [ eq : evid_nd ] .",
    "finally , we remark that an important ingredient of the bayesian complexity is the point estimator for the kl divergence .",
    "here we adopt the posterior mean as an estimate , but other simple alternatives are certainly possible , for instance the posterior peak ( or mode ) , or the posterior median .",
    "the choice of an optimal estimator is still matter of research ( see _ e.g. _  section [ sec : cosmo ] and the comments at the end of ref .",
    "the important aspect is that the posterior pdf should be summarized by only one number , namely the value plugged into the kl estimator .",
    "this is obviously going to be a very bad description for highly complex pdf s , exhibiting for instance long , banana  shaped degeneracies .",
    "no single number can be expected to summarize accurately such a pdf . on the other hand , for fairly gaussian pdf s",
    "all the different estimators ( mean , median and peak ) reduce to the same quantity .",
    "this clearly calls for using normal directions in parameter space  @xcite , which make the posterior as gaussian as possible , a procedure that it would be wise to follow whenever possible for many other good reasons ( _ e.g. _ , better and faster mcmc convergence ) .",
    "we now turn to the question of how we can use the model likelihood and complexity together as a tool for model selection .",
    "we shall see that the bayesian complexity provides a way to assess the constraining power of the data with respect to the model at hand and to break the degeneracy among models with approximately equal model likelihood .",
    "let us consider two models @xmath69 and @xmath70 with different numbers of parameters , @xmath71 ( but in general the two models need not to be nested ) .",
    "if the additional parameters of model @xmath70 are required by the data , then the likelihood of model @xmath70 will be larger and @xmath70 will have larger posterior odds , thus it should be ranked higher in our preference than model @xmath69 .",
    "however , even if the extra parameters of model @xmath70 are not strictly necessary , they can lead to over  fitting of the data and compensate the occam s penalty factor in ( [ eq : evid_nd ] ) sufficiently to lead to a comparable marginal likelihood for both models .",
    "the effective complexity provides a way to break the degeneracy between the quality  of  fit term ( @xmath72 ) and the occam s razor factor in the marginal likelihood , and enables us to establish whether the data is good enough to support the introduction of the additional parameters of model @xmath70 .    to summarize ,",
    "we are confronted with the following scenarios :    1 .",
    "@xmath73 : model @xmath70 is clearly favored over model @xmath69 and the increased number of parameters is justified by the data .",
    "2 .   @xmath74 and @xmath75 : the quality of the data is sufficient to measure the additional parameters of the more complicated model , but they do not improve its likelihood by much .",
    "we should prefer model @xmath69 , with less parameters .",
    "@xmath74 and @xmath76 : both models have a comparable likelihood and the effective number of parameters is about the same . in this case the data is not good enough to measure the additional parameters of the more complicated model and we can not draw any conclusions as to whether the additional complexity is warranted .",
    "we illustrate these cases by computing the model likelihood and effective complexity of a toy model in the next section .      as a specific example of the linear model described in section [ sec : linmod1 ] , we consider the classic problem of fitting data drawn from a polynomial of unknown degree .",
    "the models that we test against the data are a collection of polynomials of increasing order , with input complexity @xmath77 , where @xmath10 is the order of the polynomial .",
    "the question is then whether our model selection can correctly recover the order @xmath78 of the polynomial from which the data are actually drawn .",
    "the data covariance matrix is taken to be diagonal and with a common standard deviation for all points , @xmath79 , while the prior over the polynomial coefficients is a multivariate gaussian with covariance matrix given by the identity matrix .",
    "for definitiveness , we will take the underlying model from which the data are generated to have @xmath80 parameters .",
    "first we draw @xmath81 data points with noise @xmath82 .",
    "we plot in figure [ fig : c1 ] the resulting model likelihood and effective complexity as a function of the input model complexity , @xmath10 .",
    "the likelihood of the models increases rapidly until @xmath83 and then decreases slowly , signaling that @xmath84 parameters are not justified .",
    "( we plot the logarithm of the logarithm of the model likelihood as the models with @xmath85 parameters are highly disfavored and would otherwise not fit onto the figure  an example of case ( 1 ) of the list in the previous section ) .",
    "the effective complexity on the other hand continues to grow until @xmath86 , at which point the data is unable to constrain more complex models and @xmath87 becomes constant . we conclude that the model with @xmath88 is the one preferred by data and that additional parameters are not needed , although the data could have supported them .",
    "this is case ( 2 ) in the list of the previous section .",
    "bayesian effective complexity @xmath87 ( solid black line , left ",
    "hand vertical scale ) and model likelihood ( red circles , right  hand scale ) as a function of the number of parameters , for @xmath89 data points with small noise .",
    "the dashed blue line is the number of parameters for reference .",
    "the errorbars on the model likelihood values are smaller than the symbols on this scale , while the bayesian complexity is independent of the noise realization ( _ i.e. _ , error ",
    "free ) for linear models .",
    "the bayesian analysis correctly concludes that the best model is the one with @xmath88 parameters.,title=\"fig:\",width=264 ] +    next we decrease the number of data points to @xmath90 , in which case we obviously can not recover more than four parameters .",
    "it comes as no surprise that the model likelihood stops increasing at @xmath91 , see figure  [ fig : c2 ] .",
    "but the effective complexity also flattens at @xmath91 , which means that the data can not deal with more than four parameters , irrespective of the underlying model ! in this case , corresponding to point ( 3 ) of the list in the previous section , we conclude that the available data do not support more than 4 effective parameters . on the other hand , we recognize that the flattening of the model likelihood at @xmath91 does not necessarily mean that the underlying model has four parameters .",
    "we thus must hold our judgment until better data become available .     as in figure",
    "[ fig : c1 ] , but now using only @xmath90 data points .",
    "the maximum effective complexity that the data can support is @xmath92 , and the flattening of the model likelihood at the same value does not allow to conclude that models with more parameters are disfavored.,title=\"fig:\",width=264 ] +    as an alternative to decreasing the number of data points , we can achieve a similar data degradation effect by keeping @xmath81 data points but by increasing the noise to @xmath93 .",
    "we obtain a result similar to the previous case , which is plotted in figure  [ fig : c3 ] .     as in figure",
    "[ fig : c1 ] , but now with @xmath81 data points and large noise . as in figure",
    "[ fig : c2 ] , the maximum complexity supported by the data is smaller than the underlying true model complexity , @xmath80.,title=\"fig:\",width=264 ] +    we conclude by emphasizing once more that in general the outcome of model selection based on assessing the model likelihood and effective complexity depends on the interplay of two factors .",
    "the first is the predictive power of the model , as encoded in the prior .",
    "the second is the constraining power of the data .",
    "we now apply the above tools to the question of how many cosmological parameters are necessary to describe current cosmic microwave background ( cmb ) anisotropy measurements . we make use of the following cmb data : wmap , acbar , cbi , vsa and boomerang 2003 . to provide an additional regularization ( especially in view of including spatial curvature ) we also use the hst limits on the hubble parameter , @xmath94 km / s / mpc .",
    "we should note that this strongly increases the power of the cmb data .",
    "we use both the first - year wmap alone ( wmap1 ) as well as the data for the first three years ( wmap3 ) .    for each set of cosmological parameters we create a converged mcmc chain using the publicly available code",
    "` cosmomc ` @xcite .",
    "we then compute the bayesian complexity from the chain through eq .",
    "( [ eq : complex_as_chi ] ) .",
    "the model likelihood is evaluated with the savage - dickey method ( see @xcite and references therein ) : for a model that is nested within a larger model by fixing one parameter , @xmath95 , to a value @xmath52 , the bayes factor between the two models is given by the posterior of the larger model at @xmath96 ( normalized and marginalized over all other parameters ) divided by the prior at this point . in this way it is possible to derive all the model likelihoods in a hierarchy , starting from the most complex model ( which is assigned an arbitrary model likelihood , in our case 1 ) .",
    "since errors tend to accumulate through the intermediate steps necessary to reach the simpler models , and as a cross  check , we additionally computed the model likelihoods with nested sampling  @xcite for the wmap1 data . within the errorbars , we did not find any appreciable discrepancy between the two methods .",
    "in this analysis we use four basic cosmological parameters , namely b_4 = \\{_b h^2,_m h^2,_,a_s } , where @xmath97 ( @xmath98 ) is the baryon ( matter ) density relative to the critical energy density , @xmath99 km / s / mpc is the fudge factor , @xmath100 is the ratio of the sound horizon to the last scattering angular diameter distance and @xmath101 , with @xmath102 the power spectrum of adiabatic density fluctuations at a scale @xmath103 mpc@xmath104 .",
    "we then add three more parameters in various combinations , to study whether they are necessary and supported by the observations .",
    "the additional parameters are : the reionization optical depth @xmath105 , the scalar spectral index @xmath106 and the spatial curvature ( parameterized by its contribution to the hubble equation , @xmath107 ) .",
    "the scalar spectral index is either fixed to @xmath108 ( the case of a scale invariant power spectrum of initial fluctuations ) , or else chosen with a gaussian prior , @xmath109 .",
    "we find that @xmath110 .",
    "we choose the prior of the reionization optical depth to reflect our current lack of understanding of how reionization proceeds .",
    "we choose it flat within @xmath111 and then add an exponential falloff : ( ) ( - ) > 0.15 .",
    "the models without this parameter have @xmath112 , and @xmath113 .",
    "the curvature contribution is either set to zero , @xmath114 , if we do not include the parameter , or else is used with a flat prior @xmath115 .",
    "the value of the prior for a flat universe is @xmath116 .",
    "we find that adding curvature as a free parameter when using the wmap 3yr data leads to a non - gaussian posterior for which the mean as a point - estimate for the kl divergence in eq .",
    "( [ eq : c_b_rewritten ] ) is not representative .",
    "we opt here for a slightly modified estimator , given by the average of the @xmath31 evaluated at the mean and the mode of the posterior . for a gaussian posterior",
    "this reduces to the mean point - estimator but it appears to be somewhat more stable .    [ cols=\"<,^,^,^,<\",options=\"header \" , ]     we quote our results in table  [ tab : cmbcplx3 ] ( wmap3 ) and table  [ tab : cmbcplx1 ] ( wmap1 ) , while figure  [ fig : cmbcomplex ] gives a graphical representation .",
    "the model likelihoods are quoted relative to the model with the most parameters .",
    "we find that using wmap1 we can measure all the parameters for the models with four and five parameters .",
    "for @xmath117 , the effective complexity increases more slowly than the number of input parameters , but we can still measure at least six parameters with cmb+hst . with wmap3 we can measure all six parameters of the @xmath118 model .",
    "we conclude that the new wmap3 data augmented by the hst determination of @xmath119 can measure all seven parameters considered in this analysis .",
    "we plot the model likelihood ( normalized to the model with the most parameters ) versus the bayesian effective complexity for the models of table  [ tab : cmbcplx3 ] .",
    "a downward  pointing arrow indicates the bayesian complexity of models that lie outside the boundary of the figure . , title=\"fig:\",width=264 ] +    taking into account the model likelihood values , we find that the models @xmath120 , @xmath121 and ( to a lesser extent ) @xmath122 as well as @xmath123 are strongly disfavoured .",
    "this shows that @xmath114 is preferred by current data , in agreement with the result of ref .",
    "@xcite . in general , adding in a non ",
    "zero spatial curvature leads to a well measurable decrease in the model probability that , together with the increase in effective model complexity , reinforces our belief that @xmath107 can be safely neglected for the time being .",
    "of course this result is partially a reflection of our choice of prior on @xmath107 .",
    "however , it is important to remember that had we halved the range of this prior , the likelihoods for models with non ",
    "zero curvature would have only doubled .",
    "this would not change the results significantly .",
    "alternatively , an inflation  motivated prior of the type @xmath124 would render the parameter unmeasured and irrelevant . in this case adding",
    "it would not change the effective complexity or the model likelihood at all .",
    "we also find that the basic set @xmath120 must be augmented by either @xmath106 or @xmath105 .",
    "the inclusion of both parameters at the same time was optional with the first year wmap data only , but using wmap3 we find that @xmath125 has a significantly higher model likelihood than all other models investigated here . also ,",
    "where with wmap1 we only gained half an effective parameter when going from @xmath126 or @xmath127 to @xmath125 , we now gain one full parameter .",
    "thus we can now measure both parameters at the same time .",
    "overall , we conclude that @xmath128 was a good and sufficient base model until a few months ago .",
    "now @xmath125 should be used .",
    "a wider prior on @xmath106 would have only a minimal impact on the complexity of models including a tilt , since @xmath106 seems to be rather well  measured when considered alone ( ie , not in combination with @xmath105 ) .",
    "inclusion of a non  vanishing curvature is discouraged by bayesian model comparison .",
    "we find that a model with 6 parameters is sufficient to explain the current cmb data , even though all seven effective parameters can be constrained now .",
    "this analysis demonstrates that the bayesian complexity estimator ( [ eq : complex_as_chi ] ) works with real  world data and gives useful additional information for model comparison .",
    "in this work we introduced the bayesian complexity as a measure of the effective number of parameters in a model .",
    "we discussed extensively its properties and its usefulness in the context of linear models , where it can be computed analytically .",
    "we showed that it corresponds to the number of parameters for which the width of the posterior probability distribution is significantly narrower than the width of the prior probability distribution .",
    "these parameters can be considered to have been well measured by the data given our prior assumptions in the model .",
    "we also showed that for linear models the bayesian complexity probes the trace of the posterior covariance matrix , while the model likelihood is sensitive to the determinant .",
    "we argued that the bayesian complexity allows to test for cases where the data is not informative enough for the model likelihood to be a reliable indicator of model performance , and provided an explicit example .",
    "finally , we applied the combination of model likelihood and bayesian complexity to the question of how many ( and which ) parameters are measured by current cmb data , complemented by the hst limit on the hubble parameter .",
    "we limited ourselves to the family of @xmath0cdm models with a power  law spectrum of primordial perturbations .",
    "we demonstrated that  in addition to the energy density in baryons and matter , the cmb peak location parameter @xmath100 and the amplitude of the initial perturbations  we need to consider now both the reionisation optical depth and the scalar spectral index .",
    "flat models are disfavoured .",
    "the effective complexity shows that the cmb data can measure all seven parameters in this scheme .",
    "as the bayesian complexity is very easy to compute from a mcmc chain , we hope that it will be used routinely in future data analyses in conjunction with the model likelihood for model building assessment",
    ". it will help to determine if the data is informative enough to measure the parameters under consideration .",
    "further work is needed to study the performance of the bayesian complexity in situations with a strongly non ",
    "gaussian posterior .",
    "we thank andrew liddle , pia mukherjee and glenn starkman for useful discussions .",
    "is supported by the swiss nsf .",
    "is supported by the royal astronomical society through the sir norman lockyer fellowship . d.p .",
    "is supported by pparc .",
    "the calculations were performed on the `` myri '' cluster of the university of geneva and the uk national cosmology supercomputer ( cosmos ) in cambridge .",
    "here we compute first the model likelihood for the linear model , using the gaussian prior .",
    "an analogous computation can be found in @xcite . returning to bayes theorem ( [ eq : bayes ] ) ,",
    "the posterior pdf is given by a multinormal gaussian with fisher information matrix @xmath41 f = l + p [ eq : post ] and mean @xmath129 given by | = f^-1l _ 0 .",
    "the model likelihood evaluates to [ eq : modlike ]      this can be easily interpreted by looking at its three components : the quality of fit of the model is encoded in @xmath72 , which represents the best  fit likelihood .",
    "thus a model that fits the data better will be favored by this term .",
    "the term involving the determinants of @xmath54 and @xmath41 is a volume factor ( the so called _ occam s factor _ ) .",
    "as @xmath130 , it penalizes models with a large volume of wasted parameter space , _ i.e. _  those for which the parameter space volume @xmath131 that survives after arrival of the data is much smaller than the initially available parameter space under the model prior , @xmath132 .",
    "finally , the exponential term suppresses the likelihood of models for which the parameters values that maximize the likelihood , @xmath52 , differ appreciably from the expectation value under the posterior , @xmath129 .",
    "therefore when we consider a model with an increased number of parameters we see that its model likelihood will be larger only if the quality  of  fit increases enough to offset the penalizing effect of the occam s factor .",
    "let us now turn to the computation of the bayesian complexity , eq .  .",
    "using the posterior mean ( denoted by an overbar ) as a point estimator for the effective chi  square , we obtain from ^2(| ) = ^2(_0 ) + ( |-_0)^t l ( |-_0 ) .",
    "the expectation value of the @xmath31 under the posterior is given by = ^2(_0 ) + ( l ( -_0)^t ( -_0 ) ) .",
    "we concentrate on the second term and write @xmath133 .",
    "the total term in the expectation value then becomes @xmath134 .",
    "the first term of this expression is just the posterior covariance matrix @xmath135 .",
    "the last term combines with @xmath136 to @xmath137 .",
    "the cross terms vanish since @xmath138 .",
    "all taken together , we obtain for the complexity _ b & = & ( l(-|)^t ( -| ) ) + & = & ( l f^-1 ) using the relation ( [ eq : post ] ) we can rewrite the complexity as _ b & = & \\{(f - p)f^-1 } + & = & _ 0 - \\{p f^-1 } .",
    "[ eq : dic_gauss ] thus while the model likelihood depends on the determinant of the fisher matrices , the complexity depends on their trace .",
    "another important point worth highlighting is that for linear models the complexity does not depend on the degree of overlap between the prior and the posterior , nor on the noise realization ( as long as the noise covariance matrix is known ) .",
    "liddle , mon . not .",
    ". soc . * 351 * , l49-l53 ( 2004 ) ; t.d .",
    "saini , j. weller and s.l .",
    "bridle , mon . not .",
    ". soc . * 348 * , 603 ( 2004 ) ; b.a .",
    "bassett , p.s .",
    "corasaniti and m. kunz , astrophys.j . * 617 * , l1 ( 2004 ) ; g. lazarides , r.r . de austri and r. trotta , phys .",
    "d * 70 * , 123527 ( 2004 ) ; a. niarchou , a.h .",
    "jaffe , l. pogosian , phys.rev .",
    "d69 ( 2004 ) 063515 ; p. mukherjee et al , astro - ph/0512484 ( 2005 ) ; m. beltran et al , phys . rev .",
    "d * 71 * , 063532 ( 2005 ) ; m. kunz et al , phys .",
    "d * 73 * , 023511 ( 2006 ) .",
    "r. trotta , astro - ph/0504022 ( 2005 ) .",
    "p. mukherjee , d. parkinson and a.r .",
    "liddle , astro - ph/0508461 ( 2005 ) .",
    "mackay , _ information theory , inference , and learning algorithms _ , cambridge university press ( 2003 ) .",
    "shannon , bell sys .",
    "j. * 27 * , 379 and 623 ( 1948 ) .",
    "spiegelhalter et al , j.r .",
    "b * 64 * , 583 ( 2002 ) .",
    "d. feldmann , information theory , excess entropy and computational mechanics , available from ` http://hornacek.coa.edu/dave/ ` e.t .",
    "jaynes and g.l .",
    "bretthorst ( ed . ) , _ probability theory : the logic of science _ , cambridge university press ( 2003 ) .",
    "press et al , _ numerical recipes in c _ ( second edition ) , cambridge university press ( 1992 ) .",
    "corasaniti et al , phys .",
    "d * 70 * , 083006 ( 2004 ) .",
    "a. lewis and s. bridle , phys.rev .",
    "d * 66 * , 103511 ( 2002 ) .",
    "the code is available from ` http://cosmologist.info ` a. kosowsky , m. milosavljevic and r. jimenez , phys .",
    "d * 66 * , 63007 ( 2002 ) ; m. chu , m. kaplinghat and l. knox , astrophys",
    ". j. * 596 * , 725 ( 2003 ) ."
  ],
  "abstract_text": [
    "<S> we introduce a statistical measure of the effective model complexity , called the _ </S>",
    "<S> bayesian complexity_. we demonstrate that the bayesian complexity can be used to assess how many effective parameters a set of data can support and that it is a useful complement to the model likelihood ( the evidence ) in model selection questions . </S>",
    "<S> we apply this approach to recent measurements of cosmic microwave background anisotropies combined with the hubble space telescope measurement of the hubble parameter . using mildly non  informative priors , </S>",
    "<S> we show how the 3-year wmap data improves on the first - year data by being able to measure both the spectral index and the reionization epoch at the same time . </S>",
    "<S> we also find that a non - zero curvature is strongly disfavored . </S>",
    "<S> we conclude that although current data could constrain at least seven effective parameters , only six of them are required in a scheme based on the @xmath0cdm concordance cosmology . </S>"
  ]
}