{
  "article_text": [
    "mapreduce has been established as a standard programming model for parallel computing in data centers or computational grids and it is currently used for several applications including search indexing , web analytics or data mining .",
    "however , data centers consume an enormous amount of energy and hence , energy efficiency has emerged as an important issue in the data - processing framework .",
    "several empirical works have been carried - out in order to study different mechanisms for the reduction of the energy consumption in the mapreduce setting and especially for the hadoop framework @xcite .",
    "the main mechanisms for energy saving are the _ power - down _ mechanism where in periods of low - utilization some servers are switched - off , and the _ speed - scaling _ mechanism ( or dvfs for dynamic voltage frequency scaling ) where the servers speeds may be adjusted dynamically @xcite . until lately , most work in the mapreduce framework were focused on the power - down mechanism , but recently , wirtz and ge @xcite showed that for some computation intensive mapreduce applications the use of intelligent speed - scaling may lead to significant energy savings . in this paper , we study power - aware mapreduce scheduling in the speed scaling setting from a theoretical point of view .    in a typical mapreduce framework ,",
    "the execution of a mapreduce job creates a number of map and reduce tasks .",
    "each map task processes a portion of the input data and outputs a number of key - value pairs .",
    "all key - value pairs having the same key are then given to a reduce task which processes the values associated with a key to generate the final result .",
    "this means that each reduce task can not start before the completion of the last map task of the same job . in other words",
    ", there is a complete bipartite graph implying the precedences between map and reduce tasks of a job .",
    "however , the map tasks of a job can be executed in parallel and the same holds for its reduce tasks .    in what follows",
    "we consider a set of mapreduce jobs that have to be executed on a set of speed - scalable processors , i.e. , on processors that can adjust dynamically their speed @xcite . in the speed",
    "scaling setting , each task is associated with a work volume instead of a processing time and the scheduler has to decide not only the processor and the time interval where a task is executed , but also its speed over time , taking into account the energy consumption .",
    "high processor s speeds are in favor of performance at the price of high energy consumption .",
    "each job consists of a set of map tasks and a set of reduce tasks , with every task having a positive work volume . each job is also associated with a positive weight representing its importance / priority , and a release date ( or arrival time ) . like in  @xcite",
    ", we consider that the map and the reduce tasks of each job are _ preassigned _ to the processors and in this way we take into account data locality , i.e. the fact that each map task has to be executed on the server where its data are located .",
    "given that the preemption of tasks , i.e. the possibility of interrupting a task and resuming it later , may cause important overheads we do not allow it .",
    "this is also the case often in practice : hadoop does not offer the possibility of preemption @xcite .",
    "our goal is to schedule all the tasks to the processors , so as to minimize the total weighted completion time of jobs respecting a given budget of energy .",
    "[ [ related - work . ] ] related work .",
    "+ + + + + + + + + + + + +    chang et al .",
    "@xcite consider a set of mapreduce jobs with their map and reduce tasks preassigned to processors and their goal is to minimize the total weighted completion time of jobs .",
    "they proposed approximation algorithms of ratios 3 and 2 for arbitrary and common release dates , respectively .",
    "however , they do not consider neither distinction nor dependencies between map and reduce tasks of a job .",
    "moreover , their model falls into a well - studied problem known as _ concurrent open - shop _",
    "( or _ order scheduling _ ) for which the same approximation results are known ( see  @xcite and the references therein )",
    ". extending on the above - mentioned model , chen et al .",
    "@xcite , proposed a more realistic one which takes into account the dependencies among map and reduce tasks and derived an 8-approximation algorithm for the same objective .",
    "moreover , they managed to model also the transfer of the output of map tasks to reduce tasks and to derive a 58-approximation algorithm for this generalization . in a third model proposed by moseley et al .",
    "@xcite , the dependencies between map and reduce tasks of a job are also taken into account while the assignment of tasks to processors is not given in advance .",
    "the authors studied the preemptive variant for both the case of identical and unrelated processors .",
    "they proposed constant approximation ratios of 12 and 6 , respectively .",
    "for the unrelated processors case , they focused on the special case where each job has a single map and a single reduce task . for the latter case on a single map and a single reduce processor they also proposed a qptas which becomes a ptas for a fixed number of processing times of tasks . recently , in  @xcite the authors proposed a @xmath0-approximation algorithm for the unrelated processors case with multiple map and reduce tasks per job .    in the energy - aware setting , angel et al .",
    "@xcite proposed approximation algorithms for the problem of minimizing the total weighted completion time on unrelated parallel processors , under a model where the processing time and the energy consumption of the jobs are speed dependent .",
    "other works in this setting , related to our problem , deal with single processor problems .",
    "megow et al .",
    "@xcite recently proposed a ptas for the problem of minimizing the total weighted completion time on a single speed - scalable processor .",
    "[ [ our - results - and - organization - of - the - paper ] ] our results and organization of the paper + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we adopt the mapreduce model of  @xcite where the tasks are preassigned to processors but extended with dependencies between map and reduce tasks as in chen et al .",
    "@xcite in the speed scaling setting @xcite . after a formal statement of our problem and notation ,",
    "we present , in section  [ se : lpa ] , a polynomial time lp - based @xmath1-energy @xmath1-approximation algorithm which allows energy augmentation , i.e. , it may use more energy than an optimal solution which always respects the energy budget , while using discretization of the possible speed values and list scheduling in the order of tasks @xmath2-points ( see e.g. @xcite ) . as we show",
    ", there is a tradeoff between the approximation ratio and energy augmentation as a function of @xmath2 , where the schedule is converted to a constant - factor approximation for our problem . in section  [",
    "se : cp ] , we are interested in natural list scheduling policies such as first come first serve ( fcfs ) and smith rule ( sr ) . however ,",
    "in our context we need to determine the speeds of every task in order to respect the energy budget .",
    "for that , we propose a convex programming relaxation of our problem when an order of the jobs is prespecified .",
    "this relaxation can be solved in polynomial time to arbitrary precision by the ellipsoid algorithm @xcite .",
    "then we combine the solution of this relaxation with fcfsand srand we compare experimentally their effectiveness .",
    "finally , we conclude in section  [ se : con ] .",
    "in the sequel we consider a set @xmath3 of @xmath4 mapreduce jobs to be executed on a set @xmath5 of @xmath6 speed - scalable processors .",
    "each job is associated with a positive weight @xmath7 and a release date @xmath8 and consists of a set of map tasks and a set of reduce tasks that are preassigned to the @xmath6 processors .",
    "we denote by @xmath9 the set of all tasks of all jobs , and by @xmath10 and @xmath11 the sets of all map and reduce tasks , respectively . each task @xmath12",
    "is associated a non - negative work volume @xmath13 .",
    "we consider each job having at least one map and one reduce task and that each job has at most one task , either map or reduce , assigned to each processor .",
    "map or reduce tasks can run simultaneously on different processors , while the following precedence constraints hold for each job : every reduce task can start its execution after the completion of all map tasks of the same job .    for a given schedule",
    "we denote by @xmath14 and @xmath15 the completion times of each job @xmath16 and each task @xmath12 , respectively .",
    "note that , due to the precedence constraints of map and reduce tasks , @xmath17 . by @xmath18",
    "we denote the makespan of the schedule , i.e. , the completion time of the job which finishes last .",
    "let also , @xmath19 , @xmath20 , @xmath21 , @xmath22 and @xmath23 .    in this paper , we combine this abstract model for mapreduce scheduling with the speed scaling mechanism for energy saving @xcite ( see also @xcite for a recent review ) .",
    "in this setting the power required by a processor running at time @xmath24 with speed @xmath25 is equal to @xmath26 , for a constant @xmath27 ( typical values of @xmath28 are between 2 and 3 ) and its energy consumption is power integrated over time , i.e. , @xmath29 .    due to the convexity of the speed - to - power function , a key property of our problem",
    "is that each task runs at a constant speed during its whole execution .",
    "so , if a task @xmath30 is executed at a speed @xmath31 , the time needed for its execution ( processing time ) is equal to @xmath32 and its energy consumption is @xmath33 .",
    "moreover , we are given an energy budget @xmath34 and the goal is to schedule _ non - preemptively _ all the tasks to the @xmath6 processors , so as to minimize the total weighted completion time of the schedule , i.e. , @xmath35 , without exceeding the energy budget @xmath34 .",
    "we refer to this problem as mapreduceproblem .",
    "as already mentioned above , the special case of the mapreduceproblem where there are not dependencies between map and reduce tasks of each job and each processor runs at a constant speed , reduces to the _ concurrent open - shop _",
    "problem which is known to be strongly @xmath36-complete  @xcite .",
    "it is also easy to adapt @xmath36-completeness reductions like the one for the concurrent open - shop problem in the speed scaling setting , and therefore , the mapreduceproblem is also strongly @xmath36-hard .",
    "in this section we present a constant - factor approximation algorithm for the mapreduceproblem .",
    "our algorithm allows energy augmentation and derives a @xmath1-energy @xmath1-approximation schedule for the problem . as we show",
    ", there is a tradeoff where the schedule can be converted to a constant - factor approximate schedule for the mapreduceproblem .",
    "our algorithm is based on a formulation of the problem as a linear programming relaxation .",
    "then , we transform the solution obtained by the linear program to a feasible schedule for the mapreduceproblem using the technique of @xmath2-points .      before presenting the linear programming formulation ,",
    "our first step is to discretize the possible speed values by loosing a factor of @xmath37 with respect to an optimal solution . in order to do this ,",
    "we need the following propositions that bound the length of an optimal schedule and the possible speed values .    [ prop : t ] the makespan of any optimal schedule for the mapreduceproblem is at most @xmath38    consider an optimal schedule for the mapreduceproblem . by definition ,",
    "we have that @xmath39 .",
    "hence , it holds that @xmath40 .    in order to give an upper bound to @xmath41 ,",
    "consider an instance of our problem where the weight @xmath7 and the release date @xmath8 of each job @xmath16 are rounded up to @xmath42 and @xmath43 , respectively .",
    "moreover , assume that in this instance all tasks have work equal to @xmath44 .",
    "consider now an arbitrary order @xmath45 of the jobs .",
    "we create a feasible schedule @xmath46 for the modified instance as follows .",
    "all tasks run with the same speed @xmath47 , hence each task has a processing time @xmath48 .",
    "note that this speed allows us to execute all tasks without exceeding the energy budget . as all tasks have the same processing time",
    ", we can consider the time horizon partitioned into time slots of length @xmath49 starting from @xmath43 .",
    "for each job @xmath50 , @xmath51 , we execute its map tasks at time @xmath52 and its reduce tasks at time @xmath53 .",
    "then , for the objective value @xmath54 of this schedule it holds that @xmath55 the objective value of schedule @xmath46 is clearly an upper bound on the objective value @xmath41 of an optimal schedule for the initial instance and the proposition follows .",
    "[ prop : speedbound ] for the speed @xmath31 of any task @xmath12 in the optimal schedule it holds that @xmath56    the processing time @xmath57 of a task @xmath58 in an optimal schedule can not exceed the maximum completion time , that is @xmath59 and , since by proposition  [ prop : t ] it holds that @xmath60 , the lower bound follows .",
    "the energy consumption of any task can not exceed the energy budget , that is @xmath61 and the upper bound follows .",
    "let @xmath62 and @xmath63 be an upper and a lower bound , respectively , on the speed of any task .",
    "given these bounds , we discretize the interval @xmath64 $ ] geometrically . in other words ,",
    "we assume that the processors can only run according to one of the following speeds : @xmath65 , where @xmath66 is the smallest integer such that @xmath67 .",
    "note that @xmath68 and hence the number of possible speeds is polynomial to the size of the instance and to @xmath69 .",
    "we denote by @xmath70 the set of all possible discrete speed values .",
    "let also @xmath71 .",
    "[ le : discretespeeds ] there is a feasible @xmath37-approximate schedule for the mapreduceproblem in which each task @xmath12 runs at a speed @xmath72 .",
    "let an optimal schedule for our problem and consider the speed of each task @xmath12 rounded down to the closest @xmath73 value .",
    "as the speeds are decreased , the energy consumption of @xmath74 does not exceed @xmath34 .",
    "moreover , the execution time of all tasks , and hence the completion time of every job and the optimal objective value increase by a factor at most @xmath37 .    henceforth we will consider the mapreduceproblem in which each task @xmath12 runs at a single speed @xmath72 .",
    "we call this version of the problem ds - mapreduce .      in what follows",
    "we give an interval - indexed linear programming relaxation of the ds - mapreduceproblem . in order to do this , we discretize the time horizon of an optimal schedule as follows . by proposition  [ prop : t ] , in any optimal schedule , all jobs are executed during the interval @xmath75 $ ] .",
    "we partition @xmath75 $ ] into the intervals @xmath76,(\\lambda,\\lambda(1+\\delta)],(\\lambda(1+\\delta),\\lambda(1+\\delta)^2],\\ldots,(\\lambda(1+\\delta)^{u-1},\\lambda(1+\\delta)^u]$ ] , where @xmath77 is a small constant , @xmath78 is a constant that we will define later , and @xmath79 is the smallest integer such that @xmath80 .",
    "let @xmath81 and @xmath82 , for @xmath83 .",
    "moreover , let @xmath84 $ ] , for @xmath85 , and @xmath86 be the length of the interval @xmath87 , i.e. , @xmath88 and @xmath89 , @xmath90 .",
    "note that , the number of intervals is polynomial to the size of the instance and to @xmath91 , as @xmath92 .",
    "let @xmath93 be the potential processing time for each task @xmath12 if it is executed entirely with speed @xmath72 .",
    "for each @xmath94 , @xmath95 and @xmath96 , we introduce a variable @xmath97 that corresponds to the portion of the interval @xmath87 during which the task @xmath30 is executed with speed @xmath98 .",
    "in other words , @xmath99 is the time that task @xmath30 is executed within the interval @xmath87 at speed @xmath98 , or equivalently , @xmath100 is the fraction of the task @xmath30 that is executed within @xmath87 at speed @xmath98 .",
    "note that the number of @xmath97 variables is polynomial to the size of the instance , to @xmath69 and to @xmath91 .",
    "furthermore , for each task @xmath12 , we introduce a variable @xmath15 , which corresponds to the completion time of @xmath30 .",
    "finally , let @xmath14 , @xmath101 , be the variable that corresponds to the completion time of job @xmath50 .",
    "( lp ) is a linear programming relaxation of the ds - mapreduceproblem .",
    "@xmath102    our objective is to minimize the sum of weighted completion times of all jobs . for each task @xmath12 , the corresponding constraint  ( [ lp : p1 ] ) ensures that @xmath30 is entirely executed .",
    "constraints  ( [ lp : p2 ] ) enforce that the total amount of processing time that is executed within an interval @xmath87 can not exceed its length . in  @xcite ,",
    "the authors proposed a lower bound for the completion time of a job .",
    "this lower bound can be adapted to our problem and for the completion time of a task @xmath12 leads to a corresponding constraint  ( [ lp : p3 ] ) .",
    "constraints  ( [ lp : p4 ] ) ensure that the completion time of each job is the maximum over the completion times of all its tasks .",
    "constraint  ( [ lp : p5 ] ) ensures that the given energy budget is not exceeded .",
    "note that the value @xmath103 for each @xmath72 is a fixed number .",
    "constraints  ( [ lp : p6 ] ) imply the precedence constraints between the map and the reduce tasks of the same job , as they enforce that the fraction of a map task that is executed up to each time point should be at least the fraction of a reduce task of the same job executed up to the same time point ; hence , each map task completes before all reduce tasks of the same job .",
    "constraints  ( [ lp : p7 ] ) do not allow tasks of a job to be executed before their release date .    in what follows",
    ", we denote an optimal solution to ( lp ) by @xmath104 .      in this section",
    "we use ( lp ) to derive a feasible schedule for the ds - mapreduceproblem .",
    "depending on the choice of some parameters , this schedule may exceed the energy budget . as we show",
    ", there is a tradeoff where a constant factor approximation ratio can be derived .",
    "our algorithm is based on the idea of list scheduling in order of @xmath2-points  @xcite . in general ,",
    "an @xmath2-point of a job is the first point in time where an @xmath2-fraction of the job has been completed , where @xmath105 is a constant that depends on the analysis . in this paper",
    ", we will define the @xmath2-point @xmath106 of a task @xmath12 as the minimum @xmath107 , @xmath108 , such that at least an @xmath2-fraction of @xmath13 is accomplished up to the interval @xmath109 to ( lp ) , i.e. , @xmath110 thus , once our algorithm has computed an optimal solution @xmath111 , @xmath112 , @xmath113 to ( lp ) , it calculates the corresponding @xmath2-point , @xmath106 , for each task @xmath94 .",
    "then , combining the ideas of  @xcite with the notion of @xmath2-points  @xcite , we create a feasible schedule as follows : for each processor @xmath114 , we consider a priority list @xmath115 of its tasks such that tasks with smaller @xmath2-point have higher priority . a crucial point in our analysis",
    "is that we consider that a task @xmath12 becomes _ available _ for the algorithm after the time @xmath116 . moreover ,",
    "if @xmath117 then we need also all tasks @xmath118 to be completed in order @xmath30 to be considered as available .",
    "for each task @xmath12 , we use a constant speed @xmath119 , where @xmath120 is the processing time of @xmath30 used by our algorithm , and @xmath121 is a constant that we define later and describes the tradeoff between the energy consumption and the weighted completion time of jobs . at each time point",
    "where a processor @xmath114 is available , our algorithm selects the highest priority available task in @xmath115 which has not been yet executed .",
    "note that our algorithm always create a feasible solution as we do not insist on selecting the highest priority task if this is not available .",
    "algorithm @xmath122gives a formal description of our algorithm .",
    "algorithm @xmath122    compute an optimal solution @xmath104 to @xmath123 .",
    "compute the @xmath2-point @xmath124 , the processing time @xmath57 and the speed @xmath31 .",
    "compute the priority list @xmath115 .",
    "select the first available task , let @xmath30 , in @xmath115 which has not been yet executed .",
    "schedule @xmath30 , non - preemptively , with processing time @xmath57 .",
    "+ let @xmath15 be the completion time of task @xmath30 .",
    "compute its completion time @xmath125 .",
    "note that the processing time of a task @xmath12 to an optimal solution to ( lp ) is @xmath126 hence , the energy consumption @xmath127 for the execution of @xmath30 to an optimal solution to ( lp ) may be smaller or bigger than the energy consumption @xmath128 for the execution of @xmath30 by the algorithm . in order to give the relation between these two quantities , we need the following technical lemma .    [",
    "le : technical ] let @xmath129 and @xmath130 be positive values and @xmath131 .",
    "then , it holds that @xmath132    the expression of the statement can be written equivalently as follows .",
    "@xmath133 note that the function @xmath134 is convex for @xmath131 .",
    "thus , by the jensen s inequality we have that @xmath135 which is translated as @xmath136 therefore , in order to show inequality ( [ eq : technical ] ) , it suffices to show that @xmath137 thus , it suffices to prove that @xmath138 an equivalent representation of the above expression is @xmath139 the last inequality is always true , as @xmath140 and hence the lemma follows .",
    "[ le : speed ] let @xmath141 and @xmath128 be the energy consumption of the task @xmath94 to the optimal solution to ( lp ) and to the solution of algorithm @xmath122 , respectively .",
    "it holds that @xmath142    by the definition of @xmath128 we have that @xmath143 since for each speed @xmath144 , @xmath93 , the above equality can be written as @xmath145 hence , by using lemma  [ le : technical ] we get @xmath146 by the definition of @xmath2-points we have that @xmath147 , and thus @xmath148 and the lemma follows .",
    "the following lemma provides a lower bound to the completion time @xmath112 of the task @xmath12 given by the ( lp ) .    if @xmath149 , then for each task @xmath12 it holds that @xmath150 .",
    "[ le : clowerbound ]    recall that @xmath106 corresponds to the interval @xmath151 $ ] .",
    "if we select @xmath149 , then there is no task with @xmath2-point to the interval @xmath152 .",
    "hence , we can consider that the @xmath2-point of each task @xmath12 corresponds to an interval of the form @xmath153 $ ] .",
    "starting from constraint  ( [ lp : p3 ] ) we have that @xmath154 where the last inequality holds by constraint  ( [ lp : p1 ] ) and as by the definition of @xmath2-point we know that @xmath155 .",
    "the following theorem gives the approximation ratio of algorithm @xmath122 .",
    "algorithm @xmath122is a @xmath156-energy @xmath157-approximation algorithm for the ds - mapreduceproblem , where @xmath158 , @xmath121 and @xmath77 .",
    "[ thm : main ]    consider the schedule @xmath74 produced by algorithm @xmath122and let @xmath159 be any map task",
    ". recall that @xmath115 is the priority list of processor @xmath160 .",
    "let @xmath161 be the list of tasks with priority higher than the priority of @xmath30 in @xmath115 , including @xmath30 .",
    "then , for @xmath15 it holds that @xmath162 as @xmath30 is always available after @xmath163 , as a map task . for the total processing time of jobs in @xmath164 we have that @xmath165 where the last inequality holds by applying constraint  ( [ lp : p2 ] ) of the ( lp )",
    "thus , from inequality  ( [ eq : up ] ) we have @xmath166 for each map task @xmath12 .",
    "consider now a job @xmath16 and let @xmath117 be a reduce task of @xmath50 .",
    "moreover , let @xmath118 be the map task of @xmath50 that completes last in @xmath74 , i.e. , @xmath167 . by definition",
    ", @xmath30 becomes available at time @xmath168 .",
    "note that @xmath169 where the first inequality holds by inequality  ( [ eq : map ] ) and the second by the constraint  ( [ lp : p6 ] ) of ( lp ) .",
    "let again @xmath164 be the list of tasks with higher priority than @xmath30 in @xmath115 , including @xmath30 .",
    "if in the schedule @xmath74 the processor @xmath160 at time @xmath24 executes a task @xmath170 , then for the completion time of @xmath30 it holds that @xmath171 because @xmath30 is available after time @xmath24 and it has higher priority than any task @xmath172 .",
    "as before , we have that @xmath173 moreover , for the processing time of @xmath174 it holds that @xmath175 as @xmath174 is executed at time @xmath24 and hence it is available .",
    "then , by equation  ( [ eq : red ] ) we have @xmath176    as @xmath177 , using lemma  [ le : clowerbound ] we get @xmath178 and by using constraint ( [ lp : p4 ] ) of ( lp ) @xmath179 since the above inequality holds for each processor @xmath114 , it must also hold for @xmath180 and thus @xmath181 if we sum up all weighted completion times in @xmath74 we yield @xmath182 and as @xmath183 is a lower bound to the objective value of an optimal solution for the ds - mapreduceproblem , the theorem follows .    note that in the absence of precedence constraints between the tasks , the above analysis can be improved . indeed , we can consider that all tasks are map tasks , and hence an upper bound to their completion time into the schedule created by algorithm @xmath122is given by inequality  ( [ eq : map ] ) . then , the following corollary holds .",
    "[ cor : noprec ] algorithm @xmath122is a @xmath156-energy @xmath184-approximation algorithm for the ds - mapreduceproblem without precedence constraints , where @xmath158 , @xmath121 and @xmath77 .",
    "moreover , in the absence of both precedence constraints between tasks and release dates of jobs , our analysis can be further improved .",
    "as before , we can consider that all tasks are map tasks .",
    "in addition , we can drop the demand that a task @xmath12 becomes available for the algorithm after the time @xmath163 .",
    "hence , inequality  ( [ eq : up ] ) is simplified to @xmath185 , as all tasks are released at time 0 and they are available at any time .",
    "then , the following corollary holds .",
    "[ cor : noprecreal ] algorithm @xmath122is a @xmath156-energy @xmath186-approximation algorithm for the ds - mapreduceproblem without precedence constraints and release dates , where @xmath158 , @xmath121 and @xmath77 .    by combining lemma  [ le : discretespeeds ] , theorem  [ thm : main ] and corollaries  [ cor : noprec ] and  [ cor : noprecreal ] , and as we can select an @xmath187 such that @xmath188 , the following theorem holds .",
    "there is a @xmath156-energy @xmath189-approximation algorithm for the mapreduceproblem , a @xmath156-energy @xmath190-approximation algorithm for the mapreduceproblem without precedence constraints , and a @xmath156-energy @xmath191-approximation algorithm for the mapreduceproblem without precedence constraints and release dates , where @xmath158 , @xmath121 and @xmath192 .    in fig.[fig :",
    "tradeoff ] we depict a tradeoff between energy augmentation and approximation ratio for some practical values of @xmath28 .",
    "note that , by choosing @xmath193{\\alpha}}$ ] , energy augmentation is not allowed and the schedule can be converted to a constant - factor approximate schedule . in this case the following theorem holds .",
    "there is a @xmath194{\\alpha})^2 + 3\\alpha \\sqrt[\\beta-1]{\\alpha}+1}{(\\alpha \\sqrt[\\beta-1]{\\alpha})^2(1-\\alpha)}(1+\\varepsilon)$]-approximation algorithm for the mapreduceproblem , a @xmath195{\\alpha}+1}{\\alpha \\sqrt[\\beta-1]{\\alpha}(1-\\alpha)}(1+\\varepsilon)$]-approximation algorithm for the mapreduceproblem without precedence constraints , and a @xmath196{\\alpha}(1-\\alpha)}(1+\\varepsilon)$]-approximation algorithm for the mapreduceproblem without precedence constraints and release dates , where @xmath158 and @xmath192 .",
    "the ratios of the above theorem can be optimized by selecting the appropriate value of @xmath2 for each @xmath28 .",
    "table  [ tbl : results ] gives the achieved ratios for practical values of @xmath28 .",
    ".approximation ratios for the mapreduceproblem for different values of @xmath28 .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]",
    "we are interested in natural list scheduling policies such as first come first serve ( fcfs ) and smith rule ( sr ) . however ,",
    "in our context we need to determine the speeds of every task in order to respect the energy budget . for that",
    ", we propose a convex programming relaxation of our problem when an order of the jobs is prespecified .",
    "let @xmath197 be a given order of the jobs .",
    "consider now the restricted version of the mapreduceproblem where for each processor @xmath114 the tasks are forced to be executed according to this order .",
    "we shall refer to this problem as the mapreduce@xmath198problem .",
    "note that , the order is the same for all processors .",
    "we write @xmath199 if job @xmath16 precedes job @xmath200 in @xmath201 .",
    "we propose a convex program that considers the order @xmath201 as input and returns a solution that is a lower bound to the optimal solution for the mapreduce@xmath198problem .    in order to formulate our problem as a convex program ,",
    "let @xmath57 be a variable that corresponds to the processing time of task @xmath12 .",
    "moreover , for each task @xmath12 , we introduce a variable @xmath15 that determines the completion time of @xmath30 .",
    "finally , let @xmath14 , @xmath101 , be the variable that corresponds to the completion time of job @xmath50 .",
    "consider the following convex programming formulation of the mapreduce@xmath198problem .",
    "@xmath202    the objective function of ( cp ) is to minimize the weighted completion time of all jobs .",
    "constraint  ( [ cp : p1 ] ) guarantees that the energy budget is not exceeded .",
    "constraints  ( [ cp : p2 ] ) and  ( [ cp : p3 ] ) give lower bounds on the completion time of each task @xmath12 , based on the release dates and the precedence constraints , respectively .",
    "note that , if we do not consider precedences between the tasks , then ( cp ) will return the optimal value of the objective function , instead of a lower bound of it , as constraints  ( [ cp : p2 ] ) describe in a complete way the completion times of the tasks .",
    "however , this is not true for constraints  ( [ cp : p3 ] ) which are responsible for the precedence constraints .",
    "finally , constraints  ( [ cp : p4 ] ) ensure that the completion time of each job is the maximum over the completion times among all of its tasks .    as the optimal solution to ( cp )",
    "does not necessarily describe a feasible schedule , we need to apply an algorithm that uses the processing times found by ( cp ) and the order @xmath201 so as to create a feasible schedule for the mapreduce@xmath198problem , and hence for the mapreduceproblem . in fact , it suffices to apply , for example , the lines  6 - 8 of algorithm @xmath122 , by considering the same order for all processors .",
    "plot coordinates ( 37.52,0 ) ( 33.32,10 ) ( 29.97,20 ) ( 27.25,30 ) ( 24.99,40 ) ( 23.10,50 ) ( 21.49,60 ) ( 20.10,70 ) ( 18.90,80 ) ( 17.84,90 ) ( 16.91,100 ) ; plot coordinates ( 32.25,0 ) ( 29.80,10 ) ( 27.75,20 ) ( 26.02,30 ) ( 24.53,40 ) ( 23.24,50 ) ( 22.11,60 ) ( 21.11,70 ) ( 20.22,80 ) ( 19.42,90 ) ( 18.69,100 ) ; plot coordinates ( 29.62,0 ) ( 27.91,10 ) ( 26.46,20 ) ( 25.20,30 ) ( 24.10,40 ) ( 23.13,50 ) ( 22.27,60 ) ( 21.49,70 ) ( 20.79,80 ) ( 20.15,90 ) ( 19.57,100 ) ;    ] plot coordinates ( 5,1919/1000 ) ( 10,12857/1000 ) ( 15,38352/1000 ) ( 20,85177/1000 ) ( 25,174398/1000 ) ; plot coordinates ( 5,2321/1000 ) ( 10,16222/1000 ) ( 15,47570/1000 ) ( 20,100876/1000 ) ( 25,200904/1000 ) ; plot coordinates ( 5,1259/1000 ) ( 10,8594/1000 ) ( 15,26120/1000 ) ( 20,60228/1000 ) ( 25,119361/1000 ) ; plot coordinates ( 5,1001/1000 ) ( 10,6311/1000 ) ( 15,18943/1000 ) ( 20,41997/1000 ) ( 25,86365/1000 ) ;      in this section we propose different orders of jobs and we discuss how far is an optimal solution for the mapreduce@xmath198problem using these orders with respect to the optimal solution for the mapreduceproblem .",
    "two standard orders of jobs are the following .",
    "first come first serve ( fcfs ) : for each pair of jobs @xmath203 , if @xmath204 then @xmath199 in @xmath201 .",
    "smith rule ( sr ) : for each pair of jobs @xmath203 , if @xmath205 then @xmath199 in @xmath201 .",
    "the following propositions present negative results concerning the approximation ratio that we can achieve if we use the fcfsor the srorder .",
    "[ prop : fcfs ] let @xmath206 and @xmath207 be the optimal solutions for the mapreduceand the mapreduce@xmath208problems , respectively .",
    "there is an instance for which it holds that @xmath209 .",
    "consider an instance consisting of @xmath6 processors and @xmath4 jobs , where @xmath210 .",
    "the release date of each job @xmath16 is @xmath211 , for a very small @xmath212 , and its weight @xmath213 .",
    "each job @xmath16 consists of @xmath6 tasks , one per processor .",
    "moreover , the task @xmath12 is a map task only if @xmath214 ; otherwise @xmath30 is a reduce task . for each task @xmath215 ,",
    "let @xmath216 . for each task @xmath117 ,",
    "let @xmath217 .",
    "let also @xmath218 and @xmath219 .",
    "note that , if @xmath220 then the processing time of each reduce task can be considered to be very small in both the optimal schedules for the mapreduceand the mapreduce@xmath208problems .",
    "so , we can ignore the execution time and the energy consumption of the reduce tasks .",
    "we only consider the precedence constraints that they imply .    in an optimal solution for the mapreduceproblem ,",
    "the map task of job @xmath50 starts at time @xmath211 . due to the convexity and the fact that @xmath213 for each @xmath16 , we can assume that all map tasks will be executed with the same speed ;",
    "hence the processing time of each map task is approximately equal to @xmath221{\\frac{m}{e}}=m$ ] , as @xmath218 and @xmath219 .",
    "thus , the completion time of each job is approximately equal to @xmath6 , and hence @xmath222 .    on the other hand , in an optimal solution for the mapreduce@xmath208problem the map tasks are not executed in parallel , as we are forced to respect the order and the precedence constraints .",
    "ignoring again the processing times of the reduce tasks , we can assume that the map task of job @xmath50 starts at the completion time of job @xmath223 . in order to find the speed @xmath224 of each map task @xmath225 into an optimal solution for the mapreduce@xmath208problem",
    ", we have to solve the following convex program .",
    "@xmath226 the objective of this convex program corresponds to the objective of the mapreduce@xmath208problem for the given instance , while the constraint ensures that the selected speeds respect the energy budget . by applying the karush - kuhn - tucker conditions to this program",
    "we get that @xmath227 . by replacing this to the objective we get @xmath228    as @xmath229 and @xmath218",
    ", the proposition follows .",
    "[ prop : sr ] let @xmath206 and @xmath230 be the optimal solutions for the mapreduceand the mapreduce@xmath231problems , respectively .",
    "there is an instance for which it holds that @xmath232 .",
    "we consider a simplified instance which consists of only one processor and does not take into account map and reduce tasks and hence precedences . in this instance",
    "the critical issue is the release dates . for each job @xmath50 , @xmath233 , we have @xmath234 , @xmath213 and @xmath235 , while for the job @xmath4 we have @xmath236 , @xmath237 and @xmath238 , where @xmath239 is a big number .",
    "let @xmath218 and @xmath219 .",
    "in an optimal schedule for the mapreduceproblem , the jobs @xmath240 are scheduled consecutively starting from time 0 , while the job @xmath4 is scheduled starting from time @xmath241 .",
    "let @xmath242 and @xmath243 be parts of the energy budget used for the execution of the jobs @xmath240 and @xmath4 , respectively .",
    "clearly , it holds that @xmath244 .",
    "hence , following similar analysis as in proposition  [ prop : fcfs ] for the mapreduce@xmath208problem , for the total weighted completion time of the jobs @xmath240 it holds that @xmath245 the processing time of job @xmath4 is @xmath243 , and hence its completion time is @xmath246 .",
    "therefore , for the optimal solution for the mapreduceproblem we have that @xmath247 as this function is minimized for @xmath248 .    on the other hand , in an optimal schedule for the mapreduce@xmath231problem",
    ", the jobs are scheduled starting from @xmath241 according to the srorder , i.e. , @xmath249 . as we can choose an @xmath250 such that @xmath251",
    ", we can assume that all jobs have the same work to execute .",
    "then , following similar analysis as in proposition  [ prop : fcfs ] for the mapreduce@xmath208problem , we have that @xmath252 .",
    "as @xmath241 can be arbitrary large , the proposition follows .      in this section ,",
    "our goal is to compare the fcfsand srpolicies with respect to the quality of the solution that they produce .",
    "our simulations have been performed on a machine with a cpu intel xeon x5650 with 8 cores , running at 2.67ghz .",
    "the operating system of the machine is a linux debian 6.0 .",
    "we used matlab with cvx toolbox .",
    "the solver used for the convex program is sedumi .",
    "the instance of the problem consists of a matrix @xmath253 that corresponds to the work of the tasks , two vectors of size @xmath4 that correspond to the weights and the release dates of jobs , a precedence graph for the tasks of the same job , the energy budget and the value of @xmath28 .",
    "similarly with @xcite , the instance consists of @xmath254 processors and up to @xmath255 jobs .",
    "each job has 20 map and 10 reduce tasks , which are preassigned at random to a different processor .",
    "the work of each map task is selected uniformly at random in @xmath256 $ ] , while the work of each reduce task @xmath257 is equal to a random number in @xmath256 $ ] plus @xmath258 , taking into account the fact that reduce tasks have more work to execute than map tasks .",
    "the weight of each job is selected uniformly at random in @xmath256 $ ] .",
    "for the release date of a job , we select with probability 1/2 every interval @xmath259 $ ] . then , the release date is equal to a random value in this interval .",
    "the energy budget that we used is @xmath260 .",
    "we have also set @xmath219 .",
    "we set the desired accuracy of the returned solution of the convex program to be equal to @xmath261 . for each number of jobs",
    "we have repeated the experiments with 10 different matrices .",
    "the results we present below , concern the average of these 10 instances .",
    "the benchmark as well as the code we used in our experiments are freely available at + http://www.ibisc.univ - evry.fr/~vchau / research / mapreduce/.    as mentioned before , the ( cp ) does not lead to a feasible solution for our problem .",
    "in order to get such a solution we apply the following algorithm . at each time",
    "@xmath24 where a processor becomes available we select to schedule the task @xmath30 of higher priority such that : ( i ) @xmath30 is already released at @xmath24 , ( ii ) if @xmath30 is a reduce task , then all map tasks of the same job have been already completed at @xmath24 , and ( iii ) @xmath30 has not been yet executed .    as shown in fig .",
    "[ fig : exp ] the heuristic based on fcfsoutperforms the heuristic based on sr .",
    "in fact , the first heuristic gives up to @xmath262 better solutions that the second one for different values of @xmath4 .",
    "surprisingly , the situation is completely inverse if we consider the corresponding solutions of the convex programs .",
    "more precisely , the convex programming relaxation using srleads to @xmath263 smaller values of the objective function with respect to the convex programming relaxation using fcfs .",
    "moreover , we can observe that the ratio between the final solution of each heuristic with respect to the lower bound for the mapreduce@xmath198problem given by the convex program is equal to 1.46 for fcfsand 2.43 for sr ; the variance is less than 0.1 in both cases .",
    "however , as we already mentioned , this ratio can not be considered as the approximation ratio for the mapreduceproblem , as its optimal solution can be significantly smaller than the optimal solution for the mapreduce@xmath198problem using the fcfsand srorders .",
    "we presented a constant - approximation algorithm for the problem of scheduling a set of mapreduce jobs in order to minimize their total weighted completion time under a given budget of energy .",
    "our algorithm uses an optimal solution to an lp relaxation in interval - indexed variables and converts it to a feasible non - preemptive schedule of the mapreduceproblem using the idea of list scheduling in order of a - points .",
    "moreover , we proposed a convex programming relaxation of the problem when a prespecified order of jobs is given .",
    "based on the solution of this convex programming relaxation , we explored the efficiency of standard scheduling policies , by presenting counterexamples for them as well as by experimentally evaluating their performance .",
    "it has to be noticed that our results can be extended also to the case where multiple map or reduce tasks of a job are executed on the same processor .",
    "an interesting direction for future work concerns the online case of the problem .",
    "although , it can be proved that there is no an @xmath1-competitive deterministic algorithm ( see theorem  13 in  @xcite ) , a possible way to overcome this is to consider resource ( energy ) augmentation , or to study the closely - related objective of a linear combination of the sum of weighted completion times of the jobs and of the total consumed energy .",
    "h.  chang , m.  s. kodialam , r.  r. kompella , t.  v. lakshman , m.  lee , and s.  mukherjee .",
    "scheduling in mapreduce - like systems for fast completion time . in _",
    "ieee proceedings of the 30th international conference on computer communications _ , pages 30743082 , 2011 .",
    "f.  chen , m.  s. kodialam , and t.  v. lakshman .",
    "joint scheduling of processing and shuffle phases in mapreduce systems . in _",
    "ieee proceedings of the 31st international conference on computer communications _ , pages 11431151 , 2012 .",
    "l.  a. hall , d.  b. shmoys , and j.  wein . scheduling to minimize average completion time : off - line and on - line algorithms . in _ proceedings of the 7th acm - siam symposium on discrete algorithms _ ,",
    "pages 142151 , 1996 .",
    "b.  moseley , a.  dasgupta , r.  kumar , and t.  sarls .",
    "on scheduling in map - reduce and flow - shops . in _ proceedings of the 23rd acm symposium on parallel algorithms and architectures ( spaa ) _ , pages 289298 , 2011 .",
    "martin skutella .",
    "list scheduling in order of -points on a single machine . in evripidis bampis , klaus jansen , and claire kenyon , editors , _ efficient approximation and online algorithms : recent progress on classical combinatorial optimization problems and new applications _ , volume 3484 of _ lecture notes in computer science _",
    ", page 250291 .",
    "springer , 2006 ."
  ],
  "abstract_text": [
    "<S> mapreduce is emerged as a prominent programming model for data - intensive computation . in this work , we study power - aware mapreduce scheduling in the speed scaling setting first introduced by yao et al .  [ </S>",
    "<S> focs 1995 ] . </S>",
    "<S> we focus on the minimization of the total weighted completion time of a set of mapreduce jobs under a given budget of energy . using a linear programming relaxation of our problem </S>",
    "<S> , we derive a polynomial time constant - factor approximation algorithm . </S>",
    "<S> we also propose a convex programming formulation that we combine with standard list scheduling policies , and we evaluate their performance using simulations . </S>"
  ]
}