{
  "article_text": [
    "a @xmath0th - order @xmath1-dimensional real tensor @xmath2 consists of @xmath3 entries in real numbers : @xmath4 @xmath2 is called _ symmetric _ if the value of @xmath5 is invariant under any permutation of its indices @xmath6 . recall the definition of tensor product , @xmath7 is a vector in @xmath8 with its @xmath9th component as @xmath10 a real symmetric tensor @xmath2 of order @xmath0 dimension @xmath1 uniquely defines a @xmath0th degree homogeneous polynomial function @xmath11 with real coefficient by @xmath12 we call that the tensor @xmath2 is positive definite if @xmath13 for all @xmath14 .    in 2005 , qi @xcite and lim @xcite proposed the definition of eigenvalues and eigenvectors for higher order tenors , independently . furthermore , in @xcite , these definitions were unified by chang , person and zhang .",
    "let @xmath2 and @xmath15 be real - valued , @xmath0th - order @xmath1-dimensional symmetric tensors .",
    "assume further that @xmath0 is even and @xmath15 is positive definite .",
    "we call @xmath16 is a * generalized eigenpair * of @xmath17 if @xmath18    when the tensor @xmath15 is an identity tensor @xmath19 such that @xmath20 for all @xmath21 @xcite , the eigenpair reduces to @xmath22-eigenpair @xcite which is defined as a pair @xmath16 satisfying @xmath23 another special case is that when @xmath24 with @xmath25 @xcite , the real scalar @xmath26 is called an @xmath27-eigenvalue and the real vector @xmath28 is the associated @xmath27-eigenvector of the tensor @xmath2 @xcite .    in the last decade ,",
    "tensor eigenproblem has received much attention in the literature @xcite , which has numerous applications in magnetic resonance imaging @xcite , image analysis @xcite , data fitting @xcite , quantum information @xcite , automatic control @xcite , higher order markov chains @xcite , spectral graph theory @xcite , multi - label learning @xcite , and so on . in @xcite ,",
    "a positive semidefinite diffusion tensor ( psdt ) model was proposed to approximate the apparent diffusion coefficient ( adc ) profile for high - order diffusion tensor imaging , where the smallest z - eigenvalue need to be nonnegative to guarantee the positive definiteness of the diffusivity function .",
    "based on all of the z - eigenvalues , a generalized fractional anisotropy ( gfa ) was proposed to characterize the anisotropic diffusion profile for psdt .",
    "gfa is rotationally invariant and independent from the choice of the laboratory coordinate system . in automatic control @xcite , the smallest eigenvalue of tensors could reflect the stability of a nonlinear autonomous system . in @xcite , the principal z - eigenvectors can depict the orientations of nerve fibers in the voxel of white matter of human brain .",
    "recently , a higher order tensor vessel tractography was proposed for segmentation of vascular structures , in which the principal directions of a 4-dimensional tensor were used in vessel tractography approach @xcite .    in general , it is np - hard to compute eigenvalues of a tensor @xcite . in @xcite , a direct method to calculate all of z - eigenvalues",
    "was proposed for two and three dimensional symmetric tensors . for general symmetric tensors ,",
    "a shifted higher order power method was proposed for computing z - eigenpairs in @xcite .",
    "recently , in @xcite , an adaptive version of higher order power method was presented for generalized eigenpairs of symmetric tensor . in order to guarantee the convergence of power method",
    ", they need a shift to force the objective to be ( locally ) concave / convex . in this case , the power method is a monotone gradient method with unit - stepsize . by using fixed - point analysis",
    ", linear convergence rate is established for the shifted higher order power method @xcite .",
    "however , similarly to the case of matrix , when the largest eigenvalue is close to the second dominant eigenvalue , the convergence of power method will be very slow @xcite .",
    "in the recent years , there are various optimization approaches were proposed for tensor eigenvalue problem @xcite . in @xcite , han proposed an unconstrained optimization model for computing generalized eigenpair of symmetric tensors . by using bfgs method to solve the unconstrained optimization , the sequence will be convergent superlinearly .",
    "a subspace projection method was proposed in @xcite for z - eigenvalues of symmetric tensors .",
    "recently , in @xcite , hao , cui and dai proposed a trust region method for z - eigenvalues of symmetric tensor and the sequence enjoys a locally quadratic convergence rate . in @xcite , ni and qi employed newton method for the kkt system of optimization problem , and obtained a quadratically convergent algorithm for finding the largest eigenvalue of a nonnegative homogeneous polynomial map . in @xcite , an inexact steepest descent method was proposed for computing eigenvalues of large scale hankel tensors . since nonlinear optimization methods may stop at a local optimum , a sequential semi - definite programming method was proposed by hu et al .",
    "@xcite for finding the extremal z - eigenvalues of tensors .",
    "moreover , in @xcite , a jacobian semi - definite relaxation approach was presented to compute all of the real eigenvalues of symmetric tenors .    in practice ,",
    "one just need to compute extremal eigenvalues or all of its local maximal eigenvalues , for example in mri @xcite . on the other hand ,",
    "when the order or the dimension of a tensor grows larger , the optimization problem will become large - scale or huge - scale .",
    "therefore , we would like to investigate one simple and low - complexity method for finding tensor eigenpairs . in this paper",
    ", we consider an adaptive gradient method for solving the following nonlinear programming problem : @xmath29 where @xmath30 denote the unit sphere , i.e. , @xmath31 , @xmath32 denotes the euclidean norm . by some simple calculations",
    ", we can get its gradient and hessian , as follows @xcite : @xmath33 and its hessian is @xmath34 where @xmath35 , and @xmath36 is a matrix with its component as @xmath37    according to ( [ gradient ] ) , we can derive an important property for the nonlinear programming problem ( [ max - optimization - problem ] ) that the gradient @xmath38 is located in the tangent plane of @xmath30 at @xmath28 @xcite , since @xmath39    let @xmath40 is a constrained stationary point of ( [ max - optimization - problem ] ) , i.e. , that @xmath41 then we can claim that * every constrained stationary point * of ( [ max - optimization - problem ] ) * must be a stationary point * of @xmath42 since @xmath43 should be hold for all @xmath44 otherwise , if @xmath45 , we could choose @xmath46 , and then @xmath47 .",
    "suppose @xmath48 and denote @xmath49 .",
    "by @xmath50 , we know that any kkt point of ( [ max - optimization - problem ] ) will be a solution of the system of equations ( [ generalized eigenpair ] ) . before end of this section",
    ", we would like to state the following theorem and its proof is omitted .",
    "if the gradient @xmath38 at @xmath28 vanishes , then @xmath51 is a generalized eigenvalue and the vector @xmath28 is the associated generalized eigenvector .",
    "the rest of this paper is organized as follows . in the next section ,",
    "we introduce some existed gradient methods for tensor eigenvalue problems . in section 3 ,",
    "based on a curvilinear search scheme , we present a inexact gradient method .",
    "then , we establish its global convergence and linear convergence results under some suitable assumptions .",
    "section 4 provides numerical experiments to show the efficiency of our gradient method .",
    "finally , we have a conclusion section .",
    "the symmetric higher - order power method ( s - hopm ) was introduced by de lathauwer , de moor , and vandewalle @xcite for solving the following optimization problem : @xmath52 this problem is equivalent to finding the largest z - eigenvalue of @xmath2 @xcite and is related to finding the best symmetric rank-1 approximation of a symmetric tensor @xmath53}$ ] @xcite .    ' '' ''     + given a tensor @xmath54}$ ] , an initial unit iterate @xmath55 .",
    "let @xmath56 + * for * @xmath57 * do * + * 1 : * @xmath58 + * 2 : * @xmath59 + * 3 : * @xmath60 + * end for * +    ' '' ''    the cost per iteration of power method is @xmath61 , mainly for computing @xmath62 . let @xmath63 .",
    "set @xmath64 , @xmath65 , then the main iteration could be reformulated as @xmath66 , which is a projected gradient method with unit - stepsize .",
    "kofidis and regalia @xcite pointed out that s - hopm method can not guarantee to converge . by using convexity theory",
    ", they show that s - hopm method could be convergent for even - order tensors under the convexity assumption on @xmath67 . for general symmetric tensors , a shifted s - hopm ( ss - hopm ) method",
    "was proposed by kolda and mayo @xcite for computing z - eigenpairs .",
    "one shortcoming of ss - hopm is that its performance depended on choosing an appropriate shift .",
    "recently , kolda and mayo extended ss - hopm for computing generalized tensor eigenpairs , called geap method which is an adaptive , monotonically convergent , shifted power method for generalized tensor eigenpairs ( [ generalized eigenpair ] ) .",
    "they showed that geap method is much faster than the ss - hopm method due to its adaptive shift choice .    ' '' ''     + given tensors @xmath54}$ ] and @xmath68}$ ] , and an initial guess @xmath69 .",
    "let @xmath70 if we want to find the local maxima ; otherwise , let @xmath71 for seeking local minima",
    ". let @xmath72 be the tolerance on being positive / negative definite .",
    "+ * for * @xmath57 * do * + * 1 : * precompute @xmath73,@xmath74,@xmath62,@xmath75,@xmath76 , @xmath77 + * 2 : * @xmath78 + * 3 : * @xmath79 + * 4 : * @xmath80 + * 5 : * @xmath81 + * 6 : * @xmath59 + * end for * +    ' '' ''    in @xcite , ng , qi and zhou proposed a power method for finding the largest h - eigenvalue of irreducible nonnegative tensors .",
    "it is proved in @xcite that nqz s power method is convergent for primitive nonnegative tensors .",
    "further , zhang et .",
    "al @xcite established its linear convergence result and presented some updated version for essentially positive tensors and weakly positive tensors , respectively .",
    "however , similarly to the case of matrix , when the largest eigenvalue is close to the second dominant eigenvalue , the convergence of power method will be very slow @xcite .      in @xcite ,",
    "hao , cui and dai proposed a sequential subspace projection method ( sspm ) for z - eigenvalue of symmetric tensors . in each iteration of sspm method , one need to solve the following 2-dimensional subproblem :    @xmath82    let @xmath83 .",
    "the point in @xmath84 can be expressed as @xmath85    if @xmath86 , then sspm method will reduce to the power method . for simplicity ,",
    "if @xmath87 , the iterate can be expressed as @xmath88 with @xmath89 . in order to solve ( [ 2-dimenional subproblem ] ) ,",
    "one need to solve a equation like @xmath90 .",
    "for each iteration , the computational cost of sspm method is @xmath0 times than that of power method .",
    "as shown in @xcite , the main computational cost of sspm is the tensor - vector multiplications @xmath62 and @xmath91 ( defined in @xcite ) , which requires @xmath61 operations and @xmath92 operations , respectively .",
    "indicated by the idea in @xcite , we can present a gradient method with optimal stepsize for computing the generalized tensor eigenpairs problem ( [ max - optimization - problem ] ) .",
    "but we do nt want to present it here , since the computational cost per iterate is more expensive than power method . in this section ,",
    "we firstly present the following inexact gradient method , and then establish its global convergence and linear convergence results under some suitable assumptions .    ' '' ''     + given tensors @xmath54}$ ] and @xmath68}$ ] , an initial unit iterate @xmath55 , parameter @xmath93 .",
    "let @xmath94 be the tolerance .",
    "set k=0 ; calculate gradient @xmath95 . + * while * @xmath96 * do * + * 1 : * generate a stepsize @xmath97 such that @xmath98 satisfying @xmath99 * 2 : * update the iterate @xmath100 , calculate @xmath101 . +",
    "* end while * +    ' '' ''    it is clear that @xmath102 .",
    "moreover , by using ( [ xtgx ] ) , we can show the first - order gain per iterate is @xmath103 . since the spherical feasible region @xmath30 is compact",
    ", @xmath104 is positive and bounds away from zero , we can get that all the functions and gradients of the objective ( [ max - optimization - problem ] ) at feasible points are bounded @xcite , i.e. , there exists a constant @xmath105 such that for all @xmath106 , @xmath107    the following theorem indicates that the algorithm 3 is convergent to the kkt point of the problem ( [ max - optimization - problem ] ) .",
    "the constructive proof is motivated by the idea in @xcite .    [ the : globalconvergence ] suppose that the gradient @xmath38 is lipschitz continuous on the unit shpere .",
    "let @xmath108 is generated by algorithm 3 .",
    "then the inexact curvilinear search condition defined in ( [ linesearch ] ) is well - defined and there exists a positive constant @xmath109 such that @xmath110 furthermore , @xmath111    * proof . *",
    "firstly , we have @xmath112 furthermore , we can obtain @xmath113 let @xmath114 , using ( [ xtgx ] ) , we can derived that for any constant @xmath115 , there exists a positive scalar @xmath116 such that for all @xmath117 $ ] , @xmath118    considering the gap between @xmath119 and @xmath120 , similarly to the proof of lemma 4.2 in @xcite , we can get @xmath121dt\\\\ & + \\int_0^{\\alpha}[g(x_k(t))-g(x_k(0))]^tx'_k(0)dt\\\\ & ( \\mbox{using ( \\ref{eq : dxk } ) , ( \\ref{eq : derivefxa } ) , and lipschitz condition})\\\\ \\ge&\\alpha \\|g_k\\|^2-m\\int_0^{\\alpha}\\|x'_k(t)-x'_k(0)\\|dt - l\\|g_k\\|\\int_0^{\\alpha}\\|x_k(t)-x_k(0)\\|dt\\\\ \\ge&\\alpha \\|g_k\\|^2-m\\|g_k\\|^2\\int_0^{\\alpha } \\frac{t}{\\sqrt{1-t^2\\|g_k\\|^2}}dt - l\\|g_k\\|\\int_0^{\\alpha}\\sqrt{2 - 2\\sqrt{1-t^2\\|g_k\\|^2}}dt\\\\ \\end{split}\\ ] ] without loss of generality , assume that @xmath122 , then for @xmath123 , we have @xmath124 and @xmath125 so , we can obtain that @xmath126    set @xmath127 , we have @xmath128 for all @xmath129 $ ] .",
    "it follows from ( [ eq : gapfk1 ] ) that ( [ linesearch ] ) holds for all @xmath129 $ ] with @xmath130 .",
    "so , by using a backward strategy in curvilinear search , one can claim that the stepsize @xmath131 is bounded from below .",
    "that is to say , there exists a positive constant @xmath132 ( e.g. @xmath133 ) such that ( [ eq : boundofstepsize ] ) holds . as @xmath42 is bounded on the unit sphere , by ( [ eq : boundofstepsize ] ) , it is easy to prove that @xmath134",
    ". therefore , @xmath135 namely , the algorithm 3 is globally convergent .",
    "@xmath136 + in the rest of this section , we would like to establish the linear convergence rate of the algorithm 3 under the assumption of second order sufficient condition . for convenience , rewrite ( [ max - optimization - problem ] ) as @xmath137 the lagrangian function is @xmath138 and its gradient and hessian are @xmath139 at the kkt point @xmath140 , we have @xmath141 so , @xmath142 . by ( [ xtgx ] ) ,",
    "we know @xmath143 we can formulate the sufficient conditions for @xmath144 being a strict local maximizer of ( [ reformula - max - optimization - problem ] ) as : @xmath145 and @xmath146 where @xmath147 , and @xmath148 by theorem [ the : globalconvergence ] , the sequence @xmath149 generated by algorithm 3 is convergent to a kkt point @xmath144 with @xmath145 .",
    "if we further assume that the assumption of second sufficient condition ( [ eq:2ordercondition ] ) holds at @xmath144 , then the following linear convergence theorem could be established for algorithm 3 .",
    "let @xmath108 is generated by algorithm 3 .",
    "suppose that the gradient @xmath38 is lipschitz continuous on the unit sphere and the second sufficient condition ( [ eq:2ordercondition ] ) holds at the kkt point @xmath144 .",
    "then @xmath150 converges to @xmath151 linearly .",
    "* proof . * in order to show @xmath150 converges to @xmath151 linearly , we need to prove @xmath152 to end of this , we firstly deduce that @xmath153 .",
    "project @xmath154 on the orthogonal space of @xmath155 , we get @xmath156 it is clear that @xmath157 , since @xmath158 .",
    "notice that @xmath159 , we can obtain that @xmath160    by the taylor expansion , and the second order sufficient condition ( [ eq:2ordercondition ] ) , we have @xmath161    secondly , let us consider the following limitation :    @xmath162    let @xmath163 be the orthogonal complement of the vector @xmath164 . for any @xmath165",
    ", there exist @xmath166 such that @xmath167 suppose that the cholesky decomposition of the positive definite matrix @xmath168 is @xmath169 . denoting @xmath170 .",
    "notice that @xmath171 we can derive that : @xmath172 where @xmath173 is the smallest eigenvalue of the matrix @xmath168 .",
    "therefore , we have @xmath174 it follows from ( [ eq : boundofstepsize ] ) and ( [ eq : limofdeltaf ] ) that    @xmath175    the proof is completed .",
    "in this section , we present some numerical results to illustrate the effectiveness of the proposed adaptive gradient ( ag ) method , which was compared with the geap method  an adaptive shifted power method proposed by tamara g. kolda and jackson r. mayo @xcite .",
    "the experiments were done on a laptop with intel core 2 duo cpu with a 4 gb ram , using matlab r2014b , and the tensor toolbox @xcite .",
    "we set the parameter @xmath176 and initial guess of the stepsize @xmath177 in ( [ linesearch ] ) is generated by @xmath178 .",
    "if this initial guess can not satisfy the line search condition ( [ linesearch ] ) , then we truncate it as @xmath179 , and try it again .",
    "generally , once or twice is enough in our experiments .    in all numerical experiments ,",
    "we stop the iterates once @xmath180 .",
    "the maximum iterations is 500 .",
    "the following example is originally from @xcite and was used in evaluating the ss - hopm algorithm in @xcite and the geap algorithm in @xcite for computing z - eigenpairs .",
    "[ fig : ag - geap - z - eigen ] @xmath181{ag - geap - z - eigenvalue-5.1.eps } } \\end{array}\\ ] ]    _ example 1 _ ( kofidis and regalia @xcite ) .",
    "let @xmath182}$ ] be the symmetric tensor defined by @xmath183    to compare the convergence in terms of the number of iterations .",
    "figure 1 shows the results for computing z - eigenvalues of @xmath2 from * _ example 1 _ * , and the starting point is @xmath184 $ ] . in this case , both of adaptive gradient ( ag ) method and geap method can find the largest z - eigenvalue 0.8893 .",
    "ag method just need run 19 iterations in 0.168521 seconds while geap method need run 63 iterations in 0.469648 seconds .",
    "we used 1000 random starting guesses , each entry selected uniformly at random from the interval @xmath185 $ ] .",
    "for each set of experiments , the same set of random starts was used . for the largest eigenpair",
    ", we list the number of occurrences in the 1000 experiments .",
    "we also list the median number of iterations until convergence , the average error and the average run time in the 1000 experiments in tables 1 - 4 .",
    "as we can see from tables 1 - 4 , adaptive gradient ( ag ) method is much faster than geap method and could reach the largest eigenpair with a higher probability .",
    "+    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ fig : ag - geap - z - eigen ] @xmath181{ag - geap - h - eigenvalues-7.eps } } \\end{array}\\ ] ]    we used 100 random starting guesses to test ag method and geap method for computing h - eigenvalues of @xmath2 from * _ examples 5 - 7_*. for each set of experiments , the same set of random starts was used . for the largest eigenpair",
    ", we list the number of occurrences in the 100 experiments .",
    "we also list the median number of iterations until convergence , the average error and the average run time in the 100 experiments in tables 5 - 7 .",
    "as we can see from table 5 , geap method fails to stop in 500 iterations for all of the 100 test experiments for example 5 .",
    "but geap can slowly approach to the largest h - eigenvalue in 81 test experiments as shown in figure 3 .",
    "adaptive gradient ( ag ) method much faster than geap method and could reach the largest eigenpair with a higher probability .",
    "especially , for examples 6 and 7 , adaptive gradient ( ag ) method could find the largest h - eigenvalue in all of the 100 experiments .",
    "in this paper , we introduced an adaptive gradient ( ag ) method for generalized tensor eigenpairs , which could be viewed as an inexact version of the gradient method with optimal stepsize for finding z - eigenvalues of tensor in @xcite .",
    "what we have done is to use an inexact curvilinear search condition to replace the constraint on optimal stepsize .",
    "so , the computational complexity of ag method is much cheaper than sspm method in @xcite .",
    "global convergence and linear convergence rate are established for the ag method for computing generalized eigenpairs of symmetric tensor .",
    "some numerical experiments illustrated that the ag method is faster than geap and could reach the largest eigenpair with a higher probability .",
    "this work was supported in part by the national natural science foundation of china ( no.61262026 , 11571905 , 11501100 ) , ncet programm of the ministry of education ( ncet 13 - 0738 ) , jgzx programm of jiangxi province ( 20112bcb23027 ) , natural science foundation of jiangxi province ( 20132bab201026 ) , science and technology programm of jiangxi education committee ( ldjh12088 ) , program for innovative research team in university of henan province ( 14irtsthn023 ) .",
    "l. bloy , r. verma ,  on computing the underlying fiber directions from the diffusion orientation distribution function \" , in medical image computing and computer - assisted intervention miccai , vol . 2008 .",
    "springer : berlin / heidelberg , 2008 : 1 - 8 .          k. chang , k. pearson , t. zhang ,  primitivity , the convergence of the nqz method , and the largest eigenvalue for nonnegative tensors \" , _ siam journal on matrix analysis and applications _ 32 ( 2011 ) 806 - 819 .",
    "s. hu , g. li , l. qi , y. song ,  finding the maximum eigenvalue of essentially nonnegative symmetric tensors via sum of squares programming on the largest eigenvalue of a symmetric nonnegative tensor \" , _ journal of optimization theory and applications _ 2013 .",
    "l. sun , s. ji and j. ye , ",
    "hypergraph spectral learning for multi - label classification \" , in proceedings of the 14th acm sigkdd international conference on knowledge discovery and data mining , acm , 2008 , pp .",
    "668 - 676 ."
  ],
  "abstract_text": [
    "<S> high order tensor arises more and more often in signal processing , data analysis , higher - order statistics , as well as imaging sciences . in this paper , an adaptive gradient ( ag ) </S>",
    "<S> method is presented for generalized tensor eigenpairs . global convergence and linear convergence rate </S>",
    "<S> are established under some suitable conditions . </S>",
    "<S> numerical results are reported to illustrate the efficiency of the proposed method . comparing with the geap method , an adaptive shifted power method proposed by tamara g. kolda and jackson r. mayo [ siam j. matrix anal . </S>",
    "<S> appl . , 35 ( 2014 ) , pp . </S>",
    "<S> 1563 - 1581 ] , the ag method is much faster and could reach the largest eigenpair with a higher probability .    </S>",
    "<S> * keywords : * higher order tensor , eigenvalue , eigenvector , gradient method , power method . </S>"
  ]
}