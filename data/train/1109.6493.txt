{
  "article_text": [
    "in 1961 , james and stein proposed shrinkage estimates which outperform in mean square accuracy the maximum likelihood estimates in the problem of estimating the mean of a multidimensional gaussian vector with unit covariance matrix ( james and stein 1961 ) .",
    "this result stimulated the development of the theory of improved estimation for different regression models with dependent errors .",
    "fourdrinier and strawderman and fourdrinier and wells solved the problem of improved parametric estimation in the regression with dependent non - gaussian observations under spherically symmetric distributions of the noise ( fourdrinier and strawderman 1996 ; fourdrinier and wells 1994 ) .",
    "fourdrinier and pergamenshchikov and konev and pergamenchtchikov investigated the problem of improved estimation in nonparametric setting ( fourdrinier and pergamenshchikov 2007 ; konev and pergamenchtchikov 2010 ) .    in this paper , we consider the problem of improved parametric estimation for a continuous time regression with dependent non - gaussian noise of pulse type .",
    "the noise is specified by the ornstein  uhlenbeck process which is known to capture important distributional deviation from gaussianity and to be appropriate for modelling various dependence structures ( barndorff - nielsen and shephard 2001 ) .",
    "consider a regression model satisfying the equation @xmath1 where @xmath2  ( the notation @xmath3 holds for transposition ) is the vector of unknown parameters from a compact set @xmath4 ;  @xmath5  are one - periodic @xmath6 functions , orthonormal in the space @xmath7 $ ] .",
    "the noise @xmath8  in is assumed to be a non - gaussian ornstein  uhlenbeck process obeying the following stochastic differential equation @xmath9 where @xmath10 ,  @xmath11  is a levy process which is the mixture @xmath12 of a standard brownian motion @xmath13  and a compound poisson process @xmath14  defined as @xmath15 where @xmath16  is a poisson process with the intensity @xmath17 ,  and @xmath18  is a sequence of i.i.d .",
    "gaussian random variables with parameters ( 0,1 ) .",
    "the noise parameters @xmath19  and @xmath20  are unknown .    the problem is to construct improved estimates for the unknown vector parameter @xmath21  on the basis of observations @xmath22 ,  which have higher precision as compared with the least squares estimates ( lse ) .",
    "it will be observed that the regression model is conditionally gaussian given the @xmath23-algebra@xmath24  generated by the poisson process . in section [ sec:3 ] it is shown that the problem of estimating parameter @xmath21  in can be reduced to that of estimating the mean in a conditionally gaussian distribution with a random covariance matrix depending on the unknown nuisance parameters .",
    "this enables one to construct a shrinkage estimates for unknown parameters @xmath25  in .",
    "the main result is given in theorem [ th.sec:3.1 ] which claims that the proposed estimate has less risk than the lse .",
    "in section [ sec:2 ] we propose a special modification of james  stein procedure for solving the problem of estimating the mean in a conditionally gaussian distribution .",
    "this procedure allows one to control the mean square accuracy of estimates .",
    "it is shown ( theorem [ th.sec:2.1 ] ) that this estimate has less mean square risk than the usual lse .",
    "the rest of the paper is organized as follows . in section [ sec:4 ]",
    "we apply theorem [ th.sec:2.1 ] to the problem of parameter estimation in a discrete time regression under a gaussian autoregressive noise with unknown parameters .",
    "appendix contains some technical results .",
    "in section [ sec:3 ] , we will shown that the initial problem of estimating the parameters @xmath25  in model reduces to the following one .",
    "suppose that the observation @xmath26  is a @xmath27-dimensional random vector which obeys the equation @xmath28 where @xmath21  is an unknown constant vector parameter from some compact set @xmath4 ,  @xmath29  is a conditionally gaussian random vector with zero mean and the covariance matrix @xmath30 ,  i.e. @xmath31 ,  where @xmath32  is some fixed @xmath23-algebra .",
    "the problem is to estimate @xmath21 . consider a shrinkage estimate for @xmath21  of the form @xmath33 where @xmath34  is a positive constant which will be specified later .",
    "the choice of this estimate is motivated by the need to control the quadratic risk @xmath35 in the case of conditionally gaussian model .",
    "it will be observed that such control can not be provided by the ordinary james  stein estimate ( obtained from by the change of @xmath36 to @xmath37 ) .    in order to get an explicit upper bound for the quadratic risk of estimate we impose some conditions on the random covariance matrix @xmath30 .",
    "assume that    @xmath38  _ there exists a positive constant @xmath39,such that the minimal eigenvalue of matrix @xmath30  satisfies the inequality _",
    "@xmath40    @xmath41  _ the maximal eigenvalue of the matrix @xmath30  is bounded on some compact set @xmath4  from above in the sense that _",
    "@xmath42 _ where @xmath43  is a known positive constant . _",
    "further we will introduce some notation .",
    "let denote the difference of the risks of estimate and lse @xmath44as @xmath45 we will need also the following constant @xmath46 where @xmath47,@xmath48    [ th.sec:2.1 ]    let the noise @xmath29  in have a conditionally gaussian distribution @xmath49  and its covariance matrix @xmath30  satisfy conditions @xmath50with some compact set @xmath4 .",
    "then the estimator with @xmath51dominates the lse @xmath52  for any @xmath53 ,  i.e. @xmath54 ^ 2.\\ ] ]    first we will find the lower bound for the random variable @xmath55 .",
    "[ le.sec:2.2 ] under the conditions of theorem 2.1 @xmath56    the proof of lemma is given in the appendix .    in order to obtain the upper bound for @xmath57  we will adjust the argument in the proof of stein s lemma ( james and stein 1961 ) to the model with a random covariance matrix .",
    "we represent the risks of lse and of as @xmath58 r(\\theta^{*},\\theta)=r(\\hat{\\theta},\\theta)+\\e_{\\theta}[\\e((g(y)-1)^{2}\\|y\\|^{2}|{{\\cal g}})]\\\\[2 mm ] + 2\\sum_{j=1}^{p}\\e_{\\theta}[\\e((g(y)-1)y_{j}(y_{j}-\\theta_j)|{{\\cal g}})],\\end{gathered}\\ ] ] where @xmath59 .    denoting @xmath60  and applying the conditional density of distribution of a vector @xmath26  with respect to @xmath23-algebra",
    "@xmath32 @xmath61 one gets @xmath62    making the change of variable @xmath63  and assuming @xmath64 ,  one finds that @xmath65 where @xmath66  denotes the @xmath67-th  element of matrix @xmath68 .",
    "these quantities can be written as @xmath69    thus , the risk for an estimator takes the form @xmath70|_{u = y}\\right).\\end{gathered}\\ ] ]    therefore , one has @xmath71 where @xmath72 this implies that @xmath73    since @xmath74 ,  one comes to the inequality @xmath75 from here , it follows that @xmath76 taking into account the condition @xmath38  and the lemma  [ le.sec:2.2 ] , one obtains @xmath77 minimizing the function @xmath78  with respect to @xmath34 ,  we come to the desired result , i.e. @xmath79 ^ 2.\\ ] ]    hence theorem  [ th.sec:2.1 ] .",
    "[ co.sec:2.3 ] let in the noise @xmath80  with the positive definite non random covariance matrix @xmath81  and @xmath82",
    ".  then the estimator with @xmath83  dominates the lse for any @xmath53  and compact set @xmath4,i.e .",
    "@xmath54 ^ 2.\\ ] ]    note that if @xmath84  then @xmath85 ^ 2.\\ ] ]    [ co.sec:2.4 ] if @xmath86 and @xmath87  in model then the risk of estimate is given by the formula @xmath88 ^ 2=:r_p.\\ ] ]    by applying the stirling s formula for the gamma function @xmath89 one can check that @xmath90  as @xmath91.the behavior of the risk for small values of @xmath27is shown in fig.1 .",
    "it will be observed that in this case the risk of the james  stein estimate @xmath92  remains constant for all @xmath93 ,  i.e. @xmath94 and the risk of the lse @xmath95  is equal to @xmath27  and tends to infinity as @xmath91 .    at @xmath87 .  , scaledwidth=90.0% ]",
    "in this section we use the estimate to a non - gaussian continuous time regression model to construct an improved estimate of the unknown vector parameter @xmath21 .  to this end",
    "we reduce first the initial continuous time regression model to a discrete time model of the form with a conditionally gaussian noise .    a commonly used estimator of an unknown vector @xmath21  in model on the basis of observations @xmath96  is the lse @xmath97  with the components @xmath98 from here and , one has @xmath99 where @xmath100  is the random vector with the coordinates @xmath101 note that the vector @xmath100  has a conditionally gaussian distribution with a zero mean and conditional covariance matrix @xmath102  with the elements @xmath103    thus the initial problem of estimating parameter @xmath21  in can be reduced to the that of estimating parameter @xmath21  in a conditionally gaussian regression model .",
    "[ th.sec:3.1 ] let the regression model be given by the equations  , @xmath104 .",
    "then , for any @xmath105  and @xmath53 ,  the estimator of @xmath21@xmath106 dominates the lse @xmath95:@xmath107 ^ 2.\\ ] ]    to prove this theorem one can apply theorem [ th.sec:2.1 ] . to this end it suffices to check conditions @xmath38,@xmath41  on the matrix @xmath108 .",
    "the verification of conditions @xmath38  and @xmath41  is given in the appendix .",
    "in this section we consider the problem of improved estimating the unknown mean of a multivariate normal distribution when the dispersion matrix is unknown and depends on some nuisance parameters .",
    "the models of autoregressive type are widely used in time series analysis ( anderson 1994 ; brockwell and davis 1991 ) .",
    "let the noise @xmath109  in be described by a gaussian autoregression process @xmath110 where @xmath111 ,  @xmath112  and @xmath113  are independent gaussian ( 0,1 ) random variables .",
    "assume that the parameter @xmath114  in is unknown and belongs to interval @xmath115 $ ] ,  where @xmath116  is known number .",
    "it is easy to check that the covariance of the noise @xmath29  has the form    @xmath117       a & 1 & \\ldots & a^{p-2 } \\\\[2 mm ]       &   \\ddots & &    \\\\[2 mm ] a^{p-1 } & a^{p-2 } & \\ldots & 1 \\end{array } \\right)\\ ] ]    [ pr.sec:4.1 ] let the noise @xmath29 in be specified by equation with @xmath118 $ ] .",
    "then , for any @xmath119 ,  the lse is dominated by the estimate @xmath120 in the sense that @xmath121    we note that @xmath122 .",
    "now we will estimate of the maximal eigenvalue of matrix @xmath123 .  from the definition @xmath124 one",
    "has @xmath125 applying the cauchy ",
    "bunyakovskii inequality yields @xmath126 thus , @xmath127 by applying theorem [ th.sec:2.1 ] we come to the assertion of proposition [ pr.sec:4.1 ] .",
    "from , one has @xmath128 taking the repeated conditional expectation and noting that the random vector @xmath29  is conditionally gaussian with zero mean , one gets @xmath129 making the change of variable @xmath130  and applying the estimation @xmath131,we find @xmath132            [ [ the - verification - of - the - conditions - bfc_1and - bfc_2on - the - matrix - v_ncal - g . ] ] the verification of the conditions @xmath38  and @xmath41  on the matrix @xmath108 .",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    now we establish some properties of a stochastic integral @xmath136 with respect to the process .",
    "we will need some notations .",
    "let us denote @xmath137 where @xmath138 is @xmath6 function integrated on any finite interval .",
    "we introduce also the following transformation @xmath139 of square integrable @xmath6 functions @xmath138 and @xmath140 . here",
    "@xmath141            [ pr.sec:5.2 ]",
    "let @xmath138 and @xmath140 be bounded left - continuous @xmath151 functions measurable with respect to @xmath152 ( the product @xmath23 algebra created by @xmath153 and @xmath32 ) .",
    "then @xmath154 and @xmath155    by the ito formula one has @xmath156 \\nonumber & + \\varrho^{2}_{{\\mathchoice{2}{2}{\\lower.25ex\\hbox{$\\scriptstyle2 $ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle2$}}}}\\ , \\sum_{{\\mathchoice{l\\ge 1}{l\\ge 1}{\\lower.25ex\\hbox{$\\scriptstylel\\ge 1 $ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel\\ge 1$}}}}\\,f(t_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$}}}})\\,g(t_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$}}}})\\,y^{2}_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$ } } } } \\chi_{{\\mathchoice{\\{t_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$}}}}\\le t\\}}{\\{t_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$}}}}\\le t\\}}{\\lower.25ex\\hbox{$\\scriptstyle\\{t_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$}}}}\\le t\\}$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle\\{t_{{\\mathchoice{l}{l}{\\lower.25ex\\hbox{$\\scriptstylel$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstylel$}}}}\\le t\\}$ } } } } \\\\[2 mm ] \\nonumber & + \\int^{t}_{{\\mathchoice{0}{0}{\\lower.25ex\\hbox{$\\scriptstyle0 $ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle0$}}}}\\ , ( f(s)i_{{\\mathchoice{s-}{s-}{\\lower.25ex\\hbox{$\\scriptstyles-$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyles-$}}}}(g ) + g(s)i_{{\\mathchoice{s-}{s-}{\\lower.25ex\\hbox{$\\scriptstyles-$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyles-$}}}}(f)))\\d u_{{\\mathchoice{s}{s}{\\lower.25ex\\hbox{$\\scriptstyles$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyles$}}}}\\,.\\end{aligned}\\ ] ] taking the conditional expectation @xmath157 , on the set @xmath158 , yields @xmath159 & + a \\int^{t}_{{\\mathchoice{0}{0}{\\lower.25ex\\hbox{$\\scriptstyle0 $ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyle0$}}}}\\ , \\left ( f(s ) \\e(i_{{\\mathchoice{s}{s}{\\lower.25ex\\hbox{$\\scriptstyles$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyles$}}}}(g ) \\xi_{{\\mathchoice{s}{s}{\\lower.25ex\\hbox{$\\scriptstyles$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyles$}}}}|{{\\cal g } } ) + g(s ) \\e(i_{{\\mathchoice{s}{s}{\\lower.25ex\\hbox{$\\scriptstyles$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyles$}}}}(f)\\xi_{{\\mathchoice{s}{s}{\\lower.25ex\\hbox{$\\scriptstyles$ } } { \\lower0.25ex\\hbox{$\\scriptscriptstyles$}}}}|{{\\cal g } } ) \\right ) \\d s\\,.\\end{aligned}\\ ] ]          notice that by one can the matrix @xmath108  present as @xmath165 where @xmath166  is non random matrix with elements @xmath167 and @xmath168  is a random matrix with elements @xmath169 .",
    "\\end{gathered}\\ ] ] this implies that @xmath170 therefore @xmath171 and we come to the assertion of lemma [ lem.sec:5.4 ] .",
    "fourdrinier d. , strawderman w.e .",
    "( 1996 ) a paradox concerning shrinkage estimators : should a known scale parameter be replaced by an estimated value in the shrinkage factor ?",
    "j. multivariate anal . 59(2):109 - 140      james w. , stein c. ( 1961 ) estimation with quadratic loss , in : proceedings of the fourth berkeley symposium on mathematics statistics and probability , vol . 1 , university of california press , berkeley , p.361 - 380"
  ],
  "abstract_text": [
    "<S> the paper considers the problem of estimating the parameters in a continuous time regression model with a non - gaussian noise of pulse type . </S>",
    "<S> the noise is specified by the ornstein  uhlenbeck process driven by the mixture of a brownian motion and a compound poisson process . improved estimates for the unknown regression parameters , based on a special modification of the james  stein procedure with smaller quadratic risk than the usual least squares estimates , are proposed . </S>",
    "<S> the developed estimation scheme is applied for the improved parameter estimation in the discrete time regression with the autoregressive noise depending on unknown nuisance parameters .    </S>",
    "<S> * keywords * non - gaussian parametric regression @xmath0 improved estimates @xmath0 pulse noise @xmath0 ornstein  uhlenbeck process @xmath0 quadratic risk @xmath0 autoregressive noise    * mathematics subject classification ( 2010 ) * 62h12 - 62m10 </S>"
  ]
}