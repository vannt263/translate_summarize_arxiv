{
  "article_text": [
    "estimation of covariance matrices and their inverses ( a.k.a .",
    "precision matrices ) is of fundamental importance in almost every aspect of statistics , ranging from the principal component analysis [ @xcite ] , graphical modeling [ @xcite ] , classification based on the linear or quadratic discriminant analysis [ @xcite ] , and real - world applications such as portfolio selection [ @xcite ] and wireless communication [ @xcite ] .",
    "suppose we have @xmath0 temporally observed @xmath1-dimensional vectors @xmath2 , with @xmath3 having mean zero and covariance matrix @xmath4 whose dimension is @xmath5 .",
    "our goal is to estimate the covariance matrices @xmath6 and their inverses @xmath7 based on the data matrix @xmath8 . in the classical situation where @xmath1 is fixed , @xmath9 and @xmath3 are mean zero independent and identically distributed ( i.i.d . )",
    "random vectors , it is well known that the sample covariance matrix @xmath10 is a consistent and well behaved estimator of @xmath11 , and @xmath12 is a natural and good estimator of @xmath13 . see @xcite for a detailed account",
    ". however , when the dimensionality @xmath1 grows with @xmath0 , random matrix theory asserts that @xmath14 is no longer a consistent estimate of @xmath11 in the sense that its eigenvalues do not converge to those of @xmath11 ; see , for example , the marenko ",
    "pastur law [ @xcite ] or the tracy  widom law [ @xcite ] .",
    "moreover , it is clear that @xmath15 is not defined when @xmath14 is not invertible in the high - dimensional case with @xmath16 .    during the last decade",
    ", various special cases of the above covariance matrix estimation problem have been studied . in most of the previous papers",
    "it is assumed that the vectors @xmath17 are i.i.d . and thus the covariance matrix @xmath18 is time - invariant .",
    "see , for example , @xcite ( @xcite ) , @xcite , @xcite ( @xcite ) , where consistency and rates of convergence are established for various regularized ( banded , tapered or thresholded ) estimates of covariance matrices and their inverses . as an alternative regularized estimate for sparse precision matrix",
    ", one can adopt the lasso - type entry - wise 1-norm penalized likelihood approach ; see @xcite .",
    "other estimates include the cholesky decomposition based method [ @xcite ] , neighborhood selection for sparse graphical models [ @xcite ] , regularized likelihood approach [ @xcite ] and the sparse matrix transform [ @xcite ] .",
    "@xcite considered covariance matrix estimation for univariate stationary processes .",
    "the assumption that @xmath19 are i.i.d .",
    "is quite restrictive for situations that involve temporally observed data . in @xcite and @xcite",
    "the authors considered time - varying gaussian graphical models where the sampling distribution can change smoothly over time .",
    "however , they assume that the underlying random vectors are independent . using nonparametric smoothing techniques",
    ", they estimate the time - vary covariance matrices in terms of covariance matrix functions .",
    "their asymptotic theory critically depends on the _ independence _ assumption .",
    "the importance of estimating covariance matrices for dependent and nonstationary processes has been increasingly seen across a wide variety of research areas . in modeling spatial  temporal data ,",
    "@xcite proposed quadratic nonlinear dynamic models to accommodate the interactions between the processes which are useful for characterizing dynamic processes in geophysics [ @xcite ] . @xcite",
    "considered non - gaussian clutter and noise processes in space ",
    "time adaptive processing , where the space ",
    "time covariance matrix is important for detecting airborne moving targets in the nonstationary clutter environment [ @xcite ] . in finance ,",
    "@xcite considered multivariate stochastic volatility models parametrized by time - varying covariance matrices with heavy tails and correlated errors .",
    "@xcite investigated the markowitz portfolio selection problem for optimal returns of a large number of stocks with hidden and heterogeneous gaussian graphical model structures .",
    "in essence , those real - world problems pose a number of challenges : ( i ) nonlinear dynamics of data generating systems , ( ii )  temporally dependent and nonstationary observations , ( iii ) high - dimensionality of the parameter space and ( iv ) non - gaussian distributions .",
    "therefore , the combination of more flexible nonlinear and nonstationary components in the models and regularized covariance matrix estimation are essential to perform related statistical inference .",
    "in contrast to the longstanding progresses and extensive research that have been made in terms of heuristics and methodology , theoretical work on estimation of covariance matrices based on high - dimensional time series data is largely untouched . in this paper",
    "we shall substantially relax the i.i.d .",
    "assumption by establishing an asymptotic theory that can have a wide range of applicability .",
    "we shall deal with the estimation of covariance and precision matrices for high - dimensional stationary processes in sections  [ sec : stationary ] and [ sec : precision_statproc ] , respectively .",
    "section  [ sec : stationary ] provides a rate of convergence for the thresholded estimator , and section  [ sec : precision_statproc ] concerns the graphical lasso estimator for precision matrices . for locally stationary processes , an important class of nonstationary processes , we shall study in section  [ sec : covariancematrixestiamtion_nonstatproc ] the estimation of time - varying covariance and precision matrices .",
    "this generalization allows us to consider time - varying covariance and precision matrix estimation under temporal dependence ; hence our results significantly extend previous ones by @xcite and @xcite .",
    "furthermore , by assuming a mild moment condition on the underlying processes , we can relax the multivariate gaussian assumption that was imposed in @xcite and  @xcite [ and also by @xcite ( @xcite ) in the i.i.d . setting ] .",
    "specifically , we shall show that , thresholding on the kernel smoothed sample covariance matrices , estimators based on the localized graphical lasso procedure are consistent estimators for time - varying covariance and precision matrices .    to deal with temporal dependence , we shall use the functional dependence measure of @xcite . with the latter , we are able to obtain explicit rates of convergence for the thresholded covariance matrix estimates and illustrate how the dependence affects the rates . in particular ,",
    "we show that , based on the moment condition of the underlying process , there exists a threshold value .",
    "if the dependence of the process does not exceed that threshold , then the rates of convergence will be the same as those obtained under independence .",
    "on the other hand , if the dependence is stronger , then the rates of convergence will depend on the dependence .",
    "this phase transition phenomenon is of independent interest .",
    "we now introduce some notation .",
    "we shall use @xmath20 to denote positive constants whose values may differ from place to place .",
    "those constants are independent of the sample size @xmath0 and the dimension @xmath1 .",
    "for some quantities @xmath21 and @xmath22 , which may depend on @xmath0 and @xmath1 , we write @xmath23 if @xmath24 holds for some constant @xmath25 that is independent of @xmath0 and @xmath1 and @xmath26 if there exists a constant @xmath27 such that @xmath28 .",
    "we use @xmath29 and @xmath30 . for a vector @xmath31 , we write @xmath32 and for a matrix @xmath11 , @xmath33 , @xmath34 , @xmath35 and @xmath36 . for a random vector @xmath37 , write @xmath38 , @xmath39 , if @xmath40^{1/a } < \\infty$ ] .",
    "in this section we shall assume that @xmath41 is a @xmath1-dimensional stationary process of the form @xmath42 where @xmath43 is an @xmath44-valued measurable function , @xmath45 is a shift process and @xmath46 are i.i.d",
    ". random vectors .",
    "following , we can view @xmath47 and @xmath3 as the input and the output of a physical system , respectively , and @xmath48 is the transform representing the underlying physical mechanism . the framework ( [ eq : casual ] ) is quite general .",
    "some examples are presented in @xcite .",
    "it can also be conveniently extended to locally stationary processes ; see section  [ sec : covariancematrixestiamtion_nonstatproc ] .    write @xmath49 and @xmath50 , the data matrix observed at time points @xmath51 . here",
    "we shall consider estimation of the @xmath5 covariance matrix @xmath52 based on the realization @xmath53 , while section  [ sec : precision_statproc ] concerns estimation of its inverse .",
    "we consider frobenius and spectral norm convergence of the _ thresholded estimator _",
    "@xmath54 where @xmath55 is the sample covariance matrix defined in ( [ eq : samplecovariancematrix ] ) ; see @xcite .",
    "it was shown in the latter paper that , with a properly chosen @xmath56 , @xmath57 is a consistent estimator when @xmath58 [ see ( [ eqn : strongell_qball ] ) ] and @xmath41 are i.i.d .",
    "sub - gaussian .",
    "our rates of convergence depend on the dependence of the process and the moment conditions , which can be quite mild .",
    "our main theoretical result is given in section  [ subsec : covmat_stat_nonlinproc ] . to obtain a consistent estimate for @xmath11",
    ", we need to impose regularization conditions .",
    "in particular , we shall assume that @xmath11 is weakly dependent in that most of its entries are small , by providing a bound on the tail empirical process of covariances .",
    "some examples are provided in section  [ sec : stprocess ] with applications to spatial  temporal processes .      to establish a convergence theory for covariance matrix estimates",
    ", we shall use the functional dependence measure of @xcite . recall that @xmath59 , @xmath60 , where @xmath61 is the @xmath62th coordinate projection of the @xmath44-valued measurable function @xmath63 . for @xmath64 , the functional dependence measure of @xmath65",
    "is defined by @xmath66 where @xmath67 , @xmath68 and @xmath69 is such that @xmath70 , @xmath71 , are i.i.d . in other words",
    ", @xmath72 is a coupled version of @xmath65 with @xmath73 in the latter replaced by an i.i.d .",
    "copy @xmath69 . in @xcite functional dependence measures",
    "were computed for some commonly used linear and nonlinear stationary processes .",
    "we shall assume that the short - range dependence ( srd ) condition holds , @xmath74 if ( [ eq : srdtail ] ) fails , the process @xmath75 may exhibit long - range dependence , and the asymptotic behavior can be quite different .",
    "a nonlinear process satisfying ( [ eq : srdtail ] ) is given in example  [ exmp : nonlin_stat ] , while example  [ exmp : linstat ] concerns linear processes .",
    "theorems [ thmm : f08122 ] and [ thmm : spectral ] provide rates of convergence under the normalized frobenius norm and the spectral norm for the thresholded estimate @xmath76 , respectively .",
    "the constants  @xmath25 therein are independent of @xmath0 , @xmath56 and @xmath1 .",
    "[ thmm : f08122 ] assume that there exist @xmath77 , @xmath78 , @xmath79 and a positive constant @xmath80 such that @xmath81 and @xmath82 for all @xmath83 .",
    "let @xmath84 and @xmath85 .",
    "define @xmath86 and @xmath87 then there exists a constant @xmath25 , independent of @xmath56 , @xmath0 and @xmath1 , such that @xmath88    [ rem : a031046 ] if @xmath89 , elementary calculations indicate that@xmath90 . hence the right - hand side of ( [ eq : a807148 ] ) is @xmath91 .",
    "the term @xmath92 is needed if @xmath93 .    by theorem  [ thmm : f08122 ] , if @xmath94 , then @xmath95 .",
    "better convergence rates can be achieved if @xmath96 by choosing a larger threshold ; see cases ( i)(iii ) in corollary  [ cor : m140825 ] below .",
    "[ cor : m140825 ] assume that the conditions of theorem  [ thmm : f08122 ] hold .",
    "let @xmath97 ; let @xmath98 if @xmath99 and @xmath100 if @xmath101 .",
    "let @xmath102 be the unique solution to the equation @xmath103 .",
    "if @xmath104 , then there is a fixed constant @xmath105 such that @xmath106 for all @xmath107 $ ] .",
    "if @xmath108 and @xmath109 , let @xmath110 solve @xmath111 , then @xmath112 . if @xmath108 , @xmath113 and @xmath114 , let @xmath115 be the solution to the equation @xmath116 over the interval @xmath117 $ ] , then @xmath118 . if @xmath119 , then the right - hand side of ( [ eq : a807148 ] ) is @xmath120 for all @xmath121 and @xmath122 .",
    "theorem  [ thmm : f08122 ] and corollary  [ cor : m140825 ] describe how the frobenius rate of convergence depends on the sample size @xmath0 , the dimension @xmath1 , the smallness measure quantified by the function @xmath123 and the heaviness of tails ( moment conditions ) and strength of dependence which are characterized by @xmath124 and @xmath125 , respectively .",
    "it suggests the interesting dichotomy phenomenon : under the weaker dependence condition @xmath126 , the thresholded estimate @xmath76 has the same convergence rates as those obtained under independence .",
    "however , the convergence becomes slower under stronger temporal dependence with @xmath127 .",
    "the phase transition occurring at @xmath128 .",
    "the theorem also provides information about the optimal threshold @xmath56 , as revealed in its proof .",
    "the optimal threshold balances the bias or the smallness function @xmath123 , the tail function @xmath129 and the variance component which roughly corresponds to the gaussian - type function @xmath130 . under different conditions ,",
    "the optimal threshold assumes different forms ; see corollaries  [ cor : f_stationary ] and [ cor : fexp ] .",
    "proof of theorem  [ thmm : f08122 ] we first assume @xmath89 .",
    "note that @xmath131 ^ 2 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & \\le & 2\\sum _ { j , k=1}^p { \\mathbb{e}}\\bigl(w_{j k}^2 \\bigr ) + 2b(u/2),\\end{aligned}\\ ] ] where @xmath132 and @xmath133 let events @xmath134 , @xmath135 and @xmath136 we shall consider these three terms separately . write @xmath137 .",
    "_ case _ i : on the event @xmath138 , since the functional dependence measure for the product process @xmath139 , @xmath140 , satisfies @xmath141 \\\\[-8pt ] \\nonumber & \\le & \\mu(\\theta_{i , 2 q , j } + \\theta_{i , 2 q , k}),\\end{aligned}\\ ] ] it follows from the moment inequality theorem  2.1 in @xcite that @xmath142 where @xmath143 is a constant only depending on @xmath124 .",
    "let @xmath144 .",
    "then @xmath145    _ case _ ii : on the event @xmath146 , we observe that @xmath147 \\nonumber\\\\ & \\le & 2{\\mathbb{e}}\\bigl[\\xi_{jk}^2 \\mathbb{i}\\bigl(|\\sigma_{jk}|\\ge u/2,|\\hat{\\sigma}_{jk}| <",
    "u\\bigr ) \\bigr ] \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } + 2 { \\mathbb{e}}\\bigl[\\hat{\\sigma}_{jk}^2 \\mathbb{i}\\bigl(| \\sigma_{jk}|\\ge u/2,|\\hat{\\sigma}_{jk}| <",
    "u\\bigr)\\bigr ] \\\\ & \\le & 2\\bigl(c_1n^{-1}+u^2\\bigr)\\mathbb{i}\\bigl(| \\sigma_{jk}|\\ge u/2\\bigr).\\nonumber\\end{aligned}\\ ] ]    _ case _ iii : on the event @xmath148 , let @xmath149 } \\nonumber\\\\ & = & { \\mathbb{e}}\\bigl[\\xi_{jk}^2 \\mathbb{i}\\bigl(|\\hat { \\sigma}_{jk}| \\ge u , |\\sigma_{jk}|",
    "< u/2 , | \\xi_{jk}|>u/2\\bigr)\\bigr ] \\\\ & \\le & { \\mathbb{e}}\\bigl[\\xi_{jk}^2 \\mathbb{i}\\bigl(|\\xi_{jk}|>u/2\\bigr)\\bigr].\\nonumber\\end{aligned}\\ ] ] then @xmath150 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & \\le & 2 \\delta_{jk } + 2\\sigma_{jk}^2\\mathbb{i}\\bigl(|\\sigma_{jk}| < u/2\\bigr).\\end{aligned}\\ ] ] since the functional dependence measure for the product process @xmath151 satisfies ( [ eq : julypdm ] ) , under the decay condition @xmath152 , @xmath126 , we have by theorem  2(ii ) in  @xcite that @xmath153 holds for all @xmath154 .",
    "using integration by parts , we obtain @xmath155 & = & v^2{\\mathbb{p}}\\bigl(|\\xi_{j k}|>v\\bigr ) + \\int_{v^2}^\\infty { \\mathbb{p}}\\bigl(|\\xi_{j k}| > \\sqrt{w}\\bigr ) \\,dw \\nonumber\\\\ & \\le & v^2 \\biggl[\\frac { c_2n}{(n v)^q } + c_3e^{-c_4n v^2 } \\biggr ] \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } + \\int_{v^2}^\\infty \\biggl [ \\frac{c_2n}{(n\\sqrt{w})^q } + c_3e^{-c_4nw } \\biggr ] \\,dw \\\\ & = & c_5 n^{1-q } v^{2-q } + c_3 \\bigl((c_4n)^{-1}+v^2\\bigr)e^{-c_4n v^2},\\nonumber\\end{aligned}\\ ] ] where @xmath156 . by ( [ eq : f241256 ] ) , we also have @xmath157 \\le\\min\\biggl(\\|\\xi_{jk } \\|^2_2 , { { \\|\\xi_{jk}\\|^q_q } \\over{v^{q-2 } } } \\biggr ) \\lesssim\\min\\biggl ( { 1\\over n } , { v^{2-q } \\over n^{q/2}}\\biggr).\\ ] ]    combining cases i , ii and iii , by ( [ eqn : bias_part_general ] ) and ( [ eq : july131])([eq : f24112 ] ) , we have @xmath158 \\\\[-8pt ] \\nonumber & & { } + \\min \\biggl({1\\over n } , { u^{2-q } \\over n^{q/2 } } , h(u ) + g(c u ) \\biggr ) = : m_0(u),\\end{aligned}\\ ] ] where @xmath159 , and the constant of @xmath160 is independent of @xmath1 , @xmath56 and @xmath0 .",
    "if @xmath161 , then ( [ eq : a807148 ] ) clearly follows from the inequality @xmath162 .",
    "if @xmath163 , we also have ( [ eq : a807148 ] ) since in this case @xmath164 and the right - hand side of ( [ eq : a807148 ] ) has the same order of magnitude @xmath165 .    the other cases with @xmath166 and @xmath167 can be similarly handled .",
    "the key difference is that , instead of ( [ eq : july14840 ] ) , we shall now use the following versions of nagaev inequalities which can allow stronger dependence : @xmath168 see also @xcite .",
    "proof of corollary  [ cor : m140825 ] let @xmath169 be the term on the right - hand side of  ( [ eq : a807148 ] ) .",
    "we now minimize @xmath169 over @xmath170 .",
    "let @xmath171 then @xmath172 . clearly , @xmath173 .",
    "let @xmath174 .",
    "if @xmath101 , then for some constant @xmath143 , we have @xmath175 .",
    "also we have @xmath176 . hence @xmath177.\\ ] ]",
    "note that the equation @xmath103 has a unique solution @xmath178 on @xmath179 , and the function @xmath180 $ ] is decreasing over @xmath161 .",
    "a plot of the function in ( [ eqn : fmax ] ) is given in figure  [ fig : m15608](a ) .",
    "let @xmath181 be the minimizer of the right - hand side of  ( [ eqn : fmax ] )",
    ". for ( i ) , assume @xmath182 for some @xmath183",
    ". then @xmath181 satisfies @xmath184 , which implies @xmath185 , and hence ( i ) follows .",
    "note that ( ii ) follows in view of @xmath186 and @xmath187 .",
    "similarly we have ( iii ) since @xmath188 . the last case ( iv )",
    "is straightforward since @xmath189 for all @xmath190 .    if @xmath191 , assume @xmath192 , and then ( [ eqn : fmax ] ) still holds with @xmath193 therein replaced by @xmath194 .",
    "a plot for this case is given in figure  [ fig : m15608](b ) .",
    "note that @xmath195 if @xmath196 .",
    "then we can similarly have ( i)(iv ) .    from the proof of corollary  [ cor : m140825 ] ,",
    "if @xmath197 , in case  ( iii ) , we can actually have the following dichotomy : let @xmath198 be the solution to the equation @xmath199 .",
    "then the minimizer @xmath200 $ ] if @xmath201 and @xmath202 $ ] if @xmath203 . for @xmath101 , ( [ eqn : fmax ] )  indicates that @xmath204 is not needed ; see also remark  [ rem : a031046 ]",
    ".    using the argument for theorem  [ thmm : f08122 ] , we can similarly establish a spectral norm convergence rate .",
    "@xcite considered the special setting with i.i.d .",
    "our theorem  [ thmm : spectral ] is a significant improvement by relaxing the independence assumption , by obtaining a sharper rate and by presenting a moment bound . as in theorem",
    "[ thmm : f08122 ] , we also have the phase transition at @xmath205 .",
    "note that @xcite only provides a probabilistic bound .",
    "[ thmm : spectral ] let the moment and the dependence conditions in theorem  [ thmm : f08122 ] be satisfied .",
    "let @xmath206 and @xmath207 , for @xmath208 , and @xmath127 , respectively .",
    "define @xmath209 and @xmath210 .",
    "then there exists a constant  @xmath25 , independent of @xmath56 , @xmath0 and @xmath1 , such that @xmath211 \\\\[-8pt ] \\nonumber   & & { } + p \\min \\biggl ( { 1 \\over\\sqrt{n } } , { u^{1-q/2 } \\over n^{q/4 } } , \\bigl(h(u ) + g(cu)\\bigr)^{1/2 } \\biggr),\\end{aligned}\\ ] ] where @xmath212 and @xmath213 are given in ( [ eqn : d_u ] ) and ( [ eqn : d_u1 ] ) , respectively .",
    "we shall only deal with the weaker dependent case with @xmath126 .",
    "the other cases similarly follow .",
    "recall the proof of theorem  [ thmm : f08122 ] for @xmath214 , @xmath215 and @xmath216 .",
    "let matrices @xmath217 .",
    "similar to ( [ eqn : bias_part_general ] ) , let @xmath218",
    ". then @xmath219 let @xmath220 and @xmath221 , where @xmath222 is a large constant . since @xmath223 , by ( [ eq : july14840 ] ) , @xmath224 \\,d z \\\\ & \\lesssim & m^2_\\ast(u/2),\\nonumber\\end{aligned}\\ ] ] where @xmath225 .",
    "similar to ( [ eq : july132 ] ) , since @xmath226 on @xmath146 , @xmath227 using the idea of ( [ eq : july647 ] ) , we have @xmath228 \\\\[-8pt ] \\nonumber & \\le&2 \\sum_{j , k } \\xi_{jk}^2 { \\mathbb{i}}\\bigl(|\\xi_{jk}| > u/2\\bigr ) + 2 b_\\ast^2(u/2).\\end{aligned}\\ ] ] by ( [ eqn : delta_bound])([eq : f24112 ] ) and ( [ eqn : f06524])([eqn : spectral_a3 ] ) , we have ( [ eq : f06626 ] ) since @xmath229 .",
    "the bounds in theorems [ thmm : f08122 ] and [ thmm : spectral ] depend on the smallness measures , the moment order @xmath124 , the dependence parameter @xmath125 , the dimension @xmath1 and the sample size  @xmath0 .",
    "the problem of selecting optimal thresholds is highly nontrivial .",
    "our numeric experiments show that the cross - validation based method has a reasonably good performance . however , we are unable to provide a theoretical justification of the latter method , and pose it as an open problem .",
    "[ exmp : nonlin_stat ] we consider the nonlinear process  @xmath41 defined by the iterated random function @xmath230 where @xmath46 s are i.i.d .",
    "innovations , and @xmath231 is an @xmath44-valued and jointly measurable function , which satisfies the following two conditions : ( i ) there exists some @xmath232 such that @xmath233 and ( ii ) @xmath234 then , it can be shown that @xmath3 defined in ( [ eq : iteratedrandomfun_stat ] ) has a stationary ergodic distribution @xmath235 and , in addition , @xmath41 has the _ geometric moment contraction _ ( gmc ) property ; see @xcite for details .",
    "therefore , we have @xmath236 and theorems [ thmm : f08122 ] and [ thmm : spectral ] with @xmath101 and @xmath237 can be applied .",
    "[ exmp : linstat ] an important special class of ( [ eq : casual ] ) is the vector linear process @xmath238 where @xmath239 , are @xmath5 matrices , and @xmath46 are i.i.d .",
    "mean zero random vectors with finite covariance matrix @xmath240 .",
    "then @xmath3 exists almost surely with covariance matrix @xmath241 if the latter converges .",
    "assume that the innovation vector @xmath242 , where @xmath243 are i.i.d .",
    "with mean zero , variance @xmath244 and @xmath245 , @xmath77 , and the coefficient matrices @xmath246 satisfy @xmath247 , @xmath248 . by rosenthal s inequality , the functional dependence measure @xmath249 , and hence by ( [ eq : srdtail ] ) @xmath250 . by theorem",
    "[ thmm : f08122 ] , the normalized frobenius norm of the thresholded estimator has a convergence rate established in ( [ eq : a807148 ] ) with @xmath251 , @xmath252 and @xmath253 .",
    "note that our moment condition relaxes the commonly assumed sub - gaussian condition in previous literature  [ @xcite ] .",
    "for the vector ar(1 ) process @xmath254 , where @xmath255 is a real matrix with spectral norm @xmath256 , it is of form ( [ eq : linearprocess ] ) with @xmath257 , and the functional dependence measure @xmath258 .",
    "the rates of convergence established in ( [ eq : a807148 ] ) hold with @xmath101 and @xmath259",
    ".      the thresholded estimate @xmath57 may not be positive definite .",
    "here we shall propose a simple modification that is positive definite and has the same rate of convergence .",
    "let @xmath260 be its eigen - decomposition , where @xmath261 is an orthonormal matrix and @xmath262 is a diagonal matrix . for @xmath154 , consider @xmath263 where @xmath264 and @xmath265 is the rate of convergence in ( [ eq : a807148 ] ) .",
    "let @xmath266 be the diagonal elements of @xmath267 .",
    "then we have by theorem  [ thmm : f08122 ] that @xmath268 , and consequently @xmath269 if @xmath270 , since @xmath271 , we have @xmath272",
    ". then @xmath273 .",
    "note that the eigenvalues of @xmath274 are bounded below by @xmath275 , and thus it is positive definite . in practice",
    "we suggest using @xmath276 .",
    "the same positive - definization procedure also applies to the spectral norm and its rate can be similarly preserved .      in this section",
    "we shall compute the smallness measure @xmath123 for certain class of covariance matrices , so that theorem  [ thmm : f08122 ] is applicable .",
    "we consider some widely used spatial processes .",
    "let the vectors @xmath277 , @xmath278 , be observed at sites @xmath279 .",
    "assume that the covariance function between @xmath280 and @xmath281 satisfies @xmath282 where @xmath283 is a distance between sites @xmath284 and @xmath285 , and @xmath286 is a real - valued function with @xmath287 and @xmath288 .",
    "for example , we can choose @xmath289 as the euclidean distance between sites @xmath290 and @xmath291 .",
    "assume that , as @xmath292 , @xmath293 where the index @xmath294 characterizes the spatial dependence , or @xmath295 where @xmath296 is the characteristic length - scale , and @xmath297 condition ( [ eq : a811028 ] ) outlines the geometry of the sites @xmath298 , and @xmath299 can be roughly interpreted as the correlation dimension .",
    "it holds with @xmath300 if @xmath284 are @xmath301 points in a disk or a square , and @xmath302 if @xmath303 . the rational quadratic covariance function [ @xcite ] is an example of ( [ eq : a811032 ] ) , and it is widely used in spatial statistics , @xmath304 where @xmath305 is the smoothness parameter and @xmath306 is the length scale parameter .",
    "we now provide a bound for @xmath123 . by ( [ eq : a811032 ] ) and ( [ eq : a811028 ] ) , as @xmath307 , the covariance tail empirical process function @xmath308 for some constant @xmath309 independent of @xmath0 , @xmath56 and @xmath1 .",
    "if @xmath310 , then @xmath311 in the strong spatial dependence case with @xmath312 , we have @xmath313 to this end , it suffices to prove this relation with @xmath314 .",
    ". then @xmath316    class ( [ eq : a821010 ] ) allows the @xmath317-exponential covariance function with @xmath318 , and some matrn covariance functions [ @xcite ] that are widely used in spatial statistics . with ( [ eq : a811028 ] ) , following the argument in ( [ eqn : a02849 ] ) , we can similarly have @xmath319    corollary  [ cor : f_stationary ] of theorem  [ thmm : f08122 ] concerns covariance matrices satisfying ( [ eqn : a02736 ] ) . slightly more generally , we introduce a decay condition on the tail empirical process of covariances .",
    "note that ( [ eqn : a02736 ] ) is a special case of ( [ eqn : sparsity_def ] ) with @xmath320 and @xmath321 . for ( [ eqn : rationalquad_covfuns ] ) with possibly large length scale parameter @xmath296",
    ", we can let @xmath322 .",
    "similarly , corollary  [ cor : fexp ] can be applied to @xmath286 satisfying ( [ eq : a821010 ] ) and the class @xmath323 defined in ( [ eqn : expsparsity ] ) , with @xmath324 and @xmath325 .",
    "[ def : csc ] for @xmath326 , let @xmath327 , @xmath328 , be the collection of @xmath5 covariance matrices @xmath329 such that @xmath330 and , for all @xmath331 , @xmath332 and @xmath323 , @xmath333 , be the collection of @xmath329 with @xmath330 and @xmath334    [ cor : f_stationary ] assume ( [ eqn : sparsity_def ] ) .",
    "let conditions in theorem  [ thmm : f08122 ] be satisfied and @xmath126 .",
    "let @xmath335 . if @xmath336 , then for @xmath337 , @xmath338 .",
    "if @xmath339 and @xmath340 , let @xmath341 , then @xmath342 . if @xmath339 and @xmath343 then the equation @xmath344 has solution @xmath345^{1/2}$ ] and @xmath346 . if @xmath347 , then the right - hand side of ( [ eq : a807148 ] ) is @xmath348 for @xmath349 and @xmath350 .    in particular , if @xmath351 , @xmath352 , then we have or if @xmath353 , @xmath354 , @xmath355 or @xmath356 holds , respectively .",
    "similar to ( [ eqn : a02849 ] ) , we have @xmath357 .",
    "note that the solution @xmath102 to the equation @xmath103 satisfies @xmath358 .",
    "then by corollary  [ cor : m140825 ] , ( i)(iv ) follow from elementary but tedious manipulations .",
    "details are omitted .    by taking into consideration of @xmath359 in the tail empirical process condition ( [ eqn : sparsity_def ] )",
    ", we can view @xmath360 as the _ effective dimension_. corollary  [ cor : f_stationary ] describes the choice of the optimal threshold @xmath56 at different regions of the effective dimension @xmath360 and the sample size @xmath0 .",
    "case ( i ) [ resp . , ( iv ) ] corresponds to the overly large ( resp .",
    ", small ) dimension case .",
    "the most interesting cases are ( ii ) and ( iii ) . for the former , the tail function @xmath212 determines the rate of convergence with a larger threshold  @xmath110 , while for the latter with moderately large dimension the gaussian - type function @xmath213 leads to the optimal threshold @xmath361 .    [ cor : fexp ] assume ( [ eqn : expsparsity ] ) .",
    "let conditions in theorem  [ thmm : f08122 ] be satisfied with @xmath101 and @xmath362 .  if @xmath363 , then for @xmath337 , @xmath364 .",
    "if @xmath339 and @xmath365 , let @xmath366 and @xmath367 . then @xmath342 . if @xmath368 , let @xmath369 . if @xmath370 let @xmath371 . then @xmath372 . if @xmath373 in is less than @xmath374 , then the right - hand side of  ( [ eq : a807148 ] ) is @xmath120 for @xmath349 and @xmath350 .",
    "we have @xmath375",
    ". we shall again apply corollary  [ cor : m140825 ] . case ( i )",
    "is straightforward . for ( ii )",
    ", we note that the equation @xmath376 has solution @xmath377 . under ( iii ) , the equation @xmath378 has solution @xmath379 .",
    "corollaries [ cor : f_stationary ] and [ cor : fexp ] deal with the weaker dependence case with @xmath101 . by corollary  [ cor : m140825 ]",
    ", similar versions can be obtained for @xmath380 .",
    "details are omitted .",
    "@c@     for the uniform random sites model on the @xmath381 ^ 2 $ ] square with three different scale length parameters : @xmath382 and @xmath383.,title=\"fig : \" ] +    [ cols=\"^,^ \" , ]     @c@     for the uniform random sites model on the @xmath381 ^ 2 $ ] square with three different scale length parameters : @xmath382 and @xmath383.,title=\"fig : \" ] +    as a numeric example , we use the rational quadratic covariances ( [ eqn : rationalquad_covfuns ] ) to illustrate the rates of convergence given in theorem  [ thmm : f08122 ] and corollary  [ cor : m140825 ] .",
    "we choose @xmath384 , @xmath385 , @xmath386 , the moment @xmath387 and consider the weaker ( @xmath388 ) and stronger ( @xmath389 ) temporal dependence cases .",
    "we first generate @xmath1 random sites uniformly distributed on the @xmath390 square ; see figure  [ fig : rational_quad_cov_mat](a ) .",
    "figure  [ fig : rational_quad_cov_mat](b ) , [ fig : rational_quad_cov_mat](c ) and [ fig : rational_quad_cov_mat](d ) show three @xmath391 rational quadratic covariance matrices ( [ eqn : rationalquad_covfuns ] ) respectively with length scale parameters @xmath382 and @xmath383 , which correspond to different levels of spatial dependence .",
    "next , we calculate the terms in corollary  [ cor : m140825 ] for the thresholded estimator . the results are shown in figure  [ fig : m15608 ] . in the plots",
    ", @xmath178 is the solution of @xmath392 .",
    "note that , @xmath181 , the minimizer of @xmath393 $ ] over @xmath161 , can be either @xmath110 or @xmath115 .",
    "we observe that when the spatial dependence decreases , that is , the covariance matrix @xmath11 has more small entries [ e.g. , figure  [ fig : rational_quad_cov_mat](d ) ] , a larger threshold is needed to yield the optimal rate of convergence . when the temporal dependence increases ( i.e. , @xmath394 ) , a  larger threshold is needed and the rate of convergence is slower than the one in the weaker dependence case ( i.e. , @xmath395 ) .",
    "@c@    ) and stronger ( @xmath389 ) temporal dependence cases.,title=\"fig : \" ] +   + ) and stronger ( @xmath389 ) temporal dependence cases.,title=\"fig : \" ] +      we now compare ( [ eqn : sparsity_def ] ) with the commonly used sparsity condition defined in terms of the _ strong @xmath396-ball _ [ @xcite ] @xmath397 when @xmath398 , ( [ eqn : strongell_qball ] ) becomes @xmath399 , a sparsity condition in the rigid sense .",
    "we observe that condition ( [ eqn : sparsity_def ] ) defines a broader class of sparse covariance matrices in the sense that @xmath400 , which follows from @xmath401 hence corollary  [ cor : f_stationary ] generalizes the consistency result of @xmath76 in @xcite to the non - gaussian time series .",
    "note that our convergence is in @xmath402 norm , while the error bounds in previous work [ see , e.g. , @xcite ( @xcite ) ] are of probabilistic nature ; namely in the form @xmath403 is bounded with large probability under the strong @xmath396-ball conditions .",
    "the reverse inclusion @xmath404 may be false since the class @xmath405 specifies the uniform size of sums in matrix columns , whereas ( [ eqn : sparsity_def ] ) can be viewed as an overall smallness measure over all entries of the matrix . as an example , consider the covariance matrix @xmath406 where @xmath407 so that @xmath11 is positive - definite .",
    "then for any threshold level @xmath408 , @xmath409 and for any @xmath410$],@xmath411 . in both cases , we may choose @xmath412 .",
    "on the other hand , @xmath413 .",
    "so @xmath414 for any @xmath415 with @xmath416 .    with the strong @xmath396-ball and sub - gaussian conditions",
    ", @xcite showed that the minimax rate under the bregman divergence is @xmath417 . observing that the upper bounds in corollary  [ cor : f_stationary ] is established under the larger parameter space @xmath418 where @xmath419 and milder polynomial moments conditions",
    ", the lower bound of @xcite automatically becomes a lower bound in our setup .",
    "therefore , in the moderately high - dimensional situation with weaker temporal dependence , we can conclude that the frobenius norm bound in corollary  [ cor : f_stationary](iii ) is minimax rate optimal .",
    "[ cor : frisk_bound_minimax_stationary_covmat ] let @xmath420 . under the conditions in corollary",
    "[ cor : f_stationary ] and in addition assume @xmath421 for some @xmath422 .",
    "then @xmath423 where the inf is taken over all possible estimators based on the data @xmath424 .",
    "we next compare our theorem  [ thmm : spectral ] with the result in section  2.3 of @xcite , where the special class ( [ eqn : strongell_qball ] ) is considered . assuming @xmath425",
    ", they obtained the _",
    "probabilistic bound _",
    "@xmath426 and @xmath309 is a sufficiently large constant . as a natural requirement for consistency , we assume @xmath427 , namely @xmath428 . since @xmath429 , we have @xmath430 and @xmath431 .",
    "consider the weaker dependence case with @xmath101 .",
    "note that in ( [ eq : f06626 ] ) @xmath432 is nondecreasing , while all other three functions are nonincreasing .",
    "let @xmath433 , @xmath434 , @xmath435 be the solutions to the equations @xmath436 , @xmath437 , and @xmath438 , respectively ; let @xmath439 .",
    "for a sufficiently large constant @xmath440 , @xmath441 and hence the right - hand side of ( [ eq : f06626 ] ) is of order @xmath442 if @xmath443 .",
    "let @xmath444 and @xmath445 .",
    "note that @xmath446 if @xmath447 and @xmath448 if @xmath449 . in both cases we have by elementary calculations that @xmath450 .",
    "similarly , we have @xmath451 and @xmath452 .",
    "hence @xmath453 and our rate of convergence @xmath454 is sharper .",
    "based on theorem  [ thmm : spectral ] and the above discussion , we have :    [ cor : spectralf08 ] let the conditions in theorem  [ thmm : f08122 ] be satisfied and @xmath101 .",
    "let @xmath455 .",
    "assume @xmath456 , @xmath457 and @xmath458 , @xmath459 .",
    "let @xmath460 , @xmath461 , @xmath462 , @xmath463 and @xmath464 . if @xmath465 , then @xmath466 . if @xmath467 , then @xmath468 .",
    "as a straightforward estimate for precision matrices , one can invert the regularized covariance matrix estimates . however , this inversion procedure may cause the precision matrix estimate to lose sparsity .",
    "sparsity of the precision matrix @xmath469 has important statistical meaning because a zero entry in @xmath470 reflects the conditional independence when @xmath3 are multivariate gaussian . in the graphical model representation",
    ", @xmath471 indicates that there is a missing edge between node  @xmath472 and node @xmath62 .",
    "performance bounds for estimating @xmath13 under dependence is useful for statistical learning problems . for direct estimation of precision matrices that can preserve sparsity",
    ", one can adopt entry - wise 1-norm penalized likelihood approaches ; see @xcite , which we refer them as lasso - type precision matrix estimators .",
    "@xcite proposed a graphical lasso model and developed a computationally efficient and scalable algorithm for estimating large precision matrices .",
    "this 1-norm penalized multivariate gaussian likelihood approach was also considered by @xcite .",
    "consistency of the graphical lasso were studied in  @xcite .",
    "the precision matrix estimation procedure considered here is the graphical lasso model [ @xcite ] which minimizes the objective function @xmath473 where @xmath474 is the penalty to be determined later . in ( [ eq : graphicallasso ] ) @xmath475 means that @xmath476 is positive - definite .",
    "here we assume the maximum eigenvalue @xmath477 or equivalently the minimum eigenvalue of @xmath11 is larger than @xmath478 .",
    "note that we do not assume the minimum eigenvalue of @xmath13 is uniformly bounded below from zero . to introduce an asymptotic theory for the estimate @xmath479 , we recall ( [ eqn : d_u ] ) and ( [ eqn : d_u1 ] ) of theorem  [ thmm : f08122 ] for the definition of the functions @xmath212 and @xmath213 and also @xmath480 and @xmath253 .",
    "an analogue of the function @xmath481 in this context is @xmath482 recall corollary  [ cor : m140825 ] for @xmath483 .",
    "it is interesting and surprising to note that the structure of theorem  [ thmm : inv ] is very similar to that in theorem  [ thmm : f08122 ] .",
    "however , the main idea for the proof of theorem  [ thmm : inv ] seems quite different , and our key argument here is based on convex minimization .",
    "it is also interesting to note that our rate of convergence is expressed in terms of the @xmath484 norm ; see ( [ eq : a805948 ] ) , while in the previous literature probabilistic bounds are obtained ; see @xcite .",
    "the constant @xmath25 in theorem  [ thmm : inv ] can be the same as the one in theorem  [ thmm : f08122 ] .",
    "[ thmm : inv ] let the moment and the dependence conditions in theorem  [ thmm : f08122 ] be satisfied and @xmath485",
    ". then @xmath486 where @xmath25 is independent of @xmath487 and @xmath1 .",
    "let @xmath488 be the solution to the equation @xmath489 then @xmath490 .",
    "[ rem : a8091031 ] as an immediate consequence of theorem  [ thmm : inv ] , if the entries @xmath491 of the inverse matrix @xmath13 satisfy ( [ eqn : sparsity_def ] ) with @xmath492 , then we have by the argument in ( [ eqn : a02849 ] ) that @xmath493 . similarly , if @xmath491 satisfy ( [ eqn : expsparsity ] ) , then @xmath494",
    ". therefore corollaries [ cor : f_stationary ] and [ cor : fexp ] are still valid in the context of precision matrix estimation .",
    "proof of theorem [ thmm : inv ] using @xmath495 , we see that @xmath496 minimizes @xmath497 hence @xmath498 .",
    "let @xmath499 . by taylor s expansion , @xmath500 +",
    "\\lambda\\bigl(|\\omega+\\delta|_1-|\\omega|_1\\bigr ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } + { \\operatorname{vec}}(\\delta)^\\top \\biggl[\\int_0 ^ 1(1-v ) \\omega_v^{-1 } \\otimes\\omega_v^{-1 } \\,dv \\biggr]{\\operatorname{vec}}(\\delta),\\end{aligned}\\ ] ] where @xmath501 denotes the kronecker product . write @xmath502 , @xmath503 and @xmath504",
    ". let @xmath505 be the complement of @xmath506 .",
    "then @xmath507 where the matrix @xmath508 .",
    "assume @xmath509 .",
    "by  ( [ eq : a8051043 ] ) , @xmath510 using the arguments for theorem  1 in @xcite , we have by ( [ eq : uniformclass_omega ] ) that @xmath511{\\operatorname{vec}}(\\delta ) \\ge\\frac{1}{4}\\varepsilon_0 ^ 2| \\delta|_f^2,\\ ] ] and by letting the penalty @xmath485 that @xmath512 where , for a matrix @xmath11 , @xmath513 and @xmath514 . by the cauchy ",
    "schwarz inequality , @xmath515 , where @xmath516 . by ,",
    "@xmath517 since @xmath518 , there exists a deterministic constant @xmath309 such that @xmath519 then ( [ eq : a805948 ] ) follows from ( [ eq : a8071127 ] ) and by choosing @xmath56 to minimize the right - hand side of ( [ eq : a8051141 ] ) ; see the argument in ( [ eqn : fmax ] ) . the case with @xmath520",
    "can be similarly handled with special care ( [ eq : f24112 ] ) being taken into ( [ eq : a8071127 ] ) .",
    "@xcite studied the graphical lasso estimator with off - diagonal entries penalized by the 1-norm . for i.i.d .",
    "@xmath1-variate vectors with polynomial moment condition , they showed that if @xmath521 for some @xmath522 , where @xmath523 is the maximum degree in the gaussian graphical model , then @xmath524 where @xmath290 is the number of nonzero off - diagonal entries in @xmath13 . for @xmath525",
    ", we can choose @xmath526 .",
    "note that @xmath527 and thus @xmath528 . by remark  [ rem : a8091031 ] ,",
    "corollary  [ cor : f_stationary ] holds . under case ( ii )",
    ", ( iii ) ] , our rate of convergence is @xmath529 [ resp . , @xmath530 .",
    "elementary calculations show that both of our rates are of order @xmath531 .",
    "hence our bounds are much better than ( [ eqn : rate_ravikumar ] ) , the one obtained in @xcite .",
    "we now compare our results with the clime ( constrained @xmath532-minimization for inverse matrix estimation ) method , a non - lasso type estimator proposed in  @xcite , which is to @xmath533 @xcite showed that with @xmath0 i.i.d .",
    "@xmath1-variate observations , if @xmath534 , then the rate of convergence for the clime estimator under the normalized frobenius norm is @xmath535 , where @xmath536 is the upper bound for the matrix @xmath532-norm on the true precision matrix , and @xmath537 is in ( [ eqn : strongell_qball ] ) .",
    "we see that the rates of convergence under the normalized frobenius norm are the same for both papers .",
    "this rate of convergence is in general better than those obtained for the lasso - type estimators in the polynomial moment case [ @xcite ] .",
    "following @xcite , we can consider the slightly modified version of the graphical lasso : let @xmath538 and @xmath539 be the correlation matrix ; let @xmath540 and @xmath541 be their sample versions , respectively .",
    "let @xmath542 .",
    "we estimate @xmath543 by @xmath544 , where @xmath545 let @xmath546 : assuming the moment and dependence conditions in theorem  [ thmm : inv ] are satisfied and @xmath547 , and then @xmath548 holds if @xmath549 \\lesssim \\lambda$ ] .",
    "details of the derivation of ( [ eqn : spectral - rate - precision - mat ] ) is given in the supplementary material [ @xcite ] .",
    "if @xmath13 satisfies @xmath550 [ @xcite ] , we have @xmath551 with @xmath552 .",
    "simple calculations show that , if @xmath553 and @xmath554 , then for @xmath555 $ ] , we have by ( [ eqn : spectral - rate - precision - mat ] ) that @xmath556 , and it reduces to theorem  2 in @xcite .",
    "the time series processes considered in sections  [ sec : stationary ] and  [ sec : precision_statproc ] are stationary . in many situations the stationarity assumption can be violated , and the graphical structure is time - varying .",
    "one may actually be interested in how the covariance matrices and dependence structures vary with respect to time .",
    "@xcite and @xcite studied the estimation of covariance matrices for independent , locally stationary gaussian processes .",
    "both requirements can be quite restrictive in practice .",
    "here we shall consider nonstationary processes that can be both dependent and non - gaussian with mild moment conditions , thus having a substantially broader spectrum of applicability . to allow such nonstationary processes , following the framework in  @xcite",
    ", we shall consider locally stationary process @xmath557 where @xmath558 is a jointly measurable function such that the uniform stochastic lipschitz continuity holds : there exists @xmath559 for which @xmath560.\\ ] ] in examples [ ex : a08152][ex : a08154 ] below we present some popular models of locally stationary processes .",
    "let @xmath561 .",
    "the preceding condition ( [ eq : ulc ] ) suggests local stationarity in the sense that , for a fixed @xmath562 and bandwidth @xmath563 with @xmath564 , @xmath565 indicating that the process @xmath41 over the range @xmath566 can be approximated by the _ stationary _",
    "process @xmath567 .",
    "the locally stationarity property suggests that the data generating mechanism @xmath568 at time @xmath472 is close to the one @xmath569 at time @xmath570 if @xmath571 is small .",
    "hence the following covariance matrix function is continuous : @xmath572 the covariance matrix @xmath573 of @xmath3 can then be estimated by the approximate stationary process @xmath574 , by using the nadaraya  watson or other smoothing techniques . recall that in the stationary case the thresholded estimator is defined as @xmath575 , where @xmath576 is the sample covariance matrix given in ( [ eq : samplecovariancematrix ] ) . to estimate @xmath577 , we substitute @xmath14 by the kernel smoothed version @xmath578 write @xmath579 . in ( [ eq",
    ": kernelestimator_sigma ] ) , @xmath580 is a symmetric , nonnegative kernel with bounded support in @xmath581 $ ] and @xmath582 . as per convention",
    ", we assume that the bandwidth @xmath583 satisfies the natural condition : @xmath563 and @xmath584 . the thresholded covariance estimator for nonstationary processes",
    "is then defined as @xmath585 parallelizing theorem  [ thmm : f08122 ] , we give a general result for the thresholded estimator for time - varying covariance matrices of the nonstationary , nonlinear high - dimensional time series . as in ( [ eq : functiondependencemeasure_stat ] ) and ( [ eq : srdtail ] ) , we similarly define the functional dependence measure @xmath586 where @xmath587 .",
    "we also assume that ( [ eq : srdtail ] ) holds . for presentational simplicity let @xmath101 .",
    "let @xmath588 , @xmath589 , @xmath590 theorem  [ th : a08946 ] provides convergence rates for the thresholded covariance matrix function estimator @xmath591 .",
    "due to the nonstationarity , the bound is worse than the one in theorem  [ thmm : f08122 ] since we only use data in the local window @xmath592 $ ] .",
    "therefore , in the nonstationary case a larger sample size is needed for achieving the same level of estimation accuracy .",
    "[ th : a08946 ] assume @xmath593 } |\\sigma''_{jk}(t)| < \\infty$ ] and @xmath89 . under the moment and",
    "dependence conditions of theorem  [ thmm : f08122 ] , we have @xmath594 uniformly over @xmath595 $ ] , where @xmath25 is independent of @xmath596 and @xmath1 .",
    "let @xmath597 . under the condition on @xmath598",
    ", we have @xmath599 uniformly over @xmath600 and @xmath595 $ ] .",
    "hence @xmath601 .",
    "it remains to deal with @xmath602 . with a careful check of the proof of theorem  [ thmm : f08122 ] ,",
    "if we replace @xmath603 and @xmath604 therein by @xmath605 and @xmath606 , respectively , then we can have @xmath607 if the following nagaev inequality holds : @xmath608 the above inequality follows by applying the nonstationary nagaev inequality in section  4 in @xcite to the process @xmath609 , @xmath610 . note that the functional dependence measure of the latter process is bounded by @xmath611 ; see ( [ eq : julypdm ] ) and ( [ eq : a08141 ] ) .",
    "if in ( [ eq : kernelestimator_sigma ] ) we use the local linear weights [ @xcite ] , then it is easily seen based on the proof of theorem  [ th : a08946 ] that ( [ eq : a808148 ] ) holds over the whole interval @xmath612 $ ] , and the boundary effect is removed .",
    "this applies to the theorem  [ thmm : a08209 ] below as well .",
    "a similar result can be obtained for estimating evolutionary precision matrices of high - dimensional nonstationary processes @xmath613 where @xmath577 is given in ( [ eq : sigmat ] ) . as in the stationary case",
    ", we assume that @xmath614 satisfies ( [ eq : uniformclass_omega ] ) for all @xmath615 $ ] . the actual estimation procedure of @xmath614 based on the data @xmath616 is a variant of the graphical lasso estimator of @xmath13 , which minimizes the following objective function : @xmath617 where @xmath618 is the kernel smoothed sample covariance matrix given in ( [ eq : kernelestimator_sigma ] ) .",
    "the same minimization program is also used in  @xcite . as in ( [ eq : a08206 ] ) and ( [ eqn : a81049 ] ) , let @xmath619 as in ( [ eqn : a805928 ] ) , choose @xmath620 . for the estimator ( [ eq : a08204 ] ) , we have the following theorem .",
    "we omit the proof since it is similar to the one in theorems [ thmm : inv ] and [ th : a08946 ] .",
    "[ thmm : a08209 ] assume @xmath593 } |\\omega''_{jk}(t)| < \\infty$ ] and @xmath89 . under the moment and dependence conditions of theorem  [ thmm : f08122 ] , we have @xmath621 uniformly over @xmath595 $ ] , where @xmath25 is independent of @xmath596 and @xmath1 .",
    "let @xmath622 be the solution to the equation @xmath623 .",
    "then @xmath624 .",
    "[ ex : a08152 ] let @xmath625 be a stationary @xmath1-dimensional process with mean @xmath626 and identity covariance matrix .",
    "then the modulated process @xmath627 has covariance matrix @xmath573 .",
    "@xcite considered the special setting in which @xmath628 are i.i.d .",
    "standard gaussian vectors , and hence @xmath3 are independent .",
    "[ ex : a08153 ] consider the nonstationary linear process @xmath629 where @xmath630",
    "are continuous matrix functions",
    ". we can view ( [ eq : nonstationarylp ] ) as a time - varying version of ( [ eq : linearprocess ] ) , a framework also adopted in  @xcite .",
    "as in example  [ exmp : linstat ] , we assume a uniform version @xmath631    [ ex : a08154 ] we consider a nonstationary nonlinear example adapted from example  [ exmp : nonlin_stat ] .",
    "let the process @xmath41 be defined by the iterated random function @xmath632 where @xmath633 is an @xmath44-valued and jointly measurable function that may change over time .",
    "as in example  [ exmp : nonlin_stat ] , we assume @xmath634 satisfy : ( i ) there exists some @xmath232 such that @xmath635 ; ( ii ) @xmath636 then @xmath41 have the gmc property with @xmath637 . therefore , theorem  [ th : a08946 ] can be applied with @xmath101 and @xmath638 .",
    "we thank two anonymous referees , an associate editor and the editor for their helpful comments that have improved the paper ."
  ],
  "abstract_text": [
    "<S> we consider estimation of covariance matrices and their inverses ( a.k.a . </S>",
    "<S> precision matrices ) for high - dimensional stationary and locally stationary time series . in the latter case </S>",
    "<S> the covariance matrices evolve smoothly in time , thus forming a covariance matrix function . using the functional dependence measure of wu [ _ proc . </S>",
    "<S> natl . </S>",
    "<S> acad . </S>",
    "<S> sci . </S>",
    "<S> usa _ </S>",
    "<S> * 102 * ( 2005 ) 1415014154 ( electronic ) ] , we obtain the rate of convergence for the thresholded estimate and illustrate how the dependence affects the rate of convergence . </S>",
    "<S> asymptotic properties are also obtained for the precision matrix estimate which is based on the graphical lasso principle . </S>",
    "<S> our theory substantially generalizes earlier ones by allowing dependence , by allowing nonstationarity and by relaxing the associated moment conditions .    , </S>"
  ]
}