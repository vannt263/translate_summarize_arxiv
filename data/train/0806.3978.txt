{
  "article_text": [
    "information estimates are used to characterize the amount of information that a spike train contains about a stimulus @xcite .",
    "they are motivated by information theory @xcite and widely believed to estimate the mutual information ( or mutual information rate ) between stimulus and spike train response .",
    "they are frequently calculated using data from experiments where the stimulus and response are dynamic and time - varying hsu:2004,reich:2001,reinagel:2000,nirenberg:2001 .    for mutual information to be properly defined ,",
    "see for example @xcite , the stimulus and response must be considered random , and when the estimates are obtained from time - averages , they should also be stationary and ergodic . in practice",
    "these assumptions are usually tacit , and information estimates , such as the _ direct method _ proposed by @xcite , can be made without explicit consideration of the stimulus .",
    "this can lead to misinterpretation .",
    "the purpose of this note is to show that the direct method information estimate can be reinterpreted as the average divergence across time of the conditional response distribution from its overall mean ; in the absence of stationarity and ergodicity :    1 .",
    "information estimates do not necessarily estimate mutual information , but 2 .",
    "potentially useful interpretations can still be made by referring back to the time - varying divergence .",
    "although our results are specialized to the direct method with the plug - in entropy estimator , they should hold more generally regardless of the choice of entropy estimator .",
    "the fundamental issue concerns stationarity : methods that assume stationarity are unlikely to be appropriate when stationarity appears to be violated . in the non - stationary case , our second result should be of use , as would be other methods that explicitly consider the dynamic and non - stationary nature of the stimulus and response ; see for instance @xcite .",
    "we begin with a brief review of the direct method and plug - in entropy estimator .",
    "this is followed by results showing that the information estimate can be recast as a time - average .",
    "this characterization leads us to the interpretation that the information estimate is actually a measure of variability of the stimulus conditioned response distribution .",
    "this observation is first made in the finite number of trials case , and then formalized by a theorem describing the limiting behavior of the information estimate as the number of trials tends to infinity .",
    "following the theorem is discussion about the interpretation of the limit , and examples that illustrate the interpretation with a proposed graphical plot .",
    "in the direct method a time - varying stimulus is chosen by the experimenter and then repeatedly presented to a subject over multiple trials .",
    "the observed responses are conditioned by the same stimulus .",
    "two types of variation in the response are considered :    1 .   variation across time ( potentially related to the stimulus ) , and 2 .",
    "trial - to - trial variation .",
    "figure  1(a ) shows an example of data from such an experiment .",
    "the upper panel is a raster plot of the response of a field l neuron of an adult male zebra finch during synthetic song stimulation .",
    "the lower panel is a plot of the audio signal corresponding to the natural song .",
    "details of the experiment can be found in @xcite .",
    "let us consider the random process @xmath0 representing the value of the stimulus and response at time @xmath1 during trial @xmath2 .",
    "the response is made discrete by dividing time into bins of size @xmath3 and then considering _ words _ ( or patterns ) of spike counts formed within intervals ( overlapping or non - overlapping ) of @xmath4 adjacent time bins .",
    "the number of spikes that occur in each time bin become the letters in the words .",
    "@xmath5 corresponds to these words , and may belong to a countably infinite set ( because the number of spikes in a bin is theoretically unbounded ) . in the raster plot of figure  1(a )",
    "the time bin size is @xmath6 millisecond , and the vertical lines demarcate non - overlapping words of length @xmath7 time bins .    given the responses @xmath8 , the direct method considers two different entropies :    1 .",
    "the _ total entropy _",
    "@xmath9 of the response , and 2 .",
    "the local _ noise entropy _",
    "@xmath10 of the response at time @xmath11 .",
    "the total entropy is associated with the stimulus conditioned distribution of the response across all times and trials .",
    "the local noise entropy is associated with the stimulus conditioned distribution of the response at time @xmath11 across all trials .",
    "these quantities are calculated directly from the neural response , and the difference between the total entropy and the average ( over @xmath11 ) noise entropy is what @xcite call `` the information that the spike train provides about the stimulus . ''",
    "@xmath9 and @xmath10 depend implicitly on the length @xmath4 of the words .",
    "normalizing by @xmath4 and considering large @xmath4 leads to the total and local entropy rates that are defined to be @xmath12 and @xmath13 , respectively , when they exist .",
    "the direct method of strong:1998 prescribed an extrapolation for estimating these limits , however they do not necessarily exist when the stimulus and response process are non - stationary .",
    "when there is stationarity , estimation of entropy for large @xmath4 is potentially difficult , and extrapolation from a few small choices of @xmath4 can be suspect .",
    "since we are primarily interested in the non - stationary case , we do not address these issues and refer the reader to kennel:2005,gao:2006 for larger discussion on the stationary case .",
    "for notational simplicity , the dependence on @xmath4 will be suppressed in the remainder of the text .",
    "[ [ the - plug - in - entropy - estimate ] ] the plug - in entropy estimate + + + + + + + + + + + + + + + + + + + + + + + + + + + +    @xcite proposed estimating @xmath9 and @xmath10 by plug - in with the corresponding empirical distributions : @xmath14 and @xmath15 note that @xmath16 is also the average of @xmath17 across @xmath1 .",
    "so the direct method _ plug - in _ estimates of @xmath9 and @xmath10 are @xmath18 and @xmath19 respectively .",
    "the direct method plug - in information estimate is @xmath20",
    "the direct method information estimate is not only the difference of entropies shown in ( [ eq : infodifference ] ) , but also a time - average of divergences .",
    "the empirical distribution of response across all trials and times ( [ eq : phat ] ) is equal to the average of @xmath17 over time . that is @xmath21 and so @xmath22 \\log \\hat{p}(r ) \\\\      & = \\frac{1}{n } \\sum_{t=1}^n \\sum_r \\hat{p}_t(r ) \\log \\hat{p}_t(r ) - \\frac{1}{n } \\sum_{t=1}^n \\sum_r \\hat{p}_t(r ) \\log \\hat{p}(r ) \\\\",
    "\\label{eq : kldivergence }      & = \\frac{1}{n } \\sum_{t=1}^n \\sum_r \\hat{p}_t(r ) \\log \\frac{\\hat{p}_t(r)}{\\hat{p}(r)}.\\end{aligned}\\ ] ] the quantity that is averaged over time in ( [ eq : kldivergence ] ) is the kullback - leibler divergence between the empirical time @xmath11 response distribution @xmath17 and the average empirical response distribution @xmath16 .",
    "since the same stimulus is repeatedly presented to the subject , and there is no evolution in the response , over multiple trials , the following _ repeated trial assumption _ is natural :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ conditional on the stimulus @xmath23 the @xmath24 trials @xmath25 are independent and identically distributed ( i.i.d . ) .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    under this assumption @xmath26 are conditionally i.i.d . for each fixed @xmath11 and @xmath27 .",
    "furthermore , the law of large numbers guarantees that as the number of trials @xmath24 increases the empirical response distribution @xmath28 converges to its conditional expected value for each fixed @xmath11 and @xmath27 .",
    "thus @xmath28 and @xmath29 can be viewed as estimates of @xmath30 , defined by @xmath31 and @xmath32 , defined by @xmath33 respectively .",
    "@xmath34 is average response distribution across time @xmath1 conditional on the entire stimulus @xmath35 .",
    "so the quantity that is averaged over time in ( [ eq : kldivergence ] ) should be viewed as a plug - in estimate of the kullback - leibler divergence between @xmath36 and @xmath34 .",
    "we emphasize this by writing @xmath37 this observation will be formalized by the theorem of the next section .",
    "for now we summarize the above with a proposition .",
    "[ pro : klidentity ] the information estimate is the time - average @xmath38 .",
    "this decomposition of the information estimate is analogous to the decomposition of mutual information that @xcite call the `` specific surprise , '' while `` specific information '' is analogous to the alternative decomposition , @xmath39 .\\ ] ] an important difference is that here the stimulus itself is a function of time and the decompositions are given in terms of time - dependent quantities .",
    "it is possible that these quantities can reveal dynamic aspects of the stimulus and response relationship .",
    "this will be explored further in sections  [ sub : interpretation_in_non_stationary_case ] and [ subsec : visual ] .",
    "there are two directions in which the amount of observed response data can be increased : length of time @xmath40 , and number of trials @xmath24 .",
    "the information estimate is the average of @xmath41 over time , and may not necessarily converge as @xmath40 increases .",
    "this could be due to @xmath42 being non - stationary and/or highly dependent in time .",
    "even when convergence may occur , the presence of serial correlation in @xmath41 ( see the autocorrelation in panel ( b ) of figures 2 for example ) can make assessments of uncertainty in @xmath43 difficult .",
    "assuming that the stimulus and response process is stationary and not too dependent in time could guarantee convergence , but this could be unrealistic . on the other hand , the repeated trial assumption is appropriate if the same stimulus is repeatedly presented to the subject over multiple trials .",
    "it is also enough to guarantee that the information estimate converges as the number of trials @xmath24 increases .",
    "we prove the following theorem in the appendix .",
    "[ thm : information ] suppose that @xmath36 has finite entropy for all @xmath44 .",
    "then under the repeated trial assumption @xmath45          = \\frac{1}{n}\\sum_{t=1}^n d(p_t||\\bar{p})\\ ] ] with probability 1 , and in particular the following statements hold uniformly for @xmath1 with probability 1 :    1 .",
    "@xmath46 , 2 .",
    "@xmath47 , and 3 .",
    "@xmath48 for @xmath1 , [ thm : information : kl ]    where @xmath49 is the kullback - leibler divergence defined by , @xmath50 and @xmath51 is the entropy of the distribution @xmath52 , defined by @xmath53    note that if stationary and ergodicity do hold , then @xmath36 for @xmath1 is also stationary and ergodic and @xmath34 are stimulus conditional distributions , and hence random variables potentially depending on @xmath54 . ] .",
    "so its average , @xmath55 , is guaranteed by the ergodic theorem to converge pointwise to @xmath56 as @xmath57 . moreover ,",
    "if @xmath58 can only take on a finite number of values , then @xmath59 also converges to the marginal entropy @xmath60 of @xmath58 .",
    "likewise , the average of the conditional entropy @xmath61 also converges to the expected conditional entropy : @xmath62 .",
    "so in this case the information estimate does indeed estimate mutual information .    however , the primary consequence of the theorem is that , in the absence of stationarity and ergodicity , the information estimate @xmath43 does not necessarily estimate mutual information .",
    "the three particular statements show that the time - varying quantities @xmath63 $ ] and @xmath41 converge individually to the appropriate limits , and justify our assertion that the information estimate is a time - average of plug - in estimates of the corresponding time - varying quantities .",
    "thus , the information estimate can always be viewed as an estimate of the time - average of either @xmath49 or @xmath64$]stationary and ergodic or not .",
    "the kullback - leibler divergence @xmath49 has a simple interpretation : it measures the dissimilarity of the time @xmath11 response distribution @xmath36 from its overall average @xmath34 .",
    "so as a function of time , @xmath49 measures how the conditional response distribution varies across time , relative to its overall mean . this can be seen in a more familiar form by considering the leading term of the taylor expansion ,",
    "@xmath65 ^ 2}{\\bar{p}(r | s_1,\\ldots , s_n ) }      + \\cdots.\\ ] ] thus , its average is in this sense a measure of the average variability of the response distribution .",
    "it is , of course , possible that characteristics of the response are due to confounding factors rather than the stimulus .",
    "furthermore , the presence of additional noise in either process would weaken a measured relationship between stimulus and response , compared to its strength if the noise were eliminated . setting these concerns aside",
    ", the variation of the response distribution @xmath36 about its average provides information about the relationship between the stimulus and the response . in the stationary and ergodic case",
    ", this information may be averaged across time to obtain mutual information . in more general settings averaging across time may not provide a complete picture of the relationship between stimulus and response . instead , we suggest examining the time - varying @xmath49 directly , via graphical display as discussed next .",
    "the plug - in estimate @xmath41 is an obvious choice for estimating @xmath66 , but it turns out that estimating @xmath49 is akin to estimating entropy . since the trials are conditionally i.i.d .",
    ", the coverage adjustment method described in @xcite can be used to improve estimation of @xmath49 over the plug - in estimate .",
    "the appendix contains the details of this .",
    "figures 1 and 2 show the responses of the same field l neuron of an adult male zebra finch under two different stimulus conditions .",
    "details of the experiment and the statistics of the stimuli are described in @xcite .",
    "panel ( a ) of the figures shows the stimulus and response data . in figure  1",
    "the stimulus is synthetic and stationary by construction , while in figure  2 the stimulus is a natural song .",
    "panel ( b ) of the figures shows the coverage adjusted estimate of the divergence @xmath49 plotted as a function of time .",
    "95% confidence intervals were formed by bootstrapping entire trials , i.e. an entire trial is either included in or excluded from a bootstrap sample .    the information estimate going along with each divergence plot is the average of the solid curve representing the estimate of @xmath49 .",
    "it is equal to 0.77 bits ( per 10 millisecond word ) in figure  1(b ) and 0.76 bits ( per 10 millisecond word ) in figure  2(b ) . although the information estimates are nearly identical , the two plots are very different .    in the first case , the stimulus is stationary by construction and it appears that the time - varying divergence is too .",
    "its fluctuations appear to be roughly of the same scale across time , and its local mean is relatively stable .",
    "the average of the solid curve seems to be a fair summary .    in the second case",
    "the stimulus is a natural song .",
    "the isolated bursts of the time - varying divergence and relatively flat regions in figure  2(b ) suggest that the response process ( and the divergence ) is non - stationary and has strong serial correlations .",
    "the local mean of the divergence also varies strongly with time . summarizing @xmath67 by its",
    "time - average hides the time - dependent features of the plot .",
    "more interestingly , when the divergence plot is compared to the plot of the stimulus in figure  2 , there is a striking coincidence between the location of large isolated values of the estimated divergence and visual features of the stimulus waveform .",
    "they tend to coincide with the boundaries of the bursts in the stimulus signal .",
    "this suggests that the spike train may carry information about the onset / offset of bursts in the stimulus .",
    "we discussed this with the theunissen lab and they confirmed from their strf models that the cell in the example is an offset cell .",
    "it tends to fire at the offsets of song syllables  the bursts of energy in the stimulus waveform .",
    "they also suggested that a word length within the range of 3050 milliseconds is a better match to the length of correlations in the auditory system .",
    "we regenerated the plots for words of length @xmath68 ( not shown here ) and found that the isolated structures in the divergence plot became even more pronounced .",
    "estimates of mutual information , including the plug - in estimate , may be viewed as measures of the strength of the relationship between the response and the stimulus when the stimulus and response are jointly stationary and ergodic .",
    "many applications , however , use non - stationary or even deterministic stimuli , so that mutual information is no longer well defined . in such non - stationary cases",
    "do estimates of mutual information become meaningless ?",
    "we think not , but the purpose of this note has been to point out the delicacy of the situation , and to suggest a viable interpretation of information estimates , along with the divergence plot , in the non - stationary case .    in using stochastic processes to analyze data there is an implicit practical acknowledgment that assumptions can not be met precisely : the mathematical formalism is , after all , an abstraction imposed on the data ; the hope is simply that the variability displayed by the data is similar in relevant respects to that displayed by the presumptive stochastic process .",
    "the `` relevant respects '' involve the statistical properties deduced from the stochastic assumptions .",
    "the point we are trying to make is that highly non - stationary stimuli make statistical properties based on an assumption of stationarity highly suspect ; strictly speaking , they become void .    to be more concrete ,",
    "let us reconsider the snippet of natural song and response displayed in figure 2 .",
    "when we look at the less than 2 seconds of stimulus amplitude given there , the stimulus is not at all time - invariant : instead , the stimulus has a series of well - defined bursts followed by periods of quiescence . perhaps , on a very much longer time scale , the stimulus would look stationary . but a good stochastic model on a long time scale would likely require long - range dependence .",
    "indeed , it can be difficult to distinguish non - stationarity from long - range dependence @xcite , and the usual statistical properties of estimators are known to breakdown when long - range dependence is present @xcite .",
    "given a short interval of data , valid statistical inference under stationarity assumptions becomes highly problematic . to avoid these problems",
    "we have proposed the use of the divergence plot , and a recognition that the `` bits per second '' summary is no longer mutual information in the usual sense .",
    "instead we would say that the estimate of information measures magnitude of variation of the response as the stimulus varies , and that this is a useful assessment of the extent to which the stimulus affects the response as long as other factors that affect the response are themselves time - invariant .",
    "in other deterministic or non - stationary settings the argument for the relevance of an information estimate should be analogous . under stationarity and ergodicity , and",
    "indefinitely many trials , the stimulus sets that affect the response  whatever they are  will be repeatedly sampled , with appropriate probability , to determine the variability in the response distribution , with time - invariance in the response being guaranteed by the joint stationarity condition .",
    "this becomes part of the intuition behind mutual information . in the deterministic or non - stationary settings information estimates",
    "do not estimate mutual information , but they may remain intuitive assessments of strength of effect .",
    "the authors thank the theunissen lab at the university of california , berkeley for providing the data set and helpful discussion .",
    "they also thank an anonymous reviewer for comments that greatly improved the manuscript .",
    "v.  q.  vu was supported by a nsf vigre graduate fellowship and nidcd grant dc 007293 .",
    "b.  yu was supported by nsf grants dms-03036508 , dms-0605165 , dms-0426227 , aro grant w911nf-05 - 1 - 0104 , nsfc grant 60628102 , and a fellowship from the john simon guggenheim memorial foundation .",
    "this work began while kass was a miller institute visiting research professor at the university of california , berkeley .",
    "support from the miller institute is greatly appreciated .",
    "kass s work was also supported in part by nimh grant ro1-mh064537 - 04 .",
    "the main idea behind coverage adjustment is to adjust estimates for potentially unobserved values .",
    "this happens in two places : estimation of @xmath36 and estimation of @xmath70 . in the first case ,",
    "unobserved values affect the amount of weight that @xmath17 , defined in ( [ eq : pthat ] ) in the main text , places on observed values . in the second case",
    "unobserved values correspond to missing summands when plugging @xmath17 into the kullback - leibler divergence .",
    "@xcite gives a more thorough explanation of these ideas .",
    "let @xmath71 the sample coverage , or total @xmath36-probability of observed values @xmath27 , is estimated by @xmath72 defined by @xmath73 the number in the numerator of the fraction refers to the number of singletons ",
    "patterns that were observed only once across the @xmath24 trials at time @xmath11 .",
    "then the coverage adjusted estimate of @xmath36 is the following shrunken version of @xmath17 : @xmath74 @xmath34 is estimated by simply averaging @xmath75 : @xmath76 the coverage adjusted estimate of @xmath66 is obtained by plugging @xmath75 and @xmath77 into the kullback - leibler divergence , but with an additional weighting on the summands according to the inverse of the estimated probability that the summand is observed : @xmath78 the additional weighting is to correct for potentially missing summands .",
    "( this is also explained in detail in @xcite . )",
    "confidence intervals for @xmath49 can be obtained by bootstrap sampling entire trials , and applying @xmath79 to the bootstrap replicate data .",
    "[ lem : dct ] let @xmath80 and @xmath81 for @xmath82 be sequences of measurable , integrable functions defined on a measure space equipped with measure @xmath83 , and with pointwise limits @xmath84 and @xmath85 , respectively .",
    "suppose further that @xmath86 and @xmath87 .",
    "then @xmath88    by linearity of the integral , @xmath89 since @xmath90 , fatou s lemma implies @xmath91 the limit inferior on the inside of the right - hand integral is equal to @xmath92 by assumption .",
    "combining with the previous two displays and the assumption that @xmath93 gives @xmath94    [ pf : nonstationary ] the main statement of the theorem is implied by the three numbered statements together with proposition  [ pro : klidentity ] .",
    "we start with the second numbered statement . under the repeated trial assumption ,",
    "@xmath95 are conditionally i.i.d . given the stimulus @xmath23 .",
    "so corollary 1 of @xcite , can be applied to show that @xmath96 with probability 1 .",
    "this proves the first numbered statement .",
    "we will use lemma  [ lem : dct ] to prove the first numbered statement . for each @xmath27",
    "the law of large numbers asserts @xmath97 with probability 1 .",
    "so for each @xmath27 , @xmath98 and @xmath99 with probability 1 .",
    "fix a realization where ( [ eq : sumconverges][eq : fm ] ) hold and let @xmath100 and @xmath101 .\\ ] ] then for each @xmath27 @xmath102 and @xmath103      = : g(r).\\ ] ] the sequence @xmath80 is dominated by @xmath81 because @xmath104 \\\\      \\label{eq : logineq }      & \\leq -\\hat{p}_t(r ) [ \\log \\hat{p_t}(r ) - \\log n ] \\\\      & = g_m(r)\\end{aligned}\\ ] ] for all @xmath27 , where ( [ eq : logineq ] ) uses the fact that @xmath105 is an increasing function . from ( [ eq : sumconverges ] ) we also have that @xmath106 .",
    "clearly , @xmath80 and @xmath81 are summable .",
    "moreover @xmath107 by assumption .",
    "so @xmath108 and the conditions of lemma  [ lem : dct ] are satisfied .",
    "thus @xmath109 averaging over @xmath110 gives @xmath111 for realizations where ( [ eq : sumconverges][eq : fm ] ) hold .",
    "this proves the first numbered statement because the probability of all such realizations is 1 .    for the third numbered statement",
    "we begin with the expansions @xmath112 and @xmath113 the second numbered statement and ( [ eq : kldenom ] ) imply @xmath114 with probability 1 .",
    "this proves the third numbered statement .",
    "riccardo barbieri , loren  m. frank , david  p. nguyen , michael  c. quirk , victor solo , matthew  a. wilson , and emery  n. brown .",
    "dynamic analyses of information encoding in neural ensembles . , 16(2):277307 , 2004 .",
    "anne hsu , sarah m  n woolley , thane  e fremouw , and frdric  e theunissen .",
    "modulation power and phase spectrum of natural sounds enhance neural encoding performed by single auditory neurons .",
    ", 24(41):92019211 , 2004 ."
  ],
  "abstract_text": [
    "<S> information estimates such as the `` direct method '' of strong et al . </S>",
    "<S> ( 1998 ) sidestep the difficult problem of estimating the joint distribution of response and stimulus by instead estimating the difference between the marginal and conditional entropies of the response . </S>",
    "<S> while this is an effective estimation strategy , it tempts the practitioner to ignore the role of the stimulus and the meaning of mutual information . </S>",
    "<S> we show here that , as the number of trials increases indefinitely , the direct ( or `` plug - in '' ) estimate of marginal entropy converges ( with probability 1 ) to the entropy of the time - averaged conditional distribution of the response , and the direct estimate of the conditional entropy converges to the time - averaged entropy of the conditional distribution of the response . under joint stationarity and ergodicity of the response and stimulus </S>",
    "<S> , the difference of these quantities converges to the mutual information . </S>",
    "<S> when the stimulus is deterministic or non - stationary the direct estimate of information no longer estimates mutual information , which is no longer meaningful , but it remains a measure of variability of the response distribution across time . </S>"
  ]
}