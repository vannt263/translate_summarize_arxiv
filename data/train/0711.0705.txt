{
  "article_text": [
    "the compound channel consists of a set of channels indexed by @xmath1 with the same input and output alphabets but different conditional probabilities . in the setting of the compound channel only one actual channel @xmath2",
    "is used in all transmissions .",
    "the transmitter and the receiver know the family of channels but they have no prior knowledge of which channel is actually used .",
    "there is no distribution law on the family of channels and the communication has to be reliable for all channels in the family .",
    "blackwell et al .",
    "@xcite and independently wolfowitz @xcite showed that the capacity of a compound channel consisting of memoryless channels only , and without feedback , is given by @xmath3 where @xmath4 denotes the input distribution to the channel , @xmath5 denotes the conditional probability of a memoryless channel indexed by @xmath2 , and the notation @xmath6 denotes the mutual information of channel @xmath7 for the input distribution @xmath8 , i.e. , @xmath9 the capacity in ( [ eqn : capacitymemorylesscompoundnofb ] ) is in general less than the capacity of every channel in the family .",
    "wolfowitz , who coined the term `` compound channel , '' showed that if the transmitter knows the channel @xmath2 in use , then the capacity is given by @xcite @xmath10 where @xmath11 is the capacity of the channel indexed by @xmath2 .",
    "this shows that knowledge at the transmitter of the channel @xmath2 in use helps in that the infimum of the capacities of the channels in the family can now be achieved . in the case",
    "that @xmath12 is a finite set , then it follows from wolfowitz s result that @xmath13 is the feedback capacity of the memoryless compound channel , since the transmitter can use a training sequence together with the feedback to estimate @xmath2 with high probability . in this paper",
    "we show that when @xmath12 is not limited to finite cardinality , the feedback capacity of the memoryless compound channel is given by @xmath14 .",
    "one might be tempted to think that for a compound channel with memory , feedback provides a means to achieve the infimum of the capacities of the channels in the family .",
    "however this is not necessarily true , as we show in example [ example : compoundge ] , which is taken from @xcite and applied to the compound gilbert - elliot channel with feedback .",
    "that example is found in section [ section : compoundge ] .",
    "a comprehensive review of the compound channel and its role in communication is given by lapidoth and narayan @xcite .",
    "of specific interest in this paper are compound channels with memory which are often used to model wireless communication in the presence of fading @xcite . lapidoth and telatar @xcite derived the following formula for the compound channel capacity of the class of finite state channels ( fsc ) when there is no feedback available at the transmitter .",
    "@xmath15 where @xmath16 denotes the initial state of the fsc , and @xmath17 and @xmath18 denote the input distribution and channel conditional probability for block length @xmath19 .",
    "lapidoth and telatar s achievability result makes use of a universal decoder for the family of finite - state channels .",
    "the existence of the universal decoder is proved by feder and lapidoth in @xcite by merging a finite number of maximum - likelihood decoders , each tuned to a channel in the family @xmath12 .    throughout this paper",
    "we use the concepts of causal conditioning and directed information which were introduced by massey in @xcite .",
    "kramer extended those concepts and used them in @xcite to characterize the capacity of discrete memoryless networks .",
    "subsequently , three different proofs  tatikonda and mitter @xcite , permuter , weissman and goldsmith @xcite and kim @xcite  have shown that directed information and causal conditioning are useful in characterizing the feedback capacity of a point - to - point channel with memory .",
    "in particular , this work uses results from @xcite that show that gallager s",
    "4,5 ) upper and lower bound on capacity of a fsc can be generalized to the case that there is a time - invariant deterministic feedback , @xmath20 , available at the encoder at time @xmath21 .    in this paper",
    "we extend lapidoth and telatar s work for the case that there is deterministic time - invariant feedback available at the encoder by replacing the regular conditioning with the causal conditioning .",
    "then we use the feedback capacity theorem to study the compound gilbert - elliot channel and the memoryless compound channel and to specify a class of compound channels for which the capacity is zero if and only if the feedback capacity is zero .",
    "the proof of the feedback capacity of the fsc is found in section [ section : converse ] , which describes the converse result , and section [ section : achievability ] , where we prove achievability . as a consequence of the capacity result ,",
    "we show in section [ section : compoundge ] that feedback does not increase the capacity of the compound gilbert - elliot channel .",
    "we next show in section [ section : iff ] that for a family of stationary and uniformly ergodic markovian channels , the capacity of the compound channel is positive if and only if the feedback capacity of the compound channel is positive . finally , we return to the memoryless compound channel in section [ section : memorylesscompound ] and make use of our capacity result to provide a proof of the feedback capacity .",
    "the notation we use throughout is as follows .",
    "a capital letter @xmath22 denotes a random variable and a lower - case letter , @xmath23 , denotes a realization of the random variable .",
    "vectors are denoted using subscripts and superscripts , @xmath24 and @xmath25 .",
    "we deal with discrete random variables where a probability mass function on the channel input is denoted @xmath26 and @xmath27 denotes a mass function on the channel output .",
    "when no confusion can result , we will omit subscripts from the probability functions , i.e. , @xmath28 will denote @xmath29 .",
    "the problem we consider is depicted in figure [ fig:1 ] .",
    "a message @xmath30 from the set @xmath31 is to be transmitted over a compound finite state channel with time - invariant deterministic feedback .",
    "the family @xmath12 of finite state channels has a common state space @xmath32 and common finite input and output alphabets given by @xmath33 and @xmath34 . for a given channel @xmath35 the channel output at time @xmath21",
    "is characterized by the conditional probability @xmath36 which satisfies the condition @xmath37 .",
    "the channel @xmath2 is in use over the sequence of @xmath19 channel inputs .",
    "the family @xmath12 of channels is known to both the encoder and decoder , however , they do not have knowledge of the channel @xmath2 in use before transmission begins .    [ ] [ ] [ 0.8]@xmath38[][][0.8]message [ ] [ ] [ 0.8]encoder [ ] [ ] [ 0.7]@xmath39 [ ] [ ] [ 0.8]@xmath40 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8 ] [ ] [ ] [ 0.7]@xmath42 [ ] [ ] [ 0.8]@xmath43 [ ] [ ] [ 0.8]@xmath41[][][0.8]@xmath44 [ ] [ ] [ 0.8]decoder [ ] [ ] [ 0.8]@xmath45 [ ] [ ] [ 0.8]feedback generator [ ] [ ] [ 0.8]@xmath46 [ ] [ ] [ 0.8]@xmath47 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]unit delay [ ] [ ] [ 0.8]@xmath48 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath49[][][0.8]estimated [ ] [ ] [ 0.8]message        the message @xmath30 is encoded such that at time @xmath21 the codeword symbol @xmath50 is a function of @xmath30 and the feedback sequence @xmath51 . for notational convenience , we will refer to the input sequence @xmath52 as simply @xmath53 .",
    "the feedback sequence is a time - invariant deterministic function of the output @xmath54 and is available at the encoder with a single time unit delay .",
    "the function performed on the channel output @xmath54 to form the feedback @xmath55 is known to both the transmitter and receiver before communication begins .",
    "the decoder operates over the sequence of channel outputs @xmath56 to form the message estimate @xmath57 .    for a given initial state @xmath58 and channel @xmath35 ,",
    "the channel causal conditioning distribution is given by @xmath59 additionally we will make use of massey s directed information @xcite .",
    "when conditioned on the initial state and channel , the directed information is given by @xmath60 our capacity result will involve a maximization of the directed information over the input distribution @xmath61 which is defined as @xmath62 we make use of some of the properties provided in @xcite in our work , including the following three which we restate for our problem setting .    1 .",
    "@xmath63 ( * ? ? ?",
    "* eq . ( 3 ) )",
    "* lemma 1 ) [ haimchainrulecausalcond ] 2 .",
    "@xmath64 , where random variable @xmath65 denotes the state of the finite - state channel ( * ? ? ?",
    "* lemma 5 ) 3 .   from ( * ? ? ?",
    "* lemma 6 ) [ haimdirectedinfoequivi ] , @xmath66    note that properties [ haimchainrulecausalcond ] ) and [ haimdirectedinfoequivi ] ) hold since @xmath67 for our feedback setting , where it is assumed that the state @xmath16 is not available at the encoder .    for a given initial state @xmath16 and channel @xmath2 the average probability of error in decoding message @xmath44",
    "is given by @xmath68 where @xmath69 is a function of the message @xmath44 and of the feedback @xmath70 .",
    "the average ( over messages ) error probability is denoted @xmath71 , where @xmath72 .",
    "we say that a rate @xmath73 is achievable for the compound channel with feedback as shown in figure [ fig:1 ] , if for any @xmath74 there exists a code of fixed blocklength @xmath19 and rate @xmath73 , i.e. @xmath75 , such that @xmath76 for all @xmath77 and @xmath58 .",
    "equivalently , rate @xmath73 is achievable if there exists a sequence of rate-@xmath73 codes such that @xmath78 this definition of achievable rate is identical to that given in previous work on the compound channel without feedback . a different definition for the compound channel with feedback",
    "could also be considered ; for instance , in @xcite , the authors consider codes of variable blocklength and define achievability accordingly .",
    "the capacity is defined as the supremum over all achievable rates and is given in the following theorem .",
    "[ thrm : fbcapcompoundfsc ] the feedback capacity of the compound finite state channel is given by @xmath79    theorem [ thrm : fbcapcompoundfsc ] is proved in section [ section : converse ] , which shows the existence of @xmath80 and proves the converse , and section [ section : achievability ] , where achievability is established .",
    "we first state the following proposition , which shows that the capacity @xmath80 as defined in theorem [ thrm : fbcapcompoundfsc ] exists . the proof is found in appendix [ app : existenceproof ] .",
    "[ prop : existencec ] let @xmath81 then @xmath82 is well defined and converges for @xmath83 .",
    "in addition , let @xmath84 then @xmath85    to prove the converse in theorem [ thrm : fbcapcompoundfsc ] , we assume a uniform distribution on the message set , for which @xmath86 . since the message is independent of the channel parameters @xmath87 and we apply fano s inequality as follows .",
    "@xmath88 for any code we have @xmath89 and therefore @xmath90 by combining the above statement with proposition [ prop : existencec ] we have @xmath91 then for a sequence of codes of rate @xmath73 with @xmath92 , this implies @xmath93 .",
    "before proving achievability , we mention a simple case which follows from previous results . if the set @xmath12 has finite cardinality , then achievability follows immediately from the results in ( * ? ? ?",
    "* theorem 14 ) , which are true for any finite state channel with feedback .",
    "hence , we can construct a finite state channel where the augmented state is @xmath94 and by assuming that the initial distribution is positive for all @xmath95 then we get that for any latexmath:[$\\theta \\in \\theta ,    @xmath73 is achievable if @xmath97    more work is needed in the achievability proof when the set @xmath12 is not restricted to finite cardinality .",
    "this is outlined in the following subsections in three steps . in the first step ,",
    "we assume that the decoder knows the channel @xmath2 in use and we show in theorem [ thrm : achievabilitythetaknown ] that if @xmath98 and if the decoder consists of a maximum - likelihood decoder , then there exist codes for which the error probability decays uniformly over the family @xmath12 and exponentially in the blocklength .",
    "the codes used in showing this result are codes of blocklength @xmath99 where each sub - block of length @xmath38 is generated i.i.d . according to some distribution . in the second step ,",
    "we show in lemma [ lemma : typeoncodetree ] that if instead the codes are chosen uniformly and independently from a set of possible blocklength-@xmath99 codes , then the error probability still decays uniformly over @xmath12 and exponentially in the blocklength . in the third and final step ,",
    "we show in theorem [ thrm : univdecoderforseparable ] and lemma [ lemma : compoundfscisstronglysep ] that for codes chosen uniformly and independently from a set of blocklength-@xmath99 codes , there exists a decoder that for every channel @xmath77 achieves the same error exponent as the maximum - likelihood decoder tuned to @xmath2 .    in the sections that follow",
    ", @xmath100 denotes the set of probability distributions on @xmath101 causally conditioned on @xmath102 .",
    "we begin by proving that if the decoder is tuned to the channel @xmath35 in use , i.e. , if the decoder knows the channel @xmath2 in use , and if @xmath103 then the average error probability approaches zero .",
    "this is proved through the use of random coding and maximum likelihood ( ml ) decoding .",
    "the encoding scheme consists of randomly generating a _ code - tree _ for each message @xmath44 , as shown in figure [ f_codetree](b ) for the case of binary feedback .",
    "a code - tree has depth @xmath19 corresponding to the blocklength and level @xmath21 designates a set of @xmath104 possible codeword symbols .",
    "one of the @xmath104 symbols is chosen as the input @xmath50 according to the feedback sequence @xmath105 .",
    "the first codeword symbol is generated as @xmath106 .",
    "the second codeword symbol is generated by conditioning on the previous codeword symbol and on the feedback , @xmath107 for all possible values of @xmath108 .",
    "for instance , in the binary case , @xmath109 , two possible values ( branches ) of @xmath110 will be generated and the transmitted codeword symbol will be selected from among these two values according to the value of the feedback @xmath111 .",
    "subsequent codeword symbols are generated similarly , @xmath112 for all possible @xmath105 .",
    "for a given feedback sequence @xmath70 , the input distribution , corresponding to the distribution on a path through the tree of depth @xmath19 , is @xmath113    [ ] [ ] [ 0.8]@xmath114 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath115 [ ] [ ] [ 0.8]@xmath116 [ ] [ ] [ 0.8]@xmath117 [ ] [ ] [ 0.8]@xmath118 [ ] [ ] [ 0.8]@xmath119 [ ] [ ] [ 0.8]@xmath120    [ ] [ ] [ 0.8]@xmath114 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath115 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath115 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath121 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath117 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath117 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath117 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath119 [ ] [ ] [ 0.8]@xmath122 [ ] [ ] [ 0.8]@xmath41 [ ] [ ] [ 0.8]@xmath123    [ ] [ ] [ 0.8]@xmath124 [ ] [ ] [ 0.8]@xmath125 [ ] [ ] [ 0.8]@xmath126 ( no feedback )    [ ] [ ] [ 0.8](a ) codeword ( no feedback ) [ ] [ ] [ 1](b ) code - tree [ ] [ ] [ 1](c ) concatenated code - tree        a code - tree of depth @xmath19 is a vector of @xmath127 symbols , where @xmath128 and each element in the vector takes value from the alphabet @xmath129 .",
    "we denote a random code - tree by @xmath130 and a realization of the random code - tree by @xmath131 .",
    "the probability of a tree @xmath132 is uniquely determined by @xmath133 .",
    "for instance , consider the case of binary feedback , @xmath134 , and a tree of depth @xmath135 , for which @xmath136 .",
    "a code - tree is a vector @xmath137 where @xmath138 is the symbol sent at time @xmath116 , @xmath139 is the symbol sent at time @xmath118 for feedback @xmath140 , and @xmath141 is the symbol sent at time @xmath118 for feedback @xmath142 . then @xmath143 which is uniquely determined by @xmath144 . in general , for a code - tree of depth @xmath19 , the following holds .",
    "@xmath145    a code - tree for each message @xmath44 is randomly generated , and for each message @xmath44 and feedback sequence @xmath70 the codeword @xmath146 is unique . the decoder is made aware of the code - trees for all messages .",
    "assuming that the ml decoder knows the channel @xmath2 in use , it estimates the message as follows . @xmath147",
    "as shown in @xcite , since @xmath148 is uniquely determined by @xmath44 and @xmath105 and since @xmath149 is a deterministic function of @xmath150 , we have the equivalence @xmath151 so the ml decoder can be described as @xmath152 let @xmath153 denote the average ( over messages ) error probability incurred when a code of blocklength @xmath19 is used over channel @xmath2 with initial state @xmath16 .",
    "the following theorem bounds the error probability uniformly in @xmath95 when the decoder knows the channel @xmath77 in use .",
    "the theorem is proved in appendix [ app : achievabilitythetaknown ] .",
    "[ thrm : achievabilitythetaknown ] for a compound fsc with initial state @xmath58 , input alphabet @xmath33 , and output alphabet @xmath34 , assuming that the decoder knows the channel @xmath2 in use , then there exists a code of rate @xmath73 and blocklength @xmath99 , where @xmath154 and @xmath38 is chosen such that @xmath155 , for which the error probability @xmath156 of the ml decoder satisfies @xmath157 for any @xmath77 , where @xmath158    the result in theorem [ thrm : achievabilitythetaknown ] is shown by the use of a randomly - generated code - tree of depth @xmath99 for each message @xmath44 . for every feedback sequence @xmath159 ,",
    "the corresponding path in the code - tree is generated by the input distribution @xmath160 given by @xmath161 where @xmath162 is the distribution that achieves the supremum in @xmath163 .",
    "the random codebook @xmath164 used in proving theorem [ thrm : achievabilitythetaknown ] consists of @xmath165 code - trees .",
    "each code - tree in the codebook is a concatenated code - tree with depth @xmath99 consisting of @xmath166 code - trees , each of depth @xmath38 . for a given feedback sequence @xmath159 ( corresponding to a certain path in the concatenated code - tree ) the codeword is generated by @xmath167 .",
    "an example of a concatenated code - tree is found in figure [ f_codetree](c ) .      in this subsection",
    "we show that the result in theorem [ thrm : achievabilitythetaknown ] implies that the error probability can be similarly bounded when codewords are chosen uniformly over a set . in other words , we convert the random coding exponent given in theorem [ thrm : achievabilitythetaknown ] , where it is assumed that the codebook consists of concatenated code - trees of depth @xmath99 in which each sub - tree of depth @xmath38 is generated i.i.d . according to @xmath162 , to a new random coding exponent for which the concatenated code - trees in the codebook are chosen uniformly from a set of concatenated code - trees .",
    "this alternate type of random coding , where the concatenated code - trees are chosen uniformly from a set , is the coding approach subsequently used to prove the existence of a universal decoder .",
    "we first introduce the notion of types on code - trees .",
    "let @xmath168 denote the concatenation of @xmath166 depth-@xmath38 code - trees @xmath169 , where @xmath170 is defined in ( [ eqn : codetreenumsymbols ] ) and @xmath171 .",
    "the type ( or empirical probability distribution ) of a concatenated code - tree @xmath168 is the relative proportion of occurrences of each code - tree @xmath172 .",
    "equivalently , @xmath166 multiplied by the type of @xmath168 indicates the number of times each depth-@xmath38 code - tree from the set @xmath173 occurs in the concatenated code - tree @xmath168 .",
    "let @xmath174 denote the set of types of concatenated code - trees of depth @xmath99 .",
    "let @xmath175 denote the average probability of error incurred when a code - tree of depth @xmath19 and rate @xmath73 drawn according to a distribution @xmath176 is used over the channel @xmath177 .",
    "we now prove the following result .",
    "[ lemma : typeoncodetree ] given @xmath178 , let @xmath179 denote the distribution given by the n - fold product of @xmath180 , i.e. , @xmath181 for a given type @xmath182 , let @xmath183 denote the distribution that is uniform over the set of concatenated code - trees of type @xmath184 . for every distribution @xmath185 there exists a type @xmath182 whose choice depends on @xmath180 and @xmath166 but not on @xmath177 such that @xmath186 for all @xmath177 , where @xmath187 tends to 0 as @xmath188 .",
    "the proof follows the approach of ( * ? ? ?",
    "* lemma 3 ) except that our codebook consists of code - trees rather than codewords ; we include this proof for completeness in describing the notion of types on code - trees .",
    "given a codebook @xmath164 of rate @xmath189 chosen according to @xmath190 , we can construct a sub - code @xmath191 of rate @xmath73 in the following way .",
    "let @xmath192 denote the type with the highest occurrence in @xmath164 .",
    "the number of types in @xmath164 is upper bounded by @xmath193 , so the number of concatenated code - trees of type @xmath192 is lower bounded by @xmath194 .",
    "we construct the code @xmath191 by picking the first @xmath165 concatenated code - trees of type @xmath192 .",
    "since @xmath191 is a sub - code of @xmath164 , its average probability of error is upper bounded by the average probability of error of @xmath164 times @xmath195 .",
    "conditioned on @xmath192 , the codewords in @xmath191 are mutually independent and uniformly distributed over a set of concatenated code - trees of type @xmath192 . since @xmath164 is a random code , the type @xmath192 is also random , and let @xmath196 denote the distribution of @xmath192 . pick a realization of the type @xmath192 , denoted @xmath184 , that satisfies @xmath197 .",
    "( this is possible since the number of types is upper bounded by @xmath198 . )",
    "then @xmath199 and @xmath200    combining this result with theorem [ thrm : achievabilitythetaknown ] , we have that there exists a type @xmath182 such that when the codewords are chosen uniformly from the type class of @xmath184 , given by the distribution @xmath201 , the average probability of error is bounded as @xmath202   \\right\\}\\end{aligned}\\ ] ]    it is then possible to choose @xmath203 such that for all @xmath204 , @xmath205 and @xmath206 which implies that the probability of error is bounded as @xmath207      we next show that when a codebook is constructed by choosing code - trees uniformly from a set , there exists a universal decoder for the family of finite - state channels with feedback .",
    "this result is shown in the following four steps .",
    "* we define the notion of a strongly separable family @xmath12 of channels given by the causal conditioning distribution .",
    "the notion of strong separability means that the family is well - approximated by a finite subset of the channels in @xmath12 . *",
    "we prove that for strongly separable @xmath12 and code - trees chosen uniformly from a set , there exists a universal decoder .",
    "* we describe the universal decoder which `` merges '' the ml decoders tuned to a finite subset of the channels in @xmath12 .",
    "* we show that the family of finite - state channels given by the causal conditioning distribution is a strongly separable family .",
    "our approach follows precisely the approach of feder and lapidoth @xcite except that our codebook consists of concatenated code - trees ( rather than codewords ) and our channel is given by the causal conditioning distribution .",
    "let @xmath168 denote a concatenated code - tree of depth @xmath99 , @xmath208 where @xmath209 , and let @xmath210 denote a set of such code - trees , @xmath211 . as described in lemma [ lemma : typeoncodetree ] , @xmath210 will be the set of code - trees of type @xmath182 and the code - tree for each message will be chosen uniformly from this set , i.e. @xmath212 for any @xmath213 .",
    "as described below , for a given output sequence @xmath214 , ml decoding will correspond to comparing the functions @xmath215 , @xmath213 .",
    "note that comparing the functions @xmath215 is equivalent to comparing the channel causal conditioning distributions since @xmath216 as shown below . @xmath217 in the above , @xmath218 holds since @xmath105 is a known , deterministic function of @xmath219 and @xmath220 holds since the code - tree @xmath168 together with the feedback sequence @xmath105 uniquely determines the channel input @xmath148 .    for notational convenience ,",
    "the results below on the universal decoder are stated for blocklength @xmath19 , where @xmath130 denotes a code - tree of depth @xmath19 and @xmath221 denotes a set of such code - trees .",
    "these results extend to the set of concatenated code - trees @xmath210 and any exceptions are described in the text .",
    "furthermore , we introduce the following notation : @xmath222 denotes the ml decoder tuned to channel @xmath2 ; @xmath223 denotes the average ( over messages and codebooks chosen uniformly from a set ) error probability when decoder @xmath224 is used over channel @xmath2 ; and @xmath225 denotes the average ( over messages ) error probability when codebook @xmath164 and decoder @xmath224 is used over channel @xmath2 .",
    "a family of channels @xmath226 defined over common input and output alphabets @xmath227 is said to be _ strongly separable _ for the input code - tree sets @xmath228 , @xmath229 , if there exists some @xmath230 that upper bounds the error exponents in the family , i.e. , that satisfies @xmath231 such that for every @xmath74 and blocklength @xmath19 , there exists a subexponential number @xmath232 ( that may depend on @xmath233 and on @xmath234 ) of channels @xmath235 @xmath236 that well approximate any @xmath77 in the following sense : for any @xmath77 there exists @xmath237 , @xmath238 , so that @xmath239 and @xmath240 [ def : strongsep ]    the notion of strong separability means that the family @xmath12 is well - approximated by a finite subset @xmath241 of the channels in the family . in order to prove that the family of finite - state channels with feedback is separable , we will need a value @xmath233 that satisfies ( [ eqn : defstrsepmu ] ) .",
    "the error probability @xmath242 is lower bounded by the probability that the output sequence @xmath243 corresponding to two different messages is the same for a given realization of the channel and code - tree . for a random code - tree",
    "this is lower bounded by a uniform memoryless distribution on the channel output .",
    "then @xmath244 and a suitable candidate for @xmath233 is @xmath245 .",
    "the following theorem shows the existence of a universal decoder for a strongly separable family and input code - tree sets @xmath221 .",
    "the proof follows from the proof of theorem 2 in @xcite except that we replace the channel conditional distribution @xmath246 with the causal conditioning distribution @xmath247 .",
    "[ thrm : univdecoderforseparable ] if a family of channels defined over common finite input and output alphabets @xmath248 is strongly separable for the input code - tree sets @xmath228 , then there exists a sequence of rate-@xmath73 blocklength-@xmath19 codes @xmath249 and a sequence of decoders @xmath250 such that @xmath251    the universal decoder @xmath252 in theorem [ thrm : univdecoderforseparable ] is given by `` merging '' the ml decoders tuned to channels @xmath253 , @xmath254 , that are used to approximate the family @xmath12 . in order to describe the merging of the ml decoders , we first present the ranking function @xmath255 .",
    "a ml decoder tuned to the channel @xmath2 can be described by a ranking function @xmath255 defined as the mapping latexmath:[\\[m_{\\theta } : b_{nm } \\times { \\cal y}^{nm } \\rightarrow \\{1,2,\\hdots ,    @xmath168 that is most likely given output @xmath214 , rank 2 denotes the second most likely code - tree , and so on . for",
    "a given received sequence @xmath214 , every code - tree in the set @xmath210 is assigned a rank .",
    "for code - trees",
    "@xmath257 , @xmath258 by ( [ eqn : pyconda = pycauscondx ] ) , comparing the function @xmath215 is equivalent to comparing the channel causal conditioning distribution @xmath259 . letting @xmath222",
    "denote the ml decoder tuned to @xmath2 , we can describe the decoder as @xmath260 where @xmath261 represents the code - tree chosen for message @xmath44 , @xmath262 . in the case that multiple code - trees maximize the likelihood @xmath215 for a given @xmath214 , the ranking function @xmath255 determines which code - tree ( and correspondingly message ) is chosen by the decoder . in the case that the same code - tree from @xmath210 is chosen for more than one message , the ranks will be identical and a decoding error will occur . note that for a given output sequence @xmath214 , the decoder @xmath263 will not always return the code - tree @xmath213 for which @xmath264 , since the code - tree @xmath168 may or may not be in the codebook .    now consider a set of @xmath265 channels from the family @xmath12 , given by @xmath266 .",
    "the codebooks for these @xmath265 channels will be drawn randomly from the set @xmath210 .",
    "( note that the same set @xmath210 is used for all channels @xmath253 since , as shown in lemma [ lemma : typeoncodetree ] , the type @xmath182 is chosen independent of the channel @xmath177 . )",
    "the @xmath265 ml decoders matched to these channels , denoted @xmath267 , can be merged as shown in @xcite .",
    "the merged decoder @xmath268 is described by its ranking function @xmath269 which is a mapping latexmath:[\\[m_{u_k } : b_{nm } \\times { \\cal y}^{nm } \\rightarrow \\{1,2,\\hdots ,    for each output sequence @xmath214 .",
    "the ranking @xmath269 is established for a given @xmath214 by assigning rank 1 to the code - tree for which @xmath271 , rank 2 to the code - tree for which @xmath272 , rank 3 to the code - tree for which @xmath273 , and so on . after considering the code - trees with rank 1 for all @xmath274 ,",
    "the code - trees with rank 2 in @xmath274 , @xmath275 are considered in order and added into the ranking @xmath269 .",
    "the process continues until the code - trees with rank @xmath276 for all @xmath274 have been assigned a rank in @xmath269 . throughout this process ,",
    "if a code - tree has already been ranked , it is simply skipped over , and its original ( higher ) ranking is maintained . the rank of a code - tree in @xmath269 can be upper bounded according to its rank in @xmath274 as shown in @xcite and stated as follows . @xmath277",
    "this bound on the rank in @xmath269 implies another ( looser ) upper bound .",
    "@xmath278 equation ( [ eqn : upperboundrankmerged ] ) can be used to upper bound the error probability when sequences output from the channel @xmath279 are decoded by the merged decoder @xmath268 .",
    "this is a key element of the proof of theorem [ thrm : univdecoderforseparable ] .",
    "finally , we state the lemma below , which shows that the family of finite - state channels defined by the causal conditioning distribution is strongly separable . together with theorem [ thrm : univdecoderforseparable ] , this establishes existence of a universal decoder for the problem we consider , and completes our proof of achievability .",
    "the family of all causal - conditioning finite - state channels defined over common finite input , output , and state alphabets @xmath280 is strongly separable in the sense of definition [ def : strongsep ] for any input code - tree sets @xmath228 .",
    "[ lemma : compoundfscisstronglysep ]    see appendix [ app : compoundfscisstronglysep ] .",
    "the gilbert - elliot channel is a widely used example of a finite state channel .",
    "it has a state space consisting of ` good ' and ` bad ' states , @xmath281 and in either of these two states , the channel is a binary symmetric channel ( bsc ) .",
    "the gilbert - elliot channel is a stationary and ergodic markovian channel , i.e. , @xmath282 is satisfied and the markov process described by @xmath283 is a stationary and ergodic process . for a given channel @xmath2 , the bsc crossover probability is given by @xmath284 for @xmath285 and @xmath286 for @xmath287 .",
    "the channel state @xmath288 forms a stationary markov process with transition probabilities @xmath289 for a given @xmath2 , the gilbert - elliot channel is equivalent to the following additive noise channel @xmath290 where @xmath291 denotes modulo-2 addition and @xmath292 .",
    "conditioned on the state process @xmath293 , the noise @xmath294 forms a bernoulli process given by @xmath295 for a given channel @xmath2 , the capacity of the gilbert - elliot channel is found in @xcite and is achieved by a uniform bernoulli input distribution .",
    "the following example illustrates that the feedback capacity of a channel with memory is in general _ not _ given by @xmath296 as in the memoryless case .",
    "@xcite [ example : compoundge ] consider the example of a gilbert - elliot channel where @xmath297 for @xmath298 with feedback . the compound feedback capacity of this channel is zero because assuming that we start in the bad state , for any blocklength @xmath19 , the channel that corresponds to @xmath299 , will remain in the bad state for the duration of the transmission with probability @xmath300 .",
    "while the channel is in the bad state the probability of error for decoding the message is positive with or without feedback , hence no reliable communication is possible .",
    "however if we fix @xmath2 , then the capacity @xmath301 is at least @xmath302 , because we can use a deep enough interleaver to make the channel look like memoryless bsc with crossover probability @xmath303 .",
    "a gilbert - elliot channel is described by the four parameters @xmath304 and @xmath284 that lie between 0 and 1 and for any fixed @xmath19 , @xmath305 is continuous in those parameters .",
    "the continuity of @xmath305 follows from the fact that @xmath306 is continuous in the four parameters for any @xmath307 , and also because ( as shown in appendix [ app : compoundfscisstronglysep ] in eqns .",
    "( [ eqn : equivfederlapidoth63 ] ) and ( [ eqn : equivfederlapidoth64 ] ) ) we can express @xmath305 as @xmath308    let us denote by @xmath309 the closure of the family of channels .",
    "hence instead of @xmath310 we can write @xmath311 since @xmath309 is compact and since @xmath312 is continuous in @xmath177 .",
    "now , let @xmath313 denote the uniform distribution over @xmath314 .",
    "we have @xmath315 where @xmath218 follows from the fact that @xmath316 and @xmath220 follows from the fact that for any channel a uniform distribution maximizes its capacity",
    ". therefore we can restrict the maximization to the uniform distribution @xmath317 instead of @xmath318 .",
    "hence feedback does not increase the capacity of the compound gilbert - elliot channel .",
    "this result holds for any family of fscs for which the uniform input distribution achieves the capacity of each channel in the family and is closely related to alajaji s result @xcite that feedback does not increase the capacity of discrete additive noise channels .",
    "in this section we show that the capacity of a compound channel that consists of stationary and uniformly ergodic markovian channels is positive if and only if it is positive for the case that feedback is allowed .",
    "the intuition of this result comes mainly from lemma [ lemma : directed0iff ] that states that @xmath319 the reason our proof is restricted to the family of channels that are stationary and uniformly ergodic markovian is because for this family of channels we can show that the capacity is zero only if for every finite @xmath19 , @xmath320    a stationary and ergodic markovian channel is a fsc where the state of the channel is a stationary and ergodic markov process that is not influenced by the channel input and output . in other words ,",
    "the conditional probability of the channel output and state given the input and previous state is given by @xmath321 where the markov process , described by the transition probability @xmath283 , is stationary and ergodic .",
    "we say that the family of channels is _ uniformly ergodic _ if all channels in the family are ergodic and for all @xmath322 there exists an @xmath323 such that for all @xmath324 @xmath325 where @xmath326 is the stationary ( equilibrium ) distribution of the state for channel @xmath2 .",
    "we define the sequence @xmath327 as @xmath328    [ thrm : zerofbciffzeronfbc ] the channel capacity of a family of stationary and uniformly ergodic markovian channels is positive if and only if the feedback capacity of the same family is positive .",
    "since a memoryless channel is a fsc with only one state , the theorem implies that the feedback capacity of a memoryless compound channel is positive if and only if it is positive without feedback .",
    "the theorem also implies that for a stationary and ergodic point - to - point channel ( not compound ) , feedback does not increase the capacity for cases that the capacity without feedback is zero .",
    "the stationarity of the channels in theorem [ thrm : zerofbciffzeronfbc ] is not necessary since according to our achievability definition , if a rate is less than the capacity , it is achievable regardless of the initial state .",
    "we assume stationarity here in order to simplify the proofs .",
    "the uniform ergodicity is essential to the proof that is provided here but there are also other family of channels that have this property . for instance , for the regular point - to - point gaussian channel",
    "this result can be concluded from factor two result that claims that feedback at most doubles capacity ( c.f . , @xcite ) .",
    "the proof of theorem [ thrm : zerofbciffzeronfbc ] is based on the following lemmas .",
    "we refer the reader to appendix [ app : lemmasforiff ] for the proofs of these lemmas .",
    "[ lemma : qkqm_inequality ] for any channel with feedback , if the input to the channel is distributed according to @xmath329 then @xmath330    [ lemma : capacity_stationary_ergodic_markovian ] the feedback capacity of a family of stationary and uniformly ergodic markovian channels is @xmath331 the limit of @xmath327 exists and is equal to @xmath332 .",
    "[ lemma : directed0iff ] let the input distribution to an arbitrary channel be uniform over the input @xmath314 , i.e. , @xmath333 . if under this input distribution @xmath334 , then the channel has the property that @xmath335 for all @xmath336 and this implies that @xmath337    _ proof of theorem [ thrm : zerofbciffzeronfbc ] _ : let @xmath338 denote the capacity without feedback and @xmath339 denote the capacity with feedback",
    ". @xmath340 is trivial . to show that @xmath341 , we use lemma [ lemma : capacity_stationary_ergodic_markovian ] to conclude that",
    "since @xmath342 then @xmath343 and therefore for any @xmath344 , @xmath345 in order to conclude the proof , we show that if ( [ eqn : compound_no_feedback2 ] ) holds , then it also holds when we replace @xmath346 by @xmath347 .",
    "since @xmath348 is continuous in @xmath349 and since the set @xmath12 is a subset of the unit simplex which is bounded , then the infimum over the set @xmath12 can be replaced by the minimum over the closure of the set @xmath12 .",
    "since ( [ eqn : compound_no_feedback2 ] ) holds also for the case that @xmath346 is restricted to be the uniform distribution , then lemma [ lemma : directed0iff ] implies that the channel that satisfies @xmath335 for all @xmath350 is in the closure of @xmath12 and therefore @xmath351",
    "recall that the capacity of the memoryless compound channel ( without feedback ) is @xcite @xmath352 wolfowitz also showed @xcite that when @xmath2 is known to the encoder , the capacity of the memoryless compound channel is given by switching the @xmath353 and the @xmath354 , i.e. , @xmath355 in this section we make use of theorem [ thrm : fbcapcompoundfsc ] to show that ( [ eqn : capacityencoderknowstheta ] ) is equal to the feedback capacity of the memoryless compound channel .      based on wolfowitz",
    "s result it is straightforward to show that if the family of memoryless channels is finite , @xmath356 , then the feedback capacity of the compound channel is given by switching the @xmath354 and the @xmath357 , @xmath358 this result can be achieved in two steps .",
    "given a probability of error @xmath359 , first , the encoder will use @xmath360 uses of the channels in order to estimate the channel with probability of error less than @xmath361 . since the number of channels is finite such an @xmath360 exists . in the second step",
    "the encoder will use a coding scheme with blocklength @xmath166 adapted for the estimated channel to obtain an error probability that is smaller than @xmath361 .",
    "hence we get that the total error of the code of length @xmath362 is smaller than @xmath363 .      for the case that the number of channels is infinite , the argument above does not hold , since there is no guarantee that for any @xmath359 there exists a blocklength @xmath364 such that a @xmath365 code achieves an error less than @xmath363 for all channels in the family . however , we are able to establish the feedback capacity using our capacity theorem for the compound fsc , and the result is stated in the following theorem .",
    "[ thrm : capacitymemorylesscompound ] the feedback capacity of the memoryless compound channel is @xmath366    theorem [ thrm : capacitymemorylesscompound ] is a direct result of theorem [ thrm : fbcapcompoundfsc ] and the following lemma .",
    "[ l_memoryless_eq ] for a family @xmath12 of memoryless channels we have @xmath367    the proof of lemma [ l_memoryless_eq ] requires two lemmas , which we state below .",
    "the proofs of lemmas [ l_donotloosemuch ] and [ l_estimate_channel ] are found in appendix [ app : lemmasformemorylesscompound ] .",
    "[ l_donotloosemuch ] let @xmath368 and @xmath369 .",
    "for two conditional distributions @xmath370 and @xmath371 with @xmath372 where @xmath373 as @xmath374 .",
    "[ l_estimate_channel ] for any @xmath375 , any @xmath322 and any channel @xmath376 , there exists an @xmath360 such that we can choose a channel @xmath377 as a function of @xmath360 inputs and outputs such that @xmath378 where @xmath379 denotes the @xmath380 distance between the estimated channel @xmath381 and the actual channel @xmath376 , i.e. , @xmath382    _ proof of lemma [ l_memoryless_eq ] : _ we prove the equality by showing the following two inequalities hold : @xmath383 where @xmath384 as @xmath385 .",
    "inequality ( [ e_memoryless1 ] ) is proved by the fact that @xmath386 is less than or equal to @xmath387 and by the fact that for a memoryless channel an i.i.d input maximizes the directed information .",
    "@xmath388    in order to prove inequality ( [ e_memoryless2 ] )",
    "we consider the following input distribution .",
    "the first @xmath360 inputs are used to estimate the channel and we denote the estimated channel as @xmath389 . after the first @xmath360 inputs , the input distribution is the i.i.d distribution that maximizes the mutual information between the input and the output for the channel @xmath390 . according to lemma [ l_estimate_channel ] , we can estimate the channel to within an @xmath380 distance smaller than @xmath322 with probability greater than @xmath391 , where @xmath375 . according to lemma [ l_donotloosemuch ] , by adjusting the input distribution to a channel that is at @xmath380 distance less than @xmath234 from the actual channel in use ,",
    "we lose an amount that goes to zero as @xmath392 . under the input distribution described above we have the following sequence of inequalities . @xmath393    * and ( f ) follow from a change of notation . *",
    "follows the fact that we sum fewer elements .",
    "the parameter @xmath360 is a function of @xmath322 and @xmath375 and is determined according to lemma [ l_estimate_channel ] .",
    "for brevity of notation we denote @xmath394 simply as @xmath360 .",
    "* follows from the fact that @xmath395 .",
    "* follows from the fact that the estimated channel is a random variable denoted as @xmath396 and it is a deterministic function of @xmath397 as described in lemma [ l_estimate_channel ] .",
    "* follows by restricting the input distribution @xmath347 to one that uses first @xmath360 uses of the channel to estimate as described in lemma [ l_estimate_channel ] , and then uses an i.i.d distribution , i.e. , for @xmath398 , @xmath399 .",
    "* follows from the fact that with probability @xmath391 we have that the @xmath380 distance @xmath400 and by applying lemma [ l_donotloosemuch ] , which states that for this case we lose @xmath401 where @xmath402 as @xmath392 .",
    "* follows from the fact that @xmath403 is identical to @xmath404 .",
    "finally , since @xmath360 is fixed for any @xmath322 , @xmath375 then we can achieve any value below @xmath405 for large @xmath19 .",
    "therefore inequality ( [ e_memoryless2 ] ) holds .",
    "the compound channel is a simple model for communication under channel uncertainty .",
    "the original work on the memoryless compound channel without feedback characterizes the capacity @xcite , which is less than the capacity of each channel in the family , but the reliability function remains unknown .",
    "an adaptive approach to using feedback on an unknown memoryless channel is proposed in @xcite , where coding schemes that universally achieve the reliability function ( the burnashev error exponent ) for certain families of channels ( e.g. , for a family of binary symmetric channels ) are provided . by using the variable - length coding approach in @xcite ,",
    "the capacity of the channel in use can be achieved . in our work , we consider the use of fixed length block codes and aim to ensure reliability for every channel in the family ; as a result , our capacity is limited by the infimum of the capacities of the channels in the family . for the compound channel with memory that we consider , we have characterized an achievable random coding exponent , but the reliability function remains unknown .",
    "the encoding and decoding schemes used in proving our results have a number of practical limitations , including the memory requirements for storing codebooks consisting of concatenated code - trees at both the transmitter and receiver as well as the complexity involved in merging the maximum - likelihood decoders tuned to a number of channels that is polynomial in the blocklength . as such",
    ", our work motivates a search for more practical schemes for feedback communication over the compound channel with memory .",
    "the proposition is nearly identical to ( * ? ? ? * proposition 1 ) except that we replace @xmath406 by @xmath407 and @xmath408 by @xmath61 using results from @xcite on directed mutual information and causal conditioning .",
    "we first prove the following lemma , which is needed in the proof of proposition [ prop : existencec ] .",
    "the lemma shows that directed information is uniformly continuous in @xmath347 . for our time - invariant deterministic feedback model , @xmath409 , and the lemma holds for any such feedback .",
    "[ lemma : continuitydirectedinfo ]",
    "_ ( uniform continuity of directed information ) _ if @xmath410 and @xmath411 are two causal conditioning distributions such that @xmath412 then for a fixed @xmath413 @xmath414    directed information can be expressed as a difference between two terms @xmath415 .",
    "let us consider the total variation of @xmath416 , @xmath417 by invoking the continuity lemma of entropy ( * ? ? ?",
    "* theorem 2.7 , p33 ) we get , @xmath418 and @xmath419 are the entropies induced by @xmath420 and @xmath421 , respectively .",
    "now let us consider the difference @xmath422 .",
    "@xmath423 by combining inequalities ( [ eq_hdiff1 ] ) and ( [ eq_hdiff2 ] ) we conclude the proof of the lemma .    by lemma",
    "[ lemma : continuitydirectedinfo ] , @xmath424 is uniformly continuous in @xmath425 . since @xmath425 is a member of a compact set , the maximum over @xmath425 is attained and @xmath82 is well - defined .    next , we invoke a result similar to ( * ? ? ?",
    "* lemma 5 ) . given integers @xmath426 and @xmath38 such that @xmath427 , input sequences @xmath428 and @xmath429 with corresponding output sequences @xmath430 and @xmath431 , let @xmath425 be defined as @xmath432 then @xmath433 this result follows from ( * ? ? ?",
    "* lemma 5 ) and ( * ? ? ?",
    "* lemma 5 )",
    ".    finally , if we let @xmath434 and @xmath435 achieve the maximizations in @xmath436 and @xmath437 , respectively , then we have @xmath438 or equivalently , @xmath439 clearly @xmath440 , and by the convergence of a super - additive sequence , @xmath441 .",
    "the theorem is proved through a collection of results in @xcite and @xcite .",
    "let @xmath442 denote the error probability of the ml decoder when a random code - tree of blocklength @xmath19 is used at the encoder . @xmath443 the following corollary to (",
    "* theorem 8) bounds the expected value @xmath444 $ ] , where the expectation is with respect to the randomness in the code .",
    "the result holds for any initial state @xmath16 .",
    "[ cor : boundeproberror ] suppose that an arbitrary message @xmath44 , @xmath445 , enters the encoder with feedback and that ml decoding tuned to @xmath2 is employed .",
    "then the average probability of decoding error over the ensemble of codes is bounded , for any choice of @xmath446 , @xmath447 , by @xmath448 \\leq ( e^{nr}-1)^{\\rho } \\sum_{y^n } \\left [ \\sum_{x^n } q(x^n||z^{n-1 } ) p(y^n||x^n,\\theta)^{\\frac{1}{1+\\rho } } \\right]^{1+\\rho}.\\ ] ]    identical to ( * ? ? ?",
    "* proof of theorem 8) except that @xmath349 is replaced by @xmath247 .",
    "next , we let @xmath153 denote the average ( over messages ) error probability incurred when a code - tree of blocklength @xmath19 is used over channel @xmath2 with initial state @xmath16 . using corollary [ cor : boundeproberror ]",
    ", we can bound @xmath153 as in the following corollary to ( * ? ? ?",
    "* theorem 9 )    [ cor : boundproberrorfe ] for a compound fsc with @xmath449 states where the codewords are drawn independently according to a given distribution @xmath450 and ml decoding tuned to @xmath2 is employed , the average probability of error @xmath153 for any initial state @xmath58 , channel @xmath77 , and @xmath446 , @xmath451 is bounded as @xmath452 where @xmath453^{1+\\rho}\\end{gathered}\\ ] ]    identical to ( * ? ? ?",
    "* proof of theorem 9 ) except for : ( i ) we replace @xmath305 by @xmath454 , ( ii ) we consider the error averaged over all messages ( rather than the error for an arbitrary message @xmath44 ) , and ( iii ) we assume a fixed input distribution @xmath425 rather than minimizing the error probability over all @xmath425 .",
    "the two results stated above provide us with a bound on the error probability , however , the bound depends on the channel @xmath2 in use . instead , we would like to bound the error probability uniformly over the class @xmath12 . to do",
    "so we cite the following two lemmas from previous work .",
    "[ lemma : fsuperadditive ] given @xmath455 and @xmath185 , let @xmath456 and define @xmath457 then @xmath458 as defined in corollary [ cor : boundproberrorfe ] satisfies @xmath459    identical to ( * ? ? ?",
    "* proof of lemma 11 ) except that we replace @xmath305 by @xmath454 .",
    "[ lemma : bounde ] @xmath460    the lemma follows from ( * ? ? ? * lemma 2 ) , which holds for a channel @xmath177 and input distribution @xmath461 satisfying @xmath462 and @xmath463 .",
    "we now follow the technique in @xcite by using lemmas [ lemma : fsuperadditive ] and [ lemma : bounde ] to bound the error probability independent of both @xmath16 and @xmath2 . for a given rate @xmath98 ,",
    "let @xmath464 and pick @xmath38 in such a way that @xmath155 . then @xmath465 let @xmath466 be the input distribution that achieves the supremum in @xmath163 , i.e. , @xmath467 next , we use @xmath162 to define a distribution @xmath468 for a sequence of length @xmath99 , @xmath154 , as follows .",
    "@xmath469    for this new input distribution and sequence of length @xmath99 , we can bound the error exponent @xmath470 as shown below . @xmath471 where @xmath218 is due to lemma [ lemma : fsuperadditive ]",
    ", @xmath220 follows from lemma [ lemma : bounde ] , and @xmath472 follows from ( [ eqn : qm * ] ) . as in @xcite",
    ", we can maximize the lower bound on the error exponent by setting @xmath473 . with this choice of @xmath446",
    "we have @xmath474 theorem [ thrm : achievabilitythetaknown ] follows by combining ( [ eqn : lowerboundfnm ] ) with the result in corollary [ cor : boundproberrorfe ] ( for blocklength @xmath99 ) .",
    "to prove the lemma , we must first establish two equalities relating the channel causal conditioning distribution @xmath454 to the channel probability law @xmath475 .",
    "the following set of equalities hold .",
    "@xmath476 where @xmath218 is due to ( * ? ? ?",
    "* lemma 2 ) and @xmath220 follows from our assumption that the input distribution @xmath69 does not depend on the state sequence @xmath477 . by the chain rule for causal conditioning (",
    "* lemma 1 ) , ( [ eqn : chainrulecc ] ) implies that @xmath478 also , @xmath479 where @xmath472 follows from the definition of the compound finite - state channel . having established equations ( [ eqn : equivfederlapidoth63 ] ) and ( [ eqn : equivfederlapidoth64 ] ) , lemma [ lemma : compoundfscisstronglysep ] follows immediately from ( * ? ? ?",
    "* lemma 12 ) , where the conditional probability @xmath475 is quantized and the quantization cells are represented by channels @xmath480 . the proof of our result differs only in that the upper bound on the error exponents in the family is given by @xmath481 .",
    "the proof of lemma [ lemma : qkqm_inequality ] is based on an identity that is given by kim in ( * ? ? ?",
    "* eq . ( 9 ) ) : @xmath482    _ proof of lemma [ lemma : qkqm_inequality ] : _ using kim s identity we have @xmath483 now we bound the sum in the last equality , @xmath484 where @xmath218 follows from the assumption that @xmath485 .",
    "_ proof of lemma [ lemma : capacity_stationary_ergodic_markovian ] _ : the proof consists of two parts . in the first part we show that @xmath486 is sup - additive and therefore @xmath487 . in the second part",
    "we prove the capacity of the family of stationary and uniformly ergodic markovian channels by showing that @xmath488 where @xmath82 is defined in ( [ eqn : c_ndefinition ] ) .",
    "_ first part : _ we show that the sequence @xmath327 is sup - additive and therefore the limit exists .",
    "let integers @xmath426 and @xmath38 be such that @xmath427 and denote input distributions @xmath489 , and @xmath435 in shortened forms as @xmath490 , and @xmath180 .",
    "we have , @xmath491\\nonumber \\\\ & \\stackrel{}{\\geq}&\\max_{q_kq_m } \\left[\\inf_{\\theta } i(x^k\\to y^k |\\theta)+\\inf_{\\theta } i(x_{k+1}^n\\to y_{k+1}^n |\\theta)\\right]\\nonumber \\\\ & \\stackrel{=}{}&\\max_{q_k } \\inf_{\\theta } i(x^k\\to y^k |\\theta)+\\max_{q_m}\\inf_{\\theta } i(x_{k+1}^n\\to y_{k+1}^n |\\theta)\\nonumber \\\\ & \\stackrel{(c)}=&\\max_{q_k } \\inf_{\\theta } i(x^k\\to y^k |\\theta)+\\max_{q(x^m||z^{m-1})}\\inf_{\\theta } i(x^m\\to y^m |\\theta)\\nonumber \\\\ & \\stackrel{=}{}&kc_k^{markovian}+mc_m^{markovian},\\end{aligned}\\ ] ] where @xmath218 follows by restricting the maximization to causal conditioning probabilities of the product form @xmath492 , @xmath220 follows from lemma [ lemma : qkqm_inequality ] , and @xmath472 follows from stationarity of the channel . _",
    "second part : _ we show that @xmath493 . due to lemma 5 in @xcite , @xmath494 , therefore it is enough to prove that latexmath:[\\[\\label{eqn : diff_s0_s0 } \\lim_{n \\rightarrow \\infty}\\frac{1}{n } \\left [ \\max_{q_{x^n||z^{n-1 } } } \\inf _ { \\theta }   i(x^n\\to y^n|s_0,\\theta ) - \\max_{q_{x^n||z^{n-1 } } } \\inf _ { \\theta , s_0 } i(x^n\\to y^n ,    always positive , hence it is enough to upper bound it by an expression that goes to zero as @xmath385 .",
    "again by lemma 5 in @xcite we can bound the second term in ( [ eqn : diff_s0_s0 ] ) , @xmath496 where ( a ) holds for every @xmath497 and is due to lemma [ lemma : qkqm_inequality ] and ( b ) holds by the stationarity of the channel .",
    "hence , ( [ e_bound_directed ] ) implies that we can bound the difference , @xmath498 inequality ( a ) is due to the fact that @xmath499 and due to ( [ e_bound_directed ] ) . inequality ( b ) holds since for a uniformly ergodic family of channels , @xmath500 for all @xmath501 implies that for any input distribution @xmath502 and any channel @xmath2 , @xmath503 ) by @xmath19 , and since @xmath234 can be arbitrarily small and @xmath426 is fixed for a given @xmath234 , then ( [ eqn : diff_s0_s0 ] ) holds .",
    "_ proof of lemma [ lemma : directed0iff ] _ : from the assumption of the lemma we have @xmath504 by assuming a uniform input distribution , @xmath505 and by using the fact that if the kullback leibler divergence @xmath506 is zero , then @xmath507 for all @xmath508 , we get that ( [ eqn : directed_as_divergence ] ) implies that @xmath335 for all @xmath509 .",
    "it follows that @xmath510 \\\\   & = & \\max_{q_{x^n||y^{n-1 } } } e[0]=0.\\end{aligned}\\ ] ]      _ proof of lemma [ l_donotloosemuch ] : _ the proof is based on the fact that @xmath511 is uniformly continuous in @xmath376 , namely for any @xmath8 , @xmath512 where @xmath513 as @xmath374 ( the uniform continuity of mutual information is a straightforward result of the uniform continuity of entropy ( * ? ? ?",
    "* theorem 2.7 ) ) .",
    "we have , @xmath514 where the last inequality is due to ( [ e_uniform_cont_mutual ] ) .",
    "we conclude the proof by bounding the last term in ( [ e_bounding_diff ] ) by @xmath515 , which implies that if we let @xmath516 then ( [ e_l_diff ] ) holds .",
    "@xmath517 similarly , we have @xmath518 , and therefore @xmath519    _ proof of lemma [ l_estimate_channel ] : _ the channel @xmath520 is chosen by finding the conditional empirical distribution induced by an input sequence consisting of @xmath521 copies of each symbol of the alphabet @xmath129 .",
    "we estimate the conditional distribution @xmath522 separately for each @xmath523 .",
    "we insert @xmath524 for @xmath525 uses of the channel and we estimate the channel distribution when the input is @xmath524 as the type of the output which is denoted as @xmath526 . from sanov s theorem ( cf .",
    "* theorem 12.4.1 ) ) we have that the probability that type @xmath526 will be at @xmath380-distance larger than @xmath527 from @xmath522 is upper bounded by @xmath528 where @xmath529 denotes the divergence between the two distributions . using pinsker s inequality (",
    "* lemma 12.6.1 ) we have that @xmath530 and therefore , @xmath531 the term @xmath532 goes to zero as @xmath38 goes to infinity for @xmath533 and therefore , for any @xmath534 we can find an @xmath38 such that @xmath535 .",
    "finally we have , @xmath536 where the inequality on the right is due to the union bound .",
    "the authors would like to thank their advisors - anthony ephremides and tsachy weissman - as well as prakash narayan for useful discussions on this topic and andrea goldsmith for organizing the roundtable research discussion at isit06 which led to the conception of this work .",
    "h.  h. permuter , t.  weissman , and a.  j. goldsmith , `` finite state channels with time - invariant deterministic feedback , '' sep 2006 , submitted to ieee trans . inform . theory .",
    "availble at http://arxiv.org/abs/cs/0608070v1 ."
  ],
  "abstract_text": [
    "<S> in this work we find the capacity of a compound finite - state channel with time - invariant deterministic feedback . </S>",
    "<S> the model we consider involves the use of fixed length block codes . </S>",
    "<S> our achievability result includes a proof of the existence of a universal decoder for the family of finite - state channels with feedback . as a consequence of our capacity result </S>",
    "<S> , we show that feedback does not increase the capacity of the compound gilbert - elliot channel . </S>",
    "<S> additionally , we show that for a stationary and uniformly ergodic markovian channel , if the compound channel capacity is zero without feedback then it is zero with feedback </S>",
    "<S> . finally , we use our result on the finite - state channel to show that the feedback capacity of the memoryless compound channel is given by @xmath0 .    </S>",
    "<S> compound channel , feedback capacity , finite state channel , directed information , causal conditioning probability , gilbert - elliot channel , universal decoder , code - trees , types of code - trees , sanov s theorem , pinsker s inequality </S>"
  ]
}