{
  "article_text": [
    "an important scientific problem in applied sciences is to forecast some quantity of interest of dynamical systems that exhibit multiscale behavior .",
    "traditional approaches ( see e.g. , review paper @xcite ) often assume some knowledge about the underlying dynamics and proceed by deriving an effective equation for a set of preselected variables . instead of working with the trajectories associated with the full solutions ,",
    "one is interested in a reduced model in which only the quantities of interest are involved .",
    "these quantities of interest are generally referred to as the _ coarse - grained _ variables . in the case",
    "when the dynamics of the coarse - grained variables is of primary interest , the effective model provides an efficient means to simulate directly the coarse - grained variables , without having to keep track of the remaining degrees of freedom .",
    "an elegant framework for deriving the effective model is the mori - zwanzig projection @xcite , which has recently become an extremely important tool to simplify complex dynamical systems @xcite . in particular , this derivation led to a set of generalized langevin equation ( gles ) , a typical result of the mori - zwanzig procedure .",
    "a notable feature of the gle is a memory term which represents the history - dependence of the effective dynamics , along with a random noise term , which incorporates the influence of the remaining degrees of freedom .",
    "unfortunately , solving the resulting gle still remains as a challenge .",
    "for example , the memory function has been expressed as an infinite series @xcite , and it may exhibit very slow decay .",
    "the implication is that a long history of the solution has to be kept in order to evaluate the integral in the gle .",
    "the evaluation of the integral has to be done at every time step , which adds great complexity to the entire computation .",
    "furthermore , incorporating the random noise term is not straightforward .",
    "a simple approach proposed by @xcite is to approximate the memory kernel in the gle model with a delta function ( but with a carefully chosen damping parameter ) .",
    "this certainly introduces additional modeling error that is difficult to quantify . in problems that arise from biological systems ,",
    "the memory function in the gle can be computed by matching the auto - correlation function of the coarse - grained variables .",
    "for instance , for the gles derived from newton s equations of motion in classical mechanics , one can derive an integral equation for the memory function @xcite .",
    "but this approach requires the computation of the velocity correlation function for the _ full model _ , which clearly is a challenge .",
    "furthermore , the solution procedure for the integral equation is often not reliable .",
    "another approach to approximate the gle is by using an extended markovian system , which can be done using a projection to the krylov subspace approximation @xcite .",
    "this approach , however , requires the knowledge of the full model , especially the interaction among all the degrees of freedom .",
    "but this approach suggests that the full gle models with strong memory effects can be approximated by an extended system with a few auxiliary variables and this key result motivates the present work .",
    "the main idea of the present work is to apply a rational approximation to the kernel function in the gle and a colored noise approximation to the orthogonal dynamics in the gle such that the resulting parametric model is markovian .",
    "we subsequently use the stability conditions established in @xcite as guidelines to ensure non blow - up solutions in the resulting models . rather than deriving the explicit dependence of the parameters in the resulting markovian models in terms of the true solutions ( and/or the parameters in the original dynamics ) , we estimate these parameters by solving an inverse problem , filtering partially observed noisy measurement of the dynamics .",
    "this approach is often useful when ( a ) there is a large amount of training data , e.g. , from experimental observation of part of the system ; ( b ) the explicit form of the gles is difficult to obtain ; ( c ) we do nt have access to the exact solutions of the full dynamical systems .",
    "computationally , since the resulting model is markovian , we do nt need to explicitly compute the memory terms and we do nt need to store the solution history . more importantly ,",
    "compared to direct numerical approximation of the integro - differential equations associated with the gle model , solving the reduced parametric system requires much less computational cost .",
    "we will demonstrate our modeling approach on the nonlinear schrdinger equation ( nls ) , which finds many applications in various areas of applied physics .",
    "of our particular interest is the statistical - mechanics aspects , which has been well studied theoretically @xcite .",
    "our goal is to predict the equilibrium statistical behavior of the low - order wave numbers .",
    "we should stress out that developing reduced models for the nls equations is highly nontrivial in the following sense . since the solutions of nls equation exhibit strong correlation time with nontrivial autocorrelation function , the memory feedback from the unresolved scales is non - negligible and need to be appropriately accounted .",
    "moreover , the equilibrium statistics of the solutions are highly non - gaussian with bimodal distribution",
    ". we will proceed by applying a rational approximation and a colored noise approximation , subsequently , to the kernel functions and orthogonal dynamics of a gle , derived by chorin and coworkers @xcite .",
    "subsequently , we estimate the parameters of the resulting model with an adaptive parameter estimation scheme that is recently developed in @xcite .",
    "we will then validate the forecasting skill by comparing moments up to order four , a two - time correlation function statistics , and marginal densities of the coarse - grained variables .",
    "the remaining part of the paper is organized as follows . in section  [ sec2 ] ,",
    "we state the problem and provide a short review of the gle deduced by chorin and coworkers  @xcite . in section  [ sec3 ] , we construct the parametric models . the procedure for the parametric estimation method in @xcite",
    "is formally described in section  [ sec4 ] .",
    "to be self - contained , we include a pseudo - algorithm in the appendix . in sections  [ sec5]-[sec6 ] , numerical results are then presented to demonstrate the effectiveness of the reduced models .",
    "we close this paper with a short summary and discussion in section  [ sec7 ] .",
    "we consider the nonlinear schrdinger equation ( nls ) , @xmath0 in one space dimension . for simplicity , we apply a periodic boundary conditions on a non - dimensionalized domain @xmath1 $ ] . here",
    ", the solutions of can be described by the fourier series , @xmath2 this turns the pde into a set of odes for the fourier modes , @xmath3 with dispersion relation given by , @xmath4    of particular importance to the statistical mechanics interpretation of is the hamiltonian structure of the system , with the hamiltonian given by , @xmath5 where , @xmath6 with this hamiltonian , we can rewrite as follows , @xmath7 numerically , we can simulate the solutions of with pseudo - spectral methods , e.g. @xcite , of for finite wave numbers , @xmath8 .",
    "the initial condition can be prepared using a monte - carlo algorithm .",
    "we assume that the resulting solutions are the underlying dynamics .    in this paper , we are interested to construct a low - dimensional parametric model to predict low - frequency modes of , given noisy observations of the corresponding modes at discrete - times .",
    "namely , @xmath9 where @xmath10 denotes the upper bound of the observed / resolved modes that are much smaller than the dimensionality of the underlying dynamics @xmath11 , @xmath12 . in ,",
    "the noises @xmath13 are i.i.d .",
    "gaussian with mean zero and unknown error covariance , @xmath14 . to achieve this goal",
    ", our strategy is to exhaust our physical knowledge of the model to deduce an appropriate ansatz for the parametric model and then apply a recently developed , adaptive ensemble kalman filter based , parameter estimation method @xcite to specify the parameters in the corresponding model as well as the observation noise covariance , @xmath14 .",
    "before we discuss our main strategy , we briefly review a classical dimensional - reduction mori - zwanzig formalism @xcite , which underpins the choice of ansatz for our parametric models in the remaining of this section .",
    "a general framework for reducing the dimension associated with a complex dynamical system is the mori - zwanzig projection formalism @xcite , which was originally developed to deal with non - equilibrium processes in statistical mechanics .",
    "this approach relies on a projection operator , denoted by @xmath15 , which separates out the quantities of interest and identifies terms of different nature .",
    "in particular , for a system of initial value problem in the form , @xmath16 and an arbitrary reduced quantity , @xmath17 , which is a function of @xmath18 , the mori - zwanzig procedure yields an exact equation for @xmath17 @xcite , @xmath19 where the first term in usually represents the reversible part of the dynamics and it represents the  markovian \" term . here , the differential operator @xmath20 corresponds to the generator of the dynamical system in and it is defined with respect to initial condition @xmath21 as follows , @xmath22 and we use semigroup notation @xmath23 to denote the evolution operator that maps the solutions forward in time as follows , @xmath24 .",
    "the second term depends on @xmath17 at all times between @xmath25 and @xmath26 so it incorporates the _ memory effect _ as a result of coarse - graining , and it dictates a strong coupling with the remaining degrees of freedom through a memory kernel , @xmath27 where @xmath28 the term in is referred to as the _ orthogonal dynamics _ and if the initial condition @xmath21 is random , then @xmath29 is a stochastic forcing .",
    "equation   is often called a _ generalized langevin equation _ ( gle ) .",
    "the most appealing aspect of the gle in is that it is _",
    "exact_. however , solving the gle in directly is not much simpler than solving the full system in since one has to estimate the orthogonal dynamics in and the memory kernel function in .",
    "we should point out that the gle for the ode in is nonunique since there are different choices for the projection operator @xmath15 .",
    "for example , in the work of mori @xcite , an orthogonal projection is employed , which is often appropriate when the problem can be formulated in a hilbert space . for the nls equation in , chorin   et",
    "al @xcite used a projection operator that is the conditional expectation with respect to the canonical ensemble @xmath30 to deduce an effective equation for selected ( low - frequency ) fourier modes , @xmath31 .",
    "this choice is motivated by the statistical mechanics aspect of the nls @xcite . since the calculation is usually quite cumbersome",
    ", an expansion around the gaussian distribution @xmath32 was introduced , which is appropriate for systems at low temperature , @xmath33 to see this , one can introduce a change of variables in the gibbs distribution , @xmath34 as a result , the distribution can be written as @xmath35 at low temperature when @xmath36 the distribution is approximately gaussian .",
    "furthermore , higher order terms in the equation are much less important , since statistically , @xmath37 is of the order @xmath38    in the simplest case when @xmath39 only the zeroth fourier mode is retained and the effective equation takes the form of @xcite , @xmath40 where @xmath41 is a positive constant , and @xmath42 and @xmath43 are complex valued kernel functions with complicated expressions @xcite ( they are written as infinite series ) .",
    "furthermore , in solving , the history of the solution has to be stored and the integral has to be approximated by appropriate quadrature formulas _ at each step _ of the time integration .",
    "all these operations add up to significant computational cost and it is also unclear how the approximation by @xmath44 affects the modeling error .    rather than computing these kernels directly ,",
    "we take a different approach here . in particular , we will model the memory terms in and the stochastic process @xmath45 with an appropriate ansatz of parametric equations .",
    "subsequently , we estimate the corresponding parameters from noisy observations such that the resulting markovian model gives accurate equilibrium statistical estimates for the selected fourier modes that are retained : @xmath31 .",
    "here we discuss our approach in approximating the gles using parametric models that involve explicitly few parameters .",
    "these approximations are constructed in such a way that the approximate model can be re - written into a memory - less form , leading to a markovian dynamics to facilitate the numerical implementation . to clarify the exposition",
    ", we first discuss the case where we only retain the zeroth mode .",
    "we assume the form of the gle , but we approximate the memory terms using rational functions so that parameters can be introduced .",
    "we then provide the resulting parametric form for the more general case which retain more fourier modes , @xmath46 .",
    "now we will construct an ansatz for approximating the first memory term in .",
    "to this end , we introduce a parameter @xmath48 and an auxiliary function @xmath49 to denote the first memory term , @xmath50 and our plan is to find a set of differential equations for solving @xmath49 .",
    "first , taking the laplace transform on , we arrive at , @xmath51 where we denote @xmath52 to be the laplace transform ( defined on frequency domain @xmath53 ) of any function @xmath54 that is locally integrable on @xmath55 .",
    "the key idea is to approximate the kernel function , @xmath56 , using a rational function , @xmath57 where @xmath58 is the second parameter to be determined .",
    "this particular form of the rational function is chosen to ensure the stability of the resulting parametric model , as we will explain below . in principle , these two coefficients , @xmath59 and @xmath60 , can be determined with pad approximations ( or more general rational approximations ) of the exact kernel . in model reduction problems ,",
    "this is known as the _ moment matching _",
    "procedure @xcite , where for linear dynamical systems , these parameters can be explicitly connected to properties of the original problem .",
    "the main departure of the current approach from those existing methods is mainly that we leave them as parameters and later infer them with a filtering procedure , learning from partially observed noisy time series .    converting and back to the time domain",
    ", we find that @xmath49 satisfies a differential equation , @xmath61 for low temperature case , one can neglect the second memory term in that involves @xmath62 since this higher - order term is negligible as explained before in section  [ morizwanzig ] . with this perspective , we propose the following parametric model , @xmath63 where we have introduced two additional non - negative parameters @xmath41 and @xmath64 . in principle , these parameters can be determined from the mori - zwanzig reduction procedure , which might involve lengthy calculations . however , since the derivation in @xcite employed further approximations using the ( conditional ) gaussian distribution , the resulting values for @xmath41 and @xmath64 may not be optimal . therefore , we kept the form of the equations suggested by the mori - zwanzig formalism , but leave @xmath41 and @xmath64 as additional parameters , which we will determine using a filtering procedure .",
    "we have also introduced a white noise @xmath65 . when the second equation is analytically solved and subsequently substituted into the first equation , this white noise will become a colored - noise approximation to the random process @xmath29 .",
    "when both @xmath59 and @xmath60 are real - valued parameters , the second equation represents an ornstein - uhlenbeck process @xcite .",
    "but here @xmath49 is a more general gaussian process .",
    "we should also point out that the reduced system of parametric equations in is a special case of the physics constrained nonlinear regression model described in @xcite in the following sense . in compact form , we can write as a system of four - dimensional real valued sdes , @xmath66 \\,dt + \\sigma dw,\\label{sde}\\end{aligned}\\ ] ] where we define @xmath67 , and @xmath68 is a standard two - dimensional wiener process .",
    "in addition , we define @xmath69 and @xmath70 such that , @xmath71    one of the main results in @xcite states that if the fokker - planck operator of the sde in is hypoelliptic , and suppose also that all eigenvalues of @xmath72 have negative real part and there exists an appropriate norm under which the inner product @xmath73 , then solutions of is geometrically ergodic . for our parametric model above , the stability condition is met when @xmath74 and the dissipation of the energy of the nonlinear terms is satisfied under an inner product with respect to @xmath75 .",
    "we should note that one can repeat the same calculation for approximating the second memory term in but the resulting nonlinear terms will not conserve energy and can be unstable ( see appendix a ) . based on this consideration",
    ", we ignore approximating the second memory terms in this paper . instead , we will only consider the parametric model in which guarantees non - blowup solutions .",
    "a simple extension of the two - parameter scalar parametric approximation model in is to allow @xmath60 and @xmath49 to be vectors , here denoted by @xmath76 and @xmath77 , respectively , to emphasize the multi - dimensional representation .",
    "this leads to an extended model , @xmath78 for instance , the matrices @xmath72 and @xmath79 can be chosen in the following form , @xmath80 , \\quad    \\sigma    =     \\left [    \\begin{array}{cc }       \\sigma_1 & 0\\\\       0 &",
    "\\sigma_2       \\end{array }       \\right].\\ ] ] the corresponding extended model has a parameter space of dimension 10 .",
    "we will see that this model will give improved estimates compared to in higher temperature case .",
    "similar extensions can be found by increasing the dimension of @xmath72 , @xmath76 and @xmath77 .      in general",
    ", we can keep those modes @xmath81 with @xmath31 , and @xmath10 indicates the range of the modes to be kept . in this case",
    ", we first define the coarse - grained energy , @xmath82 the model _ without _ memory can be written as follows , @xmath83 motivated by and the mori - zwanzig procedure @xcite , let us consider a parametric model as follows , @xmath84 the auxiliary functions are assumed to be zero initially , i.e. , @xmath85 since they are introduced to approximate the memory terms .    to see the energy dissipation mechanism , we can define the lyapunov functional , @xmath86 direct calculations yield , @xmath87 consequently , we require that @xmath88 to guarantee non - blow up solutions .",
    "in this section , we describe formally how to estimate the parameters of the reduced models ( e.g. , , , or ) , given noisy observations @xmath89 of @xmath90 , where @xmath91 are solutions of the full system in for @xmath92 and @xmath93 at discrete time step @xmath94 : @xmath95 with an unknown observation error covariance @xmath14 , where @xmath14 is an @xmath96 diagonal matrix with @xmath81-th diagonal component , @xmath97 . to simplify the discussion ,",
    "let us classify the parameters in our reduced model to two types .",
    "we refer to the parameters in the deterministic terms in the reduced models as the  deterministic parameters \" , @xmath98 , and the amplitude of the stochastic forcings as the  stochastic parameters \" ,",
    "for example , in , the deterministic and stochastic parameters are , @xmath100 , and @xmath101 , respectively .",
    "we split the parameters into two types because the algorithm that we use to estimate @xmath98 is simply a standard augmentation method while the algorithm to estimate @xmath99 is the adaptive method for estimating covariances which preserves the positivity of @xmath102 and @xmath14 .",
    "let us also define vector @xmath103 to simplify the notation below .",
    "the main idea of the parameterization method is to apply bayes theorem to obtain a posterior distribution of the augmented state and parameters at each time step @xmath94 when observations become available , @xmath104 where @xmath105 denotes the prior distribution of the augmented state and parameters at time @xmath94 and @xmath106 denotes the likelihood function of the augmented state and parameters , corresponding to the observation model in , that is , @xmath107 . the parameterization method can be formally described as follows : since @xmath108 by definition of the conditional distribution , we can rewrite as follows : @xmath109 where we use another bayes theorem , @xmath110 , to obtain . here",
    ", the first step in the filtering algorithm is to estimate @xmath111 by applying bayes theorem to the last two components of .",
    "subsequently , we implement the bayes theorem one more time in to obtain the posterior distribution of the augmented @xmath112 .    to avoid unobservability of the stochastic parameters due to sparse observations with dimension less than the number of stochastic parameters , @xmath99 , in our implementation , we include information from past observations up to lag @xmath113 . at each time step @xmath94 , instead of solving -",
    ", we formally solve @xmath114 where , similar as before , the first step is to estimate @xmath115 by applying bayes theorem to the last two components of .",
    "subsequently , we implement the bayes theorem one more time in to obtain the posterior distribution of the augmented @xmath112 .    in our numerical implementation",
    ", we use the method in @xcite which uses gaussian approximation to solve these inverse problems . at time @xmath116",
    ", we assume that we have prior ensemble estimates of @xmath117 at times @xmath118 and the associated observations at these times .",
    "we assume that the deterministic parameter is persistence , that is , @xmath119 .",
    "the first step is to apply etkf method @xcite to obtain posterior ensemble estimates of the augmented variable @xmath120 , incorporating observations in , which is a gaussian approximation of @xmath121 . to start the algorithm",
    ", one can just repeat this etkf algorithm @xmath122-times to obtain the prior ensemble estimates at time @xmath123 with fixed parameters @xmath124 .",
    "now , at @xmath116 , we start the secondary filter to update @xmath99 .",
    "the key idea of the secondary filter is to view the posterior density function @xmath115 as a likelihood function of @xmath99",
    ". notice that while this posterior density is gaussian with respect to variables @xmath125 , its dependence on @xmath99 can be described non - uniquely ( for example , see @xcite ) . here",
    ", we will adopt the estimation method of @xcite that is based on belanger s formulation @xcite with a likelihood function corresponding to the following pseudo - observation model , @xmath126 here , components of @xmath127 are the product of the forecast error estimates in the observation space ( which are also known as innovations ) , @xmath128 where @xmath129 denotes the mean prior estimate that is empirically estimated with an ensemble average . in , the observation operator @xmath130 and the noise covariance matrix @xmath131 are functions of @xmath132 and @xmath98 and they will be constructed recursively .",
    "we should also note that in our implementation , @xmath131 is approximated under a gaussian assumption ( see appendix b below for detail ) . with the pseudo - observation model in",
    ", a secondary kalman filter is implemented @xmath122-times to sequentially update the posterior mean and covariance estimate of @xmath99 , accounting for pseudo - observations @xmath133 one at a time .",
    "to be self - contained , we provide a pseudo algorithm of this method in appendix b below .",
    "we should note that there are other methods to approximate the secondary bayes update in that use different observation model in and do not use kalman update ( see e.g. , @xcite ) .",
    "in this section , we present the results from three numerical tests , where the parametric models for @xmath47 ( equations and ) are estimated and further assessed .",
    "we assume that the observation time interval @xmath134 .",
    "the time series , consisting of @xmath135 observations , is generated by using the strang s splitting method in time , which has been implemented in @xcite for the nls equation .",
    "the data is generated from the solutions of the nls in the fourier domain , with @xmath136 .",
    "we then perform a parameter estimation method with an ensemble kalman filter based method @xcite , which was described in the previous section .",
    "the forecast is generated with the 4th order runge - kutta method with step size @xmath137    following the estimation , we verify the forecasting skill of the reduced models with the estimated parameter set as follows .",
    "we take the estimated parameters and run the reduced models forward in time for a sufficiently long time . then , based on the long trajectory , we compare the low - order statistics up to order - four and the time correlation function . the auto - correlation is computed based on the wiener - khinchin theorem .",
    "namely , we take the fourier transform of the data , @xmath138 and compute the power spectrum , @xmath139 the correlation function is then given by the inverse fourier transform of the power spectrum .",
    "this procedure is often more efficient than the direct approach , i.e. , @xmath140      we first consider a low temperature case with @xmath142 ( @xmath143 ) , and estimate the parameters in the model for @xmath47 .",
    "this model contains two ( complex ) deterministic parameters @xmath59 and @xmath60 , together with two real valued parameters @xmath41 and @xmath64 .",
    "we set the observation noise error with variance , @xmath144 . in fig .",
    "[ param_fig1 ] , we show the estimated solutions along with the observed values during the estimation procedure .",
    "very good agreement with the true values has been found .",
    "we also monitor the predicted values of the deterministic parameters , and the history is presented in fig .",
    "[ param_fig2 ] .",
    "meanwhile , the stochastic parameter @xmath145 has settled to a constant value , and the variance of the observation noise has been correctly predicted , as indicated in fig .",
    "[ param_fig3 ] .",
    "another observation is that the predicted values of the parameters @xmath41 and @xmath64 are @xmath146 and @xmath147 , which are quite different from the values calculated from the mori - zwanzig s projection procedure ( @xmath148 and @xmath149 , respectively ) corresponding to an approximate gaussian measure , @xmath150 . we should point out that if we fix these two parameters to be those from the mori - zwanzig projection and run the filtering procedure to estimate the remaining parameters , @xmath151 , the resulting estimates are completely innaccurate",
    "this suggests that while the perturbation approach @xcite suggests the explicit forms of the reduced model , it is more natural to adaptively estimate all the parameters which reconfirms the results in @xcite .",
    "moreover , in general , there can be non - unique parameters that provide the same equilibrium statistics ( for e.g. , see proposition 1(d ) in @xcite ) .     and @xmath49 for the parametric model .",
    "]    , @xmath60 , @xmath41 and @xmath64.,scaledwidth=60.0% ]    . ]",
    "next we evaluate the forecasting skill and check the accuracy of the climatological statistics of the resulting solution @xmath47 .",
    "we observe from fig .",
    "[ verify1 ] that the qualitative behavior of the path - wise solutions is well captured .",
    "the solution @xmath49 , which was introduced to replace the memory term , exhibits much faster oscillations .",
    "further , from fig . [ verify2 ] , we observe that the distribution and the time correlation of the true solution are accurately predicted",
    ". we also report the accuracy of the first four moment estimates in table  [ tab0 ] , where the errors are on the order of @xmath152 .        .",
    "as comparison , the statistics of the true solution is also shown.,title=\"fig : \" ] . as comparison ,",
    "the statistics of the true solution is also shown.,title=\"fig : \" ]      we now turn to observations obtained from a higher temperature simulation with @xmath154 ( @xmath155 ) . in this case ,",
    "the time series for @xmath47 exhibits faster oscillations and slightly larger amplitude .",
    "we also observe the amplitude and frequency of the oscillations are somewhat sensitive to the initial conditions since the system is not ergodic .",
    "based on the data , we estimate the first reduced model , and then check the statistics of the resulting model . due to the higher temperature ,",
    "the variance of @xmath47 is much bigger .",
    "therefore , we set a higher value for observation noise variance , @xmath156 .",
    "we see from fig .",
    "[ verify01-rk4 ] that the accuracy is not as satisfactory as in the previous case . in particular , the peaks of the marginal density are not well captured and the variance is underestimated ( see table  [ tab0 ] ) although the other statistics are accurately estimated .",
    "also , the correlation function is inaccurate beyond the first oscillation .     using the first reduced model .,title=\"fig : \" ]   using the first reduced model .,title=\"fig : \" ]    as comparison , we consider the next parametric model , represented by the equation , which contains 4 complex deterministic parameters and two real ones . with the parameters obtained from the filtering procedure , we perform a similar statistical verification .",
    "the results , including the histogram and the time correlation functions , are illustrated in fig .",
    "[ verify01-rk6d ] .",
    "it is clear that the extension has offered improved accuracy in the resulting histogram .",
    "table [ tab0 ] summarized the statistics ( four moments ) of @xmath47 obtained from the three tests , compared to the true values .",
    "we notice that for the higher temperature case , the model yields much better estimates for the second moment",
    ". however , the estimated correlation function is only slightly improved up to time @xmath157 relative to the result from the model in .     using the reduced model .,title=\"fig : \" ]   using the reduced model .,title=\"fig : \" ]    .comparison of the equilibrium statistics of @xmath158 for the three tests . [ cols=\"^,^,^,^,^,^,^ \" , ]     [ table2 ]",
    "this paper presented a modeling approach that blends some physical knowledge about the underlying dynamics and the availability of training data to predict low - frequency modes of the nls equation .",
    "in particular , we use the mori - zwanzig formalism as guidelines to construct effective parametric models and apply an adaptive ensemble kalman filter to estimate the parameters .",
    "the novelty here is that we approximate the memory term and the orthogonalized dynamics of a generalized langevin equation obtained from the mori - zwanzig expansion with a rational function and a colored noise , respectively .",
    "it turns out that the resulting parametric model here is an example of the physics constrained nonlinear regression modeling approaches proposed in @xcite .",
    "this serendipity allows one to use the stability conditions established in @xcite to ensure non - blow up solutions of the resulting parametric model .",
    "compared to the full gle , these models have advantages in practical implementations because they do not involve memory .",
    "the climatological forecasting skill of the proposed parametric model was verified in terms of the first four moments , marginal densities , and correlation functions for various temperatures .",
    "for low temperature case , high predictive skill of fourier mode @xmath47 is obtained with a reduced model with a scalar parameterization for the memory term .",
    "for higher temperature case where the scale - gap is smaller than the low temperature case , the problem becomes more challenging .",
    "in this situation , we showed that one can improve the estimates either with a two - dimensional parameterization for the memory term in or with fitting more modes into a model with more retained modes in .    with the encouraging results in this paper",
    ", we plan to apply this modeling strategy on other applications such as on coarse - grained biomolecular models @xcite in our future research . in general problems , however , the success of this modeling approach will depend mostly on the choice of the ansatz for modeling the memory terms . as it has been theoretically established in @xcite , if the ansatz is adequate , then it is possible to obtain , both , accurate climatological statistical forecasting and optimal filtering .",
    "our nls example in this paper empirically suggested that our ansatz is optimal in this case .",
    "other potential issue is in the parameter estimation strategy which can be expensive when more observations are included . while many cheaper parameterization methods are available ( such as regression - based or maximum likelihood - based algorithms ) , these methods are often inferior to the adaptive method applied in the present work even when adequate ansatz is used as shown in @xcite .",
    "therefore , improving the numerical efficiency of the adaptive parameter estimation scheme that we used here @xcite or its variant ( see e.g. , @xcite ) will be the key for successful applications in more complex problems .    the research of jh is partially supported by the the onr muri grant n00014 - 12 - 1 - 0912 , onr grant n00014 - 13 - 1 - 0797 , and the nsf grant dms-1317919 .",
    "37ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]  + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * , ( ) @noop * * ,   ( ) in  @noop _ _  ( ,  )  pp .   @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) ,   @noop ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop ( )",
    "mathematically , one can also include the second memory term using the similar rational approximation for the high temperature case when this term is not negligible . denoting the other kernel function as , @xmath159 where @xmath160 is an additional parameter and approximate the laplace transform of the kernel function , @xmath161 where we assume that @xmath162 and @xmath160 are real valued parameters .",
    "the function @xmath163 follows the differential equation , @xmath164 adding white noises into , we obtain a parametric model given by , @xmath165 where we have added an equation of @xmath166 to represent the second memory term in .",
    "the problem here is that the nonlinear terms do not conserve energy since we can not control the nonlinear terms in the equation for @xmath166 unless for @xmath167 .",
    "we suspect that there probably exists different approximations ( other than the rational functions ) for these kernel functions that give stable parametric models and these are beyond the scope of this paper .",
    "based on this consideration , we do not implement the parametric model in in this paper .",
    "this appendix provides a pseudo - algorithm of the estimation method proposed in @xcite .",
    "consider the following filtering problem , @xmath168 where @xmath169 denote the augmented state and deterministic parameters . here",
    ", we assume a persistence model for the deterministic parameters , @xmath170 .",
    "we attempt to estimate @xmath171 as well as @xmath172 and @xmath14 , on - the - fly . essentially , @xmath172 and @xmath14 are the stochastic parameters through the following relation , @xmath173 and our aim is to estimate @xmath174 , @xmath175 . for the model in ,",
    "the augmented state - parameters are @xmath176 , the number of stochastic parameters are @xmath177 , where @xmath178 , and @xmath179    starting with time index @xmath180 , we provide an ensemble of prior statistical estimates , @xmath181 , of size @xmath11 for the primary filter and prior mean @xmath182 and covariance @xmath183 , for the secondary filter .",
    "the primary filter for estimating @xmath184 is described in steps 1 - 3 , while the secondary filter for estimating @xmath99 is described in steps 4 - 9 .    1 .",
    "apply the etkf to obtain the analysis ensemble estimate , @xmath185 .",
    "let s denote the corresponding kalman gain and innovation as follows , @xmath186 where @xmath187 denotes the prior ensemble average .",
    "see @xcite for the detail etkf algorithm .",
    "2 .   propagate each ensemble member with the deterministic part of the model in to obtain , @xmath188 and form the posterior ensemble by adding a gaussian noise , @xmath189 3 .",
    "define an ensemble approximation for the linear tangent model , @xmath190 where each column vectors of @xmath191 and @xmath192 are the deterministic forecast ensemble perturbations and the analysis ensemble perturbations , consecutively . in",
    ", we denote pseudo - inverse by @xmath193 .",
    "4 .   define @xmath194 and @xmath195 .",
    "5 .   for each @xmath196 , construct an observation operator for @xmath197 , starting with @xmath198 , let @xmath199 6 .   for each @xmath196",
    ", construct an observation operator for @xmath200 , where @xmath201 .",
    "set @xmath202 7 .",
    "approximate @xmath203 .",
    "suppose if @xmath204 is @xmath10-dimensional .",
    "define @xmath205 8 .",
    "consider the pseudo observation model for the secondary filter , @xmath206 where in our case , @xmath207 , @xmath208 , @xmath209 , and for each pair of indices @xmath210 , construct @xmath211 note that @xmath68 is constructed , assuming gaussian and independent noises , @xmath212 .",
    "components of matrix @xmath68 in can be rewritten as follows , @xmath213 9 .",
    "perform a secondary kalman filter @xmath122-times to sequentially update @xmath214 with observation models in one at the time , assuming that the dynamics of these parameters are persistence , @xmath215 .",
    "now we can repeat step  1 above for the new assimilation time ."
  ],
  "abstract_text": [
    "<S> reduced models for the ( defocusing ) nonlinear schrdinger equation are developed . </S>",
    "<S> in particular , we develop reduced models that only involve the low - frequency modes given noisy observations of these modes . the ansatz of the reduced parametric models are obtained by employing a rational approximation and a colored noise approximation , respectively , on the memory terms and the random noise of a generalized langevin equation that is derived from the standard mori - zwanzig formalism . </S>",
    "<S> the parameters in the resulting reduced models are inferred from noisy observations with a recently developed ensemble kalman filter - based parameterization method . </S>",
    "<S> the forecasting skill across different temperature regimes are verified by comparing the moments up to order four , a two - time correlation function statistics , and marginal densities of the coarse - grained variables . </S>"
  ]
}