{
  "article_text": [
    "the problem of ensemble refinement @xcite becomes increasingly important as structural biology enters a new era in which dynamic and partially disordered biomolecular structures come into focus.@xcite such systems play central roles in biology , both in functional cellular processes ranging from signal transduction to the formation of large cellular structures , and in disease , including neurodegenerative diseases such as parkinson s and alzheimer s .",
    "a broad range of methods have been developed to refine models of ( bio)molecular structures against experimental data from x - ray crystallography , nuclear magnetic resonance ( nmr ) spectroscopy , electron microscopy ( em ) , solution x - ray or neutron scattering ( saxs , sans ) , and other methods . by and large , these refinement methods operate under the assumption that a single or a few well ordered structures should account for all the measurements . however , refinement of a single ( or possibly a few ) copies is not appropriate in systems with significant disorder . for unfolded @xcite or intrinsically disordered proteins ( idp),@xcite such as the @xmath0-synuclein peptide involved in parkinson s disease,@xcite we expect that a very broad range of structures is present in solution .",
    "none of these structures may individually satisfy all measurements , and even if one did , it may be highly atypical . instead",
    ", most observables accessible to experiment report on averages over the entire ensemble of structures , and as such only the appropriate average over a model ensemble should match the experiment .     of a dihedral angle @xmath1 from single - copy refinement [ blue ; eqs .",
    "( [ eq:1]-[eq:3 ] ) ; every member of the ensemble is expected to satisfy the measurement individually ] and ensemble refinement [ green ; eq .",
    "( [ eq:34 ] ) ; the ensemble average is expected to satisfy the measurement ] . the prior or reference distribution ( magenta ) used in the refinements is bimodal , i.e. , with two dominant rotamer states .",
    "as indicated by the vertical black line , the observable is @xmath2 for single - copy refinement and @xmath3 for ensemble refinement , with `` experimental '' error @xmath4 in both cases , and @xmath5 .",
    "arrows indicate the changes in the relative weights of the two rotamers in the optimal bayesian ensemble refined distribution .",
    "[ fig:1],width=302 ]    ensemble refinement is a challenging inverse problem in which one aims to characterize the high - dimensional configuration space of a molecular system on the basis of limited experimental information .",
    "it is therefore essential that ensemble refinement methods can properly integrate data from a broad range of experiments @xcite that may report on molecular size and shape ( e.g. , from saxs , sans , or hydrodynamic measurements @xcite ) , the proximity ( e.g. , from cross - links ) or distance between atoms and residues [ e.g. , from fluorescence resonant energy transfer ( fret ) , nmr,@xcite including nuclear overhauser effects ( noe ) , or double electron - electron resonance ( deer ) measurements @xcite ] , the local chemical environment and structure ( e.g. , from nmr chemical shifts @xcite and j - couplings @xcite or x - ray absorption spectroscopy ) , all the way to measures of the global structure ( e.g. , from x - ray crystal diffraction or electron microscopy @xcite ) . taking into account the uncertainties of the different experiments",
    "@xcite is critical for the construction of a properly weighted configurational ensemble .",
    "inverse problems are typically ill - conditioned , i.e. , sensitive to input parameter variations , and underdetermined .",
    "such problems with high sensitivity and low data - to - parameter ratios are usually tackled through regularization , for instance by assuming near - uniform and smooth solutions .",
    "bayesian statistics offers a particularly elegant route for the inference of probabilistic models from data ( see , e.g. , ref . for a general overview and ref . for a pioneering application to biomolecular studies ) . in effect , the assumed prior distributions of the model parameters serve as regularizing factors , @xmath6 written as a proportionality without the normalizing factor .",
    "@xmath7 is the posterior distribution of the model , and @xmath8 is the prior that expresses our expectations on the model and its parameters in the absence of new data .",
    "@xmath9 is the conditional probability of observing the data given the model , which for given data is the likelihood of the model .",
    "consequently , we will in the following refer to @xmath9 as the likelihood function .",
    "importantly , in the absence of new data ( or for non - informative data ) , one simply recovers the prior .",
    "the importance of ensemble refinement is best illustrated by a simple example that anticipates some of the theoretical developments in this work .",
    "figure  [ fig:1 ] contrasts the stark differences in the results for single - copy and ensemble refinements of a simple model system with a prior or reference distribution with two dominant rotamers . in single - copy refinement",
    ", we determine how well each dihedral angle @xmath1 individually agrees with the observation @xmath10 .",
    "this posterior probability @xmath11 is concentrated in a sharp peak around the target value . by contrast ,",
    "in ensemble refinement we seek a probability density @xmath12 that is consistent with the observed average @xmath13 .",
    "this @xmath14 retains the character of the reference distribution that reflects the underlying physics , while redistributing some population from one rotamer to the other .    here",
    ", we will describe both formal and practical approaches toward inferring ensemble distributions from diverse data .",
    "we will formulate the ensemble refinement problem first formally in a bayesian framework in which the posterior is a functional that quantifies the relative probability of different ensemble probability densities @xmath15 for configurations @xmath16 .",
    "experimental uncertainties @xcite are taken into account from the outset , which allows us to combine data from a variety of measurements .",
    "we then study algorithms to realize bayesian ensemble refinement in practice .",
    "first , we will describe a method with which existing ensembles can be reweighted to match experiment . by variational maximization of the bayesian posterior functional over the ensemble probability densities",
    "@xmath15 , we will derive an optimal bayesian ensemble density @xmath17 , eq .",
    "( [ eq:34 ] ) , for the continuous case and eq .",
    "( [ eq:18 ] ) for the discrete case .",
    "applied to sub - ensembles drawn according to the prior , this reweighting method turns out to be equivalent to the maximum entropy refinement procedure in the ensemble refinement of saxs ( eros ) method @xcite ( which is different from the `` ensemble refinement with orientational restraints '' method with the same acronym @xcite ) . in the limit of infinite sample size",
    ", the reweighting method converges to the optimal bayesian ensemble refinement .",
    "then , we will describe a bayesian replica ensemble refinement method to perform ensemble refinement on the fly by running molecular simulations of identical copies of the system with a bias on the averages calculated over these replicas .",
    "if the biasing potential is proportional to chi - squared ( as twice the negative log - likelihood for gaussian errors ) scaled by the number of replicas @xmath18 [ see eq .",
    "( [ eq:21 ] ) ] , one recovers the optimal bayesian ensemble refinement [ eq .",
    "( [ eq:34 ] ) ] in the limit of an infinite number of replicas . at the other extreme , in the limit of a single replica , the common - property refinement [ eq .",
    "( [ eq:7 ] ) ] is recovered , in which every member of the ensemble is expected to satisfy the measurements individually , not just in the ensemble average . to speed up the convergence to the optimal bayesian distribution with increasing number of replicas @xmath18 , we show how eros and replica refinement ( as well as other ensemble - biased simulation methods ) can be combined with the help of free - energy reweighting methods , resulting in the `` bayesian inference of ensembles '' ( bioen ) method .",
    "an adaptive algorithm designed to sample directly from the optimal ensemble distribution , without multiple replicas , is presented in an appendix .    to illustrate the formal theory and the practical replica simulation approaches",
    ", we will introduce analytically or numerically tractable models of ensemble refinement .",
    "the solutions obtained for these models allow us to assess the mutual consistency of the methods , and to demonstrate the need for a size - consistent treatment in the bayesian replica ensemble refinement with respect to the number of replicas .",
    "we also sketch how dynamic properties can be integrated in ensemble refinement , albeit approximately , and how the parameter expressing the confidence in the reference ensemble distribution can be chosen .",
    "we conclude by a summary of the main results and a discussion of possible applications , including the optimization of potential energy functions used for molecular simulations .",
    "before venturing into ensemble refinement , we introduce notation and the general framework in the context of the more familiar single - copy refinement . here",
    "one assumes that a single configuration can explain all measured data .",
    "different configurations can then be ranked , in a probabilistic manner , by their respective abilities to do so .    in the following ,",
    "we will use @xmath16 to denote individual configurations . in a typical application to a molecular system",
    ", @xmath16 could be the @xmath19-dimensional vector @xmath20 of the cartesian coordinates @xmath21 of the @xmath22 atoms . in single - copy refinement",
    ", we assume that one would ideally ( i.e. , without error ) measure values @xmath23 of observable @xmath24 , with @xmath25 , for a given configuration @xmath16 .",
    "the actual values observed ( measured ) are @xmath26 . by contrast , in ensemble refinements described below , the measured values of the observables will instead depend on the distribution over the entire configuration space , not just on a single configuration @xmath16 .    using a reference distribution @xmath27 as a prior , in single - copy refinement",
    "we want to construct a posterior @xmath28 in configuration space that ranks configurations @xmath16 by their consistency with both experimental measurements and prior .",
    "the prior @xmath27 could for instance be the boltzmann distribution for a simulation model described by a particular potential energy function @xmath29 , i.e. , @xmath30/\\int d{\\mathbf{x } } ' \\exp[-\\beta u({\\mathbf{x}}')]$ ] at reciprocal temperature @xmath31 with @xmath32 the boltzmann constant and @xmath33 the absolute temperature , or a statistical distribution of conformers of the protein data bank.@xcite the normalized posterior distribution according to eq .",
    "( [ eq:0 ] ) is then @xmath34 where @xmath35 is the likelihood of @xmath16 for data given as a set of @xmath36 measured values , @xmath37 .",
    "the posterior @xmath38 gives the probability density that configuration @xmath16 is the single configuration underlying the data .    in cases",
    "where the statistical errors are gaussian , we define the likelihood function is @xmath39 where @xmath40 ^ 2 } { \\sigma_i^2}.\\ ] ] and @xmath41 is the standard deviation of measurement @xmath24 . for simplicity , we assume in eq .",
    "( [ eq:3 ] ) that the errors in the different measurements @xmath24 are uncorrelated . in the more general case of correlated errors",
    ", one can use @xmath42 where @xmath43 is a vector of deviations , with elements @xmath44 , and @xmath45 is the symmetric covariance matrix of the statistical errors ( where for uncorrelated errors @xmath46 and @xmath47 for @xmath48 ) . note that the measurements @xmath24 can be from different measurements ( say , nmr and single - molecule fret ) or from the same measurement ( say , intensities at different wave vectors in a saxs measurement ) .    in practice , single - copy bayesian refinement",
    "can then be performed by sampling directly from the posterior @xmath28 , e.g. , by running equilibrium simulations with an effective energy function @xmath49 .",
    "alternatively , representative configurations can first be sampled from the reference distribution @xmath27 and then reweighted by the likelihood according to eq .",
    "( [ eq:1 ] ) .      in an alternative bayesian formulation ,",
    "we think of @xmath15 not as a posterior @xmath28 ranking individual configurations @xmath16 with respect to their mutual consistency with prior and data , but as an actual probability density of @xmath16 in configuration space defining an ensemble . as a consequence , prior , likelihood , and",
    "posterior become functionals of the probability density @xmath15 in configuration space .",
    "we note that such `` hyperensembles '' have been studied by crooks as models of nonequilibrium states.@xcite functional approaches are also used in variational bayesian methods.@xcite    to construct a prior in the space of probability densities @xmath15 , with @xmath50 and @xmath51 , we use the kullback - leibler divergence or relative entropy with @xmath27 as reference distribution ( i.e. , @xmath27 no longer _ is _ the prior , but _ defines _ the prior ) .",
    "we note that other measures of the difference between distributions could be used to regularize the bayesian refinement .",
    "the relative entropy provides us with a positive - definite measure of deviation between @xmath15 and the reference distribution @xmath27 . by weighting these deviations exponentially ,",
    "we arrive at a prior functional @xmath52 \\propto \\exp\\left(-\\theta \\int d{\\mathbf{x}}\\ , p({\\mathbf{x}})\\ln\\frac{p({\\mathbf{x}})}{p_0({\\mathbf{x}})}\\right),\\ ] ] with a parameter @xmath53 expressing the level of confidence in the reference ensemble , and therefore in the underlying potential energy surface ( force field ) and the exhaustiveness of our sampling of @xmath27 .",
    "high confidence is expressed through large values of @xmath54 .",
    "the choice of the confidence factor @xmath54 will be discussed in the section on _ practical considerations _ below . here and",
    "in the following , we use a calligraphic font for functionals , and square brackets for their arguments .",
    "the posterior functional then becomes @xmath55 } \\nonumber\\\\&\\propto &    \\exp\\left(-\\theta \\int d{\\mathbf{x}}\\ ,      p({\\mathbf{x}})\\ln\\frac{p({\\mathbf{x}})}{p_0({\\mathbf{x}})}\\right ) { \\cal p}[\\mathrm{data}|p({\\mathbf{x}})].\\end{aligned}\\ ] ] in the following , we will first consider the case where the measured observables are properties common to all configurations before considering the case where the observables are ensemble averages .    [ [ ensemble - refinement - for - properties - common - to - all - configurations . ] ] ensemble refinement for properties common to all configurations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in some cases , ensemble refinement should be used even if the observables are properties of individual configurations . as an example , consider a disulfide bond or other chemical cross - link that is present in essentially all proteins within a system . with respect to other degrees of freedom",
    ", the configurations may be disordered .",
    "such cases require ensemble refinement , but with an experimental restraint that acts on each ensemble member individually .",
    "to quantify deviations from the observations , we use an approximate likelihood functional . for given @xmath15 and gaussian errors",
    "@xmath41 , the probability of the data is proportional to @xmath56 ^ 2/2\\sigma_i^2 ) \\approx \\exp(-\\int d{\\mathbf{x}}\\,p({\\mathbf{x}})\\sum_i[y_i({\\mathbf{x}})-y_i^{{(\\mathrm{obs})}}]^2/2\\sigma_i^2)$ ] , ignoring higher - order fluctuations in the squared errors . with this approximation",
    ", we arrive at @xmath57 = e^{-\\chi^2[p({\\mathbf{x}})]/2},\\ ] ] where @xmath58 = \\sum_i \\int d{\\mathbf{x}}\\,p({\\mathbf{x } } )    \\frac{\\left[y_i({\\mathbf{x } } ) -   y_i^{{(\\mathrm{obs})}}\\right]^2 } { \\sigma_i^2}\\ ] ] is the mean - squared error of the common observables @xmath23 , scaled by @xmath59 .    to make progress , we now determine the normalized probability density @xmath17 that maximizes the posterior functional @xmath60 $ ] .",
    "we define @xmath61 \\equiv -\\ln { \\cal p}[p({\\mathbf{x}})|{\\{y_i^{{(\\mathrm{obs})}}\\ } } ] + \\lambda \\int d{\\mathbf{x}}\\ ,      p({\\mathbf{x } } ) } \\nonumber\\\\    & = & { \\theta\\int d{\\mathbf{x}}\\ , p({\\mathbf{x}})\\ln\\frac{p({\\mathbf{x}})}{p_0({\\mathbf{x}})}}\\\\ & &   + \\sum_i \\int    d{\\mathbf{x}}\\ , p({\\mathbf{x } } ) \\frac{\\left[y_i({\\mathbf{x}})-y_i^{{(\\mathrm{obs})}}\\right]^2 }    { 2\\sigma_i^2 }    + \\lambda \\int d{\\mathbf{x}}\\ , p({\\mathbf{x } } ) , \\nonumber\\end{aligned}\\ ] ] where the lagrange multiplier @xmath62 is used to ensure normalization , @xmath51 .",
    "@xmath63 trades off deviations of @xmath15 from the reference distribution against deviations between the predicted and measured observables . setting the functional derivative with respect to @xmath15 to zero results in @xmath64}\\nonumber \\\\ & & + \\sum_i    \\frac{\\left[y_i({\\mathbf{x}})-y_i^{{(\\mathrm{obs})}}\\right]^2 }    { 2\\sigma_i^2}+\\lambda = 0.\\end{aligned}\\ ] ] by solving this equation for @xmath65",
    ", we obtain an explicit expression for the optimal probability density in common - property ensemble refinement , @xmath66 ^ 2 }     { 2\\theta\\sigma_i^2}\\right),\\ ] ] which can then be normalized to one by integration over @xmath16 .",
    "we note that for @xmath5 , this distribution is identical to the bayesian posterior of single - copy refinement in eqs .",
    "( [ eq:1]-[eq:3 ] ) .",
    "this procedure is closely related to the maximum - entropy method . in typical maximum - entropy approaches ,",
    "measurements are imposed as strict constraints . by contrast , in the maximum - entropy formalism of gull and daniell,@xcite noise is taken into account through a @xmath67 term .",
    "however , @xmath67 enters in the form of a constraint to match exactly an `` expected value '' , and @xmath54 is the corresponding lagrange multiplier enforcing this constraint . here , by contrast , we have no _ a priori _ expectations concerning the exact @xmath67 to be achieved in refinement .",
    "instead , we express our confidence in the reference distribution through the choice of @xmath54 ( even though in practice , @xmath54 may be adjusted ; see below ) .",
    "we note that later maximum entropy approaches accounting for noise in the data do not always draw this distinction @xcite and minimize functionals similar or identical to @xmath63 .",
    "[ [ refinement - using - ensemble - averages . ] ] refinement using ensemble averages .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    next we assume that the measured quantities @xmath68 are averages of observables @xmath69 over an ensemble of structures , as represented by the functional @xmath70 = \\int d{\\mathbf{x}}\\,p({\\mathbf{x}}){y}_i({\\mathbf{x}}).\\ ] ] for simplicity and concreteness , we assume gaussian errors and a likelihood functional correspondingly defined as @xmath71 = e^{-\\chi^2[p({\\mathbf{x}})]/2},\\ ] ] where @xmath72 = \\sum_i \\frac{\\left[\\int d{\\mathbf{x}}\\,p({\\mathbf{x}}){y}_i({\\mathbf{x } } ) -      y_i^{{(\\mathrm{obs})}}\\right]^2 } { \\sigma_i^2}\\ ] ] for a set of measurements @xmath24 of ensemble - averaged observables @xmath69 . for correlated errors , eq .",
    "( [ eq:4 ] ) becomes @xmath73 = \\mathbf{\\delta y}^t \\bm{\\sigma}^{-1 } \\mathbf{\\delta y}\\ ] ] with @xmath74 .",
    "we note that the general formalism is of course not limited to gaussian errors .",
    "substituting @xmath75 $ ] for @xmath67 will lead to the corresponding expressions for more general likelihood functions .",
    "we note further that more general functionals can arise , e.g. , if the measurements @xmath76 $ ] report on functions of averages , with measurements of the variance as the simplest case .    in practice",
    ", one also has to deal with uncertainties @xmath77 in the forward calculation of the observables @xmath69 from individual configurations @xmath16 .",
    "such uncertainties often exceed the statistical errors @xmath78 in the measurements .",
    "assuming that the two are uncorrelated , they can be lumped together , @xmath79 . finally ,",
    "both errors can only be estimated with some uncertainty . in a bayesian formulation ,",
    "errors can be treated as nuisance parameters and integrated out.@xcite    [ [ sampling - from - the - bayesian - posterior - functional - in - ensemble - refinement . ] ] sampling from the bayesian posterior functional in ensemble refinement .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the above formulation appears to be of limited practical value , as one would have to sample in function space .",
    "one possible way to perform such sampling in practice is to discretize the problem .",
    "for instance , clustering can be used to break up the configuration space into discrete subsets . if a set of configurations is drawn from the reference distribution @xmath27 , the relative weight @xmath80 of each cluster",
    "@xmath0 would then be proportional to the number of its members . for cluster @xmath0 ,",
    "the value for the observable @xmath24 is @xmath81 , such that eq .",
    "( [ eq:11 ] ) becomes @xmath82 = \\sum_i    \\frac{\\left(\\sum_{\\alpha=1}^n w_\\alpha { y}_i^\\alpha - y_i^{{(\\mathrm{obs})}}\\right)^2 } { \\sigma_i^2}\\ ] ] with normalized weights @xmath83 .",
    "these weights could then be sampled according to @xmath84 \\propto } \\\\    & & \\exp\\left(-\\theta \\sum_\\alpha w_\\alpha      \\ln \\frac{w_\\alpha}{w_\\alpha^0}-\\sum_i      \\frac{\\left(\\sum_{\\alpha}w_\\alpha { y}_i^\\alpha - y_i^{{(\\mathrm{obs})}}\\right)^2 }      { 2\\sigma_i^2}\\right ) , \\nonumber\\end{aligned}\\ ] ] again under the normalization constraint , @xmath85 .",
    "equation  ( [ eq:14 ] ) is the discrete analog of eq .",
    "( [ eq:9a ] ) .",
    "this form of bayesian ensemble refinement can also be applied to a collection of @xmath18 individual configurations , without clustering . if one starts from an equilibrium ensemble of @xmath86 drawn from the reference distribution @xmath27 , then @xmath87 .",
    "[ [ optimal - configuration - space - distribution - from - bayesian - ensemble - reweighting . ] ] optimal configuration space distribution from bayesian ensemble reweighting . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    instead of sampling the probability densities @xmath15 or @xmath88 from the posterior functional , we can again try to find the most probable @xmath15 or @xmath83 , as in eqs .",
    "( [ eq:5]-[eq:7 ] ) above .",
    "configurations @xmath16 sampled according to this optimal @xmath17 define representative ensembles . to find the extremum of the posterior functional , we follow the same variational approach as above and",
    "maximize the posterior functional in eq .",
    "( [ eq:9a ] ) with respect to the probability density @xmath15 . as optimization function",
    ", we use the negative logarithm of the posterior @xmath89 , with a lagrange multiplier @xmath62 to enforce normalization . for gaussian errors ,",
    "we obtain @xmath90=\\theta \\int d{\\mathbf{x}}\\ , p({\\mathbf{x}})\\ln\\frac{p({\\mathbf{x}})}{p_0({\\mathbf{x}})}}\\\\ & & +    \\sum_i \\frac{\\left[\\int d{\\mathbf{x}}\\ , p({\\mathbf{x}}){y}_i({\\mathbf{x } } ) -y_i^{{(\\mathrm{obs})}}\\right]^2 }    { 2\\sigma_i^2 } + \\lambda\\int d{\\mathbf{x}}\\ , p({\\mathbf{x}}),\\nonumber\\end{aligned}\\ ] ] taking on a form that has been postulated as a starting point for a maximum entropy approach.@xcite here , eq .",
    "( [ eq:15 ] ) is a direct consequence of posterior maximization , which would allow us to obtain corresponding log - posteriors also for more non - kullback - leibler priors and more complicated likelihood functions [ e.g. , for rigorous common - property refinement without the approximation preceding eq .",
    "( [ eq:10a ] ) ] . in such cases ,",
    "postulating a proper maximum entropy formulation can be difficult .",
    "variational optimization of @xmath63 results in @xmath91}\\\\ & & + \\sum_i    \\frac{{y}_i({\\mathbf{x}})[\\int d{\\mathbf{x } } ' p({\\mathbf{x}}'){y}_i({\\mathbf{x}}')-y_i^{{(\\mathrm{obs } ) } } ] }    { \\sigma_i^2}+\\lambda = 0 , \\nonumber \\end{aligned}\\ ] ] which can be solved formally to give @xmath92 }        { \\theta\\sigma_i^2}\\right]}. \\nonumber\\end{aligned}\\ ] ] we recognize eq .",
    "( 8) of ref . ,",
    "albeit with a somewhat different interpretation . there , @xmath93 appears as a lagrange multiplier `` @xmath62 '' that has to be determined self - consistently such that the @xmath67 for @xmath17 matches a desired value , following the maximum - entropy prescription of gull and daniell;@xcite here , @xmath54 is a parameter that expresses a priori the confidence in the reference distribution .",
    "the normalization factor in eq .",
    "( [ eq:34 ] ) can be determined by integration [ which is equivalent to determining our lagrange multiplier @xmath62 in eq .",
    "( [ eq:16 ] ) ] . for correlated errors of the ensemble averages , eq .",
    "( [ eq:12 ] ) , the exponent in eq .",
    "( [ eq:34 ] ) should be replaced by @xmath94.\\ ] ] because the weight function @xmath17 appears inside the square in the @xmath67 term of eq .",
    "( [ eq:15 ] ) , we have ended up with a nonlinear integral equation , eq .",
    "( [ eq:34 ] ) , for @xmath17 that will usually be difficult to solve , in particular for high - dimensional problems .",
    "we note , however , that for refinement without explicit consideration of errors , adaptive methods have been developed.@xcite uncertainties are considered by beauchamp et al.,@xcite albeit with a number of additional priors introduced for constants acting as weight factors in their bias . in appendix",
    "[ app : a ] , we introduce an adaptive algorithm to sample configurations according to the optimal bayesian ensemble distribution , eqs .",
    "( [ eq:34 ] ) and ( [ eq:34b ] ) , without the need of multiple replicas .    the above procedure can also be applied to problems with a set of @xmath18 discrete configurations .",
    "we determine their optimal weights @xmath95 , @xmath96 , by maximizing the negative log - posterior @xmath97 the extremum of this negative log - posterior satisfies the following set of coupled nonlinear equations @xmath98,\\nonumber\\end{aligned}\\ ] ] which can be solved , for instance , by iteration , starting from @xmath80 ( see below ) .",
    "alternatively , one can use simulated annealing or other optimization methods to locate the global minimum of @xmath63 .",
    "we note that the resulting optimal weights @xmath95 coincide exactly with the eros weights @xcite if all @xmath18 configurations are reweighted . in an illustrative example",
    "below , we will also consider the case where sets of @xmath22 configurations are drawn from @xmath27 and reweighted according to eros . in the limit of @xmath99",
    ", each structure enters this starting ensemble with the correct relative weight .",
    "after eros reweighting , using eq .",
    "( [ eq:18 ] ) with @xmath100 , one thus converges to the optimal bayesian ensemble refinement weights @xmath95 for @xmath99 .",
    "this convergence will be illustrated in a numerical example .",
    "the optimal weights @xmath95 determined self - consistently from eq .",
    "( [ eq:18 ] ) can be used for reweighting of an ensemble of structures drawn from the reference distribution .",
    "however , we can not use these weights directly to sample the ensemble of configurations on the fly , lacking explicit solutions of eqs .",
    "( [ eq:34 ] ) and ( [ eq:18 ] ) ( but see the appendix for an adaptive method ) .    to circumvent the problem",
    ", we adopt a replica - based approach in which averaged observables are calculated over multiple copies of the system.@xcite in the replica simulations , @xmath18 copies ( replicas ) @xmath86 of a molecular system are simulated in parallel using the same energy function @xmath101 , subject in addition to a biasing potential that attempts to match the observables obtained by averaging over the @xmath18 copies to the experimental measurements .",
    "we require that for a single replica , @xmath102 , one recovers the result of common - property ensemble refinement , eq .",
    "( [ eq:7 ] ) . at the other extreme , @xmath103 ,",
    "we want to recover the optimal bayesian configuration space distribution , eq .",
    "( [ eq:34 ] ) .",
    "we use @xmath18 equally weighted replicas @xmath104 to define a function space of realizable probability densities , @xmath105 , where @xmath106 is dirac s delta function . to determine the relative weight of these @xmath15 , and in turn of the underlying replica states @xmath107",
    ", we use the posterior functional eq .",
    "( [ eq:9a ] ) with the likelihood in eq .",
    "( [ eq:10 ] ) , @xmath108\\propto e^{-\\theta\\int      d{\\mathbf{x}}\\,p({\\mathbf{x}})\\ln\\frac{p({\\mathbf{x}})}{p_0({\\mathbf{x}})}-\\chi^2/2}}\\nonumber\\\\    & & \\propto e^{\\frac{\\theta}{n}\\sum_\\alpha \\ln      p_0({\\mathbf{x}}_\\alpha)-\\chi^2/2 } \\\\    & & = \\prod_\\alpha \\left[p_0({\\mathbf{x}}_\\alpha)\\right]^{\\frac{\\theta}{n } }    e^{-\\sum_{i=1}^m\\left[\\frac{1}{n}\\sum_{\\gamma=1}^n         y_i({\\mathbf{x}}_\\gamma)-y_i^{{(\\mathrm{obs})}}\\right]^2/2\\sigma_i^2}.\\nonumber\\end{aligned}\\ ] ] for the evaluation of the entropy integral in the exponent , we coarse - grained @xmath106 as @xmath109 for @xmath110 and 0 otherwise ; divided out a term proportional to @xmath111 because we only require relative posterior probabilities ; and then took the limit @xmath112 . having chosen @xmath18-replica distributions as function space , the @xmath15 are now parametrized by @xmath107 , and in eq .",
    "( [ eq:19 ] ) the posterior functional has become a function that can be interpreted as the sampling distribution of the replica states @xmath107 . here",
    ", we are interested in sampling from the extremum of the posterior functional , i.e. , the optimal bayesian ensemble distribution . to suppress fluctuations around the extremum as @xmath113 , we take the posterior function in eq .",
    "( [ eq:19 ] ) to a power growing with @xmath18 . taking it to the power @xmath114",
    ", we arrive at the replica sampling distribution @xmath115 ^ 2 }      { \\theta\\sigma_i^2}\\right]\\!,\\nonumber\\end{aligned}\\ ] ] with the boltzmann factor for potential energy @xmath29 defining the reference distribution .",
    "the second term in the exponent defines the biasing potential applied to the ensemble of replicas .",
    "we now show that under eq .",
    "( [ eq:21 ] ) in the limit @xmath103 , individual replicas indeed sample configurations according to the optimal bayesian ensemble refinement distribution in eq .",
    "( [ eq:34 ] ) . without loss of generality , we determine the distribution of replica 1 , since all replicas are equivalent . to this end , we rewrite the last term in the exponent of eq .",
    "( [ eq:21 ] ) as @xmath116 ^ 2 }    { \\theta\\sigma_i^2}}\\nonumber\\\\ & = & \\sum_i\\left [    \\frac{{y}_i^2({\\mathbf{x}}_1)}{2\\theta\\sigma_i^2n}+\\frac{{y}_i({\\mathbf{x}}_1)}{\\theta\\sigma_i^2 }    \\left(\\frac{1}{n}\\sum_{\\alpha=2}^n      { y}_i({\\mathbf{x}}_\\alpha)-y_i^{{(\\mathrm{obs})}}\\right)\\right]\\nonumber\\\\ & & + \\sum_i\\frac{n\\left(\\frac{1}{n}\\sum_{\\alpha=2}^n      { y}_i({\\mathbf{x}}_\\alpha)-y_i^{{(\\mathrm{obs})}}\\right)^2}{2\\theta\\sigma_i^2}.\\end{aligned}\\ ] ] since the first term on the right is of order @xmath117 and the second term is of order @xmath118 , the first term vanishes in the limit of @xmath103 . in this limit , we can use a mean field approximation for the second term , @xmath119 .",
    "the last term on the right of eq .",
    "( [ eq:35 ] ) is independent of @xmath120 and thus cancels in the normalization of the resulting distribution over @xmath121 . in the limit of @xmath103",
    ", we thus arrive at a probability density for replica 1 ( and , by symmetry , for all others ) of @xmath122 }        { \\theta\\sigma_i^2}\\right]},\\nonumber\\end{aligned}\\ ] ] which is indeed identical to the probability density of optimal bayesian ensemble refinement in configuration space , eq .",
    "( [ eq:34 ] ) .",
    "below , this identity will be demonstrated explicitly for two analytically tractable models , and for a numerical model .",
    "equation ( [ eq:21 ] ) for the probability density in bayesian replica ensemble refinement is nearly identical to that obtained by cavalli et al.@xcite as a weighted integral over the maximum entropy solution with strict constraints on the observables .",
    "however , there is one crucial difference : their @xmath67 in the exponent of the reweighting factor is missing the factor @xmath18 scaling the biasing potential with the number of replicas .",
    "not scaling @xmath67 by @xmath18 would result in decoupling of the replicas , as shown explicitly below .",
    "indeed , early replica ensemble - refinement simulations introduced the scale factor @xmath18 empirically,@xcite and it appears in a recent preprint@xcite released shortly after submission of this paper and release of a preprint .    roux and weare @xcite also considered a maximum entropy approach with strict constraints on the ensemble averages .",
    "in addition , these authors examined the convergence behavior of @xmath18-replica simulations . for the specific example of a gaussian reference distribution and a harmonic restraint on the mean , roux and weare @xcite found that to recover the mean exactly for large @xmath18 , the effective spring constant in the biasing potential had to grow faster than linearly in @xmath18 . for the general case ,",
    "the choice of the spring constant was left open . in our bayesian formulation ,",
    "we account for the uncertainties of the measured averages .",
    "it is therefore not to be expected that the measurements are satisfied strictly in the refined ensemble .",
    "this will be illustrated below by the analytical solution for the analogous problem of a gaussian reference distribution within our bayesian framework .",
    "more generally , the explicit accounting for errors @xmath41 provides a basis for combining different measurements in a properly balanced manner .    on the basis of the preceding analysis",
    ", we note that if the @xmath67 term in eq .",
    "( [ eq:21 ] ) were scaled by @xmath123 instead of @xmath18 , with @xmath124 , then replica ensemble refinement would exhibit a `` phase transition '' as a function of the exponent @xmath125 in the `` thermodynamic limit '' of infinitely many replicas , @xmath103 . for sub - linear scaling , @xmath126 , the effect of the @xmath67 bias",
    "vanishes with increasing @xmath18 and the replica ensemble gradually falls back to the reference distribution ; for super - linear scaling , @xmath127 , the @xmath67 bias diverges to infinity everywhere except at states that satisfy the constraints exactly , making it equivalent to a sum of delta functions that impose strict constraints on the averages ; only for linear scaling , @xmath128 , replica sampling converges to the distribution of optimal bayesian ensemble refinement .",
    "this @xmath18-scaling becomes explicit in the gaussian models studied by roux and weare @xcite and below .      in the following ,",
    "we describe the bioen algorithm that simultaneously addresses the possible shortcomings of eros and bayesian replica ensemble refinement and helps us in the choice of the @xmath54 parameter . by combining eros and replica simulations",
    ", one can accelerate the convergence toward the optimal bayesian ensemble .",
    "this combination also makes it possible to obtain optimal bayesian ensembles for a wide range of @xmath54 values without the need to run actual bayesian replica simulations for all of them .",
    "covering a broad @xmath54 range is important in practice to choose a suitable confidence parameter @xmath54 that achieves a good balance between reference distribution and data .    in eros",
    ", one can work with large numbers @xmath22 of structures without significant computational costs ; however , if @xmath27 and @xmath17 have little overlap in configuration space , then these structures may not be representative of the refined ensemble , resulting in slow convergence with increasing @xmath22 , as shown below .",
    "by contrast , the computational cost of sampling bayesian replica ensembles with large @xmath18 is high . to accelerate the convergence toward @xmath18-independent optimal bayesian ensemble refinement",
    ", one can combine the replica and eros methods ( and the adaptive method described in the appendix ) .",
    "even with relatively small @xmath18 , the replica simulations can be used to enrich the sample of configurations fed into eros refinement . to give these configurations @xmath16 the proper weight proportional to @xmath27 , with @xmath16 coming from different simulations with and without bias , one can for instance use a histogram - free version of the multidimensional weighted histogram analysis method ( wham).@xcite    we first need to reweight the @xmath18-replica states in different simulations according to the reference distribution . by running @xmath18 unbiased , uncoupled simulations according to @xmath129 ( or , simply , one unbiased simulation as a source for configurations @xmath86 that are then combined at random to form pseudo @xmath18-replica states ) and one or several biased , coupled simulations ( e.g. , with different @xmath130 ) according to @xmath131 ,",
    "one obtains representative sets of @xmath18-replicas states . to combine them , one needs to assign the proper relative weight @xmath132 to the @xmath133-th sampled @xmath18-replica state @xmath134 in run @xmath24 , as given by the reference distribution @xmath135 .",
    "following ref . , we first determine the free energies @xmath136 of each @xmath18-replica simulation @xmath24 ( @xmath137 ) by iteratively solving the coupled set of equations @xmath138}},\\ ] ] where @xmath139 by definition .",
    "the outer sums on the right extend over the @xmath140 runs ( indexed by @xmath141 and @xmath142 ) and the @xmath143 @xmath18-replica states ( indexed by @xmath133 ) in run @xmath141 . the biasing potential is defined as @xmath144 in biased runs @xmath24 , and @xmath145 in unbiased ones , with @xmath67 as in eq .",
    "( [ eq:21 ] ) . to obtain the relative weight @xmath132 of replica state @xmath133 in run @xmath24 , @xmath134",
    ", corresponding to the reference distribution , we set the @xmath146-term in eq .",
    "( 2 ) of ref . equal to one for only this replica state and to zero for all others .",
    "we then obtain @xmath147}\\right]^{-1},\\ ] ] where the sum extends over the different simulations @xmath142 .",
    "each of the @xmath18 configurations @xmath86 in a given replica state @xmath134 then has the same relative weight @xmath132 as the replica state .",
    "the resulting set of configurations together with their estimated relative weights can then be used as input for an eros refinement according to eq .",
    "( [ eq:18 ] ) . with the resulting eros - refined weights ,",
    "one obtains the bioen ensemble of configurations enriched by the biased @xmath18-replica simulations , yet properly reweighted to correct for effects of finite numbers @xmath18 of replicas .",
    "importantly , this reweighting approach also allows one to obtain optimally reweighted ensembles for different @xmath54 , simply by re - running eros .",
    "[ [ ensemble - reweighting - of - the - mean . ] ] ensemble reweighting of the mean .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to illustrate and test the ensemble reweighting formalisms described above , we consider a simple , analytically tractable problem .",
    "consider a one - dimensional configuration coordinate @xmath148 with a gaussian reference distribution @xmath149 and the mean @xmath150 as observable , with uncertainty @xmath151 , such that @xmath152 ^ 2}{\\sigma^2}.\\ ] ] this problem is closely related to a gaussian model for replica simulations studied by roux and weare,@xcite with the difference that here we explicitly account for the uncertainty @xmath151 in the measured mean @xmath153 .",
    "the negative log - posterior eq .",
    "( [ eq:15 ] ) becomes @xmath154 & = & \\theta\\int dx\\ , p(x)\\ln \\frac{p(x)}{p_0(x ) } +    \\frac{\\left[\\int dx\\,p(x)x - y\\right]^2}{2\\sigma^2}\\nonumber\\\\ & &    + \\lambda \\int dx\\,p(x).\\end{aligned}\\ ] ] the extremum of @xmath63 satisfies @xmath155 +    \\frac{x\\left[\\int dx'\\,p(x')x'-y\\right]}{\\sigma^2 }      + \\lambda\\ ] ] this integral equation can be solved with a gaussian ansatz , @xmath156 $ ] , with @xmath54 set to one without loss of generality , since a change in @xmath54 here corresponds to a rescaled @xmath157 ( see the appendix for an alternative solution method using generating functions ) . by substituting the ansatz into the integral equation and solving for the coefficients of powers of @xmath148",
    ", it follows that the mean of the optimal probability density is @xmath158 the optimal probability density of bayesian ensemble refinement is thus a gaussian , @xmath159}.\\ ] ] the variance @xmath160 remains unchanged from the reference distribution , but the mean is shifted from zero toward the ensemble average @xmath153 according to the relative weights of the variances in the reference distribution , @xmath160 , and in the @xmath67 error , @xmath157 . in the limit @xmath161 , the mean approaches the measurement @xmath153 ; in the opposite limit @xmath162 , the mean remains near that of the reference distribution , i.e. , at zero .",
    "bayesian replica ensemble refinement for this problem is also analytically tractable . for @xmath18 replicas , with @xmath5 , we have @xmath163}{(2\\pi s^2)^{n/2}}.\\ ] ] since this replica probability density is symmetric in exchanges of the @xmath164 , all replicas sample the same space , and we can integrate out all @xmath164 but @xmath120 to obtain a marginalized replica probability density @xmath165 the gaussian integrals can be carried out , resulting in @xmath166 being gaussian with mean @xmath167 and variance @xmath168 $ ] . for @xmath102",
    ", we recover the probability of common - property refinement , eq .",
    "( [ eq:7 ] ) , with variance @xmath169 . in the limit of @xmath103 ,",
    "the variance approaches @xmath160 .",
    "we thus have @xmath170 , with @xmath171 the optimal bayesian ensemble refinement result in eq .",
    "( [ eq:26 ] ) .",
    "series expansion shows that this limit is approached asymptotically as @xmath172=f(x)/n+{\\cal o}(1/n^2)$ ] with @xmath173 for fixed @xmath148 , i.e. , as @xmath174 in the error of the logarithm of the probability density .",
    "importantly , if we had left out the factor @xmath18 scaling the @xmath67 in the exponent of eq .",
    "( [ eq:27 ] ) , the mean would instead have been @xmath175 .",
    "the mean would thus approach zero , i.e. , the value of the reference distribution , as the number of replicas is increased , @xmath103 , irrespective of the uncertainty @xmath176 .",
    "this result makes it clear that the @xmath67 in the replica model has to be scaled by @xmath18 to obtain a result that is size - consistent in the number of replicas @xmath18 .    [ [ ensemble - reweighting - of - the - second - moment . ] ] ensemble reweighting of the second moment .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    another analytically tractable ensemble reweighting problem is obtained for a non - linear observable , the second moment @xmath177 , again for the gaussian reference distribution in eq .",
    "( [ eq:22 ] ) . for the second moment",
    ", we have @xmath178 ^ 2/\\sigma^2 $ ] .",
    "the optimal solution then has to satisfy the integral equation @xmath179 +    \\frac{x^2\\left[\\int dx'\\,p(x')x'^2-y\\right]}{\\sigma^2 }      + \\lambda~.\\ ] ] to solve this integral equation , we make a gaussian ansatz with zero mean and variance @xmath180 , @xmath181/(2\\pi t^2)^{1/2}$ ] , with @xmath54 set to one without loss of generality , and find @xmath182^{1/2 } }    { 4s^2}.\\ ] ] in the limit of no uncertainty in the measurement , @xmath183 , we find that @xmath184 , i.e. , we have a gaussian with exactly the measured second moment . in the other limit of complete uncertainty ,",
    "@xmath185 , we have @xmath186 , i.e. , no change relative to the reference distribution . in between , @xmath180 is a nonlinear interpolation between these two extremes .",
    "this problem is also analytically tractable for bayesian replica ensemble refinement . for @xmath18 replicas with @xmath187 ,",
    "the joint probability density is @xmath188.\\nonumber\\end{aligned}\\ ] ] to integrate out @xmath189 to @xmath190 , we introduce @xmath191-dimensional spherical coordinates , with @xmath192 .",
    "the marginalized distribution then becomes @xmath193 \\nonumber\\\\      & \\equiv & \\int dr\\,f(r|x_1)\\nonumber\\end{aligned}\\ ] ] as it turns out , the remaining one - dimensional integral can be carried out analytically , giving an expression in terms of confluent hypergeometric functions . however , to take the @xmath103 limit , it is advantageous to use a saddle - point approximation of the integrand in terms of a gaussian , @xmath194 $ ] , that becomes increasingly accurate as @xmath18 increases .",
    "we find that the variance @xmath195 in @xmath196 becomes independent of @xmath18 and @xmath148 in the limit of large @xmath18 , such that only the value @xmath197 at the extremum needs to be considered in the construction of the marginalized distribution of @xmath148 . in the limit of @xmath103 ,",
    "@xmath198 depends on @xmath148 as @xmath199 where the @xmath200 value cancels in the normalization of the marginalized distribution @xmath201 .",
    "the marginalized distribution of @xmath148 is thus a gaussian centered at zero with variance @xmath180 , as given in eq .",
    "( [ eq:30 ] ) .",
    "the result of bayesian replica ensemble refinement thus converges to the probability density of optimal bayesian ensemble refinement in the limit of large @xmath18 .",
    "this correspondence once again stresses the importance of scaling the @xmath67 by @xmath18 to maintain proper coupling and convergence in the limit of large numbers @xmath18 of replicas .     from bayesian replica ensemble refinement with @xmath18 replicas ( lines )",
    "compared to the optimal bayesian ensemble refinement solution @xmath171 ( open squares ) and to the reference distribution @xmath202 ( thin line with open circles ) .",
    "arrows indicate changes relative to @xmath202 .",
    "( bottom ) error @xmath203 $ ] in @xmath204 scaled by the number of replicas @xmath18 .",
    "part of the scatter is a reflection of the stochastic monte carlo sampling of the bayesian distributions @xmath201 .",
    "[ fig:2],width=302 ]    [ [ convergence - of - bayesian - replica - ensemble - refinement . ] ] convergence of bayesian replica ensemble refinement .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to examine the convergence of the bayesian replica ensemble refinement with the number @xmath18 of replicas towards the optimal bayesian ensemble refinement solution , we have performed metropolis monte carlo simulations for a one - dimensional system defined by a double - well potential energy function @xmath205 ^ 2/b^4 $ ] with @xmath206 and @xmath207 .",
    "we have discretized the potential at @xmath208 with @xmath209 .",
    "the mean of the corresponding reference distribution @xmath210 $ ] is at @xmath211 . in the ensemble refinement , we set the target half - way between the maximum and the upper minimum , at @xmath212 .",
    "the resulting @xmath67 then becomes @xmath213 , with @xmath151 set to one .",
    "systems with @xmath214 replicas were sampled with monte carlo simulations , and the distributions @xmath201 averaged over all replicas calculated .",
    "figure [ fig:2 ] compares the resulting distributions @xmath201 of @xmath148 from bayesian replica ensemble refinement to @xmath171 from optimal bayesian ensemble refinement .",
    "we find that the ensemble reweighted distributions shift contributions from the left well to the right well to match the target mean , but by and large retain the shape within each well of the potential @xmath215 defining the reference distribution .",
    "the only exception is @xmath216 , where the restraint on the mean effectively pulls one of the replicas out of the first minimum into the barrier region .",
    "we also find numerically that the distributions @xmath201 , averaged over all replicas @xmath18 , converge asymptotically ( for large @xmath18 ) to the optimal bayesian ensemble refinement solution @xmath171 as @xmath217 for @xmath218 .",
    "numerical results for the master curve @xmath219 $ ] are shown in the bottom panel of figure [ fig:2 ] bottom ; the actual error in an @xmath18-replica simulation is approximately @xmath220-th of @xmath221 .",
    "the probability density from bayesian replica ensemble refinement thus appears to converge asymptotically as @xmath220 to the optimal bayesian result , as in the first analytically tractable example above .",
    "[ [ convergence - of - eros . ] ] convergence of eros .",
    "+ + + + + + + + + + + + + + + + + + + +    we have used the same model to examine the convergence of eros in the case where @xmath22 representative configurations are drawn according to @xmath202 and then reweighted according to eq .",
    "( [ eq:18 ] ) .",
    "specifically , we have drawn @xmath22 values of @xmath148 with replacement according to the boltzmann distribution for the double - well potential with @xmath209 .",
    "the resulting @xmath22 points , indexed as @xmath222 , were then reweighted according to eq .",
    "( [ eq:18 ] ) , with @xmath223 for all @xmath24 .",
    "the resulting eros weights were then averaged for each of the @xmath36 possible values of @xmath148 , @xmath224 where @xmath225 indicates an average over repeated selections of samples of size @xmath22 , and @xmath226 if @xmath227 and zero otherwise . in this way , we estimated the expected weight of configuration @xmath0 in repeated eros runs using @xmath22 representative ensembles .    in figure  [ fig:3 ] , we show that the distribution @xmath228 obtained by repeated reweighting indeed converges to the optimal bayesian ensemble refinement result in the limit of large @xmath22 . for the specific example",
    ", the error in @xmath229 scales as @xmath230 .",
    "interestingly , the relative error obtained for eros samples of size @xmath22 is comparable to that of bayesian replica ensemble refinement with @xmath22 replicas .     from eros with @xmath22 configurations drawn according to @xmath231 ( lines ) compared to the optimal bayesian ensemble refinement solution @xmath171 ( open squares ) .",
    "( bottom ) error @xmath232 $ ] in @xmath229 scaled by the sample size @xmath22 .",
    "part of the scatter is a reflection of the stochastic monte carlo sampling of configurations in eros .",
    "[ fig:3],width=302 ]    [ [ bioen - improves - convergence - by - combining - eros - and - replica - simulations . ] ] bioen improves convergence by combining eros and replica simulations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we have also tested the bioen combination of eros and replica simulations to speed up convergence to the optimal bayesian ensemble distribution .",
    "figure  [ fig:4 ] demonstrates the dramatic improvement achieved in the combined method for the model of figures  [ fig:2 ] and [ fig:3 ] .",
    "after eros reweighting of the configurations sampled in unbiased and biased runs with @xmath216 , 4 , and 8 replicas , we find that the significant systematic errors in the bayesian replica ensemble distributions @xmath201 disappear , and only small , primarily statistical errors remain .     in the ensemble distributions obtained from regular replica ensemble refinement with @xmath216 , 4 , and 8 replicas ( purple line with symbols ) relative to the optimal bayesian ensemble distribution @xmath171 ( shown in the top panel ) .",
    "also shown is the error of the bioen method combining eros and replica refinement ( green lines ) .",
    "please note the change in scale of the vertical axes for different @xmath18 .",
    "see figure  [ fig:2 ] for the error in @xmath204 without bioen for larger numbers of replicas @xmath18 .",
    "[ fig:4],width=302 ]      [ [ combining - common - property - and - ensemble - average - refinement . ] ] combining common - property and ensemble - average refinement .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for simplicity , we have so far dealt separately with observables reporting on properties common to all configurations and on ensemble averages .",
    "however , these data can be combined readily within the above formalisms . in the respective posteriors , the likelihood terms according to eqs .",
    "( [ eq:10a ] ) and ( [ eq:10 ] ) simply have to be multiplied .",
    "the optimal bayesian ensemble distribution then becomes @xmath233 ^ 2 }          { 2\\theta\\sigma_i^2}\\right]}}\\\\    & & \\times\\exp{\\left[-\\sum_{i = m+1}^m        \\frac{{y}_i({\\mathbf{x}})\\left[\\int d{\\mathbf{x } } ' { p^{\\mathrm{(opt})\\!}}({\\mathbf{x}}'){y}_i({\\mathbf{x}}')-y_i^{{(\\mathrm{obs})}}\\right ] }        { \\theta\\sigma_i^2}\\right ] } , \\nonumber\\end{aligned}\\ ] ] for @xmath141 restraints on common properties , and @xmath234 restraints on ensemble averages .",
    "equation  ( [ eq:34a ] ) is a combination of eqs .",
    "( [ eq:7 ] ) and ( [ eq:34 ] ) .",
    "an analogous expression generalizes eq .",
    "( [ eq:18 ] ) for the discrete case .",
    "[ [ data - from - single - molecule - experiments . ] ] data from single - molecule experiments .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    data from single - molecule experiments can be incorporated in the different refinement procedures . in principle",
    ", one could even fit the data individually , one molecule at a time , using single - copy refinement . in a more practical approach , one can use the techniques of ensemble refinement to fit the single - molecule data lumped together in a way that produces not just averages but also distributions of observables .",
    "an example are fret efficiencies @xmath235 measured by single - molecule spectroscopy . as a basis for ensemble",
    "refinement,@xcite one can for instance determine fret - efficiency histograms @xmath236 from photon arrival trajectories and use the deviations between histogram counts calculated for an ensemble model , @xmath237 $ ] , and measured in experiment , @xmath238 , to construct a @xmath67 , with appropriate error models .",
    "with such a @xmath67 , one can then use both eros @xcite and bayesian replica ensemble refinement .",
    "[ [ sec : dynamics ] ] dynamic , time - dependent data such as nmr noes",
    ". + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    many relevant observables are not just functions of a configuration , @xmath239 , but depend also on the dynamics .",
    "examples are the noe intensity and other nmr relaxation parameters that depend on the rotational and translational dynamics of the spin system.@xcite whereas it is outside of the scope of this article to refine an entire dynamical model to such data , we can make some progress in this direction by considering a reduced problem .",
    "ignoring self - consistency issues , we can attempt to refine an ensemble of configurations @xmath16 that evolve in time under the hamiltonian of the molecular simulation energy function @xmath29 defining the reference distribution @xmath27 , but are distributed according to @xmath17 instead of @xmath27 .    to calculate the observables associated with a particular configuration @xmath16 , one can use trajectory segments passing through @xmath16 .",
    "each of the sample configurations @xmath16 would then serve as an initial value , with maxwell - boltzmann velocities , for one or multiple trajectory segments of length @xmath240 . to center the trajectories at @xmath16 with respect to time , one can run trajectory pairs of length @xmath240 , initiated from @xmath16 with sign - inverted maxwell - boltzmann velocities , one running forward and the other running backward in time . stitching the two segments together at @xmath16 , after sign",
    "- inverting the velocities of the backward segment , one obtains a continuous trajectory of length @xmath241 centered time - wise at @xmath16 . for each of these trajectories ,",
    "the time - dependent observable @xmath242 $ ] can be calculated , possibly averaged by repeated runs over different choices of initial maxwell - boltzmann velocities .",
    "the @xmath243 calculated in this manner can be treated as simple functions of @xmath244 to enter the @xmath67 in the same way as static data .",
    "the trajectory length @xmath241 should be set such that the @xmath243 can be calculated with reasonable accuracy ( i.e. , as multiples of the relevant correlation times ) .",
    "after refinement , one obtains an ensemble of configurations that jointly account for the time - dependent observables yet stay close to the reference distribution .",
    "we note that ( possibly overlapping ) trajectory segments could also be obtained from long equilibrium trajectories , or even from @xmath18-replica simulations .    as the simplest approach of refining also the actual dynamics",
    ", one can perform in addition time scaling , @xmath245 , which could for instance account for incorrect viscosities of the water model used in the molecular dynamics simulations .",
    "the time - scale parameter @xmath0 can then be optimized as well in the ensemble refinement .    [",
    "[ solving - the - eros - equations . ] ] solving the eros equations .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + +    one can obtain the eros weights in eq .",
    "( [ eq:18 ] ) by numerical minimization of @xmath63 in eq .",
    "( [ eq:17 ] ) , which can be accomplished by a variety of techniques with and without gradient calculations .",
    "alternatively , one can solve eq .",
    "( [ eq:18 ] ) directly , for instance by iteration until self - consistency is achieved .",
    "a possible route is to start from the weights @xmath246 in the reference distribution , and then iterate eq .",
    "( [ eq:18 ] ) to get an updated estimate of @xmath95 .",
    "this procedure can be repeated until the change in old and new approximations drops below a chosen threshold .",
    "we found that mixing the old and new approximations geometrically , as @xmath247 with @xmath248 , led to stable fixed - point iterations .",
    "the mixing parameter @xmath148 controls stability ( @xmath249 ) and speed ( @xmath250 ) .",
    "we further improved the stability and convergence behavior by starting at a large value of @xmath54 , where the deviations from the reference distribution @xmath80 are small , and then reducing @xmath54 in repeated fixed - point iterations to sweep out a broad @xmath54 range . the resulting eros refinements for different @xmath54 can help us in the choice of @xmath54 , as discussed next .",
    "[ [ sec : theta ] ] choosing the confidence factor @xmath54 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the bayesian ensemble refinement methods described here contain one free parameter , the factor @xmath54 that enters the prior and quantifies the level of confidence one has in the reference probability density @xmath27 .",
    "large values of @xmath54 express high confidence ( for instance , if one uses a well - tested atomistic force field instead of a more approximate coarse - grained representation , both being well sampled ) . whereas formally , one would choose @xmath54 before refinement , in practice one may want to readjust this choice after the fact to achieve a better balance between reference distribution and data . by reporting the chosen @xmath54 and the corresponding kullback - leibler divergence between reference and optimal distribution",
    ", the inference process becomes transparent .",
    "akin to l - curve selection in other regularization approaches to inverse problems,@xcite one can find an appropriate value of @xmath54 by plotting the kullback - leibler divergence ( relative entropy ) @xmath251 against the @xmath67 obtained in eros reweighting for different values of @xmath54 . as discussed above , eros reweighting can ( and should ) be performed even when bayesian replica ensemble refinement is used to obtain the configurations to avoid finite-@xmath18 effects .",
    "the value of @xmath252 can then be interpreted as the average error in the energy function @xmath29 used to define the reference distribution , since by definition @xmath253 $ ] for @xmath254 $ ] , given that the additive constant in @xmath255 is chosen such that the partition functions ( and thus free energies ) of the optimal and reference distribution are identical , @xmath256=\\int d{\\mathbf{x}}\\exp[-\\beta u({\\mathbf{x}})]$ ] .",
    "this allows one to choose a @xmath54 value on the basis of expectations concerning the magnitude of this error.@xcite conversely , one can also take a more pragmatic approach and choose a value of @xmath54 at the kink of the @xmath257-versus-@xmath67 curve , where a further decrease in @xmath54 does not produce a significant improvement in the fit quality but causes a large deviation from the reference distribution , as measured by @xmath257 .",
    "this approach is taken in the mera web server for the refinement of peptide ramachandran maps against nmr data.@xcite there one accepts a @xmath67 a certain percentage point ( say , 25 % ) above the minimal @xmath67 obtained for @xmath258 .    finally , the confidence parameter @xmath54 can also be treated as a nuisance parameter with an uninformative prior , @xmath259 for @xmath53 .",
    "one could include @xmath54 in the maximization of the posterior or attempt to integrate it out in a weighted average over ensembles obtained for fixed @xmath54 .",
    "we have described different bayesian approaches to ensemble refinement , established their interrelations , and shown how they can be applied to experimental data .",
    "the bayesian approaches allow one to integrate a wide variety of experiments , including experiments reporting on properties common to all configurations and on averages over the entire ensemble .",
    "we started from a bayesian formulation in which the posterior is a functional that ranks the quality of the configurational distributions .",
    "we then derived expressions for the optimal probability distribution in configuration space . for discrete configurations",
    ", we found that this optimal distribution is identical to that obtained by the eros model.@xcite to perform ensemble refinement `` on the fly '' , or to enhance the sampling of relevant configurations in cases where the reference distribution and the optimal ensemble density have limited overlap , we considered replica - simulation methods in which a restraint is imposed through a biasing potential that acts on averages over all replicas .",
    "we showed using a mean - field treatment that to obtain a size - consistent result , the biasing potential has to be scaled by the number @xmath18 of replicas , i.e. , the restraint has to become stiffer as more replicas are included .",
    "then , the bayesian replica ensemble refinement converges to the optimal bayesian ensemble refinement in the limit of infinitely many replicas , @xmath103 .",
    "this result clarifies the need to scale the biasing potential , which arises also in maximum entropy treatments with strict constraints,@xcite with the number of replicas @xmath18 to obtain a size - consistent result .",
    "an adaptive method , as described in the appendix , provides a possible alternative to replica - based approaches .",
    "the bioen approach combines the replica and eros refinement methods .",
    "the replica simulations are used to create an enriched sample of configurations .",
    "a free - energy calculation is used to determine the appropriate weights according to the reference ensemble .",
    "the optimal weights according to bayesian ensemble refinement are then determined by eros .",
    "this combined approach addresses the shortcomings of either method , i.e. , the need to work with relatively small @xmath18 in replica simulations , and potentially limited overlap of reference and optimized distribution in eros .",
    "using free - energy reweighting methods , it may also be possible to include configurations from other types of ensemble - biased simulations , including those designed to satisfy measurements exactly.@xcite because of the flexibility and expected rapid convergence , the bioen method combining bayesian replica and eros refinement should perform well in practical applications .    in two examples that are analytically tractable and one requiring numerical calculations",
    ", we demonstrated the equivalence of the different methods in the appropriate limits .",
    "we also studied the convergence properties of bayesian replica simulations with the number of replicas @xmath18 , and of eros reweighting with the sample size @xmath22 .",
    "our examples showed similar convergence of the log - probability of the two refinement approaches to the optimal limit as @xmath220 and @xmath230 , respectively .",
    "the bioen approach also addresses a major issue in bayesian ensemble refinement , namely the choice of @xmath54 .",
    "this parameter enters the prior to express our confidence in the reference distribution .",
    "we find that in the optimal bayesian ensemble distributions , a change in @xmath54 is simply equivalent to a uniform scaling of all squared gaussian errors @xmath260 . since eros reweighting is usually orders of magnitudes less costly than sampling multiple replicas in coupled molecular simulations , one can efficiently obtain estimates of the relative entropy @xmath257 for different @xmath54 . from plots of @xmath257 against @xmath67",
    "one can make an educated choice of @xmath54 , as in other regularization approaches to inverse problems.@xcite    finally , the reweighting of individual structures , either directly using eros or in the combined approach , should prove useful in the optimization of potential energy functions by fitting them to experimental data ( see , e.g. , refs . ) .",
    "if one has a good understanding of the sources of the errors in the energy surface @xmath29 , parameters in @xmath261 can be fitted directly , as was done , e.g. , for the star force fields of proteins @xcite and for rna.@xcite at the other extreme , bayesian approaches have been used before to infer entire energy functions.@xcite here , we suggest to concentrate on the change in weight of structures @xmath86 , i.e. , @xmath262 , which defines the required change @xmath263 in the potential energy to match experiment . by examining the correlation of this force field error @xmath263 with elements of the force field ( e.g. , peptide dihedral angles @xcite or base stacking interactions @xcite )",
    ", it might be possible to identify sources of the error and then correct for them .",
    "it is important to emphasize that a number of assumptions enter the ensemble refinement procedure .",
    "the central ( and declared ! ) assumption is that of a reference distribution . here",
    "it may be possible to use combinations of multiple potential energy functions @xmath264 , representing different force fields or conditions @xmath133 , that jointly cover the relevant phase space better than any potential alone .",
    "one way to mix such potentials is by using a multistate model,@xcite @xmath265 $ ] , where @xmath266 is the mixing `` temperature '' and @xmath267 are energy offsets that weight the different force fields .",
    "another important challenge is that one has to estimate errors both in the measurements and in the calculation of the observables .",
    "procedures to account for uncertainties in the error estimates have been developed.@xcite within the present framework , one could include error distributions in the maximization of the log - posterior , or average over optimal solutions obtained for different errors .",
    "in addition , in many cases the gaussian error model may not be appropriate .",
    "as discussed , to handle more general error models , one can substitute the log - likelihood @xmath268 $ ] for @xmath269 in @xmath270 $ ] .",
    "overall , we expect our exploration of different bayesian ensemble refinement approaches to serve both as a basis for practical applications and as a starting point for further investigations .",
    "in particular , we have here not considered an orthogonal refinement approach in which one seeks to represent the ensemble by a minimal set of structures.@xcite as we had shown before , eros and minimal ensemble refinement , properly interpreted , can give consistent results.@xcite however , the relation of the different methods is not well understood , e.g. , concerning the limiting behavior for large sample sizes .",
    "we thank drs .",
    "pilar cossio and roberto covino , and profs .",
    "andrea cavalli , kresten lindorff - larsen , benot roux , andrej sali , and michele vendruscolo for helpful discussions .",
    "this work was supported by the max planck society .",
    "we define probability densities of the observables alone by integrating out all other degrees of freedom , @xmath271,\\\\    p({\\mathbf{y}})&=&\\int d{\\mathbf{x}}\\,p({\\mathbf{x}})\\prod_{i=1}^m\\delta[y_i - y_i({\\mathbf{x } } ) ] .",
    "\\end{aligned}\\ ] ] according to eqs .",
    "( [ eq:34 ] ) and ( [ eq:34b ] ) , these two distributions are related to each other , @xmath272 for possibly correlated gaussian errors , where the generalized forces @xmath273 have to be determined self - consistently such that @xmath274 as a consequence , @xmath275 $ ] where superscript @xmath33 indicates the transpose in vector - matrix notation .",
    "the biasing potential thus assumes a functional form linear in the @xmath23 , as seen in standard maximum entropy approaches ( see , e.g. , refs . ) , but the generalized forces @xmath276 take on different values here .",
    "we also note that the forces @xmath273 defining the optimal distribution can be interpreted mechanically . with eq .",
    "( [ eq : a2 ] ) one finds that the mean force trying to `` restore '' the reference distribution , @xmath277 /\\partial{\\mathbf{y}}=-\\theta^{-1 } \\bm{\\sigma}^{-1}\\mathbf{f}$ ] , is exactly balanced by the mean force to fit the data , @xmath278 , up to a factor @xmath54 , with @xmath67 from eq .",
    "( [ eq:4 ] ) with @xmath279 instead of @xmath280 .",
    "formally , the @xmath273 can be obtained by solving @xmath36 coupled nonlinear equations .",
    "we define generating functions @xmath281 $ ] and @xmath282 $ ] , assuming that the integrals exist .",
    "multiplying eq .",
    "( [ eq : a2 ] ) by @xmath283 and integrating over @xmath284 , we obtain @xmath285 }    { \\phi_0\\left[-\\theta^{-1}(\\bm{\\sigma}^{-1}){\\mathbf{f}}\\right]},\\ ] ] where the denominator ensures normalization , @xmath286 . with @xmath287 ,",
    "( [ eq : a3 ] ) for the vector of forces @xmath288 becomes satisfy @xmath289}{\\partial \\mathbf{z}}\\right|_{{\\mathbf{z}}=0}-\\mathbf{y}^{{(\\mathrm{obs})}},\\ ] ] where @xmath290 is the cumulant generating function of the reference distribution of observables .    in cases",
    "where the equations can not be solved directly , one can determine the force - vector @xmath288 adaptively .",
    "in the following , we present a simple algorithm that can be combined with existing simulation procedures .",
    "this approach is related to that of white and voth,@xcite in which an adaptive gradient - based method is used to construct a distribution in which the @xmath291-averages exactly match @xmath279 . here ,",
    "by contrast , we include measurement errors and thus do not demand exact agreement with the observed values . instead",
    ", the @xmath273 have to be determined self - consistently to satisfy eq .",
    "( [ eq : a3 ] ) . in our adaptive optimization",
    ", we adjust the generalized forces @xmath273 `` on the fly '' according to the running averages of the @xmath291 , @xmath292 - \\mathbf{y}^{{(\\mathrm{obs})}},\\ ] ] with initial value @xmath293 $ ] . here",
    ", the trajectory @xmath294 evolves according to the time - dependent potential energy @xmath295 .",
    "we note that by extending the phase space to include both @xmath16 and @xmath288 , this algorithm can be cast in a markovian form .",
    "if @xmath296 is the liouville evolution operator for the phase space density of @xmath16 according to the molecular dynamics or monte carlo simulation protocol and potential @xmath297 with fixed @xmath288 , then the extended phase space density @xmath298 satisfies a markovian liouville - type evolution equation @xmath299     \\rho.\\ ] ] using this relation , one can show that for the gaussian example in the main text , with the mean as observable and overdamped diffusion for the dynamics of @xmath16 , the adaptive sampling is globally converging to the optimal bayesian ensemble distribution .",
    "for the example in fig .",
    "[ fig:2 ] , with monte carlo sampling of @xmath148 , we observed convergence numerically .",
    "we note that in the adaptive determination of the generalized forces @xmath273 defining the posterior distribution , variants of eq .",
    "( [ eq : a6 ] ) are possible .",
    "in particular , one can average between an initial guess @xmath300 and the evolving mean , e.g. , as @xmath301[t^{-1}\\int_0^t d\\tau\\,{\\mathbf{y}}[{\\mathbf{x}}(\\tau ) ] - \\mathbf{y}^{{(\\mathrm{obs})}}]$ ] , where @xmath302 is a weight function that decreases to zero with time , e.g. , @xmath303 for a suitably chosen relaxation time @xmath304 .",
    "55ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ] ",
    "+ 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop * * ( ) link:\\doibase    10.1016/j.str.2015.05.013 [ * * ,   ( ) ] @noop * * ,   ( ) link:\\doibase 10.1126/science.1228565 [ * * ,   ( ) ] @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) link:\\doibase 10.1002/pro.2511 [ * * ,   ( ) ] link:\\doibase    10.1007/s10858 - 015 - 9971 - 2 [ ( ) ] @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( ,  ,  ) @noop * * ,   ( ) link:\\doibase 10.1103/physreve.75.041119",
    "[ * * ,   ( ) ] @noop * * ,   ( ) @noop * * ,   ( ) `` , ''  ( ,  ,  )  chap .  ,  ed . @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop `` , ''   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * , ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * , ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( )"
  ],
  "abstract_text": [
    "<S> we describe different bayesian ensemble refinement methods , examine their interrelation , and discuss their practical application . with ensemble refinement , </S>",
    "<S> the properties of dynamic and partially disordered ( bio)molecular structures can be characterized by integrating a wide range of experimental data , including measurements of ensemble - averaged observables . </S>",
    "<S> we start from a bayesian formulation in which the posterior is a functional that ranks different configuration space distributions . by maximizing this posterior , we derive an optimal bayesian ensemble distribution . for discrete configurations , </S>",
    "<S> this optimal distribution is identical to that obtained by the maximum entropy `` ensemble refinement of saxs '' ( eros ) formulation . </S>",
    "<S> bayesian replica ensemble refinement enhances the sampling of relevant configurations by imposing restraints on averages of observables in coupled replica molecular dynamics simulations . </S>",
    "<S> we show that the strength of the restraint should scale linearly with the number of replicas to ensure convergence to the optimal bayesian result in the limit of infinitely many replicas . in the `` bayesian inference of ensembles '' ( bioen ) </S>",
    "<S> method , we combine the replica and eros approaches to accelerate the convergence . an adaptive algorithm can be used to sample directly from the optimal ensemble , without replicas . </S>",
    "<S> we discuss the incorporation of single - molecule measurements and dynamic observables such as relaxation parameters . the theoretical analysis of different bayesian ensemble refinement approaches provides a basis for practical applications and a starting point for further investigations . </S>"
  ]
}