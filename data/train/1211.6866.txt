{
  "article_text": [
    "krylov iterative solvers @xcite have been very popular for the large sparse linear system @xmath5 where @xmath1 is a real nonsingular @xmath6 matrix and @xmath7 is an @xmath8-dimensional real vector .",
    "however , when @xmath1 has bad spectral property or is ill conditioned , the solvers generally exhibit extremely slow convergence and necessitate preconditioning techniques .",
    "sparse approximate inverse ( sai ) preconditioning aims to compute a preconditioner @xmath9 directly so as to improve the conditioning of ( [ eq : axb ] ) for the vast majority of problems , and it is nowadays one class of important general - purpose preconditioning techniques for krylov solvers @xcite .",
    "there are two typical kinds of sai preconditioning approaches .",
    "one of them constructs a factorized sparse approximate inverse .",
    "an effective algorithm of this kind is the approximate inverse ( ainv ) algorithm , which is derived from the incomplete ( bi)conjugation procedure @xcite .",
    "the other kind is based on f - norm minimization and is inherently parallelizable .",
    "this kind of preconditioners are more robust and general .",
    "the approach constructs @xmath9 by minimizing @xmath10 for a specified pattern of @xmath11 that is either prescribed in advance or determined adaptively , where @xmath12 denotes the f - norm of a matrix .",
    "the most popular f - norm minimization - based sai preconditioning technique may be the adaptive sparse approximate inverse ( spai ) procedure  @xcite , which has been widely used .",
    "the adaptive power sparse approximate inverse ( psai ) procedure with dropping , advanced in @xcite and called psai(@xmath13 ) , is also an effective f - norm minimization - based sai preconditioning technique and has been shown to be at least competitive with spai numerically and can outperform spai for some practical problems . a hybrid version , i.e. , the factorized approximate inverse ( fsai ) preconditioning based on f - norm minimization , has been introduced in @xcite .",
    "fsai is generalized to block form , called bfsai in @xcite . an adaptive algorithm in @xcite",
    "is presented that generates automatically the nonzero pattern of a bfsai preconditioner .",
    "in addition , the idea of f - norm minimization is generalized in @xcite by introducing a sparse readily inverted _ target _ matrix @xmath14 .",
    "@xmath11 is then computed by minimizing @xmath15 over a space of matrices with a prescribed sparsity pattern , where @xmath16 is the generalized f - norm defined by @xmath17 with @xmath18 being some symmetric positive definite matrix , the superscript @xmath14 denotes the transpose of a matrix or vector .",
    "a good comparison of factorized sai and f - norm minimization based sai preconditioning approaches can be found in @xcite .",
    "spare approximate inverses have been shown to provide effective smoothers for multigrid ; see , e.g. , @xcite .",
    "for a comprehensive survey on preconditioning techniques , we refer the reader to @xcite .    throughout this paper ,",
    "we will frequently use two keywords  regular sparse \" and  irregular sparse \" for a matrix . by regular sparse ,",
    "we , qualitatively and sensibly , mean that all the columns of the matrix are sparse , and no column has much more nonzero entries than the others . by irregular sparse , we mean that there are some ( relatively ) dense columns , each of which has considerably more nonzero entries than the other sparse columns .",
    "we call such columns irregular and denote by @xmath2 the number of them . problem ( [ eq : axb ] ) with @xmath1 irregular sparse is quite common and arises in semiconductor device problem , power network problem , circuit simulation , optimization problem and many others @xcite .",
    "quantitatively , we will declare a matrix irregular sparse if it has at least one column that has @xmath19 nonzero entries or more , where @xmath20 is the average number of nonzero entries per column . under this definition ,",
    "we investigate all the matrices in the university of florida sparse matrix collection @xcite , which contains 2649 matrices and of them 1978 are square .",
    "we find that 682 out of these 1978 matrices are irregular sparse .",
    "that is , 34% of the matrices in the collection are irregular sparse . in the collection , there are some social networks , citation networks , and other graphs that are not typically viewed as linear systems .",
    "they often have dense columns . but even if these matrices are removed , there are 30% irregular sparse matrices , 464 out of 1554 , in the collection .",
    "so irregular sparse linear systems have a wide broad of applications .",
    "the possible success of any sai preconditioning procedure is based on the crucial assumption that @xmath1 has good sparse approximate inverses . under this assumption , throughout the paper we consider the case that @xmath1 is irregular sparse .",
    "it is empirically observed that good sparse approximate inverses of @xmath1 are irregular sparse too . in the context , we are concerned with the adaptive spai and psai(@xmath13 ) procedures . as is seen , since the number of nonzero entries in an individual column of the final @xmath11 in spai is bounded by the maximum number of most profitable indices per loop times the maximum loops , a column of @xmath11 may have not enough nonzero entries . as a result ,",
    "@xmath11 obtained by spai may not approximate @xmath21 well and thus may be ineffective for preconditioning .",
    "moreover , it can be justified that the spai algorithm may be very costly to implement . for psai(@xmath13 ) , we may also suffer the unaffordable overhead from solving some possibly large ls problems ( [ lsprob ] ) , although it is more likely to construct effective preconditioners no matter whether @xmath1 is regular sparse or not .",
    "remarkably , it turns out that the situation mentioned above is improved substantially if @xmath1 is regular sparse . with reasonable parameters , spai and psai(@xmath13 )",
    "are efficient .",
    "furthermore , spai and , especially , psai(@xmath13 ) are more likely to construct effective sparse approximate inverses .",
    "it is well known @xcite that the computational consumption , stability and effectiveness of factorized sai preconditioners are generally sensitive to reorderings of @xmath1 .",
    "unfortunately , reorderings do not help for spai and psai(@xmath13 ) .",
    "the reason is that reorderings do not change the irregularity of sparsity patterns of @xmath1 , @xmath21 and good sparse approximate inverses of @xmath1 .",
    "therefore , with reorderings used , spai and psai(@xmath13 ) may still be very costly to implement , and spai may still be ineffective for preconditioning .",
    "we refer the reader to @xcite for the relevant arguments about spai , which are valid for psai(@xmath13 ) as well .    because of the above features",
    ", we naturally come up with the idea of transforming the irregular sparse problem ( [ eq : axb ] ) into some regular sparse one(s ) , on which spai and psai(@xmath13 ) may work well .",
    "it will appear that the sherman  morrison ",
    "woodbury formula @xcite provides us a powerful tool and can be used a key step towards our goal .",
    "we present an approach to splitting @xmath1 into a regular sparse matrix @xmath3 and a matrix of low rank @xmath2 and to transforming ( [ eq : axb ] ) into the @xmath4 new linear systems with the same coefficient matrix @xmath3 . by exploiting the sherman  morrison ",
    "woodbury formula , we can recover the solution of ( [ eq : axb ] ) from the ones of the @xmath4 systems directly .",
    "we consider numerous practical issues on how to obtain a desired splitting of @xmath1 , how to define and compute an approximate solution of ( [ eq : axb ] ) via those of the new systems , and how accurately we should solve the new systems , etc .",
    "a remarkable merit of this approach is that spai and psai(@xmath13 ) are efficient to construct possibly effective preconditioners for the new systems , making krylov solvers converge fast .",
    "the price we pay is to solve @xmath4 linear systems .",
    "but a great bonus is that we only need to construct one effective sparse approximate inverse @xmath11 efficiently for the @xmath4 systems .",
    "the price is generally insignificant as it is typical that the construction of an effective @xmath11 dominates the whole cost of krylov iterations even in a parallel computing environment @xcite . as a matter of fact , due to inherent parallelizations of spai and psai(@xmath13 ) ,",
    "sai type precondtioners are attractive for solving a sequence of linear systems with the same coefficient matrix , as has been addressed in the literature , e.g. , @xcite .",
    "therefore , given the fact that irregular sparse linear systems are quite common in applications , our approach widely extends the practicality of spai and psai(@xmath13 ) .",
    "the paper is organized as follows . in  [ overview ] , we review spai and psai(@xmath13 ) procedures and shed light on the above - mentioned features for @xmath1 irregular sparse and regular sparse , respectively . in  [ approach ] , we describe our new approach for solving ( [ eq : axb ] ) with @xmath1 irregular sparse . in  [ issue ] we consider numerous theoretical and practical issues , establishing some results on the nonsingularity of @xmath3 obtained from certain important and widely useful classes of matrices @xmath1 and drawing some claims on the conditioning of @xmath3 . in  [ numerexp ] ,",
    "we report numerical experiments to demonstrate the superiority of our approach to spai and psai(@xmath13 ) applied to ( [ eq : axb ] ) directly and the superiority of psai(@xmath13 ) to spai for both irregular and regular sparse linear systems .",
    "finally , we conclude the paper in  [ conclude ] .",
    "in this section , we overview the spai and psai(@xmath13 ) procedures and shed light on the facts that ( i ) spai and psai(@xmath13 ) may be very costly and ( ii ) spai may not be effective for preconditioning if @xmath1 is irregular sparse .    during loops each of spai and psai(@xmath13 )",
    "solves a sequence of constrained optimization problems of the form @xmath22 where @xmath23 is the set of matrices with a given sparsity pattern @xmath24 .",
    "denote by @xmath25 the set of @xmath8-dimensional vectors whose sparsity pattern is @xmath26 . then ( [ miniz ] ) is decoupled into @xmath8 independent constrained least squares ( ls ) problems @xmath27 with @xmath28 the @xmath29-th column of the @xmath6 identity matrix @xmath30 .",
    "here and hereafter , the norm @xmath31 denotes the vector 2-norm or the matrix spectral norm . for each @xmath29 , let @xmath32 , @xmath33 be the set of indices of nonzero rows of @xmath34 , @xmath35 and @xmath36 .",
    "then ( [ subprob ] ) is reduced to the smaller unconstrained ls problems @xmath37 which can be solved by the qr decomposition in parallel .",
    "spai and psai(@xmath13 ) determine the sparsity pattern @xmath38 dynamically : starting with a simple initial pattern , say the pattern of @xmath28 , @xmath38 is augmented or adjusted adaptively until the residual norm @xmath39 falls below a given tolerance or the maximum number of augmentations is reached .",
    "the distinction between spai and psai(@xmath13 ) lies in the way that @xmath40 is augmented or adjusted . as is clear from  [ spai ] and  [ psai ] ,",
    "the already existing positions of nonzero entries in @xmath41 for spai are retained in subsequent loops , and at each loop a few most profitable indices added , which are selected from a certain new set generated at the current loop .",
    "psai(@xmath13 ) aims to adaptively drop the entries whose sizes are below certain tolerances and only retain the remaining large ones during the determination of @xmath11 , and @xmath40 is adjusted dynamically by not only absorbing new members but also discarding the positions where the entries of @xmath41 become small during loops .",
    "in other words , at each loop psai(@xmath13 ) determines the positions of entries of large magnitude in a globally optimal sense , while spai does the job locally by adding a few ones from a local pattern generated at the current loop .",
    "therefore , psai(@xmath13 ) may capture a more effective sparsity pattern of @xmath21 than spai .",
    "denote by @xmath42 the sparsity pattern of @xmath41 after @xmath43 loops of augmentation starting with a given initial pattern @xmath44 , and by @xmath45 the set of indices of nonzero rows of @xmath46 .",
    "let @xmath47 , and @xmath48 be the solution of ( [ lsprob ] ) .",
    "then the residual of ( [ subprob ] ) is @xmath49 for @xmath50 , denote by @xmath51 the set of indices @xmath43 for which @xmath52 , and by @xmath53 the set of indices of nonzero columns of @xmath54",
    ". then @xmath55 constitutes the new candidates for augmenting @xmath42 in the next loop .",
    "grote and huckle  @xcite suggest to select several most profitable indices from @xmath56 and augment them to @xmath42 to obtain a new sparsity pattern @xmath57 of @xmath41 .",
    "they do this as follows : for each @xmath58 , consider the one - dimensional minimization problem @xmath59 whose solution is @xmath60 the 2-norm @xmath61 of the new residual @xmath62 satisfies @xmath63 the set @xmath64 of the most profitable indices @xmath65 consists of those associated with a few , say , 1 to 5 , smallest @xmath61 and is added to @xmath42 to obtain @xmath57",
    ". update @xmath45 to get @xmath66 by adding the set @xmath67 of indices of new nonzero rows corresponding to @xmath68 .",
    "the new augmented ls problem ( [ lsprob ] ) is solved by updating @xmath48 instead of resolving it .",
    "proceed in such a way until @xmath69 or @xmath43 attains the prescribed maximum @xmath70 of loops , where @xmath71 is a given mildly small tolerance , say @xmath72 .",
    "_ remark 1 .",
    "_ let us consider the computational complexity of spai . for @xmath1 regular sparse , it is straightforward to verify that @xmath73 is also sparse , and both @xmath51 and @xmath56 have only a few elements . as a result ,",
    "the cardinal number of @xmath42 is small , so is the order of @xmath74 in ( [ lsprob ] ) .",
    "therefore , it is cheap to determine the set @xmath64 of the most profitable indices and solve ( [ lsprob ] ) .",
    "however , the situation deteriorates severely when @xmath1 is irregular sparse .",
    "for example , assume that the @xmath29-th column @xmath75 of @xmath1 is relatively dense , and denote @xmath76 .",
    "if @xmath77 and we take @xmath78 , the residual @xmath79 is as dense as @xmath75 in the first loop of spai , causing that @xmath51 , @xmath80 and @xmath56 have big cardinal numbers .",
    "keep in mind that @xmath42 has a very small cardinal number for all @xmath43 as both the maximum of loops and the number of most profitable indices are small .",
    "then it is easily checked that @xmath81 , @xmath80 and @xmath56 always have very big cardinal numbers in subsequent loops . as a consequence ,",
    "suppose that @xmath75 is fully dense , at each loop we have to compute almost @xmath8 numbers @xmath61 , order them and select the most profitable indices in @xmath56 .",
    "generally , at some loop , once a nonzero index of @xmath41 corresponds to an irregular column of @xmath1 , then the resulting residual @xmath82 must be dense , generating big cardinalities of @xmath83 and @xmath56 at the current loop and in subsequent loops .",
    "so spai may be very costly to implement for @xmath1 irregular sparse .    _",
    "remark 2_. spai provides a right preconditioner .",
    "one should notice that @xmath84 may not be so when @xmath1 is irregular sparse .",
    "naturally , one might apply spai to @xmath84 and computes a left preconditioner @xmath11 , whose transpose @xmath85 is a right preconditioner .",
    "however , spai may still be very costly to implement in this way , and in fact it may be more costly : suppose that the @xmath29-th row @xmath86 of @xmath84 is fully dense .",
    "then when computing the @xmath65-th column @xmath87 of @xmath11 , @xmath88 , since @xmath86 is dense , we generally have @xmath89 .",
    "this means that the @xmath29-th component of the residual @xmath90 is nonzero and thus @xmath91 at each loop . as a result ,",
    "when computing each column @xmath87 of @xmath11 , the cardinalities of @xmath53 and @xmath56 are about @xmath8 at each loop , and we have to compute almost @xmath8 numbers @xmath92 , order them and select a few most profitable indices at each loop .",
    "such kind of feature is the same for all @xmath88 .",
    "consequently , the situation is now more severe than spai working on @xmath1 directly , and generally it is more costly to apply spai to @xmath84 when @xmath1 is irregular sparse .    _",
    "remark 3_. for the irregular sparse @xmath1 , suppose that it has good sparse approximate inverses .",
    "then they are typically irregular sparse too .",
    "suppose the @xmath29-th column of a good sparse approximate inverse is irregular .",
    "the description of spai shows that if @xmath75 is irregular , i.e. , relatively dense , then the sets @xmath56 in ( [ candi ] ) have big cardinal numbers during loops .",
    "nevertheless , spai simply takes the set @xmath64 to be _ only a few _ most profitable indices from @xmath56 at each loop .",
    "if the cardinality of @xmath64 is fixed small , say 5 , the default value as suggested and used in @xcite , then the @xmath29-th column of @xmath11 is sparse and may not approximate the @xmath29-th column of @xmath21 well unless the loops @xmath70 is large enough .",
    "since the number of nonzero entries in an individual column of the final @xmath11 in spai is bounded by the maximum number of most profitable indices per loop times the maximum loops @xmath70 , which is fairly small , say 20 ( the default value is 5 in @xcite ) , a column of @xmath11 may have not enough nonzero entries . as a result , @xmath11 obtained by spai",
    "may not approximate @xmath21 well and is thus ineffective for preconditioning .    _",
    "remark 4_. we point out that there are pathological regular sparse matrices whose good sparse approximate inverses are irregular sparse .",
    "thus it is possible , and in fact quite common in practice , that a regular sparse matrix causes problems for spai .",
    "this is a case for sparse matrices arising from finite differences , volumes or elements of some pdes .",
    "a two - level sparse approximate inverse preconditioning proposed by chen @xcite attempts to handle this class of problems , in which spai is used to first compute a right preconditioner @xmath93 of @xmath1 and then compute a left preconditioner @xmath94 of the sparsification of @xmath95 .",
    "numerically , this procedure can be effective for preconditioning a number of problems , but it lacks theoretical justification and may encounter difficulty since the sparsification of @xmath95 is crucial but it can only be done empirically .",
    "we first review the basic psai ( bpsai ) procedure @xcite . from the cayley ",
    "hamilton theorem , @xmath21 can be expressed as a matrix polynomial of @xmath1 of degree @xmath96 with @xmath97 : @xmath98 with @xmath99 and @xmath100 @xmath101 being certain constants .",
    "denote @xmath102 by the sparsity pattern of a matrix or vector , and write the matrix @xmath103 .",
    "it is obvious that @xmath104 . for a given small positive integer @xmath105",
    ", the pattern @xmath24 of a sparse approximate inverse @xmath11 is taken as a subset of @xmath106 in bpsai .",
    "therefore , the pattern @xmath40 of the @xmath29-th column @xmath41 of @xmath11 is a subset of @xmath107 since @xmath108    the adjustment of @xmath40 for @xmath109 proceeds dynamically as follows . for @xmath110 ,",
    "denote by @xmath42 the sparsity pattern of @xmath41 at the current loop @xmath43 and by @xmath45 the set of indices of nonzero rows of @xmath46 .",
    "set @xmath111 .",
    "then @xmath112 with @xmath113 .",
    "the sparsity pattern @xmath57 is updated as @xmath114 . in the next loop we solve the augmented ls problem @xmath115 with @xmath116 , where @xmath67 is the set of indices of new nonzero rows corresponding to the set @xmath117 .",
    "this problem corresponds to the small ls problem ( [ lsprob ] ) , whose solution can be updated from @xmath118 efficiently .",
    "proceed in such a way until @xmath69 or @xmath119 .",
    "it has been proved in ( * ? ? ?",
    "* theorem 1 ) that for @xmath78 , as @xmath70 increases , @xmath11 obtained by bpsai may become increasingly denser quickly once one column in @xmath1 is irregular sparse .",
    "in order to control the sparsity of @xmath11 and construct an effective preconditioner , some reasonable dropping strategies should be used .",
    "two practical psai(@xmath120 ) and psai(@xmath13 ) algorithms have been proposed in @xcite .",
    "psai(@xmath120 ) aims to retain at most @xmath120 entries of large magnitude in @xmath41 . to be more flexible",
    ", @xmath120 may vary with @xmath29 . since an effective sparsity pattern of @xmath21 is generally unknown in advance",
    ", it is difficult to prescribe a reasonable @xmath120 .",
    "this is a shortcoming similar to spai .",
    "in contrast , for the newly computed @xmath41 at loop @xmath43 , psai(@xmath13 ) drops those entries of small magnitude below certain tolerances @xmath13 and retains only the ones of large magnitude .",
    "therefore , psai(@xmath13 ) is more reasonable and reliable to capture an effective sparsity pattern of @xmath21 and determines the corresponding entries .",
    "a central issue is the selection of dropping tolerance @xmath13 .",
    "this issue is mathematically nontrivial , and @xmath13 has strong effects on the effectiveness of psai(@xmath13 ) and many other sai preconditioning procedures .",
    "the authors @xcite have proposed effective and robust dropping criteria for psai(@xmath13 ) and all the static f - norm minimization - based sai preconditioning procedures . for psai(@xmath13 ) , it is shown that , at loop @xmath121 , a nonzero entry @xmath122 is dropped for @xmath123 if @xmath124 is the number of nonzero entries in a vector or matrix , and @xmath125 is the stopping tolerance for spai and bpsai .",
    "we comment that @xmath41 in ( [ tolk ] ) is the newly computed one at loop @xmath43 .",
    "this criterion makes @xmath11 as sparse as possible and meanwhile has similar preconditioning quality to the possibly much denser one obtained by bpsai @xcite .",
    "more precisely , for the final @xmath11 obtained by psai(@xmath13 ) , if ( [ tolk ] ) is used then the residual norm @xmath126 when the residual norm of each column of the preconditioner obtained by bpsai falls below @xmath71 .",
    "_ if @xmath1 is regular sparse , it is direct to justify that the size of @xmath74 in ( [ lsprob ] ) is small for a small @xmath70 and it is cheap to solve ( [ lsprob ] ) .",
    "however , the situation changes sharply for @xmath1 irregular sparse .",
    "suppose that the @xmath29-th column @xmath75 of @xmath1 is relatively dense and and take @xmath78 .",
    "then the resulting @xmath41 is also relatively dense since @xmath127 .",
    "therefore , ( [ lsprob ] ) is a relatively large ls problem .",
    "since @xmath114 , ( [ lsprob ] ) is always a large ls problem at each subsequent loop @xmath121 .",
    "therefore , if @xmath75 is dense , psai(@xmath13 ) is very costly and impractical .",
    "as we have seen , spai and psai(@xmath13 ) may be very costly to implement for @xmath1 irregular sparse , and @xmath11 obtained by spai may be ineffective for preconditioning ( [ eq : axb ] ) . in this section ,",
    "we attempt to transform ( [ eq : axb ] ) into some regular sparse ones , making spai and psai(@xmath13 ) more practical to construct possibly effective @xmath11 .",
    "it turns out that the following sherman  morrison  woodbury formula ( see @xcite and @xcite ) provides us a powerful tool for our purpose .",
    "[ thm1 ] let @xmath128 with @xmath129 .",
    "if @xmath1 is nonsingular , then @xmath130 is nonsingular if and only if @xmath131 is nonsingular .",
    "furthermore , @xmath132    the formula is typically of interest for @xmath133 , and it is called the sherman  morrison formula when @xmath134 . for a good survey on the history and applications ,",
    "we refer the reader to @xcite .    for our purpose ,",
    "assume that the @xmath135-th columns of @xmath1 are irregular and the remaining @xmath136 ones are sparse .",
    "denote by @xmath137 the matrix consisting of the @xmath2 irregular columns of @xmath1 , and by @xmath138 the sparsification of @xmath139 that drops some of its nonzero entries , so that each column of @xmath140 is as sparse as the other @xmath136 columns of @xmath1 .",
    ". then the nonzero entries of @xmath142 are just those dropped ones of @xmath139 .",
    "let @xmath3 be the matrix that is obtained from @xmath1 by replacing its dense columns @xmath143 by the sparse vectors @xmath144 , @xmath145 .",
    "then @xmath3 is regular sparse and satisfies @xmath146 where @xmath147 with @xmath148 the @xmath149-th column of the @xmath6 identity matrix @xmath150 .",
    "assume that @xmath3 is nonsingular .",
    "then it follows from ( [ sherman  morrison ",
    "woodbury1 ] ) that @xmath151 therefore , the solution of ( [ eq : axb ] ) is @xmath152 this amounts to solving a new regular sparse linear system @xmath153 and the other @xmath2 regular sparse linear systems @xmath154 if the exact solutions to ( [ regprob ] ) and ( [ auxi ] ) were available , we would get the solution @xmath155 of ( [ eq : axb ] ) from ( [ update ] ) by solving the small @xmath156 linear system with the coefficient matrix @xmath157 and the right - hand side @xmath158 .",
    "we can summarize the above approach as procedure * .",
    "find @xmath2 and @xmath139 , and sparsify @xmath139 to get @xmath140 .",
    "define @xmath159 and the regular sparse matrix @xmath160 , where @xmath147 .",
    "solve @xmath4 linear systems ( [ regprob ] ) and ( [ auxi ] ) for @xmath161 and @xmath162 , respectively .",
    "let @xmath163 and compute the solution @xmath155 of @xmath0 by @xmath164    for regular sparse systems ( [ regprob ] ) and ( [ auxi ] ) , we suppose that only iterative solvers are viable in our context .",
    "now , a big and direct reward is that spai and psai(@xmath13 ) can be implemented much more efficiently to construct preconditioners for the @xmath4 regular sparse systems with the same regular sparse coefficient matrix @xmath3 .",
    "furthermore , compared with the irregular sparse case , spai is more likely to construct an effective preconditioner now .",
    "when iterative solvers are used , recovering an _ approximate _ solution of ( [ eq : axb ] ) via those of ( [ regprob ] ) and ( [ auxi ] ) is quite involved and is not as simple as procedure * indicates , in which the exact @xmath161 and @xmath165 are assumed . in order to use procedure * to develop a practical iterative solver for ( [ eq : axb ] ) , we first need to handle several theoretical and practical issues .",
    "the first issue is about the quantitative meaning of irregular columns , by which we define @xmath139 .",
    "obviously , like sparsity itself and many other quantities in numerical analysis , it appears impossible to give a precise definition of it .",
    "in fact , it is also unnecessary to do so . in our experiments",
    ", we empirically find that the threshold @xmath19 is a good choice , where @xmath166 is the average number of nonzero entries per column of @xmath1 .",
    "if the number of nonzero entries in a column exceeds @xmath19 , then we mark it as an irregular column . based on this criterion , we determine all the irregular columns of @xmath1 and the number @xmath2 of them .",
    "numerically , we have found that other thresholds ranging from @xmath167 to @xmath168 work well and exhibit no essential difference .",
    "so our approach is insensitive to thresholds .",
    "the second issue is which nonzero entries in @xmath139 should be dropped to get @xmath140 and generate @xmath3 . in principle , the number @xmath169 of nonzero entries in each column of @xmath140 should be comparable to @xmath20 . for the choice of @xmath169 , to be unique , we simply propose taking @xmath170 .",
    "given such @xmath169 , there may be many dropping ways .",
    "two obvious approaches can be adopted .",
    "the first approach is to retain the diagonal and the @xmath171 nonzero entries nearest to the diagonal in each column of @xmath139 .",
    "the second approach is to retain the diagonal and the other @xmath171 largest entries in magnitude of each column of @xmath139 .",
    "numerically , two approaches have exhibited very similar behavior .",
    "therefore , we will take the first approach and report the results obtained .",
    "the third issue is on the non - singularity of @xmath3 , which is crucial both in theory and practice .",
    "first of all , we present the following results .",
    "[ thm3 ] @xmath3 constructed above is nonsingular for the following classes of matrices :    @xmath1 is strictly ( row or column ) diagonally dominant .",
    "@xmath1 is irreducibly ( row or column ) diagonally dominant .",
    "@xmath1 is an @xmath11-matrix .",
    "if @xmath1 is strictly ( row or column ) diagonally dominant , @xmath3 is so too since it removes some off - diagonal nonzero entries of @xmath1 .",
    "therefore , @xmath3 is nonsingular @xcite .",
    "( ii ) . for @xmath1",
    "irreducibly ( row or column ) diagonally dominant , @xmath3 is either irreducible or reducible .",
    "if @xmath3 is irreducible , then it must be irreducibly ( row or column ) diagonally dominant .",
    "so @xmath3 is nonsingular @xcite .",
    "if @xmath3 is reducible , without loss of generality we suppose that there is a permutation matrix @xmath172 such that @xmath173 where @xmath174 and @xmath175 are irreducibly square matrices . since @xmath1 is irreducibly ( row or column ) diagonally dominant , @xmath176 is so too .",
    "partition @xmath177 conformingly .",
    "then each of @xmath178 and @xmath179 must have nonzero entries ; otherwise , @xmath1 is reducible .",
    "so @xmath180 and @xmath181 must be ( row or column ) diagonally dominant , and at least one row or column in each of them is strictly diagonally dominant .",
    "note that all the nonzero entries of @xmath182 are the same as the corresponding ones of @xmath176 .",
    "therefore , @xmath174 and @xmath175 are ( row or column ) diagonally dominant , and at least one row or column in each of them is strictly diagonally dominant .",
    "so , both @xmath174 and @xmath175 are ( row or column ) diagonally dominant .",
    "since @xmath174 and @xmath175 are irreducible , both of them are nonsingular @xcite , which means that @xmath182 is nonsingular , so is @xmath3 .",
    "( iii ) . by the definition in @xcite ,",
    "an @xmath11-matrix assumes its non - singularity .",
    "theorem 3.25 of @xcite states that any matrix @xmath183 obtained from the nonsingular @xmath11-matrix @xmath1 by setting certain off - diagonal entries of @xmath1 to zero is also a nonsingular @xmath11-matrix .",
    "since our @xmath3 is just such a @xmath183 , it is nonsingular .",
    "three classes of matrices in the theorem have a wide broad of applications , e.g. , discretizations of second - order odes and elliptic pdes , quantum chemistry , information theory , stochastic process , systems theory , networks and modern economics , to name only a few .",
    "strictly row diagonally dominant matrices are a class of special @xmath18-matrices ; see ( * ? ? ?",
    "* theorem 3.27 , p. 92 ) .",
    "actually , the first two classes of matrices in the theorem can be extended to more general forms , as the following corollary states .",
    "[ cor1 ] @xmath3 is nonsingular for the following matrices :    there exists a nonsingular diagonal matrix @xmath184 for which @xmath185 or @xmath186 is strictly row or column diagonally dominant , respectively .",
    "there exist permutation matrices @xmath172 and @xmath187 for which @xmath188 is strictly ( row or column ) diagonally dominant .",
    "there are a nonsingular diagonal matrix @xmath184 and permutation matrices @xmath172 and @xmath187 for which @xmath189 or @xmath190 is strictly row or column diagonally dominant .",
    "the irreducible analogues of the matrices in ( i)(iii ) .",
    "@xmath1 is a h - matrix .",
    "the proofs of parts ( i)(iv ) are direct from those of theorem  [ thm3 ] .",
    "assertion 5 holds because of a result of @xcite , which states that a necessary and sufficient condition for @xmath1 to be a @xmath18-matrix is that there exists a nonsingular diagonal matrix @xmath184 for which @xmath185 is strictly row diagonally dominant .    from the proof of (",
    "v ) in corollary  [ cor1 ] , we see that h - matrices are a subclass of matrices in ( i )",
    ".    there should be more classes of matrices for which the resulting @xmath3 is nonsingular theoretically .",
    "we do not pursue this topic further in this paper . for a general real - world",
    "nonsingular @xmath1 even though it does not belong to the classes of matrices in theorem  [ thm3 ] and corollary  [ cor1 ] .",
    "the fourth issue is on the conditioning of @xmath3 . for a general @xmath1 ,",
    "it is possible to get either a well - conditioned or ill - conditioned @xmath3 . given that @xmath1 is generally ill conditioned , the former is more preferable , but we should not expect too much and the latter is more possible . theoretically speaking , @xmath3 may be worse or better conditioned than @xmath1 . for a general irregular sparse @xmath1 , numerical experiments will indicate that @xmath191 is rarely worse conditioned and in fact often , though not always , better conditioned than @xmath1 .",
    "however , for the first and third classes of matrices in theorem  [ thm3 ] , we can analyze the conditioning of @xmath3 and show that @xmath3 may be generally better conditioned than @xmath1 .",
    "more precisely , for a strictly ( row or column ) diagonally dominant matrix @xmath1 , it is expected that the 1-norm condition number @xmath192 or the infinity norm condition number @xmath193 is generally no more and may be considerably smaller than @xmath194 or @xmath195 .",
    "similar claims hold for an @xmath11-matrix @xmath1 . next we first look into the case that @xmath1 is strictly row diagonally dominant .",
    "the case that @xmath1 is strictly column diagonally dominant can be treated similarly .",
    "denote @xmath196 , and define the quantities @xmath197 since that @xmath198 and all the nonzero entries @xmath199 , we have @xmath200 , with some strict inequalities holding as @xmath3 drops some off - diagonal nonzero entries in the @xmath2 irregular columns of @xmath1 .",
    "more precisely , from the definitions of @xmath201 and @xmath202 , it can be easily verified that if @xmath203 is in the set of the row indices of nonzero entries in @xmath142 then @xmath204 .",
    "the number of such @xmath203 is ( much ) bigger than @xmath2 and can be very near to @xmath8 whenever @xmath1 has a fully dense column as @xmath205 nonzero entries are dropped from an irregular column and put into a column of @xmath142 . since @xmath1 is strictly row diagonally dominant , we have @xmath206 .",
    "a result of @xcite gives the following general bounds @xmath207 with @xmath208 if all @xmath209 and @xmath210 if all @xmath211 ; see , e.g. , @xcite . since @xmath212 with some strict inequalities holding , the bound for @xmath213 can be smaller than that for @xmath214 . on the other hand",
    ", it always holds that @xmath215 so , @xmath216 is generally no more than @xmath217 and may be considerably smaller than the latter , provided that some dropped nonzero entries @xmath218 from @xmath1 are not small . for @xmath1 strictly column diagonally dominant , we define similar @xmath201 and @xmath202 in the column sense .",
    "then similar discussions and the same claims can be made for the 1-norm condition number @xmath192 and @xmath194 .",
    "the unique difference is that the number of the corresponding @xmath219 in the column sense is exactly @xmath2 since @xmath1 and @xmath3 only have @xmath2 different columns .",
    "therefore , for @xmath1 strictly ( row or column ) diagonally dominant , it is expected that @xmath3 is better conditioned than @xmath1 .",
    "for @xmath1 an @xmath11-matrix , define @xmath220 it is known that @xmath221 and @xmath222 for @xmath223 by the definition of @xmath11-matrix .",
    "then it is seen from @xmath202 defined above that @xmath224 .",
    "similarly , we have @xmath225 .",
    "it is proved in @xcite that if there is a positive diagonal matrix @xmath184 such that @xmath226 then @xmath227 and furthermore @xmath228 if @xmath229 .",
    "note that we have @xmath230 with some strict inequalities holding as @xmath3 drops some negative off - diagonal entries @xmath218 from @xmath1 and @xmath184 is positive .",
    "thus , the upper bound for @xmath231 is generally smaller than that for @xmath214 . noticing that @xmath232",
    ", we expect that @xmath193 is generally no more and can be smaller than @xmath195 .",
    "similar discussions and claim go to @xmath186 and the 1-norm condition numbers of @xmath1 and @xmath3 .",
    "the fifth issue is on the existence of good sparse approximate inverses of @xmath3 .",
    "the existence is generally definitive .",
    "we argue as follows : since @xmath233 , for a given positive integer @xmath70 , we have @xmath234 and @xmath235 .",
    "assume that we take the initial sparsity @xmath236 and implement spai and psai(@xmath13 ) @xmath70 loops for @xmath1 .",
    "then it is known @xcite that the sparsity patterns of @xmath11 obtained by spai and psai(@xmath13 ) are bounded by @xmath237 and @xmath238 , respectively .",
    "the same are true for the sparsity patterns of @xmath11 obtained by spai and psai(@xmath13 ) for @xmath3 .",
    "this means that the effective envelops for the sparsity patterns of @xmath11 obtained by spai and psai(@xmath13 ) for @xmath3 are contained in those for @xmath1 for the same @xmath70 . as a consequence",
    ", it is expected that @xmath3 has good sparse approximate inverses when @xmath1 does .",
    "the last important issue is how to select stopping criteria for krylov iterations for @xmath4 linear systems so as to recover an approximate solution of ( [ eq : axb ] ) with the prescribed accuracy .",
    "it is seen from ( [ update1 ] ) that the solution @xmath155 of ( [ eq : axb ] ) is formed from the ones of the @xmath4 new systems . recall that the @xmath4 linear systems are now supposed to be solved approximately by preconditioned krylov solvers .",
    "our concerns are ( i ) how to define an approximate solution @xmath239 from the @xmath4 approximate solutions of ( [ regprob ] ) and ( [ auxi ] ) , and ( ii ) how accurately we should solve ( [ regprob ] ) and ( [ auxi ] ) such that @xmath239 satisfies @xmath240 . as it appears below , it is direct to settle down the first concern , but the second concern is involved .",
    "[ thm2 ] let @xmath241 and @xmath242 , be the approximate solutions of ( [ regprob ] ) and ( [ auxi ] ) , respectively , and define @xmath243 and the residuals @xmath244 , @xmath245 .",
    "assume that @xmath246 is nonsingular with @xmath147 , and define @xmath247 .",
    "take @xmath248 to be an approximate solution of ( [ eq : axb ] )",
    ". then if @xmath249 and @xmath250 we have @xmath251    replacing @xmath165 and @xmath161 by their approximations @xmath252 and @xmath241 in ( [ update1 ] ) , we get ( [ composi ] ) , which is naturally an approximate solution of ( [ eq : axb ] ) .",
    ". we obtain @xmath254 from which it follows that @xmath255 by definition of @xmath256 and @xmath257 , we have @xmath258 . if ( [ stopb ] ) and @xmath259 it is seen from ( [ rry ] ) that ( [ stop ] ) holds . since the above relation is just ( [ stopu2 ] ) , the theorem holds .",
    "we point out that because of ( [ rry ] ) our stopping criterion ( [ stopu2 ] ) may be conservative . furthermore , @xmath256 is moderate if @xmath246 is well conditioned , and it may be large if @xmath246 is ill conditioned . since @xmath2 is supposed very small , this theorem indicates that the @xmath2 linear systems ( [ auxi ] ) need to be solved with the accuracy at the level of @xmath260 .",
    "we may solve them by krylov solvers either simultaneously in the parallel environment or independently in the sequential environment .",
    "an alternative approach is to solve them using block krylov solvers .",
    "note that @xmath256 can not be computed until iterations for ( [ regprob ] ) and ( [ auxi ] ) terminate , but the stopping criterion ( [ stopu2 ] ) depends on @xmath256 .",
    "therefore , the computation of @xmath256 and termination of iterations interacts . in implementations",
    ", we simply replace @xmath256 by 1 in ( [ stopu2 ] ) and stop iterative solvers for the @xmath2 systems ( [ auxi ] ) with the modified accuracy requirement .",
    "because of this inaccuracy , ( [ stop ] ) may fail to meet but @xmath261 should be at the level of @xmath260 . in later numerical experiments , we will find that @xmath262 works very well and makes ( [ stop ] ) hold for almost all the test problems , and the right - hand sides of ( [ stop ] ) are only a little bit bigger than @xmath260 in the rare cases where ( [ stop ] ) does not meet .",
    "we make some further comments on ( [ stopu2 ] ) . for a real - world problem",
    ", if @xmath1 is poorly scaled , it is well known that a preprocessing is generally done that uses scaling to equilibrate @xmath1 so that its columns and/or rows are nearly the same in norm . without loss of generality ,",
    "suppose that @xmath263 is comparable to them in size ; otherwise , we replace @xmath7 by a scaled @xmath264 with @xmath265 a scaling factor and instead solve the equivalent problem @xmath266 , so that @xmath267 is comparable to the norms of columns of the equilibrated @xmath1",
    ". then the sizes of @xmath268 are typically around @xmath269 .",
    "we suppose that such processing is performed . as a result ,",
    "@xmath2 linear systems ( [ auxi ] ) are solved with the accuracy at the level of @xmath260 .",
    "therefore , we need not worry about the issue of small @xmath268 for a given problem .",
    "in this section , we test our approach and compare it with the approach that preconditions ( [ eq : axb ] ) by spai and psai(@xmath13 ) directly .",
    "we report the numerical experiments obtained by the biconjugate gradient stablized ( bicgstab ) with spai and psai(@xmath13 ) preconditioning on ( [ eq : axb ] ) and ( [ regprob ] ) , ( [ auxi ] ) , respectively .",
    "such combinations give rise to four algorithms , and we name them standard - spai , new - spai , and standard - psai(@xmath13 ) and new - psai(@xmath13 ) , abbreviated as s - spai , n - spai and s - psai(@xmath13 ) , n - psai(@xmath13 ) , respectively .",
    "the experiments consists of three subsections , and our aims are quadruple : ( i ) we demonstrate the considerable efficiency superiority of n - spai to s - spai and that of n - psai(@xmath13 ) to s - psai(@xmath13 ) .",
    "( ii ) with the same parameters used in spai , we show that preconditioners obtained by n - spai are more effective than the corresponding ones obtained by s - spai .",
    "( iii ) with the same parameters used in psai(@xmath13 ) , we illustrate that the preconditioners by s - psai(@xmath13 ) and n - psai(@xmath13 ) are equally effective for preconditioning each problem , provided that they can be computed .",
    "( iv ) we illustrate that if the numbers of nonzero entries of preconditioners , i.e. , the sparsity of preconditioners , are ( almost ) the same then psai(@xmath13 ) is more effective than spai for preconditioning both irregular and regular sparse linear systems .",
    "the results mean that psai(@xmath13 ) captures a sparsity pattern of @xmath21 and @xmath270 more effectively than spai and thus generate better preconditioners .",
    "they also imply that even for regular sparse linear systems , spai may be ineffective for preconditioning .",
    "we mention that , for other krylov solvers , such as bicg , cgs and the restarted gmres(20 ) , we have done similar numerical experiments and had the same findings as above .",
    "so it suffices to only report and evaluate the results obtained by bicgstab .    before testing our approach ,",
    "we look into all the matrices in the university of florida sparse matrix collection @xcite and give illustrative information on where irregular sparse linear systems come from , how common they are in practice , how big @xmath2 can be and how dense irregular columns .",
    "we divide matrices into their problem domains , and sort them by percentage of matrices in that domain that are irregular .",
    "table [ tabl : irr ] lists the relevant information , where `` per .",
    "irreg '' denotes the percentage of irregular matrices in each domain , `` # reg .",
    "prob '' and `` # irreg .",
    "prob '' are the numbers of regular and irregular matrices in each domain , respectively .",
    "matrices labeled as  graphs \" in the collection are excluded , many of which are irregular , but not all are linear systems .",
    ".statistics of regular / irregular problems in the sparse matrix collection @xcite [ cols=\"^,^,^,^\",options=\"header \" , ]     based on tables  [ psaidata][spaidata2 ] , we next compare the preconditioning effectiveness of s - spai and s - psai(@xmath13 ) and that of n - spai and n - psai(@xmath13 ) , respectively .    as @xmath271 indicates , obviously ,",
    "n - psai(@xmath13 ) is often considerably superior to n - spai for all the test matrices except for @xmath3 resulting from _",
    "rajat12_. the results on the last six larger matrices are more illustrative , where psai(@xmath13 ) exhibited a considerable superiority to spai for regular sparse linear systems . particularly , for _",
    ", when n - spai is applied , bicgstab consumed exactly the maximum 500 iterations to achieve the accuracy requirement , while n - psai(@xmath13 ) only used the maximum 200 iterations ; for _ dc3 _ , the preconditioner produced by n - psai(@xmath13 ) is much more effective than that obtained by n - spai , and bicgstab preconditioned by n - psai(@xmath13 ) is seven times faster than that by n - spai .    when applied to the original irregular sparse ( [ eq : axb ] ) directly , s - psai(@xmath13 ) shows more substantial improvements over s - spai . for _ asic_100k _ , s - psai(@xmath13 ) failed to compute @xmath11 .",
    "s - spai consumed 78 hours to construct a sparse approximate inverse @xmath11 of it , but @xmath11 is much poorer than that obtained by n - spai and bicgstab failed to converge after 500 iterations with the actual relative residual norm @xmath272 . the reason should be that good approximate inverses of the matrix are irregular sparse , but some columns of @xmath11 are too sparse to capture enough entries of large magnitude in the corresponding columns of @xmath21 . for the first four matrices , we see from tables  [ psaidata][spaidata2 ] that two @xmath11 for each @xmath1 have very comparable sparsity , but the results clearly illustrate that s - psai(@xmath13 ) is considerably more effective for preconditioning than s - spai for the four matrices . in terms of @xmath271 , s - psai(@xmath13 ) is eight times , three times , twice and nearly one and a half times as fast as s - spai for the four problems , respectively , as the corresponding @xmath271 indicate .",
    "so s - psai(@xmath13 ) results in a more substantial acceleration of bicgstab than s - spai .",
    "this justifies that psai(@xmath13 ) captures a better sparsity pattern of @xmath21 than spai for @xmath1 irregular sparse and computes a more effective preconditioner .    summarizing the above",
    ", we conclude that psai(@xmath13 ) itself is effective for preconditioning no matter whether a matrix is regular sparse or not , while spai may work well for regular sparse matrices but may be ineffective when @xmath1 is irregular sparse . even for regular sparse linear systems , psai(@xmath13 ) can outperform spai considerably for preconditioning . taking the construction cost of preconditioners by spai and psai(@xmath13 ) into account , to make them computationally practical",
    ", we should apply them to regular sparse linear systems .",
    "therefore , for an irregular sparse linear system , a good means is to transform it into some regular sparse problems , so that spai and psai(@xmath13 ) are relatively efficient for computing possibly effective sparse approximate inverses .    as a last note",
    ", we make some comments on the computational efficiency of spai and psai(@xmath13 ) . since they are different procedures that are derived from different principles and have different features , the computational complexity of each of them is quite involved , and the efficiency depends on several factors including the pattern of @xmath1 itself .",
    "it appears very hard , if not impossible , to compare their flops .",
    "hence we can not draw any definitive conclusion on the efficiency comparison of spai and psai(@xmath13 ) .",
    "there must be cases where one procedure wins the other , and vice versa .",
    "note that the psai(@xmath13 ) code is experimental and written in matlab language for a sequential computing environment , so the performance ( run times ) of psai(@xmath13 ) may be far from optimized .",
    "in contrast , the spai 3.2 code is well programmed in c / mpi designed for distributed parallel computers . a parallel psai(@xmath13 )",
    "code in c / mpi or fortran is involved and will be left as our future work .",
    "we will expect that the performance of psai(@xmath13 ) is improved substantially .",
    "the spai and psai(@xmath13 ) procedures are quite effective for preconditioning linear systems arising from a lot of real - world problems . however , the situation is rather disappointing for quite common irregular sparse linear systems . in this case ,",
    "none of spai and psai(@xmath13 ) works well generally due to the very high cost and/or possibly excessive storage requirement of constructing preconditioners .",
    "however , for a regular sparse linear system , we have shown that spai and , especially , psai(@xmath13 ) are efficient to construct possibly effective preconditioners .",
    "motivated by this crucial feature and exploiting the sherman ",
    "woodbury formula , we have transformed the original irregular sparse linear system into some regular sparse ones , for which spai and psai(@xmath13 ) are practically efficient .",
    "we have considered numerous theoretical and practical issues , including the non - singularity and conditioning of @xmath3 .",
    "we have proved that @xmath3 is ensured to be nonsingular for a number of important classes of matrices , and that its conditioning is generally better than @xmath1 for some of them .",
    "we have derived stopping criteria for iterative solutions of new systems , so that the approximate solution of the original problem achieves the prescribed accuracy . since irregular matrices are quite common in practice",
    ", we have extended the applicability of spai and psai(@xmath13 ) substantially .",
    "numerical experiments have demonstrated that our approach works well and improves the performance of spai and psai(@xmath13 ) substantially .",
    "our approach may be applicable to factorized sparse approximate inverse preconditioning procedures @xcite .",
    "although reorderings of @xmath1 may help such procedures to reduce fill - ins , enhance robustness and improve numerical stability when constructing factorized sai preconditioners , this is not always so and in fact even may make things worse for @xmath1 irregular sparse @xcite .",
    "our approach may be combined with reorderings to construct effective factorized sai preconditioners for regular sparse linear systems resulting from the irregular sparse one .",
    "finally , we should point out that the performance results in this paper ( run times ) are for modest sized and not very large problems , for which a direct solver may be faster than any of the iterative solvers .",
    "but the goal of our paper is to provide a new algorithm that is efficient to construct effective sparse approximate inverses so as to reduce the number of iterations required substantially .",
    "this has implications for very large matrices for which direct solvers are not feasible and for a parallel computing environment .",
    "* acknowledgments*. we thank the editor professor davis very much for his valuable data analysis on all the test matrices in @xcite and for his suggestions .",
    "we are very indebted to the referees for their comments and suggestions .",
    "all of these made us improve the presentation of the paper substantially .                                , _",
    "preconditioning for sparse linear systems at the dawn of the 21st century : history , current developments , and future perspectives _ , isrn applied mathematics volume 2012 , article i d 127647 , 49 pages , doi:10.5402/2012/127647 ."
  ],
  "abstract_text": [
    "<S> we investigate the spai and psai preconditioning procedures and shed light on two important features of them : ( i ) for the large linear system @xmath0 with @xmath1 irregular sparse , i.e. , with @xmath1 having @xmath2 relatively dense columns , spai may be very costly to implement , and the resulting sparse approximate inverses may be ineffective for preconditioning . </S>",
    "<S> psai can be effective for preconditioning but may require excessive storage and be unacceptably time consuming ; ( ii ) the situation is improved drastically when @xmath1 is regular sparse , that is , all of its columns are sparse . in this case , both spai and psai are efficient . </S>",
    "<S> moreover , spai and , especially , psai are more likely to construct effective preconditioners . motivated by these features , we propose an approach to making spai and psai more practical for @xmath0 with @xmath1 irregular sparse . </S>",
    "<S> we first split @xmath1 into a regular sparse @xmath3 and a matrix of low rank @xmath2 . then exploiting the sherman  morrison  </S>",
    "<S> woodbury formula , we transform @xmath0 into @xmath4 new linear systems with the same coefficient matrix @xmath3 , use spai and psai to compute sparse approximate inverses of @xmath3 efficiently and apply krylov iterative methods to solve the preconditioned linear systems . theoretically , we consider the non - singularity and conditioning of @xmath3 obtained from some important classes of matrices . </S>",
    "<S> we show how to recover an approximate solution of @xmath0 from those of the @xmath4 new systems and how to design reliable stopping criteria for the @xmath4 systems to guarantee that the approximate solution of @xmath0 satisfies a desired accuracy . </S>",
    "<S> given the fact that irregular sparse linear systems are common in applications , this approach widely extends the practicability of spai and psai . </S>",
    "<S> numerical results demonstrate the considerable superiority of our approach to the direct application of spai and psai to @xmath0 .    </S>",
    "<S> preconditioning , sparse approximate inverse , irregular sparse , regular sparse , the sherman  morrison  </S>",
    "<S> woodbury formula , f - norm minimization , krylov solver    65f10 </S>"
  ]
}