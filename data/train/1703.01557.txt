{
  "article_text": [
    "* _ two ways of motivation : _ *    option 1 : a programmable framework using a probabilistic logic language for ssl , rooted on the proppr framework .    option 2 : a * tool perspective * : ( 1 ) intro proppr and using it for classification ; ( 2 ) imposing constraints and how proppr conducts training with examples from constraints ( declarative language and plate diagram can be given after here ) ; ( 3 ) experiments on two tasks .",
    "most semi - supervised learning ( ssl ) methods operate by introducing `` soft constraints '' on how a learned classifier will behave on unlabeled instances , with different constraints leading to different ssl methods . for example , logistic regression with entropy regularization @xcite and transductive svms @xcite constrain the classifier to make confident predictions at unlabeled points , and similarly , many graph - based ssl approaches require that the instances associated with the endpoints of an edge have similar labels or embeddings @xcite . other weakly - supervised methods also can be viewed as imposing constraints on predictions made by a classifier : for instance , in distantly - supervised information extraction , a useful constraint requires that the classifier , when applied to the set @xmath0 of mentions of an entity pair that is a member of relation @xmath1 , classifies at least one mention in @xmath0 as a positive instance of @xmath1 @xcite .",
    "here we propose a general approach to modeling ssl constraints .",
    "we will define a succinct declarative language for specifying semi - supervised learners .",
    "we call our declarative ssl framework the _ d - learner_.    as observed by previous researchers @xcite , the appropriate use of ssl is often domain - specific .",
    "the d - learner framework allows us to define various constraints easily .",
    "combined with a tuning strategy with bayesian optimization , we can collectively evaluate the effectiveness of these constraints so as to obtain tailor - made ssl settings for individual problems . to examine the efficacy of d - learner",
    ", we apply it to two tasks : link - based text classification and relation extraction . for the link - based classification task",
    ", we will show the flexibility of our declarative language by defining several ssl constraints for exploiting network structures .",
    "for the relation extraction task , we will show how our declarative language can express several intuitive problem - specific ssl constraints .",
    "comparison against existing ssl methods @xcite shows that d - learner achieves significant improvements over the state - of - the - art on two domains .",
    "we begin with a simple example .",
    "the left - hand side of figure  [ ppr : sl+mutex ] illustrates how traditional supervised classification can be expressed declaratively in a logic program .",
    "following the conventions used in logic programming , capital letters are universally quantified variables .",
    "we extend traditional logic programs by allowing rules to be annotated with a set of _ features _ , which will be then weighted to define a strength for the rule ( as we will describe below ) .",
    "the symbol _ true _ is a goal that always succeeds .",
    "picklabel(@xmath2 ) @xmath3 true _ ,  , _",
    "picklabel(@xmath4 ) @xmath3 true_. ]    d - learner programs are associated with a backward - chaining proof process , can be read either as logical constraints , or as a non - deterministic program , which is invoked when a query is submitted . if the queries processed by the program are all of the form _",
    "predict(@xmath5,y ) _ where @xmath5 is a constant , the theory on the left - hand side of figure  [ ppr : sl+mutex ] can be interpreted as saying : ( 1 ) to prove the goal of the form _",
    "predict(@xmath5,y)_i.e . , to predict a label @xmath6 for the instance @xmath5non - deterministically pick a possible class label @xmath7 , and then prove the goal _",
    "classify(@xmath5,@xmath8 ) _ ; ( 2 ) proofs for every the goal _",
    "classify(@xmath5,@xmath7 ) _ immediately succeed , with a strength based on a weighted combination of the features in the set @xmath9 .",
    "this set is encoded in the usual way as a sparse vector @xmath10 , with one dimension for every object of the form @xmath11 where @xmath7 is a class label and @xmath12 is a vocabulary word .",
    "for example , if the vocabulary contains the word _ hope _ and _ sports",
    "_ is a possible label , then one feature in @xmath10 might be active exactly when document @xmath5 contains the word @xmath13 and @xmath14 .",
    "the set of proofs associated with this theory are described by the plate diagram on the left - hand side of figure  [ fig : sl+mutex ] , where the upward - pointing blue arrows denote logical implication , and the repetition suggested by the plates has the same meaning as in graphical models .",
    "the black arrows will be explained below .",
    "the d - learner constraints are implemented in a logic programming language called proppr @xcite , whose semantics are defined by a slightly different graph .",
    "the proppr graph contains the same set of nodes as the proof graph , but is weighted , and has a different edge set  namely , the downward - pointing black arrows , which run opposite to the implication edges . for this example , these edges describe a forest , with one tree for each labeled example @xmath5 .",
    "each tree in the forest begins branching at distance two from the root , and will have @xmath15 nodes labeled _ true _ where @xmath15 is the number of possible class labels .",
    "the forest is further augmented with some additional edges : in particular , we will add a self - loop to each _ true _ node , and a `` reset '' edge that returns to the root ( the curved upward - pointing arrows ) for each non-_true _ node .",
    "( to simplify , the reset and self - loop edges are only shown in the first plate diagram . )",
    "the light upward - pointing edges will have an implicit weight of @xmath16 , for some fixed @xmath17 , unannotated edges have an implicit weight of one , and the weight of feature - annotated edges will be discussed below .",
    "finally , if feature vector @xmath18 is associated with a rule , then the edges produced using that rule are annotated with the feature vector .",
    "( we abbreviate @xmath10 with @xmath19 in the figure . )",
    "the weight of such an edge is @xmath20 , where @xmath21 is a learned parameter vector , and @xmath22 is a nonlinear `` squashing '' function ( e.g. , @xmath23 ) . in the example , these edges are the ones that exit a node labeled _",
    "classify(@xmath5,@xmath7)_. these are labeled with the function @xmath20 , and recall that the vector @xmath19 encodes features associated with the document @xmath5 with the label @xmath7 .",
    "we can now define a markov process , based on repeatedly transitioning from a node @xmath24 to its neighbors , using the normalized outgoing edges of @xmath24 as a transition function .",
    "this process is well - defined for any set of positive edge weights ( and positivity can be ensured by the choice of @xmath22 ) and any graph architecture , and will assign a probability score @xmath25 to every node @xmath24 .",
    "conceptually , it can be viewed as a probabilistic proof process , with the `` reset '' corresponding to abandoning a proof @xcite .",
    "imagine that @xmath21 has been trained so that for an example @xmath5 with true label @xmath26 , @xmath27 has the largest score over all the possible labels @xmath7 .",
    "note that every _ true _ node corresponds to a label @xmath7 .",
    "it is not hard to see that the ordering of @xmath28 over the _ true _ nodes in the graph for @xmath5 maintains a close correspondence to classification performance . in particular , even though the graph is locally normalized , the @xmath19-labeled edges compete with the upward - pointing `` reset '' edges that lead to the root , so a larger weight will direct more of the probability mass of the walk toward the _ true _ node associated with label @xmath29 .",
    "more specifically , the _ true _ node associated with label @xmath26 will have the highest @xmath30 of any _ true _ node in the graph .",
    "thus , in the example , _ there is a close connection between the problem of setting @xmath21 to minimize empirical loss of the classifier , and setting @xmath21 to satisfy constraints on the markov - walk scores of the nodes in the graph .",
    "_    in proppr , it is possible to train weights to maximize or minimize the score of a particular query response : i.e. , one can say that for the query _",
    "predict(@xmath31 ) _ responses where @xmath32 are `` positive '' and responses where @xmath33 for @xmath34 are `` negative '' .",
    "specifically a positive example @xmath35 incurs a loss of @xmath36 , where @xmath37 is the set of _ true _ nodes that support answer @xmath35 , and a negative example incurs a loss of @xmath38 .",
    "the training data needed for the supervised learning case is indicated in the bottom of the left - hand of the plate diagram .",
    "learning is performed by stochastic gradient descent @xcite .",
    "we finally turn to the right - hand parts of figures [ ppr : sl+mutex ] and [ fig : sl+mutex ] , which are a d - learner specification for an ssl method .",
    "these rules are a consistency test to be applied to each _ unlabeled _ example @xmath5 . in ordinary classification tasks ,",
    "any two distinct classes @xmath7 and @xmath39 should be mutually exclusive .",
    "the theory on the right - hand side of figure  [ ppr : sl+mutex ] asserts that a `` mutual exclusion failure '' ( mutexfailure ) occurs if @xmath5 can be classified into two distinct classes . and @xmath39 .",
    "] the corresponding plate diagram is shown in figure [ fig : sl+mutex ] , simplified by omitting the `` reset '' edges and the self - loops on _ true _ nodes . to ( softly )",
    "enforce this constraint , we need only to introduce negative examples for each unlabeled example @xmath5 , specifying that proofs for the goal _",
    "mutexfailure(@xmath5 ) _ should have low scores .",
    "conceptually , this constraint encodes a common bias of ssl systems : the decision boundaries should be drawn in low - probability regions of the space .",
    "for instance , transductive svms maximize the `` unlabeled data margin '' based on the low - density separation assumption that a good decision hyperplane lies on a sparse area of the feature space @xcite . in this case , if a decision boundary is close to an unlabeled example , then more than one _ classify _ goals will succeed with a high score .",
    "the example above explains in detail how to specify one particular type of constraint which is widely used in the past ssl works . of course , if this was the only type of constraint that could be specified , d - learner would not be of great interest : the value of d - learner is that it allows one to succinctly specify ( and implement ) many other plausible constraints that can potentially improve learning .",
    "here we show its application in the task of link - based text classification .",
    "many real - world datasets contain interlinked entities ( e.g. publications linked by citation relation ) and exhibit correlations among labels of linked entities .",
    "link - based classification aims at improving classification accuracy by exploiting such link structures besides the attribute values ( e.g. , text features ) of each entity . in this task",
    ", we classify each publication into a pre - defined class , e.g. neural_networks .",
    "each publication has features in two views : text content view and citation view .",
    "thus , each publication can be represented as features of its terms or features of its citations .",
    "similar to the classifier defined above , we have the text view classifier : +    and the citation view classifier : +    mutual - exclusivity constraints and of the text and citation classifiers are defined in the same way as done in section [ sec : ssl_mutex ]",
    ".    * cotraining constraints . *",
    "d - learner coordinates the classifiers of two views to make consistent predictions on testing data by imposing penalty when they disagree : +    * propagation constraints . * an initial narrative of label propagation ( lp ) algorithms is that the neighbors of a good labeled example should be classified consistently with it . to express this in the text view ,",
    "d - learner penalizes the violators by : +    where pickmutex(y,!y ) can be replace with pickmutex(!y , y ) to get another constraint , and sim1 is defined as : +    if x1 cites or is cited by z , then near(x1,z ) is true .",
    "thus , lpfailure1 encourages the publications that have a citation path to x to take the same label as x.    extending the above one - step walk based sim1 clause , we define a `` two - step '' walk based one : +    accordingly , the constraint using sim2 is referred to as .",
    "it encourages the publications cite or are cited by the same publication to have the same label .    * regularization constraints . * * * d - learner can implement the well - studied regularization technique in ssl by smoothing the behavior of a classifier on unlabeled data . specifically , for an unlabeled example , we smooth its label and its neighbors labels by : +        we use three datasets from @xcite : citeseer , cora and pubmed , with their statistical information given in table [ t : getoor_data ] . for each dataset",
    ", we use 1,000 publications for testing , and at most 5,000 publications for training . among the training publications ,",
    "we randomly pick 20 as labeled examples for each class , and the remaining ones are used as unlabeled .",
    "after the examples are prepared , we employ proppr to learn multi - class classifiers with @xmath40 .",
    "the maximum epoch number is 40 , and we find the training usually converges in less than 10 epochs .",
    "note that constraints are combined with equal weights , and we control the effect of different constraints by using different numbers of examples from them ( details discussed later ) .",
    "we compare with two supervised learning baselines : sl - svm and sl - proppr , which only employ the text view features to train classifiers with svms and proppr , respectively . for sl - svm , a linear kernel is used to train multi - class classifiers .",
    "another compared baseline is ssl - naive , which simply uses all examples of all constraints in the semi - supervised learning , regardless their actual effects .",
    "d - learner invokes a tuning strategy to determine how to use the constraints .",
    "we employ accuracy as the evaluation metric , which is defined as the ratio between the number of correct predictions and the number of total predictions ( i.e. the number of testing examples ) .",
    "the results are given in table [ t : link_clf_results ] .",
    "d - learner consistently outperforms sl - proppr on all three datasets , and the relative improvements are about 1.5% to 5% . compared with sl - svm ,",
    "d - learner achieves better results on the cora and pubmed datasets , with relative improvements about 12% and 6% , respectively . for the citeseer dataset , d - learner s performance is comparable to sl - svm . although each constraint has its own intuitive interpretation , ssl - naive does not perform well , particularly poor on citeseer and cora , which suggests that the appropriate use of ssl is often domain - specific @xcite , and needs more careful assessment on the heuristics .    * *      to exploit how to use those domain - specific constraints , d - learner incorporates a bayesian optimization based tuning method @xcite , in which a learning algorithm s generalization performance is modeled as a sample from a gaussian process .",
    "the released package , spearmint , allows one to program a wrapper for the communication with the tuned algorithm , while searching the optimal parameters .",
    "specifically , the wrapper passes the parameters suggested by spearmint into the learning algorithm , then collects the results from the algorithm for spearmint to generate a new suggestion .    instead of adding a weight to control one constraint s effect",
    ", we adopt a more straightforward way which tunes the example number of a constraint to use .",
    "thus , we have 6 parameters : the example numbers of cofailure ( # cf ) , lpfailure1 ( # lpf1 ) , lpfailure2 ( # lpf2 ) , mutexfailuret ( # mft ) , mutexfailurec ( # mfc ) , and smoothfailure ( # sf ) , as listed in table [ t : params_linked_data ] .",
    "lpfailure2 is found helpful on all three datasets .",
    "the mutual - exclusivity constraint of text view is helpful on cora and pubmed datasets .",
    "cotraining classifiers of both citation view and text content view does not improve the results , it is because , in the three datesets , the average citation number of publications is about 1.5 to 2.5 , which is too sparse to train a reliable citation view classifier .",
    "we note that in many nlp tasks , there are a plausible number of task - specific constraints which can be easily formulated by a domain expert . in this section",
    "we will describe a number of constraints of _ relation extraction for entity - centric corpora_. each document in an entity - centric corpus describes aspects of a particular entity ( called _ subject or title entity _ ) , e.g. each wikipedia article is such a document .",
    "relation extraction from an entity - centric document is reduced to predicting the relation between the subject entity ( i.e. the first argument of the relation ) and an entity mention ( i.e. the second argument ) in the document .",
    "for example , from a drug article `` aspirin '' , if the target relation is `` sideeffects '' , we need to predict for each candidate ( extracted from a single sentence ) whether it is a side effect of `` aspirin '' .",
    "if no such relation holds , we predict the special label `` other '' .",
    "this task in medical domain was initially proposed in @xcite , where a reader could find more details .",
    "the second argument of a relation is usually of a particular unary type .",
    "for example , the second argument of the `` interactswith '' relation is always a drug .",
    "therefore , a plausible idea is to impose an agreement constraint between relation and type classifiers , analogous to enforcing agreements between classifiers of different views .    with our declarative language",
    ", we can coordinate the tasks of predicting relation and type as follows : +    where _",
    "pickreallabel(y ) _ consists of trivial rules for each non-``other '' relation label , and _",
    "inranget(y , t1 ) _ consists of trivial rules for each relation label and its value range type , _",
    "pickmutext(t1,t2 ) _ consists of trivial rules of each distinct pair of type labels , _ predictr _ and _ predictt _ are relation and type classifiers , respectively . in other words , _",
    "cofailure(@xmath5 ) _ says we should penalize cases where a mention @xmath5 is predicted having the real relation label y ( i.e. not `` other '' ) , whose range is type t1 , but the predicted type of @xmath5 is t2 which is mutually exclusive with t1 .",
    "thus , by specifying that proofs for the goal _",
    "cofailure(@xmath5 ) _ having low scores , classifiers that make this sort of error will be downweighted .",
    "mutual - exclusivity constraints and of type and relation classifiers are defined in the same way as we did before",
    ".      * document constraint . *",
    "if an entity string appears as multiple mentions in a document , they should have the same relation label ( relative to the subject ) , or some have `` other '' label : +    for example , if `` heartburn '' appears twice in the `` aspirin '' document , we should make consistent predictions , i.e. `` sideeffects '' for them , or predict one or both as `` other '' . note that a mention can also refer to a coordinate - term list , such as `` vomiting , headache and nausea '' , of multiple nps .",
    "in fact , we compute the intersection of the np sets of two mentions , and if it is not empty , this constraint applies , such as for `` vomiting , and headache '' and `` vomiting , and heartburn '' .    * sentence constraint*. for each sentence , we require that only one mention can be labeled as a particular relation ( this requirement is usually satisfied in real data ) : +    where x1 and x2 are a pair of mentions extracted from a single sentence",
    ". this constraint penalizes extracting multiple relation objects from a single sentence .",
    "for example , from the sentence `` some products that may interact with this drug include : aliskiren and ace inhibitors '' , `` some products '' and `` aliskiren and ace inhibitors '' should not be labeled as the interactswith relation simultaneously .",
    "* section title constraint .",
    "* in some entity - centric corpora , the content of a document is organized into sections .",
    "this constraint basically says , for two mentions of the same np in the same section ( currently determined simply by matching section titles ) of two documents , they should have the same relation label , relative to their own document subjects : +    e.g. , if `` heartburn '' appears in `` adverse reactions '' sections of drugs `` aspirin '' and `` singulair '' , it is plausible to infer both mentions are `` sideeffects '' , or one or both are `` other '' .",
    "we follow the experimental settings in @xcite .",
    "the drug corpus is dailymed containing 28,590 articles , and the disease corpus is wikidisease containing 8,596 articles .",
    "3 and 5 relations are extracted for drugs and diseases , respectively . a pipeline s task",
    "then is to extract all correct values of the second argument of a given relation from a test document .",
    "the range types of the second arguments of relations are also defined in their paper . for details about preprocessing , features , distant labeling seeds , annoated evaluation and tuning datasets",
    ", please refer to @xcite .      without manually labeled training relation examples ,",
    "we employ the seed triples from freebase to distantly label articles in the corpora , as done in @xcite .",
    "for instance , the triple sideeffects(aspirin , heartburn ) will label a mention `` heartburn '' from the aspirin article as an example of sideeffects relation .",
    "the raw data by distant labeling is very noisy @xcite , so we employ the distillation method with lp in @xcite to get a cleaner set of relation examples , where the top k ( the tuning for k will be described later ) most confident , scored by lp , examples are used as training data .",
    "we also pick a set of mentions from the testing corpus that are not labeled as any relation as general negative examples .",
    "the training type examples are collected by diel  @xcite , which extracts the instances of those range types from freebase as seeds , and extends the seed set by lp .",
    "after that , we pick top k as training examples .",
    "similarly , we also randomly pick a number of general negative type examples .",
    "we compare against three existing methods : _ multir _ , @xcite which models each relation mention separately and aggregates their labels using a deterministic or ; _ mintz++ _ from @xcite , which improves on the original model from @xcite by training multiple classifiers , and allowing multiple labels per entity pair ; and _ miml - re _",
    "@xcite which has a similar structure to _ multir _ , but uses a classifier to aggregate the mention level predictions into an entity pair prediction .",
    "we used the publicly available code from the authors for the experiments .",
    "we tuned their parameters , including negative example number , epoch number ( for both _ multir _ and _ miml - re _ ) , and training fold number ( for _ miml - re _ ) , directly on the evaluation data and report their best performance .",
    "we have 4 supervised learning baselines via distant supervision : ds - svm , ds - proppr , ds - dist - svm , and ds - dist - proppr .",
    "the first two use the distantly - labeled examples from the testing corpora as training data , while the last two use the distilled examples as training data , as done for d - learner . for ds - proppr and ds - dist - proppr ,",
    "proppr is used to learn multi - class classifiers , as done for d - learner . for ds - svm and ds - dist - svm ,",
    "binary classifiers are trained with svms , and ds - dist - svm is exactly the _ diejob_target _ in @xcite .",
    "we evaluate the performance of different systems from an ir perspective : a title entity ( i.e. , document name ) and a relation together act as a query , and the extracted nps as retrieval results .",
    "the evaluation results are given in table [ t : prf ] .",
    "the systems with `` * '' are directly tuned on the evaluation data .",
    "other systems are tuned with a tuning dataset .",
    "for the disease domain , d - learner achieves its best performance without using any examples from the cofailure constraint , while for the drug domain , it achieves its best performance without using any examples from docfailure , secfailure , and titlefailure constraints .",
    "the tuning details will be discussed later .",
    ".5     .5     d - learner outperforms all the other systems .",
    "compared with multir , mintz++ , and miml - re , the relative improvements under f1 are 19% to 27% in the disease domain , and 112% to 159% in the drug domain .",
    "this result shows that our d - learner framework has overall superiority on the relation extraction task , mainly because of its capability of specifying these tailor - made constraints .",
    "the precision values of d - learner are much higher than all compared systems on both domains . for recall ,",
    "d - learner performs much better than multir , mintz++ , and miml - re on the drug domain , and not as good as them on the disease domain .",
    "the svm - based baselines outperform the proppr - based baselines , which shows that the good performance of d - learner on this task is not because of using proppr for its implementation .",
    "comparisons between ds - x and ds - dist - x show that the distillation step is useful for better results .",
    "precision - recall curves are given in figures [ fig : disease_pr ] and [ fig : drug_pr ] .",
    "for the disease domain , d - learner s precision is almost always better , at the same recall level , than any of the other methods . for the drug domain , d - learner s precision is generally better after the recall level 0.05 .",
    "as listed in table [ t : params_tuning ] , we have 10 parameters : the number of training relation examples ( # r ) , picked from the top of the ranked ( by lp - based distillation ) distantly labeled examples ; the number of general negative relation examples ( # nr ) ; the numbers of type examples ( # t ) and general negative type examples ( # nt ) ; the example numbers of cofailure ( # cf ) , mutexfailurer ( # mfr ) , mutexfailuret ( # mft ) , docfailure ( # df ) , titlefailure ( # tf ) , and sentfailure ( # sf ) . the unlabeled constraint examples are randomly picked from the entire testing corpus .",
    "the 2nd and the 5th columns give the value ranges , and the 3rd and 6th columns give the step sizes while searching the optimal .",
    "ten pages are annotated as the tuning data .",
    "we optimize the performance of d - learner under f1 , and the obtained optimal parameters are given in the 4th and 7th columns .",
    "for both domains , d - learner achieves better results by using a portion of top - ranked relation examples as training data .",
    "( this observation is consistent with the comparisons of ds - dist - proppr vs. ds - proppr , and ds - dist - svm vs. ds - svm . )",
    "it shows that using a smaller but cleaner set of training examples , combined with some ssl constraints and cotraining , can improve the performance .    for the disease domain , d - learner performs better without cotraining type classifiers , thus , the optimal type related parameters , i.e. # t , # nt , # cf , and # mft , for disease are 0 .",
    "while for the drug domain , cotraining can bootstrap the performance of relation extraction .",
    "one explanation is that for a domain , e.g. drug , in which the second argument values of relations are less ambiguous ( e.g. symptom entity for `` sideeffect '' ) , the cotrained type classifiers are accurate so that they can help the relation classifier learner explore useful features that are not captured by relation training examples . for the disease domain ,",
    "the second argument values of relations have more ambiguity ( e.g. `` age '' and `` gender '' for the relation `` riskfactors '' ) , which causes the learnt type classifiers inaccurate and unhelpful for the relation classifiers .",
    "with respect to the tailor - made constraints for relation extraction , i.e. , docfailure , sentfailure , and titlefailure , d - learner also behaves differently on the two domains .",
    "for the drug domain , it is optimal not using any examples from them , but for the disease domain , using all examples from them is preferred .",
    "note that the above conclusions on the effect of relation extraction constraints and cotraining type classifiers are under the integral setting of d - learner , i.e. using both constraints and cotraining simultaneously .",
    "more tuning experiments show that for the drug domain , only using those relation extraction constraints also improves the performance , however , the improvement is not as significant as only using the cotraining setting ; for the disease domain , only using the cotraining setting also improves the results , but not as much as only using those constraints .",
    "while using both constraints and cotraining in the integral setting , it does not necessarily end up with a value - added effect .    for both domains ,",
    "the mutexfailure examples are found always helpful .",
    "note that when @xmath41 , the mutexfailuret constraint becomes meaningless , thus , @xmath42 for the disease domain .",
    "another point to mention is that , as shown in table [ t : params_tuning ] , the optimal values of some parameters are the maximums in their ranges , which indicates that d - learner is likely to achieve even better performance if more examples of these constraints are used .",
    "we proposed a general approach , d - learner , to modeling ssl constraints .",
    "it can approximate traditional supervised learning and many natural ssl heuristics by declaratively specifying the desired behaviors of classifiers . with a bayesian optimization based tuning strategy",
    ", we can collectively evaluate the effectiveness of these constraints so as to obtain tailor - made ssl settings for individual problems .",
    "the efficacy of this approach is examined in two tasks , link - based text classification and relation extraction , and encouraging improvements are achieved .",
    "there are a few open questions to explore : adding hyperparameters ( e.g. , different weights for different constraints ) ; adding more control over the supervised loss versus the constraint - based loss ; and testing the approach on other tasks .",
    "lidong bing , sneha chaudhari , richard wang  c , and william  w cohen . improving distant supervision for information extraction using label propagation through lists . in _ proceedings of the 2015 conference on empirical methods in natural language processing _ ,",
    "pages 524529 , lisbon , portugal , september 2015 .",
    "association for computational linguistics .",
    "lidong bing , mingyang ling , richard  c. wang , and william  w. cohen .",
    "distant ie by bootstrapping using lists and document structure . in _ proceedings of the thirtieth aaai conference on artificial intelligence ,",
    "february 12 - 17 , 2016 , phoenix , arizona , usa . _ , pages 28992905 , 2016 .",
    "lidong bing , bhuwan dhingra , kathryn mazaitis , jong  hyuk park , and william  w. cohen .",
    "bootstrapping distantly supervised ie using joint learning and small well - structured corpora . in",
    "_ proceedings of the thirty first aaai conference on artificial intelligence , february 4 - 9 , 2017 , san francisco , california , usa . _ , 2017 .",
    "raphael hoffmann , congle zhang , xiao ling , luke zettlemoyer , and daniel  s weld .",
    "knowledge - based weak supervision for information extraction of overlapping relations . in _ proceedings of the 49th annual meeting of the association for computational linguistics : human language technologies - volume 1 _ , pages 541550 .",
    "association for computational linguistics , 2011 .",
    "thorsten joachims .",
    "transductive inference for text classification using support vector machines . in _ proceedings of the sixteenth international conference on machine learning _ , icml 99 , pages 200209 , san francisco , ca , usa , 1999 .",
    "morgan kaufmann publishers inc .",
    "mike mintz , steven bills , rion snow , and dan jurafsky .",
    "distant supervision for relation extraction without labeled data . in _ proceedings of the joint conference of the 47th annual meeting of the acl and the 4th international joint conference on natural language processing of the afnlp : volume 2-volume 2 _ , pages 10031011 .",
    "association for computational linguistics , 2009 .",
    "jasper snoek , hugo larochelle , and ryan  p adams .",
    "practical bayesian optimization of machine learning algorithms . in f.  pereira , c.  j.  c. burges , l.  bottou , and k.  q. weinberger , editors , _ advances in neural information processing systems 25 _ , pages 29512959 .",
    "curran associates , inc . , 2012 .",
    "mihai surdeanu , julie tibshirani , ramesh nallapati , and christopher  d. manning .",
    "multi - instance multi - label learning for relation extraction . in _ proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning _ , emnlp - conll 12 , pages 455465 , stroudsburg , pa , usa , 2012 .",
    "association for computational linguistics .",
    "william  yang wang , kathryn mazaitis , and william  w cohen .",
    "programming with personalized pagerank : a locally groundable first - order probabilistic logic . in _ proceedings of the 22nd",
    "acm international conference on conference on information & knowledge management _ , pages 21292138 .",
    "acm , 2013 .",
    "dengyong zhou , olivier bousquet , thomas  navin lal , jason weston , and bernhard schlkopf .",
    "learning with local and global consistency .",
    "in _ advances in neural information processing systems 16 [ neural information processing systems , nips 2003 , december 8 - 13 , 2003 , vancouver and whistler , british columbia , canada ] _ , pages 321328 , 2003 ."
  ],
  "abstract_text": [
    "<S> we propose a general approach to modeling semi - supervised learning ( ssl ) algorithms . </S>",
    "<S> specifically , we present a declarative language for modeling both traditional supervised classification tasks and many ssl heuristics , including both well - known heuristics such as co - training and novel domain - specific heuristics . in addition to representing individual ssl heuristics , we show that multiple heuristics can be automatically combined using bayesian optimization methods </S>",
    "<S> . we experiment with two classes of tasks , link - based text classification and relation extraction . </S>",
    "<S> we show modest improvements on well - studied link - based classification benchmarks , and state - of - the - art results on relation - extraction tasks for two realistic domains . </S>"
  ]
}