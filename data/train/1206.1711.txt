{
  "article_text": [
    "the experimental study of quantum mechanical systems has made huge progress recently motivated by quantum information science .",
    "producing and manipulating many - body quantum mechanical systems have been relatively easier over the last decade .",
    "one of the most essential goals in such experiments is to reconstruct quantum states via quantum state tomography ( qst ) .",
    "the qst is an experimental process where the system is repeatedly measured with different elements of a positive operator valued measure ( povm ) .",
    "most popular methods for estimating the state from such data are : linear inversion @xcite , @xcite , maximum likelihood @xcite , @xcite , @xcite , @xcite , @xcite and bayesian inference @xcite , @xcite , @xcite ( we also refer the reader to @xcite and references therein ) .",
    "recently , different approaches brought up - to - date statistical techniques in this field .",
    "the estimators are obtained via minimization of a penalized risk .",
    "the penalization will subject the estimator to constraints . in @xcite",
    "the penalty is the von neumann entropy of the state , while @xcite , @xcite use the @xmath7 penalty , also known as the lasso matrix estimator , under the assumption that the state to be estimated has low rank .",
    "these last papers assume that the number of measurements must be minimized in order to recover all the information that we need .",
    "the ideas of matrix completion is indeed , that , under the assumptions that the actual number of underlying parameters is small ( which is the case under the low - rank assumption ) only a fraction of all possible measurements will be sufficient to recover these parameters .",
    "the choice of the measurements is randomized and , under additional assumptions , the procedure will recover the underlying density matrix as well as with the full amount of measurements ( the rates are within @xmath8 factors slower than the rates when all measurements are performed ) .    in this paper",
    ", we suppose that a reasonable amount @xmath3 ( e.g. @xmath9 ) of data is available from all possible measurements .",
    "we implement a method to recover the whole density matrix and estimate its rank from this huge amount of data .",
    "this problem was already considered by gu , kypraios and dryden  @xcite who propose a maximum likelihood estimator of the state .",
    "our method is relatively easy to implement and computationally efficient .",
    "its starting point is a linear estimator obtained by the moment method ( also known as the inversion method ) , which is projected on the set of matrices with fixed , known rank .",
    "a data - driven procedure will help us select the optimal rank and minimize the estimators risk in frobenius norm .",
    "we proceed by minimizing the risk of the linear estimator , penalized by the rank . when estimating the density matrix of a @xmath1-qubits system , our final procedure has the risk ( squared frobenius norm ) bounded by @xmath10 , where @xmath2 between 1 and @xmath5 is the rank of the matrix .",
    "the inversion method is known to be computationally easy but less convenient than constrained maximum likelihood estimators as it does not produce a density matrix as an output .",
    "we revisit the moment method in our setup and argue that we can still transform the output into a density matrix , with the result that the distance to the true state can only be decreased in the proper norm .",
    "we shall indicate how to transform the linear estimator into a physical state with fixed , known rank .",
    "finally , we shall select the estimator which fits best to the data in terms of a rank - penalized error .",
    "additionally , the rank selected by this procedure is a consistent estimator of the true rank @xmath2 of the density matrix .",
    "we shall apply our procedure to the real data issued from experiments on systems of 4 to 8 ions .",
    "trapped ion qubits are a promising candidate for building a quantum computer .",
    "an ion with a single electron in the valence shell is used .",
    "two qubit states are encoded in two energy levels of the valence electrons , see @xcite , @xcite , @xcite .",
    "the structure of the paper is as follows .",
    "section 2 gives notation and setup of the problem . in section 3",
    "we present the moment method .",
    "we first change coordinates of the density matrix in the basis of pauli matrices and vectorize the new matrix . we give properties of the linear operator which takes this vector of coefficients to the vector of probabilities @xmath11 .",
    "these are the probabilities to get a certain outcome @xmath12 from a given measurement indexed by @xmath13 and that we actually estimate from data at our disposal .",
    "we prove the invertibility of the operator , i.e. identifiability of the model ( the information we measure enables us to uniquely determine the underlying parameters ) .",
    "section 4 is dedicated to the estimation procedure .",
    "the linear estimator will be obtained by inversion of the vector of estimated coefficients .",
    "we describe the rank - penalized estimator and study its error bounds .",
    "we study the numerical properties of our procedure on example states and apply them to experimental real - data in section 5 .",
    "the last section is dedicated to proofs .",
    "we have a system of @xmath1 qubits .",
    "this system is represented by a @xmath14 density matrix @xmath0 , with coefficients in @xmath15 .",
    "this matrix is hermitian @xmath16 , semidefinite positive @xmath17 and has @xmath18 .",
    "the objective is to estimate @xmath0 , from measurements of many independent systems , identically prepared in this state .    for each system ,",
    "the experiment provides random data from separate measurements of pauli matrices @xmath19 on each particle .",
    "the collection of measurements which are performed writes @xmath20 where @xmath21 is a vector taking values in @xmath22 which identifies the experiment .",
    "the outcome of the experiment will be a vector @xmath23 .",
    "it follows from the basic principles of quantum mechanics that the outcome of any experiment indexed by @xmath24 is actually a random variable , say @xmath25 , and that its distribution is given by : @xmath26 where the matrices @xmath27 denote the projectors on the eigenvectors of @xmath28 associated to the eigenvalue @xmath29 , for all @xmath30 from 1 to @xmath1 .    for the sake of simplicity",
    ", we introduce the notation @xmath31 as a consequence we have the shorter writing for  : @xmath32 .",
    "the tomographic inversion method for reconstructing @xmath0 is based on estimating probabilities @xmath33 by @xmath34 from available data and solving the linear system of equations @xmath35 it is known in statistics as the method of moments .",
    "we shall use in the sequel the following notation : @xmath36 denotes the frobenius norm and @xmath37 the operator sup - norm for any @xmath38 hermitian matrix @xmath39 , @xmath40 is the euclidean norm of the vector @xmath41 .    in this paper , we give an explicit inversion formula for solving ( [ loi ] ) .",
    "then , we apply the inversion procedure to equation ( [ loihat ] ) and this will provide us an unbiased estimator @xmath4 of @xmath0 . finally ,",
    "we project this estimator on the subspace of matrices of rank @xmath42 ( @xmath42 between 1 and @xmath5 ) and thus choose , without any a priori assumption , the estimator which best fits the data .",
    "this is done by minimizing the penalized risk @xmath43 where the minimum is taken over all hermitian , positive semidefinite matrices @xmath44 .",
    "note that the output is not a proper density matrix .",
    "our last step will transform the output in a physical state .",
    "the previous optimization program has an explicit and easy to implement solution .",
    "the procedure will also estimate the rank of the matrix which best fits data .",
    "we actually follow here the rank - penalized estimation method proposed in the slightly different problems of matrix regression .",
    "this problem recently received a lot of attention in the statistical community @xcite and chapter 9 in @xcite . here , we follow the computation in @xcite .    in order to give such explicit inversion formula we first change the coordinates of the matrix @xmath0 into a vector @xmath45 on a convenient basis",
    "the linear inversion also gives information about the quality of each estimator of the coordinates in @xmath46 .",
    "thus we shall see that we have to perform all measurements @xmath47 in order to recover ( some ) information on each coordinate of @xmath46 .",
    "also , some coordinates are estimated from several measurements and the accuracy of their estimators is thus better .    to our knowledge",
    ", this is the first time that rank penalized estimation of a quantum state is performed .",
    "parallel work of gu _ et al . _",
    "@xcite addresses the same issue via the maximum likelihood procedure .",
    "other adaptive methods include matrix completion for low - rank matrices @xcite and for matrices with small von neumann entropy @xcite .",
    "note the problem of state tomography with mutually unbiased bases , described in section  [ section_notations ] , was considered in refs .",
    "@xcite . in this section ,",
    "we introduce some notation used throughout the paper , and remind some facts that were proved for example in  @xcite about the identifiability of the model .    a model is identifiable if , for different values of the underlying parameters , we get different likelihoods ( probability distributions ) of our sample data .",
    "this is a crucial property for establishing the most elementary convergence properties of any estimator .",
    "the first step to explicit inversion formula is to express @xmath0 in the @xmath1-qubit pauli basis .",
    "in other words , let us put @xmath48 and @xmath49 .",
    "for all @xmath50 , denote similarly to ( [ mes ] ) @xmath51 then , we have the following decomposition : @xmath52    we can plug this last equation into   to obtain , for @xmath53 and @xmath54 , @xmath55 finally , elementary computations lead to @xmath56 for any @xmath57 and @xmath58 , while @xmath59 for any @xmath57 , @xmath60 and @xmath61 denotes the kronecker symbol .    for any @xmath62 ,",
    "we denote by @xmath63 .",
    "the above calculation leads to the following fact , which we will use later .",
    "[ propprob ] for @xmath53 , and @xmath54 , we have @xmath64    let us consider , for example , @xmath65 , then the associated set @xmath66 is empty and @xmath67 is the only probability depending on @xmath68 among other coefficients .",
    "therefore , only the measurement @xmath69 will bring information on this coefficient .",
    "whereas , if @xmath70 , the set @xmath71 contains 2 points . there are @xmath72 measurements @xmath73 , ... , @xmath74 that will bring partial information on @xmath75 .",
    "this means , that a coefficient @xmath76 is estimated with higher accuracy as the size of the set @xmath71 increases .    for",
    "the sake of shortness , let us put in vector form : @xmath77 our objective is to study the invertibility of the operator @xmath78    thanks to fact  [ propprob ] , this operator is linear .",
    "it can then be represented by a matrix @xmath79 _ { ( \\mathbf{r},\\mathbf{a})\\in(\\mathcal{r}^n\\times \\mathcal{e}^{n}),\\b \\in\\mathcal{m}^n}$ ] , we will then have : @xmath80 and from fact  [ propprob ] we know that @xmath81 we want to solve the linear equation @xmath82 . recall that @xmath71 is the set of indices where the vector @xmath83 has an @xmath84 operator .",
    "denote by @xmath85 the cardinality of the set @xmath71 .",
    "[ inversion ] the matrix @xmath86 is a diagonal matrix with non - zero coefficients given by @xmath87 as a consequence the operator is invertible , and the equation @xmath88 has a unique solution : @xmath89    in other words , we can reconstruct @xmath90 from @xmath91 , in the following way : @xmath92 this formula confirms the intuition that , the larger is @xmath85 , the more measurements @xmath47 will contribute to recover the coefficient @xmath75 .",
    "we expect higher accuracy for estimating @xmath75 when @xmath85 is large .",
    "in practice , we do not observe @xmath93 for any @xmath24 and @xmath94 .",
    "for any @xmath24 , we have a set of @xmath3 independent experiments , whose outcomes are denoted by @xmath95 , @xmath96 .",
    "our setup is that the @xmath97 are independent , identically distributed ( i.i.d . )",
    "random variables , distributed as @xmath98 .",
    "we then have a natural estimator for @xmath99 : @xmath100 we can of course write @xmath101 .",
    "we apply the inversion formula to the estimated vector @xmath102 . following proposition  [ inversion ] we can define : @xmath103 put it differently : @xmath104 and then , the linear estimator obtained by inversion , is @xmath105    the next result gives asymptotic properties of the estimator @xmath106 of @xmath46 .",
    "[ lambda ] the estimator @xmath106 of @xmath46 , defined in ( [ vechat ] ) has the following properties :    1 .",
    "it is unbiased , that is @xmath107 = \\vec{\\rho}$ ] ; 2 .",
    "it has variance bounded as follows @xmath108 3 .   for any @xmath109 , @xmath110    note again that the accuracy for estimating @xmath75 is higher when @xmath85 is large .",
    "indeed , in this case more measurements bring partial information on @xmath75 .",
    "the concentration inequality gives a bound on the norm @xmath111 which is valid with high probability .",
    "this quantity is related to @xmath112 in a way that will be explained later on .",
    "the bound we obtain above depends on @xmath113 , which is expected as @xmath114 is the total number of parameters of a full rank system .",
    "this factor appears in the hoeffding inequality that we use in order to prove this bound .",
    "we investigate low - rank estimates of @xmath0 defined in ( [ mathat ] ) .",
    "from now on , we follow closely the results in @xcite which were obtained for a matrix regression model , with some differences as our model is different .",
    "let us , for a positive real value @xmath115 study the estimator : @xmath116 , \\ ] ] where the minimum is taken over all hermitian matrices @xmath44 . in order to compute the solution of this optimization program",
    ", we may write it in a more convenient form since @xmath117\\nonumber \\\\ & = \\min_k \\min_{r : { \\rm rank}(r)=k } \\left [ \\left\\|r -   \\hat{\\rho } \\right\\|_f^{2 } + \\nu \\cdot k \\right ] .\\end{aligned}\\ ] ]    an efficient algorithm is available to solve the minimization program   as a spectral - based decomposition algorithm provided in @xcite .",
    "let us denote by @xmath118 the matrix such that @xmath119 $ ] .",
    "this is a projection of the linear estimator on the space of matrices with fixed ( given ) rank @xmath42 .",
    "our procedure selects automatically out of data the rank @xmath120 .",
    "we see in the sequel that the estimators @xmath121 and @xmath122 actually coincide .",
    "we study the statistical performance from a numerical point of view later on .",
    "[ res ] for any @xmath123 put @xmath124 .",
    "we have on the event @xmath125 that @xmath126 where @xmath127 for @xmath128 are the eigenvalues of @xmath0 ordered decreasingly .",
    "note that , if @xmath129 , for some @xmath2 between 1 and @xmath5 , then the previous inequality becomes @xmath130 let us study the choice of @xmath131 in theorem  [ res ] such that the probability of the event @xmath132 is small . by putting together the previous theorem and proposition  [ lambda ]",
    ", we get the following result :    [ corbornnudata ] for any @xmath123 put @xmath124 and for some small @xmath133 choose @xmath134 then , we have @xmath135 with probability larger than @xmath136 .    again , if the true rank of the underlying system is @xmath2 , we can write that , for any @xmath137 and for some small @xmath133 : @xmath138 with probability larger than @xmath136 .",
    "if @xmath139 denotes the trace norm of a matrix , we have @xmath140 for any matrix @xmath141 of size @xmath142 .",
    "so , we deduce from the previous bound that @xmath143    the next result will state properties of @xmath120 , the rank of the final estimator @xmath122 .",
    "[ hatrank ] if there exists @xmath42 such that @xmath144 and @xmath145 for some @xmath146 in @xmath147 $ ] , then @xmath148    from an asymptotic point of view , this corollary means that , if @xmath2 is the rank of the underlying matrix @xmath0 , then our procedure is consistent in finding the rank as the number @xmath3 of data per measurement increases . indeed ,",
    "as @xmath149 is an upper bound of the norm @xmath112 , it tends to 0 asymptotically and therefore the assumptions of the previous corollary will be checked for @xmath150 . with a finite sample ,",
    "we deduce from the previous result that @xmath120 actually evaluates the first eigenvalue which is above a threshold related to the largest eigenvalue of the noise @xmath151 .",
    "in this section we implement an efficient procedure to solve the optimization problem   from the previous section .",
    "indeed , the estimator @xmath152 will be considered as an input from now on .",
    "it is computed very efficiently via linear operations and the real issue here is how to project this estimator on a subspace of matrices with smaller unknown rank in an optimal way .",
    "we are interested in two aspects of the method : its ability to select the rank correctly and the correct choice of the penalty .",
    "first , we explore the penalized procedure on example data and tune the parameter @xmath115 conveniently . in this way , we evaluate the performance of the linear estimator and of the rank selector .",
    "we then apply the method on real data sets .",
    "the algorithm for solving is given in @xcite .",
    "we adapt it to our context and obtain the simple procedure .",
    "* algorithm * : + : the linear estimator @xmath4 and a positive value of the tuning parameter @xmath115 + : an estimation @xmath120 of the rank and an approximation @xmath121 of the state matrix .    1 .",
    "compute the eigenvectors @xmath153 $ ] corresponding to the eigenvalues of the matrix @xmath154 sorted in decreasing order .",
    "2 .   let @xmath155 .",
    "3 .   for @xmath156 ,",
    "let @xmath157 and @xmath158 be the restrictions to their @xmath42 first columns of @xmath159 and @xmath160 , respectively .",
    "4 .   for @xmath156 ,",
    "compute the estimators @xmath161 .",
    "compute the final solution @xmath121 , where , for a given positive value @xmath115 , @xmath120 is defined as the minimizer in @xmath42 over @xmath162 of @xmath163    the constant @xmath42 in the above procedure plays the role of the rank and then @xmath118 is the best approximation of @xmath164 with a matrix of rank @xmath42 . as a consequence , this approach provides an estimation of both of the matrix @xmath0 and of its rank @xmath2 by @xmath165 and @xmath166 , respectively .",
    "obviously , this solution is strongly related to the value of the tuning parameter @xmath115 . before dealing with how to calibrate this parameter ,",
    "let us present a property that should help us to reduce the computational cost of the method .",
    "the above algorithm is simple but requires the computation of @xmath5 matrices in step 3 and step 4 .",
    "we present here an alternative which makes possible to compute only the matrix @xmath167 that corresponds to @xmath168 , and then reduce the storage requirements .",
    "remember that @xmath120 is the value of @xmath42 minimizing the quantity in step 5 of the above algorithm .",
    "let @xmath169 be the ordered eigenvalues of @xmath170 . according to ( * ? ? ?",
    "* proposition 1 ) , it turns out that @xmath120 is the largest @xmath42 such that the eigenvalue @xmath171 exceeds the threshold @xmath172 : @xmath173 as a consequence , one can compute the eigenvalues of the matrix @xmath170 and set @xmath120 as in  .",
    "this value is then used to compute the best solution @xmath121 thanks to step 1 to step 4 in the above algorithm , with the major difference that we restrict step 3 and step 4 to only @xmath174 .",
    "example data    we build artificial density matrices @xmath0 with a given rank @xmath2 in @xmath175 .",
    "these matrices are @xmath176 with @xmath177 and 5 .",
    "to construct such a matrix , we take @xmath0 as @xmath178 , the diagonal matrix with its first @xmath2 diagonal terms equal @xmath179 , whereas the others equal zero",
    ".    we aim at testing how often we select the right rank based on the method illustrated in   as a function of the rank @xmath2 , and of the number @xmath3 of repetitions of the measurements we have in hand .",
    "our algorithm depends on the tuning parameter @xmath115 .",
    "we use and compare two different values of the threshold @xmath115 : denote by @xmath180 and @xmath181 the values the parameter @xmath115 provided in theorem  [ res ] and corollary  [ corbornnudata ] respectively .",
    "that is , @xmath182 as established in theorem  [ res ] , if the tuning parameter @xmath115 is of order of the parameter @xmath183 , the solution of our algorithm is an accurate estimate of @xmath0 .",
    "we emphasize the fact that @xmath183 is nothing but the estimation error of our linear estimator @xmath184 .",
    "we study this error below . on the other hand",
    ", the parameter @xmath181 is an upper bound of @xmath180 that ensures that the accuracy of estimation remains valid with high probability ( _ cf .",
    "_ corollary  [ corbornnudata ] ) .",
    "the main advantage of @xmath181 is that it is completely known by the practitioner , which is not the case of @xmath180 .",
    "* rank estimation . *",
    "our first goal consists in illustrating the estimation power of our method in selecting the true rank @xmath2 based on the calibrations of @xmath115 given by  .",
    "we provide some conclusions on the number of repetitions @xmath3 of the measurements needed to recover the right rank as a function of this rank .",
    "figure  [ figerrorvsrank ] illustrates the evolution of the selection power of our method based on @xmath183 ( blue stars ) on the one hand , and based on @xmath185 ( green squares ) on the other hand .",
    "[ figerrorvsrank ]    two conclusions can be made .",
    "first , the method based on @xmath183 is powerful .",
    "it almost always selects the right rank .",
    "it outperforms the algorithm based on @xmath186 .",
    "this is an interesting observation .",
    "indeed , @xmath186 is an upper bound of @xmath183 .",
    "it seems that this bound is too large and can be used only for particular settings .",
    "note however that in the variable selection literature , the calibration of the tuning parameter is a major issue and is often fixed by cross - validation ( or other well - known methods ) .",
    "we have chosen here to illustrate only the result based on our theory and we will provide later an instruction to properly calibrate the tuning parameter @xmath115 .",
    "the second conclusion goes in the direction of this instruction . as expected , the selection power of the method ( based on both @xmath183 and @xmath186 ) increases when the number of repetition @xmath3 of the measurements increases . compare the figure for @xmath187 repetitions to the figure for @xmath9 repetitions in figure  [ figerrorvsrank ] .",
    "moreover , for ranks smaller than some values , the methods always select the good rank . for larger ranks , they perform poorly .",
    "for instance with @xmath187 ( a small number of measurements ) , we observe that the algorithm based on @xmath186 performs poorly when the rank @xmath188 , whereas the algorithm based on @xmath183 is still excellent .",
    "+ actually , the bad selection when @xmath2 is large does not mean that the methods perform poorly .",
    "indeed our definition of the matrix @xmath0 implies that the eigenvalues of the matrix decrease with @xmath2 .",
    "they equal to @xmath179 .",
    "therefore , if @xmath149 is of the same order as @xmath179 , finding the exact rank becomes difficult since this calibration suggests that the eigenvalues are of the same order of magnitude as the error .",
    "hence , in such situation , our method adapts to the context and find the effective rank of @xmath0 .",
    "as an example , let consider our study with @xmath177 , @xmath187 and @xmath189 .",
    "based on @xmath190 repetitions of the experiment , we obtain a maximal value of @xmath191 equal to @xmath192 .",
    "this value is quite close to @xmath193 , the value of the eigenvalues of @xmath0 .",
    "this explains the fact that our method based on @xmath180 failed in one iteration ( among @xmath190 ) to find the good rank . in this context",
    "@xmath181 is much larger than @xmath193 and then our method does not select the correct rank with this calibration in this setting .",
    "+ let us also mention that we explored numerous experiments with other choices of the density matrix @xmath0 .",
    "the same conclusion remains valid .",
    "when the error of the linear estimator @xmath194 which is given by @xmath191 is close to the square of the smallest eigenvalue of @xmath0 , finding the exact rank is a difficult task .",
    "however , the method based on @xmath180 is still good , but fails sometimes .",
    "we produced data from physically meaningful states : the ghz - state and the w - state for @xmath177 qubits , as well as a statistical mixture @xmath195 , for @xmath196 and @xmath197 note that the rank of @xmath198 is 4 .",
    "[ figopnorm ]    * calibration of the tuning parameter @xmath115 . * the quantity @xmath191 seems to be very important to provide a good estimation of the rank @xmath2 ( or more precisely of the effective rank ) .",
    "then it is interesting to observe how this quantity behaves .",
    "figure  [ figopnorm ] ( above @xmath187 and @xmath199 , and middle @xmath9 and @xmath200 ) illustrates how @xmath183 varies when the rank increases . except for @xmath201",
    ", it seems that the value of @xmath183 is quite stable .",
    "these graphics are obtained with particular values of the parameters @xmath3 and @xmath2 , but similar illustrations can be obtained if these parameters change .",
    "+ the main observation according to the parameter @xmath115 is that it decreases with @xmath3 ( see figure  [ figopnorm ] - below ) and is actually independent of the rank @xmath2 ( with some strange behavior when @xmath201 ) .",
    "this is in accordance with the definition of @xmath186 which is an upper bound of @xmath183 .",
    "real - data analysis    in the next paragraph , we propose a 2-steps instruction for practitioners to use our method in order to estimate a matrix @xmath0 ( and its rank @xmath2 ) obtained from the data @xmath95 we have in hand with @xmath202 and @xmath203 .    *",
    "real data algorithm : * + : for any measurement @xmath202 we observe @xmath204 .",
    "+ : @xmath120 and @xmath205 , estimations of the rank @xmath2 and @xmath0 respectively .",
    "+ the procedure starts with the linear estimator @xmath4 and consists in two steps :    _ step a. _ use @xmath4 to simulate repeatedly data with the same parameters @xmath1 and @xmath3 as the original problem .",
    "use the data to compute synthetic linear estimators and the mean operator norm of these estimators .",
    "they provide an evaluation of the tuning parameter @xmath206 .",
    "_ step b. _ find @xmath120 using and construct @xmath205 .",
    "we have applied the method to real data sets concerning systems of 4 to 6 ions , which are smolin states further manipulated . in figure  [ figeigenvalues ]",
    "we plot the eigenvalues of the linear estimator and the threshold given by the penalty . in each case",
    ", the method selects a rank equal to 2 .",
    "[ figeigenvalues ]",
    "we present here a method for reconstructing the quantum state of a system of @xmath1 qubits from all measurements , each repeated @xmath3 times .",
    "such an experiment produce a huge amount of data to exploit in efficient way .",
    "we revisit the inversion method and write an explicit formula for what is here called the linear estimator .",
    "this procedure does not produce a proper quantum state and has other well - known inconvenients .",
    "we consider projection of this state on the subspace of matrices with fixed rank and give an algorithm to select from data the rank which best suits the given quantum system .",
    "the method is very fast , as it comes down to choosing the eigenvalues larger than some threshold , which also appears in the penalty term .",
    "this threshold is of the same order as the error of the linear estimator .",
    "its computation is crucial for good selection of the correct rank and it can be time consuming .",
    "our algorithm also provides a consistent estimator of the true rank of the quantum system .",
    "our theoretical results provide a penalty term @xmath115 which has good asymptotic properties but our numerical results show that it is too large for most examples .",
    "therefore we give an idea about how to evaluate closer the threshold by monte - carlo computation .",
    "this step can be time consuming but we can still improve on numerical efficiency ( parallel computing , etc . ) .    in practice ,",
    "the method works very well for large systems of small ranks , with significant eigenvalues . indeed , there is a trade - off between the amount of data which will give small estimation error ( and threshold ) and the smallest eigenvalue that can be detected above this threshold .",
    "neglecting eigenvalues comes down to reducing the number of parameters to estimate and reducing the variance , whereas large rank will increase the number of parameters and reduce the estimation bias .",
    "* acknowledgements : * we are most grateful to mdlin gu and to thomas monz for useful discussion and for providing us the experimental data used in this manuscript .",
    "* proof of proposition  [ inversion ] * actually , we can compute @xmath207 in case @xmath208 , we have @xmath209 in case @xmath210 , we have either @xmath211 or @xmath212 .",
    "if we suppose @xmath211 , @xmath213 indeed , if this is not 0 it means @xmath214 outside the set @xmath215 , that is @xmath216 which contradicts our assumption .",
    "if we suppose @xmath217 , we have either @xmath210 on the set @xmath218 and in this case one indicator in the product is bound to be 0 , or we have @xmath219 on the set @xmath218 . in this last case , take @xmath220 in the symmetric difference of sets @xmath221 .",
    "then , @xmath222 @xmath223        [ thmtropp ] let @xmath225 , ... , @xmath226 be independent centered self - adjoint random matrices with values in @xmath227 , and let us assume that there are deterministic self - adjoint matrices @xmath228 , ...",
    ", @xmath229 such that , for all @xmath230 , @xmath231 is a.s .",
    "then , for all @xmath232 , @xmath233 where @xmath234",
    ".    we have : @xmath235 note that the @xmath236 , for @xmath237 and @xmath53 , are iid self - adjoint centered random matrices .",
    "moreover , we have : @xmath238 this proves that @xmath239 is nonnegative where @xmath240 .",
    "so we can apply theorem  [ thmtropp ] , we have : @xmath241 and so @xmath242 we put @xmath243 this leads to : @xmath110 @xmath223    * proof of theorem  [ res ] * from the definition ( [ pen ] ) of our estimator , we have , for any hermitian , positive semi - definite matrix @xmath44 , @xmath244 we deduce that @xmath245 further on , we have @xmath246 we apply two times the inequality @xmath247 for any real numbers @xmath248 and @xmath249 .",
    "we actually use @xmath250 and @xmath251 , respectively , and get @xmath252 by rearranging the previous terms , we get that for any hermitian matrix @xmath44 @xmath253 provided that @xmath254 . by following @xcite ,",
    "the least possible value for @xmath255 is @xmath256 if the matrices @xmath44 have rank @xmath42 .",
    "moreover , this value is obviously attained by the projection of @xmath0 on the space of the eigenvectors associated to the @xmath42 largest eigenvalues .",
    "this helps us conclude the proof of the theorem . @xmath223",
    "* proof of corollary  [ hatrank ] * recall that @xmath120 is the largest @xmath42 such that @xmath257 .",
    "we have @xmath258 now , @xmath259 and @xmath260 .",
    "thus , @xmath261 and this is smaller than @xmath262 , by the assumptions of the corollary . @xmath223"
  ],
  "abstract_text": [
    "<S> we introduce a new method to reconstruct the density matrix @xmath0 of a system of @xmath1-qubits and estimate its rank @xmath2 from data obtained by quantum state tomography measurements repeated @xmath3 times . </S>",
    "<S> the procedure consists in minimizing the risk of a linear estimator @xmath4 of @xmath0 penalized by given rank ( from 1 to @xmath5 ) , where @xmath4 is previously obtained by the moment method . </S>",
    "<S> we obtain simultaneously an estimator of the rank and the resulting density matrix associated to this rank . </S>",
    "<S> we establish an upper bound for the error of penalized estimator , evaluated with the frobenius norm , which is of order @xmath6 and consistency for the estimator of the rank . </S>",
    "<S> the proposed methodology is computationaly efficient and is illustrated with some example states and real experimental data sets . </S>"
  ]
}