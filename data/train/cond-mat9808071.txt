{
  "article_text": [
    "mean field approximation ( mfa ) is a well - known technique in statistical physics . in general ,",
    "mfa is defined as a collection of techniques to calculate averages over multivariate distributions , which uses the idea of ignoring higher order correlations and solving self - consistent equations . in this paper",
    ", we restrict ourselves to a type of mfa , which treat distributions with discrete variables , specifically binary variables .",
    "mfa has a long history in physics .",
    "it is , however , a recent topic in the area of statistical information processing .",
    "it is in the late 1980s that the mean field approximation is used in the study of boltzmann machine learning @xcite .",
    "an application in image processing @xcite also draws much attention of researchers in the area and works with cluster versions of mfa appeared .",
    "recently , a higher order version of mfa ( tap equation ) is also applied to the decoding of error correcting codes @xcite and boltzmann machine learning .",
    "a relation between tap equation and belief propagation in bayesian network is discussed in @xcite .",
    "the aim of this paper is to apply the idea of mfa to a _ variable selection _ problem . in our bayesian treatment , variable selection for multiple regression",
    "is formulated as estimation of discrete parameters that indicate a subset of the explanatory variables .",
    "then , a mean field approximation is introduced for the calculation of the posterior marginals on the space of submodels , each of which is specified by a subset of the explanatory variables .",
    "although mfa is closely related to the hopfield - tank approach @xcite to combinatorial optimization , there is an important conceptual difference . while the hopfield - tank method is a tool for optimization",
    ", mfa is a method of approximation of the averages over a distribution .",
    "mfa is a tool that is useful in the problems where averages over a multivariate distribution play important roles .",
    "two important examples of such problems are the calculation of the averages over a gibbs distribution in statistical physics and the calculation of the posterior averages in bayesian statistics . in the present context",
    ", mfa gives posterior averages of discrete parameters , each of which gives the weight or `` the probability of the existence '' of the corresponding explanatory variable .",
    "these averages take the values between 0 and 1 . at this point",
    ", our work is distinguished from other works on variable selection with a hopfield - tank method , say , sakai @xcite .",
    "here we overview the issues on the generalization in neural network models and present the motivation of our work in the context of the interpolation between ridge regression and variable selection .",
    "generalization from a finite set of examples is one of the most important topics in the study of artificial neural network in these ten years .",
    "actually , it is a repeat of a major theme in statistics  how can we beat overfitting and get a better performance of prediction ? for multiple regression models ( noisy linear perceptrons ) and other types of feed - foward network models , two extreme approaches are known .",
    "an extreme is _ ridge regression _ or _ linear weight decay _",
    ", where a penalty function of a quadratic form of synaptic weights ( regression coefficients ) is added to the target function that is optimized in the learning process @xcite .",
    "information from given data is shared among a number of weights with this way of learning .",
    "the other extreme is _ variable selection _ or _ pruning _",
    ", where we select a subnetwork ( a submodel ) with as small number of adjustable connections as that is enough to represent the essence of the data .",
    "information is concentrated on small number of weights with the way of learning .",
    "it is natural to interpolate these two extremes and develop methods , such as a `` fuzzy variable selection '' .",
    "a number of works in this direction are classified into two categories . a way to realize to such",
    "a interpolation is the use of _ generalized ridge regression _ , or , _ nonlinear weight decay _",
    ", where a nonquadratic penalty function is used as the penalty to the magnitude of synaptic weights ( regression coefficients ) @xcite .",
    "another approach to this problem is the introduction of weighted averages over a set of models with different architectures ( with different sets of explanatory variables ) , instead of using a single `` best '' model .",
    "methods of this category are most easily formulated in a bayesian framework , where the posterior distribution on a space of submodels are treated as well as the posterior distribution on a parameter space of each submodel @xcite .",
    "our work belongs to the second category .",
    "we are mostly interested in the algorithm for the computation of posterior marginals in a space of submodels .",
    "the introduction of the posterior distribution on a set of submodels results in combinatorial explosion of the requirement of computer resources , when the submodels are not linearly ordered .",
    "the calculation with exact enumeration is possible only when the number of the variables is small @xcite .",
    "a potentially powerful tool for this problem is markov chain monte carlo algorithm ( mcmc ) @xcite .",
    "the monte carlo approach is , however , still computationally expensive and often suffers from slow convergence .    at this point",
    "we introduce a mean field approximation . as already suggested in the previous section ,",
    "the essence of our idea is the application mfa for calculation of the posterior average on the space of submodels . in the next section",
    ", we will give a formulation and basic notations for the implementation of the idea to a linear network , i.e. , a multiple regression model .",
    "the likelihood function of a multiple regression model is @xmath0 @xmath1 where @xmath2 and @xmath3 are the @xmath4th observation of the response variable ( the output of the network ) and the explanatory variables ( the inputs of the network ) respectably .",
    "@xmath5 is the number of the observations and @xmath6 is the maximum number of explanatory variables available in data .",
    "we introduce a set of indicators @xmath7 for the description of submodels .",
    "if and only if @xmath8 , the variable @xmath9 appears in the submodel specified by @xmath10 . using the indicators @xmath10 , a submodel is written as @xmath11 @xmath12 with this submodel , the maximum likelihood estimator of the regression coefficient @xmath13 is obtained as the solution of the corresponding normal equation @xmath14 when @xmath8 . here ,",
    "sufficient statistics @xmath15 are defined as @xmath16 , @xmath17 , @xmath18 .",
    "when we define @xmath19 and rearrange the terms , we have @xmath20 @xmath21 @xmath22 @xmath23 using @xmath24 . for later convenience",
    ", the indicator @xmath25 that takes the values of @xmath26 is introduced .    to define a bayesian model",
    ", we should specify prior distributions .",
    "the prior distribution of the indicators @xmath27 is assumed as @xmath28 where the hyperparameter @xmath29 determines the degree of the penalty to submodels with larger number of explanatory variables . in this paper",
    ", we also assume that the prior of the coefficients @xmath30 is the uniform density in @xmath31 .",
    "then the posterior distribution @xmath32 of @xmath33 is formally defined as a gibbs distribution @xmath34 with the energy @xmath35 and the normalization @xmath36 .",
    "diverges with the uniform prior for @xmath30 .",
    "apparently , this is not significant in our approximation , where a point estimate @xmath37 of the regression coefficients is used .",
    "it might cause , however , some difficulties , e.g. in a bayesian interpretation of the strength @xmath29 of the penalty . ] here and hereafter , @xmath38 denote the summation over @xmath39 configurations of @xmath27 .",
    "now , we discuss an application of mean field approximation to variable selection for multiple regression .",
    "first , we consider the posterior distribution of @xmath27 conditioned with a given set @xmath30 of regression coefficients , @xmath40 the normalization constant @xmath41 is defined by @xmath42 consider a distribution @xmath43 and the kl - divergence @xmath44 between @xmath45 and @xmath46 .",
    "an easy calculation shows that @xmath47 the values @xmath48 of @xmath49 that minimize @xmath50 is a solution of the `` self - consistent equation '' @xmath51    from the inequality @xmath52 , the relation @xmath53 is derived , where @xmath54 the equality is hold only when the distributions @xmath45 and @xmath46 are identical .",
    "the form of the distribution ( [ mfdist ] ) suggests that the @xmath55th component @xmath56 of a solution of the equation ( [ self ] ) is regarded as an approximation of the averages of the indicator @xmath25 over the distribution ( [ posts ] ) .",
    "thus the posterior probability @xmath57 that the variable @xmath9 is contained in a submodel is approximated by @xmath58 we can also regard @xmath59 as an approximation of @xmath60 . the self - consistent equation ( [ self ] ) and the expressions ( [ mfprob ] ) and ( [ mffree ] ) are the essence of the mfa in our problem .",
    "so far , we are working with a given set of regression coefficients @xmath30 . in our problem",
    ", they are unknown parameters .",
    "a way to deal with them is to replace them with point estimates that maximize the likelihood @xmath61 marginalized over @xmath27 .",
    "is not fully justified in a bayesian paradigm and leads to an approximation beyond the conventional use of mfa , it greatly simplified the situation . ] it is represented by the normalization constant @xmath41 of ( [ zs ] ) as @xmath62 this marginal likelihood ( [ ml ] ) is approximated by the mfa as @xmath63 the values @xmath64 that maximize ( [ mff ] ) is a solution of a `` soft version '' of the normal equation should be considered as a function of @xmath64 in the derivation .",
    "however , with the use of @xmath65 , the result coincides with that of a naive calculation . ]",
    "@xmath66 the equation ( [ mfnormal ] ) is , in fact , a equation nonlinear in @xmath64 , because the probability @xmath67 is a function of @xmath37 implicitly determined with ( [ mfprob ] ) and ( [ self],[hh],[jj ] ) . in actual implementation , the following algorithm is used to solve the set of equations .",
    "\\(1 ) set the initial values @xmath68 of @xmath49 and @xmath69 .",
    "\\(2 ) solve the equation of @xmath70 @xmath71 with @xmath72 .",
    "\\(3 ) iterate the equation @xmath73 here , the variables @xmath74,@xmath75 are defined by ( [ hh],[jj ] ) with @xmath76 .",
    "@xmath77 is a constant that satisfies @xmath78 .",
    "\\(4 ) set @xmath79 .",
    "check the convergence and return to the step ( 2 ) , if necessary . if convergence is ensured , set @xmath80 and @xmath81 and exit .",
    "we can also estimate the variance @xmath82 of the residual with the maximization of ( [ mff ] ) .",
    "for this purpose , we add a step for the estimation of @xmath82 to the iteration . in our implementation",
    ", the value of @xmath82 is updated once per the iteration .",
    "in this section , we discuss an application of the proposed method to a real world example .",
    "we use the dataset taken from p.244 of belsley et al .",
    "this dataset , known as boston housing data , is also analyzed by other authors with different methods  @xcite .",
    "there are thirteen explanatory variables ( @xmath83 ) and one response variable , the logarithm of the median value of owner - occupied homes for each area of boston .",
    "the sample size is @xmath84 .",
    "our results on this data are shown in fig.1 and fig.2 . in the experiment ,",
    "the variance @xmath82 is estimated from the data .",
    "although we observe local optima with some parameters of @xmath29 , they are not serious and the solution with the highest frequency at each value of @xmath29 is plotted in the figures .",
    "fig.1 :  the results with some typical values of @xmath29 .",
    "the index @xmath55 of a variable is shown in the horizontal axis and the probability @xmath85 is shown in the vertical axis .",
    "+    fig.2 :  the change of @xmath86 is shown with @xmath29 . the number used as a symbol indicates the index  @xmath55 of the variable .",
    "the positions of the symbols are slightly perturbed by noise to prevent complete overlap between them .",
    "lines are accurate .    in fig.2",
    ", the probability @xmath86 takes the value @xmath87 , for @xmath88 , even with a larger value of the penalty @xmath29 .",
    "it seems consistent with the result in @xcite . for most of the other variables ,",
    "the probability @xmath86 shows a sudden jump between 0 and 1 , or , a simple decay with @xmath89 , as the value of @xmath29 is increased .",
    "however , for some @xmath55 , say @xmath90 , it shows a gradual decay and has nontrivial values between @xmath91 and @xmath92 in a range of @xmath29 .",
    "in the present treatment , the strength @xmath29 of the penalty to the number of variables is a free hyperparameter . it is not a serious fault when we use the proposed method as a tool for _ regression diagnostics _ , i.e. , we observe the behavior of solutions in different values of @xmath29 and get information on data . a criterion to determine the optimal value of @xmath29 is , however , required , when the method is used as a fully automatic tool for the prediction .",
    "although there are a few possible approaches , their implementations and tests are left for future studies .",
    "another important problem is the study of the multiple solutions in our iterative procedure .",
    "for example , when near loss of the linear independence among explanatory variables ( _ colinearity _ ) exists , the algorithm is likely to have multiple fixed points .",
    "note that the existence of local optima is usually regarded as a mere difficulty of algorithms in information processing .",
    "there seems , however , often a possibility of exploration of the nature of the data from the emergence of multiple extremum .",
    "it is interesting if the proposed algorithm provides an example for such a study . for this purpose",
    ", it requires more investigation into the behavior of the algorithm in complex situations .",
    "iba , y. , `` metropolis - type monte carlo algorithm and quasi - bayesian estimation procedure : an application to a change - point problem '' , _ proceedings of the institute of statistical mathematics _ ,",
    "* vol.39 * , no.2 , 225244 ( 1991 ) ( in japanese with english summary ) .",
    "phillips , d.b . and smith , a.f.m . ,",
    "`` bayesian model comparison via jump diffusions '' , _ markov chain monte carlo in practice _ , chap.13 , eds .",
    "gilks , w.r . ,",
    "richardson , s . and",
    "spiegelhalter , d.j . ,",
    "chapman and hall ( 1996 ) .",
    "sakai , h .",
    ", `` an application of bic type method to harmonic analysis and a new criterion for order determination of an ar process '' , _",
    "ism cooperative research report 20 _ , the institute of statistical mathematics(tokyo ) ( 1993 ) .",
    "weigend , a.s . and rumelhart , d.e .",
    ", `` generalization through minimal networks with application to forecasting '' , _ computing science and statistics _ , proceedings of the 23rd symposium on the interface , 156163 , interface foundation , fairfax station , va ."
  ],
  "abstract_text": [
    "<S> variable selection for a multiple regression model ( noisy linear perceptron ) is studied with a mean field approximation . in our bayesian framework , </S>",
    "<S> variable selection is formulated as estimation of discrete parameters that indicate a subset of the explanatory variables . </S>",
    "<S> then , a mean field approximation is introduced for the calculation of the posterior averages over the discrete parameters . </S>",
    "<S> an application to a real world example , boston housing data , is shown .    </S>",
    "<S> probabilistic and statistical methods , learning and generalization , mean field approximation , model selection , multiple regression , bayes </S>"
  ]
}