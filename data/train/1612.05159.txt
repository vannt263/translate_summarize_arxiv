{
  "article_text": [
    "marvin minsky s `` society of mind '' theory postulates that our behaviour is not the result of a single cognitive agent , but rather the result of a society of individually simple , interacting processes called agents @xcite .",
    "the power of this approach lies in specialization : different agents can have different representations , different learning processes , and so on . on a larger scale , our society as a whole validates this approach : our technological achievements are the result of many cooperating specialized agents .    in reinforcement learning ( rl ) , where the goal is to learn a policy for an agent interacting with an initially unknown environment , the importance of breaking large tasks into smaller pieces has long been recognized .",
    "specifically , there has been a lot of work on hierarchical rl methods , which decompose a task in a hierarchical way into subtasks @xcite .",
    "hierarchical learning can help accelerate learning on individual tasks by mitigating the exploration challenge of sparse - reward problems .",
    "one of the most popular frameworks for this is the options framework @xcite , which extends the standard rl framework based on markov decision processes ( mdp ) with temporally - extended actions . in this paper",
    ", we propose a framework of communicating agents that aims to generalize the traditional hierarchical decomposition to allow for more flexible task decompositions .",
    "for example , decompositions where multiple subtasks have to be solved in parallel , or in cases where a subtask does not have a well - defined end , but rather is a continuing process that needs constant adjustment , such as walking through a crowded street .",
    "we refer to our framework as separation - of - concerns .",
    "what is unique about our proposed framework is the way agents cooperate . to enable cooperation",
    ", the reward function of a specific agent not only has a component that depends on the environment state , but also a component that depends on the communication actions of the other agents .",
    "depending on the specific mixture of these components , agents have different degrees of independence .",
    "in addition , because the reward in general is state - specific , an agent can show different levels of dependence in different parts of the state - space .",
    "typically , in areas with high environment - reward , an agent will act independent of the communication actions of other agents , while in areas with low environment - reward , an agent s policy will depend strongly on the communication actions .",
    "in this initial work , we focus primarily on the theoretical aspects of our framework .",
    "formally , our framework can be seen as a sequential multi - agent decision making system with non - cooperative agents .",
    "this is a challenging setting , because from the perspective of one agent , the environment is non - stationary due to the learning of the other agents .",
    "we address this by defining trainer agents with a fixed policy . learning with these trainer agents",
    "can occur in two ways : 1 ) by pre - training agents and then freezing their policy , or 2 ) by learning in parallel using off - policy learning .",
    "the rest of this document is organized as follows . in section 2 ,",
    "we discuss related work . in section 3 ,",
    "we introduce our model .",
    "section 4 contains a number of experiments .",
    "section 5 discusses the results and future work . finally , section 6 concludes .",
    "* hierarchical learning / options * our work builds upon the long line of work on hierarchical learning .",
    "@xcite introduced the maxq framework , which decomposes the value function in a hierarchical way .",
    "@xcite introduced the options framework .",
    "options are temporally extended actions consisting of an initialization set , an option policy and a termination condition .",
    "effectively , applying options to an mdp , changes it into a semi - mdp .",
    "options have also been popular as a mechanism for skill discovery",
    ".    there has been significant work on option discovery . in the tabular setting , useful subgoal states can be identified , for example , by using heuristics based on the visitation frequency @xcite , by using graph partitioning techniques @xcite , or by using the frequency with which state variables change @xcite . with function approximation ,",
    "finding good subgoals becomes significantly more challenging .",
    "@xcite assumed that subgoal states were given ( hence , only the option policy needs to be learned ) .",
    "@xcite perform option discovery by identifying ` purposes ' at the edge of a random agent s visitation area .",
    "learning options towards those edge - purposes brings the agent quickly to a new region where it can continue exploration . @xcite",
    "propose a new architecture to learn the policy over options , the options themselves , as well as their respective termination conditions .",
    "this is accomplished without defining any particular subgoal .",
    "only the number of options is known beforehand .",
    "@xcite study hierarchical rl in the context of deep reinforcement learning . in their setting",
    ", a high - level controller specifies a goal for the low - level controller .",
    "once the goal is accomplished , the top - level controller selects a new goal for the low - level controller .",
    "the system was trained in two phases : in the first phase the low - level controller is trained on a set of different goals ; in the second phase the high - level and low - level controllers are trained in parallel .",
    "@xcite also use a system with a high - level and a low - level controller , but the high - level controller continuously sends a modulation signal to the low - level controller , affecting its policy .",
    "this setup can be seen as a special case of the framework that we introduce .",
    "* multi - agent systems * there is also a large amount of work on multi - agent rl ( e.g. , see @xcite for an overview ) .",
    "the standard multi - agent configuration includes multiple agents which are acting on the environment simultaneously and which receive rewards individually based on the joint actions .",
    "this can naturally be modelled as a stochastic game .",
    "multi - agent systems can be divided into fully cooperative , fully competitive or mixed tasks ( neither cooperative nor competitive ) . for a fully cooperative task , all agents share the same reward function .",
    "early works includes the integrated learning system ( ils ) by @xcite , which integrates heterogeneous learning agents ( such as search - based and knowledge - based ) under a central controller through which the agents critique each other s proposals .",
    "alternatively , @xcite proposed learning with an external critic ( lec ) and learning by watching ( lbw ) , which advocate learning from other agents in a social settings .",
    "it has been shown that a society of @xmath0 q - learning agents , which are watching each other can learn @xmath1 faster than a single q - learning agent .",
    "more recently , @xcite used a framework of communicating agents based on deep neural networks to solve various complex tasks .",
    "they evaluated two learning approaches .",
    "for the first approach , each agent learns its own network parameters , while treating the other agents as part of the environment .",
    "the second approach uses centralized learning and passes gradients between agents .    for fully competitive tasks , which typically deal with the two - agent case only , the agents have opposing goals ( the reward function of one agent is the negative of the reward function of the other",
    "our work falls into the mixed category , which does not pose any restrictions on the reward functions .",
    "a lot of work in the mixed setting focuses on static ( stateless ) tasks .",
    "separation of concerns ( soc ) can benefit from ideas and theoretical advances in multi - agent frameworks .",
    "nevertheless , it should be noted that there are key differences between soc and multi - agent methodologies .",
    "intrinsically , soc splits a single - agent problem into multiple parallel , communicating agents with simpler and more focused , but different objectives ( skills ) .",
    "in this section , we introduce our model for dividing a single - agent task into a multi - agent task .      our single - agent task is defined by a markov decision process ( mdp ) , consisting of the tuple @xmath2 , where @xmath3 is the set of states ; @xmath4 the set of actions ; @xmath5 indicates the probability of a transition to state @xmath6 , when action @xmath7 is taken in state @xmath8 ; @xmath9 indicates the reward for a transition from state @xmath10 to state @xmath11 under action @xmath12 ; finally , the discount factor @xmath13 specifies how future rewards are weighted with respect to the immediate reward .",
    "actions are taken at discrete time steps according to policy @xmath14 , which maps states to actions .",
    "the goal is to maximize the discounted sum of rewards , also referred to as the _ return _ :",
    "@xmath15 we call the agent of the single - agent task the _ flat agent_.    we expand this task into a system of @xmath0 communicating agents as follows . for each agent @xmath16",
    ", we define an environment action - set @xmath17 , a communication action - set @xmath18 , and a _ learning objective _ , defined by a reward function @xmath19 plus a discount factor @xmath20 . furthermore , we define an action - mapping function @xmath21 that maps the joint environment - action space to an action of the flat agent .",
    "the agents share a common state - space @xmath22 consisting of the state - space of the flat agent plus the joint communication actions : @xmath23 .",
    "see figure [ fig : soc model ] for an illustration for @xmath24 .        at time @xmath25",
    ", each agent @xmath16 observes state @xmath26 and selects environment action @xmath27 and communication action @xmath28 according to policy @xmath29 .",
    "action @xmath30 is fed to the environment , which responds with an updated state @xmath31 .",
    "the environment also produces a reward @xmath32 , but this reward is only used to measure the overall performance of the soc model . for learning , each agent @xmath16 uses its own reward function @xmath33 to compute reward @xmath34 .",
    "an important property of the soc model is that the reward function of a particular agent depends on the communication actions of the other agents .",
    "this provides a clear incentive for an agent to react in response to communication , even in the case of full observability .",
    "for example , agent a can ` ask ' agent b to behave in a certain way via a communication action that rewards agent b for this behaviour .",
    "full observability is not an explicit requirement of our framework .",
    "the general model described above can be extended in different ways .",
    "in particular , extensions that allow for further specialization of agents will increase the benefit of the soc model as a whole .",
    "some examples are :    * state abstractions / information hiding : because the agents have different objectives , they can use different state - abstractions .",
    "* action - selection at different frequencies .",
    "* state - dependent gamma ( such that terminal states can be defined ) * state - dependent action sets .",
    "* example : hierarchical configuration * + one important example is the case where the agents are organized in a way that decomposes a task hierarchically . as an example , consider a configuration consisting of three agents : agent 0 is the top - level agent , and agent 1 and agent 2 are two bottom - level agents .",
    "the top - level agent only has communication actions , specifying which of the bottom level agents is in control .",
    "that is , @xmath35 and @xmath36 .",
    "agent 1 and agent 2 both have a state - dependent action - set that gives access to the environment actions @xmath4 if they have been given control by agent 0 .",
    "that is , for agent 1 : @xmath37 and vice - versa for agent 2 . by allowing agent 0 to only switch its action",
    "once the agent currently in control has reached a terminal state ( by either storing a set of terminal state conditions itself , or by being informed via a communication action ) , a typical hierarchical task decomposition is achieved .",
    "this example illustrates that our soc model is a generalization of a hierarchical model . like any multi - agent system , obtaining stable performance for some soc configurations can be challenging . in the next section",
    ", we discuss some learning strategies for obtaining stability .",
    "a common learning approach for mixed - strategy multi - agent systems like ours is to use a single - agent algorithm for each of the agents .",
    "while there is no convergence guarantee in this case , it often works well in practice @xcite . in this section",
    ", we examine under which conditions exact convergence can be guaranteed .    by assigning a stationary policy to all agents , except agent @xmath16",
    ", an implicit mdp is defined for agent @xmath16 with state space @xmath22 , reward function @xmath19 and ( joint ) action space @xmath38 .",
    "the proposition holds if the next state @xmath39 only depends on the current state @xmath40 and joint action @xmath41 . because the policies of all agents other than agent @xmath16 are fixed , knowing @xmath40 fixes a distribution over the environment and communication actions for each of the other agents .",
    "the distribution over these environment actions , together with the environment action of agent @xmath16 determines a distribution for the random variable @xmath31 .",
    "together with the ( distribution over ) communication actions this fixes a distribution for @xmath42 .",
    "it follows from proposition 1 that if we also define a policy for agent @xmath16 , then we get a well - defined value - function .",
    "let @xmath43 be a tuple of policies , assigning a policy to each agent : @xmath44 .",
    "we can now define a value - function @xmath45 with respect to reward function and discount factor of agent @xmath16 as follows : @xmath46 using this , we define the following independence relation between agents .    agent @xmath16 is independent of agent @xmath47 if the value @xmath48 does not depend on the policy of agent @xmath47",
    ". agent @xmath16 is dependent of agent @xmath47 if it is not independent of agent @xmath47 .",
    "note that the agent - state @xmath49 includes the communication action of agent @xmath47 .",
    "hence , this independence definition still allows for a dependence on the most recent communication action of agent @xmath47 ; the independence is only with respect to the future actions of agent @xmath47 , that is , its policy .",
    "a simple example of a case where this independence relation holds is the hierarchical case , where the actions of the top agent remain fixed until the bottom agent reaches a terminal state .",
    "to agent @xmath16 means that agent @xmath16 depends on agent @xmath47 .",
    "circles represent regular agents ; diamonds represent trainer agents . the cyclic graph shown in ( b )",
    "can be transformed into an acyclic graph by adding trainer agents . a trainer agent for agent",
    "@xmath16 defines fixed behaviour for the agents that agent @xmath16 depends on to ensure stable learning .",
    ", width=302 ]    using the independence definition above , we can draw a _ dependency graph _ of an soc system , which is a directed graph with arrows showing the dependence relations : an arrow pointing from agent @xmath47 to agent @xmath16 means that agent @xmath16 depends on agent @xmath47 . in general , a dependency graph can be acyclic ( containing no directed cycles , see figure [ fig : dependency graphs]a ) or cyclic ( containing directed cycles , see figure [ fig : dependency graphs]b ) .    if the dependency graph is an acyclic graph , using single - agent q - learning to train the different agents is especially straightforward , as shown by the following theorem .    when the dependency graph is acyclic , using q - learning for each agent in parallel yields convergence under the standard ( single - agent ) conditions .    because the graph is acyclic , some agents are independent of any of the other agents and",
    "hence will converge over time .",
    "once the q - values of these independent agents are sufficiently close to their optimal values ( such that the distance to the optimal value is smaller than the difference between two actions ) , their policies will not change anymore .",
    "once this occurs , the agents that only depend on these independent agents will converge .",
    "after this , the agents one step downstream in the graph will converge , and so on , until all agents have converged .",
    "@xmath50    in many cases",
    ", the dependency graph will be cyclic . in this case ,",
    "convergence of q - learning is not guaranteed .",
    "however , we can transform a cyclic graph into an acyclic one by using _ trainer agents_.    a _ trainer agent _ , assigned to a particular agent @xmath16 , is a fixed - policy agent that generates behaviour for the agents that agent @xmath16 depends on .",
    "assigning a trainer agent to agent @xmath16 implicitly defines a stationary mdp for agent @xmath16 with a corresponding optimal policy that can be learned .",
    "hence , agent @xmath16 only depends on the trainer agent .",
    "the trainer agent itself is an independent agent .",
    "hence , trainer agents can be used to break cycles in dependency graphs .",
    "note that a cyclic graph can be transformed into an acyclic one in different ways . in practice , which agents are assigned trainer agents is a design choice that depends on how easy it is to define effective trainer behaviour . in the simplest case ,",
    "a trainer agent can just be a random or semi - random policy .",
    "as an example , the graph shown in figure [ fig : dependency graphs]b can be transformed into the graph shown in figure [ fig : dependency graphs]c by defining a trainer agent @xmath51 for agent @xmath52 that generates behaviour for agent 2 and 3 , and a trainer agent @xmath53 for agent @xmath54 that generates behaviour for agent @xmath52 and @xmath55 .",
    "learning with trainer agents can occur in two ways .",
    "the easiest way is to pre - train agents with their respective trainer agents , then freeze their weights and train the rest of the agents . alternatively , all agents could be learned in parallel , but the agents that are connected to a trainer agent use off - policy learning to learn values that correspond to the policy of the trainer agent , while the behaviour policy is generated by the regular agents .",
    "off - policy learning can be achieved by importance sampling , which corrects for the frequency at which a particular sample is observed under the behaviour policy versus the frequency at which it is observed under the target policy .",
    "this can be achieved as follows .",
    "consider agent @xmath16 with actions @xmath56 that depends on agent @xmath47 with actions @xmath57 .",
    "furthermore , consider that agent @xmath16 has a trainer agent @xmath58 attached to it mimicking behaviour for agent @xmath47 . in other words ,",
    "agent @xmath58 also has actions @xmath57 . at any moment in time",
    ", the actual behaviour is generated by agents @xmath16 and @xmath47 .",
    "if at time @xmath25 , agent @xmath47 selects action @xmath59 , while the selection probability for that action is @xmath60 , and the selection probability for that same action is @xmath61 for trainer agent @xmath58 , then the off - policy update for agent @xmath16 is : @xmath62    obtaining convergence does not necessarily mean that the policy that is obtained is a good policy .",
    "in the next section , we address the optimality of the policy .      in the context of hierarchical learning , @xcite defines _ recursive optimality _ as a type of local optimality , in which the policy for each subtask is optimal given the policies of its children - subtasks .",
    "the recursive optimal policy is the overall policy that consists of the combination of all the locally optimal policies .",
    "the recursive optimal policy is in general worse than the optimal policy for a flat agent , but it can be much easier to find .",
    "we can define a similar form of optimality for an soc model :    if the dependency graph of an soc model is acyclic ( with or without adding trainer agents ) , then we define a recursive optimal soc policy @xmath44 as the policy consisting of all locally optimal policies , that is , policy @xmath63 is optimal for agent @xmath16 , given the policies of the agents that it depends on",
    ".    the learning strategies discussed in the previous section will converge to the recursive optimal policy .",
    "how close this policy is to the optimal policy depends on the specific decomposition , that is , the communication actions , the agent - reward functions and , potentially , the employed trainer agents .",
    "in this section , we illustrate the application of the soc model using two examples .",
    "first , we demonstrate the method on a tabular domain .",
    "then , we consider a pixel - based game and combine soc with deep reinforcement learning .",
    "the aim of the first experiment is to show the scalability of the soc model . for this",
    ", we use a navigation domain , shown in figure [ fig : navigation ] .",
    "the action set consists of a move forward action , a turn clockwise and a turn counterclockwise action .",
    "furthermore , we add a varying number of extra ` no - op ' actions ( actions without effect ) to control the complexity of the domain .",
    "the reward is -5 when the agent bumps into a wall and -1 for all other transitions .",
    "we compare a flat agent with an soc configuration consisting of a high and low - level agent .",
    "the high - level agent communicates a compass direction to the low - level agent , @xmath64 , and has no environmental actions ( @xmath65 ) .",
    "the low - level agent in turn has access to all environmental actions @xmath66 and no communication actions ( @xmath67 ) .",
    "the reward function of the high - level agent is such that it gets -1 on each transition .",
    "the reward function of the low - level agent is such that it gets -5 for hitting the wall and + 1 if it makes a move in the direction requested by the high - level agent .",
    "all agents are trained with q - learning and use @xmath68-greedy exploration with a fixed @xmath68 of 0.01 and a step - size of 0.1 .     clockwise and turn @xmath69 counter - clockwise .",
    "furthermore , additional ` no - op ' actions are added to increase the complexity of the learning task.,width=151 ]    the left of figure [ fig : nav_learning ] shows the learning behaviour for tasks with different levels of complexity .",
    "specifically , we compare tasks with 5 , 10 and 20 no - op actions .",
    "while the complexity has only a small affect on the performance of the soc method , it affects the flat agent considerably .",
    "this is further illustrated by the right graph of figure [ fig : nav_learning ] , which shows the average return over 4000 episodes for a different number of no - op actions .",
    "overall , these results clearly illustrate the ability of the soc model to improve the scalability .",
    "note that implementing this with a hierarchical approach would require the high - level agent to know the available compass directions in each grid - cell to avoid giving the low - level agent a goal that it can not fulfill ( e.g. , it can not move north while it is in the top - left corner ) .",
    "by contrast , the high - level agent of the soc system does not require this information .",
    "_ comparison of the soc model and a flat agent in the navigation domain for three different task complexities .",
    "_ [ right ] _ scalability of soc learning : the average return over the first 4000 episodes on the navigation task as function of the number of added no - op actions.,title=\"fig:\",width=245 ] _ comparison of the soc model and a flat agent in the navigation domain for three different task complexities .",
    "_ [ right ] _ scalability of soc learning : the average return over the first 4000 episodes on the navigation task as function of the number of added no - op actions.,title=\"fig:\",width=249 ]      in our second example , we compare a flat agent with the soc model on the game catch .",
    "catch is a simple pixel - based game introduced by @xcite .",
    "the game consists of a 24 by 24 screen of binary pixels in which the goal is to catch a ball that is dropped at a random location at the top of the screen with a paddle that moves along the bottom of the screen . in our case , both the ball and the paddle consist of just a single pixel .",
    "the available actions are @xmath70 , @xmath71 and @xmath72 .",
    "the agent receives + 1 reward for catching the ball , -1 if the ball falls of the screen and 0 otherwise .",
    "similar to the navigation domain , our soc model consists of a high - level and a low - level agent .",
    "the high - level agent has no direct access to the environment actions , but it communicates a desired action to the low - level agent : @xmath73 .",
    "the low - level agent has direct access to the environment actions and no communication actions : @xmath74 and @xmath67 .",
    "furthermore , the high - level agent has a discount factor of 0.99 and access to the full screen , whereas the low - level agent has a discount factor of 0.65 and uses a bounding box of 10 by 10 pixels around the paddle .",
    "the low - level agent only observes the ball when its inside the bounding box .",
    "the high - level agents receives a reward of + 1 if the ball is caught and -1 otherwise ; the low - level agent receives the same reward plus a small positive reward for taking the action suggested by the high - level agent .",
    "the high - level agent takes actions every 2 time steps , whereas the low - level agent takes actions every time step .",
    "both the flat agent and the high - level and low - level agents are trained using dqn @xcite . the flat agent uses a convolutional neural network defined as follows .",
    "the 24x24 binary image is passed through two convolutional layers , followed by two dense layers .",
    "both convolutional layers have 32 filters of size ( 5,5 ) and a stride of ( 2,2 ) .",
    "the first dense layer has 128 units , followed by the output layer with 3 units .",
    "the network uses the same activations and initializations as in @xcite .",
    "the high - level agent in the soc system uses an identical architecture to that of the flat agent .",
    "however , due to the reduced state size for the low - level agent , it only requires a small dense network .",
    "the network flattens the 10x10 input and passes it through two dense layers with 128 units each . the output is then concatenated with a 1-hot vector representing the communication action of the high - level agent . the merged output",
    "is then passed through one final dense layer with 3 units ( see figure [ fig : catch_model ] ) .",
    "the left graph of figure [ fig : catch ] shows the results of the comparison .",
    "the soc model learns significantly faster than the flat agent . to show the importance of the co - operation between the low - level and the high - level agent",
    ", we performed an additional experiment where we varied the additional reward the low - level agent gets for taking the action suggested by the high - level agent .",
    "the results are shown in the right graph of figure [ fig : catch ] .",
    "if the additional reward is 0 , the low - level agent has no incentive to listen to the high - level agent and will act fully independent ; alternatively , if the additional reward is very high , it will always follow the suggestion of the high - level agent . because both agents are limited ( the high - level agent has a low action - selection frequency , while the low - level agent has a limited view ) , both these situations are undesirable .",
    "the ideal low - level agent is one that acts neither fully independent nor fully dependent .    _ learning speed comparison on catch ( one epoch corresponds with 40 episodes ) . _",
    "[ right ] _ effect of the communication reward on the final performance of the soc system .",
    ", title=\"fig:\",width=245 ] _ learning speed comparison on catch ( one epoch corresponds with 40 episodes ) . _",
    "[ right ] _ effect of the communication reward on the final performance of the soc system .",
    ", title=\"fig:\",width=245 ]",
    "the experiments from the previous section showed the validity of the separation of concerns principle : separating a task into multiple related sub - tasks can result in considerable speed - ups . in the presented experiments ,",
    "the decomposition was made a priori . in future work",
    ", we would like to focus on ( partially ) learning the decomposition . in this case",
    ", we do not necessarily expect an advantage on single tasks , due to the cost of learning the decomposition . in the transfer learning setting , however , where a high initial cost for learning a representation can be offset by many future applications of that representation , it could prove to be useful .    the soc configuration we used in both our examples consisted of a high - level agent that only communicates and a low - level agent that only performs environment actions .",
    "another direction for future work is to explore alternative configurations and use more than two agents .",
    "the reward function in reinforcement learning often plays a double role : it acts as both the performance objective , specifying what type of behaviour is desired , as well as the learning objective , that is , the feedback signal that modifies the agent s behaviour . that these two roles do not always combine well into a single function",
    "becomes clear from domains with sparse rewards , where learning can be prohibitively slow .",
    "work on intrinsic motivation aims to mitigate this issue by providing an additional reward function to steer learning . in a sense",
    ", the soc model takes this a step further : the performance objective , consisting of the reward function of the environment , is fully separated from the learning objective of the agents , consisting of the agent s reward function .    this clear separation between performance objective and learning objective",
    "further separates the soc model from options .",
    "options , once learned , aggregate the rewards obtained from the environment .",
    "hence , the top - level agent of a hierarchical system based on options learns a value function based on the environment reward .",
    "we argue for a clearer separation .",
    "we presented initial work on a framework for solving single - agent tasks using multiple agents . in our framework ,",
    "different agents are concerned with different parts of the task .",
    "our framework can be viewed as a generalization of the traditional hierarchical decomposition .",
    "we identified conditions under which convergence of q - learning occurs ( to a recursive optimal policy ) and showed experiments to validate the approach .",
    "imek , . , wolfe , a.  p. , and barto , a.  g. identifying useful subgoals in reinforcement learning by local graph partitioning . in _ in proceedings of the international conference on machine learning ( icml ) _ , 2005 .",
    "foerster , j.  n. , assael , y.  m. , de  freitas , n. , and whiteson , s. learning to communicate with deep multi - agent reinforcement learning . in _ proceedings of advances in neural information processing systems ( nips )",
    "_ , 2016 .",
    "kulkarni , t.  d. , narasimhan , k.  r. , saeedi , a. , and tenenbaum , j.  b. hierarchical deep reinforcement learning : integrating temporal abstraction and intrinsic motivation , 2016 .",
    "arxiv:1604.06057 [ cs.lg ] .",
    "mnih , v. , kavukcuoglu , k. , silver , d. , rusu , a.  a. , veness , j. , bellemare , m.  g. , graves , a. , riedmiller , m. , fidjeland , a.  k. , ostrovski , g. , petersen , s. , beattie , c. , sadik , a. , antonoglou , i. , kumaran , h. king  d. , wierstra , d. , legg , s. , and hassabis , d. human - level control through deep reinforcement learning .",
    ", 518:0 529533 , 2015 .",
    "silver , b. , frawley , w. , iba , g. , vittal , j. , and bradford , k. : a framework for multi - paradigmatic learning . in _ proceedings of the seventh international conference on machine learning ( icml )",
    "_ , pp .   348356 , 1990 ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a framework for solving a single - agent task by using multiple agents , each focusing on different aspects of the task . </S>",
    "<S> this approach has two main advantages : 1 ) it allows for specialized agents for different parts of the task , and 2 ) it provides a new way to transfer knowledge , by transferring trained agents . </S>",
    "<S> our framework generalizes the traditional hierarchical decomposition , in which , at any moment in time , a single agent has control until it has solved its particular subtask . </S>",
    "<S> we illustrate our framework using a number of examples . </S>"
  ]
}