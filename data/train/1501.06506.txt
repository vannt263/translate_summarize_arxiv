{
  "article_text": [
    "the ever growing amount of astronomical data provided by the new large scale digital surveys in a wide range of wavelengths of the electromagnetic spectrum has been challenging the way astronomers carry out their everyday analysis of astronomical sources and we can safely assert that the human ability to directly visualize and correlate astronomical data has been pushed to its limits in the past few years . as a consequence of the fact that data have become too complex to be effectively managed and analysed with traditional tools , a new methodological shift is emerging and data mining ( dm ) techniques are becoming more and more popular in tackling knowledge discovery problems . a typical problem which is addressed with these new techniques",
    "is that of the evaluation of photometric redshifts .",
    "the request for accurate photometric redshifts ( photo - z ) has increased over the years due both to the advent of a new generation of multi - band surveys ( see for example connolly et al .",
    "1995 , @xcite ) and to the availability of large public datasets which allowed to pursue a wide variety of scientific cases .",
    "ongoing and future large - field public imaging projects , such as pan - starrs ( farrow et al . 2014 , @xcite ) , kids ( kilo - degree survey ) , des ( dark energy survey , @xcite ) , the planned surveys with lsst ( large synoptic survey telescope , ivezic et al .",
    "2009 , @xcite ) and euclid ( red book , @xcite ) , rely on accurate photo - z to achieve their scientific goals .",
    "photo - z are in fact essential in constraining dark matter and dark energy through weak gravitational lensing ( serjeant 2014 , @xcite ) , for the identification of galaxy clusters and groups ( e.g. capozzi et al .",
    "2009 , @xcite ) , for type ia supernovae , and for the study of the mass function of galaxy clusters ( albrecht et al .",
    "2006 , @xcite , peacock et al .",
    "2006 , @xcite , and umetsu et al .",
    "2012 , @xcite ) , just to quote a few applications .",
    "photometric filters integrate fluxes over a quite large interval of wavelengths and , therefore , the accuracy of photometric redshift reconstruction is worse than that of spectroscopic redshifts . on the other hand , in the absence of the minimal telescope time necessary to determine spectroscopically the redshifts for all sources in a sample , photometric redshifts methods provide a much more convenient way to estimate the distance of such sources .",
    "the physical mechanism responsible for the correlation existing between the photometric features and the redshift of an astronomical source , is the change in the observed fluxes caused by the fact that , due to the stretch introduced by the redshift , prominent features of the spectrum move across the different filters of a photometric system .",
    "this mechanism implies a non - linear mapping between the photometric parameter space of the galaxies and the redshift values .",
    "this non linear mapping function can be inferred using advanced statistical and data mining methods in order to evaluate photometric estimates of the redshift for a large number of sources .",
    "all existing implementations can be broadly categorized into two classes of methods : theoretical and empirical . _",
    "theoretical methods _ use template based spectral energy distributions ( seds ) , obtained either from observed galaxy spectra or from synthetic models .",
    "these methods require an extensive a - priori knowledge about the physical properties of the objects , hence they may be biased by such information .",
    "they , however , represent the only viable method when dealing with faint objects outside the spectroscopic limit ( hildebrandt et al .",
    "2010 , @xcite and references therein ) .",
    "when accurate and multi - band photometry for a large number of objects is complemented by spectroscopic redshifts for a statistically significant sub - sample of the same objects , _ empirical methods _ might offer greater accuracy .",
    "this sample needs , however , to be statistically representative of the parent population .",
    "the spectroscopic redshifts of this sub - sample are then used to constrain the fit of an interpolating function mapping the photometric parameter space .",
    "different methods differ mainly in the way such interpolation is performed .    from the data mining point of view , the evaluation of photo - z is a supervised learning problem ( tagliaferri et al .",
    "2002 , @xcite ) , ( hildebrandt et al . 2010 , @xcite , where a set of _ examples _ is used by the method to learn how to reconstruct the relation between the _ parameters _ and the _ target _ ( brescia 2012 , @xcite ) . in the specific case of photometric redshifts , the parameters are fluxes , magnitudes or colors of the sources while the targets are the spectroscopic redshifts .    a con of this approach being that , as it happens for all interpolative problems , such methods may suffer to extrapolate and therefore they are effective only when applied to galaxies with photometry that lie within the range of fluxes / magnitudes and redshifts well sampled by the training set . in this paper",
    "we present photoraptor ( photometric research application to redshift ) , namely a java based desktop application capable to solve regression and classification problems which has been finely tuned for photo - z estimation .",
    "it embeds a machine learning ( ml ) algorithm , in the specific case a particular instance of a multi - layer neural network , and special tools dedicated to pre- and post - processing data .",
    "the machine learning model is the mlpqna ( multi layer perceptron trained by the quasi newton algorithm ) , which has proven to be particularly powerful photo - z estimator , also in presence of relatively small spectroscopic knowledge base ( kb ) ( cavuoti et al .",
    "2012 , @xcite ) , ( brescia et al . 2013 , @xcite ) . the application is available for download from the dame program web site .",
    "this paper is organized as follows : in sect .",
    "[ photoraptor ] we describe the java application ; in sect .  [ photoz ] we discuss in some details how the evaluation of photometric redshifts is performed . sect .",
    "[ others ] describe other functionalities provided by the application , while sect .",
    "[ comparison ] is dedicated to a comparison between photoraptor and an alternative public machine learning tool . finally in sect .",
    "[ conclusions ] we outline some lessons which were learned during the implementation of photoraptor and draw some future developments .",
    "everyone who has used neural methods to produce photometric redshift evaluation knows that , in order to optimize the results in terms of features , neural network architecture , evaluation of the internal and external errors , many experiments are needed . when coupled with the needs of modern surveys , which require huge data sets to be processed",
    ", it clearly emerges the need for a user friendly , fast and scalable application .",
    "this application needs to run client - side , since a great part of astronomical data is stored in private archives that are not fully accessible on line , thus preventing the use of remote applications , such as those provided by the dameware tool ( brescia et al .",
    "2014 , @xcite ) .",
    "the code of the application was developed in java language and runs on top of a standard java virtual machine , while the machine learning model was implemented in c++ language to increase the core execution speed .",
    "therefore different installation packages are provided to support the most common platforms . moreover ,",
    "the application includes a wizard , which can easily introduce the user through the various functionalities offered by the tool .",
    "the fig .",
    "[ fig : main ] shows the main window of the program .",
    "the main features of photoraptor can be summarized as it follows :    * _ data table manipulation_. it allows the user to navigate throughout his / her data sets and related _ metadata _ , as well as to prepare data tables to be submitted for experiments .",
    "it includes several options to perform the editing , ordering , splitting and shuffling of table rows and columns .",
    "a special set of options is dedicated to the missing data retrieval and handling , for instance not - a - number ( nan ) or not calculated / observed parameters in some data samples ; * _ classification experiments_. the user can perform general classification problems , i.e. automatic separation of an ensemble of data by assigning a common label to an arbitrary number of their subsets , each of them grouped on the base of a hidden similarity .",
    "the classification here is intended as _ supervised _ , in the sense that there must be given a subsample of data for which the right output label has been previously assigned , based on the _ a priori _ knowledge about the treated problem .",
    "the application will learn on this known sample to classify all new unknown instances of the problem ; * _ regression experiments_. the user can perform general regression problems , i.e. automatic learning to find out an embedded and unknown analytical law governing an ensemble of problem data instances ( patterns ) , by correlating the information carried by each element ( features or attributes ) of the given patterns . also the regression is here intended in a _ supervised _ way , i.e. there must be given a subsample of patterns for which the right output is _ a priori _ known . after training on such kb , the program will be able to apply the hidden law to any new pattern of the same problem in the proper way ; * _ photo - z estimation_. within the _ supervised _ regression functionality , the application offers a specialized toolset , specific for photometric redshift estimation .",
    "after the training phase , the system will be able to predict the right photo - z value for any new sky object belonging to the same type ( in terms of photometric input features ) of the knowledge base ; * _ data visualization_. the application includes some @xmath0 and @xmath1 graphics tools , for instance multiple histograms and multiple @xmath0/@xmath1 scatter plots .",
    "such tools are often required to visually inspect and explore data distributions and trends ; * _ data statistics_. for both classification and regression experiments a statistical report is provided about their output . in the first case ,",
    "the typical confusion matrix ( stehman 1997 , @xcite ) is given , including related statistical indicators such as classification efficiency , completeness , purity and contamination for each of the classes defined by the specific problem .",
    "for what the regression is concerned , the application offers a dedicated tool , able to provide several statistical relations between two arbitrary data vectors ( usually two columns of a table ) , such as average ( bias ) , standard deviation ( @xmath2 ) , root mean square ( rms ) , median absolute deviation ( mad ) and the _ normalized _ mad ( nmad , hoaglin et al .",
    "1983 , @xcite ) , the latter specific for the photo - z quality estimation , together with percentages of _ outliers _ at the common threshold @xmath3 and at different multiples of @xmath2 ( brescia et al .",
    "2014 , @xcite ) , ( ilbert et al . 2009 ,",
    "@xcite ) .    in fig .",
    "[ fig : workflow ] the layout of a general photoraptor experiment workflow is shown .",
    "it is valid for either regression and classification cases .",
    "the core of the photoraptor application is its ml model , for instance the mlpqna method .",
    "it is a multi layer perceptron ( mlp ; rosenblatt 1961 , @xcite ) neural network ( fig .",
    "[ fig : mlpqna ] ) , which is among the most used feed - forward neural networks in a large variety of scientific and social contexts .",
    "the mlp is trained by a learning rule based on the quasi newton algorithm ( qna ) .",
    "the qna is a variable metric method for finding local maxima and minima of functions ( davidon 1991 , @xcite ) . the model based on this learning rule and on the mlp network topology",
    "is then called mlpqna .",
    "qna is based on newton s method to find the stationary ( i.e. the zero gradient ) point of a function . in particular",
    ", the qna is an optimization of newton s learning rule , because the implementation is based on a statistical approximation of the hessian of the error function , obtained through a cyclic gradient calculation .    in photoraptor",
    "the quasi newton method was implemented by following the known l - bfgs algorithm ( limited memory - broyden fletcher goldfarb shanno ; byrd 1994 , @xcite ) , which was originally designed for problems with a very large number of features ( hundreds to thousands ) , because in this case it is worth having an increased iteration number due to the lower approximation precision because the overheads become much lower .",
    "this is particularly useful in astrophysical data mining problems , where usually the parameter space is dimensionally huge and is often afflicted by a low signal - to - noise ratio .",
    "the analytical description of the method has been described in the contexts of both classification ( brescia et al .",
    "2012 , @xcite ) and regression ( brescia et al .",
    "2013 , @xcite and cavuoti et al .",
    "2012 , @xcite ) . in the present work ,",
    "we focus the attention on its parameter setup and correct use within the presented framework .",
    "in practice , the problem of photo - z evaluation consists in finding the unknown function which maps the photometric set of features ( magnitudes and/or colors ) into the spectroscopic redshift space .",
    "if a consistent fraction of the objects with spectroscopic redshifts is available , the problem can in fact be approached as a data mining regression problem , where the a priori knowledge ( i.e. the spectroscopic redshifts forming the kb ) , is used to uncover the mapping function .",
    "this function can then be used to derive photo - z for objects without the spectroscopic counterpart information . without entering into much details , which can be found in the literature quoted below and in the references therein",
    ", we just outline that our method has been successfully used in many experiments done on different kbs , often composed through accurate cross - matching among public surveys , such as sdss for galaxies ( brescia et al .",
    "2014 , @xcite ) , ukidss , sdss , galex and wise for quasars ( many of the following figures are referring to this experiment ; brescia et al .",
    "2013 , @xcite ) , goods - north for the phat1 contest ( cavuoti et al .",
    "2012 , @xcite ) and clash - vlt data for galaxies ( biviano et al . 2013 , @xcite ) .",
    "other photo - z prediction experiments are in progress as preparatory work for the euclid mission ( laureijs et al .",
    "2011 , @xcite ) and the kids survey projects .",
    "the fundamental premise to use photoraptor is that the user must preliminarily know how to represent the data and , as trivial as it might seem , it is worth to explicitly state that the user must : _ ( i ) _ be conscious of the target of his experiment , such as for instance a regression or classification ; and _ ( ii ) _ possess a deep knowledge of the used data . in what follows",
    "we shall call features the input parameters ( i.e. , for instance , fluxes , magnitudes or colors in the case of photo - z estimation ) .",
    "+ _ data formats _ + in order to reach an intelligible and homogeneous representation of data sets , it is mandatory to preliminarily take care of their internal format , to transform the pattern features , and to force them to assume a uniform representation before submitting them to the training process . in this respect",
    "real working cases might be quite different .",
    "photoraptor can ingest and/or produce data in any of the following supported formats :    * fits @xcite : tabular / image ; * ascii @xcite : ordinary text , i.e. space separated values ; * votable : vo ( virtual observatory ) compliant xml - based documents ; * csv @xcite : comma separated values ; * jpeg @xcite : joint photographic expert group , as image output type .    _",
    "missing data _ + very frequently , data tables have empty entries ( sparse matrix ) or missing ( lack of observed values for some features in some patterns ) . missing values ( marlin 2008 , @xcite ) are frequently ( but not always ) identified by special entries in the patterns , like not - a - number , out - of - range , negative values in a numeric field normally accepting only positive entries etc . missing data is among the most frequent source of perturbation in the learning process , causing confusion in classification experiments or mismatching in regression problems .",
    "this is especially true for astronomy where inaccurate or missing data are not only frequent , but very often can not be simply neglected since they carry useful information . to be more specific ,",
    "missing data in astronomical databases can be of two types :    * type i : true missing data which were not collected . for instance a given region of the sky or a single object",
    "was not observed in a given photometric band , thus leading to a missing information .",
    "these missing data may arise also from the simple fact that data , coming from any source and related to a generic experiment , are in most case not expressly collected for data mining purposes and , when originally gathered , some features were not considered relevant and thus left unchecked ; * type ii : upper limits or non - detections ( i.e. object too faint to be detected in a given band ) . in this case",
    "the missing datum conveys very useful information which needs to be taken into account into the further analysis .",
    "it needs to be noticed , however that , often upper limits are not measured in absence of a detection and therefore this makes these missing data undistinguishable from type i.    in other words , missing data in a data set might arise from unknown reasons during data collecting process ( type i ) , but sometimes there are very good reasons for their presence in the data since they result from a particular decision or as specific information about an instance for a subset of patterns ( type ii ) .",
    "this fact implies that special care needs to be put in the analysis of the possible presence ( and related causes ) of missing values , together with the decision on how to submit these missing data to the ml method in order to take into account such special cases and prevent wrong behaviors in the learning process .",
    "data entries affected by missing attributes , i.e. patterns having fake values for some features , may be used within the knowledge base used for a photo - z experiment . in particular",
    "they can be used to differentiate the data sets with an incremental quantity of affected patterns , useful to evaluate their noise contribution to the performance of the photo - z estimation after training .",
    "theoretically it has to be expected that a greater amount of missing data , evenly distributed in both training and test sets , induces a greater deterioration in the quality of the results .",
    "this precious information may be indeed used to assign different indices of quality to the produced photo - z catalogues .",
    "the organization of data sets with different rates of missing data can be performed through photoraptor by means of a series of options . +        the fig .",
    "[ fig : nan ] shows the panel dedicated to define and quantify the presence of missing or bad data within the user tables .",
    "the panel allows : ( _ i _ ) to quantify the number of wrong values to be retained / removed in / from the data patterns ; ( _ ii _ ) to completely remove the data patterns affected by the presence of @xmath4 occurrences ; ( _ iii _ ) to assign arbitrary symbols to wrong or missing entries in the dataset ( i.e. symbols like `` @xmath5 '' , `` @xmath4 '' or whatever ) .",
    "+ _ data editing _",
    "+ at the photoraptor core is the mlpqna neural model . in this respect , before launching any experiment , it may be necessary to manipulate data in order to fulfill the requirements in terms of training and test patterns ( data set rows ) and features ( data set columns ) representation as well as contents : _ ( i ) _ both the training and test data files must contain the same number of input and target columns , and the columns must be in the same order ; _ ( ii ) _ the target columns must always be the last columns of the data file ; _ ( iii ) _ the input columns ( features ) must be limited to the physical parameters , without any other type of additional columns ( like column identifiers , object coordinates etc . ) ; _ ( iv ) _ all input data must be numerical values ( no categorical entries are allowed ) .",
    "the application makes available a set of specific options to inspect and modify data file entries .",
    "every time a new data table is loaded , a new window shows the complete table properties ( fig .",
    "[ fig : mainedit ] ) , for instance : name , metadata , path and the number of columns and rows .        for a currently loaded table",
    "it is possible to select a subset of the needed columns . after the selection ,",
    "a table subset is created and , if the option _ row shuffle _ is enabled , the subset rows are also randomly shuffled .",
    "the random shuffling operation is useful to avoid systematic trends during the training phase and to ensure the homogeneity in the distribution of training and test patterns .",
    "this last property is , in fact , directly connected to the necessity to split the initial data into disjoint data sets , to be used for the training and testing phases , respectively .",
    "this is a simple action made possible by the",
    "_ split _ option . when the table is selected in the _ table list _",
    ", the user must give two different names for the split files ( in this case _ train _ and _ test _ ) and two different percentages of the original data set .",
    "it is important to observe that , generally speaking , in machine learning supervised methods three different subsets for every experiment are generally required from the available kb : _ (",
    "i ) _ the _ training set _",
    ", to train the method in order to acquire the hidden correlation among the input features ; _ ( ii ) _ the _ validation set _ ,",
    "used to check and validate the training in particular against the loss of generalization capabilities ( a phenomenon also known as overfitting ) ; and _ ( iii ) _ the _ test set _ , used to evaluate the overall performances of the model ( brescia et al .",
    "2013 , @xcite ) . in the version of the mlpqna model implemented in the photoraptor application",
    ", the validation is embedded into the training phase , by means of the standard leave - one - out k - fold cross validation mechanism ( geisser 1975 , @xcite ) .",
    "therefore , before any photo - z experiment , it is needed to split the data set in only two subsets , for instance , the training and test sets .",
    "there is no any analytical rule to _ a priori _ decide the percentages of the splitting operation . according to the direct experience , an empirical rule of thumb suggests to use @xmath6 and @xmath7 for training and test sets , respectively ( kearns 1996 , @xcite ) .",
    "but certainly it depends on the initial amount of available kb . for example",
    "also @xmath8 vs @xmath9 and @xmath10 vs @xmath11 could be in principle used in case of large datasets ( over ten thousand patterns ) .",
    "the percentage depends also on the quality of the available kb .",
    "when both photometry and spectroscopy are particularly clean and precise , with a high s / n , there could also be possible to obtain high performances by training just on half of the kb .",
    "on the other hand , the more patterns are available for test , the more consistent will be the statistical evaluation of the experiment performances .",
    "+ _ data plotting _ + within the photoraptor application there are also instruments , based on stilts toolset ( taylor 2006 , @xcite ) , capable to generate different types of plots ( some examples are shown in fig .  [",
    "fig : histosample ] , [ fig : scatter ] and [ fig:3dscatter ] ) .",
    "these options are particularly suited during the preparation phase of the data for the experiments .",
    "the graphical options selectable by user are :    * multi - column histograms ; * multiple 2d and 3d scatter plots .    _",
    "data feature selection _",
    "+ learning by examples stands for a training scheme operating under supervision of an oracle capable to provide the correct , already known , outcome for each of the training sample .",
    "this outcome is properly a class or value of the examples and its representation depends on the available kb and on its intrinsic nature even though in most cases it is based on a series of numerical attributes , related to the extracted kb , organized and submitted in an homogeneous way .",
    "therefore , a fundamental step for any machine learning experiment is to decide which features to use as input attributes for the patterns to be learned . in the specific case of photo - z estimation , for a given data sets ,",
    "it is necessary to inspect and check which types of fluxes ( bands ) and combinations ( magnitudes , colors ) is more effective .    in practice",
    ", the user must maximize the information carried by hidden correlations among different bands , magnitudes and zspec available . in spite of what can be thought , not always the maximum number of available parameters should be suitable to train a machine learning model .",
    "the experience demonstrates , in fact , that it is more the quality of data , than the quantity of features and patterns , the crucial key to obtain the best prediction results ( brescia et al . 2013 , @xcite ) .",
    "this phase is very time consuming and usually requires many tens or even hundreds of experiments .",
    "of course , the exact number of experiments depends on a variety of factors , among which , the number of photometric bands and magnitudes for which a high quality of zspec entries is available in the kb ; the photometric and spectroscopic quality of the data , the type of magnitudes ( i.e. aperture , total or isophotal magnitudes , etc . ) , the completeness of the spectroscopic coverage within the kb and the spectroscopic range . in the authors experience , quite often , the optimal combination turned out to be the feature set obtained from the colors plus one reference magnitude for each region of the electro - magnetic spectrum ( broadly divided in uv , optical , near infrared , far infrared , etc . ) @xcite .",
    "this can be understood by remembering that colors convey more information than the single related magnitudes , since from the basic equation defining magnitudes it is easy to see that a magnitude difference corresponds to a flux ratio and hence in the derived colors an ordering relationship among features is always implicitly assumed .",
    "after having prepared the kb , the user should have two subset tables ready to be submitted for a photo - z experiment . by looking at the fig .",
    "[ fig : workflow ] the experiment consists of a pre - determined sequence of steps , for instance : _ ( i ) _ training and validation of the model network ; _ ( ii ) _ blind test of the trained model network ; _ ( iii ) _ run , i.e. the execution on new data samples of a well trained , validated and tested network .",
    "we outline that for the first two steps , the basic rule is to use disjointed but homogeneous data subsets , because all empirical photo - z methods in general may suffer to extrapolate outside the range of parameter distributions covered by the training .",
    "in other words , outside the limits of magnitudes and spectroscopic redshift ( zspec ) imposed by the training set , these methods do not ensure optimal performances .",
    "therefore , in order to remain in a safe condition , the user must perform a selection of test data according to the training sample limits .",
    "none of the objects included in the training sample should be included in the test sample and , moreover , only the data set used for the test has to be used to generate performance statistics . in other words",
    "the test must be blind , i.e. containing only objects never submitted to the network before .    for what the training is concerned ,",
    "this phase embeds two processing steps : the training of the mlpqna model network and its validation .",
    "it is in fact quite frequent for machine learning models to suffer of an _ overfitting _ on training data , affecting and badly conditioning the training performances .",
    "the problem arises from the paradigm of supervised machine learning itself .",
    "any ml model is trained on a set of training data in order to become able to predict new data points .",
    "therefore its goal is not just to maximize its accuracy on training data , but mainly its predictive accuracy on new data instances .",
    "indeed , the more computationally stiff is the model during training , the higher would be the risk to fit the noise and other peculiarities of the training sample in the new data @xcite .",
    "the technique implemented within photoraptor , i.e. the so called _ leave - one - out cross validation _ , does not suffer of such drawback ; it can avoid overfitting on data and is able to improve the generalization performance of the ml model . in this way ,",
    "validation can be implicitly performed during training , by enabling at setup the standard leave - one - out k - fold cross validation mechanism @xcite .",
    "the automatized process of the cross - validation consists in performing @xmath12 different training runs with the following procedure : ( i ) splitting of the training set into @xmath12 random subsets , each one composed by the same percentage of the data set ( depending on the @xmath12 choice ) ; ( ii ) at each run the remaining part of the data set is used for training and the excluded percentage for validation . while avoiding overfitting , the k - fold cross validation leads to an increase of the execution time estimable around @xmath13 times the total number of runs .",
    "concerning the photo - z experiment setup , special care must be paid to the setup of the training parameters , because all the other use cases , for instance the test and run ( i.e. the execution on new data ) , require only the specification of the proper input data set , and to recall the internal model configuration as it was frozen at the end of training ( fig .",
    "[ fig : setup ] ) .",
    "we can group the mlpqna model training parameters into three subsets : _ network topology _",
    ", _ learning rule _ setup and _ validation _ setup .",
    "* * network topology*. it includes all parameters related to the mlp network architecture ; * * _ number of input neurons_. in terms of input data set it corresponds to the number of columns of the data table , ( also named as input features of the data sample , i.e. number of fluxes , magnitudes or colors composing the photometric information of each object in the data ) , except for the target column ( i.e. the spectroscopic redshift ) , which is related to the single output neuron of the regression network .",
    "more in general , in the case of classification problems , the number of output neurons depends on the number of desired classes ; * * _ number of neurons in the first hidden layer_. as a rule of thumb , it is common practice to set this number to @xmath14 , where n is the number of input neurons .",
    "but it can be arbitrarily chosen by the user ; * * _ number of neurons in the second hidden layer_. this is an optional parameter . although not required in normal conditions , as stated by the known universal approximation theorem @xcite , some problems dealing with a parameter space of very high complexity , i.e. with a large amount of distribution irregularities , are better treated by what was defined as _ deep _ networks , i.e. networks with more than one computational ( hidden ) layer @xcite . as a rule of thumb , it is reasonable to set this number to @xmath15 , where @xmath16 is the number of input neurons .",
    "but it is strongly suggested to use a number strictly lower than the dimension of the first hidden layer ; * * _ number of neurons in the output layer_. this number is obviously forced to @xmath17 for regression problems , while in case of classification this quantity depends on the number of classes as present within the treated problem ; * * _ trained network weights_. this parameter is related to the matrix of weights ( internal connections among neurons ) .",
    "a weight matrix exists only after having performed one training session at least .",
    "therefore , this parameter is left empty at the beginning of any experiment .",
    "but , for all other use cases ( test or run ) , it is required to load a previously trained network .",
    "however this parameter could also be used to perform further training cycles for an already trained network ( i.e. in case of an incremental learning experiment ) . * * validation setup * : all parameters related to the optional training validation process ; * * _ cross validation k value_. when the cross validation is enabled , this value is related to the automatic procedure that splits in different subsets the training data set , applying a k - step cycle in which the training error is evaluated and its performances are validated .",
    "reasonable values are between @xmath18 and @xmath19 , depending on the amount of training data used .",
    "the k - fold cross validation intrinsically tries to avoid overfitting . nonetheless , in rare cases ( such as a wrong choice of the k parameter with respect to the train set dimension )",
    ", a residual overfitting may occur . therefore if the user wants to verify it , he / she should simply inspect the results , usually by comparing train with test performance . whenever training accuracy is much better than test one ,",
    "this is a typical clue of overfitting presence .",
    "therefore , when cross validation with a proper k choice is enabled , by definition , it should avoid such events .",
    "the k parameter choice is not deterministic , but regulated by a rule of thumb , depending on the amount of training patterns .",
    "we remind also that this value strongly affects the overall computing time of the experiment . * * learning rule setup*. it includes all parameters related to the qna learning rule ; * * _ maximum number of iterations at each hessian approximation cycle_. the typical range for such value is @xmath20 $ ] , depending on the best compromise between the requested precision and the complexity of the problem .",
    "it can affect the computing time of the training ; * * _ number of hessian approximation cycles_. namely the number of approximation cycles searching for the best value close to the hessian of the error .",
    "if set to zero , the max number of iterations will be used for a single cycle . at each cycle",
    "the algorithm performs a series of iterations along the direction of the minimum error gradient , trying to approximate the hessian value .",
    "a reasonable range is @xmath21 $ ] , although also in this case the exact value depends on the final precision required .",
    "if set to a high value , it is recommended to enable the cross validation option ( see below ) , to prevent overfitting occurrence ; * * _ training error threshold_. this is one of the stopping criteria of the algorithm ( alternative to the couple of the parameters _ iterations _ and _ cycles _ ) .",
    "it is the training error threshold ( a value of @xmath22 is typical for photo - z experiments ) .",
    "* * _ learning decay_. this value determines the analytical _ stiffness _ of the approximation process .",
    "it affects the expression of the weight updating law , by adding the term @xmath23 .",
    "its range may vary from a minimum value of @xmath24 ( very low stiffness ) up to @xmath25 ( very high stiffness ) .",
    "also in this case if a very low value is adopted , it is recommended to enable the cross validation option ( see below ) , to prevent overfitting occurrence .    the error calculated by the mlpqna model during the training",
    "is evaluated for all the presented input patterns in terms of the difference between the known target values and the calculated outputs of the model .",
    "the error function in the regression case is based on the least mean square ( lse ) + tychonov regularization @xcite .",
    "this function is defined as follows :    @xmath26    where @xmath16 is the number of input patterns , @xmath27 and @xmath28 are the network output and the pattern target respectively , @xmath29 is the decay input parameter and @xmath30 the norm of the network weight matrix .",
    "+ regularization of the weight decay is the most important issue within the model mechanisms .",
    "when the regularization factor is accurately chosen , then the generalization error of the trained neural network can be improved , and the training can be accelerated .",
    "if the best regularization parameter @xmath29 is unknown , it could be experimented by varying its value within the allowed range , from a weak up to the strong regularization . in order to achieve the weight decay rule ,",
    "we internally minimize a more complex merit function :    @xmath31    here @xmath32 is the training error , @xmath33 is the sum of the squares of the network weights , and the decay coefficient @xmath29 controls the amount of smoothing applied to the network .",
    "optimization is performed from the initial point and until the successful stopping of the optimizer has been reached .",
    "+ searching for the best decay value is a typical trial - and - error procedure .",
    "it is usually performed by training the network with different values of the parameter @xmath29 , from the lower value ( no regularization ) to the infinite value ( strongest regularization ) . by inspecting statistical results at each stage of the procedure",
    "the overfitting tendency can be monitored by continuously changing the decay factor .",
    "a zero decay usually corresponds to an overfitted network .",
    "very large decay means instead an underfitted network . between these extreme values",
    "there is a range of networks which reproduce the dataset with different degrees of precision and smoothness .",
    "after having successfully terminated a training session , the model will produce ( among several output files ) a final network weight matrix ( file by default called _",
    "trainedweights.txt_ ) and the network configuration setup ( file by default called _ frozen_train_net.txt _ ) , which can be used during next experiment steps ( test and run use cases ) , together with the respective input data sets .",
    "interpolative methods , such as mlpqna , have the advantage that the training set is made up of real objects . in this sense ,",
    "any empirical method intrinsically takes into account effects such as the filter band - pass and flux calibrations , even though the difficulty in extrapolating to regions of the input parameter space not well sampled in the training data is one of the main drawbacks @xcite .",
    "this is why a strong requirement of empirical methods is that the training set must be large enough to cover properly the parameter space in terms of colors , magnitudes , object types and redshift .",
    "if this is true , then the calibrations and corresponding uncertainties are well known and only limited extrapolations beyond the observed locus in color - magnitude space are required . hence , under the conditions described above about the consistency of the training set , a realistic way to measure photometric uncertainties is to compare the photometric redshifts estimation with spectroscopic measures in the test samples .",
    "all individual experiments should be evaluated in a consistent and objective manner through an homogeneous set of statistical indicators .",
    "we remark that all statistical results reported throughout this paper are referred to the blind test data sets only .",
    "in fact , it is good practice to evaluate the results on data ( i.e. the test set ) which have never been presented to the network during any of the training or validation phases . as easy to understand ,",
    "the combination of test and training data might introduce a straightforward systematic bias which could mask reality .",
    "within photoraptor we use a specific algorithm to generate statistics . for each experiment , given a list of @xmath16 blind test samples for @xmath34 and @xmath35 ,",
    "we define : @xmath36    where @xmath37 is the normalized @xmath38 . by indicating with @xmath39 either @xmath38 or @xmath37 , we calculate the following statistical indicators : @xmath40 ^ 2}{n } } \\nonumber \\\\",
    "rms(x ) & = & \\sqrt{\\frac{\\sum^n_{i=1 } x_i^2}{n } } \\nonumber \\\\   mad ( x ) & = & median ( \\mid x \\mid ) \\nonumber \\\\   nmad ( x ) & = & 1.4826 \\times median ( \\mid x \\mid ) \\nonumber\\end{aligned}\\ ] ]    there is also a relation between the root mean square ( rms ) and the standard deviation @xmath2 : @xmath41 , but @xmath42 is the _ variance _ , so we have @xmath43 .",
    "therefore , for a direct comparison of results , in terms of distance of @xmath44 ( @xmath45 ) from the distribution of @xmath38 , it is much more precise to use the standard deviation as main indicator , rather than the simple rms .",
    "there is often a confusion about the relation between photometric and spectroscopic redshifts used to apply the statistical indicators .",
    "for instance , the performance could be very different if the simple @xmath38 is used instead of the @xmath37 .",
    "the idea is that the @xmath38 can not represent the best statistical indicator in the specific case of photometric redshift prediction .",
    "the velocity dispersion error , intrinsically present within the photometric estimation , is not uniform over a wide range of spectroscopic redshift and therefore the related statistics is not able to give a consistent estimation . on the contrary",
    ", the normalized term @xmath37 introduces a more uniform information , correlating in a more correct way the variation of photometric estimation , and thus permitting a more consistent statistical evaluation at all ranges of spectroscopic redshift .    for what the analysis of the catastrophic outliers is concerned , according to @xcite , the parameter @xmath46 enables the identification of outliers in photometric redshifts derived through sed fitting methods ( usually evaluated through numerical simulations based on mock catalogues ) .",
    "in fact , in the hypothesis that the redshift error @xmath37 is gaussian , the catastrophic redshift error limit would be constrained by the width of the redshift probability distribution , corresponding to the @xmath47 confidence interval , i.e. with @xmath48 . in our case , however , photo - z are empirical , i.e. not based on any specific fitting model and it is preferable to use the standard deviation value @xmath49 derived from the photometric cross matched samples , although it could overestimate the theoretical gaussian @xmath2 , due to the residual spectroscopic uncertainty as well as to the method training error . therefore , we consider as catastrophic outliers the objects with @xmath50 .",
    "this although it is common practice to indicate as outliers all objects with @xmath51 , ( thus included in the provided statistics ) .",
    "it is also important to notice that for empirical methods it is useful to analyze the correlation between the @xmath52 and the standard deviation @xmath53 calculated on the data sample for which @xmath54 .",
    "in fact , the quantity @xmath55 is smaller than the value of the @xmath56 . in such condition",
    "we can assert that the pseudo - gaussian distribution of @xmath57 is mostly influenced by the presence of catastrophic outliers .",
    "all the described statistical indicators are provided by photoraptor as the output of every photo - z estimation test and are stored in specific files ( by default named as _ test_statistics.txt _ ) . for completeness",
    "we also provide a similar statistics file as the output of any training session ( fig .",
    "[ fig : stats ] ) . but",
    "its use is only to allow a quick comparison between training and test , just in order to verify the absence of any overfitting occurrence .",
    "+        besides the statistics files , photoraptor makes also available some graphical tools , useful to perform a visual inspection of photo - z experiments .",
    "in particular a @xmath0 scatter plot to show the trend of photo - z vs zspec ( fig .",
    "[ fig : zphot ] ) , as well as a set of histograms useful to graphically evaluate the distributions of quantities @xmath38 and @xmath37 .",
    "to complete the description of the resources made available by photoraptor , we wish to stress that besides photometric redshift estimation ( to be intended as a specific type of regression experiment ) , the user has the possibility to perform generic regression as well as multi - class classification experiments .    for a generic regression problem ,",
    "all the above functionalities described in the case of the photo - z , remain still valid , with the only straightforward exception for the statistics produced , which is generated for generic quantities formulated below .",
    "@xmath58    also in the case of the multi - class classification , the above considerations and options remain still valid with only some differences , described in what follows .    during the training setup ( fig .",
    "[ fig : clsetup ] ) , there are two specific options , not foreseen for regression problems :    * _ output neurons_. the number of neurons of the output layer ( which is forced to be @xmath17 in the regression experiments ) , in this case corresponds to the number of different classes present in the training sample .",
    "it is required that the class identifiers should have a binary format label .",
    "for instance , in a three - class problem , the target classes are represented in three columns labeled respectively , as @xmath59 , @xmath60 and @xmath61 ; * _ cross entropy _ : this optional parameter , if enabled , replaces the standard training error evaluation ( for instance the mse between output and target values ) .",
    "its meaning is discussed below .",
    "the cross entropy ( ce ) error function was introduced to address classification problem evaluation in a consistent statistical fashion @xcite .",
    "the ce method consists of two phases : _ ( i ) _ it generates a random data sample ( trajectories , vectors , etc . ) according to a specified mechanism ; _",
    "( ii ) _ it updates the parameters of the random mechanism based on the data to produce a _",
    "better _ sample in the next iteration .    in practice ,",
    "a data model is created based on the training set , and its ce is measured on a test set to assess how accurately the model is predicting the test data .",
    "the method compares indeed two probability distributions , @xmath62 the true distribution of data in the data set , and @xmath63 which is the distribution of data as predicted by the model .",
    "since the true distribution is unknown , the ce can not be directly calculated , while an estimate of ce is obtained using the following expression :    @xmath64    where @xmath65 is the chosen training set , corresponding to the true distribution @xmath62 , @xmath16 is the number of objects in the test set , and @xmath66 is the probability of the event @xmath39 as estimated from the training set .",
    "another difference with respect to regression experiments is of course the statistics produced to evaluate the results outcoming from a classification experiment . in this case , at the base of the statistical indicators adopted , there is the commonly known _ confusion matrix _ , which can be used to easily visualize the classification performance @xcite : each column of the matrix represents the instances in a predicted class , while each row represents the instances in the real class ( fig .  [ fig : cloutput ] ) .",
    "one benefit of a confusion matrix is the simple way in which it allows to see whether the system is mixing different classes or not .",
    "more specifically , for a generic two - class confusion matrix ,    @xmath67    we then use its entries to define the following statistical quantities :    * : @xmath68 .",
    "defined as the ratio between the number of correctly classified objects and the total number of objects in the data set .",
    "in our confusion matrix example it would be : @xmath69 * : @xmath70 .",
    "defined as the ratio between the number of correctly classified objects of a class and the number of objects classified in that class .",
    "in our confusion matrix example it would be : @xmath71 @xmath72 * : @xmath73 .",
    "defined as the ratio between the number of correctly classified objects in that class and the total number of objects of that class in the data set .",
    "in our confusion matrix example it would be : @xmath74 @xmath75 * : @xmath76 .",
    "it is the dual of the purity , namely it is the ratio between the misclassified objects in a class and the number of objects classified in that class , in our confusion matrix example will be : @xmath77 @xmath78    all these statistical indicators are packed in an output file , produced at the end of the test phase of any classification experiment .",
    "the mlpqna machine learning method , embedded into photoraptor , has been already tested in several classification cases . in brescia",
    "2012 , @xcite , we compared the performances of mlpqna with other machine learning based classifiers and traditional techniques as well , in terms of accuracy of identifying candidate globular clusters in the ngc 1399 hst single - band data . in cavuoti",
    "2014 , @xcite , we compared mlpqna with standard mlp and support vector machine to photometrically classify agns in the sdss dr4 archive . finally , we recently have exploited the mlpqna to perform classification experiments within sdss dr10 archive , aimed at photometrically identifying quasars from the whole sample including also galaxies and stars , as well as to verify the possibility to disentangle normal galaxies from objects with a peculiar spectrum , ( brescia et al .",
    "2015 , @xcite ) .",
    "we performed a simple comparison between photoraptor and an alternative machine learning tool publicly available : the scikit - learn toolset @xcite .",
    "the comparison is based on the photo - z estimation by means of a supervised non - linear regression experiment , by directly comparing the statistical performances between the mlpqna model provided through photoraptor and the widely known ensemble method based on random forest @xcite , which uses a random subset of candidate data features to build an ensemble of decision trees .",
    "the data set used for the experiment was obtained by merging the photometry from four different surveys ( ukidss , sdss , galex and wise ) , including derived colors and reference magnitudes for each band as internal features , thus covering a wide range of wavelengths from the uv to the mid - infrared . while the spectroscopic redshifts , ( i.e. the zspec target values ) were derived from selected quasars of the sdss - dr7 database .",
    "the complete kb consisted of @xmath79 objects , from which the 60% used as training set and the residual 40% as blind test set ( see brescia et al .",
    "2013 , @xcite , for more details ) .",
    "we remark also that in that case , our mlpqna has been directly compared with other several photo - z estimation methods ( see references therein ) , achieving best results .         after having trained the two ml models with the same training set ,",
    "their photo - z estimation results have been compared in terms of statistics and residual analysis ( outlier percentages ) .",
    "the results are shown in fig .  [",
    "fig : zphotcomp ] and reported in tab .",
    "[ qso : tab : comparison ] . from the comparison ,",
    "it results apparent that mlpqna performs better than random forest , especially in the high - redshift zone ( i.e. at @xmath80 ) , showing a more robust prediction capability also in the sparsely populated regions of the parameter space .",
    "in addition , unlike the photoraptor resource , in order to setup and run the random forest model provided by the scikit - learn package , as well as to prepare and execute the experiments , some manipulations of the source code have been necessary .",
    "the reason is that the scikit - learn package is provided as a library to be imported in a user - defined script code , which implies a certain knowledge of the python programming language .    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     although we reported a use case example where photoraptor has been tested on a dataset of about @xmath81 samples , we want to emphasize that the reliability of our resource has been already verified for data sets up to @xmath82 samples .",
    "however , in such cases the computational cost of the experiment becomes very high , although the regression accuracy does not seem to require such amount of data in the training set .",
    "therefore , as general rule of thumb , a good compromise between computational time and performance could be to limit the training sample to about @xmath83 samples .",
    "in addition , our model mlpqna has been tested in a public photo - z contest ( phat1 , hildebrandt et al .",
    "2010 , @xcite , and cavuoti et al .",
    "2012 , @xcite ) , resulting as one of the best interpolative methods . in another work ( brescia et al .",
    "2014 , @xcite ) we published a catalogue of photometric redshifts for the sdss dr9 release , by comparing our prediction accuracy with other machine learning methods .",
    "more recently photoraptor has been used by an independent group ( hoyle et al .",
    "2014 , @xcite ) , that performed a regression feature analysis with sdss dr10 galaxies by comparing our resource with random forest ( adaboost , @xcite ) and fann artificial neural networks ( nissen 2003 , @xcite ) .",
    "driven by the advances in the digital detectors and computing technology , astronomy has become an immensely data - rich science .",
    "this exponential data avalanche continues .",
    "it enables some exciting new science , but poses many non - trivial challenges that are common to many other data - driven fields .",
    "nowadays the technological evolution of astronomical instruments has been so fast to render physically impossible to move the data from their original repositories .",
    "the real goal of science , namely data analysis and knowledge discovery , begins after all the data processing and data delivery through the archives .",
    "this requires some powerful new approaches to data exploration and analysis , leading to knowledge discovery and understanding .",
    "this implies that , as it has always been asked for but never implemented , we must be able to move the programs and not the data .",
    "therefore , the future of any data - driven service depends on the capability and possibility of moving the data mining applications to the data centers hosting the data themselves . in such scenario",
    ", photoraptor represents our test bench of a desktop application prototype capable to fulfill this concept . as a final perspective , we want to address the still open problem to find an efficient , reliable and standard way to provide single photo - z errors in the case of interpolative methods .",
    "we have recently started to investigate such problem and intend to improve photoraptor in the next future with such kind of a tool .",
    "the authors wish to thank the anonymous referee for all very useful comments and suggestions .",
    "mb wishes to thank the financial support of the 7th european framework programme for research grant fp7-space-2013 - 1 , _ vialactea - the milky way as a star formation engine_. the authors also wish to thank the financial support of project f.a.r.o .",
    "iii tornata ( university federico ii of naples ) .",
    "gl acknowledges financial contribution through the prin - miur 2012 _ cosmology with the euclid space mission_. +    albrecht , a. , bernstein , g. , cahn , r. et al .",
    ", report of the dark energy task force , ( 2006 ) ansi ( american national standards institute ) et al . , american national standard code for information interchange . the institute ( 1977 )",
    "bengio , y. , & lecun , j. , in large - scale kernel machines .",
    "mit press ( 2007 ) biviano , a. , et al .",
    ", clash - vlt : the mass , velocity - anisotropy , and pseudo - phase - space density profiles of the z=0.44 galaxy cluster macs 1206.2 - 0847 .",
    "a&a , 558 , a1 , 22 pages ( 2013 ) breiman , l. , random forests .",
    "machine learning , springer eds .",
    ", 45 , 1 , 25 - 32 ( 2001 ) brescia m. , new trends in e - science : machine learning and knowledge discovery in databases .",
    "78 pages , horizons in computer science research , thomas s. clary ( eds . ) , series horizons in computer science vol . 7 ,",
    "nova science publishers , isbn : 978 - 1 - 61942 - 774 - 7 ( 2012 ) brescia , m. , cavuoti , s. , paolillo , m. , longo , g. , puzia , t. , the detection of globular clusters in galaxies as a data mining problem , mnras , 421 , 2 , 1155 ( 2012 ) brescia , m. , cavuoti , s. , dabrusco , r. , longo , g. , & mercurio , a. , photometric redshifts for quasars in multi band surveys .",
    "apj , 772 , 140 ( 2013 ) brescia , m. , et al .",
    ", dameware : a web cyberinfrastructure for astrophysical data mining .",
    "pasp , 126 , 942 , 783 - 797 ( 2014 ) brescia , m. , cavuoti , s. , de stefano , v. , longo , g. , a catalogue of photometric redshifts for the sdss - dr9 galaxies , a&a , 568 , a126 ( 2014 ) brescia , m. , cavuoti , s. , longo , g. , automated physical classification in the sdss dr10 .",
    "a catalogue of candidate quasars , mnras , accepted ( in press ) ( 2015 ) byrd , r.h , nocedal , j. , and schnabel , r.b .",
    ", mathematical programming , 63 , 129 ( 1994 ) capozzi , d. , de filippis , e. , paolillo , m. , dabrusco , r. , longo , g. , the properties of the heterogeneous shakhbazyan groups of galaxies in the sdss .",
    "monthly notices of the royal astronomical society , volume 396 , issue 2 , 900 - 917 , ( 2009 ) cavuoti , s. , brescia , m. , longo , g. , mercurio , a. , photometric redshifts with quasi newton algorithm ( mlpqna ) .",
    "results in the phat1 contest , a&a , vol .",
    "546 , a13 , 1 - 8 ( 2012 ) cavuoti , s. , brescia , m. , dabrusco , r. , longo , g. , paolillo , m. , photometric classification of emission line galaxies with machine - learning methods .",
    "mnras , vol .",
    "437 , 1 , 968 - 975 ( 2014 ) collister , a. a. & lahav , o. , annz : estimating photometric redshifts using artificial neural networks .",
    "pasp , 116 , 345 ( 2004 ) connolly , a.j . ,",
    "csabai , i. , szalay , a.s . ,",
    "koo , d.c . ,",
    "kron , r.g . ,",
    "munn , j.a .",
    ", slicing through multicolor space : galaxy redshifts from broadband photometry .",
    "astronomical journal v.110 , p.2655 ( 1995 ) cybenko , g. , approximations by superpositions of sigmoidal functions .",
    "mathematics of control , signals , and systems , 2 , 303 ( 1989 ) the dark energy survey collaboration , the dark energy survey , white paper submitted to the dark energy task force , 42 pages , arxiv:0510346 ( 2005 ) davidon , w.c . ,",
    "siam journal on optimization ( 1991 ) dietterich , t. , overfitting and undercomputing in machine learning .",
    "computing surveys , 27 , 326 ( 1995 ) drucker , h. , improving regressors using boosting techniques .",
    "proceedings of icml97 , morgan kaufmann publishers inc .",
    ", san francisco , ca , usa , pp .",
    "107 - 115 ( 1997 ) euclid red book , esa technical document , esa / sre(2011)12 , issue 1.1 , arxiv:1110.3193 ( 2011 ) farrow , d.j .",
    "et al . , pan - starrs1 : galaxy clustering in the small area survey 2 .",
    "mnras , 437 , 748 - 770 ( 2014 ) geisser , s. , the predictive sample reuse method with applications .",
    "journal of the american statistical association , 70 ( 350 ) , 320 - 328 ( 1975 ) groetsch , c.v .",
    ", the theory of tikhonov regularization for fredholm equations of the first kind , pitman , boston ( 1984 ) hildebrandt , h. , arnouts , s. , capak , p. , wolf , c. et al .",
    "a&a , 523 , 31 ( 2010 ) hoaglin , d.c . ,",
    "mosteller , f. , & tukey , j.w . , understanding robust and exploratory data analysis , new york : wiley c(1983 ) hoyle , b. , rau , m.m .",
    ", zitlau , r. , seitz , s. , weller , j. , feature importance for machine learning redshifts applied to sdss galaxies , sumitted to mnras , arxiv:1410.4696 , ( 2014 ) ilbert , o. , capak , p. , salvato , m. , et al . , cosmos photometric redshifts with 30-bands for @xmath84 .",
    "the astrophysical journal 690 , 1236 ( 2009 ) ivezic , z. , et al .",
    "( the lsst team ) , the lsst science book , v2.0 ( 2009 ) kearns , m. , a bound on the error of cross validation using the approximation and estimation rates , with consequences for training - test split , neural information processing 8 , d.s .",
    "touretzky , m , c . mozer and m.e .",
    "hasselmo ( eds . ) , morgan kaufmann , 183 - 189 ( 1996 ) laureijs , r. , et al . ,",
    "euclid definition study report , esa / sre(2011)12 , issue 1.1 ( 2011 ) marlin , b.m . , missing data problems in machine learning , library and archives : canada ( 2008 ) mobasher , b. , capak , p. , scoville , et al .",
    ", photometric redshifts of galaxies in cosmos . the astrophysical journal supplement series , vol . 172 , 1 , 117 - 131 ( 2007 ) nissen , s. , implementation of a fast artificial neural network library . technical report . department of computer science university of copenhagen ( diku ) , ( 2003 ) peacock , j.a . , schneider , p. , efstathiou , g. et al .",
    ", esa - eso working group on fundamental cosmology , esa - eso working group on fundamental cosmology , tech . rep .",
    "( 2006 ) pedregosa , f. , et al .",
    ", scikit - learn : machine learning in python .",
    "jmlr 12 , 2825 - 2830 ( 2011 ) pennebaker , w.b . , and mitchell , j.l . , jpeg still image data compression standard , ( 3rd ed . ) ( 1993 ) provost , f. , fawcett , t. , kohavi , r. , the case against accuracy estimation for comparing induction algorithms .",
    "proceedings of the 15th international conference on machine learning .",
    "morgan kaufmann eds .",
    ", 445 - 553 ( 1998 ) repici , j. , how to : the comma separated value ( csv ) file format .",
    "creativyst inc .",
    "( 2010 ) rosenblatt , f. , principles of neurodynamics : perceptrons and the theory of brain mechanisms .",
    "spartan books , washington dc ( 1961 ) rubinstein , r.y .",
    ", kroese d.p .",
    ", the cross - entropy method : a unified approach to combinatorial optimization , monte - carlo simulation , and machine learning , springer - verlag , new york ( 2004 ) serjeant , s. , up to 100,000 reliable strong gravitational lenses in future dark energy experiments .",
    "apj , 793 , 1 , l10 , ( 2014 ) stehman , s.v . , selecting and interpreting measures of thematic classification accuracy .",
    "remote sensing of environment 62 , 1 , 77 - 89 ( 1997 ) tagliaferri , r. , longo , g. , andreon , s. , capozziello , s. , donalek , c. , giordano , g. , neural networks and photometric redshifts , neural nets .",
    "lecture notes in computer science , vol . 2859 , 226 - 234 ( 2002 ) taylor , m.b .",
    ", stilts - a package for command - line processing of tabular data .",
    "proceedings of the astronomical data analysis software and systems xv asp conference series , vol .",
    "351 , p. 666",
    "( 2006 ) umetsu , k. , medezinski , e. , nonino , m. , et al .",
    "clash : mass distribution in and around macs j1206.2 - 0847 from a full cluster lensing analysis .",
    "apj , 755 , 1 , 56 ( 2012 ) wells , d.c . , greisen , e.w . ,",
    "harten , r.h . , fits : a flexible image transport system .",
    "astronomy & astrophysics supplement series , vol .",
    "44 , p. 363"
  ],
  "abstract_text": [
    "<S> photometric redshifts ( photo - z ) are crucial to the scientific exploitation of modern panchromatic digital surveys . in this paper </S>",
    "<S> we present photoraptor ( photometric research application to redshift ) : a java / c++ based desktop application capable to solve non - linear regression and multi - variate classification problems , in particular specialized for photo - z estimation . </S>",
    "<S> it embeds a machine learning algorithm , namely a multi - layer neural network trained by the quasi newton learning rule , and special tools dedicated to pre- and post - processing data . </S>",
    "<S> photoraptor has been successfully tested on several scientific cases . </S>",
    "<S> the application is available for free download from the dame program web site . </S>"
  ]
}