{
  "article_text": [
    "with the increasing popularity of social networks , there exist many interesting and difficult problems , such as friends recommendation , information propagation , etc . in this paper ,",
    "we study the problem of social trust prediction , which aims to estimate the positive or negative relationship among the users based on the existing trustness information associated with them .",
    "this problem plays an important role in social networks as the system can block the invitation from someone that the user does not trust , or recommend new friends who enjoys a high reputation .",
    "naturally , the social trust problem can be formulated within the matrix completion framework  @xcite @xcite .",
    "that is , the @xmath0-th entry of the observed data matrix @xmath1 is a 1-bit code implying that the @xmath2-th user trusts the @xmath3-th user if @xmath4 . here",
    ", @xmath5 denotes the number of users .",
    "however , what we observe is only a small fraction of the entries , whose values are zero .",
    "and our goal is to estimate the missing entries according to the 1-bit measurements in @xmath6 .",
    "note that the problem is ill - posed if no assumption is imposed on the structure of the data . to solve the problem ,",
    "a number of methods are proposed .",
    "generally , existing social trust prediction methods fall into three categories .",
    "the first category is based on similarity measures or the structural context similarity  @xcite @xcite @xcite @xcite , motivated by the intuition that an individual tends to trust their neighbors , or the ones with similar trusted people .",
    "the second is based on low rank matrix completion  @xcite @xcite @xcite , which assumes that the underlying data matrix is low - rank or can be approximate by a low - rank matrix .",
    "the third one models the problem as a binary classification one and utilizes techniques such as logistic regression @xcite .",
    "* challenges .",
    "* however , there are two issues emerging in social trust prediction which are not well characterized by the algorithms in previous works .",
    "first , the value of the observed entry is either 1 or @xmath7 , which is analogous to the binary classification problem .",
    "but in our problem , we are handling much more complex matrix data .",
    "fortunately ,  @xcite presented a maximum margin matrix factorization framework that unifies the binary problem for vector case and matrix case .",
    "the key idea in their work is a low - norm matrix factorization , which will also be utilized in this paper .",
    "second , the locations of the entries are sampled non - uniformly , which gaps the theory and practice for a lot of matrix completion algorithms . to tackle this challenge , we suggest using the max - norm as a convex surrogate for the rank function , which is shown to be superior to the well - known nuclear norm when addressing the non - uniform data  @xcite .    * our contributions * are two - folds : 1 ) to the best of our knowledge , we are the first to address the social trust prediction problem by utilizing a max - norm constrained formulation .",
    "2 ) although a max - norm constrained problem can be solved by sdp solvers and an accurate enough solution can be achieved , we here utilize a projected gradient algorithm that is scalable to large scale datasets .",
    "we empirically show the improvement of our formulation for the non - uniform 1-bit benchmarks compared to state - of - the - art solvers .",
    "social interaction is investigated intensively in the last decades .",
    "the social interaction indicates the friendship , support , enemy or disapproval as shown in figure [ trusg ] .",
    "online users rely on the trustworthiness information to filter information , establish collaboration or build social bonds .",
    "social networks rely on the trust information to make recommendation , attract users from other circles , or influencing public opinions .",
    "thus , the exploration of social trust has a wide range of applications , and has emerged as an important topic in social network research .",
    "a number of methods are proposed .",
    "one kind of methods are based on the similarity measurement .",
    "specifically , jaccard s coefficient is commonly used to measure the probability two items that have a relationship .",
    "inspired by the metric , jeh and widom  @xcite proposed a domain - independent similarity measurement , simrank .",
    "@xcite directly defined a score to verify the correlation between common neighbors .",
    "some methods are based on relational data modeling , structural proximity measures and stochastic relational model @xcite @xcite @xcite .",
    "the above mentioned methods are mainly derived from the solutions of link prediction .",
    "the link prediction is oriented to network - level prediction , whereas social trust prediction focuses on person - level .",
    "another class of methods are derived from the collaborative filtering methods , such as clustering techniques @xcite , model - based methods @xcite , and the matrix factorization models @xcite @xcite . however , the data matrix of trust has some structure properties different from the user - item matrix , such as transitivity .",
    "meanwhile , the social trust in reality is extremely sparse .",
    "for instance , facebook has hundreds of millions of users , but most of them have less than 1,000 friends . besides",
    ", the people with similar personality tend to behave similarly . to sum up",
    ", the data matrix of social trust has both sparse and low - rank structure .",
    "thus , the social trust prediction problem is especially suitable for the matrix completion model .",
    "that is the focus of our paper .",
    "the problem of matrix completion is to recover a low - rank matrix from a subset of entries @xcite , which is given by : @xmath8 where @xmath9 is the data matrix , @xmath10 is the recovered matrix , and @xmath11 is the index set of observed entries .",
    "th optimization problem   is not only np - hard , but requires double exponential time complexity with the number of samples  @xcite . to solve the above problem ,",
    "one alternative is to use nuclear norm as a relaxation to the rank function : @xmath12 where @xmath13 denotes the sum of singular values of matrix @xmath10 .",
    "@xcite developed a first - order procedure to solve the convex problem ( [ tracen ] ) , namely singular value thresholding ( svt ) .",
    "@xcite minimized the rank minimization by the singular value projection ( svp ) algorithm .",
    "@xcite solved the problem by first trimming each row and column with too few entries , then compute the truncated svd of the trimmed matrix . under certain conditions , it showed accurate recovery on the order of @xmath14 samples ( @xmath5 is the number of samples , @xmath15 is the rank of recovered matrix ) . with the rapid development of matrix completion problem , some more efficient methods have been proposed @xcite@xcite@xcite@xcite .",
    "however , all the methods mentioned above use the nuclear norm as the surrogate to the rank , whose exact recovery can be guaranteed only when the data are sampled uniformly , which is not practical in real world applications . on the other hand , recent empirical on max - norm  @xcite",
    "shows promising results for non - uniform data if one utilize the max - norm as a surrogate  @xcite .",
    "notably , for some specific problems , such as collaborative filtering ,  @xcite proved that the generalization error bound for max - norm is better than the nuclear norm .",
    "more recently ,  @xcite reported encouraging results on the subspace recovery task ( which is closely relevant to matrix completion ) . since the social trust data is non - uniformly sampled , we believe that a max - norm regularized formulation can better handle the challenge than the nuclear norm .",
    "our formulation is also inspired by a recent theoretical study on matrix completion with 1-bit measurement  @xcite , which established a minimax lower bound on the general sampling model and derived the optimal convergence rate in terms of frobenius norm loss .",
    "furthermore , there are several practical algorithms to solve max - norm regularized or max - norm constrained problems , see  @xcite and  @xcite for example .",
    "after review of related work in section  [ relat ] , we introduce the notations and formulate the problem in section  [ sec : notation ] .",
    "then we give algorithm to solve the max - norm constrained 1-bit matrix completion ( mmc ) problem in section  [ sec : alg ] .",
    "meanwhile , we also provide an equivalent sdp formulation for the mmc , which can be accurately solved at the expense of efficiency .",
    "then we report the empirical study on two benchmark datasets in section  [ sec : exp ] .",
    "section  [ sec : conclusion ] concludes this paper and discusses possible future work .",
    "in this section , we introduce the notations that will be used in this paper . capital letters such as @xmath16 are used for matrices and lowercase bold letters such as @xmath17 denotes vectors .",
    "the @xmath2-th row and @xmath3-th column of a matrix @xmath16 is denoted by @xmath18 and @xmath19 respectively , and the @xmath0-th entry is denoted by @xmath20 . for a vector @xmath17 , we use @xmath21 to denote its @xmath2-th element .",
    "we denote the @xmath22 norm of a vector @xmath17 by @xmath23 . for a matrix @xmath24",
    ", we denote the frobenius norm by @xmath25 and @xmath26 denotes the maximum @xmath22 row norm of @xmath16 , _",
    "@xmath27 we further define the _ max - norm _ of @xmath16 @xcite , @xmath28 where we enumerate all possible factorizations to obtain the minimum .",
    "* intuition on max - norm .",
    "* at a first sight , the max - norm is hard to understand .",
    "we simply explain why it is a tighter approximation to the rank function than the nuclear norm .",
    "again , we write the nuclear norm of @xmath16 as a factorization form  @xcite : @xmath29 note that the frobenius norm is the sum of the square of the @xmath22 row norm .",
    "thus , a nuclear norm regularizer actually constrains the average of the @xmath22 row norm , while the max - norm constrains the maximum of the @xmath22 row norm !",
    "given the observed data @xmath9 , we are interested in approximating @xmath6 with a low - rank matrix @xmath10 , which can be formulated by , @xmath30 where @xmath11 is an index set of observed entries and @xmath15 is some expected rank .",
    "@xmath31 is a projection operator on a matrix @xmath16 such that @xmath32 if @xmath33 and zero otherwise .",
    "however , it is usually intractable to optimize the above program as the rank function is non - convex and non - continuous  @xcite .",
    "one common approach is to use the nuclear norm as a convex surrogate to the rank function .",
    "however , it is well known that the nuclear norm can not well handle the non - uniform data . motivated by the recent progress in max - norm  @xcite",
    ", we use the max - norm as an alternative convex relaxation , which gives the following formulation : @xmath34 where @xmath35 is some tunable parameter .",
    "the max - norm is convex and moreover , it can be solved by any sdp solver .",
    "formally , we have the following lemma :    [ lem : max ] for any matrix @xmath36 and @xmath37 , @xmath38 if and only if there exist @xmath39 and @xmath40 , such that @xmath41 is semi - definite positive and each diagonal element in @xmath42 and @xmath43 is upper bounded by @xmath35 .    with lemma  [ lem : max ] on hand",
    ", one can formulate problem  [ eq : main prob ] as an sdp : @xmath44,\\ j \\in [ n],\\\\ & \\begin{bmatrix } a & x\\\\ x{^\\top } & b \\end{bmatrix } \\succeq 0 .",
    "\\end{split}\\ ] ] and this program can be solved by any sdp solver to obtain accurate enough solution",
    ".    however , sdp solvers are not scalable to large matrices .",
    "thus , in this paper , we apply a projected gradient method to solve problem  , which is due to  @xcite .",
    "a key technique is the reformulation of the max - norm  .",
    "assume that the rank of the optimal solution @xmath45 produced by the sdp   is at most @xmath15 .",
    "then we can safely factorize @xmath46 , with @xmath47 and @xmath48 . combining the factorization and the definition ,",
    "we obtain the following equivalent program : @xmath49 note that the gradient of the objective function w.r.t . @xmath50 and",
    "@xmath51 can be easily computed .",
    "that is , @xmath52 here , for simplicity we define @xmath53 the inequality constraints can be addressed by a projection step .",
    "that is , when we have a new iterate @xmath54 at the @xmath55-th iteration , we can check if they violate the constraints . if not",
    ", we can proceed to the next iteration . otherwise , we can scale the rows of @xmath50 and/or @xmath51 by @xmath56 and/or @xmath57 respectively . in this way",
    ", we have the projection operator : @xmath58 if we further pick the step size @xmath59 via the armijo rule  @xcite , it can be shown that the sequence of @xmath54 will converge to a stationary point  @xcite .",
    "the algorithm is summarized in algorithm  [ alg : all ] .",
    "@xmath60 ( observed samples ) , parameters @xmath35 , initial solution @xmath61 , maximum iteration @xmath62 . optimal solution @xmath63 . compute the gradient by eq .",
    ": @xmath64 compute the step size @xmath59 according to armijo rule .",
    "compute the new iterate : @xmath65    the benefits of applying the factorization on the max - norm are two - folds : 1 ) the memory cost can be significantly reduced from @xmath66 of sdp to @xmath67 .",
    "2 ) it facilitates the projected gradient algorithm , which is computationally efficient when working on large matrices ( see section  [ sec : exp ] ) .",
    "however , note that problem   is non - convex .",
    "fortunately ,  @xcite proved that as long as we pick a sufficiently large value for @xmath15 , then any local minimum of eq .",
    "is a global optimum . in section  [ sec :",
    "exp ] , we will report the influence of @xmath15 on the performance . actually , in algorithm  [ alg : all ] , the stopping criteria is set to be a maximum iteration .",
    "one may also check if it reaches a local minima as the stopping criteria , as discussed in  @xcite .",
    "the @xmath35 is the only tunable parameter in our algorithm . for our problem ,",
    "note that the data is of 1-bit measurements , _",
    "i.e. _ , @xmath68 for @xmath69 . also note that @xmath70 .",
    "thus , @xmath71 . so we have @xmath72 . however , if we choose a large @xmath35 , the estimation @xmath73 may deviate away from @xmath74 .",
    "we find that @xmath75 lead to satisfactory improvement .",
    "in this section , we empirically evaluate our method for the matrix completion performance .",
    "we will first introduce the used datasets . in the experimental settings , we present the comparative methods and evaluation metrics .",
    "then we report encouraging results on two benchmark datasets .",
    "we also examine the influence of matrix rank @xmath15 .",
    "we conduct the experiments on two benchmark datasets : epinions and slashdot . in these two datasets ,",
    "the users are connected by explicit positive ( trust ) or negative ( distrust ) links ( _ i.e. , _ the 1-bit measurements in @xmath6 ) . the first dataset contains 119,217 nodes ( users ) and 841,000 edges ( links ) , 85.0% of which are positive .",
    "the slashdot dataset contains 82,144 users and 549,202 links , and 77.4% of the edges are labeled as positive .",
    "table [ dataset ] gives a summary description about the subset used in our experiment .",
    "it is clear that the distribution of links are not uniform since each user has his / her individual preference and own friendship network . following  @xcite , we select 2,000 users with the highest degrees from each dataset to form the observation matrix @xmath6 .",
    ".description of 2 datasets [ cols=\"<,>,>\",options=\"header \" , ]     * training and testing . * we randomly split the dataset for training and testing . in particular , the number of observation measurements @xmath11 for training ranges from 10% to 60% , with step size 10% . for each split , we run all the algorithms for 20 trials , with the training data in each trail being randomly sampled .",
    "then , we report the mean and standard deviation of mae and rmse over all 20 trials .",
    "we report detailed results from table  [ maeepin ] to table  [ rmseslash ] . from the results in tables [ maeepin ] and [ rmseepin ]",
    ", we observe that mmc outperforms the other methods in terms of both evaluation metrics on the epinions dataset most of the time .",
    "in particular , when there are few observations 10 % ( which indicates a hard task ) , mmc obtains the rmse of 0.466 , much better than optspace ( 0.530 ) , svp ( 0.610 ) and rrmc ( 0.650 ) . except on the case with 60% observed entries",
    ", optspace obtains the smallest mse with 0.197 , but our algorithm is comparative with 0.206 . in a nut of shell , the gap between mmc and the baselines becomes larger as the fraction of observed entries decreases .",
    "similarly , our method achieves the least mae and rmse on the slashdot dataset ( see table [ maeslash ] and [ rmseslash ] ) .",
    "for instance with 30 % observed entries , mmc obtains the mae with less than 0.4 , much better than the comparative methods , such as svt ( 0.513 ) , optspace ( 0.427 ) , svp ( 0.501 ) and rrmc ( 0.686 ) . in terms of rmse , in the case of 20% observed entries , the rmse values of other methods are all above 0.7 while our method reaches 0.679 . in sum , our method is superior than the comparative methods on two real - life datasets in terms of mse and rmse most of the time .     on the epinions dataset.,title=\"fig:\",scaledwidth=45.0% ]",
    "since we have studied the effectiveness of our method , here we examine the computational efficiency in table  [ time ] , which is important for practical applications . to test the time complexity of the methods , we report the averaged time cost on the epinions dataset with 10% observed entries and slashdot with 20% observed entries . to illustrate the trade - off between accuracy and efficiency",
    ", we also report the mae and rmse . as we see , svp is the most efficient method , whose running time is 0.84 seconds on epinions while ours is 1.92 seconds .",
    "on slashdot , it also achieves the best performance in terms of efficiency",
    ". however , our method enjoys a significant improvement of mae and rmse compared to all baselines . also , our algorithm is orders of magnitude faster than other three baselines",
    "this implies that mmc favors a good trade - off between the accuracy and efficiency .",
    "the non - convex reformulation   requires an explicit rank estimation @xmath15 on the true matrix . in this section",
    ", we investigate the influence of @xmath15 on the epinions dataset as an example .",
    "the rank @xmath15 is chosen from [ 1 , 5 , 50 , 100 , 300 , 500 ] and the results are plot in figure  [ figrank ] .",
    "we observe that the rank has little influence on the performance .",
    "this is possibly because that the actual data has a low - rank structure ( close to rank one ) .",
    "and from  @xcite , we know that if @xmath15 is larger than the actual rank , any local minimum of eq .   is also a global optima .",
    "in this paper , we formulated the social trust prediction in the matrix completion framework .",
    "in particular , due to the special structure of the social trust problem , _",
    "i.e. , _ the measurements are 1-bit and the observed entries are non - uniformly sampled , we presented a max - norm constrained 1-bit matrix completion ( mmc ) algorithm .",
    "since sdp solvers are not scalable to large scale matrices , we utilized a non - convex reformulation of the max - norm , which facilitates an efficient projected gradient decent algorithm .",
    "we empirically examined our algorithm on two benchmark datasets . compared to other state - of - the - art matrix completion formulations ,",
    "mmc consistently outperformed them , which meets with recently developed theories on max - norm .",
    "we also studied the trade - off between the accuracy and efficiency and observed that mmc achieved superior accuracy while keeping comparable computational efficiency .",
    "the max - norm has been studied for several years and in many applications , such as collaborative filtering , clustering , subspace recovery .",
    "it is empirically and theoretically shown to be superior than the popular nuclear norm .",
    "this work investigates the power of max - norm for social trust problem and demonstrates encouraging results .",
    "it is interesting and promising to apply max - norm as a convex surrogate to other practical problems such as face recognition , subspace clustering etc ."
  ],
  "abstract_text": [
    "<S> social trust prediction addresses the significant problem of exploring interactions among users in social networks . </S>",
    "<S> naturally , this problem can be formulated in the matrix completion framework , with each entry indicating the trustness or distrustness . however , there are two challenges for the social trust problem : 1 ) the observed data are with sign ( 1-bit ) measurements ; 2 ) they are typically sampled non - uniformly . </S>",
    "<S> most of the previous matrix completion methods do not well handle the two issues . </S>",
    "<S> motivated by the recent progress of max - norm , we propose to solve the problem with a 1-bit max - norm constrained formulation . </S>",
    "<S> since max - norm is not easy to optimize , we utilize a reformulation of max - norm which facilitates an efficient projected gradient decent algorithm . </S>",
    "<S> we demonstrate the superiority of our formulation on two benchmark datasets . </S>"
  ]
}