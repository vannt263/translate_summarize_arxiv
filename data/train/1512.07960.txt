{
  "article_text": [
    "histogram , a non - parametric density estimator , has been used extensively for analyzing the probability density function of continuous variables @xcite .",
    "histograms are so flexible that they can model various properties of the underlying density like multi - modality , although they usually demand a large number of samples to obtain a good estimate .",
    "thus the histogram method can not be applied directly to a sparse data set , or a collection of units with a small data set . due to the improvement of technology ,",
    "however , it has recently become more important to analyze such diverse data of continuous variables as purchase timing @xcite , period of word appearance @xcite , check - in location @xcite and neuronal spike time @xcite , which could be sparse in many cases .    in this paper , by employing the probabilistic topic model called latent dirichlet allocation @xcite , we propose a novel bayesian approach to estimating probability density functions .",
    "the proposed method estimates a unit s density function as a mixture of basis histograms , in which the unit s density function is characterized by a small number of mixture weights , alleviating the sparsity problem in the conventional histogram method .",
    "furthermore , the model optimizes the bin width , as well as the heights , at the level of individual bases .",
    "thus the model can implement a variable - width bin histogram as a mixture of regularly - binned histograms of different bin widths .",
    "we show that the estimation procedure in the proposed model can be performed by using the fast and easy - to - implement collapsed gibbs sampling @xcite .",
    "we apply the proposed method to synthetic data , clarifying that it performs well .",
    "= 2.20 mm    ll symbol & definition    ' '' ''     + @xmath0 & number of units    ' '' ''     + @xmath1 & number of variables in unit @xmath2    ' '' ''     + @xmath3 & total number of variables    ' '' ''     + @xmath4 & number of basis histograms    ' '' ''     + @xmath2 & @xmath2th unit ,  @xmath5    ' '' ''     + @xmath6 & @xmath7th variable in collection    ' '' ''     + @xmath8 & unit which generated @xmath6    ' '' ''     + @xmath9 & latent variable of @xmath6 ,  @xmath10    ' '' ''     +    = 1.0 mm    ll &    ' '' ''     + @xmath11 & half - open range of variable    ' '' ''     + @xmath12 & lower boundary of @xmath13th bin    ' '' ''     + @xmath14 & number of bins in @xmath15th histogram    ' '' ''     + @xmath16 & weight of @xmath15th histogram on unit @xmath2    ' '' ''     + @xmath17 & probability mass of @xmath13th bin    ' '' ''     + @xmath18 & in @xmath15th histogram ,  @xmath19    ' '' ''     + @xmath20 & index of bin within which @xmath6 falls    ' '' ''     + @xmath18 & under @xmath15th bin number , @xmath21    ' '' ''     +    suppose that we have a collection of @xmath0 units , each of which consists of @xmath1 continuous variables generated by each unit @xmath2 ( @xmath22 ) . for convenience ,",
    "we number all the variables in the collection from @xmath23 to @xmath24 ( in an arbitrary order ) , and define a set of collections , @xmath25 and @xmath26 , where @xmath6 and @xmath8 are the @xmath7th variable and the unit which generated it , respectively .",
    "the notation is summarized in table [ tb1 ] .    in our proposed model",
    ", it is assumed that each of the continuous variable , @xmath6 , be generated from a mixture of histograms .",
    "in fact , as a generative process , a histogram can be described as a piecewise - constant probability density function @xcite , which is a key point of our model construction .",
    "we first provide a description of the piecewise - constant distribution .       from histogram",
    "can be implemented by the two steps : draw a bin index @xmath27 ( @xmath28 ) from a multinomial distribution ; and draw @xmath6 from the uniform distribution defined over the corresponding bin range @xmath29 .",
    "( b ) graphical model representation of histlda , a model which includes a histogram.,title=\"fig : \" ] +    histogram method describes an underlying density function by ; ( i ) discretizing a half - open range of variable , @xmath30 , into @xmath31 contiguous intervals ( bins ) of equal width , ( @xmath32)@xmath33 ; and ( ii ) assigning a constant probability density , @xmath34 , to each bin region , @xmath35 , for @xmath36 . here",
    "the lower boundary of bin , @xmath12 , is given by , @xmath37 .",
    "in it , a continuous variable @xmath38 follows a piecewise - constant distribution , @xmath39 or equivalently , @xmath40 where @xmath41 is the probability mass of the @xmath13th bin region @xmath35 , and @xmath42 represents a discretization operator that transforms a continuous variable @xmath38 into the corresponding bin index , defined by @xmath43.\\ ] ]    it should be emphasized here that eqs .",
    "( [ eq_hist]-[eq_hist2 ] ) suggest that the observation process @xmath44 can be decomposed into the following two processes ( see fig .",
    "[ fig_histlda]a ) : ( i ) draw a bin index @xmath45 from a multinomial distribution with parameter @xmath46 ; ( ii ) draw a continuous variable from an uniform distribution defined over the corresponding bin region , @xmath47 . without the second process",
    ", a mixture model of @xmath44 reduces to the original latent dirichlet allocation ( lda ) @xcite , in which observed variables are discrete . in the next section",
    ", we construct a mixture of histograms by incorporating a uniform process into the original lda .",
    "we call the proposed model the _ histogram latent dirichlet allocation _ ( histlda ) .      histlda estimates unit @xmath2 s probability density function as a mixture of basis histograms as follows : @xmath48 where @xmath4 is the number of mixture components , @xmath49 is a latent variable indicating the component from which the variable is drawn , @xmath50 is the set of bin numbers , @xmath51 ( @xmath52 ) is the probability masses of the @xmath15th histogram , @xmath53 is its set , and @xmath54 ( @xmath55 ) is the weights of the @xmath4 components on unit @xmath2 .",
    "each of the basis histograms , @xmath56 , is described by eq .",
    "( [ eq_hist ] ) .",
    "note that the set of basis histograms is shared by all the units , and heterogeneity across units is represented only through the weight @xmath57 .    in accordance with lda @xcite",
    ", our histlda assumes the following generative process for a set of collections , @xmath25 and @xmath26 :    1 .   for each basis",
    "histogram @xmath58 : * draw number of bins @xmath59 ( @xmath60 ) * draw probability mass @xmath61 ( @xmath62 , @xmath21 ) 2 .   for each unit @xmath63 :",
    "* draw basis weight @xmath64 ( @xmath65 , @xmath4 ) 3 .   for each observation in collection @xmath66 :",
    "* draw basis @xmath67 multinomial ( @xmath68 , @xmath4 ) * draw bin index @xmath69 multinomial ( @xmath70 ) * draw variable @xmath71 uniform ( @xmath72 ) ,    where @xmath73 is the uniform distribution defined over an interval @xmath74 $ ] , @xmath75 is the symmetric dirichlet distribution of @xmath76 random variables with parameter @xmath77 , @xmath78 is the multinomial distribution of @xmath76 categories with equal choice probability @xmath77 , @xmath65 and @xmath62 are dirichlet parameters , and @xmath79 is the maximum number of bins to be considered .",
    "the histlda is an extension of the original lda in that ( i ) the number of bins ( vocabulary ) , as a random variable , is not necessarily the same among the bases ( topics ) , and ( ii ) observation is not a discrete bin index ( word ) drawn from a multinomial distribution , but a continuous variable further drawn from a uniform distribution defined over the corresponding bin s interval ( see fig .  [ fig_histlda ] ) .",
    "also , it is worth noting that the histlda is a novel extension of knuth s bayesian binning model @xcite into hierarchical structure .    because being conjugate to dirichlet priors , the multinomial parameters , @xmath57 and @xmath80 , can be marginalized out from the generative model , leading to the joint distribution of data @xmath81 , its latent basis @xmath82 , and the set of bin numbers @xmath83 , as follows : @xmath84 where @xmath85 is the gamma function , @xmath86 is the number of times a variable of unit @xmath2 has been assigned to basis @xmath15 , @xmath87 is the number of times that a variable assigned to the @xmath15th basis histogram is addressed to the @xmath13th bin of the histogram , and @xmath88 .",
    ", according to the posterior , each of all the variables is allocated to one of the bases , colored in red , yellow and blue , respectively . given the data set allocated to each basis , the bin number is optimized at the level of individual bases based on the posterior ( binning ) , and again the variables are re - allocated under the new bin numbers .",
    "the optimization is performed accurately due to the enough size of the data .",
    "the procedure is repeated before the joint posterior @xmath89 is converged . in collapsed gibbs sampling ,",
    "the heights of each histogram are not estimated explicitly , making the estimation easier to implement .",
    "( c ) for a unit , each of the weights of the bases , @xmath57 , are estimated by counting the proportion of the unit s data allocated to each basis , represented by a pie chart .",
    "finally , the density function of each unit is estimated as a mixture of the basis histograms.,title=\"fig : \" ] +    given a collection of observed variables , @xmath90 , we can estimate latent basis and number of bins , @xmath91 and @xmath83 , based on the posterior distribution , @xmath92 . in practice , by using the simple and easy - to - implement collapsed gibbs sampling @xcite , we obtain samples of @xmath91 and @xmath83 following @xmath93@xmath94@xmath95 , from which the weight @xmath57 and the probability mass @xmath80 , as well as the hyperparameters @xmath65 and @xmath62 , are estimated efficiently .",
    "given a set of bin numbers @xmath83 and the current state of all but one latent variable @xmath9 , denoted by @xmath96 , the basis assignment to @xmath7th variable is sampled from the following multinomial distribution : @xmath97 and given @xmath91 and all but one bin number @xmath21 , denoted by @xmath98 , the @xmath15th bin number @xmath21 is sampled from the following multinomial distribution : @xmath99 where @xmath100 represents the count that does not include the current assignment of @xmath9 .",
    "here @xmath101 represents the @xmath15th histogram s bin index within which @xmath6 falls , defined as @xmath102.\\ ] ] eqs .",
    "( [ eq_gibbs1 ] ) and ( [ eq_gibbs2 ] ) are easily derived from eq .",
    "( [ eq_joint ] ) ( not shown here ) .    in each sampling of @xmath91 and @xmath83 ,",
    "the hyperparameters of the dirichlet priors , @xmath65 and @xmath62 , can be updated by using the fixed - point iteration method described in @xcite as follows : @xmath103 where @xmath104 is the digamma function defined by the derivative of @xmath105 . for details , see appendix [ ap1 ] .    in practice , we set the initial hyperparameters as @xmath106 = @xmath107 = 0.5 ( jeffreys non - informative prior @xcite ) , and draw the initial @xmath108 from @xmath109 . the algorithm of the collapsed gibbs sampling in histlda is summarized in algorithm 1 ( see also fig .  [ fig_schema]b ) .      by repeating the collapsed gibbs sampling ( [ eq_gibbs1]-[eq_hyper_it ] ) before the convergence",
    "is achieved , we first estimate the hyperparameters , @xmath110 and @xmath111 , as the last updated values . at the same time",
    ", we also estimate the set of bin numbers , @xmath112 , as the last updated values .",
    "next , given @xmath110 , @xmath111 and @xmath113 , we further draw @xmath114 samples of basis assignment , [ @xmath115 , according to eq .",
    "( [ eq_gibbs1 ] ) , and obtain the posterior mean estimate of the weight @xmath116 and probability mass @xmath117 as follows : @xmath118 where @xmath119 , @xmath120 and @xmath121 represent the sufficient statistics in the @xmath122th sample , @xmath123 . in the following experiment ,",
    "@xmath114 was set at @xmath124 .    in figure",
    "[ fig_schema ] , we give an intuitive explanation of the estimation procedure in histlda .",
    "[ tb2 ]    l collapsed gibbs sampling in histlda + set @xmath4 and @xmath125 , and initialize @xmath126 , and @xmath127 for @xmath128 .",
    "+ draw @xmath91 from dirichlet ( @xmath65 , @xmath4 ) .",
    "+ count sufficient statistics @xmath129 , @xmath130 and @xmath131 for @xmath132 , @xmath128 . +",
    "* repeat * + @xmath133 to @xmath4 * do * + draw @xmath21 from eq .",
    "( [ eq_gibbs2 ] ) . + update @xmath129 for @xmath132 , under a new @xmath21 .",
    "+   + @xmath134 to @xmath3 * do * + set @xmath135 ,  @xmath136 ,  @xmath137 .",
    "+ draw @xmath9 from eq .",
    "( [ eq_gibbs1 ] ) . +",
    "set @xmath138 ,  @xmath139 ,  @xmath140 .",
    "+   + update @xmath65 and @xmath62 based on eq .",
    "( [ eq_hyper_it ] ) . + * until * posterior @xmath141 is converged .",
    "the error bars represent standard deviations of ise when the density estimation was performed three times using the three sets of data generated .",
    "( b ) three units examples of estimated probability density functions for @xmath142 .",
    "the solid line represents the density function estimated by each method , and the dashed line represents the true density function .",
    ", title=\"fig : \" ] +    to confirm that the histlda works well on a sparse data set , that is , a collection of units consisting of a small size of variables , we evaluated the performance of histlda together with the reference methods on synthetic data .",
    "as the reference methods , we adopted two histogram methods , the bayesian binning method proposed by knuth @xcite ( knuth ) and penalized - maximum likelihood method by birg and rozenholc @xcite ( br ) , and a parametric method , gaussian mixture model ( gmm ) .",
    "the three methods estimate a unit s probability density function based on its own observed variables .",
    "we made synthetic data in the scenario that a unit generated continuous variables from a complex probability density function comprising the following three different types of distributions : the normal distribution with mean @xmath23 and variance @xmath143 , the exponential distribution with rate parameter @xmath144 , and the uniform distribution defined over the interval @xmath145 $ ] . here each unit was characterized by the mixing proportions , which were sampled from a uniform or flat dirichlet distribution with respect to each unit . generating a collection comprising @xmath146 units , each of which had @xmath147 variables , we estimated the units underlying probability density functions from the data , and evaluated the goodness of the estimation in terms of the average integrated squared error ( ise ) of probability density function : @xmath148 where the range of variable , @xmath149 , was set at @xmath150 , and @xmath151 and @xmath152 represent the intended and the estimated probability density functions , respectively . in the experiment , the number of mixture components was set at three for both histlda and gmm .",
    "the data size per unit , @xmath147 , was specified as @xmath153 , @xmath124 , @xmath154 , @xmath155 , @xmath156 , @xmath157 .",
    "figure [ fig_ise]a compares histlda s ise against the results achieved by the reference methods , demonstrating that histlda performed better than the other methods for all the data size per unit , @xmath147 . the comparison between histlda and the other histogram methods ( knuth and br ) found that histlda performed relatively much better even in the small @xmath147 .",
    "this suggests that our histlda copes with the sparsity problem in the usual histogram methods .",
    "also , figure [ fig_ise]a shows that the histogram methods ( histlda , knuth and br ) performed better when the data size was larger , while the performance of gmm was not improved significantly .",
    "parametric approaches like gmm , which work robustly in sparse data , usually perform poorly under the wrong assumption of underlying distribution . in the experiment ,",
    "the underlying density function of each unit was far from gaussian ( see fig .  [",
    "fig_ise]b ) .    figure  [ fig_ise]b shows three units examples of estimated probability density functions for @xmath147 = @xmath124 .",
    "each unit had a complex and unit - specific distribution , but histlda obtained a good estimate of each distribution by adopting small bin widths ( large number of bins ) . generally , the bin width of histogram is estimated to be smaller in a larger data set @xcite , of which situation is realized in histlda by allocating the whole data of all the units into a small number of basis histograms ( see fig .",
    "[ fig_schema ] ) . furthermore in fig .",
    "[ fig_ise]b , histlda seems to have adjusted bin widths depending on the location : large bin width was used around 0 , and small one being used around 1 .",
    "histlda implements a variable - width bin histogram by way of a mixture of regular histograms with different bin widths .",
    "figure  [ fig_estimation ] displays how the bin number ( bin width ) for each basis histogram was optimized in the collapsed gibbs sampling .     and hyperparameter ( @xmath65 , @xmath62 ) versus iteration number .",
    "( b ) the sampled bin number and the estimated basis histograms at the 4th , 10th , and 500th iteration . in each figure , the three distributions depicted by the dashed lines represent the underlying normal , exponential , and uniform distributions.,title=\"fig : \" ] +",
    "we have proposed a novel histogram density estimation that overcomes the sparsity of data , by incorporating the latent dirichlet allocation ( lda ) into the histogram method .",
    "the proposed method estimates a density function as a mixture of histograms , of which bin numbers or bin widths are optimized together with their heights , at the level of individual histograms . by way of a mixture of regularly - binned histograms of different bin widths",
    ", it can implement a variable - width bin histogram .",
    "as with the lda , all the estimation procedure is performed by using the fast and easy - to - implement collapsed gibbs sampling .",
    "we assessed the goodness of the proposed method by examining synthetic data , and demonstrated that it performed well when data sets were sparse .",
    "based on the empirical bayes method , the hyperparameters in the dirichlet distributions , @xmath65 and @xmath62 , can be estimated by maximizing the marginal likelihood , @xmath158 the maximization is performed by using the monte carlo em algorithm , leading to the following update rule , @xmath159,\\ ] ] where @xmath160 and @xmath161 are the updated values of hyperparameters , and @xmath162 $ ] represents the expectation with respect to the posterior distribution of @xmath91 and @xmath83 under the previous estimate of the hyperparameters , ( @xmath163 ) .",
    "this time , we employ the stochastic em , a variant of monte carlo em algorithm , in which the expectation is replaced by a sample taken from the posterior distribution ( [ eq_gibbs1]-[eq_gibbs2 ] ) , as @xmath164 where @xmath165 and @xmath166 are the values of @xmath91 and @xmath83 sampled by the collapsed gibbs sampling with the previous hyperparameters , ( @xmath163 ) . although having no analytical forms , eq .",
    "( [ eq_maxmax ] ) can be computed by iterating the following fixed - point equation @xcite : @xmath167 where @xmath104 is the digamma function defined by the derivative of @xmath105 , and @xmath168 , @xmath169 and @xmath170 represent the realization of @xmath86 , @xmath87 and @xmath131 in the collapsed gibbs sampling with the previous hyperparameters , ( @xmath163 ) , respectively ."
  ],
  "abstract_text": [
    "<S> the histogram method is a powerful non - parametric approach for estimating the probability density function of a continuous variable . </S>",
    "<S> but the construction of a histogram , compared to the parametric approaches , demands a large number of observations to capture the underlying density function . </S>",
    "<S> thus it is not suitable for analyzing a sparse data set , a collection of units with a small size of data . in this paper , by employing the probabilistic topic model , we develop a novel bayesian approach to alleviating the sparsity problem in the conventional histogram estimation . </S>",
    "<S> our method estimates a unit s density function as a mixture of basis histograms , in which the number of bins for each basis , as well as their heights , is determined automatically . </S>",
    "<S> the estimation procedure is performed by using the fast and easy - to - implement collapsed gibbs sampling . </S>",
    "<S> we apply the proposed method to synthetic data , showing that it performs well . </S>"
  ]
}