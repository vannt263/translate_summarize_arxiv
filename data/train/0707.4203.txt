{
  "article_text": [
    "sparse recovery problems arise in many applications ranging from medical imaging to error correction .",
    "suppose @xmath1 is an unknown @xmath2-dimensional signal with at most @xmath3 nonzero components : @xmath4 we call such signals @xmath5-sparse .",
    "suppose we are able to collect @xmath6 nonadaptive linear measurements of @xmath1 , and wish to efficiently recover @xmath1 from these .",
    "the measurements are given as the vector @xmath7 , where @xmath8 is some @xmath9 measurement matrix .",
    "as discussed in @xcite , exact recovery is possible with just @xmath10 .",
    "however , recovery using only this property is not numerically feasible ; the sparse recovery problem in general is known to be np - hard .",
    "nevertheless , massive recent work in the emerging area of compressed sensing demonstrated that for several natural classes of measurement matrices @xmath8 , the signal @xmath1 can be exactly reconstructed from its measurements @xmath11 with @xmath12 in other words , the number of measurements @xmath6 should be almost linear in the sparsity @xmath5 . survey @xcite contains some of these results ; the compressed sensing webpage @xcite documents progress in this area .",
    "the two major algorithmic approaches to sparse recovery are methods based on @xmath0-minimization and iterative methods ( matching pursuits ) .",
    "we now briefly describe these methods .",
    "then we propose a new iterative method that has advantages of both approaches .",
    "this approach to sparse recovery has been advocated over decades by donoho and his collaborators ( see e.g. @xcite ) .",
    "the sparse recovery problem can be stated as the problem of finding the sparsest signal @xmath1 with the given measurements @xmath11 : @xmath13 where @xmath14 .",
    "donoho and his associates advocated the principle that for some measurement matrices @xmath8 , the highly non - convex combinatorial optimization problem @xmath15 should be equivalent to its convex relaxation @xmath16 where @xmath17 denotes the @xmath18-norm of the vector @xmath19 . the convex problem @xmath20 can be solved using methods of convex and even linear programming .",
    "the recent progress in the emerging area of compressed sensing pushed forward this program ( see survey @xcite ) . a necessary and sufficient condition of exact sparse recovery is that the map @xmath8 be one - to - one on the set of @xmath5-sparse vectors .",
    "cands and tao @xcite proved that a stronger quantitative version of this condition guarantees the equivalence of the problems @xmath15 and @xmath20 .",
    "a measurement matrix @xmath8 satisfies the _ restricted isometry condition _",
    "( ric ) with parameters @xmath21 for @xmath22 if we have @xmath23    the restricted isometry condition states that every set of @xmath24 columns of @xmath8 forms approximately an orthonormal system .",
    "one can interpret the restricted isometry condition as an abstract version of the uniform uncertainty principle in harmonic analysis ( @xcite , see also discussions in @xcite and @xcite ) .",
    "[ recovery ric ] assume that the measurement matrix @xmath8 satisfies the restricted isometry condition with parameters @xmath25 .",
    "then every @xmath5-sparse vector @xmath26 can be exactly recovered from its measurements @xmath27 as a unique solution to the convex optimization problem @xmath20 .    in a lecture on compressive sampling",
    ", cands sharpened this to work for the restricted isometry condition with parameters @xmath28 .",
    "measurement matrices that satisfy the restricted isometry condition with number of measurements as in include random gaussian , bernoulli and partial fourier matrices .",
    "section  [ s : ensembles ] contains more detailed information .",
    "an alternative approach to sparse recovery is via iterative algorithms , which find the support of the @xmath5-sparse signal @xmath1 progressively .",
    "once @xmath29 is found correctly , it is easy to compute the signal @xmath1 from its measurements @xmath30 as @xmath31 , where @xmath32 denotes the measurement matrix @xmath8 restricted to columns indexed by @xmath33 .",
    "a basic iterative algorithm is orthogonal matching pursuit ( omp ) , popularized and analyzed by gilbert and tropp in @xcite , see @xcite for a more general setting .",
    "omp recovers the support of @xmath1 , one index at a time , in @xmath5 steps . under a hypothetical assumption that @xmath8 is an isometry , i.e. the columns of @xmath8 are orthonormal",
    ", the signal @xmath1 can be exactly recovered from its measurements @xmath30 as @xmath34 .",
    "the problem is that the @xmath9 matrix @xmath8 is never an isometry in the interesting range where the number of measurements @xmath35 is smaller than the ambient dimension @xmath2 . even though the matrix is not an isometry ,",
    "one can still use the notion of coherence in recovery of sparse signals . in that setting ,",
    "greedy algorithms are used with incoherent dictionaries to recover such signals , see @xcite , @xcite , @xcite . in our setting , for random matrices one expects the columns to be approximately orthogonal , and the _ observation vector _",
    "@xmath36 to be a good approximation to the original signal @xmath1 .",
    "the biggest coordinate of the observation vector @xmath37 in magnitude should thus be a nonzero coordinate of the signal @xmath1 .",
    "we thus find one point of the support of @xmath1",
    ". then omp can be described as follows .",
    "first , we initialize the residual @xmath38 . at each iteration",
    ", we compute the observation vector @xmath39 . denoting by @xmath40 the coordinates selected so far ,",
    "we solve a least squares problem and update the residual @xmath41 to remove any contribution of the coordinates in @xmath40 .",
    "omp then iterates this procedure @xmath5 times , and outputs a set @xmath40 of size @xmath5 , which should equal the support of the signal @xmath1 .",
    "tropp and gilbert @xcite analyzed the performance of omp for gaussian measurement matrices @xmath8 ; a similar result holds for general subgaussian matrices .",
    "they proved that , for every fixed @xmath5-sparse @xmath2-dimensional signal @xmath1 , and an @xmath9 random gaussian measurement matrix @xmath8 , omp recovers ( the support of ) @xmath1 from the measurements @xmath30 correctly with high probability , provided the number of measurements is @xmath42 .",
    "the @xmath0-minimization method has _",
    "strongest known guarantees _ of sparse recovery .",
    "once the measurement matrix @xmath8 satisfies the restricted isometry condition , this method works correctly for all sparse signals @xmath1 .",
    "no iterative methods have been known to feature such uniform guarantees , with the exception of chaining pursuit @xcite and the hhs algorithm @xcite which however only work with specifically designed structured measurement matrices .",
    "the restricted isometry condition is a natural abstract deterministic property of a matrix .",
    "although establishing this property is often nontrivial , this task is _ decoupled from the analysis _ of the recovery algorithm .",
    "@xmath0-minimization is based on linear programming , which has its advantages and disadvantages .",
    "one thinks of linear programming as a black box , and any development of fast solvers will reduce the _ running time _ of the sparse recovery method . on the other hand ,",
    "it is not very clear what this running time is , as there is no strongly polynomial time algorithm in linear programming yet .",
    "all known solvers take time polynomial not only in the dimension of the program @xmath2 , but also on certain condition numbers of the program . while for some classes of random matrices the expected running time of linear programming solvers",
    "can be bounded ( see the discussion in @xcite and subsequent work in @xcite ) , estimating condition numbers is hard for specific matrices .",
    "for example , there is no result yet showing that the restricted isometry condition implies that the condition numbers of the corresponding linear program is polynomial in @xmath2 .",
    "orthogonal matching pursuit is quite _ fast _ , both theoretically and experimentally .",
    "it makes @xmath5 iterations , where each iteration amounts to a multiplication by a @xmath43 matrix @xmath44 ( computing the observation vector @xmath37 ) , and solving a least squares problem in dimensions at most @xmath45 ( with matrix @xmath46 ) .",
    "this yields strongly polynomial running time . in practice ,",
    "omp is observed to perform faster and is easier to implement than @xmath0-minimization @xcite . for more details ,",
    "see @xcite .",
    "orthogonal matching pursuit is quite _ transparent _ : at each iteration , it selects a new coordinate from the support of the signal @xmath1 in a very specific and natural way .",
    "in contrast , the known @xmath0-minimization solvers , such as the simplex method and interior point methods , compute a path toward the solution .",
    "however , the geometry of @xmath0 is clear , whereas the analysis of greedy algorithms can be difficult simply because they are iterative .    on the other hand , orthogonal matching pursuit has _",
    "weaker guarantees _ of exact recovery . unlike @xmath0-minimization",
    ", the guarantees of omp are non - uniform : for each _ fixed _ sparse signal @xmath1 and not for _ all _ signals , the algorithm performs correctly with high probability .",
    "rauhut has shown that uniform guarantees for omp are impossible for natural random measurement matrices @xcite .",
    "moreover , omp s condition on measurement matrices given in @xcite is _ more restrictive _ than the restricted isometry condition . in particular",
    ", it is not known whether omp succeeds in the important class of partial fourier measurement matrices .",
    "these open problems about omp , first stated in @xcite and often reverberated in the compressed sensing community , motivated the present paper .",
    "we essentially settle them in positive by the following modification of orthogonal matching pursuit .      this new algorithm for sparse recovery will perform correctly for all measurement matrices @xmath8 satisfying the restricted isometry condition , and for all sparse signals .",
    "when we are trying to recover the signal @xmath1 from its measurements @xmath30 , we can use the observation vector @xmath36 as a good _ local approximation _ to the signal @xmath1 .",
    "namely , the observation vector @xmath37 encodes correlations of the measurement vector @xmath26 with the columns of @xmath8 . note that @xmath8 is a dictionary , and so since the signal @xmath1 is sparse , @xmath26 has a sparse representation with respect to the dictionary . by the restricted isometry condition , every @xmath5 columns form approximately an orthonormal system .",
    "therefore , every @xmath5 coordinates of the observation vector @xmath37 look like correlations of the measurement vector @xmath26 with the orthonormal basis and therefore are close in the euclidean norm to the corresponding @xmath5 coefficients of @xmath1 .",
    "this is documented in proposition  [ p : cons ] below .",
    "the local approximation property suggests to make use of the @xmath5 biggest coordinates of the observation vector @xmath37 , rather than one biggest coordinate as omp did .",
    "we thus force the selected coordinates to be more regular ( ie .",
    "closer to uniform ) by selecting only the coordinates with comparable sizes . to this end",
    ", a new _ regularization _ step will be needed to ensure that each of these coordinates gets an even share of information .",
    "this leads to the following algorithm for sparse recovery :    regularized orthogonal matching pursuit ( romp )    the identification and regularization steps of romp can be performed efficiently . in particular , the regularization step does _ not _ imply combinatorial complexity , but actually can be done in linear time .",
    "the running time of romp is thus comparable to that of omp in theory , and is often better than omp in practice .",
    "we discuss the runtime in detail in section  [ s : implementation ] .",
    "the main theorem of this paper states that romp yields exact sparse recovery provided that the measurement matrix satisfies the restricted isometry condition .",
    "[ t : main ] assume a measurement matrix @xmath8 satisfies the restricted isometry condition with parameters @xmath47 for @xmath48 .",
    "let @xmath1 be an @xmath5-sparse vector in @xmath49 with measurements @xmath30 .",
    "then romp in at most @xmath5 iterations outputs a set @xmath40 such that @xmath50    this theorem is proved in section [ s : proof ] .",
    "* theorem  [ t : main ] guarantees _ exact sparse recovery_. indeed , it is easy to compute the signal @xmath1 from its measurements @xmath30 and the set @xmath40 given by romp as @xmath51 , where @xmath46 denotes the measurement matrix @xmath8 restricted to columns indexed by @xmath40 .",
    "theorem  [ t : main ] gives _ uniform guarantees _ of sparse recovery .",
    "indeed , once the measurement matrix satisfies a deterministic condition ( ric ) , then our algorithm romp correctly recovers _ every _ sparse vector from its measurements .",
    "uniform guarantees have been shown to be impossible for omp @xcite , and it has been an open problem to find a version of omp with uniform guarantees ( see @xcite ) .",
    "theorem  [ t : main ] says that romp essentially settles this problem .",
    "* the logarithmic factor in @xmath52 may be an artifact of the proof . at this moment",
    ", we do not know how to remove it .",
    "* 4 . * measurement matrices known to satisfy the restricted isometry condition include random _ gaussian , bernoulli and partial fourier matrices _ , with number of measurements @xmath35 almost linear in the sparsity @xmath5 , i.e. as in .",
    "section  [ s : ensembles ] contains detailed information .",
    "it has been unknown whether omp gives sparse recovery for partial fourier measurements ( even with non - uniform guarantees ) .",
    "romp gives sparse recovery for these measurements , and even with uniform guarantees .",
    "the rest of the paper is organized as follows . in section  [",
    "s : ensembles ] we describe known classes of measurement matrices satisfying the restricted isometry condition . in section  [ s : proof ] we give the proof of theorem  [ t : main ] . in section",
    "[ s : implementation ] we discuss implementation , running time , and empirical performance of romp .",
    "we would like to thank the referees for a thorough reading of the manuscript and making useful suggestions which greatly improved the paper .",
    "the only known measurement matrices known to satisfy the restricted isometry condition with number of measurements as in are certain classes of random matrices .",
    "the problem of deterministic constructions is still open .",
    "the known classes include : subgaussian random matrices ( in particular , gaussian and bernoulli ) , and random partial bounded orthogonal matrices ( in particular , partial fourier matrices ) .    throughout the paper ,",
    "@xmath53 denote positive absolute constants unless otherwise specified .    a _ subgaussian random matrix _",
    "@xmath8 is a matrix whose entries are i.i.d .",
    "subgaussian random variables with variance @xmath54 .",
    "a random variable @xmath55 is subgaussian if its tail distribution is dominated by that of the standard gaussian random variable : there are constants @xmath56 such that @xmath57 for all @xmath58 .",
    "examples of subgaussian random variables are : standard gaussian , bernoulli ( uniform @xmath59 ) , and any bounded random variables .    a _ partial bounded orthogonal matrix _ @xmath8 is formed by @xmath35 randomly uniformly chosen rows of an orthogonal @xmath60 matrix @xmath61 , whose entries are bounded by @xmath62 , for some constant @xmath63 .",
    "an example of @xmath61 is the discrete fourier transform matrix .",
    "taking measurements @xmath11 with a partial fourier matrix thus amounts to observing @xmath35 random frequencies of the signal @xmath1 .",
    "the following theorem documents known results on the restricted isometry condition for these classes of random matrices .",
    "[ ric matrices ] consider an @xmath9 measurement matrix @xmath8 , and let @xmath64 , @xmath65 , and @xmath66 .",
    "\\1 . if @xmath8 is a _ subgaussian matrix _ , then with probability @xmath67 the matrix @xmath68 satisfies the restricted isometry condition with parameters @xmath69 provided that @xmath70 2 . if @xmath8 is a _ partial bounded orthogonal matrix _ , then with probability @xmath67 the matrix @xmath71 satisfies the restricted isometry condition with parameters @xmath69 provided that @xmath72 in both cases , the constant @xmath73 depends only on the confidence level @xmath74 and the constants @xmath75 from the definition of the corresponding classes of matrices .",
    "* 1 . * the first part of this theorem is proved in @xcite .",
    "the second part is from @xcite ; a similar estimate with somewhat worse exponents in the logarithms was proved in @xcite .",
    "see these results for the exact dependence of @xmath73 on the confidence level @xmath74 ( although usually @xmath74 would be chosen to be some small constant itself . )    * 2 . * in theorem  [ t : main ] , we needed to use ric for @xmath76 . an immediate consequence of theorem  [ ric matrices ] is that subgaussian matrices satisfy such ric for the number of measurements @xmath77 and partial bounded orthogonal matrices for @xmath78 these numbers of measurements guarantee exact sparse recovery using romp .",
    "we shall prove a stronger version of theorem  [ t : main ] , which states that _ at every iteration _ of romp , at least @xmath79 of the newly selected coordinates are from the support of the signal @xmath1 .",
    "[ t : it ] assume @xmath8 satisfies the restricted isometry condition with parameters @xmath47 for @xmath48 .",
    "let @xmath80 be an @xmath5-sparse vector with measurements @xmath30 .",
    "then at any iteration of romp , after the regularization step , we have @xmath81 , @xmath82 and @xmath83 in other words , at least @xmath79 of the coordinates in the newly selected set @xmath84 belong to the support of @xmath1 .",
    "in particular , at every iteration romp finds at least one new coordinate in the support of the signal @xmath1 . coordinates outside the support",
    "can also be found , but guarantees that the number of such `` false '' coordinates is always smaller than those in the support .",
    "this clearly implies theorem  [ t : main ] .    before proving theorem  [ t : it ] we explain how the restricted isometry condition will be used in our argument .",
    "ric is necessarily a local principle , which concerns not the measurement matrix @xmath8 as a whole , but its submatrices of @xmath5 columns .",
    "all such submatrices @xmath46 , @xmath85 , @xmath86 are almost isometries .",
    "therefore , for every @xmath5-sparse signal @xmath1 , the observation vector @xmath87 approximates @xmath1 locally , when restricted to a set of cardinality @xmath5 .",
    "the following proposition formalizes these local properties of @xmath8 on which our argument is based .",
    "[ p : cons ] assume a measurement matrix @xmath8 satisfies the restricted isometry condition with parameters @xmath47",
    ". then the following holds .    1 .",
    "_ ( local approximation ) _ for every @xmath5-sparse vector @xmath88 and every set @xmath89 , @xmath86 , the observation vector @xmath90 satisfies @xmath91 2 .   _",
    "( spectral norm ) _ for any vector @xmath92 and every set @xmath89 , @xmath93 , we have @xmath94 3 .   _",
    "( almost orthogonality of columns ) _ consider two disjoint sets @xmath95 , @xmath96 .",
    "let @xmath97 denote the orthogonal projections in @xmath98 onto @xmath99 and @xmath100 , respectively .",
    "then @xmath101    part 1 .",
    "let @xmath102 , so that @xmath103 .",
    "let @xmath104 denote the identity operator on @xmath105 . by the restricted isometry condition , @xmath106 since @xmath107",
    ", we have @xmath108 the conclusion of part  1 follows since @xmath109 .    part 2",
    ". denote by @xmath110 the orthogonal projection in @xmath49 onto @xmath111 .",
    "since @xmath93 , the restricted isometry condition yields @xmath112 this yields the inequality in part 2 .",
    "the desired inequality is equivalent to : @xmath113 let @xmath114 so that @xmath115 . for any @xmath116 , there are @xmath117 so that @xmath118 by the restricted isometry condition , @xmath119 by the proof of part 2 above and since @xmath120 , we have @xmath121 this yields @xmath122 which completes the proof .",
    "we are now ready to prove theorem [ t : it ] .",
    "the proof is by induction on the iteration of romp .",
    "the induction claim is that for all previous iterations , the set of newly chosen indices @xmath84 is nonempty , disjoint from the set of previously chosen indices @xmath40 , and holds .",
    "let @xmath40 be the set of previously chosen indices at the start of a given iteration .",
    "the induction claim easily implies that @xmath123 let @xmath84 , @xmath124 , be the sets found by romp in the current iteration . by the definition of the set @xmath84",
    ", it is nonempty .",
    "let @xmath125 be the residual at the start of this iteration .",
    "we shall approximate @xmath126 by a vector in @xmath127 .",
    "that is , we want to approximately realize the residual @xmath126 as measurements of some signal which lives on the still unfound coordinates of the the support of @xmath1 . to that end",
    ", we consider the subspace @xmath128 and its complementary subspaces @xmath129 the restricted isometry condition in the form of part  3 of proposition  [ p : cons ] ensures that @xmath130 and @xmath131 are almost orthogonal .",
    "thus @xmath131 is close to the orthogonal complement of @xmath130 in @xmath132 , @xmath133    [ fig : cap ]     we will also consider the signal we seek to identify at the current iteration , its measurements , and its observation vector : @xmath134    lemma  [ l : uj ] will show that @xmath135 for any small enough subset @xmath136 is small , and lemma  [ c : uj0 ] will show that @xmath137 is not too small .",
    "first , we show that the residual @xmath126 has a simple description :    [ residual ] here and thereafter , let @xmath138 denote the orthogonal projection in @xmath98 onto a linear subspace @xmath139 . then @xmath140    by definition of the residual in the algorithm , @xmath141 . since @xmath142",
    ", we conclude from the orthogonal decomposition @xmath143 that @xmath144 .",
    "thus @xmath145 .    to guarantee a correct identification of @xmath146 , we first state two approximation lemmas that reflect in two different ways the fact that subspaces @xmath131 and @xmath147 are close to each other .",
    "this will allow us to carry over information from @xmath131 to @xmath147 .",
    "[ c : proj ] we have @xmath148    by definition of @xmath130 , we have @xmath149 .",
    "therefore , by lemma  [ residual ] , @xmath150 , and so @xmath151 now we use part 3 of proposition  [ p : cons ] for the sets @xmath40 and @xmath152 whose union has cardinality at most @xmath153 by .",
    "it follows that @xmath154 as desired .",
    "[ l : uj ] consider the observation vectors @xmath155 and @xmath156 . then for any set @xmath157 with @xmath158",
    ", we have @xmath159    since @xmath160 , we have by lemma  [ c : proj ] and the restricted isometry condition that @xmath161 to complete the proof , it remains to apply part 2 of proposition  [ p : cons ] , which yields @xmath162 .",
    "we next show that the energy ( norm ) of @xmath37 when restricted to @xmath124 , and furthermore to @xmath84 , is not too small . by the approximation lemmas",
    ", this will yield that romp selects at least a fixed percentage of energy of the still unidentified part of the signal . by the regularization step of romp , since all selected coefficients have comparable magnitudes , we will conclude that not only a portion of energy but also of the _ support _ is selected correctly",
    ". this will be the desired conclusion .",
    "[ c : uj ] we have @xmath163 .",
    "let @xmath33 = @xmath152 . since @xmath164 , the maximality property of @xmath124 in the algorithm implies that @xmath165 furthermore , since @xmath166 , by part 1 of proposition  [ p : cons ] we have @xmath167 putting these two inequalities together and using lemma  [ l : uj ] , we conclude that @xmath168 this proves the lemma .",
    "we next bound the norm of @xmath37 restricted to the smaller set @xmath84 .",
    "we do this by first noticing a general property of regularization :    [ l : reg ] let @xmath169 be any vector in @xmath170 , @xmath171 .",
    "then there exists a subset @xmath172 with comparable coordinates : @xmath173 and with big energy : @xmath174    we will construct at most @xmath175 subsets @xmath176 with comparable coordinates as in ( [ e : comp ] ) , and such that at least one of these sets will have large energy as in ( [ e : big ] ) .",
    "let @xmath177 , and consider a partition of @xmath178 using sets with comparable coordinates : @xmath179 let @xmath180 , so that @xmath181 for all @xmath182 , @xmath183 .",
    "then the set @xmath184 contains most of the energy of @xmath169 : @xmath185 thus @xmath186 therefore there exists @xmath187 such that @xmath188 which completes the proof .    in our context , lemma  [ l : reg ] applied to the vector @xmath189 along with lemma  [ c : uj ] directly implies :    [ c : uj0 ] we have",
    "@xmath190    we now finish the proof of theorem  [ t : it ] .    to show the first claim , that @xmath84 is nonempty",
    ", we note that @xmath191 .",
    "indeed , otherwise by we have @xmath192 , so by the definition of the residual in the algorithm , we would have @xmath193 at the start of the current iteration , which is a contradiction",
    ". then @xmath81 by lemma  [ c : uj0 ] .    the second claim , that @xmath82 , is also simple .",
    "indeed , recall that by the definition of the algorithm , @xmath194 .",
    "it follows that the observation vector @xmath39 satisfies @xmath195 . since by its definition the set @xmath124 contains only nonzero coordinates of @xmath37 we have @xmath196 .",
    "since @xmath197 , the second claim @xmath82 follows .    the nontrivial part of the theorem is its last claim , inequality .",
    "suppose it fails .",
    "namely , suppose that @xmath198 , and thus @xmath199 set @xmath200 . by the comparability property of the coordinates in @xmath84 and since @xmath201 , there is a fraction of energy in @xmath202 : @xmath203 where the last inequality holds by lemma  [ c : uj0 ] .",
    "on the other hand , we can approximate @xmath37 by @xmath204 as @xmath205 since @xmath206 and using lemma  [ l : uj ] , we have @xmath207 furthermore , by definition of @xmath146 , we have @xmath208 .",
    "so , by part 1 of proposition  [ p : cons ] , @xmath209 using the last two inequalities in , we conclude that @xmath210 this is a contradiction to  ( [ e : ubig ] ) so long as @xmath211 .",
    "this proves theorem  [ t : it ] .",
    "the identification step of romp , i.e. selection of the subset @xmath124 , can be done by _ sorting _ the coordinates of @xmath37 in the nonincreasing order and selecting @xmath5 biggest .",
    "many sorting algorithms such as mergesort or heapsort provide running times of @xmath212 .",
    "the regularization step of romp , i.e. selecting @xmath197 , can be done fast by observing that @xmath84 is an _ interval _ in the decreasing rearrangement of coefficients .",
    "moreover , the analysis of the algorithm shows that instead of searching over all intervals @xmath84 , it suffices to look for @xmath84 among _ @xmath213 consecutive intervals _ with endpoints where the magnitude of coefficients decreases by a factor of @xmath214 .",
    "( these are the sets @xmath176 in the proof of lemma  [ l : reg ] ) .",
    "therefore , the regularization step can be done in time @xmath215 .",
    "in addition to these costs , the @xmath216-th iteration step of romp involves _ multiplication _ of the @xmath43 matrix @xmath44 by a vector , and solving the _",
    "least squares problem _ with the @xmath217 matrix @xmath46 , where @xmath218 . for unstructured matrices ,",
    "these tasks can be done in time @xmath219 and @xmath220 respectively .",
    "since the submatrix of @xmath8 when restricted to the index set @xmath40 is near an isometry , using an iterative method such as the conjugate gradient method allows us to solve the least squares method in a constant number of iterations ( up to a specific accuracy . )",
    "using such a method then reduces the time of solving the least squares problem to just @xmath221 .",
    "thus in the cases where romp terminates after a fixed number of iterations , the total time to solve all required least squares problems would be just @xmath221 . for structured matrices , such as partial fourier",
    ", these times can be improved even more using fast multiply techniques .",
    "in other cases , however , romp may need more than a constant number of iterations before terminating , say the full @xmath215 iterations . in this case",
    ", it may be more efficient to maintain the qr factorization of @xmath46 and use the modified gram - schmidt algorithm . with this method , solving all the least squares problems",
    "takes total time just @xmath222 however , storing the qr factorization is quite costly , so in situations where storage is limited it may be best to use the iterative methods mentioned above .",
    "romp terminates in at most @xmath153 iterations .",
    "therefore , for unstructured matrices using the methods mentioned above and in the interesting regime @xmath223 , _ the total running time of romp is o(dnn)_. this is the same bound as for omp @xcite .      in many applications",
    ", one needs to recover a signal @xmath1 which is not sparse but close to being sparse in some way .",
    "such are , for example , compressible signals , whose coefficients decay at a certain rate ( see @xcite , @xcite ) . to make romp work for such signals",
    ", one can replace the stopping criterion of exact recovery @xmath224 by `` repeat @xmath5 times or until @xmath224 , whichever occurs first '' .",
    "note that we could amend the algorithm for sparse signals in this way as well , allowing for a specific level of accuracy to be attained before terminating .",
    "this section describes our experiments that illustrate the signal recovery power of romp .",
    "we experimentally examine how many measurements @xmath35 are necessary to recover various kinds of @xmath5-sparse signals in @xmath49 using romp . we also demonstrate that the number of iterations romp needs to recover a sparse signal is in practice at most linear the sparsity .    first we describe the setup of our experiments . for many values of the ambient dimension @xmath2 , the number of measurements @xmath35 , and the sparsity @xmath5 , we reconstruct random signals using romp . for each set of values",
    ", we generate an @xmath9 gaussian measurement matrix @xmath8 and then perform @xmath225 independent trials .",
    "the results we obtained using bernoulli measurement matrices were very similar . in a given trial",
    ", we generate an @xmath5-sparse signal @xmath1 in one of two ways . in either case",
    ", we first select the support of the signal by choosing @xmath5 components uniformly at random ( independent from the measurement matrix @xmath8 ) . in the cases where we wish to generate flat signals , we then set these components to one . in the cases where we wish to generate sparse compressible signals , we set the @xmath226 component of the support to plus or minus @xmath227 for a specified value of @xmath228 .",
    "we then execute romp with the measurement vector @xmath30 .",
    "figure  [ fig : percent ] depicts the percentage ( from the @xmath225 trials ) of sparse flat signals that were reconstructed exactly .",
    "this plot was generated with @xmath229 for various levels of sparsity @xmath5 .",
    "the horizontal axis represents the number of measurements @xmath35 , and the vertical axis represents the exact recovery percentage .",
    "we also performed this same test for sparse compressible signals and found the results very similar to those in figure  [ fig : percent ] .",
    "our results show that performance of romp is very similar to that of omp which can be found in @xcite .",
    "figure  [ fig:99 ] depicts a plot of the values for @xmath35 and @xmath5 at which @xmath230 of sparse flat signals are recovered exactly .",
    "this plot was generated with @xmath231 .",
    "the horizontal axis represents the number of measurements @xmath35 , and the vertical axis the sparsity level @xmath5 .",
    "theorem  [ t : main ] guarantees that romp runs with at most @xmath215 iterations .",
    "figure  [ fig : itsromp ] depicts the number of iterations executed by romp for @xmath232 and @xmath233 .",
    "romp was executed under the same setting as described above for sparse flat signals as well as sparse compressible signals for various values of @xmath234 , and the number of iterations in each scenario was averaged over the @xmath225 trials .",
    "these averages were plotted against the sparsity of the signal .",
    "as the plot illustrates , only @xmath214 iterations were needed for flat signals even for sparsity @xmath5 as high as @xmath235 .",
    "the plot also demonstrates that the number of iterations needed for sparse compressible is higher than the number needed for sparse flat signals , as one would expect .",
    "the plot suggests that for smaller values of @xmath234 ( meaning signals that decay more rapidly ) romp needs more iterations .",
    "however it shows that even in the case of @xmath236 , only @xmath237 iterations are needed even for sparsity @xmath5 as high as @xmath238 .",
    "a. gilbert , m. strauss , j. tropp , r. vershynin , _ algorithmic linear dimension reduction in the @xmath0 norm for sparse vectors _ , submitted . conference version in : algorithmic linear dimension reduction in the l1 norm for sparse vectors , allerton 2006 ( 44th annual allerton conference on communication , control , and computing ) .",
    "m. rudelson , r.vershynin , _ on sparse reconstruction from fourier and gaussian measurements _ , communications on pure and applied mathematics , to appear .",
    "conference version in : ciss 2006 ( 40th annual conference on information sciences and systems ) .",
    "r. vershynin , _ beyond hirsch conjecture : walks on random polytopes and smoothed complexity of the simplex method _ , submitted .",
    "conference version in : focs 2006 ( 47th annual symposium on foundations of computer science ) , 133142 ."
  ],
  "abstract_text": [
    "<S> this paper seeks to bridge the two major algorithmic approaches to sparse signal recovery from an incomplete set of linear measurements  @xmath0-minimization methods and iterative methods ( matching pursuits ) . </S>",
    "<S> we find a simple regularized version of orthogonal matching pursuit ( romp ) which has advantages of both approaches : the speed and transparency of omp and the strong uniform guarantees of @xmath0-minimization . </S>",
    "<S> our algorithm romp reconstructs a sparse signal in a number of iterations linear in the sparsity , and the reconstruction is exact provided the linear measurements satisfy the uniform uncertainty principle . </S>"
  ]
}