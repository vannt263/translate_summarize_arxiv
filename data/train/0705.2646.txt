{
  "article_text": [
    "clustering based on a measure of similarity is a crucial problem which appears throughout scientific data analysis . for an overview",
    "see @xcite .",
    "recently , a powerful algorithm called affinity propagation ( ap ) based on _ message passing _ was proposed by frey and dueck @xcite . as reported impressively in the original publication",
    ", this algorithm achieves a considerable improvement over standard clustering methods like k - means @xcite , spectral clustering @xcite and super - paramagnetic clustering @xcite .    based on an _ ad hoc _ pairwise similarity function between data points , ap seeks to identify each cluster by one of its elements , the so - called _",
    "each point in the cluster refers to this exemplar , each exemplar is required to refer to itself as a self - exemplar .",
    "this hard constraint forces clusters to appear as stars of radius one : there is only one central node , and all other nodes are directly connected to it .",
    "subject to this constraint , ap seeks at maximizing the overall similarity of all data points to their exemplars .",
    "the solution to this hard combinatorial task is approximated following the ideas of belief - propagation @xcite .",
    "one of the important points of ap is its computational efficiency : whereas a naive implementation of belief propagation for @xmath0 data points leads to @xmath1 messages which have to be determined self - consistently , the elegant formulation of frey and dueck allows to work with @xmath2 messages only .",
    "therefore the algorithm is feasible even in the presence of very large data sets .",
    "albeit its impressive power in a wide range of applications @xcite , ap in its present form suffers from a number of drawbacks .",
    "the most important ones related to the present work are :    * the hard constraint in ap relies strongly on _ cluster - shape regularity _ : elongated or irregular multi - dimensional data might have more than one simple cluster center .",
    "ap may force division of single clusters into separate ones .",
    "* since all data points in a cluster must point to the same exemplar , all information about both the _ internal structure _ and the _ hierarchical merging / dissociation _ of clusters is lost .",
    "* ap has _ robustness limitations _ : a small perturbation of similarities may influence the choice of one or few exemplars , and the hard constraint may trigger an avalanche leading to a different partitioning of the data set into clusters .",
    "this point is particularly important in the presence of noise in the data as , _",
    "e.g. _ , in microarray measurements .",
    "* ap forces each exemplar to point to itself .",
    "a relaxation of the hard constraint may allow for cluster structures including _",
    "second- or higher - order pointing processes_.    these problems may be solved by modifying the original optimization task of ap . as a first step",
    "we relax the hard constraint by introducing a finite penalty term for each constraint violation .",
    "this softening can be chosen in a way that the computational complexity of the algorithm remains unchanged , but its performance on biological test sets is improved considerably . moreover , relaxing the constraint helps in gaining valuable insight into the hierarchical structure of the clustering , increasing result robustness at the same time . by tuning the cluster number",
    "we see the merging of two clusters into a single one , or the dissociation of single clusters into two separated ones .",
    "given a set @xmath3 of @xmath0 data points , the original algorithm of frey and dueck takes as input a collection of real valued similarities @xmath4 between the pairs @xmath5 .",
    "the choice of the similarity measure is heuristic , it depends on the nature of data to be clustered . in the case of high - dimensional data as present in gene - expression analysis , similarity may be measured by the pearson correlation coefficient or the negative pairwise euclidean distance . however , for the algorithm described below it is not even necessary that the similarities are symmetric .",
    "the algorithm searches for a mapping @xmath6 which maps each data point @xmath7 to its exemplar @xmath8 which itself is a data point .",
    "this mapping shall minimize the cost function ( or energy ) @xmath9 = -\\sum_{\\mu=1}^n s(\\mu , c_\\mu)$ ] which equals minus the overall similarity of the data points to their exemplars .",
    "in the original ap algorithm @xmath10 is restricted by @xmath0 hard constraints : whenever a data point is selected as an exemplar by another data point , it has to be its own self - exemplar .    in this setting , we need to specify self - similarities @xmath11 .",
    "they describe the availability of data points for being self - exemplars ( and thus to serve as a cluster center ) .",
    "since all data points are _ a priori _ equally suitable to play such a role , one is naturally led to choose all self - similarities to have some common value @xmath12 .",
    "the free model parameter @xmath13 acts like a chemical potential in statistical physics , setting the prior likelihood of the number of self exemplars ( and of separated clusters consequently ) .",
    "for very small value of @xmath13 , every data point prefers to be its own exemplar , and the number of clusters equals the number of data points . in the opposite extreme case of large @xmath13 ,",
    "self - exemplars have high cost in @xmath9 $ ] .",
    "all data points are collected in one large cluster with a single exemplar . for intermediate values",
    "@xmath13 acts as a tuning parameter for the cluster number which decreases monotonously with @xmath13 .",
    "frey and dueck argue that , if the data set has some underlying structure , the correct clustering can be identified by a comparably broad range of values of the self - similarity in which the inferred cluster structure does not change .",
    "if data are not sparse and clusters are symmetrically shaped , then affinity propagation works very well and produces the correct clustering in a very short time .",
    "finding the cost minimum of @xmath9 $ ] subject to the self - exemplar constraint is a computationally hard task .",
    "it can be achieved exactly only for very small systems .",
    "the central idea of ap is therefore to identify the exemplars using message passing ( belief propagation , bp ) as a heuristic strategy @xcite : real - valued messages between pairs of data points are updated recursively until a stable clustering emerges .",
    "the original ap equations are a direct application of bp ( or , equivalently , max - sum @xcite ) to the clustering problem .",
    "there are two types of messages exchanged between data points @xcite : the _ responsibility _ @xmath14 is sent from data point @xmath7 to candidate exemplar @xmath15 ; it reflects the accumulated evidence that @xmath7 chooses @xmath15 as its cluster exemplar .",
    "the _ availability _",
    "@xmath16 is sent from candidate exemplar @xmath15 to datum @xmath7 ; it reflects the appropriateness for @xmath15 to be an exemplar for @xmath7 as a result of the self - exemplar constraint . as mentioned before",
    ", the original ap imposes constraints on exemplars to be self - exemplars .",
    "we modify the algorithm of frey and dueck by softening this hard constraint .",
    "we write the constraint attached to a given data point @xmath7 as follows , with @xmath17 $ ] : @xmath18 = \\left\\ {    \\begin{array}{lll }      p & { \\rm if } & \\exists \\nu:~c_{\\nu}=\\mu,~c_{\\mu } \\ne \\mu \\\\[0.1 cm ]      1 & { \\rm else . } &    \\end{array }    \\right .",
    "\\label{constraints}\\ ] ] the first case assigns a penalty @xmath19 if data point @xmath7 is chosen as an exemplar by some other data point @xmath15 , without being a self - exemplar . the hard - constraint limit of frey and dueck is recovered by setting @xmath19 to zero . for @xmath20 ,",
    "@xmath21 $ ] becomes identically one , the minimization task of @xmath9 $ ] becomes unconstrained and independent for all data points , thus each data point chooses his nearest neighbor as an exemplar .",
    "an intermediate value of @xmath19 allows to interpolate between these two extreme cases .",
    "it presents a compromise between the minimization of @xmath9 $ ] on one hand , and the search for compact clusters on the other hand .",
    "finally we introduce a positive real - valued parameter @xmath22 weighing the relative importance of the cost minimization with respect to the constraints . in a statistical - physics perspective",
    ", this parameter can be seen as a formal inverse temperature .",
    "its introduction allows us to define the probability of an arbitrary clustering @xmath10 as @xmath23 = \\frac{1}{z } \\exp\\left ( - \\beta e_1[{\\bf c } ] \\right)\\ \\prod_\\mu \\ \\chi^{(p)}_{\\mu } [ { \\bf c } ]    \\label{partition}\\ ] ] where the partition function @xmath24 serves to normalize @xmath25 $ ] . in this setting ,",
    "both clusterings of non - optimal cost and of many violated self - exemplar constraints are suppressed exponentially .",
    "the task of finding high - scoring @xmath10 can be understood as a minimization problem with the modified cost function @xmath26 = -\\sum_{\\mu=1}^{n } s(\\mu , c_{\\mu})- \\frac 1\\beta \\sum_{\\mu=1}^{n } \\ln\\left ( \\chi^{(p)}_{\\mu}[{\\bf c } ] \\right ) \\label{energy}\\ ] ] ap is recovered by taking @xmath27 since any violated constraint sets @xmath25 $ ] to zero in eq .",
    "( [ partition ] ) . for general @xmath19 ,",
    "the optimal clustering @xmath28 can be determined component - wise by maximizing the marginal probabilities , @xmath29 \\label{eq : marginal}\\ ] ] for all data points @xmath7 .      in the limit @xmath30 ,",
    "( [ partition ] ) becomes concentrated to the true cost minima .",
    "looking at eq .",
    "( [ energy ] ) it becomes obvious that @xmath19 has to scale as @xmath31 in order to have some non - trivial effect . in this limit , we find the scap equations ( with @xmath32 , see sec .",
    "[ methods ] ) : @xmath33 \\nonumber \\\\",
    "r(\\mu,\\mu ) & = & { \\max}\\big[\\!-\\tilde p , s(\\mu,\\mu )   - { \\max}_{\\lambda \\ne \\mu}\\{s(\\mu,\\lambda )   + a(\\mu,\\lambda)\\ } \\big ] \\nonumber \\\\",
    "a(\\mu,\\nu ) & = & { \\min}\\big [ 0,\\ r(\\nu,\\nu ) + \\sum_{\\lambda \\ne \\mu } { \\max}(0,\\ r(\\lambda,\\nu ) \\big ] \\label{wp } \\\\",
    "a(\\mu,\\mu ) & = & { \\min}\\big[{\\tilde p},\\ \\sum_{\\lambda \\ne \\mu }   { \\max}\\{0,r(\\lambda,\\mu)\\ } \\big ] \\ .",
    "\\nonumber\\end{aligned}\\ ] ] these @xmath34 equations are closed and can be solved iteratively . following eq .",
    "( [ eq : marginal ] ) the exemplar @xmath35 of any data point @xmath7 can be computed by maximizing the marginal _ a posteriori _ probability : @xmath36    \\label{cstar}\\ ] ] compared to original ap , scap amounts to an additional threshold on the self - availabilities @xmath37 and the self - responsibilities @xmath38 : for small enough @xmath39 , @xmath40 in many cases , up to @xmath41 ( or @xmath42 , where every site chooses its best first neighbor as its exemplar . at the same time , beyond a certain threshold the self responsibility @xmath38 is substituted with @xmath43 . for @xmath44 ( _ i.e. _ @xmath27 ) the original ap equations are recovered .    in practice",
    ", this means that variables are discouraged to be self exemplars beyond a give threshold , even in the case someone is already pointing at them .",
    "the resulting clustering is more stable and obviously allows for a hierarchical structure where @xmath45 can point to @xmath7 that can point to @xmath15 etc .",
    "also loops are possible . in most of the tests performed ( both on artificial and biological cancer data )",
    "clusters are almost tree - like besides a dimer .",
    "the iterative solution of eqs .  ( [ wp ] ) can be implemented in the following way :    1 .   define the similarity @xmath4 for each set of data points .",
    "choose the values of the self - similarity @xmath13 and of the constraint strength @xmath46 .",
    "initialize all @xmath47 2 .   for all @xmath48 , first update the @xmath0 _ responsibilities _ @xmath14 and then the @xmath0 _ availabilities _ @xmath49 , using eqs .",
    "( [ wp ] ) .",
    "3 .   identify the exemplars @xmath8 by looking at the maximum value of @xmath50 for given @xmath7 , according to eq .",
    "( [ cstar ] ) .",
    "repeat steps 2 - 3 till there is no change in exemplars for a large number of iterations ( we used 10 - 100 iterations ) .",
    "if not converged after @xmath51 iterations ( typically 100 - 1000 ) , stop the algorithm .",
    "three notes are necessary at this point :    * step 3 is formulated as a sequential update : for each data point @xmath7 , all outgoing responsibilities and then all incoming availabilities are updated before moving to the next data point . in numerical experiments",
    "this was found to converge faster and in a larger parameter range than the damped parallel update suggested by frey and dueck in @xcite .",
    "dependence of the result on initial conditions was not observed . + * the naive implementation of the update equations ( [ wp ] ) requires @xmath34 updates , each one of computational complexity @xmath52 .",
    "a factor @xmath0 can be gained by first computing the unrestricted max and sum once for a given @xmath7 , and then implying the restriction only inside the internal loop over @xmath15 .",
    "like this , the total complexity of a global update is @xmath2 and thus feasible even for very large data sets .",
    "+ * belief propagation on loopy graphs is not guaranteed to converge .",
    "we observe , however , efficient convergence of the sequential update over wide parameter ranges . to handle the possibility of non - convergence , we have introduced a cutoff in the number of iterations .",
    "if this is reached , the algorithm stops , and the actual parameter combination is discarded .      in many clustering tasks input data consist of high - dimensional vectors , a specific example being genome - wide microarrays .",
    "frequently only few components of these vectors carry useful information about the cluster structure , extracting such cluster signatures is of crucial importance in understanding the mechanisms behind the cluster structure .    in the following , we will use the specific case of microarray data .",
    "therefore we use the notion gene for a component of the input vector , even if at this stage the discussion is still general . the total number of genes is denoted by @xmath53 .",
    "we propose a simple measure of the influence of single genes on the total similarity measure of a cluster , as compared to random choices of the exemplar selection @xmath10 . for simplicity , we assume the similarity between data points @xmath54 and @xmath55 to be additive in single - gene contributions @xmath56 this is true , _",
    "e.g. _ , for the pearson correlation or the negative square euclidean distance .",
    "it can be easily generalized to similarity measures which are given by a monotonous function of a sum over gene contributions ( like the negative of the euclidean distance which is the square root of the sum of single - gene contributions ) .",
    "having found a clustering given by the exemplar selection @xmath10 , we can calculate the similarity of a cluster @xmath57 defined as a connected component of the directed graph given by @xmath10 .",
    "it is given by @xmath58 as a sum over single - gene contributions @xmath59 these have to be compared to random exemplar choices which are characterized by their mean @xmath60 and variance @xmath61 ^ 2 =   \\sum_{\\mu\\in c } \\left\\ { \\frac 1n \\sum_{\\nu=1}^n   s(x_\\mu^i , x_\\nu^i ) ^2 - \\left [ \\frac 1n \\sum_{\\nu=1}^n   s(x_\\mu^i , x_\\nu^i )   \\right]^2 \\right\\}\\ ] ] the relevance of a gene can now be ranked according to @xmath62 which measures the distance of the actual @xmath63 from the distribution of random exemplar mappings .",
    "genes can be ranked according to the value of @xmath64 , highest - ranking genes are considered a cluster signature .",
    "the same procedure can be carried through for each cluster independently , but also for cluster combinations .",
    "the data consist of measurements of sepal length , sepal width , petal length and petal width , performed for 150 flowers , chosen from three species of the flower iris .",
    "it is a benchmark problem for clustering @xcite .",
    "super - paramagnetic clustering is able to cluster 125 of the data points correctly , leaving 25 points unclustered @xcite .",
    "when we apply ap on iris data , we identify three clusters making 16 errors .",
    "with scap , we identify them with just nine errors .",
    "we use the manhattan distance measure for the similarity function ,",
    "i.e latexmath:[$s(\\mu,\\nu)=-\\sum_{i=1}^{4 }     we saw that the species iris setosa separates without any errors . on increasing the value of @xmath46",
    ", the iris setosa cluster stays intact and the clusters for versicolor and virginica merge with each other , reflecting the fact that they are closer to each other than to setosa .",
    "the errors occur because some samples from these species were closer to samples from other species than to their own .",
    "we used a test data set monitoring the expression levels of more than 7000 genes for 42 patients , which were previously correctly classified into 5 diagnosis types by an _ a posteriori _ assessment method @xcite ( 10 medulloblastoma , 10 malignant glioma , 10 atypical teratoid / rhabdoid tumors , 4 normal cerebella , 8 primitive neuroectodermal tumors ) .",
    "each array was filtered , log - normalized to mean zero and variance one , resulting in @xmath66 genes . due to this choice pearson correlation and negative square euclidean distance",
    "are equivalent .",
    "the diagnosis information was not used during clustering , but only for checking the algorithmic outcome .    _ imposing five clusters in ap and scap : _ since we knew that the correct clustering was to identify five different patterns , our first approach was to tune @xmath13 and @xmath46 in order to get five clusters .",
    "first , we fixed @xmath46 to infinity ( original ap ) and changed @xmath13 finding around @xmath67 the desired number of 5 clusters with 8 errors .",
    "the error was calculated _ a posteriori _ by counting every data point which referring to an exemplar of a different diagnosis .",
    "next we fixed @xmath13 to a sufficiently large value ( the result becomes insensitive on @xmath13 once the latter takes large values ) , and we changed @xmath46 . in this case , for @xmath68 we got 6 clusters with 8 errors , for @xmath69 4 clusters with again 8 errors .",
    "5 clusters were not found to correspond to any extended @xmath46-region .",
    "note that in both cases all errors occur in the last cluster : samples supposed to take diagnosis 5 ( pnet ) rarely find an exemplar of the same class .",
    "instead they distribute over the other four diagnoses .    _ clustering with ap : _ then , instead of fixing the number of clusters , we changed @xmath13 continuously for @xmath70 .",
    "we counted the number of clusters and of errors as a function of @xmath13 , see fig .",
    "[ fig:01 ] .",
    "the algorithm ground state ( configuration of maximum marginals values ) in the limit of @xmath71 is a single cluster .",
    "the first non trivial clustering occurs when the number of clusters remain unchanged for a stable range of @xmath13 values . in this preliminary study , we took that to be the actual predicted data clustering .",
    "hence , by looking at fig .",
    "[ fig:01 ] , we would conclude that there are three well - distinguishable clusters in the present data set .",
    "look , however , to the number of errors : it is found to be 14 - 15 in this range , basically due to the wrong assignment of two entire classes to only three exemplars .",
    "four or five clusters can be imposed and lead to lower error values , but require fine - tuning of @xmath13 .    _ clustering with scap : _ we than fix @xmath13 to be very large and change only @xmath46 .",
    "for @xmath72 we start with seven clusters , this number decreases rapidly as @xmath46 increases , see fig .",
    "[ fig:02 ] . as before , the point at which the number of clusters is robust against changes in @xmath46",
    "was taken as the best scap clustering . from fig .",
    "[ fig:02 ] we conclude that scap identifies @xmath73 clusters .",
    "the number of errors in classification is 8 .    , for @xmath74 ( the original ap algorithm ) .",
    "based on this we would conclude that the data has three nontrivial clusters.,width=7 ]    .",
    "this plot suggests that the data has four clusters.,width=7 ]    right from @xmath72 , where each data point chooses its closest neighbor as his exemplar , errors are due to misclassifications of the fifth diagnosis ( pnet ) .",
    "the other data points select exemplars of the same diagnosis , but various clusters of same diagnosis exist . only in the case of four clusters , as shown in fig .",
    "[ fig:05 ] , each of the first four diagnoses is assembled in an isolated cluster , with the pnet arrays distributed over the three cancer - related clusters . the normal tissue ( 30 - 33 in the figure ) is well - separated from all others .",
    "only if we go towards three clusters , it merges with diagnosis type ( 0 - 9 ) ( medulloblastoma ) , showing that these two are closer in expression in between them than compared to others .",
    "a more detailed analysis of the brain cancer data is provided in the supplementary material .",
    "note that scap also provides information about the internal organization inside the clusters .",
    "we find , _",
    "e.g. _ , that the misclassified patterns are always peripherical cluster elements .",
    "no other data point refers to them .",
    "this information is lost in ap . due to the hard constraint",
    "all points belonging to the same cluster refer to the same exemplar , and information about the internal cluster structure is not contained in @xmath28 . a graphical representation of the cluster structure in this case is contained in the supplementary material .    in @xcite data",
    "were clusterized using hierarchical clustering . even if the overall cluster structure is similar to the one we found ,",
    "there is no clear - cut clustering into 4 - 5 classes , some arrays ( well - clustered with scap ) were only added at very late stages of hierarchical clustering .",
    "the global nature of scap leads to a better clustering performance than the local and greedy hierarchical clustering .",
    "another interesting point comes from the comparison of our clustering results with the supervised classification results of @xcite .",
    "there , a number of state - of - the - art classification algorithms is applied , with training sets containing 2/3 , test sets containing 1/3 of the data points .",
    "dettling finds that the minimal generalization error made is 23.8% , corresponding to ca .",
    "10 errors on a data set of cardinality 42 .",
    "it is interesting to note that scap in the clustering corresponding to 4 clusters makes only 8 errors . note that training in @xcite is done on a subset of patterns , but supervision in this case seems to add no valuable information to the unsupervised clustering results .",
    "last but not least , we use the procedure described above to extract cluster signatures in the most stable case of four clusters depicted in fig .",
    "[ fig:05 ] .",
    "the lists of the highest ranking genes together with their relevance value @xmath64 is given in the supplementary material .",
    "the number of statistically relavant genes ( we consider a threshold @xmath75 ) depends on the cluster and is largest for the normal tissue ( 42 genes ) , it is much smaller in particular for the first cluster ( ca .",
    "4 genes ) .",
    "if we take the first 15 - 25 genes per cluster , _",
    "i.e. _ , an overall signature of 60 - 100 genes , we already find basically the same clustering as before , only two new errors of previously well - assigned patterns appear . at gene signatures 120 - 240 , only one of these errors survives .",
    "we therefore find that the signature found in this way carries most of the information needed for the clustering .",
    "note also , that due to the fact that in an unsupervised way we did not separate the fifth diagnosis type into a single cluster , we do not have by definition a cluster signature for this cancer type .          _ lymphoma cancer data : _",
    "we used a data set of 62 patients for 4026 genes , showing 3 different diagnosis @xcite . in the limit of @xmath46 going to infinity , we find the first nontrivial clustering for @xmath13 between @xmath76 . in this regime",
    "ap group data into 3 sets , making with 3 error .",
    "for very high @xmath13 and varying @xmath46 , the 3-groups clustering becomes more stable and robust , while the algorithm makes just one assignment prediction error . in this case",
    ", dettling finds a minimal generalization error of 0.95% , corresponding to less than one error in 62 patterns .",
    "supervision adds some information , even if clustering itself makes only one error .",
    "_ srbct cancer data : _ this set has 63 samples with 2308 genes and 4 expression diagnosis patterns @xcite . for @xmath46 going to infinity , the best tuning - robust estimates groups cluster data into 5 clusters making as many as 22 errors . on the other hand , with finite @xmath46",
    ", scap finds a regime of 4 clusters , making only 7 assignment errors . here ,",
    "dettling reports only 1.24% generalization error in supervised classification , corresponding to less than one error on 63 patterns .",
    "classification thus performs considerably better than clustering alone .",
    "_ leukemia : _ this set has 72 samples with 3571 genes and 2 diagnoses @xcite . in the case of infinite @xmath46 , the original ap groups data into 2 clusters with 4 errors , while for variable @xmath46 ( fixing @xmath13 very large ) modified ap finds 2 clusters with 2 errors .",
    "also classification leads to 2.5% of errors , a result which is slightly better than our clustering result .",
    "in the process of choosing exemplars , we need to calculate marginals @xmath77 @xmath78 is the probability that data point @xmath7 chooses point @xmath79 as its exemplar .",
    "the calculation of marginals can be done iteratively via a message - passing algorithm called belief propagation ( bp ) @xcite .",
    "it is exact on tree factor graphs but usable heuristically in the general case .",
    "together with a generalized larger family of message - passing algorithms , it was shown to be very powerful in solving np - hard combinatorial problems on locally tree - like structures @xcite .",
    "recently , the applicability of bp was also shown to be efficient in some important problems giving rise to dense and loopy factor graphs @xcite .",
    "looking at figures ( [ ap - fg ] ) and ( [ messages ] ) , bp computes beliefs @xmath80 for the marginal probabilities as products of messages @xmath81 coming from each compatibility constraint , times the local prior computed as the exponential of the similarity between point @xmath7 and its putative exemplar @xmath79 . up to overall normalization , we write : @xmath82 where @xmath22 plays the role of an annealing parameter measuring the relative importance given to the priors compared to the information passed by the messages .",
    "message @xmath81 can be interpreted , as the probability that constraint @xmath45 alone forces @xmath7 to select exemplar @xmath79 .",
    "it can be calculated via the following self - consistent equations @xmath83 where the @xmath84 functions @xmath85 can be seen as probabilities that data point @xmath7 chose @xmath79 to be its exemplar if constraint @xmath45 were absent in eq .",
    "( [ partition ] ) . these probabilities",
    "are called _ cavity probabilities _ , because the disregarding of one data point / constraint effectively carves a cavity in the original factor graph .",
    "the _ link direction _ of functions @xmath86s and @xmath87s is shown if fig .",
    "( [ ap - fg ] ) together with the problem s factor graph .",
    "( [ messages ] ) shows a pictorial representation of the flow of messages ( [ firstbp - a ] ) and ( [ firstbp - b ] ) .    along the lines of @xcite and @xcite , but bearing in mind the modified form for the compatibility constraints , eq .",
    "( [ firstbp - a ] ) can be simplified after a few manipulations in the following way , depending on cases : @xmath88   \\label{aa } \\\\",
    "( \\mu \\ne \\nu ) \\ ; \\wedge \\ ; ( c = \\mu ) & \\to & a_{\\mu \\to \\nu}(\\mu )   = \\frac{1}{z^a_{\\mu \\to \\nu } } \\left [   p + ( 1-p ) b_{\\mu \\to \\mu}(\\mu ) ) \\right ]   \\nonumber \\\\",
    "( \\mu \\ne \\nu ) \\ ; \\wedge \\ ; ( c \\ne \\mu ) & \\to & a_{\\mu \\to \\nu}(c : c \\ne \\mu )   = \\frac{1}{z^a_{\\mu \\to \\nu } } \\big [   p + ( 1-p ) * \\nonumber \\\\ & & * \\big ( ( b_{\\mu \\to \\mu}(\\mu ) ) + \\prod_{\\lambda \\ne \\nu }   ( 1 - b_{\\lambda \\to \\mu}(\\mu ) ) \\big ) \\big ] \\nonumber\\end{aligned}\\ ] ] with @xmath89 being normalization constants .",
    "it is remarkable that the number of effectively independent quantities present in eqs .",
    "( [ firstbp - a ] ) is much smaller than the apparent @xmath1 real valued numbers . indeed , functional messages @xmath86 take only 2 different values : @xmath90 , that from now on will be called @xmath91 to avoid index redundancy , and @xmath92 independently on @xmath79 , as long as it is @xmath93 .",
    "the exchange of indexes in @xmath94 is pure convention , but it has been introduced for coherency with the definition of availabilities given in @xcite .",
    "it follows immediately from the normalization condition that @xmath95 .    for the cavity probability functions , manipulation of eq .",
    "( [ firstbp - b ] ) involving the use of the last normalization condition and of the normalization constant rescaling , leads to @xmath96 @xmath97 with @xmath98 guaranteeing normalization @xmath99 .",
    "messages @xmath100 have been also renamed with a symbol coherent with the _ responsibility - availability _ notation of @xcite .",
    "it can be seen that self consistent equations close into the @xmath101 quantities @xmath91 and @xmath102 alone .",
    "indeed , the effective dependence on the exemplar choice is dropped , and the computational size of the problem reduces by a factor @xmath0 .            the set of equations of @xmath86 and @xmath103 can be solved iteratively via the bp algorithm .",
    "the case in which one is interested not in the whole form of the posterior probability function , but only in retaining information about the most probable exemplar chosen by each data point , can be seen to be equivalent to taking the @xmath30 limit where availabilities @xmath104 and responsibilities @xmath105 are introduced in the following way : @xmath106 and treating the exponential scaling in a regime where prior similarities between data points @xmath107 are of the same order of magnitude of the valued of @xmath104 and @xmath105 .",
    "the rescaling of responsibilities can be freely done as it does not change the number of independent variables . from the last definitions one",
    "is led to equations @xmath108 where the last relation already assumes a large-@xmath22 limit with non - degeneracy of the most probable value of the cavity probabilities .",
    "this hypothesis is equivalent to having non - degenerate choices of exemplars for all data points , i.e. , to the existence of a single optimal clustering identified via the scap algorithm as the unique ground state of the system energy ( [ energy ] ) .",
    "this is a sensible assumption , but it is not always satisfied in interesting cases .",
    "studying the degenerate number and behavior of clustering choices is another crucial question that is only partially answered by the introduction of the relaxation parameter @xmath19 and will be the subject of further work beyond this paper . in the large @xmath22 regime , in order to work with quantities all with the same scaling , it is useful to define @xmath109 and consider @xmath39 , @xmath104 and @xmath105 fixed varying @xmath22 .",
    "equating eqs .",
    "( [ abeta ] ) and ( [ bbeta ] ) with eqs .",
    "( [ aa ] ) and ( [ bb ] ) respectively , and extracting the leading terms in the large @xmath22 limit assuming no degeneracy , the following equations are found , using eq .",
    "( [ ttbeta ] ) : @xmath33 \\nonumber \\\\",
    "r(\\mu,\\mu ) & = & s(\\mu,\\mu )   - { \\max}_{\\lambda \\ne \\mu}\\big[s(\\mu,\\lambda )   + a(\\mu,\\lambda)\\big ] \\\\",
    "a(\\mu,\\nu ) & = & { \\min}\\big [ 0,{\\max}(-{\\tilde p } , { \\min}(0,\\ r(\\nu,\\nu ) ) ) + \\sum_{\\lambda \\ne \\mu } { \\max}(0,\\ r(\\lambda,\\nu ) \\big ] \\nonumber \\\\",
    "a(\\mu,\\mu ) & = & { \\min}\\big[{\\tilde p},\\ \\sum_{\\lambda \\ne \\mu }   { \\max}\\{0,r(\\lambda,\\mu)\\ } \\big ] \\ .",
    "\\nonumber \\label{wp-0}\\end{aligned}\\ ] ] making another change of variables redefining the self - responsibilities as @xmath110   - { \\max}_{\\mu}\\ { r(\\mu,\\mu ) \\ } \\to r(\\mu,\\mu)\\ ] ] we get , in terms of the rescaled quantities , @xmath111 leading to scap equations ( [ wp ] ) . is redundant if self - responsibilities are negative , as it is usually the case . ] after convergence , marginals can be written @xcite as @xmath112 in the @xmath30 limit , one can write equation   ( [ cstar ] ) .",
    "affinity propagation is a new powerful tool for unsupervised clustering . it has many very strong points .",
    "first it is very efficient , convergence to the final clustering is very fast , the latter appears to be independent on the initialization of messages .",
    "second , due to its hard constraints ap identifies exemplars which are prototypical data points representing a whole cluster .",
    "this last point is , however , also a first limitation of the original ap algorithm . if clusters can not be well - represented by a single cluster exemplar , ap has to fail .",
    "the hard constraint renders the algorithm greedy , and small fluctuations in the similarity measure may trigger avalanches in the exemplar choice leading to different clusterings for only slightly modified model parameters .",
    "we have introduced a soft - constraint version of affinity propagation which is able to cure a part of these problems without loosing the efficiency of the original ap :    * by relaxing the hard constraint on clusters exemplars , we could introduce a parameter ( @xmath46 ) controlling the algorithm greediness .",
    "@xmath46 is a better tuning parameter than @xmath13 ( it is more informative and leads to more robust and stable clustering ) and it is easier to interpret the statistical meaning of its tuning process . +",
    "* clusters are more robust than in the original formulation of the algorithm . moreover , even though a second _ a priori _ free - parameter is introduced , the overall dependence of the algorithm on free parameters is reduced , and an optimal tuning strategy naturally emerges . +",
    "* the cluster structure can be efficiently probed .",
    "this concerns the internal structure of the clusters since scap is able to identify central and peripherical nodes of each clusters , as well as the hierarchical organization leading to a process of cluster merging if cluster number is reduced by looking to less fine structures .",
    "+ * in the case of high - dimensional data , the relation between data points and their exemplars can be used to extract a sparse cluster signature . in the case of brain tumors ,",
    "we have found that 20 - 40 genes per cluster are sufficient to reproduce almost the same clustering as found using all genes .",
    "+    we conclude that scap is more efficient than ap in particular in the case of noisy , irregularly organized data - and thus in biological applications concerning microarray data .",
    "the computational efficiency of scap allows there to treat also very large data sets .",
    "we acknowledge very useful discussions with alfredo braunstein , andrea pagnani and riccardo zecchina . m.l . would like to thank the malawi polytechnic for hospitality during the preparation of the manuscript .",
    "the work of s. and m.w . was supported by the ec via the strep gennetec ( `` genetic networks : emergence and complexity '' ) .",
    "pomeroy , s. , tamayo , p. , gaasenbeek , m. , sturla , l. , angelo , m. , mclaughlin , m. , kim , j. , goumnerova , l. , black , p. , lau , c. et al .",
    "prediction of central nervous system embryonal tumor outcome based on gene expression .",
    "_ nature _ * 415 * , 436 - 442 ."
  ],
  "abstract_text": [
    "<S> _ motivation : _ similarity - measure based clustering is a crucial problem appearing throughout scientific data analysis . recently , </S>",
    "<S> a powerful new algorithm called affinity propagation ( ap ) based on message - passing techniques was proposed by frey and dueck @xcite . in ap , each cluster is identified by a common exemplar all other data points of the same cluster refer to , and exemplars have to refer to themselves . albeit its proved power , ap in its present form suffers from a number of drawbacks . </S>",
    "<S> the hard constraint of having exactly one exemplar per cluster restricts ap to classes of regularly shaped clusters , and leads to suboptimal performance , _ </S>",
    "<S> e.g. _ , in analyzing gene expression data .    </S>",
    "<S> _ results : _ this limitation can be overcome by relaxing the ap hard constraints . </S>",
    "<S> a new parameter controls the importance of the constraints compared to the aim of maximizing the overall similarity , and allows to interpolate between the simple case where each data point selects its closest neighbor as an exemplar and the original ap . </S>",
    "<S> the resulting soft - constraint affinity propagation ( scap ) becomes more informative , accurate and leads to more stable clustering . </S>",
    "<S> even though a new _ a priori _ free - parameter is introduced , the overall dependence of the algorithm on external tuning is reduced , as robustness is increased and an optimal strategy for parameter selection emerges more naturally . </S>",
    "<S> scap is tested on biological benchmark data , including in particular microarray data related to various cancer types . </S>",
    "<S> we show that the algorithm efficiently unveils the hierarchical cluster structure present in the data sets . </S>",
    "<S> further on , it allows to extract sparse gene expression signatures for each cluster .    </S>"
  ]
}