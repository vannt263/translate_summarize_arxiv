{
  "article_text": [
    "we consider a linear ill - posed problem that can be modeled as @xmath0 where @xmath1 is the desired solution , @xmath2 models the forward process , @xmath3 is additive random noise , and @xmath4 is observed data .",
    "the goal is to obtain an approximate solution of @xmath5 given @xmath6 and @xmath7 .",
    "prior information regarding the probability distribution of @xmath8 may be incorporated , if known .",
    "the matrix @xmath6 is ill - conditioned and its singular values decay to zero without significant spectral gap . due to the ill - posed nature of the problem , the exact ( inverse ) solution of will be contaminated by noise and may not be a good approximation of @xmath5 .",
    "thus , we seek an approximation to the solution by solving a nearby problem that is well - posed , a process called _ regularization_. a well - known regularization method is tikhonov regularization  @xcite , @xmath9 where @xmath10 is a regularization parameter and @xmath11 is a regularization matrix .",
    "the matrix @xmath11 must have the same number of columns as @xmath6 , and we assume that the null spaces of @xmath6 and @xmath11 intersect trivially such that the matrix @xmath12 $ ] has full row rank and the solution @xmath13 is unique . if @xmath10 and @xmath11 are chosen appropriately , the solution to   should approximate the desired solution @xmath5 .",
    "typical choices of @xmath11 include the identity matrix and the discrete first or second derivative operators .",
    "if @xmath14 , we say that is in _",
    "standard form_. otherwise , we say it is in _",
    "general form_. for the latter case , we can transform the problem to standard form .",
    "that is , we can use the substitution @xmath15 if @xmath11 is invertible , or otherwise , we can define @xmath16 , where @xmath17 is the @xmath6-weighted generalized inverse of @xmath11 defined by @xmath18 ; see  ( * ? ? ?",
    "* section  2.3 ) for further details .",
    "next we provide a motivating example to illustrate the potential benefits of considering the general - form tikhonov regularization . using the deriv2 test problem from @xcite ,",
    "we provide in figure  [ fig : differentl ] reconstructions for various choices of @xmath19 where in each case , the regularization parameter @xmath10 was selected to produce the smallest mean squared error .     on the reconstructed solution . here , the true solution is provided , along with reconstructions corresponding to @xmath20 ( standard tikhonov ) and @xmath21 , where @xmath22 represents a discrete approximation of the first derivative . ]    this example illustrates that @xmath14 is not suited for all problems and that better reconstructions may be obtained with @xmath23 .",
    "we remark that this example is used for illustrative purposes only , so specific details are not included here .",
    "complete numerical investigations will be presented in section  [ sec : numerics ] .",
    "the difficulty in selecting an appropriate regularization matrix has recently led researchers to investigate an extension of the general - form tikhonov regularization that contains several regularization terms instead of only one .",
    "this extension is known as the multi - parameter tikhonov problem , @xmath24 where multiple regularization parameters @xmath25 $ ] and multiple regularization matrices @xmath26 are incorporated .",
    "several studies on the multi - parameter tikhonov problem include @xcite .",
    "a key ingredient for both the one - parameter   and multi - parameter   tikhonov problems is the choice of the regularization parameter(s ) .",
    "standard techniques to find the regularization parameter for tikhonov regularization have been studied extensively in the literature and include the discrepancy principle , the l - curve criterion , and the generalized cross - validation ( gcv ) method  @xcite .",
    "some of these techniques have been extended to the multi - parameter tikhonov problem .",
    "for example , a higher dimension l - curve is considered in @xcite , the discrepancy principle is discussed in @xcite , and an extension of gcv is described in @xcite . in @xcite , the regiska s parameter choice rule is generalized to the multi - parameter tikhonov case .    in this paper , we propose a learning approach for computing _ optimal _ regularization parameters for both tikhonov problems   and  .",
    "previous work on learning approaches in the context of regularization methods for solving inverse problems can be found in  @xcite . however",
    ", none of these works specifically address the general - form tikhonov and multi - parameter tikhonov problems .",
    "we follow the work of chung et .",
    "@xcite where training data is used to find optimal regularization parameters for a variety of spectral regularization methods , including standard form tikhonov regularization .",
    "we first extend these approaches to the general - form tikhonov problem .",
    "for general - form tikhonov , the generalized singular value decomposition ( gsvd )  @xcite of @xmath27 can be used to write the solution to as a spectral filtered solution .",
    "we take advantage of this fact to develop efficient methods for computing an optimal regularization parameter , and compare its performance to that of filtered svd solutions on several numerical examples .",
    "another main contribution of our work is to develop an efficient learning approach for the multi - parameter tikhonov problem .",
    "the extension to the multi - parameter case is more complex , not only in terms of finding multiple @xmath10 s , but also because in general , there is no natural extension of the gsvd for @xmath6 and multiple regularization matrices @xmath28 . for problems where matrices @xmath6 and @xmath28 are _ simultaneously diagonalizable _",
    "( sd ) , such as those arising in image processing , we propose an efficient learning approach for computing optimal regularization parameters .",
    "however , for problems that do not exhibit this nice property , we follow a framework for regularization introduced in  @xcite , where matrix approximations are used to compute regularization parameters , but the original problem is solved .",
    "it is worth mentioning that in  @xcite and @xcite , multi - parameter learning approaches for denoising problems ( @xmath29 ) are proposed . in",
    "@xcite a parameter learning approach for multiple @xmath30-norm regularization terms is presented .",
    "the learning problem is formulated as a bilevel optimization problem and solved using semismooth newton methods .",
    "although the same techniques could be extended to our problem , we believe that such an approach applied to the problem where @xmath31 would be more computationally expensive than the one we propose here .",
    "the paper is organized as follows .",
    "background on the general - form tikhonov regularization problem is provided in section [ sec : background ] .",
    "we provide the solution as a spectral filtered solution and describe some standard approaches for selecting regularization parameters .",
    "in section [ sec : optfilt ] , we extend the optimal filter framework to both the general - form tikhonov regularization problem and the multi - parameter tikhonov regularization problem and describe an empirical bayes risk framework for computing optimal regularization parameters . in particular , numerical methods for computing optimal regularization parameters for the multi - parameter problem are discussed .",
    "numerical results are presented in section [ sec : numerics ] , and conclusions and future work can be found in section [ sec : conclusions ] .",
    "a closed form solution to the general - form tikhonov regularization problem   can be obtained using the gsvd of the matrix pair @xmath32 .",
    "given matrices @xmath33 , and @xmath34 , the gsvd can be obtained by first considering the _ reduced _ qr - factorization of the stacked matrix , @xmath35 = \\left[\\begin{array}{c } { { \\bf q}}_{{{\\bf a } } } \\\\ { { \\bf q}}_{{{\\bf l } } } \\end{array}\\right ] { { \\bf r}}.\\ ] ] we have @xmath36 and @xmath37 .",
    "let @xmath38 and @xmath39 be the cs decomposition @xcite of \\{@xmath40 } , where @xmath41 , @xmath42 , and @xmath43 are orthogonal matrices ; @xmath44 where @xmath45 is diagonal and @xmath46 is diagonal ( not necessarily square ) , satisfying @xmath47 .",
    "the existence of the cs decomposition can be found in  ( * ? ? ?",
    "* section  22.1 ) . then the gsvd of @xmath32 is defined as follows @xmath48 where for convenience",
    ", we have ordered the entries in @xmath49 and @xmath50 such that @xmath51 @xmath52 using the gsvd , the general - form tikhonov solution can be written as @xmath53 where @xmath54 is a diagonal matrix with _ filter factors _ ,",
    "@xmath55 on the diagonal .",
    "notice that for @xmath56 , @xmath57 can be written as @xmath58 , where @xmath59 are the generalized singular values .",
    "it is easy to see that for standard - form tikhonov regularization where @xmath60 and @xmath49 contain the right singular vectors and singular values of @xmath6 ( with reordering ) and @xmath61 for all @xmath62 , the filter factors are given by @xmath63 .",
    "therefore , by allowing @xmath23 , we not only affect the definition of the filter factors , but also alter the basis vectors @xmath64 used to represent the regularized solution @xmath13 .",
    "for ill - posed inverse problems , it is well known that the problem satisfies the discrete picard condition  @xcite .",
    "that is , the values @xmath65 decay on average faster than the values @xmath66 , until an index is reached where the noise components dominate the solution .",
    "after that point , the coefficients @xmath65 stabilize around the noise level and @xmath66 s continue decreasing , resulting in amplification of errors in the reconstruction .",
    "a sample picard plot is provided in figure  [ fig : picard ] .",
    "since the signal is contained primarily in the subspace spanned by @xmath64 for small @xmath62 and including @xmath64 for larger @xmath62 results in errors , a good value of the regularization parameter @xmath10 should filter out the terms in   for larger values of @xmath62 .",
    "in other words , the filter factors determined by @xmath10 should go to zero to counteract the amplification of the error .",
    "figure  [ fig : picard ] shows filter factors corresponding to the parameter @xmath10 that produces the smallest mean squared errors .    [ cols=\"^,^ \" , ]",
    "in this paper , we described a learning approach to compute optimal regularization parameters for general - form and multi - parameter tikhonov regularization .",
    "we formulated the problem as an empirical bayes risk minimization problem and developed numerical algorithms for computing the optimal or near - optimal regularization parameters .",
    "various error measures were considered .",
    "numerical results illustrate that gsvd filtered solutions give better performance than optimal svd filtered solution , with less training data .",
    "in addition , optimal parameters for multi - parameter tikhonov can provide results comparable to gcv , with less computational cost for a large set of images , and these parameters can be used to reconstruct different types of images .",
    "f.  s.  viloche bazn , l.  s. borges , and j.  b. francisco . on a generalization of regiska s parameter choice rule and its numerical realization in large - scale multi - parameter tikhonov regularization .",
    ", 219(4):21002113 , 2012 .",
    "j.  liu , t.  liu , l.  de  rochefort , j.  ledoux , i.  khalidov , w.  chen , j.  tsiouris , c.  wisnieff , p.  spincemaille , m.  prince , and y.  wang .",
    "morphology enabled dipole inversion for quantitative susceptibility mapping using structural consistency between the magnitude image and the susceptibility map .",
    ", 59(3):25602568 , 2012 ."
  ],
  "abstract_text": [
    "<S> in this work we consider the problem of finding optimal regularization parameters for general - form tikhonov regularization using training data . </S>",
    "<S> we formulate the general - form tikhonov solution as a spectral filtered solution using the generalized singular value decomposition of the matrix of the forward model and a given regularization matrix . </S>",
    "<S> then , we find the optimal regularization parameter by minimizing the average of the errors between the filtered solutions and the true data . </S>",
    "<S> we extend the approach to the multi - parameter tikhonov problem for the case where all the matrices involved are simultaneously diagonalizable . for problems where this is not the case </S>",
    "<S> , we describe an approach to compute optimal or near - optimal regularization parameters by using operator approximations for the original problem . </S>",
    "<S> several tests are performed for 1d and 2d examples using different norms on the errors , showing the effectiveness of this approach .    spectral filtering , regularization , multi - parameter tikhonov , optimal filters , learning approach </S>"
  ]
}