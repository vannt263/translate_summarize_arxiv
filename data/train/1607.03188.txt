{
  "article_text": [
    "the importance of markov chain monte carlo techniques in bayesian inference shows no signs of diminishing . however , despite an industry of elaborations , all commonly used methods are variants on the metropolis - hastings ( mh ) algorithm @xcite and rely on innovations which date back over 60 years . all mh algorithms essentially simulate realisations from a discrete reversible ergodic markov chain with invariant distribution @xmath0 which is ( or is closely related to ) the _ target _ distribution , i.e. the posterior distribution in a bayesian context .",
    "the mh algorithm gives a beautifully simply though flexible recipe for constructing markov chains with the right invariant properties , requiring only local information about @xmath0 ( typically pointwise evaluations of @xmath0 and , in certain implementations of the algorithm , its derivative at the current and proposed new locations ) to complete each iteration .",
    "however new complex modelling and data paradigms are seriously challenging these established methodologies .",
    "firstly , the restriction of traditional mcmc to reversible markov chains is a serious limitation .",
    "it is now well - understood both theoretically @xcite and heuristically @xcite that non - reversible chains offer potentially massive advantages over reversible counterparts .",
    "the need to escape reversibility , and create momentum to aid mixing throughout the state space is certainly well - known , and motivates a number of the most ingenious modern mcmc methods , including the popular hamiltonian mcmc ( hmc , @xcite ) .",
    "inspired by analogy to hamiltonian dynamics , hmc works in discrete time by approximating the trajectories of hamiltonian flow , and using these as proposals within a mh algorithm . whilst the proposals are based on the non - reversible hamiltonian dynamics ,",
    "they resulting mh algorithm turns out to be reversible , albeit on an enlarged state space .    until a recent breakthrough @xcite it has not been possible to construct generic non - reversible mcmc methods .",
    "@xcite introduce a general framework for _ lifted _ markov chains which embeds the distribution of interest in a space of higher dimension , incorporating in addition a velocity component designed to create momentum through the state space and break down _ random - walk_-type behaviour of the chain . in this way it brings to fruition the ideas first postulated and studied in simple cases in @xcite .",
    "a remaining difficulty of the algorithm of @xcite is that the method depends on choosing a quantity which determines the type of momentum generated , the selection of which is non - trivial and of significant influence on the algorithmic efficiency",
    ".    in @xcite , the application of @xcite to a popular model in statistical physics , the curie - weiss model , was analysed , and its high - dimensional limit was shown to behave like a continuous - time piecewise deterministic stochastic process termed the _ zig - zag process_. we shall see that the zig - zag process provides a practically implementable algorithm with some remarkable properties .    a second major obstacle to the application of mcmc for bayesian inference in challenging problems is the need to process potentially massive data - sets",
    ". it can be impractical to carry out large numbers of mh iterations in reasonable time scales .",
    "this has led to a range of alternatives to mh that use sub - samples of the data at each iteration @xcite , or that partition the data into shards , run mcmc on each shard , and then attempt to combine the information from these different mcmc runs @xcite .",
    "however all of these methods introduce some form of approximation error . the final sample will be draws from some approximation to the posterior , and the quality of the approximation can be impossible to evaluate .",
    "this paper introduces the multi - dimensional zig - zag sampling algorithm ( zz ) and its variants ( zz - ss , zz - cv ) .",
    "these methods overcome the restrictions of the lifted markov chain approach of @xcite as they do not depend on the introduction of momentum generating quantities .",
    "it is also amenable to the use of sub - sampling ideas .",
    "the dynamics of the zig zag process depends on the target distribution through the gradient of the logarithm of the target . for bayesian applications",
    "this is a sum , and is easy to estimate unbiasedly using subsampling .",
    "moreover , zig - zag with sub - sampling ( zz - ss ) retains the exactness of the required invariant distribution .",
    "furthermore , if we also use control variate ideas , to reduce the variance of our subsampling estimator of the gradient , the resulting zig zag with control variates ( zz - cv ) algorithm has remarkable _ super - efficient _ scaling properties for large data sets .",
    "we will call an algorithm _ super - efficient _ if it is able to generate independent samples from the target distribution at a higher efficiency than if we would draw from the target distribution at the cost of evaluating all data .",
    "the only situation we are aware of where we can implement super - efficient sampling is with simple conjugate models , where the likelihood function has a low - dimensional summary statistic . in this case , the cost of computing the parameters of the posterior distribution is @xmath1 , where @xmath2 is the number of observations .",
    "once we have performed this pre - computation , we can obtain independent samples from the posterior distribution at a cost of @xmath3 , by using the functional form of the posterior distribution with the pre - computed parameters inserted . in applied statistical settings it is usually not feasible to work with conjugate prior distributions . in these situations ,",
    "standard monte carlo methods require us to evaluate all observations at every iteration , and each iteration will be of @xmath1 . by comparison , zz - cv can be super - efficient , in that it replicates the computational efficiency of working with a conjugate prior distribution : after a pre - computation of @xmath1 , we are able to obtain independent samples at a cost of @xmath3",
    ".    this breakthrough is based upon the zig - zag process , a continuous time piecewise deterministic markov process ( pdmp ) , with trajectories which we will now briefly describe . given a @xmath4-dimensional target density @xmath0 , assumed to be differentiable and positive , zig - zag is a continuous - time non - reversible stochastic process with continuous and piecewise linear trajectories on @xmath5 .",
    "it moves with constant velocity , @xmath6 , until a change of direction event occurs at which one of the velocity components switches sign .",
    "the event time and choice of which direction to reverse is controlled by a collection of state - dependent switching rates , @xmath7 which in turn are constrained via an identity which ensures that @xmath0 is a stationary distribution for the process .",
    "the process intrinsically is constructed in continuous - time , and it can be easily simulated using standard poisson thinning arguments as we shall see in section [ sec : sim ] .    the use of piecewise deterministic markov processes ( pdmps ) such as the zig - zag processes is an exciting and mostly unexplored area in mcmc .",
    "the first occurrence of a pdmp for sampling purposes is in the computational physics literature ( @xcite ) , which in one dimension coincides with the zig - zag process . in @xcite",
    "this method is given the name _ bouncy particle sampler _ , analysed in some detail and extended in several directions .",
    "in multiple dimensions the zig - zag process and bouncy particle sampler ( bps ) are different processes : both are pdmps which move along straight line segments , but the zig - zag process changes direction in only a single component at each switch , whereas the bouncy particle sampler reflects the full direction vector in the level curves of the density function .",
    "as we will see in section  [ sec : ergodicity ] , this difference seems to have a beneficial effect on the ergodic properties of the zig - zag process .",
    "the one - dimensional zig - zag process is analysed in detail in e.g. @xcite .",
    "historically @xcite introduced the zig - zag process with constant switching rates , known since then as the telegraph process .",
    "a continuous - time sequential monte carlo algorithm for scalable bayesian inference with big data ( the scale algorithm ) is given in @xcite , based on the dynamic propagation of weights on a simulated diffusion sample path .",
    "although both scale and zig - zag have similar motivation and are intrinsically continuous - time in their approaches , they are otherwise very different , and both methods have their clear advantages .",
    "one important advantage that zig - zag has over scale is that it avoids the issue of controlling the stability of importance weights .",
    "it also has the advantage of being simpler to implement .",
    "on the other hand the scale algorithm has the property that it is well - adapted for the use of parallel architecture computing , and has particularly simple scaling properties for big data .",
    "we shall structure the paper as follows . in section [ sec : zz ]",
    "we shall introduce the canonical zig - zag property and explore some of its basic properties .",
    "section [ sec : sim ] describes various strategies for implementing the zig - zag in practice , all based around poisson thinning ideas .",
    "the zig - zag is then extended in section [ sec : big ] to the context where the the gradient @xmath8 is intractable but can be readily estimated unbiasedly .",
    "this is applied to the big data context via sub - sampling and an order of magnitude efficiency gain is subsequently achieved by a further control variate modification . in section [",
    "sec : scaling ] we will describe the behaviour of the computing costs of implementing the algorithm ( incorporating both algorithm convergence time and computation costs ) scales with the size of the data set for the zz and zz - ss algorithms .",
    "these results are supported through experiments and examples in section [ sec : examples ] including a favourable comparison with the recently popular stochastic gradient langevin dynamics approximation method for big data mcmc ( @xcite ) .      for a topological space @xmath9",
    "let @xmath10 denote the space of continuous functions on @xmath9 and let @xmath11 denote the borel @xmath12-algebra .",
    "we write @xmath13 .",
    "if @xmath14 is differentiable then @xmath15 denotes the function @xmath16 . throughout this paper",
    "we will work with the topological space @xmath17 , where the topology is the product topology of the euclidean topology on @xmath5 and the discrete topology on @xmath18 .",
    "elements in @xmath19 will often be denoted by @xmath20 with @xmath21 and @xmath22 .",
    "for @xmath23 differentiable in its first argument we will use the shorthand notation @xmath24 to denote the function @xmath25 , @xmath26 .",
    "we will first define the zig - zag process via its generator , before giving an informal description of the process .",
    "we will then describe the dynamics of the process more formally through a general recipe for simulating this continuous - time stochastic process .    for @xmath27 ,",
    "let @xmath28 denote the operation of flipping the @xmath29-th bit in a binary vector @xmath22 , i.e. @xmath30)_i : = \\left\\ { \\begin{array}{ll } \\theta_i \\quad & i \\neq k \\\\                               - \\theta_i \\quad & i = k.                              \\end{array } \\right.\\ ] ] let @xmath31 ; we will refer to @xmath32 as the _ switching rate _ throughout this paper .",
    "define a densely defined operator @xmath33 on @xmath34 by @xmath35 ) - f(\\xi , \\theta ) ) \\right\\ } , \\quad ( \\xi , \\theta ) \\in e,\\ ] ] for @xmath36 such that @xmath37 has compact support and is differentiable for all @xmath22 .",
    "the operator @xmath33 , extended to its maximal domain @xmath38 , is the generator of a piecewise deterministic markov process satisfying the strong markov property ( @xcite ) .",
    "the trajectories will be denoted by @xmath39 and can be described as follows : at random times a single component of @xmath40 flips . in between these switches ,",
    "@xmath41 is linear with @xmath42 .",
    "the rates at which the flips in @xmath40 occur are time inhomogeneous : the @xmath43-th component of @xmath44 switches at rate @xmath45 .      for a given @xmath46",
    ", we may construct a trajectory of @xmath47 of the markov process with generator @xmath33 and initial condition @xmath20 as follows .",
    "* let @xmath48 .",
    "* for @xmath49 * * let @xmath50 , @xmath51 * * for @xmath52 , let @xmath53 be distributed according to @xmath54 * * let @xmath55 and let @xmath56 .",
    "* * let @xmath57 . *",
    "* let @xmath58    this procedure defines a sequence of _ skeleton points _ @xmath59 in @xmath60 , which correspond to the time and position at which the direction of the process changes . the trajectory @xmath61 represents the position of the process at time @xmath62 until time @xmath63 ( ie for @xmath64 ) .",
    "the time until the next skeleton event is characterized as the smallest time of a set of events in @xmath4 simultaneous point processes , where each point process corresponds to switching events of a different component of the velocity .",
    "for the @xmath43-th of these point processes , events occur at rate @xmath65 , and @xmath66 is defined to be the time to the first event for the @xmath43-th component .",
    "the component for which the earliest event occurs is indicated by @xmath67 .",
    "this both defines @xmath68 , the time between the @xmath69th and @xmath29th skeleton point , and the component @xmath67 of the velocity that switches .",
    "the piecewise deterministic trajectories @xmath70 are now obtained as @xmath71    since the switching rates are continuous and hence bounded on compact sets , and since @xmath72 will travel a fixed finite distance within any finite time interval , within any bounded time interval there will be only finitely many switches almost surely .",
    "the above procedure provides a mathematical construction of a markov process with @xmath33 as its generator , as well as ( almost ) an algorithm which simulates this process . the only step in this procedure which presents a computational challenge is the simulation of the random times @xmath73 and a significant part of this paper will consider obtaining these in a numerically efficient way .    in figure",
    "[ fig : basic - examples ] trajectories of the zig - zag process are displayed for a few examples of invariant distributions .",
    "the name of the process is derived by the _ zig - zag _ nature of paths that the process produces .",
    "0.45 -coordinate of the process . in the two - dimensional examples ( c ) and",
    "( d ) , the trajectories in @xmath74 of @xmath75 are plotted.,title=\"fig : \" ]    0.45 -coordinate of the process . in the two - dimensional examples ( c ) and ( d ) , the trajectories in @xmath74 of @xmath75 are plotted.,title=\"fig : \" ]    0.45 -coordinate of the process . in the two - dimensional examples ( c ) and",
    "( d ) , the trajectories in @xmath74 of @xmath75 are plotted.,title=\"fig : \" ]    0.45 -coordinate of the process . in the two - dimensional examples ( c ) and ( d ) ,",
    "the trajectories in @xmath74 of @xmath75 are plotted.,title=\"fig : \" ]      the most important aspect of the zig - zag process is that in many cases the switching rates are directly related to an easily identifiable invariant distribution .",
    "let @xmath76 denote the space of continuously differentiable functions on @xmath5 .",
    "we introduce the following assumption .",
    "[ ass : invariant - measure ] for some function @xmath77 satisfying @xmath78 we have @xmath79 )   = \\theta_i \\partial_i \\psi(\\xi ) \\quad \\mbox{for all } \\",
    "( \\xi,\\theta ) \\in e ,   i = 1 , \\dots , d.\\ ] ]    throughout this paper we will often refer to @xmath80 as the _ negative log density_. let @xmath81 denote the measure on @xmath82 such that , for @xmath83 and @xmath22 , @xmath84 with @xmath85 denoting lebesgue measure on @xmath5 .",
    "[ thm : invariant_measure ] suppose assumption  [ ass : invariant - measure ] holds .",
    "let @xmath86 denote the probability distribution on @xmath19 such that @xmath86 has radon - nikodym derivative given by @xmath87 where @xmath88 .",
    "then the markov process @xmath89 with generator @xmath33 has invariant distribution @xmath86 .    write @xmath90 ) - f(\\xi,\\theta))$ ] , so that @xmath91 .",
    "let @xmath92 .",
    "then for @xmath52 , @xmath93 ) - f(\\xi , \\theta ) ) \\right\\ } \\exp(-\\psi(\\xi ) ) \\ d \\xi \\\\ &   \\\\ & = \\frac 1 z \\sum_{\\theta",
    "\\in \\{-1,+1\\}^d } \\int_{{\\mathbb r}^d } \\left\\ { - \\theta_i \\partial_i \\psi(\\xi )   +   \\lambda_i ( \\xi , f_i[\\theta ] )   - \\lambda_i ( \\xi,\\theta )   \\right\\ } f(\\xi , \\theta ) \\exp(-\\psi(\\xi ) ) \\ d \\xi \\\\ & = 0.\\end{aligned}\\ ] ] hence @xmath94 , which by ( * ? ? ?",
    "* theorem 4.9.17 ) establishes invariance of @xmath86 .",
    "we see that under the invariant distribution of the zig - zag process , @xmath95 and @xmath96 are independent of each other , with @xmath95 having density proportional to @xmath97 and @xmath96 having a uniform distribution on the points in @xmath18 .    for @xmath98 ,",
    "let @xmath99 and @xmath100 denote the positive and negative parts of @xmath101 , respectively .",
    "we will often use the trivial identity @xmath102 without comment .",
    "the following result characterizes the switching rates for which   holds .",
    "[ prop : characterization_lambda ] suppose @xmath103 is continuous .",
    "then assumption  [ ass : invariant - measure ] is satisfied if and only if there exists a continuous function @xmath104 such that for all @xmath52 and @xmath46 , @xmath105)$ ] and , for @xmath77 satisfying  , @xmath106    it is straightforward to verify that if @xmath32 satisfies  , with @xmath107 as specified , then it also satisfies  .",
    "conversely , suppose @xmath32 satisfies   and define @xmath108 then a straightforward computation yields @xmath109 ) = 0 $ ] .",
    "now suppose for some @xmath110 , and @xmath111 , @xmath112 .",
    "first suppose @xmath113 .",
    "then @xmath114 which is in contradiction with the requirement that @xmath115 .",
    "on the other hand , if @xmath116 , then @xmath117 ) = \\gamma_i(\\xi , f_i[\\theta ] ) + 0 = \\gamma_i(\\xi,\\theta ) < 0 $ ] , again a contradiction .",
    "it follows that @xmath118 for all @xmath111 , @xmath110 .",
    "the definition of the zig - zag process can be extended to have different speed in different directions , i.e. with a generator of the form @xmath119 ) - f(\\xi , \\theta ) ) \\right\\ } , \\quad ( \\xi , \\theta ) \\in e , \\varphi \\in   \\mathcal d(l),\\ ] ] where @xmath120 for @xmath52 . in this case @xmath86 as in theorem  [ thm : invariant_measure ] is invariant if and only if @xmath121 ) = a_i \\partial_i \\psi(\\xi).\\ ] ] note that after a rescaling the zig - zag process with generator   is obtained .",
    "we will not consider this additional flexibility in this paper to keep the exposition as simple as possible .",
    "one application of the zig - zag process is as an alternative to mcmc for sampling from posterior distributions in bayesian statistics .",
    "we show here that it is straightforward to derive a class of zig - zag processes that have a given posterior distribution as their invariant distribution .",
    "importantly , the dynamics of the zig - zag process only depend on knowing the posterior distribution up to a constant of proportionality .    to keep notation consistent with that used for the zig - zag process ,",
    "let @xmath21 denote a vector of continuous parameters .",
    "we are given a prior density function for @xmath95 , which we denote by @xmath122 , and observations @xmath123 .",
    "our model for the data defines a likelihood function @xmath124 .",
    "thus the posterior density function is @xmath125 we can write @xmath126 in the form of the previous section , @xmath127 where @xmath128 , and @xmath129 is the unknown normalising constant . now assuming that @xmath130 and @xmath131 are both continuously differentiable with respect to @xmath95 , we have from   that a zig - zag process with rates @xmath132 will have the posterior density @xmath126 as the marginal of its invariant distribution @xmath86 .",
    "we call the zig - zag process with these rates the _ canonical zig - zag process _ for the negative log density @xmath80 .",
    "as explained in proposition  [ prop : characterization_lambda ] , we can construct a family of zig - zag processes with the same invariant distribution by choosing any set of functions @xmath133 , for @xmath134 , which take non - negative values and for which @xmath135)$ ] , and setting @xmath136 the intuition here is that @xmath137 is the rate at which we transition from @xmath96 to @xmath138 $ ] .",
    "the condition @xmath135)$ ] means that we increase by the same amount both the rate at which we will transition from @xmath96 to @xmath138 $ ] and vice versa . as our invariant distribution places the same probability of being in a state with velocity @xmath96 as that of being in state @xmath138 $ ] , these two changes in rate cancel out in terms of their effect on the invariant distribution .",
    "however , changing the rates in this way does impact the dynamics of the process , with larger @xmath139 values corresponding to more frequent changes in the velocity , @xmath96 , of the zig - zag process .",
    "thus intuitively we would expect the resulting process to mix more slowly than the canonical zig - zag process .    under the assumption that the zig - zag process has the desired invariant distribution and that it is ergodic , it follows from the birkhoff ergodic theorem that for any bounded continuous function @xmath140 , @xmath141 for any initial condition @xmath142 .",
    "sufficient conditions for ergodicity will be discussed in the following section .",
    "let us mention that taking @xmath107 to be positive and bounded everywhere , ensures ergodicity of the zig - zag process , as will be established in theorem  [ thm : positive - irreducible ] .",
    "we have established in section  [ sec : invariant - dist ] that for any continuously differentiable , positive density @xmath0 on @xmath5 a zig - zag process can be constructed that has @xmath0 as its marginal stationary density with respect to the spatial coordinate @xmath95 . in order for ergodic averages",
    "@xmath143 of the zig - zag process to converge asymptotically to @xmath144 , we further require @xmath70 to be ergodic , i.e. to admit a _",
    "unique _ invariant distribution .",
    "this section addresses ergodicity of the zig - zag process and is independent of other sections so can be skipped if desired .",
    "the issue of ergodicity is directly related to the requirement that @xmath70 is irreducible , i.e. the state space is not reducible into components which are each invariant for the process @xmath70 . for the one - dimensional zig - zag process , ( exponential ) ergodicity",
    "has already been established under mild conditions ( @xcite ) .",
    "as we will discuss below , irreducibility and thus ergodicity can be established for large classes of multi - dimensional target distributions , such as i.i.d .",
    "gaussian distributions , and also if the switching rates @xmath137 are positive for all @xmath52 , and @xmath46 .",
    "let @xmath145 denote the transition kernel of the zig - zag process with initial condition @xmath146 , i.e. @xmath147 a function @xmath140 is called _ norm - like _ if @xmath148 for all @xmath22 .",
    "let @xmath149 denote the total variation norm on the space of signed measures .",
    "first we consider the one - dimensional case .",
    "[ ass : exp - ergodicity-1d ] suppose @xmath150 and there is a constant @xmath151 such that    * @xmath152 , and * @xmath153 .",
    "* theorem 5 ) [ prop : ergodicity-1d ] suppose assumption  [ ass : exp - ergodicity-1d ] holds .",
    "then there exists a function @xmath154 which is norm - like such that the zig - zag process is @xmath155-exponentially ergodic , i.e. there exists a constant @xmath156 and @xmath157 such that @xmath158    [ ex : gaussian - ergodic ] as an example of fundamental importance , which will also be used in the proof of theorem  [ thm : positive - irreducible ] , consider a one - dimensional gaussian distribution . for simplicity",
    "let @xmath126 be centred , @xmath159 for some @xmath160 .",
    "according to   the switching rates take the form @xmath161 as long as @xmath107 is bounded from above , assumption  [ ass : exp - ergodicity-1d ] is satisfied . in particular",
    "this holds if @xmath107 is equal to a non - negative constant .",
    "we say a probability density function @xmath0 is of _ product form _ if @xmath162 , where @xmath163 are one - dimensional probability density functions .",
    "one of the key properties of the zig - zag process is that when its target density is of product form it can be seen as a product of independent zig - zag processes . in this case",
    "the negative log density is of the form @xmath164 and hence the switching rate for the @xmath43-th component of @xmath96 is given by @xmath165 as long as @xmath166 , i.e. if @xmath167 only depends on the @xmath43-th coordinate of @xmath95 , the switching rate of coordinate @xmath43 is independent of the other coordinates @xmath168 , @xmath169 .",
    "it follows that the switches of the @xmath43-th coordinate can be generated by a one - dimensional time inhomogeneous poisson process , which is independent of the switches in the other coordinates . as a consequence",
    "the @xmath4-dimensional zig - zag process @xmath170 is equal to the tensor product of @xmath4 zig - zag processes @xmath171 , @xmath52 .",
    "suppose @xmath172 is the transition kernel of a markov chain on a state space @xmath19 .",
    "we say that the markov chain associated to @xmath173 is _ mixing _ if there exists a probability distribution @xmath0 on @xmath19 such that @xmath174 for any continuous time markov process with family of transition kernels @xmath175 we can consider the associated _ time - discretized process _ , which is a markov chain with transition kernel @xmath176 for a fixed @xmath177 .",
    "the value of @xmath178 will be of no significance in our use of this construction .",
    "[ prop : product - mixing ] suppose @xmath0 is of product form and @xmath179 admits the representation   with @xmath167 only depending on @xmath180 for @xmath52 .",
    "furthermore suppose that for every @xmath52 , the one - dimensional time - discretized zig - zag process corresponding to switching rate @xmath181 is mixing in @xmath182 .",
    "then the time - discretized @xmath4-dimensional zig - zag process with switching rates @xmath183 is mixing .",
    "in particular , the multi - dimensional zig - zag process admits a unique invariant distribution in @xmath184 .    this is a direct consequence of the decomposition of the @xmath4-dimensional zig - zag process as @xmath4 one - dimensional zig - zag processes and lemma  [ lem : tv - convergence - product ] , which may be found in the appendix .",
    "[ ex : gaussian - multidimensional ] as a continuation of example  [ ex : gaussian - ergodic ] , consider the simple case in which @xmath0 is of product form with each @xmath185 a centered gaussian density function with variance @xmath186 .",
    "it follows from proposition  [ prop : product - mixing ] and example  [ ex : gaussian - ergodic ] that the multi - dimensional canonical zig - zag process ( i.e. the zig - zag process with @xmath187 ) is mixing , or more generally for any @xmath107 which is bounded from above and which satisfies the condition @xmath188 .",
    "this is fundamentally different from the bouncy particle sampler ( @xcite ) , which is not ergodic for an i.i.d .",
    "gaussian without ` refreshments ' of the momentum variable , i.e. resampling the momentum at exponentially distributed times .",
    "according to the following result , if the switching rates are strictly positive , the zig - zag process is ergodic .",
    "[ thm : positive - irreducible ] suppose @xmath189 , in particular @xmath137 is positive for all @xmath190 and @xmath46 .",
    "then there exists at most a single invariant measure for the zig - zag process with switching rate @xmath32 .",
    "the proof of this result consists essentially of a girsanov change of measure with respect to a zig - zag process targetting an i.i.d .",
    "standard normal distribution , which we know to be irreducible .",
    "the irreducibility then carries over to the zig - zag process with the stated switching rates .",
    "a detailed proof can be found in the appendix .    based on numerous experiments",
    ", we conjecture that the canonical multi - dimensional zig - zag process , i.e. with switching rates identical to zero on large parts of the state space , is ergodic in general ( i.e. not only for product distributions ) under only mild conditions , possibly just the stated assumptions for invariance of @xmath0",
    ". a detailed investigation of ergodicity of the zig - zag process will be the subject of a forthcoming paper .",
    "as mentioned earlier , the main computational challenge is an efficient simulation of the random times @xmath191 introduced in section  [ sec : construction ] . we will focus on simulation by means of poisson thinning .",
    "[ prop : poisson - thinning ] let @xmath192 and @xmath193 be continuous such that @xmath194 for @xmath51 .",
    "let @xmath195 be the increasing finite or infinite sequence of points of a poisson process with rate function @xmath196 .",
    "for all @xmath43 , delete the point @xmath197 with probability @xmath198 .",
    "then the remaining points @xmath199 form a non - homogeneous poisson process with rate function @xmath200 .    now for a given initial point @xmath142 , let @xmath201 , for @xmath52 , and suppose we have available continuous functions @xmath202 such that @xmath203 for @xmath52 and @xmath51 .",
    "we call these @xmath204 _ computational bounds _ for @xmath205",
    ". we can use proposition  [ prop : poisson - thinning ] to obtain the first switching times @xmath206 from a ( theoretically infinite ) collection of _ proposed switching times _ @xmath207 given the initial point @xmath20 , and use the obtained skeleton point at time @xmath208 as a new initial point ( which is allowed by the strong markov property ) with the component @xmath209 of @xmath96 switched .",
    "in fact , the strong markov property of the zig - zag process simplifies the computational procedure even further : we can draw for each component @xmath52 the first proposed switching time @xmath210 , determine @xmath211 and decide whether the appropriate component of @xmath96 is switched at this time with probability @xmath212 , where @xmath213 .",
    "then since @xmath214 is a stopping time for the markov process , we can use the obtained point of the zig - zag process at time @xmath214 as new starting point , regardless of whether we switch a component of @xmath96 at the obtained skeleton point . a full computational procedure for simulating the zig - zag process",
    "is now given by algorithm  [ alg : general ] .",
    "input : initial condition @xmath142 .",
    "+ output : a sequence of skeleton points @xmath215 .",
    "1 .   @xmath216 .",
    "2 .   for @xmath49 1 .   define @xmath217 for @xmath51 and @xmath52 .",
    "2 .   for @xmath52 ,",
    "let @xmath218 denote computational bounds for @xmath219 .",
    "3 .   draw @xmath220 such that @xmath221 .",
    "4 .   @xmath222 and @xmath213 .",
    "5 .   @xmath223 6 .   with probability @xmath212 ,",
    "* @xmath224 $ ] , + otherwise * @xmath225 .",
    "we now come to the important issue of obtaining computational bounds for the zig - zag proces , i.e. useful upper bounds for the switching rates @xmath219 .",
    "if we can compute the inverse function @xmath226 of @xmath227 , we can simulate @xmath220 using the cdf inversion technique , i.e. by drawing i.i.d .",
    "uniform random variables @xmath228 and setting @xmath229 , @xmath230 .",
    "let us ignore the subscript @xmath43 for a moment .",
    "important examples of computational bounds are piecewise affine bounds of the form @xmath231 , with @xmath232 , and the constant upper bounds @xmath233 for @xmath234 .",
    "( trivially , the simulated random time is @xmath235 for @xmath236 . )",
    "it is also possible to simulate using the combined rate @xmath237 . in all of these cases ,",
    "the integrals @xmath238 are piecewise linear or quadratic and non - decreasing , so that an explicit expression for the inverse function @xmath239 can be obtained and is straightforward to implement .",
    "the computational bounds are directly related to the algorithmic efficiency of zig - zag sampling . from algorithm  [ alg : general ] , it is clear that for every simulated time @xmath214 a single component of @xmath32 needs to be evaluated , which corresponds by   to the evaluation of a single component of the gradient of the negative log density @xmath80 .",
    "the magnitude of the computational bounds @xmath218 will determine how far the zig - zag process will have moved in the state space before such a new evaluation of a component of @xmath32 is required , and in this paper we will pay close attention to the scaling of @xmath240 with respect to the the number of available observations in a bayesian inference setting .",
    "if there are constants @xmath241 such that @xmath242 , @xmath230 , then we can use the global upper bounds @xmath243 for @xmath51 . indeed , for @xmath142 , @xmath244 in this case algorithm  [ alg : general ] may be applied , letting @xmath245 for @xmath52 at every iteration .",
    "this particularly simple situation arises for example with heavy - tailed distributions .",
    "e.g. if @xmath0 is cauchy , then @xmath246 , and consequently @xmath247 .      a case which will often recur is the situation",
    "in which there exists a positive definite matrix @xmath248 such that @xmath249 for every @xmath21 . here",
    "@xmath250 denotes the hessian matrix of @xmath80 .",
    "this is illustrated graphically in figure  [ fig : hessian - bounds ] . in this case",
    "we can obtain a piecewise affine computational bound , as follows .",
    "denote the euclidean inner product in @xmath5 by @xmath251 . for @xmath252",
    "$ ] the @xmath253-norm on @xmath5 and the induced matrix norms are both denoted by @xmath254 . for symmetric matrices @xmath255 we write @xmath256 if @xmath257 for every @xmath258 , or in words , if @xmath259 dominates @xmath260 in the positive definite ordering .",
    "we let @xmath261 denote the canonical basis vectors in @xmath5 .    for an initial value @xmath142 , we move along the trajectory @xmath262 .",
    "let @xmath263 denote an upper bound for @xmath264 , @xmath265 and let @xmath266 .",
    "note that for a general symmetric matrices @xmath267 for which @xmath256 , we have for any @xmath268 that @xmath269 applying this inequality we obtain for @xmath52 , @xmath270 it thus follows that @xmath271 hence the general zig - zag algorithm may be applied taking @xmath272 with @xmath263 and @xmath273 as specified above . a complete procedure for zig - zag sampling for a log density with dominated hessian is provided in algorithm  [ alg : dominated - hessian ] .",
    "input : initial condition @xmath142 .",
    "+ output : a sequence of skeleton points @xmath215 .",
    "2 .   @xmath274 , @xmath52 .",
    "3 .   @xmath275 , @xmath26 .",
    "4 .   for @xmath276 1 .   draw @xmath277 such that @xmath278 , @xmath52 .",
    "2 .   @xmath279 and @xmath213 .",
    "@xmath280 4 .",
    "@xmath281 , @xmath52 .",
    "5 .   with probability @xmath282 ,",
    "* @xmath224 $ ] + otherwise * @xmath283 .",
    "@xmath284 ( re - using the earlier computation )    we could also have applied the inequality   to obtain the estimate @xmath285 which would have resulted in the choice @xmath286 . however , the scaling of this computational bound is typically of larger magnitude : a single component of @xmath287 is @xmath288 ( assuming elements of @xmath289 are @xmath3 ) , and therefore taking the vector norm results in a complexity @xmath290 .",
    "in contrast , the size of the upper bound using @xmath291 is only @xmath288 .",
    "throughout this section we assume the derivatives of @xmath80 admit the representation @xmath292 with @xmath293 functions in @xmath294 .",
    "the motivation for considering such a class of density functions is the problem of sampling from a posterior distribution for big data .",
    "the key feature of such posteriors is that they can be written as the product of a large number of terms .",
    "for example consider the simplest example of this , where we have @xmath2 independent data points and for which the likelihood function is @xmath295 for some probability density or probability mass function @xmath155 . in this case",
    "we can write the negative log density @xmath80 associated with the posterior distribution as an average @xmath296 where @xmath297 , and we could choose @xmath298 .",
    "it is crucial that every @xmath299 is a factor @xmath1 cheaper to evaluate than the full derivative @xmath300 .",
    "we will describe two successive improvements over the basic zig - zag sampling ( zz ) algorithm specifically tailored to the situation in which   is satisfied .",
    "the first improvement consists of a sub - sampling approach that means we need to calculate only one of the @xmath301 at each simulated time , rather than sum of all @xmath2 of these quantities .",
    "this sub - sampling approach ( referred to as zig - zag with sub - sampling , zz - ss ) comes at the cost of an increased computational bound .",
    "our second improvement is to use control variate ideas to reduce this bound , resulting in the zig - zag with control variates ( zz - cv ) algorithm .",
    "let @xmath302 denote a linear trajectory originating in @xmath46 , i.e. @xmath303 . define a collection of switching rates along the trajectory @xmath304 by @xmath305 we will make use of computational bounds @xmath218 as before , which this time bound @xmath306 uniformly .",
    "more specifically , let @xmath307 be continuous and satisfy @xmath308 we will generate random times according to the computational upper bounds @xmath218 as before . however , we now use a two - step approach to deciding whether to switch or not at the generated times . as before , for @xmath52 let @xmath309 be simulated random times for which @xmath310 and let @xmath311 , and @xmath213 .",
    "then switch component @xmath67 of @xmath96 with probability @xmath312 , where @xmath313 is drawn uniformly at random , independent of @xmath214 .",
    "this ` sub - sampling ' procedure is provided in pseudo - code in algorithm  [ alg : sub - sampling ] .",
    "depending on the choice of @xmath314 , we will refer to his algorithm as zig - zag with sub - sampling ( zz - ss , section  [ sec : sub - sampling - global - bound ] ) or zz - cv ( section  [ sec : sub - sampling - control - variates ] ) .",
    "[ thm : subsampling - rate ] algorithm  [ alg : sub - sampling ] generates a skeleton of a zig - zag process with switching rates given by @xmath315 and invariant distribution @xmath86 given by  .",
    "conditional on @xmath214 , the probability that component @xmath67 of @xmath96 is switched at time @xmath214 is seen to be @xmath316 = \\frac{\\frac 1 n \\sum_{j=1}^n m_{i_0}^j(\\tau)}{m_{i_0}(t ) } = \\frac{m_{i_0}(\\tau)}{m_{i_0}(\\tau)},\\ ] ] where @xmath317 by proposition  [ prop : poisson - thinning ] we thus have an effective switching rate @xmath181 for switching the @xmath43-th component of @xmath96 given by  .",
    "finally we verify that the switching rates @xmath183 given by   satisfy  .",
    "indeed , @xmath318 ) & = \\frac 1 n \\sum_{j=1}^n \\left\\ { \\left ( \\theta_i e_i^j(\\xi ) \\right)^+ -   \\left ( \\theta_i e_i^j(\\xi ) \\right)^- \\right\\ } \\\\ & = \\frac 1 n \\sum_{j=1}^n \\theta_i e_i^j(\\xi ) = \\theta_i \\partial_i \\psi(\\xi ) .",
    "\\end{aligned}\\ ] ] by theorem  [ thm : invariant_measure ] , it follows that the associated zig - zag process has the stated invariant distribution .",
    "input : initial condition @xmath142 .",
    "+ output : a sequence of skeleton points @xmath215 .",
    "1 .   @xmath216 .",
    "2 .   for @xmath49 1 .   define @xmath319 for @xmath51 , @xmath52 and @xmath320 .",
    "2 .   for @xmath52 ,",
    "let @xmath218 denote computational bounds for @xmath306 , i.e. satisfying  .",
    "3 .   draw @xmath220 such that @xmath321 .",
    "4 .   @xmath322 and @xmath323 .",
    "5 .   @xmath223 6 .",
    "draw @xmath324 .",
    "7 .   with probability @xmath325 ,",
    "* @xmath224 $ ] , + otherwise * @xmath225 .",
    "the important advantage of using zig - zag in combination with sub - sampling is that at every iteration of the algorithm we only have to evaluate a single component of @xmath314 , which reduces algorithmic complexity by a factor @xmath1 .",
    "however this may come at a cost .",
    "firstly , the computational bounds @xmath218 may have to be increased which in turn will increase the algorithmic complexity of simulating the zig - zag sampler . also , the dynamics of the zig - zag process will change , because the actual switching rates of the process are increased .",
    "this increases the diffusivity of the continuous time markov process , and affects the mixing properties in a negative way .",
    "a straightforward application of sub - sampling is possible if we can write @xmath326 such that @xmath327 are globally bounded , i.e. there exist positive constants @xmath328 such that @xmath329 in this case we may take @xmath330 so that   is satisfied .",
    "the corresponding version of algorithm  [ alg : sub - sampling ] will be called zig - zag with sub - sampling ( zz - ss ) .",
    "suppose again that @xmath80 admits the representation  , and further suppose that the derivatives @xmath331 are globally and uniformly lipschitz , i.e. , there exist constants @xmath332 such that for some @xmath333 $ ] and all @xmath52 , @xmath320 , and @xmath334 , @xmath335 to use these lipschitz bounds we need to choose a reference point @xmath336 in @xmath95-space , so that we can bound the derivative of the log density based on how close we are to this reference point .",
    "now if we choose any fixed reference point , @xmath337 , we can use a control variate idea to write @xmath338 , \\quad \\xi \\in { \\mathbb r}^d , \\quad i = 1 , \\dots , d.\\ ] ] this suggests using @xmath339 the reason for defining @xmath340 in this manner is to try and reduce the variability of the value of these terms as we vary @xmath341 . by the lipschitz condition we have @xmath342 , and",
    "thus the variability of the @xmath340s will be small if @xmath95 is close to @xmath336 .",
    "the first term for @xmath299 suggests the reference point @xmath336 should be close to the mode of the posterior . under standard asymptotics",
    "we expect a draw from the posterior for @xmath95 to be @xmath343 from the posterior mode .",
    "thus if we have a procedure for finding a reference point @xmath336 which is within @xmath344 of the posterior mode then this would ensure @xmath345 is @xmath344 if @xmath95 is drawn from the posterior .",
    "for such a choice of @xmath336 we would have @xmath346 of @xmath347 .    using the lipschitz condition ,",
    "we can now obtain computational bounds of @xmath219 for a trajectory @xmath348 originating in @xmath20 .",
    "define @xmath349 where @xmath350 and @xmath351 . then   is satisfied .",
    "indeed , using lipschitz continuity of @xmath352 , @xmath353    implementing this scheme requires some pre - processing of the data .",
    "first we need a way of choosing a suitable reference point @xmath336 to find a value close of the mode using an approximate or exact numerical optimization routine",
    ". the complexity of this operation will be @xmath1 .",
    "once we have found such a reference point we have an one - off @xmath1 cost of calculating @xmath346 for each @xmath134 . however , once we have paid this upfront computational cost , the resulting zig - zag sampler can be super - efficient .",
    "this is discussed in more detail in section  [ sec : scaling ] , and demonstrated empirically in section  [ sec : examples ] .",
    "the version of algorithm  [ alg : sub - sampling ] resulting from this choice of @xmath314 and @xmath240 will be called zig - zag with control variates ( zz - cv ) .",
    "when choosing @xmath354 , in general there will be a trade - off between the magnitude of @xmath355 and of @xmath356 , which may influence the scaling of zig - zag sampling with dimension .",
    "for example , we will see in section  [ sec : gaussian ] that for i.i.d .",
    "gaussian components , the choice @xmath357 is optimal . when the situation is not so clear , choosing the euclidean norm ( @xmath358 ) is as reasonable as any other choice",
    "in this section we provide an informal argument for how ( i ) canonical zig - zag , and ( ii ) zig - zag with control variates and sub - sampling , behave for big data .    for the moment we fix @xmath359 .",
    "we will consider a posterior with negative log density @xmath360 where @xmath361 are i.i.d .",
    "drawn from @xmath362 .",
    "let @xmath363 denote the maximum likelihood estimator ( mle ) for @xmath95 based on data @xmath364 .",
    "introduce the coordinate transformation @xmath365    as @xmath366 the posterior distribution in terms of @xmath367 will converge to a multivariate gaussian distribution with mean 0 and covariance matrix given by the inverse of the expected information @xmath368 ; see e.g. @xcite .      first let us obtain a taylor expansion of the switching rate for @xmath95 close to @xmath369 .",
    "we have @xmath370 the first term vanishes by the definition of the mle . expressed in terms of @xmath367 , the switching rates are @xmath371 with respect to the coordinate @xmath367 , the canonical zig - zag process has constant speed @xmath372 in each coordinate , and by the above computation , a switching rate of @xmath373 . after a rescaling of the time parameter by a factor @xmath372 ,",
    "the process in the @xmath367-coordinate becomes a zig zag process with unit speed in every direction and switching rates @xmath374 if we let @xmath375 , the switching rates converge almost surely to those of a zig zag process with switching rates @xmath376 where @xmath368 denotes the expected information .",
    "these switching rates correspond to the limiting gaussian distribution with covariance matrix @xmath377 .    in this limiting zig - zag process",
    ", all dependence on @xmath2 has vanished .",
    "starting from equilibrium , we require a time interval of @xmath3 ( in the rescaled time ) to obtain an essentially independent sample . going back to the original time scale",
    ", this corresponds to a time interval of @xmath344 .",
    "as long as the computational bound in the zig - zag algorithm is @xmath378 , this can be achieved using @xmath3 proposed switches .",
    "the computational cost for every proposed switch is @xmath1 , because the full data @xmath379 needs to be processed in the computation of the true switching rate at the proposed switching time .",
    "_ we conclude that the computational complexity of the zig - zag ( zz ) algorithm per independent sample is @xmath1 , provided that the computational bound is @xmath378 .",
    "_ this is the best we can expect for any standard monte carlo algorithm ( where we will have a @xmath3 number of iterations , but each iteration is @xmath1 in computational cost ) .    to compare , if the computational bound is @xmath380 for some @xmath381 , then we require @xmath382 proposed switches before we have simulated a total time interval of length @xmath344 , so that , with a complexity of @xmath1 per proposed switching time , the zig - zag algorithm has total computational complexity @xmath383 .",
    "so , for example , with global bounds we have that the computational bound is @xmath1 ( as each term in the log density is @xmath3 ) , and hence zz will have total computational complexity of @xmath384 .",
    "consider algorithm  [ alg : dominated - hessian ] in the one - dimensional case , with the second derivative of @xmath80 bounded from above by @xmath385 .",
    "we have @xmath386 as @xmath387 is the sum of @xmath2 terms of @xmath3 .",
    "the value of @xmath388 is kept fixed at the value @xmath389 .",
    "next @xmath101 is given initially as @xmath390 and increased by @xmath391 until a switch happens and @xmath101 is reset to @xmath392 .",
    "because of the initial value for @xmath101 , switches will occur at rate @xmath378 so that @xmath214 will be @xmath344 , and the value of @xmath101 will remain @xmath378 .",
    "hence the magnitude of the computational bound @xmath393 is @xmath378 .",
    "now we will study the limiting behaviour as @xmath375 of zz - cv introduced in section  [ sec : sub - sampling - control - variates ] . in determining the computational bounds we take @xmath358 for simplicity , e.g. in  .",
    "also for simplicity assume that @xmath394 has lipschitz constant @xmath395 ( independent of @xmath320 ) and write @xmath396 , so that   is satisfied . in practice",
    "there may be a logarithmic increase with @xmath2 in the lipschitz constants @xmath395 as we have to take a global bound in @xmath2 , see e.g.   in section  [ sec : logistic ] .",
    "for the present discussion we ignore such logarithmic factors .",
    "we assume reference points @xmath336 for growing @xmath2 are determined in such a way that @xmath397 is @xmath344 .",
    "for definiteness , suppose there exists a @xmath4-dimensional random variable @xmath398 such that @xmath399 in distribution , with the randomness in @xmath398 independent of @xmath400 .",
    "we can look at cv - zz with respect to the scaled coordinate @xmath367 as @xmath401 .",
    "denote the reference point for the rescaled parameter as @xmath402 .",
    "the essential quantities to consider are the switching rate estimators @xmath299 .",
    "we estimate @xmath403 we find that @xmath404 under the stationary distribution .    by slowing down the zig - zag process in @xmath367 space by @xmath372 ,",
    "the continuous time process generated by zz - cv will approach a limiting zig - zag process with a certain switching rate of @xmath3 . in general",
    "this switching rate will depend on the way that @xmath336 is obtained . to simplify the exposition , in the following computation we assume @xmath405 .",
    "rescaling by @xmath406 , and developing a taylor approximation around @xmath369 , @xmath407 by theorem  [ thm : subsampling - rate ] , the rescaled effective switching rate for zz - cv is given by @xmath408 where @xmath409 denotes expectation with respect to @xmath9 , with density @xmath410 , and the convergence is a consequence of the law of large numbers .",
    "if @xmath336 is not exactly equal to @xmath369 , the limiting form of @xmath411 will be different , but the important point is that it will be @xmath3 , which follows from the bound on @xmath412 above .",
    "just as with zz , the rescaled zig - zag process underlying zz - cv converges to a limiting zig - zag process with switching rate @xmath413 .",
    "since the computational bounds of zz - cv are @xmath378 , a completely analogous reasoning to the one for zz algorithm above ( section  [ sec : scaling - zz ] ) leads to the conclusion that @xmath3 proposed switches are required to obtain an independent sample . however , in contrast with the zz - algorithm , the zz - cv algorithm is designed in such a way that the computational cost per proposed switch is @xmath3 .",
    "_ we conclude that the computational complexity of the zz - cv algorithm is @xmath3 per independent sample .",
    "_ this provides a factor @xmath2 increase in efficiency over standard mcmc algorithms , resulting in an _ unbiased _",
    "algorithm for which _ the computational cost of obtaining an independent sample does not depend on the size of the data_.      the arguments above assume we are at stationarity  and how quickly the two algorithms converge is not immediately clear .",
    "note however that for sub - sampling zig - zag it is possible to choose the reference point @xmath336 as starting point , thus avoiding much of the issues about convergence .    in some sense",
    ", the good computational scaling of zz - cv is leveraging the asymptotic normality of the posterior , but in such a way that zz - cv always samples from the true posterior .",
    "thus when the posterior is close to gaussian it will be quick ; when it is far from gaussian it may well be slower but will still be `` correct '' .",
    "this is fundamentally different from other algorithms ( e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ) that utilise the asymptotic normality in terms of justifying their approximation to the posterior .",
    "such algorithms are accurate if the posterior is close to gaussian , but may be inaccurate otherwise , and it is often impossible to quantify the size of the approximation in practice .",
    "there are essentially two different ways of using the zig - zag skeleton points which we obtain by using e.g. algorithms  [ alg : general ] , [ alg : dominated - hessian ] , or [ alg : sub - sampling ] .",
    "the first possible approach is to collect a number of samples along the trajectories . for , this",
    "suppose we have simulated the zig - zag process up to time @xmath414 , and we wish to collect @xmath29 samples .",
    "this can be achieved by setting @xmath415 , and setting @xmath416 for @xmath417 , with the continuous time trajectory @xmath418 defined as in section  [ sec : construction ] .",
    "this approach offers a straightforward way to compare with discrete time mcmc algorithms .",
    "effectively , the continuous time zig - zag process determined by the family of transition kernels @xmath419 is transformed into a discrete time markov chain with transition kernel @xmath420 . in order to approximate @xmath144 numerically for some function @xmath421 of interest",
    ", we can use the usual ergodic average @xmath422 an issue with this approach is that we have to decide on the amount of samples we wish to use . taking the number of samples of the same order as",
    "the number of switches made along the zig - zag trajectory is a good rule of thumb .",
    "it is important that one does not make the mistake of using the switching points of the zig zag process as samples , as these points are not distributed according to @xmath0 . in particular , the switching points are biased towards the tails of the target distribution .",
    "an alternative approach is intrinsically related to the continuous time and piecewise linear nature of the zig - zag trajectories .",
    "this approach consists of continuous time integration of the zig - zag process , which can in many cases be performed exactly . by the continuous time",
    "ergodic theorem , for @xmath155 as above , @xmath144 can be estimated as @xmath423 since the output of the zig - zag algorithms consists of a finite number of skeleton points @xmath424 , we can express this as @xmath425 due to the piecewise linearity of @xmath41 , in many cases these integrals can be computed exactly , e.g. for the moments , @xmath426 , @xmath427 . in cases",
    "where the integral can not be computed exactly , numerical quadrature rules can be applied .",
    "an advantage of this method is that we do not have to make an arbitrary decision on the number of samples to extract from the trajectory .      in order to perform numerical experiments",
    "we compute , for an obtained continuous time zig - zag trajectory @xmath70 , the associated effective sample size ( ess ) corresponding to a continuous observable @xmath14 .",
    "we say that the central limit theorem ( clt ) holds for @xmath428 if , as @xmath429 , the distribution of @xmath430 converges in distribution to a centred normal distribution with variance @xmath431 , called the _ asymptotic variance_. the asymptotic variance can be estimated by dividing an obtained trajectory @xmath432 into @xmath433 intervals ( `` batches '' ) of length @xmath434 . under the assumption that the batch are sufficiently large",
    ", we have that @xmath435 has approximately a @xmath436 distribution , for @xmath437 . making the further approximating assumption that the random variables @xmath438 are independent ( which is reasonable if the batches themselves are sufficiently long ) , we may estimate @xmath431 as the sample variance of @xmath439 , i.e. we use the estimator @xmath440 with @xmath441 .",
    "we also estimate the mean and variance of @xmath442 under @xmath0 by @xmath443 which converge almost surely as @xmath444 to the true mean and variance under the condition that the zig - zag process is ergodic . the estimate for effective sample size",
    "is now given as @xmath445      we use the term _ epoch _ as a unit of computational cost , corresponding to the number of iterations required to evaluate the complete gradient of @xmath446 .",
    "this means that for the basic zig - zag algorithm ( without sub - sampling ) , an epoch consists of exactly one iteration , and for the sub - sampled variants of the zig - zag algorithm , an epoch consists of @xmath2 iterations .",
    "the cpu running times per epoch of the various algorithms we consider are equal up to a constant factor . to assess the scaling of various algorithms , we use _ ess per epoch_. consider any classical mcmc algorithm based upon the metropolis - hastings acceptance rule . since every iteration requires an evaluation of the full density function to compute the acceptance probability , we have that the ess per epoch for such an algorithm is bounded from above by one .",
    "similar observations apply to all other known mcmc algorithms capable of sampling asymptotically from the exact target distribution .    there",
    "do exist several conceptual innovations based on the idea of sub - sampling , which have some theoretical potential to overcome the fundamental limitation of one ess per epoch sketched above .",
    "we will briefly discuss the two most prominent examples of such methods : the pseudo - marginal method ( pmm , @xcite ) , and stochastic gradient langevin dynamics ( sgld , @xcite ) , which we will consider in more detail .",
    "the pmm is based upon using a positive unbiased estimator for a possibly unnormalized density .",
    "obtaining an unbiased estimator of a product is much more difficult than obtaining one for a sum .",
    "furthermore , it has been shown to be impossible to construct an estimator that is guaranteed to be positive without other information about the product , such as a bound on the terms in the product @xcite .",
    "therefore the pmm does not apply in a straightforward way to vanilla mcmc in bayesian inference and we will not consider the pmm further here .      for notational simplicity we will focus on a 1-dimensional target , though the arguments below apply more generally .",
    "the sgld algorithm consists of stochastic updates of the form @xmath447 where @xmath448 is a sequence of positive step sizes , @xmath449 are independent @xmath450 random variables , and where @xmath451 is an unbiased estimator of @xmath452 for @xmath21 .",
    "in practice , @xmath453 will be constructed using randomly sampled batches of fixed size @xmath454 , @xmath455 where @xmath456 are drawn uniformly without replacement from @xmath457 . under certain conditions , in particular on the decay of the step sizes to 0 as @xmath458 , sgld provides an asymptotically unbiased approximation of the target distribution @xmath0 ; see @xcite for a detailed analysis .",
    "however the monte carlo error of the resulting algorithm decays at a slower rate than for standard mcmc algorithms .    as in section",
    "[ sec : sim ] it is natural to study the behaviour of sgld for a scaled variable , @xmath459 , that converges to a fixed distribution as @xmath366 . with respect to the reparametrization @xmath367",
    ", the updates   correspond to @xmath460 with @xmath461 .",
    "we see that @xmath442 has to scale as @xmath462 in order for the noise to be of @xmath3 in the @xmath367-coordinate",
    ". therefore we let @xmath463 for some @xmath464 .",
    "the error of using the sgld algorithm with a fixed step - size @xmath465 is analysed in @xcite . to first order",
    "the error is governed by the relative sizes of the variance of the estimator of the drift and the variance of the driving noise .",
    "furthermore , it is possible to correct for this error providing the latter variance is greater than the former .",
    "first we calculate the variance of the estimator of the drift .",
    "define @xmath160 by @xmath466 where the variance is with respect to the randomness induced by @xmath467 , drawn uniformly among @xmath457 . then by the expression for the variance for sampling without replacement (",
    "* section 7.3.1 ) , and using @xmath468 , @xmath469 this is @xmath470 . by comparison",
    "the variance of the driving noise is @xmath3 . if we want the former to be less than the latter we will need @xmath471 to be @xmath1 .",
    "that is we will need to sub - sample a fixed proportion of the data at each iteration .",
    "thus the advantage of sgld over a method that does not use sub - sampling can at best be by a constant factor , and sgld can not be super - efficient .",
    "the only potential to develop sgld to be super - efficient would be to substantially reduce the variance of the estimator of the drift , for example by using the control variate idea we use within zz - cv ( see also * ? ? ?",
    "consider the well known toy problem in bayesian statistics of estimating the mean of a gaussian distribution .",
    "this problem has the advantage that it allows for an analytical solution which can be compared with the numerical solutions obtained by zig - zag sampling and other methods .",
    "we assume that conditional on a parameter @xmath21 , independent observations @xmath472 have distribution @xmath473 .",
    "we put a prior distribution @xmath474 on @xmath95 .",
    "this leads to a negative log density @xmath475 we compute @xmath476 and @xmath477 for any trajectory @xmath303 , we have @xmath478 with @xmath479 \\quad \\mbox{and } \\quad   b_i = \\frac 1 { \\rho^2 } + \\frac{n}{\\sigma^2 } , \\quad i = 1 , \\dots , d.\\ ] ] we see that in this case we can construct computational bounds @xmath480 which are exact , so that all proposed switching times will be accepted . the corresponding algorithm will be simply denoted by zz .",
    "since there is no global bound on the switching rate there is no straightforward way to implement the naive sub - sampling method of section  [ sec : sub - sampling - global - bound ] .",
    "however , because the hessian of @xmath80 is constant it is possible to apply the sub - sampling method with control variates of section  [ sec : sub - sampling - control - variates ] .",
    "in fact , because the data has an additive effect on the gradient of @xmath80 , we have for arbitrary @xmath336 that @xmath481 we see that the sub - sampling switching rates are exactly equal to the canonical switching rates , and the two continuous time stochastic processes coincide , once we note that we can pre - compute @xmath482 in the expression for @xmath483 .",
    "however the computational bounds of the two algorithms are not equal , and it will be of interest to see how zig - zag with control variates behaves if @xmath336 is not chosen to be exactly equal to the mode .",
    "the mode of @xmath0 is given by @xmath484 and the one - off cost of computing this quantity is @xmath1 .",
    "alternatively we can choose to use a sub - sampling of @xmath485 to obtain a value @xmath336 close to the posterior mode .",
    "as we require @xmath336 to be within @xmath344 of the mode , the size of such a sample should be at least proportional to @xmath2 . to be specific",
    ", we will consider the case where @xmath336 is determined randomly by @xmath486 where @xmath487 for some constant @xmath488 $ ] , and @xmath489 are drawn randomly without replacement from @xmath490 .",
    "the corresponding zig - zag algorithms are denoted by zz - socv ( sub - optimal control variates , for @xmath336 an approximation to the mode ) , and zz - cv ( for @xmath491 ) .",
    "the constants @xmath492 determining the computational bounds ( as described in section  [ sec : sub - sampling - control - variates ] ) are given by @xmath493 for @xmath52 , regardless of the choice of @xmath333 $ ] .",
    "choosing @xmath357 will give optimal scaling of @xmath263 and @xmath273 with respect to dimension in the computational bound @xmath494 .",
    "in our first numerical experiment , we compare the mean square error ( mse ) for several algorithms , namely basic zig - zag ( zz ) , zig - zag with control variates ( zz - cv ) , zig - zag with `` sub - optimal '' control variates ( zz - socv ) , and stochastic gradient langevin dynamics ( sgld ) . here",
    "basic zig - zag refers to zig - zag , where we pretend that every iteration requires the evaluation of @xmath2 observations ( whereas in practice , we can pre - compute @xmath495 ) .",
    "parameter values are @xmath496 , @xmath497 ( for the true value of the mean parameter ) , @xmath498 ( specifying the gaussian posterior distribution ) and @xmath499 , @xmath500 ( for the sgld parameters ) .",
    "the value of @xmath336 for zz - socv is based on a sub - sample of size @xmath501 .",
    "the mse for the second moment using sgld does not decrease beyond a fixed value , indicating the presence of bias in sgld .",
    "this bias does not appear in the different versions of zig - zag sampling , agreeing with the theoretical result that ergodic averages over zig - zag trajectories are consistent .",
    "furthermore we see a significant relative increase in efficiency for zz-(so)cv over basic zz when the number of observations is increased , agreeing with the scaling results of section  [ sec : scaling ] . in this experiment , the difference in mse between zz - socv and zz - cv is of the same order of magnitude as the relative size of the sub - sample of the data used in computing @xmath336 , i.e. a factor 10 .    0.45   or @xmath502 observations , for a one - dimensional gaussian posterior distribution ( section  [ sec : gaussian ] ) .",
    "displayed are sgld ( green ) , zz - cv ( magenta ) , zz - socv ( dark magenta ) , zz ( black ) .",
    "the displayed dots represent averages over experiments based on randomly generated data from the true posterior distribution.,title=\"fig : \" ]    0.45   or @xmath502 observations , for a one - dimensional gaussian posterior distribution ( section  [ sec : gaussian ] ) . displayed",
    "are sgld ( green ) , zz - cv ( magenta ) , zz - socv ( dark magenta ) , zz ( black ) .",
    "the displayed dots represent averages over experiments based on randomly generated data from the true posterior distribution.,title=\"fig : \" ]    0.45   or @xmath502 observations , for a one - dimensional gaussian posterior distribution ( section  [ sec : gaussian ] ) . displayed",
    "are sgld ( green ) , zz - cv ( magenta ) , zz - socv ( dark magenta ) , zz ( black ) .",
    "the displayed dots represent averages over experiments based on randomly generated data from the true posterior distribution.,title=\"fig : \" ]    0.45   or @xmath502 observations , for a one - dimensional gaussian posterior distribution ( section  [ sec : gaussian ] ) . displayed",
    "are sgld ( green ) , zz - cv ( magenta ) , zz - socv ( dark magenta ) , zz ( black ) .",
    "the displayed dots represent averages over experiments based on randomly generated data from the true posterior distribution.,title=\"fig : \" ]      consider a binary data set @xmath503 , @xmath320 given @xmath4-dimensional covariates @xmath504 , @xmath320 ( with @xmath505 for all @xmath341 ) assumed to come from the logistic regression model , @xmath506 with parameter @xmath21 . for",
    "any given prior probability distribution @xmath507 on the value of @xmath95 , we obtain for the posterior density function @xmath508 for simplicity we assume a flat prior on @xmath21 , i.e. @xmath507 is constant .",
    "the corresponding negative log density function is now given by @xmath509 for the @xmath29-th derivative we find @xmath510 in order to prepare for sub - sampling , we can write @xmath511 , with @xmath512 we compute for @xmath320 , @xmath513 and @xmath514 using the estimate @xmath515 , we find that the global bound   holds with @xmath516 so that we can use the sub - sampling method with a global bound on the switching rate , discussed in section  [ sec : sub - sampling - global - bound ] .",
    "furthermore , using the bound @xmath517 , we have @xmath518 so that we can use the zig - zag algorithm for dominated hessian ( without sub - sampling ) , discussed in section  [ sec : sampling - dominated - hessian ] . finally , using analogous estimates , we find that @xmath519 from which it follows that   is satisfied with @xmath520 enabling the use of the sub - sampling method with control variates , discussed in section  [ sec : sub - sampling - control - variates ] .",
    "if @xmath361 are drawn independently from any ( sub-)gaussian distribution , taking the maximum in   results in @xmath521 , using e.g. ( * ? ? ?",
    "* lemma 5.1 ) .",
    "if on the other hand all @xmath361 are taken ( not necessary independently ) from a bounded set , then trivially @xmath522 .      in this numerical experiment",
    "we compare how the effective sample size per epoch ( esspe ) grows with the number of observations @xmath2 for several zig - zag algorithms .",
    "recall from the discussion in section  [ sec : ess - per - epoch ] that for any mcmc which does not use sub - sampling , the esspe should be equal to a constant smaller than one , and if , it existed , an algorithm able to generate an independent sample by processing all data would have an esspe exactly equal to one .",
    "the results of this experiment are shown in figure  [ fig : ess - per - epoch ] . in both the plots of ess per epoch ( see ( a ) and ( c ) ) ,",
    "the best linear fit for zz - cv has slope approximately 0.95 , which is in close agreement with the scaling analysis of section  [ sec : scaling ] .",
    "the other algorithms have roughly a horizontal slope , corresponding to a linear scaling with the size of the data . as a result , zz - cv is the only algorithm for which the ess per cpu second is approximately constant as a function of the size of the data ( see ( b ) and ( d ) ) .",
    "hence we see zz - cv obtains an esspe which is roughly linearly increasing with the number of observations @xmath2 .",
    "the other versions of the zig - zag algorithms have a esspe which is approximately constant with respect to @xmath2 .",
    "these statements apply regardless of the dimensionality of the problem .    0.45    [ fig : ess - per - epoch ]    0.45    0.45    [ fig : ess - per - epoch ]    0.45",
    "[ lem : tv - convergence - product ] suppose @xmath523 is given as the product of measures , @xmath524 , with @xmath525 probability measures on borel spaces @xmath526 for @xmath527 and fixed @xmath528 , @xmath529 .",
    "suppose for every @xmath52 there exists a measure @xmath185 such that @xmath530",
    ". then @xmath531      it suffices to prove the result for @xmath534 .",
    "write @xmath535 . for @xmath532 , @xmath536 , define @xmath537 .",
    "note that for any such @xmath155 and @xmath527 , @xmath538 with @xmath539 . therefore , as @xmath540 , @xmath541 also , for any @xmath542 , @xmath543 with supremum norm less than or equal to one .",
    "therefore , for all @xmath542 , as @xmath540 , @xmath544 hence by bounded convergence , as @xmath540 , @xmath545 combining   and   gives , for any @xmath546 , @xmath547    a discrete time markov chain in @xmath19 with transition kernel @xmath173 is called @xmath548-irreducible if there exists a non - trivial borel measure @xmath548 on @xmath19 such that , whenever @xmath549 for @xmath550 and @xmath551 , there exists a @xmath527 such that @xmath552 .",
    "[ lem : mixing=>irreducible ] suppose the markov chain on @xmath19 with transition kernel @xmath553 is mixing with respect to its unique invariant probability distribution @xmath0",
    ". then the transition kernel @xmath173 is @xmath0-irreducible .",
    "let @xmath557 denote @xmath4 independent poisson processes , each with constant rate @xmath558 defined on a filtered probability space @xmath559 .",
    "given @xmath142 , define a stochastic processes @xmath560 for @xmath52 and let @xmath561 .",
    "then under @xmath562 , @xmath563 corresponds to a zig - zag process started in @xmath146 with constant switching rate @xmath107 .",
    "denote the transition kernel for this process by @xmath564 .",
    "write @xmath565 . for @xmath46",
    "define a stochastic process @xmath566 on @xmath567 by @xmath568 since @xmath569 for all @xmath570 , @xmath571 , and @xmath46 , it follows that @xmath572 is bounded away from 0 for all @xmath52 and @xmath573 . using this local boundedness property the processes @xmath574 are a.s .",
    "positive martingales . for fixed @xmath146 , the probability measure @xmath575 on @xmath576 , defined by the radon - nikodym derivative @xmath577",
    "is such that under @xmath575 , the processes @xmath578 have time inhomogeneous rate @xmath572 .",
    "let @xmath145 denote the probability distribution of @xmath70 under @xmath579 , and similarly @xmath580 for the distribution under @xmath562 for @xmath51",
    ". then @xmath581,\\ ] ] whence for all @xmath146 and @xmath51 , @xmath582 and @xmath564 are equivalent .",
    "now take @xmath583 to be equal to the switching rates for a standard normal target distribution with excessive switching rate @xmath107 , i.e. @xmath584 and repeat the above construction to obtain transition probabilities @xmath585 .",
    "it follows that the transition probabilities @xmath586 and @xmath587 are equivalent for all @xmath51 and @xmath46 . from proposition",
    "[ prop : product - mixing ] and example  [ ex : gaussian - multidimensional ] it follows that the time discretization of the zig - zag process with transition kernels @xmath588 is mixing . by lemma  [ lem : mixing=>irreducible ] , it follows that the transition kernels @xmath588 correspond to a @xmath548-irreducible process . by the equivalence of the transition kernels @xmath586 and @xmath587",
    ", this property carries over the the zig - zag process with switching rates @xmath32 .",
    "it follows that there can be at most a single unique invariant distribution for the time discretization of the zig - zag process , and this property carries over to the continuous time zig - zag process ."
  ],
  "abstract_text": [
    "<S> standard mcmc methods can scale poorly to big data settings due to the need to evaluate the likelihood at each iteration . </S>",
    "<S> there have been a number of approximate mcmc algorithms that use sub - sampling ideas to reduce this computational burden , but with the drawback that these algorithms no longer target the true posterior distribution . </S>",
    "<S> we introduce a new family of monte carlo methods based upon a multi - dimensional version of the zig - zag process of @xcite , a continuous time piecewise deterministic markov process . </S>",
    "<S> while traditional mcmc methods are reversible by construction ( a property which is known to inhibit rapid convergence ) the zig - zag process offers a flexible non - reversible alternative which we observe to often have favourable convergence properties . </S>",
    "<S> the dynamics of the zig - zag process correspond to a constant velocity model , with the velocity of the process switching at events from a point process . </S>",
    "<S> the rate of this point process can be related to the invariant distribution of the process . </S>",
    "<S> if we wish to target a given posterior distribution , then rates need to be set equal to the gradient of the log of the posterior . </S>",
    "<S> unlike traditional mcmc , we show how the zig - zag process can be simulated without discretisation error , and give conditions for the process to be ergodic . </S>",
    "<S> most importantly , we introduce a sub - sampling version of the zig - zag process that is an example of an _ exact approximate scheme_. that is , if we replace the true gradient of the log posterior with an unbiased estimator , obtained by sub - sampling , then the resulting approximate process still has the posterior as its stationary distribution . furthermore , </S>",
    "<S> if we use a control - variate idea to reduce the variance of our unbiased estimator , then both heuristic arguments and empirical observations show that zig - zag can be super - efficient : after an initial pre - processing step , essentially independent samples from the posterior distribution are obtained at a computational cost which does not depend on the size of the data .    </S>",
    "<S> ,    ; </S>"
  ]
}