{
  "article_text": [
    "the graph matching problem is among the most important challenges of graph processing , and plays a central role in various fields of pattern recognition . roughly speaking ,",
    "the problem consists in finding a correspondence between vertices of two given graphs which is optimal in some sense .",
    "usually , the optimality refers to the alignment of graph structures and , when available , of vertices labels , although other criteria are possible as well .",
    "a non - exhaustive list of graph matching applications includes document processing tasks like optical character recognition @xcite , image analysis ( 2d and 3d ) @xcite , or bioinformatics @xcite .    during the last decades ,",
    "many different algorithms for graph matching have been proposed .",
    "because of the combinatorial nature of this problem , it is very hard to solve it exactly for large graphs , however some methods based on incomplete enumeration may be applied to search for an exact optimal solution in the case of small or sparse graphs .",
    "some examples of such algorithms may be found in @xcite .",
    "another group of methods includes approximate algorithms which are supposed to be more scalable .",
    "the price to pay for the scalability is that the solution found is usually only an approximation of the optimal matching .",
    "approximate methods may be divided into two groups of algorithms .",
    "the first group is composed of methods which use spectral representations of adjacency matrices , or equivalently embed the vertices into a euclidean space where linear or nonlinear matching algorithms can be deployed .",
    "this approach was pioneered by umeyama @xcite , while further different methods based on spectral representations were proposed in @xcite .",
    "the second group of approximate algorithms is composed of algorithms which work directly with graph adjacency matrices , and typically involve a relaxation of the discrete optimization problem .",
    "the most effective algorithms were proposed in @xcite .",
    "an interesting instance of the graph matching problem is the matching of labeled graphs . in that case ,",
    "graph vertices have associated labels , which may be numbers , categorical variables , etc ... the important point is that there is also a similarity measure between these labels .",
    "therefore , when we search for the optimal correspondence between vertices , we search a correspondence which matches not only the structures of the graphs but also vertices with similar labels . some widely used approaches for this application only use the information about similarities between graph labels . in vision , one such algorithm is the shape context algorithm proposed in @xcite , which involves an efficient algorithm of node label construction .",
    "another example is the blast - based algorithms in bioinformatics such as the inparanoid algorithm @xcite , where correspondence between different protein networks is established on the basis of blast scores between pairs of proteins .",
    "the main advantages of all these methods are their speed and simplicity . however , the main drawback of these methods is that they do not take into account information about the graph structure .",
    "some graph matching methods try to combine information on graph structures and vertex similarities , examples of such method are presented in @xcite .",
    "in this article we propose an approximate method for labeled weighted graph matching , based on a convex - concave programming approach which can be applied for matching of graphs of large sizes .",
    "our method is based on a formulation of the labeled weighted graph matching problem as a quadratic assignment problem ( qap ) over the set of permutation matrices , where the quadratic term encodes the structural compatibility and the linear term encodes local compatibilities .",
    "we propose two relaxations of this problem , resulting in one quadratic convex and one quadratic concave minimization problem on the set of doubly stochastic matrices . while the concave relaxation has the same global minimum as the initial qap , solving it is also a hard combinatorial problem .",
    "we find a local minimum of this problem by following a solution path of a family of convex - concave minimization problems , obtained by linearly interpolating between the convex and concave relaxations .",
    "starting from the convex formulation with a unique local ( and global ) minimum , the solution path leads to a local optimum of the concave relaxation .",
    "we refer to this procedure as the path algorithm .",
    "we perform an extensive comparison of this path algorithm with several state - of - the - art matching methods on small simulated graphs and various qap benchmarks , and show that it consistently provides state - of - the - art performances while scaling to graphs of up to a few thousands vertices on a modern desktop computer .",
    "we further illustrate the use of the algorithm on two applications in image processing , namely the matching of retina images based on vessel organization , and the matching of handwritten chinese characters .    the rest of the paper is organized as follows :",
    "section [ sec : pb_description ] presents the mathematical formulation of the graph matching problem . in section [ sec : convex - concave_theory ] , we present our new approach .",
    "then , in section [ sec : num_exp ] , we present the comparison of our method with umeyama s algorithm @xcite and the linear programming approach @xcite on the example of artificially simulated graphs . in section [ sec : qap ] , we test our algorithm on the qap benchmark library , and we compare obtained results with the results published in @xcite for the qbp algorithm and graduated assignment algorithms .",
    "finally , in section [ sec : vision ] we present two examples of applications to real - world image processing tasks .",
    "a graph @xmath0 of size @xmath1 is defined by a finite set of vertices @xmath2 and a set of edges @xmath3 .",
    "we consider only undirected graphs with no self - loop , i.e. , such that @xmath4 and @xmath5 for any vertices @xmath6 .",
    "each such graph can be equivalently represented by a symmetric adjacency matrix @xmath7 of size @xmath8 , where @xmath9 is equal to one if there is an edge between vertex @xmath10 and vertex @xmath11 , and zero otherwise .",
    "an interesting generalization is a weighted graph which is defined by association of nonnegative real values @xmath12 ( weights ) to all edges of graph @xmath13 .",
    "such graphs are represented by real valued adjacency matrices @xmath7 with @xmath14 .",
    "this generalization is important because in many applications the graphs of interest have associated weights for all their edges , and taking into account these weights may be crucial in construction of efficient methods . in the following , when we talk about `` adjacency matrix '' we mean a real - valued `` weighted '' adjacency matrix",
    ".    given two graphs @xmath13 and @xmath15 with the same number of vertices @xmath1 , the problem of matching @xmath13 and @xmath15 consists in finding a correspondence between vertices of @xmath13 and vertices of @xmath15 which aligns @xmath13 and @xmath15 in some optimal way .",
    "we will consider in section [ sec : dummy_nodes ] an extension of the problem to graphs of different sizes .",
    "for graphs with the same size @xmath1 , the correspondence between vertices is a permutation of @xmath1 vertices , which can be defined by a permutation matrix @xmath16 , i.e. , a @xmath17-valued @xmath18 matrix with exactly one entry @xmath19 in each column and each row .",
    "the matrix @xmath16 entirely defines the mapping between vertices of @xmath13 and vertices of @xmath15 , @xmath20 being equal to @xmath19 if the @xmath10-th vertex of @xmath13 is matched to the @xmath11-th vertex of @xmath15 , and @xmath21 otherwise . after applying the permutation defined by @xmath16 to the vertices of @xmath15",
    "we obtain a new graph isomorphic to @xmath15 which we denote by @xmath22 .",
    "the adjacency matrix of the permuted graph , @xmath23 , is simply obtained from @xmath24 by the equality @xmath25 .    in order to assess whether a permutation @xmath16 defines a good matching between the vertices of @xmath13 and those of @xmath15 ,",
    "a quality criterion must be defined .",
    "although other choices are possible , we focus in this paper on measuring the discrepancy between the graphs after matching , by the number of edges ( in the case of weighted graphs , it will be the total weight of edges ) which are present in one graph and not in the other . in terms of adjacency matrices , this number can be computed as : @xmath26 where @xmath27 is the frobenius matrix norm defined by @xmath28 .",
    "a popular alternative to the frobenius norm formulation ( [ eq : adj_distance ] ) is the @xmath19-norm formulation obtained by replacing the frobenius norm by the @xmath19-norm @xmath29 , which is equal to the square of the frobenius norm @xmath30 when comparing @xmath17-valued matrices , but may differ in the case of general matrices .",
    "therefore , the problem of graph matching can be reformulated as the problem of minimizing @xmath31 over the set of permutation matrices .",
    "this problem has a combinatorial nature and there is no known polynomial algorithm to solve it @xcite .",
    "it is therefore very hard to solve it in the case of large graphs , and numerous approximate methods have been developed .",
    "an interesting generalization of the graph matching problem is the problem of _ labeled _ graph matching . here",
    ", each graph has associated labels to all its vertices and the objective is to find an alignment that fits well graph labels and graph structures at the same time . if we let @xmath32 denote the cost of fitness between the @xmath10-th vertex of @xmath13 and the @xmath11-th vertex of @xmath15 , then the matching problem based only on label comparison can be formulated as follows : @xmath33 where @xmath34 denotes the set of permutation matrices .",
    "a natural way of unifying ( [ eq : context_matching ] ) and ( [ eq : adj_distance ] ) to match both the graph structure and the vertices labels is then to minimize a convex combination @xcite : @xmath35 that makes explicit , through the parameter @xmath36 $ ] , the trade - off between cost of individual matchings and faithfulness to the graph structure .",
    "a small @xmath37 value emphasizes the matching of structures , while a large @xmath37 value gives more importance to the matching of labels .      before describing how we propose to solve ( [ eq : adj_distance ] ) and ( [ eq : f_0_alpha ] ) , we first introduce some notations and bring to notice some important characteristics of these optimization problems .",
    "they are defined on the set of permutation matrices , which we denoted by @xmath34 .",
    "the set @xmath34 is a set of extreme points of the set of doubly stochastic matrices , that is , matrices with nonnegative entries and with row sums and column sums equal to one : @xmath38 , where @xmath39 denotes the @xmath1-dimensional vector of all ones @xcite .",
    "this shows that when a linear function is minimized over the set of doubly stochastic matrices @xmath40 , a solution can always be found in the set of permutation matrices .",
    "consequently , minimizing a linear function over @xmath34 is in fact equivalent to a linear program and can thus be solved in polynomial time by , e.g. , interior point methods @xcite .",
    "in fact , one of the most efficient methods to solve this problem is the hungarian algorithm , which uses a specific primal - dual strategy to solve this problem in @xmath41 @xcite .",
    "note that the hungarian algorithm allows to avoid the generic @xmath42 complexity associated with a linear program with @xmath43 variables .    at the same time",
    "@xmath34 may be considered as a subset of orthonormal matrices @xmath44 of @xmath40 and in fact @xmath45 .",
    "an ( idealized ) illustration of these sets is presented in figure  [ fig : qcvqcc_relaxation ] : the discrete set @xmath34 of permutation matrices is the intersection of the convex set @xmath40 of doubly stochastic matrices and the manifold @xmath46 of orthogonal matrices .",
    "set of orthogonal matrices , @xmath40  set of doubly stochastic matrices , @xmath47set of permutation matrices.,width=226 ]      a good review of graph matching algorithms may be found in @xcite . here",
    ", we only present a brief description of some approximate methods which illustrate well ideas behind two subgroups of these algorithms . as mentioned in the introduction , one popular approach to find approximate solutions to the graph matching problem",
    "is based on the spectral decomposition of the adjacency matrices of the graphs to be matched . in this approach , the singular value decompositions of the graph adjacency matrices",
    "are used : @xmath48 where the columns of the orthogonal matrices @xmath49 and @xmath50 consist of eigenvectors of @xmath51 and @xmath24 respectively , and @xmath52 and @xmath53 are diagonal matrices of eigenvalues .",
    "if we consider the rows of eigenvector matrices @xmath49 and @xmath50 as graph node coordinates in eigenspaces , then we can match the vertices with similar coordinates through a variety of methods @xcite .",
    "however , these methods suffer from the fact that the spectral embedding of graph vertices is not uniquely defined .",
    "first , the unit norm eigenvectors are at most defined up to a sign flip and we have to choose their signs synchronously .",
    "although it is possible to use some normalization convention , such as choosing the sign of each eigenvector in such a way that the biggest component is always positive , this usually does not guarantee a perfect sign synchronization , in particular in the presence of noise .",
    "second , if the adjacency matrix has multiple eigenvalues , then the choice of eigenvectors becomes arbitrary within the corresponding eigen - subspace , as they are defined only up to rotations @xcite .",
    "one of the first spectral approximate algorithms was presented by umeyama @xcite . to avoid the ambiguity of eigenvector selection , umeyama proposed to consider the absolute values of eigenvectors . according to this approach",
    ", the correspondence between graph nodes is established by matching the rows of @xmath54 and @xmath55 ( which are defined as matrices of absolute values ) .",
    "the criterion of optimal matching is the total distance between matched rows , leading to the optimization problem : @xmath56 or equivalently : @xmath57 the optimization problem ( [ eq : u_optimization ] ) is a linear program on the set of permutation matrices which can be solved by the hungarian algorithm in @xmath41 @xcite .",
    "the second group of approximate methods consists of algorithms which work directly with the objective function in ( [ eq : adj_distance ] ) , and typically involve various relaxations to optimizations problems that can be efficiently solved .",
    "an example of such an approach is the linear programming method proposed by almohamad and duffuaa in @xcite .",
    "they considered the @xmath19-norm as the matching criterion for a permutation matrix @xmath58 : @xmath59 the linear program relaxation is obtained by optimizing @xmath60 on the set of doubly stochastic matrices @xmath40 instead of @xmath61 : @xmath62 where the 1-norm of a matrix is defined as the sum of the absolute values of all the elements of a matrix .",
    "a priori the solution of ( [ eq : adj_distance_l1 ] ) is an arbitrary doubly stochastic matrix @xmath63 , so the final step is a projection of @xmath64 on the set of permutation matrices ( we let denote @xmath65 the projection of @xmath64 onto @xmath66 ) : @xmath67 or equivalently : @xmath68 the projection ( [ eq : projsurp ] ) can be performed with the hungarian algorithm , with a complexity cubic in the dimension of the problem .",
    "the main disadvantage of this method is that the dimensionality ( i.e. , number of variables and number of constraints ) of the linear program ( [ eq : projsurp ] ) is @xmath69 , and therefore it is quite hard to process graphs of size more than one hundred nodes .",
    "other convex relaxations of ( [ eq : adj_distance ] ) can be found in @xcite and @xcite . in the next section",
    "we describe our new algorithm which is based on the technique of convex - concave relaxations of the initial problems ( [ eq : adj_distance ] ) and ( [ eq : f_0_alpha ] ) .",
    "let us start the description of our algorithm for unlabeled weighted graphs .",
    "the generalization to labeled weighted graphs is presented in section [ sec : addlabels ] .",
    "the graph matching criterion we consider for unlabeled graphs is the square of the frobenius norm of the difference between adjacency matrices ( [ eq : adj_distance ] ) .",
    "since permutation matrices are also orthogonal matrices ( i.e. , @xmath70 and @xmath71 ) , we can rewrite @xmath31 on @xmath34 as follows : @xmath72 the graph matching problem is then the problem of minimizing @xmath31 over @xmath61 , which we call * gm * : @xmath73      a first relaxation of * gm * is obtained by expanding the convex quadratic function @xmath31 on the set of doubly stochastic matrices @xmath40 : @xmath74 the * qcv * problem is a convex quadratic program that can be solved in polynomial time , e.g. , by the frank - wolfe algorithm @xcite ( see section [ sec : fw ] for more details ) .",
    "however , the optimal value is usually not an extreme points of @xmath40 , and therefore not a permutation matrix . if we want to use only * qcv * for the graph matching problem",
    ", we therefore have to project its solution on the set of permutation matrices , and to make , e.g. , the following approximation : @xmath75 although the projection @xmath76 can be made efficiently in @xmath41 by the hungarian algorithm , the difficulty with this approach is that if @xmath77 is far from @xmath34 then the quality of the approximation ( [ eq : qcv_proj ] ) may be poor : in other words , the work performed to optimize @xmath31 is partly lost by the projection step which is independent of the cost function . the path algorithm that we present later",
    "can be thought of as a improved projection step that takes into account the cost function in the projection .",
    "we now present a second relaxation of * gm * , which results in a concave minimization problem .",
    "for that purpose , let us introduce the diagonal degree matrix @xmath78 of an adjacency matrix @xmath7 , which is the diagonal matrix with entries @xmath79 for @xmath80 , as well as the laplacian matrix @xmath81 .",
    "@xmath7 having only nonnegative entries , it is well - known that the laplacian matrix is positive semidefinite  @xcite .",
    "we can now rewrite @xmath31 as follows : @xmath82\\\\ & + ||l_gp - pl_h||^2_f\\ , .",
    "\\end{split}\\ ] ] let us now consider more precisely the second term in this last expression : @xmath83\\\\ & = \\underbrace { { { \\rm tr } } pp^td_g^tl_g}_{\\sum d_g^2(i)}+\\underbrace { { { \\rm tr } } l_hd_h^tp^tp}_{\\sum d_h^2(i)}-\\underbrace { { { \\rm tr } } p^td_g^tpl_h}_{\\sum d_g(i)d_{p(h)}(i)}\\\\ & \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad    -\\underbrace { { { \\rm tr } } d_h^tp^tl_gp}_{\\sum d_{p(h)}(i)d_g(i)}\\\\ & = \\sum ( d_g(i)-d_{p(h)}(i))^2 = \\|d_g - d_{p(h)}\\|^2_f\\\\ & = \\|d_gp - pd_h\\|^2_f\\ , .",
    "\\end{split}\\ ] ] plugging ( [ eq : toto2 ] ) into ( [ eq : toto1 ] ) we obtain @xmath84 where we introduced the matrix @xmath85 and we used @xmath86 to denote the kronecker product of two matrices ( definition of the kronecker product and some important properties may be found in the appendix [ sec : kron ] ) .",
    "let us denote @xmath87 the part of ( [ eq : last_expr_f1 ] ) which depends on @xmath16 : @xmath88 from ( [ eq : last_expr_f1 ] ) we see that the permutation matrix which minimizes @xmath89 over @xmath61 is the solution of the graph matching problem . now , minimizing @xmath90 over @xmath91 gives us a relaxation of ( [ eq : gm ] ) on the set of doubly stochastic matrices .",
    "since graph laplacian matrices are positive semi - definite , the matrix @xmath92 is also positive semidefinite as a kronecker product of two symmetric positive semi - definite matrices  @xcite .",
    "therefore the function @xmath87 is concave on @xmath91 , and we obtain a concave relaxation of the graph matching problem : @xmath93 interestingly , the global minimum of a concave function is necessarily located at a boundary of the convex set where it is minimized@xcite , so the minimium of @xmath90 on @xmath91 is in fact in @xmath61 .    at this point , we have obtained two relaxations of * gm * as nonlinear optimization problems on  @xmath91 : the first one is the convex minimization problem * qcv * ( [ eq : qcv ] ) , which can be solved efficiently but leads to a solution in @xmath91 that must then be projected onto @xmath61 , and the other is the concave minimization problem * qcc * ( [ eq : qcc ] ) which does not have an efficient ( polynomial ) optimization algorithm but has the same solution as the initial problem * gm*. we note that these convex and concave relaxation of the graph matching problem can be more generally derived for any quadratic assignment problems @xcite .",
    "we propose to approximately solve * qcc * by tracking a path of local minima over @xmath91 of a series of functions that linearly interpolate between @xmath94 and @xmath90 , namely : @xmath95 for @xmath96 .",
    "for all @xmath97 $ ] , @xmath98 is a quadratic function ( which is in general neither convex nor concave for @xmath99 away from zero or one ) .",
    "we recover the convex function @xmath100 for @xmath101 , and the concave function @xmath89 for @xmath102 .",
    "our method searches sequentially local minima of @xmath98 , where @xmath99 moves from @xmath21 to @xmath19 .",
    "more precisely , we start at @xmath101 , and find the unique local minimum of @xmath100 ( which is in this case its unique global minimum ) by any classical qp solver .",
    "then , iteratively , we find a local minimum of @xmath103 given a local minimum of @xmath98 by performing a local optimization of @xmath103 starting from the local minimum of @xmath98 , using for example the frank - wolfe algorithm .",
    "repeating this iterative process for @xmath104 small enough we obtain a path of solutions @xmath105 , where @xmath106 and @xmath105 is a local minimum of @xmath98 for all @xmath97 $ ] .",
    "noting that any local minimum of the concave function @xmath89 on @xmath91 is in @xmath61 , we finally output @xmath107 as an approximate solution of * gm*.    our approach is similar to graduated non - convexity @xcite : this approach is often used to approximate the global minimum of a non - convex objective function .",
    "this function consists of two part , the convex component , and non - convex component , and the graduated non - convexity framework proposes to track the linear combination of the convex and non - convex parts ( from the convex relaxation to the true objective function ) to approximate the minimum of the non - convex function .",
    "the path algorithm may indeed be considered as an example of such an approach .",
    "however , the main difference is the construction of the objective function . unlike @xcite",
    ", we construct two relaxations of the initial optimization problem , which lead to the same value on the set of interest ( @xmath34 ) , the goal being to choose convex / concave relaxations which approximate in the best way the objective function on the set of permutation matrices .",
    "the pseudo - code for the path algorithm is presented in figure [ fig : algo_schema ] .",
    "the rationale behind it is that among the local minima of @xmath90 on @xmath91 , we expect the one connected to the global minimum of @xmath100 through a path of local minima to be a good approximation of the global minima .",
    "such a situation is for example shown in figure [ fig : qcvqcc_path ] , where in @xmath19 dimension the global minimum of a concave quadratic function on an interval ( among two candidate points ) can be found by following the path of local minima connected to the unique global minimum of a convex function .    1 .",
    "* initialization * : 1 .",
    "@xmath108 2 .",
    "@xmath109  convex optimization problem , global minimum is found by frank - wolfe algorithm .",
    "* cycle over @xmath99 * : + while @xmath110 + 1 .",
    "@xmath111 2 .   if @xmath112 then + @xmath113 + else @xmath114 is found + by frank - wolfe starting from @xmath105 + @xmath115 + 3 .",
    "* output * : @xmath116     ( @xmath101 )  initial convex function , @xmath117 ( @xmath102 )  initial concave function , bold black line  path of function minima @xmath105 ( @xmath118),width=302 ]    more precisely , and although we do not have any formal result about the optimality of the path optimization method ( beyond the lack of global optimality , see appendix [ app : toy ] ) , we can mention a few interesting properties of this method :    * we know from ( [ eq : last_expr_f1 ] ) that for @xmath119 , @xmath120 , where @xmath121 is a constant independent of @xmath16 . as a result",
    ", it holds for all @xmath97 $ ] that , for @xmath119 : @xmath122 this shows that if for some @xmath99 the global minimum of @xmath123 over @xmath91 lies in @xmath124 , then this minimum is also the global minimum of @xmath31 over @xmath61 and therefore the optimal solution of the initial problem .",
    "hence , if for example the global minimum of @xmath98 is found on @xmath61 by the path algorithm ( for instance , if @xmath125 is still convex ) , then the path algorithm leads to the global optimum of @xmath117 . this situation can be seen in the toy example in figure [ fig : qcvqcc_path ] where , for @xmath126 , @xmath98 has its unique minimum at the boundary of the domain . *",
    "the sub - optimality of the path algorithm comes from the fact that , when @xmath99 increases , the number of local minima of @xmath98 may increase and the sequence of local minima tracked by path may not be global minima",
    ". however we can expect the local minima followed by the path algorithm to be interesting approximations for the following reason .",
    "first observe that if @xmath127 and @xmath128 are two local minima of @xmath98 for some @xmath97 $ ] , then the restriction of @xmath98 to @xmath129 being a quadratic function it has to be concave and @xmath127 and @xmath128 must be on the boundary of @xmath91 .",
    "now , let @xmath130 be the smallest @xmath99 such that @xmath98 has several local minima on @xmath91 .",
    "if @xmath127 denotes the local minima followed by the path algorithm , and @xmath128 denotes the `` new '' local minimum of @xmath131 , then necessarily the restriction of @xmath131 to @xmath129 must be concave and have a vanishing derivative in @xmath128 ( otherwise , by continuity of @xmath98 in @xmath99 , there would be a local minimum of @xmath98 near @xmath128 for @xmath99 slightly smaller than @xmath130 ) .",
    "consequently we necessarily have @xmath132 .",
    "this situation is illustrated in figure [ fig : qcvqcc_path ] where , when the second local minimum appears for @xmath133 , it is worse than the one tracked by the path algorithm . more generally , when `` new '' local minima appear , they are strictly worse than the one tracked by the path algorithm",
    ". of course , they may become better than the path solution when @xmath99 continues to increase .",
    "of course , in spite of these justifications , the path algorithm only gives an approximation of the global minimum in the general case . in appendix [ app : toy ] , we provide two simple examples when the path algorithm respectively succeeds and fails to find the global minimum of the graph matching problem .",
    "our path following algorithm may be considered as a particular case of numerical continuation methods ( sometimes called path following methods ) @xcite . these allow to estimate curves given in the following implicit form : @xmath134 in fact",
    ", our path algorithm corresponds to a particular implementation of the so - called generic predictor corrector approach @xcite widely used in numerical continuation methods .    in our case",
    ", we have a set of problems @xmath135 parametrized by @xmath136 $ ] .",
    "in other words for each @xmath99 we have to solve the following system of karush - kuhn - tucker ( kkt ) equations : @xmath137 where @xmath138 is a set of active constraints , i.e. , of pairs of indices @xmath139 that satisfy @xmath140 , @xmath141 codes the conditions @xmath142 and @xmath143 , @xmath144 and @xmath145 are dual variables .",
    "we have to solve this system for all possible sets of active constraints @xmath138 on the open set of matrices @xmath16 that satisfy @xmath146 for @xmath147 , in order to define the set of stationary points of the functions @xmath98 . now",
    "if we let @xmath148 denote the left - hand part of the kkt equation system then we have exactly ( [ eq : analysis_t ] ) with @xmath149 . from the implicit function theorem @xcite",
    ", we know that for each set of constraints @xmath138 , @xmath150 is a smooth 1-dimensional curve or the empty set and can be parametrized by @xmath99 . in term of the objective function @xmath123",
    ", the condition on @xmath151 may be interpreted as a prohibition for the projection of @xmath123 on any feasible direction to be a constant .",
    "therefore the whole set of stationary points of @xmath123 when @xmath99 is varying from 0 to 1 may be represented as a union @xmath152 where each @xmath153 is homotopic to a 1-dimensional segment .",
    "the set @xmath154 may have quite complicated form .",
    "some of @xmath153 may intersect each other , in this case we observe a bifurcation point , some of @xmath153 may connect each other , in this case we have a transformation point of one path into another , some of @xmath153 may appear only for @xmath155 and/or disappear before @xmath99 reaches 1 . at the beginning the path algorithm starts from @xmath156 ,",
    "then it follows @xmath157 until the border of @xmath91 ( or a bifurcation point ) . if such an event occurs before @xmath102 then path moves to another segment of solutions corresponding to different constraints @xmath138 , and keeps moving along segments and sometimes jumping between segments until @xmath102 .",
    "as we said in the previous section one of the interesting properties of path algorithm is the fact that if @xmath158 appears only when @xmath159 and @xmath160 is a local minimum then the value of the objective function @xmath131 in @xmath160 is greater than in the point traced by the path algorithm .      in this section",
    "we provide a few details relevant for the efficient implementation of the path algorithms .",
    "[ [ frank - wolfe ] ] frank - wolfe + + + + + + + + + + +    among the different optimization techniques for the optimization of @xmath161 starting from the current local minimum tracked by the path algorithm , we use in our experiments the frank - wolfe algorithm which is particularly suited to optimization over doubly stochastic matrices @xcite .",
    "the idea of the this algorithm is to sequentially minimize linear approximations of @xmath31 .",
    "each step includes three operations :    1 .",
    "estimation of the gradient @xmath162 , 2 .",
    "resolution of the linear program @xmath163 , 3 .",
    "line search : finding the minimum of @xmath161 on the segment @xmath164 $ ] .",
    "an important property of this method is that the second operation can be done efficiently by the hungarian algorithm , in @xmath41 .",
    "[ [ efficient - gradient - computations ] ] efficient gradient computations + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    another essential point is that we do not need to store matrices of size @xmath165 for the computation of @xmath166 , because the tensor product in @xmath167 can be expressed in terms of @xmath18 matrix multiplication : @xmath168 the same thing may be done for the gradient of the convex component @xmath169\\\\ \\mbox { where } & q=(i\\otimes a_g -a_h^t\\otimes i)^t(i\\otimes a_g -a_h^t\\otimes i)\\\\ \\nabla f_0(p)&=2q\\mbox{vec}(p)\\\\ & = 2\\mbox{vec}(a_g^2 p - a^t_gpa^t_h - a_gpa_h+pa_h^2 ) \\end{split}\\ ] ]    [ [ initialization ] ] initialization + + + + + + + + + + + + + +    the proposed algorithm can be accelerated by the application of newton algorithm as the first step of * qcv * ( minimization of @xmath31 ) .",
    "first , let us rewrite the * qcv * problem as follows : @xmath170 where @xmath171 is the matrix which codes the conditions @xmath172 and @xmath173 .",
    "the lagrangian has the following form @xmath174 where @xmath144 and @xmath175 are lagrange multipliers .",
    "now we would like to use newton method for constrained optimization @xcite to solve ( [ eq : syseq_newton ] ) .",
    "let @xmath176 denote the set of variables associated to the set of active constraints @xmath177 at the current points , then the newton step consist in solving the following system of equations : @xmath178{lll } 2q & b^t&i_a \\\\ b & 0&0\\\\ i_a&0&0 \\end{array}\\right ] \\left [ \\begin{array}[c]{l } \\mbox{vec}(p ) \\\\",
    "\\nu\\\\ \\mu_a \\end{array}\\right ] = \\left [ \\begin{array}[c]{l } 0 \\\\ 1\\\\ 0 \\end{array}\\right ] \\begin{array}[c]{l } n^2~\\mbox{elements , } \\\\ 2n ~\\mbox{elements,}\\\\ \\mbox{\\footnotesize\\ # of act . ineq . cons . }",
    "\\end{array } \\label{eq : kkt_newton_full}\\ ] ] more precisely we have to solve ( [ eq : kkt_newton_full ] ) for @xmath16 .",
    "the problem is that in general situations this problem is computationally demanding because it involves the inversion of matrices of size @xmath179 . in some particular cases",
    ", however , the newton step becomes feasible .",
    "typically , if none of the constraints @xmath180 are active , then ( [ eq : kkt_newton_full ] ) takes the following form .",
    "] : @xmath178{ll } 2q & b^t \\\\",
    "b & 0 \\end{array}\\right ] \\left [ \\begin{array}[c]{l } \\mbox{vec}(p ) \\\\",
    "\\nu \\end{array}\\right ] = \\left [ \\begin{array}[c]{l } 0 \\\\ 1 \\end{array}\\right ] ~~ \\begin{array}[c]{l } n^2~\\mbox{elements}\\ , , \\\\ 2n ~\\mbox{elements}\\ , . \\end{array } \\label{eq : kkt_newton}\\ ] ] the solution is then obtained as follows : @xmath181 because of the particular form of matrices @xmath182 and @xmath171 , the expression ( [ eq : p_kkt ] ) may be computed very simply with the help of kronecker product properties in @xmath41 instead of @xmath183 . more precisely ,",
    "the first step is the calculation of @xmath184 where @xmath185 .",
    "the matrix @xmath186 may be represented as follows : @xmath187 therefore the @xmath139-th element of @xmath188 is the following product : @xmath189 where @xmath190 is the @xmath10-th row of @xmath171 and @xmath191 is @xmath190 reshaped into a @xmath18 matrix .",
    "the second step is an inversion of the @xmath192 matrix @xmath188 , and a sum over columns @xmath193 .",
    "the last step is a multiplication of @xmath186 by @xmath194 , which can be done with the same tricks as the first step .",
    "the result is the value of matrix @xmath195 .",
    "we then have two possible scenarios :    1 .   if @xmath196 , then we have found the solution of ( [ eq : syseq_newton ] ) .",
    "otherwise we take the point of intersection of the line @xmath197 and the border @xmath198 as the next point and we continue with frank - wolfe algorithm",
    ". unfortunately we can do the newton step only once , then some of @xmath199 constraints become active and efficient calculations are not feasible anymore . but even in this case the newton step is generally very useful because it decreases a lot the value of the objective function .",
    "[ [ dlambda - adaptation - strategy ] ] @xmath104-adaptation strategy + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in practice , we found it useful to have the parameter @xmath104 in the algorithm of figure [ fig : algo_schema ] vary between iterations . intuitively , @xmath104 should depend on the form of the objective function as follows : if @xmath200 is smooth and if increasing the parameter @xmath99 does not change a lot the form of the function , then we can afford large steps , in contrast , we should do a lot of small steps in the situation where the objective function is very sensitive to changes in the parameter @xmath99 .",
    "the adaptive scheme we propose is the following .",
    "first , we fix a constant @xmath201 , which represents the lower limit for @xmath104 .",
    "when the path algorithm starts , @xmath104 is set to @xmath202 .",
    "if we see after an update @xmath203 that @xmath204 then we double @xmath104 and keep multiplying @xmath104 by @xmath205 as long as @xmath204 . on the contrary , if @xmath104 is too large in the sense that @xmath206 , then we divide @xmath104 by @xmath205 until the criterion @xmath204 is met , or @xmath207 .",
    "once the update on @xmath104 is done , we run the optimization ( frank - wolfe ) for the new value @xmath208 .",
    "the idea behind this simple adaptation schema is to choose @xmath104 which keeps @xmath209 just below @xmath210 .",
    "[ [ sec : stop_criterion ] ] stopping criterion + + + + + + + + + + + + + + + + + +    the choice of the update criterion @xmath209 is not unique . here",
    "we check whether the function value has been changed a lot at the given point .",
    "but in fact it may be more interesting to trace the minimum of the objective function . to compare the new minimum with the current one , we need to check the distance between these minima and the difference between function values .",
    "it means that we use the following condition as the stopping criterion @xmath211    although this approach takes a little bit more computations ( we need to run frank - wolfe on each update of @xmath104 ) , it is quite efficient if we use the adaptation schema for @xmath104 .    to fix the values @xmath212 and @xmath213 we use a parameter @xmath188 which defines a ratio between these parameters and the parameters of the stopping criterion used in the frank - wolfe algorithm : @xmath214 ( limit value of function decrement ) and @xmath215 ( limit value of argument changing ) : @xmath216 and @xmath217 .",
    "the parameter @xmath188 represents an authorized level of stopping criterion relaxation when we increment @xmath99 . in practice , it means that when we start to increment @xmath99 we may move away from the local minima and the extent of this move is defined by the parameter @xmath188 .",
    "the larger the value of @xmath188 , the further we can move away and the larger @xmath104 may be used . in other words",
    ", the parameter @xmath188 controls the width of the tube around the path of optimal solutions .      here",
    "we present the complexity of the algorithms discussed in the paper .",
    "* umeyama s algorithm has three components : matrix multiplication , calculation of eigenvectors and application of the hungarian algorithm for ( [ eq : u_optimization ] ) .",
    "complexity of each component is equal to @xmath41 .",
    "thus umeyama s algorithm has complexity @xmath41 . *",
    "lp approach ( [ eq : adj_distance_l1 ] ) has complexity @xmath42 ( worst case ) because it may be rewritten as an linear optimization problem with @xmath218 variables  @xcite .    in the path algorithm , there are three principal parameters which have a big impact on the algorithm complexity .",
    "these parameters are @xmath219 , @xmath220 , @xmath188 and @xmath1 .",
    "the first parameter @xmath221 defines the precision of the frank - wolfe algorithm , in some cases its speed may be sublinear @xcite , however it should work much better when the optimization polytope has a `` smooth '' border  @xcite .",
    "the influence of the ratio parameter @xmath188 is more complicated . in practice , in order to ensure that the objective function takes values between @xmath21 and @xmath19 , we usually use the normalized version of the objective function : @xmath222 in this case if we use the simple stopping criterion based on the value of the objective function then the number of iteration over @xmath99 ( number of frank - wolfe algorithm runs ) is at least equal to @xmath223 where @xmath224 .",
    "the most important thing is how the algorithm complexity depends on the graph size @xmath1 . in general",
    "the number of iterations of the frank - wolfe algorithm scales as @xmath225 where @xmath226 is the conditional number of the hessian matrix describing the objective function near a local minima @xcite .",
    "it means that in terms of numbers of iterations , the parameter @xmath1 is not crucial .",
    "@xmath1 defines the dimensionality of the minimization problem , while @xmath226 may be close to zero or one depending on the graph structures , not explicitly on their size . on the other hand",
    ", @xmath1 has a big influence on the cost of one iteration . indeed ,",
    "in each iteration step we need to calculate the gradient and to minimize a linear function over the polytope of doubly stochastic matrices .",
    "the gradient estimation and the minimization may be done in @xmath41 . in section [ sec :",
    "synres ] we present the empirical results on how algorithm complexity and optimization precision depend on @xmath188 ( figure [ fig : path_m_d]b ) and @xmath1 ( figure [ fig : timing ] ) .",
    "if we match two labeled graphs , then we may increase the performance of our method by using information on pairwise similarities between their nodes .",
    "in fact one method of image matching uses only this type of information , namely shape context matching @xcite . to integrate the information on vertex similarities we use the approach proposed in ( [ eq : f_0_alpha ] ) , but in our case",
    "we use @xmath123 instead of @xmath31 @xmath227 the advantage of the last formulation is that @xmath228 is just @xmath123 with an additional linear term . therefore we can use the same algorithm for the minimization of @xmath228 as the one we presented for the minimization of @xmath161 .      often in practice",
    "we have to match graphs of different sizes @xmath229 and @xmath230 ( suppose , for example , that @xmath231 ) . in this case we have to match all vertices of graph @xmath15 to a subset of vertices of graph @xmath13 . in the usual case when @xmath232 , the error ( [ eq : adj_distance ] ) corresponds to the number of mismatched edges ( edges which exist in one graph and",
    "do not exist in the other one ) .",
    "when we match graphs of different sizes the situation is a bit more complicated .",
    "let @xmath233 denote the set of vertices of graph @xmath13 that are selected for matching to vertices of graph @xmath15 , let @xmath234 denote all the rest .",
    "therefore all edges of the graph @xmath13 are divided into four parts @xmath235 , where @xmath236 are edges between vertices from @xmath237 , @xmath238 are edges between vertices from @xmath239 , @xmath240 and @xmath240 are edges from @xmath237 to @xmath239 and from @xmath239 to @xmath237 respectively . for undirected graphs",
    "the sets @xmath240 and @xmath240 are the same ( but , for directed graphs we do not consider , they would be different ) .",
    "the edges from @xmath238 , @xmath240 and @xmath241 are always mismatched and a question is whether we have to take them into account in the objective function or not . according to the answer we have three types of matching error ( four for directed graphs ) with interesting interpretations .    1 .",
    "we count only the number of mismatched edges between @xmath15 and the chosen subgraph @xmath242 .",
    "it corresponds to the case when the matrix @xmath16 from ( [ eq : adj_distance ] ) is a matrix of size @xmath243 and @xmath244 rows of @xmath16 contain only zeros .",
    "we count the number of mismatched edges between @xmath15 and the chosen subgraph @xmath242 .",
    "and we also count all edges from @xmath238 , @xmath240 and @xmath241 . in this case",
    "@xmath16 from ( [ eq : adj_distance ] ) is a matrix of size @xmath245 .",
    "and we transform matrix @xmath24 into a matrix of size of size @xmath245 by adding @xmath244 zero rows and zero columns .",
    "it means that we add dummy isolated vertices to the smallest graph , and then we match graphs of the same size .",
    "we count the number of mismatched edges between @xmath15 and chosen subgraph @xmath242 .",
    "and we also count all edges from @xmath240 ( or @xmath241 ) .",
    "it means that we count matching error between @xmath15 and @xmath246 and we count also the number of edges which connect @xmath246 and @xmath247 . in other words",
    "we are looking for subgraph @xmath246 which is similar to @xmath15 and which is maximally isolated in the graph @xmath13 .",
    "each type of error may be useful according to context and interpretation , but a priori , it seems that the best choice is the second one where we add dummy nodes to the smallest graph .",
    "the main reason is the following .",
    "suppose that graph @xmath15 is quite sparse , and suppose that graph @xmath13 has two candidate subgraphs @xmath248 ( also quite sparse ) and @xmath249 ( dense ) .",
    "the upper bound for the matching error between @xmath15 and @xmath250 is @xmath251 , the lower bound for the matching error between @xmath15 and @xmath252 is @xmath253 .",
    "so if @xmath254 then we will always choose the graph @xmath255 with the first strategy , even if it is not similar at all to the graph @xmath15 .",
    "the main explanation of this effect lies in the fact that the algorithm tries to minimize the number of mismatched edges , and not to maximize the number of well matched edges .",
    "in contrast , when we use dummy nodes , we do not have this problem because if we take a very sparse subgraph @xmath256 it increases the number of edges in @xmath257(the common number of edges in @xmath246 and @xmath247 is constant and is equal to the number of edges in @xmath13 ) and finally it decreases the quality of matching .",
    "in this section we compare the proposed algorithm with some classical methods on artificially generated graphs . our choice of random graph types is based on @xcite where authors discuss different types of random graphs which are the most frequently observed in various real world applications ( world wide web , collaborations networks , social networks , etc ... ) .",
    "each type of random graphs is defined by the distribution function of node degree @xmath258 .",
    "the vector of node degrees of each graph is supposed to be an i.i.d sample from @xmath259 . in our experiments",
    "we have used the following types of random graphs :    [ cols=\"<,<\",options=\"header \" , ]",
    "we have presented the path algorithm , a new technique for graph matching based on convex - concave relaxations of the initial integer programming problem .",
    "path allows to integrate the alignment of graph structural elements with the matching of vertices with similar labels .",
    "its results are competitive with state - of - the - art methods in several graph matching and qap benchmark experiments .",
    "moreover , path has a theoretical and empirical complexity competitive with the fastest available graph matching algorithms .",
    "two points can be mentioned as interesting directions for further research . first , the quality of the convex - concave approximation is defined by the choice of convex and concave relaxation functions .",
    "better performances may be achieved by more appropriate choices of these functions .",
    "second , another interesting point concerns the construction of a good concave relaxation for the problem of directed graph matching , i.e. , for asymmetric adjacency matrix . such generalizations would be interesting also as possible polynomial - time approximate solutions for the general qap problem .",
    "the path algorithm does not generally find the global optimum of the np - complete optimization problem . in this appendix",
    "we illustrate with two examples how the set of local optima tracked by path may or may not lead to the global optimum .    more precisely , we consider two simple graphs with the following adjacency matrices :    @xmath260 and @xmath261 .",
    "let @xmath262 denote the cost matrix of vertex association @xmath263 let us suppose that we have fixed the tradeoff @xmath264 , and that our objective is then to find the global minimum of the following function : @xmath265 as explained earlier , the main idea underlying the path algorithm is to try to follow the path of global minima of @xmath266 ( [ eq : f_lambda_alpha ] )",
    ". this may be possible if all global minima @xmath267 form a continuous path , which is not true in general . in the case of small graphs we can find the exact global minimum of @xmath268 for all @xmath99 .",
    "the trace of global minima as functions of @xmath99 is presented in figure [ fig : ex_3](a ) ( i.e. , we plot the values of the nine parameters of the doubly stochastic matrix , which are , as expected , all equal to zero or one when @xmath102 ) .",
    "when @xmath99 is near @xmath269 there is a jump of global minimum from one face to another .",
    "however if we change the linear term @xmath262 to @xmath270 then the trace becomes smooth ( see figure [ fig : ex_3](b ) ) and the path algorithm then finds the globally optimum point . characterizing cases where the path is indeed smooth",
    "is the subject of ongoing research .",
    "the kronecker product of two matrices @xmath271 is defined as follows : @xmath272        r.  s.  t. lee and j.  n.  k. liu , `` an oscillatory elastic graph matching model for recognition of offline handwritten chinese characters , '' _ third international conference on knowledge - based intelligent information engineeing systems _ , pp . 284287 , 1999 .",
    "r.  singh , j.  xu , and b.  berger , `` pairwise global alignment of protein interaction networks by matching neighborhood topology , '' _ the proceedings of the 11th international conference on research in computational molecular biology ( recomb ) _",
    ", 2007 .",
    "y.  wang , f.  makedon , j.  ford , and h.  huang , `` a bipartite graph matching framework for finding correspondences between structural elements in two proteins , '' _ proceedings of the 26th annual international conference of the ieee embs _ , pp . 29722975 , 2004 .",
    "d.  conte , p.  foggia , c.  sansone , and m.  vento , `` thirty years of graph matching in pattern recognition , '' _ international journal of pattern recognition and artificial intelligence _ ,",
    "265298 , 2004 ."
  ],
  "abstract_text": [
    "<S> we propose a convex - concave programming approach for the labeled weighted graph matching problem . </S>",
    "<S> the convex - concave programming formulation is obtained by rewriting the weighted graph matching problem as a least - square problem on the set of permutation matrices and relaxing it to two different optimization problems : a quadratic convex and a quadratic concave optimization problem on the set of doubly stochastic matrices . </S>",
    "<S> the concave relaxation has the same global minimum as the initial graph matching problem , but the search for its global minimum is also a hard combinatorial problem . </S>",
    "<S> we therefore construct an approximation of the concave problem solution by following a solution path of a convex - concave problem obtained by linear interpolation of the convex and concave formulations , starting from the convex relaxation . </S>",
    "<S> this method allows to easily integrate the information on graph label similarities into the optimization problem , and therefore to perform labeled weighted graph matching . </S>",
    "<S> the algorithm is compared with some of the best performing graph matching methods on four datasets : simulated graphs , qaplib , retina vessel images and handwritten chinese characters . in all cases , </S>",
    "<S> the results are competitive with the state - of - the - art .    * keywords * : graph algorithms , graph matching , convex programming , gradient methods , machine learning , classification , image processing </S>"
  ]
}