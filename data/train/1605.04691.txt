{
  "article_text": [
    "the main aim of this paper is to let agents solve tasks by ultimately avoiding aversive signals forever .",
    "this approach entails an interesting and perhaps quite strong guarantee on the agent performance .",
    "the motivation is partly to understand how animals are successful in solving problems , like navigation  @xcite , with limited sensory information and unpredictable effects in the environment .",
    "the animal should find food or return home before it gets lost or becomes exhausted .",
    "we study a general framework in which agents need to avoid problems in tasks .",
    "if the agent encounters a problem , an aversive signal is received . this way the agent could learn to avoid the problem , by avoiding the usage of actions and action - sequences that lead to aversive signals .",
    "the general idea is sketched in figure  [ fig : aversive ] .",
    "before we discuss our approach , we first briefly discuss two important ingredients of the framework , namely , partial information and non - determinism .        [ [ partial - information - and - non - determinism ] ] partial information and non - determinism + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    first , we assume that the agent is given only partial information , as follows : each encountered task state is projected to a set of features .",
    "this is a propositional representation , where each feature is a true / false question posed about the state  @xcite .",
    "the number of features determines the granularity by which states can be perceived by agents .",
    "therefore , the behavior of the agent will be based on feature - action associations , and not on ( direct ) state - action associations .",
    "each application can choose its own features and its own way of computing them .",
    "examples of features are : detected edges in images , impulses through sensors , temporal events over streams , and - or combinations thereof , etc . in this paper , we assume that tasks have only a finite number of features , although there could still be many features .",
    "perhaps not surprisingly , theoretical investigations show how hard it is to solve tasks under partial information , see e.g.  @xcite .. ]    second , we allow tasks to be non - deterministic .",
    "this means that the effect of some action - applications to states can not be predicted . in this paper",
    ", we assume that non - determinism is an inherent property of tasks .",
    "although partial information also limits the reasoning within the agent , and therefore generally prevents accurate predictions , it remains a separate assumption to allow tasks themselves to be non - deterministic .",
    "for example , one may consider tasks in which features actually provide complete information , and where the agent could still struggle with non - determinism .",
    "[ [ strategies ] ] strategies + + + + + + + + + +    the focus of this paper to understand agents based on their behavior in tasks , which could be a useful way to understand intelligence in general  @xcite . as remarked earlier , in this paper , agent behavior will be based on feature - action associations .",
    "conceptually , we may think of the agent as having a set @xmath0 of possibly allowed feature - action pairs , and whenever the agent encounters a task state @xmath1 , the agent ( thinks it ) is allowed to perform all actions @xmath2 for which there is a feature @xmath3 observed in state @xmath1 such that @xmath4 .",
    "we also refer to @xmath0 as a policy .",
    "we say that a set @xmath0 of feature - action pairs constitutes a _ strategy _ for a start state if @xmath0 will never lead to an aversive signal when starting from that start state .",
    "we note that it is not always sufficient for the states near the aversive signals to steer away from them , because sometimes the agent may get trapped in a zone of the state space that does not immediately give aversive signals but from which it is impossible to reliably escape the aversive signals .",
    "the agent should avoid such zones , which could require that the agent anticipates aversive signals from very early on .    our aim in this paper is to reason about the existence of such successful strategies for classes of tasks , and to discuss an algorithm to find such strategies automatically .",
    "a main challenge throughout this study is posed by the compression of state information into features and the uncontrollable outcomes due to non - determinism .    [",
    "[ reward - based - value - estimation - seems - unsuitable ] ] reward - based value estimation seems unsuitable + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    before presenting more details of our algorithm , we first argue that algorithms based on ( numerical ) reward - based value estimation do not always appear suitable for reliably finding problem - avoiding strategies .    on the theory side ,",
    "convergence proofs of value estimation algorithms often require the learning step - size to decrease over time , see e.g.  @xcite .",
    "intuitively , convergence of the estimated values arises because the decreasing learning step - size makes it harder and harder for the agent to learn as time progresses .",
    "however , we would like to avoid putting such limits on the agent , because : ( 1 ) it is useful to also study more flexible agents because they might sometimes better describe real - world agents ; ( 2 ) in practice it might be difficult to estimate in what exact way the learning step - size should decrease ; and , ( 3 ) also in practice , there are no guarantees on what the estimated values will eventually be after a certain amount of time has passed , because the estimates depend strongly on random fluctuations during task performance ( due to non - determinism ) .    in practice , a non - decreasing step - size , although potentially useful to model flexible agents that keep learning from their latest experiences  @xcite , can lead to problems of its own .",
    "we illustrate this with the example task shown in figure  [ fig : reward - task ] .",
    "there is a start state @xmath5 , and two actions @xmath6 and @xmath7 that lead back to state @xmath5 .",
    "we assume complete information for now , i.e. , state @xmath5 is presented completely to the agent as a feature with the same information , namely , the identity of state @xmath5 .",
    "suppose that the state - action pair @xmath8 always gives reward @xmath9 .",
    "but for the pair @xmath10 , the reward could be either @xmath11 or @xmath12 .",
    "although the pair @xmath10 is clearly preferable over the pair @xmath8 in case of positive reward , there is the risk of incurring a strong negative reward .",
    "the negative reward represents an aversive signal . in the perspective of strategies from above , note that @xmath8 constitutes a strategy : constantly executing action @xmath6 in state @xmath5 leads to an avoidance of aversive signals forever .",
    ".45        [ fig : reward - task ]    .45        [ fig : aversive - task ]    but the agent will not necessarily learn to avoid @xmath10 when a hidden task mechanism could periodically deceive the agent by issuing higher rewards under action @xmath7 . concretely , let @xmath13 be a strictly positive natural number . to represent the outcome of action @xmath7 , suppose that we constantly give reward @xmath11 during the first @xmath13 times @xmath10 is applied ; the next @xmath13 times we give @xmath12 ; the following @xmath13 times we again give @xmath11 , and so on .",
    "we call this the @xmath13-swap semantics . for each outcome",
    ", the empirical probability would be @xmath14 : indeed , the observed frequency of each outcome converges to @xmath14 as we perform more applications of action @xmath7 .",
    "we can choose @xmath13 arbitrarily large ; this does not change the empirical probability of each outcome .    without the restriction on learning step - size",
    ", it seems that value estimation algorithms can get into trouble on the above setting because we can set @xmath13 so large that after a while the agent starts to believe that the outcome would remain fixed .",
    "for example , we could start with reward @xmath11 for the pair @xmath10 during the first @xmath13 applications , and the agent starts believing that the reward really is @xmath11 . then come the next @xmath13 applications , where we repeatedly give reward @xmath12 , and the agent starts believing that the reward really is @xmath12 .",
    "we can swap the two outcomes forever , each for a period of @xmath13 applications , and the agent will never make up its mind about the behavior of action @xmath7 in state @xmath5 . this effect is illustrated in figure  [ fig : reward - pattern ] .",
    ".45 , by alternating actions @xmath6 and @xmath7 , but with constant step - size @xmath15 ( and discounting factor @xmath16 ) .",
    "the resulting value estimates for the state - action pairs @xmath8 and @xmath10 are plotted against time .",
    "the outcome of @xmath10 was either @xmath11 or @xmath12 , as determined by the @xmath13-swap semantics where @xmath17 ; this semantics is relative to the applications of @xmath7 , and not relative to the global time steps.,title=\"fig:\",scaledwidth=100.0% ]    .45 , by alternating actions @xmath6 and @xmath7 , but with constant step - size @xmath15 ( and discounting factor @xmath16 ) .",
    "the resulting value estimates for the state - action pairs @xmath8 and @xmath10 are plotted against time .",
    "the outcome of @xmath10 was either @xmath11 or @xmath12 , as determined by the @xmath13-swap semantics where @xmath17 ; this semantics is relative to the applications of @xmath7 , and not relative to the global time steps.,title=\"fig:\",scaledwidth=100.0% ]    although the above example is very simple , real - world tasks could still exhibit problems similar to the @xmath13-swap semantics . even if such problems are identified and understood , perhaps there are no good solutions for them as the problems might be outside the range of control for the agent . in this paper",
    "we would like to learn to avoid the aversive signals forever , even under quite adversary semantics of tasks like the @xmath13-swap semantics .",
    "[ [ avoidance - learning ] ] avoidance learning + + + + + + + + + + + + + + + + + +    in the example of figure  [ fig : reward - task ] , we would like the agent to make up its mind more quickly that action @xmath7 leads to aversive signals .",
    "an idea is to let the agent ( monotonically ) increase its estimate of the value of a feature - action pair .",
    "we should immediately observe , however , that this idea will not work when feedback remains to be modeled as reward , as in the example : once the outcome of @xmath10 is observed to be @xmath11 ; then remembering @xmath11 would lead to a preference of @xmath10 over @xmath8 , causing a reward - seeking agent to ( accidentally ) encounter negative rewards , i.e. , aversive signals , indefinitely under @xmath13-swap semantics .",
    "fortunately , the idea of increasing estimates seems to work when feedback is modeled with aversive signals , even in face of non - determinism .",
    "indeed , @xcite has previously proposed a learning algorithm in tasks where actions have numeric costs , representing aversive signals . by repeatedly remembering the highest observed cost for a state - action pair ( with the @xmath18-operator ) , and by choosing actions",
    "to minimize such costs , the agent learns to steer away from high costs .",
    "we would like to further elaborate this idea and how it relates to the notion of aversion - avoiding strategies mentioned above .    in our framework , we only explicitly model aversive signals , as boolean flags : the flag `` true '' would mean that an aversive signal is present .",
    "this leads to a framework that is conceptually neat and computationally efficient .",
    "because a policy is either successful in avoiding aversive signals forever , or it is not , the choice of a boolean model aligns well with our motivation to study the relationship between learning and successful strategies . to illustrate , the example of figure  [ fig : reward - task ] would be represented by figure  [ fig : aversive - task ] , where only the aversive signal is explicitly represented .",
    "in general , the boolean flags will act like borders , to demarcate undesirable areas in the state space .",
    "reward is now only implicit : by using a strategy , as mentioned earlier , the agent can stay away from the aversive signals forever .    in the above setting with explicit aversive signals , we describe an avoidance learning algorithm , called a - learning , in which the agent repeatedly flags feature - action pairs that lead to aversive signals , or , as an effect thereof , to states for which all proposed actions are flagged ( based on the observed features ) .",
    "intuitively , the flags indicate `` danger '' . on the example of figure  [ fig : aversive - task ] , a - learning  flags @xmath10 at the first occurrence of an aversive signal under action @xmath7 ; and , importantly , the strategy @xmath8 is never flagged . ) , the flagged feature - action pairs are removed from the agent s memory . ]",
    "there is no second chance for changing the agents mind .",
    "this gives one of the strongest convergence notions in learning , namely , fixpoint convergence , where the agent eventually stops changing its mind about the outcome of actions .",
    "if there really is a strategy , avoidance learning will carve out a subset of good feature - action pairs from the mass of all feature - action pairs .",
    "this way , it seems that avoidance learning could be useful in making the agent eventually avoid aversive signals forever .",
    "this provides the guaranteed agent performance we would like to better understand , as remarked at the beginning of the introduction .    [",
    "[ meaning - of - optimality ] ] meaning of optimality + + + + + + + + + + + + + + + + + + + + +    in this paper we view an agent as being optimal if it can ( learn to ) avoid aversive signals forever .",
    "there is no explicit concept of reward .",
    "depending on the setting , or application , aversive signals can originate from diverse sources and together they can describe a very detailed image of what the agent is allowed to do , and what the agent is not allowed to do .",
    "one obtains a rich conceptual setting for reasoning about agent performance .    for example , suppose a robotic agent should learn to move boxes in a storehouse as fast as possible .",
    "we could emit an aversive signal when the robot goes beyond a ( reasonable ) time limit .",
    "any other constraints , perhaps regarding battery usage , can be combined with the first constraint by adding more signals .",
    "[ [ outline ] ] outline + + + + + + +    this paper is organized as follows .",
    "we discuss related work in section  [ sec : relwork ] .",
    "we introduce fundamental concepts like tasks , and strategies , in section  [ sec : fund ] .",
    "we present and analyze our avoidance learning algorithm in section  [ sec : alg ] .",
    "one of our results is that if there is a strategy for a start state then the algorithm will preserve the strategy .",
    "this mechanism can be used to materialize strategies if they exist . to better understand the nature of strategies , we prove the existence of strategies for a family of grid navigation tasks in section  [ sec : grid ] .",
    "the idea of avoiding aversive signals , or problems in general , is related to safe reinforcement learning  @xcite . there",
    ", the goal is essentially to perform reinforcement learning , often based on approximation techniques for optimizing numerical reward , with the addition of avoiding certain problematic areas in the task state space .",
    "an example could be to train a robot for navigation tasks but while avoiding damage to the robot as much as possible . in the current paper ,",
    "feedback to the agent consists of the aversive signals .",
    "reward becomes more implicit , as it lies in the avoidance of aversive signals .",
    "therefore , the viewpoint in this paper is that the agent is called optimal when it eventually succeeds in avoiding all aversive signals forever ; there is no notion of optimizing reward .",
    "the approach is related to a trend identified by  @xcite , namely , the modification of the optimality criterion . the work by @xcite is closely related to our work .",
    "the framework by @xcite provides feedback to the agent in the form of numerical cost signals , which , from the perspective of this paper , could be seen as aversive signals . similar to our @xmath13-swapping example in the introduction ( figure  [ fig : reward - task ] )",
    ", @xcite provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions . the learning algorithm proposed by @xcite maps each state - action pair to the worst outcome ( or cost ) , by means of the @xmath18-operator . by remembering the highest incurred cost for a state - action pair",
    ", the agent in some sense learns about `` walls '' in the state space that constrain its actions towards lower costs .",
    "the avoidance learning algorithm discussed in this paper ( section  [ sec : alg ] ) is similar in spirit to the one by @xcite .",
    "a deviation , however , is that we assume here a boolean interpretation of aversive signals , which leads to a neat and computationally efficient framework .",
    "we additionally identify the concept of strategies , under which the agent can avoid aversive signals forever . our interest lies in understanding such avoidance strategies and their relationship to the avoidance learning algorithm .",
    "moreover , we also focus on partial information , by letting the agent only observe features instead of full states .",
    "for a set @xmath19 , let @xmath20 denote the powerset of @xmath19 , i.e. , the set of all subsets of @xmath19 . a task is a tuple @xmath21 where    * @xmath22 is a nonempty set of states ; * @xmath23 is a finite subset of start states ; stands for `` begin '' . ] * @xmath24 is a nonempty finite set of actions ; * @xmath25 is a nonempty finite set of features ; * @xmath26 is the transition function ; * @xmath27 is the feature function ; and * @xmath28 is the set of aversive signals ,    where all states @xmath29 are reachable in the sense that there is a sequence @xmath30 with @xmath31 , @xmath32 , and @xmath33 for each @xmath34 .    the function @xmath35 maps each pair @xmath36 to a set of possible successor states , representing non - determinism .",
    "the function @xmath37 associates a set of features to each state ; an agent interacting with the task can only observe states through features and can therefore not directly observe states .",
    "the meaning of a pair @xmath38 is that the agent could witness an aversive signal when performing action @xmath2 in state @xmath1 . in state @xmath1 , then the agent witnesses an aversive signal infinitely often during the application of @xmath2 at state @xmath1 , but this signal could sometimes be omitted .",
    "see also section  [ sub : fairness ] . ]",
    "[ ex : task - two - states ] we define an example task @xmath21 as follows : @xmath39 ; @xmath40 ; @xmath41 ; @xmath42 ; regarding @xmath35 , we define @xmath43 regarding @xmath37 , we define @xmath44 and , we define @xmath45 . the task is depicted in figure  [ fig : task - two - states ] .",
    "@xmath46    .",
    "the basic graphical notation is explained in figure  [ fig : intro - task ] . inside the circles , we write the state identifier followed by a semicolon and the features of the state.,scaledwidth=30.0% ]    note that the function @xmath37 maps each state to a set of features .",
    "similarly , the function @xmath35 maps each state - action pair to a set of successor states .",
    "however , an agent interacts with each function in a different way , as follows . for a state @xmath1",
    ", we assume that an agent can always observe all features in @xmath47 simultaneously .",
    "this way , the function @xmath37 may be viewed as being deterministic . in contrast , for a state - action pair @xmath48 , we select only one successor state from @xmath49 to proceed with the task .",
    "the function @xmath37 remains deterministic throughout this paper .",
    "the framework still allows us to consider tasks in which the agent can sometimes observe a certain feature and sometimes it can not .",
    "thereto we can define richer states , in which , say , the status of sensors is stored ; if a state @xmath1 says that a sensor is broken , then @xmath47 could omit the feature that would otherwise be generated by the sensor . @xmath46",
    "our definition of task resembles that of a standard markov decision process  @xcite , but we have added features and aversive signals",
    ". there can be many features , actions , and start states .",
    "and we allow an infinite number of states .",
    "@xmath46      since the agent may only see features , and not states directly , agent behavior has to be based on feature - action associations .",
    "let @xmath21 be a task .",
    "policy _ for @xmath50 is a total function @xmath51 .",
    "we allow features to be mapped to empty sets of actions .",
    "if the task is understood from the context , for a state @xmath29 we define @xmath52 i.e. , @xmath53 is the set of all actions that are proposed by the policy @xmath54 based on the features in @xmath1 .",
    "we say that a state @xmath1 is _ blocked in @xmath54 _ if @xmath55 , i.e. , the policy does not propose actions for @xmath1 .    for a state @xmath1",
    ", we do not view @xmath47 as an atomic signature to which actions should be associated .",
    "instead , the definition of @xmath53 indicates that each feature in @xmath47 may independently propose its own actions , regardless of what is proposed by other features .",
    "all proposed actions are collected into a set , by means of the union - operator .",
    "therefore , features are little actors that become active at appropriate times and that suggest to the agent what actions are ( supposedly ) allowed .",
    "this viewpoint resembles the way that an individual neuron ( or a small group of neurons ) in the brain could represent a distinct concept and could be individually linked to actions  @xcite .",
    "it is the goal of the learning algorithm ( section  [ sec : alg ] ) to remove feature - action associations that lead to aversive signals or , as a result of such removals , to blocked states .",
    "we now consider the following definition :    [ def : strategy ]    a policy @xmath54 is called a _",
    "strategy _ for a start state @xmath31 if    1 .",
    "[ enu : strategy - start ] @xmath56 ; 2 .",
    "[ enu : strategy - followup ] @xmath57 , @xmath58 , 1 .",
    "[ enu : strategy - successor ] @xmath59 we have @xmath60 ; and , 2 .",
    "[ enu : strategy - avs ] @xmath61 .",
    "@xmath46    in words : a policy is a strategy for a start state @xmath62 if the policy acts upon @xmath62 ; and , for any states upon which the policy acts , the reached successor states can also be acted upon , and the policy never causes aversive signals .",
    "intuitively , to use a strategy , for each encountered state @xmath1 we first select some ( arbritary ) feature @xmath63 that satisfies @xmath64 , and we subsequently select an arbitrary action @xmath65 .",
    "the definition of strategy demands properties in a global fashion , possibly also for states that would not be explored when strictly following the strategy .",
    "this condition however ensures that learning algorithms can never have negative experiences when they perform actions suggested by the strategy ; see section  [ sec : alg ] .",
    "suppose @xmath54 is a strategy , and let @xmath3 be a feature with @xmath64 .",
    "intuitively , the definition of strategy says that @xmath3 is a reliable feature , in the sense that every time we see it , we may safely perform all actions in @xmath66 , without the risk of encountering blocked states and aversive signals .",
    "this is related to the markov assumption  @xcite , because we do not have to remember any features that were seen during previous time steps , and we may instead choose actions based on just @xmath3 by itself .",
    "@xmath46    [ ex : partial - strategy ] consider the task from example  [ ex : task - two - states ] .",
    "there is no strategy for start state @xmath5 , but there is a strategy @xmath54 for start state @xmath67 defined as : @xmath68 and @xmath69 . @xmath46    the following property illustrates that strategies are resilient to adding new features . in practical applications",
    ", this means that the addition of new kinds of features will not destroy previously existing strategies .",
    "[ result : features ] let @xmath70 be a task .",
    "let @xmath71 be a set of features that is disjoint from @xmath72 .",
    "let @xmath73 be another task that is almost the same as @xmath74 except that @xmath75 and for each state @xmath1 the constraint @xmath76 holds",
    ". uses the features of @xmath72 in the same way as @xmath74 .",
    "] let @xmath62 be a start state , and suppose that a policy @xmath77 is a strategy for @xmath62 in @xmath74 .",
    "then @xmath54 is also a strategy for @xmath62 in @xmath78 .",
    "we show that the conditions of strategy in definition  [ def : strategy ] are satisfied for @xmath54 in @xmath78 . to better show which task is involved , for a state @xmath1 and a task index @xmath79",
    ", we write @xmath80 when @xmath54 is used in @xmath78 , we assume @xmath81 for each @xmath82 .",
    "above we have also assumed that @xmath83 .",
    "[ [ conditionenustrategy - start ] ] condition  [ enu : strategy - start ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    since @xmath54 is a strategy for @xmath62 in @xmath74 , we have @xmath84 .",
    "this implies there is some @xmath85 with @xmath64 .",
    "since @xmath86 by assumption , we obtain @xmath87 .",
    "[ [ conditionenustrategy - followup ] ] condition  [ enu : strategy - followup ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath1 be a state and assume there is some action @xmath88 .",
    "first we argue that @xmath89 . there must be a feature @xmath90 with @xmath65 .",
    "but since @xmath54 only knows features in @xmath72 , we have @xmath91 .",
    "hence , @xmath89 .",
    "we first handle condition  [ enu : strategy - successor ] .",
    "let @xmath92 . because @xmath89 and @xmath54 satisfies condition  [ enu : strategy - successor ] in @xmath74 , we know @xmath93 .",
    "so there is a feature @xmath94 with @xmath95 . since @xmath96 , we know @xmath97 , as desired",
    "now we handle condition  [ enu : strategy - avs ] . because @xmath89 and @xmath54 satisfies condition  [ enu : strategy - avs ] in @xmath74 , we know @xmath98 . since @xmath99",
    ", we obtain @xmath100 , as desired .",
    "we present and study an avoidance learning algorithm , and its relationship to the concept of strategy introduced in section  [ sub : strategy ] .",
    "algorithm  [ alg : global ] is an avoidance learning algorithm .",
    "the algorithm describes how the agent interacts with the task , and how feature - action combinations are forgotten as the direct or indirect result of aversive signals .",
    "some aspects of the interaction are not under control of the agent , in particular how a successor state is chosen by means of function @xmath35 , and how features are derived from states by means of function @xmath37 .",
    "we now provide more discussion of the algorithm .",
    "henceforth , we will refer to algorithm  [ alg : global ] as a - learning .",
    "@xmath101 [ line : init - mem ]    @xmath1 : = choose from @xmath102 [ line : init - state ]    the essential product of a - learning  is a set @xmath103 that represents the allowed feature - action pairs ; the symbol @xmath0 stands for `` possibilities '' . at any time , the set @xmath0 uniquely defines a policy @xmath54 as follows : for each @xmath104 , we define @xmath105 . regarding notation , for any state @xmath1 , we write @xmath106 to denote the set @xmath53 of proposed actions , where @xmath54 is the unique policy defined by @xmath0 .",
    "we now explain the steps of a - learning  in more detail .",
    "* line  [ line : init - mem ] initializes @xmath0 with all feature - action pairs .",
    "we will gradually remove pairs if they lead to @xmath107 or to blocked states ( that are created by removals of the first kind ) .",
    "* line  [ line : init - state ] selects a random start state .",
    "the control flow of the algorithm is redirected here each time we want to restart the task .",
    "but we never re - initialize @xmath0 .",
    "+ task restarts may be requested by a - learning  itself ( see below ) , or externally by the training framework in which a - learning  is running .",
    "* line  [ line : start - fail ] requests a task restart in case the chosen start state is blocked .",
    "this allows more exploration from the other start states .",
    "as we will see later in theorem  [ theo : learn]([enu : theo - learn - preserve ] ) , if no actions remain for a start state then this start state has no strategy .",
    "* at line  [ line : loop ] , the algorithm enters a learning loop . the loop is only exited to satisfy task restart requests , at line  [ line : desired - restart ] .",
    "* at line  [ line : action ] , we choose an action @xmath2 to apply to current state @xmath1 based on the set @xmath106 of still allowed actions . at line  [ line : succ - state ] , we are subsequently given a successor state @xmath108 , chosen arbitrarily from @xmath49 .",
    "* next , at line  [ line : feedback ] , we check whether we have encountered @xmath107 or if successor state @xmath108 is blocked . in either case",
    "we exclude from @xmath0 the feature - action pairs that caused us to apply action @xmath2 in state @xmath1 ( line  [ line : exclude ] ) , and we restart the task ( line  [ line : fail ] ) . * if we do not encounter @xmath107 and state @xmath108 is not blocked , then we proceed with the while loop ( line  [ line : continue ] )",
    ".    note that in general there are multiple runs of a - learning  on a task , because of the choice on action selection and the choice on successor state .",
    "each run of a - learning  is infinitely long .",
    "nonetheless , there is always an eventual fixpoint on the set @xmath0 because after the initialization we only remove feature - action pairs .",
    "there are only a finite number of possible feature - action pairs , although there could be many .",
    "when the run is clear from the context , we write @xmath109 to denote the fixpoint of @xmath0 obtained in that run .    for conceptual convenience",
    ", we can divide each run of a - learning  into trials by using the task restarts as dividers : whenever we execute line  [ line : init - state ] , the previous trial ends and the next trial begins .",
    "each trial is thus a sequence @xmath30 , where @xmath62 is a start state , @xmath110 is the last state of the trial , and @xmath111 for each @xmath34 . ) to divide runs into trials , and not the encounter of start states .",
    "this means that in principle we allow @xmath112 for some or all @xmath34 . ]",
    "there is no stopping condition in the algorithm because in general we may not be able to detect when the agent has explored the task sufficiently to be successful at avoiding aversive signals .",
    "@xmath46    [ remark : greedy ] we would like to emphasize that a - learning  is always greedy in avoiding @xmath107 .",
    "this is an important deviation from the @xmath113-greedy exploration principle  @xcite , where at each time step the agent chooses a random action with small probability @xmath114 $ ] .",
    "we do not use that mechanism here because otherwise the agent keeps running the risk of encountering aversive signals  @xcite .",
    "@xmath46    the reason for requesting a task restart at line  [ line : fail ] is that sometimes the agent could become stuck in a zone of the state space where there are only blocked states or aversive signals . in that case , if we want the agent to start removing feature - action pairs to prevent future aversive signals , we should first transport the agent to a zone in the state space without blocked states and aversive signals .",
    "for example , in a robot navigation problem , the robot could learn to avoid pits , but once it enters a pit it can perhaps not reliably escape without the help of an external supervisor .",
    "@xmath46    [ remark : memory ] algorithm  [ alg : global ] explicitly stores the allowed feature - action pairs in a set @xmath0 .",
    "this is an intuitive perspective for the theory developed in this paper .",
    "however , in practice it may sometimes be more efficient to store the opposite information , namely , the removed feature - action pairs . this way",
    "all allowed feature - action pairs can still be uniquely recovered . using the analogy of a planar map , where aversive signals are borders between neutral zones on the one hand and undesirable zones on the other hand",
    ", there could be a decreased memory usage in storing only the border ( i.e. , the removed feature - action pairs ) if the borders are simple shapes instead of irregular shapes with many protrusions .",
    "@xmath46      the following theorem helps to understand what a - learning  computes .",
    "[ theo : learn ] for all tasks @xmath21 , for each @xmath31 , for each run of a - learning , where @xmath109 denotes the fixpoint ,    1 .",
    "[ enu : theo - learn - preserve ] if there is a strategy for @xmath62 then @xmath115 .",
    "[ enu : theo - learn - discover ] if @xmath115 then every trial for @xmath62 after the fixpoint avoids blocked states and @xmath107 .",
    "we consider the two properties separately .",
    "[ [ propertyenutheo - learn - preserve ] ] property  [ enu : theo - learn - preserve ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose there is a strategy @xmath54 for @xmath62 .",
    "we show that the feature - action pairs of @xmath54 are preserved in @xmath109 , so that @xmath56 would imply @xmath115 . towards a contradiction ,",
    "suppose that a - learning  removes a pair @xmath116 from @xmath0 where @xmath65 ; let @xmath116 be the first such pair that is removed .",
    "the removal has happened as follows : we reach a state @xmath1 with @xmath63 and we perform @xmath2 , and either the successor state @xmath92 is blocked or we receive an aversive signal .",
    "we discuss each case in turn .",
    "let @xmath0 denote the remaining feature - action pairs just before we remove @xmath116 .",
    "note that @xmath65 and @xmath63 together imply @xmath117 .    *",
    "suppose that @xmath108 is blocked .",
    "since @xmath54 is a strategy , by condition [ enu : strategy - successor ] of definition  [ def : strategy ] , we have assumed @xmath60 .",
    "so , there is a feature @xmath118 and an action @xmath119 .",
    "since @xmath116 is the first pair of @xmath54 that is removed , we still have @xmath120 .",
    "but then @xmath121 , and @xmath108 is actually not blocked ; we have found a contradiction .",
    "* suppose that an aversive signal was received when applying @xmath2 to @xmath1 , which implies @xmath38 .",
    "this immediately contradicts the assumption that @xmath54 satisfies condition  [ enu : strategy - avs ] of definition  [ def : strategy ] .",
    "[ [ propertyenutheo - learn - discover ] ] property  [ enu : theo - learn - discover ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    suppose @xmath115 . towards a contradiction ,",
    "suppose that after the fixpoint there is a trial for start state @xmath62 where we encounter a state @xmath1 and we perform an action @xmath2 such that either the successor state is blocked or we receive an aversive signal .",
    "suppose we conceptually halt the offending trial at the first encountered problem . we have followed a path : @xmath122 for some @xmath92 .",
    "we have @xmath123 for each @xmath124 .",
    "we note in particular that @xmath125 .",
    "next we distinguish two cases , depending on the type of problem .",
    "* suppose that @xmath126 .",
    "then a - learning  now removes @xmath127 from @xmath0 .",
    "but then we will no longer propose action @xmath2 for state @xmath1 , which was previously allowed by the fixpoint .",
    "then @xmath109 would be an invalid fixpoint , which is a contradiction . *",
    "suppose that an aversive signal is received when applying @xmath2 to @xmath1 , which implies @xmath38 .",
    "we make a similar reasoning as in the previous case : a - learning  removes @xmath127 from @xmath0 .",
    "again @xmath109 would be an invalid fixpoint .",
    "suppose that a task has a strategy for each start state .",
    "in that case , theorem  [ theo : learn ] tells us that every run of a - learning  will eventually avoid blocked states and aversive signals .",
    "the agent therefore makes a transition from first discovering the strategies to later exploiting the strategies .",
    "the opposite is not necessarily true : there are tasks for which exist runs that eventually avoid blocked states and aversive signals , but without there being a strategy in the sense of definition  [ def : strategy ] .",
    "this is illustrated by the task in figure  [ fig : aversive - task - long ] .",
    "consider a run where the first application of action @xmath7 in state @xmath5 results in an aversive signal , and after which we immediately restart the task . in that run , there is no further exploration to state @xmath67 , which causes @xmath128 ; hence , @xmath129 .",
    "however , note that if the internal restart request at line  [ line : fail ] of algorithm  [ alg : global ] would sometimes not be handled immediately , but a few steps later , then some runs will not preserve the pair @xmath130 . @xmath46    .",
    "but there is a run of a - learning  in which the feature - action pair @xmath130 is preserved .",
    "the graphical notation is explained in figure  [ fig : task - two - states].,scaledwidth=40.0% ]    the insights of theorem  [ theo : learn ] could be used as follows .",
    "first , although proving that a strategy exists helps in understanding guarantees on the agent performance , programming the strategy by hand could be tedious and time - consuming .",
    "so , property  [ enu : theo - learn - preserve ] could be used to materialize strategies once they are proven to exist .",
    "second , if one does not know whether a strategy exists , property  [ enu : theo - learn - discover ] could be used to perform a preliminary search for strategies .",
    "although the discovered strategies might not be easily interpreted , they could serve as inspiration for a theoretical study of strategies for the tasks at hand . a practical consideration",
    ", however , is that it might not be possible to efficiently detect the fixpoint , i.e. , typically one does not know if a fixpoint has been reached when a - learning  has not removed feature - action pairs for a while .",
    "@xmath46      so far we have silently allowed all possible runs of a - learning .",
    "for example , we did not explicitly demand that the agent actually must receive an aversive signal when applying an action @xmath2 to a state @xmath1 where @xmath38 .",
    "the aversive signal could also be omitted .",
    "this brings us to the topic of fairness  @xcite .",
    "intuitively , for this paper , fairness would mean that there is sufficient exploration of the task . a practical application of a - learning  ( algorithm  [ alg : global ] )",
    "could take the following fairness assumptions into account :    * if we execute line  [ line : init - state ] infinitely often then we choose each start state infinitely often ; * to fully learn the task from each start state , we infinitely often issue external task restarts at line  [ line : desired - restart ] ; those restarts are not requested by a - learning  itself ; * at line  [ line : action ] , if we encounter the same pair of a state @xmath1 and set @xmath0 infinitely often then we choose each action @xmath131 infinitely often ; * at line  [ line : succ - state ] , if we apply action @xmath2 infinitely often to state @xmath1 then each successor state @xmath92 is visited infinitely often from an application of @xmath2 to @xmath1 ; * at line  [ line : feedback ] , if we perform action @xmath2 in state @xmath1 infinitely often , where @xmath38 , then the agent should infinitely often receive an aversive signal when applying @xmath2 to @xmath1 ;    the only aspect of fairness that can be directly influenced by the agent itself , is the action selection at line  [ line : action ] . for this purpose",
    ", a random number generator can be used to select random indices in an array - representation of the proposed actions .",
    "note that theorem  [ theo : learn ] also works for unfair runs .",
    "every run has a fixpoint on @xmath0 , whether the run is fair or not .",
    "but by exploring fewer states , or by issuing fewer aversive signals , an unfair run essentially makes it easier for the agent to avoid aversive signals .",
    "this way , some feature - action pairs could remain forever , even though a more fair exploration of the task could have removed them .    also , because the notion of strategy in definition  [ def : strategy ] is rather strong , it is not possible for a fair run or an unfair run to confront the agent with a situation that leads to the failure of a strategy .",
    "the agent will never be disappointed in the exploitation of the strategy .",
    "we study a simple class of grid navigation problems .",
    "let @xmath132 denote the set of integers .",
    "for any two points @xmath133 , denoting @xmath134 and @xmath135 , we recall the definition of @xmath136-distance between @xmath137 and @xmath138 : @xmath139    a _ simple grid navigation problem _ is a quintuple @xmath140 , where    * @xmath141 and @xmath142 are the dimensions of a terrain ; * @xmath143 is a set of start locations ; * @xmath144 is a set of possible target locations ; and , * @xmath145 is a time limit ,    with the following assumptions ,    * @xmath146 , we assume @xmath147 ; and , * @xmath148 , we assume @xmath147 .",
    "the intuition is that at the beginning of a session we select a start location @xmath149 and an initial active target location @xmath150 and we should navigate from @xmath151 to @xmath152 within time @xmath153 . whenever we reach the active target location @xmath152 we choose another target location @xmath154 and we should now navigate from @xmath152 to @xmath155 within time @xmath153 .",
    "this relocation of the active target may be repeated an arbitrary number of times .",
    "but at any moment we may also begin a new session , in which we again choose a start location and initial target location .",
    "there are infinitely many sessions .",
    "the available actions are : left , right , up , down , left - up , left - down , right - up , right - down , and wait . importantly : failure to respect the time @xmath153 results in an aversive signal ; we aim to eventually avoid such aversive signals .    for a location @xmath156 and an action @xmath6 , we now define the possible successor locations that result from the application of @xmath6 to @xmath157 ; we denote this set as @xmath158 .",
    "a set of multiple possible successors is used to represent non - determinism .",
    "an empty set of of successors is used to say that the action would lead outside the considered terrain .",
    "we assume the following actions to be deterministic : left , right , up , and down . the other , `` diagonal '' , actions are non - deterministic .",
    "for example , for each @xmath159 , @xmath160 @xmath161and , @xmath162 we make the assumption that the direction of the positive y - axis corresponds to `` downward '' .",
    "we now define the task structure @xmath163 that corresponds to the above grid problem @xmath164 . here",
    "it will be convenient to view states and features as structured objects , with components ; for an object @xmath165 with a component @xmath166 , we write @xmath167 to access the component .",
    "* the set @xmath22 consists of all triples @xmath1 with components _ agent _ , _ target _ , and _ time _ , satisfying the following constraints : @xmath168 and @xmath169 are both in the set @xmath170 , and @xmath171 ; * the set @xmath102 consists of those states @xmath1 where @xmath172 , @xmath173 , and @xmath174 ; * @xmath175 ; * the set @xmath25 consists of all pairs @xmath3 with components _ offset _ and _ time _ , satisfying the constraints : @xmath176 and @xmath177 ; * the transition function @xmath35 is described by algorithm  [ alg : grid - trans ] ; for a state @xmath29 and action @xmath178 , the set @xmath49 consists of all states that could possibly be returned by algorithm  [ alg : grid - trans ] upon receiving input @xmath48 ; * regarding @xmath37 , for each @xmath29 , we define @xmath179 where @xmath104 is the single feature for which @xmath180 and @xmath181 ; and , * @xmath182 .",
    "@xmath183 : = @xmath168 @xmath184 : = choose from @xmath185      [ result : grid ] for each grid problem @xmath164 , there is a strategy for each start state in @xmath186 .",
    "denote @xmath163 .",
    "we define one policy that is a strategy for all start states .",
    "first , we define an auxiliary set @xmath187 to consist of all features @xmath3 for which @xmath188 where @xmath189 is the @xmath136-norm of a point @xmath157 .",
    "intuitively , such features indicate that the deterministic distance from the agent location to the target location  where we only use the actions left , right , up , and down  can be bridged within the remaining time .",
    "we now define a policy @xmath54 . for all @xmath190",
    "we define @xmath81 , and for each @xmath82 , denoting @xmath191 , we define @xmath192 as mentioned earlier , we define downwards as the direction of the positive y - axis .",
    "the case where @xmath193 occurs when the agent is located at the target .",
    "implies that the situation where @xmath194 only occurs when the agent reaches some target location and the next target location is the same as the old target location . ]",
    "let @xmath31 .",
    "we show that @xmath54 is a strategy for @xmath62 , according to definition  [ def : strategy ] .",
    "[ [ conditionenustrategy - start - of - definitiondefstrategy ] ] condition  [ enu : strategy - start ] of definition  [ def : strategy ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we show that @xmath56 . by assumption on @xmath62 , we have @xmath195 , @xmath196 , and @xmath197 . by using the distance assumptions on locations in @xmath164",
    ", we obtain @xmath198 . letting @xmath3",
    "be the single feature in @xmath199 , we see that @xmath200 , which implies that @xmath82 .",
    "hence @xmath64 , which implies @xmath56 .",
    "[ [ conditionenustrategy - successor - of - definitiondefstrategy ] ] condition  [ enu : strategy - successor ] of definition  [ def : strategy ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath29 .",
    "suppose there is some action @xmath117 .",
    "let @xmath3 denote the single feature of @xmath1 .",
    "we have @xmath65 , which implies @xmath82 .",
    "let @xmath92 .",
    "we must show that @xmath60 .",
    "let @xmath201 be the single feature of @xmath108 .",
    "we will show that @xmath202 , which implies @xmath95 , and further that @xmath60 . based on algorithm  [ alg : grid - trans ] , we reason about what has happened during the application of action @xmath2 to state @xmath1 .",
    "* suppose the if - test at line  [ line : grid - reach - target ] succeeds , i.e. , the agent reaches the target location .",
    "then @xmath203 , and @xmath204 where we use the distance assumption between target locations .",
    "overall , @xmath205 ; hence , @xmath202 .",
    "* suppose the if - test at line  [ line : grid - reach - target ] does not succeed , i.e. , the agent did not yet reach the target location .",
    "it must be that @xmath206 , because otherwise @xmath194 , which implies @xmath207 , and the test at line  [ line : grid - reach - target ] would have succeeded ( see previous case ) .",
    "so , @xmath208 .",
    "+ first , we observe that @xmath209 indeed , this property holds because ( 1 ) the locations @xmath168 and @xmath210 are inside the convex terrain ; ( 2 ) the action @xmath2 is given deterministic movement semantics ( i.e. , there is precisely one outcome ) , causing @xmath211 to be both inside the terrain and strictly closer to @xmath210 .",
    "+ second , we also observe that @xmath212 since @xmath213 by definition and @xmath214 ( which follows from @xmath82 ) .",
    "+ overall , we may now write @xmath215 in the second line we have used @xmath82 . we conclude that @xmath202 .    [ [ conditionenustrategy - avs - of - definitiondefstrategy ] ] condition  [ enu : strategy - avs ] of definition  [ def : strategy ] + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath29 .",
    "suppose there is some action @xmath117 , which implies that the single feature @xmath3 of @xmath1 must be in @xmath71 .",
    "by definition of @xmath71 , we have @xmath216 .",
    "hence , @xmath214 , which implies @xmath61 , as desired .",
    "the policy defined in the proof of proposition  [ result : grid ] is in general not the maximal strategy , in the sense that the policy could be extended with more actions than currently specified . for instance , if the time limit is high then the agent can randomly wander around before it becomes sufficiently urgent to reach a target location .",
    "the agent may also use the diagonal actions , like left - up , if the time limit is not violated under either of the three outcomes .",
    "@xmath46    it is possible to extend the above setting of grid navigation to richer state representations , by including for example the locations of additional objects ( that do not influence the agent ) .",
    "if this new information would be communicated to the agent with a set of features that is disjoint from the set of old features in section  [ sub : grid - defs ] , then proposition  [ result : features ] tells us that the strategy described in the proof of proposition  [ result : grid ] is still valid .",
    "we have used the notion of strategies to reason about the successful avoidance of aversive signals in tasks .",
    "we have shown that our avoidance learning algorithm always preserves those strategies .",
    "now we discuss some interesting topics for further work .      in this paper",
    "we have considered a framework in which features are essentially black boxes , in the sense that we do not assume anything about the way that they are computed .",
    "hence , we do not know how features are related to the task environment",
    ". it would be interesting to develop more detailed insights into how features can be designed , to ensure that strategies , or similarly successful policies , are possible .",
    "in particular , it seems fascinating to explore possible connections between our framework and neuron - like models , where features would be represented by neurons or by small groups of neurons .",
    "it is currently an open question whether or not feature learning in the brain is a completely unsupervised process  @xcite , i.e. , it is not known whether feature creation is influenced by rewarding or aversive signals .",
    "so , in a general theory , it might be valid to consider feature learning as a separate , unsupervised , module .",
    "this approach could lead to a conceptually simple framework of agent behavior and feature detection simultaneously . concretely",
    ", the approach could enable the results in this paper to be linked to various feature detector algorithms .",
    "in this paper we have assumed that the set of features is fixed at the beginning of the learning process .",
    "this could be suitable for many applications , as there is no fixed limit on how many features there are , as long as there are finitely many .",
    "but it seems intriguing to introduce new features while the agent is performing the task . in the technical approach of this paper , however , a newly inserted feature likely proposes wrong actions if we would initially associate all actions to the feature .",
    "in general we still insist that aversive signals are avoided , and therefore the wrong actions need to be unlearned as soon as possible",
    ".    a way to soften the introduction of new features , could be to reintroduce reward into the framework . concretely",
    ", a feature @xmath3 may only propose an action @xmath2 if the pair @xmath116 has been observed to be correlated to reward , either directly , or transitively by means of eligibility traces  @xcite .",
    "this idea introduces a threshold for proposing actions .",
    "of course any feature - action pairs introduced in this way could still lead to aversive signals .",
    "for example , there could be spurious features ( e.g. features that randomly appear ) to which no actions should be linked , or perhaps the rewarding signals contradict the aversive signals , or some actions that give reward could also give aversive signals ( as in the example of the introduction ) . to resolve priority issues , one could view avoidance learning as having the highest precedence , where reward is used as a softer ranking mechanism on the allowed actions .",
    "possibly , an agent that keeps learning new features will keep making mistakes .",
    "how to cope with new features therefore seems a relevant question .",
    "the answers could perhaps also help to understand animal behavior and consciousness .",
    "thereto one could consider other notions of success than the avoidance of aversive signals investigated in this paper ."
  ],
  "abstract_text": [
    "<S> we study a framework where agents have to avoid aversive signals . </S>",
    "<S> the agents are given only partial information , in the form of features that are projections of task states . </S>",
    "<S> additionally , the agents have to cope with non - determinism , defined as unpredictability on the way that actions are executed . </S>",
    "<S> the goal of each agent is to define its behavior based on feature - action pairs that reliably avoid aversive signals . </S>",
    "<S> we study a learning algorithm , called a - learning , that exhibits fixpoint convergence , where the belief of the allowed feature - action pairs eventually becomes fixed . </S>",
    "<S> a - learning  is parameter - free and easy to implement . </S>"
  ]
}