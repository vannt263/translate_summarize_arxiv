{
  "article_text": [
    "a core part of risk management in the banking industry is the identification of risk factors and the generation of scenarios for a one up to ten days time horizon .",
    "this task involves dealing with basically three important issues and technical requirements .",
    "first , the scenarios need to extract the market information from the available time series of the risk factors .",
    "mathematically speaking , this problem is tackled via the identification of random variables that describe the risk factors one or ten days ahead and that match the most important stylized facts in the time series . a major complication in this classical statistics ( or econometrics )",
    "question is that the available time series are extremely short ( about 500 business days ) in comparison with the number of risk factors ( several thousands ) .",
    "even if the time series were long enough , standard statistical methods would take an excessively heavy computational effort to come up with reliable solutions for such a high dimensional problem .",
    "second , the shortness of the time series implies that one has to separately model extreme events , or even middle size events , in order to complement the information provided by the time series and to allow for cautious prediction .",
    "the third issue is that the procedure for scenarios generation should be computationally fast since they have to be generated every ( business ) day in order to be able to evaluate the risk to which all the portfolios of the institution are exposed .",
    "there are two families of ways to deal with the problem of scenario generation in risk management , namely the historical and the distributional approaches .",
    "both strategies produce random variables from which one can sample the scenarios :    * distributional or parametric approach*. the scenarios are constructed by sampling a multivariate parametric distribution that has been calibrated using the historical data .",
    "one can also fit certain distributions to vectors of risk factors and then use a copula to put together all these lower dimensional models .",
    "there are two major difficulties associated to this method : first , most statistical estimators of the distribution parameters that need to be calibrated yield excessively wide confidence intervals for the estimated coefficients due to the relative shortness of the historical time series in comparison with the dimensionality of the problem .",
    "second , the available historical data may not correspond to any of the standard frequently used parametric distributions .    * historical approach*.",
    "the scenarios are obtained by directly sampling the historical data ( eventually after rescaling ) .",
    "this method is much privileged in practice due to its ease of implementation and because it does not require any assumption on the historical distributions of the risk factors .",
    "this does not mean that this method is hypothesis free ; indeed , its statistical legitimacy comes from the so called bootstrap method ( see for instance  @xcite ) , whose requirements are not always satisfied , the most important of them being the iid ( independent and identically distributed ) character of the risk factors returns .",
    "additionally , this method is obviously incapable of predicting rare ( extreme ) phenomena that did not happen before in the past ; consequently these rare phenomena need to be added `` by hand '' ( which is not a disadvantage but simply means that the time series is too short to reflect all possible risks ) .",
    "several strategies have been proposed to tackle these problems ; a particularly sophisticated and powerful one is the so called filtered historical simulation @xcite , where the historical returns are `` filtered '' by volatility functions calibrated to each time series using garch type processes .    * empirical calibration of sdes*.",
    "the approach that we present in this article is reminiscent of both the historical and the distributional approaches and tries to `` interpolate '' between them .",
    "it allows us to properly capture the stylized facts on dependence of the given time series and to easily carry out additional distributional tuning .",
    "we create the random variable for future scenarios based on * stochastic differential equations * , which describe the local dynamics of the respective risk factors ( `` distributional aspect '' ) .",
    "the stochastic differential equations are standard models from mathematical finance like the black - merton - scholes , cox - ingersoll - ross , heath - jarrow - morton models , etc , which are adequately chosen for each individual class of risk factors ; in particular all these equations are * free of arbitrage*. the use of risk management models which could also be used for pricing or hedging derivative contracts is a tremendous conceptual advantage .",
    "this way we bridge between the worlds of trading and hedging .",
    "in particular the approach allows to actually come up with conceptually coherent methods for risk management which could in principle also be used for trading purposes .",
    "we calibrate the sdes not via an optimization process as it is customary , but by directly using the historical data to construct local characteristics that capture the stylized facts of the time series .",
    "the method relies on the central limit theorem for covariance processes . under certain conditions that we specify later on in the paper",
    "the calibrated spdes converge properly if the observed data stem from an spde ( which is basically the law of large numbers behind the construction ) .",
    "heavy tails are generated by distinguishing between `` trading time '' and `` clock time '' ( randomization of the period of simulation ) and by adding extreme events , possibly with heavy tailed distributions .",
    "the first effect reflects that the flow of information runs at different speeds under different market volatility conditions and is responsible for `` middle - size '' events ; the second effect reflects the possibility of the occurrence of rare ( extreme ) events not present in the historical data .    having the filtered historical simulation approach ( fhs )  @xcite in mind , the empirical calibration approach can be seen as the counterpart of fhs on the side of mathematical finance , i.e. , instead of using the garch - methodology to capture volatility , we apply standard models from mathematical finance and random time changes for the same purpose ( see section [ calibration ] and [ fine_tuning ] ) . to be precise , the issue of capturing volatility correctly is addressed by rescaling with respect to realized volatility parameters of the time series via a sliding window method and by imposing specific assumptions on the dependence of volatility on the underlying risk factors .",
    "in this section we outline the basic setting of wiener driven stochastic ( partial ) differential equations , which underlie the time evolution of risk factors . for stochastic ( partial ) differential equations we refer to @xcite as main reference .",
    "details on term structure equations can be found in @xcite .",
    "our formal setup consists of a filtered probability space @xmath0 , where @xmath1 is a complete filtration and @xmath2 a probability measure .",
    "the basis is assumed to carry a @xmath3-dimensional standard brownian motion @xmath4 .",
    "we use as models for the risk factors stochastic differential equations from mathematical finance ; this leads us to stochastic partial differential equations of the following general type : let @xmath5 be the hilbert space where risk factors take values , then we consider @xmath6 where @xmath7 is an invertible linear map on some closed subspace @xmath8 containing the real span @xmath9 of the set of * return directions * @xmath10 ( see remark [ returns ] for the use of the word return here ) .",
    "such volatility structures are usually called * constant direction volatilities * , see for instance @xcite . the `` volatility factor '' @xmath11 is chosen appropriately for the respective risk factors in order to exclude immediate arbitrages ( like , e.g. , negative interest rates ) . it should be interpreted as an * a priori given * factor governing the shape of the support of the risk factors ; it is hence a * geometric factor*. the vector field @xmath12 corresponds to appropriately chosen no - arbitrage conditions ; @xmath13 is another vector field that lies in the span @xmath9 and corresponds to an appropriate girsanov type change of measure .",
    "we suppose that the usual conditions on the drift @xmath12 hold , that is , it can be written as @xmath14 for some @xmath15 , where @xmath16 denotes the generator of a strongly continuous semigroup which in our case will always be a shift to the right or the identity semigroup on the hilbert space of risk factors @xmath5 .",
    "the maps @xmath17 are smooth and all their derivatives are bounded ( @xmath18-bounded map ) ; they represent the * no arbitrage conditions*. certainly  in view of existence and uniqueness  we have to assume global lipschitz properties for the respective vector fields .",
    "our driving example will be a joint interest rate ( ir ) and foreign exchange ( fx ) market for @xmath19 countries with different currencies and interest rate levels ( @xmath20 ) .",
    "the risk factors of this market are the different log - exchange rates and forward interest rates . for the basic equations of this market we refer to @xcite .",
    "denote by @xmath21 and @xmath22 , @xmath23 the time @xmath24 prices of the domestic and foreign default - free , zero coupon bonds , where @xmath25 denotes the different economies where exchange rates and zero coupon bonds are observed .",
    "the bonds are maturing at @xmath26 .",
    "we define the domestic and foreign instantaneous forward rates as @xmath27 using the musiela parametrization and the standard heath - jarrow - morton drift condition we can write down the domestic forward rate dynamics under the domestic martingale measure @xmath28 analogously under the foreign martingale measure we obtain a heath - jarrow - morton equation for the foreign forward rate dynamics . if the dynamics of the spot log - exchange rate process with respect to the domestic martingale measure is as follows @xmath29 we obtain under the domestic martingale measure the foreign forward rate dynamics as @xmath30 for @xmath31 .",
    "recall the no - arbitrage conditions , namely , the processes @xmath32 are ( local ) martingales on @xmath33 $ ] for @xmath34 and @xmath35 .",
    "this system of stochastic partial differential equations ( spdes ) is of the form , when we consider a vector of risk factors @xmath36 forming an element of a large hilbert space @xmath37 by abuse of notation we use the same letters @xmath38 and @xmath11 for the respective fields on the hilbert space of risk factors , such that we obtain an equation of type . notice that we can easily perform a change of measure in order to attain the physical measure .",
    "this corresponds then to introducing an appropriately modified drift , denoted by the additional term @xmath39 .",
    "in this section we propose a calibration to ( diffusive parts of ) the historical risk factor time series , which does not need neither optimization procedures nor the numerical evaluation of the model in question . instead of using those standard approaches , which pose serious problems in high dimensions , we will rather consider a `` layman s calibration '' of the model , where the characteristics of the spde are deterministically and directly calculated from the observed time series .",
    "this corresponds to the `` historical approach '' , which is standard for several branches of scenario generation , however , we only apply the historical approach at an infinitesimal level ( thinking of a time - tick , usually from hours up to days in scenario generation , as the infinitesimal element ) : the global dynamics is then calculated by means of a stochastic differential equation whose local features are determined by the time series .",
    "we suppose that we are in the setting of section [ basic_setting ] and we assume a time series , i.e. a sample of equation , on equidistant grid points of distance @xmath40 , denoted by @xmath41 .",
    "our goal consists of estimating the volatility directions @xmath42 out of the observations @xmath41 in a simple way ( we regard @xmath43 as the last observation , which appeared one time tick @xmath40 ago ) .",
    "we do really aim for an estimation in the proper sense , i.e. , we would like to have a limit theorem for spdes that relates the number of observations @xmath44 , the time - distance @xmath40 of two contiguous observations , and the number of volatility directions @xmath45 .",
    "we certainly do not want the dimension of the hilbert space of risk factors to enter into the construction , since it might be infinite .    for later use and for the sake of simplicity we state the stochastic partial differential equation of type calibrated to the given sample @xmath41 @xmath46 where @xmath11 is a known , non - vanishing geometric function on the risk factors describing the local dynamics .",
    "we specify in the sequel the necessary conditions for an appropriate limit theorem .",
    "we denote by @xmath47 the dependence of the solution process @xmath48 on the number @xmath44 of observations ( which also determines the distance @xmath49 between two observations ) .",
    "the calibrated equation is defined on an independent stochastic basis with a @xmath50 dimensional brownian motion @xmath51 .",
    "notice here that @xmath52 is specified by no - arbitrage conditions from the volatility structure , as given in the equations and .",
    "furthermore we have to assume the following technical assumption , which allows to construct strong solutions and not only mild ( respectively weak ) ones . for all necessary details see @xcite and @xcite .    [ ass_lambda ] the constant volatility directions @xmath38 are elements of @xmath53 , which is the domain where all powers of @xmath16 are defined",
    "( see for instance @xcite .",
    "furthermore @xmath54 is assumed to be a lipschitz map .",
    "consider equation where @xmath11 is a given map and @xmath55 a given initial value , but the directions @xmath38 are unknown .",
    "we assume assumption [ ass_lambda ] .",
    "we collect a time series of observations @xmath41 on an equidistant time grid of width @xmath40 that cover an interval of length @xmath56 .",
    "refining the observations by making @xmath57 smaller and smaller leads to the following convergence statement : @xmath58 in distribution for any @xmath59 if @xmath60 .",
    "the underlying limit result is the following gaussian one , @xmath61    let @xmath5 be the risk factor hilbert space and @xmath62",
    ". denote by @xmath63 the bilinear operator defined by @xmath64 , for @xmath65 .",
    "linear combinations of these ( one - dimensional ) bilinear operators are dense in the set of hilbert - schmidt operators ( with respect to the hilbert - schmidt norm ) .",
    "let @xmath66 be a hilbert space valued it process given by @xmath67 for a family @xmath68 of square - integrable processes ; then , the quadratic covariation @xmath69 is approximated with respect to the hilbert - schmidt norm by @xmath70 see for instance @xcite .",
    "the gaussian random variable in equation  ( [ gaussian_simulated_equation_explicit_form ] ) has a covariance matrix @xmath71 which precisely corresponds , up to the factor @xmath72 , with the standard statistical estimator the returns ( see remark [ returns ] for the use of the word return here ) of the sample @xmath41 multiplied by @xmath73 . on the other hand ,",
    "this estimator converges , as @xmath44 tends to infinity , to @xmath74 this is due to the fact that the quadratic covariation of the stochastic process @xmath75 is given by @xmath76 , whence @xmath77 is approximated with respect to the hilbert - schmidt norm by @xmath78 notice here that @xmath79 .",
    "this proves the equality of and .    in order to prove the convergence result  ( [ main convergence result ] )",
    "we will have to apply the following limit theorem in distribution for a sequence of stochastic partial differential equations : let @xmath80 and @xmath81 be the unique solutions of the equations @xmath82 where @xmath83 for @xmath59 .",
    "then @xmath84 in distribution as @xmath85 .",
    "in order to show  ( [ limit theorem we need ] ) , we start by noticing that , due to the previous consideration we have @xmath86 { } b\\ ] ] in distribution on the hilbert space @xmath5 . the limit theorem  ( [ limit theorem",
    "we need ] ) can be concluded from the stability theorems in @xcite and the uniqueness in law established in @xcite : first , by uniqueness in law , the actual brownian motion in use is not relevant .",
    "hence , we can interpret the convergence of @xmath87 as @xmath85 with respect to independent brownian motions @xmath88 , each with a certain volatility @xmath89 and associated to a certain eigendirection @xmath90 , that is , we can rewrite in law @xmath91 this yields , for each @xmath92 , vector fields converging to a limit as @xmath85 , namely the eigensystem of @xmath93 , if one carefully enumerates the volatilities @xmath94 in increasing order and keeps track of multiple eigenvalues .",
    "second , if we reformulate the stochastic partial differential equations and with respect to the sequence of independent brownian motions @xmath95 , we find ourselves with a sequence of vector fields converging to well - specified limits .",
    "finally , the required statement follows from the stability result in proposition 9.1 of @xcite .",
    "the methodology in the previous theorem is related to the one in @xcite in the sense that we read from the quadratic variation process and its approximations the relevant information for the a posteriori construction of a process which is close in law to the originally observed one .",
    "indeed , in @xcite the quadratic variation process and its approximations are  via harmonic analysis methods  investigated in order to reveal information on the non - linear volatility vector fields .",
    "in contrast , we do assume knowledge on the type of non - linearity in the equation and tackle the problem of determining the volatility directions @xmath96 .",
    "the convergence results based on wiener s theorem , as stated in @xcite , could be similarly applied .",
    "notice that we do not need to calculate empirical covariance matrices of the time series in order to to capture the local correlation structure of * all * risk factors in the calibrated equation .",
    "up to some technical complications one could also consider @xmath97 here .",
    "besides the calibration of the risk factor spdes proposed in equation we have to take into account stochastic volatility .",
    "we express stochastic volatility in our setting through the appearance of random times , which can be nicely interpreted as difference between physical time and trading time . as",
    "the spirit of this article is to embed a static sampling problem ( in contrast to the distributional approach where only two random variables are described ) into a dynamic problem defined by stochastic ( partial ) differential equations , we also embed the random time into a random time change , which can then a fortiori be observed through an stochastically changing volatility process .",
    "this approach is a continuous time version of garch - approaches , which are applied in filtered historical simulation in risk management ( see @xcite ) . here",
    ", in contrast , we `` calculate '' stochastic volatility along the time series as realized volatility and rescale the time series by this realized volatility before we apply those rescaled ( `` filtered '' ) returns to calibrate the underlying stochastic differential equations as proposed in equation    in this section we therefore introduce the additional methodology , which allows us to include these changing volatility effects of the observed diffusion processes and which permits jumps",
    ". both phenomena , stochastic volatility and jumps , are crucial to capture important stylized facts of financial time series .",
    "for instance , when we model rare ( extreme ) events we those events as additional jumps added to the diffusive setting of equation . recall that these additional jumps also change the drift @xmath12 due to no arbitrage conditions . in this section",
    "we assume the existence of a basis @xmath98 of the respective hilbert spaces of risk factors in order to speak about the risk factors @xmath99 , which denote the projection of @xmath100 onto the @xmath101-th basis element .",
    "[ returns ] we shall apply in the sequel the notion _ return _ for two slightly different concepts .",
    "when considering a time series @xmath41 , which is a collection of vectors , then we shall call any difference @xmath102 an ( observed ) return ( of risk factors ) , where we should in fact speak of a vector of returns @xmath103 of single risk factors @xmath99 . running indices for risk factors",
    "are always denoted by @xmath104 , the index along the time series is usually @xmath105 .",
    "the length of the time series will be always be @xmath44 .",
    "we define rare events using known time series of the risk factors and by the risk manager s very own intuition .",
    "let @xmath41 denote a time series of risk factors .",
    "we form a time series of returns @xmath106 for @xmath107 from it and estimate the empirical ( co-)variance @xmath108 among its different components .",
    "additionally we define local extractions of the time series of length @xmath109 , i.e. @xmath110 , for @xmath111 and for @xmath112 .",
    "the empirical ( co-)variance of this extraction is denoted by @xmath113 and corresponds to the estimation of the volatility looking back @xmath109 days , usually called ( local ) realized volatility .",
    "we now consider a diffusive equation of type with jumps and random time change .",
    "more precisely , let @xmath114 be a random time change for risk factor @xmath104 , i.e. @xmath115 is a non - negative , locally integrable previsible stochastic process ( with respect to the natural filtration of the brownian and poissonian componentes ) , whence @xmath116 is an almost surely increasing random time .",
    "then the equation to be calibrated looks like @xmath117 where @xmath118 denotes a poisson process with values in @xmath5 and jump rate @xmath119 calibrated from the time series .",
    "the solution of this random spde can be written through the random time change by solving up to the time @xmath120 for the @xmath101-th risk factor .",
    "it is possible to estimate the volatility levels @xmath121 from the market up to an ( a priori unknown ) factor by observing a proper quadratic variation process between two jumps at time @xmath122 and @xmath123 , namely @xmath124    as a second notation we introduce the ratios of local empirical volatility and today s local volatility , namely @xmath125 which is defined for each risk factor @xmath104 separately .",
    "we denote this quantity by @xmath126 at time @xmath105 in the time series .",
    "notice that this quantity is well - defined , even under the regime of a random time change , since the ( unknown ) proportionality factor cancels out .",
    "[ rescaled_returns ] let @xmath41 denote a time series of risk factors .",
    "we form a time series of returns @xmath106 for @xmath107 from it and denote by @xmath127 for risk factor @xmath104 and @xmath128 the rescaled returns of the time series .    in order to make the choice of extreme events more stable along time",
    ", one can choose @xmath129 for re - scaling the returns and for extracting extreme events independently .",
    "in the subsequent example this is the case .    in order to calibrate the equation in the spirit of we start from the set of rescaled returns and separate from it the set of extreme returns .    a return at index @xmath130 is said to be extreme at level @xmath131 with @xmath66 violations if @xmath132 holds true for at least @xmath66 different risk factors @xmath104 .",
    "the set of all indices belonging to extreme events at level @xmath131 with @xmath66 violations is denoted by @xmath133 and is a subset of the set of all indices @xmath134 .    by means of the set",
    "@xmath133 we can now define a jump structure , where with a certain jump rate and jump measure jumps from @xmath133 are mixed ( in an independent fashion ) into the diffusion process calibrated with returns from the set @xmath135 .    for the purpose of numerical efficiency of the simulation we shall most of the time neglect the fact that the random time change depends on each risk factor @xmath136 .",
    "actually the simulation time @xmath137 is chosen with respect to the distribution of @xmath138 , which is estimated from an averaged realized variance structure .",
    "in our case the parameters of the spde are specified by the the empirical calibration method outlined in section [ calibration ] and by an a priori choice of the geometric term @xmath11 .",
    "therefore , in order to fully characterize the equation , the following points need to be taken into account :    1 .",
    "determine the sets @xmath133 and @xmath135 by fixing a level @xmath139 at which events are considered as extreme and a number @xmath66 of violations .",
    "the number of involved brownian motions is the cardinality of @xmath140 .",
    "the volatility directions are given by rescaled returns using equation .",
    "the semigroup @xmath141 is the shift semigroup for term structure of the risk factors , otherwise the identity .",
    "@xmath11 is chosen a priori according to the ( geometric ) structure of the problem , i.e. , some risk factors have to stay positive ( like interest rates ) , others are free ( like log - prices ) .    for the evaluation of the spde we propose an explicit - implicit euler scheme",
    "( see for instance @xcite ) , which has the following one - step euler structure :    * prepare an initial risk factor @xmath142 .",
    "* trigger a jump ( with a specified jump rate ) , or not .",
    "if a jump is triggered , then sample its size using the jump measure and add it to @xmath142 .",
    "* choose brownian increments @xmath143 with variance @xmath144 along the time increment @xmath144 .",
    "* evaluate @xmath145 * terminate the euler step .    in our concrete implementations",
    "the choice of @xmath11 depends on the respective risk factors .",
    "for instance , for interest rates we have chosen fields of linear operators depending via square - root like functions on the level of yield at certain maturities .",
    "in this last section we present results from a concrete implementation of a scenario generator for five foreign currencies ( chf , gbp , jpy , usd , zar ) and six yield curves , i.e. , five foreign yield curves and one domestic yield curve ( euro zone ) . for this purpose",
    "we use market data from march 2006 to march 2009 , including in particular the highly volatile events of the last quarter of 2008 . in total , this risk management problem consists of @xmath146 risk factors , even though more points on the yield curve could have been evaluated . in a given day , the available historical time series has a length of @xmath147 business days back in time , i.e. , in march 2009 we look back up to march 2007 , for instance . the implementation is based on section [ basic_setting ] and the systems of equations presented therein . in the light of sections [ calibration ] and section [ fine_tuning ] the realized variances ( with @xmath148 ) and the set of extreme events ( for the calculation of extreme events we used @xmath149 )",
    "are calculated : the number of violations @xmath150 and the level @xmath151 yield a number of extreme events of @xmath152 ( for @xmath153 , i.e.  an extreme event occurs if we have one violation for one risk factor , we obtain @xmath154 extreme events ) .",
    "the jump rate is chosen to be @xmath155 percent ( yielding about @xmath156 jump events in @xmath157 scenarios ) .",
    "the random time change has been chosen ad - hoc to be @xmath158 days in @xmath159 percent of the cases and @xmath160 days in @xmath161 percent of the cases for the sake of simplicity .",
    "a precise calibration might yield even better results .",
    "the following pictures show several single distinguished historical and generated distributions of risk factors .",
    "the historical distributions are rescaled by @xmath162 as described in definition [ rescaled_returns ] .",
    "the last figure shows the backtesting results for a randomly chosen roll - over portfolio of linear fx- and ir - instruments , i.e. , the times to maturity of instruments in the portfolio are kept constant along the backtesting .",
    "the distributional plots show on the left hand side the historical distribution of log - returns for one day and on the right hand side the simulated distributions of log - returns for one day .",
    "it is apparent that our simulated scenarios resemble the historical distributions by smoothening them in a consistent way along all risk factors and by preserving extreme risks .",
    "we obtain backtesting results of four var - violations and two portfolio - loss - values beyond the level of expected shortfall in 250 days at a @xmath163 confidence level .                    in the third set of plots",
    "the mass transport effect is seen in an even more extreme way , since for zar a lot of returns have historically been zero . in this sense",
    "the constructed scenario generator also has a `` repairing '' effect on data .",
    "finally , we present backtesting results of a two roll - over portfolios during @xmath164 business days at the @xmath163 confidence level .",
    "returns are in red , var in blue and the value of expected shortfall is in green .",
    "one can clearly see the increase of the level of var ( and expected shortfall ) during the events of the last quarter 2008 .",
    "we have omitted the scaling on the y - axis as violations are only a level of relative size ."
  ],
  "abstract_text": [
    "<S> we provide a new dynamic approach to scenario generation for the purposes of risk management in the banking industry . </S>",
    "<S> we connect ideas from conventional techniques  like historical and monte carlo simulation  and we come up with a hybrid method that shares the advantages of standard procedures but eliminates several of their drawbacks . instead of considering the static problem of constructing one or ten day ahead distributions for vectors of risk factors , we embed the problem into a dynamic framework , where any time horizon can be consistently simulated . </S>",
    "<S> additionally , we use standard models from mathematical finance for each risk factor , whence bridging the worlds of trading and risk management .    </S>",
    "<S> our approach is based on stochastic differential equations ( sdes ) , like the hjm - equation or the black - scholes equation , governing the time evolution of risk factors , on an empirical calibration method to the market for the chosen sdes , and on an euler scheme ( or high - order schemes ) for the numerical evaluation of the respective sdes . the empirical calibration procedure presented in this paper </S>",
    "<S> can be seen as the sde - counterpart of the so called filtered historical simulation method ; the behavior of volatility stems in our case out of the assumptions on the underlying sdes . </S>",
    "<S> furthermore , we are able to easily incorporate `` middle - size '' and `` large - size '' events within our framework always making a precise distinction between the information obtained from the market and the one coming from the necessary a - priori intuition of the risk manager .    results of one concrete implementation are provided .    </S>",
    "<S> * key words : * risk management , stochastic ( partial ) differential equation , calibration , historical simulation , time series , jump processes </S>"
  ]
}