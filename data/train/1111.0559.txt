{
  "article_text": [
    "edge sparsity in an undirected graphical model ( markov random field ) encodes conditional independence via graph separation .",
    "essentially , graphical models detangle the global interconnections between the random variables of a joint distribution into localized neighborhoods . any distribution @xmath2 consistent with the graphical model must abide by these simplifying constraints .",
    "thus , the graph learning problem is equivalent to a model class selection problem .",
    "let @xmath3 be an undirected graph on @xmath4 vertices and @xmath5 edges .",
    "let @xmath6 denote a random vector with distribution @xmath2 , where variable @xmath7 is associated to vertex @xmath8 .",
    "graphical model selection attempts to find the simplest graph , often dubbed the _ concentration graph _ ,",
    "consistent with the underlying distribution .",
    "recent work in graphical model selection exploits the local structure of the underlying distribution to derive consistent neighborhoods for each random variable . in terms of graphs ,",
    "the neighborhood set of a vertex @xmath9 is @xmath10 .",
    "more importantly , for undirected graphical models , @xmath11 is the markov blanket of @xmath9 , where @xmath12 is rendered conditionally independent of all other variables given @xmath11 : @xmath13 . to estimate the neighborhood conditional probabilities @xmath14 ,",
    "these methods employ pseudo - likelihood measures , specifically @xmath0 regularized regression : the lasso [ [ tibsh1 ] ] .",
    "compared to other @xmath15 penalty based regularization schemes , the @xmath0 penalty enjoys the dual properties of convexity and sparseness by straddling the boundary between the two domains . by treating @xmath12 as the response variable and @xmath16 as the predictors in a generalized linear model",
    ", the @xmath0 regularization penalty can recover an appropriately sparse representation of @xmath11 . reconstructing the full edge set of the graph using the estimated neighborhood @xmath17 ,",
    "allows for two alternate definitions : @xmath18 which we call and ; or @xmath19 which we call or .",
    "ravikumar et .",
    "al [ [ ravi1 ] ] , [ [ ravi2 ] ] consider the problem of estimating the graph structure associated with a gaussian markov random field and the ising model .",
    "their main result shows that under certain assumptions , the problem of neighborhood selection can be accurately estimated with a sample size of @xmath20 for high dimensional regimes where @xmath21 is the max degree , and ( @xmath22 ) .",
    "note that the number of samples needed is further improved in [ [ ravi1 ] ] to @xmath23 for gmrf model selection .",
    "meinshausen et .",
    "al [ [ graphlasso ] ] also examine the gmrf case , and provide an asymptotic analysis of consistency under relatively mild conditions along with an alternate @xmath24 penalty .",
    "we build upon this previous work by extending the @xmath0 penalized neighborhood estimation framework to use the elastic net [ [ elasticnet ] ] @xmath25 penalty and expanding the scope of graphical model recovery to include the multinomial discrete case . while the lasso performs beautifully in many settings , it has its drawbacks .",
    "in particular , when @xmath26 , the lasso can only select at most @xmath27 variables .",
    "moreover , for highly correlated covariates , the lasso tends to select a single variable to represent the entire group . by incorporating the @xmath28 penalty term ,",
    "the elastic net is able to retain the lasso s sparsity while selecting highly correlated variables together .",
    "additionally , we introduce a novel scheme for augmenting neighborhood recovery by pooling pair - wise neighborhood union estimates .",
    "the idea is to infer the joint neighborhood of a pair of nodes @xmath29 ( not necessarily adjacent ) , and obtain the neighborhood of node @xmath30 by combining all the information given by the @xmath31 pairs of nodes containing node @xmath30 .",
    "the frequency with which nodes appear in a specially designed neighbor list for node @xmath30 gives us a weighted ranking of nodes in terms of their neighbor likelihood .",
    "this method can be combined with the usual neighborhood recovery to extract more information from a possibly insufficient set of samples .",
    "undirected graphical models encode the factorization of potential functions over cliques , which in their most basic form , are comprised of 1st and 2nd order interactions : functions that map node and edge values to the real line : @xmath32 which for max - entropy exponential family distributions , can be written as @xmath33 note that @xmath34 represents the normalization constant or partition function .    for continuous random variables , the most common exponential family mrf representation is the multivariate gaussian with sufficient statistics @xmath35.@xmath36 the @xmath37 symmetric pairwise parameter matrix @xmath38 , known as the inverse covariance matrix of x denotes the partial correlations between pair of nodes , given the remaining nodes .",
    "every edge @xmath39 will have a non - zero entry in @xmath38 and each row @xmath40 of @xmath38 specifies the graph neighborhood @xmath41 .",
    "conversely , the sparsity of @xmath38 reveals the conditional independencies of the graph where @xmath42 .",
    "conditional neighborhood expectations can be represented by a linear model : @xmath43 .    in the binary case , the mrf distribution can be described using an ising model where @xmath44 , and @xmath45 .",
    "the full probability distribution takes the following form , which omits first order terms : @xmath46 the conditional neighborhood probability @xmath47 is defined as : @xmath48 taking the hessian of the local conditional probability gives the fisher information matrix for @xmath49 , much like partial correlations in the gaussian concentration matrix , zero entries in the fisher information matrix indicate conditional independence . extending the discrete parameterization to variables with @xmath50 states , requires an expansion in terms where the edge potential functions @xmath51 now describe a set of parameterized indicator variables @xmath52 representing the @xmath53 possible value pairs between @xmath49 and @xmath54 . @xmath55 as described in [ [ expfamily ] ] , this particular representation is over complete since the indicator functions satisfy a variety of linear relationships @xmath56 .",
    "however , despite the lack of a guaranteed unique solution , the factorization can still satisfy the desired neighborhood recovery criterion .",
    "a simplified variant of the general discrete parameterization is the potts model where each @xmath51 is defined by two indicator functions denoting node agreement and disagreement for arbitrary @xmath57 .",
    "we observe that in the ising model , the form of @xmath51 may be be recast as @xmath58 .",
    "@xmath59 note that the potts model only requires a single parameter and generalizes the ising model to @xmath60 states .    to extend neighborhood estimation from the binary ising model case to a discrete parameterization",
    ", we note that the neighborhood conditional probability takes the form @xmath61 which is equivalent , after a variable transformation from a discrete feature space to indicators , to the classical multinomial logistic regression equation : @xmath62 where @xmath63 with an additional singleton indicator variable @xmath64 always set to 1 . with the conditional probability equations in hand",
    ", we can approach the problem of neighborhood estimation as a generalized linear regression .",
    "building on previous model selection work using the @xmath0 penalty , we extend the approach , to use the combined @xmath65 penalty approach of the elastic net [ [ elasticnet ] ] , which for the basic linear model takes the form : @xmath66 the elastic net performance surpasses the @xmath0 penalty under noisy conditions and where groups of highly correlated variables exist in the graph . however , as noted by bunea [ [ bunea ] ] , the additional @xmath28 smoothing penalty should be small relative to the @xmath0 term to preserve sparsity .",
    "many authors have extended the elastic net penalty to additional regression models , covering a broad swath of the generalized linear realm . for the linear gaussian case , we use the original elastic net package of zhou and hastie [ [ elasticnet ] ] . for binary and multinomial regression",
    "we rely on the glmnet library of friedman et .",
    "al [ [ fried1 ] ] .",
    "to evaluate the elastic net for gaussian mrf model selection , we generate the distribution inverse covariance matrix @xmath67 in the following way .",
    "we set @xmath68 whenever @xmath69 , and then perturb the diagonal of the matrix @xmath70 , with @xmath71 large enough to force all eigenvalues of @xmath38 to pe positive .",
    "we experimentally choose @xmath71 , starting from @xmath72 and increasing it in increments of @xmath73 until we get a value that makes @xmath38 positive definite . in the case of the binary and discrete models , we require a more complicated procedure based on mcmc sampling .",
    "however , given the size of our graphs , the direct gibbs sampling approach proved to be computationally expensive because of its long mixing times and slow mode exploration when the temperature ( the @xmath74 s in our case ) is low .",
    "to overcome this difficulty , we turn to the swendsen - wang algorithm .",
    "this method generates an augmented graph @xmath75 , where @xmath76 and @xmath77 contains @xmath78 iff @xmath79 and @xmath80 are incident .",
    "given this formulation , @xmath81 is bipartite between the @xmath82 nodes and @xmath83 nodes .",
    "thus in the joint distribution of @xmath81 , the markov blanket of @xmath83 will only consist of elements in @xmath82 and vice versa .",
    "the random variables assigned to @xmath83 can only take the values 0 and 1 .",
    "we define the conditional probabilities of @xmath82 and @xmath83 as :    * @xmath84 is given by considering the nodes @xmath85 s.t .",
    "@xmath86 is incident with @xmath40 and @xmath87 .",
    "@xmath88 if @xmath89 and 0 otherwise .",
    "* @xmath90 is such that all nodes in the same component ( in the graph when we consider only the edges @xmath86 s.t .",
    "@xmath91 ) have the same value , and each component takes each of the @xmath60 possible values with equal probability .",
    "essentially , the algorithm generates mcmc samples by alternately updating the values of @xmath82 and @xmath83 using gibbs sampling .",
    "although the augmentation substantially increases the number of vertices , the algorithm creates a markov chain that explores the space of outcomes much more rapidly . for the details of the swendsen - wang algorithm",
    ", we refer the reader to [ [ swendsenwang ] ] and [ [ mackay ] ] . by introducing an @xmath28 penalty term to the regression model , the maximization problem in the elastic net setup becomes @xmath92 with @xmath93 .",
    "we experimented with several values of @xmath94 for different number of samples , and observed the type i and type ii probability errors .    in figure [ surfaceandor ]",
    "we show 3d plots of the total error rates as a function of the number of samples and the @xmath94 parameter , for the and and or neighborhood estimation .",
    "several observations can be made from these plots .",
    "first note that larger values of @xmath94 performs worse than smaller values , no matter what the sample size is .",
    "the best recovery rates are achieved when @xmath94 is very small .",
    "also note that the and neighborhood selection perform much worse than its or counterpart when @xmath94 is large .",
    "the figure [ p40errvsn ] plots the error rates versus the sample size , for a fixed @xmath93 .",
    "note that the chosen graph has @xmath95 vertices and is of maximum degree @xmath96 , and it can not be recovered without errors even when the number of samples scales as @xmath97 .",
    "the first family of graphs we tested were the star graphs and the more general clique of star graphs .",
    "we denote by @xmath98 the clique - star graph obtained from @xmath99 copies of a star graph connected to the remaining @xmath100 vertices of degree @xmath72 ] by connecting all star centers among them , or in other words we add @xmath21 different neighbors to each vertex in a clique of size @xmath99 .",
    "@xmath102 is just the standard star graph .",
    "note that the maximum degree in @xmath98 is @xmath103 and the total number of vertices is @xmath104 .    for a graph @xmath105",
    "we let @xmath106 denote the edge density of the graph , i.e. @xmath107 .",
    "the reason we introduce this parameter in our simulations is to observe the impact of edge density on the recovery rates when the maximum degree and the number of samples are fixed . as our simulations show , recovering the graph structure is significantly harder when the graph has a higher density but the same fixed maximum degree @xmath21 . to test this",
    ", we generate a star graph with maximum degree @xmath21 and edge density @xmath108 , and then start adding edges among the lower degree neighbors to obtain a new graph with new edge density @xmath109 . note that this edge density dependence can be equivalently formulated in terms of the average degree @xmath110 of a graph by the formula @xmath111 .",
    "the error rates are averaged over @xmath112 runs for a fixed graph @xmath105 on different samples of size @xmath27 .",
    "additionally , @xmath94 was chosen by discretizing the interval @xmath113 $ ] into @xmath114 equally sized subintervals .",
    "figure [ star2rhos ] plots the error recovery rates for @xmath115 with @xmath116 ( top ) and for the graph obtained by adding edges to @xmath117 until @xmath118(bottom ) , with both graphs having the same maximum degree @xmath119 .",
    "note that for these graphs @xmath120 and as seen in the top plot of figure [ star2rhos ] , a bit over @xmath121 samples are enough to bring the error rates to zero .",
    "however , the bottom shows that even with @xmath122 times more samples , we can only recover @xmath123 with a @xmath124 error rate .",
    "we repeat the above experiment for the clique - star graph @xmath125 with @xmath126 and edge density @xmath127 , and the graph @xmath128 obtained by adding edges to @xmath129 while keeping the maximum degree @xmath130 unchanged . in this case ,",
    "@xmath131 and we successfully recover @xmath129 only when the sample size exceeds @xmath132 , due to higher edge density ( plot omitted ) .",
    "figure [ wstar2rhos ] shows the error rates when we increase the edge density to @xmath133 , which emphasizes the increase in sample size required for graph recovery .",
    "note that in both simulations the @xmath94 penalty was rarely of any help , and in most cases @xmath134 achieved the best error rates .",
    "-axis ) versus the @xmath94 parameter ( @xmath135-axis ) for the graphs @xmath136 with @xmath137 ( top ) and @xmath123 with @xmath138 ( bottom).,scaledwidth=80.0% ]    a second type of graph we considered was the community graph , denoted by @xmath139 , which consists of @xmath40 groups of highly connected nodes , where each group has size @xmath87 , so @xmath140 .",
    "two vertices within the same group ( or community ) are connected with probability @xmath141 , while nodes that belong to two different communities share an edge with a smaller probability @xmath142 .",
    "these community structures are a common feature of complex networks , and have the property that nodes within a group are much more connected to each other than to the rest of the network ( for @xmath143 ) . in application , these communities may represent groups of related individuals in social networks , topically related web pages or biochemical pathways , and thus their identification is of central importance . to completely understand the modular structure of such graphs , one should be able to both detect overlapping communities and make meaningful statements about their hierarchies [ [ ohcom ] ] .",
    "figure [ com32 ] plots the error rates in the recovery of a @xmath144 graph with @xmath145 , @xmath146 , and @xmath147 .",
    "@xmath148 , however again even with 9000 samples , the error rate is still over 10 percent . when few samples are available , @xmath134 achieves the best error rates , but as we increase the number of samples we notice that the elastic net method with @xmath149 performs slightly better than when @xmath150 .",
    "while this improvement is not significant , it hints that the additional @xmath28 regularization may produce better results in some cases .    .",
    "figures [ ising64 ] and [ potts64 ] depict the performance of the elastic net neighborhood estimator over a range of discrete mrf graphs .",
    "the graphs evaluated for the ising and potts model are random graphs with bounded maximum degree .",
    "all experiments were run over a sample range covering the @xmath151 edge recovery threshold and @xmath152 values ranging from 0.5 to 1 , where @xmath153 .",
    "results from multiple trials were averaged for the ising model . due to the computational load of the _ glmnet _ multinomial regression for large data sizes ,",
    "only single run results are shown for the potts model .",
    "smaller @xmath152 values are omitted from the plot in order to limit the scale and improve clarity .",
    "unless otherwise noted , the plots represent and neighborhood unions , with or neighborhood unions showing similar performance .",
    "as seen in the plots , the elastic net neighborhood estimator recovers the underlying graph with high probability under corresponding @xmath154 sample sizes , validating our formulation of the discrete model neighborhood estimation as a multinomial logistic regression .",
    "similar to the gaussian case , the effect of the @xmath28 penalty @xmath94 , ( while @xmath24 is set to @xmath155 ) tends to benefit neighborhood recovery mostly at small sample values and when @xmath152 is close to 1 .",
    "oversized @xmath28 penalties introduce an inordinate number of noise edges , but small @xmath28 penalties reduce the chance of missing edges with weak correlation which the @xmath0 penalty rejects . when the @xmath28 penalty is non - zero , the minimization function is strictly convex and allows the estimator to select additional nodes that exhibit highly correlated behavior by effectively averaging their contribution .",
    "the @xmath94 parameter provides , in essence , a trade - off between precision and recall , as it can be seen in figure [ isingpotts64rp ] , especially when the number of samples is small , which is the case in many high dimensional @xmath156 applications .",
    "while the @xmath157 curve consistently provides the highest graph recovery precision over all sample sizes , the actual number of recovered edges may be extremely limited due to the sparsity constraint . for the ising model graph depicted in the figure ,",
    "the smallest sample size 1200 with @xmath157 gives a precision of 0.88 with a recall of 0.7 . by introducing a @xmath94 term with @xmath158 , the precision drops to 0.8 but",
    "the recall improves to 0.79 , which is better than a 1 to 1 trade - off . as expected , with large sample sizes , the benefit of the @xmath94 parameter diminishes as shown by the nearly vertical slope of the large sample size curves .",
    "similarly , the potts model recall - precision plot also displays this trend , albeit in a compressed fashion since the neighborhood estimator is able to recover the graph at a smaller sample size , rendering the larger sample size curves uninformative . from these results",
    "we can say that the additional presence of an @xmath159 penalty may yield substantial benefits for @xmath156 situations where the goal is to extract relevant correlation information from small sample sizes .",
    "the technique introduced in this section is useful in neighborhood reconstruction especially in the case of regular graphs , graphs with a small gap between known maximum and minimum degree , or when we would like to obtain a likelihood ranking of the @xmath160 possible neighbors of a fixed node @xmath30 .",
    "the idea is to infer neighborhoods not only for one vertex at a time , but for pairs of vertices , which may or may not be adjacent .",
    "we denote by @xmath161 the estimation of the neighborhood of node @xmath30 , as given by the optimization in equation ( [ enetgmrfeq ] ) .",
    "we denote by @xmath162 the set of neighbors of nodes @xmath30 and @xmath163 , i.e. @xmath164 , in other words @xmath162 is the union of the neighborhoods of nodes @xmath30 and @xmath163 , minus the edge @xmath29 if it exists .",
    "we now define the following optimization problem similar to the one in equation ( [ enetgmrfeq ] )    @xmath165    after grouping the terms of @xmath24 and @xmath94 and approximating @xmath166 by @xmath167 , and @xmath168 by @xmath169 , we approximate ( [ tij ] ) by @xmath170 where in the last step we make the change of variable @xmath171 and we denote by @xmath172 the regression coefficients of the sum of variables @xmath173 and @xmath174 against the remaining variable .",
    "we now define the estimated neighborhood of a pair of vertices @xmath29 , not necessarily adjacent , to be @xmath175 , in other words @xmath176 is an estimate of @xmath162 .",
    "note that for a vertex @xmath177 it may be the case that @xmath178 is adjacent to either @xmath30 or @xmath163 or perhaps both .",
    "for a fixed node @xmath30 , we obtain its neighborhood in the following way .",
    "we let @xmath179 denote the list obtained by concatenating the pair neighborhoods of node @xmath30 @xmath180 where @xmath181 denotes union with repetitions .",
    "we denote by @xmath182 the concatenation of the estimated pair neighborhoods , i.e. @xmath183 . note that @xmath179 includes all nodes that are neighbors of @xmath30 , each appearing with multiplicity @xmath184 . if @xmath163 is a neighbor of @xmath30 , then @xmath185 for all @xmath186 , i.e. @xmath184 times . in the absence of errors",
    "@xmath188 , and with the exception described in the next paragraph , we can correctly recover the neighborhood of node @xmath30 by picking the most frequent elements from @xmath189 , i.e. all nodes which appear in the list exactly @xmath184 times . in the case of errors ,",
    "we obtain an estimate for the neighborhood of node @xmath30 by selecting the most frequent elements in @xmath190 .",
    "also , if there are no errors , @xmath179 contains all other nodes @xmath191 of @xmath105 at least once .",
    "this is obvious if @xmath192 , as @xmath163 appears @xmath184 times in @xmath193 as explained above .",
    "if @xmath194 , then pick @xmath60 a neighbor of @xmath163 ( @xmath60 exists since we assumed @xmath105 is connected ) and it must be that @xmath195 .",
    "note that a non - neighbor node @xmath163 of @xmath30 can appear @xmath196 times in @xmath197 if @xmath163 is connected to all nodes in @xmath198 , in which case we ( incorrectly ) add @xmath163 to the neighborhood of node @xmath30 .",
    "similarly , if @xmath30 is connected to all nodes in @xmath199 , then @xmath30 appears in @xmath200 with multiplicity @xmath184 and we ( incorrectly ) mark @xmath30 as being in the neighborhood of node @xmath163 .",
    "in other words , @xmath30 and @xmath163 appear in each other s neighborhood lists , thus rendering our approach incorrect , whenever @xmath30 and @xmath163 are both connected to all other @xmath184 vertices in the graph .",
    "however , for random graphs this scenario occurs with a very low probability .",
    "as shown in figure [ neighborhistogram ] , true neighbors of node @xmath30 occur most frequently in @xmath182 . when ordering vertices in @xmath182 based on their frequency , most of the true neighbors appear at the top of the list .",
    "however , errors occur and false neighbors sometimes precede true neighbors .",
    "note however that there are cases when the single neighborhood estimation performs much worse and omits many true neighbors .",
    "we have seen how histograms based on neighborhoods of pairs are useful in determining a likelihood ranking of possible neighbors of a given node .",
    "the top @xmath201 most frequent elements in @xmath182 are the most likely neighbors of @xmath30 .",
    "the problem now becomes how to select this threshold value @xmath201 for each list .",
    "if @xmath201 is too small then true neighbors might be left out , and if @xmath201 is too big then we will introduce false neighbors .",
    "we make an additional observation that improves on the accuracy of the above ordering obtained from @xmath182 .",
    "denote by @xmath202 the matrix formed from lists @xmath182 by letting @xmath203 equal the frequency of node @xmath204 in list @xmath182 . to incorporate the symmetry between two neighboring nodes :",
    "if @xmath30 is a neighbor of @xmath163 then @xmath163 is also a neighbor of @xmath30 , we build the symmetric matrix @xmath205 . the intuition here is to average out the votes received by nodes @xmath30 and @xmath163 in their respective rows .",
    "suppose that @xmath206 , but @xmath163 does not rank highly in @xmath182",
    ". however , it may be the case that @xmath30 ranks highly in @xmath207 , and helps in identifying that @xmath29 are indeed neighbors in @xmath105 .",
    "one can think of this method as averaging out the bad information ( noisy edges ) and boosting up the good information ( correct edges ) .    finally , another alternative would be to first row normalize @xmath202 and then construct the symmetric matrix described above .",
    "we divide each row in @xmath202 by the largest entry in that row and obtain the row stochastic matrix @xmath208 , whose row @xmath30 may be thought of as ranked probabilities of the possible neighbors of @xmath30 .",
    "we then build the row stochastic matrix @xmath209 , as described above .    as mentioned earlier ,",
    "the main problem is finding the threshold @xmath201 for each row @xmath30 to separate the neighbors for non - neighbors of @xmath30 . if we know a priori what the degree of each node is , then one way to pick the neighbors would be to select the most frequent @xmath210 entries in @xmath182 .",
    "alternatively , if we know that the graph is almost regular of degree @xmath9 , or in other words that the average degree of the graph g is @xmath9 but the degree distribution has very little variance around @xmath9 , then we can again select the top @xmath9 most frequent entries in @xmath182 . another way one can choose a threshold is to plot the frequency values in order and look for a big jump in the graph",
    "this idea is illustrated in the right plot of figure [ neighborhistogram ] .",
    "note how the frequency values decreases suddenly within two steps from 26 ( for node 6 ) to 22 ( for node 15 ) and then to 18 ( for node 35 ) .",
    "such large sudden drops in the ordered list of frequency values hint at a good threshold point .",
    "table [ oraclecomparison ] shows the results of an experiment that illustrates the above ideas when we have oracle information about the degree of each node .",
    "we observe that the symmetric matrix @xmath211 works better than just using @xmath202 , and that @xmath212 performs better than @xmath211 .",
    "note also that the type i and ii errors are now more balanced both in the and and or case .",
    "when doing the estimation @xmath213 , in the and case almost all the errors were coming from missing edges , while in the or scenario almost all errors were given by false edges . picking the threshold @xmath201 allows for a trade - off between these two type of errors .",
    "method & i & ii & total & i & ii & total + @xmath213 & 0.04 & 0.58 & 0.63 & 0.12 & 0.36 & 0.48 + @xmath214,@xmath202 & 0.04 & 0.27 & 0.31 & 0.32 & 0.09 & 0.41 + @xmath214,@xmath211 & 0.07 & 0.17 & 0.24 & 0.15 & 0.05 & 0.20 +    ' '' ''    @xmath214,@xmath212 & 0.05 & 0.14 & 0.19 & 0.135 & 0.045 & 0.18 +    [ oraclecomparison ]    it would be interesting to compare the results of our new pair neighborhood recovery to the original single neighborhood method , when both techniques take into account the knowledge of node degree .",
    "one way to incorporate this information into the single neighborhood method is to avoid using 10-fold cross validation and replace it with the following procedure .",
    "we have seen that when regressing variable @xmath30 against all other variables , the elastic net method does not return a single @xmath215 vector corresponding to the optimal @xmath24 value , but rather computes an entire matrix ( or sequence ) where each row corresponds to a value of @xmath24 at which an additional variable  turns on  . instead of picking the empirically optimal row with the 10-fold",
    "cross validation method as our @xmath215 vector , we can simply pick the first row which has @xmath216 nonzero entries , where @xmath216 is the degree of node @xmath30 .",
    "one can also interpret the order in which the remaining @xmath160 variables  turn on `` as a way of ranking the potential neighbors . a variable which ' ' turns on ",
    "sooner on the elastic net path is more likely to be a neighbor of @xmath30 than a node which activates later on .",
    "this ranking can also be obtained by our pair - wise neighborhood union estimate , however we produce more than just a simple ranking .",
    "the frequency of each node in @xmath217 combines additional information , and one can interpret this ordering as a weighted ranking of possible neighbors of @xmath30 . this additional information may capture longer range correlations that elude single neighborhood estimation , as suggested in a recent paper of bento and montanari [ [ difficultgraph ] ] .",
    "in this paper we considered the problem of estimating the graph structure associated with a gmrf , the ising model and the potts model .",
    "building on previous work using @xmath0 penalized neighborhood estimation , we experimented with the an additional @xmath28 penalty term .",
    "simulations across the three models show that a small but non - negligible @xmath94 penalty term improves the edge recovery rates when the sample size is small by trading precision for recall .",
    "we make the observation that in the gmrf model , the addition of the @xmath28 penalty term does not have much influence on the recovery rates .",
    "numerical simulations confirm our hypothesis that the lower bounds on the number of samples needed for recovery should not only be a function of the maximum degree @xmath21 and number of nodes @xmath218 , but also of the edge density @xmath219 ( or equivalently the average degree of the graph ) .",
    "we also introduce a new method for improving the neighborhood recovery by considering pair - wise neighborhood unions which produce a ranking of @xmath160 nodes in @xmath105 with respect to their likelihood of being adjacent to the remaining node .",
    "this can be thought of as a way to incorporate local information ( rankings ) at each node into a globally consistent edge structure estimation of the graph @xmath105 ."
  ],
  "abstract_text": [
    "<S> structure learning in random fields has attracted considerable attention due to its difficulty and importance in areas such as remote sensing , computational biology , natural language processing , protein networks , and social network analysis . </S>",
    "<S> we consider the problem of estimating the probabilistic graph structure associated with a gaussian markov random field ( gmrf ) , the ising model and the potts model , by extending previous work on @xmath0 regularized neighborhood estimation to include the elastic net @xmath1 penalty . </S>",
    "<S> additionally , we show numerical evidence that the edge density plays a role in the graph recovery process . </S>",
    "<S> finally , we introduce a novel method for augmenting neighborhood estimation by leveraging pair - wise neighborhood union estimates . </S>"
  ]
}