{
  "article_text": [
    "estimation of population covariance matrices from samples of multivariate data has draw many attentions in the last decade owing to its fundamental importance in multivariate analysis . with dramatic advances in technology in recent years , various research fields , such as genetic data , brain imaging , spectroscopic imaging , climate data and so on , have been used to deal with massive high - dimensional data sets , whose sample sizes can be very small relative to dimension . in such settings , the standard and",
    "the most usual sample covariance matrices often performs poorly @xcite .",
    "fortunately , regularization as a class of new methods to estimate covariance matrices has recently emerged to overcome those shortages of using traditional sample covariance matrices .",
    "these methods encompass several specified forms , banding @xcite , tapering @xcite and thresholding @xcite for instance .    moreover , there are many cases where the model is known to be structured in several ways at the same time . in recent years , one of research contents is to estimate a covariance matrix possessing both sparsity and positive definiteness .",
    "for instance , rothman @xcite gave the following model : @xmath8 where @xmath9 is the sample covariance matrix , @xmath10 is the frobenious norm , @xmath11 is the element - wise @xmath0-norm and @xmath12 . from the optimization viewpoint , ( [ rest ] ) is similar to the graphical lasso criterion @xcite which also has a log - determinant part and the element - wise @xmath0-penalty .",
    "rothman @xcite derived an iterative procedure to solve ( [ rest ] ) .",
    "while xue , ma and zou @xcite omitted the log - determinant part and considered the positive definite constraint @xmath13 for some arbitrarily small @xmath14 : @xmath15 they utilized an efficient alternating direction method ( adm ) to solve the challenging problem ( [ xest ] ) and established its convergence properties .",
    "most of the literatures , e.g.,@xcite , required the population covariance matrices being positive definite , and thus there is no essence of pursuing the low - rank of the estimator .",
    "by contrast , newly appeared research topic is to consider simultaneously the sparsity and low - rank of a structured model , which implies that the population covariance matrices are no longer restricted to the positive definite matrix cone and can be relaxed to the positive semidefinite cone .",
    "in addition , the models with structure of being simultaneously the sparsity and low - rank are widely applied into practice , such as sparse signal recovery from quadratic measurements and sparse phase retrieval , see @xcite for example . moreover , richard et al .",
    "@xcite showed that both sparse and low - rank model can be derived in covariance matrix when the random variables are highly correlated in groups , which means this covariance matrix has a block diagonal structure .    with stimulations of those ideas ,",
    "we construct the following convex model encompassing the @xmath0-norm and nuclear norm for estimating the covariance matrix : @xmath16 where @xmath17 are tuning parameters .",
    "the @xmath0-norm penalty @xmath18 is also called lasso - type penalty and is used to encourage sparse solutions . the nuclear norm , @xmath19 with @xmath20 being the eigenvalue of @xmath21 , is the trace norm when @xmath22 and ensures low - rank solutions of ( [ est ] ) . here",
    "we inroduce the _ approximate rank _ to interpret the low - rank , which is defined as @xmath23 being the smallest number such that @xmath24 where @xmath25 are the singular value of @xmath26 with @xmath27 , and @xmath28 could be chosen based on the needs , throughout our paper we fix @xmath29 for simplicity .",
    "the contributions of this paper mainly center on two aspects .",
    "for one thing , being different from @xcite , we establish the theoretical statistical theory under different assumptions rather than giving the generalized error bound of the estimation .",
    "especially , we acquire the estimation rate @xmath30 under the frobenious norm error , which improves the optimal rate @xmath31 where the low - rank property of the estimator does not be considered @xcite and @xmath32 is the samples dimension with @xmath33 . for another , we take advantage of the alternating direction method of multipliers ( admm ) , also can be seen in @xcite , to combat our problem ( [ est ] ) .",
    "the organization of this paper is as follows . in section [ sec2 ]",
    "we will present some theoretical properties of the estimator derived by the proposed model ( [ est ] ) .",
    "after that the alternating direction method of multipliers ( admm ) is going to be introduced to combat the problem , and numerical experiments are projected to show the performance of this method in sections [ sec3 ] and [ sec4 ] respectively .",
    "we make a conclusion in the last section .",
    "before the main part , we hereafter introduce some notations . @xmath34 and @xmath35 denote the expectation of @xmath36 and the probability of the incident @xmath26 occurring respectively .",
    "@xmath37 is the number of entries of the set @xmath38 .",
    "normal distribution with mean @xmath39 and covariance @xmath21 is written as @xmath40 .",
    "say @xmath41 if for every @xmath42 , there is a @xmath43 such that @xmath44 for all @xmath45 , and say @xmath46 if @xmath47 .",
    "if there are two constants @xmath48 such that @xmath49 , we write as @xmath50 .    for given",
    "observed independently and identically distributed ( i.i.d . for short ) @xmath32-variate random variables",
    "@xmath51 @xmath52 with covariance matrix @xmath53 and @xmath54 , the goal is to estimate the unknown matrix @xmath53 based on the sample @xmath55 .",
    "this problem is called covariance matrix estimation which is of fundamental importance in multivariate analysis .    given a random sample @xmath56 from @xmath57 ( without loss of generality ) and a population covariance matrix @xmath58 , the sample covariance matrix is @xmath59 where @xmath60 .",
    "denote @xmath38 the support set of the population covariance matrix @xmath61 as @xmath62    [ ass1 ] for all @xmath32 , @xmath63 , where @xmath64 is a constant .",
    "assumption [ ass1 ] is a common used condition in covariance matrix estimation , on which a useful lemma based is recalled here for the sequel analysis .",
    "one can also refer it in @xcite .    [ zz ]",
    "let  @xmath65 be i.i.d .",
    "@xmath66 and assumption [ ass1 ] holds , _ _ ( _ _ i.e .",
    ", for all @xmath32 , @xmath67__)__. then , if  @xmath68 , @xmath69 where constants @xmath70 and @xmath71 depend on @xmath72 only .",
    "another lemma which plays an important role in our main results is stated below .",
    "[ i m ] suppose that assumption [ ass1 ] holds , @xmath73 .",
    "then for @xmath14 sufficiently small , @xmath74 where @xmath75 is defined as @xmath76 .",
    "* proof *  first make the eigenvalue decomposition of @xmath61 ( @xmath77 ) as @xmath78 where @xmath79 with @xmath80 is the matrix composed of eigenvectors , @xmath81 is a diagonal matrix generated by eigenvalues with @xmath82 and @xmath83 . + by denoting @xmath84 with @xmath85 and @xmath86 , which implies @xmath87 , we consider the model @xmath88 clearly , from ( [ est ] ) we have @xmath89 which implies @xmath90 . for a given @xmath14 sufficiently small and any @xmath91 ( i.e. , @xmath92 ) , we compute @xmath93 for convenience we denote @xmath94",
    ". then for i , it holds @xmath95 for ii , we obtain by noting @xmath96 and @xmath97 that , @xmath98 from the h@xmath99lder inequality , one can prove that @xmath100 for iii , combining with ( [ hd1 ] ) we get that @xmath101 since @xmath102 and ( [ hd2 ] ) , @xmath103 therefore , by @xmath73 @xmath104 hence we prove that if @xmath73 and @xmath102 , for any @xmath91 , it holds @xmath105 in addition , from ( [ delta ] ) , we have @xmath106 which implies that @xmath107 .",
    "otherwise , we suppose @xmath108 , then @xmath109 . since for any @xmath91 , it follows @xmath110 which is contradicted with the fact @xmath111 is a convex function and @xmath112 , because @xmath113 finally @xmath107 indicates that @xmath114 due to @xmath115 . hence the desired result is obtained.@xmath116    then in order to acquiring the rate of the estimation , the two following commonly used assumptions are needed to introduced , and",
    "also can be seen @xcite .",
    "assumption [ ass2 ] holds , for example , if @xmath117 are gaussian .",
    "[ ass2 ] @xmath118 hold for all @xmath119 and @xmath120 , where @xmath121 and @xmath122 are two constants .",
    "[ ass3 ] @xmath123 hold for all @xmath124 , where some @xmath125 and @xmath126 is a constant .    built on the two assumptions ,",
    "we give our main results with regard to rates of the estimator of ( [ est ] ) .",
    "[ th1 ] for some @xmath127 , let @xmath128 be a sufficiently large constant and suppose assumptions [ ass1 ] and [ ass2 ] hold . if @xmath129 and @xmath130 , then @xmath131    * proof *  since assumptions [ ass1 ] and [ ass2 ] hold , a fact employed by rothman et al .",
    "@xcite is that for @xmath132 sufficiently small , @xmath133 where @xmath134 and @xmath135 are some constants .",
    "we then apply the bound @xmath130 and lemma [ i m ] with @xmath136 to obtain that @xmath137 evidently , @xmath138 can be arbitrarily close to one by choosing @xmath128 sufficiently large.@xmath116    [ co1 ] for some @xmath127 , let @xmath128 be a sufficiently large constant such that @xmath139 , and suppose assumptions [ ass1 ] , [ ass2 ] hold .",
    "if @xmath140 and @xmath141 , then @xmath142    clearly , if the @xmath143 in assumption [ ass1 ] , the better rate @xmath144 would reduce to @xmath145 .",
    "it is worth mentioning that under the the assumption [ ass1 ] , the minimax optimal rate of convergence under the frobenius norm in theorem 4 of @xcite is @xmath145 which also has been obtained by @xcite .",
    "however , to attain the same rate in the presence of the log - determinant barrier term ( [ rest ] ) , rothman @xcite instead would require that @xmath146 , the minimal eigenvalue of the true covariance matrix , should be bounded away from zero by some positive constant , and also that the barrier parameter should be bounded by some positive quantity .",
    "@xcite illustrated this theory requiring a lower bound on @xmath146 is not very appealing .",
    "[ th2 ] let @xmath147 be a sufficiently large constant and suppose that assumptions [ ass1 ] and [ ass3 ] hold . if @xmath148 and @xmath149 , then @xmath150    * proof *  since assumptions [ ass1 ] and [ ass3 ] hold , one can modify a result of bickel & levina ( 2008a ) and show that for @xmath132 sufficiently small @xmath151 where @xmath152 is a constant .",
    "we then apply the bound @xmath149 and lemma [ i m ] with @xmath153 to get @xmath154 apparently , the bound @xmath155 can be arbitrarily close to one by taking @xmath147 sufficiently large.@xmath116",
    "in this section , we will construct the alternating direction method of multipliers ( admm ) to solve problem ( [ est ] ) . by introducing an auxiliary variable @xmath156 , problem ( [ est ] )",
    "can be rewritten as @xmath157 the constraint @xmath158 can be put into the objective function by using an indicator function :    ( 0)=  0 ,  0 +  + ,  .",
    "this leads to the following equivalent reformulation of ( [ est1 ] ) : @xmath159 recently , the alternating direction method of multipliers ( admm ) has been studied extensively for solving ( [ est2 ] ) . a typical iteration of admm for solving ( [ est2 ] )",
    "can be described as    [ ad1]^k+1:= _  ( ^k,,^k ) + [ ad2]^k+1:= _  ( , ^k+1,^k ) + [ ad3]^k+1:=^k-(^k+1-^k+1 ) ,    where the augmented lagrangian function @xmath160 is defined as @xmath161 in which @xmath162 is the lagrange multiplier and @xmath163 is a penalty parameter .",
    "note that admm ( [ ad1]-[ad3 ] ) can be written explicitly as +    [ ad11]^k+1:=_0  _ * + -(^k+^k)_f^2 + [ ad22]^k+1:= _  _ 1 + -(_n+^k+1-^k)_f^2 + [ ad33]^k+1:=^k-(^k+1-^k+1)/.    we now show that the two subproblems ( [ ad11 ] ) and ( [ ad22 ] ) can be easily solved . for the subproblem ( [ ad11 ] ) , @xmath164 where @xmath165 denote the projection of a matrix @xmath36 onto the convex positive semidefinite cone @xmath166 . namely @xmath167 , where @xmath168 and @xmath169    the solution of the second subproblem ( [ ad22 ] ) is given by the @xmath0-shrinkage operation @xmath170 where @xmath171 and @xmath172 is a sign function .    therefore , combining with ( [ ad33])-([shr1 ] ) , the whole algorithm is written as follows    >",
    "l *  admm :  alternating direction method of multipliers * +  initialize @xmath173 + *  repeat * +  compute @xmath174 ; +  compute @xmath175   +  compute @xmath176 + *  untill * convergence + *  return * +    to end this section , we prove that the sequence @xmath177 produced by the alternating direction method of multipliers ( table [ admm ] ) converges to @xmath178 , where @xmath179 is an optimal solution of ( [ est2 ] ) and @xmath180 is the optimal dual variable .",
    "now we label some necessary notations for the ease of presentation .",
    "let @xmath181 be a @xmath182 matrix defined as @xmath183 the weighted norm @xmath184 stands for @xmath185 and the corresponding inner product @xmath186 is @xmath187 . before presenting the main theorem with regard to the global convergence of admm",
    ", we introduce the following lemma .",
    "[ conle ] assume that @xmath188 is an optimal solution of @xmath189 and @xmath180 is the corresponding optimal dual variable associated with the equality constraint @xmath190",
    ". then the sequence @xmath191 produced by admm satisfies @xmath192 where @xmath193 and @xmath194 .",
    "based on the lemma above , the convergent theorem can be derived immediately .",
    "[ conth]the sequence @xmath191 generated by algorithm 1 from any starting point converges to an optimal solution of @xmath189 .",
    "in this section we will exploit the proposed method admm to tackle two examples , one of which possessed the block structured population covariance matrix , and another utilized the banded population covariance matrix .",
    "actually as the constraint @xmath22 , our proposed model ( [ est ] ) is equivalent to @xmath195 so similar to the method in @xcite , one can solve the soft - thresholding estimator @xmath196 to initialize the @xmath197 . if the derived @xmath198 then the recovered sparse and low - rank semidefinite estimator @xmath199 . in our stimulation",
    ", we uniformly initialize @xmath200 as the matrix with all entries being 1 , @xmath197 as zero matrix and @xmath201 respectively . unlike @xmath202 and @xmath203",
    ", @xmath39 does not change the final covariance estimator , thus we fixed @xmath204 just for simplicity and the stop criteria is set as @xmath205 for the sample dimensions , we always take @xmath206 and @xmath207 .",
    "analogous to the model , modified slightly here , emerged in @xcite who synthesized @xmath4 samples @xmath208 for a block diagonal population covariance matrix @xmath209 , we will use @xmath210 blocks of random sizes , and each block is generated by @xmath211 where the entries of @xmath212 are drawn i.i.d . from the uniform distribution on @xmath213 $ ] .",
    "evidently , the rank of @xmath61 produced in the way is @xmath214 .",
    "corresponding matlab code of generating @xmath215 is @xmath216 , thereby deriving the sample covariance matrix @xmath217 what is worth mentioning is that if @xmath61 is a positive definite matrix , the solution our method obtains would be a positive definite matrix with full rank .",
    "but fortunately , compared to some largest singular values of @xmath61 , the left are relatively small so that can be ignored . here",
    ", therefore , we consider the approximate rank ( [ apr ] ) . in addition , we say the sparsity of a matrix @xmath218 by @xmath219 captypefigure        captypefigure        apart from the approximate @xmath220 and the sparsity @xmath221 of the sparse and low - rank semidefinite estimator @xmath222 , we also take advantage of other two types of errors to show the selection performance of our proposed method admm : @xmath223 where @xmath224 stands for the false positive rate , which means the rate of significant variables that are unselected over the whole zero entries , and @xmath225 denotes the true positive rate , which implies the ratio of significant variables that are selected over the entire none zero elements .    for more visualized purpose , we plot the population covariance , sample covariance an the recovered covariance . from figures [ fig1 ] and [ fig2 ] ,",
    "the left population covariance is @xmath61 , the median sample covariance stands for @xmath9 and the right recovered covariance denotes @xmath75 .",
    "the yellow region stands for the sparse area in which the values are quite close ( or most of them equal ) to zero , while the green zone is the place where the none zero entries locate .",
    "moreover the deeper the green color is , the larger the value stands .",
    "evidently the recovered covariance matrices are quite dependent on the sample covariance matrix .",
    "captypefigure        captypefigure",
    "we then report average results over 100 runs .",
    "timing ( in seconds ) was carried out on a cpu 2.6ghz desktop . in computation , the sparse parameter @xmath202 and the low - rank parameter @xmath203 are given in corresponding stimulations respectively .",
    "@xmath197 ia taken as the zero matrix .",
    "as we mentioned before the rank of the produced @xmath61 actually is given , i.e. , @xmath226 .",
    "table [ tab1 ] shows the performance of our approach under different dimensions @xmath207 and various blocks @xmath214 .",
    "obviously , all the @xmath227 nearly tends to the true rank @xmath214 .",
    "the values of @xmath224 and @xmath225 are quite desirable , which manifests that the selection performance of admm is very well .",
    "moreover , the cpu time reveals the method runs relatively fast .",
    "in addition , with the @xmath214 rising , for example @xmath228 , the @xmath224 is decreasing while @xmath224 is increasing , and the time spent by the method is also ascending .",
    "in addition , for small dimension such as @xmath229 and large block such as @xmath230 , admm performs unstably because various stimulations can not be recovered .",
    "c c c c c c c c   + &  @xmath32  &  @xmath227  &  @xmath231  &  @xmath232  &  @xmath224&@xmath225  &  time   + & 100 & 5.0 & 0.2356 & 0.1148 & 0.1392 & 0.9863 & 0.775 + @xmath233&200 & 5.0 & 0.2362 & 0.1121 & 0.1414 & 0.9911 & 3.364 + & 500&5.4&0.2225 & 0.0990 & 0.1399 & 0.9748 & 32.49 + & 200 & 9.90 & 0.1228 & 0.0565 & 0.0716 & 0.9766 & 3.580 + @xmath234&500&10.8 & 0.1163 & 0.0598 & 0.0649 & 0.9371 & 33.03 + & 1000&12.0&0.1199 & 0.0604 & 0.0692 & 0.9092 & 321.6 + & 200 & 19.3 & 0.0601 & 0.0339 & 0.0324 & 0.8571 & 4.165 + @xmath235&500 & 20.5 & 0.0569 & 0.0268 & 0.0328 & 0.9368 & 38.81 + & 1000&23.5 & 0.0564 & 0.0291 & 0.0316 & 0.8821 & 368.7 + & 200 & 181.0 & 0.0364 & 0.0189 & 0.0201 & 0.8850 & 16.09 + @xmath230&500&47.0 & 0.0223 & 0.0122 & 0.0127 & 0.8006 & 49.43 + & 1000&54.2&0.0221 & 0.0134 & 0.0123 & 0.7447 & 449.5 +    to simply observe the performance of our proposed method under different parameters @xmath236 and initialized @xmath197 , we fix @xmath237 and @xmath238 . from table",
    "[ tab11 ] , results of left columns of @xmath239 and time are generated from the initialized @xmath240 , and results of right columns are produced with @xmath241 .",
    "one can easily to discern that when @xmath242 and @xmath241 , the performance of the method is relatively bad regardless of what @xmath203 is taken due to the @xmath227 and @xmath225 are undesirable . by contrast , when @xmath243 and @xmath244 , it behaves much better , particularly when @xmath245 .",
    "moreover , with the increasing of @xmath202 , the rate @xmath224 of significant variables that are unselected is rising , even though the percentage @xmath225 of significant variables that are selected is ascending as well . by comparing the effectiveness of those two initialized point @xmath240 and @xmath201 , as shown in the table , @xmath246 and time generated from @xmath240",
    "are basically same , which means method with zero starting point performs more stable .",
    "but when @xmath243 and @xmath244 , admm with @xmath241 generates larger @xmath225 than that from @xmath240 , moreover it needs less computational time in all stimulations regardless of the parameters .",
    "c c c c c c   + @xmath202 &  @xmath203  &  @xmath227  &  @xmath224&@xmath225  &  time   + & 0.05&5.0  12.0&0.1365  0.0565 & 1.0000  0.5666 & 3.379  2.716 + @xmath247&0.25&5.5  8.5&0.1313  0.0507 & 0.8813  0.4783 & 4.857  3.803 + & 0.50&5.0  7.5&0.1303  0.0615 & 0.9967  0.4273 & 3.604  4.040 + & 0.05&5.0",
    "5.0&0.1371  0.1362 & 0.9980  0.9843 & 3.584  1.885 + @xmath248&0.25&5.0  5.0&0.1367  0.1365 & 0.9972  0.9972 & 3.963  2.897 + & 0.50&5.2  5.2&0.1318  0.1317 & 0.9653  0.9661 & 4.134  3.477 + & 0.05&5.0  5.0 & 0.1375  0.2008 & 1.0000  1.0000 & 3.319  1.822 + @xmath244&0.25&5.4  5.8&0.1575  0.2169 & 0.9432  1.0000 & 4.068  2.580 + & 0.50&5.0",
    "6.0&0.1451  0.2035 & 0.9881  1.0000 & 3.933  3.791 +      in this part we consider the population covariance matrix with banded structure which has been emerged in @xcite . to be more exact , the population covariance matrix @xmath249 has the following formula @xmath250 we first report average results over 100 replicators and take the sparse parameter @xmath251 and the low - rank parameter @xmath252 respectively .",
    "information listed in table [ tab2 ] shows the performance of our approach under different dimensions @xmath207 and two distinct starting point @xmath253 and @xmath201 .",
    "c c c c c c c c c + &  @xmath32  &  @xmath254  &  @xmath227  &  @xmath231  &  @xmath232  &  @xmath224&@xmath225  &  time   + & 100&90 & 33.3 & 0.1810 & 0.1209 & 0.0718 & 0.9750 & 1.421 + @xmath240&200&176&66.6&0.0927 & 0.0606 & 0.0351 & 0.9851 & 7.560 + & 500&487&327.0 & 0.0376 & 0.0211 & 0.0174 & 0.9791 & 63.06 + & 1000&847&334.2&0.0189&0.0133&0.0067&0.9233&851.3 + & 100&90 & 33.3 & 0.1810 & 0.1209 & 0.0718 & 0.9750 & 1.417 + @xmath241&200&176 & 66.6 & 0.0927 & 0.0606 & 0.0351 & 0.9851 & 7.576 + & 500&487&327.0 & 0.0376 & 0.0211 & 0.0174 & 0.9791 & 60.02 + & 1000&847&334.2&0.0189&0.0133&0.0067&0.9233&866.2 +    as we can discern in table [ tab2 ] , compared with @xmath61 , the @xmath227 and @xmath232 are relatively small , and the former ascends while the latter descends with the rise of @xmath32 .",
    "in addition , in example [ ex1 ] the block structured @xmath61 whose @xmath255 leads to the estimator the rank of @xmath75 is also close to @xmath214 .",
    "being distinct with that , in this example , the @xmath254 increases with the dimension @xmath32 and is not low - rank , but the recovered solution @xmath75 has been rendered the relatively low - rank property .",
    "the values of @xmath224 and @xmath225 are both quite desirable , which manifests that the selection performance of admm is very well in this example .",
    "moreover , the cpu time reveals the method runs extremely fast as well .",
    "in addition , under such parameters z@xmath256 , admm behaves nearly identically even though the starting point @xmath197 are different .",
    "captypefigure        captypefigure        then from figure [ fig3 ] , one can check that the recovered covariance matrices @xmath75 are quite dependent on the sample covariance matrix @xmath9 , and the selection performance are relatively well because @xmath224 is pretty small while @xmath225 is close to @xmath257 .    to simply observe the behavior of admm under different parameters @xmath236 and initialized @xmath197 , we fix @xmath237 and @xmath258 . as indicated in table",
    "[ tab21 ] , results of left columns of @xmath239 and time are generated from the initialized point @xmath240 , and results of right columns are produced with @xmath241 .",
    "it is clear that the cpu time cost by the method with starting point @xmath201 is almost less than that from zero initialization . with the @xmath202 rising ,",
    "tpr generated by the method with @xmath241 is increasing to 1 , while that with @xmath240 basically stabilizes at 0.97 . in terms of the @xmath227 , the proposed method with starting point @xmath201",
    "will not create a lower rank solution , comparing with @xmath240 .",
    "the reason for this phenomenon probably is that @xmath201 is a sparse point but not low rank ; from ( [ este ] ) if @xmath201 is an approximately semidefinite positive matrix , algorithm will stop after a few iterations which results in the solution is not a desired low - rank one .",
    "c c l c c c   + @xmath202 &  @xmath203  &  @xmath227 &  @xmath224&@xmath225  &  time   + & 0.25&64.2  126.2 & 0.0356  0.0168 & 0.9835  0.4383 & 7.439  5.725 + @xmath248&0.50 & 66.0  95.2 & 0.0345   0.0126 & 0.9888  0.3877 & 8.682  8.474 + & 0.75 & 62.6  78.0 & 0.0381  0.0102 & 0.9780  0.3986 & 7.852  11.05 + & 0.25 & 67.4  103.0&0.0323  0.0377 & 0.9528  0.9580 & 8.996  4.818 + @xmath244&0.50&65.8  80.0&0.0370  0.0385 & 0.9834  0.9845 & 8.445  7.148 + & 0.75&67.8  67.8&0.0353  0.0353 & 0.9783  0.9783 & 8.728  8.451 + & 0.25 & 67.8  167.0&0.0336  0.0726 & 0.9792  1.0000 & 9.508  2.510 + @xmath259&0.50&64.0  157.8 & 0.0351  0.0750 & 0.9807  1.0000 & 8.596  1.802 + & 0.75&68.0  166.4&0.0322  0.0731&0.9696  1.0000&8.470  2.183 +",
    "we have acquired a positive semidefinite estimator , being simultaneously sparse and low - rank , from samples of the covariance matrices through utilizing @xmath0 norm and nuclear norm penalties .",
    "the theoretical properties manifest that in high - dimensional settings the estimator we have constructed performs very well .",
    "meantime , the efficient admm with global convergence has possessed several merits illustrated by the numerical simulations , such as less computational time and beautiful recovered effectiveness .",
    "the work was supported in part by the national basic research program of china ( 2010cb732501 ) , the national natural science foundation of china ( 11171018 , 71271021,11301022 ) .",
    "77 bickel , p. and levina , e. : regularized estimation of large covariance matrices .",
    "statist . *",
    "36 * , 199 - 227 ( 2008 ) .",
    "bickel , p. and levina , e. : covariance regularization by thresholding .",
    "statist . *",
    "36 * , 2577 - 2604 ( 2008 ) .",
    "cai , t. and liu , w. : adaptive thresholding for sparse covariance matrix estimation . j.",
    "assoc . * 106 * , 1 - 13 ( 2011 ) .",
    "cai , t. , zhang , c. : and zhou , h. , optimal rates of convergence for covariance matrix estimation .",
    "statist . *",
    "38 * , 2118 - 2144 ( 2010 ) .",
    "cai t t , yuan m. : adaptive covariance matrix estimation through block thresholding .",
    "ann.statist . * 40 * , 2014 - 2042 ( 2012 ) .",
    "cai , t. and zhou , h. : minimax estimation of large covariance matrices under @xmath0-norm .",
    "sinica * 22 * , 1319 - 1378 ( 2012 ) .",
    "cai , t. and zhou , h. : optimal rates of convergence for sparse covariance matrix estimation . ann.statist . * 40 * , 2389 - 2420 ( 2012 ) .",
    "el karoui , n. : operator norm consistent estimation of large dimensional sparse covariance matrices .",
    "statist . *",
    "36 * , 2717 - 2756 ( 2008 ) .",
    "friedman , j. , hastie , t. and tibshirani , r. , sparse inverse covariance estimation with the graphical lasso .",
    "* 9 * , 432 ( 2008 ) .",
    "furrer , r. and bengtsson , t. : estimation of high - dimensional prior and posterior covariance matrices in kalman filter variants .",
    "j. multivariate anal . * 98 * , 227 - 255 ( 2007 ) .",
    "johnstone , i. : on the distribution of the largest eigenvalue in principal components analysis .",
    ", 295 - 327 ( 2001 ) .",
    "liu h , wang l , zhao t. : sparse covariance matrix estimation with eigenvalue constraints .",
    "journal of computational and graphical statistics , ( 2013 accepted ) .",
    "oymak , s. , jalali , a. , fazel , m. , eldar , y. c. and hassibi , b. : simultaneously structured models with application to sparse and low - rank matrices .",
    "arxiv preprint arxiv:1212 - 3753 ( 2012 to appear ) .",
    "richard , e. , savalle , p. a. , and vayatis , n. : estimation of simultaneously sparse and low rank matrices .",
    "arxiv preprint arxiv : 64 - 74 ( 2012 to appear ) .",
    "rothman , a. : positive definite estimators of large covariance matrices .",
    "biometrika * 99 * , 733 - 740 ( 2012 ) .",
    "rothman , a. , levina , e. , and zhu , j. : generalized thresholding of large covariance matrices .",
    "* 104 * , 177 - 186 ( 2009 ) .",
    "wu , w. and pourahmadi , m. : nonparametric estimation of large covariance matrices of longitudinal data .",
    "biomet . * 90 * , 831 - 844 ( 2003 ) .",
    "xue , l. , ma , s. and zou , h. : positive definite @xmath0 penalized estimation of large covariance matrices .",
    "assoc . , * 107 * , 1480 - 1491 ( 2012 ) .",
    "yuan x.m . : alternating direction method of multipliers for covariance selection models .",
    "journal of scientific computing * 51 * , 261 - 273 ( 2012 ) .",
    "* proof of lemma [ conle ] *  for convenience , we denote that @xmath260 clearly , @xmath261 is a convex function .",
    "since @xmath188 is an optimal solution of @xmath189 , which satisfies the following kkt conditions : @xmath262  note that the optimality conditions for the first subproblem in admm , i.e. , the subproblem with respect to @xmath156 in @xmath263 , are given by @xmath264 this together with @xmath265 , i.e. , @xmath266 , we have @xmath267 combining @xmath268 and @xmath269 and using the fact that @xmath270 is a monotone operator , we get @xmath271  the optimality conditions for the second subproblem ( i.e. , the subproblem with respect to @xmath21 ) in @xmath272 are given by @xmath273 this together with @xmath265 , i.e. , @xmath266 , we have @xmath274 similarly , combining @xmath275 and @xmath276 , using the fact that @xmath277 is a monotone operator , we get @xmath278 the summation of @xmath279 and @xmath280 gives @xmath281 where the equality because of @xmath282 and @xmath283 simple algebraic derivation from @xmath284 yields the following inequality : @xmath285 rearranging the right hand side of @xmath286 using @xmath287 and @xmath288 , then @xmath286 can be reduced to @xmath289 using the notation of @xmath290 and @xmath291 , the inequality above can be rewritten as @xmath292 combining @xmath293 with the following identity @xmath294 we get @xmath295 now , using @xmath276 for @xmath296 instead of @xmath297 , we get , @xmath298 combining @xmath299 and using the fact that @xmath300 is a monotone function , we obtain , @xmath301 which means @xmath302 by substituting @xmath303 into @xmath304 , we get the desired result @xmath305.@xmath116    * proof of theorem [ conth ] *  from lemma [ conle ] , we can get that +  ( a )  @xmath306 ; +  ( b )  @xmath307 lies in a compact region ; +  ( c )  @xmath308 is monotonically non - increasing and thus converges .",
    "+ connecting with notations of @xmath309 and @xmath310 , it holds that @xmath311 and @xmath312 , which together with @xmath265 imply that @xmath313 and @xmath314 . from @xmath315",
    ", @xmath307 has a subsequence @xmath316 that converges to @xmath317 , i.e. , @xmath318 and @xmath319 .",
    "also we have @xmath320 from @xmath314 .",
    "therefore , @xmath321 is a limit point of @xmath191 .",
    "note that @xmath322 and @xmath323 respectively imply that @xmath324 those together with @xmath325 , it reduces that @xmath326 @xmath327 and @xmath325 mean that @xmath321 is an optimal solution to @xmath189 .",
    "therefore , we showed that any limit point of @xmath191 is an optimal solution to @xmath189.@xmath116"
  ],
  "abstract_text": [
    "<S> this paper aims at achieving a simultaneously sparse and low - rank estimator from the semidefinite population covariance matrices.we first benefit from a convex optimization which develops @xmath0-norm penalty to encourage the sparsity and nuclear norm to favor the low - rank property . for the proposed estimator </S>",
    "<S> , we then prove that with large probability , the frobenious norm of the estimation rate can be of order @xmath1 under a mild case , where @xmath2 and @xmath3 denote the number of sparse entries and the rank of the population covariance respectively , @xmath4 notes the sample capacity . </S>",
    "<S> finally an efficient alternating direction method of multipliers with global convergence is proposed to tackle this problem , and meantime merits of the approach are also illustrated by practicing numerical simulations .    </S>",
    "<S> [ section ] [ theorem]lemma [ theorem]corollary [ theorem]remark [ theorem]proposition [ theorem]assumption [ theorem]definition [ theorem]construction [ theorem]example [ theorem ] [ claim_nb]claim    a            shenglong zhou@xmath5{0.98,0.00,0.00}{*}}$ ] ,  naihua xiu@xmath5{0.98,0.00,0.00}{*}}$ ] ,  ziyan luo@xmath5{0.98,0.00,0.00}{+}}$ ] ,  lingchen kong + @xmath6{0.98,0.00,0.00}{^*}~$]department of applied mathematics + @xmath7{0.98,0.00,0.00}{^+}}$ ]  state key laboratory of rail traffic control and safety + @xmath7{0.98,0.00,0.00}{^{\\underset{+}{*}}}}$ ]  beijing jiaotong university , beijing 100044 , p. r. china +    * keywords : * covariance matrices , sparse , low - rank , rate of estimation , alternating direction method of multipliers </S>"
  ]
}