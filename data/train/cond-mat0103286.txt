{
  "article_text": [
    "the strategy used by populations of neurons to code for external stimuli or behavioural correlates is a major issue which has been recently investigated both through data analysis and theoretical modelling .",
    "the mutual information between external correlates and the spiking activity of the population is one way to assess such coding quantitatively @xcite .",
    "several analyses have focused on the coding of a _ discrete _ set of stimuli ( @xcite , see @xcite for a review ) , which is the paradigm used in many experiments@xcite . in this situation",
    "the mutual information is bounded by the entropy of the stimulus set .",
    "some theoretical studies have also considered the coding of stimuli varying in a _ continuous _",
    "domain @xcite , which is interesting with respect to basic properties like orientation in visual stimuli , frequency in auditory stimuli , velocity and position in motor actions .",
    "in particular in @xcite the authors have studied the asymptotic ( large population ) behaviour of the mutual information , with respect to a stimulus with a continuously varying dimension .    yet",
    "no study has been proposed so far considering a mixture of both continuous and discrete features , which is obviously closer to real world stimuli or behavioural correlates .",
    "moreover the initial rise of the mutual information for small but increasing population size is more relevant for a comparison with estimates from real data , at least as far as the possibility of having simultaneous recordings from very large populations of neurons is restricted to very few cases .",
    "we have recently analyzed data recorded in the motor areas of behaving monkeys , in the laboratory of eilon vaadia .",
    "the monkeys moved a manipulandum in several possible directions ( approximating a continuous correlate ) and with different combinations of arms ( 4 _ types _ of movement , i.e. a discrete correlate ) . in trying to characterize the neural coding of these movements",
    ", we were particularly interested in whether , as it is reasonable to expect , different motor areas differ , at least quantitatively , in their coding properties .",
    "the results , obtained from records of activity in areas m1 and sma , will be reported elsewhere @xcite ; but they suggest the importance of developing theoretical models of how populations of neurons might code simultaneously discrete and continuous correlates .",
    "for example , one clear conclusion has been that type and direction are not _ independent _ dimensions of the movement , in the sense for example that the information about direction , extracted from the activity recorded with all movement types , is much lower ( roughly half ) of the average information about direction , obtained with a single movement type .",
    "can we embody similar properties in a model of the scheme used by neurons to code movements ?",
    "what would then be the dependence of the mutual information on the number of the possible types of movement ?",
    "how would it depend on the resolution with which the continuous correlate is sampled ?",
    "how on the level of noise affecting the firing patterns ?",
    "in a recent work @xcite some of these questions have been investigated for a set of @xmath1 discrete stimuli , under the assumption that neurons fire independently of one another to each stimulus and that the distribution of the firing rates is gaussian .",
    "the linear and the quadratic approximations to the mutual information as a function of population size were studied analytically , in the limit of large noise , as well as the approach to the ceiling in the case of small noise .",
    "we generalize this study considering both a discrete and a continuous dimension in the stimulus , referring specifically to motor actions characterized by a direction and a `` type '' .",
    "nonetheless , our model is equally applicable to other complex correlates .",
    "we introduce a more realistic conditional firing rate distribution , than the simple gaussian one , and find a simple resulting correction to the gaussian model : the analytical expression of the mutual information remains the same , except for a renormalization of the expansion parameter .",
    "we then evaluate the information loss when the original activity distribution is averaged across the discrete correlate , as is sometimes the case in the analysis of real data , and the mutual information is evaluated solely with respect the continuously varying feature .",
    "averaging out dimensions in the stimulus corresponds to losing accuracy in its description , and hence the information loss .",
    "our theoretical analysis allows a direct comparison with real curves ; we present one comparison and discuss possible causes for the discrepancy between data and model . in particular , correlations between neurons , that are not included in the model , might play a relevant role , enhancing or decreasing redundancy in population coding @xcite .",
    "this issue will be the object of future work .",
    "first , we consider a coding scheme where the distribution of the firing rates conditional to the movements is gaussian , similarly to the case examined in @xcite .",
    "this assumption implies that negative rates have a non zero probability to occur , but it allows an easier analytical treatment . we will examine a more realistic scheme later on .",
    "consider the following distribution :    @xmath4 ; \\label{dist}\\ ] ]    @xmath5 is the firing rate of neuron @xmath6 ; @xmath7 and @xmath8 parameterize respectively the direction and the type of movement ; @xmath9 is the average firing rate of the neuron with the movement parameterized by @xmath7,@xmath8 .",
    "in general , the directional tuning of real cells in motor cortices is modulated by the type of movement performed .",
    "we show an example of this modulation , with the typical shape of tuning curves , in fig.[tuning_data ] ( data kindly provided by eilon vaadia ) .",
    "the modulation of the preferred direction looks weaker than the overall amplitude modulation .    for our model",
    "we have considered the following function :    @xmath10    @xmath11 ; \\label{tuning2}\\ ] ]    @xmath12 is a quenched random variable distributed between @xmath13 and @xmath14 .",
    "the meaning of eq.([dist]),([tuning_tot]),([tuning2 ] ) is that the firing rate of neuron @xmath6 given the movement parameterized by ( @xmath7,@xmath8 ) follows a gaussian distribution centered around a tuning curve @xmath15 whose flatness is modulated through the parameters @xmath12 . if @xmath16 is zero for some particular @xmath8 the firing of the cell does not depend , for that movement type , on the direction of the movement . on the other hand if @xmath12 assumes a fixed value for all @xmath8 the directional tuning does not modulate with the type of movement .",
    "tuning curves with a cosinusoidal shape have already been considered to model the directional selectivity of sensory neurons @xcite .",
    "fig.[tuning_2 ] shows the amplitude of the tuning curve .",
    "we neglect the modulation of the preferred direction with the type , as it would burden the analytical calculations ; it will be the object of future analyses .",
    "we are interested in the mutual information @xcite between the neuronal firing rates and the movements :    @xmath17    where the distribution @xmath18 is given in eq.([dist ] ) and @xmath19 is a short notation for the average across the quenched variables @xmath20,@xmath21 .",
    "in fact we are not interested in a particular realization of the tuning , but in the average across all its possible realizations .",
    "eq.([info ] ) can be written as :    @xmath22    @xmath23    @xmath24 \\right\\rangle_{\\varepsilon,\\vartheta^0 } ; \\label{outent}\\ ] ]    the calculation of the equivocation @xmath25 is straightforward and the result is additive in the population size :    @xmath26 ; \\label{final_eq}\\ ] ]    the linearity in @xmath0 is standard whenever the conditional distribution of firing rates can be factorized across neurons , as in eq.([dist ] ) .",
    "the evaluation of the rate entropy @xmath27 can be carried out introducing @xmath28 replicas @xcite for both the discrete and the continuous dimensions @xmath7 and @xmath8 , which allows to get rid of the logarithm in eq.([outent ] ) :    @xmath29\\right\\rangle_{\\varepsilon,\\vartheta^0 } -1\\right ) ; \\end{aligned}\\ ] ]    integrating over @xmath30 and rearranging terms one obtains :    @xmath31    @xmath32      an exact analytical evaluation of eq.([entropy ] ) is not possible without resorting to some approximation . in line with the analysis performed in @xcite we assume now that the quenched randomness is uncorrelated and identically distributed across neurons :    @xmath34 @xmath35^n=\\frac{1}{(2\\pi)^n};\\ ] ]    we assume also that @xmath36 @xmath37",
    ". then one can write :    @xmath38    we consider now the limit of large noise @xmath39 ; in this case , since @xmath40 we can expand @xmath41 ; keeping only terms of order @xmath42 , with @xmath43 and @xmath44 we obtain :    @xmath45    to first order in @xmath46 we obtain :    @xmath47    this result is valid for a generic directional tuning curve @xmath48 .",
    "we consider now our specific choice , eq.([tuning2 ] ) , examining the simplest case @xmath49 first . after averaging across direction selectivities @xmath50 and integrating over continuous replicas @xmath51",
    "we obtain    @xmath52\\right)-1\\right);\\end{aligned}\\ ] ]    where we have defined @xmath53 . to perform the average across the quenched variables",
    "@xmath54 we assume that they are equally distributed across @xmath1 movement types , namely :    @xmath55^p;\\ ] ]    in this case the sum over indexes @xmath56 and @xmath57 generates @xmath58 identical terms .",
    "the summation on discrete replicas yields a factor @xmath59 multiplying the term @xmath60 and a factor @xmath61 multiplying the term @xmath62 , since this last term is non zero only when @xmath63 .",
    "taking the limit @xmath64 yields    @xmath65\\nonumber\\\\ & & + \\frac{1}{\\ln2}\\frac{n(\\eta^0)^2}{4\\sigma^2 } \\left[\\frac{p-1}{p}2\\left(\\alpha(\\alpha-1)+\\frac{1}{4}\\right)\\lambda_1+\\frac{1}{4}\\lambda_2\\right ] ; \\label{final_ent}\\end{aligned}\\ ] ]    @xmath66 ^ 2 ; \\label{lambda1}\\ ] ]    @xmath67    from eqs.([final_ent ] ) and ( [ final_eq ] ) the final expression for the mutual information can be written @xmath68 ; \\label{final_info}\\ ] ]    in the more general case of a power @xmath69 of the cosine , in eq.([tuning2 ] ) , it is easy to show that the final result can be expressed as    @xmath70 ; \\label{final_info2}\\ ] ]    where :    @xmath71    @xmath72    the calculation of the coefficient of the second order term , which multiplies @xmath73 in eq.([r_appr ] ) , is slightly more complex and implies integration of terms with 4-replica interaction .",
    "the detailed analytical evaluation is given in the appendix .",
    "the final result up to the quadratic approximation reads :    @xmath74-\\frac{n^2(\\eta^0)^4}{2(4\\sigma^2)^2}\\right.\\\\ & & \\left.\\left\\{\\frac{p-1}{p^2}2\\left(2\\left(\\alpha - a_1\\right)^2\\lambda_1\\right)^2+\\left[\\frac{p-1}{p}\\left(\\lambda_1-\\lambda_2\\right)^2+\\frac{(\\lambda_2)^2}{p}\\right]\\left[\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\right]\\right\\}\\right\\};\\nonumber \\label{final_info4}\\end{aligned}\\ ] ]    where the expressions of @xmath75,@xmath76,@xmath77,@xmath78 are given respectively in eq.([lambda1]),([lambda2]),([a_1]),([a_2 ] ) in the limit of large @xmath1 we have    @xmath79\\right .\\nonumber\\\\ & & \\left .-\\frac{n^2(\\eta^0)^4}{2(4\\sigma^2)^2}\\left\\{\\left(\\lambda_1-\\lambda_2\\right)^2\\left[\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\right]\\right\\}\\right\\ } ; \\label{largep}\\end{aligned}\\ ] ]    fig.[res_vary_g ] shows the linear and the quadratic approximations of eq.([final_info4 ] ) for different values of the expansion parameter @xmath80 .",
    "it is easy to see that , for very small values of @xmath81 , linear and quadratic approximations roughly coincide , while when @xmath82 the quadratic approximation begins to fail and one should add higher orders in perturbation theory .",
    "fig.[res_p_g](a ) shows the dependence of the mutual information on the number of types @xmath1 .",
    "the dependence on @xmath1 is weak for the linear approximation and somewhat stronger for the quadratic one . in both cases",
    "an increase in the number of discrete correlates @xmath1 produces an increase in the mutual information .",
    "the distance between the linear and the quadratic approximation remains asymptotically finite ( for @xmath83 ) , contrary to what happens in the case of discrete stimuli alone @xcite .",
    "fig.[res_p_g](b ) shows the dependence of the mutual information on the width of the directional tuning ( see fig.[tuning_2],(b ) ) .",
    "since we are considering the case when the noise @xmath39 is large , a very narrow tuning in @xmath7 corresponds to a larger overlap in the conditional probabilities @xmath84 , for most angles @xmath7 .",
    "a consequence is that , especially in the linear approximation , the mutual information is a ( slowly ) decreasing function of @xmath85 .",
    "we consider now the case when the number of neurons is large .",
    "since we deal with an infinite number of stimuli the mutual information is unbounded .",
    "thus we expect that when the number of neurons becomes large and the noise is finite the mutual information tends asymptotically to infinity .    in order to study this limit",
    "we discretize the @xmath86 space into a finite set of @xmath87 angles @xmath88 , and then we take the limit @xmath89 .    the entropy of the responses @xmath27 can be written    @xmath90 \\right\\rangle_{\\varepsilon,\\vartheta^0 } \\label{outent2}\\ ] ]    where in analogy with eq.([dist ] ) we define    @xmath91 ; \\label{dist2}\\ ] ]    and we discretize the average across the directional selectivities @xmath92 as well    @xmath93    this situation corresponds to the case when each neuron can discriminate across different angles @xmath88 with a resolution @xmath2 .",
    "the calculation can be carried out introducing replicas as in the previous case .",
    "one gets    @xmath94    where @xmath95 ; \\label{exp_discr}\\ ] ] and we have assumed symmetry across neurons in the quenched randomness and in the parameters characterizing the conditional distribution @xmath96 .",
    "now we take the limit @xmath97 . as it is evident from eq.([exp_discr ] ) , @xmath98 and @xmath99 when @xmath100 and @xmath101 for each pair of indexes ( @xmath85,@xmath57 ) .",
    "thus when @xmath97 the only terms which survive in the sum on replicas are the ones with @xmath102 and @xmath103 .",
    "since we have @xmath1 stimuli @xmath8 and @xmath104 stimuli @xmath105 the total number of terms is @xmath106 .",
    "substituting this value in the sums over replicas in eq.([outent_discr ] ) and putting @xmath99 one obtains an expression for the entropy of the responses @xmath27 , which summed to the equivocation as in eq.([final_eq ] ) gives the final result for the mutual information :    @xmath107    now we remember that @xmath108 . taking the limit to continuous angles , @xmath89 , it is easy to see that asymptotically the mutual information tends logarithmically to infinity .",
    "so far we have considered the case where the rate distribution for each neuron is normal .",
    "this assumption implies that negative rates have a non zero probability to occur ; the more the average rate is small and close to zero , the more this probability becomes large .",
    "the bias introduced by the inclusion of negative rates in the space of possible states might be even more serious since we have considered the limit of large noise , where the tail of the distribution in the domain @xmath109 acquires a significant weight .",
    "cutting the distribution at zero is not enough to assign the proper weight to under - threshold activity : each time the summation of the inputs coming from other units is lower than threshold the neuron remains silent , and this occurs with a well defined probability .",
    "a natural choice for the rate distribution @xmath18 is a thresholded gaussian plus a @xmath110 peak in zero ( _ tg _ model ) :    @xmath111\\theta(\\eta_i)+2(1-{{\\rm erf}}(\\tilde{\\eta}_i(\\vartheta , s)/\\sigma)\\delta(\\eta_i)\\theta(-\\eta_i ) \\label{dist_corr}\\ ] ]    where @xmath112 is the heaviside step function , @xmath9 is the same as defined in eq.([tuning_tot ] ) and @xmath113 is the error function :    @xmath114    the factor multiplying the @xmath110 function ensures a correct normalization and it assigns the proper weight to the peak in zero , which is larger the more the average rate @xmath9 is close to zero . a similar distribution has already been considered in networks of threshold linear neurons @xcite .    the analytical evaluation of the mutual information is obviously more difficult than in the case of the simple gaussian , because of the presence of the error function , which can not be integrated exactly . nonetheless in the limit of large @xmath39 it is possible to evaluate both the linear and the quadratic approximation in @xmath0 and",
    "thus quantify the impact of the correction with respect to the gaussian case , eq.([final_info4 ] ) .",
    "we remind the expression of the equivocation , eq.([equiv ] ) :    @xmath115    assuming independence among neurons in the conditional probability @xmath116 , eq.([equiv1 ] ) can be written    @xmath117    in the specific case of the distribution ( [ dist_corr ] ) it is easy to show that @xmath118 ^ 2/2\\sigma^2}\\right\\rangle_{\\varepsilon,\\vartheta^0}\\!\\!\\!-\\!\\frac{1}{2}\\left(1+\\ln(2\\pi\\sigma^2)\\right)\\!\\left\\langle{{\\rm erf}}(\\tilde{\\eta}(\\vartheta , s)/\\sigma)\\right\\rangle_{\\varepsilon,\\vartheta^0}\\nonumber\\\\ & & + \\left\\langle \\int_0^\\infty d\\eta \\delta(\\eta)\\left(1-{{\\rm erf}}(\\tilde{\\eta}(\\vartheta , s)/\\sigma)\\right)\\left[\\ln\\delta(\\eta)+\\ln2 + \\ln\\left(1-{{\\rm erf}}(\\tilde{\\eta}(\\vartheta , s)/\\sigma)\\right)\\right]\\right\\rangle_{\\varepsilon,\\vartheta^0}.\\end{aligned}\\ ] ]    to proceed with the calculation we have to be careful with the integration of the delta function .",
    "in fact it is easy to show that the integration of the product @xmath119 yields a logarithmic divergence .",
    "since the mutual information must remain finite with a finite number of neurons @xmath0 , we expect this divergence to cancel exactly with an analogous term in the rate entropy and , in fact , in the next section , we will show that this is the case . for the moment we use the equality    @xmath120    assuming as usual that the quenched disorder is identically distributed across neurons and stimuli and that @xmath121 is like in eq.([dist ] ) we can write :    @xmath122 ^ 2/2\\sigma^2 } \\right\\rangle_{\\varepsilon,\\vartheta^0}\\right.\\nonumber\\\\ & & \\left.+2\\left\\langle\\left[1-{{\\rm erf}}(\\tilde{\\eta}(\\vartheta , s)/\\sigma)\\right]\\right\\rangle_{\\varepsilon,\\vartheta^0}\\ln\\frac{\\epsilon}{2}- 2\\left\\langle\\left[1-{{\\rm erf}}(\\tilde{\\eta}(\\vartheta , s)/\\sigma)\\right ] \\ln\\left[1-{{\\rm erf}}(\\tilde{\\eta}(\\vartheta , s)/\\sigma)\\right ] \\right\\rangle_{\\varepsilon,\\vartheta^0}\\right\\}. \\label{eq_need_appr}\\end{aligned}\\ ] ]    the average across quenched disorder can not be performed if we do not resort to some approximation .",
    "since we have already focused on the limit of large @xmath39 it is natural to consider an expansion of the error function in eq .",
    "( [ erf ] ) for a small value of its argument :    @xmath123    approximating all the error functions in eq.([eq_need_appr ] ) we obtain :    @xmath124    where in line with the approximation used in the case of the simple gaussian distribution we have omitted terms of order @xmath125 with @xmath126 .",
    "we reconsider eq.([outent ] ) . using replicas and assuming that the quenched randomness is identically distributed across neurons we obtain    @xmath127    where @xmath128 is given in eq.([dist_corr ] ) . integrating over @xmath129 yields @xmath130\\right\\rangle_{\\varepsilon,\\vartheta^0}^n\\end{aligned}\\ ] ] where we have used eq.([delta ] ) to integrate the @xmath110 function in eq.([dist_corr ] ) , and the expression of @xmath131 is like in eq.([gen_r ] ) . using the approximation ( [ erf_appr ] ) for the error function and considering the expansion for small @xmath28",
    "@xmath132 we obtain @xmath133 where @xmath134 ; \\label{c}\\ ] ]    @xmath135 . \\label{gkl}\\ ] ]    it is simple to verify that @xmath136 remains finite when @xmath28 goes to zero .",
    "now we expand in powers of @xmath0 up to the second order :    @xmath137    where we have omitted terms that are @xmath138 when @xmath64 .",
    "this quantity has to be summed over continuous and discrete replicas , after having explicitly performed the average across quenched disorder in eqs.([c ] ) and ( [ gkl ] ) .",
    "it is easy to show that @xmath136 cancels exactly with analogous terms in the equivocation , eq.([equiv_corr ] ) .",
    "the evaluation of the linear and quadratic term in the mutual information can be performed with a similar technique to the one used in the case of the gaussian distribution and it involves averaging terms with @xmath139 and @xmath140replica interactions .",
    "the final expression for the mutual information in the linear and quadratic approximation reads    @xmath141-\\frac{n^2(\\eta^0)^4}{2(4\\sigma^2)^2}\\left(\\frac{1}{2}+\\frac{1}{\\pi}\\right)^2\\right.\\\\ & & \\left.\\left\\{\\frac{p-1}{p^2}2\\left(2\\left(\\alpha - a_1\\right)^2\\lambda_1\\right)^2+\\left[\\frac{p-1}{p}\\left(\\lambda_1-\\lambda_2\\right)^2+\\frac{(\\lambda_2)^2}{p}\\right]\\left[\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\right]\\right\\}\\right\\}.\\nonumber \\label{final_info4_corr}\\end{aligned}\\ ] ]    comparing eq.([final_info4 ] ) and ( [ final_info4_corr ] ) it is evident that modifying the gaussian model into the more realistic _",
    "tg _ model has no effect on the analytical expression of the mutual information , except for a renormalization of the expansion parameter @xmath142 :    @xmath143    fig.[res_vary ] shows the effect of the renormalization for different values of @xmath81 . the mutual information is lower in the _ tg _ model than in the gaussian approximation , as expected .",
    "we have explored whether the two models can fit the information rise estimated from real data . since the analytical expression of the mutual information is the same in both cases the fit does not change between the two models .",
    "fig.[res_fit ] shows the comparison between the information estimated from a sample of cells recorded in the right primary motor cortex @xcite and the prediction given by either of the two theoretical distributions . in the limit of large",
    "@xmath39 the gaussian model fails to provide a good fit , but we can conclude that this failure is not due to the inclusion of negative rates in the distribution .",
    "fig.[tuning_data ] suggests that the directional tuning of real cells is modulated , albeit moderately , by the type of movement .",
    "in fact the analysis of real data has proved that the coding of the direction is not unique , but it is specific to the complex correlate which is being considered @xcite ( here , which arm moves ) .",
    "more in general , distinct features characterizing a complex stimulus are not expected to be coded independently of one another .",
    "this raises the question of how central representations of external correlates are constructed and which are the basic featural components of these representations .",
    "of course the categorization of natural stimuli is arbitrary and the more accurate a description is provided , the higher the dimensionality of the stimulus set . since an infinite number of different descriptors could be chosen to characterize a stimulus , any ( finite ) categorization has the effect of emphasizing some _ relevant _ features and averaging out other _ irrelevant _ features .",
    "an obvious consequence is that we end up , even involuntarily , evaluating how some features are coded _ on average _ , with respect to the dimension we have chosen , explicitly or implicitly , to neglect .",
    "thus , with correlates which have one continuous and one discrete dimension , one might wonder which are the relationships among the information carried about the total number of continuous+discrete dimensions , the information carried about the continuous dimension , disregarding the discrete dimension and , finally , the information carried about the continuous dimension , if a single value of the discrete dimension had been fixed when recording neural activity . in other words , suppose that we investigate how the direction is coded _ on average _ across different types of movement .",
    "this corresponds to averaging the full distribution @xmath116 on @xmath8 :    @xmath144    the resulting expression of the mutual information is :    @xmath145    the analytical evaluation is very similar to the cases already discussed .    as usual",
    ", the mutual information can be expressed as the difference between the entropy of the responses @xmath27 and the equivocation @xmath146 , where    @xmath147    @xmath148",
    "\\right\\rangle_{\\varepsilon,\\vartheta^0}. \\label{outent_avg}\\ ] ]    we focus on the more realistic _",
    "tg _ model , eq.([dist_corr ] ) ; the entropy of the responses is obviously independent on the chosen categorization of the stimuli ; in the limit of large @xmath39 the procedure is precisely the one followed in the previous section .",
    "the difference with respect to the cases already discussed is in the evaluation of the equivocation : since @xmath149 is obtained averaging the distribution ( [ dist_corr ] ) across @xmath8 , we need to introduce discrete replicas @xmath150 , as when evaluating the entropy of the responses . then the calculation is straightforward and the basic steps are given in the previous section and in the appendix .",
    "since in one case ( for the entropy of the responses ) we sum both over discrete and over continuous replicas and in the other case ( for the equivocation ) we sum only on discrete replicas , it is clear that all terms that do not involve 2 or more replica interacting cancel out .",
    "more in detail , if the evaluation of the entropy of the responses requires the analytical calculation of averages like @xmath151 ( see the appendix ) , these averages disappear in the evaluation of the equivocation , since replica indexes are only for the discrete variable @xmath8 .",
    "the final result for the mutual information up to the quadratic approximation reads :    @xmath152\\right .\\nonumber\\\\ & & \\left .-\\frac{n^2(\\eta^0)^4}{2(4\\sigma^2)^2 } \\left(\\frac{1}{2}+\\frac{1}{\\pi}\\right)^2\\left\\{\\frac{p-1}{p^2 } 8\\left[\\left(\\alpha - a_1\\right)^4-\\left(a_2 - 2\\alpha a_1+\\alpha^2\\right)^2\\right]\\left(\\lambda_1\\right)^2\\right.\\right.\\nonumber\\\\ & & \\left.\\left.+\\left[\\frac{p-1}{p}\\left(\\lambda_1-\\lambda_2\\right)^2+\\frac{(\\lambda_2)^2}{p}\\right ] \\left[\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\right]\\right\\}\\right\\}. \\label{final_info4_avg}\\end{aligned}\\ ] ]    fig.[res_p_avg](a ) compares the _ averaged _ information , @xmath153 , with the _ full _ information @xmath154 , where in the case of @xmath154 we have put @xmath155 .",
    "the full information calculated with @xmath155 in fact gives the curve one would obtain , on average , by considering only one movement type ( or value of the discrete correlate ) at a time . as one could expect , averaging the distribution across the discrete correlates results in an information loss .",
    "moreover , the the _ full _ information with @xmath156 movement types is obviously above the _ specific _ one , obtained by setting @xmath155 ( compare fig.[res_vary ] and [ res_p_avg](a ) ) .",
    "fig.[res_p_avg](b ) shows the dependence of the full and averaged information on the number @xmath1 of discrete correlates .",
    "contrary to the _ full _ information , the _ averaged _ information decreases monotonically with @xmath1 , both in the linear and in quadratic approximation .",
    "as one would expect , averaging the distribution across a large number @xmath1 of correlates is equivalent to a regularization of the activity distribution , which results in a lower mutual information .",
    "we have studied a model of the coding of discrete+continuous stimuli by a population of @xmath0 neurons , referring to the specific case of movements categorized according to a direction and a type .",
    "we have shown that , asymptotically in the limit of large populations of neurons , the mutual information tends to infinity logarithmically with the resolution in the continuous dimension .",
    "this result aside , we have focused on the initial rise of the mutual information with the population size , which may offer a more direct comparison with the analysis of real data .    in the limit of large noise we have therefore derived an analytical formula for such initial rise of the information , up to the quadratic approximation .",
    "we have examined the dependence on the number of discrete correlates and on the width of the directional tuning . a comparison with the information estimated from real data , in which the linear term is used as a fit coefficient , has shown that the quadratic approximation fails to capture the deviation from linearity .    we have then considered a more realistic model for the conditional firing distribution , a thresholded gaussian with a @xmath110-peak .",
    "we have shown that this more realistic distribution simply renormalizes the expansion parameter applicable to the gaussian distribution .",
    "therefore , the discrepancy in the fit to real data does not originate in the firing rate distribution .",
    "there are several possible reasons for this discrepancy :    * the value of the expansion parameter @xmath157 corresponding to the best fit in fig.([res_fit ] ) is quite high ( @xmath158 ) .",
    "this value is in the range where the quadratic approximation is expected to fail on its own ( see fig.([res_vary_g]),([res_vary ] ) ) . adding higher orders in perturbation theory might improve the fit .",
    "moreover , we have neglected terms of order @xmath125 with @xmath126 , which might be non negligible when @xmath81 becomes large and @xmath0 is not too small . *",
    "information estimates from real data are often biased because of poor sampling .",
    "several procedures have been proposed to correct the bias @xcite , but the improvement given by the correction is not precisely quantifiable , and sampling biases can not be discarded for good . * both with the _ tg _ model and in the gaussian approximation , we have assumed that neurons fire independently of one another to each movement , but the analysis of extracellular recordings has shown that correlations may play a non - negligible role in the coding @xcite .",
    "finally , we have examined the effect of averaging the distribution across the discrete correlates , evaluating the mutual information with respect to the continuously varying dimension alone . as expected , averaging the distribution across @xmath8 results in an information loss , which is more serious the larger the number @xmath1 of discrete correlates .",
    "further developments of this work include : the introduction of weak correlations in the signal of different neurons and the analysis of the information transfer to other stages of processing , given this as the coding scheme in the input layer of the network . in the specific case of movement coding these research directions might help model how information is transmitted down the motor system , in the planning and execution of motor tasks .",
    "we show here how to evaluate the coefficient of the quadratic term in eq.([r_appr ] ) , that we write again :      this quantity has to be integrated over continuous and discrete replicas .",
    "first we perform the average across the quenched variable @xmath160 . expanding the products it is easy to see that the quantities to be averaged are all like @xmath161 and @xmath162 , that we have already calculated in eq.([a_1]),([a_2 ] ) .",
    "another average that we need is @xmath151 , with @xmath163 :    @xmath164 ^ 2+\\frac{1}{2}\\left(\\frac{1}{2^{2m-1}}\\right)^2\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^2\\cos\\left\\{\\left(m-\\nu\\right)\\left(\\vartheta_k-\\vartheta_l\\right)\\right\\}\\right\\}. \\label{a_3}\\ ] ]      since terms like @xmath165 , with @xmath163 are zero when integrated on @xmath105,@xmath166 the only term which requires a careful evaluation in performing the integration on continuous replicas is the product @xmath167 .",
    "@xmath168 ^ 4+\\frac{1}{4}\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\left(\\delta_{k\\varrho}\\delta_{\\mu l}+\\delta_{k\\mu}\\delta_{\\varrho l}\\right)\\right\\}.\\ ] ]      @xmath169x\\nonumber\\\\ & & x\\left[\\left(\\alpha - a_1\\right)^2\\left\\langle(\\varepsilon_{s_{\\varrho}}\\!-\\!\\varepsilon_{s_{\\mu}})^2\\right\\rangle_{\\varepsilon}+\\left(a_2\\!-\\!(a_1)^2\\right)\\left\\langle\\varepsilon_{s_{\\varrho}}^2\\!+\\!\\varepsilon_{s_{\\mu}}^2\\right\\rangle_{\\varepsilon}\\right]\\nonumber\\\\ & & + \\left\\langle\\varepsilon_{s_k}\\varepsilon_{s_l}\\right\\rangle_{\\varepsilon}\\left\\langle\\varepsilon_{s_{\\varrho}}\\varepsilon_{s_{\\mu}}\\right\\rangle_{\\varepsilon}\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\left(\\delta_{k\\varrho}\\delta_{\\mu l}+\\delta_{k\\mu}\\delta_{\\varrho l}\\right ) \\label{mid_sum}\\end{aligned}\\ ] ]    where @xmath77 and @xmath78 are defined in eq.([a_1 ] ) and ( [ a_2 ] ) . to correctly perform the summation across the discrete replicas and to keep only terms of order @xmath28 in the limit @xmath64",
    ", we consider separately the different contributions to the sum :            @xmath174\\left[\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\right]\\right\\};\\ ] ] where the expressions of @xmath75,@xmath76 are given in eqs.([lambda1 ] ) , ( [ lambda2 ] ) . considering the result",
    "obtained at the first order , eq.([final_info2 ] ) , it is easy to derive the expression for the mutual information up to the second order in @xmath46 :    @xmath74\\right .\\nonumber\\\\ & & \\left .-\\frac{n^2(\\eta^0)^4}{2(4\\sigma^2)^2}\\left\\{\\frac{p-1}{p^2}2\\left(2\\left(\\alpha - a_1\\right)^2\\lambda_1\\right)^2+\\left[\\frac{p-1}{p}\\left(\\lambda_1-\\lambda_2\\right)^2+\\frac{\\lambda_2 ^ 2}{p}\\right]\\left[\\left(\\frac{1}{2^{2m-1}}\\right)^4\\sum_{\\nu=0}^{m-1}\\left[\\left(\\begin{array}{c}2m\\\\\\nu\\end{array}\\right)\\right]^4\\right]\\right\\}\\right\\}. \\label{final_infoapp}\\end{aligned}\\ ] ]              alessandro treves .",
    "information coding in higher sensory and memory areas . in f.",
    "moss and s.  gielen , editors , _ neuro - informatics and neural modelling _ , volume  4 of _ handbook of biological physics _ , pages 803829 , amsterdam , 2000 .",
    "elsevier .",
    "e.  t. rolls , a.  treves , and m.  j. tove . the representational capacity of the distributed encoding of information provided by populations of neurons in the primate temporal visual cortex .",
    ", 114:149162 , 1997a ."
  ],
  "abstract_text": [
    "<S> in a recent study the initial rise of the mutual information between the firing rates of @xmath0 neurons and a set of @xmath1 discrete stimuli has been analytically evaluated , under the assumption that neurons fire independently of one another to each stimulus and that each conditional distribution of firing rates is gaussian . yet </S>",
    "<S> real stimuli or behavioural correlates are high - dimensional , with both discrete and continuously varying features . </S>",
    "<S> moreover , the gaussian approximation implies negative firing rates , which is biologically implausible . here </S>",
    "<S> , we generalize the analysis to the case where the stimulus or behavioural correlate has both a discrete and a continuous dimension , like orientation and shape could be in a visual stimulus , or type and direction in a motor action . the functional relationship between the firing patterns and the continuous correlate is expressed through the tuning curve of the neuron , using two different parameters to modulate its width and its flatness . in the case of large noise </S>",
    "<S> we evaluate the mutual information up to the quadratic approximation as a function of population size . </S>",
    "<S> we also show that in the limit of large @xmath0 and assuming that neurons can discriminate between continuous values with a resolution @xmath2 , the mutual information grows to infinity like @xmath3 when @xmath2 goes to zero . </S>",
    "<S> then we consider a more realistic distribution of firing rates , truncated at zero , and we prove that the resulting correction , with respect to the gaussian firing rates , can be expressed simply as a renormalization of the noise parameter . finally , we demonstrate the effect of averaging the distribution across the discrete dimension , evaluating the mutual information only with respect to the continuously varying correlate . </S>"
  ]
}