{
  "article_text": [
    "provision for mechanisms capable of handling gross errors caused by possible arbitrarily large model deviations is a typical prerequisite in computer vision .",
    "such deviations are not unusual in real - world applications where data contain artifacts due to occlusions , illumination changes , shadows , reflections or the appearance of new parts / objects . in most cases ,",
    "such phenomena can not be described by a mathematically well - defined generative model and are usually referred as outliers in learning and parameter estimation .    in this paper",
    ", we propose a new avenue for principal component analysis ( pca ) , perhaps the most classical tool for dimensionality reduction and feature extraction in pattern recognition .",
    "standard pca estimates the @xmath1rank linear subspace of the given data population , which is optimal in a least - squares sense .",
    "unfortunately @xmath0 norm enjoys optimality properties only when noise is i.i.d .",
    "gaussian ; for data corrupted by outliers , the estimated subspace can be arbitrarily biased .",
    "robust formulations to pca , such as robust covariance matrix estimators @xcite , are computationally prohibitive for high dimensional data such as images . robust approaches , well - suited for computer vision applications , include @xmath2 @xcite , robust energy function @xcite and weighted combination of nuclear norm and @xmath2 minimization @xcite .",
    "@xmath2-based approaches can be computational efficient , however the gain in robustness is not always significant .",
    "the m - estimation framework of @xcite is robust but suitable only for relatively low dimensional data or off - line processing . under weak assumptions @xcite ,",
    "the convex optimization formulation of @xcite perfectly recovers the low dimensional subspace of a data population corrupted by sparse arbitrarily large errors ; nevertheless efficient reformulations of standard pca can be orders of magnitude faster .    in this paper",
    "we look at robust pca from a completely different perspective .",
    "our scheme _ does not _ operate on pixel intensities .",
    "in particular , we replace pixel intensities with gradient orientations .",
    "we define a notion of pixel - wise image dissimilarity by looking at the distribution of gradient orientation differences ; intuitively this must be approximately uniform in @xmath3 .",
    "we then assume that local orientation mismatches caused by outliers can be also well - described by a uniform distribution which , under some mild assumptions , is canceled out when we apply the cosine kernel .",
    "this last observation has been noticed in recently proposed schemes for image registration @xcite . following this line of research ,",
    "we show that a cosine - based distance measure has a functional form which enables us to define an explicit mapping from the space of gradient orientations into a high - dimensional complex sphere where essentially linear complex pca is performed .",
    "the mapping is one - to - one and therefore pca - based reconstruction in the original input space is direct and requires no further optimization .",
    "similarly to standard pca , the basic computational module of our scheme requires the eigen - decomposition of a covariance matrix , while high dimensional data can be efficiently analyzed following the strategy suggested in turk and pentland s eigenfaces @xcite .",
    "let us denote by @xmath4 the @xmath5dimensional vector obtained by writing image @xmath6 in lexicographic ordering .",
    "we assume that we are given a population of @xmath7 samples @xmath8\\in \\re^{p\\times n}$ ] . without loss of generality",
    ", we assume zero - mean data .",
    "pca finds a set of @xmath9 orthonormal bases @xmath10\\in \\re^{p\\times k}$ ] by minimizing the error function @xmath11 the solution is given by the eigenvectors corresponding to the @xmath12 largest eigenvalues obtained from the eigen - decomposition of the covariance matrix @xmath13 .",
    "finally , the reconstruction of @xmath14 from the subspace spanned by the columns of @xmath15 is given by @xmath16 , where @xmath17 is the matrix which gathers the set of projection coefficients .",
    "for high dimensional data and small sample size ( sss ) problems ( i.e. @xmath18 ) , an efficient implementation of pca in @xmath19 ( instead of @xmath20 ) was proposed in @xcite . rather than computing the eigen - analysis of @xmath13 , we compute the eigen - analysis of @xmath21 and make use of the following theorem + * theorem i * + define matrices @xmath22 and @xmath23 such that @xmath24 and @xmath25 with @xmath26 .",
    "let @xmath27 and @xmath28 be the eigenvectors corresponding to the non - zero eigenvalues @xmath29 and @xmath30 of @xmath22 and @xmath23 , respectively .",
    "then , @xmath31 and @xmath32 .",
    "we formalize an observation for the distribution of gradient orientation differences which does not appear to be well - known in the scientific community .",
    "consider a set of images @xmath33 . at each pixel location",
    ", we estimate the image gradients and the corresponding gradient orientation . , where @xmath34 , @xmath35 and @xmath36 are filters used to approximate the ideal differentiation operator along the image horizontal and vertical direction respectively .",
    "possible choices for @xmath36 include central difference estimators of various orders and discrete approximations to the first derivative of the gaussian .",
    "] we denote by @xmath37 the set of orientation images and compute the orientation difference image @xmath38 we denote by @xmath39 and @xmath40 the @xmath5dimensional vectors obtained by writing @xmath41 and @xmath42 in lexicographic ordering and @xmath43 the set of indices corresponding to the image support .",
    "we introduce the following definition . + * definition * images @xmath44 and @xmath45 are pixel - wise dissimilar if @xmath46 , @xmath47 . +",
    "not surprisingly , nature is replete with images exemplifying definition 1 .",
    "this , in turn , makes it possible to set up a naive image - based random generator . to confirm this",
    ", we used more than @xmath48 pairs of image patches of resolution @xmath49 randomly extracted from natural images @xcite . for each pair",
    ", we computed @xmath50 and formulated the following null hypothesis    * @xmath51 : @xmath52 .    which was tested using the kolmogorov - smirnov test @xcite . for a significance level equal to @xmath53 ,",
    "the null hypothesis was accepted for @xmath54 of the image pairs with mean @xmath55-value equal to @xmath56 . in a similar setting , we tested matlab s random generator .",
    "the null hypothesis was accepted for @xmath57 of the cases with mean @xmath55-value equal to @xmath58 .",
    "[ fig1 ] ( a)-(b ) show a typical pair of image patches considered in our experiment .",
    "[ fig1 ] ( c ) and ( d ) plot the histograms of the gradient orientation differences and 40,000 samples drawn from matlab s random number generator respectively .",
    "given the set of our images @xmath59 , we compute the corresponding set of orientation images @xmath60 and measure image correlation using the cosine kernel @xmath61=cn(\\mathcal{p } )   \\label{oc}\\ ] ] where @xmath62 $ ] .",
    "notice that for highly spatially correlated images @xmath63 and @xmath64 .",
    "assume that there exists a subset @xmath65 corresponding to the set of pixels corrupted by outliers . for @xmath66 , we have @xmath67=c_1n(\\mathcal{p}_1)\\label{oc1}\\ ] ] where @xmath68 $ ] .",
    "not unreasonably , we assume that in @xmath69 , the images are pixel - wise dissimilar according to definition 1 .",
    "for example , fig .",
    "[ fig2 ] ( a)-(b ) show an image pair where @xmath69 is the part of the face occluded by the scarf and fig .",
    "[ fig2 ] ( c ) plots the distribution of @xmath70 in @xmath69 .     for the part of face occluded by the scarf .",
    "]    before proceeding for @xmath69 , we need the following theorem @xcite . + * theorem ii * + let @xmath71 be a random process and @xmath72 then :    * @xmath73=0 $ ] for any non - empty interval @xmath74 of @xmath75 .",
    "* if @xmath71 is mean ergodic , then @xmath76 .",
    "we also make use of the following approximation @xmath77dt \\approx \\sum_{k\\in \\mathcal{p } } \\cos [ \\delta\\mbox{\\boldmath$\\phi$}_{ij}(k ) ] \\label{approx1}\\ ] ] where with some abuse of notation , @xmath50 is defined in the continuous domain on the left hand side of ( [ approx1 ] ) .",
    "completely analogously , the above theorem and approximation hold for the case of the sine kernel .    using the above results , for @xmath69",
    ", we have @xmath78 \\simeq 0\\label{ocp2}\\ ] ] it is not difficult to verify that @xmath0-based correlation i.e. the inner product between two images will be zero if and only if the images have interchangeably black and white pixels . our analysis and ( [ ocp2 ] ) show that cosine - based correlation of gradient orientations allows for a much broader class of uncorrelated images .",
    "overall , unlike @xmath0-based correlation where the contribution of outliers can be arbitrarily large , @xmath79 measures correlation as @xmath80 , i.e. the effect of outliers is approximately canceled out .      to show how ( [ oc ] ) can be used as a basis for pca",
    ", we first define the distance @xmath81\\}\\label{doc}\\ ] ] we can write ( [ doc ] ) as follows @xmath82\\}\\nonumber\\\\ & = & \\frac{1}{2}|| e^{j\\mbox{\\boldmath$\\phi$}_i } - e^{j\\mbox{\\boldmath$\\phi$}_j } ||^2\\end{aligned}\\ ] ] where @xmath83^t$ ] .",
    "the last equality makes the basic computational module of our scheme apparent .",
    "we define the mapping from @xmath84 onto a subset of complex sphere with radius @xmath85 @xmath86 and apply linear complex pca to the transformed data @xmath87 .    using the results of the previous subsection , we can remark the following + * remark i * if @xmath88 with @xmath89 , then @xmath90 \\simeq c_1n(\\mathcal{p}_1)$ ] + * remark ii * if @xmath91 , then @xmath90 \\simeq 0 $ ] and @xmath92 \\simeq 0 $ ] .",
    "further geometric intuition about the mapping @xmath87 is provided by the chord between vectors @xmath87 and @xmath93 @xmath94 using @xmath95 , the results of remark 1 and 2 can be reformulated as @xmath96 and @xmath97 respectively .",
    "overall , @xmath98 summarizes the steps of our pca of gradient orientations",
    ". * algorithm 1 . * _ _ estimating the principal subspace__**inputs : * * a set of @xmath7 orientation images @xmath99 of @xmath55 pixels and the number @xmath12 of principal components . * step 1 . * obtain @xmath39 by writing @xmath41 in lexicographic ordering . * step 2 . * compute @xmath100 , form the matrix of the transformed data @xmath101\\in \\mathcal{c}^{p\\times n}$ ] and compute the matrix @xmath102 . * step 3 . *",
    "compute the eigen - decomposition of @xmath103 and denote by @xmath104 and @xmath105 the @xmath1reduced set .",
    "compute the principal subspace from @xmath106",
    ". * step 4 . * reconstruct using @xmath107 . * step 5 . *",
    "go back to the orientation domain using @xmath108 .",
    "let us denote by @xmath109 the set of image indices and @xmath110 any subset of @xmath111 .",
    "we can conclude the following + * remark iii * if @xmath112 with @xmath113 @xmath114 , @xmath115 and @xmath116 , then , @xmath117 eigenvector @xmath118 of @xmath119 such that @xmath120 .    a special case of remark iii",
    "is the following + * remark iv * if @xmath121 , then @xmath122 and @xmath123 .    to exemplify remark iv , we computed the eigen - spectrum of 100 natural image patches . in a similar setting , we computed the eigen - spectrum of samples drawn from matlab s random number generator .",
    "[ fig3 ] plots the two eigen - spectrums .",
    "finally , notice that our framework also enables the direct embedding of new samples .",
    "@xmath124 2 summarizes the procedure.*algorithm 2 . *",
    "_ _ embedding of new samples__**inputs : * * an orientation image @xmath125 of @xmath55 pixels and the principal subspace @xmath15 of @xmath98 .",
    "* obtain @xmath126 by writing @xmath125 in lexicographic ordering .",
    "* step 2 . * compute @xmath127 and reconstruct using @xmath128 . * step 3 .",
    "* go back to the orientation domain using @xmath129 .",
    "the estimation of a low - dimensional subspace from a set of a highly - correlated images is a typical application of pca @xcite . as an example",
    ", we considered a set of 50 aligned face images of image resolution @xmath130 taken from the yale b face database @xcite .",
    "the images capture the face of the same subject under different lighting conditions .",
    "this setting usually induces cast shadows as well as other specularities .",
    "face reconstruction from the principal subspace is a natural candidate for removing these artifacts .",
    "we initially considered two versions of this experiment .",
    "the first version used the set of original images . in the second version , 20@xmath131 of the images",
    "was artificially occluded by a @xmath132 `` baboon '' patch placed at random spatial locations .",
    "for both experiments , we reconstructed pixel intensities and gradient orientations with @xmath0 pca and pca of gradient orientations respectively using the first 5 principal components .    fig .",
    "[ fig4 ] and fig .",
    "[ fig5 ] illustrate the quality of reconstruction for 2 examples of face images considered in our experiments . while pca - based reconstruction of pixel intensities is visually appealing in the first experiment , fig .",
    "[ fig4 ] ( g)-(h ) clearly illustrate that , in the second experiment , the reconstruction suffers from artifacts . in",
    "contrary , fig .",
    "[ fig5 ] ( e)-(f ) and ( g)-(h ) show that pca - based reconstruction of gradient orientations not only reduces the effect of specularities but also reconstructs the gradient orientations corresponding to the `` face '' component only .",
    "this performance improvement becomes more evident by plotting the principal components for each method and experiment .",
    "[ fig6 ] shows the 5 dominant eigenfaces of @xmath0 pca .",
    "observe that , in the second experiment , the last two eigenfaces ( fig .",
    "[ fig6 ] ( i ) and ( j ) ) contain `` baboon '' ghosts which largely affect the quality of reconstruction . in contrary , a simple visual inspection of fig .",
    "[ fig7 ] reveals that , in the second experiment , the principal subspace of gradient orientations ( fig . [ fig7 ] ( f)-(j ) ) is artifact - free which in turn makes dis - occlusion in the orientation domain feasible .    finally , to exemplify remark 3 , we considered a third version of our experiment where 20@xmath131 of the images were replaced by _ the same _",
    "@xmath130 `` baboon '' image .",
    "[ fig8 ] ( a)-(e ) and ( f)-(j ) illustrate the principal subspace of pixel intensities and gradient orientations respectively .",
    "clearly , we may observe that @xmath0 pca was unable to handle the extra - class outlier . in contrary ,",
    "pca of gradient orientations successfully separated the `` face '' from the `` baboon '' subspace i.e. no eigenvectors were corrupted by the `` baboon '' image .",
    "note that the `` face '' principal subspace is not the same as the one obtained in versions 1 and 2 .",
    "this is because only 80@xmath131 of the images in our dataset was used in this experiment .",
    "we introduced a new concept : pca of gradient orientations .",
    "our framework is as simple as standard @xmath0 pca , yet much more powerful for efficient subspace - based data representation .",
    "central to our analysis is the distribution of gradient orientation differences and the cosine kernel which provide us a consistent way to measure image dissimilarity .",
    "we showed how this dissimilarity measure can be naturally used to formulate a robust version of pca .",
    "extensions of our scheme span a wide range of theoretical topics and applications ; from statistical machine learning and clustering to object recognition and tracking .",
    "q.  ke and t.  kanade , _ robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming _ , in ieee computer society conference on computer vision and pattern recognition , cvpr ( 2005 ) .",
    "g.  tzimiropoulos , v.  argyriou , s.  zafeiriou , and t.  stathaki , _ robust fft - based scale - invariant image registration with image gradients _ , ieee transactions on pattern analysis and machine intelligence , accepted for publication ( 2010 ) .",
    "georghiades , p.n .",
    "belhumeur and d.j .",
    "kriegman , _ from few to many : illumination cone models for face recognition under variable lighting and pose _ , ieee transactions on pattern analysis and machine intelligence , 23 ( 2001 ) , pp .  643660 ."
  ],
  "abstract_text": [
    "<S> we introduce the notion of principal component analysis ( pca ) of image gradient orientations . as </S>",
    "<S> image data is typically noisy , but noise is substantially different from gaussian , traditional pca of pixel intensities very often fails to estimate reliably the low - dimensional subspace of a given data population . </S>",
    "<S> we show that replacing intensities with gradient orientations and the @xmath0 norm with a cosine - based distance measure offers , to some extend , a remedy to this problem . </S>",
    "<S> our scheme requires the eigen - decomposition of a covariance matrix and is as computationally efficient as standard @xmath0 pca . </S>",
    "<S> we demonstrate some of its favorable properties on robust subspace estimation .    principal component analysis , gradient orientations , cosine kernel    notation     +    [ cols= \" < , < \" , ] </S>"
  ]
}