{
  "article_text": [
    "finite element numerical integration forms an indispensable part of practically all complex finite element codes .",
    "apart from special formulations for specific problems ( such as e.g. applications in structural mechanics for beams , plates etc . ) where it can be avoided using some analytically obtained formulae , it is universally used for creating entries to finite element stiffness matrices and load vectors ( matrices and right hand side vectors for systems of linear equations ) .",
    "the recent progress in hardware for numerical computations caused the widespread use of multi - core and massively multi - core ( often termed many - core ) processors with vector processing capabilities .",
    "these created the necessity to reconsider the implementations of scientific codes , taking into account the possible options for multi - threading and the changing execution environment characteristics .",
    "the aim of the research presented in the current paper is to thoroughly analyse the process of finite element numerical integration for first order approximations , in a possibly broad context , and to propose a set of guidelines for designing efficient integration procedures for multi - core processors , taking into account simd modes of execution .",
    "the paper shows also some variants of the algorithm , that are implemented for different hardware platforms and tested in practice .",
    "the paper is organised as follows .",
    "first , finite element numerical integration is described in the form investigated in our work .",
    "then , in section 3 its implementation , in the context of finite element simulations and for different types of problems , elements and computing platforms is analysed .",
    "several variants of the procedure are developed , and , in the next section , their optimization and mapping to different processor architectures is discussed .",
    "section 5 presents the results of computational experiments that test the performance of procedures with short discussion of results .",
    "the last two sections briefly document other works related to the subject presented in the current paper and present the final conclusions .",
    "in the current paper , we consider the finite element method as a tool for finding approximate solutions to partial differential equations , specified in a computational domain @xmath0 with suitable boundary conditions imposed on @xmath1 , based on weak formulations that can be expressed ( with some details omitted ) as follows @xcite :    find an unknown function @xmath2 belonging to a specified function space @xmath3 , such that _",
    "( ^i;j _ , i _ , j + ^i;0 _ , i + ^0;i _ , i + ^0;0 ) d+ btl = [ weak_form ] + = _ ( ^0 + ^i _ , i ) d+ btr for every test function @xmath4 defined in a space @xmath5 .",
    "above , @xmath6 and @xmath7 , @xmath8 denote the coefficients specific to the weak formulation ( with @xmath9 being the number of physical space dimensions ) , ",
    "@xmath10  denotes differentiation with respect to space coordinates and summation convention for repeated indices is used . @xmath11 and",
    "@xmath12 denote boundary terms , usually associated with integrals over the boundary @xmath1 .",
    "the task of calculating boundary terms is usually much less computationally demanding , and we neglect it in the current paper , when discussing implementations for multi- and many - core architectures . in the actual computations described later in the paper",
    "this task is performed by the part of the code executed by cpu cores , using standard finite element techniques .",
    "there are many possible finite element approximation spaces that can be defined for triangulations of @xmath0 into a set of finite elements @xmath13 , @xmath14 in the current paper we concentrate on scalar problems and the popular case of first order approximations , i.e. spaces spanned by elementwise linear ( or multilinear ) basis functions , associated with element vertices .",
    "vertices with the associated basis functions form the nodes of the finite element mesh , with their number denoted by @xmath15 .",
    "piecewise linear basis functions are defined for simplex elements ( triangles in 2d and tetrahedra in 3d ) , multilinear basis functions are specified for other types of elements ( quadrilateral , prismatic , hexahedral , etc . )    after representing unknown and test functions as linear combinations of basis functions @xmath16 , @xmath17 , the system ( [ weak_form ] ) is transformed to a set of linear equations for the unknown vector of degrees of freedom @xmath18 : _",
    "u^s = b^r r=1, .. ,n_n [ linear_system_global ] with the entries in the global system matrix @xmath19 ( the stiffness matrix ) computed as ( with boundary terms neglected ) : ^rs = _ ( _ i _ j c^i;j ^r_,i ^s_,j + _ i c^i;0 ^r_,i ^s _ + _ i c^0;i ^r ^s_,i + c^0;0 ^r ^s ) d[sm_global ]    the entries to the global right hand side vector @xmath20 ( the load vector ) are calculated according to the formula ( again with boundary terms neglected ) : ^r = _ ( _ i d^i ^r_,i + d^0 ^r ) d[lv_global ]    the solution of a single problem ( [ weak_form ] ) ( or equivalently the system ( [ linear_system_global ] ) ) may constitute the only step of the finite element solution process or may form a single stage of more complex solution strategies , e.g. for time dependent and/or nonlinear problems . in the latter case",
    "the efficient accomplishment of numerical integration becomes even more important for the overall performance of simulation programs .",
    "the solution of a single finite element problem represented by ( [ weak_form ] ) or ( [ linear_system_global ] ) requires the creation of global stiffness matrix and load vector entries , followed by their use in obtaining the final solution of the problem .",
    "a single entry in the global stiffness matrix or load vector consists of an integral over the whole computational domain plus some boundary terms . the integral over the computational domain , on which we concentrate in the current paper , can be expressed as the sum of contributions from individual elements : _ ... d= _ e _ _ e ... d    the entries can be either computed individually , in a double loop over all entries of the global stiffness matrix , or can be computed in a loop over all elements , with all the entries related to a given element computed at once .",
    "we consider in the current paper only the latter strategy .",
    "compared to the first strategy where each finite element is visited several times , this saves some element related calculations but leaves as a result element contributions that have to be further processed .",
    "our choice is motivated by the fact that for many types of problems , including nonlinear , the overhead of repeated element calculations may become too high . in our approach we do not decide , however , how element contributions are used further in the solution procedure . different options , with the assembly of the global stiffness matrix @xcite or without assembly , in the so called matrix - free approaches @xcite , can be applied .",
    "the element contributions to the global stiffness matrix form a small dense matrix @xmath21 , with each entry corresponding to a pair of element shape functions @xmath22 ( that are used for creation of specific global basis functions ) .",
    "the entries of the element stiffness matrix ( with boundary terms neglected ) are given by the formula : ( a^e)^rs = _ _ e ( _ i _ j c^i;j ^r_,i ^s_,j + _ i c^i;0 ^r_,i ^s + _",
    "i c^0;i ^r ^s_,i + c^0;0 ^r ^s ) d[sm_local ] where now indices @xmath23 and @xmath24 are local indices , with the range from 1 to @xmath25  the number of shape functions for a given element .    the element right hand side vector , that forms the element contribution to the global right hand side vector , is computed based on the formula ( with boundary terms neglected ) : ( b^e)^r = _ _ e ( _ i d^i ^r_,i + d^0 ^r ) d [ lv_local ]      we consider in our paper the creation of element stiffness matrices and load vectors using numerical integration and transformations to reference elements .",
    "both procedures are related , since quadratures for numerical integration are usually defined for reference elements .",
    "shape functions are defined for a reference element of a given type of geometry and approximation .",
    "each real element in the finite element mesh is treated as an image of the associated reference element under a suitable geometric transformation . denoting the physical coordinates of points in the real element by @xmath26 , the transformation ( defined separately for each element ) from the reference space with coordinates @xmath27",
    "is expressed as @xmath28 ( with the inverse transformation given by @xmath29 ) .",
    "the parameters of the transformation are used for calculations of global derivatives of shape functions that involve the elements of the jacobian matrix of the inverse transformation @xmath29 ( the elements of jacobian matrices @xmath30 and @xmath31 with their determinants , are later on , when considering actual computations , termed together as jacobian terms ) .",
    "the transformation from the reference element @xmath32 to the real element @xmath33 is used to perform the change of variables in integrals from ( [ sm_local ] ) and ( [ lv_local ] ) .",
    "taking an example integral from ( [ sm_local ] ) and applying the change of variables we obtain : _ _ e _ i _ j c^i;j d= _ _ i _ j c^i;j _ k _ z d[change_of_variables ] where @xmath34 denotes the shape functions defined for the reference element .",
    "the application of numerical integration quadratures ( in practical examples we always use gaussian quadratures ) leads to the transformation ( for brevity we use global derivatives of shape functions ) : _ _ e _ i _ j c^i;j d & & _ q=1^n_q .",
    "( _ i _ j c^i;j ) |_^q w^q [ quadrature ] where @xmath35 denotes the coordinates of the subsequent integration points with the associated weights @xmath36 , @xmath37 ( @xmath38 being the number of integration points related to the type of element , the degree of approximation and the form of coefficients @xmath39 ) .",
    "the input for the generic procedure of numerical integration for a single real element , that implements ( [ sm_local ] ) and ( [ lv_local ] ) , with ( [ change_of_variables ] ) , ( [ quadrature ] ) and similar expressions for other terms taken into account , is given by :    * the type of element , the type and the order of approximation * the set of local coordinates and weights for integration points * possibly the set of values of shape functions and their local derivatives at all integration points for the reference element ( they can be also computed on the fly during integration ) * the set of geometry degrees of freedom for the real element * the set of problem dependent parameters that are used for computing coefficients @xmath39 and @xmath40 at integration points ( for simple problems these may be the values of coefficients at integration points themselves )    [ specification_of_numerical_integration ]    the output of the procedure is formed by the element stiffness matrix @xmath21 and the element load vector @xmath41 .",
    "algorithm [ num_int_generic ] shows a generic procedure for creating a sequence of element stiffness matrices and load vectors , assuming they have the same type and order of approximation , with the data for the respective reference element ( integration data and shape functions data ) assumed to be available in local data structures .",
    "the algorithm shows how numerical integration calculations can be performed .",
    "as it can be seen , there are three subsequent , separate loops over integration points in the algorithm .",
    "they can be fused together , as it is often done in practice .",
    "the presented form of algorithm [ num_int_generic ] has been chosen to show that , with the suitable arrangement of calculation stages , the final computations of stiffness matrix entries are performed in five nested loops , with the order of loops being arbitrary .",
    "each order can , however , lead to different execution performance , that additionally depends on the hardware on which the code runs .",
    "this opens multiple ways for optimizing the execution characteristics of finite element numerical integration that we investigate in the following sections .",
    "we treat the procedure of numerical integration as one of computational kernels of finite element codes . by this",
    "we want to stress its importance for finite element computations and its influence on the performance of codes . to further clarify its place in finite element programs",
    "we describe a way in which , in the setting assumed in the paper , it interacts with the rest of the code .",
    "we consider the implementation of finite element numerical integration in codes designed for large scale calculations .",
    "the number of elements in such calculations often exceeds million and even can reach several billions . the only way to ensure the scalable execution of programs for that size of problems is",
    "to consider distributed memory systems and message passing .",
    "the standard way of implementation in this case is some form of domain decomposition approach . in the current paper",
    "we assume the design of finite element codes , in which the issues of message passing parallelization and multi - core acceleration are separated @xcite . in this",
    "design multithreading concerns only single node execution and a separate layer of software combines single node components into a whole parallel code .    as a result",
    "we consider the algorithm of numerical integration executed for all elements of a given subdomain .",
    "we assume the size of the subdomain to exceed one hundred thousand of elements , the size that justifies the use of massively multi - core accelerators . we allow for complex mesh and field data structures related to adaptive capabilities , many types of elements , several approximation fields and different approximation spaces in a single simulation .",
    "therefore we assume that the finite element data concerning elements and approximation fields are stored in standard global memory accessible to cpus .",
    "this memory should ensure the sufficient storage capacity and a latency minimizing way of accessing data .",
    "hence , the input data to numerical integration comes from finite element data structures stored in cpu memory .",
    "the output of numerical integration is formed by element stiffness matrices and load vectors . in order to allow for broad range of scenarios of using the computed entries ( standard direct , frontal , iterative solvers , with or without matrix assembly )",
    "we assume that the output arrays after being computed , can be stored in some memory , available to threads performing subsequent solution steps ( this can even include registers if e.g. assembly operation is designed as extension of procedures performing numerical integration ) .",
    "hence , we discuss the problem of effective storing of element arrays in memory of the processor performing integration , but consider also the case , where the data are not stored at all . in both cases",
    "the further use of element stiffness matrices and load vectors depends on implementation details of possible assembly and solver procedures .      as can be seen in algorithm [ num_int_generic ]",
    ", numerical integration in the form considered in the current paper , consists of several loops .",
    "each of these loops can be parallelized , leading to different parallel algorithms with different optimization options and performance characteristics .    for the analysis in the current paper we consider parallelization of only one loop  the loop over finite elements .",
    "due to the sufficiently large number of elements in the loop ( as assumed for message passing implementation of large scale problems ) and the embarrassingly parallel character of the loop this choice seem to be obvious . by analysing it in details ,",
    "we want to investigate its optimization and performance characteristics , but also to show possible limitations that can lead to different parallelization strategies .",
    "numerical integration is an embarrassingly parallel algorithm , but the later stages , assembly and linear system solution , are not . to allow for efficient execution of these stages ,",
    "further assumptions are made . as was described when developing algorithm [ num_int_generic ] , elements of a single subdomain are divided into sets of elements of the same type and degree of approximation ( to made them use the same reference element shape functions and integration quadratures ) . in order to allow for parallel assembly",
    ", the elements are further divided into subsets with different colors assigned .",
    "elements of a single color do not contribute to the same entries in the global stiffness matrix and their stiffness matrices can be easily assembled into the global stiffness matrix in a fully parallel way , with no data dependencies .      despite the fact that finite element numerical integration can be expressed by two simple formulas ( [ sm_local ] ) and ( [ lv_local ] ) , there exist many variations of the algorithm ( even when considering only first order approximations ) , that should be taken into account when designing computer implementations .",
    "first , there are different types of problems solved , scalar and vector , linear and non - linear .",
    "depending on the weak statement of the corresponding problem , the set of pde coefficients can be either sent as input to the procedure or the input to the procedure can form just the arguments of a special , problem dependent function , that calculates the final coefficients . in the current paper",
    "we consider the first option , with the assumption that the coefficients for element stiffness matrices are constant over the whole finite element .",
    "the coefficient matrices for different problems ( matrices @xmath42 in algorithm [ num_int_generic ] ) can have varying non - zero structure . to make our implementations , to certain extent , independent of these variations",
    ", we consider the organization of computations with the final calculations of updates to the stiffness matrix ( two loops over space dimensions @xmath43 and @xmath44  lines 17 - 22 in algorithm [ num_int_generic ] ) always performed inside all other loops .",
    "with such a solution , these final calculations are left as problem dependent and should be optimized separately , for each problem , either manually or using some automatic tools @xcite .",
    "another variation is related to different geometrical types of finite elements : linear , multilinear and geometrically higher order ( e.g. isoparametric for higher order approximations ) .",
    "the main difference , from the point of view of numerical integration , is between linear simplex elements , for which jacobian terms are constant over the whole element , and all other types for which jacobian terms have to be computed separately for each integration point . in our study",
    "we compare two types of elements : geometrically linear tetrahedral elements and geometrically multi - linear prismatic elements ( both are elements without curved boundaries , with geometry degrees of freedom given by vertices coordinates ) .",
    "for the two different types of geometry of elements we consider different variations of numerical integration for first order approximations .",
    "we explicitly take into account the fact that for geometrically linear elements the derivatives of geometric shape functions and solution shape functions are constant over each element and that the corresponding calculations can be moved out of the loop over integration points .",
    "finally , we consider several options for arranging the order of calculations in algorithm [ num_int_generic ] .",
    "for the three loops , over integration points ( @xmath45 ) and twice over shape functions ( @xmath46 and @xmath47 ) , we consider the different orderings and denote them by qss , sqs and ssq ( the symbol reflects the order implied by the subscripts of the loop indices ) .",
    "we assume that there are no separate loops for computing the additional quantities , jacobian terms and global derivatives of shape functions , but that the loops are fused together . combining together the options discussed above we present six algorithms ( [ qss_prism ] , [ qss_tetra ] , [ sqs_prism ] , [ sqs_tetra ] , [ ssq_prism ] , [ ssq_tetra ] ) for the final calculations of entries to @xmath21 ( and @xmath41 as well ) .",
    "the algorithms correspond to :    * different loop orderings  qss , sqs and ssq versions * different types of elements  @xmath48 for geometrically linear elements and @xmath49 for all other types ( it can be used for geometrically linear elements , but is less efficient ) * different types of problems  through the delegation of final updates to problem specific calculations .    in the algorithms it is assumed that the values of jacobian terms and the global derivatives of all shape functions are calculated just after entering the calculations for a given integration point",
    "the global derivatives are stored in some temporary array and later used in calculations . in practical calculations , in order to save the storage for temporary arrays , another strategy can be used with the values of global derivatives for each shape function computed just at the beginning of the body of the loop for this shape function .",
    "the drawback of the approach is that the calculations in the innermost loop over shape functions are redundantly repeated for each shape function .",
    "store the whole @xmath21 and @xmath41 as output of the procedure    calculate @xmath50 and * vol * + store the whole @xmath21 and @xmath41 as output of the procedure    calculate @xmath50 and * vol",
    "* +    calculate @xmath50 and * vol * +    the six variants of numerical integration can be considered as the optimizations of the algorithm , corresponding to classical techniques of loop interchange and loop invariant code motion .",
    "we perform them explicitly , to help compilers and to make the output for the same variant from different compilers similar .",
    "there are other different optimizations possible , such as common subexpression elimination , loop unrolling or induction variable simplification . in order to preserve the portability of the procedures for different processor architectures , we leave the application of these and other optimizations to the compiler",
    "instead , aiming particularly at optimization of the execution on graphics processors , we explicitly consider the placement of different arrays appearing in calculation in different levels of memory hierarchy .",
    "before we present the details , we briefly introduce the programming model and example hardware platforms for which we design the implementations .",
    "we want to compare the optimizations and performance of execution for numerical integration on several types of hardware  multi - core cpus , gpus , xeon phi  but do not want to consider a radically different programming models .",
    "we choose a programming model that offers the explicit usage of shared memory ( necessary for gpus ) , but that can be used also for architectures based on standard cpu cores ( we include in this category also xeon phi architecture ) .",
    "the model has to allow for efficient vectorization , as well as other standard optimizations .",
    "the model that we choose is based on models developed for gpus ( cuda @xcite , opencl @xcite ) , but tries to simplify them .",
    "the goal is to obtain the model simple enough , so that application programmers can use it , but having enough features to allow for high performance implementations on different contemporary hardware .",
    "our final implementations are all done in opencl due to the fact of existing software development tools for all processors considered .",
    "we assume that the code is written in the spmd ( single program multiple data ) fashion , for threads that execute only scalar operations .",
    "several threads can be grouped together , so that hardware can run them in lockstep , creating vectorised parts of the code .",
    "it is up to the compiler and the hardware to perform such vectorization .",
    "this vectorization is , in current programming environments , hidden from the programmer , but can not be neglected when analysing the performance of implementation .",
    "the vectorization is considered the standard form of execution for gpus and an important performance improvement for architectures with standard cpu cores .",
    "because of the lockstep execution , we will call a group of threads performing vectorised operations a simd group ( this term directly corresponds to the notions of nvidia _ warps _ and amd _ wavefronts _ ) .",
    "the number of threads in a simd group depends on scheduling and execution details of each processor architecture .",
    "for the contemporary gpus it is usually equal to 32 or 64 , while for the current cpu like cores it is related to the width of vector registers and can be equal to 4 , 8 or 16 .",
    "one or several simd groups form a set of threads , that is called a _",
    "threadblock _ in cuda and a _ workgroup _ in opencl .",
    "we include that notion in the programming model and use the opencl word _",
    "workgroup_. threads in a single workgroup are run on the same processor core ( by the core we understand a standard cpu core , an nvidia streaming multiprocessor , an opencl computing unit ) .",
    "they share access to explicitly available fast memory ( that we call further _ shared _ memory , adopting the cuda convention , as opposed to using the opencl notion of _ local _ memory ) and can be explicitly synchronised using the barrier construct .",
    "threads from different workgroups are not synchronised and form a space of threads , executing concurrently a part of the code constituting a ( computational ) kernel .",
    "we assume that the whole code , in our case a code performing finite element simulations , that contains complex and large data structures , is executed in the traditional way on standard cpus with only several kernels _ offloaded _ to ( massively ) multi - core accelerators .",
    "the act of offloading is not a necessary step in executing codes in our model . for future architectures , having standard memories accessible to massively multi - core processors with simd capabilities , it can be eliminated , with kernels denoting at that moment only carefully parallelized , vectorised and optimized parts of codes .",
    "a programmer in our model can explicitly specify a storage location for selected kernel variables , as global or shared memory . in that case ,",
    "specific , hardware dependent , considerations has to be performed to take into account , and possible optimize , different ways of accessing data in different levels of memory hierarchy .",
    "more specifically we assume that the accesses from different threads in the same simd group executing a single memory operation can be organized in such a way that they concern subsequent memory locations in memory .",
    "if two levels of memory are involved , the rule of subsequent accesses is considered more important for the slower memory .",
    "the above , simple , guideline should work well for gpus , where it should allow for forming fast _ coalesced _ memory accesses to global memory @xcite .",
    "when it can not be applied , more complex rules for performing coalesced global memory accesses and avoiding bank conflicts for shared memory should be applied . also for cpu cores",
    ", the guideline should allow for fast vectorised memory accesses .",
    "if it turns out that the compiler and the hardware can not perform such operations , more traditional approaches , exploiting spatial and temporal locality for each thread can be exploited .    the variables with no explicit storage location form a set of automatic variables , that are private to each thread .",
    "we assume that actual calculations are always performed using a set of registers available to a thread .",
    "if the set is large enough , all automatic variables can be stored in registers .",
    "this is the optimal situation , that can often happen for simple kernels executed on gpus .",
    "when this is not the case , register spilling occurs , the values are stored in different levels of cache memory and eventually can be transferred , with relatively high cost , to global memory .",
    "optimal implementation of an algorithm has to consider specific features of hardware platforms for which it is designed . in the paper",
    "we try to find , for the particular problem of finite element numerical integration , the most important dependencies between the processor architectures and the performance of codes .",
    "we consider three types of computing devices , popular in hpc and having a strong presence on the current top500 lists @xcite .",
    "we do not describe the platforms in details , but compare several characteristics , important , in our view , for execution of numerical integration procedures ( and definitely other scientific codes as well ) that are further exploited when optimizations of these procedures are analysed .",
    "the first platform is a standard contemporary multi - core processor , represented by intel xeon e5 - 2620 .",
    "it has 6 sandy bridge cores , each of which supports two - way multithreading .",
    "the second platform , an intel xeon phi 5110p accelerator card has 60 cores , derived from pentium architecture and supporting 4-way multithreading ( our opencl implementations can use 59 cores ) .",
    "the third platform is an nvidia tesla k20 m accelerator card with kepler ( gk110 ) gpu .",
    "& tesla & xeon & xeon + & k20 m & phi & e5 - 2620 + & ( gk110 ) & 5110p & ( @xmath512 ) + number of cores & 13 & 60 ( 59 ) & 2@xmath516 +   + number of simd lanes ( sp / dp ) & 192/64 & 16/8 & 8/4 + number of registers & 65536x32bit & 4x32x512bit & 2x16x256bit + shared memory ( sm ) size [ kb ] & 16 or 48 & 32 ( ? ) & 32 ( ? ) + l1 cache memory size [ kb ] & 64@xmath52sm size & 32 & 32 + l2 cache memory size [ kb ] & @xmath53118 & 512 & 256 +    table [ processors ] compares several important characteristics of the considered processors .",
    "the data are selected to be important for the performance oriented programmers and reflects purely hardware features and some details of programming environments . by the core , as it was already mentioned ,",
    "we denote a cpu core or a compute unit ( streaming multiprocessor ) for gpus . for certain computing environments ( as e.g. opencl that we use in numerical tests )",
    "the fact of supporting multithreading is reflected by multiplying the number of compute units ( intel opencl compiler reports the number of computing units for xeon phi equal to 236 , and for xeon e5 - 2620 equal to 12 ) . since hyperthreading does not necessarily lead to proportional performance improvement , we leave in the table the number of physical cores for the xeon processors .",
    "the notion of simd lanes , in the meaning that we use in the paper , reflects the ability of a hardware to perform floating point operations ( single or double precision ) , as it is revealed to programmers through the speed of execution of suitable low level instructions .",
    "the notion of simd lanes is directly related to the notion of maximal floating point performance , expressed in tflops , that can be computed by multiplying the maximal number of operations completed by a simd lane in a single cycle ( usually 2 for fma or mad operations ) by the number of simd lanes and the frequency of core operation in thz . for cpu cores ( in xeon and xeon phi processors )",
    "simd lanes form parts of vector units , for nvidia gpus simd lanes correspond to cuda cores and double precision floating point units .",
    "the number of registers , reveals the number reported in instruction sets and programming manuals .",
    "the actual performance of the codes depends on this number but also on the number of physical resources associated ( e.g. in standard cpu cores , the number of physical registers is larger than the number available in instruction sets due to the capabilities of out - of - order execution ) .",
    "the picture of fast memory resources for contemporary processors is complex .",
    "not only the details of hardware implementation ( the number of banks , bus channels , communication protocols ) , but also the configurations change from architecture to architecture . for gpus shared memory forms a separate hardware unit ( but in the case of nvidia gpus",
    "its size can be selected by the programmer ) . for standard cpus , even if the programming model includes the notion of shared memory , it can be neglected in the actual execution ( as is the case e.g. for intel opencl compiler that , reportedly , maps it to standard dram memory and uses standard for cpu cache hierarchy mechanisms @xcite  the reason for placing  ?  in table [ processors ] ) .",
    "the practical computing power of processors is related to the number of simd lanes but there may exist important differences between the different types of processor cores : each one can require a different number of concurrently executing threads ( per single simd lane ) to reach its maximal practical performance ( this fact concerns also the performance of memory transfers ) . for standard cpu cores ( as in the xeon processor ) , one thread per single lane can suffice to reach the near peak performance .",
    "this is related to the complex architecture of cores with out - of - order execution and hardware prefetching . for simpler xeon phi cores , 4 threads are necessary to reach the peak performance .",
    "even more threads are required for nvidia gpus : usually several threads are sufficient to hide arithmetic operation latencies , while several tens of threads may be needed ( in the case when the threads do not issue concurrent memory requests ) to allow for hiding the latencies of dram memory operations @xcite .",
    "these facts have important consequences for designing high performance codes for the processors .",
    "the speed of execution strongly depends on the speed of providing data from different levels of memory hierarchy ( registers , caches , dram ) .",
    "it is better to use resources that are faster , but their amount is limited for each core .",
    "table [ memory_resources ] presents the resources available to a single thread when the number of threads per simd lane is equal 1 for standard sandy bridge core in the xeon cpu and 4 for xeon phi and kepler gpu .",
    "the number of threads per simd lane is selected to some extent arbitrarily , but can be understood as the minimal number of threads when high performance of execution is sought .    it can be seen that the processors exhibit different characteristics .",
    "standard xeon cores ( sandy bridge in this example ) have relatively large caches with less registers .",
    "xeon phi cores , that are based on previous intel cpu designs , still have large caches ( although smaller than for sandy bridge ) , but , according to avx standard , possess twice as much registers .",
    "kepler streaming multiprocessors offer relatively large number of registers with relatively small fast memory ( especially l2 ) .",
    "if software requirements , for register per thread or shared memory per workgroup of threads , are large , then the gpu , either reduces the number of concurrently executed threads ( that usually decreases the performance ) or even can not execute the code at all .",
    "all considered processors exhibit complex behaviour with respect to using the memory hierarchy .",
    "first generations of gpus had not caches and , in order to get maximum performance , the programmers had to ensure that the number of requested registers did not exceed the hardware limits and that the dram memory accesses strictly adhered to the rules of coalescing , specific to different generations of gpu architectures .",
    "current architectures , due to the use of caches , may not result in excessive penalties for not obeying the two mentioned above principles .",
    "this makes the picture of gpu performance less clear : sometimes it is better to allow for register spilling ( when due to the too large number of requested registers the data has to be stored in cache ) , if this decreases the requirements for shared memory , allowing more threads to run concurrently .",
    ".memory resources per single thread , in terms of the number of double precision data that can be stored , for the processors used in the study ( 1 thread per simd lane for xeon cpu and 4 threads per simd lane for xeon phi and kepler gpu assumed  explanation in text ) .",
    "[ cols=\"<,^,^,^ \" , ]     the situation is much more complex for shared memory accesses and floating point operations .",
    "first , for different variants of kernels there will be different organization of operations .",
    "moreover , for different organizations compilers can perform different sets of optimizations .",
    "we present the estimated numbers for shared memory accesses and floating point operations in table [ accesses_and_operations ] . in deriving the estimates we tried to take into account the possible optimizations , related e.g. to the facts that certain values are constant during calculations ( e.g. derivatives of shape functions for different integration points within the tetrahedral element , pde coefficients for different integration points in the convection - diffusion case ) and that the resulting stiffness matrices may be symmetric .",
    "calculations in all versions of numerical integration algorithm contain several phases , such as :    * computing real derivatives of geometrical shape functions ( 9 operations for tetrahedral elements , 126 operations per integration point for prisms ) * computing jacobian terms ( 49 operations in all cases , but performed once for tetrahedra and repeated for each integration point for prisms ) * computing real derivatives for shape functions ( 60 operations for tetrahedra and 90 operations per integration point for prismatic elements ) * calculating the entries for the stiffness matrix and the load vector    the final calculations are performed in a triple loop over integration points and shape functions , but in many cases the loops are fully unrolled by the compiler and the resulting numbers of operations are the same for all variants of numerical integration .",
    "this happens for tetrahedral elements due to many calculations moved outside all loops ( since derivatives of shape functions are constant over the element ) . for prismatic elements the fact that several calculations must be repeated for each integration point results in much larger number of operations for the sqs and especially ssq versions of the algorithm , as compared to the qss version .",
    "based on the estimated numbers of global memory accesses and floating point operations we calculate arithmetic intensities ( defined as the number of operations performed per global memory access ) for the different variants of integration kernels and different cases considered .",
    "the calculated values , reported in table [ accesses_and_operations ] , can be compared with performance characteristics of the processors used in the study .",
    "we present such characteristics in table [ processors_performance ] . for each processor",
    "we report its peak performance and benchmark performance for the two types of operations  memory accesses and floating point operations ( as benchmarks we consider stream for the memory bandwidth and dense matrix - matrix multiplication ( dgemm ) for floating point performance ; moreover from now on we restrict our analysis to only double precision calculations , as more versatile than single precision ) .",
    "based on these data and assuming the simple roofline model of processor performance @xcite , the limiting arithmetic intensity , i.e. the value of arithmetic intensity that separates the cases of the maximal performance limited by memory bandwidth from that limited by the speed of performing arithmetic operations ( pipeline throughput ) , is calculated for each processor and theoretical and benchmark performance .    comparing the data in table [ accesses_and_operations ] and table [ processors_performance ]",
    "we see that the performance when running different variants of numerical integration kernels can be either memory bandwidth or processor performance ( pipeline throughput ) limited . for tetrahedral elements",
    "the performance for both example problems is memory limited for all architectures ( excluding the not optimal ssq version ) . for prismatic elements , even when benchmark ( i.e. higher ) limiting arithmetic intensity is considered , the performance of kernel execution should be pipeline limited for xeon and xeon phi architectures . for the kepler architecture and the optimal qss variant",
    "the performance will be memory limited for poisson problem , while is on the border between being memory or pipeline limited for the convection - diffusion example .",
    "for other variants the performance is pipeline limited but the large number of operations in these cases makes them far from optimal .",
    "based on the data in tables [ accesses_and_operations ] and [ processors_performance ] we perform final estimates of execution time for different variants of numerical integration and different architectures . we calculate separately the estimated time determined by the number of global memory accesses and the benchmark ( stream ) memory bandwidth of processors and the time determined by the number of floating point operations and the arithmetic throughput in dgemm .",
    "table [ execution_times ] contains the longer times from the two , computed as the estimate for each architecture .",
    "the values in table [ execution_times ] form lower bounds for the times possible to achieve .",
    "there are many factors that can slow down the execution of algorithms .",
    "the most important include :    * too low occupancy for gpus ( too small number of concurrently executing threads in order to effectively hide instruction or memory latencies )  caused by excessive requirements for registers or shared memory * shared and cache memory accesses for gpus , not considered in table [ execution_times ] and caused by either explicit use of shared memory or register spilling to caches ( or even to global memory ) * using other than fma or mad instructions ",
    "peak performance is obtained by all the considered processors for algorithms using almost exclusively fma or mad operations that double the performance as compared to using other floating point instructions * employing barriers to ensure the proper usage of shared memory     & & + & + & & & & +   + qss algorithm & 2.00 & 3.67 & 2.89 & 4.44 + sqs algorithm & 2.00 & 9.47 & 2.89 & 11.36 + ssq algorithm & 2.00 & 49.89 & 2.89 & 59.30 +   + qss algorithm & 1.68 & 3.21 & 2.43 & 5.72 + sqs algorithm & 1.68 & 12.40 & 2.43 & 14.87 + ssq algorithm & 1.68 & 57.87 & 2.43 & 69.40 +   + qss algorithm & 4.30 & 15.00 & 6.21 & 26.70 + sqs algorithm & 4.30 & 57.87 & 6.21 & 69.40 + ssq algorithm & 4.30 & 304.87 & 9.02 & 362.40 +      the last issue that we deal with , is the one that is often considered first  the time required for transfer of data between host memory and accelerator memory ( when these two memories are separate , i.e. when accelerator is connected with the cpu using pcie bus , as is the case for tesla k20 m and xeon phi ) .",
    "the reason why we consider it last is that , based on the discussions performed so far , it is evident that if numerical integration is considered alone , without considering other procedures of finite element processing , the time required for transferring input and output data ( or even input data alone ) strongly dominates the time for performing actual computations .",
    "this becomes obvious after realizing that the performance of numerical integration kernels is usually memory bound or close to being memory bound , that the amount of data in global memory accessed by accelerator during calculations is the same as that of host - accelerator memory transfer and that the bandwidth of pcie is several times lower than that of either gpu or xeon phi global memory .",
    "because of that we do not discuss this issue here . apart from the fact that such a discussion should include broader context of finite element simulations , there is another reason for not going into details .",
    "the solution with pcie cpu - accelerator connection is universally considered as one of the most important performance bottlenecks for accelerator computing .",
    "hardware providers are considering many different options that should change this situation .",
    "when these solutions become available , the detailed analyses of the influence of host memory - accelerator memory transfer on performance of codes can be performed ( for some initial experience see e.g. @xcite ) .",
    "we test the three versions ( qss , sqs , ssq ) of numerical integration kernels for the four described example cases ( poisson and convection - difffusion for tetrahedral and prismatic elements ) and the three selected processor architectures intel xeon e5 - 2620 , intel xeon phi 5110p and nvidia tesla k20 m with kepler gk110 architecture . for all processors we use 64-bit linux with 2.6.32 kernel . for xeon processors we use intel sdk for opencl applications version 4.5 ( with opencl 1.2 support ) .",
    "for nvidia gpu we use cuda sdk version 5.5 with opencl 1.1 support and 331.20 driver .",
    "we use the same kernels for all three architectures , changing only the size of workgroups : 8 for xeon , 16 for xeon phi and 64 for kepler .    for each particular kernel and problem",
    "we perform a series of experiments with different combinations of parameters determining the use of shared memory in computations . in table",
    "[ results_time ] we present the best times for each kernel , indicating additionally what percentage of the best performance ( associated with the times presented in table [ execution_times ] ) was achieved . figures [ figure_time ] , [ figure_abs_perf ] and [ figure_rel_perf ] illustrates the comparison of performance for each case of problem and approximation and different processors .",
    "& & + & + & & & & +   + qss algorithm & 2.02 ( 99% ) & 12.80 ( 29% ) & 4.46 ( 65% ) & 15.58 ( 29% ) + sqs algorithm & 2.02 ( 99% ) & 38.39 ( 25% ) & 6.05 ( 48% ) & 64.81 ( 18% ) + ssq algorithm & 2.02 ( 99% ) & 94.99 ( 53% ) & 6.13 ( 47% ) & 194.93 ( 30% ) +   + qss algorithm & 8.77 ( 19% ) & 25.09 ( 13% ) & 17.11 ( 14% ) & 33.29 ( 17% ) + sqs algorithm & 11.35 ( 15% ) & 41.20 ( 30% ) & 15.97 ( 15% ) & 60.18 ( 25% ) + ssq algorithm & 11.43 ( 15% ) & 101.20 ( 65% ) & 16.53 ( 15% ) & 132.46 ( 59% ) +   + qss algorithm & 19.31 ( 22% ) & 49.74 ( 30% ) & 26.74 ( 23% ) & 64.27 ( 42% ) + sqs algorithm & 18.77 ( 23% ) & 128.07 ( 45% ) & 24.86 ( 25% ) & 133.14 ( 52% ) + ssq algorithm & 17.76 ( 24% ) & 385.33 ( 79% ) & 26.11 ( 35% ) & 475.41 ( 76% ) +            the detailed results of experiments lead to several remarks belonging to different domains of interest . for practitioners performing finite element simulations on modern hardware",
    "the interesting facts include :    * the execution time of finite element integration depends strongly not only on the order of approximation ( this fact is obvious from the classical complexity analysis ) , but also on the type of element and the kind of problem solved .",
    "the differences are not so significant as in the case of different approximation orders @xcite , but still can reach an order of magnitude * due to large differences in execution time the problem of proper mapping to hardware and optimization of numerical integration may be more or less important for the time of the whole finite element simulation , depending also e.g. on the time required by the solver of linear equations ( if present ) @xcite )    for finite element software developers , especially in the hpc domain , there are the following general observations :    * large resource requirements for some variants of numerical integration ( e.g. convection - diffusion problems and prismatic elements ) , limits the performance achieved by the gpu architecture in our comparison ; the reason lies mainly in too small number of threads ( simd groups ) that can run concurrently on a single core ( streaming multiprocessor ) * the qss variants of the numerical integration algorithm turned out to be the most versatile and efficient ; due to the proper optimizations in the source code and performed by the compilers , the performance of qss kernels was either the fastest or slower by only several percent than the fastest version * detailed results of experiments showed that the performance on our gpu ( as is also the case for other contemporary gpu architectures ) is sensitive to the way the data are read and written to memory , coalescing memory accesses turned out to be more important for the kepler processor than for both xeons * when the performance of algorithm is memory bound and there is a significant influence of the way the data are accessed in global memory ( as is the case e.g. for kepler architecture , tetrahedral element and poisson problem ) , there is a growing importance of data layout in memory ( and the use of output data by other components of finite element codes ) @xcite * the gpu architecture turned out to be also more sensitive to the use of opencl shared memory ( as suggested by the intel opencl compiler manual @xcite , this had no significant impact on the performance of xeons ) * together , the changes in the way data are read and written to the global memory and the changes in shared memory usage caused the differences in execution time of the order 2 - 3 for xeons , and more than 5 for kepler    in general , for some test cases , the performance of obtained portable numerical integration kernels can be considered satisfactory .",
    "still , for each architecture and most of the cases considered , the benchmark results suggest that it should be possible to achieve execution times several times shorter ( from 3 times for kepler architecture , up to almost 8 times for xeon phi ) .",
    "this could however usually be obtained by either considering the specialized domain specific optimizing compilers or by loosing the portable character of the kernels and introducing architecture specific optimizations into the source code .",
    "another possibility is to consider different parallelization strategies , the option that we plan to follow in subsequent publications .    finally there are some observations concerning the hardware itself ( these observations are common to many algorithms , but are confirmed for our example case of finite element numerical integration ) :    * our example gpu , tesla k20 m , turned out to be the fastest processor in the comparison ; the observations made by several authors @xcite show that xeon phi processors applied to different scientific computing algorithms , although usually faster than standard xeon cpus , are usually slower than the recent hpc targeted gpus * the detailed results of experiments for particular optimization variants of kernels showed significant resemblance of xeon and xeon phi performance , indicating the similarities between these architectures .",
    "the subject of numerical integration on modern multi - core processors , especially graphics processors , has been investigated in several contexts .",
    "the first works concerned gpu implementations for specific types of problems , such as higher order fem approximations in earthquake modelling and wave propagation problems @xcite , gpu implementations of some variants of discontinuous galerkin approximation @xcite or higher order approximations for electromagnetics problems @xcite .",
    "the second important context for which finite element numerical integration was considered , is the creation of domain specific languages and compilers .",
    "interesting works for this approach include @xcite .",
    "the research in the current paper can be understood as laying theoretical and experimental grounds for possible future finite element compilers , able to create optimized code for all types of problems , elements and approximations , as well as different processor architectures .    finally , numerical integration , in more or less details ,",
    "have been discussed in the context of the whole simulation procedure by the finite element codes .",
    "some works in this area include @xcite . usually more attention to numerical integration has been given in articles that consider the process of creation of systems of linear equations for finite element approximations , with the important examples such as @xcite .",
    "the present paper forms a continuation of our earlier works , devoted solely to the subject of finite element numerical integration .",
    "the first papers considered higher order approximations , starting with first theoretical and experimental investigations in @xcite and culminating in larger articles devoted to the implementation and performance of numerical integration kernels for multi - core processors with vector capabilities @xcite ( especially ibm power xcell processor ) and gpus @xcite .",
    "the investigations in the current paper on one hand touch the narrower subject of only first order approximations , but on the other hand concern two important types of finite elements ( geometrically linear and non - linear ) and are performed , in a unifying way , for three different processor architectures used in scientific computing .",
    "this scope , combined with the depth of investigations concerning the performance of opencl kernels , differentiate the article from the other on this subject .",
    "we have investigated the optimization and performance on current multi- and many - core processors for an important computational kernel in scientific computing , the finite element numerical integration algorithm for first order approximation .",
    "we have used a unifying approach of opencl for programming model and implementation on three popular architectures in scientific computing : intel xeon ( sandy bridge ) , intel xeon phi and nvidia kepler .",
    "we have demonstrated that this approach allows for exploiting multi - core and vector capabilities of processors and for obtaining satisfactory levels of performance , as compared to the theoretical and benchmark maxima , not loosing the portability of the developed code ( we have used the same opencl kernels for all architectures ) .",
    "the investigations in the paper reveal that , even for the simple problems , elements and approximations as considered in the paper , there is a significant variation of required resources and associated complexities for the algorithm , that can lead to different problems when mapping to architectures of modern processors .",
    "nevertheless , the results of computational experiments show that for all the cases considered in the paper , the numerical integration algorithm can be successfully ported to massively multi - core architectures , and hence , when used in finite element codes should not form a performance bottleneck .",
    "the presented detailed analyses indicate what conditions must be met in order to obtain the best performance of the kernels and what performance can be expected when numerical integration is used on different processors for different types of problems and finite element approximations .        , _ a modular design for parallel adaptive finite element computational kernels _ , in computational science  iccs 2004 , 4th international conference , krakw , poland , june 2004 , proceedings , part ii , m.  bubak , g.d .",
    "van albada , p.m.a .",
    "sloot , and j.j .",
    "dongarra , eds .",
    "3037 of lecture notes in computer science , springer , 2004 , pp .  155162 .    , _ scalability analysis for a multigrid linear equations solver _ , in parallel processing and applied mathematics , proceedings of viith international conference , ppam 2007 , gdansk , poland , 2007 , r.  wyrzykowski , j.  dongarra , k.  karczewski , and j.  wasniewski , eds .",
    "4967 of lecture notes in computer science , springer , 2008 , pp .",
    "12651274 .      , _ a parallel node - based solution scheme for implicit finite element method using \\{gpu } _ , procedia engineering , 61 ( 2013 ) , pp .  318  324 .",
    "25th international conference on parallel computational fluid dynamics .            , _ automatically optimized gpu acceleration of element subroutines in finite element method _ , in application accelerators in high performance computing ( saahpc ) , 2012 symposium on , july 2012 , pp .  141144 .",
    ", _ towards a complete fem - based simulation toolkit on gpus : unstructured grid finite element geometric multigrid solvers with strong smoothers based on sparse approximate inverses _ , computers & fluids , 80 ( 2013 ) , pp",
    ".  327  332 . selected contributions of the 23rd international conference on parallel fluid dynamics parcfd2011 .      , _ a novel finite element method assembler for co - processors and accelerators _ , in proceedings of the 3rd workshop on irregular applications : architectures and algorithms , new york , ny , usa , 2013 , acm , pp .",
    "1:11:8 .",
    ", _ finite element numerical integration on powerxcell processors _ , in ppam09 : proceedings of the 8th international conference on parallel processing and applied mathematics , berlin , heidelberg , 2010 , springer - verlag , pp .  517524 .                  , _ opencl - based implementation of an unstructured edge - based finite element convection - diffusion solver on graphics hardware _ , international journal for numerical methods in engineering , 89 ( 2012 ) , pp .",
    "16351651 .",
    ", _ finite element numerical integration on gpus _ , in ppam09 : proceedings of the 8th international conference on parallel processing and applied mathematics , berlin , heidelberg , 2010 , springer - verlag , pp .",
    "411420 .",
    ", _ gpu acceleration of data assembly in finite element methods and its energy implications _ , in application - specific systems , architectures and processors ( asap ) , 2013 ieee 24th international conference on , june 2013 , pp .",
    "321328 .      ,",
    "_ accelerators for technical computing : is it worth the pain ? a tco perspective _ , in supercomputing , julianmartin kunkel , thomas ludwig , and hanswerner meuer , eds .",
    "7905 of lecture notes in computer science , springer berlin heidelberg , 2013 , pp .  330342 ."
  ],
  "abstract_text": [
    "<S> the paper presents investigations on the implementation and performance of the finite element numerical integration algorithm for first order approximations and three processor architectures , popular in scientific computing , classical cpu , intel xeon phi and nvidia kepler gpu . a unifying programming model and portable opencl implementation is considered for all architectures . variations of the algorithm due to different problems solved and different element types are investigated and several optimizations aimed at proper optimization and mapping of the algorithm to computer architectures are demonstrated . </S>",
    "<S> performance models of execution are developed for different processors and tested in practical experiments . </S>",
    "<S> the results show the varying levels of performance for different architectures , but indicate that the algorithm can be effectively ported to all of them . </S>",
    "<S> the general conclusion is that the finite element numerical integration can achieve sufficient performance on different multi- and many - core architectures and should not become a performance bottleneck for finite element simulation codes . </S>"
  ]
}