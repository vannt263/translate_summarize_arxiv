{
  "article_text": [
    "graphics processing units ( gpus ) have been developed originally as co - processors meant to fast deal with graphics tasks . in recent years",
    "the video game market developments compelled gpus manufacturers to increase the floating point calculation performance of their products , by far exceeding the performance of standard cpus in floating point calculations .",
    "the architecture evolved toward programmable many - core chips that are designed to process in parallel massive amounts of data .",
    "these developments suggested the possibility of using gpus in the field of high - performance computing ( hpc ) as low - cost substitutes of more traditional cpu - based architectures : nowadays such possibility is being fully exploited and gpus represent an ongoing breakthrough for many computationally demanding scientific fields , providing consistent computing resources at relatively low cost , also in terms of power consumption ( watts / flops ) .",
    "due to their many - core architectures , with fast access to the on - board memory , gpus are ideally suited for numerical tasks allowing for data parallelism , for simd ( single instruction multiple data ) parallelization .",
    "the numerical simulation , by monte carlo algorithms , of the path integral formulation of quantum field theories , discretized on a euclidean space - time lattice , is a typical such task : one has to sample the distribution for a system with many degrees of freedom and mostly local interactions .",
    "quantum chromodynamics ( qcd ) , the quantum gauge theory describing strong interactions , is a typical example where numerical simulations represent the best tool to investigate systematically specific features of the theory and give an answer to many important unsolved questions , regarding e.g. color confinement , deconfinement and the values of hadron masses .",
    "lattice qcd and its computational needs has represented a challenge in the field of hpc since more than 30 years , being often the stimulus for new machine developments ( think e.g. of the series of ape machines  @xcite ) .",
    "the introduction of gpus in lattice qcd calculations started with the seminal work of ref .",
    "@xcite , in which the native graphics apis were used , but the real explosion of interest in the field followed the introduction of nvidia s cuda ( compute unified device architecture ) platform , that effectively disclosed the field of gpgpu ( general purpose gpu @xcite ) , providing a more friendly programming environment .",
    "gpus have maintained their role of co - processors in most numerical applications , where they are used as accelerators for some specific , time demanding purposes . in the same spirit ,",
    "most of previous studies on the application of gpus to lattice qcd calculations were mainly aimed at using them together with the standard architectures in order to speed up some specific steps , typically the expensive dirac matrix inversion .",
    "our intent is instead to use gpus in substitution of the usual architectures , actually performing the whole simulation by them : one still needs a cpu to run the main program , but mostly in the role of a mere controller of the gpu instruction flow .",
    "to achieve this result we found simpler to write a complete program from scratch instead of using existing software packages , in order to have a better control of all the steps to be performed and ultimately transferred to the gpu .",
    "our implementation uses nvidia s cuda platform together with a standard c++ serial control program running on cpu .",
    "the specific case considered in the present study regards qcd on a hypercubic lattice with quark fields discretized in the standard staggered formulation .",
    "the paper is organized as follows . in section  [ simalg ]",
    "we give more details about the lattice discretization of qcd considered in our study and the simulation algorithm adopted . in section  [ gpufeatures ] we review some of the fundamental features of gpu architectures . in section  [ implem ]",
    "we describe our implementation of the algorithm on gpus and discuss the achieved performances .",
    "finally , in sections  [ opencl ] and  [ parallel ] we discuss some preliminary comparisons with performances obtained with opencl and multigpu implementations of our code .",
    "a preliminary report about our implementation has been presented in ref .",
    "qcd is a quantum field theory based on the symmetry under local non - abelian gauge ( color ) transformations belonging to the group of special unitary @xmath0 complex matrices , @xmath1 .",
    "it describes six different flavors of spin 1/2 colored particles ( quarks ) , which transform in the fundamental ( triplet ) representation of @xmath1 and interact through the gauge field , which lives in the algebra of the color gauge group and describes 8 colored , spin 1 particles known as gluons .",
    "an elegant , gauge invariant lattice discretization of the theory is given in terms of gauge link variables @xmath2 , where @xmath3 indicates a lattice site and @xmath4 is one of the four euclidean space - time directions @xcite .",
    "they are the elementary parallel transporters belonging to the gauge group and associated to each elementary link connecting two neighboring sites of the lattice .",
    "hence in total we have @xmath5 @xmath1 matrices , where @xmath6 is the number of lattice sites along direction @xmath4 , which in present simulations is typically not larger than @xmath7 .",
    "quark fields @xmath8 instead live on lattice sites and carry a color index , hence they are complex color triplets , one for each flavor and dirac index .",
    "the discretized euclidean feynman path integral , giving e.g. a representation of the thermal partition function , is written as @xmath9-\\bar{\\psi}m[u]\\psi}\\end{aligned}\\ ] ] where @xmath10 is the pure gauge part of the action , describing gluon - gluon interactions and written in terms of traces of products of link variables over closed loops , while @xmath11\\psi$ ] is a bilinear form in the fermionic variables , which describes quark - gluon interactions , with @xmath12 $ ] a large sparse matrix written in terms of the gauge link variables .    the functional integration in eq .",
    "( [ pathint ] ) is over all link variables ( gauge group invariant integration for each link ) and all quark fields .",
    "actually , due to their fermionic nature , the quark fields appearing in the path integral are grassmann anticommuting variables ; the best way we know to numerically deal with them is to integrate them explicitly .",
    "that results in the appearance of the determinant of the fermion matrix @xmath12 $ ] : @xmath13-\\bar{\\psi}m[u]\\psi } \\propto \\int \\mathscr{d}u \\det(m[u ] ) e^{-s_g[u ] } \\ ; .\\end{aligned}\\ ] ] notice that , in general , a fermion determinant appears for each quark species and that the determinant term becomes a trivial constant when all quarks have infinite mass and decouple ( pure gauge or quenched limit ) .",
    "one can show that , apart from specific difficult cases ( e.g. qcd at finite baryon density ) , the integrand in eq .",
    "( [ determinant ] ) is a positive quantity , admitting a probabilistic interpretation , so that one can approach the numerical computation of the path integral by importance sampling methods , which are typically based on dynamic monte carlo algorithms .",
    "the most difficult , time consuming part in such algorithms consists in taking properly into account the fermion determinant @xmath14)$ ] .",
    "the best available method is to introduce dummy bosonic complex fields @xmath15 , which come in the same number as the original fermionic variables and are known as pseudo - fermions  @xcite : @xmath16 ) \\right)^2 e^{-s_g[u ] } \\propto \\int \\mathscr{d}u \\mathscr{d}\\phi \\",
    ", \\exp\\big(-s_g[u]-\\phi^ * \\big(m[u]^{\\dag}m[u]\\big)^{-1}\\phi\\big)\\end{aligned}\\ ] ] where we have explicitly considered the case of two identical quark species , as in the case of two light flavors ( @xmath17 and @xmath18 quarks ) with all other flavors decoupled .",
    "the particular lattice discretization implemented in the present study considers a simple plaquette action for the pure gauge term , i.e. products of four gauge link variables around the elementary closed square loops ( plaquettes ) of the lattice , and a standard staggered discretization for the fermionic term .",
    "that means that fermion fields living on lattice sites have only color indexes ( dirac indexes can be reconstructed afterwards combining fields living on different lattice sites ) , while the fermionic matrix reads as follows : @xmath19 = a m \\delta_{n_1,n_2 }   + { 1 \\over 2 } \\sum_{\\nu=1}^{4}\\eta_\\nu ( n_1 ) \\left (   u_\\nu ( n_1 )   \\delta_{n_1,n_2-\\hat\\nu } -   u^{\\dag}_\\nu ( n_1-\\hat\\nu ) \\delta_{n_1,n_2+\\hat\\nu }   \\right ) \\:,\\end{aligned}\\ ] ] where @xmath20 and @xmath21 are 4-vectors with integer components labeling lattice sites , @xmath22 is an elementary lattice versor , @xmath23 are known as staggered fermion phases and @xmath24 is the lattice spacing .",
    "color indexes are implicit in eq .",
    "( [ stag ] ) ( the identity in color space is understood for the mass term proportional to @xmath25 ) .",
    "the staggered discretization differs from other ( e.g. wilson ) fermion discretizations for the absence of the additional dirac index : that implies lighter algebra computations which have an effect both on the overall computational complexity and on the maximal performances , as we shall explain in details later on .    a particular feature of the staggered fermion matrix in eq .",
    "( [ stag ] ) is that it describes four flavors . when simulating a different number of flavors one has to use a trick known as rooting .",
    "@xmath26 flavors of equal mass are described by the following partition function @xmath27 ) \\right)^{n_f/4 } e^{-s_g[u ] } \\ , .\\end{aligned}\\ ] ]      a convenient algorithm to simulate the action in is the hybrid monte carlo  @xcite ( hmc ) .",
    "the idea is very simple and it is conveniently exposed by using as an example the case of a single variable with action @xmath28 , i.e. distributed proportionally to @xmath29 . as a first step a dummy variable @xmath30 , corresponding to a momentum conjugate to @xmath31 , is introduced using the following identity : @xmath32 it is trivial that expectation values over @xmath31 are untouched by the introduction of @xmath30 , which is a stochastically independent variable .",
    "the basic idea of the hmc algorithm is to sample the distribution in @xmath30 and @xmath31 by first extracting a value of @xmath30 according to its gaussian distribution and then performing a joint molecular dynamics evolution of @xmath30 and @xmath31 which keeps the total `` energy '' @xmath33 unchanged , obtaining an updated value of @xmath31 as a final result . going into more details , the hmc proceeds as follows :    1 .",
    "a random initial momentum is generated with probability @xmath34 ; 2 .",
    "starting from the state @xmath35 , a new trial state @xmath36 is generated by numerically solving in the fictitious time @xmath37 the equations of motion derived from the action @xmath38 , @xmath39 such equations are integrated numerically using a finite time step @xmath40 . as a consequence",
    "the energy is conserved only up to some power of @xmath40 , depending on the integration scheme adopted ; 3 .",
    "the new state @xmath36 is accepted with probability @xmath41 where @xmath42 ( metropolis step ) .",
    "it can be shown ( see @xcite ) that the sequence of the @xmath31 configurations obtained in this way is distributed with the correct @xmath43 probability provided the solution of the equation of motion satisfies the requirements    * the evolution is reversible , @xmath44 * the evolution preserves the measure of the phase space , @xmath45    a large class of integrators that satisfy these two constraints are the so - called symmetric symplectic integrators , the simplest member of this class being the leap - frog or @xmath46 scheme ( for improved schemes see @xcite ) .    in the particular case of the action in , the momenta are conjugate to the gauge link variables : they are therefore @xmath0 complex matrices @xmath47 ( one for each gauge link ) living in the algebra of the gauge group , i.e. they are traceless hermitian matrices writable as @xmath48 where @xmath49 are the group generators , and the action associated with momenta is simply given by @xmath50 . a convenient implementation of the picture above",
    "is then given by the so - called @xmath51 algorithm of ref .",
    "@xcite :    1 .",
    "a vector @xmath52 of complex gaussian random numbers is generated and the pseudofermion field is initialized by @xmath53^{\\dag } r$ ] , in such a way that the probability distribution for @xmath15 is proportional to @xmath54 ; 2 .",
    "the momenta are initialized by gaussian random matrices ( i.e. each @xmath55 is extracted as a normally distributed variable ) ; 3 .",
    "the gauge field and momenta are updated by using the equations of motion ; 4 .",
    "the final value of the action is computed and the metropolis step performed .",
    "point 3 is the more time consuming , since the calculation of the force requires at each step to solve the sparse linear system @xmath56^{\\dag}m[u]\\big)x=\\phi\\ ] ] which is usually performed by means of krylov methods ( see @xcite ) . for staggered fermions",
    "a complication is the presence of the @xmath57th root of the determinant in the action : becomes @xmath58^{\\dag}m[u]\\big)^{n_f/4}x=\\phi\\ ] ] where @xmath26 is the number of degenerate flavors . in order to overcome this problem the rational hybrid monte carlo ( rhmc )",
    "was introduced in @xcite , in which the root of the fermion matrix is approximated by a rational function , which is then efficiently computed by means of the shifted versions of the krylov solvers ( see @xcite ) .    in order to speed - up the simulations , the following common tricks were implemented    * even / odd preconditioning @xcite * multi - step integrator , with action divided in gauge and fermion part @xcite * improved integrator , second order minimum norm , see @xcite * multiple pseudo - fermions to reduce the fermion force magnitude and increase integration step size @xcite * different rational approximations and stopping residuals for the metropolis and the molecular dynamic steps @xcite",
    "in this section we will review the main features of the gpus architecture which are to be taken into account in order to efficiently use their computational capabilities .",
    "modern gpus are massively parallel computing elements , composed of hundreds of cores grouped into multiprocessors . the typical architecture of a modern nvidia graphic card is outlined in and the most important point for the following is the presence of three different storage levels . roughly speaking",
    "the architecture of ati cards is similar .",
    "primary storage is provided by the device memory , which is accessible by all multiprocessors but has a relatively high latency . within the same multiprocessor ,",
    "cores have also access to local registers and to shared memory .",
    "shared memory is accessible by the threads of the same multiprocessor and its access is orders of magnitude faster than device memory one , being very close to the computing units ; however , while the total amount of device memory is of order of few gbs , the local storage is only 16 kb per multiprocessor both for the registers and for the shared memory , so that it is typically impossible to use just these local fast memories .    in order to hide the latency time of the device memory",
    "it is convenient to have a large number of threads in concurrent execution , so that when data are needed from device memory for some threads , the ones ready to execute are immediately sent to computation .",
    "the highest bandwidth from device memory is achieved when a group of 16 threads accesses a contiguous memory region ( coalesced memory access ) , because its execution requires just one instruction call , saving a lot of clock - cycles .",
    "this will be crucial in the following , when discussing the storage model for the gauge configuration .",
    "double precision capability was introduced with nvidia s gt200 generation , the first one specifically designed having in mind hpc market , and by now there is only a factor @xmath59 between the peak performance in single and double precision . in the specifications of the gpus used in this work",
    "are reported .",
    ".specifications of the nvidia cards used in this work .",
    "[ cols= \" < , < , < , < , < , < \" , ]     in general we have 3 different stages to synchronize the border : we build the buffer border from the field , we transfer it to the device , then we flush it into the field . in the particular case of parallelization along the t direction",
    "we can reduce these steps to only the transfer one .",
    "the build and flush stages add an overhead of about @xmath60 on big lattices ( comparable to a @xmath61 on the single gpu ) that reduces slowly when further increasing the lattice size . in order to hide the time needed for transfer , we try to overlap it with computation , and in particular with the shift update inside the inverter code .",
    "regarding performances ( see table  [ parallel_tab ] ) , on a @xmath62 we have an efficiency , compared to the single gpu case , of @xmath63 on two s2050 ( boost @xmath64 ) , of @xmath65 on two c1060 ( @xmath66 ) and of @xmath67 ( @xmath68 ) on four c1060 ; in the last case , increasing the lattice size , we can reach @xmath69 ( @xmath70 ) on a @xmath71 lattices .",
    "all tests have been performed by splitting along the y direction and the inefficiencies can be explained by the use of the two additional kernels needed to align the border before communication . on large lattices",
    "we obtain therefore a good scaling , comparable to what reported in ref .  @xcite , which is promising for the extension to the multinode implementation at which we are currently working . on smaller lattices , instead , the transfer time can not be hidden anymore and the boost decreases rapidly .",
    "we have a current new line of development to overcome the splitting problem , based on building the border according to the general topology of a given lattice operator , which will permit to compute the border part of an operator separately from the interior part , in order to overlap not only with the shift update but also with internal computations of the operator .",
    "such improvement may be particularly useful in the case of improved multi - link actions and operators and may also introduce a better memory access pattern .",
    "the extremely high computation capabilities of modern gpus make them attractive platforms for high - performance computations .",
    "previous studies on lattice qcd applications have been devoted almost exclusively to the dirac matrix inversion problem .",
    "we have shown that it is possible to use gpus to efficiently perform a complete simulation , without the need to rely on more traditional architectures : in this case the gpu is not just an accelerator , but the real computer .",
    "our strategy therefore has been that of bringing as much as possible of the computations on the gpu , leaving for the cpu only light or control tasks : in particular the whole molecular dynamics evolution of gauge fields and momenta , which is the most costly part of the hybrid monte carlo algorithm , runs completely on the gpu , thus reducing the costly cpu - gpu communications at the minimum .",
    "following such strategy , we have developed a single gpu code based on cuda and tested it on c1060 and c2050 architectures .",
    "we have been able to reach boost factors up to @xmath72 as compared to what reached by a twin traditional c++ code running on a single cpu core .",
    "our code is currently in use to study the properties of strong interactions at finite temperature and density and the nature of the deconfinement transition , in particular first production results have been reported in ref .",
    "@xcite .",
    "a point worth noting is that in our implementation we have to rely on the reliability of the gpu .",
    "if the gpu is used just for the dirac matrix inversion the result can then be directly checked on the cpu without introducing significant overhead in the computation .",
    "such a simple test can not be performed if the gpu is used to perform a complete md trajectory .",
    "for this reason it was mandatory to use gpus of the tesla series .    during the editorial processing of this paper",
    "it was signaled us that also the twqcd collaboration uses gpus to completely perform the hybrid monte carlo update of qcd with optimal domain wall fermions ( their first results were published in @xcite ) .    reported performances make surely gpus the preferred choice for medium size lattice groups who need enough computational power at a convenient cost , in this sense they already represent a breakthrough for the lattice community .",
    "our current lines of development regard the extension of our code to opencl and to multigpu architectures and we have reported preliminary results about that in section  [ further ] : that will open to possibility to use gpu clusters with fast connection links ( see for instance ref .",
    "@xcite ) in order to make the gpu technology available also for large scale simulations .",
    "it s a pleasure to thank a.  di  giacomo : without his encouragement and support much of the work reported here could not have been realized .",
    "test simulations have been run mostly on two gpu farms located in pisa and genoa and provided by infn .",
    "we thank massimo bernaschi , edmondo orlotti and the ape group in rome for the possibility of an early usage of fermi cards during the first stages of our test runs .",
    "we thank t .- w .",
    "chiu and m.  a.  clark for useful comments and one of the referees for his suggestion to improve the reversibility .",
    "m.  a.  clark , a.  d.  kennedy _ accelerating dynamical fermion computations using the rational hybrid monte carlo ( rhmc ) algorithm with multiple pseudofermion fields_. phys .",
    "lett . * 98 * , 051601 ( 2007 ) [ arxiv : hep - lat/0608015 ] .",
    "m.  a.  clark , r.  babich , k.  barros , r.  c.  brower , c.  rebbi _ solving lattice qcd systems of equations using mixed precision solvers on gpus_. comput .",
    "commun . * 181 * , 1517 ( 2010 ) [ arxiv:0911.3191 [ hep - lat ] ] .",
    "d.  gddeke , r.  strzodka , s.  turek _ performance and accuracy of hardware - oriented native- , emulated- and mixed - precision solvers in fem simulations_. international journal of parallel , emergent and distributed systems * 22 * , 221 ( 2007 ) .",
    "b.  joo , b.  pendleton , a.  d.  kennedy , a.  c.  irving , j.  c.  sexton , s.  m.  pickles , s.  p.  booth _ instability in the molecular dynamics step of hybrid monte carlo in dynamical fermion lattice qcd simulations_. phys .",
    "d * 62 * , 114501 ( 2000 ) [ arxiv : hep - lat/0005023 ] .",
    "chiu , t .- h .",
    "hsieh , y .- y .  mao ( for the twqcd collaboration ) _ topological susceptibility in two flavors lattice qcd with the optimal domain - wall fermion_. phys .",
    "b * 702 * , 131 ( 2011 ) [ arxiv:1105.4414 ] .",
    "r.  ammendola , a.  biagioni , o.  frezza , f.  l.  cicero , a.  lonardo , p.  s.  paolucci , d.  rossetti , a.  salamon , g.  salina , f.  simula , l.  tosoratto , p.  vicini _ apenet+ : high bandwidth 3d torus direct network for petaflops scale commodity clusters _ [ arxiv:1102.3796 ] ."
  ],
  "abstract_text": [
    "<S> we report on our implementation of the rhmc algorithm for the simulation of lattice qcd with two staggered flavors on graphics processing units , using the nvidia cuda programming language . </S>",
    "<S> the main feature of our code is that the gpu is not used just as an accelerator , but instead the whole molecular dynamics trajectory is performed on it . after pointing out the main bottlenecks and how to circumvent them , </S>",
    "<S> we discuss the obtained performances . </S>",
    "<S> we present some preliminary results regarding opencl and multigpu extensions of our code and discuss future perspectives .    lattice qcd , graphics processing units </S>"
  ]
}