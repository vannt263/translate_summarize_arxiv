{
  "article_text": [
    "in @xcite , csiszr considered a generalized notion of the divergence between two probability distributions , a.k.a .  the _ f  divergence _ , by replacing the negative logarithm function , of the classical divergence , @xmath3,\\ ] ] with a general convex function in @xcite , hence the name f  divergence . ]",
    "@xmath4 , i.e. , @xmath5 when the f  divergence was applied to the joint distribution ( in the role of @xmath6 ) and the product of marginals ( in the role of @xmath7 ) of two random variables , it yielded a generalized notion of mutual information , @xmath8 which was shown in @xcite to obey a data processing inequality , thus extending the well known data processing inequality of the ordinary mutual information ( see , e.g. , ( * ? ? ?",
    "* section 2.8 ) ) .",
    "the same ideas were introduced independently by ziv and zakai @xcite , with the primary motivation of using it to obtain sharper distortion bounds for classes of simple codes for joint source  channel coding ( e.g. , of block length @xmath9 ) , as well as certain situations of signal detection and estimation ( see also @xcite ) . the idea was to define both a `` rate  distortion function , '' @xmath10 and a `` channel capacity , '' @xmath11 , by minimization and maximization ( respectively ) of the mutual information pertaining to @xmath4 , and to derive a lower bound on the distortion @xmath12 from the data processing inequality @xmath13 in the sequel , this will be referred to as the 1973 version of the generalized data processing theorem . in a somewhat less well known work @xcite , zakai and ziv",
    "have substantially further generalized their data processing theorems , so as to apply an even more general information measures , and this will be referred to as the 1975 version .",
    "this generalized information measure was in the form @xmath14 where @xmath4 is now an arbitrary convex function of @xmath15 variables and @xmath16 are arbitrary positive measures ( not necessarily probability measures ) that are defined consistently with the markov conditions and where @xmath17 . it was shown in ( * ? ? ? * theorem 7.1 ) that the distortion bounds obtained from ( [ zzim75 ] ) are tight in the sense that there always exist a convex function @xmath4 and measures @xmath18 that would yield the exact distortion pertaining to the optimum communication system , and so , there is no room for improvement of this class of bounds .",
    "and @xmath18 depends on the optimum encoder and decoder . ]    by setting @xmath19 , @xmath20 , where @xmath21 are @xmath22 particular letters in the alphabet of @xmath23 , and @xmath24 , they defined yet another generalized information measure that satisfies the data processing theorem as @xmath25 where the expectation is taken w.r.t .",
    "the joint distribution @xmath26 in both @xcite and @xcite , there are many examples how these data processing inequalities can be used to improve on earlier distortion bounds .    the data processing theorems of csiszr and zakai and ziv form one aspect of this work . the other aspect , which may seem unrelated at first glance ( but will nevertheless be shown here to be strongly related ) is the second law of thermodynamics , or more precisely , _",
    "boltzmann s h  theorem .",
    "_ the second law of thermodynamics tells that in an isolated physical system ( i.e. , when no energy flows in or out ) , the entropy can not decrease over time . since one of the basic postulates of statistical physics",
    "tells that all states of the system , which have the same energy , also have the same probability in equilibrium , it follows that the stationary ( equilibrium ) distribution of these states must be uniform , because all accessible states must have the same energy when the system is isolated . indeed ,",
    "if the state of this system is designated by a markov process , @xmath1 with a uniform stationary state distribution , the boltzmann h",
    " theorem tells that the shannon entropy of @xmath27 , @xmath0 , can not decrease with @xmath2 , which is a restatement of the second law .",
    "we show , in this paper , that the generalized data processing theorems of @xcite , @xcite , and @xcite on the one hand , and the boltzmann h  theorem , on the other hand , are all special cases of a more general principle , which asserts that a certain generalized information measure , applied to the underlying markov process must be a monotonic function of time .",
    "this unified framework provides a new perspective on the generalized data processing theorem .",
    "beyond the fact that this new perspective may be interesting on its own right , it naturally suggests to exploit certain degrees of freedom of the ziv ",
    "zakai generalized mutual information that may lead to better bounds , for a given choice of the convex function that defines this generalized mutual information .",
    "these additional degrees of freedom may be important , because the variety of convex functions @xmath28 which are convenient to work with , is rather limited .",
    "the fact that better bounds may indeed be obtained is demonstrated by an example .    the outline of the remaining part of this paper is as follows . in section [ background",
    "] , we provide some background on markov processes with a slight physical flavor , which will include the notion of detailed balance , global balance , as well as known results like the boltzmann h  theorem , and its generalizations to information measures other than the entropy . in section [ main ] , we relate the generalized version of the boltzmann h  theorem and the generalized data processing theorems and formalize the uniform framework that supports both .",
    "this is done , first for the 1973 version @xcite of the ziv ",
    "zakai data processing theorem ( along with an example ) , and then for the 1975 version by zakai and ziv @xcite . finally , in section [ summary ]",
    ", we summarize and conclude .",
    "many dynamical models of a physical system describe the microscopic state ( or microstate , for short ) of this system as a markov process , @xmath1 , either in discrete time or in continuous time . in this section",
    ", we discuss a few properties of these processes as well as the evolution of information measures associated with them , like entropy , divergence and more .",
    "returning to the case where the process @xmath1 pertaining to our isolated system has not necessarily reached equilibrium , let us take a look at the entropy of the state @xmath64 the boltzmann h  theorem ( see , e.g. , ( * ? ? ?",
    "7 ) , ( * ? ? ?",
    "* sect.3.5 ) , @xcite @xcite ) asserts that @xmath0 is monotonically non  decreasing .",
    "this result is a restatement of the second law of thermodynamics , which tells that the entropy of an isolated system can not decrease with time . to see why this is true , we next show that detailed balance implies @xmath65 where for convenience",
    ", we denote @xmath66 by @xmath67 .",
    "now , @xmath68\\nonumber\\\\ & = -\\sum_x\\dot{p}_t(x)\\log p_t(x)\\nonumber\\\\ & = -\\sum_x\\sum_{x'}w_{x'x}[p_t(x')-p_t(x)]\\log p_t(x))\\nonumber\\\\ & = -\\frac{1}{2}\\sum_{x , x'}w_{x'x}[p_t(x')-p_t(x)]\\log p_t(x)-\\nonumber\\\\ & \\frac{1}{2}\\sum_{x , x'}w_{x'x}[p_t(x)-p_t(x')]\\log p_t(x')\\nonumber\\\\ & = \\frac{1}{2}\\sum_{x , x'}w_{x'x}[p_t(x')-p_t(x)]\\cdot[\\log p_t(x')-\\log p_t(x)]\\nonumber\\\\ & \\ge 0,\\end{aligned}\\ ] ] where in the second line we used the fact that @xmath69 , in the third line we used detailed balance ( @xmath52 ) , and the last inequality is due to the increasing monotonicity of the logarithmic function : the product @xmath70\\cdot[\\log p_t(x')-\\log p_t(x)]$ ] can not be negative for any pair @xmath71 , as the two factors of this product are either both negative , both zero , or both positive .",
    "thus , @xmath0 can not decrease with time .",
    "theorem has a discrete  time analogue : if a finite  state markov process has a symmetric transition probability matrix ( which is the discrete  time counterpart of the above detailed balance property ) , which means that the stationary state distribution is uniform , then @xmath0 is a monotonically non  decreasing sequence .    a well  known paradox , in this context ,",
    "is associated with the notion of the _ arrow of time .",
    "_ on the one hand , we are talking about time  reversible processes , obeying detailed balance , but on the other hand , the increase of entropy suggests that there is asymmetry between the two possible directions that the time axis can be exhausted , the forward direction and the backward direction .",
    "if we go back in time , the entropy would decrease .",
    "so is there an arrow of time ?",
    "this paradox was resolved , by boltzmann himself , once he made the clear distinction between equilibrium and non  equilibrium situations : the notion of time reversibility is associated with equilibrium , where the process @xmath1 is stationary .",
    "on the other hand , the increase of entropy is a result that belongs to the non  stationary regime , where the process is on its way to stationarity and equilibrium . in the latter case ,",
    "the system has been initially prepared in a non  equilibrium situation .",
    "of course , when the process is stationary , @xmath0 is fixed and there is no contradiction .",
    "so far we discussed the property of detailed balance only for an isolated system , where the stationary state distribution is the uniform distribution .",
    "how is the property of detailed balance defined when the stationary distribution is non  uniform ? for a general markov process , whose steady state ",
    "distribution is not necessarily uniform , the condition of detailed balance , which means time ",
    "reversibility @xcite , reads @xmath72 in the continuous  time case . in the discrete  time case ( where @xmath2 takes on positive integer values only ) , it is defined by a similar equation , except that @xmath73 and @xmath74 are replaced by the corresponding one  step state transition probabilities , i.e. , @xmath75 where @xmath76 the physical interpretation is that now our system is ( a small ) part of a much larger isolated system , which obeys detailed balance w.r.t .  the uniform equilibrium distribution , as before .",
    "a well known example of a process that obeys detailed balance in its more general form is the m / m/1 queue with an arrival rate @xmath77 and service rate @xmath78 ( @xmath79 ) . here",
    ", since all states are arranged along a line , with bidirectional transitions between neighboring states only ( see fig.[mm1 ] ) , there can not be any cyclic probability flux .",
    "the steady  state distribution is well  known to be geometric @xmath80 which indeed satisfies the detailed balance @xmath81 for all @xmath35 .",
    "thus , the markov process @xmath1 , designating the number of customers in the queue at time @xmath2 , is time ",
    "reversible .    for the sake of simplicity , from this point onward ,",
    "our discussion will focus almost exclusively on discrete  time markov processes , but the results to be stated , will hold for continuous  time markov processes as well",
    ". we will continue to denote by @xmath36 the probability of @xmath82 , except that now @xmath2 will be limited to take on integer values only .",
    "the one  step state transition probabilities will be denoted by @xmath83 , as mentioned earlier .",
    "how does the h  theorem extend to situations where the stationary state distribution is not uniform ?",
    "in @xcite , it is shown ( among other things ) that the divergence , @xmath84 where @xmath85 is a stationary state distribution , is a monotonically non  increasing function of @xmath2 .",
    "does this result have a physical interpretation , like the h  theorem and the second law of thermodynamics ? when it comes to non  isolated systems , where the steady state distribution is non  uniform , the extension of the second law of thermodynamics , replaces the principle of increase of entropy by the principle of decrease of free energy , or equivalently , the decrease of the difference between the free energy at time @xmath2 and the free energy in equilibrium . the information  theoretic counterpart of this free energy difference",
    "is the divergence @xmath86 ( see , e.g. , @xcite ) .",
    "thus , the monotonic decrease of @xmath86 has a simple physical interpretation of free energy decrease , which is the natural extension of the entropy increase . indeed , particularizing this to the case where @xmath87 is the uniform distribution ( as in an isolated system ) , then @xmath88 which means that the decrease of the divergence is equivalent to the increase of entropy , as before .",
    "however , here the result is more general than the h ",
    "theorem from an additional aspect : it does not require detailed balance .",
    "it only requires the existence of the stationary state distribution .",
    "note that even in the earlier case , of an isolated system , detailed balance , which means symmetry of the state transition probability matrix ( @xmath89 ) , is a stronger requirement than uniformity of the stationary state distribution , as the latter requires merely that the matrix @xmath83 would be doubly stochastic , i.e. , @xmath90 for all @xmath91 , which is weaker than symmetry of the matrix itself .",
    "the results shown in @xcite are , in fact , somewhat more general : let @xmath92 and @xmath93 be two time ",
    "varying state  distributions pertaining to the same markov chain , but induced by two different initial state distributions , @xmath94 and @xmath95 , respectively . then @xmath96 is monotonically non  increasing .",
    "this is easily seen as follows : @xmath97 where the last inequality follows from the data processing theorem of the divergence : the divergence between two joint distributions of @xmath98 is never smaller than the divergence between corresponding marginal distributions of @xmath99 .",
    "another interesting special case of this result is obtained if we now take the first argument of the divergence to the a stationary state distribution : this will mean that @xmath100 is also monotonically non  increasing .    in ( * ? ? ?",
    "* theorem 1.6 ) , there is a further extension of all the above monotonicity results , where the ordinary divergence is actually replaced by the f  divergence ( though the relation to the f  divergence is not mentioned in @xcite ) : if @xmath1 is a markov process with a given state transition probability matrix @xmath83 , then the function @xmath101 is monotonically non  increasing , provided that @xmath4 is convex .",
    "moreover , @xmath102 monotonically strictly decreasing if @xmath4 is strictly convex and @xmath103 is not identical to @xmath56 ) . to see why this is true ,",
    "define the backward transition probability matrix by @xmath104 obviously , @xmath105 for all @xmath91 , and so , @xmath106 by the convexity of @xmath4 : @xmath107 now , a few interesting choices of the function @xmath4 may be considered : as proposed in @xcite , for @xmath108 , we have @xmath109 , and we are back to the aforementioned result in @xcite . another interesting choice is @xmath110 , which gives @xmath111 .",
    "thus , the monotonicity of @xmath100 is also obtained as a special case . as a special case of the monotonicity of @xmath112 .",
    "this will require a slight further extension of this information measure , to be carried out later on .",
    "] yet another choice is @xmath113 , where @xmath114 $ ] is a parameter .",
    "this would yield the increasing monotonicity of @xmath115 , a ` metric ' that plays a role in the theory of asymptotic exponents of error probabilities pertaining to the optimum likelihood ratio test between two probability distributions ( * ? ? ?",
    "* chapter 3 ) .",
    "in particular , the choice @xmath116 yields balance between the two kinds of error and it is intimately related to the bhattacharyya distance . in the case of detailed balance",
    ", there is another physical interpretation of the approach to equilibrium and the growth of @xmath102 @xcite : returning , for a moment , to the realm of continuous  time markov processes , we can write the master equations as follows : @xmath117\\ ] ] where @xmath118^{-1}=[p(x)w_{xx'}]^{-1}$ ] .",
    "imagine now an electrical circuit where the indices @xmath119 designate the various nodes .",
    "nodes @xmath35 and @xmath120 are connected by a wire with resistance @xmath121 and every node @xmath35 is grounded via a capacitor with capacitance @xmath43 ( see fig .",
    "[ circuit ] ) . if @xmath36 is the charge at node @xmath35 at time @xmath2 , then the master equations are the kirchoff equations of the currents at each node in the circuit .",
    "thus , the way in which probability spreads across the states is analogous to the way charge spreads across the circuit and probability fluxes are now analogous to electrical currents . if we now choose @xmath122 , then @xmath123 which means that the energy stored in the capacitors dissipates as heat in the wires until the system reaches equilibrium , where all nodes have the same potential , @xmath124 , and hence detailed balance corresponds to the situation where all individual currents vanish ( not only their algebraic sum ) .",
    "we have seen , in the above examples , that various choices of the function @xmath4 yield various f  divergences , or ` metrics ' , between @xmath125 and @xmath103 , which are both marginal distributions of a single symbol @xmath35 .",
    "what about joint distributions of two or more symbols ?",
    "consider , for example , the function @xmath126 where @xmath4 is convex as before .",
    "here , by the same token , @xmath127 is the f  divergence between the joint probability distribution @xmath128 and the product of marginals @xmath129 , namely , it is the generalized mutual information of @xcite,@xcite , and @xcite , as mentioned in the introduction .",
    "now , using a similar chain of inequalities as before , we get the non  decreasing monotonicity of @xmath127 as follows : @xmath130 this time , we assumed only the markov property of @xmath131 ( not even homogeneity ) .",
    "this is , in fact , nothing but the 1973 version of the generalized data processing theorem of ziv and zakai @xcite , which was mentioned in the introduction .",
    "in spite of the general resemblance ( via the notion of the f  divergence ) , the last monotonicity result , concerning @xmath127 , and the monotonicity of @xmath96 , do not seem , at first glance , to fall in the framework of the monotonicity of the f ",
    "divergence @xmath112 .",
    "this is because in the latter , there is an additional dependence on a stationary state distribution that appears neither in @xmath96 nor in @xmath127 .",
    "however , two simple observations can put them both in the framework of the monotonicity of @xmath112 .",
    "the first observation is that the monotonicity of @xmath132 continues to hold ( with a straightforward extension of the proof ) if @xmath36 is extended to be a vector of time varying state distributions @xmath133 , and @xmath4 is taken to be a convex function of @xmath15 variables .",
    "moreover , each component @xmath134 does not have to be necessarily a probability distribution .",
    "it can be any function @xmath135 that satisfies the recursion @xmath136 let us then denote @xmath137 and assume that @xmath4 is jointly convex in all its @xmath15 arguments .",
    "then the redefined function @xmath138 is monotonically non  increasing with @xmath2 .",
    "the second observation is rooted in convex analysis , and it is related to the notion of the perspective of a convex function and its convexity property @xcite . here , a few words of background are in order .",
    "let @xmath139 be a convex function of the vector @xmath140 and let @xmath141 be an additional variable .",
    "then , the function @xmath142 is called the _ perspective function _ of @xmath4 . a well  known property of the perspective operation is conservation of convexity , in other words , if @xmath4 is convex in @xmath143 , then @xmath144 is convex in @xmath145",
    ". the proof of this fact , which is straightforward , can be found , for example , in ( * ? ? ?",
    "* , subsection 3.2.6 ) ( see also @xcite ) and it is brought here for the sake of completeness : letting @xmath146 and @xmath147 be two non  negative numbers summing to unity and letting @xmath148 and @xmath149 be given , then @xmath150 putting these two observations together , we can now state the following result :    [ hdpt ] let @xmath151 where @xmath4 is a convex function of @xmath15 variables and @xmath152 are arbitrary functions that satisfy the recursion @xmath153 and where @xmath154 is moreover strictly positive . then",
    ", @xmath155 is a monotonically non  increasing function of @xmath2 .    using the above mentioned observations , the proof of theorem [ hdpt ] is straightforward : letting @xmath87 be a stationary state distribution of @xmath1 , we have : @xmath156 since @xmath144 is the perspective of the convex function @xmath4 , then it is convex as well , and so , the monotonicity of @xmath155 follows from the first observation above .",
    "it is now readily seen that both @xmath96 and @xmath127 are special cases of @xmath155 and hence we have covered all special cases seen thus far under the umbrella of the more general information functional @xmath155 .",
    "it is important to observe that the same idea exactly can be applied , first of all , to the 1973 version of the ziv ",
    "zakai data processing theorem ( regardless of the above described monotonicity results concerning markov processes ) : consider the generalized mutual information functional @xmath157 where @xmath158 and @xmath159 are arbitrary functions that are consistent with the markov conditions , i.e. , for any markov chain @xmath160 , these functions satisfy @xmath161 then , @xmath162 satisfies a data processing inequality , because , again @xmath163 which is a zakai ",
    "ziv information functional of the 1975 version @xcite and hence it satisfies a data processing inequality .    what functions , @xmath164 and @xmath159 , can be consistent with the markov conditions ?",
    "two such functions are , of course , @xmath165 and @xmath166 , which bring us back to the 1973 ziv ",
    "zakai information measure .",
    "we can , of course , swap their roles and obtain a generalized version of the lautum information @xcite , which is also known to satisfy a data processing inequality . for additional options ,",
    "let us consider a communication system , operating on single symbols ( block length 1 ) , where the source symbol @xmath167 is mapped into a channel input @xmath168 , by a deterministic encoder @xmath169 , which is then fed into the channel @xmath170 , and the channel output @xmath171 is in turn mapped into the reconstruction symbol @xmath172 .",
    "as is argued in @xcite , the function @xmath173 is consistent with the markov conditions for any given source symbol @xmath174 .",
    "indeed , since the encoder is assumed deterministic , @xmath175 , and it is easily seen that @xmath176 and @xmath177 of course , every linear combination of all these functions is also consistent with the markov conditions .",
    "thus , we can take @xmath178 and @xmath179 where @xmath180 and @xmath181 are the ( arbitrary ) coefficients of these linear combinations ( with the limitation that @xmath182 for all @xmath183 , with at least one @xmath184 ) .",
    "thus , we may define @xmath185\\cdot q\\left ( \\frac{t_0p(x , y)+\\sum_{x_i\\in{{\\cal x}}}t_i p(x)p(y|x_i ) } { s_0p(x , y)+\\sum_{x_i\\in{{\\cal x}}}s_i p(x)p(y|x_i)}\\right),\\ ] ] or , equivalently , @xmath186\\cdot q\\left ( \\frac{t_0p(y|x)+\\sum_{x_i\\in{{\\cal x}}}t_i p(y|x_i ) } { s_0p(y|x)+\\sum_{x_i\\in{{\\cal x}}}s_i p(y|x_i)}\\right).\\ ] ] moreoever , to eliminate the dependence on the specific encoder , we can think of @xmath21 as independent random variables , take the expectation w.r.t .  their randomness ( in the same spirit as in @xcite ) , and obtain the following information measure @xmath187\\cdot q\\left ( \\frac{t_0p(y|x)+\\sum_it_i p(y|x_i ) } { s_0p(y|x)+\\sum_{i}s_i p(y|x_i)}\\right)\\right\\},\\ ] ] where the expectation is w.r.t .",
    "the product measure of @xmath188 , @xmath189 .",
    "these are the most general information measures , that obey a data processing inequality , that we can get with a univariate convex function @xmath4 .",
    "for example , returning to eq .",
    "( [ withoutexp ] ) and taking @xmath190 , @xmath191 , @xmath192 ( @xmath193 , a parameter ) , and @xmath194 , @xmath195 , we have @xmath196 , and @xmath166 , and the resulting generalized mutual information reads @xmath197\\cdot q\\left(\\frac{p(y)}{p(y|x)+sp(y)}\\right).\\ ] ] the interesting point concerning these generalized mutual information measures is that even if we remain in the framework of the 1973 version of the ziv  zakai data processing theorem ( as opposed to the 1975 version )",
    ", we have added an extra degrees of freedom ( in the above example , the parameter @xmath198 ) , which may be used in order to improve the obtained bounds .",
    "if the inequality @xmath199 can be transformed into an inequality on the distortion @xmath12 , where the lower bound depends on @xmath198 , then this bound can be maximized w.r.t.the parameter @xmath198 .",
    "if the optimum @xmath200 yields a distortion bound which is larger than that of @xmath201 , then we have improved on @xcite for the given choice of the convex function @xmath4 .",
    "sometimes this optimization may not be a trivial task , but even if we can just identify one positive value of @xmath198 ( including the limit @xmath202 ) that is better than @xmath201 , then we have improved on the generalized data processing bound of @xcite , which corresponds to @xmath201 .",
    "this additional degree of freedom may be important , because , as mentioned in the introduction , the variety of convex functions @xmath28 which are convenient to work with , is somewhat limited ( most notably , the functions @xmath203 , @xmath204 , @xmath205 and some piecewise linear functions @xcite,@xcite ) .",
    "the next example demonstrates this point .",
    "+ _ example .",
    "_ consider the information functional ( [ simplextension ] ) with the convex function @xmath205 .",
    "then , the corresponding generalized mutual information is @xmath206\\cdot\\sqrt{\\frac{p(v)}{p(v|u)+sp(v)}}\\nonumber\\\\ & = -\\sum_{u , v}p(u)\\sqrt{p(v)[p(v|u)+sp(v)]}\\nonumber\\\\ & = -\\sum_{u , v}p(u)p(v)\\sqrt{s+\\frac{p(v|u)}{p(v)}}.\\end{aligned}\\ ] ] consider now the above  described problem of joint source  channel coding , for the following source and channel : the source is designated by a random variable @xmath207 , which is uniformly distributed over the alphabet @xmath208 .",
    "the reproduction variable , @xmath209 , takes on values in the same alphabet , i.e. , @xmath210 and the distortion function is @xmath211 which means that errors other than @xmath212 are strictly forbidden . therefore the channel from @xmath207 to @xmath209",
    "must be of the form @xmath213 where @xmath214 are parameters taking values in @xmath215 $ ] and complying with the distortion constraint @xmath216 the channel is a noise ",
    "free @xmath217ary channel , i.e. , its input and output alphabets are @xmath218 with @xmath219 for @xmath220 , and @xmath221 otherwise .    obviously , the case @xmath222 is not interesting because the data can be conveyed error ",
    "free by trivially connecting the source to the channel . in the other extreme ,",
    "where @xmath223 , there must be some channel input symbol to which at least three source symbols are mapped .",
    "in such a case , it is impossible to avoid at least one of the forbidden errors in the reconstruction . thus , the interesting cases are those for which @xmath224 , or equivalently , @xmath225 $ ] , where @xmath226 .",
    "we next derive a distortion bound based on the generalized data processing theorem , in the spirit of @xcite and @xcite , where we now have the parameter @xmath198 as a degree of freedom .    as for the source ,",
    "let us suppose that in addition to the distortion constraint , we impose the constraint that the distribution of the reproduction variable @xmath209 , just like @xmath207 , must be uniform over its alphabet , namely , @xmath227 for all @xmath228 . in this case , @xmath229\\nonumber\\\\ & = \\frac{1}{k^2}\\sum_{u=0}^{k-1}\\left[\\sqrt{s+k\\epsilon_u}+ \\sqrt{s+k(1-\\epsilon_u)}\\right]+\\left(1-\\frac{2}{k}\\right)\\sqrt{s}\\nonumber\\\\ & \\le\\frac{1}{k^2}\\cdot k\\left[\\sqrt{s+kd}+ \\sqrt{s+k(1-d)}\\right]+\\left(1-\\frac{2}{k}\\right)\\sqrt{s}\\nonumber\\\\ & = \\frac{1}{k}\\left[\\sqrt{s+kd}+ \\sqrt{s+k(1-d)}\\right]+\\left(1-\\frac{2}{k}\\right)\\sqrt{s},\\end{aligned}\\ ] ] where the inequality follows from the fact that the maximum of the concave function @xmath230,\\ ] ] subject to the distortion constraint ( [ distortionconstraint ] ) , is achieved when @xmath231 for all @xmath232 .",
    "thus , @xmath233-\\left(1-\\frac{2}{k}\\right)\\sqrt{s}.\\ ] ]    as for the channel , we have : @xmath234+\\sum_xp^2(x)\\sqrt{s+\\frac{1}{p(x)}}\\nonumber\\\\ & = \\sqrt{s}+\\sum_xp^2(x)\\left(\\sqrt{s+\\frac{1}{p(x)}}-\\sqrt{s}\\right)\\nonumber\\\\ & = \\sqrt{s}+\\sum_xp^2(x)\\cdot\\frac{1/p(x)}{\\sqrt{s+1/p(x)}+\\sqrt{s}}\\nonumber\\\\ & = \\sqrt{s}+\\sum_x\\frac{p(x)}{\\sqrt{s+1/p(x)}+\\sqrt{s}}.\\end{aligned}\\ ] ] the function @xmath235 $ ] is convex in @xmath2 ( for fixed @xmath198 ) since @xmath236 for all @xmath237 , as can readily be verified .",
    "thus , @xmath238 is minimized by the uniform distribution @xmath239 , @xmath240 , which leads to the ` capacity ' expression : @xmath241 applying now the data processing theorem , @xmath242 we obtain , after rearranging terms @xmath243 squaring both sides , we have : @xmath244}\\ge \\left[\\frac{k}{\\sqrt{s}+\\sqrt{s+l}}+2\\sqrt{s}\\right]^2\\ ] ] or @xmath245}\\ge \\left[\\frac{k}{\\sqrt{s}+\\sqrt{s+l}}+2\\sqrt{s}\\right]^2 -2s - k,\\ ] ] which after squaring again and applying some further straightforward algebraic manipulations , gives eventually the following inequality on the distortion @xmath12 : @xmath246 where @xmath247 ^ 2-\\frac{4s(s+k)}{k^2}.\\ ] ] the resulting lower bound on the distortion is the smaller of the two solutions of the equation @xmath248 , which is @xmath249 thus , the larger is @xmath250 , the better is the bound .",
    "the choice @xmath201 , which corresponds to the usual ziv  zakai bound for @xmath205 , yields @xmath251 ^ 2=\\left(\\frac{k}{l}-1\\right)^2=(\\theta-1)^2.\\ ] ] however , it turns out that @xmath201 is not the best choice of @xmath198 .",
    "we next examine the limit @xmath202 . to this end",
    ", we derive a lower bound to @xmath250 which is more convenient to analyze in this limit .",
    "note that for @xmath252 , it is guaranteed that the expression in the square brackets of the expression defining @xmath250 , is positive , which means that an upper bound on @xmath253 would yield a lower bound to @xmath250 .",
    "thus , upper bounding @xmath253 by @xmath254 we get @xmath255 ^ 2 - 4s^2 - 4ks\\nonumber\\\\ & \\ge\\left[\\left(\\frac{k}{\\sqrt{s}(2+l/2s)}+2\\sqrt{s}\\right)^2 - 2s - k\\right]^2 - 4s^2 - 4ks\\nonumber\\\\ & = k^2\\left(\\frac{4s - l}{4s+l}\\right)^2+\\frac{16k^4s^2}{(4s+l)^4}-\\frac{8kls}{4s+l}+ \\frac{16k^2s^2}{(4s+l)^2}+\\frac{8k^3s(4s - l)}{(4s+l)^3}\\nonumber\\\\ & { \\stackrel{\\delta } { = } } k^2\\psi_0(s),\\end{aligned}\\ ] ] where between the second and the third lines , we have skipped some standard algebraic operations .",
    "taking now the limit @xmath202 , we obtain @xmath256 which yields a better bound than the bound of @xmath201 since @xmath257 for all @xmath258 .",
    "it is interesting to compare this also to the classical data processing theorem : since @xmath259 and @xmath260 then the ordinary data processing theorem yields the bound @xmath261 since @xmath262 and @xmath263 within the relevant range of @xmath264 , the bound pertaining to @xmath202 is also better than the classical bound for this case .",
    "this completes the description of the example .",
    "@xmath265    finally , we should comment that the monotonicity result concerning @xmath155 contains as special cases , not only the h  theorem , as well as all other earlier mentioned monotonicity results , but also the 1975 zakai ",
    "ziv generalized data processing @xcite .",
    "consider a markov chain @xmath266 , where @xmath207 , @xmath209 and @xmath63 are random variables that take on values in ( finite ) alphabets , @xmath267 , @xmath268 , and @xmath269 , respectively .",
    "let us now map between the markov chain @xmath270 and the markov process @xmath1 in the following manner : @xmath271 is assigned to the state @xmath120 of the process at time @xmath2 , whereas @xmath272 corresponds and @xmath269 may be different ( finite ) alphabets , @xmath35 and @xmath120 , of the original markov process , must taken on values in the same alphabet .",
    "assuming , without loss of generality , that @xmath273 and @xmath274 , then for the purpose of this mapping , we can unify these alphabets to be both @xmath275 and complete the missing elements of the extended transition matrix @xmath276 in a consistent manner , according to the actual support of each distribution .",
    "we omit further technical details herein . ] to @xmath35 at time @xmath277 . now , defining accordingly , @xmath278 and @xmath279 then due to the markov property of @xmath270 , both measures satisfy the recursion with @xmath276 playing the role of @xmath280 and @xmath281 simply as an index .",
    "] of @xmath282 .",
    "i.e. , @xmath283 and @xmath284 thus , for @xmath285 , the monotonicity of @xmath155 is nothing but the data processing of the classical mutual information . for a general function @xmath4 of one variable ( @xmath286 ) ,",
    "this gives the generalized data processing theorem of @xcite .",
    "furthermore , letting @xmath4 be a general convex function of @xmath15 variables , and @xmath287 as before , we get the more general form of the data processing inequality of @xcite .",
    "the above extension of the h  theorem gives rise to a seemingly more general data processing theorem than in @xcite , as it is not necessary to let @xmath154 be the actual joint probability distribution .",
    "however , when looking at the entire class of convex functions with an arbitrary number of arguments , this is not really more general , as the corresponding generalized mutual information can readily be transformed back to the form of the 1975 zakai ",
    "ziv information functional using again the perspective operation .",
    "indeed , as mentioned in the introduction and shown in ( * ? ? ?",
    "* theorem 7.1 ) , the class of generalized mutual information measures studied therein can not be improved upon in the sense that there always exist choices of @xmath4 and @xmath18 that provide tight bounds on the distortion of the optimum system .",
    "the main contributions of this work can be summarized as follows : first , we have establisehd a unified framework and a relationship between ( a generalized version of ) the second law of thermodynamics and the generalized data processing theorems of zakai and ziv .",
    "this unified framework turns out to strengthen and expand both of these pieces of theory : concerning the second law of thermodynamics , we have identified a significantly more general information measure , which is a monotonic function of time , when it operates on a markov process . as for the generalized ziv ",
    "zakai data processing theorem , we have proposed a wider class of information measures obeying the data processing theorem , which includes free parameters that may be optimized so as to tighten the distortion bounds .",
    "interesting discussions with j.  ziv and m.  zakai are acknowledged with thanks .",
    "d.  andelman , `` bounds according to a generalized data processing theorem , '' m.sc",
    ".  dissertation , department of electrical engineering , technion , israel institute of technology , haifa , israel , october 1974 .",
    "b.  dacorogna and p.  marchal , `` the role of perspective functions in convexity , polyconvexity , rank  one convexity and separate convexity , '' + http://caa.epfl.ch/publications/2008-the_role_of_perspective_functions_in_convexity.pdf"
  ],
  "abstract_text": [
    "<S> we draw relationships between the generalized data processing theorems of zakai and ziv ( 1973 and 1975 ) and the dynamical version of the second law of thermodynamics , a.k.a .  </S>",
    "<S> the boltzmann h  </S>",
    "<S> theorem , which asserts that the shannon entropy , @xmath0 , pertaining to a finite  state markov process @xmath1 , is monotonically non  decreasing as a function of time @xmath2 , provided that the steady  state distribution of this process is uniform across the state space ( which is the case when the process designates an isolated system ) . </S>",
    "<S> it turns out that both the generalized data processing theorems and the boltzmann h  theorem can be viewed as special cases of a more general principle concerning the monotonicity ( in time ) of a certain generalized information measure applied to a markov process . </S>",
    "<S> this gives rise to a new look at the generalized data processing theorem , which suggests to exploit certain degrees of freedom that may lead to better bounds , for a given choice of the convex function that defines the generalized mutual information . </S>",
    "<S> + * index terms : * data processing inequality , convexity , perspective function , h  theorem , thermodynamics , detailed balance .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + haifa 32000 , israel + </S>"
  ]
}