{
  "article_text": [
    "_ title of program : _ pvegas.c + _ computer and operating system tested : _",
    "convex spp1200 ( spp - ux 4.2 ) , intel x86 ( linux 2.0 with smp ) , dec alpha ( osf1 3.2 and digital unix ) , sparc ( solaris 2.5 ) , rs/6000 ( aix 4.0 ) + _ programming language : _ ansi - c + _ no . of lines in distributed routine : _ 530",
    "the monte carlo method frequently turns out to be the only feasible way to get numerical results of integrals when ill - behaved integrands are involved of which no a priori - knowledge about their behaviour is available .",
    "it not only handles step functions and gives reliable error estimates but also has the desireable feature that the rate of convergence is dimension - independent . in the framework of the xloops - project  @xcite , for instance",
    ", massive two - loop feynman - diagrams with exterior momenta are calculated analytically as far as possible with sometimes very ill - behaved integrals left for numerical evaluation over finite two- or three - dimensional volumes .",
    "if a function @xmath0 needs to be integrated over a @xmath1-dimensional volume @xmath2 , one can evaluate @xmath3 over @xmath4 random sample - points @xmath5 with @xmath6 and compute the estimate @xmath7 which has a convergence - rate of @xmath8 for large @xmath4 .",
    "similarly , @xmath9 has basically the same behaviour , if the probability density @xmath10 is normalized to unity in @xmath2 : @xmath11 the introduction of the weight - function @xmath12 is equivalent to a transformation in the integration variables @xmath13 where the transformation leaves the boundary @xmath14 unchanged .",
    "the adaptive monte carlo method now tries to effectively improve the rate of convergence by choosing @xmath10 properly .",
    "as is well known , the modified variance @xmath15 for large @xmath4 is given by : @xmath16 with @xmath17 the central limit theorem implies that for square integrable @xmath3 the distribution of @xmath18 around the true value becomes gaussian and @xmath15 in  ( [ sigmasquared ] ) is a reliable error - estimate .",
    "as every method of selecting a proper @xmath12 must rely on information about the integrand , only approximate and/or iterative methods are practically in use .",
    "the popular ` vegas`-algorithm uses two of these . we will sketch them in a brief discussion of ` vegas ` in section  [ sec : about ] .",
    "section  [ sec : macro ] contains some warnings about a sometimes seen oversimplified macro - parallelized ` vegas ` and in section  [ sec : micro ] our approach is presented together with some real - world measurements of efficiency . at some places",
    "explicit variable - names are mentioned for those readers familiar with g.  p.  lepage s original code  @xcite .",
    "the two techniques used by ` vegas ` to enhance the rate of convergence are _ importance sampling _ and _ stratified sampling_. importance sampling tries to enhance a weight - function @xmath10 by drawing from previous iterations .",
    "it is well known , that the variance @xmath15 is minimized , when @xmath19 this method concentrates the density @xmath12 where the function is largest in magnitude .",
    "stratified sampling attempts to enhance the @xmath20-behaviour of mc integration by choosing a set of random numbers which is more evenly distributed than plain random numbers are .",
    "( recall that the simplest method of stratification would evaluate the function on a cartesian grid and thus converge as @xmath21 . )",
    "this is done by subdividing the volume @xmath2 into a number @xmath22 of hypercubes @xmath23 and performing an mc integration over @xmath24 sample - points in each .",
    "the variance in each hypercube can be varied by shifting the boundaries of the hypercubes between successive iterations ( figure  [ fig : grids]a shows an initial grid , [ fig : grids]b the grid at a later stage ) .",
    "the optimal grid is established when the variance is equal in all hypercubes .",
    "this method concentrates the density @xmath12 where both the function and its gradient are large in magnitude .",
    "the split - up of @xmath2 into @xmath22 hypercubes turns out to be the key - point in efficiently parallelizing ` vegas ` .    the way ` vegas ` iterates hypercubes across the whole volume is designed in a dimension - independent way . in effect it just amounts to @xmath1 loops packed into each other , each iterating from the lower limit of integration to the upper one .",
    "we ll see in section  [ sec : micro ] how this looping can be exploited for parallelization with variable grain - size . for a more thorough discussion of ` vegas ` the reader is referred to the literature  @xcite .",
    "the most straightforward approach to make use of a parallel machine with @xmath25 processors is to simply replicate the whole job . instead of having one processor",
    "calculate @xmath4 sample - points , @xmath25 instances of the integrator ( `` workers '' ) are started , each sampling @xmath26 points .",
    "subsequently , the results from each processor are averaged taking into account their differing error - estimates .",
    "we call this approach macro - parallelization .",
    "it is immediately clear that this is trivial to implement and usually results in good performance since the amount of communication among processors is minimized .",
    "this approach , however , results in @xmath25 different grids , each less fine than the original one . if the same number of points is sampled in each hypercube and the overall number of points are equal , the amount by which the grid will be coarser is given by the dilution of hypercubes which is @xmath27 .",
    "furthermore , in an extreme situation some of the workers might accidentally miss an interesting area and return wrong results way outside the estimated error - bounds and thus completely fail to adapt .",
    "we have seen realizations of a slightly improved method which does not suffer from overestimation of single partial results .",
    "this method spawns @xmath25 workers , again each evaluating @xmath26 points , and lets each evaluate the cumulative variables and send them to the parent which adds them and subsequently computes the new grid .",
    "this method will still suffer from coarse grids but it will adapt more cleanly . in effect , it amounts to synchronizing the grids between workers .",
    "table  [ tab : comparison ] exemplifies the problems typical for macro - parallelization .",
    "it shows the results of an integration over a unit - square .",
    "the test - function was a narrow gaussian peak with width @xmath28 and normalized such that the exact result of the integration is unity .",
    "all runs were typical ` vegas`-calls : the first 10 iterations were used only to refine the grid , their results were discarded ( entry - point 1 in ` vegas`-jargon ) . in that particular case",
    "the macro - parallelized version took 5 iterations until every processor had `` detected '' the peak .",
    "the ones that failed to detect it returned very small values with small error - bounds and the common rules for error - manipulation then grossly overestimated the weight of these erroneous results .",
    "the unparallelized version in contrast was able to adapt to the function s shape very early .",
    "the last 5 iterations were cumulative : each iteration inherited not only the grid but also the result of the previous one ( entry - point 2 ) .",
    "note also that after the grids have adapted to the situation , the macro - parallelized ` vegas ` without synchronization still returns misleading error - bounds .",
    "[ tab : comparison ]    .",
    "15 iterations of two macro - parallelized ` vegas ` ( with @xmath29 ) integrating a sharp gaussian contrasted with an unparallelized run .",
    "equal numbers of function calls were sampled in each run . [ cols=\"<,^,^,^,^\",options=\"header \" , ]     the macro - parallelized version with grid - synchronization performs better than the one without but still is less able to adapt to the specific integrand , as expected .",
    "of course the results of this extreme situation are less pronounced for better - behaved integrands but the general result always holds .",
    "it is just a manifestation of a fact well - known to people using ` vegas ` : few large iterations generally result in better estimates than many small ones .",
    "what is desired is a method that parallelizes the algorithm but still has the same numerical properties as the sequential version .",
    "as has been shown in the previous section , this can not be achieved on a macroscopic level . fortunately , ` vegas ` does offer a convenient way to split up the algorithm and map it onto a parallel architecture . still using a well - understood farmer - worker - model , our approach tries to distribute the hypercubes that make up the domain of integration to the workers .",
    "mc integration does not exhibit any boundaries which would need extensive communication among workers but it does need some accounting - synchronization to guarantee that each hypercube is evaluated exactly once .",
    "a straightforward broadcast - gather - approach would require one communication per hypercube and would thus generate an irresponsible amount of overhead spoiling efficent scalability .",
    "we therefore suggest having each processor evaluate more than one hypercube before any communication is done .",
    "let @xmath30 be the number of equal fractions the whole volume is split up into .",
    "ideally , we should require the number @xmath30 of fractions to be much smaller than the number @xmath22 of hypercubes @xmath31 : @xmath32 .",
    "the problem of dynamic load - balancing can in practice be solved by making the number of fractions @xmath30 much larger than the number of processors @xmath25 : be an integer multiple of @xmath25 would fit best on @xmath25 nodes .",
    "however , this is only valid if one assumes that the function exhibits the same degree of complexity in the whole volume and if each node is equally fast . both assumptions are usually unjustified . ]",
    "@xmath33 .",
    "we thus arrive at the constraint : @xmath34    this inequality can be satisfied in the following convenient way opening up an algorithmically feasible way to implement it : projecting the @xmath1-dimensional volume onto a @xmath35-dimensional subspace defines a set of @xmath35-dimensional sub - cubes .",
    "the set of original hypercubes belonging to the same sub - cube make up one fraction to be done by one worker in a single loop .",
    "we thus identify @xmath30 with the number of sub - cubes .",
    "because the hypercubes belonging to different sub - cubes can be evaluated concurrently we call the @xmath35-dimensional subspace the _ parallel space _ and its orthogonal complement the @xmath36-dimensional _ orthogonal space _ ( @xmath37 ) . choosing @xmath38 and @xmath39",
    "can be expected to satisfy  ( [ constraint ] ) for practical purposes  ( figure [ fig : method ] ) .",
    "an important issue for every mc - effort is the random number generator ( rng ) .",
    "there are two different ways to tackle the problems arising in parallel simulations  @xcite :    * one single rng pre - evaluates a sequence of random numbers which are then assigned without overlap to the processors . *",
    "every processor gets a rng of its own and some method has to guarantee that no correlations spoil the result .    a look at amdahl s law shows that the second approach is the more attractive one .",
    "amdahl s law relates the speedup @xmath40 for parallel architectures with the number of processors @xmath25 and the fraction of code @xmath41 which is executed in parallel : @xmath42 use of concurrently running rngs increases @xmath41 which in turn results in an improved speedup .",
    "most compiler libraries provide linear congruential generators which generate sequential pseudorandom numbers by the recurrence @xmath43 with carefully selected @xmath44 , @xmath45 and @xmath46 . because of their short period and long - range correlations reported by de  matteis and pagnutti  @xcite which make parallelization dangerous this type is not suited for large mc - simulations .    for our case we therefore decided to build on a slightly modified shift register pseudorandom number generator ( sr )  @xcite .",
    "this widely employed class of algorithms ( r250 is one example ) generates random - bit sequences by pairwise xoring bits from some given list of @xmath47 binary numbers @xmath48 : @xmath49    here , @xmath50 represents the exclusive - or ( xor ) operator and @xmath47 and @xmath51 are chosen such that the trinomial @xmath52 is primitive modulo two .",
    "the so defined ` tausworthe - sequence '  @xcite is known to have periodicity @xmath53 .",
    "thus , every combination of @xmath47 bits occurs exactly once with the only exception being @xmath47 subsequent zeros ( which would return the trivial sequence of zeros only , if it occured ) .",
    "tables of `` magic - numbers '' @xmath47 and @xmath51 can be found in the literature  @xcite and are provided with our program - sources .",
    "note that owing to its exponential growth with @xmath47 , the periodicity easily reaches astronomical lengths which can never be exploited by any machine .",
    "uniformly distributed random @xmath54-bit integers can now be constructed easily by putting together columns of bits from several instances of the same tausworthe - sequence @xmath55 with predefined delays @xmath56 : @xmath57 floating - point numbers in the range @xmath58 can subsequently be computed by dividing @xmath59 by @xmath60 . in the continuous limit of large @xmath54",
    "such a random - sequence will have mean @xmath61 , variance @xmath62 as well as the enormous length of the original bit - sequences which in turn guarantees @xmath1-space uniformity for @xmath63 .",
    "in addition , this method is extremely fast because the machine s word - size and xor - operation from the native instruction - set can be exploited .",
    "the good properties of this class of generators can be ruined by improper initialization .",
    "lewis and payne  @xcite for instance , suggested initializing the tausworthe - sequence with every bit set to one , introduce a common delay @xmath64 between each column of bits and throw away the first @xmath65 iterations in order to leave behind initial correlations .",
    "this is not only slow ( even if a short - cut described by i.  dek  @xcite is used ) , but also results in perspicuous correlations if @xmath47 only becomes large enough .",
    "this is a direct result of the exponential growth of the period while the delay and initial iterations grow only linearly .",
    "a quicker and less cumbersome initialization procedure was suggested by kirkpatrick and stoll  @xcite .",
    "they noted that initializing the tausworthe - sequence with random - bits from some other generator , will define an ( unknown ) offset somewhere between @xmath66 and @xmath53 in the sequence  ( [ tauswortherecurrence ] ) from which iteration can proceed .",
    "initializing every column of bits in the integer - sequence  ( [ bittoint ] ) with such random - numbers defines different offsets and thus implicitly defines a set of delays @xmath67 as well as the starting - point of the whole sequence .",
    "this method does clearly not suffer from initial correlations .",
    "the method of kirkpatrick and stoll offers a clean and efficient way for parallelization : as many generators as there are processors can be initialized by random numbers from some generator , for example a simple and well - understood linear congruential one .",
    "only the @xmath68 of each of the @xmath25 generators need to be filled .",
    "the probability that two of these generators will produce the same sequence because they join the same set of delays @xmath67 can be made arbitrary small by simply choosing @xmath47 big enough .",
    "to rule out correlations among the @xmath25 sequences is equivalent to assuming there are no interactions between the shift - register generator and the linear congruential generator .",
    "indeed , the methods and the underlying theory are quite different .",
    "the method is however still plagued by the known flaws , common to all shift register generators .",
    "one examples is the triplet - correlation  @xcite .",
    "it can in principle be cured by an expansion of the method described in  @xcite . in the case of ` vegas ` however , we see no reason why high - quality rngs should be needed at all and we therefore advocate using simple generators with @xmath69 : stratification lets short - range - correlations only take effect within the hypercubes inside the rectangular grid where very few points are sampled and long - range - correlations become washed out by the grid shifting between iterations .",
    "this view is supported by the observation that correlations in sr - generators seem to have been discovered only in calculations more sophisticated than plain mc integration  @xcite .",
    "figure  [ fig : scalings ] shows the efficiency at integrating a function consisting of the sum of 8 dilogarithms computed with a method suggested in  @xcite .",
    "the parameters have been chosen such that all the characteristic properties become visible in one single run .",
    "the five - dimensional volume was split up into a two - dimensional parallel space and a three - dimensional orthogonal space with each axis subdivided into 21 intervals .",
    "@xmath70 points were evaluated in each iteration .",
    "what we see are some minor fluctuations modulated on a rather good overall efficiency .",
    "the spp1200 consists of hypernodes with 8 processors running in real shared memory each , hence the drop - off at @xmath71 where the second hypernode across an interconnect is first touched .",
    "the behaviour for small @xmath25 is thus machine - specific .",
    "the sawtooth for larger @xmath25 , in contrast , is characteristic for the algorithm : as the test function does not involve steps or other changes in cost of evaluation , most processors terminate the job assigned to them rather simultaneously .",
    "so , at @xmath72 we see each processor evaluating 11 of the @xmath73 fractions and then one processor evaluating the single remaining one .",
    "the algorithm thus needs 12 times the time necessary for evaluating one fraction while at @xmath74 it needs only 11 .",
    "this behaviour can easily be stopped by raising the dimension of the parallel space to three for instance , thus decreasing the grain - size .",
    "the obvious drawback is an incremented communication - overhead .",
    "the ideal split - up has to be determined individually for each combination of hardware and problem . for a given @xmath25",
    ", astute users will probably tune their parameters @xmath4 and @xmath35 judiciously in order to take advantage of one of the peaks in figure  [ fig : scalings ] .    ",
    "@=11     @=12    ( 0,0)(1,1 ) ",
    "@=11    @border(0.1700,0.1888 ) ( 0.1775,0.1888 )    @border(0.9840,0.1888 ) ( 0.9765,0.1888 )    @border(0.1700,0.2303 ) ( 0.1775,0.2303 )    @border(0.9840,0.2303 ) ( 0.9765,0.2303 )    @border(0.1700,0.2719 ) ( 0.1775,0.2719 )    @border(0.9840,0.2719 ) ( 0.9765,0.2719 )    @border(0.1700,0.2719 ) ( 0.1850,0.2719 )    @border(0.9840,0.2719 ) ( 0.9690,0.2719 )    ( 0.1540,0.2719)0.85 @border(0.1700,0.3134 ) ( 0.1775,0.3134 )    @border(0.9840,0.3134 ) ( 0.9765,0.3134 )    @border(0.1700,0.3549 ) ( 0.1775,0.3549 )    @border(0.9840,0.3549 ) ( 0.9765,0.3549 )    @border(0.1700,0.3965 ) ( 0.1775,0.3965 )    @border(0.9840,0.3965 ) ( 0.9765,0.3965 )    @border(0.1700,0.4380 ) ( 0.1775,0.4380 )    @border(0.9840,0.4380 ) ( 0.9765,0.4380 )    @border(0.1700,0.4796 ) ( 0.1775,0.4796 )    @border(0.9840,0.4796 ) ( 0.9765,0.4796 )    @border(0.1700,0.4796 ) ( 0.1850,0.4796 )    @border(0.9840,0.4796 ) ( 0.9690,0.4796 )    ( 0.1540,0.4796)0.9 @border(0.1700,0.5211 ) ( 0.1775,0.5211 )    @border(0.9840,0.5211 ) ( 0.9765,0.5211 )    @border(0.1700,0.5627 ) ( 0.1775,0.5627 )    @border(0.9840,0.5627 ) ( 0.9765,0.5627 )    @border(0.1700,0.6042 ) ( 0.1775,0.6042 )    @border(0.9840,0.6042 ) ( 0.9765,0.6042 )",
    "@border(0.1700,0.6457 ) ( 0.1775,0.6457 )    @border(0.9840,0.6457 ) ( 0.9765,0.6457 )    @border(0.1700,0.6873 ) ( 0.1775,0.6873 )    @border(0.9840,0.6873 ) ( 0.9765,0.6873 )    @border(0.1700,0.6873 ) ( 0.1850,0.6873 )    @border(0.9840,0.6873 ) ( 0.9690,0.6873 )    ( 0.1540,0.6873)0.95 @border(0.1700,0.7288 ) ( 0.1775,0.7288 )    @border(0.9840,0.7288 ) ( 0.9765,0.7288 )    @border(0.1700,0.7704 ) ( 0.1775,0.7704 )    @border(0.9840,0.7704 ) ( 0.9765,0.7704 )",
    "@border(0.1700,0.8119 ) ( 0.1775,0.8119 )    @border(0.9840,0.8119 ) ( 0.9765,0.8119 )    @border(0.1700,0.8535 ) ( 0.1775,0.8535 )    @border(0.9840,0.8535 ) ( 0.9765,0.8535 )    @border(0.1700,0.8950 ) ( 0.1775,0.8950 )    @border(0.9840,0.8950 ) ( 0.9765,0.8950 )    @border(0.1700,0.8950 ) ( 0.1850,0.8950 )    @border(0.9840,0.8950 ) ( 0.9690,0.8950 )    ( 0.1540,0.8950)1 @border(0.1700,0.1680 ) ( 0.1700,0.1780 )    @border(0.1700,0.8950 ) ( 0.1700,0.8850 )    @border(0.1881,0.1680 ) ( 0.1881,0.1780 )    @border(0.1881,0.8950 ) ( 0.1881,0.8850 )    @border(0.2062,0.1680 ) ( 0.2062,0.1780 )    @border(0.2062,0.8950 ) ( 0.2062,0.8850 )    @border(0.2243,0.1680 ) ( 0.2243,0.1780 )    @border(0.2243,0.8950 ) ( 0.2243,0.8850 )    @border(0.2424,0.1680 ) ( 0.2424,0.1780 )    @border(0.2424,0.8950 ) ( 0.2424,0.8850 )",
    "@border(0.2604,0.1680 ) ( 0.2604,0.1780 )    @border(0.2604,0.8950 ) ( 0.2604,0.8850 )    @border(0.2785,0.1680 ) ( 0.2785,0.1780 )    @border(0.2785,0.8950 ) ( 0.2785,0.8850 )    @axes(0.2966,0.1680 ) ( 0.2966,0.8950 )    @border(0.2966,0.1680 ) ( 0.2966,0.1880 )    @border(0.2966,0.8950 ) ( 0.2966,0.8750 )    ( 0.2966,0.1260)8 @border(0.3147,0.1680 ) ( 0.3147,0.1780 )    @border(0.3147,0.8950 ) ( 0.3147,0.8850 )    @border(0.3328,0.1680 ) ( 0.3328,0.1780 )    @border(0.3328,0.8950 ) ( 0.3328,0.8850 )",
    "@border(0.3509,0.1680 ) ( 0.3509,0.1780 )    @border(0.3509,0.8950 ) ( 0.3509,0.8850 )    @border(0.3690,0.1680 ) ( 0.3690,0.1780 )    @border(0.3690,0.8950 ) ( 0.3690,0.8850 )    @border(0.3871,0.1680 ) ( 0.3871,0.1780 )    @border(0.3871,0.8950 ) ( 0.3871,0.8850 )    @border(0.4052,0.1680 ) ( 0.4052,0.1780 )    @border(0.4052,0.8950 ) ( 0.4052,0.8850 )    @border(0.4232,0.1680 ) ( 0.4232,0.1780 )    @border(0.4232,0.8950 ) ( 0.4232,0.8850 )    @axes(0.4413,0.1680 ) ( 0.4413,0.8950 )    @border(0.4413,0.1680 ) ( 0.4413,0.1880 )    @border(0.4413,0.8950 ) ( 0.4413,0.8750 )    ( 0.4413,0.1260)16 @border(0.4594,0.1680 ) ( 0.4594,0.1780 )    @border(0.4594,0.8950 ) ( 0.4594,0.8850 )    @border(0.4775,0.1680 ) ( 0.4775,0.1780 )    @border(0.4775,0.8950 ) ( 0.4775,0.8850 )    @border(0.4956,0.1680 ) ( 0.4956,0.1780 )    @border(0.4956,0.8950 ) ( 0.4956,0.8850 )    @border(0.5137,0.1680 ) ( 0.5137,0.1780 )    @border(0.5137,0.8950 ) ( 0.5137,0.8850 )    @border(0.5318,0.1680 ) ( 0.5318,0.1780 )    @border(0.5318,0.8950 ) ( 0.5318,0.8850 )    @border(0.5499,0.1680 ) ( 0.5499,0.1780 )    @border(0.5499,0.8950 ) ( 0.5499,0.8850 )    @border(0.5680,0.1680 ) ( 0.5680,0.1780 )    @border(0.5680,0.8950 ) ( 0.5680,0.8850 )    @axes(0.5860,0.1680 ) ( 0.5860,0.8950 )    @border(0.5860,0.1680 ) ( 0.5860,0.1880 )    @border(0.5860,0.8950 ) ( 0.5860,0.8750 )    ( 0.5860,0.1260)24 @border(0.6041,0.1680 ) ( 0.6041,0.1780 )    @border(0.6041,0.8950 ) ( 0.6041,0.8850 )    @border(0.6222,0.1680 ) ( 0.6222,0.1780 )    @border(0.6222,0.8950 ) ( 0.6222,0.8850 )    @border(0.6403,0.1680 ) ( 0.6403,0.1780 )    @border(0.6403,0.8950 ) ( 0.6403,0.8850 )    @border(0.6584,0.1680 ) ( 0.6584,0.1780 )    @border(0.6584,0.8950 ) ( 0.6584,0.8850 )    @border(0.6765,0.1680 ) ( 0.6765,0.1780 )    @border(0.6765,0.8950 ) ( 0.6765,0.8850 )    @border(0.6946,0.1680 ) ( 0.6946,0.1780 )    @border(0.6946,0.8950 ) ( 0.6946,0.8850 )    @border(0.7127,0.1680 ) ( 0.7127,0.1780 )    @border(0.7127,0.8950 ) ( 0.7127,0.8850 )    @axes(0.7308,0.1680 ) ( 0.7308,0.8330 )    @axes(0.7308,0.8750 ) ( 0.7308,0.8950 )    @border(0.7308,0.1680 ) ( 0.7308,0.1880 )    @border(0.7308,0.8950 ) ( 0.7308,0.8750 )    ( 0.7308,0.1260)32 @border(0.7488,0.1680 ) ( 0.7488,0.1780 )    @border(0.7488,0.8950 ) ( 0.7488,0.8850 )    @border(0.7669,0.1680 ) ( 0.7669,0.1780 )    @border(0.7669,0.8950 ) ( 0.7669,0.8850 )    @border(0.7850,0.1680 ) ( 0.7850,0.1780 )    @border(0.7850,0.8950 ) ( 0.7850,0.8850 )    @border(0.8031,0.1680 ) ( 0.8031,0.1780 )    @border(0.8031,0.8950 ) ( 0.8031,0.8850 )    @border(0.8212,0.1680 ) ( 0.8212,0.1780 )    @border(0.8212,0.8950 ) ( 0.8212,0.8850 )    @border(0.8393,0.1680 ) ( 0.8393,0.1780 )    @border(0.8393,0.8950 ) ( 0.8393,0.8850 )    @border(0.8574,0.1680 ) ( 0.8574,0.1780 )    @border(0.8574,0.8950 ) ( 0.8574,0.8850 )    @axes(0.8755,0.1680 ) ( 0.8755,0.8330 )",
    "@axes(0.8755,0.8750 ) ( 0.8755,0.8950 )    @border(0.8755,0.1680 ) ( 0.8755,0.1880 )    @border(0.8755,0.8950 ) ( 0.8755,0.8750 )    ( 0.8755,0.1260)40 @border(0.8936,0.1680 ) ( 0.8936,0.1780 )    @border(0.8936,0.8950 ) ( 0.8936,0.8850 )",
    "@border(0.9116,0.1680 ) ( 0.9116,0.1780 )    @border(0.9116,0.8950 ) ( 0.9116,0.8850 )    @border(0.9297,0.1680 ) ( 0.9297,0.1780 )    @border(0.9297,0.8950 ) ( 0.9297,0.8850 )    @border(0.9478,0.1680 ) ( 0.9478,0.1780 )    @border(0.9478,0.8950 ) ( 0.9478,0.8850 )    @border(0.9659,0.1680 ) ( 0.9659,0.1780 )    @border(0.9659,0.8950 ) ( 0.9659,0.8850 )    @border(0.9840,0.1680 ) ( 0.9840,0.1780 )    @border(0.9840,0.8950 ) ( 0.9840,0.8850 )    @border(0.1700,0.1680 ) ( 0.9840,0.1680 ) ( 0.9840,0.8950 ) ( 0.1700,0.8950 ) ( 0.1700,0.1680 )    ( 0.0420,0.5315)@xmath75 ( 0.5770,0.0630)number of processors @xmath25 ( 0.5770,0.9580)efficiency on a 48-processor convex spp1200 ( 0.8570,0.8540)5-d problem @solid(0.8730,0.8540 ) ( 0.9520,0.8540 )    @solid(0.9840,0.5565 ) ( 0.9840,0.5565 ) ( 0.9659,0.6277 ) ( 0.9478,0.4126 ) ( 0.9297,0.4764 ) ( 0.9116,0.5434 ) ( 0.8936,0.6211 ) ( 0.8755,0.4507 ) ( 0.8574,0.5069 ) ( 0.8393,0.5883 ) ( 0.8212,0.6654 ) ( 0.8031,0.5064 ) ( 0.7850,0.5915 ) ( 0.7669,0.6584 ) ( 0.7488,0.5479 ) ( 0.7308,0.6423 ) ( 0.7127,0.5272 ) ( 0.6946,0.6342 ) ( 0.6765,0.5519 ) ( 0.6584,0.6511 ) ( 0.6403,0.5770 ) ( 0.6222,0.6650 ) ( 0.6041,0.6456 ) ( 0.5860,0.6032 ) ( 0.5680,0.5958 ) ( 0.5499,0.6010 ) ( 0.5318,0.6576 ) ( 0.5137,0.6244 ) ( 0.4956,0.6339 ) ( 0.4775,0.6363 ) ( 0.4594,0.6079 ) ( 0.4413,0.6453 ) ( 0.4232,0.6513 ) ( 0.4052,0.6743 ) ( 0.3871,0.7041 ) ( 0.3690,0.7076 ) ( 0.3509,0.6848 ) ( 0.3328,0.7056 ) ( 0.3147,0.7167 ) ( 0.2966,0.7009 ) ( 0.2785,0.7886 ) ( 0.2604,0.8388 ) ( 0.2424,0.8356 ) ( 0.2243,0.8471 ) ( 0.2062,0.8446 ) ( 0.1881,0.8678 ) ( 0.1700,0.8950 )     @=12",
    "we have shown , that for ill - behaved test functions in adaptive mc integrators it is essential to use large sets of sample - points at a time . under these circumstances",
    "a macro - parallelization is not satisfying stringent numerical needs . for the xloops project  @xcite",
    ", we have developed a version of ` vegas ` which does parallelization on a smaller scale and has the same numerical properties as the original one .",
    "for @xmath76 the grain - size of the algorithm becomes a parameter .",
    "the algorithm can be used as a complete drop - in replacement for the common ` vegas ` .",
    "it is currently being used in xloops , where it does the last steps in integrating massive 2-loop feynman diagrams .",
    "a portable implementation in ansi- of the outlined algorithm running on every modern smp - unix ( either featuring pthreads  @xcite , draft  4 pthreads or cps - threads ) can be found at ftp://higgs.physik.uni - mainz.de / pub / pvegas/. hints on how to use it can be found at the same place . using the strutures outlined above",
    ", it should be easy to implement a ` mpivegas ` running on machines with distributed memory .",
    "upon demand , we can provide such a routine , using the mpi message - passing standard  @xcite .",
    "it is a pleasure to thank alexander frink of thep for clarifying discussions about parallelization and his contribution to making the code stable and karl schilcher for making this work possible .",
    "i also wish to thank bas tausk and dirk kreimer of thep as well as markus tacke of our university s computing - center and burkhard dnweg of max - planck - institute for polymer research for stimulating discussions .",
    "this work is supported by the ` graduiertenkolleg elementarteilchenphysik bei hohen und mittleren energien ' at university of mainz .",
    "000 l.  brcher , j.  franzkowski , a.  frink , d.  kreimer : _ introduction to xloops , _ hep - ph/9611378 l.  brcher : _ xloops , a package calculating one- and two - loop diagrams , _ nucl .",
    "instr . and meth .",
    "res . * a 389*. 327 - 332 , ( 1997 ) g.  p.",
    "lepage : _ a new algorithm for adaptive multidimensional integration , _ j. comput . phys . * 27 * , 192 - 203 , ( 1978 ) g.  p.",
    "lepage : _ vegas  an adaptive multi - dimensional integration program , _ publication clns-80/447 , cornell university , 1980 w.  press , s.  teukolsky , w.  vetterling , b.  flannery : _ numerical recipes in c , _ ( second edition ) cambridge university press , 1992 .",
    "a.  de  matteis , s.  pagnutti , _ parallelization of random number generators and long - range correlations , _ numer . math .",
    "* 53 * , 595 - 608 , ( 1988 ) r.  c.  tausworthe : _ random numbers generated by linear recurrence modulo two , _ math",
    "* 19 * , 201 - 209 , ( 1965 ) t.  h.  lewis , w.  h.  payne : _ generalized feedback shift register pseudorandom number algorithm , _ j. of the assoc . for computing machinery * 20 * , 456 - 468 , ( 1973 ) s.  kirkpatrick , e.  p.  stoll : _ a very fast shift - register sequence random number generator ,",
    "_ j. comput",
    "40 * , 517 - 526 , ( 1981 ) i.  dek : _ uniform random number generators for parallel computers , _",
    "parallel computing * 15 * , 155 - 164 , ( 1990 ) f.  schmid , n.  b.  wilding : _ errors in monte carlo simulations using shift register random number generartors _ int .",
    "mod . phys .",
    "* c 6 * , 781 - 787 , ( 1995 ) a.  heuer , b.  dnweg , a.  ferrenberg : _ considerations on correlations in shift register pseudorandom number generators and their removal , _ comput .",
    ". commun . * 103 * , 1 - 9 , ( 1997 ) i.  vattulainen , t.  ala - nissila , k.  kankaala : _ physical tests for random numbers in simulations , _ phys .",
    "* 73 * , 2513 - 2516 , ( 1994 ) p.  d.",
    "coddington : _ analysis of random number generators using monte carlo simulation , _ int .",
    "* c 5 * , 547 - 560 , ( 1994 ) g.  t  hooft , m.  veltman .",
    "_ scalar one - loop integrals .",
    "* b 153 * , 365 , ( 1979 ) b.  nichols , d.  buttlar , j.  proulx farrell : _ pthreads programming , _",
    "oreilly , sebastopol , ( 1996 ) university of tennessee , knoxville , tennessee , ( 1995 )"
  ],
  "abstract_text": [
    "<S> monte carlo ( mc ) methods for numerical integration seem to be embarassingly parallel on first sight . </S>",
    "<S> when adaptive schemes are applied in order to enhance convergence however , the seemingly most natural way of replicating the whole job on each processor can potentially ruin the adaptive behaviour . using the popular vegas - algorithm as an example </S>",
    "<S> an economic method of semi - micro parallelization with variable grain - size is presented and contrasted with another straightforward approach of macro - parallelization . </S>",
    "<S> a portable implementation of this semi - micro parallelization is used in the xloops - project and is made publicly available .    </S>",
    "<S> * keywords : * parallel computing , grain - size , monte carlo integration , tausworthe , gfsr . </S>"
  ]
}