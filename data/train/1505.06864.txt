{
  "article_text": [
    "the finite - difference discretization scheme for the multidimensional second order partial differential equations ( pdes ) leads to a system with the block - tridiagonal matrix .",
    "a precision of the numerical solution of the pde depends on the number of knots over each dimension that usually approches thousands or even tens of thousands . in this case , each block of the matrix becomes large and sparse , but the block - tridiagonal structure is kept .",
    "a combination of the finite - difference method over one coordinate and the basis set methods  @xcite over other coordinates generally produces block - tridiagonal matrix with dense blocks .",
    "the intermediate case of the basis set with local support , namely splines  @xcite , results in the band blocks with bandwidth comparable with the number of bands appeared from the finite - difference method . an adaptation of the gauss elimination  @xcite to the block - tridiagonal systems is known as the thomas algorithm ( ta )  @xcite which is also called as the matrix sweeping algorithm  @xcite . in this algorithm ,",
    "the idea of gauss elimination is applied to the blocks themselves . a stable and time proved realization of another gauss elimination based techinque , lu decomposition , for general matrices is available in packages like lapack  @xcite .",
    "the direct application of this realization of the lu decomposition is often not feasible because of the large size of the matrix . as a result ,",
    "the ta and special variants of lu decomposition are appropriate techniques for such large problems .",
    "the ta is robust and quite fast , but serial and thus hardly parallelized .",
    "the only available parallelization is at the level of operations with matrices .",
    "this reason does not allow one to use the algorithm at the modern computational facilities efficiently . in order to enable parallel and much faster solution ,",
    "we have developed the decomposition method ( dm ) for the block - tridiagonal matrix systems .",
    "the idea of the method consists in rearranging the initial matrix into equivalent one , namely with the `` arrowhead '' structure , which allows a parallel solution .",
    "the matrix is logically reduced to some new diagonal independent blocks , sparse auxiliary ones along the right and bottom sides , and a coupling supplementary block .",
    "the solution includes parallel inversion of the independent blocks and solving the supplementary problem .",
    "the dm includes using the ta for dealing with the new blocks of the initial matrix , but the parallel structure and possible recursivity of the method lead to remarkable growth of the performance .",
    "the speedup of the dm depends on the size of the supplementary matrix system . by default , this system is solved sequentially by the ta . if its size is relatively small , then the dm gives linear growth of the performance with increase of the number of computing units ( processors ) .",
    "as the nonparallelized part of the method steadily enlarges , the linear growth is slowed down . the speedup with respect to the ta achieves its maximum at some number of computing units and then decreases .",
    "this issue can specifically be overcome by applying the dm recursively to each independent block and to the supplementary problem with the coupling matrix . in this case , the maximum computational speedup with respect to the ta can be increased by several times in comparison to the nonrecursive application of the dm .",
    "the concept of rearranging the initial matrix into `` arrowhead '' form has already been proposed for ordinary tridiagonal matrices .",
    "the method comes from the domain decomposition , where the idea to divide a large problem into small ones which can be solved independently was introduced , see ref .",
    "@xcite and references therein . in ref .",
    "@xcite this idea is illustrated by the example of tridiagonal matrix obtained from the finite - difference discretization of the one - dimensional laplace operator .",
    "it is stated there that the matrix of the supplementary problem in this method is the smallest one among the similar block methods under consideration .",
    "therefore , the method is supposed to be the most efficient . moreover , it is close to the cyclic reduction method  @xcite and , as it is shown in ref .",
    "@xcite , is asymptotically equivalent to that method .",
    "the dm is closely related to the so - called divide - and - conquer method implemented in scalapack for banded matrices  @xcite , but designed directly to the block - tridiagonal matrix systems .    in this paper",
    ", we describe the dm for the block - tridiagonal matrix system in detail .",
    "we show how one should solve the obtained new rearranged system and provide the algorithm for it .",
    "we underline in the text that the steps of the proposed algorithm can be executed in parallel .",
    "the analysis of the number of multiplicative operations for the dm is given .",
    "we analytically estimate the ratio of the multiplicative operations for the ta to the same quantity for different cases of application of the dm .",
    "this ratio is directly related to the computational speedup and thus we estimate the performance of the dm with respect to the ta . as a validation of the analytical results , we performed tests of the dm at the parallel supercomputing facility which confirmed our estimations .",
    "the block - tridiagonal matrix systems arise in many applied mathematics and physics problems .",
    "we are unable to enumerate here all possible applications , but briefly mention a few ones related to the quantum few - body systems .",
    "the three - body scattering problem in configuration space has been firstly reduced to the boundary value problem in ref .",
    "@xcite where the numerical technique for the solution has been also described .",
    "this pioneering work gave rise to a class of papers inheriting the common approach for the numerical solution of the similar problems  @xcite .",
    "in the present paper , as an example of the application of the dm , we describe the computational scheme for solution of the @xmath0-wave faddeev integro - differential equations and compare its performance with the ta or the dm .",
    "the paper is organized as follows . in section  2 , we describe in detail the dm and provide the algorithm for its implementation . we give a brief description of the conventional ta in section  3 .",
    "the analytical estimation of the number of multiplicative operations of the dm is presented in section  4 . in section  5",
    ", the computational speedup of the dm with respect to the ta for various types of applications is given .",
    "the desired parameters of the computational system for reaching the maximum speedup are also provided .",
    "the validation of the analytical results is described in section  6 . in section  7 , we describe a possible application of the dm for solving the @xmath0-wave faddeev integro - differential equation and give achieved time reduction .",
    "section  8 summarizes the article by providing the conclusion .",
    "[ cols=\"^ \" , ]     the number of multiplicative operations for each stage of the dm is summarized in tab .",
    "[ tab2 ] . for solving one independent subsystem with different vectors in rhs using the ta , at the forward stage one calculates the auxiliary blocks @xmath1 for @xmath2 and @xmath3 for @xmath4 . here",
    "@xmath5 is the number of blocks on the diagonal of the @xmath6-th subsystem .",
    "it takes @xmath7 multiplicative operations .",
    "these calculations are general for both equations in  ( [ e231 ] ) .",
    "the auxiliary blocks @xmath8 are calculated for each rhs separately .",
    "since the second equation in  ( [ e231 ] ) has the complete rhs , it takes @xmath9 operations to compute all @xmath8 .",
    "the sparse structure of the rhs of the first equation in  ( [ e231 ] ) leads to @xmath10 or @xmath11 multiplicative operations for the case when the vector in rhs has only the top nonzero block or only bottom nonzero block , respectively . at the backward stage ,",
    "the number of multiplicative operations is @xmath12 and @xmath13 for the first and second equation in  ( [ e231 ] ) . in total , the first subsystem ( @xmath14 ) takes @xmath15 multiplicative operations , the last one ( @xmath16 ) takes @xmath17 , the intermediate ones @xmath18 take @xmath19 .",
    "the calculation of the products @xmath20 and @xmath21 takes @xmath22 and @xmath23 multiplicative operations for @xmath24 , whereas for @xmath25 the products take only @xmath11 and @xmath26 multiplicative operations .",
    "the construction of matrices  ( [ e2311 ] ) includes only additive operations and , therefore , their contribution is ignored .",
    "the solution of the supplementary matrix system  ( [ e232 ] ) of the block size @xmath27 takes @xmath28 multiplicative operations .",
    "the last stage , namely obtaining the remaining unknown vector  ( [ endsolution ] ) from the solution of the supplementary matrix system , takes @xmath29 and @xmath30 multiplicative operations for @xmath24 and @xmath25 , respectively .    as a result",
    ", the total number of multiplicative operations of the dm is given as @xmath31 where the last term corresponds to operations needed to solve supplementary matrix equation  ( [ e232 ] ) and @xmath5 is the size of @xmath6-th subsystem .",
    "it is worth noting that @xmath5 can be different for different @xmath6 and satisfies the only restriction @xmath32      the recursive call of the dm implies using the dm also for solving the supplementary matrix equation  ( [ e232 ] ) and the independent subsystems  ( [ e231 ] ) .",
    "we will firstly consider calling the dm for solving the supplementary matrix equation and secondly for solution of the independent subsystems .",
    "after that , the formulas for the case of calling the dm both for solution of the independent subsystems and the supplementary matrix equation will be derived .",
    "the recursive application of the dm for solving the supplementary matrix equation of size @xmath27 , constructed on matrix @xmath33  ( [ e232 ] ) , leads to the change of the number of multiplicative operations in the general scheme , see tab .",
    "in particular , if we divide the supplementary matrix equation into @xmath34 subsystems , then the last term in eq .",
    "( [ dmtotal ] ) becomes @xmath35 where @xmath36 is the size of @xmath37-th subsystem of the matrix equation  ( [ e232 ] ) .",
    "if the subsystems are of equal size , then @xmath38 for @xmath39 .      the solution of the independent subsystems in eq .",
    "( [ e231 ] ) can be done by calling the dm for each such subsystem . if we divide @xmath6-th subsystem ( @xmath40 ) into @xmath41 subsubsystems , then for solving eq .",
    "( [ e231 ] ) for @xmath14 the number of multiplicative operations equals to @xmath42 + \\left ( 4j_{1}-6 \\right ) n^{3 } + \\left ( 3j_{1}-5 \\right ) n^{2}l \\end{array }   \\label{dmrec121}\\ ] ] and for @xmath16 @xmath43 + \\left ( 6j_{m}-10 \\right ) n^{3 } + \\left ( 3j_{m}-5 \\right ) n^{2}l , \\end{array }   \\label{dmrec1212}\\ ] ] where @xmath44 is the block - size of the @xmath45-th subsubsystem for the @xmath6-th subsystem . for @xmath24",
    "the analogous number is @xmath46 + \\left ( 7j_{k}-11 \\right ) n^{3 } + \\left ( 3j_{k}-5 \\right ) n^{2}l . \\end{array }   \\label{dmrec122}\\ ] ] in order to calculate the total number of multiplicative operations one needs to add multiplications for calculations of @xmath47 and @xmath48 , multiplications in eq .",
    "( [ endsolution ] ) , sum up it over @xmath6 , and to add multiplications for solution of eq .",
    "( [ e232 ] ) .",
    "in order to estimate the computational speedup of the dm with respect to the ta , one needs to study the relation of overall times of calculations for dm and for the ta . since the dm can be executed in parallel on @xmath49 processors , in addition to the time of serial calculations , the overall time includes the largest time of computation among all parallel processors .",
    "the overall computation time is directly related to the number of total serial multiplicative operations . therefore , to estimate the computational speedup , we evaluate the ratio of the serial multiplicative operations for the ta and for the dm @xmath50 below , we will firstly study the computational speedup of the dm without recursivity and after that with it .",
    "the standard usage of the dm includes the ta for solution of the equations for independent subsystem as well as for supplementary problem constructed with coupling matrix @xmath33 .",
    "let us consider following cases :    * the first one is when the number of subsystems equals to the number of parallel processors @xmath51 and all subsystems are of equal size @xmath52 for @xmath40 . in this case",
    "@xmath53 and the computational speedup @xmath54 is given by @xmath55 so , the computational speedup increases linearly @xmath56 with respect to number of processes @xmath49 for @xmath57 .",
    "the computational speedup depends only on the ratio @xmath58 .",
    "this dependence as well as linear growth of the speedup are shown in fig .",
    "[ speedup1 ] for @xmath59 and @xmath60 .",
    "one can see that the speedup slightly increases if number of vectors @xmath61 in rhs approaches @xmath62 .",
    "moreover , it is clearly seen from the plot , that the speedup decreases when the number of processors ( and the logical subsystems ) becomes large .",
    "the reason for this effect is the growth of the serial part of the dm , i.e. of the supplementary matrix system .",
    "the maximum computational speedup is achieved for the number of processors @xmath63 which is close to @xmath64 for @xmath65 . *",
    "the second case is when the number of subsystems equals to the number of processors @xmath51 and subsystems are not of equal size @xmath5 for @xmath40 . since the first and last processors perform less operations than other ones , it is optimal to define the equal time for each subsystem by varying @xmath5 for @xmath40 .",
    "let us suppose the sizes of the first and last subsystems to be increased in @xmath66 and @xmath67 times , respectively .",
    "then , the equal sizes of @xmath6th subsystems for @xmath18 is @xmath68 the computational speedup in this case is defined as @xmath69 one can easily derive from tab .",
    "[ tab2 ] that @xmath70 $ ] and @xmath71 $ ] for @xmath72 .",
    "therefore , for @xmath73 and @xmath65 , one approximately obtaines @xmath74 and @xmath75 .",
    "[ speedup2 ] shows that the computational speedup for @xmath65 increases slightly if one defines larger first and last blocks and simultaneously equal computation time for each parallel processor . for @xmath76",
    ", the similar almost negligible difference is kept . *",
    "the last case is when the number of subsystems is proportional to the number of processors @xmath77 for @xmath78 and subsystems are of the same size as in the previous case . in this case",
    ", each processor is feeded by a queue of @xmath45 subsystems which are executed sequentially .",
    "changing eq .",
    "( [ speedupmpnk ] ) allows one to estimate the speedup in this case : @xmath79 the additional factor @xmath45 in front of brackets in the denominator comes from the fact that the queue for each processor consists of @xmath45 subsystems . the result for @xmath80",
    "is shown in fig .",
    "[ speedup3 ] .",
    "it is clearly seen that the maximum speedup is diminished with increase of @xmath45 .",
    "nevertheless , for small number of processors the linear growth of speedup is kept independently of @xmath45 . for larger @xmath49 ,",
    "the serial part of the dm drastically increases , especially for larger @xmath45 .",
    "this fact leads to the considerable decrease of the speedup .",
    "the studied cases bring us to the conception for application of the dm .",
    "one should follow the first or second considered case : subsystems should be of equal size or to have equal execution time .",
    "for both cases the number of subsystems should be equal to number of processors .",
    "the equal execution time is achieved by choosing the size of the first and last subsystems to be about @xmath81 and @xmath82 of the size of each other subsystem . since the difference between described cases is almost negligible , if the size of each subsystem is large , the initial matrix can be simply divided to equal subsystems . the computational speedup with respect to the sequential ta",
    "is shown in fig .",
    "[ speedup1 ] .",
    "we will firstly consider calling the dm for solving only the supplementary matrix equation and secondly calling it for solution of the independent subsystems and thirdly both for solution of the independent subsystems and the supplementary matrix equation .",
    "based on the previous subsections and particularly eq .",
    "( [ dmrec1 ] ) , we now additionally assume that all subsystems @xmath40 are of equal size @xmath83 . as we have seen in previous sections ,",
    "this assumption does not significantly affect the performance .",
    "if we divide the supplementary matrix equation  ( [ e232 ] ) into @xmath34 independent equal subsystems to apply on each of them the dm , then the number of serial multiplicative operations is given as @xmath84 where @xmath38 for @xmath39 is the size of each subsystem . as a result , for @xmath85 ,",
    "the following cases can be considered :    * the first case is if @xmath86 and @xmath49 are kept fixed .",
    "then , the minimum of eq .",
    "( [ dmrec2 ] ) ( and the maximum of the computational speedup ) is achieved for @xmath87 besides , to apply the dm the condition @xmath88 is required .",
    "it is satisfied as @xmath89 .",
    "if @xmath90 , then eq .  ( [ suppmatrm ] ) does not define the maximum of the speedup , instead it is achieved at the boundary @xmath91 . as @xmath65 , eq .",
    "( [ suppmatrm ] ) is reduced to @xmath92 and the computational speedup @xmath54 is given as @xmath93 fig .",
    "[ speeduprec13 ] shows the considerable increase of the speedup in case of the recursive call of the dm for the supplementary matrix equation . *",
    "the second case is if only @xmath86 is fixed .",
    "this case corresponds to the situation when the number of processors , @xmath49 , is arbitrary and one has to choose the appropriate number to reach maximum speedup .",
    "one can find parameters of the extremum of eq .",
    "( [ dmrec2 ] ) and show that it is a minimum using the matrix of the second derivatives . in this case , the minimum of eq .",
    "( [ dmrec2 ] ) is achieved for @xmath94 { \\frac{(n+1)^{2}}{3 } \\left ( 5+\\frac{2}{1+l / n } \\right ) } , \\quad m=\\frac{p^{2}}{n+1}.\\ ] ] since @xmath95 , the parameters  ( [ suppmatrpm ] ) give minimum if @xmath96 , which is usually satisfied .",
    "the computational speedup , calculated for @xmath60 , as a function of @xmath49 and @xmath34 is represented in fig .",
    "[ speeduprec2 ] .",
    "the contour plot shows that the unique maximum ( red color ) of the speedup exists and is achieved by the given formulas  ( [ suppmatrpm ] ) .    to summarize the considered cases ,",
    "it should be pointed out that the recursive call of the dm for the supplementary matrix equation leads to remarkable growth of the performance , see fig .",
    "[ speeduprec13 ] . using the described cases",
    ", one can choose the parameters of the algorithm based on its own computational facilities and the parameters of the initial matrix in order to achieve the maximum performance , see fig .",
    "[ speeduprec2 ] .    , needed for the supplementary matrix system and number of processors , @xmath49 , for solution of the initial matrix by the dm .",
    "the maximum of computational speedup is shown by red color .",
    "the bottom dashed area indicates the domain where the condition @xmath95 is not satisfied.,scaledwidth=50.0% ]      now we consider parallelization of the solution of each of the @xmath97 independent subsystems in eq .",
    "( [ e231 ] ) . solving the supplementary matrix equation  ( [ e232 ] ) remains to be serial .",
    "we apply the dm for each equal independent subsystem , namely divide each subsystem with @xmath98 blocks on the diagonal into @xmath99 independent equal subsubsystems .",
    "( one can see that nonequal subsubsystems lead to the considerable reduce of the performance . ) then , the total number of used processors is @xmath100 and the number of serial multiplicative operations is given as @xmath101 where @xmath102 and @xmath103 . in the case when @xmath49 ( @xmath96 ) is kept fixed , using the method of lagrange multipliers  @xcite one can obtain that the maximum speedup is achieved for the values of parameters @xmath104 the resulted computational speedup for @xmath65 is shown in fig .",
    "[ speeduprec5 ] .",
    "it is clearly seen that for small @xmath49 the speedup behaves as @xmath105 . for larger @xmath49",
    ", the speedup reaches its maximum and exceeds that obtained when the dm is recursively called for the supplementary matrix equation .",
    "one can also analytically obtain the maximum speedup for the case when @xmath49 is arbitrary .",
    "it is achieved for the parameter @xmath106 and @xmath99 obtained as a real positive solution of the quartic equation @xmath107 however , in practice , it is much simpler to solve eq .",
    "( [ p3dmrec3 ] ) numerically if the size of the block , @xmath62 , number of rhs vectors , @xmath61 , and @xmath86 are known .",
    "an example of reached speedup for the case when @xmath59 , @xmath108 , @xmath60 as a function of @xmath97 and @xmath99 is shown in fig .",
    "[ speeduprec4 ] .",
    "the dashed area corresponds to the domain where the condition @xmath96 is not hold and the parallel solution of the independent subsystems is not possible .",
    ", the initial matrix is divided into and the number of subsubsystems , @xmath99 , each subsystem is divided into .",
    "the total number of parallel processors involved in computation is @xmath100 .",
    "the maximum of computational speedup is shown by the dark red color .",
    "the dashed area corresponds to the domain where the condition @xmath96 is not hold.,scaledwidth=50.0% ]      if we apply the dm to both the independent subsystems and the supplementary matrix equation , then the number of serial multiplicative operations is the combination of eq .",
    "( [ dmrec2 ] ) and eq .",
    "( [ dmrec3 ] ) . keeping the notation of the previous case and assuming also that @xmath100 , the number of serial multiplicative operations is given as @xmath109 where @xmath102 , @xmath103 and @xmath34 is the number of independent subsystems to which the supplementary matrix equation is divided into .    keeping @xmath49 ( @xmath96 ) to be fixed",
    ", one can obtain the maximum computational speedup if the parameters are @xmath110{3 \\left ( 5+\\frac{2}{1+l / n } \\right ) } } \\left ( 3p+\\frac{4p}{1+l / n}+\\frac{2l(n+1)}{n+l } \\right)^{2/3 } , \\\\ m = \\sqrt{\\frac{m ( 7n^{3}+5n^{2}l)}{3(n^{3}+n^{2}l ) } } = \\frac{1}{\\sqrt[3]{9 } } \\left ( 3p+\\frac{4p}{1+l / n}+\\frac{2l(n+1)}{n+l } \\right)^{1/3 } \\left ( 5+\\frac{2}{1+l / n } \\right)^{1/3 } \\end{split } \\label{p1dmrec4}\\ ] ] and obviously @xmath111 .",
    "the comparison of the computational speedup for this case with previous cases is shown in fig .",
    "[ speeduprec5 ] .",
    "it is clearly seen that the computational speedup in this case is larger than in previous cases .",
    "unfortunately , this large speedup is achieved only for large total number of involved parallel processors , @xmath49 , that is practically not feasible .",
    "if the total number of involved processors @xmath49 is free , then the maximum computational speedup is achieved for values of the parameters @xmath112 and @xmath99 is the real positive solution of the cubic equation @xmath113 in fig .  [ speeduprec5 ] , the parameters of the analytical maximum provided by formulas  ( [ p2dmrec4],[p3dmrec4 ] ) are @xmath114 , @xmath115 , @xmath116 .",
    "described analytical estimations have been validated using the smp system with 64 processors ( intel xeon cpu x7560 2.27ghz ) and 2 tb of shared operative memory .",
    "the red hat enterprise linux server 6.2 and gcc 4.4.6 ( 20110731 ) compiler are installed in the system .",
    "the dm has been implemented as an independent program written in c with calls of the corresponding lapack 3.5.0 fortran subroutines  @xcite .",
    "the parallelization has been done using the openmp 3.0  @xcite .    in validation studies of the computational speedup  ( [ speedupmp ] ) only the solution time has been taken into account .",
    "this time includes time needed for computation itself as well as time needed for possible memory management during the solution .",
    "the time for generation of the initial blocks is ignored .",
    "the initial blocks have been generated by the well known discretization of the two dimensional laplace operator in the faddeev equations ( see section 6 ) with filling of zero elements by relatively small random values .",
    "this procedure guarantees that the generated matrices are well - posed .",
    "we experimentally estimated the computational speedup for the nonrecursive case when each subsystem  ( [ e231 ] ) has an equal size . in fig .",
    "[ speedupvalidation1 ] , we took the size of a block to be @xmath117 and we show the maximum speedup obtained in a series of experiments with fixed number of processors .",
    "it should be noted that the achieved speedup was equal or less than the analytical one .",
    "the averaged value of the experimentally obtained speedups is smaller than the shown values by @xmath118 .",
    "this difference may be attributed to the nonideality of the memory management of the computational system , additional system processes running simultaneously and affecting our task , and hyper - threading of the processors .",
    "these issues are clearly seen for the case when the number of parallel processors is @xmath119 and especially @xmath120 . for these cases ,",
    "the maximum achieved speedup is considerably less than the analytical one .",
    "nevertheless , the general trend of the analytical result for the equal subsystems is clearly confirmed in the validation studies .",
    "the practical behavior of the computational speedup for other cases can also be observed in the computational systems with more processors .",
    "we apply the dm to the numerical solution of the boundary value problem ( bvp ) arised from the three - body scattering problem .",
    "one of the rigorous approaches for treating the three - body scattering problems above the breakup threshold is based on the configuration space faddeev formalism  @xcite .",
    "it reduces the scattering problem to the bvp by implementing appropriate boundary conditions .",
    "the boundary conditions have been introduced by s.  p.  merkuriev  @xcite and their new representation has recently been constructed in ref .  @xcite .",
    "after discretization of the bvp at some grid we come to the matrix equation of interest and then apply the dm .",
    "below , we will describe key points of the neutron - deuteron ( @xmath121 ) scattering problem , computational scheme , and provide some results .",
    "the neutron - deuteron system under consideration is described by the differential faddeev equations  @xcite .",
    "the @xmath0-wave equations for the radial part of the faddeev wave function component appear after projection onto the states with zero orbital momentum in all pairs of the three - body system . these @xmath0-wave integro - differential faddeev equations for the intrinsic angular momentum @xmath122 in the cartesian coordinates are given by one equation  @xcite @xmath123 where @xmath124 and @xmath125 .",
    "the solution of the @xmath0-wave faddeev equation  ( [ e020 ] ) for the energy of the system above the breakup threshold ( @xmath126 ) and for the short - range two - body potential @xmath127 should satisfy the boundary condition  @xcite @xmath128 where @xmath129 , @xmath130 , as @xmath131 .",
    "the conditions @xmath132 guarantee the regularity of the solution at zero .",
    "the energy and the relative momentum of the neutron , @xmath133 , are associated with the energy of the two - body ground state , defined by the schrdinger equation @xmath134 by the formula @xmath135 .",
    "the functions @xmath136 and @xmath137 to be determined are the binary amplitude and the faddeev component of the breakup amplitude , respectively .",
    "the integral representations for these functions are of the form @xcite @xmath138 and @xmath139 where @xmath140 is the scattering two - body wave function @xmath141 and @xmath142      the solution of the bvp with equation  ( [ e020 ] ) for the ground and excited states as well as for scattering have been performed by various authors  @xcite .",
    "the solution method is based on expanding the solution in a basis of the hermite splines @xmath143 over @xmath144 and @xmath145 @xmath146 and allows one to calculate binding energies and faddeev components . for scattering , the amplitudes",
    "are then reconstructed from the faddeev components using the integral representations  ( [ e041a ] ,  [ e041b ] ) .",
    "the matrix system is obtained after the discretization of the equation  ( [ e020 ] ) at the two - dimensional grid .",
    "in addition to the difficult treatment of the boundary conditions  ( [ e040c ] ) in case of scattering , this approach has a computational disadvantage consisting in the relatively irregular and nonband structure of the matrix of the resulted system of linear equations .",
    "the block - tridiagonal structure does not arise .",
    "another approach is to rewrite the equations in hyperspherical coordinates  @xcite .",
    "this approach was firstly introduced in ref .",
    "@xcite and has been used by a number of authors  @xcite .",
    "it allows one to obtain the system of linear equations with a block - tridiagonal matrix .",
    "it is more appropriate for establishing the boundary conditions  ( [ e040c ] ) as well .",
    "taking into account the change of the unknown function as @xmath147 , the transformation to the hyperspherical ( polar ) coordinates @xmath148 leads to the following equation : @xmath149 the integration limits are defined , in turn , as @xmath150 and @xmath151 .",
    "the boundary condition  ( [ e040c ] ) can be represented as  @xcite @xmath152 where @xmath153 and @xmath154 are expressed by @xmath155 through the bessel functions @xmath156 and the hankel functions of the first kind @xmath157  @xcite .",
    "the unknown faddeev component of the breakup amplitude is expressed in this case as the expansion @xmath158 whereas the binary amplitude is given by @xmath136 . here , the functions @xmath159 form the basis generated by the eigenvalue problem for operator of the two - body subsystem .",
    "the procedure for extraction of the coefficients @xmath160 of the presented expansion is given in ref .",
    "@xcite .    the two - dimensional bvp defined by eq .",
    "( [ e050a ] ) and boundary condition  ( [ e070a ] ) is solved in the hyperspherical coordinates @xmath161 due to proper description of the boundary conditions and appropriate representation of the operator of two - body subsystem in eq .",
    "( [ e050a ] ) .",
    "therefore , the computational scheme meets the requirements of a good representation of the @xmath162-dependent operator . as a result , at @xmath163 intervals over the @xmath162-coordinate",
    "the unknown solution is expanded in the basis of cubic hermite splines @xmath164",
    "@xcite @xmath165 whereas over the @xmath166-coordinate the finite - difference scheme has been taken .",
    "at the unit interval @xmath167 $ ] , the cubic hermite splines are defined by four formulas @xmath168 the splines are shown in fig .",
    "[ fig10 ] .",
    "they transferred by the linear transformations to the two consecutive intervals of @xmath162-grid .",
    "parameterizing these intervals by @xmath169 $ ] , the splines can be written in the following way : @xmath170 \\\\           \\end{array}\\right.,\\\\ h^{2}(t)&=&\\left\\ { \\begin{array}{r@{}ll }   & t^{3}+2t^{2}+t , & t\\in[-1,0 ) \\\\                      & t^{3}-2t^{2}+t , & t\\in[0,1 ] \\\\",
    "\\end{array}\\right .. \\end{aligned}\\ ] ]    in order to obtain the appropriate @xmath162-grid , the specially chosen nonequidistant @xmath144-grid for operator of the two - body subsystem has been used and transformed by the relation @xmath171.\\ ] ] here , the parameter @xmath172 defines the @xmath144-coordinate of the right zero boundary condition for some @xmath166 .",
    "the @xmath144-grid consists of two parts : the fixed one at small @xmath144 and the stretchable one at larger @xmath144 up to @xmath172 . as @xmath166 increases",
    ", @xmath172 grows and the obtained @xmath162-grid becomes more dense near @xmath173 .",
    "the quality of the @xmath144-grid and consequently of the @xmath162-grid has been estimated by a precision of the ground state eigenvalue of the two - body hamiltonian  ( [ e0900 ] ) .",
    "the spline - expansion of the solution requires using as many as twice of numbers of coefficients in the expansion .",
    "the orthogonal collocation method  @xcite with two gauss knots within one interval is used for discretization of the studied equation . over the equidistant @xmath166-grid with the mesh parameter",
    "@xmath174 the second partial derivative of the equation ( [ e050a ] ) is approximated by the second order finite - difference formula @xmath175 this approximation generates the block - tridiagonal structure for the matrix of the linear system , which can be written at the grid @xmath176 as follows : @xmath177 = 0 \\ ] ] here , the index @xmath178 denotes the number of arc @xmath179 on which the grid @xmath176 is constructed .",
    "as one can see from eq .",
    "( [ e290 ] ) , the resulted system of linear equations has a block - tridiagonal matrix .",
    "moreover , since the inital equation is integro - differential , the diagonal blocks of the matrix are dense , whereas the offdiagonal ones are banded .",
    "the rhs supervector of the system is zero unless the last block - row consisting of terms of the boundary condition  ( [ e070a ] ) at the last arc of the hyper - radius .",
    "the proposed numerical method guarantees the precision of the obtained solution of order of @xmath180 at equidistant grid over @xmath166 and of order of @xmath181 if one designes the equidistant grid over @xmath162 .",
    "the precision over coordinate @xmath166 can be improved up to @xmath182 if one applies the numerov method  @xcite for the given problem , that holds the block - tridiagonal structure of the matrix .",
    "the increase of the accuracy over coordinate @xmath162 can be achieved by employing the quintic hermite splines @xmath183 the downside of the quintic hermite splines is that the number of collocation point in this case should be increased in more than one and half times comparing to the case of qubic ones .",
    "the calculations have been carried out for various laboratory frame energies , @xmath184  mev , using the mt  slowromancap1@-slowromancap3@ potential  @xcite for description of the two - body subsystem  ( [ e0900 ] ) .",
    "for this potential , the achieved value of two - body ground state energy is @xmath185  mev .",
    "the nonequidistant @xmath162-grid with about 500 intervals used for the precise calculations .",
    "since the orthogonal collocation method  @xcite with two gauss knots within one interval is used , the common size for a matrix of the two - body operator  ( [ e0900 ] ) is about 1000 .",
    "a grid mesh for the uniform @xmath166-grid was varied from @xmath186  fm to @xmath187  fm .    within the asymptotic approach ,",
    "the bvps consisting of the equation  ( [ e050a ] ) and the boundary condition  ( [ e070a ] ) taken at the hyper - radius @xmath188 have been solved .",
    "the expansion coefficients @xmath189 as functions of @xmath190 have been calculated and used for reconstructing of the faddeev component of the breakup amplitude @xmath191 the prelimiting breakup amplitude , @xmath192 , for @xmath193  mev at some finite value of @xmath190 is shown in fig .",
    "[ fig3040 ] ( left panel ) .",
    "the breakup amplitude @xmath194 as @xmath195 for the same energy is presented in fig .",
    "[ fig3040 ] ( right panel ) . the convergence to the limit is explicitly guaranteed by properties of the functions @xmath159 . the limiting forms of these functions as @xmath195 are explicitly known for @xmath196 $ ] : @xmath197 therefore , in contrast to the prelimiting case , the smooth behavior of the breakup amplitude near 90 degrees is observed , see the mentioned figures .",
    "c     for @xmath193  mev and @xmath198  fm .",
    "right panel : the breakup amplitude @xmath194 for @xmath193  mev and @xmath199  fm.,scaledwidth=100.0% ]     for @xmath193  mev and @xmath198  fm .",
    "right panel : the breakup amplitude @xmath194 for @xmath193  mev and @xmath199  fm.,scaledwidth=100.0% ]    the convergence of the binary amplitude @xmath200 and breakup amplitude @xmath194 as @xmath201 has been obtained .",
    "for example , the @xmath190-dependence of the inelasticity coefficient , @xmath202 , and the phase shift , @xmath203 , defined as @xmath204 are presented in fig .",
    "[ fig50 ] .",
    "the top panel shows that the calculated values of the inelasticity are identical for the three different precisions of the computations .",
    "the bottom panel shows that the decrease of the mesh step @xmath205 for the @xmath166-grid to @xmath187  fm leads to oscillating but significantly less biased values of the phase shift as @xmath190 increases .",
    "the calculations using the numerov method  @xcite confirm the finite - difference results .",
    "the oscillations are vanishing as @xmath201 and the limiting value of the amplitude can be obtained by extrapolation .",
    "nevertheless , in order to reach relatively small oscillations it is necessary to achieve values of @xmath206  fm .    )",
    "@xmath202 ( top panel ) , phase shift @xmath203 ( bottom panel ) for @xmath193  mev as functions of @xmath190 .",
    "the dashed lines represent the values obtained using the second - order finite - differences  ( [ e20100 ] ) at the @xmath166-grid with mesh @xmath207 and @xmath208 , whereas the solid line shows the values obtained using the numerov method  @xcite with @xmath209 .",
    "the obtained inelasticity coefficients coincide with a good precision .",
    ", scaledwidth=80.0% ]    the obtained values of the binary amplitude for different laboratory frame energies are in a good agreement with the binary amplitudes in ref .",
    "the obtained breakup amplitude @xmath194 was compared with the results in ref .",
    "this comparison for @xmath193  mev is presented in fig .",
    "[ fig60 ] .",
    "the results are in good agreement for values of @xmath210 , whereas for @xmath211 the differences are observed .",
    "the typical overall time needed for solution includes the time for construction of the matrix system , for solving it , and for calculating the scattering amplitudes . in practical calculations ,",
    "the first time was comparable with the second one .",
    "the asymptotic approach requires only the last row of the solution of the matrix system , so the amplitudes are calculated relatively fast . for example , the overall time of the solution for the number of blocks on the diagonal @xmath212 with the ta is about 11 hours .",
    "the application of the dm with @xmath120 computing units allowed us to reduce the solution time from 11 hours to about one hour , i.e. the overall time reduced by a factor of 10 . for larger @xmath86 and for the method using the integral representation ,",
    "the overall speedup was not so large , but comparable .",
    "the obtained growth of the performance by a factor of up to ten was practically achieved for various computational studies of the given physical problem .     for @xmath193  mev .",
    "the results of ref .",
    "@xcite are also shown for comparison.,scaledwidth=80.0% ]",
    "the dm for parallel solution of the block - tridiagonal matrix systems has been described in detail and the features of the method increasing the performance have been given .",
    "we have shown that rearranging the initial matrix system into equivalent one with the `` arrowhead '' structure of the new matrix allows one its parallel solving .",
    "the new matrix consists of diagonal independent blocks , sparse auxiliary ones along the right and bottom sides , and the coupling supplementary block of smaller size .",
    "the analytical estimation of the number of multiplicative operations of the method and computational speedup with respect to the serial ta has been performed .",
    "we studied the standard nonrecursive application of the dm as well as the recursive one .",
    "in recursive application , the dm is applied to the initial matrix system and , then , to the obtained independent subsystems and the supplementary matrix equation .",
    "the various cases arised in practice have been considered .",
    "the maximum computational speedup and the parameters providing the achieved speedup have been analytically obtained .",
    "the recursive application of the method allowed us to considerably increase the speedup in comparison to the standard nonrecursive use of the dm .",
    "for the considered cases , the achieved analytical speedup with respect to the ta was varied by a factor up to fifty .",
    "the analytical estimations for the standard nonrecursive case have been validated in practical calculations of solving the matrix system originated from the bvp for the two - dimensional integro - differential faddeev equations .",
    "the numerical scheme for solution of these equations as well as the illustrative results have been presented .",
    "the overall growth of the performance by a factor of up to ten has been practically achieved in the computations .",
    "the work was supported by russian foundation for basic research grant no .",
    "14 - 02 - 00326 and by saint petersburg state university within the project no .",
    "all calculations were carried out using the facilities of the `` computational center of spbsu '' .    00 w.  h.  press , s.  a.  teukolsky , w.  t.  vetterling , b.  p.  flannery , numerical recipes : the art of scientific computing , cambridge university press , new  york ,  2007 .",
    "j.  h.  ahlberg , e.  n.  nilson , j.  l.  walsh , theory of splines and their applications , academic press , new  york ,  1967 .",
    "e.  isaacson , h.  b.  keller , analysis of numerical methods , wiley , new  york ,  1966 .",
    "l.  h.  thomas , elliptic problems in linear difference equations over a network , watson sci .",
    "comput . lab .",
    "rept . , columbia university , new  york ,  1949 .",
    "a.  a.  samarskii , e.  s.  nikolaev , methods for solving finite - difference equations , nauka , moscow ,  1978 .",
    "e.  anderson , et  al .",
    ", lapack users guide , 1999 , http://www.netlib.org/lapack/lug/ p.  e.  bjrstad , o.  b.  widlund , siam j. numer .",
    "23 ( 1986 ) 1097 .",
    "j.  m.  ortega , introduction to parallel and vector solution of linear systems , springer , new  york ,  1988 .",
    "h.  s.  stone , acm  trans . math",
    ". softw . 1 ( 1975 ) 289 .",
    "v.  mehrmann , parallel comput . 19",
    "( 1993 ) 257 .",
    "a.  cleary , j.  dongarra , report no .",
    "ut - cs-97 - 358 ( lawn no .",
    "125 ) , university of tennessee ,  1997 .",
    "s.  p.  merkuriev , c.  gignoux , a.  laverne , ann .",
    "( 1976 ) 30 .",
    "a.  a.  kvitsinskii , et al .",
    "17 ( 1986 ) 267 . c.  r.  chen , et al .",
    ", phys . rev . c. 39 ( 1989 ) 1261 .",
    "e.  a.  kolganova , a.  k.  motovilov and s.  a.  sofianos , j.  phys .",
    "( 1998 ) 1279 .",
    "p.  a.  belov , s.  l.  yakovlev , phys .",
    "76 ( 2013 ) 126 .",
    "a.  a.  samarskii , a.  v.  gulin , numerical methods , nauka , moscow ,  1989 . j.  j.  dongarra , et  al .",
    ", linpack users guide , siam , philadelphia ,  1979 .",
    "d.  p.  bertsekas , constrained optimization and lagrange multiplier methods , academic press , new  york ,  1982 .",
    "openmp application program interface , 2008 , http://www.openmp.org/mp-documents/spec30.pdf l.  d.  faddeev , s.  p.  merkuriev , quantum scattering theory for several particle systems , springer , dordrecht ,  1993 .",
    "a.  kuperin , s.  p.  merkuriev , a.  a.  kvitsinskii , sov . j. nucl .",
    "37 ( 1983 ) 857 [ yad . fiz . 37 ( 1983 )",
    ". s.  l.  yakovlev , i.  n.  filikhin , phys .",
    "56 , no .  12 ( 1993 ) 1676 .",
    "w.  glckle , g.  l.  payne , phys .",
    "c 45 ( 1992 ) 974 .",
    "v.  a.  roudnev , s.  l.  yakovlev , chem .",
    "328 ( 2000 ) 97 .",
    "r.  lazauskas and j.  carbonell , phys .",
    "rev . c. 84 ( 2011 ) 034002 .",
    "a.  simonov , sov .",
    "phys . 3 ( 1966 ) 461 .",
    "a.  m.  badalyan , yu .",
    "a.  simonov .",
    "phys . 3 ( 1966 ) 1033",
    ". s.  i.  vinitskii , b.  l.  markovski , and a.  a.  suzko , sov .",
    "55 ( 1992 ) 371 . m.  abramowitz and i.  a.  stegun , handbook of mathematical functions with formulas , graphs , and mathematical  tables , dover , new  york ,  1972",
    ". p.  a.  belov , s.  l.  yakovlev , phys .",
    "77 ( 2014 ) 344 . g.  l.  payne , in : proceedings of the  8th autumn school on the  models and methods in few - body physics , lisboa ,  1986 , ed .  by l.  s.  ferreira et  al . , springer - verlag , berlin ,  1987 ,  p.  64 .",
    "m.  prenter , splines and variational methods , wiley , new  york ,  1989 . v. m. suslov and b. vlahovic , phys .",
    "c 69 ( 2004 ) 044003 .",
    "j.  l.  friar , g.  l.  payne , and w.  glckle , et  al .",
    "c 51 ( 1995 ) 2356 ."
  ],
  "abstract_text": [
    "<S> the decomposition method which makes the parallel solution of the block - tridiagonal matrix systems possible is presented . the performance of the method is analytically estimated based on the number of elementary multiplicative operations for its parallel and serial parts . the computational speedup with respect to the conventional sequential thomas algorithm </S>",
    "<S> is assessed for various types of the application of the method . </S>",
    "<S> it is observed that the maximum of the analytical speedup for a given number of blocks on the diagonal is achieved at some finite number of parallel processors . </S>",
    "<S> the values of the parameters required to reach the maximum computational speedup are obtained . </S>",
    "<S> the benchmark calculations show a good agreement of analytical estimations of the computational speedup and practically achieved results . </S>",
    "<S> the application of the method is illustrated by employing the decomposition method to the matrix system originated from a boundary value problem for the two - dimensional integro - differential faddeev equations . </S>",
    "<S> the block - tridiagonal structure of the matrix arises from the proper discretization scheme including the finite - differences over the first coordinate and spline approximation over the second one . </S>",
    "<S> the application of the decomposition method for parallelization of solving the matrix system reduces the overall time of calculation up to 10 times .    block - tridiagonal matrix , decomposition method , thomas algorithm , parallel solution , computational speedup , three - body systems , faddeev equations </S>"
  ]
}