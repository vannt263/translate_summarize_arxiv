{
  "article_text": [
    "in complex systems like biological networks , for example neural networks , a basic principle is that their functioning is based on the correlation and interaction of their different parts .",
    "while correlation between two sources is well understood , and can be quantified by shannon s mutual information ( see for example @xcite ) , there is still no generally accepted theory for interactions of three nodes or more .",
    "if we label one of the nodes as `` output '' , the problem is equivalent to determine how much two ( or more ) input nodes interact to yield the output .",
    "this concept is known in common language as `` synergy '' , which means `` working together '' , or performing a task that would not be feasible by the single parts separately .",
    "there are a number of important works which address the topic , but the problem is still considered open .",
    "the first generalization of mutual information was _ interaction information _",
    "( introduced in @xcite ) , defined for three nodes in terms of the joint and marginal entropies : @xmath1 interaction information is defined symmetrically on the joint distribution , but most approaches interpret it by looking at a channel , rather than a joint distribution , @xmath2 . for example , we can rewrite equivalently in terms of mutual information ( choosing @xmath3 as `` output '' ) : @xmath4 where we see that it can mean intuitively `` how much the whole @xmath5 gives more ( or less ) information about @xmath3 than the sum of the parts separately '' .",
    "another expression , again equivalent , is : @xmath6 which we can interpret as `` how much conditioning over @xmath3 changes the correlation between @xmath7 and @xmath8 '' ( see @xcite ) . unlike mutual information , interaction information carries a sign :    * @xmath9 : _ synergy_. conditioning on one node _",
    "increases _ the correlation between the remaining nodes . or , the whole gives more information than the sum of the parts .",
    "example : xor function .",
    "* @xmath10 : _",
    "redundancy_. conditioning on one node",
    "_ decreases _ , or _ explains away _ the correlation between the remaining nodes . or , the whole gives less information than the sum of the parts .",
    "example : @xmath11 .",
    "* @xmath12 : _",
    "3-independence_. conditioning on one node has no effect on the correlation between the remaining nodes . or",
    ", the whole gives the same amount of information as the parts separately .",
    "the nodes can nevertheless still be conditionally dependent .",
    "example : independent nodes .",
    "but the nodes are not independent , see @xcite . ]    as argued in @xcite , @xcite , and @xcite , however , this is not the whole picture .",
    "there are systems which exhibit both synergetic and redundant behavior , and interaction information only quantifies the _ difference _ of synergy and redundancy , with a priori no way to tell the two apart . in a system with highly correlated inputs , for example , the synergy would remain unseen , as it would be cancelled by the redundancy .",
    "moreover , this picture breaks down for more than three nodes .",
    "another problem , pointed out in @xcite and @xcite , is that redundancy ( as for example in @xmath11 ) can be described in terms of pairwise interactions , not triple , while synergy ( as in the xor function ) is purely threewise .",
    "therefore , @xmath13 compares and mixes information quantities of different nature .",
    "a detailed explanation of the problem for two inputs is presented in @xcite and it yields a decomposition (  partial information decomposition , pid ) as follows : there exist two non - negative quantities , _ synergy _ and _ redundancy _ , such that @xmath14 or equivalently : @xmath15 moreover , they define _ unique information _ for the inputs @xmath7 and @xmath16 as : @xmath17 so that the total mutual information is decomposed positively : @xmath18 what these quantities intuitively mean is :    * redundancy  information available in both inputs ; * unique information  information available only in one of the inputs ; * synergy  information available only when both inputs are present , arising purely from their interaction .",
    "in this formulation , if one finds a measure of synergy , one can automatically define compatible measures of redundancy and unique information ( and viceversa ) , provided that the measure of synergy is always larger or equal to @xmath19 , and that the resulting measure of redundancy is less or equal than @xmath20 and @xmath21 .",
    "synergy , redundancy , and unique information are defined on a channel , and choosing a different channel with the same joint distribution ( e.g. @xmath22 ) may yield a different decomposition .    in @xcite",
    "is presented an overview of ( previous ) measures of synergy , and their shortcomings in standard examples . in the same paper",
    "is then presented a newer measure for synergy , defined equivalently in @xcite as : @xmath23 where @xmath24 is the space of distributions with prescribed marginals : @xmath25 this measure satisfies interesting properties ( proven in @xcite and @xcite ) , which make it compatible with williams and beer s pid , and with the intuition in most examples .",
    "however , it was proven in @xcite that such an approach can _ not _ work in the desired way for more than three nodes ( two inputs ) .",
    "our approach uses information geometry @xcite , extending previous work on hierarchical decompositions @xcite and complexity @xcite .",
    "( compare the related approach on information decomposition pursued in @xcite . )",
    "the main tools of the present paper are kl - projections , and the pythagorean relation that they satisfy .",
    "this allows ( as in @xcite ) to form hierarchies of interactions of different orders in a geometrical way . in the present problem",
    ", we decompose mutual information between inputs and outputs of a channel @xmath26 , for two inputs , as : @xmath27 where @xmath28 quantifies synergy ( as in equation ) , and @xmath29 integrates all the lower order terms ( @xmath30 ) , quantifying the so - called _ union information _",
    "( see @xcite ) .",
    "one may want to use this measure of synergy to form a complete decomposition analogous to , but this does not work , as in general it is not true that @xmath31 .",
    "for this reason , we keep the decomposition more coarse , and we do not divide union information into unique and redundant .    for more inputs",
    "@xmath32 , the decomposition generalizes to : @xmath33 where higher orders of synergy appear .    until now",
    ", there seems to be no way of rewriting the decomposition of @xcite and @xcite in a way consistent with information geometry , and more in general , williams and beer s pid seems hard to write as a geometric decomposition .",
    "a comparison between @xmath28 and the measure @xmath34 of @xcite and @xcite is presented in section [ jfr ] .",
    "there we show that @xmath35 , and we argue , with a numerical example , that @xmath34 overestimates synergy at least in one case .    for a small number of inputs @xmath36 , our quantities are easily computable with the standard algorithms of information geometry ( like iterative scaling @xcite ) .",
    "this allowed to get precise quantities for all the examples considered .",
    "we consider a set of @xmath37 input nodes @xmath38 , taking values in the sets @xmath32 , and an output node , taking values in the set @xmath8 .",
    "we write the input globally as @xmath39 .",
    "for example , in biology @xmath8 can be the phenotype , and @xmath7 can be a collection of genes determining @xmath8 .",
    "we denote by @xmath40 the set of real functions on @xmath8 , and with @xmath41 the set of probability measures on @xmath7 .",
    "we can model the channel from @xmath7 to @xmath8 as a markov kernel ( called also stochastic kernel , transition kernel , or stochastic map ) @xmath26 , that assigns to each @xmath42 a probability measure on @xmath8 ( for a detailed treatment , see @xcite ) . here",
    "we will consider only finite systems , so we can think of a channel simply as a transition matrix ( or stochastic matrix ) , whose rows sum to one . @xmath43 the space of channels from @xmath7 to @xmath8 will be denoted by @xmath44 .",
    "we will denote by @xmath7 and @xmath8 also the corresponding random variables , whenever this does not lead to confusion .",
    "conditional probabilities define channels : if @xmath45 and the marginal @xmath46 is strictly positive , then @xmath47 is a well - defined channel .",
    "viceversa , if @xmath48 , given @xmath49 we can form a well - defined joint probability : @xmath50    an `` input distribution '' @xmath49 is crucial also to extend the notion of divergence from probability distributions to channels . the most natural way of doing it is the following .",
    "let @xmath49 , let @xmath51 .",
    "then : @xmath52    defined this way , @xmath53 is affine in @xmath54 .",
    "it is worth noticing that @xmath53 is in general _ not _ equal to @xmath55 .",
    "moreover , it has an important compatibility property .",
    "let @xmath56 be joint probability distributions on @xmath57 , and let @xmath58 be the kl - divergence .",
    "then : @xmath59    we will now illustrate our geometric ideas in channels with one , two , and three input nodes , then we present some examples . the general case will be addressed in section [ decomp ] .",
    "[ [ mutual - information - as - motivation . ] ] mutual information as motivation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is a well - known fact in information theory that shannon s mutual information can be written as a kl - divergence : @xmath60 from the point of view of information geometry , this can be interpreted as a `` distance '' between the real distribution and a product distribution that has exactly the same marginals , but maximal entropy .",
    "in other words , we have : @xmath61 the distribution given by @xmath62 is optimal in the sense that : @xmath63        the divergence between @xmath54 and a submanifold is , as usual in geometry , the `` distance '' between @xmath54 and the `` closest point '' on that submanifold , which in our case is the geodesic projection w.r.t . the mixture connection .    [ [ extension - to - channels . ] ] extension to channels .",
    "+ + + + + + + + + + + + + + + + + + + + + +    we can use the same insight with channels . instead of a joint distribution on @xmath37 nodes ,",
    "we consider a channel from an input @xmath7 to an output @xmath8 .",
    "suppose we have a family @xmath64 of channels , and a channel @xmath26 that may not be in @xmath64 .",
    "then , just as in geometry , we can define the `` distance '' between @xmath26 and @xmath64 .",
    "let @xmath54 be an input distribution .",
    "the divergence between a channel @xmath26 and a family of channels @xmath64 is given by : @xmath65 if the minimum is uniquely realized , we call the channel @xmath66 the _ kl - projection _ of @xmath26 on @xmath64 ( and simply `` a '' kl - projection if it is not unique )",
    ".    we will always work with compact families , so the minima will always be realized , and for strictly positive @xmath54 they will be unique ( see section [ decomp ] for the details ) .",
    "we will consider families @xmath64 for which the kl - divergence satisfies a pythagorean equality ( see figure 2 below for some intuition ) : @xmath67 for every @xmath68 .",
    "these families ( technically , closures of exponential families ) are defined in section [ decomp ] .        [",
    "[ one - input . ] ] one input .",
    "+ + + + + + + + + +    consider first one input node @xmath7 , with input distribution @xmath46 , and one output node @xmath8 .",
    "a _ constant channel _",
    "@xmath26 in @xmath44 is a channel whose entries do not depend on @xmath7 ( more precisely : @xmath69 for any @xmath70 ) .",
    "this denomination is motivated by the following properties :    * they correspond to channels that do not use any information from the input to generate the output . *",
    "the output distribution given by @xmath26 is a probability distribution on @xmath8 which does not depend on @xmath7 . *",
    "deterministic constant channels are precisely constant functions .",
    "we call @xmath71 the family of constant channels .",
    "take now any channel @xmath48 .",
    "if we want to quantify the dependence in @xmath26 of @xmath8 on @xmath7 we can then look at the divergence of @xmath26 from the constant channels : @xmath72 the minimum is realized in @xmath73 .",
    "we have that : @xmath74 so that consistently with our intuition , the dependence of @xmath8 on @xmath7 is just the mutual information . from the channel point of view , it is simply the divergence from the constant channels .",
    "( a rigorous calculation is done in section [ decomp ] . )",
    "[ [ two - inputs . ] ] two inputs .",
    "+ + + + + + + + + + +    consider now two input nodes with input probability @xmath54 and one output node .",
    "we can again define the family @xmath71 of constant channels , and the same calculations give : @xmath75 this time , though , we can say a lot more : the quantity above can be decomposed . in analogy with the independence definition for probability distributions , we would like to define a split channel as a product channel of its parts : @xmath76 .",
    "unfortunately , the term on the right would be in general not normalized , so we replace the condition by a weaker one .",
    "we call the channel @xmath77 _ split _ if it can be written as : @xmath78 for some _ functions _ @xmath79 , which in general are not themselves channels ( in particular , @xmath80 ) .",
    "we call @xmath81 the family of split channels",
    ". this family corresponds to those channels that do not have any synergy .",
    "this is a special case of an exponential family , analogous to the family of product distributions of figure 1 .",
    "the examples `` single node '' and `` split channel '' in the next section belong exactly to this family .",
    "take now any channel @xmath77 .",
    "in analogy with mutual information , we call _ synergy _ the divergence : @xmath82 simply speaking , our synergy is quantified as the deviation of the channel from the set @xmath81 of channels without synergy",
    ".    we can now project @xmath26 first to @xmath81 , and then to @xmath71 . since @xmath71 is a subfamily of @xmath81 , the following pythagoras relation holds from :",
    "@xmath83 if in analogy with the one - input case we call the last quantity @xmath29 , we get from and : @xmath84 the term @xmath29 measures how much information comes from single nodes ( but it does not tell which nodes ) .",
    "the term @xmath28 measures how much information comes from the synergy of @xmath85 and @xmath16 in the channel .",
    "the example `` xor '' in the next section will show this .",
    "if we call @xmath86 the whole @xmath44 , we get @xmath87 and : @xmath88    [ [ three - inputs . ] ] three inputs .",
    "+ + + + + + + + + + + + +    consider now three nodes @xmath89 with input probability @xmath54 , and a channel @xmath26 .",
    "we have again : @xmath90 this time we can decompose the mutual information in different ways .",
    "we can for example look at split channels , i.e. in the form : @xmath91 for some @xmath92 . as in the previous case ,",
    "we call this family @xmath81 . or we can look at more interesting channels , the ones in the form : @xmath93 for some @xmath94 .",
    "we call this family @xmath86 , and it is easy to see that : @xmath95 where @xmath71 denotes again the constant channels , and @xmath96 denotes the whole @xmath44 .",
    "we define again : @xmath88 this time , the pythagorean relation can be nested , and it gives us : @xmath97 the difference between pairwise synergy and threewise synergy is shown in the `` xor '' example in the next section .",
    "now that we have introduced the measure for a small number of input , we can study the examples from the literature @xcite , and show that our measure is consistent with the intuition .",
    "the general case will be more in rigor introduced in section [ decomp ] .",
    "here we present some examples of decomposition for well - known channels .",
    "all the quantities have been computed using an algorithm analogous to iterative scaling ( as in @xcite ) .",
    "[ [ single - node - channel . ] ] single node channel .",
    "+ + + + + + + + + + + + + + + + + + + +    the easiest example is considering a channel which only depends on @xmath85 , i.e. : @xmath98    for example , consider 3 binary input nodes @xmath89 with constant input probability , and one binary output node @xmath8 which is an exact copy of @xmath85 .",
    "then we have exactly one bit of single node information , and no higher order terms .",
    "geometrically , @xmath26 lies in @xmath81 , so the only nonzero divergence in equation is @xmath99 .",
    "as one would expect , @xmath100 and @xmath101 vanish , as there is no synergy of order 2 and 3 .",
    "[ [ split - channel . ] ] split channel .",
    "+ + + + + + + + + + + + + +    the second easiest example is a more general channel which obeys equation . in particular , consider 3 binary input nodes @xmath89 with constant input probability ( so , the @xmath102 are independent ) , and output @xmath103 . as channel",
    "we simply take the identity map @xmath104 . in this particular case : @xmath105 we have 3 bits of mutual information , which are all single node ( but from different nodes ) .",
    "since : @xmath106 which is a special case of , @xmath107 , and so @xmath100 and @xmath101 in equation are again zero .        [",
    "[ correlated - inputs . ] ] correlated inputs .",
    "+ + + + + + + + + + + + + + + + + +    consider 3 perfectly correlated binary nodes , each one with uniform marginal probability . as output",
    "take a perfect copy of one ( hence , all ) of the inputs .",
    "we have again one bit of mutual information , which could come from any of the nodes , but no synergy , as no two nodes are interacting in the channel .",
    "the input distribution has correlation , but this has no effect on the channel , since the channel is simply copying the value of @xmath85 ( or @xmath16 or @xmath108 , equivalently ) .",
    "therefore again @xmath107 .",
    "of the terms in equation , again the only non - zero is @xmath99 .",
    "this example in the literature is used to motivate the notion of redundancy .",
    "a `` redundant channel '' is in our decomposition exactly equivalent to a single node channel , since it contains exactly the same amount of information .",
    "[ [ parity - xor . ] ] parity ( xor ) .",
    "+ + + + + + + + + + + + +    the standard example of synergy is given by the xor function , and more generally by the parity function between two or more nodes .",
    "for example , consider 3 binary input nodes @xmath89 with constant input probability , and one binary output node @xmath8 which is given by @xmath109 .",
    "we have 1 bit of mutual information , which is purely arising from a pairwise synergy ( of @xmath85 and @xmath16 ) , so this time @xmath110 .",
    "the function xor is _ pure _ synergy , so @xmath100 is the only non - zero term in .",
    "if instead @xmath8 is given by the threewise parity function , or @xmath111 , we have again 1 bit of mutual information , which now is purely arising from a threewise synergy , so here @xmath112 , and the only non - zero term in is @xmath101 .        in these examples",
    "there are no terms of lower order synergy , but the generic elements of @xmath86 and @xmath96 usually do contain a nonzero lower part .",
    "consider for instance the next example .",
    "[ [ and - and - or . ] ] and and or .",
    "+ + + + + + + + + + +    the other two standard logic gates , and and or , share the same decomposition .",
    "consider two binary nodes @xmath113 with uniform probability , and let @xmath8 be @xmath114 ( or @xmath115 ) .",
    "there is again one bit of mutual information , which comes mostly from single nodes , but also from synergy .",
    "geometrically , this means that and and or are channels in @xmath86 which lie close to the submanifold @xmath81 .    [",
    "[ xorloses . ] ] xorloses .",
    "+ + + + + + + + +    here we present a slightly more complicated example , coming from @xcite .",
    "we have three binary nodes @xmath89 , where @xmath113 have uniform probabilities , and an output node @xmath116 , just like in the `` xor '' example .",
    "now we take @xmath108 to be perfectly correlated with @xmath116 , so that @xmath8 could get the information either from @xmath108 or from the synergy between @xmath85 and @xmath16 .",
    "we have one bit of mutual information , which can be seen as entirely coming from @xmath108 , and so the synergy between @xmath85 and @xmath16 is not adding anything .        [ [ xorduplicate . ] ] xorduplicate .",
    "+ + + + + + + + + + + + +    again from @xcite .",
    "we have 3 binary nodes @xmath89 , where @xmath113 have uniform probabilities , while @xmath117 . the output is @xmath118 ,",
    "so it could get the information either from the synergy between @xmath85 and @xmath16 , or @xmath16 and @xmath108 .",
    "there is one bit of mutual information , which is coming from a pairwise interaction .",
    "again , it does not matter between whom",
    ".        it should be clear from the examples here that decomposing only _ by order _ , and not by the specific subsets , is crucial .",
    "for example , in the `` input correlation '' example , there is no natural way to decide from _ which single node _ the information comes , even if it is clear that the interaction is of order 1 .",
    "here we try to give a general formulation , for @xmath37 inputs , of the quantities defined in section [ idea ] . as in the introduciton we call the set of input nodes @xmath119 of cardinality @xmath37 , and we consider a subset of the nodes @xmath13 .",
    "we denote the joint random variable @xmath120 by @xmath121 , and we denote the complement of @xmath13 in @xmath119 by @xmath122 . the case @xmath123 in section [ idea ]",
    "should motivate the following definition .",
    "let @xmath124 .",
    "we call @xmath125 the space of functions who only depend on @xmath121 and @xmath8 : @xmath126 let @xmath127 .",
    "we call @xmath128 the space of channels which can be written as a product of functions of @xmath125 with the order of @xmath13 at most @xmath26 : @xmath129 where @xmath130 denotes the closure in @xmath44 .",
    "intuitively , this means that @xmath131 does not only contain terms in the form given in the curly brackets , but also limits of such terms",
    ". stated differently , the closure of a set includes not only the set itself , but also its boundary .",
    "this is important , because when we project to a family , the projection may lie on the boundary . in order for the result to exist , the boundary",
    "must then be included .",
    "this way :    * @xmath71 is the space of constant channels ; * @xmath132 is the whole @xmath44 ; * @xmath133 if and only if @xmath134 ; * for @xmath135 we recover exactly the quantities of section [ idea ] .",
    "the family @xmath131 is also the closure of the family in the form : @xmath136 where : @xmath137 such families are known in the literature as _ exponential families _ ( see for example @xcite ) . in particular",
    ", it is compact ( for finite @xmath37 ) , so that the infimum of any function on @xmath131 is always a minimum .",
    "this means that for a channel @xmath26 and an input distribution @xmath54 : @xmath138 always exists .",
    "if it is unique , for example if @xmath54 is strictly positive , we define the unique kl - projection as : @xmath139 @xmath140 has the property that it defines the same output probability on @xmath8 .",
    "let @xmath48 , let @xmath141 .",
    "then the @xmath142-wise synergy of @xmath26 is ( if the kl - projections are unique ) : @xmath88 for more clarity , we call the @xmath143-wise synergy `` single node information '' or `` single - node dependence '' .    for @xmath144 , we can look at its divergence from @xmath71 .",
    "if we denote @xmath73 by @xmath145 : @xmath146 if @xmath26 is not strictly positive , we take the convention @xmath147 , and we discard the zero terms from the sum . since @xmath148 but @xmath145 is constant in @xmath149 , it can _ not _ happen that for some @xmath150 , @xmath151 but @xmath152 .",
    "( the very same is true for all kl - projections @xmath140 , since @xmath153 . ) for all other terms , becomes : @xmath154 on the other hand , the pythagorean relation implies : @xmath155 and iterating : @xmath156 in the end , we get : @xmath157",
    "this decomposition is always non - negative , and it depends on the input distribution . the terms in can be in general difficult to compute exactly .",
    "nevertheless , they can be approximated with iterative procedures .",
    "the measure of synergy , or respectively complementary information , defined in @xcite and @xcite , is : @xmath158 where @xmath24 is the space of prescribed marginals : @xmath159 our measure of synergy can be written , for two inputs , in a similar form : @xmath160 where @xmath161 , in addition to the constraints of @xmath24 , prescribes also the input : @xmath162 clearly @xmath163 , so : @xmath164 which implies that : @xmath165    we argue that not prescribing the input leads to overestimating synergy , because the subtraction in includes a possible difference in the correlation of the input distributions",
    ".    for example , consider @xmath166 binary and correlated , but not _ perfectly _ correlated .",
    "( for perfectly correlated nodes , as in section [ examples ] , @xmath167 , so there is no difference between the two measures . ) in detail , consider the channel : @xmath168 and the input distribution : @xmath169 for @xmath170 , the correlation becomes perfect , and the two measures of synergy are both zero .",
    "for @xmath171 , our measure @xmath172 is zero , as clearly @xmath173 .",
    "@xmath34 is more difficult to compute , but we can give a ( non - zero ) lower bound in the following way .",
    "first we fix two values @xmath174 .",
    "we consider the joint distribution @xmath175 , and look at the marginals : @xmath176 we define the family @xmath24 as the set of joint probabilities which have exactly these marginals .",
    "if we increase @xmath177 , we can always find an @xmath178 such that the marginals do not change : @xmath179 i.e. such that @xmath180 .",
    "now we can look at the mutual information of @xmath181 and of @xmath175 . if they differ , and ( for example ) the former is larger , then : @xmath182 is a well - defined lower bound for @xmath183 . with a numerical simulation we can show graphically that the mutual information is indeed not constant within the families @xmath24 .",
    "[ impl ]   as a function of @xmath184 ( brighter is higher ) .",
    "each red line represents a family @xmath24 of fixed marginals .",
    "while the lines of fixed mutual information and the families of fixed marginals look qualitatively similar , they do not coincide exactly , which means that @xmath185 varies within the @xmath24.,title=\"fig : \" ]    from the picture we can see that the red lines ( families @xmath24 for different initial values ) approximate well the lines of constant mutual information , at least qualitatively , but they are not exactly equal .",
    "this means that for most points @xmath54 of @xmath24 , the quantity : @xmath186 will be non - zero .",
    "more explicitly , we can plot the increase in mutual information as @xmath54 varies in @xmath24 , for example as a function of @xmath177 .",
    "this is always larger or equal than the the difference between the mutual information and its minimum in @xmath24 ( i.e. @xmath34 ) .",
    "we can see that it is positive , which implies that @xmath187 is also positive .",
    "[ expl ]   versus @xmath177 . for each @xmath188 $ ]",
    "we can find an @xmath178 such that the joint @xmath189 lies in @xmath24 .",
    "the increase in mutual information as @xmath177 varies is a lower bound for @xmath34 , which is then in general non - zero.,title=\"fig : \" ]    we can see in figure 3 that , especially for very large or very small values of @xmath178 and @xmath177 ( i.e. very strong or very weak correlation ) , @xmath34 captures the behaviour of mutual information quite well .",
    "these limits are precisely deterministic and constant kernels , for which most approaches in quantifying synergy coincide .",
    "this is the reason why the examples studied in @xcite give quite a satisfying result for @xmath34 ( in their notation , @xmath190 ) . for the less",
    "studied ( and computationally more complex ) intermediate values , like @xmath191 , the approximation is instead far from accurate , and in that interval ( see figure 4 ) there is a sharp increase in @xmath13 , which leads to overestimating synergy .",
    "using information geometry , we have defined a non - negative decomposition of the mutual information between inputs and outputs of a channel .",
    "the decomposition divides the mutual information into contributions of the different orders of synergy in a unique way .",
    "it does _ not _ , however , divide the mutual information into contributions of the different subsets of input nodes as williams and beer s pid @xcite would require .    for two inputs , our measure of synergy",
    "is closely related to the well - received quantification of synergy in @xcite and @xcite .",
    "our measure though works in the desired way for an arbitrary ( finite ) number of inputs . differently from @xcite and @xcite ,",
    "anyway , we do not define a measure for redundant or `` shared '' information , nor unique information of the single inputs or subsets .",
    "the decomposition depends on the choice of an input distribution . in case of input correlation",
    ", redundant information is counted automatically only once .",
    "this way there is no need to quantify redundancy separately .",
    "in general there is no way to compute our quantities in closed form , but they can be approximated by an iterative scaling algorithm ( see for example @xcite ) .",
    "the results are consistent with the intuitive properties of synergy , outlined in @xcite and @xcite ."
  ],
  "abstract_text": [
    "<S> the decomposition of channel information into synergies of different order is an open , active problem in the theory of complex systems . </S>",
    "<S> most approaches to the problem are based on information theory , and propose decompositions of mutual information between inputs and outputs in several ways , none of which is generally accepted yet .    we propose a new point of view on the topic . </S>",
    "<S> we model a multi - input channel as a markov kernel . </S>",
    "<S> we can project the channel onto a series of exponential families which form a hierarchical structure . </S>",
    "<S> this is carried out with tools from information geometry , in a way analogous to the projections of probability distributions introduced by amari . </S>",
    "<S> a pythagorean relation leads naturally to a decomposition of the mutual information between inputs and outputs into terms which represent single node information ; pairwise interactions ; and in general @xmath0-node interactions .    </S>",
    "<S> the synergy measures introduced in this paper can be easily evaluated by an iterative scaling algorithm , which is a standard procedure in information geometry .    </S>",
    "<S> * keywords : * synergy , redundancy , hierarchy , projections , divergences , interactions , iterative scaling , information geometry . </S>"
  ]
}