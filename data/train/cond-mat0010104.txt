{
  "article_text": [
    "experiments usually measure the response of a physical system to an input .",
    "the measured set of outputs together with the corresponding inputs is then used to determine the parameters of some assumed model .",
    "for instance , for determining the electric resistance one would measure the resulting current for several values of the applied voltage .",
    "the voltage acts as a sort of control parameter . for time",
    "series the corresponding control parameter would be the time instances at which the signal is sampled .",
    "the most common procedure for measuring physical parameters is to sample the underlying physical signal equidistantly with respect to the control parameter .",
    "however , it is often not clear how the accuracy of the parameters to be measured depends on the range over which the control parameter is varied .",
    "this is typical for signals of the form @xmath1 , which decay exponentially in the control parameter @xmath2 .",
    "for instance , if one happens to choose the maximum value of the control parameter to be much larger than the unknown decay constant @xmath3 many samples would obviously be `` pure noise '' , thus reducing the accuracy of the estimate of @xmath3 .",
    "the situation is particularly severe for sums of exponentials .",
    "there , the sampling rate clearly determines the smallest resolvable decay constant . since for equidistant sampling those parts of the signal with larger decay constants",
    "are sampled more often , the estimates become the more accurate the larger the decay constant . hence , a lot of sampling is wasted in order to estimate the easily to determine larger decay constants .    an experimentalist measuring a spectrum of decay constants",
    "consequently faces the question how to choose the values of the control parameters in order to obtain accurate measurements while restricting the maximum number of experiments to @xmath0 .",
    "this task is basically one of design of experiment , a discipline well - known in statistics @xcite . due to the nonlinear dependence on the decay constants",
    "this is a nontrivial task , however .",
    "it would involve an optimization with respect to the control parameters of the @xmath0 measurements to be done .    in this paper",
    ", we take a simplified approach by posing the question : after gathering @xmath0 data points , at what value of the control parameter should the next measurement be done ?",
    "we propose an on - line algorithm which samples optimally by maximizing the gain in information on the parameters to be measured . by this",
    ", the experiment is designed `` sequentially '' or `` on - line '' rather than completely from the onset .    due to its design",
    ", the algorithm will choose at each step of the experiment the value of the control parameter at which the next experiment should be performed .",
    "in section [ secmaxent ] we choose the entropy of the posterior as an information measure and introduce the algorithm for sequential maxent sampling .",
    "we show analytically that the algorithm reduces to finding that value of the control parameter for which the width of the predictive distribution is maximum .",
    "section [ secmonoexp ] gives an application of the algorithm to exponentially decaying signals and section [ secequidistant ] compares the performance of the algorithm with equidistant sampling . in section [ secmultiexp ]",
    "we apply the algorithm to exponentially decaying signals having more than one decay constant .",
    "finally , section [ secsummary ] summarizes the results and provides a conclusion .",
    "assume we have determined a discrete data set @xmath4 , sampled from @xmath5 at discrete instances of the control parameter @xmath6 , with a model equation @xmath7 where @xmath8 is the signal and @xmath9 represents noise in the problem .",
    "usually one has some parameterized form of @xmath8 and the aim of the experiment is to determine ( at least some of ) these parameters . having obtained @xmath10 , the knowledge of these parameters is contained in the posterior distribution @xmath11 where @xmath12 denotes the whole set of parameters to be determined . for the prior probability of the noise @xmath13",
    "we take the least informative prior @xcite , i.e. a gaussian of zero mean and variance @xmath14 .",
    "the aim of the experiment is to maximize the information about @xmath12 during the course of the experiment .",
    "using the negative entropy of the posterior as a measure of information this amounts to minimizing the entropy @xmath15 the most informative experiment would then be defined by those values of the control parameter @xmath2 which satisfy @xmath16 , implying @xmath17 in general this equation will be too difficult to solve .",
    "moreover , the optimization ( [ fullyoptimized ] ) is not advisable , since it relies on a fixed model for @xmath8 . in general , there might be different optimal experiments for different models .",
    "these difficulties can be avoided if one chooses to design the experiment in a sequential way . instead of determining the values of all @xmath18 one could ask the question : after gathering @xmath0 data points , at what value of the control parameter @xmath19 should the next measurement be done in order to gain as much information as possible about the parameters @xmath12 to be estimated ?",
    "the answer to this is obtained by varying @xmath20 with respect to the value of the control parameter @xmath19 at the next experiment .",
    "that is , one has to determine the minimum of @xmath21 with respect to @xmath19 .",
    "note that one has to average out the unknown outcome @xmath22 of the next experiment using the predictive distribution @xmath23 .",
    "the predictive reflects the knowledge about @xmath22 given the previously obtained data @xmath10 and the control parameter of the next experiment .",
    "our sequential approach to design optimal experiments is somewhat similar to so - called query learning in neural networks .",
    "there one searches for such training examples that speed up the learning process considerably @xcite .",
    "based on this similarity we name the value of the control parameter @xmath19 which minimizes ( [ stq ] ) the _ query _ @xmath24 .    applying bayes rule one can rewrite the r.h.s .  of ( [ stq ] ) in the form @xmath25 note that the only dependence on @xmath19 is via the entropy of the predictive distribution @xmath26 hence , in order to minimize the entropy of the posterior with a new measurement @xmath27 and @xmath0 old measurements @xmath10 , one has to _ maximize _ the entropy of the predictive distribution .",
    "the reason for this is that @xmath24 is that value of the control parameter for which the prediction is least secure .",
    "since the derived sampling procedure is based on a maximum entropy criterion , we call it sequential maxent sampling .",
    "two notes are in place here , concerning the implementation of the above algorithm : according to ( [ fullentropy ] ) , only @xmath23 is needed in order to determine @xmath24 . by definition @xmath28 for model functions of the form",
    "@xmath29 the integration with respect to the model parameters @xmath30 can be simplified if one treats the parameters @xmath31 as nuisance parameters , see @xcite for details . in the same manner ,",
    "the variance of the noise in ( [ signal ] ) can be integrated out analytically using jeffreys prior .",
    "hence , the only parameters that actually need to be integrated over in ( [ defpred ] ) are the @xmath32 , which enter @xmath8 in a nonlinear manner , in general .    for sufficiently large values of @xmath0",
    "the predictive distribution can be well approximated by a gaussian of width @xmath33 .",
    "the most probable value of @xmath34 will then be @xmath35 , which depends on @xmath19 via @xmath33 .",
    "maximization of @xmath36 then amounts to searching for that value of @xmath19 for which the most probable value of @xmath34 is minimum , see figure [ fig_predictive ] .",
    "( 325,115)(0,0 ) ( 0,0 )    ( 325,115 )    ( 0,0 )    ( 325,115 )    ( 165.00,52.50)(0,0)[lc]@xmath36 ( 80.00,-10.00)(0,0)[lc]@xmath22 ( 255.00,-10.00)(0,0)[lc]@xmath19",
    "as a first example we apply the proposed sequential maxent sampling procedure to an exponentially decaying signal with only one decay constant : @xmath37 as before , @xmath9 denotes additive noise of variance @xmath38 .",
    "it is obvious that the sampling technique proposed in section [ secmaxent ] can not be applied from scratch , i.e. without any experimental data at hand . for estimating @xmath32 , for instance",
    ", one needs to have at least two measurements at different time instances since one needs at least two points to fit a straight line to @xmath39 . here",
    "we use three initial measurements , because two measurements lead to a posterior of width 0 which is difficult to handle numerically .    for the simulation results shown in the following",
    ", the required three initial measurements have been generated by drawing @xmath18 , @xmath40 from a uniform distribution in @xmath41 $ ] .",
    "in general , one would perform the initial measurements within an interval which is of the order of the decay time @xmath42 .",
    "note that this choice should be possible in principle , since an experimentalist will have some prior knowledge about the decay constant .",
    "however , such a choice is not mandatory , as is exemplified in figure [ fig_tq ] .",
    "there we show simulation results for @xmath43 and @xmath44 . in the latter case the initial measurements are at times much smaller than the decay constant @xmath45 , yet",
    "the sequential maxent sampling algorithm is capable of tracking the signal .",
    "( 325,115)(0,0 ) ( 0,0 )    ( 325,115 )    ( 0,0 )    ( 325,115 )    ( 165.00,50.00)(0,0)[lc]@xmath24 ( -10.00,50.00)(0,0)[lc]@xmath24 ( 80.00,-10.00)(0,0)[lc]@xmath0 ( 255.00,-10.00)(0,0)[lc]@xmath0    figure [ fig_tq ] shows the evolution of sample times @xmath24 for increasing length @xmath0 of the experiment .",
    "we find the remarking behavior that the sampling times `` oscillate '' between @xmath46 and @xmath47 .",
    "note that the period is not strictly equal to one , since sometimes two or more consecutive measurements are at either @xmath46 or @xmath48 .",
    "the oscillating behavior can be traced back to the functional form of @xmath49 , cf .",
    "figure [ fig_predictive ] .",
    "the entropy @xmath49 has two maxima for @xmath50 , the absolute maximum determining @xmath24 . by construction of maxent sampling the next measurement will be placed at @xmath51 and @xmath52 will decrease relative to @xmath53 .",
    "after one or more measurements the predictive entropy at the hitherto global maximum will be lower than at the hitherto local maximum and the two maxima will change their roles .    from figure [ fig_tq ] it is evident that the larger sampling times are not exactly at @xmath54 .",
    "instead , the values of @xmath24 have some spread which increases with the variance @xmath14 of the additive noise , see figure [ fig_histo ] .",
    "( 325,115)(0,0 ) ( 0,0 )    ( 325,115 )    ( 0,0 )    ( 325,115 )    ( 80.00,-10.00)(0,0)[lc]@xmath24 ( 255.00,-10.00)(0,0)[lc]@xmath24    in the course of the experiment the standard errors of the parameters to be estimated decrease with @xmath55 , asymptotically , as expected , cf .  @xcite .",
    "see figure [ fig_asymptotics ] for an example .",
    "( 325,115)(0,0 ) ( 0,0 )    ( 325,115 )    ( 170.00,-5.00)(0,0)[lc]@xmath56    a note is in place here concerning the oscillating behavior of the control parameter . for time",
    "series , the control parameter @xmath2 would be time . at first glance , it might seem impossible to perform sequential maxent sampling since time can only be increased but not decreased . for many types of experiments ,",
    "however , one can actually decrease the time of the next measurement by performing a new experiment .",
    "for instance , in measuring nmr relaxation times one would first magnetize the probe which defines the onset of the experiment , @xmath57 , and then measure the magnetization for several @xmath58 . for maxent sampling , one would have to magnetize , measure at @xmath24 , magnetize again , and so on .",
    "hence , @xmath24 can increase or decrease in consecutive experiments .",
    "it is instructive to compare the results obtained for sequential maxent sampling with other sampling methods .",
    "the most common alternative method is probably equidistant sampling where the experiments are performed at @xmath0 equidistantly spaced values of the control parameter . for a signal of form ( [ monoexponential ] )",
    "the lowest of these values is obviously @xmath57 while the largest @xmath59 is up to the choice of the experimentalist .",
    "clearly , there is an optimal choice of @xmath59 . for , choosing @xmath60 would result in sampling of noise only , while @xmath61 would give a poor estimate of @xmath32 as the noiseless signal @xmath62 hardly varies in this range .",
    "this behavior is confirmed by simulations . in figure [ fig_equi ]",
    "we show the standard errors of the bayesian estimates of @xmath63 , @xmath32 , @xmath38 for various values of @xmath59 .",
    "as can be seen , the standard error of @xmath63 increases with @xmath59 while the standard error of @xmath32 has some minimum around @xmath64 ; the standard error of the estimate of @xmath38 is basically independent of @xmath59 .",
    "consequently , in choosing @xmath59 there is a tradeoff between estimating @xmath63 ( leading to @xmath65 ) and @xmath32 ( leading to @xmath66 ) .",
    "besides this tradeoff , not knowing @xmath32 one could determine @xmath59 only based on the prior knowledge on @xmath32 .",
    "in contrast to that , sequential maxent sampling , as described in the previous section , does not require the setting of any maximum control parameter @xmath59 and places its measurements automatically .",
    "( 325,115)(0,0 ) ( 0,0 )    ( 325,115 )    ( 170.00,-5.00)(0,0)[lc]@xmath59    in addition , maxent sampling leads to a much better resolution of the parameters to be estimated .",
    "this is exemplified in figure [ fig_comparison ] , which compares the obtained standard errors of @xmath63 , @xmath32 for maxent and equidistant sampling .",
    "( 325,115)(0,0 ) ( 0,0 )    ( 325,115 )    ( 0,0 )    ( 325,115 )    ( 80.00,-10.00)(0,0)[lc]@xmath56 ( 255.00,-10.00)(0,0)[lc]@xmath56",
    "exponentially decaying signals can be found in many physical phenomena as first order differential equations are among the most common in physics . however , the spectrum @xmath67 of a multi - exponential signal @xmath68 is difficult to estimate .",
    "the reason is that the required inverse ( discrete ) laplace transform is known to be an ill - posed problem @xcite .",
    "a small amount of noise added to @xmath8 leads to a nonvanishing contribution to the spectrum even in the limit of a very small perturbation .",
    "moreover , the quality of the spectrum to be estimated from measurements depends strongly on the values of the control parameter @xmath2 at which the measurements are performed @xcite .",
    "for instance , choosing @xmath69 certainly rules out the detection of any @xmath70 in ( [ multiexponential ] ) . some good prior knowledge about the values of @xmath71 to be expected in the signal would be very helpful .",
    "but even such knowledge would leave open the question how to choose the sampling times @xmath18 . for instance",
    ", in porous materials @xcite the magnetization decays approximately like @xmath72 i.e. relaxation times and corresponding amplitudes rapidly decrease like @xmath73 . how does one have to choose the sampling times in order to resolve the spectrum up to a desired @xmath74 ?    by construction of the algorithm ,",
    "sequential maxent sampling provides the answer to this question .",
    "the choice of @xmath74 fixes the model function ( [ multiexponential ] ) .",
    "given some initial data , the best query @xmath24 can be computed as described in section [ secmaxent ] .",
    "figure [ fig_multitq ] shows the simulation result for the values of @xmath24 for a signal of form ( [ porous ] ) with @xmath75 .",
    "( 325,115)(0,0 ) ( 0,0 )    ( 325,115 )    ( 0,0 )    ( 325,115 )    ( 165.00,68.00)(0,0)[lc]@xmath36 ( -12.00,68.00)(0,0)[lc]@xmath24 ( 70.00,-10.00)(0,0)[lc]@xmath22 ( 255.00,-10.00)(0,0)[lc]@xmath19    as in the monoexponential case , the values of the sampled control parameters oscillate between @xmath57 and the characteristic time constants in the signal , i.e. @xmath76 and @xmath77 in our case .",
    "note that in figure [ fig_multitq ] the signal is also sampled at times @xmath78 larger than the largest characteristic time constant the reason for this is probably due to the fact that we have not imposed any priors on the parameters @xmath32 . in order to rule out the presence of any @xmath79",
    "one would clearly have to sample at large values of @xmath2 .",
    "figure [ fig_multitq ] also depicts the characteristic dependence of the predictive entropy on @xmath19 . as in section [ secmonoexp ]",
    "the location of its maxima characterizes the sampled values of the control parameter @xmath2 .    in figure [ fig_multipost ]",
    "we show the resulting posterior on @xmath32 ( with @xmath80 and @xmath38 treated as nuisance parameters ) in order to compare with equidistant sampling .",
    "as can be seen , sequential maxent sampling leads to lower error bars in the estimates of @xmath32 .",
    "in particular the width of the posterior in the large decay constant @xmath81 is considerably larger for equidistant sampling . for signals of the form ( [ porous ] )",
    "this is exactly what one would expect since the term with the smaller decay constant @xmath82 is sampled more frequently than the term decaying with @xmath81 . for sequential maxent sampling , however , the latter term is sampled more frequently , cf .",
    "figure [ fig_multitq ] .",
    "( 325,180)(0,0 ) ( 0,0 )    ( 325,180 )    ( 0,0 )    ( 325,180 )    ( 165.00,65.00)(0,0)[lc]@xmath81 ( -20.00,65.00)(0,0)[lc]@xmath81 ( 70.00,-10.00)(0,0)[lc]@xmath82 ( 255.00,-10.00)(0,0)[lc]@xmath82    despite the good performance of sequential maxent sampling it has to be stressed that it becomes impracticable in its current form for @xmath83 . for a low number @xmath0 of experiments",
    "it will be difficult to find evidence for the decay constant @xmath84 in ( [ porous ] ) . a low number of experiments @xmath0 will result in a degeneracy in the estimates of some of the @xmath85 leading to divergences in the posterior .",
    "the remedy here would be to incorporate model selection as described in @xcite into the algorithm .",
    "this would probably result in an algorithm which starts with the hypothesis @xmath86 for low @xmath0 and increases @xmath74 as soon as the relative probability for such a hypothesis is larger .",
    "starting from the question at what value of the control parameter one should perform the next experiment after having performed @xmath0 measurements , we have analytically derived a sequential sampling procedure which maximizes the information on the parameters to be estimated . looking for the query @xmath19 that leads to a maximum information gain ( minimization of the posterior entropy ) led us to the conclusion that one should find @xmath24 that _ maximizes _ the entropy of the predictive distribution .",
    "the reason for this is that @xmath24 is the value of the control parameter for which the prediction is least secure .",
    "we have applied the constructed maxent sampling procedure to the fitting of exponential signals .",
    "we found that the resulting queries oscillate between the time constants present in the signal .",
    "the width of the distribution of queries around the time constants depends on the variance of noise by which the signal is corrupted .    for purely monoexponential signals",
    "we have compared sequential maxent sampling with equidistant sampling and found a considerably better accuracy of the estimates of maxent sampling .",
    "the application of sequential maxent sampling to multiexponential signals is of particular interest because of their frequent appearance in science and the difficulties associated with the inverse laplace transform .",
    "we have motivated that proper sampling helps to perform the inversion task .",
    "we have exemplified that sequential maxent sampling is capable of tracking the spectrum quickly .",
    "however , so far we have applied the algorithm only to an assumed constant number of exponentially decaying terms .",
    "this is a bit unrealistic , since typically there would be infinitely many such terms , like the nmr signals mentioned in section [ secmultiexp ] .",
    "moreover , the assumption of a large fixed number of exponential terms in the signal leads to degeneracies and hence divergences in the posterior .",
    "we hope that these divergences can be removed by hypothesizing a small number @xmath74 of exponential terms at the onset of the experiment and increasing this number whenever the relative probability favors the hypothesis of a larger value of @xmath74 .",
    "finally we note that due to the nonlinear dependence of the model functions on the decay constants @xmath32 results could only be obtained numerically . for models",
    "completely linear in the model parameters it is possible to do the calculation of query times @xmath24 analytically @xcite , however .",
    "for such a case , the result is rather trivial , leading to @xmath87 ."
  ],
  "abstract_text": [
    "<S> in this paper we pose the question : after gathering @xmath0 data points , at what value of the control parameter should the next measurement be done ? we propose an on - line algorithm which samples optimally by maximizing the gain in information on the parameters to be measured . </S>",
    "<S> we show analytically that the information gain is maximum for those potential measurements whose outcome is most unpredictable , i.e.  for which the predictive distribution has maximum entropy . </S>",
    "<S> the resulting algorithm is applied to exponential analysis . </S>"
  ]
}