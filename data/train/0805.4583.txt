{
  "article_text": [
    "thermal heating in electronic systems is strongly related to performance limitation , aging , reliability and safety issues .",
    "high performance - density and small physical size ( area or volume ) make thermal heating important and challenging to address .",
    "this is enhanced by the trend of modern ( micro-)electronics technology to pack more and faster operations within the smallest possible physical area in order to increase performance , reduce cost and size , and therefore expand the potential applications of the product and make it more profitable .",
    "electrical power dissipation into heat raises the local temperature of the circuit ; more accurately , the temperature depends on the circuit activity .",
    "the temperature influences the power of the intrinsic noise in the circuit which in turn reduces the effective communication or computation capacity of the circuit .",
    "this `` negative '' performance feedback is expected to become a bottleneck of future technology @xcite , @xcite .",
    "this work aims to add this dimension to our understanding of the coupling mechanism between communication and computation performance and thermal heating . to this",
    "end a class of communication channels is introduced , where the channel s noise power depends dynamically on the channel s activity , and its channel capacity is studied .    to support the previous statements and motivate the mathematical development of this new class of channels we first discuss the underlying physical mechanism that connects circuit activity with power consumption and thermal heating .",
    "thermal heating is unavoidable in electronic circuits . every circuit block converts part of the power it draws from the power supply network ( and to certain extent from its interconnections with other blocks ) into heat which raises the local temperature .",
    "a circuit block in a microchip occupies certain physical space within which heat is distributively generated and diffused according to the _ heat diffusion equation _",
    "( ignoring other heat sources ) @xmath1 where @xmath2 is the volumetric heat capacity of the material , @xmath3 is the change in temperature over time , @xmath4 is the divergence , @xmath5 is the distributed thermal resistance , @xmath6 is the temperature gradient , and @xmath7 is the power density of the added heat , @xcite , @xcite .    in many cases",
    "the diffusion equation can be replaced by the corresponding _ ordinary differential equation _ ( ode ) that provides a lumped model of the thermal dynamics .",
    "consider for example a microchip ( die ) , made out of material of lower thermal resistance , which is internally heated by the activity of circuits and transfers the heat to the environment ( e.g. , air ) which has much higher resistance . in this case",
    "we can write @xmath8 where @xmath9 is the heat capacity of the microchip ( die ) , @xmath10 is the thermal resistance between the die and the environment ( e.g. , air ) , @xmath11 is the temperature of the environment , and @xmath12 is the instantaneous heat generated , i.e. , the electrical power converted into heat by the circuit .    solving with the assumption that at time @xmath13 we have @xmath14 with @xmath11 being fixed , we obtain @xmath15 if the circuit operates based on a reference clock of period @xmath16 , can be approximated by its discrete version @xmath17 where @xmath18 denotes the set of positive integers , and where the sequences @xmath19 and @xmath20 are the samples at integer multiples of @xmath16 of @xmath21 and @xmath22 , respectively .",
    "equation shows the fading memory effect of temperature .",
    "note that also captures discrete versions of distributed or higher order lumped approximations of the diffusion equation .",
    "every electronic circuit has some intrinsically generated noise .",
    "this noise is added to the received signal degrading its quality . especially in the popular class of circuits based on mos transistors @xcite , this noise is dominated by a thermal noise component that is stationary gaussian , and in most applications it can be considered white .",
    "the variance of the thermal noise @xmath23 follows the johnson - nyquist formula @xmath24 where @xmath25 is the considered bandwidth , @xmath26 is the temperature of the receiver circuit block , and @xmath27 is a proportionality constant @xcite , @xcite , @xcite .",
    "the transmission of information is typically associated with dissipation of energy into heat .",
    "thus , in view of and , this motivates a channel model where the variance @xmath28 of the additive noise is determined by the history of the power of the transmitted signal , i.e. , @xmath29 where @xmath30 is the transmitted symbol at time @xmath31 , and where @xmath32 and @xmath33 will be defined in section  [ sec : channelmodel ] .",
    "the rest of this paper is organized as follows .",
    "section  [ sec : channelmodel ] describes the channel model in more detail .",
    "section  [ sec : capacity ] discusses channel capacity and lists some important properties thereof .",
    "the main results are presented in section  [ sec : result ] .",
    "the proofs of the results are given in sections  [ sec : lowsnr ] and [ sec : highsnr ] .",
    "section  [ sec : summary ] concludes with a summary .",
    "we consider the communication system depicted in figure  [ fig1 ] . the message @xmath34 to be transmitted over the channel",
    "is assumed to be uniformly distributed over the set @xmath35 for some positive integer @xmath36 .",
    "the encoder maps the message to the length-@xmath37 sequence @xmath38 , where @xmath37 is the _ block - length_. in the absence of feedback , the sequence @xmath39 is a function of the message @xmath34 , i.e. , @xmath40 for some mapping @xmath41",
    ". here @xmath42 stands for @xmath43 , and @xmath44 denotes the set of real numbers . if there is a feedback link , then @xmath45 , @xmath46 is not only a function of the message @xmath34 but also of the past channel output symbols @xmath47 , i.e. , @xmath48 for some mapping @xmath49 . the receiver guesses the transmitted message @xmath34 based on the @xmath37 channel output symbols @xmath50 , i.e. , @xmath51 for some mapping @xmath52 .",
    "[ cc][cc]transmitter [ cc][cc]channel [ cc][cc]receiver [ cc][cc]delay [ b][b]@xmath34 [ b][b]@xmath53 [ b][b]@xmath45 [ b][b]@xmath54 [ b][b]@xmath47    conditional on @xmath55 , the time-@xmath0 channel output @xmath56 is given by @xmath57 where @xmath58 is a zero - mean , unit - variance , stationary & weakly - mixing random process , drawn independently of @xmath34 , and being of finite fourth moment and of finite differential entropy rate , i.e. , @xmath59 see @xcite for a definition of weak mixing .",
    "for example , @xmath58 could be a stationary & ergodic gaussian process @xcite .",
    "in particular , the case of most interest is when @xmath58 are independent and identically distributed ( iid ) , zero - mean , unit - variance gaussian random variables , and the reader is encouraged to focus on this case .",
    "the parameter @xmath32 is assumed to be positive .",
    "it accounts for the temperature of the device when the transmitter is silent .",
    "the coefficients @xmath60 , @xmath31 are nonnegative and bounded , i.e. , @xmath61 they characterize the dissipation of the heat produced by the transmission of the message @xmath34 .",
    "is monotonically nonincreasing , i.e. , @xmath62 for @xmath63 .",
    "this assumption is , however , not required for the results stated in this paper . ]",
    "an example for a heat dissipation profile that satisfies is the _ geometric _ heat dissipation profile where @xmath33 is a geometric sequence , i.e. , @xmath64 for some @xmath65 .",
    "the heat dissipation depends _",
    "inter alia _ on the efficiency of the heat sink that is employed in order to absorb the produced heat . in the above example , the heat sink s efficiency is described by the parameter @xmath66 : the smaller @xmath66 , the more efficient the heat sink . in general ,",
    "an efficient heat sink is modeled by a heat dissipation profile for which the sequence @xmath33 decays fast .",
    "we study the above channel under an average - power constraint on the inputs , i.e. , the mappings @xmath67 ( without feedback ) and @xmath68 ( with feedback ) are chosen such that  averaged over the message @xmath34 and channel outputs @xmath50the sequence @xmath39 satisfies @xmath69 and we define the signal - to - noise ratio ( snr ) as @xmath70    the results presented in this paper do not change when is replaced by a _ per - message _ average - power constraint , i.e. , when the mappings @xmath67 and @xmath68 are chosen such that , for each message @xmath71 and for any given sequence of output symbols @xmath72 , the sequence @xmath73 satisfies @xmath74 indeed , all achievability results ( which are based on schemes that ignore the feedback ) are derived under , whereas all converse results are derived under . since all mappings @xmath67 and @xmath68 that satisfy also fulfill , this implies that the achievability results as well as the converse results derived in this paper hold irrespective of whether constraint or is imposed .",
    "let the _ rate _ @xmath75 ( in nats per channel use ) be defined as @xmath76 where @xmath77 denotes the natural logarithm function .",
    "a rate is said to be _ achievable _ if there exists a sequence of mappings @xmath78 ( without feedback ) or @xmath79 ( with feedback ) and @xmath80 such that the error probability @xmath81 tends to zero as @xmath37 goes to infinity .",
    "the _ capacity _",
    "@xmath82 is the supremum of all achievable rates .",
    "we denote by @xmath83 the capacity under the input constraint when there is no feedback , and we add the subscript `` fb '' to indicate that there is a feedback link .",
    "clearly @xmath84 as we can always ignore the feedback link .    in the absence of feedback , the _ information capacity _ is defined as @xmath85 where the supremum is over all joint distributions on @xmath38 satisfying .",
    "when there is a feedback link , then we define the information capacity as @xmath86 where the supremum is over all mappings @xmath87 satisfying . by fano s inequality (",
    "2.11.1 ) no rate above @xmath88 and @xmath89 is achievable , i.e. , @xmath90 see @xcite for conditions that guarantee that @xmath88 is achievable .",
    "note that the channel is not stationary and corresponding channel outputs @xmath91 the pair @xmath92 is jointly stationary . ] since the variance of the additive noise depends on the time - index @xmath0 .",
    "it is therefore _ prima facie _ not clear whether the inequalities in hold with equality .    in this paper",
    ", we shall investigate the capacities @xmath83 and @xmath93 at low snr and at high snr . to study capacity at low snr ,",
    "we compute the _ capacities per unit cost _ defined as @xcite @xmath94 it will become apparent later that the suprema in are attained when @xmath95 tends to zero . note that implies @xmath96 at high snr , we study conditions under which capacity is unbounded in the snr .",
    "notice that when the allowed transmit power is large , then there is a trade - off between optimizing the present transmission and minimizing the interference to future transmissions . indeed ,",
    "increasing the transmission power may help to overcome the present ambient noise , but it also heats up the chip and thus increases the noise variance in future receptions . _ prima facie _ it is not clear that , as we increase the allowed transmit power , the capacity tends to infinity .",
    "we shall see that this is not necessarily the case .",
    "our main results are presented in the following two sections .",
    "section  [ sub : unitcost ] focuses on capacity at low snr and presents our results on the capacity per unit cost .",
    "section  [ sub : isbounded ] provides a sufficient condition and a necessary condition on @xmath33 under which capacity is bounded in the @xmath95 .",
    "the results presented in this section hold under the additional assumptions that @xmath97 and that @xmath58 is iid .",
    "[ prop : lowsnr ] consider the above channel model , and assume additionally that the sequence @xmath33 satisfies and that @xmath58 is iid . then @xmath98 where @xmath99 denotes the capacity of the channel @xmath100 which is a special case of for @xmath101 .",
    "see appendix  [ app : proplowsnr ] .",
    "this proposition demonstrates that the heating up can only increase the information capacity per unit cost .",
    "thus at low snr the heating effect is unharmful .    for _ gaussian _ noise , i.e. , if @xmath58 is a sequence of iid , zero - mean , unit - variance _ gaussian _ random variables , then the heating effect is beneficial .",
    "[ thm : lowsnr ] consider the above channel model , and assume additionally that the sequence @xmath33 satisfies and that @xmath58 is a sequence of iid , zero - mean , unit - variance gaussian random variables .",
    "then , irrespective of whether feedback is available or not , the corresponding capacity per unit cost is given by @xmath102    see section  [ sec : lowsnr ] .",
    "for example , for the geometric heat dissipation profile we obtain from theorem  [ thm : lowsnr ] @xmath103 thus the capacity per unit cost is monotonically _ decreasing _ in @xmath66 .",
    "the above result might be counterintuitive , because it suggests not to use heat sinks at low snr .",
    "nevertheless it can be heuristically explained by noting that the heating effect increases the _ _ channel gain__. indeed , if we split up the channel output @xmath104 into a data - dependent part    lcl _ k & = & x_k+ u_k    and a data - independent part @xmath105 ( with @xmath106 being a sequence of iid , zero - mean , variance-@xmath32 , gaussian random variables drawn independently of @xmath107 ) , then the channel gain @xmath108 for is given by @xmath109 where the supremum is over all joint distributions on @xmath38 satisfying .",
    "thus , in view of , theorem  [ thm : lowsnr ] demonstrates that the capacity per unit cost is determined by the channel gain @xmath108 .",
    "this result is not specific to but has also been observed for other channel models .",
    "for example , the same is true for fading channels whenever the additive noise is gaussian @xcite , @xcite .      while at low snr the heating effect is beneficial , at high snr it is detrimental . in fact , it turns out that capacity can be even bounded in the snr , i.e. , the capacity does not tend to infinity as the snr tends to infinity .",
    "the following theorem provides a sufficient condition and a necessary condition on @xmath33 for the capacity to be bounded .",
    "note that the results presented in this section do not require the additional assumptions made in section  [ sub : unitcost ] : we neither assume that the sequence @xmath33 satisfies nor that @xmath58 is iid .",
    "consider the channel model described in section  [ sec : channelmodel ] .",
    "then    llcl i ) & ( _ > 0 ) & & ( _ > 0 c_fb ( ) < ) [ eq : main1 ] + ii ) & ( _ = 0 ) & & ( _ > 0 c ( ) = ) , [ eq : main2 ]    where we define , for any @xmath110 , @xmath111 and @xmath112 . [ thm : highsnr ]    see section  [ sec : highsnr ] .",
    "for example , for a geometric heat dissipation we have @xmath113 and it follows from theorem  [ thm : highsnr ] that the corresponding capacity is bounded . on the other hand , for a sub - geometric heat dissipation , i.e. , @xmath114 for some @xmath65 and @xmath115",
    ", we obtain @xmath116 and theorem  [ thm : highsnr ] implies that the corresponding capacity is unbounded . roughly speaking",
    ", we can say that whenever the sequence of coefficients @xmath33 decays _ not faster than geometrically _ then capacity is _ bounded _ in the snr , and whenever the sequence of coefficients @xmath33 decays _ faster than geometrically _ then capacity is _ unbounded _ in the snr .",
    "for part i ) of theorem  [ thm : highsnr ] the assumptions that the process @xmath58 is weakly - mixing and that it has a finite fourth moment are not needed .",
    "these assumptions are only needed in the proof of part ii ) .. ] in part ii ) of theorem  [ thm : highsnr ] , the condition on the left - hand side ( lhs ) of can be replaced by @xmath117 this condition is weaker than the original condition because @xmath118    when neither the lhs of nor the lhs of hold , i.e. , @xmath119 then capacity can be bounded or unbounded .",
    "example  [ ex:1 ] exhibits a sequence @xmath33 satisfying for which the capacity is bounded , and example  [ ex:2 ] provides a sequence @xmath33 satisfying for which the capacity is unbounded .",
    "are not monotonically decreasing in @xmath120 .",
    "consequently , examples  [ ex:1 ] & [ ex:2 ] are rather of mathematical than of practical interest .",
    "nevertheless they show that when neither condition of theorem  [ thm : highsnr ] is satisfied , then one can construct simple examples yielding a bounded capacity or an unbounded capacity , thus demonstrating the difficulty of finding conditions that are necessary _ and _ sufficient for the capacity to be bounded . ]",
    "[ ex:1 ] consider the sequence @xmath33 where all coefficients with an even index are equal to @xmath121 , and where all coefficients with an odd index are @xmath122 .",
    "it satisfies because @xmath123 and @xmath124 .",
    "then the time-@xmath0 channel output @xmath54 corresponding to the channel inputs @xmath125 is given by @xmath126 where @xmath127 denotes the floor function .",
    "thus at even times the output @xmath128 , @xmath129 only depends on the `` even '' inputs @xmath130 , while at odd times the output @xmath131 , @xmath132 only depends on the `` odd '' inputs @xmath133 . by proceeding along the lines of the proof of part i ) of theorem  [ thm :",
    "highsnr ] while choosing in @xmath134 , it can be shown that the capacity of this channel is bounded .",
    "the channel can be divided into two parallel channels , one connecting the inputs and outputs at even times , and the other connecting the inputs and outputs at odd times .",
    "as both channels have the coefficients @xmath135 , it follows from theorem  [ thm : highsnr ] that the capacity of each parallel channel is bounded and therefore also the capacity of the original channel . ]",
    "[ ex:2 ] consider the sequence @xmath33 where all coefficients with an even positive index are @xmath122 , and where all other coefficients are @xmath121 .",
    "( again , we have @xmath136 and @xmath137 . ) in this case the time-@xmath0 channel output @xmath54 corresponding to @xmath125 is given by @xmath138 using gaussian inputs of power @xmath139 at even times while setting the inputs to be zero at odd times , and measuring the channel outputs only at even times , reduces the channel to a memoryless additive noise channel and demonstrates ( using the result of @xcite ) the achievability of @xmath140 which is unbounded in the @xmath95 .",
    "the two seemingly - similar examples thus lead to completely different capacity results .",
    "the crucial difference between example  [ ex:1 ] and example  [ ex:2 ] is that in the former example at even times the interference is caused by the past channel inputs at _ even _ times , whereas in the latter example at even times the interference is caused by the past channel inputs at _",
    "odd _ times .",
    "thus in example  [ ex:2 ] setting all `` odd '' inputs to zero cancels ( at even times ) the interference from past channel inputs and hence transforms the channel into an additive noise channel whose capacity is unbounded . evidently , this approach does not work for example  [ ex:1 ] .",
    "in section  [ sub : upperlow ] we derive an upper bound on the feedback capacity @xmath141 , and in section  [ sub : lowerlow ] we derive a lower bound on the capacity @xmath83 in the absence of feedback . these bounds are used in section  [ sub : asymptotic ] to derive an upper bound on @xmath142 and a lower bound on @xmath143 , which are then both shown to be equal to @xmath144 .",
    "together with this proves theorem  [ thm : lowsnr ] .",
    "the upper bound on @xmath141 is based on and on an upper bound on @xmath145 , which for our channel can be expressed , using the chain rule for mutual information , as    lcl i(m;y_1^n ) & = & _ k=1^n ( h(y_k|y_1^k-1)-h(y_k|y_1^k-1,m ) ) + & = & _ k=1^n ( h(y_k|y_1^k-1)-h(y_k|y_1^k-1,m , x_1^k ) ) + & = & _ k=1^n ( h(y_k|y_1^k-1)-h(u_k ) - ) , [ eq : upper1 ]    where the second equality follows because @xmath146 is a function of @xmath34 and @xmath47 ; and the last equality follows from the behavior of differential entropy under translation and scaling ( * ? ? ?",
    "9.6.3 & 9.6.4 ) , and because @xmath147 is independent of @xmath148 .",
    "evaluating the differential entropy @xmath149 of a gaussian random variable , and using the trivial lower bound @xmath150 , we obtain the final upper bound    lcl i(m;y_1^n ) & & _ k=1^n ( h(y_k|y_1^k-1)-(2 e^2 ) ) + & & _ k=1^n ( 1+_=1^k_k- /^2 ) + & & ( 1+_k=1^n_=1^k_k- /^2 ) + & = & ( 1+_k=1^n /^2_=0^n - k _ ) + & & ( 1+(1+)_k=1^n /^2 ) + & & ( 1+(1+)),[eq : upper2 ]    where we define @xmath151 . here the second inequality follows because conditioning can not increase entropy and from the entropy maximizing property of gaussian random variables ( * ? ? ?",
    "9.6.5 ) ; the next inequality follows by jensen s inequality ; the following equality by rewriting the double sum ; the subsequent inequality follows because the coefficients are nonnegative which implies that @xmath152 ; and the last inequality follows from the power constraint .      as aforementioned , the above channel is not stationary and it is therefore _ prima facie _ not clear whether @xmath88 is achievable . we shall sidestep this problem by studying the capacity of a different channel whose time-@xmath0 channel output @xmath153 is , conditional on the sequence @xmath154 , given by @xmath155 where @xmath58 and @xmath33 are defined in section  [ sec : channelmodel ] .",
    "this channel has the advantage that it is stationary & ergodic in the sense that when @xmath156 is a stationary & ergodic process then the pair @xmath157 is jointly stationary & ergodic .",
    "it follows that if the sequences @xmath158 and @xmath159 are independent of each other , and if the random variables @xmath45 , @xmath160 are bounded , then any rate that can be achieved over this new channel is also achievable over the original channel . indeed , the original channel can be converted into by adding @xmath161 to the channel output @xmath54 , , @xmath160 guarantees that the quantity @xmath162 is finite for any realization of @xmath163 . ] and , since the independence of @xmath164 and @xmath165 ensures that the sequence @xmath166 is independent of the message @xmath34 , it follows that any rate achievable over can be achieved over by using a receiver that generates @xmath166 and guesses then @xmath34 based on @xmath167 . is a sequence of gaussian random variables .",
    "indeed , it relies heavily on the fact that given @xmath154 the additive noise term on the right - hand side of can be written as the sum of two independent random variables , of which one only depends on @xmath164 and the other only on @xmath165 .",
    "this surely holds for gaussian random variables , but it does not necessarily hold for other distributions on @xmath58 . ]",
    "we shall consider channel inputs @xmath168 that are blockwise iid in blocks of @xmath169 symbols ( for some @xmath170 ) .",
    "thus denoting @xmath171 ( where @xmath172 denotes the transpose ) , @xmath173 is a sequence of iid random length-@xmath169 vectors with @xmath174 taking on the value @xmath175 with probability @xmath176 and @xmath177 with probability @xmath178 , for some @xmath179 . note that to satisfy the average - power constraint we shall choose @xmath180 and @xmath176 so that @xmath181    let @xmath182 . noting that the pair @xmath183 is jointly stationary & ergodic , it follows from @xcite that the rate @xmath184 is achievable over the new channel and thus yields a lower bound on the capacity @xmath83 of the original channel .",
    "we lower bound @xmath185 as    lcl",
    "i(_0^n / l -1;_0^",
    "n / l -1 ) & = & _ b=0^n / l -1 i(_b;_0^n / l -1|_0^b-1 ) + & & _ b=0^n / l -1 i(_b;_b|_0^b-1 ) + & & _ b=0^n / l -1 ( i(_b;_b|_-^b-1)-i(_-^-1;_b|_0^b ) ) , [ eq : lb1 ]    where we use the chain rule and the nonnegativity of mutual information .",
    "it is shown in appendix  [ app : tozero ] that @xmath186 this together with a cesro type theorem ( * ? ? ?",
    "4.2.3 ) yields    lcl _",
    "n i(_0^n / l -1;_0^",
    "n / l -1 ) & & i(_0;_0|_-^-1 ) -_n _ b=0^n / l -1i(_-^-1;_b|_0^b ) + & = & i(_0;_0|_-^-1),[eq : lbcesaro ]    where the first inequality follows by the stationarity of @xmath183 which implies that @xmath187 does not depend on @xmath188 , and by noting that .",
    "we proceed to analyze @xmath189 for a given sequence @xmath190 . making use of the canonical decomposition of mutual information ( e.g. , ( * ? ? ?",
    "* eq .  ( 10 ) ) ) , we have    lcl i(_0;_0|_-^-1=_-^-1 ) & = & i(x_1;_0|_-^-1=_-^-1 ) + & = & d ( p__0|x_1=x,_-^-1 p__0|x_1=0,_-^-1 ) p_x_1(x ) + & & -d(p__0|_-^-1 p__0|x_1=0,_-^-1 ) + & = & d(p__0|x_1=,_-^-1 p__0|x_1=0,_-^-1 ) + & & - d(p__0|_-^-1 p__0|x_1=0,_-^-1 ) , [ eq : lb2 ]    where the first equality follows because , for our choice of input distribution , @xmath191 and hence @xmath192 conveys as much information about @xmath193 as @xmath194 . here",
    "@xmath195 denotes relative entropy , i.e. , @xmath196 \\displaystyle + \\infty & \\displaystyle    \\textnormal{otherwise , } \\end{array } \\right.\\ ] ] and @xmath197 denote the distributions of @xmath193 conditional on the inputs @xmath198 , @xmath199 , and on @xmath190 , respectively . thus @xmath200 is the law of an @xmath169-variate gaussian random vector of mean @xmath175 and of diagonal covariance matrix @xmath201 with diagonal entries    lcl ^()__-^-1(1,1 ) & = & ^2+_=-^-1_-lx_l+1 ^ 2 + ^()__-^-1(i , i ) & = & ^2+_i-1 ^ 2+_=-^-1_- l+i-1x_l+1 ^ 2,i=2,  ,l ;    @xmath202 is the law of an @xmath169-variate , zero - mean gaussian random vector of diagonal covariance matrix @xmath203 with diagonal entries @xmath204 and @xmath205 is given by @xmath206    in order to evaluate the first term on the right - hand side ( rhs ) of we note that the relative entropy of two real , @xmath169-variate gaussian random vectors of means @xmath207 and @xmath208 and of covariance matrices @xmath209 and @xmath210 is given by    lcl d ( ) & = & _ 2 - _ 1 + + & & + _ 2 ^ -1(_1-_2 ) , [ eq : dgaussian ]    with @xmath211 and @xmath212 denoting the determinant and the trace of the matrix @xmath213 , and where @xmath214 denotes the @xmath215 identity matrix . the second term on the rhs of is analyzed in the next subsection .",
    "let @xmath216 denote the second term on the rhs of averaged over @xmath217 , i.e. ,    c = .    then using & and taking expectations over @xmath217",
    ", we obtain , again defining @xmath218 ,    lcl i(_0;_0|_-^-1 ) & = & _ i=1^l + & & -_i=2^l + & & - + & & _ i=1^l + & & -_i=2^l ( 1+_i-1 ^2/^2 ) + & & - + & & _ i=1^l + & & -_i=2^l + & & - , [ eq : lbbeforelimit ]    where the first inequality follows by the lower bound @xmath219 , which is a consequence of jensen s inequality applied to the convex function @xmath220 , @xmath221 , and by the upper bound    c ( 1+_i-1 ^2/^2 ) , i=2,  ,l ;    and the second inequality follows by and by upper bounding @xmath222    the final lower bound follows now by and    lcl _ n i(_0^n / l -1;_0^n / l -1 ) & &",
    "_ i=1^l + & & - _ i=2^l + & & - [ eq : lbfinal ]    and by recalling that @xmath223      we start with analyzing the upper bound . using that @xmath224 ,",
    "@xmath225 we have @xmath226 and we thus obtain @xmath227    in order to derive a lower bound on @xmath143 we first note that @xmath228 and proceed by analyzing the limiting ratio of the lower bound to snr as snr tends to zero . to this end we first shall show that @xmath229 we recall that for any pair of distributions @xmath230 and @xmath231 satisfying @xmath232 @xcite @xmath233 thus , for any given @xmath190 , together with @xmath234 implies that @xmath235 in order to show that this also holds when @xmath236 is averaged over @xmath217 , we derive in the following the uniform upper bound    c _ _ -^-1",
    "d(p__0|_-^-1 p__0|x_1=0,_-^-1 ) = .d(p__0|_-^-1 p__0|x_1=0,_-^-1)|__-^-1=0 .",
    "[ eq : uniformbound ]    the claim follows then by upper bounding    c .d(p__0|_-^-1",
    "p__0|x_1=0,_-^-1)|__-^-1=0    and by .    in order to prove we use that any gaussian random vector can be expressed as the sum of two independent gaussian random vectors to write the channel output @xmath193 as @xmath237 where , conditional on @xmath238 , @xmath239 and @xmath240 are @xmath169-variate , zero - mean gaussian random vectors , drawn independently of each other and having the respective diagonal covariance matrices @xmath241 and @xmath242 whose diagonal entries are given by    lcl _",
    "|_0(1,1 ) & = & ^2 + _ |_0(i , i ) & = & ^2 + _",
    "i-1x_1 ^ 2 , i=2, ",
    ",l ,    and @xmath243 thus @xmath240 is the portion of the noise due to @xmath217 , and @xmath239 is the portion of the noise that remains after subtracting @xmath240 .",
    "note that @xmath244 and @xmath240 are independent of each other because @xmath194 is , by construction , independent of @xmath217 .",
    "the upper bound follows now by    lcl d(p__0|_-^-1 p__0|x_1=0,_-^-1 ) & = & d(p__0++|_-^-1 p__0++|x_1=0,_-^-1 ) + & & d(p__0 + p__0+|x_1=0 ) + & = & .d(p__0|_-^-1 p__0|x_1=0,_-^-1)|__-^-1=0,[eq : asym1 ]    where @xmath245 denote the distributions of @xmath246 conditional on the inputs @xmath190 and on @xmath247 , respectively ; @xmath248 denotes the unconditional distribution of @xmath244 ; and @xmath249 denotes the distribution of @xmath244 conditional on @xmath250 .",
    "here the inequality follows by the data processing inequality for relative entropy ( see ( * ? ? ?",
    "2.9 ) ) and by noting that @xmath244 is independent of @xmath217 .",
    "returning to the analysis of , we obtain from and    lcl ( 0 ) & & _ 0 + & & _ 0 _ i=1^l - _ i=2^l + & = & _ i=1^l _ i-1 - _ i=2^l .    by letting first @xmath251",
    "go to infinity while holding @xmath169 fixed , and by letting then @xmath169 go to infinity , we obtain the desired lower bound on the capacity per unit cost @xmath252 thus , , and yield @xmath253 which proves theorem  [ thm : lowsnr ] .",
    "in order to show that @xmath254 implies that the feedback capacity @xmath93 is bounded , we derive a capacity upper bound which is based on and on an upper bound on @xmath255 .",
    "again we define @xmath256 .",
    "we first note that , according to , we can find an @xmath257 and a @xmath65 so that @xmath258 we continue with the chain rule for mutual information    lcl i(m;y_1^n ) & = & _ k=1^_0 i(m;y_k|y_1^k-1)+_k=_0 + 1^n i(m;y_k|y_1^k-1).[eq : firstsum ]    each summand in the first sum on the rhs of is upper bounded by    lcl i(m;y_k|y_1^k-1 ) & & h(y_k ) - h(y_k|y_1^k-1,m ) + & = & h(y_k)- - h(u_k|u_1^k-1 ) + & & ( 2e(1+_=1^k _ k- ) ) - h(u_k|u_1^k-1 ) + & & ( 2e(1 + ( _  ^+_0 _  ) _",
    "= 1^k ) ) - h(u_k|u_1^k-1 ) + & & ( 2e(1+(_ ^+_0 _  ) n ) ) -h(u_k|u_1^k-1 ) + & & ( 2e(1+(_ ^+_0 _  )",
    "n ) ) -h(u_k|u_-^k-1 ) .",
    "[ eq : firstterm ]    recall that @xmath259 is finite . here",
    "the first inequality follows because conditioning can not increase entropy ; the following equality follows because @xmath260 is a function of @xmath261 , from the behavior of entropy under translation and scaling ( * ? ? ?",
    "9.6.3 & 9.6.4 ) , and from the fact that , conditional on @xmath262 , @xmath147 is independent of @xmath263 ; the subsequent inequality follows from the entropy maximizing property of gaussian random variables and by lower bounding ; the next inequality by upper bounding each coefficient , @xmath264 ; the subsequent inequality follows from the power constraint ; and the last inequality follows because conditioning can not increase entropy .",
    "the summands in the second sum on the rhs of are upper bounded using the general upper bound for mutual information ( * ? ? ?",
    "5.1 ) @xmath265 where @xmath266 is the channel law , @xmath267 is the distribution on the channel input @xmath268 , and @xmath269 is any distribution on the output alphabet .",
    "thus any choice of output distribution @xmath269 yields an upper bound on the mutual information .",
    "we upper bound @xmath270 , @xmath271 for a given @xmath272 by choosing @xmath269 to be a cauchy distribution whose density is given by @xmath273 where we choose the scale parameter @xmath274 to be then with this choice of @xmath274 the density of the cauchy distribution is undefined . however , this event is of zero probability and has therefore no impact on the mutual information @xmath275 . ]",
    "@xmath276 with @xmath65 and @xmath277 given by .",
    "note that together with implies that @xmath278 applying to yields    lcl i(m;y_k|y_1^k-1=y_1^k-1 ) & & + ( y_k-_0 ^ 2 ) + & & + - h(y_k|m , y_1^k-1=y_1^k-1 ) ,    and we thus obtain , averaging over @xmath47 ,    lcl i(m;y_k|y_1^k-1 ) & & - h(y_k|y_1^k-1,m ) + + & & + - -.[eq : ub1 ]    we evaluate the terms on the rhs of individually .",
    "we begin with @xmath279 where we use the same steps as in the equality in and that conditioning can not increase entropy .",
    "the next term is upper bounded by    lcl & = & + & & + & = & + & & , [ eq:2 ]    where we define , for a given @xmath280 , @xmath281 here the first inequality in follows from jensen s inequality , and the second inequality follows from . similarly we use jensen s inequality along with to upper bound    lcl & & + & & 2 + .[eq:3 ]    in order to lower bound @xmath282 we need the following lemma :    [ lemma : expectedlog ] let @xmath268 be a random variable of density @xmath283 , @xmath284 .",
    "then , for any @xmath285 and @xmath286 we have @xmath287 where @xmath288 denotes the indicator function takes on the value @xmath121 if the statement is true and @xmath122 otherwise . ]",
    "; @xmath289 is defined as @xmath290 and where @xmath291 tends to zero as @xmath292 .",
    "see ( * ? ? ?",
    "* lemma 6.7 ) .",
    "we write the expectation as    lcl & = &    and lower bound the conditional expectation for a given @xmath293 by    lcl + & = & ^2(x_1^k-_0 - 1 ) - 2 + & & ^2(x_1^k-_0 - 1 ) -2 ( , ) - h^-(u_k-_0)+ ^2 [ eq : bla1 ]    for some @xmath285 and @xmath294 . here",
    "the inequality follows by splitting the conditional expectation into the two expectations    lcl + & = & + & & +    and by upper bounding then the first term on the rhs using lemma  [ lemma : expectedlog ] and the second term by @xmath295 . averaging over @xmath296 yields    lcl & & - 2(,)-h^-(u_k-_0)+^2.[eq:4 ]    note that , since @xmath297 is of unit variance , together with ( * ? ? ?",
    "* lemma 6.4 ) implies that @xmath298 is finite .    turning back to the upper bound we obtain from , , , and    lcl + & & - - h(u_k|u_-^k-1 ) + & & + + 2 + + & & -+ 2(,)+ h^-(u_k-_0 ) -^2 - + & & - + , [ eq : ub2 ]    where @xmath299 is a finite constant , and where the last inequality in follows because for any @xmath300 we have @xmath301 .",
    "note that @xmath302 does not depend on @xmath0 as the process @xmath58 is stationary .",
    "turning back to the evaluation of the second sum on the rhs of , we use that for any sequences @xmath303 and @xmath304    lcl _ k=_0 + 1^n ( a_k - b_k ) & = & _ k = n-2_0 + 1^n ( a_k - b_k - n+3_0 ) + _ k=_0 + 1^n-2_0(a_k - b_k+2_0 ) . [ eq : sums ]    defining @xmath305 and @xmath306 we have for the first sum on the rhs of    lcl _ k = n-2_0 + 1^n ( a_k - b_k - n+3_0 ) & = & _ k = n-2_0 + 1^n + & & 2_0 ( 1 + ( _ ^+_0 _ ) n ) [ eq : wholesum1 ]    which follows by lower bounding the denominator by @xmath32 , and by using then jensen s inequality together with the third and fourth inequality in .",
    "for the second sum on the rhs of we have    lcl _ k=_0 + 1^n-2_0(a_k - b_k+2_0 ) & = & _ k=_0 + 1^n-2_0 + & & _ k=_0 + 1^n-2_0 - ( n-3_0 ) + & & -(n-3_0 ) , [ eq : wholesum2 ]    where the first inequality follows by adding @xmath307 to the expectation and by upper bounding then @xmath308 , @xmath309 ; and the last inequality follows because for any given @xmath310 we have @xmath311 .",
    "we apply now , , , and to upper bound    lcl _",
    "= _ 0 + 1^n i(m;y_k|y_1^k-1 ) & & + ( 1 + ( _ ^+_0 _ ) n ) - [ eq : ub3 ]    which together with and yields    lcl i(m;y_1^n ) & & - + ( 2e ) - h(u_k|u_-^k-1 ) + & & + ( 1 + ( _ ^+_0 _ ) n ) .",
    "this converges to @xmath312 as we let @xmath37 tend to infinity , thus proving that @xmath313 implies that the capacity @xmath93 is bounded in the @xmath95 .",
    "we shall show that @xmath314 implies that the capacity @xmath83 in the absence of feedback is unbounded in the snr .",
    "part ii ) of theorem  [ thm : highsnr ] follows then by noting that @xmath315    we prove the claim by proposing a coding scheme that achieves an unbounded rate .",
    "we first note that implies that for any @xmath316 we can find an @xmath317 so that @xmath318    if there exists an @xmath317 so that @xmath319 , @xmath320 , then we can achieve the ( unbounded ) rate @xmath321 by a coding scheme where the channel inputs @xmath322 are iid , zero - mean gaussian random variables of variance @xmath323 , and where the other inputs are deterministically zero . indeed , by waiting @xmath169 time - steps , the chip s temperature cools down to the ambient one so that the noise variance is independent of the previous channel inputs and we can achieve ",
    "after appropriate normalization  the capacity of the additive white gaussian noise ( awgn ) channel @xcite .    for the more general case",
    "we propose the following encoding and decoding scheme .",
    "let @xmath324 , @xmath325 denote the codeword sent out by the transmitter that corresponds to the message @xmath326 .",
    "we choose some @xmath327 and generate the components @xmath328 , @xmath325 , @xmath329 independently of each other according to a zero - mean gaussian law of variance @xmath330 .",
    "the other components are set to zero .",
    ", @xmath331 converges to @xmath332 in probability as @xmath37 tends to infinity .",
    "this guarantees that the probability that a codeword does not satisfy the per - message power constraint and hence also the average - power constraint vanishes as @xmath37 tends to infinity . ]",
    "the receiver uses a _",
    "nearest neighbor decoder _ in order to guess @xmath34 based on the received sequence of channel outputs @xmath333 .",
    "thus it computes @xmath334 for each @xmath335 and decides on the message that satisfies @xmath336 where ties are resolved with a fair coin flip . here , @xmath337 denotes the euclidean norm , and @xmath338 and @xmath339 denote the respective vectors @xmath340 and @xmath341 .",
    "we are interested in the average probability of error @xmath342 , averaged over all codewords in the codebook , and averaged over all codebooks . by the symmetry of the codebook construction ,",
    "the probability of error corresponding to the @xmath343-th message @xmath344 does not depend on @xmath343 , and we thus conclude that @xmath345 .",
    "we further note that @xmath346 where @xmath347 which is , conditional on @xmath348 , equal to @xmath349 . in order to analyze",
    "we need the following lemma .",
    "[ lemma : typical ] consider the channel described in section  [ sec : channelmodel ] , and assume that @xmath33 satisfies .",
    "further assume that @xmath350 is a sequence of iid , zero - mean gaussian random variables of variance @xmath330 , and that @xmath351 if @xmath352 ( where @xmath353 stands for the remainder upon diving @xmath0 by @xmath169 ) .",
    "let the set @xmath354 be defined as    lcll _ & & \\ { ( , ) ^n",
    "/ l^n / l : & | ^2-(^2++^(l ) ) | < , + & & & | ^2-(^2+^(l ) ) | < } , [ eq : setdef ]    with @xmath355 being defined as @xmath356 then @xmath357 for any @xmath358 .",
    "see appendix  [ app : lemmatypical ] .    in order to upper bound the rhs of we proceed along the lines of @xcite , @xcite .",
    "we have    lcl + & & ( ( , ) _ ) + _ _ ( _ m=2^|| -(m)^2 ^2 | ( , ) , m=1 ) p ( , ) , [ eq : typical1 ]    where we use that , by the symmetry of the codebook construction , the law of @xmath359 does not depend on @xmath34 .",
    "it follows from lemma  [ lemma : typical ] that the first term on the rhs of vanishes as @xmath37 tends to infinity .",
    "since the codewords are independent of each other , conditional on @xmath348 , the distribution of @xmath360 , @xmath361 does not depend on @xmath362 .",
    "we upper bound the second term on the rhs of by analyzing @xmath363 , @xmath361 and by applying then the union of events bound .    for @xmath361 ,",
    "we have    lcl + & & \\{-s n / l ( ^2+^(l ) + ) + - n / l ( 1 - 2s ) } , ( , ) _ [ eq : typical2 ]    for any @xmath364 .",
    "this follows by upper bounding @xmath365 by @xmath366 and from chernoff s bound ( * ? ? ?",
    "5.4 ) . using that , for @xmath367 , @xmath368 it follows from the union of events bound and from that goes to zero as @xmath37 tends to infinity if for some @xmath364 the rate @xmath75 satisfies    lcl r & < & ( ^2+^(l ) + ) + ( 1 - 2s ) - .",
    "thus choosing @xmath369 yields that any rate below    l - + ( 1 + ) + + [ eq : ach1 ]    is achievable .",
    "as @xmath330 tends to infinity this converges to @xmath370    it remains to show that given we can make @xmath371 arbitrarily large . indeed",
    ", implies that @xmath372 and can therefore be further lower bounded by @xmath373 letting @xmath169 tend to infinity yields then that we can achieve any rate below @xmath374 .",
    "as this can be made arbitrarily large by choosing @xmath375 sufficiently small , we conclude that @xmath376 implies that the capacity is unbounded .",
    "we studied a model for on - chip communication with nonideal heat sinks . to account for the heating up effect",
    "we proposed a channel model where the variance of the additive noise depends on a weighted sum of the past channel input powers .",
    "the weights characterize the efficiency of the heat sink .    to study the capacity of this channel at low snr , we computed the capacity per unit cost .",
    "we showed that the heating effect is not just unharmful but can be even beneficial in the sense that the capacity per unit cost can be larger than the capacity per unit cost of a corresponding channel with ideal heat sink , i.e. , where the weights describing the dependency of the noise variance on the channel input powers are zero .",
    "this suggests that at low snr no heat sinks should be used .",
    "studying capacity at high snr , we derived a sufficient condition and a necessary condition on the weights for the capacity to be bounded in the snr .",
    "we showed that when the sequence of weights decays not faster than geometrically , then capacity is bounded in the snr . on the other hand ,",
    "if the sequence of weights decays faster than geometrically , then capacity is unbounded in the snr .",
    "this result demonstrates the importance of an efficient heat sink at high snr .",
    "fruitful discussions with ashish khisti and michle wigger are gratefully acknowledged .",
    "sergio verd s comments at the isit 2007 on our low snr results are also much appreciated .",
    "we first note that by the expression of the capacity per unit cost of a memoryless channel @xcite we have @xmath377 where @xmath378 denotes the channel law of the channel @xmath379 thus to prove proposition  [ prop : lowsnr ] it suffices to show that @xmath380 we shall obtain this result by deriving a lower bound on @xmath88 and by computing then its limiting ratio to @xmath95 as @xmath95 tends to zero .    in order to lower bound @xmath88 ,",
    "which was defined in as @xmath381 we evaluate @xmath382 for inputs @xmath156 that are blockwise iid in blocks of @xmath169 symbols ( for some @xmath170 ) . thus @xmath383 is a sequence of iid random length-@xmath169 vectors with @xmath384 taking on the value @xmath385 with probability @xmath176 and @xmath386 with probability @xmath178 , for some @xmath179 . to satisfy the power constraint we shall choose @xmath180 and @xmath176 such that @xmath387    we use the chain rule for mutual information to write    lcl i(x_1^n;y_1^n ) & = & _ b=0^n / l -1 i(x_b l+1;y_1^n|x_1^b l ) + & & _ b=0^n / l -1 i(x_bl+1;y_bl+1|x_1^bl ) , [ eq : app1 ]    where the inequality follows because reducing observations can not increase mutual information .",
    "let @xmath388 denote the maximum rate achievable on using on - off keying with on - symbol @xmath180 and with its corresponding probability @xmath389 chosen in order to satisfy the power constraint @xmath390 , i.e. , @xmath391 notice that @xmath392 , @xmath393 is a nonnegative , monotonically nondecreasing function of @xmath390 with @xmath394 . from the strict concavity of mutual information",
    "it follows that @xmath395 whenever @xmath396 . also , for a fixed @xmath180 , @xmath397 is concave in @xmath390 .",
    "consequently , for some @xmath398 , the function @xmath397 is strictly monotonic in the interval @xmath399 $ ] , and hence the supremum on the rhs of is attained for @xmath400 , @xmath399 $ ] .    by writing @xmath401 for a given @xmath402 as    lcl i(x_bl+1;y_bl+1|x_1^bl = x_1^bl ) & = & i(x_bl+1;x_bl+1+(x_1^bl)u_bl+1 ) + & = & i(x_bl+1;x_bl+1+u_bl+1 )    ( with @xmath403 defined in ) , and by using that for @xmath399",
    "$ ] the supremum on the rhs of is attained for @xmath400 we obtain    lcl i(x_bl+1;y_bl+1| x_1^bl = x_1^bl ) & = & r^()_on - off ( ) , ,    where @xmath404 .",
    "averaging over @xmath405 and combining with yields    lcl i(x_1^n;y_1^n ) & & _",
    "b=0^ n / l -1 + & & r^()_on - off ( ) , ,    where the second inequality follows by upper bounding @xmath406 , and by using that @xmath407 is monotonically increasing in @xmath390 . the lower bound on @xmath88 follows then by letting @xmath37 tend to infinity @xmath408 with this we can lower bound the information capacity per unit cost as    lcl _",
    "> 0 & & _ 0 + & & _ 0 + & = & _ 0 + & = & _ 0 , +    where the first inequality follows by lower bounding the supremum by the limit ; and where the last equality follows by substituting .    proceeding along the lines of the proof of ( * ? ? ?",
    "3 ) , it can be shown that @xmath409 and therefore @xmath410 noting that & imply @xmath411 we obtain by letting @xmath169 tend to infinity @xmath412 maximizing over @xmath251 yields then @xmath413 which , in view of , proves proposition  [ prop : lowsnr ] .",
    "we shall prove that @xmath414 let @xmath415 be defined as    lcl ^(1)_0 & & 0 + _",
    "b^(i ) & & _ b l+i-1 , ( b , i ) ^+_0 ^+\\{(0,1)}.    we have    lcl i(_-^-1;_b|_0^b ) & = & _ i=1^l i(_-^-1;_b l + i|_0^b , _",
    "b l+1^b l+i-1 ) + & & _ i=1^l ( h(_bl+i|_0^b)-h(_bl+i|_-^b ) ) + & & _ i=1^l + & & - _ i=1^l + & &",
    "_ i=1^l + & & - _ i=1^l + & = & _ i=1^l + & & _ i=1^l ( 1 + l _ = b+1^ _ ^(i ) ) ,    where the first inequality follows because conditioning can not increase entropy and because , conditional on @xmath416 , @xmath417 is independent of @xmath418 ; the next inequality follows from the entropy maximizing property of gaussian random variables ; the subsequent inequality follows because @xmath419 , @xmath420 ; and the last inequality follows because @xmath421 , @xmath420 .    by upper bounding @xmath422",
    "we obtain @xmath423 and follows by noting that implies @xmath424",
    "we shall show that for any @xmath425 @xmath426 and @xmath427 lemma  [ lemma : typical ] follows then by the union of events bound .                                  where @xmath438 denotes the covariance between @xmath433 and @xmath439 .",
    "we shall evaluate both terms on the rhs of separately .",
    "for the sake of clarity , we shall omit the details of the derivations and show only the main steps . unless otherwise stated these steps can be derived in a straightforward way using that    1 .",
    "@xmath440 is a sequence of iid , zero - mean , variance-@xmath330 gaussian random variables whose fourth moments are given by @xmath441 , while all odd moments are zero ; 2 .",
    "@xmath351 if @xmath442 ; 3 .",
    "@xmath443 ( and hence also @xmath444 ) is a zero - mean , unit - variance , stationary & weakly - mixing random process ; 4 .   and that @xmath156 and @xmath58 are independent of each other .",
    "for the first sum on the rhs of it suffices to show that @xmath445 , @xmath446 .",
    "indeed , this sum contains only @xmath447 summands and hence , when divided by @xmath448 , this sum vanishes as @xmath37 tends to infinity , given that @xmath445 , @xmath446 .",
    "we have    lcl ( y_kl+1 ^ 2 ) & = & -()^2 + & & + & = & + & = & 3 ^2 + 6 ( ^2+_=1^k _ l ) + & & + ( ^4 + 2 ^ 2_=1^k_l + 2 ^ 2 _ = 1^k _ l^2 + ^2 ( _ = 1^k _ kl)^2 ) + & & 3 ^2 + 6(^2 + ^(l ) ) + & & + ( ^4 + 2 ^ 2 ^(l)+2 ^2 _ = 1^",
    "_ l^2 + ^2(^(l))^2 )    where the second inequality follows by upper bounding @xmath449 .",
    "note that implies that @xmath355 and @xmath450 are bounded .",
    "it follows therefore by noting that @xmath451 has a finite fourth moment that ( for a finite @xmath330 ) @xmath452                  lcl + & = & _ ^n / l -1 ( 2 ^ 2 _ ( k - j)l+2 ^ 2_=1^j_l _ ( + k - j)l + & & + ( ^2+_=1^k _ l)(^2+_=1^j_ l)(-1 ) ) + & = & _ j=0^n / l -2 _",
    "= 1^n / l -1-j ( 2 ^ 2 _",
    "l+2 ^ 2_=1^j_l _ ( + ) l + & & + ( ^2+_=1^j+ _ l)(^2+_=1^j_ l)(-1 ) ) + & = & _ j=0^n / l -2 _",
    "= 1^n / l -1-j 2 ^ 2 _ l + & & + _ j=0^n / l -2 _",
    "= 1^n / l -1-j 2 ^ 2_=1^j_l _ ( + ) l + & & + _ j=0^n / l -2 _",
    "= 1^n / l -1-j(^2+_=1^j+ _",
    "l)(^2+_=1^j_ l)(-1 ) , + [ eq : typicalsum1 ]                                      r.  venkatesan , a.  kaloyeros , m.  beylansky , s.  j. souri , k.  banerjee , k.  c. saraswat , a.  rahman , r.  reif , and j.  d. meindl , `` interconnect limits on gigascale integration ( gsi ) in the 21st century , '' _ proc .",
    "89 , no .  3 , pp . 305324 , mar .",
    "2001 .",
    "a.  lapidoth and s.  m. moser ,",
    "`` capacity bounds via duality with applications to multiple - antenna systems on flat fading channels , '' _ ieee trans .",
    "inform . theory _",
    "49 , no .",
    "10 , pp . 24262467 , oct ."
  ],
  "abstract_text": [
    "<S> this work considers an additive noise channel where the time-@xmath0 noise variance is a weighted sum of the channel input powers prior to time @xmath0 . </S>",
    "<S> this channel is motivated by point - to - point communication between two terminals that are embedded in the same chip . </S>",
    "<S> transmission heats up the entire chip and hence increases the thermal noise at the receiver . </S>",
    "<S> the capacity of this channel ( both with and without feedback ) is studied at low transmit powers and at high transmit powers .    at low transmit powers , the slope of the capacity - vs - power curve at zero is computed and it is shown that the heating - up effect is beneficial . at high transmit powers , </S>",
    "<S> conditions are determined under which the capacity is bounded , i.e. , under which the capacity does not grow to infinity as the allowed average power tends to infinity . </S>"
  ]
}