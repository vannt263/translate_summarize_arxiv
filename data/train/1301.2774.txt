{
  "article_text": [
    "there are quite compelling arguments against current computer systems , because many applications ( especially in the domain of artificial intelligence ) suffer from current system imperfections .",
    "for instance , computers are not completely capable of understanding image contents ; hence , they can not search within images efficiently with respect to their contents . in order to dissect these contents , they can only process the syntactical features of the images such as color , texture , and shape .",
    "clearly , representing the content by using such low - level features can be erroneous .",
    "this problem is similar to estimating the level of happiness of a human in an image by only considering its color histogram !",
    "such complications have initiated the motivation to search for alternative approaches to the classic ai systems .",
    "one of the best proposed solutions to this problem is  crowd computing \" .",
    "crowd computing , or problem solving using crowdsourcing , utilizes humans perception ( understanding and feeling ) , and intellectual abilities to solve non - algorithmic problems .",
    "there are many complexities in using crowd computing .",
    "humans problem solving processes can not be completely modeled , and there are no generative models to predict the answers of the future problems .",
    "in addition , for each solution provided by any human for any problem some cost ( or bonus ) must be paid .",
    "moreover , usually not all participating humans are experts , and their knowledge is limited .",
    "furthermore , human decisions maybe noisy , and erroneous . finally , humans have lower computing speeds compared to the current computer systems",
    ".    a crowd computing system must overcome the aforementioned complexities . in recent years",
    ", a number of general frameworks are proposed for this purpose .",
    "we call them  crowd computing scenarios \"",
    ". we will introduce these scenarios in the next section . in practice , expressing problems in terms of those scenarios is not light - handed , and is varying for different problems and applications .",
    "in addition , in most cases , the expression leads to generating a large number of small problems which are costly to solve .    in this paper",
    ", we survey the concept of crowd computing , its challenges and complexities , and the ways to overcome its problems .",
    "the rest of paper is organized as follows . in section 2 ,",
    "crowd computing concepts , scenarios , and system design will be discussed .",
    "section 3 presents the arguments that why crowd computing is required .",
    "section 4 provides the technical aspects of crowd computing and its scenarios ; including literary methods for sample selection , user modeling , and integration of user provided solutions , as well as their analysis and comparisons .",
    "the concluding remarks and open issues are presented in the last section .",
    "crowd computing is referred to problem solving using crowdsourcing .",
    "crowdsourcing , which is semantically a composition of the terms  wisdom of crowds \" and  outsourcing \" , was coined by jeff howe in 2006 in an article in wired magazine @xcite .",
    "he also published a book by the same title in 2008 @xcite . in crowdsourcing ,",
    "a group of people are asked to contributively do a task that can not be easily done by a single individual .",
    "for example ,  wikipedia \" is one of the most recognized crowdsourcing systems . in this system ,",
    "thousands of internet users are participating in the creation of the world s largest encyclopedia .",
    "some economy and management researchers believe that crowdsourcing can change the future of business @xcite .",
    "the most remarkable applications of crowdsourcing are : creation ( e.g. wikipedia or open - source softwares ) , standby human resources ( e.g.  rent a coder \" , an active network in the field of software design and development ) , r&d ( e.g. innocentive ) , crowd funding ( e.g. kickstarter network for funding creative projects ) , forecasting ( e.g. threadless network which estimates the success rate of t - shirt designs in the market ) , organization ( e.g. digg network for organizing internet links ) and crowd computing or collective intelligence ( e.g. amazon mechanical turk network as a marketplace for providing solutions to micro tasks ) .    in this section ,",
    "we introduce crowd computing .",
    "first , we illustrate crowd computing by using an example",
    ". then we present the crowd computing scenarios .",
    "next , we propose the properties of problems and applications that are suitable to be solved using crowd computing . then , in order to correct the expectations from crowd computing , we report on performance of some crowd computing systems .",
    "finally , we describe three steps that are required to design a crowd computing system .",
    "correcting the errors of classical artificial intelligence ( ai ) systems is one of the crowd computing applications .",
    "an example of such systems is optical character recognition ( ocr ) .",
    "although , current ocr systems have acceptable performances on high quality scanned texts , they have poor performances on low quality or old faded - ink characters .",
    "crowd computing can help ocr systems in recognizing these documents more efficiently .",
    "recaptcha is an example of crowd computing system that helps ocr systems recognize low quality documents , while protecting websites from bots attempting to access restricted areas @xcite . in order to analyze scanned documents",
    ", recaptcha uses two different ocr systems .",
    "the respective outputs of these ocrs are aligned and compared with each other , and then are checked with a reference dictionary .",
    "any word that is deciphered differently by both ocr programs or is not in the dictionary will be marked as suspicious .",
    "the suspicious words images are the system sub - problems which must be recognized .",
    "recaptcha uses captcha tests .",
    "a common type of captcha test shows an image of distorted letters or digits , and asks the user to type the text shown in that image .",
    "human can recognize the distorted texts , but computers can not @xcite .",
    "a sample captcha is illustrated in fig .",
    "[ imgcaptcha]-(left ) .",
    "in contrast to a captcha test , a recaptcha test shows two words in an image ; a distorted word with known digitized correspondent , along with a word that the ocr system is unable to recognize . the first word will be used for user validation , while the second one will be used to help ocrs to recognize  suspicious \" words .",
    "users are asked to type both words correctly before being allowed through .",
    "a sample recaptcha image is shown in fig .",
    "[ imgcaptcha]-(right ) .    as users do not know which word is the control word",
    ", recaptcha assumes that if they type the control word correctly , the questionable word is also correct .",
    "in addition , in order to overcome human errors and frauds , it uses a multiple checking mechanism .",
    "those words that are consistently given a single identity by humans are recycled as control words @xcite .",
    "the tests over a dataset containing all 24080 word images of 50 random scanned articles from five different years ( 1860 , 1865 , 1908 , 1935 , and 1970 ) of the new york times archive , indicate significant results of recaptcha .",
    "recaptcha has achieved an accuracy of 99.1% , whereas the accuracy of a standard ocr has been 83.5% . in this example",
    ", 6260 words were marked as suspicious , which only about 4% of them has been recognized by both ocr programs , while 95.94% of them has been recognized by recaptcha . in the first year after launch ,",
    "more than 40,000 websites deployed recaptcha , and over 440 million words has been transcribed by recaptcha @xcite .    recaptcha does not pay any money to the websites or the users ; rather , it provides an authentication service to them . however , not all the crowd computing applications have access to such opportunity , and rather they use marketplaces that charge some money to provide human answers to their micro problems .",
    "amazon mechanical turk ( mturk ) is a famous crowd computing marketplace .",
    "mturk has more than one hundred thousand members and hundreds of thousands research and business tasks .",
    "mturk has provided developers with apis that enable them to connect their systems directly to mturk .",
    "the main reasons of mturk s popularity are : large number of members , high diversity of members knowledge , skills , locations , cultural differences and socio - economic status , low - cost labors and fast cycle of theory and test @xcite .      in a crowd computing system",
    ", each problem or application ( e.g. recognizing words of a document ) is divided into several sub - problems ( e.g. single word images ) . some of sub - problems are machine solvable ( e.g. images that are recognizable by both ocr systems in recaptcha ) , while some others are not and they need human intelligence ( e.g. suspicious word images ) . in order to remove noise , bias , and error ,",
    "the provided solutions by humans should be validated ( e.g. by a multiple checking mechanism ) . finally , the provided solutions must be integrated to extract the solution of the sub - problem ( e.g. the text of the single word image ) .",
    "recaptcha uses website users to solve the hard problems , and a multiple checking mechanism to overcome the noise due to humans mistakes , bias and error . in a general multiple checking scenario ,",
    "the sub - problem solutions are requested from a number of humans , and each human is paid a small amount of money for each solution .",
    "in addition , there is a mechanism for integrating the provided solutions .",
    "the simplest mechanism for integration is the majority voting .",
    "multiple checking is not the only scenario in crowd computing .",
    "game with a purpose ( gwap ) , and iterative tasks are also among favorite scenarios in crowed sourcing .    in general , the designers of gwaps are trying to embed their problems into a game .",
    "the first gwap , named esp , which is an online game for image annotation , was originally conceived by luis von ahn @xcite . in this game ,",
    "two online users are paired randomly by the system .",
    "the paired players do not know each other , and they do not communicate .",
    "an image is displayed to the players , and in a specified period of time , the players independently guess the image content by presenting text tags ( they do not see each others tags ) .",
    "the players win the game just in the case that one of them presents a tag which is presented before by the other player , considering the constraint that they are not allowed to use the words presented in a taboo list .",
    "the taboo list is provided by the system in order to exclude the obvious tags or the tags obtained previously for that image in the other games .",
    "the resulting tag is then used as a new annotation for that image .",
    "esp game can provide one tag for all images in the google indexed images only in a month , if it becomes a high ranked game in the online games websites ( considering the statistics of the year that the paper is published ) @xcite .     iterative \" or  collaborative tasks \" is another scenario for crowd computing . in this scenario , users build on or evaluate each others answers @xcite . here ,",
    "in contrast to parallel tasks , users have access to the other users answers . since it might bias the user s mind , it is not suitable for tasks like voting , or brainstorming @xcite .",
    "in addition , parallel and iterative paradigms can be used together .",
    "for example , in a text improving problem , each passage is improved by a user , another user improves the first user s work , and the third user selects the best of these two .",
    "another user improves the winner work , and the cycle will continue until reaching the stopping criteria @xcite .",
    "a suitable application or problem to be solved by crowd computing should have the following features :    * it should be divisible into several sub - problems .",
    "these sub - problems should be almost independent ( they could be solved in parallel )",
    ". they should also be static in time , in order to keep the validity of the integrated solution of the original problem .",
    "* a large number of sub - problems should be non - solvable by machines , and could be solved by a regular human .",
    "the solution of these sub - problems should be independent of the users , and verifiable by other users .",
    "* solving the main problem using a small group of expert people should be costly .",
    "* there should be a feasible method to divide the original problem into sub - problems , and to integrate the sub - solutions .",
    "for example , consider the content based image retrieval ( cbir ) application using image annotations . in one hand ,",
    "image annotating based on the image content , is a very hard problem and should be done by humans for millions of images . on the other hand ,",
    "collecting and organizing the images and their corresponding tags as well as searching within them can be only done efficiently by computers .",
    "then it is rational to use crowd computing for this application .",
    "there are a number of other applications that are suitable for solving by crowd computing : writing text improvement @xcite , evaluation of music similarities @xcite , measuring the relevance between results and keywords in search engines ( information retrieval ) @xcite , text translation @xcite , evaluation of common sense knowledge @xcite , affect recognition in text , image and video @xcite , building train and evaluation datasets for classic machine learning algorithms @xcite , and error - detection in classic ai systems ( such as ocr systems ) @xcite , are samples of these applications .    from another point of view ,",
    "two categories of problems are suitable to be solved using crowd computing : problems which need human consciousness , and problems which need human common sense knowledge .",
    "we will survey these two categories , in section [ whycc ] .",
    "the reported results of implemented crowd computing systems show the power of this approach in the candid practical applications .",
    "let us have a look at the results reported in @xcite , where crowd computing is used for typo and grammatical error correction , summarization and overall improvement of english texts , written by non - native writers .",
    "the authors used mturk as their human network .",
    "the reported results show that the output texts had 10 to 22% shorter length in the summarization tests , and 67% less errors in the text correction than input texts .",
    "the cost of such a task has been $ 1.41 per paragraph .",
    "although choosing the sub - problems by mturk users takes a long time ( the waiting time ) , solving sub - problems by them are quick . increasing the number of marketplaces in the future , or creating the proprietary human networks for applications , results in decreasing the waiting time",
    "disregarding the waiting time , considering the large number of users and the possibility of doing the tasks in a parallel mode , leads to decreasing the overall time of improving any text to less than five minutes .    in @xcite ,",
    "concerning the accuracy of the user answers , mturk users are compared to experts .",
    "five different categories of natural language processing tasks are used for this purpose .",
    "these categories are affect recognition in texts , word semantic similarity measuring , recognizing textual entailment , event temporal ordering and word sense disambiguating .",
    "each problem in each category is solved using one expert and multiple mturk users .",
    "the mturk answers are integrated by using the majority voting .",
    "the experimental results were significant .",
    "almost in all experiments , the quality of the majority vote was at least as good as individual expert answers .",
    "for example , in the affect recognition tasks , in five of six emotions ( except fear ) , the integrated solutions had better quality than the expert ones .",
    "the number of users that are required to obtain such results has been between 2 to 9 persons for different categories . in the word semantic similarity measuring tasks ,",
    "the correlation of 10-user answers were the same as the expert answer .",
    "also , in all tasks , the quality of the integrated solutions were directly related to the number of participant users ( the higher number of answers , the higher accuracy of the task ) . the time and the cost",
    "are reported as 840 tasks per hour and 151 tasks per dollar .",
    "there are three steps in designing a typical crowd computing system .",
    "these steps are : defining the system grand strategies , generating the sub - problems , designing and optimizing the processes .",
    "we explain each of these steps in the following .    _ defining the system grand strategies : _ in designing a crowd computing system , first , we must define the system grand strategies .",
    "examples of such strategies are : is the system active or passive ? which scenario will be used to overcome the human errors , bias and noise ? which network or human work marketplace will be used ?",
    "do users compete in proposing the solutions or not ?",
    "defining different strategies impose different effects on the whole system .",
    "for example , in a non - competitive system , the amount of reward does not affect the quality of outputs , however , it affects the waiting time @xcite , while in a competitive network , change in the rewards directly affects the quality of results @xcite .",
    "another example is the type of task assignment in the system . in active systems ,",
    "each user can be modeled by its history .",
    "these models can be used to identify the user which is best suitable to assign a specified task to . in passive systems",
    ", there is no expressive task assignment .",
    "however , the problems are so designed to maximize the probability of being selected by some group of users .",
    "it requires to know the criteria that are used by the users in selecting the problems from the pool .",
    "samples of these criteria are : the time of importing the problem to the pool , the problem s expiration time , and the assigned reward to the problem .",
    "the best values for these criteria can be estimated by using the user behavior assessments .",
    "_ generating the application s sub - problems : _ a target application or problem should be divided into several sub - problems , so that each sub - problem is solvable in a short time by a user with regular skills and knowledge . dividing the main problem into sub - problems",
    "is done according to the system main scenario . in gwaps ,",
    "each sub - problem is a game level , while in a multiple checking scenario each sub - problem is a simple classification problem .",
    "crowd computing can also be used to generate sub - problems . for example , consider the text improving application ( correcting text typos and grammatical errors ) .",
    "suppose that we want to use crowd computing and multiple checking scenario . in @xcite ,",
    "first , each paragraph is imported to the system as a simple binary classification question , and the users are requested to identify whether that paragraph requires any correction or not .",
    "then , each sentence of each paragraph that requires corrections is imported to the system as a simple binary classification question and the users are requested to identify whether that sentence requires any correction or not . in this step ,",
    "the sentences that require corrections are identified . in the next step ,",
    "each sentence is given to multiple users to revise .",
    "then , the set of revised sentences for each target sentence are imported to the system as a multiple classification problem , and the users are requested to select the best one . finally , the original sentences are replaced by the results of the last step .",
    "several factors are important in generating sub - problems .",
    "for example , how to divide problems into sub - problems ?",
    "what are the type of sub - problems ?",
    "when each sub - problem should be imported to the pool ? when is the expiration time of each sub - problem ?",
    "how many answers are required for each sub - problem ? and ,",
    "how much is the amount of reward which will be paid to the users ? providing the proper solutions to these questions determine the level of success for a crowd computing system .    a good approach for choosing the proper value for a factor in designing the sub - problems , or assessing its role in the overall performance is  user behavior assessment \" .",
    "for example , to assess the role of reward for a set of sub - problems , we can import several problems of that set with different reward amounts .",
    "then , we should measure the performance of our system ( e.g. , the spent time , and the quality of answers ) . analyzing such parameters helps us to assess the reward factor effects . in @xcite ,",
    "similar experiments are done using mturk marketplace users .",
    "the results of the experiments show that decreasing the reward amounts does not affect the quality of the solutions , while it increases the waiting time ( because users are more interested in higher reward amount problems ) .",
    "_ designing and optimizing the processes : _ different levels of user skills , overcoming users errors and bias , and user costs make the crowd computing very complicated .",
    "hence , some processes must be defined to ensure that the problems will be solved with highest quality , expending a specified budget .",
    "user modeling , sample selection , and labels integration are three main components in optimizing crowd computing systems .",
    "we survey these methods in the next sections .",
    "in one hand , computers are very fast and accurate , but they can not understand the world around them as good as humans . on the other hand ,",
    "humans understand the world , but they can not process as fast and accurate as computers .",
    "for example , consider a classification problem .",
    "a human percepts and classifies the samples ( or patterns ) in the original space ( pattern space ) . compared to humans",
    ", a computer does not understand the pattern space .",
    "each sample in the pattern space must be transferred to a feature space , by using some sensors .",
    "also , the classification procedure should be dictated to the computers by humans .",
    "in contrast to humans , computers classify the feature vectors according to the dictated procedures .",
    "although , the execution of these procedures are very fast and accurate , we may not use computers to solve all the problems .",
    "there are two main reasons for this : 1 ) in transforming the original space to the feature space a large amount of information will be lost , and 2 ) current computers are restricted to algorithmic methods , which can not reflect the human mind s complex , and probably non - algorithmic methods .    the goal of crowd computing is combining human perception , and brain power in solving non machine - solvable problems , together with computers accuracy and speed , to create systems which have never existed before .    from a cognitive point of view ,",
    "two categories of problems are proper candidates for solving by crowd computing : problems that need human consciousness , and problems which require common sense knowledge .",
    "we present these two concepts , in the rest of this section .      in @xcite",
    ", turing showed that any function that is calculable by means of an  effective \" procedure can be calculated by means of a formal method ( such as a turing machine ) . independently and at the same time",
    ", church also showed the same concept through a totally different method using @xmath0-calculus @xcite .",
    "therefore , the thesis which indicates the equivalency of formal methods and effective procedures is known as church - turing thesis .    despite the universal consensus about the church - turing thesis , and",
    "although some researchers are focused on providing a prove to this thesis ( e.g. @xcite ) , no proof has been proposed for it , yet .",
    "church - turing thesis is valid in the digital world . disregarding the analog computers which may be developed in the future ( such as quantum computers ) and restricting the so - called  current computers \" to digital machines",
    ", we can claim that current computers are effective .",
    "assuming the well - known rosenthal s higher order theories ( hot ) of consciousness @xcite , unlike current computers , in neurobiological creatures , non - algorithmic activities can be done . in those creatures , non - inferential and non - observational beliefs about a mental state can be formed by another mental states , which is called consciousness .",
    "what is the role of consciousness in problem solving ?",
    "consciousness plays an important role in humans feeling , perception , and in general understanding the world .",
    "having consciousness , humans have access to the original information , while computers information is a reflection of humans information , which is defined algorithmically to them .",
    "there are some unsuccessful attempts to create artificial consciousness .",
    "for example , connectionists believe that artificial creation of consciousness is possible , if the number of artificial neurons and connections between them is more than a specified threshold . according to @xcite ,",
    "this threshold is equal to the number of human brain neurons and the number of connections between them .",
    "implementing such systems requires a large amount of memory . according to the moore s law ,",
    "the authors claim that this amount of memory can be available within the next 20 years .",
    "other similar estimations can also be found in @xcite .",
    "common sense knowledge base ( kb ) problem deals with the facts that a human knows .",
    "the question is ; how can we capture , save and use all these facts @xcite .",
    "this problem was originally proposed by marvin minski in 1992 @xcite in the context of slow progresses in the natural languages processing ( nlp ) .",
    "he stated that computers do not access to the words and the objects meanings , as the humans do . with the  rope \" , as an example , someone can pull something , but he can not push it .",
    "he can wrap something with it , but he can not eat something with it . even a child can describe more than a hundred applications of a rope , or any other objects and words , in a few minutes . but",
    "a computer can not do that .",
    "a human - like nlp system must access to such a kb , while there is not such a kb , at all .",
    "this problem is not specific to the nlp area , but it may also exist in all other areas .",
    "for example , in the machine vision area , to recognize an object by a human as a chair , it is not necessary to see an object exactly with four legs and one back .",
    "a human can recognize any usual or unusual chair , based on its shape , functionality or its relations to the other objects in that world .",
    "a human - like machine vision system must access to all objects shapes , functionalities , and their relations to the other objects , while it does not access .",
    "creating a common sense knowledge base is very difficult , because @xcite :    1 .",
    "a large amount of information must be captured .",
    "2 .   there is not a proper knowledge representation method .",
    "3 .   updating the kb facts , is very difficult .",
    "4 .   there is no efficient method for using and inferencing that knowledge .",
    "as we mentioned before , there are three main scenarios for crowd computing : multiple checking , gwap , and iterative tasks . the procedures of gwap and iterative tasks scenarios highly depend on the application .",
    "in contrast , the procedures of multiple checking scenario can be formalized , and also optimized .    in this section ,",
    "we formally define the problem of crowd computing by using multiple checking scenario .",
    "then , we describe the main approaches for solving it , in details .",
    "consider a passive crowd computing system for solving classification problems which uses a multiple checking scenario . in this system ,",
    "as we see later , the probability of estimating true label for a sample is highly related to the number of participant labelers in labeling that sample .",
    "then , the quality of solution highly relates to the user costs .",
    "since higher qualities in presence of a specified budget is desired , there is a constrained optimization problem .",
    "consider a binary classification problem with samples @xmath1 and their unknown true labels @xmath2 .",
    "users of the system are denoted by @xmath3 . provided labels by users to samples",
    "are shown by @xmath4 , where @xmath5 means that the user did not provide any label for that sample .",
    "the final number of all collected labels for all samples ( the budget ) is limited , i.e. @xmath6 .    the goal is to estimate the gold standards ( finding @xmath7s ) that maximizes @xmath8 .",
    "the basic strategy is to assign the same budget equally to all samples , and to acquire approximately the same number of labels for all problems . also , the basic method for integrating the collected labels and estimating the @xmath7s is the majority vote : @xmath9 where @xmath10 denotes the i^th^ row of the matrix @xmath11 , and @xmath12 and @xmath13 are the number of @xmath14s and the number of non - zero elements of @xmath10 , respectively .    in the majority vote , as number of acquired labels grows up , both costs and the quality of integrated labels would increase , while higher quality alongside a specified cost is desired .",
    "how to increase the quality of solutions while fixing the costs ? there are two main approaches : 1 ) using inductive methods , and 2 ) planned and purposeful budget spending .    in the first approach high quality labels for some samples will be extracted .",
    "then , a classifier will be learned using these samples and their estimated labels .",
    "then , labels of other samples will be estimated using the designed classifier .",
    "the second approach considers user expertise and the problems properties ( e.g. , type , difficulty level ,  ) . in each step , a label is requested for the sample which acquiring a new label for it would lead to a maximum increment in the overall quality .",
    "finding such a sample is a hard decision making problem .",
    "we name this problem as  sample selection \" .",
    "sample selection can utilize the history of users activities .",
    "the user histories can be stored as statistical models .",
    "we name the process of specifying a model for user activities , and finding its parameters as  user modeling \" .",
    "sample selection , user modeling and labels integration are three main components of the second approach .    in the following ,",
    "we describe inductive approach and the three components of the second approach , in more details .      in the inductive approach ,",
    "a generative model classifies new samples . by using the active learning approach this model",
    "can be empowered by new samples .",
    "the labels of both training set and active learning samples are obtained using crowd computing . in the active learning approach , each new sample either is classified by the model , or is used to improve the model .",
    "a decision making problem arises here .",
    "there are several criteria to make the decision about the new samples .",
    "a sample criterion is the uncertainty of the label that is assigned to the new sample by the classifier @xcite .",
    "inaccuracy probability of the assigned label can be considered as the label s uncertainty .",
    "another sample criterion is expected information gain @xcite .",
    "this criterion assumes that the classifier is a parametric model . in each step , the model s parameter , @xmath15 , once is estimated by using the current training set ( @xmath16 ) , and the other time is estimated by using the current training set empowered by the new sample and its ground truth ( @xmath17 ) .",
    "the gained information by adding the new sample ( i.e. , the entropy difference between two models ) can be measured by using the kullback - leibler divergence method .",
    "the difference amount shows the suitability of the new sample for empowering the model .",
    "since the sample s ground truth ( @xmath18 ) is not known , its expected value , which is calculated by using the classifier , will be used .",
    "then , the expected information gain is @xcite : @xmath19\\right]\\ ] ] the inductive methods require that objects be described as feature vectors .",
    "hence , a transformation from the pattern space to the feature space is required . as we mentioned in  why ? \" section , we are not interested in this approach , and we will focus on the other approach .",
    "suppose that there is a specified limited budget for labeling a set of samples , and the labeling cost for all samples are equal ( uniform labeling cost ) .",
    "in addition , suppose that the labeling process is adaptive , i.e. , in each step , one label will be requested for one of the samples based on the collected labels .",
    "a rational procedure selects a sample that getting a new label for it maximizes the overall quality .",
    "there are several criteria for sample selection .",
    "the simplest criterion is selecting the sample that has the minimum number of current labels .",
    "the result of this criterion is approximately the equal number of labels for all samples .",
    "we call this criterion  uniform \" .",
    "as simpler samples need fewer labels , in most cases uniform criterion wastes the budget .",
    "sample selection based on the current labels heterogeneity , and based on the uncertainty of their integration are instances of non - uniform criteria .    _ heterogeneity criterion _ selects a sample that the heterogeneity of its current labels is minimum .",
    "the heterogeneity of a set of labels can be measured by using their entropy @xcite .",
    "entropy is a proper measure for heterogeneity in this problem , but it has an undesirable bias .",
    "for illustrating this bias , suppose that the ratio of the dominant class in the current labels of sample @xmath20 is @xmath21 .",
    "the entropy criterion selects the sample @xmath22 where , @xmath23 [ entropybias ] it means that entropy always selects the sample which its current dominant labels ratio is closer to @xmath24 .",
    "it implicitly indicates that entropy is biased in some cases toward selecting the samples with more labels than the others .",
    "for example , entropy never selects a sample with only one label in its current label set .",
    "experimental results show that using entropy results in having a few samples with many labels , and a lot of samples with few labels .    in general , entropy does not consider the number of labels .",
    "in addition , all heterogeneity - based criteria lead to poor results in case of noisy labelers . because , heterogeneity criteria do not consider the labeler expertise ( they welcome the same noisy labels for samples ) .",
    "_ uncertainty criterion _ uses the inaccuracy probability of the estimated labels .",
    "consider sample @xmath20 with @xmath25 acquired labels that @xmath26 of them indicate class @xmath14 and @xmath27 of them indicate class @xmath28 .",
    "the likelihood @xmath29 is binomial . assuming equal prior probabilities for two classes , the posterior probability @xmath30 has a @xmath31 distribution distribution , and @xmath31 distribution is the conjugate of the binomial distribution . ] .",
    "since the cumulative density function of @xmath31 distribution is regularized incomplete beta function ( @xmath32 ) , if @xmath33 the integrated label is @xmath14 , otherwise it is @xmath28 . also , the uncertainty is given by @xcite : @xmath34 the proposed criterion in @xcite does not consider the user models .",
    "but , it is extendible to use any assumed statistical model in the problem formulation . in general ,",
    "if current collected labels are stored in @xmath11 , and the assumed models are specified by the parameters set @xmath35 , then the uncertainty of sample @xmath20 based on the current data is : @xmath36    * _ conclusion : _ * uniform criterion is very simple .",
    "but , it wastes the budget , and thus is not efficient .",
    "the entropy criterion is simple and intuitive .",
    "but , it is biased .",
    "uncertainty criterion is more complex than the other criteria .",
    "but , it can use any assumed statistical model .",
    "it can also handle the noisy labelers , in case of assuming the proper user models .",
    "since little attention has been paid to the adaptive methods in the literature , only a few methods have been proposed for sample selection . moreover ,",
    "uncertainty criterion is used with restricted statistical user models in the context of sample selection .    as a guideline for future works on this topic",
    ", we mention four properties for an efficient sample selection criterion :    1 .   _ considering the current acquired labels . _ an efficient criterion must consider the number and qualities of the acquired labels for each sample , up to now .",
    "the uncertainty criterion satisfies this property .",
    "_ estimating the future .",
    "_ an efficient criterion should estimate the changes in the overall performance , after acquiring a new label for its selected sample .",
    "the criterion must select a sample that leads to the maximum expected improvement in the overall performance .",
    "none of the proposed criteria estimate the future .",
    "3 .   _ avoiding local optima .",
    "_ an efficient sample selection criterion is not greedy .",
    "it considers the overall performance , not the maximum improvement in the current step s performance .",
    "all of the presented criteria are greedy .",
    "4 .   _ avoiding bias toward selecting improper samples . _ except the entropy all of the other presented criteria are unbiased .",
    "any other efficient sample selection criterion is not also biased to select samples with undesired features .",
    "there are also some other related open questions which have not yet been addressed by any researchers .",
    "the role of considering exploration alongside exploitation , or the role of deterministic or proportionally random sample selection are samples of these questions .",
    "user modeling indicates two types of modeling : users behavior modeling and user expertise modeling .",
    "user behavior modeling is only devoted to the passive systems , while user expertise modeling play a critical role in both active and passive systems .",
    "the goal of users behavior modeling is to discover the factors that users consider in selecting some problems from the pool , in passive systems .",
    "finding these factors helps system designers to deduct policies in order to achieve optimum performance .",
    "the policies are used in designing sub - problems , including determining type of problems , rewards , time to enter the pool , and persistence length in the pool .",
    "these factors can be found using user behavior assessment tests @xcite .",
    "the goal of user expertise modeling is creating parametric statistical models for user performances , using their histories in the system .    from one point of view",
    ", various models differ in the utilized parameters and their properties .",
    "accuracy , sensitivity and specificity , and reliability are samples of model parameters . in addition , each of parameters can have different properties .",
    "for example , accuracy can be modeled in different ways , such as : one accuracy parameter for all users , one accuracy parameter per user , and different accuracy parameters for each user in facing different categories of problems .",
    "moreover , each of those can be stationary in time , or time varying .",
    "from another point of view , there are different scenarios to calculate the model parameters .",
    "the first scenario is modeling based on a dedicated training set , and using the obtained models in both sample selection and label estimation phases .",
    "the second scenario is user modeling after sample selection phase . in this approach ,",
    "all labels are acquired from users .",
    "then , using these labels , the user models and the integrated labels are simultaneously estimated .",
    "this scenario is named ",
    "one - shot \" .",
    "no training data is required in one - shot scenario .",
    "but , the user models can not be used in the sample selection phase . the last scenario adaptively updates the model parameters . in each step , a part of labels",
    "are acquired based on the current labels and the estimated models .",
    "then , the user models are updated . in this scenario ,",
    "the updated user models in each step will be used in the sample selection phase of the next step .",
    "most of the current researches use the one - shot scenario .    due to limitations , and noting that most of users behavior modeling methods are heuristic ; we do not probe the proposed methods in this area . in the rest of this section",
    ", we will survey the various proposed methods for user expertise modeling .",
    "note that the  user model \" term in the next parts refers to  user expertise model \" .    here",
    ", we will survey the various types of accuracy , sensitivity and specificity , reliability , and expertise models .",
    "the simplest type of user expertise modeling is  uniform accuracy modeling \" .",
    "it uses one accuracy parameter for all users .",
    "if we show the collected labels for problem @xmath20 by @xmath37 , uniform accuracy modeling assumes that @xmath38 . according to the binomial distribution",
    ", we have : @xmath39 where @xmath40 is the number of potentially incorrect answers , @xmath7 is the estimated label using the majority voting method , and @xmath41 is the probability that more than @xmath42 labeler propose correct labels .    according to eq . [ mv ] , @xmath41 is bigger than @xmath43 , iff @xmath44 . also , if @xmath44 as @xmath42 increases , @xmath41 increases . while , the rate of changes is decreasing .",
    "it means that the rate of changes depends on both @xmath43 and @xmath42 .",
    "for example , increasing the number of labelers leads to more significant results when @xmath45 compared to the case where @xmath46 @xcite . in fig .",
    "[ mv - nqou ] the value of @xmath41 based on the number of collected labels per each sample is shown , for different values of @xmath43 .    in @xcite ,",
    "a method is proposed to non - uniform user accuracy modeling , which considers different accuracy parameters for each user .",
    "since this method is quite similar to the previously proposed method in @xcite for user sensitivity and specificity modeling , we will only present the latter method here .",
    "iethresh was proposed for assigning problems to users in active systems @xcite . in statistics , given @xmath47 observations of variable @xmath48 from distribution @xmath49 ,",
    "interval estimation method estimates an interval that the next observation belongs to , with the probability @xmath50 .",
    "iethresh estimates accuracy intervals for all users , which show the correctness probabilities of the next provided answers by the users .",
    "iethresh selects a user with the highest accuracy interval s upper bound .",
    "higher upper bound indicates higher expected accuracy ( when the interval length is short ) or higher uncertainty ( when the interval length is long ) .",
    "then , iethresh considers both exploitation and exploration . the interval s upper bound is estimated as : @xmath51 where @xmath52 and @xmath53 are mean and standard deviation of correct answers that are provided by user @xmath48 ( the correct answers are estimated using comparing the proposed labels to the majority votes ) .",
    "the @xmath54 is the value of t - student distribution when degree of freedom is @xmath55 , and the level of confidence is @xmath56 .",
    "having enough time ( observing a lot of user answers ) , iethresh leads to very good results in active systems , even if the number of high quality users is small .",
    "sfilter assumes that the user accuracies are not stationary in time , and proposes a time varying algorithm for user accuracy modeling @xcite .",
    "sfilter is proposed for filtering out the low - quality users in active systems .",
    "the algorithm uses sequential bayesian estimation .",
    "it also assumes that the maximum rate of changes are small and known @xcite .",
    "suppose that @xmath57 represents the accuracy of user @xmath48 in time @xmath58 , @xmath59 is the provided label by the user in time @xmath58 ( @xmath60 is the label which is provided by the user for problem @xmath20 in time @xmath58 ) .",
    "the goal is to estimate @xmath61 , the posterior probability of the user accuracy .",
    "sfilter considers the following markov model for modeling the accuracy changes , which is shown in the fig .",
    "[ sfiltermmodel ] : @xmath62    \\(a ) at ( 0pt,0pt )  ; ( b ) at ( 50pt,0pt ) @xmath63 ; ( c ) at ( 100pt,0pt ) @xmath64 ; ( d ) at ( 150pt,0pt ) @xmath65 ; ( e ) at ( 200pt,0pt )  ; ( g ) at ( 50pt,-50pt ) @xmath66 ; ( h ) at ( 100pt,-50pt ) @xmath67 ; ( i ) at ( 150pt,-50pt ) @xmath68 ; /in a / b , b / c , c / d , d / e , b / g , c / h , d / i ( )  ( ) ;    this markov model states that the accuracy of each user in time @xmath58 only depends on its accuracy in time @xmath69 .",
    "and the proposed labels by the user in each time only depends on user s accuracy in that time .    using this markov model , and considering that the user accuracies are values in range @xmath70 $ ]",
    ", sfilter calculates the transition probability from @xmath57 to @xmath63 using a truncated gaussian distribution : @xmath71 where @xmath72 is the standard gaussian probability distribution , and @xmath73 is its cumulative distribution function .    having a problem s true label and the user s accuracy in time @xmath58 , the user provided label is modeled as : @xmath74 where @xmath75 is the indicator function . in practice , the value of @xmath76 in eq .",
    "[ sf33 ] is unknown .",
    "suppose that @xmath77 is the provided label by user @xmath48 in time @xmath58 for problem @xmath20 , and @xmath78 is the set of other users provided labels for that problem .",
    "we have : @xmath79 where @xmath80 is calculated by using the probability of integrated label from labels @xmath78 : @xmath81 sfilter considers @xmath82 as the prior probability of @xmath83 . using the chapman - kolmogorov equation , we have , @xmath84 where @xmath85 denotes @xmath86 , and @xmath87 .",
    "all model parameters must be updated after getting any new label .",
    "since this job is very time consuming , an incremental algorithm is also proposed in @xcite . in the incremental version of sfilter , discrete posterior approximation is estimated by using the sequential particle filtering .",
    "experiments show that if changes in user accuracies are according to the considered model , sfilter can track the changes in time @xcite !",
    "other parameters for user expertise modeling in binary labeling problems are sensitivity and specificity .",
    "sensitivity indicates the proportion of actual positive samples ( samples which belong to class @xmath14 ) that are correctly recognized .",
    "similarly , specificity indicates the proportion of actual negative samples ( samples which belong to class @xmath28 ) which are correctly recognized .",
    "the generalization of these parameters in multi - classes problems is the set of independent elements of confusion matrix .",
    "dawid and skene in 1979 proposed an expertise modeling by using confusion matrix in multi - class medical diagnosis tests @xcite .",
    "they calculated the likelihood of true answers for all samples , then maximized that by using the expectation - maximization ( em ) method .",
    "consider a multi - class classification problem with samples @xmath88 and their gold standards @xmath89 .",
    "each @xmath76 belongs to one of classes @xmath90 .",
    "the class prior probabilities are @xmath91 .",
    "also , assume that each of @xmath92 users in the system provide none , one , or multiple labels for each of samples , which are stored in matrix @xmath11 .",
    "first , we assume that the gold standards are known , and then generalize the results for the case that the gold standards are unknown .",
    "suppose that @xmath93 is the number of labels @xmath94 which is assigned to problem @xmath20 by user @xmath95 .",
    "@xmath96 is one , if label @xmath97 is a true label for problem @xmath20 and it is zero , otherwise . also , @xmath98s are the elements of the confusion matrix of user @xmath95 ( @xmath99 ) .",
    "the likelihood of extracting true labels for all samples is : @xmath100 which @xmath101 is the set of all user confusion matrices elements , @xmath102 is the set of all prior probabilities , and @xmath103 is the multinomial distribution . maximizing the likelihood , leads to the following estimations for parameters : @xmath104 in the case of unknown gold standards , the likelihood is : @xmath105 maximizing this function is complicated , so the following em algorithm is used to estimate the parameters :    * initialization : @xmath106 . *",
    "e step : extracting the sample labels , using previous step user confusion matrices and prior probabilities .",
    "@xmath107 is the label of @xmath20 with the following probability : @xmath108 * m step : updating the user confusion matrices and the prior probabilities , by using the previous step extracted labels , and eq .",
    "[ em791 ] .",
    "a bayesian approach is also proposed for sensitivity and specificity modeling .",
    "this method assumes the following prior probabilities @xcite : @xmath109 where @xmath110 and @xmath111 are the sensitivity and specificity of user @xmath48 , @xmath112 is the prior probability of class @xmath14 ( i.e. @xmath113 ) , @xmath114 and @xmath115 are the number of true and false provided answers by user @xmath48 to the problems of class @xmath14 , @xmath116 and @xmath117 are the number of true and false provided answers by user @xmath48 to the problems of class @xmath28 , @xmath118 , @xmath119 are the total number of provided labels by all users to all problems of classes @xmath14 and @xmath28 , and @xmath31 is the beta probability distribution function .    similar to the previous method , the value of required variables in the e and m steps can be extracted , as follows @xcite :    @xmath120    where , @xmath121^{i(a_{ij}=+1)}[1-\\alpha^j]^{i(a_{ij}=-1)}}\\\\   b_i&=&p(a_{i1},\\ldots , a_{ir}|y_i=-1,\\boldsymbol{\\beta})=\\prod_{j=1}^r{[\\beta^j]^{i(a_{ij}=-1)}[1-\\beta^j]^{i(a_{ij}=+1)}}\\end{aligned}\\ ] ]      a user reliability modeling and labels integration algorithm is proposed in @xcite .",
    "the authors utilize an iterative ` belief propagation'-_like _ algorithm , for this purpose .",
    "the procedure is shown in alg .",
    "[ belief_alg ] . in this algorithm",
    ", @xmath122 is the set of edges of the bipartite graph which is specified by the adjacency matrix @xmath11 , @xmath123 denotes all neighbors of @xmath124 in the graph , and @xmath125 excludes @xmath40 from the set @xmath126 .",
    "al . proposed a graphical method with belief propagation inferring algorithm for crowd computing in @xcite .",
    "they showed that the presented algorithm is a belief propagation based algorithm , if a haldane prior @xcite is considered as the prior distribution of user reliabilities .",
    "one of the most important features of the presented algorithm is its relation to low - rank matrices and singular value decomposition ( svd ) , when @xmath11 is a @xmath127-regular bipartite graph with @xmath128 @xcite .",
    "power iteration is a method to compute the leading singular vectors of a matrix . for matrix @xmath129 and two vectors @xmath130 and @xmath131 ,",
    "starting with a random initialized @xmath132 , power iteration iteratively updates @xmath133 and",
    "@xmath132 according to : @xmath134 it is known that randomized @xmath133 and @xmath132 converges linearly to the leading left and right singular vectors .",
    "these update rules are very similar to the alg .",
    "[ belief_alg ] .",
    "then , the following algorithm can be used to estimate the question answers :    1 .   compute the left and right singular vector of @xmath11 , corresponding to the top singular values of @xmath11 .",
    "2 .   since both @xmath135 and @xmath136 are valid pairs of leading singular vectors , the mass of the element values is considered to resolve the ambiguity , if @xmath137 , then @xmath138 , otherwise @xmath139 .",
    "note that the alg .",
    "[ belief_alg ] rules , and the power iteration rules are not exactly the same . in the updating rules of the algorithm , the received signals from the destination will be excluded ( ",
    "@xmath140 s in the algorithm ) .",
    "but , these signals are considered in the power iteration . in other words ,",
    "the power iteration rules is the simplification of the algorithm s rules , because the latter approximates all different @xmath141 with a common @xmath142 .",
    "some intuitions are proposed in the @xcite to justify why the top left singular vector of @xmath11 reveals the estimated labels .",
    "moreover , the obtained leading singular vectors can be used to approximate a low - rank matrix .    having matrix @xmath11 with rank @xmath143 , in low - rank matrix approximation ,",
    "the goal is to approximate @xmath11 with a matrix @xmath144 of rank at most @xmath40 ( @xmath145 ) , such that : @xmath146    according to the eckart & young theorem @xcite we have : @xmath147    where @xmath148 is approximated using @xmath149 , @xmath150 and @xmath151 are left and right singular vectors of @xmath11 , @xmath152 is a diagonal matrix containing sorted singular values of @xmath11 ( @xmath153 ) , and @xmath154 is formed by replacing the @xmath155 smallest singular values on the diagonal of @xmath152 to zero .",
    "then , as mentioned above , the second algorithm uses the rank-1 approximation of @xmath11 .",
    "none of the above models consider the difficulty level of problems . while there are some evidences which show that considering the difficulty level of problems in user modeling is useful , for some applications .    the  glad \" algorithm is proposed to model the users expertise and the difficulty level of problems @xcite .",
    "glad can also find the noisy and adversarial labelers , and in case of many number of adversarial labels , it can utilize them to produce higher quality results .",
    "glad models the difficulty of problem @xmath20 using the parameter @xmath156 where the higher value of the parameter shows the higher difficulty level of the problem . also , glad models the expertise of labeler @xmath48 using the parameter @xmath157 , where the higher value of @xmath158 indicates the higher level of expertise , and @xmath159 indicates the adversarial labeler .    the graphical schema of the model s dependencies is shown in the fig .",
    "[ gladmodel ] ( the shaded variables are observed , and the others are latent ) .",
    "( -4,4.65 ) rectangle ( 4,5.85 ) ; ( -6,4.65 ) rectangle ( -6,4.65 ) ; at ( 2.75,6 ) sample difficulties ; ( a1 ) at ( -2.05,5.25)@xmath160 ; ( a2 ) at ( 0.05,5.25)@xmath161 ; at ( 1.7,5.25 )  ; ( a3 ) at ( 3.35,5.25)@xmath162 ; ( -4,3.1 ) rectangle ( 4,4.3 ) ; at ( 3.2,4.45 ) true labels ; ( b1 ) at ( -3.25,3.7)@xmath163 ; ( b2 ) at ( -1.05,3.7)@xmath164 ; at ( 1.95,3.7 )  ; ( b3 ) at ( 3.35,3.7)@xmath165 ; ( -4,1.55 ) rectangle ( 4,2.75 ) ; at ( 2.85,2.9 ) observed labels ; ( c1 ) at ( -3.25,2.15)@xmath166 ; ( c2 ) at ( -2.15,2.15)@xmath167 ; at ( -1.2,2.15 )  ; ( c3 ) at ( -0.35,2.15)@xmath168 ; ( c4 ) at ( 0.75,2.15)@xmath169 ; ( c5 ) at ( 1.85,2.15)@xmath170 ; at ( 3.15,2.15 )  ; ( -4,0 ) rectangle ( 4,1.2 ) ; at ( 2.65,1.35 ) labeler accuracies ; ( d1 ) at ( -2.85,0.6)@xmath171 ; ( d2 ) at ( -1.75,0.6)@xmath172 ; ( d3 ) at ( -.65,0.6)@xmath173 ; at ( 1.45,0.6 )  ; ( d4 ) at ( 3.35,0.6)@xmath174 ; /in a1/c1 , a1/c2 , a2/c3 , a2/c4 , a2/c5 , b1/c1 , b1/c2 , b2/c3 , b2/c4 , b2/c5 , d1/c1 , d1/c3 , d2/c2 , d2/c4 , d3/c5 ( )  ( ) ;    glad uses the following logistic model : @xmath175    glad uses an em algorithm to estimate the latent variables . for e step ,",
    "the posterior probability of @xmath76 given @xmath11 , @xmath176 and @xmath177 is : @xmath178 where , @xmath179    in m step , to calculate the best values for the parameters , we maximize the expectation of the joint log - likelihood of the observed and hidden variables given the parameters .",
    "the objective function is : @xmath180\\\\ & = & \\sum_i{e\\left[ln p(y_i)\\right]}+\\sum_{ij}{e\\left[ln p(a_{ij}|y_i,\\alpha_j,\\beta_i)\\right]}\\\\ & = & \\sum_{i}{\\left\\lbrace p^+ ln p(y_i=+1)+p^- ln p(y_i=-1 ) \\right\\rbrace}\\\\ & & + \\sum_{ij|a_{ij}=+1}{\\left\\lbrace p^+ ln \\sigma(\\alpha_j\\beta_i ) + p^- ln \\left ( 1-\\sigma(\\alpha_j\\beta_i ) \\right ) \\right\\rbrace}\\\\ & & + \\sum_{ij|a_{ij}=-1}{\\left\\lbrace p^- ln \\sigma(\\alpha_j\\beta_i ) + p^+ ln \\left ( 1-\\sigma(\\alpha_j\\beta_i ) \\right ) \\right\\rbrace}\\end{aligned}\\ ] ] where @xmath112 and @xmath181 are the classes priors from the previous step ( are calculated using @xmath182 and @xmath183 ) .    setting the gradients of @xmath184 to zero results in non - linear equations .",
    "thus , the maximization process need to be solved using iterative methods .",
    "glad is the first model which simultaneously estimates the true label , problems difficulty , and users expertise .",
    "but , similar previous methods it does not consider the sample selection and labels acquiring phase .",
    "recently , inspiring the glad , some other methods , such as @xcite , are proposed for modeling difficulty levels of the problems .    *",
    "_ conclusion : _ * uniform accuracy is the simplest parameter for user expertise modeling .",
    "but , since it considers one accuracy parameter for all users , it is not sufficiently accurate .",
    "non - uniform accuracy modeling considers separated parameters for each of users .",
    "sfilter suppose that the user accuracies are time varying .",
    "but , it uses a simple markov model for modeling these changes .",
    "also , it assumes that the maximum rate of change is small , known and is the same for all user accuracies . the last method that utilizes accuracy as its model parameter is iethresh .",
    "this method models each user by an accuracy interval .",
    "iethresh is useful in active systems for task assignment . in each step",
    ", it assigns the selected problem to the user with highest interval upper bound .",
    "it leads to good results , even in presence of a large number of noisy users .",
    "confusion matrix is another parameter set for user modeling .",
    "this parameter set includes more detailed information about the user expertise .",
    "it is useful in problems that users have different performances in facing different classes of problems .",
    "the glad method considers the difficulty level of problems , which it might be useful in some applications .",
    "but , it leads to the nonlinear optimization problem .",
    "therefore , it is complicated , and is difficult for use in adaptive methods .",
    "non - uniform accuracy , confusion matrix based , and glad methods optimize the likelihood of getting true answers for all samples to calculate the model parameters .",
    "there are two main problems in this approach .",
    "the first problem is assuming the independency between the provided labels by different users .",
    "since , there are some correlations between provided answers by some of users , this assumption is not true in all cases .",
    "another problem is special to methods that use em algorithm to estimate the optimum values for parameters .",
    "em is sensitive to initialization values , and it might not converge to an acceptable set of values .",
    "the belief propagation based algorithm uses reliability as the parameter for the user models . unlike the em - based methods , this algorithm is not sensitive to the initial settings . an svd - based version of this iterative algorithm is also proposed .",
    "although this method relates to the svd and low - rank matrices with a rich theory behind them , but this equivalency only holds when the questions to labelers assignment graph is a @xmath127-regular bipartite graph with @xmath128 .",
    "this condition is not hold usually , in practice .",
    "another weakness of this method is simplifications in converting the original iterative algorithm to a power iteration - like algorithm . finally , as the last weakness",
    ", it is only suitable for binary classification problems .      the simplest strategy for integrating the collected labels and estimating the sample s label is majority voting .",
    "there is a complexity in using majority voting .",
    "consider five acquired labels from users with accuracies 0.55 , 0.85 , 0.75 , 0.6 , and 0.8 .",
    "the correctness probability of the majority vote , majority of the best three labels , and the best label are 0.86 , 0.90 , and 0.85 , respectively .",
    "this example illustrates that filtering out two worse labels leads to the best result .",
    "then , in some cases it is recommended to filter out some of acquired labels . in general , majority voting does not consider the quality of responses .",
    "probabilistic methods and belief propagation - inspired algorithm consider the quality of responses through user models .",
    "they are more efficient than majority voting . in these methods , integrating the collected labels depends on the type of user models .",
    "we probed the suitable methods , for each type of user modeling methods , in the previous section .",
    "there are some comments on the validity of reported experimental results in some related papers . in those papers ,",
    "the authors did not have access to real datasets .",
    "therefore , they have produced their own synthetic data , according to their assumed models .",
    "then , they show that their methods can efficiently find the user models and the sample labels .",
    "clearly , these results may not be valid . in some other papers",
    ", the authors used fair assumptions in producing the synthetic data , but they have not considered the costs , and have produced a large number of labels for each sample .",
    "the experimental results of these papers are valid theoretically , but they may not be useful in practice .    to compare the surveyed methods , we have implemented all of them .",
    "in addition , we utilized three real datasets .",
    "these datasets include binary classification problems , where the labels of their problems are acquired from mturk users . in each question of the recognizing textual entailment dataset ( rteds ) ,",
    "two sentences are presented to users and a binary choice of whether the second hypothesis sentence can be inferred from the first one , or not @xcite . in each question of temporal event recognition dataset ( tempds )",
    ", the users must choose one of the two labels  strictly before \" or  strictly after \" to represent the temporal relation between two event - pairs @xcite .",
    "in each face image of the duchenne dataset , users are asked to determine whether the face contains duchenne smile (  enjoyment \" smile ) or not ( is a  social \" smile ) ?",
    "table [ dssinfo ] contains the properties of these datasets .",
    "ccccc & * # sampels * & * # labelers * & * # labels * & * # labels / sample * + * rteds * & 800 & 164 & 8000 & 10 + * tempds * & 462 & 76 & 4620 & 10 + * duchenneds * & 159 & 17 & 1950 & 8 - 15 +    the histogram of user accuracies , and the histogram of collected labels accuracies are shown in fig .",
    "[ dshists ] , for all datasets .",
    "implementations show that the results of all sample selection methods in case of higher budgets are almost the same . while , using a few number of average labels per each sample , the uncertainty leads to better results than the other criteria ( uniform and heterogeneity ) .",
    "the comparison of sample selection criteria for tempds is shown in fig .",
    "[ sscomp ] .",
    "other datasets lead also to similar resultsmuhammadi . ] .",
    "all reported results are the average of 20 runs",
    ".    in user modeling tests , in all datasets , majority voting ( equal models for all users ) leads to the worst results , while glad leads to the best results .",
    "the comparison of user modeling methods for rteds is shown in fig .",
    "[ umcomp ] .    in fig .",
    "[ ssumcomp ] we compare the majority voting ( uniform sample selection , equal models for all users ) , uncertainty sample selection ( equal models for all users ) , and user accuracy modeling ( uniform sample selection ) with the uncertainty alongside accuracy user modeling method .",
    "note that the last method calculates sample uncertainties by using the correctness probability of estimated labels , which is obtained by utilizing user accuracy models .",
    "the results show that utilizing both sample selection and user modeling leads to worse results than utilizing each of them , solely .",
    "finally , we compared the best sample selection method ( uncertainty ) , and the best user modeling method ( glad ) with the best user responses . in the latter one , we assumed that we know the actual user accuracies , and for each sample we selected the label of the best user , as the estimated label .",
    "the results for duchenne dataset are shown in fig .",
    "[ bestcomp ] .    all obtained results for rte , temp and duchenne datasets are shown in tables [ rtedsresults]-[duchennedsresults ] .     *",
    "majority voting * & 26.84 & 19.84 & 15.63 & 12.94 & 10.36 + * entropy * & 26.84 & 23.48 & 20.93 & 16.60 & 12.22 + * uncertainty * & 26.84 & 16.29 & 11.91 & 10.03 & 10.74 + * accuracy * & 26.84 & 16.63 & 11.76 & 10.73 & 10.56 + * acc .",
    "* & 29.03 & 15.32 & 14.49 & 13.45 & 11.43 + * sen .",
    ", spe . * & 26.84 & 15.27 & 11.22 & 10.26 & 9.74 + * reliability * & 26.84 & 19.69 & 12.29 & 10.25 & 8.62 + * glad * & 26.84 & 14.77 & 10.69 & 8.91 & 7.76 +     * majority voting * & 26.37 & 16.46 & 10.94 & 7.44 & 6.35 + * entropy * & 26.37 & 21.37 & 17.13 & 12.84 & 8.39 + * uncertainty * & 26.37 & 12.78 & 7.85 & 6.68 & 6.26 + * accuracy * & 26.37 & 10.49 & 8.25 & 7.34 & 6.94 + * acc . & uncert . * & 26.83 & 10.31 & 9.37 & 9.42 & 8.02 + * sen .",
    ", spe . * & 26.37 & 10.90 & 7.91 & 7.21 & 6.94 + * reliability * & 26.37 & 11.98 & 8.27 & 7.12 & 6.30 + * glad * & 26.37 & 11.14 & 7.76 & 6.33 & 6.06 +     * majority voting * & 37.74 & 33.14 & 31.76 & 30.19 + * entropy * & 37.74 & 35.28 & 33.21 & 30.82 + * uncertainty * & 37.74 & 31.38 & 29.59 & 28.21 + * accuracy * & 37.74 & 31.38 & 28.49 & 27.89 + * acc .",
    "* & 46.48 & 33.74 & 32.61 & 35.03 + * sen .",
    ", spe . * & 37.74 & 28.77 & 28.11 & 28.40 + * reliability * & 37.74 & 33.46 & 27.45 & 26.32 + * glad * & 37.74 & 29.72 & 26.35 & 25.31 +",
    "crowd computing is a new field in computer science that combines the strengths of both humans and computers to create systems which have never existed before . in this paper",
    "we surveyed the crowd computing from three aspects : what , why , and how ?",
    "first , in the  what \" part we introduced the fundamental concepts and the various types of crowd computing systems .",
    "then , we explained the properties of suitable problems and applications for crowd computing .",
    "then , we illustrated the performance of some existing systems .",
    "finally , we described the required steps to design a crowd computing system .    in the ",
    "why \" part , we introduced human s consciousness and common sense knowledge as the shortcoming of the current computer systems , in comparison to humans .",
    "then , we discussed their roles in creating more intelligent systems , using crowd computing .",
    "in the  how \" part , we presented a survey on solving classification problems using crowd computing .",
    "we divided the past works to three sections : sample selection , user modeling , and labels integration .",
    "we also compared the surveyed methods .",
    "in addition , we discussed the strengths and weaknesses . moreover , we proposed some topics as guidelines for future works .    considering the technical open issues , we must point to the following topics : 1 ) little researches is done on adaptive scenarios , and this field requires more attention .",
    "current sample selection criteria are not effective in presence of user models .",
    "then , concentration on efficient combination of sample selection criteria and user modeling methods is a good topic for more research .",
    "2 ) in case of large amount of budget , almost all methods works well . the problem arises when the budget is low ( i.e. the user performances are low , or the number of labels are few ) .",
    "then , the future research must focus more on the cost models .",
    "3 ) finally , calculating realistic error bounds for results is another open issue for further research",
    ".    table [ guidelines ] contains the summary of all open issues for feature research in this field .",
    "costs .    - calculating realistic error bounds for results",
    ".    - using sparse representation methods",
    ".    - handling multi - class , and non - classification problems .",
    "+ * sample selection * & - utilizing user models in developing sample selection criteria .",
    "- considering the overall performance in each step .    - considering the estimation of the changes in the overall performance , after getting a new label for a sample .    - research on the role of factors such as exploration alongside exploitation , or deterministic vs. proportionally random sample selection . + * user modeling * & - proposing realistic time varying user models .    - relaxing the independency assumption between collected labels in likelihood - based methods .",
    "+ * labels integration * & - detecting and filtering out low - quality labels . +    99    little , g. , chilton , l. b. , goldman , m. and miller , r. c. ( 2009 ) , turkit : tools for iterative tasks on mechanical turk , proceedings of the acm sigkdd workshop on human computation , hcomp 09 , acm , paris , france , pp  2930 .",
    "von ahn , l. and dabbish , l. ( 2004 ) , labeling images with a computer game , proceedings of the sigchi conference on human factors in computing systems , chi 04 , acm , vienna , austria , pp  319326 .",
    "donmez , p. , carbonell , j. g. and schneider , j. ( 2010 ) , a probabilistic framework to learn from multiple annotators with time - varying accuracy , sdm , siam , pp  826837 .",
    "frank , e. and hall , m. ( 2001 ) , a simple approach to ordinal classification , ecml 01 : proceedings of the 12th european conference on machine learning , springer - verlag , london , uk , pp  145156 .",
    "raykar , v. c. , yu , s. , zhao , l. h. , valadez , g. h. , florin , c. , bogoni , l. and moy , l. ( 2010 ) , learning from crowds , journal of machine learning resources , 99 , mit press , cambridge , ma , usa , pp  12971322 .    brew , a. , greene , d. and cunningham , p. ( 2010 ) , the interaction between supervised learning and crowdsourcing , nips workshop on computational social science and the wisdom of crowds .",
    "wais , p. , lingamneni , s. , cook , d. , fennell , j. , goldenberg , b. , lubarov , d. , marin , d. , simons , h. ( 2010 ) , towards building a high - quality workforce with mechanical turk , in nips workshop on computational social science and the wisdom of crowds .",
    "donmez , p. , carbonell , j. g. and schneider , j. ( 2009 ) , efficiently learning the accuracy of labeling sources for selective sampling , in proceedings of the 15th acm sigkdd international conference on knowledge discovery and data mining , kdd 09 , acm , new york , usa , pp  259268 .",
    "sheng , v. s. , provost , f. and ipeirotis , p. g. ( 2008 ) , get another label ? improving data quality and data mining using multiple , noisy labelers , in proceeding of the 14th acm sigkdd international conference on knowledge discovery and data mining , kdd 08 , acm , new york , usa , pp .",
    "614622 .",
    "snow , r. , oconnor , jurafsky , d. and ng , a. ( 2008 ) , cheap and fast  but is it good ?",
    "evaluating non - expert annotations for natural language tasks , proceedings of emnlp-08 .",
    "yang , j. , adamic , l. a. and ackerman , m. s. ( 2008 ) , crowdsourcing and knowledge sharing : strategic user behavior on taskcn , in proceedings of the 9th acm conference on electronic commerce , ec 08 , acm , chicago , il , usa , pp  246255 .",
    "bernstein , m. s. , little , g. , miller , r. c. , hartmann , b. , ackerman , m. s. , karger , d. r. , crowell , d. and panovich , k. ( 2010 ) , soylent : a word processor with a crowd inside , in proceedings of the 23nd annual acm symposium on user interface software and technology , uist 10 , acm , new york , ny , usa , pp  313322 .",
    "howe , j. ( 2008 ) , crowdsourcing : why the power of the crowd is driving the future of business , 1 edn , crown business .",
    "malone , t. ( 2004 ) , the future of work : how the new order of business will shape your organization , your management style , and your life , harvard business school press .",
    "waltz , d. l. ( 2006 ) , evolution , sociobiology , and the future of artificial intelligence , ieee intelligent systems 21(3 ) , pp  6669 .",
    "mccarthy , j. ( 2007 ) , from here to human - level ai , artificial intelligence 171(18 ) , pp  11741182 .",
    "kurzweil , r. ( 2000 ) , the age of spiritual machines : when computers exceed human intelligence , penguin books .",
    "buttazzo , g. ( 2001 ) , artificial consciousness : utopia or real possibility ?",
    ", ieee computer 34(7 ) , pp  2430 .",
    "dershowitz , n. and gurevich , y. ( 2008 ) , a natural axiomatization of computability and proof of church s thesis , bulletin of symbolic logic 14(3 ) , pp  299350 .",
    "turing , a. m. ( 1936 ) , on computable numbers , with an application to the entscheidungsproblem , proc .",
    "london math .",
    "2(42 ) , pp  230265 .    church , a. ( 1936 ) , a note on the entscheidungsproblem the journal of symbolic logic 1(1 ) , p  4041 .",
    "copeland , b. j. ( 2008 ) , the church - turing thesis , in e. n. zalta , ed . , the stanford encyclopedia of philosophy , fall 2008 edn .",
    "rosenthal , d. m. ( 1997 ) , a theory of consciousness , in n. block , o. flanagan and g. g uzeldere , eds , the nature of consciousness : philosophical debates , the mit press , cambridge , ma .",
    "howe , j. ( 2006 ) , the rise of the crowdsourcing , wired magazine .",
    "paquet , u. , gael , j. v. , stern , d. , kasneci , g. , herbrich , r. and graepel , t. ( 2010 ) , vuvuzelas active learning for online classification , in nips workshop on computational social science and the wisdom of crowds .",
    "janssens , j. h. m. ( 2010 ) , ranking images on semantic attributes using human computation , in nips workshop on computational social science and the wisdom of crowds .",
    "minsky , m. l. ( 1992 ) , future of ai technology , toshiba review 47(2 ) .",
    "von ahn , l. , blum , m. , hopper , n. j. and langford , j. ( 2003 ) , captcha : using hard ai problems for security , in proceedings of eurocrypt , vol .",
    "2656 , pp .",
    "294311 .",
    "von ahn , l. , maurer , b. , mcmillen , c. , abraham , d. and blum , m. ( 2008 ) , re - captcha : human - based character recognition via web security measures , science 5895(321 ) , pp  14651468 .",
    "whitehill , j. , ruvolo , p. , fan wu , t. , bergsma , j. and movellan , j. ( 2009 ) , whose vote should count more : optimal integration of labels from labelers of unknown expertise , in advances in neural information processing systems 22 , pp  20352043 .",
    "eagle , n. ( 2009 ) , txteagle : mobile crowdsourcing , in proceedings of the 3rd international conference on internationalization , design and global development : held as part of hci international 2009 , idgd 09 , springer - verlag , berlin , heidelberg , pp  447456 .",
    "dawid , a. p. and skene , a. m. ( 1979 ) , maximum likelihood estimation of observer error - rates using the em algorithm , journal of the royal statistical society .",
    "series c ( applied statistics ) 28(1 ) , pp  2028 .",
    "potter , a. , mcclure , m. and sellers , k. ( 2010 ) , mass collaboration problem solving : a new approach to wicked problems , 2010 international symposium on collaborative technologies and systems , pp  398407 .",
    "little , g. , chilton , l. b. , goldman , m. and miller , r. c. ( 2010 ) , exploring iterative and parallel human computation processes , in proceedings of the acm sigkdd workshop on human computation , hcomp 10 , acm , new york , ny , usa , pp  6876 .",
    "urbano , j. , morato , j. , marrero , m. and martin , d. ( 2010 ) , crowdsourcing preference judgments for evaluation of music similarity tasks , in proceedings of the acm sigir 2010 workshop on crowdsourcing for search evaluation ( cse 2010 ) , geneva , switzerland , pp  916 .",
    "carvalho , v. r. , lease , m. and yilmaz , e. ( 2011 ) , crowdsourcing for search evaluation , sigir forum 44(2 ) , pp  1722 .",
    "gordon , j. , van durme , b. and schubert , l. k. ( 2010 ) , evaluation of commonsense knowledge with mechanical turk , in proceedings of the naacl hlt 2010 workshop on creating speech and language data with amazon s mechanical turk , csldamt 10 , association for computational linguistics , pp  159162 .",
    "grady , c. and lease , m. ( 2010 ) , crowdsourcing document relevance assessment with mechanical turk , in proceedings of the naacl hlt 2010 workshop on creating speech and language data with amazon s mechanical turk , association for computational linguistics , pp  172179 .",
    "bloodgood , m. and callison - burch , c. ( 2010 ) , using mechanical turk to build machine translation evaluation sets , in proceedings of the naacl hlt 2010 workshop on creating speech and language data with amazon s mechanical turk , association for computational linguistics , pp  208211 .",
    "corney , j. , lynn , a. , torres , c. , di maio , p. , regli , w. , forbes , g. and tobin , l. ( 2010 ) , towards crowdsourcing translation tasks in library cataloguing , a pilot study , in proceedings of the 4th ieee international conference on digital ecosystems and technologies ( dest ) , pp  572577 .",
    "karger , d. r. , oh , s. and shah , d. ( 2011b ) , budget - optimal task allocation for reliable crowdsourcing systems , corr abs/1110.3564 .",
    "karger , d. r. , oh , s. and shah , d. ( 2011a ) , budget - optimal crowdsourcing using low - rank matrix approximations , in 49th annual conference on communication , control , and computing ( allerton ) , pp  284291 .",
    "liu , q. , peng , j. and ihler , a. ( 2012 ) , variational inference for crowdsourcing , advances in neural information processing systems ( nips 2012 ) .",
    "zellner a. ( 1971 ) , an introduction to bayesian inference in econometrics , volume 17 .",
    "john wiley and sons .",
    "eckart , c. and young , g. ( 1936 ) , the approximation of one matrix by another of lower rank , psychometrika 1(3 ) , pp  211218 .",
    "welinder , p. , branson , s. , belongie , s. and perona , p. ( 2010 ) , the multidimensional wisdom of crowds , advances in neural information processing systems 23 , pp  24242432 .",
    "hari , v. and mateja  , j. ( 2009 ) , accuracy of two svd algorithms for triangular matrices , applied mathematics and computation 210(1 ) , pp  232257 .",
    "brand , m. ( 2006 ) , fast low - rank modifications of the thin singular value decomposition , linear algebra and its applications 415(1 ) , pp  2030 .",
    "boker , u. and dershowitz , n. ( 2008 ) , the church - turing thesis over arbitrary domains , 4800 , pp  199229 .",
    "boker , u. and dershowitz , n. ( 2010 ) , three paths to effectiveness , pp  135146 .",
    "paul , g. and cox , e. ( 1997 ) , beyond humanity : cyberevolution and future minds , charles river media .",
    "moravec , h. ( 2000 ) , robot : mere machine to transcendent mind , science / computers , oxford university press .",
    "dreyfus , h. l. and dreyfus , s. e. ( 1988 ) , mind over machine - the power of human intuition and expertise in the era of the computer , free press .",
    "ipeirotis , p. g. , provost , f. , sheng , v. and wang , j. ( 2010 ) , repeated labeling using multiple noisy labelers , in ceder working papers .",
    "mason , w. and suri , s. ( 2010 ) , conducting behavioral research on amazon s mechanical turk , social science research network working paper series .",
    "zhu , d. and carterette , b. ( 2010 ) , an analysis of assessor behavior in crowdsourced preference judgments , in acm sigir workshop on crowdsourcing for search evaluation .",
    "[ lastpage ]"
  ],
  "abstract_text": [
    "<S> crowd computing empowers computer systems by utilizing humans perception , and their ability to solve non - algorithmic problems . in this approach , </S>",
    "<S> a group of humans are asked to contributively solve a problem that can not be solved easily by individuals , or perfectly by computers . however , there are complexities in using humans to solve problems . </S>",
    "<S> lack of generative models , complex cost models , lower speed in comparison to computers , limitation of knowledge and skills , noise , bias and error are examples of such complexities . </S>",
    "<S> an optimized crowd computing system should overcome these complexities , and improve the quality of solutions .    </S>",
    "<S> this paper includes answers to three main questions : what is crowd computing ? </S>",
    "<S> why should one use crowd computing ? and , how to use crowd computing ? </S>",
    "<S> we will briefly answer the two former questions , while we will focus more on the latter one , specially on solving classification problems using multiple checking scenario . </S>",
    "<S> in addition , we will compare the current methods of crowed computing , and provide some guidelines for future works based on the current open issues in this field .    </S>",
    "<S> [ firstpage ]    crowd computing , crowdsourcing , mechanical turk , user modeling , sample selection , label estimation . </S>"
  ]
}