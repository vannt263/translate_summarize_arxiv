{
  "article_text": [
    "the theory of compressed sensing ( cs ) @xcite aims at reconstructing sparse or compressible signals from a small number of linear measurements compared to the dimensionality of the signal space . in short ,",
    "the signal reconstruction is possible if the underlying sensing matrix is well behaved , if it respects a restricted isometry property ( rip ) saying roughly that any small subset of its columns is `` close '' to an orthogonal basis .",
    "the signal recovery is then obtained using non - linear techniques based on convex optimization promoting signal sparsity , such as the basis pursuit program @xcite .",
    "what makes cs more than merely an interesting theoretical concept is that some classes of randomly generated matrices ( gaussian , bernoulli , partial fourier ensemble , etc ) satisfy the rip with overwhelming probability .",
    "this happens as soon as their number of rows , the number of cs measurements , is higher than a few multiples of the assumed signal sparsity .    in a realistic acquisition system , quantization of these measurements",
    "is a natural process that compressed sensing theory has to handle conveniently .",
    "one commonly used technique is to simply treat the quantization distortion as gaussian noise , which leads to reconstruction based on solving the basis pursuit denoising ( bpdn ) program ( either in its constrained or augmented lagrangian forms ) @xcite .",
    "while this approach can give acceptable results , it is theoretically unsatisfactory as the measurement error created by quantization is highly non - gaussian , being essentially uniform and bounded by the quantization bin width .",
    "an appealing requirement for the design of better reconstruction methods is the quantization consistency ( qc ) constraint , that the requantized measurements of the reconstructed signal equal the original quantized measurements .",
    "this idea , in some form , has appeared previously in the literature . near the beginning of the development of cs theory , cands et al .",
    "mentioned that the @xmath6-norm of bpdn should be replaced by the @xmath7-norm to handle more naturally the quantization distortion of the measurements @xcite .",
    "more recently , in @xcite , the extreme case of 1-bit cs is studied , when only the signs of the measurements are sent to the decoder .",
    "authors tackle the reconstruction problem by adding a sign consistency constraint in a modified bpdn program working on the sphere of unit - norm signals . in @xcite , an adaptation of both bpdn and the subspace pursuit",
    "integrates an explicit qc constraint . in @xcite , a model integrating additional gaussian noise on the measurements before their quantization",
    "is analyzed and solved with a @xmath8-regularized maximum likelihood program .",
    "however , in spite of interesting experimental results , no theoretical guarantees are given about the approximation error reached by these solutions .",
    "the qc constraint has also been used previously for image and signal processing outside of the cs field .",
    "examples include oversampled analog to digital converters ( adc ) @xcite , and in image restoration problems @xcite .    in this paper , we propose a new class of convex optimization programs , or decoders , coined the basis pursuit dequantizer of moment @xmath0 ( bpdq@xmath1 ) that model the quantization distortion more faithfully .",
    "these proceed by minimizing the sparsity of the reconstructed signal ( expressed in the @xmath8-norm ) subject to a particular data - fidelity constraint .",
    "this constraint imposes that the difference between the original and the reproduced measurements have bounded @xmath2-norm , for @xmath3 .",
    "as @xmath0 approaches infinity , this fidelity term reproduces the qc constraint as promoted initially in @xcite .",
    "however , our idea is to study , given a certain sparsity level and in function of the number of measurements available , which moment @xmath3 provides the best reconstruction result .",
    "our overall result , which surprisingly does not favor @xmath9 , may be expressed by the principle : _ given a certain sparsity level , if the number of measurements is higher than a minimal value growing with @xmath0 , in oversampled situations , by using bpdq@xmath1 instead of bpdn  =  bpdq@xmath5 the reconstruction error due to quantization can be reduced by a factor of @xmath4 .",
    "_    at first glance , it could seem counterintuitive to oversample the `` compressive sensing '' of a signal .",
    "after all , many results in compressed sensing seek to limit the number of measurements required to encode a signal , while guaranteeing exact reconstruction with high probability .",
    "however , as analyzed for instance in @xcite , this way of thinking avoids to considering the actual amount of information needed to describe the measurement vector . in the case of noiseless observations of a sparse signal , compressed sensing guarantees perfect reconstruction only for real - valued measurements , for an infinite number of bits per measurements .    from a rate - distortion perspective",
    ", the analysis shown in @xcite demonstrates also that cs is suboptimal compared to transform coding . under that point of view",
    ", the best cs encoding strategy is to use all the available bit - rate to obtain as few cs measurements as possible and quantize them as finely as possible .",
    "however , in many practical situations the quantization bit - depth per measurement is pre - determined by the hardware , for real sensors embedding cs and a fixed a / d conversion of the measurements . in that case , the only way to improve the reconstruction quality is to gather more measurements , to oversample the signal .",
    "this does not degrade one of the main interests of compressed sensing , providing highly informative linear signal measurements at a very low computation cost .",
    "the paper is structured as follows . in section",
    "[ sec : cs - quantiz - fmwk ] , we review the principles of compressed sensing and previous approaches for accommodating the problem of measurement quantization .",
    "section [ sec : new - class - decoders ] introduces the bpdq@xmath1 decoders .",
    "their stability , the @xmath10 instance optimality , is deduced using an extended version of the restricted isometry property involving the @xmath2-norm . in section [ sec : example - rip_p-2 ] , standard gaussian random matrices , whose entries are independent and identically distributed ( iid ) standard gaussian , are shown to satisfy this property with high probability for a sufficiently large number of measurements .",
    "section [ sec : bpdq - approx - error - for - quantiz ] explains the key result of this paper ; that the approximation error of bpdq@xmath1 scales inversely with @xmath4 .",
    "section [ sec : implementations ] describes the convex optimization framework adopted to solve the bpdq@xmath1 programs .",
    "finally , section [ sec : experiment ] provides experimental validation of the theoretical power of bpdq@xmath1 on 1-d signals and on an image reconstruction example .",
    "in compressed sensing ( cs ) theory @xcite , the signal @xmath11 to be acquired and subsequently reconstructed is typically assumed to be sparse or _",
    "compressible _ in an orthogonal basis @xmath12 ( wavelet basis , fourier , etc . ) .",
    "in other words , the best @xmath13-term approximation @xmath14 of @xmath15 in @xmath16 gives an exact ( for the sparse case ) or accurate ( for the compressible case ) representation of @xmath15 even for small @xmath17 .",
    "for simplicity , only the canonical basis @xmath18 will be considered here .    at the acquisition stage , @xmath15 is encoded by @xmath19 linear measurements ( with @xmath20 ) provided by a sensing matrix @xmath21 , all known information about @xmath15 is contained in the @xmath19 measurements @xmath22 , where @xmath23 are the rows of @xmath24 .    in this paper , we are interested in a particular non - ideal sensing model .",
    "indeed , as measurement of continuous signals by digital devices always involves some form of quantization , in practice devices based on cs encoding must be able to accommodate the distortions in the linear measurements created by quantization .",
    "therefore , we adopt the noiseless and uniformly quantized sensing ( or coding ) model : @xmath25\\ = \\ \\phi x + n,\\ ] ] where @xmath26 is the quantized measurement vector , @xmath27)_i=\\upalpha\\lfloor ( \\cdot)_i/\\upalpha\\rfloor + \\tfrac{\\upalpha}{2}$ ] is the uniform quantization operator in @xmath28 of bin width @xmath29 , and @xmath30\\- \\phi x$ ] is the _ quantization distortion_.    the model ( [ eq : quantiz - fmwk ] ) is a realistic description of systems where the quantization distortion dominates other secondary noise sources ( thermal noise ) , an assumption valid for many electronic measurement devices including adc . in this paper",
    "we restrict our study to using this extremely simple uniform quantization model , in order to concentrate on the interaction with the cs theory .",
    "for instance , this quantization scenario does not take into account the possible _ saturation _ of the quantizer happening when the value to be digitized is outside the operating range of the quantizer , this range being determined by the number of bits available . for",
    "compressed sensing , this effect has been studied recently in @xcite .",
    "authors obtained better reconstruction methods by either imposing to reproduce saturated measurements ( saturation consistency ) or by discarding these thanks to the `` democratic '' property of most of the random sensing matrices .",
    "their work however does not integrate the quantization consistency for all the unsaturated measurements .",
    "the study of more realistic non - uniform quantization is also deferred as a question for future research .",
    "in much previous work in cs , the reconstruction of @xmath15 from @xmath31 is obtained by treating the quantization distortion @xmath32 as a noise of bounded power ( @xmath6-norm ) @xmath33 . in this case , a robust reconstruction of the signal @xmath15 from corrupted measurements @xmath34 is provided by the basis pursuit denoise ( bpdn ) program ( or decoder ) @xcite : @xmath35 this convex optimization program can be solved numerically by methods like second order cone programming or by monotone operator splitting methods @xcite described in section [ sec : implementations ] .",
    "notice that the noiseless situation @xmath36 leads to the basis pursuit ( bp ) program , which may also be solved by linear programming @xcite .",
    "an important condition for bpdn to provide a good reconstruction is the _ feasibility _ of the initial signal @xmath15 , we must chose @xmath37 in the ( _ fidelity _ ) constraint of bpdn such that @xmath38 . in @xcite , an estimator of @xmath37 for @xmath39 is obtained by considering @xmath32 as a random vector @xmath40 distributed uniformly over the quantization bins , @xmath41)$ ] .",
    "an easy computation shows then that @xmath42 with probability higher than @xmath43 for a certain constant @xmath44 ( by the chernoff - hoeffding bound @xcite ) , where @xmath45    therefore , cs usually handles quantization distortion by setting @xmath46 , typically for @xmath47 .    when the feasibility is satisfied , the stability of bpdn is guaranteed if the sensing matrix @xmath21 satisfies one instance of the following property :    a matrix @xmath21 satisfies the ( extended ) restricted isometry property ( rip@xmath48 ) ( with @xmath49 ) of order @xmath13 and radius @xmath50 , if there exists a constant @xmath51 such that @xmath52 for all @xmath13-sparse signals @xmath53 .    in other words , @xmath24 , as a mapping from @xmath54 to @xmath55 , acts as a ( scaled ) isometry on @xmath13-sparse signals of @xmath56 .",
    "this definition is more general than the common rip @xcite .",
    "this latter , which ensures the stability of bpdn ( see theorem [ prop : l2-l1-instance - optimality - bp ] below ) , corresponds to @xmath57 in ( [ eq : rippq ] ) .",
    "the original definition considers also normalized matrices @xmath58 having unit - norm columns ( in expectation ) so that @xmath59 is absorbed in the normalizing constant .",
    "we prefer to use this extended rip@xmath48 since , as it will become clear in section [ sec : bpdq - approx - error - for - quantiz ] , the case @xmath60 and @xmath61 provides us the interesting embedding ( [ eq : rippq ] ) for measurement vectors corrupted by generalized gaussian and uniform noises . as explained below , this definition includes also other rip generalizations  @xcite .",
    "we note that there are several examples already described in the literature of classes of matrices which satisfy the rip@xmath48 for specific values of @xmath0 and @xmath62 .",
    "for instance , for @xmath57 , a matrix @xmath21 with each of its entries drawn independently from a ( sub ) gaussian random variable satisfies this property with an overwhelming probability if @xmath63 for some value @xmath64 independent of the involved dimensions @xcite .",
    "this is the case of standard gaussian random ( sgr ) matrices whose entries are iid @xmath65 , and of the bernoulli matrices with @xmath66 with equal probability , both cases having @xmath67 @xcite .",
    "other random constructions satisfying the rip@xmath68 are known ( partial fourier ensemble ) @xcite . for the case",
    "@xmath69 , it is proved in @xcite that sparse matrices obtained from an adjacency matrix of a high - quality unbalanced expander graph are rip@xmath70 ( with @xmath71 ) . in the context of non - convex signal reconstruction ,",
    "the authors in @xcite show also that gaussian random matrices satisfy the restricted @xmath0-isometry , rip@xmath48 for @xmath72 , @xmath73 , @xmath74 and appropriate redefinition of @xmath75 .",
    "the following theorem expresses the announced stability result , the @xmath10 instance optimality of bpdn , as a consequence of the rip@xmath68 .",
    "[ prop : l2-l1-instance - optimality - bp ] let @xmath11 be a signal whose compressibility is measured by the decreasing of the @xmath13-term @xmath8-approximation error @xmath76 , for @xmath77 , and @xmath14 the best @xmath13-term @xmath6-approximation of @xmath15 .",
    "let @xmath24 be a rip@xmath68 matrix of order @xmath78 and radius @xmath79 .",
    "given a measurement vector @xmath80 corrupted by a noise @xmath32 with power @xmath81 , the solution @xmath82 obeys @xmath83 for @xmath84 and @xmath85 .",
    "for instance , for @xmath86 , @xmath87 and @xmath88 .",
    "let us precise that the theorem condition @xmath89 on the rip radius can be refined ( like in @xcite ) .",
    "we know nevertheless from davies and gribonval @xcite that @xmath8-minimization will fail for at least one vector for @xmath90 .",
    "the room for improvement is then very small .",
    "using the bpdn decoder to account for quantization distortion is theoretically unsatisfying for several reasons .",
    "first , there is no guarantee that the bpdn solution @xmath91 respects the quantization consistency , @xmath92=y_{\\rm q}\\",
    "\\leftrightarrow\\ \\|y_{\\rm q } - \\phi    x^*\\|_\\infty { \\leqslant}\\tfrac{\\upalpha}{2},\\eqno{({\\bf qc})}\\ ] ] which is not necessarily implied by the bpdn @xmath6 fidelity constraint .",
    "the failure of bpdn to respect qc suggests that it may not be taking advantage of all of the available information about the noise structure in the measurements .",
    "second , from a bayesian maximum a posteriori ( map ) standpoint , bpdn can be viewed as solving an ill - posed inverse problem where the @xmath6-norm used in the fidelity term corresponds to the conditional log - likelihood associated to an additive white gaussian noise",
    ". however , the quantization distortion is not gaussian , but rather uniformly distributed .",
    "this motivates the need for a new kind of cs decoder that more faithfully models the quantization distortion .",
    "[ sec : new - class - decoders ]    the considerations of the previous section encourage the definition of a new class of optimization programs ( or decoders ) generalizing the fidelity term of the bpdn program .",
    "our approach is based on reconstructing a sparse approximation of @xmath15 from its measurements @xmath93 under the assumption that @xmath2-norm ( @xmath94 ) of the noise @xmath32 is bounded , @xmath95 for some @xmath96 .",
    "we introduce the novel programs @xmath97 the fidelity constraint expressed in the @xmath2-norm is now tuned to noises that follow a zero - mean generalized gaussian distribution of such a distribution is @xmath98 for a standard deviation @xmath99 . ] ( ggd ) of _ shape parameter _",
    "@xcite , with the uniform noise case corresponding to @xmath100 .",
    "we dub this class of decoders _",
    "basis pursuit dequantizer _ of _ moment _ @xmath0 ( or bpdq@xmath1 ) since , for reasons that will become clear in section [ sec : bpdq - approx - error - for - quantiz ] , their approximation error when @xmath101 is uniformly quantized has an interesting decreasing behavior when both the moment @xmath0 and the oversampling factor @xmath102 increase .",
    "notice that the decoder corresponding to @xmath103 has been previously analyzed in @xcite for laplacian noise .",
    "one of the main results of this paper concerns the @xmath10 instance optimality of the bpdq@xmath1 decoders , their stability when the signal to be recovered is compressible , and when the measurements are contaminated by noise of bounded @xmath2-norm . in the following theorem",
    ", we show that such an optimality happens when the sensing matrix respects the ( extended ) restricted isometry property rip@xmath104 for @xmath105 .",
    "[ prop : l2-l1-instance - optimality - bpdqp ] let @xmath11 be a signal with a @xmath13-term @xmath8-approximation error @xmath106 , for @xmath77 and @xmath14 the best @xmath13-term @xmath6-approximation of @xmath15 .",
    "let @xmath24 be a rip@xmath104 matrix on @xmath107 sparse signals with constants @xmath108 , for @xmath109 and @xmath110 .",
    "given a measurement vector @xmath34 corrupted by a noise @xmath32 with bounded @xmath2-norm , @xmath111 , the solution @xmath112 of bpdq@xmath1 obeys @xmath113 for values @xmath114 , @xmath115 , and @xmath116 given in the proof of lemma [ lemma : bound - scp - lp ] ( appendix [ sec : proof - lemma - bound - scp - lp ] ) .",
    "as shown in appendix [ sec : proof - theor - refpr ] , this theorem follows from a generalization of the fundamental result proved by cands @xcite to the particular geometry of banach spaces  @xmath2 .",
    "interestingly , it turns out that sgr matrices @xmath117 also satisfy the rip@xmath104 with high probability provided that @xmath19 is sufficiently large compared to the sparsity @xmath13 of the signals to measure .",
    "this is made formal in the following proposition , for which the proof with high probability for @xmath118 when @xmath19 exceeds a similar bound to ( [ eq : sgr - rip - measur - bound ] ) . ] is given in appendix  [ app : why - rip - p ] .",
    "[ prop : gauss - rip - inf ] let @xmath21 be a standard gaussian random ( sgr ) matrix , its entries are iid @xmath119 . then",
    ", if @xmath120 for @xmath105 and @xmath121 for @xmath9 , there exists a constant @xmath64 such that , for @xmath122 + \\log \\tfrac{2}{\\eta}\\big),\\ ] ] with @xmath123 for @xmath124 and @xmath125 for @xmath9 , @xmath24 is rip@xmath104 of order @xmath13 and radius @xmath126 with probability higher than @xmath127 .",
    "moreover , the value @xmath128 is the expectation value of the @xmath2-norm of a sgr vector @xmath40 .    roughly speaking",
    ", this proposition tells us that to generate a matrix that is rip@xmath104 with high probability , we need a number of measurements @xmath19 that grows polynomially in @xmath129 with an `` order '' @xmath130 for @xmath105 , while the limit case @xmath9 grows exponentially in @xmath129 .    notice that an asymptotic estimation of @xmath131 , for @xmath132 , can be found in @xcite for @xmath124 .",
    "however , as presented in the following lemma ( proved in appendix [ sec : proof - lemma - refl ] ) , non - asymptotic bounds for @xmath128 can be expressed in terms of @xmath133 with @xmath134 and @xmath135 .",
    "[ lem : strict - bounds - mu_p ] if @xmath40 is a sgr vector , then , for @xmath124 , @xmath136 in particular , as soon as @xmath137 for @xmath138 , @xmath139 . for @xmath9 , there exists a @xmath140 such that @xmath141 .",
    "an interesting aspect of matrices respecting the rip@xmath104 is that they approximately preserve the decorrelation of sparse vectors of disjoint supports .",
    "[ lemma : bound - scp - lp ] let @xmath142 with @xmath143 and @xmath144 and  @xmath145 , and @xmath146 .",
    "if @xmath24 is rip@xmath104 of order @xmath147 with constant @xmath148 , and of orders @xmath107 and @xmath149 with constants @xmath150 and @xmath151 , then @xmath152 and @xmath153 is given explicitly in appendix [ sec : proof - lemma - bound - scp - lp ] .",
    "it is worth mentioning that the value @xmath154 behaves as @xmath155 for large @xmath0 , and as @xmath156 for @xmath157 .",
    "therefore , this result may be seen as a generalization of the one proved in @xcite ( see lemma 2.1 ) for @xmath158 with @xmath159 . as shown in appendix",
    "[ sec : proof - lemma - bound - scp - lp ] , this lemma uses explicitly the 2-smoothness of the banach spaces @xmath2 when @xmath60 @xcite , in connection with the _ normalized duality mapping _ @xmath160 that plays a central role in the geometrical description of @xmath2 .",
    "lemma [ lemma : bound - scp - lp ] is at the heart of the proof of theorem [ prop : l2-l1-instance - optimality - bpdqp ] , which prevents the later from being valid for @xmath9 .",
    "this is related to the fact that the @xmath161 banach space is not 2-smooth and no duality mapping exists .",
    "therefore , any result for @xmath9 would require different tools than those developed here .",
    "let us now observe the particular behavior of the bpdq@xmath1 decoders on quantized measurements of a sparse or compressible signal assuming that @xmath29 is known at the decoding step . in this section , we consider that @xmath60 everywhere .    first ,",
    "if we assume in the model ( [ eq : quantiz - fmwk ] ) that the quantization distortion @xmath162 - \\phi x$ ] is uniformly distributed in each quantization bin , the simple lemma below provides precise estimator @xmath37 for any @xmath2-norm of  @xmath32 .",
    "[ lemma : expec - and - val - lp - norm - unif - vec ] if @xmath40 is a uniform random vector with @xmath163)$ ] , then , for @xmath164 , @xmath165 in addition , for any @xmath166 , @xmath167 \\ { \\leqslant}\\ e^{-2\\kappa^2}$ ] , while , @xmath168 .",
    "the proof is given in appendix [ sec : proof - expec - and - val - lp - norm - unif - vec ] .    according to this result",
    ", we may set the @xmath2-norm bound @xmath37 of the program bpdq@xmath1 to @xmath169 so that , for @xmath170 , we know that @xmath15 is a feasible solution of the bpdq@xmath1 fidelity constraint with a probability exceeding @xmath171 .",
    "second , theorem  [ prop : l2-l1-instance - optimality - bpdqp ] points out that , when @xmath24 is rip@xmath104 with @xmath146 , the approximation error of the bpdq@xmath1 decoders is the sum of two terms : one that expresses the _ compressibility error _ as measured by @xmath172 , and one , the _ noise error _ , proportional to the ratio @xmath173 .",
    "in particular , by lemma [ prop : gauss - rip - inf ] , for @xmath19 respecting ( [ eq : sgr - rip - measur - bound ] ) , a sgr sensing matrix of @xmath19 rows induces with a controlled probability @xmath174    combining ( [ eq : epsilon - p - def ] ) and the result of lemma [ lem : strict - bounds - mu_p ] , we may bound the noise error for uniform quantization more precisely . indeed , for @xmath110 , if @xmath120 , @xmath175 with @xmath176 .",
    "in addition , using a variant of the stirling formula found in @xcite , we know that @xmath177 for @xmath178 . therefore , we compute easily that , for @xmath179 , @xmath180 with @xmath181 . finally , by ( [ eq : epsilon - p - def ] )",
    ", we see that , @xmath182 with @xmath183 , where we used the bound @xmath184 and the fact that @xmath185 if @xmath186 .    in summary , we can formulate the following principle .",
    "* oversampling principle .",
    "* _ the noise error term in the @xmath10 instance optimality relation ( [ eq : l2l1-for - quantiz ] ) in the case of uniform quantization of the measurements of a sparse or compressible signal is divided by @xmath4 in oversampled sgr sensing , when the _ oversampling factor _",
    "@xmath102 is higher than a minimal value increasing with @xmath0 . _",
    "interestingly , this follows the improvement achieved by adding a qc constraint in the decoding of oversampled adc signal conversion @xcite .",
    "the oversampling principle requires some additional explanations .",
    "taking a sgr matrix , by proposition [ prop : gauss - rip - inf ] , if @xmath187 is the smallest number of measurements for which such a randomly generated matrix @xmath24 is rip@xmath104 of radius @xmath188 with a certain nonzero probability , taking @xmath189 allows one to generate a new random matrix with a smaller radius @xmath190 with the same probability of success .    therefore , increasing the _ oversampling factor _ @xmath102 provides two effects .",
    "first , it enables one to hope for a matrix @xmath24 that is rip@xmath104 for high @xmath0 , providing the desired error division by @xmath4 .",
    "second , as shown in appendix [ sec : delta - propto - m1p ] , since @xmath191 , oversampling gives a smaller @xmath126 hence counteracting the increase of @xmath0 in the factor @xmath154 of the values @xmath192 and @xmath193 .",
    "this decrease of @xmath126 also favors bpdn , but since the values @xmath194 and @xmath195 in ( [ eq : l2-l1-instance - optimality - bp ] ) are also bounded from below this effect is limited .",
    "consequently , as the number of measurements increases the improvement in reconstruction error for bpdn will saturate , while for bpdq@xmath1 the error will be divided by @xmath4 .    from this result",
    ", it is very tempting to choose an extremely large value for @xmath0 in order to decrease the noise error term .",
    "there are however two obstacles with this .",
    "first , the instance optimality result of theorem  [ prop : l2-l1-instance - optimality - bpdqp ] is not directly valid for @xmath9 .",
    "second , and more significantly , the necessity of satisfying rip@xmath104 implies that we can not take @xmath0 arbitrarily large in proposition  [ prop : gauss - rip - inf ] .",
    "indeed , for a given oversampling factor @xmath102 , a sgr matrix @xmath24 can be rip@xmath104 only over a finite interval @xmath196 $ ] .",
    "this implies that for each particular reconstruction problem , there should be an optimal maximum value for @xmath0 .",
    "we will demonstrate this effect experimentally in section [ sec : experiment ] .",
    "we remark that the compressibility error is not significantly reduced by increasing @xmath0 when the number of measurements is large .",
    "this makes sense as the @xmath2-norm appears only in the fidelity term of the decoders , and we know that in the case where @xmath36 the compressibility error remains in the bp decoder @xcite",
    ". finally , note that due to the embedding of the @xmath2-norms , @xmath197 if @xmath198 , increasing @xmath0 until @xmath199 makes the fidelity term closer to the qc .",
    "this section is devoted to the description of the convex optimization tools needed to numerically solve the basis pursuit dequantizer program .",
    "while we generally utilize @xmath60 , the bpdq@xmath1 program is convex for @xmath94 .",
    "in fact , the efficient iterative procedure we describe will converge to to the global minimum of the bpdq@xmath1 program for all @xmath94 .",
    "the bpdq@xmath1 ( and bpdn ) decoders are special case of a general class of convex problems @xcite @xmath200 where @xmath201 is seen as an hilbert space equipped with the inner product @xmath202 .",
    "we denote by @xmath203 the domain of any @xmath204 . in ( @xmath205 ) , the functions @xmath206 are assumed _",
    "( i ) _ convex functions which are not infinite everywhere , @xmath207 , _ ( ii ) _ @xmath208 , and _ ( iii ) _ these functions are lower semi - continuous ( lsc ) meaning that @xmath209 for all @xmath210 . the class of functions satisfying these three properties is denoted @xmath211 . for bpdq@xmath1 ,",
    "these two non - differentiable functions are @xmath212 and @xmath213 if @xmath214 and @xmath215 otherwise , the indicator function of the closed convex set @xmath216 .",
    "it can be shown that the solutions of problem ( @xmath205 ) are characterized by the following fixed point equation : x solves ( @xmath205 ) if and only if @xmath217 the operator @xmath218 is called the _ resolvent operator _ associated to the _ subdifferential operator _",
    "@xmath219 , @xmath220 is a positive scalar known as the proximal step size , and @xmath221 is the identity map on @xmath222 .",
    "we recall that the subdifferential of a function @xmath223 at @xmath224 is the set - valued map @xmath225 , where each element @xmath226 of @xmath219 is called a subgradient .",
    "the resolvent operator is actually identified with the _ proximity operator _ of @xmath227 , @xmath228 , introduced in @xcite as a generalization of convex projection operator .",
    "it is defined as the unique solution @xmath229 for @xmath230 . if @xmath231 for some closed convex set @xmath232 , @xmath233 is equivalent to orthogonal projection onto @xmath234 . for @xmath235 , @xmath236",
    "is given by component - wise soft - thresholding of @xmath15 by threshold @xmath237 @xcite .",
    "in addition , proximity operators of lsc convex functions exhibit nice properties with respect to translation , composition with frame operators , dilation , etc . @xcite .    in problem ( @xmath205 ) with @xmath238 , the resolvent operator @xmath239 typically can not be calculated in closed - form .",
    "monotone operator splitting methods do not attempt to evaluate this resolvent mapping directly , but instead perform a sequence of calculations involving separately the individual resolvent operators @xmath240 and @xmath241 .",
    "the latter are hopefully easier to evaluate , and this holds true for our functionals in bpdq@xmath1 .    since for bpdq@xmath1 ,",
    "both @xmath242 and @xmath243 are non - differentiable , we use a particular monotone operator splitting method known as the douglas - rachford ( dr ) splitting .",
    "it can be written as the following compact recursion formula @xcite @xmath244 where @xmath245 for any operator @xmath246 , @xmath247 for all @xmath248 , @xmath249 is the component - wise soft - thresholding operator with threshold @xmath250 and @xmath251 is the orthogonal projection onto the tube @xmath252 . from @xcite , one can show that the sequence @xmath253 converges to some point @xmath254 and @xmath255 is a solution of bpdq@xmath1 . in the next section",
    ", we provide a way to compute @xmath255 efficiently .",
    "each step of the dr iteration requires computation of @xmath256 for @xmath257 .",
    "we present an iterative method to compute this projection for @xmath3 .",
    "notice first that , defining the unit @xmath2 ball @xmath258 , we have @xmath259 with the affine operator @xmath260 .    the proximity operator of a pre - composition of a function @xmath261 with an affine operator can be computed from the proximity operator of @xmath262 .",
    "indeed , let @xmath263 and the affine operator @xmath264 with @xmath265 . if @xmath266 is a tight frame of @xmath222 , @xmath267 for some @xmath64 , we have @xmath268 @xcite .",
    "moreover , for a general bounded matrix @xmath266 , we can use the following lemma .",
    "[ lem : proxa ] let @xmath263 be a matrix with bounds @xmath269 such that @xmath270 and let @xmath271 be a sequence with @xmath272 .",
    "define @xmath273 p^{(t+1 ) } & = & x - \\phi'^ * u^{(t+1)}. \\end{array}\\ ] ] if the matrix @xmath266 is a general frame of @xmath274 , @xmath275 , then @xmath276 .",
    "in addition , @xmath277 and @xmath278 in .",
    "more precisely , both @xmath279 and @xmath280 converge linearly and the best convergence rate is attained for @xmath281 with @xmath282 . otherwise , if @xmath266 is just bounded ( @xmath283 ) , and if @xmath284 , apply , and then @xmath285 and @xmath286 at the rate @xmath287 .    in conclusion",
    ", computing @xmath288 may be reduced to applying the orthogonal projection @xmath289 by setting @xmath290 , @xmath291 and @xmath292 inside the iterative method ( [ eq : proxfbdual ] ) with a number of iterations depending on the selected application ( see section [ sec : experiment ] ) .    for @xmath158 and @xmath9 , the projector @xmath293 has an explicit form . indeed , if @xmath294 is outside the closed unit @xmath2-ball in @xmath28 , then @xmath295 ; and @xmath296 for @xmath297 .",
    "unfortunately , for @xmath298 no known closed - form for the projection exists .",
    "instead , we describe an iterative method .",
    "set @xmath299 and @xmath300 .",
    "if @xmath301 , @xmath302 . for @xmath303 ,",
    "the projection @xmath293 is the solution of the constrained minimization problem @xmath304 .",
    "let @xmath305 be its lagrange function ( for @xmath306 ) @xmath307 without loss of generality , by symmetry , we may work in the positive orthant @xmath308 and @xmath309 , since the point @xmath294 and its projection @xmath310 belong to the same orthant of @xmath28 , @xmath311 for all @xmath312 .    as @xmath313 and @xmath314",
    "are continuously differentiable , the karush - kuhn - tucker system corresponding to is @xmath315 where the solution @xmath310 is non - degenerate by strict convexity in @xmath226 @xcite , and @xmath316 the corresponding lagrange multiplier .",
    "let us write @xmath317 and @xmath318 as @xmath319 the kkt system is equivalent to @xmath320 , where the desired projection @xmath310 is then given by the first @xmath19 coordinates of @xmath321 .",
    "this defines a system of @xmath322 equations with @xmath322 unknowns @xmath323 that we can solved efficiently with the newton method .",
    "this is the main strategy underlying sequential quadratic programming used to solve general - type constrained optimization problems @xcite .",
    "given an initialization point @xmath324 , the successive iterates are defined by @xmath325 where @xmath326 is the jacobian associated to @xmath327 . if the iterates sequence @xmath328 is close enough to @xmath323 , we known that the jacobian is nonsingular as @xmath310 is non - degenerate .",
    "moreover , since that jacobian has a simple block - invertible form , we may compute ( @xcite , p.125 ) @xmath329 where @xmath330 is a diagonal matrix with @xmath331 , @xmath332 with @xmath333 for @xmath312 , @xmath334 , @xmath335 .",
    "this last expression can be computed efficiently as @xmath336 is diagonal .",
    "we initialize the first @xmath19 components of @xmath324 by the direct radial projection of @xmath294 on the unit @xmath2-ball , @xmath337 , and initialize @xmath338 .    in summary , to compute @xmath293 , we run using to calculate each update step .",
    "we terminate the iteration when the norm of @xmath339 falls below a specified tolerance .",
    "since the newton method converges superlinearly , we obtain error comparable to machine precision with typically fewer than 10 iterations .",
    "as an experimental validation of the bpdq@xmath1 method , we ran two sets of numerical simulations for reconstructing signals from quantized measurements .",
    "for the first experiment we studied recovery of exactly sparse random 1-d signals , following very closely our theoretical developments . setting the dimension @xmath340 and the sparsity level @xmath341 , we generated 500 @xmath13-sparse signals where the non - zero elements were drawn from the standard gaussian distribution @xmath119 , and located at supports drawn uniformly in @xmath342 .",
    "for each sparse signal @xmath15 , @xmath19 quantized measurements were recorded as in model ( [ eq : quantiz - fmwk ] ) with a sgr matrix @xmath343 .",
    "the bin width was set to @xmath344 .",
    "the decoding was accomplished with bpdq@xmath1 for various moments @xmath60 using the optimization algorithm described in section [ sec : implementations ] .",
    "in particular , the overall douglas - rachford procedure was run for 500 iterations . at each dr step , the method in was iterated until the relative error @xmath345 fell below @xmath346 ; the required number of iterations was dependent on @xmath19 but was fewer than 700 in all cases examined .    in figure",
    "[ fig : first - exper ] , we plot the average quality of the reconstructions of bpdq@xmath1 for various values of @xmath60 and @xmath347 $ ] .",
    "we use the quality measure @xmath348 , where @xmath15 is the true original signal and @xmath349 the reconstruction .",
    "as can be noticed , at higher oversampling factors @xmath102 the decoders with higher @xmath0 give better reconstruction performance .",
    "equivalently , it can also be observed that at lower oversampling factors , increasing @xmath0 beyond a certain point degrades the reconstruction performance .",
    "these two effects are consistent with the remarks noted at the end of section [ sec : bpdq - approx - error - for - quantiz ] , as the sensing matrices may fail to satisfy the rip@xmath104 if @xmath0 is too large for a given oversampling factor .",
    "one of the original motivations for the bpdq@xmath1 decoders is that they are closer to enforcing quantization consistency than the bpdn decoder . to check this",
    ", we have examined the `` quantization consistency fraction '' , the average fraction of remeasured coefficients @xmath350 that satisfy @xmath351 .",
    "these are shown in figure [ fig : first - exper ] ( c ) for various @xmath0 and @xmath102 .",
    "as expected , it can be clearly seen that increasing @xmath0 increases the qc fraction .",
    "an even more explicit illustration of this effect is afforded by examining histograms of the normalized residual @xmath352 for different @xmath0 . for reconstruction",
    "exactly satisfying qc , these normalized residuals should be supported on @xmath353 $ ] . in figure",
    "[ fig : normalized - residual - histograms ] we show histograms of normalized residuals for @xmath158 and @xmath354 , for the case @xmath355 .",
    "the histogram for @xmath354 is indeed closer to uniform on @xmath353 $ ] .    for the second experiment",
    ", we apply a modified version of the bpdq@xmath1 to an undersampled mri reconstruction problem . using an example similar to @xcite ,",
    "the original image is a @xmath356 pixel `` synthetic angiogram '' , @xmath357 , comprised of 10 randomly placed non - overlapping ellipses .",
    "the linear measurements are the real and imaginary parts of a fraction @xmath358 of the fourier coefficients at randomly selected locations in fourier space , giving @xmath359 independent measurements .",
    "these random locations form the index set @xmath360 with @xmath361 .",
    "experiments were carried out with @xmath362 , but we show results only for @xmath363 .",
    "these were quantized with a bin width @xmath364 , giving at most 12 quantization levels for each measurement .    for this example",
    ", we modify the bpdq@xmath1 program [ eq : bpdq ] by replacing the @xmath8 term by the total variation ( tv ) semi - norm @xcite .",
    "this yields the problem + @xmath365 where @xmath366 is the restriction of discrete fourier transform matrix @xmath327 to the rows indexed in @xmath367 .",
    "this may be solved with the douglas - rachford iteration , with the modification that @xmath368 be replaced by the proximity operator associated to @xmath237 times the tv norm , by @xmath369 .",
    "the latter is known as the rudin - osher - fatemi model , and numerous methods exist for solving it exactly , including @xcite . in this work",
    ", we use an efficient projected gradient descent algorithm on the dual problem , see @xcite . note that the sensing matrix @xmath370 is actually a tight frame , @xmath371",
    ", so we do not need the nested inner iteration .    .",
    "left , @xmath158 .",
    "right , @xmath354 .",
    "[ fig : normalized - residual - histograms ] , title=\"fig:\",height=132 ] . left , @xmath158 .",
    "right , @xmath354 .",
    "[ fig : normalized - residual - histograms ] , title=\"fig:\",height=132 ]    we show the snr of the bpdq@xmath1 reconstructions as a function of @xmath0 in figure  [ fig:2d - snr ] , averaged over 50 trials where both the synthetic angiogram image and the fourier measurement locations are randomized .",
    "this figure also depicts the snr improvement of bpdq@xmath1-based reconstruction over bpdn . for these simulations we used 500 iterations of the douglas - rachford recursion .",
    "this quantitative results are confirmed by visual inspection of figure  [ fig : examp2d ] , where we compare @xmath372 pixel details of the reconstruction results with bpdn and with bpdq@xmath1 for @xmath354 , for one particular instance of the synthetic angiogram signal .",
    "note that this experiment lies far outside of the justification provided by our theoretical developments , as we do not have any proof that the sensing matrix @xmath370 satisfies the rip@xmath104 , and our theory was developed only for @xmath8 synthesis - type regularization , while the tv regularization is of analysis type . nonetheless , we obtain results analogous to the previous 1-d example ; the bpdq@xmath1 reconstruction shows improvements both in snr and visual quality compared to bpdn .",
    "these empirical results suggest that the bpdq@xmath1 method may be useful for a wider range of quantized reconstruction problems , and also provoke interest for further theoretical study .",
    "average snr ( solid ) and snr improvement over bpdn ( dashed ) as a function of @xmath0 , for the synthetic angiogram reconstruction simulations .",
    "error bars indicate 1 standard deviation .",
    ", height=153 ]    the objective of this paper was to show that the bpdn reconstruction program commonly used in compressed sensing with noisy measurements is not always adapted to quantization distortion .",
    "we introduced a new class of decoders , the basis pursuit dequantizers , and we have shown both theoretically and experimentally that bpdq@xmath1 exhibit a substantial reduction of the reconstruction error in oversampled situations .    a first interesting question for further study would be to characterize the evolution of the optimal moment @xmath0 with the oversampling ratio .",
    "this would allow for instance the selection of the best bpdq@xmath1 decoder in function of the precise cs coding / decoding scenario .",
    "second , it is also worth investigating the existence of other rip@xmath104 random matrix constructions , using the random fourier ensemble . third , a more realistic coding / decoding scenario should set @xmath29 theoretically in function of the bit budget ( rate ) available to quantize the measurements , of the sensing matrix and of some a priori on the signal energy .",
    "this should be linked also to the way our approach can integrate the saturation of the quantized measurements @xcite .",
    "finally , we would like to extend our approach to non - uniform scalar quantization of random measurements , generalizing the quantization consistency and the optimization fidelity term to this more general setting .",
    "lj and dkh are very grateful to prof .",
    "pierre vandergheynst ( signal processing laboratory , lts2/epfl , switzerland ) for his useful advices and his hospitality during their postdoctoral stay in epfl .",
    "before proving proposition [ prop : gauss - rip - inf ] , let us recall some facts of measure concentrations @xcite .    in particular , we are going to use the concentration property of any lipschitz function over @xmath28 , @xmath327 such that @xmath373 . if @xmath374 , @xmath327 is said 1-lipschitz .",
    "[ lem : measconc - lipschitz ] if @xmath327 is lipschitz with @xmath375 , then , for the random vector @xmath40 with @xmath376 , @xmath377\\ { \\leqslant}\\ 2 e^{-{\\frac{1}{2 } } r^2 \\lambda^{-2}},\\quad { \\rm for}\\ r>0,\\ ] ] with @xmath378 and @xmath379 .",
    "a useful tool that we will use is the concept of a _",
    "net_. an @xmath37-net ( @xmath380 ) of @xmath381 is a subset @xmath382 of @xmath246 such that for every @xmath383 , one can find @xmath384 with @xmath385 . in certain cases , the size of a @xmath37-net can be bounded .",
    "[ lem : size - eps - net - sphere ] there exists a @xmath37-net @xmath382 of the unit sphere of @xmath386 of size @xmath387 .",
    "we will use also this fundamental result .",
    "[ lemma : eps - net - sufficient ] let @xmath382 be a @xmath37-net of the unit sphere in @xmath386 . then , if for some vectors @xmath388 in the banach space @xmath389 normed by @xmath390 , we have @xmath391 for all @xmath392 , then @xmath393 for all @xmath394 , with @xmath395 .    in our case ,",
    "the banach space @xmath389 is @xmath396 for @xmath397 , i.e  @xmath28 equipped with the norm @xmath398 . with all these concepts , we can now demonstrate the main proposition .",
    "let @xmath94 .",
    "we must prove that for a sgr matrix @xmath21 , with @xmath399 , with the right number of measurements @xmath19 , there exist a radius @xmath400 and a constant @xmath401 such that @xmath402 for all @xmath11 with @xmath403 .",
    "we begin with a unit sphere @xmath404 for a fixed support @xmath405 of size @xmath406 .",
    "let @xmath407 be an @xmath37-net of @xmath408 .",
    "we consider the sgr random process that generates @xmath24 and , by an abuse of notation , we identify it for a while with @xmath24 itself . in other words , we define the random matrix @xmath409 where , for all @xmath410 , @xmath411 is a random vector of probability density function ( or _ pdf _ ) @xmath412 for @xmath413 and @xmath414 ( the standard gaussian pdf ) .",
    "therefore , @xmath24 is related to the pdf @xmath415 , @xmath416 .    since the frobenius norm @xmath417 of @xmath418 and the pdf @xmath419 are invariant under a global rotation in @xmath56 of all the rows of @xmath418 , it is easy to show that for unit vector @xmath420 , @xmath421 = p_\\phi\\big[| f(\\phi_1 ) -",
    "\\mu_f | >    r\\big ] { \\leqslant}2 e^{-{\\frac{1}{2 } } r^2 \\lambda^{-2 } } $ ] , using lemma [ lem : measconc - lipschitz ] on the sgr vector @xmath422 .",
    "the above holds for a single @xmath107 . to obtain a result valid for all @xmath423 we may use the union bound . as @xmath424 by lemma [ lem : size - eps - net - sphere ] , setting @xmath425 for @xmath96 , we obtain @xmath426\\ { \\leqslant}\\ 2 \\,e^{k\\log(1 + 2\\epsilon^{-1})-{\\frac{1}{2 } } \\epsilon^2\\mu_f^2 \\lambda^{-2}},\\ ] ] for all @xmath423 .",
    "taking now @xmath427 for @xmath397 , we have @xmath428 for a sgr vector @xmath40 .",
    "the lipschitz value is @xmath429 for @xmath60 , and @xmath430 for @xmath431 .",
    "consequently , @xmath432 for all @xmath423 , with a probability higher than @xmath433 .",
    "we apply lemma [ lemma : eps - net - sufficient ] by noting that , as @xmath107 has support of size @xmath13 , ( [ lemma : conc - bound - enet ] ) may be written as @xmath434 where @xmath435 are the columns of @xmath436 corresponding to the support of @xmath107 ( we abuse notation to let @xmath437 range only over the support of @xmath107 ) .",
    "then according to lemma [ lemma : eps - net - sufficient ] we have , with the same probability bound and for @xmath438 , @xmath439 for all @xmath11 with @xmath440 .",
    "the result can be made independent of the choice of @xmath441 by considering that there are @xmath442 such possible supports .",
    "therefore , applying again an union bound , ( [ lemma : l2bound - ksupp ] ) holds for all @xmath13-sparse @xmath15 in @xmath56 with a probability higher than @xmath443}$ ] .",
    "let us bound this probability first for @xmath444 .",
    "for @xmath137 and @xmath445 , lemma [ lem : strict - bounds - mu_p ] ( page ) tells us that @xmath446 with @xmath447 .",
    "a probability of success @xmath127 with @xmath448 is then guaranteed if we select , for @xmath449 , @xmath450 + \\log \\tfrac{2}{\\eta}\\big),\\ ] ] since @xmath451 , and for @xmath146 , @xmath452 + \\log \\tfrac{2}{\\eta}\\big),\\ ] ] since @xmath453 .    from now , @xmath454 or @xmath455",
    "means that there exists a constant @xmath64 such that these inequalities hold .",
    "according to the lower bound found in section [ sec : bpdq - approx - error - for - quantiz ] , @xmath456 implying that @xmath457 . since @xmath458 for any @xmath60 and @xmath459 , we find the new sufficient conditions , @xmath460 + \\log \\tfrac{2}{\\eta}\\big),\\ ] ] for @xmath449 , and @xmath461 + \\log \\tfrac{2}{\\eta}\\big),\\ ] ] for @xmath146 .",
    "second , in the specific case where @xmath9 , since there exists a @xmath140 such that @xmath462 , with @xmath463 , @xmath464\\ + \\ \\log \\tfrac{2}{\\eta}\\big ) .",
    "$ ]    let us make some remarks about the results and the requirements of the last proposition .",
    "notice first that for @xmath158 , we find the classical result proved in @xcite .",
    "second , as for the comparison between the common rip@xmath68 proof @xcite and the tight bound found in @xcite , the requirements on the measurements above are possibly pessimistic , the exponent @xmath465 occurring in ( [ eq : p - g-2-meas - req ] ) is perhaps too small .",
    "proposition [ prop : gauss - rip - inf ] has however the merit to prove that random gaussian matrices satisfy the rip@xmath104 in a certain range of dimensionality .",
    "for @xmath466 , proposition [ prop : gauss - rip - inf ] shows that , if @xmath467 + \\log \\tfrac{2}{\\eta}\\big)$ ] for a certain constant @xmath64 , a sgr matrix @xmath21 is rip@xmath104 of order @xmath13 and radius @xmath400 with a probability higher than @xmath127 .",
    "let us assume that @xmath468 for some @xmath469 .",
    "we have , @xmath470 , and therefore , the same event occurs with the same probability bound when @xmath471 + \\frac{k}{p}\\log m - k\\log d + \\log \\tfrac{2}{\\eta}\\big)$ ] . for high @xmath19 and for fixed @xmath472 and @xmath473 ,",
    "this provides @xmath191 , which meets the previous assumption .",
    "the result for @xmath9 is due to @xcite ( see eq ( 3.14 ) ) .",
    "let @xmath40 be a sgr vector , @xmath474 for @xmath312 , and @xmath444 .",
    "first , the inequality @xmath475 follows from the application of the jensen inequality @xmath476 with the convex function @xmath477 .",
    "second , the lower bound on @xmath478 arises from the observation that for @xmath479 with @xmath480 , and for a given @xmath481 , @xmath482 for all @xmath483 .",
    "indeed , observe first that since @xmath484 for @xmath485 and @xmath486 , it is sufficient to prove the result for @xmath487 . proving amounts then to prove @xmath488 , or equivalently , @xmath489 .",
    "the lhs of this last inequality takes its minimum in @xmath490 with value @xmath491 , which provides the result .",
    "moreover , since @xmath502 and using the following approximation of the gamma function @xcite @xmath503 , valid for @xmath178 , we observe that @xmath504 that holds also if @xmath505 with @xmath94 .",
    "therefore , @xmath506 and finally @xmath507 for a constant @xmath508 independent of @xmath0 and @xmath19 .",
    "the lemma relies mainly on the geometrical properties of the banach space @xmath396 for @xmath60 . in @xcite",
    ", it is explained that this space is @xmath0-convex and 2-smooth .",
    "the smoothness involves in particular @xmath513 where @xmath514 and @xmath515 is the _ duality _ mapping of _ gauge function _ @xmath516 for @xmath517 . for the hilbert space @xmath6",
    ", the relation reduces of course to the _ polarization identity_. for @xmath2 , @xmath515 is the differential of @xmath518 , @xmath519 .",
    "let us take @xmath522 and @xmath523 with @xmath143 , @xmath144 , @xmath512 , @xmath524 and for a certain @xmath525 that we will set later .",
    "because @xmath24 is assumed rip@xmath104 for @xmath107 , @xmath149 and @xmath147 sparse signals , we deduce @xmath526 where the absolute value on the inner product arises from the invariance of the rip bound on under the change @xmath521 .",
    "the value @xmath527 is thus bounded by an expression of type @xmath528 with @xmath529 for @xmath60 given by @xmath530 and @xmath531 .",
    "since the minimum of @xmath262 is @xmath532 , we get @xmath533^{{\\frac{1}{2}}},\\end{gathered}\\ ] ] with @xmath534 .    in parallel ,",
    "a change @xmath535 in provides @xmath536 where we used the fact that @xmath537 . by summing this inequality with ( [ eq : scp_xy_first ] )",
    ", we have @xmath538 using the rip@xmath104 on @xmath522 and @xmath523 as above leads to @xmath539 with the same argument as before to explain the absolute value .",
    "minimizing over @xmath540 as above gives @xmath541^{{\\frac{1}{2}}}.\\end{gathered}\\ ] ] together , ( [ eq - pl3-a ] ) and ( [ eq - pl3-b ] ) imply @xmath542^{{\\frac{1}{2}}},\\\\   \\big[\\big(\\delta_{s+s ' } + \\bar p\\,\\tfrac{1 + \\delta_{s+s'}}{2}\\big)\\big(\\delta_{s+s ' } + \\bar p\\,\\tfrac{2 + \\delta_{s'}+\\delta_{s+s'}}{2}\\big)\\big]^{{\\frac{1}{2}}}\\big\\}.\\end{gathered}\\ ] ] it is easy to check that @xmath153 behaves as @xmath543 for @xmath544 , and as @xmath545 for @xmath157 .",
    "let us write @xmath546 .",
    "we have to characterize the behavior of @xmath547 . in the following , for any vector @xmath548 with @xmath549 ,",
    "we define @xmath550 as the vector in @xmath551 equal to @xmath226 on the index set @xmath552 and 0 elsewhere .",
    "we define @xmath553 and a partition @xmath554 of the support of @xmath555 .",
    "this partition is determined by ordering elements of @xmath556 off of the support of @xmath14 in decreasing absolute value .",
    "we have @xmath557 for all @xmath558 , @xmath559 for @xmath560 , and crucially that @xmath561 for all @xmath562 and @xmath563 .",
    "we start from @xmath564 with @xmath565 , and we are going to bound separately the two terms of the rhs . in @xcite , it is proved that @xmath566 with @xmath567 .",
    "therefore , @xmath568    let us bound now @xmath569 by using the rip@xmath104 . from the definition of the mapping @xmath160",
    ", we have @xmath570 by the hlder inequality with @xmath571 and @xmath572 , @xmath573 since @xmath574 . using lemma [ lemma : bound - scp - lp ] , as @xmath575 is 2k sparse and @xmath576 is k sparse , we know that , for @xmath577 , @xmath578 with @xmath116 , so that , using again the rip@xmath104 of @xmath24 and ( [ eq : candes - compress - bound ] ) , @xmath579    { \\leqslant}2\\epsilon\\mu_{p,2}(1+\\delta_{2k})^{\\frac{1}{2}}\\|h_{t_{01}}\\|_2     + \\mu_{p,2}^2 c_p\\|h_{t_{01}}\\|_2\\sum_{k{\\geqslant}2}\\|h_{t_k}\\|_2\\\\ { \\leqslant}2\\epsilon\\mu_{p,2}(1+\\delta_{2k})^{{\\frac{1}{2}}}\\|h_{t_{01}}\\|_2\\hfill\\\\    + \\mu_{p,2}^2c_p\\|h_{t_{01}}\\|_2\\big(\\|h_{t_{01}}\\|_2 + 2e_0(k)\\big).\\end{gathered}\\ ] ] after some simplifications , we get finally @xmath580      for a random variable @xmath581)$ ] , we compute easily that @xmath582 and @xmath583 . therefore , for a random vector @xmath40 with components",
    "@xmath584 independent and identically distributed as @xmath226 , @xmath585 and @xmath586 .    to prove the probabilistic inequality below ( [ eq : expec - and - val - lp - norm - unif - vec ] ) ,",
    "we define , for @xmath312 , the positive random variables @xmath587 bounded on the interval @xmath588 $ ] with @xmath589 . denoting @xmath590 ,",
    "the chernoff - hoeffding bound @xcite tells us that , for @xmath483 , @xmath591 { \\leqslant}e^{-2t^2 m }    $ ] .",
    "therefore , @xmath592 \\ { \\leqslant}\\ e^{-2t^2 m},\\ ] ] which gives , for @xmath593 , @xmath594 \\ { \\leqslant}\\ e^{-2\\kappa^2}.\\ ] ] the limit value of @xmath595 when @xmath100 is left to the reader .",
    "l.  jacques , d.  k. hammond , and m.  j. fadili , `` dequantizing compressed sensing with non - gaussian constraints , '' in _ proc . of ieee international conference on image processing ( icip )",
    "_ , cairo , egypt , nov . 2009 .",
    "w.  dai , h.  v. pham , and o.  milenkovic , `` distortion - rate functions for quantized",
    "compressive sensing , '' _ submitted to ieee information theory workshop ( itw ) and to ieee international symposium on informationtheory ( isit ) _",
    ", 2009 , arxiv:0901.0749 .",
    "n.  thao and m.  vetterli , `` deterministic analysis of oversampled a / d conversion and decoding improvement based on consistent estimates , '' _ signal processing , ieee transactions on [ see also acoustics , speech , and signal processing , ieee transactions on ] _ , vol .",
    "42 , no .  3 , pp .",
    "519531 , 1994 .",
    "p.  weiss , l.  blanc - feraud , t.  andre , and m.  antonini , `` compression artifacts reduction using variational methods : algorithms and experimental study , '' in _ acoustics , speech and signal processing , 2008 .",
    "icassp 2008 .",
    "ieee international conference on _ , 2008 , pp .",
    "11731176 .",
    "r.  g. baraniuk , m.  a. davenport , r.  a. devore , and m.  b. wakin , `` a simple proof of the restricted isometry property for random matrices , '' _ constructive approximation _ , vol .",
    "28 , no .  3 , pp .",
    "253263 , december 2008 .",
    "s.  mendelson , a.  pajor , and n.  tomczak - jaegermann , `` reconstruction and subgaussian operators in asymptotic geometric analysis , '' _ geometric and functional analysis _",
    "17 , no .",
    "4 , pp . 12481282 , 2007 .",
    "s. foucart and m.j .",
    "lai , `` sparsest solutions of underdetermined linear systems via @xmath597-minimization for @xmath598 , '' _ applied and computational harmonic analysis _ , vol .",
    "26 , no .  3 , pp .",
    "395407 , 2009 .",
    "m. e. davies and r. gribonval , `` restricted isometry constants where @xmath2-sparse recovery can fail for @xmath599 , '' _ information theory , ieee transactions on _ , vol .",
    "22032214 , may 2009 .",
    "j.  fuchs , `` fast implementation of a @xmath8-@xmath8 regularized sparse representations algorithm . '' in _ proceedings of the 2009 ieee international conference on acoustics , speech and signal processing_.1em plus 0.5em minus 0.4emieee computer society , 2009 , pp .",
    "33293332 .",
    "laurent jacques received the b.sc . in physics , the m.sc . in mathematical physics and the phd in mathematical physics from the universit catholique de louvain ( ucl ) ,",
    "he was a postdoctoral researcher with the communications and remote sensing laboratory of ucl in 2005 - 2006 .",
    "he obtained in oct .",
    "2006 a four - year ( 3 + 1 ) postdoctoral funding from the belgian frs - fnrs in the same lab .",
    "he was a visiting postdoctoral researcher , in spring 2007 , at rice university ( dsp / ece , houston , tx , usa ) , and from 2007 to 2009 , at the swiss federal institute of technology ( lts2/epfl , switzerland ) .",
    "his research focuses on sparse representations of signals ( 1-d , 2-d , sphere ) , compressed sensing , inverse problems , and computer vision .",
    "david k. hammond was born in loma linda , california .",
    "he received a b.s . with honors in mathematics and chemistry from the caltech in 1999 , then served as a peace corps volunteer teaching secondary mathematics in malawi from 1999 - 2001 . in 2001",
    "he began studying at the courant institute of mathematical sciences at new york university , receiving a phd in mathematics in 2007 . from 2007 to 2009 , he was a postdoctoral researcher at the ecole polytechnique federale de lausanne . since 2009 , he is postdoc at the neuroinformatics center at the university of oregon , usa .",
    "his research interests focus on image processing and statistical signal models , data processing on graph , as well as inverse problems related to eeg source localization for neuroimaging .",
    "jalal m. fadili graduated from the ecole nationale suprieure dingnieurs ( ensi ) de caen , caen , france , and received the m.sc . and ph.d .",
    "degrees in signal and image processing from the university of caen .",
    "he was a research associate with the university of cambridge ( macdonnel - pew fellow ) , cambridge , u.k . , from 1999 to 2000 .",
    "he has been an associate professor of signal and image processing since september 2001 at ensi .",
    "he was a visitor at several universities ( qut - australia , stanford university , caltech , epfl ) .",
    "his research interests include statistical approaches in signal and image processing , inverse problems , computational harmonic analysis , optimization and sparse representations .",
    "his areas of application include medical and astronomical imaging ."
  ],
  "abstract_text": [
    "<S> in this paper we study the problem of recovering sparse or compressible signals from uniformly quantized measurements . </S>",
    "<S> we present a new class of convex optimization programs , or decoders , coined basis pursuit dequantizer of moment @xmath0 ( bpdq@xmath1 ) , that model the quantization distortion more faithfully than the commonly used basis pursuit denoise ( bpdn ) program . </S>",
    "<S> our decoders proceed by minimizing the sparsity of the signal to be reconstructed subject to a data - fidelity constraint expressed in the @xmath2-norm of the residual error for @xmath3 .    </S>",
    "<S> we show theoretically that , _ </S>",
    "<S> ( i ) _ the reconstruction error of these new decoders is bounded if the sensing matrix satisfies an extended restricted isometry property involving the @xmath2 norm , and _ </S>",
    "<S> ( ii ) _ , for gaussian random matrices and uniformly quantized measurements , bpdq@xmath1 performance exceeds that of bpdn by dividing the reconstruction error due to quantization by @xmath4 . </S>",
    "<S> this last effect happens with high probability when the number of measurements exceeds a value growing with @xmath0 , in an oversampled situation compared to what is commonly required by bpdn = bpdq@xmath5 . to demonstrate the theoretical power of bpdq@xmath1 , we report numerical simulations on signal and image reconstruction problems .    </S>",
    "<S> compressed sensing , convex optimization , denoising , optimality , oversampling , quantization , sparsity . </S>"
  ]
}