{
  "article_text": [
    "in the last years a powerful set of tools to study complexity has been developed by physicists and applied to economic and social systems ; among the several topics under investigation the quantitative estimation and management of several typologies of risks @xcite , like financial risk @xcite and operational risk @xcite has recently emerged .",
    "_ operational risk _ ( or ) is defined as `` the risk of [ money ] loss resulting from inadequate or failed internal processes , people and systems or from external events '' @xcite , including legal risk , but excluding strategic and reputation linked risks . since it depends on a family of heterogeneous causes , in the past only few banks dealt with or management . starting from 2005 the approval of _ `` the new basel capital accord '' _",
    "( basel ii ) has substantially changed this picture : in fact or is now considered a critical risk factor and banks are prescribed to cope with it setting aside a certain capital charge .",
    "basel ii proposes three methods to determine this capital : i ) the _ basic indicator approach _ ( bia ) sets it to @xmath0 of the bank s gross income ; ii ) the _ standardized approach _ ( sta or tsa ) is a simple generalization of the bia : the parcentage of the gross income is different for each business line ( bl ) and varies between @xmath1 and @xmath2 ; iii ) the _ advanced measurement approach _",
    "( ama ) allows each bank to use an internally developed procedure to estimate the impact of or .",
    "both the bia and the sta seems overly simplistic , since in some way they suppose that the exposure of a bank to operational losses is proportional to its size . on the other side , an ama not only helps a bank to set aside the correct capital charge , but may even allow the _ or management _ , in the prospect of limiting the amount of future losses .",
    "each ama has to take into account two types of historical operational losses : the internal ones , collected by the bank itself , and the external ones which may belong to a database shared among several banks .",
    "nevertheless , due to the recent interest for or , only small and not adequately accurate historical databases exist and this is why each ama is required to use also assessment data produced by experts .",
    "in addition , basel ii provides a classification of operational losses in @xmath3 bls and @xmath4 loss event types ( lets ) which has to be shared by all the amas .",
    "finally , amas usually identify the capital charge with the @xmath5 @xmath6-year value - at - risk ( var ) , i.e. the @xmath7 percentile of the yearly loss distribution .    among the ama methods ,",
    "the most widely used is the _ loss distribution approach _ ( lda ) . in lda",
    "the distribution of frequency and the distribution of impact ( severity ) modeling the operational losses are separately studied for each of the @xmath8 pairs @xmath9 .",
    "lda makes two crucial assumptions : i ) frequency and severity distributions are independent for each pair ; ii ) the distributions of each pair are independent from the distributions of _ all the other _ pairs .",
    "in other words lda neglects the correlations possibly existing between the frequency or the severity of the losses occurring in different pairs .",
    "the idea of exploiting bns to study or has already been proposed in @xcite , and various approaches are possible .",
    "the main advantages offered by bns are two :    * the possible correlations among different bank processes can be captured ; * the information contained into both assessments and historical loss data can be merged in a natural way .",
    "one approach may be to design a completely different network for each bank process , trying to determine the relevant variables ( in the context of each process ) and the causal relationship among them ; this kind of network has only one output node which typically represents the loss distribution for the process under investigation .",
    "this approach has several drawbacks : i ) domain experts are needed for each process , in order to properly identify the variables and to define the topology of each network ; ii ) if the historical data needs to be used , a system monitoring all the included variables with an acceptable frequency and accuracy has to be built ; since this kind of network can easily reach large sizes ( tens of variables ) , managing such systems is quite challenging for a bank institution ; iii ) correlations across different processes are not taken into account .",
    "another approach @xcite is to design a unique network composed by a node for each process which represents its loss distribution ; all nodes are output nodes and the operational losses are sufficient to build a historical database , so that collecting the data and managing them is much more easier for a bank ; in comparison with the previous approach even the experts  task becomes simpler since their assessment reduces to an estimate of the losses over a certain time horizon ; obviously this kind of network is specifically designed for capturing the correlations among different processes .",
    "this approach resembles a way of reasoning typical of the field of the complex systems : all the `` microscopic '' details inherent to each process ( that make the basis on which the first approach is built on ) are not included in the model , assuming that they can be neglected to a certain extent .",
    "let us underline that , as regards the practical implementation inside a bank , the difference between the two approaches is huge : in the first approach tens of variables for each process need to be monitored , while in the second approach only one variable per process ( the registered losses ) has to ; considering that an ama - oriented bank has to track its own internal losses in any case , the cost of the proposed implementation is minimum .",
    "mixed approaches in which a subnetwork of the kind used in the first approach ( but usually smaller ) is nested into each node representing the loss distribution of a process are even possible .",
    "in order to define a bayesian network @xcite two elements are necessary : a set of random variables @xmath10 and a network of nodes corresponding to the random variables in @xmath11 .",
    "in particular the network must be a _ directed acyclic graph _",
    "( dag ) and the joint _ probability distribution function _",
    "( pdf ) @xmath12 must satisfy the markov condition , i.e. each random variable @xmath13 and the set of all its non - descendents must be conditionally independent , given the set of all its parents .",
    "it can be proved for discrete variables ( which turns out to be our case ) that the markov condition easily allows to calculate the joint pdf as : @xmath14 where @xmath15 is the set of random variables whose corresponding nodes are parents of the node associated with @xmath13 .",
    "both the directed links appearing in the dag and the values of the conditional probabilities @xmath16 can be learned from a dataset whose records hold the values assumed by each @xmath13 in _ independent _ experiments .",
    "even if we are not dealing here with the problem of a rigorous definition of what independent experiments are , we will be more formal about this point because it is the core of our implementation .",
    "let us associate a random variable to each node , and to each experiment : @xmath17 is the random variable associated with the @xmath18-th node and with the @xmath19-th experiment .",
    "the @xmath19-th and the @xmath20-th experiments ( @xmath21 ) are said to be independent if @xmath17 and @xmath22 are independent @xmath23 and @xmath24 .",
    "one of the fundamental reasons to use bns to estimate the or is that if correlations do exist among different processes they can be captured through the network topology ; however the correlation can extend arbitrarily over time : an example will help to clarify .",
    "suppose that an employee violates the transaction control system with fraud purposes : he succeeds in his aim and a money loss is generated in some process of the bank .",
    "as a side effect a part of the it infrastructure is damaged , but the failure is discovered and repaired only a week later : a loss is generated in the process of machinery servicing with a one week lag . at the same time the system remained partially unavailable and a certain amount of transactions failed , eventually generating losses delayed up to a week in many other processes .    in order to understand the importance of this point we need to look at the structure of a database of historical losses : each record holds the daily losses classified by the process in which they occur .",
    "the example should have made it obvious that the losses registered in different days can not be considered originating from independent experiments ( as defined in section [ sec : bayesian networks ] ) , so a database with this structure is in principle useless for learning purposes . to overcome this limitation",
    "we propose a new approach : the losses are averaged over a certain time interval @xmath25 such that the correlations of the _ averaged _ losses vanish at different times , but are still present at the _ same _ time .",
    "in such an approach the original database is replaced by a new database ( which will be called the _ extracted database _ ) of averaged losses whose number of records is @xmath26 , being @xmath27 the number of records into the original database .",
    "suppose e.g. that @xmath28 is one of the time intervals we are looking for and @xmath27 equals to @xmath6 year : this means that the _ average _ losses of a quarter of year are not correlated with the _ average _ losses of _ another _ quarter , but the _ average _ losses recorded by different processes in the _ same _ quarter are still correlated among themselves ; different quarters may be considered independent experiments , thus the extracted database can be used for learning purposes .",
    "in section [ sec : different - times correlations ] the idea of averaging the losses over a certain time interval is introduced .",
    "what we actually do is to _ sum _ all the losses belonging to the same process and the same time interval : the @xmath29-th record in the extracted database contains the aggregate loss of the records from @xmath30 to @xmath31 , obviously retaining the process classification .",
    "let us suppose again that @xmath28 : the first ( second ,  ) record in the extracted database contains the aggregate loss of the records from @xmath6 to @xmath32 ( @xmath33 to @xmath34 ,  ) in the original database .",
    "summing is equivalent to averaging but , as we are going to see , makes much more sense in view of the var calculation .",
    "after the new database has been extracted , we can start building the network defining the nodes and the allowed states of the associated variables : we set the number of states @xmath35 to @xmath36 for all the variables ; the bins are equally spaced , being @xmath37 the lower limit and the maximum _ average _ loss of each process the higher limit .    the extracted database is then used to learn the structure of the network and the conditional probabilities . as hinted in section [ sec : introduction ] , another reason why bns seems to be suitable for or estimation is that they allow integrating of the information coming from the historical database with the information coming from experts  assessment .",
    "topology constraints can be imposed before the structure learning is performed , while _ a prior _ knowledge can be embedded properly setting the marginal distributions of each variable before the conditional probability learning is performed .",
    "however , we are mainly interested in studying the correlations of the losses and thus we choose neither to impose topology constraints , nor to embed any prior knowledge about the marginal distributions of the variables .",
    "the joint pdf can then be derived using and the marginal pdf for each variable calculated .",
    "we recall here that the database entries are values assumed by the random variables associated with the nodes ( see section [ sec : bayesian networks ] ) : if the database used for the learning procedure contains the cumulative losses of a quarter ( classified by process ) the marginal pdfs obtained as the output of the bn will be the loss distribution per quarter ( classified by process ) .",
    "let us note that , provided that @xmath28 is such that the different - times correlations vanish , it is reasonable to consider the loss distributions relative to _ different _ quarters to be independent . making the further assumption that the loss distributions per quarter are the same for each quarter it is possible to calculate the loss distributions over every time horizon , by numerically convoluting the loss distributions over the time horizon @xmath25 an appropriate number of times .",
    "indeed , in order to compare the results obtained for different values of @xmath25 , we calculate the loss distributions and the var over a fixed time horizon : for this purpose @xmath27 seems the most natural time horizon to fix .",
    "let @xmath38 be the loss distribution of the @xmath18-th process over the time horizon @xmath25 and @xmath39 the value of @xmath38 in the @xmath29-th bin ; the convolution of @xmath38 by itself is defined by : @xmath40 with @xmath41 in our case . to obtain @xmath42",
    "( i.e. the loss distribution of the @xmath18-th process over the time horizon @xmath27 ) @xmath38 has to be convoluted by itself a number of times equal to the closest integer to @xmath26 : @xmath43    the var over the time horizon @xmath27 for each process is the @xmath7 percentile of the convoluted loss distribution and the total var is simply the sum of the vars of the single processes .",
    "the @xmath7 percentile of the convoluted distribution ( for each process ) can be numerically determined in the following way : the convoluted distribution is sampled @xmath44 times and the sample is arranged in increasing order : the second largest value is the @xmath7 percentile of the convoluted distribution .",
    "since this procedure involves sampling , it is repeated several times and the var is calculated as the mean of the obtained @xmath7 percentiles .",
    "as hinted before , the var may be calculated over every desired time horizon tuning the number of convolutions ; in particular the time horizon can be set to @xmath6 year , as required by basel ii , performing @xmath45 convolutions .",
    "in order to investigate our approach , we developed a reliable and tunable database of synthetic internal losses : in this way we are able to control the correlations between the different processes and some inherent features of each process .",
    "we consider the historical losses of each process as a time series and , inspired by @xcite , generalize a stochastic algorithm for generating multiple time series .",
    "we point out that this procedure allows to impose , at least in principle , arbitrary cross - correlation functions between each pair of generated time series , as well as the auto - correlation function and distribution for each generated time series .",
    "the steps of the algorithm are the followings : i ) for each process , @xmath27 values are drawn from an arbitrary distribution ; the order in which the values are extracted is considered to be a temporal order , so let us call the extracte values @xmath46 , where the subscript @xmath47 indexes the process and the argument @xmath48 defines the temporal ordering .",
    "ii ) the following quantity is calculated : @xmath49 ^ 2 } , \\ ] ] where @xmath50 is the number of processes , @xmath51 are the imposed cross - correlation ( or auto - correlation ) functions , while @xmath52 are the cross - correlation ( or auto - correlation if @xmath53 ) functions calculated from the generated data : @xmath54,\\ ] ] with @xmath55 and @xmath56 . from it",
    "follows that @xmath57 : in other words , because of its normalization , @xmath52 carries no information about the same - time correlations ; in order to make the whole procedure consistent @xmath58 must also be equal to @xmath6 : this explains why the summation over @xmath59 in starts from @xmath6 and not from @xmath37 .",
    "iii ) two values belonging to a randomly selected series are randomly chosen and exchanged , and the quantity is recalculated .",
    "iv ) if has decreased the exchange between the two values performed in the step ( iii ) is accepted , otherwise it is rejected . as @xmath52",
    "are not limited , can not be normalized and thus a threshold below which the algorithm is halted can not be set .",
    "we rather choose to iterate the algorithm until reaches a plateaux .    since we are interested in the change of the correlation between different processes with respect to the time interval @xmath25 over which the losses are averaged , we imposed auto - correlation and cross - correlation functions of the form : @xmath60 in fact making such a choice implies that the different - times correlation between the processes @xmath18 and @xmath24 should be significantly reduced averaging over a time interval @xmath61 .",
    "even though the algorithm allows to impose both distributions and @xmath51 , in practice a certain degree of compatibility may exist between them : this means that , even if reaches a plateaux , still @xmath52 and @xmath51 are significantly different . in order to overcome this limitation the algorithm",
    "is slightly modified in the following way : we generate series which are indeed longer than @xmath27 so that a larger basin of values that may fulfill the imposed constraints is available ; e.g. suppose that the values of the series are drawn from a uniform distribution and that the imposed @xmath51 have an higher degree of compatibility with another distribution : a subset of values belonging to this distribution will be selected by the algorithm .",
    "the modified algorithm obviously alters the imposed distributions ; however we see no reasons to impose strict constraints on the distributions and , on the other hand , as we are interested in studying the correlations between the processes , need a high accuracy in reproducing the @xmath51 .",
    "we investigate a sort of _ toy model _ whose number of processes is limited to @xmath62 ; this choice is the result of a trade - off between our need to considering a system complex enough to have a reasonable number of correlated processes and the convenience of using series longer enough to be able to carry out the average over time and still have a sufficient number of data to perform the learning of the network . with @xmath63",
    "it is possible to average over @xmath64 steps and still have @xmath65 patterns left for the learning .",
    "the negative exponential distribution has shown to be compatible with if the decay matrix is homogeneous , i.e. @xmath66 and @xmath24 and if @xmath67 is not too large . in the top panel of fig.[fig : corr_functions ] both @xmath52 and @xmath51 are shown for @xmath68 . in order to simulate different kinds of processes their means",
    "have been set respectively at @xmath69 , @xmath70 and @xmath71 . using a larger basin of values as described in section [ sec : synthetic data ] both the mean and the variance of the distributions do not significantly change , but a heavier tail appears .    as it is shown in the bottom panel of fig.[fig : corr_functions ] , averaging over a time interval @xmath25 leaves the form unchanged with a new decay time equal to @xmath72 .",
    "this actually means that , at the cost of reducing the length of the time series , averaging effectively removes the different - times correlations : in particular when @xmath73 ( @xmath74 ) all the different - times correlations are reduced to @xmath75 and for @xmath76 they can be considered effectively extinguished .",
    "since @xmath51 carry no information about the same - time correlations ( see section [ sec : synthetic data ] ) , in order to study them we look at the learned structure of the networks : in tab.[tab : nets ] it is shown that the number of links decreases as @xmath25 increases .",
    "this is somewhat expected since , as @xmath25 increases , the size of the extracted database reduces and it becomes more and more difficult to learn from it .",
    "however for @xmath76 the algorithm of structure learning still detects the presence of some links : since the different - times correlations are extinguished for such a large @xmath25 , they must be due to the survived same - time correlations .    to evaluate the consistency of the whole procedure we require that , for values of @xmath25 such that the different - times correlations can be neglected , the value of var does not depend on @xmath25 .    in fig.[fig :",
    "var ] the values of var with respect to @xmath25 are represented ; each point is the mean over @xmath77 realizations of the procedure described in section [ sec : learning bns by aggregate losses ] and the standard deviations are also shown . indeed from fig.[fig : var ] it can be seen that for @xmath78 the values of var are compatible among themselves . on the other hand , for @xmath79",
    "the different - times correlations are still present and the records belonging to the extracted database can not be considered independent ; nevertheless the learning algorithm for bns considers them to be independent ( see section [ sec : bayesian networks ] ) and returns unreliable loss distributions : the corresponding var values are consequently also unreliable .",
    "a novel approach , based on bayesian networks , has been proposed for the quantitative management of operational risk in the framework of the new basel capital accord .",
    "the principal features of the proposed approach are the following : 1 ) the whole topology of the network is derived from data of operational losses ; each node in the network corresponds to a bank process and the links between the nodes , which are drawn learning from data , model the causal relationships between the processes ; this scheme seems more flexible than the classification in @xmath8 pairs @xmath9 prescribed by basel ii and has the advantage of representing both the units that generate operational losses and the relationships between them .",
    "2 ) for the first time a bayesian network is used to represent the influence between correlated operational losses that take place in different days exploiting a dataset whose records represent losses occurred over @xmath25 days : using such a dataset the nodes in the network represent the aggregate loss over @xmath25 and the var over a time horizon @xmath25 can be computed . the extension to the var over the time horizon @xmath27 requires an additional assumption ( see section [ sec : learning bns by aggregate losses ] ) and is performed by convoluting the probability density functions @xmath26 times and extracting the @xmath7 percentile of the convoluted distribution .",
    "one of us ( m. bardoscia ) would like to thank m.v .",
    "carlucci for the countless suggestions and useful disscussions ."
  ],
  "abstract_text": [
    "<S> a system for operational risk management based on the computational paradigm of bayesian networks is presented . </S>",
    "<S> the algorithm allows the construction of a bayesian network targeted for each bank using only internal loss data , and takes into account in a simple and realistic way the correlations among different processes of the bank . </S>",
    "<S> the internal losses are averaged over a variable time horizon , so that the correlations at different times are removed , while the correlations at the same time are kept : the averaged losses are thus suitable to perform the learning of the network topology and parameters . </S>",
    "<S> the algorithm has been validated on synthetic time series . </S>",
    "<S> it should be stressed that the practical implementation of the proposed algorithm has a small impact on the organizational structure of a bank and requires an investment in human resources limited to the computational area .    operational risk , complex systems , bayesian networks , time series , value - at - risk    89.65.gh , 05.45.tp    91b30 , 91b84 , 37m10 , 62m10 </S>"
  ]
}