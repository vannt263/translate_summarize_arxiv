{
  "article_text": [
    "finite - state methods for statistical prediction of word sequences in natural language have had an important role in language processing research since the pioneering investigations of markov and shannon .",
    "it is clear that natural texts are not markov processes of any finite order @xcite , because of very long range correlations between words in a text such as arise from subject matter .",
    "nevertheless , low - order alphabetic @xmath0-gram models have been used effectively in tasks such as statistical language identification , spelling correction and handwriting transcription , and low - order word @xmath0-gram models have been the tool of choice for language modeling in speech recognition .",
    "the main problem with such fixed - order models is that they can not capture even relatively local dependencies that exceed model order , for instance those created by long but frequent compound names or technical terms . on the other hand , extending model order uniformly to accommodate those longer dependencies",
    "is not practical , since model size grows rapidly with model order .",
    "several methods have been proposed recently @xcite to model longer - range regularities over small alphabets while avoiding the size explosion caused by model order . in those models ,",
    "the length of contexts used to predict particular symbols is adaptively extended as long as the extension improves prediction above a given threshold .",
    "the key ingredient of the model construction is the _ prediction suffix tree _ ( pst ) , whose nodes represent suffixes of past input and specify a distribution over possible successors of the suffix .",
    "_   showed that under realistic conditions a pst is equivalent to a markov process of variable order and can be represented efficiently by a probabilistic finite - state automaton . in this paper",
    "we use psts as our starting point .",
    "the problem of sequence prediction appears more difficult when the sequence elements are _ words _ rather than characters from a small fixed alphabet .",
    "the set of words is in principle unbounded , since in natural language there is always a nonzero probability of encountering a word never seen before .",
    "one of the goals of this work is to describe algorithmic and data - structure changes that support the construction of psts over unbounded vocabularies .",
    "we also extend psts with a wildcard symbol that can match against any input word , thus allowing the model to capture statistical dependencies between words separated by a fixed number of irrelevant words .",
    "the main contribution of this paper is to show how to build models based on _ mixtures _ of psts .",
    "we use two results from machine learning and information theory .",
    "the first is that a mixture of an ensemble of experts ( models ) with suitably selected weights performs better than almost any individual member of the ensemble @xcite .",
    "the second result is that within a bayesian framework the sum over exponentially many trees can be computed efficiently using the recursive structure of the tree , as was recently shown by willems et al . .",
    "our experiments with algorithms based on those theoretical results show that a pst mixture , which can be computed almost as easily as a single pst , performs better than the maximum a posteriori ( map ) pst .",
    "an important feature of pst mixtures is that they can be built by a fully online ( adaptive ) algorithm .",
    "specifically , updates to the model structure and statistical quantities can be performed incrementally during a single pass over the training data . for each new word ,",
    "frequency counts , mixture weights and likelihood values associated with each relevant node are appropriately updated .",
    "there is not much difference in learning performance between the online and batch modes , as we will see .",
    "the online mode seems much more suitable for adaptive language modeling over longer test corpora , for instance in dictation or translation , while the batch algorithm can be used in the traditional manner of @xmath0-gram models in sentence recognition .",
    "two sets of priors are used in our bayesian model .",
    "the first set defines recursively the prior probability distribution over _ all _ possible psts .",
    "the second set , which is especially delicate because the set of possible words is not fixed , determines the probability of observing a word for the first time in a given context .",
    "this includes two possibilities : a completely new word , and a word previously observed but not in the present context .",
    "we assign these priors using a simplification of the good - turing method previously used in compression algorithms .",
    "it turns out that prediction performance is not too sensitive to particular choices of priors .    our successful application of mixture psts for word - sequence prediction and modeling make them worth considering in applications like speech recognition or machine translation if online adaptation of an existing model to new material is required",
    "of course , these techniques still fail to represent subtler aspects of syntactic and semantic information .",
    "we plan to investigate how the present work may be refined by taking advantage of distributional models of semantic relations @xcite .    in the next sections we present psts and the data structure for the word prediction problem .",
    "we then describe and shortly analyze the learning algorithm .",
    "we also discuss several implementation issues .",
    "we conclude with a evaluation of various aspects of the model on several english corpora .",
    "our models operate on a set @xmath1 of _ possible words _ over an alphabet @xmath2 .",
    "since @xmath3 is intended to represent the set of words of a natural language , we do not assume that we know it in advance .",
    "a _ prediction suffix tree _ ( pst ) @xmath4 over @xmath3 is a finite tree with nodes labeled by distinct elements of @xmath5 such that the root is labeled by the empty sequence @xmath6 , and if @xmath7 is a son of @xmath8 and @xmath8 is labeled by @xmath9 then @xmath7 is labeled by @xmath10 for some @xmath11 . therefore , in practice it is enough to associate each non - root node with the first word in its label , and the full label of any node can be reconstructed by following the path from the node to the root . in what follows",
    ", we will often identify a pst node with its label .",
    "each pst node @xmath7 is has a corresponding _ prediction function _",
    "@xmath12 $ ] , where @xmath13 . the symbol @xmath14 represents a _ novel _ event , that is the occurrence of a word not seen before in the context represented by @xmath7 .",
    "the value of @xmath15 is the _ next - word probability _ function for the given context @xmath7 .",
    "a pst @xmath4 can be used to generate a stream of words , or to compute prefix probabilities over a given stream .",
    "given a prefix @xmath16 generated so far , the context ( node ) used for prediction is found by starting from the root of the tree and taking branches corresponding to @xmath17 until a leaf is reached or the next son does not exist in the tree . consider for example the pst shown in figure  [ pst : fig ] , where some of the values of @xmath15 are : @xmath18 when observing the text ` ... long ago and the first ' , the matching path from the root ends at the node ` and the first ' .",
    "then we predict that the next word is time with probability @xmath19 and some other word not seen in this context with probability @xmath20 .",
    "the prediction probability distribution @xmath15 is estimated from empirical counts .",
    "therefore , at each node we keep a data structure representing the number of times each word appeared in the corresponding context .",
    "the _ wildcard symbol _ ` ' allows a particular word position to be ignored in prediction .",
    "for example , the text ` ... but this was ' is matched by the node label ` this * ' , which ignores the most recently read word ` was ' .",
    "wildcards allow us to model conditional dependencies of general form @xmath21 in which the indices @xmath22 are not necessarily consecutive .",
    "wildcards provide a useful capability in language modeling since syntactic structure may make a word depend less on the immediately preceding words than on words further back .",
    "one can easily verify that every standard @xmath0-gram model can be represented by a pst , but the opposite is not true .",
    "a trigram model , for instance , is a pst of depth two , where the leaves are all the observed bigrams of words .",
    "the prediction function at each node is the trigram conditional probability of observing a word given the two preceding words .",
    "within the framework of online learning , it can be proved @xcite and demonstrated experimentally that the performance of a weighted ensemble of models in which each model is weighted according to its performance ( the posterior probability of the model ) , is not worse and generally much better than any single model in the ensemble .",
    "although there might be exponentially many different psts in the ensemble , it has been recently shown @xcite that a mixture of psts can be efficiently represented for small alphabets .",
    "we will use here bayesian formalism to derive an _ online _",
    "learning procedure for mixtures of psts of words .",
    "the mixture elements are drawn from some pre - specified set @xmath23 , which in our case is typically the set of all psts with maximal depth @xmath24 for some suitably chosen @xmath25 .    in what follows",
    ", we will consider a fixed input sequence @xmath26 . to deal with boundary conditions",
    ", we will assume that the sequence is padded on the left with enough `` start - of - sequence '' symbols .",
    "we will denote by @xmath27 the input subsequence @xmath28 and by @xmath29 the prefix @xmath30 ( with appropriate initial padding ) . by convention @xmath31 if @xmath32 . for any pst @xmath4 and any sequence @xmath7 , we will denote by @xmath33 longest suffix of @xmath7 that has a corresponding node in @xmath4 , and , through our identification of pst nodes and sequences , the node itself . then @xmath4 s likelihood ( or evidence ) @xmath34 after observing @xmath35 is @xmath36 the probability of the next word given the past @xmath0 observations is then : @xmath37 where @xmath38 is the prior probability of the pst @xmath4 .",
    "a nave computation of ( [ bayes : eqn ] ) would be infeasible , because of the size of @xmath23 .",
    "instead , we use a recursive method in which the relevant quantities for a pst mixture are computed efficiently from related quantities for sub - psts .",
    "in particular , the pst prior @xmath38 is defined as follows .",
    "a node @xmath7 has a probability @xmath39 of being a leaf and a probability @xmath40 of being an internal node . in the latter case ,",
    "its sons are either a single wildcard , with probability @xmath41 , or actual words with probability @xmath42 . to keep the derivation simple ,",
    "we assume here that the probabilities @xmath39 are independent of @xmath7 and that there are no wildcards , that is , @xmath43 for all @xmath7 .",
    "context - dependent priors and trees with wildcards can be obtained by a simple extension of the present derivation .",
    "we also assume that all the trees have maximal depth @xmath25 .",
    "then @xmath44 , where @xmath45 is the number of leaves of @xmath4 of depth less than @xmath25 and @xmath46 is the number of internal nodes of @xmath4 .    to evaluate the likelihood of the whole mixture we build a tree of maximal depth @xmath25 containing all observation - sequence suffixes of length up to @xmath25 .",
    "the tree built after observing @xmath35 contains a node for each subsequence @xmath47 for @xmath48 and @xmath49 . for each such node",
    "we keep two variables .",
    "the first , @xmath50 , accumulates the likelihood the node would have if it were a leaf .",
    "that is , @xmath50 is the product of the predictions of the node on all the observation - sequence suffixes that ended at that node : @xmath51 for each new observed word @xmath52 , the likelihood values @xmath53 are derived from their previous values @xmath54 . clearly , only the nodes labeled by @xmath55 will need likelihood updates",
    ". for those nodes , the update is simply multiplication by the node s prediction for @xmath56 , while for the rest of the nodes the likelihood values do not change : @xmath57    the second variable , denoted by @xmath58 , is the likelihood of the mixture of _ all _ possible trees that have a subtree rooted at @xmath7 on the observed suffixes ( all observations that reached @xmath7 ) .",
    "@xmath58 is calculated recursively as follows : @xmath59 the recursive computation of the mixture likelihood terminates at the leaves : @xmath60 the mixture likelihood values are updated as follows : @xmath61 at first sight it would appear that the update of @xmath62 would require contributions from an arbitrarily large subtree , since @xmath3 may be arbitrarily large .",
    "however , only the subtree rooted at @xmath63 is actually affected by the update . thus the following simplification holds : @xmath64    note that @xmath58 is the likelihood of the weighted mixture of trees rooted at @xmath7 on _ all _ past @xmath0 observations , where each tree in the mixture is weighted with the appropriate prior .",
    "therefore @xmath65 where @xmath66 is the set of trees of maximal depth @xmath25 and @xmath6 is the null context ( the root node ) . combining equations ( [ bayes : eqn ] ) and ( [ mix : eqn ] ) , we see that the prediction of the whole mixture for next word @xmath56 is the ratio of the likelihood values @xmath67 and @xmath68 at the root node : @xmath69    a given observation sequence matches a unique path from the root to a leaf .",
    "therefore the time for the above computation is linear in the maximal tree depth @xmath25 . after predicting the next word the counts are updated simply by increasing by one the count of the word , if the word already exists , or by inserting a new entry for the new word with initial count set to one .",
    "our learning algorithm has , however , the advantages of not being limited to a constant context length ( by setting @xmath25 to be arbitrarily large ) and of being able to perform online adaptation .",
    "moreover , the interpolation weights between the different prediction contexts are automatically determined by the performance of each model on past observations .    in summary",
    ", for each observed word we follow a path from the root of the tree corresponding to the previous words until a longest context ( maximal depth ) is reached .",
    "we may need to add new nodes , with new entries in the data structure , for the first appearance of a word in a given context .",
    "the likelihood values of the mixture of subtrees ( equation [ mix_update : eqn ] ) are returned from each level of that recursion up to the root node .",
    "the probability of the next word is then the ratio of two consecutive likelihood values returned at the root .    for prediction without adaptation ,",
    "the same method is applied except that nodes are not added and counts are not updated . if the prior probability of the wildcard , @xmath70 , is positive , then at each level the recursion splits , with one path continuing through the node labeled with the wildcard and the other through the node corresponding to the proper suffix of the observation .",
    "thus , the update or prediction time is in that case @xmath71 .",
    "however , judicious use of pruning can make the effective depth of the tree fairly small , making update and prediction times linear in the text length .",
    "it remains to describe how the probabilities , @xmath72 are estimated from empirical counts .",
    "this problem has been studied for more than thirty years and so far the most common techniques are based on variants of the good - turing ( gt ) method @xcite . here",
    "we give a description of the estimation method that we implemented and evaluated .",
    "we are currently developing an alternative approach for cases when there is a known ( arbitrarily large ) bound on the maximal size of the vocabulary @xmath3 . after observing a certain training text ,",
    "let @xmath73 be the number of occurrences of word @xmath74 in context @xmath7 , @xmath75 be the set of words with @xmath76 , @xmath77 be the number of distinct words observed exactly @xmath0 times in context @xmath7 , @xmath78 be the number of distinct words observed following @xmath7 , and @xmath79 the total number of occurrences of @xmath7 .",
    "we seek estimates of @xmath80 for @xmath81 and of the probability @xmath82 observing after @xmath7 a word not in @xmath75 when scanning new text .",
    "the gt method sets @xmath83 .",
    "this method has been given several justifications , such as a poisson assumption on the appearance of new words @xcite .",
    "it is , however , difficult to analyze and requires keeping track of the rank of each word .",
    "our online learning method and data structures favor instead any method that is based only on word counts . in source coding",
    "it is common to set the probability of a novel event in context @xmath7 to @xmath84 , and allocate the remaining probability mass to words in @xmath75 according to @xmath85 .",
    "witten and bell report that this method performs comparably to gt but is simpler because it needs to keep track of the number of distinct words and their counts only .",
    "we also need to distinguish between occurrences of completely new words , that never occurred before in any context , and words that have occurred before in other contexts but not the present one .",
    "a coding interpretation helps us understand the issue .",
    "suppose text is being compressed according to the word probability estimates given by a pst .",
    "whenever a completely new word is to be encoded , we need to signal first that a new word occurred and then encode the identity of the word , for instance with a lower level coder based on a pst over the base alphabet @xmath2 in which the words in @xmath3 are written . for a word that has been observed before but not in the current context",
    ", it is only necessary to code the identity of the word by referring to a shorter context in which the word has already been observed ( obviously such a shortest context always exists ) multiplying the probability of the word in the shorter context by the probability that the word is new in the longer context .",
    "this product is the probability of the word in the next context .    in the case of a completely new word @xmath52",
    ", the probability of a novel event must be multiplied by @xmath86 corresponding to the probability of @xmath52 according to the model in the lower - level coder .",
    "the assumption that an independent lower - level coder is used for completely new words implies that @xmath86 is independent of the context .",
    "we can thus factor @xmath86 from @xmath87 for @xmath88 . in particular",
    ", @xmath86 will cancel out when calculating the probability of the following word @xmath56 : @xmath89 therefore , for prediction we do not in fact need to use the lower - level coder or estimate @xmath86 .",
    "we now describe refinements of the data structure and algorithm described in the previous section to support very large vocabularies and large training sets .",
    "the likelihood values @xmath58 and @xmath50 decrease exponentially with @xmath0 , causing numerical problems even if logarithmic representations are used .",
    "moreover , we are only interested in the predictions of the mixtures , not in the actual likelihoods , which are only used to weigh the predictions of different nodes .",
    "let @xmath90 be the prediction of the weighted mixture of all subtrees rooted below @xmath7 ( including @xmath7 itself ) for @xmath52 . by following the derivation presented in the preceding section",
    "it can be verified that @xmath91 where @xmath92 define @xmath93 setting @xmath94 for all @xmath7 , @xmath95 is updated as follows : @xmath96 and @xmath97 .",
    "thus , the probability of @xmath56 is propagated along the path corresponding to suffixes of the observation sequence towards the root as follows @xmath98    finally , the prediction of the full mixture of psts for @xmath52 is simply given by @xmath99 .",
    "for each node , we need frequency counts for the corresponding context and for each word that occurred in that context .",
    "however , we can avoid storing the word - in - context counts explicitly . to find how many times word @xmath74 occurred in context @xmath100 we search the tree for @xmath101 .",
    "the number of times this node was visited is also the number of occurrences of word @xmath74 in context @xmath7 .",
    "therefore , each node is both used for predicting the next word using a mixture of contexts of different lengths and for tracking the number of times a given context has occurred . for a pst mixture of bounded depth @xmath25 ,",
    "we thus need to maintain a tree of depth @xmath102 .",
    "the leaves of this tree are used for storing frequency counts for word sequences of length @xmath102 , while the internal nodes have the two roles described above . in summary , for each node @xmath7 we maintain the count @xmath103 of how many times the corresponding context has been observed .",
    "in addition , for internal nodes we keep also the node s log - likelihood ratio @xmath95 described earlier , the number @xmath104 of distinct words seen in context @xmath7 ( the species count for the context ) , and a list of the node s sons , that is , of contexts longer by one word . when observing a word @xmath105 ,",
    "the online algorithm performs two traversals of the tree , one to update the counts for the nodes @xmath106 , @xmath107 , and the other to retrieve and update the log - likelihood ratio @xmath95 and the species count @xmath108 for @xmath109 , @xmath48 .",
    "natural language is often bursty @xcite , that is , rare or new words may appear and be used relatively frequently for some stretch of text only to drop to a much lower frequency of use for the rest of the corpus .",
    "thus , a pst being build online may only need to store information about those words for a short period .",
    "it may then be advantageous to prune pst nodes and remove small counts corresponding to rarely used words .",
    "pruning is performed by removing all nodes from the suffix tree whose counts are below a threshold , after each batch of @xmath110 observations .",
    "we used a pruning frequency @xmath110 of @xmath111 and a pruning threshold of 2 in some of our experiments .",
    "[ on_line : tab ]    .the perplexity of psts for the online mode . [ cols=\"^,^,^,^,^,^ \" , ]     [",
    "nab : tab ]    pruning during online adaptation has two advantages .",
    "first , it improves memory use .",
    "second , and less obvious , predictive power may be improved .",
    "rare words tend to bias the prediction functions at nodes with small counts , especially if their appearance is restricted to a small portion of the text .",
    "when rare words are removed from the suffix tree , the estimates of the prediction probabilities at each node are readjusted to reflect better the probability estimates of the more frequent words .",
    "hence , part of the bias in the estimation may be overcome .",
    "to support fast insertions , searches and deletions of pst nodes and word counts we use a hybrid data structure .",
    "when we know in advance a ( large ) bound on vocabulary size , we represent the root node by arrays of word counts and possible sons subscripted by word indices . at other nodes ,",
    "we use _ splay _ trees @xcite to store the branches to longer contexts .",
    "splay trees support search , insertion and deletion in amortized @xmath112 time per operation .",
    "furthermore , they reorganize themselves to decrease the cost of accessing to the most frequently accessed elements , thus speeding up access to subtrees associated to more frequent contexts .",
    "we tested our algorithm in two modes . in online mode ,",
    "model structure and parameters ( counts ) are updated after each observation . in batch mode ,",
    "the structure and parameters are frozen after the training phase , making it easier to make comparisons with standard @xmath0-gram models . for our smaller experiments we used the brown corpus , the gutenberg bible , and milton s paradise lost for training and test material . for comparisons with @xmath0-gram models ,",
    "we used arpa s north - american business news ( nab ) corpus .    for batch training , we partitioned randomly the data into training and testing sets .",
    "we then trained a model by running the online algorithm on the training set .",
    "the resulting model was then frozen and used to predict the test data .    to illustrate the predictive power of the model",
    ", we used it to generate text by performing random walks over the mixture pst .",
    "a single step of the random walk was performed by going down the tree following the current context and stop at a node with the probability assigned by the algorithm to that node .",
    "once a node is chosen , a word is picked randomly by the node s prediction function .",
    "a result of such a random walk is given in figure  [ walk : fig ] .",
    "the pst was trained on the brown corpus with maximal depth of five .",
    "the output contains several well formed ( meaningless ) clauses and also cliches such as `` conserving our rich natural heritage , '' suggesting that the model captured some longer - term statistical dependencies .    in online mode the advantage of psts with large maximal depth",
    "the perplexity of the model decreases significantly as a function of the depth .",
    "our experiments so far suggest that the resulting models are fairly insensitive to the choice of the prior probability , @xmath113 , and a prior which favors deep trees performed well .",
    "table  1 summarizes the results on different texts , for trees of growing maximal depth .",
    "note that a maximal depth @xmath114 corresponds to a ` bag of words ' model ( zero order ) , @xmath115 to a bigram model , and @xmath116 to a trigram model .    in our first batch tests we trained the model on 15% of the data and tested it on the rest .",
    "the results are summarized in table  2 .",
    "the perplexity obtained in batch mode is clearly higher than that for online prediction , since only a small portion of the data was used to train the models . yet , even in this case the pst of maximal depth three is significantly better than a full trigram model . in this mode",
    "we also checked the performance of the single map model compared to the mixture of psts .",
    "this model is found by pruning the tree at the nodes that obtained the highest confidence value , @xmath50 , and using only the leaves for prediction .",
    "as shown in the table , the performance of the map model is consistently worse than the performance of the mixture .    to illustrate the use of the model in language processing",
    ", we applied it to correcting errors in corrupted text . this situation models transcription errors in dictionary - based speech and handwriting recognition systems , for example . in such systems a language model selects the most likely alternative between the several options proposed by previous stages .",
    "here we used a pst with maximal depth @xmath117 , trained on @xmath118 of the text of _ paradise lost_. several sentences in the held - out test data were corrupted in different ways .",
    "we then used the model in batch mode to evaluate the likelihood of each of the alternatives . in table",
    "3 we demonstrate one such case , where the first alternative is the correct one . the negative @xmath119 likelihood and the posterior probability ,",
    "assuming that the listed sentences are all the possible alternatives , are provided .",
    "the correct sentence gets the highest probability according to the model .",
    "finally , we trained pst mixtures of varying depths on randomly selected sentences from the nab corpus totaling approximately 32.5 million words and tested it on two corpora : ( 1 ) a standard arpa nab development test set of around 8 thousand words , and ( 2 ) a separate randomly selected set of sentences from the nab corpus , totaling around 2.8 million words .",
    "the results are summarized in table  4 , and compared with a backoff trigram model @xcite .",
    "psts are able to capture longer correlations than traditional fixed order @xmath0-grams , supporting better generalization ability from limited training data .",
    "this is especially noticeable when phrases longer than a typical @xmath0-gram order appear repeatedly in the text .",
    "the pst learning algorithm allocates a proper node for the phrase whereas a bigram or trigram model captures only a truncated version of the statistical dependencies among words in the phrase .",
    "our current learning algorithm is able to handle moderate size corpora , but we hope to adapt it to work with very large training corpora ( 100s of millions of words ) .",
    "the main obstacle to those applications is the space required for the pst .",
    "more extensive pruning may be useful for such large training sets , but the most promising approach may involve a batch training algorithm that builds a compressed representation of the final pst from an efficient representation , such as a suffix array , of the relevant subsequences of the training corpus .            r.  a.  fisher , a.  s.  corbet , c.  b.  williams",
    "the relation between the number of species and the number of individuals in a random sample of an animal population .",
    "j.  animal ecology , vol .  12 , pp .  4258 .",
    "f.  pereira , n.  tishby , and l.  lee .",
    "1993 . distributional clustering of english words . in _",
    "30th annual meeting of the association for computational linguistics _ , pages 183190 , columbus , ohio .",
    "ohio state university , association for computational linguistics , morristown , new jersey ."
  ],
  "abstract_text": [
    "<S> we describe , analyze , and evaluate experimentally a new probabilistic model for word - sequence prediction in natural language based on _ prediction suffix trees _ ( psts ) . by using efficient data structures , </S>",
    "<S> we extend the notion of pst to unbounded vocabularies . </S>",
    "<S> we also show how to use a bayesian approach based on recursive priors over all possible psts to efficiently maintain tree mixtures . </S>",
    "<S> these mixtures have provably and practically better performance than almost any single model . </S>",
    "<S> we evaluate the model on several corpora . </S>",
    "<S> the low perplexity achieved by relatively small pst mixture models suggests that they may be an advantageous alternative , both theoretically and practically , to the widely used @xmath0-gram models . </S>"
  ]
}