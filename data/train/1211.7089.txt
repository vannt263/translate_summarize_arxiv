{
  "article_text": [
    "since the introduction of compressive sensing ( cs ) @xcite , sparse recovery has received much attention and becomes a very hot topic these years @xcite . sparse recovery aims to solve the following underdetermined linear system @xmath0 where @xmath1 denotes the measurement vector , @xmath2 is a sensing matrix with more columns than rows , i.e. , @xmath3 , and @xmath4 is the sparse or compressible signal to be recovered .",
    "many algorithms have been proposed to solve the problem ( [ sparserecovery ] ) .",
    "if @xmath5 is sparse , one typical method is to consider the following @xmath6-minimization problem @xmath7 where the @xmath6 `` norm '' @xmath8 counts the nonzero elements of @xmath5 .",
    "however , it is not practical to adopt this method since it is usually solved by combinatorial search , which is np - hard .",
    "an alternate method @xcite is to replace the @xmath6 `` norm '' with the @xmath9 norm , i.e. , @xmath10 the convex @xmath9-minimization problem ( [ l1opt ] ) is also known as _ basis pursuit _ ( bp ) .",
    "it is certified that under some certain conditions @xcite , the optimal solution of @xmath9-minimization is identical to that of @xmath6-minimization .",
    "this conclusion greatly reduces the computational complexity , since @xmath9-minimization can be reformulated as a linear program ( lp ) , and be solved by numerous efficient algorithms @xcite .",
    "another family of sparse recovery algorithms is put forward based on non - convex optimization @xmath11 where @xmath12 is a sparsity - inducing penalty . the optimization problem ( [ lfopt ] )",
    "is also termed as @xmath13-minimization @xcite .",
    "these algorithms include focal underdetermined system solver ( focuss ) @xcite , iteratively reweighted least squares ( irls ) @xcite , reweighted @xmath9-minimization @xcite , smoothed @xmath6 ( sl0 ) @xcite , difference of convex ( dc ) algorithm @xcite , improved smoothed @xmath6 ( isl0 ) @xcite , and zero - point attracting projection ( zap ) @xcite .",
    "it is theoretically proved @xcite and experimentally verified @xcite that for some certain non - convex penalties , @xmath13-minimization tends to derive the sparse solution under weaker conditions than @xmath9-minimization .",
    "however , the inherent deficiency of multiple local minima in non - convex optimization limits its practical usage , where improper initial criteria might cause the solution trapped into the wrong ones .",
    "the convergence performance of some non - convex sparse recovery algorithms has been studied in literatures .",
    "for example , in @xcite , a local convergence result of irls @xcite for @xmath14-minimization with @xmath15 is established where the convergence is guaranteed in a sufficiently small neighborhood of the sparse signal . whether or not this neighborhood contains the initial solution is not discussed . in @xcite , the majorize - minimize ( mm ) subspace algorithm",
    "is proposed to solve the @xmath16 regularized problem and its convergence performance is also provided . under some certain conditions , it is shown that the generated sequence will converge to a critical point , which is not , however , proved to be the global optimum . in @xcite , the convergence performance of sl0 @xcite",
    "this is done due to the `` local convexity '' of the penalties , and sl0 needs to solve a sequence of optimization problems rather than a single @xmath13-minimization problem to guarantee convergence to the sparse signal .",
    "this paper aims to provide theoretical convergence guarantees of a non - convex approach for sparse recovery from the initial solution to the global optimum .",
    "the question , which naturally appears and mainly motivates this paper , is raised as follows .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ does there exist a computationally tractable _ algorithm that guarantees to find the sparse solution to @xmath13-minimization ?",
    "if yes , in what circumstances does this statement hold ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in this paper , exploiting the concept of _ weak convexity _",
    "@xcite to characterize the non - convexity of the penalties , the mentioned question is replied as follows .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ a computationally tractable non - convex approach is proposed with guarantees that it converges to the sparse solution provided that the non - convexity of the penalty is below a threshold . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    this paper is organized as follows .",
    "section  [ sec_pre ] introduces the preliminaries of this paper , including the projected subgradient method , the concepts of sparseness measure and weak convexity , and some related state of the art researches . in section  [ sec_main ] , the main contributions of this paper , including the non - convex approach for sparse recovery and its performance guarantees , are demonstrated . the theoretical analysis and",
    "some further discussions are provided in section  [ sec_theo ] .",
    "numerical simulations are implemented in section  [ sec_simu ] to verify the theoretical results .",
    "all of the proofs are included in section  [ sec_proof ] , and this paper is concluded in section  [ sec_conc ] .",
    "for constrained convex optimization problem , the projected subgradient method @xcite is an algorithm which is very simple to implement and easy to analyze . specifically , consider the convex optimization @xmath17 where @xmath18 is convex ( and possibly nondifferentiable ) and @xmath19 is a convex set .",
    "denote @xmath20 as the euclidean projection on @xmath21 .",
    "the projected subgradient method is given by @xmath22 where @xmath23 and @xmath24 are the @xmath25th step size and any subgradient of @xmath26 at @xmath27 , respectively .",
    "theoretical analysis @xcite reveals that this method converges to the optimum for some certain types of step size rules , e.g. the step size sequence which is square summable but not summable .    several notable differences between the projected subgradient method and the ordinary projected gradient method @xcite should be pointed out .",
    "first , the projected subgradient method applies directly to nondifferentiable convex functions while the latter does nt .",
    "second , the function value of the solution sequence can increase in the projected subgradient method .",
    "therefore , the key quantity is the euclidean distance to the optimum instead of the function value . in addition",
    ", the projected subgradient method adopts step size sequence fixed in advance rather than an exact or approximate line search as in the projected gradient method .    for non - convex @xmath13-minimization problem ( [ lfopt ] ) , the projected subgradient method is no longer applicable .",
    "the following two subsections introduce the concepts of sparseness measure and weak convexity , by which the projected subgradient method can be generalized to be applicable to @xmath13-minimization .",
    "first , a class of sparsity - inducing penalties is introduced .",
    "the penalty @xmath28 in ( [ lfopt ] ) is defined as @xmath29 where @xmath30 belongs to a class of sparseness measures @xcite satisfying the following definition  [ definition_spar ] .",
    "[ definition_spar ] the sparseness measure @xmath31 satisfies    1 .",
    "@xmath32 , @xmath30 is even and not identically zero ; 2 .",
    "@xmath30 is non - decreasing on @xmath33 ; 3 .",
    "the function @xmath34 is non - increasing on @xmath35 .",
    "as has been revealed in @xcite , the null space property with its constant @xcite is closely related to whether @xmath13-minimization is able to find the sparse signal .",
    "define @xmath36 as the vector generated by setting the entries of @xmath5 indexed by @xmath37 to zeros .",
    "[ definition_nsp ] define null space constant @xmath38 as the smallest quantity such that @xmath39 holds for any set @xmath40 with @xmath41 and for any vector @xmath42 , where @xmath43 denotes the null space of @xmath44 .",
    "based on definition  [ definition_spar ] and definition  [ definition_nsp ] , the following proposition is derived in @xcite .",
    "[ proposition_nsp ] ( theorem 2 , 3 , and 5 from @xcite ) . for penalty @xmath12 formed by @xmath30 satisfying definition  [ definition_spar ] ,",
    "the following statements hold :    1 .   if @xmath45 , then for any @xmath5 satisfying @xmath46 and @xmath47 , @xmath5 is the unique solution to ( [ lfopt ] ) ; 2 .   if @xmath48 , then there exist vectors @xmath5 and @xmath49 such that @xmath46 , @xmath50 and @xmath51 ; 3 .",
    "@xmath52 .",
    "proposition  [ proposition_nsp].1)-2 ) reveals that the null space constant is a tight quantity for the tuple @xmath53 to indicate the performance of @xmath13-minimization . here",
    "the tightness is in the sense that @xmath45 implies all @xmath54-sparse signals are the unique solutions to @xmath13-minimization , while not all @xmath54-sparse signals satisfy this if @xmath48",
    ". proposition  [ proposition_nsp].3 ) indicates that for the tuple @xmath55 , if all @xmath54-sparse signals are the unique solutions to @xmath9-minimization , i.e. @xmath56 , this also applies to @xmath13-minimization .",
    "therefore , _ in the worst case sense _ which takes over all @xmath54-sparse signals , the performance of @xmath13-minimization is at least as good as that of @xmath9-minimization .",
    "the concept of weak convexity was proposed decades ago @xcite .",
    "a real valued function @xmath30 defined on a convex subset @xmath57 is @xmath58-convex if there exists some real number @xmath58 which is the largest quantity such that the inequality @xmath59 holds for any @xmath60 and for any @xmath61 $ ] .",
    "@xmath62 , @xmath63 and @xmath64 correspond to strong convexity , convexity and weak convexity , respectively .",
    "the following proposition reveals that @xmath30 can be decomposed into the sum of a convex function and a square .",
    "[ proposition_1 ] ( proposition 4.3 from @xcite ) .",
    "function @xmath65 is @xmath58-convex if and only if there exists a convex function @xmath66 such that @xmath67 for all @xmath68 .    according to proposition  [ proposition_1 ] , weakly convex functions",
    "are also known as semi - convex functions @xcite . for any @xmath69 which denotes the interior of @xmath70 , define the directional derivative of a @xmath58-convex function @xmath30 as @xmath71 then the generalized gradient set @xcite is defined as @xmath72 if @xmath30 is convex , @xmath73 is commonly known as the subgradient set .",
    "the following proposition demonstrates an important property of @xmath58-convex functions which will be used in the theoretical analysis .",
    "[ proposition_3 ] ( proposition 4.8 from @xcite ) .",
    "let @xmath30 be @xmath58-convex on @xmath70 , then for any @xmath74 , @xmath75 , and for any @xmath76 , @xmath77      before formally introducing the main results of our paper , some related state - of - the - art researches are introduced .",
    "being aware of them might be of benefit in realizing the contributions of our paper .",
    "some recent theoretical progress has been made based on the projected subgradient method . in @xcite , the inexact projections are adopted , but these projections require approaching the exact one in the course of the algorithm .",
    "another approximate subgradient projection method is introduced in @xcite .",
    "rather than approximate projection , it considers approximate subgradient .",
    "the zap algorithm @xcite is essentially a special case of the non - convex approach introduced in our paper .",
    "the literature @xcite attempts to provide the convergence analysis of zap , yet the analysis is only for @xmath9-zap which uses the convex @xmath9 norm as the sparsity - inducing penalty . despite this fact",
    ", it already contains some important ideas which are helpful in the theoretical analysis of our paper .",
    "since the introduction of the concept of weak convexity @xcite , a branch of researches has been focused on the duality and optimality conditions for weakly convex minimization problems @xcite .",
    "these researches can be regarded as the extensions of those in convex optimization .",
    "they mainly consider the condition under which a point is the global minimizer of a weakly convex problem , which differs from the goal of our paper : providing convergence guarantees of an algorithm . in the area of sparse recovery , little attention",
    "has previously been paid to the concept of weak convexity .",
    "our paper can be regarded as a pioneer work to introduce the concept of weak convexity to the field of compressive sensing and sparse recovery , and we believe that there is still much room for further research .    to verify the theoretical analysis in our paper , numerical simulations are implemented in the setting of random gaussian sensing matrices .",
    "we have noticed that there is previous research characterizing the precise behavior of general penalization terms with gaussian sensing matrices .",
    "one may read @xcite for further reference .",
    "the main contributions of this paper are threefold . first , by combining the concept of sparseness measure with weak convexity , most commonly used sparsity - inducing penalties are characterized and some new results on the performance evaluation of @xmath13-minimization are derived .",
    "second , a non - convex algorithm based on projected subgradient method is proposed to solve @xmath13-minimization with performance guarantees .",
    "last but not the least , a uniform approximate projection is adopted in the proposed algorithm to save computational resources , and its performance guarantees as well as computational complexity analysis are provided .",
    "these contributions are demonstrated in the following subsections respectively .",
    "our work adopts weakly convex sparseness measure to constitute the sparsity - inducing penalty @xmath12 in ( [ lfopt ] ) .",
    "the definition of weakly convex sparseness measure is proposed as follows .",
    "[ definition_weak_spar ] the weakly convex sparseness measure @xmath31 satisfies    1 .",
    "@xmath32 , @xmath30 is even and not identically zero ; 2 .",
    "@xmath30 is non - decreasing on @xmath33 ; 3 .",
    "the function @xmath34 is non - increasing on @xmath35 ; 4 .",
    "@xmath30 is a weakly convex function on @xmath33 .",
    ".weakly convex sparseness measures with parameter @xmath58 + ( requirements : @xmath78 and @xmath79 ) [ cols=\"^,^,^\",options=\"header \" , ]     in the remaining content of this subsection , we consider the performance of pgg in the noisy scenario @xmath80 where @xmath81 is the @xmath54-sparse signal to be recovered and @xmath82 is the additive noise to the measurement vector .",
    "define @xmath83 as the smallest nonzero singular value of @xmath44 .",
    "the following theorem reveals the performance of pgg in the noisy scenario .",
    "[ theorem_pgg ] ( performance of pgg ) . for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , and for any positive constant @xmath84 ,",
    "if the non - convexity of @xmath12 satisfies @xmath85 the recovered solution @xmath86 by pgg satisfies @xmath87 provided that @xmath88 and @xmath89 , where @xmath90    theorem  [ theorem_pgg ] can be directly derived from lemma  [ maintheorem ] and lemma  [ theorem_new ] in section  [ sec_theo ] .    according to theorem  [ theorem_pgg ] , under some certain conditions , if the non - convexity of the penalty is below a threshold ( which is in inverse proportion to the distance between the initial solution and the sparse signal ) , the recovered solution of pgg will get into the @xmath91-neighborhood of @xmath92 . by choosing sufficiently small step size @xmath93 ,",
    "the influence of the @xmath94 term can be omitted , and the pgg method returns a stably recovered solution .",
    "if @xmath63 , @xmath12 is just a scaled version of the @xmath9 norm , and the condition ( [ theoremfor4 ] ) always holds for all @xmath95 .",
    "therefore , no constraint needs to be imposed on the distance between the initial solution and the sparse signal .",
    "this is consistent in the fact that @xmath9-minimization is convex and the initial solution can be arbitrary .",
    "in addition , larger non - convexity of the penalty induces smaller @xmath84 , i.e. , stronger constraint on the distance between the initial solution and the sparse signal , which is also an intuitive result .",
    "the initialization and the projection step of the pgg method involves the pseudo - inverse matrix @xmath96 , whose exact calculation may be computationally intractable or even impossible because of its large scale in practical applications . to reduce the computational burden , a uniform approximate pseudo - inverse matrix of @xmath97",
    "is adopted .",
    "this method is termed approximate pgg ( apgg ) method . according to appendix  [ app_cal ] which introduces approximate calculation of the pseudo - inverse matrix",
    ", we use @xmath98 to denote the approximation of @xmath99 . to characterize the approximate precision of the pseudo - inverse matrix , define @xmath100 where @xmath101 denotes the spectral norm of the matrix , and we assume @xmath102 throughout this paper . similar to theorem  [ theorem_pgg ] , the following theorem shows the performance of apgg in the noisy scenario .",
    "[ theorem_apgg ] ( performance of apgg ) . for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , and for any positive constant @xmath84 ,",
    "if the non - convexity of @xmath12 satisfies ( [ theoremfor4 ] ) and the approximate pseudo - inverse matrix @xmath98 satisfies @xmath102 , the recovered solution @xmath86 by apgg satisfies @xmath103 provided that @xmath88 and @xmath89 , where @xmath104 @xmath105 , and @xmath106 and @xmath107 are respectively specified as ( [ lemmafor4 ] ) and ( [ lemmafor5 ] ) .",
    "theorem  [ theorem_apgg ] can be directly derived from lemma  [ lemma_apgg ] and lemma  [ theorem_new ] in section  [ sec_theo ] .",
    "similar to theorem  [ theorem_pgg ] , theorem  [ theorem_apgg ] also reveals that under some certain conditions , if the non - convexity of the penalty is below a threshold , the recovered solution of apgg will get into the @xmath91-neighborhood of @xmath92 .",
    "this result is interesting since the influence of the approximate projection is only reflected on the coefficients instead of an additional error term . in the noiseless scenario with sufficiently small step size @xmath93",
    ", the sparse signal @xmath92 can be recovered with any given precision , even when a uniform approximate projection is adopted in this method .    by far , only the case of strictly sparse signal is analyzed and discussed .",
    "for compressible signal @xmath92 , assume @xmath108 .",
    "it is easily calculated that @xmath109 and @xmath110 according to theorem  [ theorem_apgg ] , the recovered solution @xmath86 of apgg will get into the @xmath111-neighborhood of @xmath112 .",
    "since @xmath112 lies in the @xmath113-neighborhood of @xmath92 , the distance between @xmath86 and @xmath92 will be no more than @xmath114 this reflects the performance degradation due to the noise and non - sparsity of the original signal .    to end up this section",
    ", we talk about the computational complexity of the apgg method .",
    "the following theorem  [ theorem_iternum ] reveals how many iterations are needed for apgg to derive the solution with desired accuracy .",
    "[ theorem_iternum ] for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , positive constant @xmath84 , vector @xmath92 with @xmath88 , and @xmath98 as an approximate pseudo - inverse matrix with @xmath102 , if the initial solution of apgg satisfies @xmath89 and the non - convexity of @xmath12 satisfies ( [ theoremfor4 ] ) , then in at most @xmath115 iterations , the recovered solution by apgg satisfies ( [ theoremfor5 ] ) , where @xmath116 and @xmath117 are respectively specified as ( [ lemmafor6 ] ) and ( [ lemmafor7 ] ) and @xmath105 .",
    "the proof is postponed to section  [ proofthiternum ]",
    ".    for calculating the approximate pseudo - inverse matrix of @xmath44 , the computational complexity of the method introduced in appendix  [ app_cal ] would be @xmath118 ( if the initialization is adopted ) or @xmath119 ( if the method iterates for at least once ) . according to",
    ", it can be derived that @xmath116 is @xmath120 , therefore theorem  [ theorem_iternum ] reveals that the number of iterations needed is @xmath121 . as for each iteration of apgg ,",
    "the computational complexity is @xmath118 .",
    "overall , the computational complexity of apgg is at most @xmath122 .",
    "this section mainly aims to establish theoretical supports for the results in section  [ sec_main ] . to begin with",
    ", some additional properties of weakly convex sparseness measure @xmath30 are revealed in the following lemma .",
    "let @xmath123 .",
    "[ lemmarest ] the weakly convex sparseness measure @xmath30 satisfies the following properties :    1 .   for all @xmath124 , @xmath125 ; 2 .   for all @xmath126 and @xmath127 , @xmath128 ; 3 .   for all @xmath129 and @xmath127 , @xmath130 ; 4 .   for all @xmath124 and @xmath76",
    ", it holds that @xmath131 5 .   for all @xmath129 , @xmath132 .",
    "the proof is postponed to section  [ prooflemmarest ] .    based on the definitions of weakly convex sparseness measure and null space constant with their properties ,",
    "a lemma is established for preparation as follows .",
    "[ lemmanew ] for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , and for any positive constant @xmath84 , the inequality @xmath133 holds for all vectors @xmath92 and @xmath5 satisfying @xmath88 and @xmath134 , where @xmath106 and @xmath107 are respectively specified as ( [ lemmafor4 ] ) and ( [ lemmafor5 ] ) .    the proof is postponed to section  [ prooflemmanew ] .",
    "the following corollary can be immediately derived from lemma  [ lemmanew ] .",
    "[ coroconstant ] for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , and for any positive constant @xmath84 , the inequality @xmath135 holds for all vectors @xmath92 and @xmath5 satisfying @xmath88 , @xmath134 , and @xmath136 , where @xmath106 and @xmath107 are specified as ( [ lemmafor4 ] ) and ( [ lemmafor5 ] ) , respectively .    the inequality ( [ lemmaform3 ] ) is somewhat similar to the concept of lipschitz continuity , but with the difference that the inequality sign is reversed . according to ( [ lemmaform3 ] ) ,",
    "if the gap between @xmath28 and @xmath137 is small , @xmath5 would not be far away from the sparse vector @xmath92 .",
    "the following lemma  [ theoremlocal ] demonstrates the main result on the local minima of @xmath13-minimization .",
    "[ theoremlocal ] for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , and for any positive constant @xmath84 , the inequality @xmath138 holds for all vectors @xmath92 and @xmath5 satisfying @xmath88 , @xmath139 and @xmath136 , where @xmath106 and @xmath107 are specified as ( [ lemmafor4 ] ) and ( [ lemmafor5 ] ) , respectively .    the proof is postponed to section  [ prooftheoremlocal ] .",
    "lemma  [ theoremlocal ] demonstrates the distribution of the local minima of @xmath13-minimization .",
    "as is revealed , for any local minimum @xmath5 in the area of ( [ theoremfor1 ] ) , it also satisfies @xmath140 therefore , lemma  [ theoremlocal ] implies that there is no local minimum in the corresponding annulus . intuitively , recalling that @xmath141 for the pgg method , if the initial solution satisfies ( [ theoremfor1 ] ) , the recovered solution is stable against the noise .",
    "the following lemma  [ maintheorem ] demonstrates the detailed convergence property of the pgg method in one iteration . for simplicity ,",
    "let @xmath5 and @xmath142 represent @xmath27 and @xmath143 , respectively .",
    "[ maintheorem ] for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , positive constant @xmath84 , and vector @xmath92 with @xmath88 , if the previous iterative solution @xmath144 of the pgg method satisfies ( [ theoremfor1 ] ) and @xmath145 where @xmath146 and @xmath106 and @xmath107 are respectively specified as ( [ lemmafor4 ] ) and ( [ lemmafor5 ] ) , the next iterative solution @xmath142 satisfies @xmath147    the proof is postponed to section  [ proofmaintheorem ] .",
    "according to lemma  [ maintheorem ] , if the iterative solution @xmath27 lies within a neighborhood of the sparse signal @xmath92 as ( [ theoremfor1 ] ) , as long as the distance between @xmath27 and @xmath92 is larger than a quantity linear in both the step size @xmath93 and the noise term @xmath148 , the next iterative solution @xmath143 will definitely get closer to @xmath92 , and the distance reduction is at least @xmath149 .",
    "therefore , in finite iterations , the iterative solution @xmath27 will get into the @xmath91-neighborhood of @xmath92 .    to ensure that the pgg method converges , we require the sufficient condition ( [ theoremfor1 ] ) satisfied for the initial solution .",
    "we can simply choose parameters such that @xmath150 the following lemma reveals that penalties with small non - convexity will result in ( [ rhocons ] ) .",
    "[ theorem_new ] for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , and for any positive constant @xmath84 , the constraint ( [ rhocons ] ) holds if the non - convexity of @xmath12 satisfies ( [ theoremfor4 ] ) .",
    "the proof is postponed to section  [ prooftheoremnew ] .",
    "next we consider the performance of the apgg method . since @xmath98 is adopted as the approximation of @xmath96 , the iterative solution of apgg no longer satisfies @xmath151 .",
    "the following lemma gives the bound of @xmath152 .",
    "[ theorem_space ] the iterative solution @xmath27 of the apgg method satisfies @xmath153 where @xmath154 is specified as ( [ lemmafor8 ] ) .",
    "the proof is postponed to section  [ prooftheoremspace ] .    according to lemma  [ theorem_space ] ,",
    "if the accurate pseudo - inverse matrix is applied , i.e. , @xmath155 , the result is consistent in the scenario with accurate projection . for any fixed approximate precision @xmath156 ,",
    "as @xmath25 approaches infinity and the step size @xmath93 is sufficiently small , the result reveals that the performance degradation caused by the approximate projection can be omitted . for the convenience of theoretical analysis , define a constant @xmath157 such that for all @xmath158 , @xmath159    since lemma  [ lemmanew ] , corollary  [ coroconstant ] , lemma  [ theoremlocal ] , and lemma  [ theorem_new ] are independent of specific algorithms , they still hold for the apgg method .",
    "the following lemma demonstrates the convergence property of apgg in one iteration , which is a counterpart of lemma  [ maintheorem ] .",
    "[ lemma_apgg ] for any tuple @xmath53 with @xmath12 formed by weakly convex sparseness measure @xmath30 and @xmath45 , positive constant @xmath84 , vector @xmath92 with @xmath88 , and @xmath98 as an approximate pseudo - inverse matrix with @xmath102 , if the previous iterative solution @xmath144 of the apgg method satisfies ( [ theoremfor1 ] ) and @xmath160 where @xmath146 and @xmath116 and @xmath117 are respectively specified as ( [ lemmafor6 ] ) and ( [ lemmafor7 ] ) , the next iterative solution @xmath142 satisfies @xmath161 where @xmath105 .",
    "the proof is postponed to section  [ prooftheoremapgg ] .",
    "in this section , several simulations are implemented to test the recovery performance of the ( a)pgg method , and to verify the theoretical analysis . the sensing matrix @xmath97 is of size @xmath162 and @xmath163 , whose entries are independently and identically distributed gaussian with zero mean and variance @xmath164 . the locations of the nonzero entries of the sparse signal @xmath92 are randomly chosen among all possible choices , and these nonzero entries satisfy gaussian distribution or symmetric bernoulli distribution with zero mean .",
    "the sparse signal is finally normalized to have unit @xmath165 norm . in all simulations ,",
    "the approximate @xmath96 is calculated using the method introduced in appendix  [ app_cal ] .    .",
    "the problem dimensions are @xmath162 and @xmath163 , and @xmath166 is the largest integer which guarantees @xmath167 successful recovery.,width=384 ]    the first experiment tests the recovery performance of the pgg method in the noiseless scenario with different sparsity - inducing penalties and different choices of non - convexity .",
    "the penalties are formed by sparseness measures in table  [ table constant ] .",
    "the parameter @xmath168 and @xmath169 is set to have desired non - convexity .",
    "the no .  1 corresponds to the @xmath9 penalty , which is tested in the same parameter settings as a benchmark .",
    "the penalties are scaled so that the parameter @xmath170 . for each penalty with some certain non - convexity ,",
    "the sparsity level @xmath54 varies from @xmath171 to @xmath172 with increment of one .",
    "the step size @xmath93 is set to @xmath173 .",
    "if the recovery snr ( rsnr ) is higher than @xmath174db , this recovery is regarded as a success .",
    "the simulation is repeated @xmath172 times to calculate the successful recovery probability versus sparsity @xmath54 .",
    "then the crucial sparsity @xmath166 , which is the largest integer which guarantees @xmath167 successful recovery , is recorded .",
    "the results when the nonzero entries of the sparse signal satisfy gaussian distribution and bernoulli distribution are presented in fig .",
    "[ maxkrho ] and fig .",
    "[ maxkrho1 ] , respectively .",
    "as can be seen from the results , as the non - convexity of the sparsity - inducing penalty increases , the performance of pgg improves at first , and degenerates when the non - convexity continues to grow . when the non - convexity approaches zero , the performances of these penalties are close to that of the @xmath9 penalty . the results support the speculation in the end of section  [ subsec_perf ] that as the non - convexity increases , the performance of @xmath13-minimization improves , and verify theorem  [ theorem_pgg ] that the non - convexity should be smaller than a threshold to guarantee the convergence of pgg .",
    "the problem dimensions are @xmath162 and @xmath163 , and @xmath166 is the largest integer which guarantees @xmath167 successful recovery.,width=384 ]    in the second experiment , the recovery performance of ( a)pgg is compared in the noiseless scenario with some typical sparse recovery algorithms , including orthogonal matching pursuit ( omp ) @xcite , the solution to @xmath9-minimization @xcite , reweighted @xmath9 minimization @xcite , isl0 @xcite , and irls @xcite . in the simulation @xmath54 varies from @xmath175 to @xmath172 .",
    "the ( a)pgg method adopts the no .  6 sparseness measure in table  [ table constant ] with non - convexity as @xmath176 , and the penalty is scaled so that @xmath170 .",
    "the step size is set to @xmath173 .",
    "the iteration number for calculating inexact pseudo - inverse matrices is @xmath177 and the average approximate precision @xmath178 .",
    "the simulation is repeated @xmath179 times to calculate the successful recovery probability versus sparsity @xmath54 .",
    "the simulation results when the nonzero entries of the sparse signal satisfy gaussian distribution and bernoulli distribution are demonstrated in fig .",
    "[ probability ] and fig .  [ probability1 ] , respectively . as can be seen , for both distributions , irls , pgg , and apgg guarantee successful recovery for larger sparsity @xmath54 than the other references .",
    "it also reveals that in the noiseless scenario with sufficiently small step size , the approximate projection has little influence on the recovery performance of apgg .     with @xmath162 and @xmath163 when the nonzero entries of the sparse signal satisfy gaussian distribution .",
    "the approximate precision of approximate @xmath96 is @xmath178.,width=384 ]     with @xmath162 and @xmath163 when the nonzero entries of the sparse signal satisfy bernoulli distribution .",
    "the approximate precision of approximate @xmath96 is @xmath178.,width=384 ]    in the last experiment , the recovery precisions of the ( a)pgg method are simulated under different settings of step size and measurement noise . in the simulation ,",
    "the nonzero entries of the sparse signal satisfy gaussian distribution and the sparsity level @xmath180 .",
    "the same sparseness measure as that in the previous experiment is adopted , and the iteration number for calculating approximate @xmath96 is @xmath181 such that @xmath182 .",
    "the simulation is repeated @xmath179 times to calculate the @xmath183 confidence interval of rsnr and the average rsnr ( which is defined as the mean relative root squared error in db ) , and the results are shown in fig .",
    "[ snrvskappa ] .",
    "as can be seen , there is almost no difference between the performance of pgg and that of apgg . in the noisy scenario ,",
    "the rsnr is dependent on both the step size and the measurement snr ( msnr ) . for fixed msnr ,",
    "as the step size decreases , the rsnr improves at first , and remains the same when the step size is sufficiently small .",
    "larger msnr results in larger rsnr limit . in the noiseless scenario ,",
    "the rsnr improves as the step size decreases , and it can be arbitrarily large by adopting sufficiently small step size .",
    "these results are accordant with theorem  [ theorem_pgg ] and theorem  [ theorem_apgg ] , which implies that the recovery error is linear in both the step size and the noise term .",
    "confidence intervals under different step sizes and msnrs with @xmath162 , @xmath163 , and @xmath180 when the nonzero entries of the sparse signal satisfy gaussian distribution .",
    "the approximate precision of approximate @xmath96 is @xmath182.,width=384 ]",
    "\\1 ) the continuity of @xmath30 can be easily checked by proposition  [ proposition_1 ] and the continuity of convex functions . as for the inequality , we only need to consider the case of @xmath184 .",
    "since @xmath185 is non - increasing on @xmath35 and @xmath186 is a finite quantity , it holds that for all @xmath184 , @xmath187 .",
    "\\2 ) it is easy to check that @xmath188 satisfies definition  [ definition_weak_spar].1)-3 ) . since @xmath189 and @xmath190 is convex",
    ", @xmath188 satisfies definition  [ definition_weak_spar].4 ) with parameter @xmath191 .",
    "in addition , since @xmath192 , the same argument as the proof of lemma  [ lemmapre].1 ) implies that @xmath193 .      according to the definition of null space",
    "constant , @xmath194 implies that for any nonzero vector @xmath195 , @xmath196 has at least @xmath197 nonzero entries , and any @xmath198 column vectors of @xmath44 are linearly independent .",
    "since @xmath30 is non - decreasing and bounded on @xmath33 , without loss of generality , we assume @xmath199 .    for any @xmath200 , define @xmath201 where @xmath202 is the smallest singular value of all @xmath198 column submatrices of @xmath97 ( @xmath202 is nonzero since any @xmath198 column vectors of @xmath44 are linearly independent ) .",
    "since @xmath30 is non - decreasing on @xmath33 , these exists @xmath203 such that for all @xmath204 and for all @xmath205 , @xmath206 .",
    "first we prove that for all @xmath204 , @xmath207 has at most @xmath54 entries with absolute value no less than @xmath208 .",
    "this is due to the fact that ( define @xmath209 as the set of index @xmath210 satisfying @xmath211 ) @xmath212 which implies @xmath213 .",
    "together with @xmath54-sparse signal @xmath92 , at most @xmath198 entries of @xmath214 are with absolute value no less than @xmath208 .",
    "now we prove that for all @xmath204 , @xmath215 .",
    "define @xmath216 and @xmath217 as the set of index @xmath210 satisfying @xmath218 , then as has been proved , @xmath219 . on the one hand , @xmath220 on the other hand , since @xmath221 , @xmath222 therefore , @xmath223    to sum up , we have proved that for any @xmath200 , there exists @xmath203 such that for all @xmath204 , @xmath215 .",
    "this directly leads to theorem  [ theorem_l0 ] .      define a class of penalties @xmath224 for @xmath225 .",
    "we first prove that for all @xmath225 , @xmath226 .",
    "this can be easily proved from the definition of the null space constant and the fact that for all @xmath225 , @xmath227 is equivalent to @xmath195 .",
    "now we prove @xmath228 .",
    "if not , according to proposition  [ proposition_nsp].3 ) , there exists @xmath229 such that for all @xmath225 , @xmath230 according to the definition of the null space constant , there exist @xmath195 and set @xmath70 with @xmath41 such that @xmath231 in addition , since for fixed @xmath232 and @xmath70 , @xmath233 there exists @xmath203 such that for all @xmath234 , @xmath235 combining ( [ app_equ2 ] ) with ( [ app_equ3 ] ) , it can be derived that @xmath236 holds for all @xmath234 , which contradicts ( [ app_equ1 ] ) .",
    "\\1 ) consider the non - trivial scenario where @xmath237 and @xmath238 are both nonzero .",
    "since @xmath185 is non - increasing on @xmath35 , it is easily checked that @xmath239 summing these two inequalities , together with the non - decreasing property of @xmath30 on @xmath33 , it holds that @xmath240    \\2 ) since @xmath30 is non - decreasing on @xmath33 , the directional derivative @xmath241 holds for all @xmath184 .",
    "therefore , the definition of the generalized gradient set ( [ definition_gen ] ) implies that for all @xmath127 , @xmath128 .",
    "\\3 ) it is easy to check that @xmath30 is also weakly convex on @xmath242 $ ] with parameter @xmath58 and that for all @xmath129 , @xmath243 .",
    "therefore we only need to consider the case of @xmath184 . due to the non - increasing property of @xmath185",
    ", it can be verified that @xmath244 holds for all @xmath245 .",
    "therefore the definition of the generalized gradient implies @xmath246    \\4 ) first , if @xmath247 satisfies the inequality ( [ lemmapre3 ] ) , it is easy to check that @xmath248 also satisfies it , therefore we only need to consider the scenario that @xmath249 .    if @xmath250 , the result is obvious since @xmath251 .",
    "if @xmath252 and @xmath253 , according to proposition  [ proposition_3 ] and the fact that @xmath30 is weakly convex with parameter @xmath58 on @xmath33 , the inequality ( [ lemmapre3 ] ) is still obvious . if @xmath252 and @xmath254 , then @xmath255 . since @xmath256 , it can be derived that @xmath257 to sum up , the inequality ( [ lemmapre3 ] ) is proved .",
    "\\5 ) assume @xmath67 and decompose @xmath258 by @xmath259 . since @xmath258 is convex , according to the definition of @xmath260 , @xmath261 .",
    "define @xmath262 and decompose @xmath263 by @xmath264 , where @xmath42 and @xmath265 , which denotes the orthogonal complement of @xmath43 .",
    "therefore @xmath266 .",
    "since @xmath83 is the smallest nonzero singular value of @xmath44 , @xmath267 supposing that @xmath92 is supported on @xmath268 and according to lemma  [ lemmarest].1 ) , it can be derived that @xmath269 by the decomposition of @xmath263 , it can be further derived from lemma  [ lemmarest].1 ) that @xmath270    on the one hand , according to the definition of null space constant , @xmath271 on the other hand , according to lemma  [ lemmapre].1 ) and ( [ lemmaproofz1 ] ) , @xmath272 since for @xmath273 , @xmath274 , it can be calculated that @xmath275 where the first inequality is due to definition  [ definition_weak_spar].3 ) .",
    "therefore ( [ lemmaproofknex1 ] ) , ( [ prooflemma2 ] ) , ( [ lemmaproofknex3 ] ) , and ( [ prooflemma3 ] ) imply @xmath276 since @xmath277 , according to ( [ lemmaproofz1 ] ) , ( [ lemmafor3 ] ) can be directly derived .",
    "according to lemma  [ lemmarest].4 ) , it can be derived that @xmath278 since @xmath279 , corollary  [ coroconstant ] and ( [ proofknexfor4 ] ) imply @xmath280 which completes the proof .",
    "define @xmath262 and @xmath281 . according to the procedure of pgg",
    ", it can be derived that @xmath282 , which further implies @xmath283 according to lemma  [ lemmarest].3 ) , the second item on the right side of ( [ proofknexfor5 ] ) can be bounded as @xmath284 the third item on the right side of ( [ proofknexfor5 ] ) can be decomposed to @xmath285 on the one hand ,",
    "according to the proof of lemma  [ theoremlocal ] , ( [ proofknexfor6 ] ) implies that @xmath286 on the other hand , @xmath287 substituting these inequalities into ( [ proofknexfor5 ] ) and according to ( [ theoremfor2 ] ) , the right side of ( [ proofknexfor5 ] ) can be bounded as @xmath288 , which arrives lemma  [ maintheorem ] .      according to the definition of @xmath106 and lemma  [ lemmarest].5 ) , @xmath289 therefore , due to ( [ theoremfor4 ] ) , the constraint ( [ rhocons ] ) holds .",
    "first , we prove that @xmath290 for @xmath291 , the initialization is @xmath292 , which satisfies @xmath293 for the @xmath294th iteration , the iterative solution obeys @xmath295 which satisfies @xmath296 together with ( [ initerror ] ) , it can be derived by recursion that @xmath297    now we turn to the proof of lemma  [ theorem_space ] .",
    "since @xmath298 , it can be derived that @xmath299 which completes the proof .",
    "similar to the proof of lemma  [ maintheorem ] , define @xmath262 and @xmath281 . according to ( [ iteration ] ) , it holds that @xmath300 , which further implies @xmath301    according to ( [ prooflemma4 ] ) and @xmath158 , for the second item on the right side of ( [ proof3 ] ) , @xmath302 for the third item , @xmath303 for the forth item , @xmath304 for the fifth item , it can be decomposed to @xmath305 according to the proof of lemma  [ theoremlocal ] , since @xmath306 , ( [ proofknexfor6 ] ) implies that @xmath307 and @xmath308 for the last item , @xmath309    together with the above inequalities , ( [ proof3 ] ) can be simplified to @xmath310 where @xmath311 and @xmath312 are specified as ( [ lemmafor9 ] ) and ( [ lemmafor10 ] ) , respectively .",
    "therefore , under the assumption ( [ itfirstnoise ] ) , inequality ( [ proofthfor1 ] ) implies ( [ lemmafor11 ] ) , which completes the proof .      assume that the iterative solution of apgg satisfies @xmath313 since lemma  [ lemma_apgg ] holds for any @xmath146 , we choose @xmath314 and the next iterative solution satisfies @xmath315 where the last inequality can be derived from the assumption ( [ proofknexfor7 ] ) .",
    "therefore , @xmath316 i.e. , the distance reduction is at least @xmath317 .",
    "since the initial solution satisfies @xmath89 , in at most @xmath318 iterations , the recovered solution by apgg satisfies ( [ theoremfor5 ] ) .    it needs to be noted that , similar to the discussions in section iii - e of @xcite , @xmath319 is just a parameter in the theoretical analysis , and the choice of @xmath319 would not influence the actual convergence of iterations of apgg . in other words ,",
    "the inequality ( [ proofknexfor8 ] ) always holds as long as the assumption ( [ proofknexfor7 ] ) holds , and this fact is independent of the choice of @xmath319 .",
    "this paper considers the convergence guarantees of a non - convex approach for sparse recovery . a class of weakly convex sparseness measures is adopted to constitute the sparsity - inducing penalties .",
    "the convergence analysis of the ( a)pgg method reveals that when the non - convexity of the penalty is below a threshold ( which is in inverse proportion to the distance between the initial solution and the sparse signal ) , the recovery error is linear in both the step size and the noise term . as for the apgg method , the influence of the approximate projection is reflected in the coefficients instead of an additional error term .",
    "therefore , in the noiseless scenario with sufficiently small step size , apgg returns a solution with any given precision .",
    "simulation results verify the theoretical analysis in this paper , and the recovery performance of apgg is not much influenced by the approximate projection .",
    "there are several future directions to be explored .",
    "the first direction is to study the performance of @xmath13-minimization for tuple @xmath320 . in this paper",
    "we mainly utilize the null space constant to characterize its performance , and it is only tight for tuple @xmath55 . for a fixed sparse signal @xmath92 , as the non - convexity @xmath321 increases ,",
    "the performance of @xmath13-minimization should be different , as is revealed in theorem  [ theorem_l0 ] and fig .",
    "[ maxkrho]-[maxkrho1 ] .",
    "the second possible direction is to improve the performance of sparse recovery by solving a sequence of optimization problems with different choices of non - convexity .",
    "the major concern would be the selection rules of the sequence of non - convexity such that the recovered solution for the previous non - convexity would lie in the convergence neighborhood for the next non - convexity .",
    "the methods of computing @xmath322 have been developed to a mature technology .",
    "they are roughly classified into two categories : direct methods @xcite and iterative methods @xcite .",
    "direct methods are mainly based on matrix decompositions , such as qr decomposition @xcite and singular value decomposition @xcite .",
    "iterative methods , on the other hand , derive the pseudo - inverse matrix iteratively . to develop more accurate solutions , they cost more computational resources . therefore , the iterative methods are preferred if approximate pseudo - inverse matrix can be applied to reduce the computational complexity .    a well - known iterative method introduced by ben - israel et al",
    "@xcite is @xmath323 with the parameter @xmath324 satisfying @xmath325 , where @xmath326 denotes the maximum absolute column sum of the matrix .",
    "simple calculation derives that @xmath327 which means this method is quadratic convergence .    in this paper",
    ", it is assumed that the approximate pseudo - inverse matrix is of the form @xmath98 , i.e. , the transpose of @xmath97 multiplied by a matrix @xmath328 .",
    "@xmath329 is considered as the approximation of @xmath330 .",
    "it is verified that most , if not all , iterative methods @xcite satisfy this assumption .",
    "e.  cands , j.  romberg , and t.  tao , `` robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , '' _ ieee trans .",
    "information theory _ ,",
    "52 , no .  2 , pp .",
    "489 - 509 , feb . 2006 .",
    "j.  tropp , j.  laska , m.  duarte , j.  romberg , and r.  baraniuk , `` beyond nyquist : efficient sampling of sparse bandlimited signals , '' _ ieee trans .",
    "information theory _ ,",
    "56 , no .  1 ,",
    "pp .  520 - 544 , jan .",
    "m.  mishali , and y.  eldar , `` from theory to practice : sub - nyquist sampling of sparse wideband analog signals , '' _ ieee journal of selected topics in signal processing _ ,",
    "vol .  4 , no .  2 , pp .",
    "375 - 391 , apr . 2010 .",
    "i.  gorodnitsky and b.  rao , `` sparse signal reconstruction from limited data using focuss : a re - weighted minimum norm algorithm , '' _ ieee trans .",
    "signal processing _ , vol .",
    "45 , no .  3 , pp .  600 - 616 , mar .",
    "1997 .",
    "h.  mohimani , m.  babaie - zadeh , and c.  jutten , `` a fast approach for overcomplete sparse decomposition based on smoothed @xmath332 norm , '' _ ieee trans .",
    "signal processing _ , vol .",
    "57 , no .  1 ,",
    "pp .  289 - 301 , jan .",
    "g.  gasso , a.   rakotomamonjy , and s.   canu , `` recovering sparse signals with a certain family of nonconvex penalties and dc programming , '' _ ieee trans . signal processing _ ,",
    "57 , no .  12 , pp .  4686 - 4698 , dec .",
    "2009 .",
    "j.  jin , y.  gu , and s.  mei , `` a stochastic gradient approach on compressive sensing signal reconstruction based on adaptive filtering framework , '' _ ieee journal of selected topics in signal processing _ ,",
    "vol .  4 , no .  2 , pp .",
    "409 - 420 , apr . 2010 .",
    "r.  gribonval and m.  nielsen , `` highly sparse representations from dictionaries are unique and independent of the sparseness measure , '' _ applied and computational harmonic analysis _ ,",
    "22 , no .  3 , pp .  335 - 355 , may 2007 .",
    "s.  foucart and m.  lai , `` sparsest solutions of underdetermined linear systems via @xmath333-minimization for @xmath334 , '' _ applied and computational harmonic analysis _ , vol .",
    "26 , no .  3 , pp .  395 - 407 , may 2009 .",
    "i.  daubechies , r.  devore , m.  fornasier , and c.  gntrk , `` iteratively reweighted least squares minimization for sparse recovery , '' _ communications on pure and applied mathematics _ ,",
    "63 , no .  1 ,",
    "pp .  1 - 38 , jan .",
    "e.  chouzenoux , a.  jezierska , j.  pesquet , and h.  talbot , `` a majorize - minimize subspace approach for @xmath16 image regularization , '' _ siam journal on imaging sciences _ , vol .  6 , no .  1 ,",
    "563 - 591 , mar .",
    "2013 .",
    "d.  lorenz , m.  pfetsch , and a.  tillmann , `` an infeasible - point subgradient method using adaptive approximate projections , '' _ computational optimization and applications _ ,",
    "56 , no .  1 ,",
    "pp .  1 - 36 , sep .",
    "2013 .",
    "x.  wang , y.  gu , and l.  chen , `` proof of convergence and performance analysis for sparse recovery via zero - point attracting projection , '' _ ieee trans .",
    "signal processing _ ,",
    "60 , no .  8 ,",
    "4081 - 4093 , aug .",
    "2012 .",
    "v.  jeyakumar and b.  m.  glover , `` characterizing global optimality for dc optimization problems under convex inequality constraints , '' _ journal of global optimization _",
    ", vol .  8 , no .  2 , pp .  171 - 187 , mar .",
    "1996 .",
    "n.  shinozaki , m.  sibuya , and k.  tanabe , `` numerical algorithms for the moore - penrose inverse of a matrix : direct methods , '' _ annals of the institute of statistical mathematics _",
    "24 , no .  1",
    ", pp .  193 - 203 , dec .",
    "n.  shinozaki , m.  sibuya , and k.  tanabe , `` numerical algorithms for the moore - penrose inverse of a matrix : iterative methods , '' _ annals of the institute of statistical mathematics _ , vol .",
    "24 , no .  1 ,",
    "621 - 629 , dec ."
  ],
  "abstract_text": [
    "<S> in the area of sparse recovery , numerous researches hint that non - convex penalties might induce better sparsity than convex ones , but up until now those corresponding non - convex algorithms lack convergence guarantees from the initial solution to the global optimum . </S>",
    "<S> this paper aims to provide performance guarantees of a non - convex approach for sparse recovery . </S>",
    "<S> specifically , the concept of weak convexity is incorporated into a class of sparsity - inducing penalties to characterize the non - convexity . </S>",
    "<S> borrowing the idea of the projected subgradient method , an algorithm is proposed to solve the non - convex optimization problem . </S>",
    "<S> in addition , a uniform approximate projection is adopted in the projection step to make this algorithm computationally tractable for large scale problems . </S>",
    "<S> the convergence analysis is provided in the noisy scenario . </S>",
    "<S> it is shown that if the non - convexity of the penalty is below a threshold ( which is in inverse proportion to the distance between the initial solution and the sparse signal ) , the recovered solution has recovery error linear in both the step size and the noise term . </S>",
    "<S> numerical simulations are implemented to test the performance of the proposed approach and verify the theoretical analysis .    </S>",
    "<S> * keywords : * sparse recovery , sparseness measure , weak convexity , non - convex optimization , projected generalized gradient method , approximate projection , convergence analysis . </S>"
  ]
}