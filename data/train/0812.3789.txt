{
  "article_text": [
    "the rising price for oil has recently shifted the focus to other possible sources of energy , preferably without adverse effects to the environment .",
    "one of the methods presently being developed is nuclear magnetic fusion .",
    "the objective of fusion research is to harness the energy provided by the fusion of hydrogen isotopes . in the fusion experiment iter , presently under construction in cadarache , france ,",
    "the necessary data to design and operate anelectricity - producing plant shall be gained .",
    "iter is a tokamak , an intermittent operating device in which strong magnetic fields confine a torus - shaped plasma .",
    "since the confinement is not perfect ( and must not be ) there are always interactions between the plasma and the plasma - facing ( wall ) components ( pfcs ) which have to be taken into account .",
    "one of the key aspects in the licensing process of iter is a strict upper limit of the total amount of radioactive tritium accumulated in the vessel walls , which is presently at 700 g tritium@xcite .",
    "the prediction of the amount of retained tritium is complicated by the material choice of iter ( fig.[fig_iter ] ) : the main vessel walls are beryllium , the strike - points are made of carbon ( cfc ) and the other parts of the divertor are tungsten . during the operation of iter the interaction of the plasma and high energy 14mev - neutrons with the vessel walls",
    "will lead to erosion , redeposition , material mixing and alloy formation .",
    "since even the hydrogen retention properties of pure materials are still subject to current research , a significant amount of additional experimental data is required to develop and calibrate the theoretical models which will be needed to process the huge number of material combinations created in iter .    however , even the first step - measuring hydrogen depth profiles in material composites - is challenging for many reasons ; here we will mention only two : a ) hydrogen and its isotopes are very volatile , which can easily distort measurements of depth profiles and b ) hydrogen is usually the main component of the residual gas in vacuum chambers which precludes the use of many well - established analysis methods .    one method which holds great promise to overcome these difficulties is the nuclear reaction analysis ( nra ) of deuterium using @xmath0he as probing particle .",
    "it is a specific and sensitive method , and has a sufficient analysis depth .",
    "however every data point takes about 30min to measure and the extraction of the concentration depth profile is an ill - posedinversion problem requiring the deconvolution of the measured data vector , here even more challenging than in rutherford backscattering@xcite . therefore the experimental setup ( ie the choice of the analysis energies )",
    "should provide a maximum of information .",
    "so far the most common choice of the @xmath0he energies for the measurements was simply equidistant .",
    "using bayesian experimental design the performance of the method can be improved considerably ( in some cases up to orders of magnitude ) and quantitative measures can bederived about the expected utility of further ( time consuming ) measurements .",
    "sectionnuclear reaction analysis the basic principle of nuclear reaction analysis is straightforward : the sample is subjected to an energetic ion beam ( here @xmath0he ) with initial energy @xmath1 and incoming angle @xmath2 , which reacts predominately with the species of interest ( deuterium ) and the products of the reaction are measured under a specified angle @xmath3 .",
    "given the total number of impinging ions n@xmath4 , the energy dependent cross - section of the reaction @xmath5 , the efficency of the detection and the geometry of the set - up @xmath6 the measured total signal counts @xmath7 depend ( in the limit of small concentrations ) linearly on the concentration profile @xmath8 of the species in the depth @xmath9 : @xmath10 where @xmath11 represents normal distributed noise .",
    "repeated measurements with different initial energies of @xmath0he provide increasing information about the deuterium depth profile .",
    "the question addressed in the following is : _ given a set of already measured data @xmath12 which measurement energy should be chosen next ? _    to evaluate eq .",
    "[ equation_1 ] we first need to specify the cross section @xmath5 and the energy @xmath13 of the incident particle on its path through the sample .",
    "the relevant cross - section for the reaction d+@xmath14 + 18.352 mev ( in standard notation written as @xmath15 ) has been ( re-)measured recently @xcite in the range of 550 kev to 6mev and the obtained cross - section values have been given in tabular form .",
    "using the same method as @xcite we added several cross - section measurements at energies below 690kev and fitted both data sets taking into account also earlier measurements @xcite @xmath16\\right)=829.98*\\frac{e^{2.83962}\\left(0.270713*e^{-2.2158e}+0.0182765\\right)}{e^{3.47626}+0.270713*e^{-1.17229e}-0.00123669}\\left[mb\\right].\\ ] ] using the levenberg - marquardt algorithm minimizing the @xmath17-misfit with the variance set to @xmath7 .",
    "he , p)@xmath18he in the laboratory system with a reaction energy of q=18.352 mev ( left ) .",
    "b ) on the right hand side the typical results for an nra measurement are shown ( simulated data of a tungsten sample with an exponentially decaying d concentration profile ( cf .",
    "eq.[eq_mock_data ] ) .",
    "the uncertainties due to the counting statistics are usually dominated by the uncertainty of the analysis current.,title=\"fig : \" ] [ fig_yield_a ]    he , p)@xmath18he in the laboratory system with a reaction energy of q=18.352 mev ( left ) .",
    "b ) on the right hand side the typical results for an nra measurement are shown ( simulated data of a tungsten sample with an exponentially decaying d concentration profile ( cf .",
    "eq.[eq_mock_data ] ) . the uncertainties due to the counting statistics are usually dominated by the uncertainty of the analysis current.,title=\"fig : \" ] [ fig_yield_b ]",
    "the cross - section is plotted in fig.[fig_yield]a and shows a broad maximum around 630 kev and is above 3 mev nearly constant at 8 mb / sr up to 6 mev ( above that there are no data available ) .",
    "the reaction energy is very high ( q=18.352 mev ) and most of the energy is transferred to the resulting proton .",
    "this leads to a very good s / n - ratio of the measurement because other particles can easily be separated by energy .",
    "the energy loss of the impinging @xmath0he - ion in the sample is determined by the _ stopping power _",
    "@xmath19 of the sample @xmath20 which can be solved to get the depth dependent energy @xmath21 for different initial energies @xmath1 .",
    "parameterizations and tables of @xmath22 for different elements are given in @xcite .",
    "since the amount of hydrogen in the sample is usually well below @xmath23 ( with the exception of a very thin surface layer ) , the influence of the hydrogen concentration on the stopping power can be neglected in most cases .      to simulate mock data for typical accelerator parameters a tungsten sample @xmath24 with a ( high ) surface concentration of 12@xmath25 deuterium , followed by an exponentially decaying deuterium concentration down to a constant background level ,",
    "described by @xmath26 has been used . which can be converted into a depth scale if the density of the material is known . in tungsten @xmath27",
    "correspond to @xmath28395 nm . ] the corresponding mock data for a set of initial energies e@xmath29=\\{500 , 700 , 1000 , 1300 , 1600 , 2000 , 2500 , 3000}kev is shown in fig .",
    "2b . the variations in the detected yields display the interplay of the increasing range of the ions with increasing energy and the reduced cross - section at higher energies modulated with the decreasing deuterium concentration at larger depths .",
    "the increase of the signal by raising the initial energy from 2500 kev to 3000 kev is already caused by the constant deuterium background of 2% .",
    "the accelerator time which would be needed to obtain the 8 data points is around one working day taking into account the necessary interleaved calibration measurements : the ion bombardment causes an energy and depth dependent loss of deuterium . commonly a first order correction is applied by normalizing the yields with respect to the yields obtained from repeated calibration measurements using the same ( typically low , e.g. 690kev ) initial energy .",
    "the uncertainty of the detector is given by poisson - statistics .",
    "however , fluctuations in the beam current measurements are very often the dominating factor , affecting the pre - factor @xmath30 in eq.[equation_1 ] .",
    "an accuracy of up to 3@xmath25 can be achieved ( e.g. by using the number of rutherford - scattered @xmath0he ions on a thin gold - coating on top of the sample as reference ) .",
    "the error of the renormalization procedure is harder to quantify .",
    "for simplicity we will use @xmath31 as uncertainty of the data in the following , acknowledging that there is room forimprovement .",
    "bayesian experimental design ( bed ) offers the tempting possibility to actively select ( and optimize ) the experimental parameters for the next measurement(s ) based on objective criteria . especially if measurements are expensive or time consuming ( like in the case of energy changes of an accelerator ) it is a huge advantage to know where to look next , so as to learn as much as possible .",
    "the problem of experimental design has already been studied by lindley back in 1956 @xcite in a bayesian setting and fedorov published his influential book in 1972 @xcite - but the limitations in computational power limited the application of experimental design almost always to simple ( linear ) problems .",
    "this situation changed in the recent years and consequently there is a renewed interest to apply bed also to ( non - linear ) real - world problems ( see e.g. @xcite and references therein or e.g. @xcite . not surprising the interest is biggest in branches of physics where the experimental possibilities are severely restricted : astronomy , fusion research , ... given the excellent account of bed in @xcite we only summarize the key principles : in a first step an appropriate _ utility function u _ has to be agreed upon : it describes the value which we assign to the measurement results of an experiment and may include parameters like costs of an experiment , duration , parameter uncertainty etc .",
    "several utility functions are considered in @xcite . with focus on parameter estimation",
    "it was proposed @xcite to use the kullback - leibler divergence ( kld ) between the posterior and the prior distributions as utility function .",
    "the kld for a new datum d is given by @xmath32 .",
    "\\label{eq_utility}\\ ] ] next we try to identify the action @xmath33 which maximizes the expected utility .",
    "expected utility because we have to account for the prediction uncertainty for @xmath34 . to compute the expected utility ( eu ) we have to average over the new datum d weighted by the marginal likelihood for the new datum given the observation of the old data @xmath35 @xmath36\\nonumber\\\\ & = & \\int\\!dd\\;p\\left(d|\\underline{d},\\eta\\right)\\int\\!d\\underline{\\alpha}\\;\\frac{p\\left(d|\\underline{\\alpha},\\underline{d},\\eta\\right)p\\left(\\underline{\\alpha}|\\underline{d},\\eta\\right ) } { p\\left(d|\\underline{d},\\eta\\right)}\\log\\left[\\frac{p\\left(d|\\alpha,\\underline{d},\\eta\\right)p\\left(\\alpha|\\underline{d},\\eta\\right)}{p\\left(\\underline{\\alpha}|\\underline{d},\\eta\\right)p\\left(d|\\underline{d},\\eta\\right)}\\right]\\nonumber\\\\ & = & \\int\\!dd\\;\\int\\!d\\underline{\\alpha}\\;p\\left(d|\\underline{\\alpha},\\underline{d},\\eta\\right)p\\left(\\underline{\\alpha}|\\underline{d},\\eta\\right)\\log\\left[\\frac{p\\left(d|\\underline{\\alpha},\\underline{d},\\eta\\right)}{p\\left(d|\\underline{d},\\eta\\right)}\\right]\\nonumber\\\\ & = & \\int\\!dd\\;\\int\\!d\\underline{\\alpha}\\;p\\left(d|\\underline{\\alpha},\\underline{d},\\eta\\right)p\\left(\\underline{\\alpha}|\\underline{d},\\right)\\log\\left[\\frac{p\\left(d|\\underline{\\alpha},\\underline{d},\\eta\\right ) } { \\int\\!d\\underline{\\alpha}\\;p\\left(d|\\underline{\\alpha},\\underline{d},\\eta\\right)p\\left(\\underline{\\alpha}|\\underline{d}\\right)}\\right ] \\label{eqn_expected_utility}\\end{aligned}\\ ] ] where we dropped the @xmath37dependence of the posterior of @xmath38 in the last line , since our knowledge about @xmath38 is not influenced by a possible future action .",
    "closer inspection of eq .",
    "[ eqn_expected_utility ] reveals that only two different probability distributions are required to compute the expected utility : the posterior distribution of @xmath38 given the old data @xmath35 , @xmath39 and the likelihood of the new datum @xmath34 based on the previous measurements , @xmath40 .      assuming that the concentration profile @xmath41 depends linearly on the concentrations @xmath42 at a given set of @xmath43 support points @xmath44 then eq.[equation_1 ] can be recast in the following form @xmath45 where the data vector @xmath35 is of size @xmath46 , the matrix @xmath47 is a @xmath48matrix and the parameter - vector @xmath49 has @xmath43 components .",
    "however , the requirement of linearity applies only to the concentration parameter vector @xmath50 , the functional form of the concentration may be much more complex , e.g. @xmath51 , although almost always @xmath41 is chosen to be constant between the different support points : @xmath52 $ ] or as linear interpolation between the support points .",
    "the noise vector @xmath53 is normally distributed @xmath54 , where @xmath55 is a diagonal matrix with the entries @xmath56 .",
    "every row of @xmath57 is given by the solution of eq .",
    "[ equation_1 ] for a specified initial energy @xmath58 , @xmath59 .",
    "the consideration of the uncertainties in the entries of the matrix due to energy straggling of the impinging particles is beyond the scope of the present paper , but see e.g. @xcite .    with a gaussian likelihood for the existing data and a flat prior for the parameters the posterior of the concentration vector reads @xmath60 with @xmath61 the posterior distribution of @xmath49 including the new data point @xmath34 with its uncertainty @xmath62",
    ", @xmath63 can similarly be cast in a gaussian form .",
    "therefore eq .",
    "[ eqn_expected_utility ] can be solved analytically @xcite and yields a simple closed form for the exponential utility @xcite : @xmath64 with @xmath65 if @xmath66 is gaussian then @xmath67 .",
    "the variation of the eu depends on the vector @xmath68 which in turn is uniquely determined by the choice of the initial energy @xmath69 .",
    "the optimum ( maximum of the eu ) can be found by a simple 1-d scan of the energy .",
    "the sequential design approach in action is displayed in fig [ fig_lineareu ] .",
    "starting from the surface the concentration at increasingly larger depth intervals is of interest .",
    "for this example the chosen depths are 0 nm , 80 nm , 240 nm , 470 nm and 950 nm .",
    "after initial measurements at 400 kev , 700 kev and 3000 kev ( representing the lower and upper limit of the useful energy range for the measurements and one calibration measurement ) the best energy for the next measurement has to be determined .",
    "the eu for this first cycle has a maximum at 1250kev ( solid line ) .",
    "after a measurement with this energy the eu for the next measurement has its maximum at 960kev and about twice the eu than before .",
    "this , on the first glance , surprising increase of the eu can be made transparent : with 5 unknowns and 5 ( informative ) measurements the solution space of this linear problem no longer covers a sub - manifold of the parameter space : it collapses and the volume of the occupied parameter space starts to be determined by the measurement uncertainties .",
    "therefore the 5-th measurement has a very high eu . in the following cycle(s )",
    "the amplitude of the eu is much lower since the subsequent measurements now gradually shrink the volume of the parameter posterior distribution .",
    "as long as the eu is above the intended threshold for new measurements ( which depends on the addressed physical problem ) further measurements are indicated .",
    "how much better is the bed derived experiment compared to an experiment with the same number but equidistant chosen initial energies ?",
    "the entropy of the parameter posterior distribution would be the obvious quantity to compare .",
    "however , for the time being , many scientists are not happy with this measure and prefer a more familiar measure , e.g. the condition number .",
    "the condition number of the ( pseudo-)inverse of @xmath47 is often used to characterize linear least squares problems @xcite and is a measure how strongly uncertainties in the data vector @xmath35 may be amplified by multiplication with the pseudo - inverse matrix . using this measure the bed optimized setting surpasses the equidistant experiment by a factor of more than 100 ( ! ) .      the analytical solution in the preceeding case was possible because several approximations have been applied : the integration range of the integration over the predicted datum ( a positive quantity ) had to be changed from @xmath70 to @xmath71 . given the actual number of counts and the uncertainties this can easily be justified .",
    "unfortunately , a similar change of the integration limits had to be applied also in the parameter integration ( from @xmath72 to @xmath73 ) and here it definitely affects the results .",
    "the analysis could be repeated substituting the analytical integration by the numerical counterparts ( e.g. using codes like vegas @xcite or mcmc approaches ) .",
    "furthermore , the uncertainty of the predicted datum d is not constant but proportional to the signal @xmath74 and therefore also the integrations over the data space have to be done numerically . under those circumstances",
    "there is no difference in the computation to a non - linear experimental design problem .",
    "_ additionally _ it turned out that the actual quantity of interest is the decay length of the hydrogen depth profile and that quite accurate data for the surface hydrogen concentration are available ( additionally measuring the @xmath75 of the @xmath15 reaction ) .",
    "therefore the optimal energy settings for the estimation of the parameters @xmath76 and @xmath77 of concentration profiles of the functional form of eq .",
    "[ eq_mock_data ] have to be computed .",
    "however , in non - linear experimental design the measured data influences the eu ( in contrast to the linear case : the maximum of the eu is independent of the actually measured data , cf .",
    ". [ eq_eu_g ] ) and this poses a practical problem : the next accelerator energy has to be determined after the previous measurement . and longer computation times to optimize the eu , causing delays , are not tolerable .    here",
    "the posterior sampling approach , suggested in @xcite , proved very valuable .",
    "it turned out that sets of posterior samples @xmath78 drawn from @xmath79 could be generated quite efficiently ( partly due to the low dimensionality of the parameter space ) . with that sample ( typically of size 1000 ) the denominator of the logarithm in eq .",
    "[ eqn_expected_utility ] is given by a simple summation @xmath80 the biggest saving comes from the fact that the posterior sample is independent from the actual value of @xmath34 and of the design action @xmath33 : all computations are reduced to repeated evaluations of the likelihood , which can efficiently be vectorized .",
    "finding the best energy is a matter of less than 5 minutes ( ! ) on contemporary hardware ( linux - pc , 2ghz ) .    in fig .",
    "[ fig_nonlineareu ] three cycles of the non - linear bed are shown : after a first measurement at 500kev the posterior distribution of @xmath81 is visualized in the upper left graph by the posterior sample .",
    "the single measurement does not allow to distinguish between a large decay constant @xmath76 and low constant offset @xmath77 or vice versa .",
    "the eu , plotted in the upper right graph , favors now a measurement at the other end of the energy range ( the maximum of the utility function is encircled ) . after a measurement with 3mev @xmath82 the area of the posterior distribution",
    "is significantly reduced ( middle row , left graph ) : the background concentration is below 3% but the decay length is still quite undetermined .",
    "the eu has a maximum at 1500 kev , still with a pretty high eu . performing a measurement with 1500kev localizes the posterior distribution around the true ( but unknown value of @xmath83 nm and @xmath84 ) .",
    "the next measurement should be performed at 1200kev but the eu is significantly lower than before : subsequent measurements are predominantly improving the statistics : a second measurement at 3 mev provides nearly the same information .",
    "are displayed .",
    "on the right hand side the eu is plotted and the maximum is indicated by a circle .",
    "the corresponding abscissa value is the suggested next measurement energy",
    ". performingthat measurement yields the posterior distribution given in the next row.__,width=529 ]",
    "the concept of bayesian experimental design allows to objectively optimize experimental designs .",
    "here we presented two different approaches to optimize nra depth profiling : first in a linear setting , allowing an analytical solution and straightforward parametric studies .",
    "second , a time - critical non - linear experimental design problem which could be tackled using posterior sampling .",
    "both optimization procedures may considerably increase the accuracy of the derived depth profiles compared to the present approach and at the same time reduce the overall measurement time by signaling a diminishing utility of further measurements . with the posterior sampling approach many sequential measurements",
    "can now be optimized on the fly : this opens up the door for a wealth of new applications of bed in the field of ion beam analysis@xcite as well as in other physical areas @xcite                          t. j. loredo , bayesian adaptive exploration in _ bayesian inference and maximum entropy methods in science and engineering _ , edited by g. erickson and y. zhai , aip , melville , ny , vol .",
    "proc * 707 * , 330 - 346 ( 2003 ) .",
    "r. fischer , bayesian experimental design - studies for fusion diagnostics in _ bayesian inference and maximum entropy methods in science and engineering _ , edited by r. fischer , r. preuss and u. von toussaint , aip , melville , ny , vol .",
    "proc * 735 * , 76 - 83 ( 2004 ) .    p. riegler and n. caticha , maxent queries and sequential sampling in _ bayesian inference and maximum entropy methods in science and engineering _ , edited by a. mohammad - djafari , aip , melville , vol .",
    "proc * 568 * , 270 - 279 ( 2001 ) ."
  ],
  "abstract_text": [
    "<S> nuclear reaction analysis with @xmath0he holds the promise to measure deuterium depth profiles up to large depths . </S>",
    "<S> however , the extraction of the depth profile from the measured data is an ill - posed inversion problem . </S>",
    "<S> here we demonstrate how bayesian experimental design can be used to optimize the number of measurements as well as the measurement energies to maximize the information gain . </S>",
    "<S> comparison of the inversion properties of the optimized design with standard settings reveals huge possible gains . </S>",
    "<S> application of the posterior sampling method allows to optimize the experimental settings interactively during the measurement process .    ' '' '' </S>"
  ]
}