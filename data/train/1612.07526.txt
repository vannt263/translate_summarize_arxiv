{
  "article_text": [
    "the symmetric tridiagonal eigenvalue problems are usually solved by the divide and conquer ( dc ) algorithm both on shared memory multicore platforms and parallel distributed memory machines .",
    "the dc algorithm is fast and stable , and well - studied in numerous references @xcite .",
    "it is now the default method in lapack  @xcite and scalapack  @xcite when the eigenvectors of a symmetric tridiagonal matrix are required .",
    "recently , the authors  @xcite used the hierarchically semiseparable ( hss ) matrices  @xcite to accelerate the tridiagonal dc in lapack , and obtained about 6x speedups in comparison with that in lapack for some large matrices on a shared memory multicore platform .",
    "the bidiagonal and banded dc algorithms for the svd problem are accelerated similarly  @xcite .",
    "the main point is that some intermediate eigenvector matrices are rank - structured matrices @xcite .",
    "the hss matrices are used to approximate them and then use fast hss algorithms to update the eigenvectors .",
    "hss is an important type of rank - structured matrices , and others include @xmath0-matrix @xcite , @xmath1-matrix @xcite , quasiseparable  @xcite and sequentially semiseparable ( sss )  @xcite . in this paper , we extend the techniques used in  @xcite to the distributed memory environment , try to accelerate the tridiagonal dc algorithm in scalapack  @xcite . to integrate hss algorithms into scalapack routines ,",
    "an efficient distributed hss construction routine and an hss matrix multiplication routine are required . in our experiments , we use the routines in strumpack ( structured matrices package ) package  @xcite , which is designed for computations with both _",
    "sparse _ and _ dense _ structured matrices .",
    "the current strumpack has two main components : _ dense matrix computation package _ and _ sparse direct solver and preconditioner_. in this work we only use its dense matrix operation part .",
    "it is written in c++ using openmp and mpi parallism , uses hss matrices , and it implements a parallel hss construction algorithm with randomized sampling  @xcite .",
    "note that some routines are available for sequential hss algorithms  @xcite or parallel hss algorithms on shared memory platforms such as hsspack  @xcite .",
    "but strumpack is the only available one for the distributed parallel hss algorithms .",
    "more details about it and hss matrices will be introduced in section  [ sec : hss ] .",
    "the scalapack routine implements the rank - one update of cuppen s dc algorithm  @xcite .",
    "we briefly introduce the main processes .",
    "assume that @xmath2 is a symmetric tridiagonal matrix , @xmath3 cuppen introduced the decomposition @xmath4 where @xmath5 and @xmath6^t$ ] with ones at the @xmath7-th and @xmath8-th entries .",
    "let @xmath9 be @xmath10 be eigen decompositions , and then   can be written as @xmath11 where @xmath12 , @xmath13 and @xmath14 the problem is reduced to computing the spectral decomposition of the diagonal plus rank - one @xmath15 by theorem 2.1 in  @xcite , the eigenvalues @xmath16 of matrix @xmath17 are the root of the secular equation @xmath18 where @xmath19 and @xmath20 are the @xmath7th component of @xmath21 and the",
    "_ _ k__th diagonal entry of @xmath22 , respectively , and its corresponding eigenvector is given by @xmath23 the eigenvectors simply computed this way may loss orthogonality . to ensure orthogonality , sorensen and tang  @xcite proposed to use extended precision . while , the implementation in scalapack uses the lwner theorem approach , instead of the extended precision approach  @xcite .",
    "the extra precision approach was used by gates and arbenz  @xcite in their implementation .",
    "the extra precision approach is  embarrassingly \" parallel with each eigenvalue and eigenvector computed without communication , but it is not portable in some platform .",
    "the lwner approach requires information about all the eigenvalues , requiring a broadcast .",
    "however , the length of communication message is @xmath24 which is trivial compared with the @xmath25 communication of eigenvectors .",
    "the excellent performance of the dc algorithm is partially due to _ deflation _",
    "@xcite , which happens in two cases .",
    "if the entry @xmath26 of @xmath21 are negligible or zero , the corresponding @xmath27 is already an eigenpair of @xmath2 .",
    "similarly , if two eigenvalues in @xmath22 are identical then one entry of @xmath21 can be transformed to zero by applying a sequence of plane rotations .",
    "all the deflated eigenvalues would be permuted to back of @xmath22 by a permutation matrix , so do the corresponding eigenvectors . then   reduces to , after deflation , @xmath28 where @xmath29 is the product of all rotations , and @xmath30 is a permutation matrix and @xmath31 are the deflated eigenvalues .    according to  ,",
    "the eigenvectors of @xmath2 are computed as @xmath32 \\begin{pmatrix } \\widehat{q } & \\\\ & i_d \\end{pmatrix}.\\ ] ] to improve efficiency , gu  @xcite suggested a permutation strategy for reorganizing the data structure of the orthogonal matrices , which has been used in scalapack .",
    "the matrix in square brackets is permuted as @xmath33 , where the first and third block columns contain the eigenvectors that have not been affected by deflation , the fourth block column contains the deflated eigenvectors , and the second block column contains the remaining columns .",
    "then , the computation of @xmath34 can be done by two parallel matrix - matrix products ( calling pblas ` pdgemm ` ) involving parts of @xmath35 and the matrices @xmath36 , @xmath37 .",
    "another factor that contributes to the excellent performance of dc is that most operations can take advantage of highly optimized matrix - matrix products .    when there are few deflations , the size of matrix @xmath35 in   will be large , and most time of dc would be cost by the matrix - matrix multiplication in  , which is confirmed by the results of example 2 in section  [ sec : hsseig ]",
    "furthermore , it is well - known that matrix @xmath35 defined as in   is a cauchy - like matrix with off - diagonally low rank property , see  @xcite .",
    "therefore , we simply use an hss matrix to approximate @xmath35 and use hss matrix - matrix multiplication routine in strumpack to compute the eigenvector matrix @xmath34 in  .",
    "since hss matrix multiplications require much fewer floating point operations than the plain matrix - matrix multiplication , ` pdgemm ` , this approach makes the dc algorithm in scalapack much faster .",
    "more details are included section  [ sec : hsseig ] and numerical results are shown in section  [ sec : num ] .",
    "the hss matrix is an important type of rank - structured matrices .",
    "a matrix is called _ rank - structured _ if the ranks of all off - diagonal blocks are relatively small compared to the size of matrix .",
    "other rank - structured matrices include @xmath0-matrix @xcite , @xmath1-matrix @xcite , quasiseparable matrices @xcite , and sequentially semiseparable ( sss ) @xcite matrices .",
    "we mostly follow the notation used in  @xcite and  @xcite to introduce hss .",
    "both hss representations and algorithms rely on a _ tree _",
    "for simplicity , we assume it is a _ binary tree _ , name it _",
    "hss tree _ , and the generalization is straightforward .",
    "assume that @xmath39 and @xmath40 is the dimension of matrix @xmath41 .",
    "each node @xmath42 of @xmath38 is associated with a contiguous subset of @xmath43 , @xmath44 , satisfying the following conditions :    * @xmath45 and @xmath46 , for a parent node @xmath42 with left child @xmath47 and right child @xmath48 ; * @xmath49 , where @xmath50 denotes the set of all leaf nodes ; * @xmath51 , @xmath52 denotes the root of @xmath38 .",
    "we assume @xmath38 is _",
    "postordered_. that means the ordering of a nonleaf node @xmath42 satisfies @xmath53 , where @xmath47 is its left child and @xmath48 is its right child .",
    "figure  [ fig : htree ] shows an hss tree with three levels and @xmath44 associated with each node @xmath42 .    a block row or column excluding the diagonal block",
    "is called an _ hss block row or column _ , denoted by @xmath54 associated with node @xmath42 .",
    "we simply call them _",
    "hss blocks_. for an hss matrix , hss blocks are assumed to be numerically low - rank .",
    "figure  [ fig : hssblock ] shows the hss blocks corresponding to node 2 .",
    "we name the maximum ( numerical ) rank of all the hss blocks by _",
    "hss rank_.    for each node @xmath42 in @xmath38 , it associates with four _ generators _",
    "@xmath55 , @xmath56 , @xmath57 and @xmath58 , which are matrices , such that @xmath59 for a leaf node @xmath42 , @xmath60 , @xmath61 , @xmath62 .",
    "a @xmath63 ( block ) hss matrix @xmath41 can be written as @xmath64 and figure  [ fig : htree ] shows its corresponding postordering hss tree .",
    "the hss representation   is equivalent to the representations in  @xcite .    to take advantage of the fast hss algorithms , we need to construct an hss matrix first .",
    "there exist many hss construction algorithms such as using svd  @xcite , rrqr(rank revealing qr )  @xcite , and so on .",
    "most of these algorithms cost in @xmath65 flops , where @xmath40 is the dimension of matrix and @xmath66 is its hss rank . in  @xcite , a randomized hss construction algorithm ( ` randhss ` ) is proposed , which combines random sampling with _ interpolative decomposition _ (",
    "i d ) , see  @xcite .",
    "the cost of ` randhss ` can be reduced to @xmath67 flops if there exists a fast matrix - vector multiplication algorithm in order of @xmath68 flops .",
    "strumpack also uses this algorithm to construct hss matrices . for completeness , ` randhss ` is restated in algorithm  [ alg : randhss ] .",
    "[ alg : randhss ] ( randomized hss construction algorithm ) given a matrix @xmath41 and integers @xmath66 and @xmath69 , generate two @xmath70 gaussian random matrices @xmath71 and @xmath72 , and then compute matrices @xmath73 and @xmath74 .",
    "` do ` @xmath75    ` for ` node @xmath42 at level @xmath76    ` if ` @xmath42 is a leaf node ,    1 .",
    "@xmath77 ; 2 .",
    "compute @xmath78 , @xmath79 ; 3 .",
    "compute the i d of @xmath80 , @xmath81 ; 4 .",
    "compute @xmath82 , @xmath83 ;    ` else `    1 .",
    "store generators @xmath84 , @xmath85 ; 2 .   compute @xmath86 , @xmath87 ; 3 .",
    "compute the i d of @xmath80 , @xmath88 ; 4 .",
    "compute @xmath89 , @xmath90 ;    ` end if `    ` end for `    ` end do `    for the root node @xmath42 , store @xmath84 , @xmath85 .",
    "the parameter @xmath66 in algorithm  [ alg : randhss ] is an estimate of the hss rank of @xmath41 , which would be chosen adaptively in strumpack  @xcite , and @xmath69 is the oversampling parameter , usually equals to @xmath91 or @xmath92 , see  @xcite . after matrix @xmath41 is represented in its hss form , there exist fast algorithms for multiplying it with a vector in @xmath67 flops  @xcite .",
    "therefore , multiplying an hss matrix with another @xmath93 matrix only costs in @xmath65 flops .",
    "note that the general matrix - matrix multiplication algorithm ` pdgemm ` costs in @xmath94 flops .",
    "strumpack is an abbreviation of _ structured matrices package _ , designed for computations with sparse and dense structured matrices .",
    "it is based on some parallel hss algorithms using randomization  @xcite .",
    "comparing with scalapack , strumpack requires more _ memory _ , since besides the original matrix it also stores the random vectors and the samples , and the generators of hss matrix .",
    "the memory overhead increases as the hss rank increases .",
    "therefore , strumpack is suitable for matrices with low off - diagonal ranks . for the our eigenvalue problems ,",
    "we are fortunate that the hss rank of intermediate eigenvector matrices appeared in the dc algorithm is not large , usually less than 100 . through the experiments in section  [ sec",
    ": num ] , we let the compression threshold of constructing hss be @xmath95-@xmath96 , to keep the orthogonality of computed eigenvectors .",
    "one advantage of strumpack is that it requires much fewer operations than the classical algorithms by exploiting the off - diagonally low rank property .",
    "another property of strumpacak , the same as other packages that explores low - rank structures ( @xmath0-matrices  @xcite and block low - rank representations ) , is irregular computational patterns , dealing with irregular and imbalanced taskflows and manipulating a collection of small matrices instead of one large one .",
    "hss algorithms requires a lower asymptotic complexity , but the flop rate with hss is often lower than with traditional matrix - matrix multiplications , blas3 kernels .",
    "we expect hss algorithms to have good performances for problems with large size . therefore , we only use hss algorithms when the problem size is large enough , just as in  @xcite .",
    "the hss construction algorithm in strumpack uses randomized sampling algorithm combined with interpolative decomposition ( i d )  @xcite , first proposed in @xcite . for this randomized algorithm",
    ", the hss rank needs to be estimated in advance , which is difficult to be estimated accurately .",
    "an adaptive sampling mechanism is proposed in strumpack , and its basic idea  @xcite is ` to start with a low number of random vectors @xmath97 , and whenever the rank found during interpolative decomposition is too large , @xmath97 is increased ' . to control the communication cost",
    ", we use a little more sample vectors ( @xmath98 ) and let ` inc_rand_hss ` relatively large which is a parameter for constructing hss matrix in strumpack .    in this work ,",
    "we mainly use two strumpack driver routines : the _ parallel hss construction _ and _ parallel hss matrix multiplication _ routines .",
    "we use these two routines to replace the general matrix - matrix multiplication routine ` pdgemm ` in hope of achieving good speedups for large matrices .",
    "we test the efficiency and scalability of strumpack by using an hss matrix firstly appeared in the test routine of strumpack  @xcite .",
    "* example 1*. we use two @xmath99 toeplitz matrices , which have been used in  @xcite . in this example",
    "we assume @xmath100 . the first one is defined as @xmath101 and @xmath102 for @xmath103 , which is diagonally dominant and yields very low hss rank ( about 2 ) .",
    "the second one is defined as @xmath104 and @xmath105 , which is a kinetic energy matrix from quantum chemistry  @xcite , @xmath106 is a discretization parameter .",
    "this matrix has slightly larger hss rank ( about @xmath107 ) .",
    "[ c]|c|c|cccccc| & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 & @xmath113 + & 185.33 & 53.58 & 13.42 & 8.02 & 3.79 & 1.89 + & const & 6.18 & 2.39 & 1.21 & 1.24 & 1.29 & 2.89 + & multi & 16.64 & 11.65 & 3.72 & 2.46 & 1.78 & 1.75 + & speedup & 8.12 & 3.82 & 2.72 & 2.17 & 1.23 & 0.41 + & const & 4.57 & 1.80 & 0.96 & 1.20 & 1.23 & 2.68 + & multi & 15.42 & 11.10 & 3.46 & 2.29 & 1.71 & 1.58 + & speedup & 9.27 & 4.15 & 3.04 & 2.30 & 1.29 & 0.44 +    from the results in table  [ tab : ex0-time ] , the hss matrix multiplication implemented in strumpack can be more than 1.2x times faster than ` pdgemm ` when the used processes are around 256 .",
    "the speedups are even better when using fewer processes .",
    "but , the hss construction and matrix multiplication algorithms are not as scalable as pdgemm , and they become slower than pdgemm when using more processes , for example more than 676 .",
    "therefore , it suggests to use no more than 256 processes when combining strumpack with scalapack .",
    "the execution time highly depends on many factors , such as parameters nb , block_hss , etc .",
    "the results in table  [ tab : ex0-time ] were obtained by choosing @xmath114 and @xmath115 .",
    "note that we did not try to find the optimal parameters .",
    "in this section we show more details of combining hss matrix techniques with scalapack . as mentioned before",
    ", the central idea is to replace ` pdgemm ` by the hss matrix multiplication algorithms .",
    "the eigenvectors are updated in the scalapack routine ` pdlaed1 ` , and therefore we modify it and call strumpack routines in it instead of ` pdgemm ` .",
    "note that after applying permutations to @xmath116 in  , matrix @xmath35 should also be permuted accordingly . from the results in  @xcite , we know that @xmath35 is a cauchy - like matrix and off - diagonally low - rank , the numerical rank is usually around @xmath117-@xmath118 .",
    "when combining with hss , we would not use gu s idea since permutation may destroy the off - diagonally low - rank structure of @xmath35 in  .",
    "we need to modify the scalapack routine ` pdlaed2 ` , and only when the size of deflated matrix @xmath119 in   is large enough , hss techniques are used , otherwise use gu s idea .",
    "denote the size of @xmath119 by @xmath120 , and it depends the architecture of particular parallel computers , and may be different for different computers .",
    "most work of dc is spent on the first two top - levels matrix - matrix multiplications .",
    "the first top - level takes nearly 50% when using 256 processes or fewer , see the results in table  [ tab : ex1-time ] and also the results in  @xcite .",
    "therefore , we could expect at most 2x speedup when replacing ` pdgemm ` with parallel hss matrix multiplications . in this subsection ,",
    "we fist use an example to show the off - diagonally low - rank property of @xmath35 in  .",
    "here we assume that the dimension of @xmath22 is @xmath121 , and the diagonal entries of @xmath22 satisfy @xmath122 , @xmath123 , @xmath124 and @xmath125 is a normalized random vector in  .",
    "then the singular values of matrix @xmath126 are illustrated in figure  [ fig : svals ] with @xmath127 . from it",
    ", we get that the singular values decay very quickly .",
    "* example 2 .",
    "* we use a symmetric tridiagonal matrix to show the percentage of time cost by the top level matrix - matrix multiplications .",
    "the diagonal entries of tridiagonal matrix @xmath41 are all two and its off - diagonal entries are all one .",
    "the size of this matrix is @xmath100 .",
    "[ c]|c|cccccccc| & @xmath108 & @xmath109 & @xmath128 & @xmath110 & @xmath111 & @xmath112 & @xmath129 & @xmath130 + top one & 59.80 & 44.13 & 30.29 & 16.92 & 9.23 & 4.77 & 1.09 & 0.53 + total & 108.04 & 77.06 & 52.78 & 30.04 & 17.70 & 10.00 & 6.20 & 6.22 + percent(% ) & 55.35 & 57.2 & 57.39 & 56.32 & 52.15 & 47.70 & 17.58 & 8.52 +    the results in table  [ tab : ex1-time ] are obtained by using optimization flags ` -o2",
    "-c -qopenmp -mavx ` , and linked with multi - threaded intel mkl . from the results in it , we can see that the top - one level matrix - matrix multiplications can take half of the total time cost by ` pdstedc ` in some case .",
    "since ` pdgemm ` in mkl has very good scalability , the percentage of top - one level matrix multiplications decreases as the number of processes increases .",
    "this example also implies that we are better not to use more than 256 processes in our numerical experiments .",
    "all the results are obtained on tianhe-2 supercomputer  @xcite , located in guangzhou , china .",
    "it employs accelerator based architectures and each compute node is equipped with two intel xeon e5 - 2692 cpus and three intel xeon phi accelerators based on the many - integrated - core ( mic ) architectures . in our experiments we only use cpu cores .",
    "* example 3 .",
    "* we use some ` difficult ' matrices  @xcite for the dc algorithm , for which few or no eigenvalues are deflated .",
    "examples include the clement - type , hermite - type and toeplitz - type matrices , which are defined as follows .",
    "the clement - type matrix  @xcite is given by @xmath131 where the off - diagonal entries are @xmath132 .",
    "the hermite - type matrix is given as  @xcite , @xmath133    the toeplitz - type matrix is defined as  @xcite , @xmath134    for the results of strong scaling , we let the dimension be @xmath135=@xmath136 , and use hss techniques only when the size of secular equation is larger than @xmath137 .",
    "the results for strong scaling are shown in figure  [ fig : strong ] .",
    "the speedups of phdc over scalapack are reported in table  [ tab : ex3-strong ] , and shown in figure  [ fig : padc - scal ] .",
    "we can see that phdc is about @xmath138 times faster than pdstedc in mkl when using 120 processes or fewer .",
    "[ c]|c|ccccc| & + & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 + clement & 2.45 & 1.91 & 1.58 & 1.42 & 1.14 + hermite & 1.92 & 1.43 & 1.30 & 1.22 & 1.00 + toeplitz & 2.30 & 1.92 & 1.58 & 1.43 & 1.26 +    the orthogonality of the computed eigenvectors by phdc are in the same order as those by scalapack , which are shown in table  [ tab : ex3-orth ] .",
    "the orthogonality of matrix @xmath116 is defined as @xmath139 , where @xmath140 is the maximum absolute value of entries of @xmath141 .",
    "[ c]|c|ccccc| & + & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 + clement & @xmath142-@xmath143 & @xmath142-@xmath143 & @xmath142-@xmath143 & @xmath142-@xmath143 & @xmath142-@xmath143 + hermite & @xmath144-@xmath143 & @xmath144-@xmath143 & @xmath144-@xmath143 & @xmath144-@xmath143 & @xmath144-@xmath143 + toeplitz & @xmath145-@xmath143 & @xmath145-@xmath143 & @xmath145-@xmath143 & @xmath145-@xmath143 & @xmath145-@xmath143 +    * example 4 . * in  @xcite",
    ", tygert shows that the spherical harmonic transform ( sht ) can be accelerated using the tridiagonal dc algorithm and one symmetric tridiagonal matrix is defined as follows , @xmath146 for @xmath147 , where @xmath148 for @xmath149 assume that the dimension of this matrix is @xmath150 and @xmath151 .",
    "the execution times of using phdc and pdstedc are reported in table  [ tab : ex4-time ] .",
    "[ c]|c|ccccc| & + & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 + pdstedc & 475.00 & 139.09 & 39.66 & 25.82 & 16.87 + phdc & 224.97 & 88.58 & 28.53 & 20.12 & 14.71 + speedup & 2.11 & 1.57 & 1.39 & 1.28 & 1.15 +      different from scalapack , the elpa routines  @xcite do not rely on blacs , all communication between different processors is handled by direct calls to a mpi library , where elpa stands for _ eigenvalue solver for petascale applications_. for its communications , elpa relies on two separate sets of mpi communicators , row communicators and column communicators , respectively ( connecting either the processors that handle the same rows or the same columns ) .",
    "for the tridiagonal eigensolver , elpa implements its own matrix - matrix multiplications , does not use pblas routine ` pdgemm ` .",
    "it is known that elpa has better scalability and is faster than mkl  @xcite .",
    "* example 5 .",
    "* we use the same matrices as in example 3 to test elpa , and compare it with the newly proposed algorithm phdc .",
    "the running times of elpa are shown in table  [ tab : time - elpa ] .",
    "figure  [ fig : spd - elpa ] shows the speedups of phdc over elpa .",
    "phdc is faster than elpa since it requires fewer floating point operations .",
    "however , its scalability is worse than elpa , and phdc becomes slower than elpa when using more than 200 processes .",
    "[ c]|c|ccccc| & + & @xmath108 & @xmath109 & @xmath110 & @xmath111 & @xmath112 + clement & 487.63 & 137.43 & 40.36 & 25.61 & 12.49 + hermite & 363.94 & 117.61 & 34.58 & 21.18 & 10.89 + toeplitz & 509.32 & 141.57 & 39.55 & 25.62 & 12.67 +",
    "by combining scalapack with strumpack , we propose a hybrid tridiagonal dc algorithm for the symmetric eigenvalue problems , which can be faster than the classical dc algorithm implemented in scalapack when using about 200 processes .",
    "the central idea is to relpace ` pdgemm ` by the hss matrix multiplication algorithms , since hss matrix algorithms require fewer flops than ` pdgemm ` .",
    "numerical results show that the scalability of hss matrix algorithms is not as good as ` pdgemm ` .",
    "the proposed phdc algorithm in this work becomes slower than the classical dc algorithm when using more processes .",
    "the authors would like to acknowledge many helpful discussions with xiangke liao , sherry li and shuliang lin .",
    "this work is partially supported by national natural science foundation of china ( nos .",
    "11401580 , 91530324 , 91430218 and 61402495 ) .",
    "e.  anderson , z.  bai , c.  bischof , s.  blackford , j.  demmel , j.  dongarra , j.  du  croz , a.  greenbaum , s.  hammarling , a.  mckenney , and d.  sorensen . .",
    "society for industrial and applied mathematics , philadelphia , pa , third edition , 1999 .",
    "t.  auckenthaler , v.  blum , h.  j. bungartz , t.  huckle , r.  johanni , l.  krmer , b.  lang , h.  lederer , and p.  r. willems .",
    "parallel solution of partial symmetric eigenvalue problems from electronic structure calculations .",
    ", 37(12):783794 , 2011 .",
    "s.  chandrasekaran , p.  dewilde , m.  gu , t.  pals , x.  sun , a.  j. van  der veen , and d.  white .",
    "fast stable solvers for sequentially semi - separable linear systems of equations and least squares problems . technical report ,",
    "university of california , berkeley , ca , 2003 .",
    "s.  chandrasekaran , m.  gu , j.  xia , and j.  zhu . a fast qr algorithm for companion matrices . in i.  a. ball and .et al . ,",
    "editors , _ recent advances in matrix and operator theory _ , pages 111143 , birkh'auser , basel , 2008 .",
    "j.  choi , j.  demmel , i.  dhillon , j.  dongarra , s.  ostrouchov , a.  petitet , k.  stanley , d.  walker , and r.c .",
    "scalapack : a portable linear algebra library for distributed memory computers - design issues and performance .",
    ", 97:115 , 1996 .",
    "a.  marek , v.  blum , r.  johanni , v.  havu , b.  lang , t.  auckenthaler , a.  heinecke , h.  bungartz , and h.  lederer .",
    "scalable parallel eigenvalue solutions for electronic structure theory and computational science .",
    ", 26:115 , 2014 ."
  ],
  "abstract_text": [
    "<S> in this paper , an efficient divide - and - conquer ( dc ) algorithm is proposed for the symmetric tridiagonal matrices based on scalapack and the hierarchically semiseparable ( hss ) matrices . </S>",
    "<S> hss is an important type of rank - structured matrices . </S>",
    "<S> most time of the dc algorithm is cost by computing the eigenvectors via the matrix - matrix multiplications ( mmm ) . in our parallel hybrid dc ( phdc ) </S>",
    "<S> algorithm , mmm is accelerated by using the hss matrix techniques when the intermediate matrix is large . </S>",
    "<S> all the hss algorithms are done via the package ` strumpack ` . </S>",
    "<S> phdc has been tested by using many different matrices . </S>",
    "<S> compared with the dc implementation in mkl , phdc can be faster for some matrices with few deflations when using hundreds of processes . </S>",
    "<S> however , the gains decrease as the number of processes increases . </S>",
    "<S> the comparisons of phdc with elpa ( the eigenvalue solvers for petascale applications library ) are similar . </S>",
    "<S> phdc is usually slower than mkl and elpa when using 300 or more processes on tianhe-2 supercomputer .    </S>",
    "<S> scalapack , divide - and - conquer , hss matrix , distributed parallel algorithm 65f15 , 68w10 </S>"
  ]
}