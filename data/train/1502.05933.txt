{
  "article_text": [
    "high dimensionality constitutes a major challenge for the development of new statistical methodologies @xcite . in the context of genomic data analysis for instance ,",
    "the number of recorded variables @xmath0 ( like gene expression ) is higher than the sample size @xmath1 , which makes classical regression and classification methods inappropriate @xcite .",
    "indeed , high dimensionality is often associated with spurious dependencies between variables , leading to singularities in the optimization processes , with neither unique nor stable solution .",
    "this challenge calls for the development of specific statistical tools , such as dimension reduction approaches that can be of two different types . on the one hand , compression techniques consist in projecting observations into a lower dimensional space to summarize the information contained in the different variables . for instance , the partial least squares ( pls ) regression @xcite is appropriate for linear regression especially with highly correlated covariates , by constructing new components as linear combinations of predictors that maximize their covariance with the response . on the other hand ,",
    "variable selection methods are based on a hypothesis of parsimony , meaning that only a few relevant variables contribute to the model fit .",
    "their purpose is to `` select '' the latters and drop the non pertinent variables from the model .",
    "an example is the lasso @xcite , with its @xmath2 penalty constraint on the norm of coefficients , which shrinks the coefficients of less relevant variables to zero @xcite .",
    "eventually , sparse pls ( spls ) regression @xcite combines compression and variable selection to reduce dimension .",
    "it introduces a selection step based on the lasso in the pls framework , constructing new components as sparse linear combinations of predictors .",
    "sparse pls actually reveals its advantages for selection over the lasso when predictors present high correlations .",
    "whereas the lasso selects only one variable among a group of relevant correlated ones @xcite , sparse pls selects all relevant predictors in correlated groups @xcite .",
    "it occurs as well that combining compression and `` sparse '' approach improves the efficiency of prediction and the accuracy of selection @xcite , compared to the lasso or even to the elastic net , introduced by @xcite .",
    "sparse pls has showed excellent performance in the case of regression with continuous responses , but it turns out that its adaptation to classification is difficult .",
    "@xcite or @xcite proposed to use sparse pls as a preliminary dimension reduction step before a standard classification method , such as discriminant analysis , following previous approaches using this idea with classical pls @xcite .",
    "another solution consists in using logistic regression , a classification method derived from generalized linear models or glms @xcite , that can manage different response distributions ( binary , multicategorical , count ) through maximum likelihood estimation .",
    "this optimization is iteratively achieved via the iteratively reweighted least squares ( irls ) algorithm @xcite .",
    "however its convergence is not guaranteed @xcite , especially in the high dimensional case .    the main difficulty when combining logistic regression with ( s)pls is that both methods rely on iterative algorithms that are not necessarily straightforward to combine , especially with the irls algorithm whose convergence is not ensured in high dimension .",
    "performing compression with ( s)pls @xcite on the categorical response as a first step before logistic regression remains counter - intuitive , because ( s)pls is designed to handle continuous response within homoskedastic models .",
    "@xcite proposed to use pls within the irls iterations to solve reweighted least squares at each step , @xcite followed this idea with sparse pls , but it appears that convergence issues of irls remain .",
    "our method will first rely on the use of ridge penalized logistic regression @xcite to ensure irls convergence . within this framework",
    "a continuous pseudo - response is generated , which makes classical pls appropriate to estimate predictor coefficients , as proposed by @xcite .    in this work",
    "we develop such a method for sparse pls in order to combine compression and variable selection in a glm framework .",
    "we also propose an adaptive version of sparse pls , inspired from the adaptive lasso @xcite , to improve the variable selection accuracy .",
    "using simulations we show the accuracy , stability and convergence of our method , compared with other state - of - the - art approaches . especially , we show that compression increases variable selection accuracy , and that our method is more stable regarding the choice of hyper - parameters by cross validation , contrary to other methods processing classification with sparse pls .",
    "we propose an updated version of the ` plsgenomics ` r - package , soon released on the cran ( http://cran.r-project.org/ ) .",
    "we will first introduce our method based on ridge irls and adaptive sparse pls .",
    "then , we will discuss its advantages compared with state - of - the - art methods .",
    "we will finish by a comparative study and eventually an application of our method to the prediction of breast cancer relapse after 5 years based on gene expression data .",
    "[ [ the - logistic - regression - model . ] ] the logistic regression model .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we observe a sample of size @xmath1 , denoted by @xmath3 , with @xmath4 the label variables in @xmath5 and @xmath6 a set of @xmath0 covariates . in the following",
    ", we will use notations @xmath7 and @xmath8^{t}}}$ ] .",
    "we use the generalized linear models ( glm ) framework @xcite to relate the predictors to the random response variable @xmath9 , using the logistic link function , such that @xmath10 with @xmath11 $ ] , @xmath12 , and @xmath13 . in the sequel",
    ", we use notation @xmath14 $ ] . denoting by @xmath15 , the log - likelihood of the model",
    "is defined by @xmath16 $ ] , and we estimate the coefficients @xmath17 by maximum likelihood .    [ [ the - irls - algorithm . ] ] the irls algorithm .",
    "+ + + + + + + + + + + + + + + + + + +    the optimization @xcite relies on a gradient descent to construct a sequence of coefficients @xmath18 , whose limit @xmath19 ( if it exists ) is the estimation of @xmath17 . in particular , a newton - raphson based algorithm gives an explicit formulation of @xmath18 such that : @xmath20,\\\\ \\end{aligned } \\right.\\ ] ] where @xmath21 is called pseudo - response , @xmath22 is the vector of estimated probabilities of success for each observation , with @xmath23 , @xmath24 is the diagonal empirical variance matrix of observations @xmath4 at step @xmath25 , with @xmath26 .",
    "each step of this algorithm , called iteratively reweighted least squares ( irls ) algorithm @xcite , can be interpreted as a regression of the pseudo - response @xmath21 onto @xmath27 , weighted by the matrix @xmath28 .",
    "thus , it achieves the successive resolution of a weighted least square problem . following the definition of @xmath21 , the irls algorithm produces a pseudo - response @xmath29 as the limit of the sequence @xmath30 computed at each iteration , which is of the form @xmath31 where @xmath19 is the solution of the likelihood optimization , and",
    "@xmath32 is a noise vector of covariance matrix @xmath33 , where @xmath34 is the limit of the matrix sequence @xmath35 .",
    "[ [ stabilizing - the - irls - with - a - ridge - penalty . ] ] stabilizing the irls with a ridge penalty .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when @xmath36 , the irls algorithm may encounter convergence issues , giving infinite estimates in the case of completely separate or quasi - completely separate data @xcite . if @xmath37 , the @xmath38 design matrix @xmath27 is of rank @xmath1 or less and therefore not full column - rank .",
    "due to identifiability concerns , it implies that the mle is not unique when it exists , and even may not exist when minimal norm solution is infinite .",
    "the convergence of irls can be guaranteed by a ridge penalization , i.e. a @xmath39 norm penalty constraint on the coefficients , defining a ridge penalized log - likelihood @xcite : @xmath40 with @xmath41 the diagonal empirical variance matrix of @xmath27 and @xmath42 the ridge penalty parameter .",
    "optimization leads to the ridge irls ( rirls ) algorithm @xcite , where the weighted regression of each irls iteration is replaced by a ridge weighted regression , hence @xmath43 .",
    "a unique solution of the penalized problem still exists and is computed as the limit of @xmath18 calculated at each step of rirls .",
    "the pseudo - response @xmath29 produced by ridge irls depends on predictors through a linear model , and thus becomes suitable for sparse pls regression , following the approach of @xcite that uses standard pls regression instead . in this heteroskedastic case ,",
    "the @xmath39 metric ( in the observation space ) is weighted by the empirical inverse covariance matrix @xmath34 , to account for the heteroskedasticity of noise @xmath32 . in the following , @xmath44 and @xmath29",
    "are beforehand centered , in order to neglect the intercept .",
    "[ [ definition - of - sparse - pls - regression . ] ] definition of sparse pls regression .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    pls regression @xcite is a compression method suitable for linear regression , particularly with correlated designs .",
    "it consists in constructing new components @xmath45 as linear combinations @xmath46 of predictors .",
    "the weight vectors @xmath47 are defined to maximize the covariance ( or squared covariance ) of these new components with the considered continuous response @xcite , that we denoted by @xmath48 for the general definition , unraveling latent structure information explaining the response within design matrix . using matrix notation , @xmath49 and @xmath50 are the respective columns of the @xmath51 matrix @xmath52 and the @xmath53 matrix @xmath54 .    in order to exclude the inherent noise introduced by non pertinent variables in the model , @xcite or @xcite",
    "introduce the sparse pls by adding a variable selection step to the pls framework .",
    "it constructs `` sparse '' components , from `` sparse '' weight vectors , whose coordinates are required to be null for covariates that are irrelevant to explain the response .",
    "the shrinkage of these weights to zero is achieved with a @xmath2 norm penalty constraint in the covariance maximization problem , following the lasso principle @xcite : @xmath55 under the constraint @xmath56 and orthogonality between components , where @xmath57 is the sparsity penalty parameter . however , such objective function is not convex , and quite difficult to optimize . to overcome this issue , a rewriting of this problem",
    "was proposed @xcite using the alternate direction method @xcite .",
    "the optimization in eq .",
    "[ eq : splsorigine ] is the sum of two terms , a concave loss and a convex penalty , that can easily be optimized separately .",
    "the approach consists in separating each term to be optimized with two different arguments instead of one , constraining these arguments to stay close .",
    "@xcite extended this formulation to the weighted @xmath39 metric case , taking into account heteroskedasticity with a weighted matrix product , introducing weighted sparse pls . in our univariate context , the new optimization problem is @xcite : @xmath58 where @xmath59 and @xmath60 are the two arguments separating the loss function , @xmath61 is proportional to the covariance matrix with respect to the weighted @xmath39 metric , @xmath62 $ ] is a parameter to be tuned that penalize the difference between the two arguments @xmath60 and @xmath59 , @xmath63 is the penalty parameters on @xmath2 norm of the vector @xmath59 , @xmath64 being the @xmath65 over @xmath0 coordinate of vector @xmath59 .",
    "[ [ adaptive - sparse - pls - regression . ] ] adaptive sparse pls regression .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we propose adjusting the @xmath2 constraint to further penalize the less significant variables , which could lead to a more accurate selection process , hence improving compression .",
    "such an approach is inspired by component wise penalization as adaptive lasso @xcite and to our knowledge has not been proposed for sparse pls yet . in our case , we use the weights @xmath66 from classical pls ( without sparsity constraint ) to adapt the @xmath2 penalty constraint on the weight vector @xmath67 .",
    "the penalty becomes @xmath68 , with @xmath69 to account for the significance of the predictor @xmath70 in component @xmath71 , higher weights in absolute values corresponding to more important variables .",
    "the sparse weight vector @xmath72 is given by the optimal @xmath59 .",
    "the closed - form solution takes into account the adaptive penalty and remains the soft - thresholding operator introduced by @xcite , applied to the dominant singular vector of @xmath73 which is independent of the parameter @xmath74 in our univariate response case but with penalty @xmath75 for @xmath65 predictor .",
    "we called this method adaptive sparse pls .",
    "one can note that it is here presented with a weighted matrix product to fit our heteroskedastic model , but it can be rewritten as classical sparse pls by replacing @xmath34 by the @xmath76 identity matrix .",
    "the active set of selected variables up to component @xmath71 is a subset of @xmath77 , defined as the variables with a non null weight in @xmath78 , and denoted by @xmath79 . at step @xmath80 ,",
    "@xmath50 is computed by solving eq .",
    "[ eq : spls ] , using @xmath44 and a deflated response , defined as the residuals of the regression of the response @xmath48 onto all the selected variables until step @xmath71 i.e. in @xmath81 @xcite . the estimation @xmath82 of @xmath83 in the model @xmath84 is obtained by the regression of @xmath29 onto selected variables in the active set @xmath81 .",
    "coefficient @xmath85 is set to zero if the predictor @xmath86 is not in the active set .",
    "finally , the estimates @xmath82 are renormalized for non centered data and used as the estimation of @xmath83 in the logistic model @xmath87 = \\text{logit}^{-1}(\\beta_0 + { \\ensuremath{{\\ensuremath{\\mathbf{x}}}_i^{t}}}{\\ensuremath{\\boldsymbol{\\beta}}}_{\\setminus 0})$ ] with @xmath88 non centered .",
    "the intercept @xmath89 is estimated by the difference @xmath90 , @xmath91 and @xmath92 being respectively the sample average of the pseudo - response and the sample average vector of predictors .",
    "our method can be summarized as follow :    1 .",
    "@xmath93 @xmath94 @xmath95 2 .",
    "center @xmath44 and @xmath29 regarding scalar product weighted by @xmath34 3 .",
    "@xmath96 @xmath94 @xmath97    the label @xmath98 of new observations @xmath99 ( non - centered ) is predicted through the logit function thanks to estimation @xmath100 .",
    "our method estimates predictor coefficients @xmath17 in the logistic model by sparse pls regression of a pseudo - response , considered as continuous and therefore in accordance with the theoretical framework of pls , while completing compression and variable selection simultaneously .",
    "our approach will be denoted by rirls - spls in the following while the method by @xcite that inspired us will be rirls - pls .",
    "[ [ hyper - parameter - choice . ] ] hyper - parameter choice .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    our method depends on a sparsity penalty parameter @xmath101 , a ridge penalty parameter @xmath102 and the number of components @xmath103 .",
    "a common procedure to choose these parameter values is cross - validation : for each possible value of hyper - parameters , learning the model on a sub - part of the training set of observations , calculating the prediction error rate on the remaining observations , and taking the values that minimize it . to reduce the sampling dependence , we choose all the parameters by 10-fold cross - validation , meaning that we average the prediction error rate over 10 decompositions of the train set with respective size of 90%/10% of observations in sample for respectively learning and testing @xcite",
    "since the pls framework has shown good compression performance in regression , several attempts have tried to adapt it to prediction with binary responses , especially in the high dimensional case , when standard methods of classification , such as nearest neighbors , discriminant analysis or logistic regression , are inappropriate @xcite .",
    "[ [ pls - and - glms . ] ] pls and glms .",
    "+ + + + + + + + + + + + +    in order to generalize pls to the glm framework , @xcite proposed to solve the weighted least square problem at each irls step with a pls regression .",
    "this algorithm follows the irls scheme but defines the sequence @xmath18 as : @xmath104 where @xmath105 are defined as previously in irls .",
    "pls regression takes the design matrix @xmath44 , the current pseudo - response @xmath21 , the number of components @xmath103 , and the weighting matrix @xmath28 as arguments , and returns the iterate @xmath106 .",
    "however , solving the weighted least squares problem at each irls step with pls does not prevent convergence issues .",
    "a modification of marx s algorithm was introduced @xcite to correct the asymptotic bias with the firth procedure @xcite , which modifies the definition of @xmath21 .",
    "nonetheless convergence of this generalized pls algorithm ( gpls ) is not ensured @xcite .    following this principle , @xcite presented a method that solves the successive weighted least square problems of irls by a sparse pls regression , with the idea that variable selection reduces the model complexity and helps to overwhelm numerical singularities .",
    "the sparse generalyzed pls ( sgpls ) algorithm is based on the @xmath107 , previously introduced .",
    "unfortunately , our simulations will show that convergence issues remain .",
    "one explanation could be that when solving the weighted least square problem at each iteration with spls ( or pls ) , the global problem can not be rewritten as the optimization of a loss .",
    "hence , contrary to irls which optimizes a likelihood , ( s)gpls is not defined by an optimization criteria over @xmath17 .    [ [ pls - as - a - preliminary - step - before - classification . ] ] pls as a preliminary step before classification .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in high dimensional cases , another approach consists in achieving dimension reduction before any classification procedure to avoid dimensionality issues .",
    "it was hence proposed to perform pls as a preliminary compression step before constructing a standard classifier using the new components @xmath52 of dimension @xmath108 , @xmath103 being chosen to be generally smaller than @xmath1 . therefore , the classification method does not encounter high dimensional settings . in this context",
    ", the pls algorithm treats the discrete response as continuous , through a recoding with multicategorical labels @xcite .",
    "one can also add variable selection by using sparse pls [ @xcite .",
    "although it might work well on some data sets , such an approach totally neglects the distinctive definition of ( sparse ) pls to handle continuous response , and it ignores the inherent heteroskedastic context .",
    "this can be summarized as follows :    1 .",
    "@xmath52 @xmath94 @xmath109 or @xmath110 2 .",
    "construct a classifier with @xmath52 ( dim .",
    "@xmath51 ) and @xmath111    pls or sparse pls are applied without any weighting in the scalar product ( i.e. @xmath34 is replaced by the @xmath76 identity matrix in the preceding ) , on the design matrix @xmath44 and the discrete response @xmath111 , with @xmath103 components , @xmath112 being the sparsity parameter for sparse pls .",
    "the classifier can be discriminant analysis , these methods are respectively called pls - da @xcite or spls - da @xcite .",
    "it was also proposed to use logistic regression as the classifier after pls @xcite or sparse pls @xcite , respectively denoted in the following pls - log and spls - log .",
    "nevertheless the previous concern is still valid , and pls - log also encounters quasi - complete separation issues @xcite , and the optimization process for logistic regression in spls - log does not converge on our simulations .    [ [ performance - evaluation . ] ] performance evaluation .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    in order to assess the performance of our method , we compare it to other state - of - the - art approaches taking into account sparsity and/or performing compression .",
    "we eventually use a `` reference '' method , called glmnet @xcite , that performs variable selection , by solving the glm likelihood maximization penalized by @xmath2 norm penalty for selection and @xmath39 norm penalty for regularization , also known as the elastic net approach @xcite .",
    "the gpls approach used in our computation comes from the archive of the former r - package ` gpls ` .",
    "the methods rirls - pls and pls - da can be found in the package ` plsgenomics ` , sgpls , spls - log and spls - da in the r - package ` spls ` , glmnet in the ` glmnet ` r - package .",
    "we first process our method and compare it to others on simulated data .",
    "the purpose is to control the model design to evaluate in which data configuration compression and selection are appropriate for classification .",
    "we assess whether our approach performs better or worse than previously proposed procedures .",
    "we also aimed at verifying if our method respects the two crucial questions about convergence and suitability for prediction and selection .",
    "[ [ block - design - and - logit - model . ] ] block design and logit model .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our simulated data are constructed to assess the interest of compression and variable selection for prediction performance .",
    "the simulations are inspired from @xcite .",
    "the purpose is to control the redundancy within predictors , meaning the degree of multicollinearity , and the relevance of each predictor to explain the response , meaning the degree of sparsity in the model .",
    "we consider a design matrix @xmath44 of dimension @xmath113 , with @xmath114 fixed , and @xmath115 , so that we examine low and high dimensional models .",
    "to simulate redundancy within predictors , @xmath44 is partitioned into @xmath116 blocks ( 10 or 50 in practice ) denoted by @xmath117 for block @xmath71 .",
    "then for each @xmath70 in the group @xmath117 , @xmath118 , with @xmath119 and some noise @xmath120 . in this framework , each @xmath121 is a latent variable , introduced to control the correlation within the blocks which is proportional to the ratio @xmath122 .",
    "the correlation between the blocks is regulated by @xmath123 , the higher @xmath123 the less dependency . in the following we consider @xmath124 or 1/3 .",
    "the true vector of predictor coefficients @xmath125 is structured according to the blocks of @xmath44 .",
    "actually , @xmath126 blocks in @xmath125 are randomly chosen among the @xmath116 ones to be associated with non null coefficients ( with @xmath127 or @xmath128 ) .",
    "all coefficients within the @xmath126 designated blocks are constant ( with value 1/2 ) . in our model , the relevant predictors contributing to the response will be those with non zero coefficient , and our purpose will be to retrieve them via selection .",
    "the response variable @xmath9 for observation @xmath129 is sampled as a bernoulli variable , with parameter @xmath130 that follows a logistic model : @xmath131 .",
    "the parameter values that are tuned by cross - validation are the following , depending on the methods that use one of them or many , the number of components @xmath103 varies from 1 to 8 , the ridge parameter @xmath132 in rirls are 31 points that are @xmath133-linearly spaced in the range @xmath134 $ ] , the sparse parameter @xmath112 for all spls approach are 10 points that are linearly spaced in the range @xmath135 $ ] .",
    "[ [ ridge - penalty - ensures - convergence . ] ] ridge penalty ensures convergence .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    convergence is an important issue associated with the use of irls when estimating glm parameters .",
    "it is also present in low - dimensional cases @xcite , and especially crucial when combining pls and irls algorithm as pointed out by @xcite for gpls . with the analysis of high dimensional data and the use of selection in the estimating process",
    ", it becomes even more essential to ensure the convergence of the optimization algorithm .",
    "to proceed , we consider the @xmath39 convergence criterion of @xmath136 between two iterations : @xmath137 . in the following we consider that the algorithm converged if the @xmath39 norm gap becomes lower than @xmath138 with a maximum number of a hundred iterations in order to limit computation time .",
    "our simulations show that ridge regularization systematically ensures convergence of the irls algorithm before performing sparse pls in our method ( rirls - spls ) , whatever the configuration of simulation : @xmath139 , @xmath140 , high or low sparsity , high or low redundancy ( see table  [ tab : simu : conv ] for an example ) . on the contrary ,",
    "approaches that uses ( sparse ) pls before or within the irls algorithm ( resp .",
    "spls - log and ( s)gpls ) do not converge quite often or even most of the time in some configurations ( table  [ tab : simu : conv ] ) . to illustrate these convergence issues we studied the convergence path of @xmath137 ( fig .",
    "not shown ) which reveals that our method converges within fifteen iterations on average whereas other methods do not often converge , and even encounter cyclic singularities .",
    "this point confirms that performing ( sparse ) pls before or within irls algorithm does not avoid convergence issues . on the contrary",
    ", it confirms the interest of the ridge regularization combined to irls procedure to ensure its convergence .",
    "moreover , this convergence seems to be faster than with other procedures ( when it occurs ) , which depicts an interesting outcome for computational time .",
    ".percentage of model fitting that converged over 75 simulations for different values of @xmath0 , when @xmath124 , @xmath141 and @xmath142 .",
    "[ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ [ compression - is - more - efficient - to - discriminate - the - response . ] ] compression is more efficient to discriminate the response .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when representing the coordinates of the first two components constructed by compression methods , i.e. the observation scores over the new axes , we can assess the performance of the compression by coloring the points according to their @xmath143-labels .",
    "an efficient compression technique would separate the @xmath143-classes with fewer components .",
    "we compare the rirls - pls , our rirls - spls , sgpls and spls - log approaches , by tuning and fitting the model on different resamplings of our data set , the number of components is not tuned and fixed to .",
    "the fig .",
    "[ fig : data : comp ] represents the first two components computed by each methods for one resampling .",
    "it appears that only the first component produced by our method ( rirls - spls ) is sufficient to discriminate the observations between the two conditions , which is consistent with the fact that the tuning procedure always chooses @xmath144 as previously mentioned . the corresponding non sparse approach ( rirls - pls )",
    "is a bit less efficient at compression since the first two components are necessary to easily separate the two @xmath143-classes , supporting our point that variable selection improves compression .",
    "however , the other methods combining sparse pls and logistic regression differently ( sgpls and rirls - log ) do not achieve a similar efficiency in the compression process .",
    "the first two components are not sufficient to separate the @xmath143-labels , as the color points are mixed , indicating that these two methods need more components to discriminate properly the @xmath143-classes , leading to a less efficient compression process .",
    "[ [ adaptive - selection - returns - less - false - positives . ] ] adaptive selection returns less false positives .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    in order to evaluate the selection process of different approaches on real data we use the stability selection concept developed by @xcite .",
    "the grid of all parameter values ( @xmath2 parameter @xmath112 , ridge parameter @xmath132 and number of components @xmath103 depending on the methods ) is denoted by @xmath145 . this principle consists in fitting the model for all points @xmath146 , then estimating the probability @xmath147 for each covariate @xmath70 to be selected over @xmath148 resamplings ( @xmath1 being the sample size , here 294 ) by each model , depending on @xmath149 .",
    "this is indeed the probability for genes @xmath70 to be in the set @xmath150 , where @xmath151 is the corresponding coefficient estimated by the considered method .",
    "we finally define the set @xmath152 of stable selected variables verifying @xmath153 , where @xmath154 is a threshold value , meaning that variables with high selection probability are kept and ones with low selection probability are disregarded .    the average number of selected variables over the entire grid @xmath145 , is denoted by @xmath155 , and defined as @xmath156 $ ] .",
    "@xcite provided a bound on the expected number of wrongly stable selected variables ( equivalent to false positives ) in @xmath152 , depending on the threshold @xmath154 , the expectation @xmath155 and the number @xmath0 of covariates ( here 10000 ) : @xmath157 \\leq \\frac{1}{2\\pi_\\text{thr } -1}\\frac{q_\\lambda^2}{p } \\label{eq : stab_sel}\\ ] ] where @xmath158 is the number of false positives i.e. @xmath159 and @xmath160 the unknown set of true relevant variables .",
    "[ eq : stab_sel ] determines the parameter grid @xmath145 that has to be used to avoid too many false positives ( corresponding to a weak @xmath2 penalization ) . in our study , the grid @xmath145 is restrained so that @xmath161 leading to @xmath162 \\leq \\rho_\\text{error}$ ] , where @xmath163 is the maximum number of false positives in the stable selected variable set @xmath152 , that we fix .",
    "for instance , when the threshold probability @xmath154 is set to 0.9 , @xmath145 is defined as a subset of the parameter grid , so that @xmath164 . indeed",
    ", @xmath155 is unknown , but estimated by the empirical average number of selected variables depending over all @xmath165 . in this context",
    ", the expected number of false positives will be lower than @xmath163 .    the stability selection analysis ( see fig .  [",
    "fig : data : sel ] ) shows that , when the number of false positives is fixed ( on average ) , our approach rirls - spls selects more genes than any other approach ( sgpls , spls - log and glmnet ) , meaning that we discover more true positives ( because the number of false positives is bound ) , hence unraveling more relevant genes than other approaches .",
    "this illustrates again the good performance in selection of our method .",
    "moreover , approaches that use sparse pls , i.e. performing selection and compression , select more variables than glmnet to achieve the same false positive rate , hence retrieving more true positives than glmnet which performs only selection .",
    "this supports our previously developed idea that combining compression and selection is very suitable for high dimensional data analysis .    , when forcing the average number of false positives to be smaller than @xmath166 .",
    "we have proposed a method that performs compression and variable selection for classification purposes .",
    "it combines ridge regularized iterative least square algorithm and sparse pls in the logistic regression context .",
    "it is particularly suitable for the case of high dimensional data , which appears to be a crucial issue nowadays in many applications such as high - throughput sequencing data analysis in genomics .",
    "our main consideration was to ensure convergence of irls algorithm , which is a critical point in logistic regression .",
    "another concern was to properly incorporate into the glm framework a dimension reduction approach that is appropriate to high dimensional cases , such as sparse pls .",
    "ridge regularization ensures the convergence of irls algorithm , which is confirmed in our simulations and tests on real data sets . applying adaptive sparse pls as a second step on the pseudo - response produced by irls respects the definition of pls regression for continuous response .",
    "moreover , combining compression and variable selection increases the prediction performance and selection accuracy of our method , which turns out to be more efficient than state - of - the - art approaches that do not use both dimension reduction techniques .",
    "furthermore it appears that previous procedures using sparse pls with logistic regression encounter convergence issues and a lack of stability in cross - validation parameter tuning process , contrary to our approach ."
  ],
  "abstract_text": [
    "<S> for a few years , data analysis has been struggling with statistical issues related to the `` curse of high dimensionality '' . in this context , </S>",
    "<S> i.e. when the number of considered variables is far larger than the number of observations in the sample , standard methods of classification are inappropriate , thus calling for the development of specific methodologies . </S>",
    "<S> we hereby propose a new approach suitable for classification in the high dimensional cases . </S>",
    "<S> it uses sparse partial least squares ( sparse pls ) performing compression and variable selection combined to ridge penalized logistic regression . </S>",
    "<S> in particular , we have developed an adaptive version of sparse pls to improve the dimension reduction process . </S>",
    "<S> simulations show the accuracy of our method , compared with other state - of - the - art approaches . </S>",
    "<S> the particular combination of the iterative optimization of logistic regression and sparse pls in our procedure appears to ensure convergence and stability concerning the hyper - parameter tuning , contrary to other methods processing classification with sparse pls . </S>",
    "<S> our results are confirmed on a real data set , using expression levels of thousands of genes concerning less than three hundred patients to predict the relapse for breast cancer . </S>",
    "<S> eventually , our approach is implemented in the ` plsgenomics ` r - package .    </S>",
    "<S> # 1    _ keywords : _ classification , compression , dimension reduction , generalized linear model , variable selection </S>"
  ]
}