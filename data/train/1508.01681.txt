{
  "article_text": [
    "the auto - regressive with moving average ( arma ) model is central to the field of time serie analysis and has been studied since the early thirties in the field of econometrics @xcite .",
    "arma time series are sequences of the form @xmath0 satisfying the following recursion @xmath1 for all @xmath2 , and we focus on the case where @xmath3 is a sequence of zero mean independent identically distributed gaussian random variables with variance denoted by @xmath4 for simplicity . in order to accomodate more applications were also studied extensively in the recent years but the gaussian case is already a challenge from the algorithmic perspective as will be discussed below ] as is well known @xcite , time series model are adequate for a wide range of phenomena in economics , engineering , social science , epidemiology , ecology , signal processing , etc . they can also be helpful as a building block in more complicated models such as garch models , which are particularly useful in financial time series analysis .",
    "two problems are to be addressed when studying arma time series :    1 .   estimate @xmath5 and @xmath6 , the intrinsic orders of the model .",
    "2 .   estimate @xmath7 and @xmath8 .    in the case",
    "where @xmath9 , the convention is to write as : @xmath10 and @xmath11 to simply called an ar process .",
    "estimation of @xmath12 is often performed using the conditional likelihood approach , given @xmath13 yielding to the standard yule - walker equations . on the other hand",
    ", the model order selection problem is often performed using a penalized log - likelihood approach such as aic , bic , .. , may also use the plain likelihood .",
    "we refer the reader to the standard text of brockwell and davis for more details on these standard problems . turning back to the full arma model",
    ", it is well known that the log - likelihood is not a concave function , and that multiple stationary points exist which can lead to severe bias when using local optimization routines for such as gradient or newton - type methods for the joint estimation of @xmath12 and @xmath14 . in shumway and stoffer @xcite and iterative procedure resembling the em algorithm",
    "is proposed , which seems more appropriate for the arma model than standard optimization algorithms .",
    "however , no convergence guarantee towards a global maximizer is provided . concerning the model selection problem , penalties play a prominent role in modern statistical theory and practice , in particular since the recent successes of the lasso in regression and its multiple generalization .",
    "the nuclear norm penalization has played an import for many problems in engineering , machine learning and statistics such as matrix completion ,  application of nuclear norm penalization to state space model estimation and model order selection using a moment - like estimator in a convex optimization framework is proposed in @xcite .",
    "the approach of @xcite is a remarkable contribution since convex model selection and state space estimation were combined for the first time in the problem of time series .",
    "however the approach of @xcite is supported by no theoretical guarantee yet .",
    "another approach for state space model estimation was proposed in @xcite where good practical performances are reported and an asymptotic analysis is provided .",
    "this method as well as the unpenalized version of the method in @xcite can be recast into the family of subspace methods ; see @xcite .",
    "in such subspace - type methods , model order selection and model estimation are decoupled and it is natural to wonder if the approach of @xcite can be refined in order to incorporate joint model selection using a nuclear norm penalty as in @xcite .",
    "based on the evidence of the practical efficiency of subspace - type methods @xcite , our goal in the present note is to propose a theoretical study of a nuclear norm penalized version of the subspace method from @xcite which incorporates the main ideas in @xcite .",
    "a real valued random discrete dynamical system @xmath15 admits a state space representation if there exists a discrete time process @xmath16 such that @xmath17 where @xmath18 is the noise , and @xmath19 , @xmath20 , @xmath21 are parameter matrices .",
    "it is well known that arma processes admit a state space representation and vice versa @xcite .",
    "the problem of predicting @xmath22 for @xmath23 based on the knowledge of @xmath24 , @xmath25 and @xmath26 can be solved easily following the approach by bauer @xcite . for",
    "given initial values @xmath27 , @xmath28 , the state space representation gives @xmath29 on the other hand , the state space representation implies that @xmath30 thus , we obtain @xmath31    in what follows , we will assume that we observe @xmath32 and that @xmath33 is such that @xmath34 .",
    "we will rewrite the prediction problem in terms of some hankel matrices . for this purpose , define @xmath35 , \\quad   \\mathcal k & = & \\left[\\bar{a}^{t-1}k,\\cdots,\\bar{a}^{2}k , \\bar{a}k , k\\right],\\end{aligned}\\ ] ] @xmath36 \\quad \\textrm { and } \\quad   \\mathcal n = &   \\left [ \\begin{array}{cccccc } 1 & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\ bk & 1 & 0 & \\cdots & \\cdots & 0 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ ba^{t-2}k & ba^{t-3}k & \\cdots & \\cdots & bk & 1   \\end{array } \\right].\\end{aligned}\\ ] ] then , we have @xmath37 & = &   \\mathcal o s_t +   \\mathcal n \\left [ \\begin{array}{c } e_{t } \\\\ \\vdots \\\\ e_{t+h } \\end{array } \\right ] \\label{1}\\end{aligned}\\ ] ] and @xmath38 +   \\left ( a - kb\\right)^{t}s_{0}. \\label{2}\\end{aligned}\\ ] ] combining ( [ 1 ] ) and ( [ 2 ] ) , we thus obtain @xmath37 & = &   \\mathcal o \\mathcal k   \\left [ \\begin{array}{c } x_0 \\\\ \\vdots \\\\ x_{t-1 } \\end{array } \\right]+   \\mathcal o \\left ( a - kb\\right)^{t}s_{0 } +   \\mathcal n \\left [ \\begin{array}{c } e_{t } \\\\ \\vdots \\\\ e_{t+h } \\end{array } \\right].\\end{aligned}\\ ] ] now , define @xmath39 \\quad \\textrm { and } \\quad   x_{future }   = & \\left [ \\begin{array}{cccc } x_{t }   &   x_{t+1 }    & \\cdots   &   x_{t - t+1 }   \\\\ x_{t+1 }   &   x_{t+2 }    & \\cdots   &   x_{t - t+2 }    \\\\",
    "\\vdots    &   \\vdots     & \\vdots   &   \\vdots        \\\\ x_{2t-1 }    &   x_{2 t }   & \\cdots   &   x_{t }      \\\\",
    "\\end{array}\\right ] .",
    "\\end{aligned}\\ ] ] both matrices are hankel matrices .",
    "the first one represents the past values and and second one the future values .",
    "define also the noise matrix @xmath40 . \\end{aligned}\\ ] ] all these hankel matrices are related by the following equation @xmath41",
    "in the last section , we showed that the matrices @xmath42 , @xmath43 and @xmath44 of the state space model entered nicely into an equation allowing prediction of future values based on past values of the dynamical system .",
    "our goal is now to use this equation to estimate the matrices @xmath42 , @xmath43 and @xmath45",
    ". one interesting feature of our procedure is that the dimension @xmath5 of the state space model can be estimated jointly with the matrices themselves .",
    "the matrix @xmath46 can be estimated using a least squares approach corresponding to solving @xmath47 this procedure will make sense if the term @xmath48 is small .",
    "this can indeed be justified if @xmath49 is large and if @xmath50 is small .",
    "let us call @xmath51 a solution of ( [ ls ] ) .",
    "an interesting property of the matrix @xmath53 is that its rank is the state s dimension @xmath5 when @xmath42 has full rank .",
    "moreover , @xmath53 has small rank compared to @xmath49 when @xmath49 is large compared to @xmath5 .",
    "therefore , one is tempted to penalize the least squares problem ( [ ls ] ) with a low - rank promoting penalty .",
    "one option is to try to solve @xmath54 the main drawback of this approach is that the rank function is non continuous and non convex .",
    "this renders the optimization problem intractable in practice .",
    "fortunately , the rank function admits a well known convex surrogate , which is the nuclear norm , i.e. the sum of the singular values , denoted by @xmath55 .",
    "+ thus , a nice convex relaxation of ( [ rankls ] ) is given by @xmath56 it has been observed in practice that nuclear norm penalized least squares provide low rank solution for many interesting estimation problems @xcite .",
    "the penalized least - squares problem ( [ nucls ] ) can be transformed into the following constrained problem @xmath57 for some appropriate choice of @xmath58 .",
    "let @xmath59 denote the covariance matrix of @xmath60^t$ ] and let @xmath61 denote the square root of @xmath62 .",
    "then , let @xmath63 be the random matrix whose components are given by @xmath64 where @xmath65 , @xmath66 and @xmath67 are independent rademacher random variables which are independent of @xmath68 , @xmath69 .",
    "let @xmath70 denote the covariance matrix of @xmath71 .",
    "let @xmath72 denote the operator defined by @xmath73 and let @xmath74 denote the adjoint of the inverse of @xmath72 .",
    "the fact that @xmath72 is invertible is easily obtained ( see section [ conseq ] ) and is seen from the fact that @xmath75 has all its eigenvalues equal to @xmath76 according to section [ specsigh ] .",
    "let @xmath77 be the operator defined by @xmath78 and let @xmath79 be the mapping @xmath80    our main result is the following theorem .",
    "[ main ] let @xmath81 be any positive real number .",
    "assume that @xmath58 is such that @xmath82 with probability less than or equal to @xmath83 for some @xmath84 .",
    "then , with probability greater than or equal to @xmath85 , @xmath86 where @xmath87    in the remainder of this section , we introduce the results , notations and tools for proving this theorem . the proof is given in section [ pf ] .",
    "for all @xmath66 and @xmath67 , let @xmath88 denote the operator defined by @xmath89 and let @xmath90 denote the operator @xmath91 the descent cone of the nuclear norm at @xmath53 , denoted by @xmath92 , is defined by @xmath93      the following result will be the key of our analysis .",
    "* @xcite * [ deter ] assume that @xmath94 let @xmath51 denote any solution of ( [ constrainedform ] ) .",
    "then , @xmath95 where @xmath96      we will closely follow the approach of tropp based on mendelson s bound . for this purpose , we will need the definition of the gaussian mean width @xmath98 of a set @xmath99 @xmath100,\\end{aligned}\\ ] ] where the expectation is taken with respect to the gaussian random vector @xmath101 taking values in @xmath102 .",
    "the statistical dimension of @xmath103 ( see e.g. @xcite ) is let us also denote by @xmath104 the quantity @xmath105 which , as one might easily check , does not depend on @xmath106 .",
    "recall that @xmath59 is the covariance matrix of @xmath60^t$ ] and that @xmath61 denotes the square root of @xmath62 .",
    "thus , @xmath107 : = \\sigma^{-\\frac12 } \\left [ \\begin{array}{c } x_0\\\\   \\vdots \\\\ x_{t-1 } \\end{array } \\right]\\end{aligned}\\ ] ] follows the standard gaussian distribution @xmath108 .",
    "let @xmath109 .",
    "we now state tropp s result .",
    "[ tropp ] define @xmath110 we have @xmath111 with probability greater than or equal to @xmath112 .",
    "see section [ prftropp ] .",
    "since @xmath114 follows the law @xmath115 , using lemma [ cki ] from the appendix , we get @xmath116 thus , setting @xmath117 we obtain @xmath118 this finally gives @xmath119 let us now compute a lower bound to the infimum of this quantity over the set of @xmath120 satisfying @xmath121 . for this purpose ,",
    "first note that @xmath122 on the other hand , simple manipulations of the optimality conditions using symmetry prove that @xmath123 therefore , @xmath124      the gaussian mean width of a set @xmath103 and its statistical dimension are related by @xmath125 see ( * ? ? ?",
    "* proposition 10.2 ) for a proof . in this subsection",
    ", we estimate the gaussian mean width of @xmath44 using its statistical dimension .      the descent cone of the nuclear norm satisfies ( * ?",
    "? ? * eq . ( 4.1 ) ) which we recall now @xmath126      using proposition 4.2 in @xcite , we obtain @xmath128 we now have to compute the polar cone of @xmath44 .",
    "we have @xmath129 recall that @xmath79 is the mapping @xmath130 then , we obtain that @xmath131      let us write the singular value decomposition of @xmath46 as @xmath132 \\left [ \\begin{array}{cc } { \\rm diag}(\\sigma_{\\mathcal o\\mathcal k } ) & 0\\\\ 0 & 0   \\end{array } \\right ] \\left [   \\begin{array}{cc } v_1 & v_2   \\end{array } \\right]^t\\end{aligned}\\ ] ] where @xmath133 is the vector of the singular values of @xmath46 .",
    "moreover , the subdifferential of the schatten norm is given by @xmath134 \\left\\ { \\left [   \\begin{array}{cc } i & 0 \\\\ 0 & y   \\end{array } \\right]\\mid \\vert y\\vert \\le 1 \\right\\ } \\left [   \\begin{array}{cc } v_1 & v_2   \\end{array } \\right]^t.\\end{aligned}\\ ] ] therefore , using ( [ troppo2 ] ) , we obtain that @xmath135 & \\le &   \\mathbb e\\left[\\min_{\\tau>0 , \\ : \\vert y\\vert\\le 1}\\vert \\mathcal t^{-1}\\left ( \\tau   \\left [   \\begin{array}{cc } u_1v_1^t & 0 \\\\ 0 & u_2yv_2^t   \\end{array } \\right ]    \\right ) -   \\tilde{h}\\vert_f^2\\right].\\end{aligned}\\ ] ] thus , we get @xmath135 & \\le    \\mathbb e\\bigg[\\min_{\\tau>0 , \\ : \\vert y\\vert\\le 1 } \\vert \\mathcal t^{-1}\\vert \\ : \\bigg ( \\tau^2 \\vert u_1v_1^t \\vert_f^2 + \\vert \\tau u_2yv_2^t - \\mathcal t_{2,2}(\\tilde{h})\\vert_f^2 \\\\ & \\hspace{2 cm }   + \\vert \\mathcal t_{1,2}(\\tilde{h } ) \\vert_f^2 + \\vert \\mathcal t_{2,1}(\\tilde{h } ) \\vert_f^2\\bigg ) \\bigg]\\end{aligned}\\ ] ] where @xmath136,\\end{aligned}\\ ] ] and the dimension of @xmath137 is @xmath138 and the dimension of @xmath139 for all other combinations of @xmath140 and @xmath141 is easily deduced from the dimension of @xmath79 . which gives , after taking @xmath142 , @xmath135 & \\le     \\sigma_{\\min}(\\mathcal t)^{-1 } \\",
    "e\\left[\\tau^2\\right ] \\ : { \\rm rank}(\\mathcal o\\mathcal k ) \\\\ & \\hspace{2 cm } + \\bigg(\\sigma_{\\max}(\\mathcal t_{1,2})^2 + \\sigma_{\\max}(\\mathcal t_{2,1})^2\\bigg ) \\ : \\mathbb e\\bigg[\\vert \\tilde{h}\\vert_f^2\\bigg]\\bigg).\\end{aligned}\\ ] ] note that @xmath143 by gordon s theorem ( * ? ? ?",
    "* theorem 10.2 ) , @xmath144 \\le 2\\ : \\sqrt{t}$ ] .",
    "moreover , by lemma [ bordel ] in the appendix , @xmath145   & \\le \\frac2{c } \\left(2\\ : ct+1\\right)+2 \\ : \\sqrt{t}.\\end{aligned}\\ ] ] on the other hand , @xmath146=2t$ ] .",
    "therefore , we obtain that @xmath147\\\\   & \\le   2\\ : \\sigma_{\\min}(\\mathcal t)^{-1 } \\ : \\bigg(\\frac1{c } \\ : \\vert \\mathcal t_{2,2}\\vert^2 \\left ( \\left(2\\ : ct+1\\right ) + c \\ :",
    "\\sqrt{t}\\right ) \\ : { \\rm rang}(\\mathcal",
    "o\\mathcal k ) \\\\ & \\hspace{1.3 cm } + 2\\ : \\bigg(\\sigma_{\\max}(\\mathcal t_{1,2})^2 +   \\sigma_{\\max}(\\mathcal t_{2,1})^2\\bigg ) \\ : t\\bigg).\\end{aligned}\\ ] ] using ( [ compwidthdim ] ) , we obtain that @xmath148      combining lemma [ tropp ] with ( [ q ] ) and ( [ wgk ] ) , we obtain that @xmath149 using that @xmath150 and @xmath151 and combining this last inequality with theorem [ deter ] , we obtain the following proposition .    [ main ] let @xmath81 be any positive real number .",
    "assume that @xmath58 is such that @xmath82 with probability less than or equal to @xmath83 for some @xmath84 .",
    "then , with probability greater than or equal to @xmath85 , @xmath86 where @xmath152    combinig this result with the bounds from section [ conseq ] , the proof is completed .",
    "the goal of the present note is to show that the performance of nuclear norm penalized subspace - type methods can be studied theoretically .",
    "we concentrated on a special approach due to bauer @xcite .",
    "our approach can easily be extended to the case of the method promoted in @xcite .",
    "our next objective for future research is to address the case of more general noise sequences such as in @xcite .",
    "in this section , we gather some technical results used in the proof of theorem [ main ] .",
    "we have @xmath153 recall that @xmath59 is the covariance matrix of @xmath60^t$ ] and that @xmath61 denotes the square root of @xmath62 .",
    "thus , @xmath107 : = \\sigma^{-\\frac12 } \\left [ \\begin{array}{c } x_0\\\\   \\vdots \\\\ x_{t-1 } \\end{array } \\right]\\end{aligned}\\ ] ] follows the standard gaussian distribution @xmath108 .",
    "recall also that @xmath109 .",
    "then , we have @xmath154 now , we have @xmath155 which gives , by markov s inequality @xmath156 thus , we obtain @xmath157      let @xmath158 we will now use the bounded difference inequality to control this quantity . for this purpose ,",
    "notice that @xmath159 for all @xmath160 in @xmath161 and @xmath162 .",
    "thus , @xmath163 & \\le & \\nu \\ : \\sqrt{t\\ : ( t-2t+2)},\\end{aligned}\\ ] ] with probability @xmath85 for all @xmath164 .",
    "now , the expected supremum can be bounded in the same manner as in ( * ? ? ?",
    "* equation 5.6 ) .",
    "@xmath165 & \\le &   \\frac2{\\xi } \\",
    ": \\mathbb e \\left [ \\sup_{\\stackrel{\\vert \\tilde{d}\\sigma^{-1/2}\\vert_f=1,}{\\tilde{d}\\in \\mathcal d(\\vert\\cdot\\vert_*,\\mathcal o\\mathcal k)\\sigma^{1/2 } } } \\quad \\sum_{s=0}^{t-1 } \\sum_{s^\\prime=0}^{t-2t+1 } \\epsilon_{s , s^\\prime } \\sum_{r=0}^{t-1 } \\tilde{d}_{s , r } z_{s^\\prime+r } \\right]\\end{aligned}\\ ] ] where @xmath166 , @xmath66 and @xmath167 are independent rademacher random variables which are independent of @xmath68 , @xmath69 .",
    "therefore , we obtain @xmath168\\\\ & & \\hspace{3 cm } + \\nu \\ : \\sqrt{t\\ : ( t-2t+2)}\\bigg ) , \\end{aligned}\\ ] ] which gives @xmath169 - \\nu \\xi . \\end{aligned}\\ ] ] let us denote by @xmath170 the quantity @xmath171 .",
    "\\end{aligned}\\ ] ] then , we have @xmath172,\\end{aligned}\\ ] ] where we recall that @xmath63 is the random matrix whose components are given by @xmath173 and @xmath70 denotes the covariance matrix of @xmath71 .",
    "let @xmath174 where @xmath72 denotes the operator defined by @xmath175 then @xmath176 is a gaussian matrix with i.i.d .",
    "components with law @xmath177 . using the invertibility of @xmath72 proved in section [ conseq ]",
    ", we get @xmath178,\\end{aligned}\\ ] ] where @xmath179 where we recall that @xmath74 is the adjoint of the inverse of @xmath72 .",
    "moreover , we have @xmath180 where @xmath181 is the smallest singular value of the operator @xmath77 defined by @xmath182 thus , @xmath183 and the proof is completed .        by gaussian concentration (",
    "* proposition 4 ) and the fact that the spectral ( operator ) norm is 1-lipschitz , we obtain that for all @xmath185 , @xmath186 + u \\right ) & \\le e^{-c u^2}\\end{aligned}\\ ] ] for some absolute positive constant @xmath187 . taking @xmath188 $ ]",
    ", we obtain that @xmath189 \\right ) & \\le e^{-4 \\ : c \\delta^2 t}.\\end{aligned}\\ ] ] thus , @xmath145 & = \\int_0^{+\\infty } \\mathbb p\\left ( \\vert \\tilde{h}\\vert^2 \\ge s \\right ) d s \\\\ & = \\int_0^{\\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2 } \\mathbb p\\left ( \\vert \\tilde{h}\\vert^2 \\ge s \\right ) d s   + \\int_{\\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2}^{+\\infty } \\mathbb p\\left ( \\vert \\tilde{h}\\vert^2 \\ge s \\right ) d s \\\\ & = \\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2   + \\int_{\\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2}^{+\\infty } \\mathbb p\\left ( \\vert \\tilde{h}\\vert \\ge \\sqrt{s } \\right ) d s \\\\ & \\le \\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2   + \\int_{\\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2}^{+\\infty } \\exp \\left(-4 \\ : c   \\left(\\frac{\\sqrt{s}-\\mathbb e[\\vert \\tilde{h}\\vert ] } { \\mathbb e[\\vert \\tilde{h}\\vert ] } \\right)^{2}\\ : t\\right)\\ : d s\\end{aligned}\\ ] ] and making the change of variable @xmath190)^2 $ ] , we obtain @xmath145 & = \\int_0^{+\\infty } \\mathbb p\\left ( \\vert \\tilde{h}\\vert^2 \\ge s \\right ) d s \\\\ & \\le \\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2   + \\int_{0}^{+\\infty } \\exp \\left(-4 \\ :    \\frac{ct}{\\mathbb e[\\vert \\tilde{h}\\vert ] ^2 } \\ : r\\right)\\ : \\left(1+\\frac1{\\sqrt{r}}\\right)\\ : d r \\\\ & \\le \\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2   + 2 \\ : \\int_{0}^{+\\infty } \\exp \\left(-4 \\ :    \\frac{ct}{\\mathbb e[\\vert \\tilde{h}\\vert ] ^2 } \\ : r\\right)\\ : d r +   \\int_{0}^{\\mathbb e[\\vert \\tilde{h}\\vert ] ^2 } \\frac1{\\sqrt{r}}\\ : d r.\\end{aligned}\\ ] ] thus , we obtain @xmath145   & \\le \\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2+\\mathbb e[\\vert \\tilde{h}\\vert ]   -     \\frac{\\mathbb e[\\vert \\tilde{h}\\vert ] ^2}{2 \\ : ct}\\ : \\left [ \\exp \\left(-4 \\ :    \\frac{ct}{\\mathbb e[\\vert \\tilde{h}\\vert ] ^2 } \\ : r\\right)\\right]_0^{+\\infty } \\\\ & \\le \\left(1+\\frac{1}{2 \\ : ct}\\right)\\mathbb e\\left[\\vert \\tilde{h}\\vert\\right]^2+\\mathbb e[\\vert \\tilde{h}\\vert ] .\\end{aligned}\\ ] ] this completes the proof .    [",
    "[ some - properties - of - sigma - sigmah - mathcal - m - mathcal - s - and - mathcal - t ] ] some properties of @xmath59 , @xmath70 , @xmath72 , @xmath77 and @xmath79 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      the spectrum of @xmath59 can be studied using the methods of grenander and szego @xcite . in @xcite ,",
    "the classical results are extended to the case of generalized fractional processes .",
    "it was shown in particular by grenander and szego in ( * ? ? ?",
    "* chapter 5 ) that @xmath191 for any eigenvalue @xmath192 of @xmath59 , where @xmath193 and @xmath194 are the essential infimum and supremum of the spectral density function @xmath195 of the process . for arma processes ,",
    "this function is just @xmath196 where @xmath197      recall that @xmath63 is the random matrix whose components are given by @xmath198 where @xmath65 , @xmath66 and @xmath67 are independent rademacher random variables which are independent of @xmath68 , @xmath69 .",
    "+ & = & 0 for @xmath207 .",
    "similarly , we can show that @xmath208}=0 $ ] for @xmath209 . as for @xmath210",
    ", we have @xmath211 = i_{t-2t+1}$ ] . thus ^h_[p , p ] = e[^t ] = ( t-2t+1)i_t it is then follows that @xmath212 .      recall that @xmath72 denotes",
    "the operator defined by @xmath73 and @xmath74 denotes the adjoint of the inverse of @xmath72 . using the resuts of section [ specsigh ]",
    ", we obtain that @xmath213 and @xmath214    using these results , we obtain that @xmath77 is the operator defined by @xmath215 and @xmath79 is the mapping @xmath216 we thus have the following results on @xmath79 .",
    "@xmath217 and @xmath218 we also obtain that @xmath219"
  ],
  "abstract_text": [
    "<S> the problem of estimating arma models is computationally interesting due to the nonconcavity of the log - likelihood function . </S>",
    "<S> recent results were based on the convex minimization . </S>",
    "<S> joint model selection using penalization by a convex norm , e.g. the nuclear norm of a certain matrix related to the state space formulation was extensively studied from a computational viewpoint . </S>",
    "<S> the goal of the present short note is to present a theoretical study of a nuclear norm penalization based variant of the method of @xcite under the assumption of a gaussian noise process .    </S>",
    "<S> * keywords : * arma models , time series , low rank model , prediction , nuclear norm penalization . </S>"
  ]
}