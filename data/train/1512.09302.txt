{
  "article_text": [
    "in this paper , we consider the following optimization problem : @xmath2 where @xmath3 is a proper closed convex function and @xmath4 is a possibly nonconvex function that has a lipschitz continuous gradient .",
    "we also assume that the proximal operator of @xmath5 , i.e. , @xmath6 is easy to compute for all @xmath7 and any @xmath8 , where @xmath9 denotes the _ unique _ minimizer .",
    "we also assume that the optimal value of is finite and is attained .",
    "problem   arises in many important contemporary applications including compressed sensing @xcite , matrix completion @xcite and image processing @xcite .",
    "since the problem instances are typically of large scale , first - order methods such as the proximal gradient algorithm @xcite are used for solving them , whose main computational efforts per iteration are the evaluations of the gradient of @xmath4 and the proximal mapping of @xmath5 . for the proximal gradient algorithm , when @xmath4 is in addition convex ,",
    "it is known that @xmath10 where @xmath11 is generated by the proximal gradient algorithm ; see , for example , ( * ? ? ? * theorem  1(a ) ) .",
    "however , the proximal gradient algorithm , in its original form , can be slow in practice ; see , for example , ( * ? ? ?",
    "* section 5 ) .",
    "various attempts have thus been made to accelerate the proximal gradient algorithm .",
    "one simple and often efficient strategy is to perform extrapolation , where _ momentum _ terms involving the previous iterations are added to the current iteration .",
    "a prototypical algorithm takes the following form @xmath12 where @xmath7 is a constant that depends on the lipschitz continuity modulus of @xmath13 , and the extrapolation coefficients @xmath14 satisfy @xmath15 for all @xmath16 .",
    "a recent example is the fast iterative shrinkage - thresholding algorithm ( fista ) proposed by beck and teboulle @xcite , which is based on nesterov s extrapolation techniques @xcite and is designed for solving with @xmath4 being convex and @xmath3 being continuous .",
    "their analysis can be directly extended to the case when @xmath3 is a proper closed convex function .",
    "the same algorithm was also independently proposed and studied by nesterov @xcite .",
    "fista takes the form and requires @xmath17 to satisfy a certain recurrence relation .",
    "it was shown in @xcite that this algorithm exhibits a faster convergence rate than the proximal gradient algorithm , which is @xmath18 where @xmath11 is generated by fista .",
    "many accelerated proximal gradient algorithms based on nesterov s extrapolation techniques have been proposed since then , and we refer the readers to @xcite and the references therein for an overview of these algorithms .    the faster convergence rate of fista in terms of objective values motivates subsequent studies on the extrapolation scheme ; see , for example , @xcite .",
    "particularly , odonoghue and cands @xcite proposed an adaptive restart scheme for @xmath14 based on fista for solving with @xmath4 being convex and @xmath19 .",
    "specifically , instead of following the recurrence relation of @xmath14 in fista for all @xmath16 , they reset @xmath20 every @xmath21 iterations , where @xmath21 is a positive number .",
    "they established _ global _ linear convergence of the function values when @xmath4 is strongly convex if @xmath21 is sufficiently large .",
    "their algorithm is robust against errors in the estimation of the strong convexity modulus of @xmath4 ; see the discussion in ( * ? ? ?",
    "* section  2.1 ) .",
    "later , attouch and chbani @xcite , and independently , chambolle and dossal @xcite , established the convergence of the whole sequence generated by for solving when @xmath4 is convex and @xmath22 for any fixed @xmath23 .",
    "more recently , tao , boley and zhang @xcite established local linear convergence of fista applied to the lasso ( i.e. , @xmath4 is a least squares loss function and @xmath3 is a positive multiple of the @xmath24 norm ) under the assumption that the problem has a unique solution that satisfies strict complementarity .",
    "johnstone and moulin @xcite considered with @xmath4 being convex , and showed that the whole sequence generated by is convergent by assuming that the extrapolation coefficients @xmath25 satisfy @xmath26 for some @xmath27 .",
    "moreover , by imposing uniqueness of the optimal solution together with a technical assumption , they showed that the sequence generated by is locally linearly convergent when applied to the lasso for a particular choice of @xmath17 .    despite the rich literature",
    ", we note that the local linear convergence of is only established for a certain type of convex problems with _",
    "unique _ optimal solutions for some specific choices of @xmath17 , which can be restrictive for practical applications .",
    "thus , in this paper , we further study the behavior of the sequence @xmath11 generated by . specifically , we discuss local linear convergence under more general conditions in the possibly nonconvex case .    in details , under the same error bound condition used in @xcite for analyzing convergence of the proximal gradient algorithm ,",
    "we show that there is a threshold @xmath28 depending on @xmath4 so that if @xmath29 , then the sequence @xmath11 generated by converges @xmath0-linearly to a stationary point of and the sequence of the objective value @xmath30 is also @xmath0-linearly convergent . in particular ,",
    "if @xmath4 is in addition convex , then @xmath28 reduces to @xmath1 and we can conclude that the sequence @xmath11 generated by fista with fixed restart is @xmath0-linearly convergent to an optimal solution of ; see section  [ sec3.3 ] . the error bound condition is satisfied for a wide range of problems including the lasso , and hence our linear convergence result concerning with a fixed @xmath31 is more general than those discussed in @xcite .",
    "the rest of this paper is organized as follows .",
    "section  [ sec2 ] presents some basic notation and preliminary materials . in section  [ sec3 ] ,",
    "we establish linear convergence of the iterates generated by the proximal gradient algorithm with extrapolation under the same error bound condition used in @xcite .",
    "linear convergence of the corresponding sequence of function values is also established .",
    "fista with restart is discussed in section  [ sec3.3 ] . in section 4 , we perform numerical experiments to illustrate our results .",
    "throughout this paper , we use @xmath32 to denote the @xmath33-dimensional euclidean space , with its standard inner product denoted by @xmath34 .",
    "the euclidean norm is denoted by @xmath35 , the @xmath24 norm is denoted by @xmath36 and the @xmath37 norm is denoted by @xmath38 .",
    "the vector of all ones is denoted by @xmath39 , whose dimension should be clear from the context . for a matrix @xmath40",
    ", we use @xmath41 to denote its transpose . finally ,",
    "for a symmetric matrix @xmath42 , we use @xmath43 and @xmath44 to denote its largest and smallest eigenvalue , respectively .    for a nonempty",
    "closed set @xmath45 , its indicator function is defined by @xmath46 moreover , we use @xmath47 to denote the distance from @xmath48 to @xmath49 , where @xmath50 .",
    "when @xmath51 is in addition convex , we use @xmath52 to denote the unique closest point on @xmath49 to @xmath48 .",
    "the domain of an extended - real - valued function @xmath53 $ ] is defined as @xmath54 .",
    "we say that @xmath55 is proper if it never equals @xmath56 and @xmath57 .",
    "such a function is closed if it is lower semicontinuous .",
    "a proper closed function @xmath55 is said to be level bounded if the lower level sets of @xmath55 are bounded , i.e. , the set @xmath58 is bounded for any @xmath59 . for a proper closed convex function @xmath60 , the subdifferential of @xmath55 at @xmath61 is given by @xmath62 we use @xmath63 to denote the proximal operator of a proper closed convex function @xmath55 at any @xmath64 , i.e. : @xmath65 we note that this operator is well defined for any @xmath66 , and we refer the readers to ( * ? ?",
    "* chapter  1 ) for properties of the proximal operator .    for an optimal solution @xmath67 of , the following first - order necessary condition always holds , thanks to ( * ? ? ?",
    "* exercise 8.8(c ) ) : @xmath68 where @xmath13 denotes the gradient of @xmath4 .",
    "we say that @xmath69 is a stationary point of if @xmath69 satisfies in place of @xmath70 ; in particular , any optimal solution @xmath67 of is a stationary point of .",
    "we use @xmath71 to denote the set of stationary points of @xmath72 .",
    "finally , we recall two notions of ( local ) linear convergence , which will be used in our convergence analysis . for a sequence @xmath73",
    ", we say that @xmath73 converges @xmath74-linearly to @xmath75 if there exist @xmath76 and @xmath77 such that @xmath78 and we say that @xmath73 converges @xmath0-linearly to @xmath75 if @xmath79 we state the following simple fact relating the two notions of linear convergence , which is an immediate consequence of the definitions of @xmath74- and @xmath0-linear convergence .",
    "we will use this fact in our convergence analysis .",
    "[ prelim ] suppose that @xmath80 and @xmath81 are two sequences in @xmath82 with @xmath83 for all @xmath16 , and @xmath80 is q - linearly convergent to zero .",
    "then @xmath84 is r - linearly convergent to zero .",
    "in this section , we present the proximal gradient algorithm with extrapolation for solving , and discuss the convergence behavior of the sequence generated by the algorithm .",
    "we recall that in our problem , the function @xmath3 is proper closed convex and @xmath4 has a lipschitz continuous gradient ; moreover , @xmath85 and @xmath86 .",
    "furthermore , we observe that any function @xmath4 whose gradient is lipschitz continuous can be written as @xmath87 , where @xmath88 and @xmath89 are two convex functions with lipschitz continuous gradients .",
    "for instance , one can decompose @xmath4 as @xmath90 for any @xmath91 , where @xmath92 is a lipschitz continuity modulus of @xmath13 .",
    "it is then routine to show that both @xmath93 and @xmath94 are convex functions with lipschitz continuous gradients .",
    "thus , without loss of generality , from now on , we assume that @xmath95 for some convex functions @xmath93 and @xmath94 with lipschitz continuous gradients . for concreteness",
    ", we denote a lipschitz continuity modulus of @xmath96 by @xmath97 , and a lipschitz continuity modulus of @xmath98 by @xmath99 . moreover , by taking a larger @xmath100 if necessary , we assume throughout that @xmath101 . then it is not hard to show that @xmath13 is lipschitz continuous with a modulus @xmath100 .",
    "we are now ready to present our proximal gradient algorithm with extrapolation .",
    "we shall discuss the convergence behavior of algorithm 1 .",
    "we note first that it is immedidate from the definition of the proximal operator that the @xmath48-update in is equivalently given by @xmath102 this fact will be used repeatedly in our convergence analysis below .",
    "our analysis also relies heavily on the following auxiliary sequence : @xmath103 for a fixed @xmath104 $ ] with @xmath105 , where @xmath11 is generated by algorithm 1 .",
    "we study the convergence properties of @xmath106 in section  [ sec3.1 ] .",
    "the results will then be used in subsequent subsections for analyzing the convergence of @xmath11 and @xmath30 .",
    "the auxiliary sequence was also used in @xcite for analyzing .",
    "we start by showing that @xmath106 is nonincreasing and convergent .",
    "[ sucgo0]let @xmath107 be a sequence generated by algorithm 1 and @xmath108 $ ] . then the following statements hold .    1 .   for any @xmath109 , we have @xmath110 2 .",
    "it holds that for all @xmath16 , @xmath111 3 .",
    "the sequence @xmath106 is nonincreasing .",
    "we first prove ( i ) .",
    "fix any @xmath109 . using the definition of @xmath112 in and the strong convexity of the objective in the minimization problem , we obtain upon rearranging terms that @xmath113 on the other hand , using the fact that @xmath13 is lipschitz continuous with a lipschitz continuity modulus @xmath100 , we have @xmath114 summing and , we see further that @xmath115 next , recall that @xmath116 .",
    "hence , we have @xmath117 since @xmath88 and @xmath89 are convex and their gradients are lipschitz continuous with moduli @xmath100 and @xmath118 , respectively , the following two inequalities hold .",
    "@xmath119 combining these relations with and recalling that @xmath95 , we see further that @xmath120 summing and , and recalling that @xmath121 , we obtain immediately .",
    "this proves ( i ) .    we now prove ( ii ) .",
    "we note first from the definition of the @xmath122-update in that @xmath123 . using this and with @xmath124",
    ", we obtain that @xmath125 from this and the definition of @xmath126 from , we see further that @xmath127 which is just .",
    "this proves ( ii ) . finally , since @xmath128 by our assumption",
    ", we have @xmath129 consequently , @xmath130 , i.e. , @xmath106 is nonincreasing .",
    "this completes the proof .",
    "the following result is an immediate consequence of lemma  [ sucgo0 ] .",
    "[ xbound]the sequence @xmath107 generated by algorithm 1 is bounded if @xmath72 is level bounded .    from lemma [ sucgo0 ] ,",
    "the sequence @xmath131 is nonincreasing .",
    "this together with the definition of @xmath132 implies that @xmath133 since @xmath72 is level bounded by assumption , we conclude that @xmath107 is bounded .",
    "[ ssucgo0]let @xmath107 be a sequence generated by algorithm 1 , and @xmath108 $ ] . then the following statements hold .    1 .",
    "the sequence @xmath106 is convergent .",
    "2 .   @xmath134 .",
    "recall that @xmath85 .",
    "hence , @xmath135 is bounded from below .",
    "this together with the fact that @xmath106 is nonincreasing from lemma  [ sucgo0 ] implies that @xmath106 is convergent .",
    "this proves ( i ) .    we now prove ( ii ) . since @xmath136",
    ", we have from that @xmath137 summing both sides of from @xmath1 to @xmath138 , we see further that @xmath139 where the nonnegativity follows from the fact that @xmath140 for all @xmath16 . since @xmath106 is convergent by ( i ) , letting @xmath141 in , we conclude that the infinite sum exists and is finite , i.e. , @xmath142 this completes the proof .    in the next lemma",
    ", we show that when @xmath17 is chosen below a certain threshold , then any accumulation point of the sequence @xmath11 generated by algorithm 1 , if exists , is a stationary point of @xmath72 .",
    "this result has been established in @xcite when the function @xmath4 is convex .",
    "indeed , in the convex case , it was shown in ( * ? ? ?",
    "* theorem  4.1 ) that the whole sequence @xmath11 is convergent . however , the following convergence result is new when the function @xmath4 is nonconvex .",
    "[ sucgo01]suppose that @xmath143 and @xmath107 is a sequence generated by algorithm 1",
    ". then the following statements hold .",
    "1 .   @xmath144 .",
    "any accumulation point of @xmath107 is a stationary point of @xmath72 .    since @xmath143",
    ", one can choose @xmath145 .",
    "then @xmath146 for all @xmath16 , and the conclusion in ( i ) follows immediately from lemma  [ ssucgo0 ] ( ii ) .",
    "we next prove ( ii ) .",
    "let @xmath147 be an accumulation point .",
    "then there exists a subsequence @xmath148 such that @xmath149 . using the first - order optimality condition of the minimization problem ,",
    "we obtain @xmath150 combining this with the definition of @xmath151 , which is @xmath152 , we see further that @xmath153\\in \\nabla f(y^{k_{i}})+\\partial g(x^{k_{i}+1}).\\ ] ] passing to the limit in , and invoking @xmath154 from ( i ) together with the continuity of @xmath13 and the closedness of @xmath155 ( see , for example , ( * ? ? ?",
    "* page  80 ) ) , we have @xmath156 meaning that @xmath147 is a stationary point of @xmath72 .",
    "this completes the proof .",
    "let @xmath157 be the set of accumulation points of the sequence @xmath107 generated by algorithm 1 .",
    "then , from corollary  [ xbound ] and lemma  [ sucgo01 ] @xmath158 , we have @xmath159 when @xmath72 is level bounded",
    ". we prove in the next proposition that @xmath72 is constant over @xmath157 if @xmath17 is chosen below a certain threshold . since @xmath72 is only assumed to be lower semicontinuous , this conclusion is nontrivial when @xmath72 has stationary points that are not globally optimal .",
    "[ hconstant]suppose that @xmath143 and @xmath107 is a sequence generated by algorithm 1 with its set of accumulation points denoted by @xmath157",
    ". then @xmath160 exists and @xmath161 on @xmath157 .",
    "fix any @xmath162 , which exists because @xmath143 .",
    "then , in view of lemmas [ ssucgo0 ] and [ sucgo01 ] , the sequence @xmath106 is convergent and @xmath163 .",
    "these together with the definition of @xmath126 imply that @xmath164 exists .",
    "we denote this limit by @xmath165 .",
    "we now show that @xmath166 on @xmath157 . if @xmath167 , then the conclusion holds trivially .",
    "otherwise , take any @xmath168 .",
    "then there exists a convergent subsequence @xmath148 with @xmath169 . from the lower semicontinuity of @xmath72 and the definition of @xmath165",
    ", we have @xmath170 on the other hand , using the definition of @xmath171 as the minimizer in , we see that @xmath172 adding @xmath173 to both sides of , we obtain further that @xmath174 next , recall that @xmath175 .",
    "thus , we have @xmath176 in addition , we also have @xmath177 since @xmath178 and @xmath179 , it follows from and that @xmath180 and hence @xmath181 . from these and",
    ", we obtain that @xmath182 thus @xmath183 from and .",
    "since @xmath168 is arbitrary , we see that @xmath161 on @xmath157 .",
    "this completes the proof .      in this subsection , we establish local linear convergence of @xmath107 and @xmath30 under the following assumption .",
    "[ errorbound ]    1 .   *",
    "( error bound condition ) * for any @xmath184 , there exist @xmath185 and @xmath186 such that @xmath187 whenever @xmath188 and @xmath189 .",
    "there exists @xmath190 , such that @xmath191 whenever @xmath192 , @xmath193 .",
    "the above assumption has been used in the convergence analysis of many algorithms , including the gradient projection and block coordinate gradient descent method , etc ; see , for example , @xcite and the references therein .",
    "the assumption consists of two parts : the first part is an error bound condition , while the second part states that when restricted to @xmath194 , the isocost surfaces of @xmath72 are properly separated . under our blanket assumptions on @xmath72 ,",
    "assumption  [ errorbound ] is known to be satisfied for many choices of @xmath4 and @xmath3 , including :    * @xmath195 , and @xmath3 is a polyhedral function , where @xmath55 is twice continuously differentiable on @xmath32 with a lipschitz continuous gradient , and on any compact convex set , @xmath55 is strongly convex ; see , ( * ? ? ?",
    "* theorem  2.1 ) and ( * ? ? ?",
    "* lemma  6 ) .",
    "this covers the well - known lasso ; * @xmath4 is a possibly nonconvex quadratic function , and @xmath3 is a polyhedral function ; see , for example , ( * ? ? ?",
    "* theorem  4 ) .",
    "the first example is convex , while the second one is possibly nonconvex .",
    "we refer the readers to @xcite and the references therein for more examples and discussions on the error bound condition .",
    "we next show that @xmath106 is @xmath74-linearly convergent under assumption  [ errorbound ] .",
    "our analysis uses ideas from the proof of ( * ? ? ?",
    "* theorem  2 ) , which studied a block coordinate gradient descent method .",
    "[ hq1 ] suppose that @xmath143 , @xmath162 and that assumption [ errorbound ] holds .",
    "let @xmath107 be a sequence generated by algorithm 1 .",
    "then the following statements hold .",
    "1 .   @xmath196 .",
    "the sequence @xmath106 is @xmath74-linearly convergent .",
    "first we prove ( i ) .",
    "observe that @xmath197 we now derive an upper bound for the first term on the right hand side of . to this end , using the nonexpansiveness property of the proximal operator ( see , for example , ( * ? ? ?",
    "* page  340 ) ) , we have @xmath198 where the last inequality follows from the fact that @xmath13 is lipschitz continuous with modulus @xmath100 . combining , and invoking the definition of @xmath112 in algorithm 1 , we see further that @xmath199 where the last inequality follows from the definition of @xmath200 in and the definition of @xmath201 . since @xmath202 by lemma [ sucgo01 ]",
    ", we conclude from that @xmath203 let @xmath204 .",
    "since @xmath106 is nonincreasing by lemma  [ sucgo0 ] , we must have @xmath205 for all @xmath16 , and consequently @xmath206 for all @xmath16 . in view of this , and assumption [ errorbound ] ( i ) , we see that for @xmath207 , there exist @xmath208 and a positive integer @xmath21",
    "so that for all @xmath209 , we have @xmath210 thus from and , we immediately obtain the conclusion in ( i ) .    we now prove ( ii ) .",
    "take an arbitrary @xmath211 , we have from that @xmath212 choose @xmath213 in to be an @xmath214 so that @xmath215",
    ". then we obtain @xmath216 in addition , recall that @xmath217 by lemma [ sucgo01 ] .",
    "this together with and shows that @xmath218 . in view of this and assumption  [ errorbound ] ( ii )",
    ", it must then hold true that @xmath219 for some constant @xmath165 for all sufficiently large @xmath16 .",
    "thus , for all sufficiently large @xmath16 , we have from that @xmath220 on the other hand , since @xmath221 is a stationary point of so that @xmath222 , we have for all @xmath16 that , @xmath223 using this and the definitions of @xmath72 , @xmath126 and @xmath165 , we see that for all sufficiently large @xmath16 , @xmath224 - \\langle",
    "-\\nabla f(\\bar x^k),x^k - \\bar x^k\\rangle - \\alpha\\|x^k - x^{k-1}\\|^2\\\\    & \\le \\frac{l}{2}\\|x^k - \\bar x^k\\|^2 - \\alpha\\|x^k - x^{k-1}\\|^2 , \\end{split}\\ ] ] where the last inequality follows from the lipschitz continuity of @xmath225 . using this ,",
    "the fact that @xmath217 by lemma  [ sucgo01 ] and the fact that @xmath226 by ( i ) , we deduce that @xmath227 where the equality follows from lemma [ sucgo0 ] ( iii ) .",
    "now , from , and , we see that for all sufficiently large @xmath16 , @xmath228 for some positive constant @xmath229 , where the third inequality follows from the definition of @xmath200 in and the definition of @xmath201 . combining this with the definition of @xmath126 , we obtain further that @xmath230 where @xmath231 , and the nonnegativity is a consequence of . on the other hand ,",
    "let @xmath232 . then @xmath233 and",
    "we see from that @xmath234 combining and , we obtain further that @xmath235 reorganizing , we see that for all sufficiently large @xmath16 , @xmath236 which implies that the sequence @xmath106 is @xmath74-linearly convergent .",
    "this completes the proof .",
    "we are now ready to prove the local linear convergence of the sequences @xmath11 and @xmath30 , using the @xmath74-linear convergence of @xmath106 .",
    "[ linearcon]suppose that @xmath143 and that assumption [ errorbound ] holds .",
    "let @xmath107 be a sequence generated by algorithm 1 .",
    "then the following statements hold .    1 .",
    "the sequence @xmath107 is @xmath0-linearly convergent to a stationary point of @xmath72 .",
    "2 .   the sequence @xmath237 is r - linearly convergent .",
    "fix any @xmath238 , which exists because @xmath143 .",
    "then , in view of lemma  [ hq1 ] , the sequence @xmath106 is @xmath74-linearly convergent . for notational simplicity ,",
    "we denote its limit by @xmath165 .",
    "let @xmath232 .",
    "then @xmath233 and we obtain from that @xmath239 where the last inequality follows from the fact that the sequence @xmath106 is nonincreasing and convergent to @xmath165 , thanks to lemmas  [ sucgo0 ] and [ ssucgo0 ] .",
    "using the above inequality and the fact that the sequence @xmath106 is @xmath74-linearly convergent , we see that there exist @xmath240 and @xmath241 such that @xmath242 for all @xmath16 . consequently , for any @xmath243 , we have @xmath244 showing that @xmath11 is a cauchy sequence and hence convergent . denoting its limit by @xmath67 and passing to the limit as @xmath245 in the above relation , we see further that @xmath246 this means that the sequence @xmath11 is @xmath0-linearly convergent to its limit , which is a stationary point of @xmath72 according to lemma  [ sucgo01 ] .",
    "this proves ( i ) .",
    "next , we prove ( ii ) .",
    "notice that for any @xmath247 , we have from the definition of @xmath126 that @xmath248 where the first inequality follows from the triangle inequality and the fact that the sequence @xmath106 is nonincreasing and convergent to @xmath165 according to lemmas  [ sucgo0 ] and [ ssucgo0 ] , and the second inequality follows from .",
    "this together with the @xmath74-linear convergence of @xmath106 and lemma  [ prelim ] implies the @xmath0-linear convergence of @xmath30 .      in this subsection , we discuss fista with restart .",
    "restart schemes for fista were proposed recently in odonoghue and cands @xcite , where they adopted as a heuristic an adaptive restart technique , and established global linear convergence of the objective value when applying their method to with @xmath4 being strongly convex and @xmath249 .",
    "the restart techniques have also been adopted in the popular software , tfocs @xcite .",
    "while they did not prove any linear convergence results for convex nonsmooth problems such as the lasso , they stated that for the lasso ,  after a certain number of iterations adaptive restarting can provide linear convergence \" ; see ( * ? ? ?",
    "* page  728 ) . in this subsection",
    ", we will explain that fista equipped with the aforementioned restart schemes is a special case of algorithm 1 .",
    "moreover , when both of their restart schemes are used for the lasso , both the sequences @xmath11 and @xmath30 are @xmath0-linearly convergent .    to proceed , we first present fista @xcite for solving with @xmath4 being in addition convex .    as one of the many variants of nesterov s accelerated proximal gradient algorithms , fista uses a specific choice of @xmath250 .",
    "according to the formula for updating @xmath25 in fista above , it holds that @xmath251 for all @xmath16 . and",
    "@xmath252 in fista , by induction , it is routine to show that @xmath253 and @xmath254 whenever @xmath247 . combining these with the definition of @xmath14 in fista",
    ", we see that @xmath251 for all @xmath16 . ] on the other hand , since @xmath4 is convex , we can choose @xmath255 and thus @xmath256 in algorithm 1 . consequently , fista can be viewed as a special case of algorithm 1 .",
    "fista with restart ( see , for example , @xcite ) is based on fista . here",
    ", we adopt the same restart schemes as in @xcite : fixed restart and adaptive restart . in the fixed restart scheme ,",
    "we choose a positive integer @xmath21 and reset @xmath257 every @xmath21 iterations , while in the adaptive restart ( gradient scheme ) , we reset @xmath258 whenever @xmath259 ; see ( * ? ? ?",
    "* eq .  13 ) .",
    "clearly , whenever the fixed restart scheme is invoked , we will have @xmath260 .",
    "thus , we have the following immediate corollary of theorem  [ linearcon ] .    [ propfista ]",
    "suppose that @xmath4 in is convex and assumption  [ errorbound ] holds .",
    "let @xmath11 be a sequence generated by fista with the fixed restart scheme or both the fixed and adaptive restart schemes",
    ". then    1 .",
    "@xmath11 converges @xmath0-linearly to a globally optimal solution of .",
    "@xmath30 converges @xmath0-linearly to the globally optimal value of .    from the discussion following assumption  [ errorbound ] , we see that the objective function in the lasso satisfies assumption  [ errorbound ] .",
    "thus , by corollary  [ propfista ] , when the fixed or both the fixed and adaptive restart schemes are used for the lasso , both the sequences @xmath11 and @xmath30 are @xmath0-linearly convergent .    before ending this subsection",
    ", we would like to point out two crucial differences between our corollary  [ propfista ] and the conclusion in @xcite .",
    "first , they concluded _ global _ linear convergence of function values for a special case of where @xmath4 is strongly convex and @xmath19 , while we obtain _ local _ linear convergence for for both @xmath11 and @xmath30 with @xmath4 being convex .",
    "second , their global linear convergence is only guaranteed if @xmath21 is chosen sufficiently large ; see ( * ? ? ?",
    "* eq .  6 ) . on the other hand",
    ", we do not have any restrictions on the number @xmath21 , the width of the restart interval .",
    "in this section , we conduct numerical experiments to study algorithm 1 under different choices of @xmath17 .",
    "we consider three different types of problems : the @xmath24 regularized logistic regression problem , the lasso , and the problem of minimizing a nonconvex quadratic function over a simplex .",
    "the first two problems are convex optimization problems , while the third problem is possibly nonconvex .",
    "we consider three different algorithms for each class of problems .",
    "for the convex problems , we consider algorithm 1 with @xmath261 ( proximal gradient algorithm ) , @xmath14 chosen as in fista , and @xmath14 chosen as in fista with both the fixed and the adaptive restart schemes . on the other hand , for the nonconvex problems , we consider algorithm 1 with @xmath261 ( proximal gradient algorithm ) and @xmath262 .",
    "we also consider fista as a heuristic .",
    "all the numerical experiments are performed in matlab 2014b on a 64-bit pc with an intel(r ) core(tm ) i7 - 4790 cpu ( 3.60ghz ) and 32 gb of ram .      in this subsection",
    ", we consider the @xmath24 regularized logistic regression problem : @xmath263 where @xmath264 , @xmath265 , @xmath266 , with @xmath267 not all the same , @xmath268 and @xmath269 is the regularization parameter .",
    "it is easy to see that is in the form of with @xmath270 where @xmath271 , and @xmath272 is the matrix whose @xmath273th row is given by @xmath274 .",
    "moreover , one can show that @xmath13 is lipschitz continuous with modulus @xmath275 .",
    "thus , in our algorithms below , we take @xmath276 and @xmath255 .    before applying algorithm 1",
    ", we need to show that @xmath277 and the solution set @xmath278 of is nonempty . to this end , we first recall that the dual problem of is given by @xmath279\\\\ \\mathrm{s.t . } & \\|a^\\top u\\|_\\infty \\le \\lambda , ~~ e^\\top u=0 , \\end{array}\\ ] ] where @xmath280 is the matrix whose @xmath273th row is @xmath281 .",
    "it can be shown that the optimal values of and are the same , and that an optimal solution of exists ; see , for example , ( * ? ? ?",
    "* theorem  3.3.5 ) .",
    "in addition , we note that because @xmath282 and @xmath267 are not all the same , the generalized slater condition is satisfied for , i.e. , there exists @xmath283 satisfying @xmath284 , @xmath285 and @xmath286 for @xmath287 . hence , by ( * ? ? ?",
    "* corollary 28.2.2 ) , an optimal solution of exists .",
    "consequently , @xmath277 and the solution set @xmath278 of is nonempty .",
    "thus , algorithm 1 is applicable .",
    "in addition , from the discussion following assumption  [ errorbound ] , assumption  [ errorbound ] is satisfied for .",
    "hence , one should expect @xmath0-linear convergence of the sequences @xmath11 and @xmath30 generated by fista with restart , in view of corollary  [ propfista ] .",
    "we now perform numerical experiments to study algorithm 1 under three choices of @xmath17 : @xmath261 as in the proximal gradient algorithm ( pg ) , @xmath14 chosen as in fista , and @xmath14 chosen as in fista with both the fixed and the adaptive restart schemes , where we perform a fixed restart every @xmath288 iterations ( fista - r500 ) .",
    "we choose @xmath289 in and initialize all three algorithms at the origin .",
    "as for the termination , we make use of the fact that for any @xmath290 , @xmath291 is an optimal solution of ( see , for example , ( * ? ? ?",
    "* theorem  31.3 ) ) .",
    "specifically , we define @xmath292 and terminate the algorithms once the duality gap and the dual feasibility violation are small , i.e. , @xmath293 we also terminate the algorithms when the number of iterations hits @xmath294 .",
    "we consider random instances for our experiments .",
    "for each @xmath295 , @xmath296 and @xmath297 , we generate an @xmath298 matrix @xmath280 with i.i.d .",
    "standard gaussian entries .",
    "we then choose a support set @xmath299 of size @xmath300 uniformly at random , and generate an @xmath300-sparse vector @xmath67 supported on @xmath299 with i.i.d .",
    "standard gaussian entries .",
    "the vector @xmath301 is then generated as @xmath302 , where @xmath303 is chosen uniformly at random from @xmath304 $ ] .",
    "our computational results are presented in figures  [ log3000 ] , [ log5000 ] and [ log8000 ] . in the plot ( a ) of each figure ,",
    "we plot @xmath305 against the number of iterations , where @xmath306 denotes the approximate solution obtained at termination of the respective algorithm ; while in the plot ( b ) of each figure , we plot @xmath307 against the number of iterations , where @xmath308 denotes the minimum of the three objective values obtained from the three algorithms .",
    "we see that both @xmath11 and @xmath30 generated by fista with both fixed and adaptive restart schemes are @xmath0-linearly convergent , which conforms with our theory .",
    "moreover , compared with fista and the proximal gradient algorithm , the algorithm with restart performs better .      in this subsection , we consider the lasso : @xmath309 where @xmath310 and @xmath311 .",
    "we observe that is in the form of with @xmath312 it is clear that @xmath4 has a lipschitz continuous gradient and @xmath313 has compact lower level sets .",
    "thus , we can apply algorithm 1 to solving .",
    "moreover , in view of the discussion following assumption  [ errorbound ] , assumption  [ errorbound ] is satisfied for .",
    "hence , according to corollary  [ propfista ] , one should observe @xmath0-linear convergence of both the sequences @xmath11 and @xmath30 generated by fista with restart .",
    "finally , it is not hard to show that @xmath13 has a lipschitz continuity modulus of @xmath314 . in view of this , in the algorithms below , we take @xmath315 and @xmath316 .    before describing our numerical experiments ,",
    "we recall that @xmath317 , where @xmath318 .",
    "the conjugate function of @xmath55 can then be easily computed as @xmath319 .",
    "hence , the dual problem of is given by @xmath320 it can be shown that the optimal values of and are the same , and moreover , an optimal solution of exists ; see , for example , ( * ? ? ?",
    "* theorem  3.3.5 ) .",
    "this dual problem will be used in developing termination criterion for our algorithms below .",
    "now we perform numerical experiments to study algorithm 1 under the same three choices of @xmath17 as in the previous subsection .",
    "we choose @xmath289 in , initialize all three algorithms at the origin and use the duality gap to terminate the algorithms .",
    "specifically , as in the previous subsection , we make use of the fact that for any optimal solution @xmath147 of , @xmath321 is an optimal solution of .",
    "hence , we define @xmath322 and terminate the algorithms once the duality gap is small , i.e. , @xmath323 we also terminate them when the number of iterations hits @xmath294 .",
    "the problems used in our experiments are generated as follows .",
    "for each @xmath295 , @xmath296 and @xmath297 , we generate an @xmath298 matrix @xmath280 with i.i.d .",
    "standard gaussian entries .",
    "we then choose a support set @xmath299 of size @xmath300 uniformly at random , and generate an @xmath300-sparse vector @xmath67 supported on @xmath299 with i.i.d .",
    "standard gaussian entries .",
    "the vector @xmath301 is then generated as @xmath324 , where @xmath325 has standard i.i.d .",
    "gaussian entries .",
    "the computational results are presented in figures  [ ls3000 ] , [ ls5000 ] and [ ls8000 ] .",
    "we plot @xmath305 against the number of iterations in part ( a ) of each figure , where @xmath306 denotes the approximate solution obtained at termination of the respective algorithm ; additionally , we plot @xmath307 against the number of iterations in part ( b ) of each figure , where @xmath308 denotes the minimum of the three objective values obtained from the three algorithms . as in the previous subsection , we see from the figures that both @xmath11 and @xmath30 generated by fista with both fixed and adaptive restart schemes are @xmath0-linearly convergent , which conforms with our theory . additionally , the algorithm with restart performs better than fista and the proximal gradient algorithm .      in this subsection , we look at problems of the following form , which are possibly nonconvex : @xmath326 where @xmath327 is a symmetric matrix that is not necessarily positive semidefinite , @xmath328 and @xmath300 is a positive number .",
    "this is an example of nonconvex quadratic programming problems , which is an important class of problems in global optimization @xcite .",
    "notice that one can rewrite in the form of by defining @xmath329 where @xmath330 .",
    "moreover , it is clear that @xmath4 has a lipschitz continuous gradient and @xmath331 is level bounded .",
    "hence , algorithm 1 can be applied to solving .",
    "furthermore , from the discussion following assumption  [ errorbound ] , assumption  [ errorbound ] is satisfied for .",
    "consequently , according to theorem  [ linearcon ] , one should expect to see @xmath0-linear convergence of both the sequences @xmath11 and @xmath30 generated by algorithm 1 when @xmath332 .",
    "finally , since @xmath333 , where @xmath334 and @xmath335 are the projections of @xmath280 onto the cone of positive semidefinite matrices and the cone of negative semidefinite matrices , respectively , we see that @xmath95 , where @xmath336 and @xmath337 . in view of this , in our experiments below , we set @xmath338 and @xmath339 so that @xmath100 and @xmath118 are the lipschitz continuity moduli of @xmath96 and @xmath98 , respectively , and @xmath101 .",
    "now we perform numerical experiments to study algorithm 1 with two choices of @xmath17 : @xmath261 ( pg ) and @xmath262 ( pg@xmath340 ) .",
    "in addition , we also perform the same experiments on fista .",
    "we initialize all three algorithms at the origin , and terminate them when the successive changes of the iterates are small , i.e. , @xmath341 we also terminate the algorithms when the number of iterations hits @xmath294 .    our test problem is generated as follows .",
    "we generate a @xmath342 matrix @xmath272 with i.i.d .",
    "standard gaussian entries .",
    "we then generate a symmetric matrix @xmath343 .",
    "finally , the vector @xmath301 is generated with i.i.d .",
    "standard gaussian entries , and @xmath300 is generated as @xmath344 , with @xmath345 chosen uniformly at random from @xmath304 $ ] .",
    "the computational results are presented in figure  [ fig : nonconvex ] .",
    "we plot @xmath346 against the number of iterations in figure  [ fig : nonconvex ] ( a ) , where @xmath306 denotes the approximate solution obtained at termination of the respective algorithm ; in addition , we plot @xmath347 against the number of iterations in figure  [ fig : nonconvex ] ( b ) , with @xmath348 being the minimum of the three objective values obtained from the three algorithms .",
    "we can see from figure  [ fig : nonconvex ] ( a ) that the sequence @xmath11 generated by algorithm 1 with @xmath262 is @xmath0-linearly convergent , which conforms with our theory .",
    "however , from figure  [ fig : nonconvex ] ( b ) , one can see that not all the algorithms are approaching @xmath348 .",
    "this is likely because the iterates generated by the algorithm got stuck at local minimizers .    to further evaluate the quality ( in terms of function values at termination ) of the approximate solution obtained by the algorithms",
    ", we perform a second experiment . in this second experiment ,",
    "we generate random instances as follows : we generate an @xmath349 matrix @xmath272 with i.i.d .",
    "standard gaussian entries and symmetrize it to form @xmath343 ; moreover , we generate a vector @xmath301 with i.i.d .",
    "standard gaussian entries , and an @xmath350 , where @xmath345 is chosen uniformly at random from @xmath304 $ ] .    in our test , for each @xmath351 , @xmath352 , @xmath353 , @xmath354 and @xmath355 , we generate @xmath356 random instances as described above .",
    "the computational results are reported in table  [ t1 ] , where we present the number of iterations averaged over the @xmath356 instances for each @xmath33 ( iter ) , and the function value at termination ( fval ) , also averaged over the @xmath356 instances .",
    "one can see that while algorithm 1 with @xmath262 ( i.e. , pg@xmath340 ) is always the fastest algorithm , the function values obtained can be slightly compromised for some instances .    .comparing pg@xmath340 , fista and pg on random instances . [ cols=\"^,^,^,^,^,^,^ \" , ]",
    "in this paper , we study the proximal gradient algorithm with extrapolation for solving a class of nonconvex nonsmooth optimization problems . based on the error bound condition",
    ", we establish the @xmath0-linear convergence of both the sequence @xmath11 generated by the algorithm and the corresponding sequence of objective values @xmath30 if the extrapolation coefficients are below the threshold @xmath357 .",
    "we further demonstrate that our theory can be applied to analyzing the convergence of fista with the fixed restart scheme for convex problems .",
    "finally , we perform some numerical experiments to illustrate our results ."
  ],
  "abstract_text": [
    "<S> in this paper , we study the proximal gradient algorithm with extrapolation for minimizing the sum of a lipschitz differentiable function and a proper closed convex function . under the error bound condition used in @xcite for analyzing the convergence of the proximal gradient algorithm , we show that there exists a threshold such that if the extrapolation coefficients are chosen below this threshold , then the sequence generated converges @xmath0-linearly to a stationary point of the problem . </S>",
    "<S> moreover , the corresponding sequence of objective values is also @xmath0-linearly convergent . </S>",
    "<S> in addition , the threshold reduces to @xmath1 for convex problems and , as a consequence , we obtain the @xmath0-linear convergence of the sequence generated by fista with fixed restart . </S>",
    "<S> finally , we present some numerical experiments to illustrate our results .    linear convergence , extrapolation , error bound , accelerated gradient method , nonconvex nonsmooth minimization , convex minimization    90c30 , 65k05 , 90c25 , 90c26 </S>"
  ]
}