{
  "article_text": [
    "since the beginning the size and complexity of the universe have been increasing continuously . as a consequence",
    "we observe today large structures consisting of galaxies .",
    "the visible superstructure of the universe is made of baryonic material , e.g. stars and gas .",
    "but the majority of the mass is in the form of dark matter , which is affected by gravity but does not interact electro - magnetically . the best model for the formation and evolution of this superstructure is called @xmath11 cold dark - matter cosmology ( @xmath11cdm)@xcite , according to which the universe is about 13.7 billion years old and comprises of about 4% baryonic matter , @xmath12% non - baryonic ( dark ) matter and 72% ( dark ) energy , indicated by the letter @xmath11 @xcite .",
    "the nature of dark matter is unknown and from an observational perspective it is hard to resolve this lacuna in our understanding .",
    "evidence for dark matter comes from a variety of observations , which includes the rapid rotation of the milky - way and other galaxies : its stars would be flung out in the absence of dark matter .",
    "weakly - interacting massive particles form the most promising explanation for dark matter @xcite , in which case the entire universe is filled with a finely grained substance that behaves like a gravitational fluid but is invisible otherwise . the gravitational force in the newtonian limit @xmath13 , which is a long - range force in particular since the enclosed mass scale @xmath14 . together with the absence of a shielding mechanism ,",
    "even very distant objects can not be ignored .",
    "the way in which dark matter interacts is therefore rather simple and well understood . as a result",
    ", we can effectively study dark matter by simulation and use these results to understand observations and to make predictions .",
    "one of the most favorable techniques to study the formation of large scale structure in the universe is by means of gravitational @xmath0-body simulations , in which each particle in the simulation represents the fluid of dark matter particles .",
    "the most widely used algorithm is the treepm method , in which the short range forces are resolved using a barnes - hut tree code , whereas the long range interactions are simulated using a particle - mesh method @xcite .",
    "this combination of methodologies provide good performance compared to using only a tree code but still at a reasonable accuracy compared to a pure particle - mesh technique , as in the treepm method we resolve short as well as long range forces reasonably well .",
    "in addition , the algorithm scales well to a large number of processors by optimized domain decomposition ( ishiyama et al . in press ) . since simulating the entire universe is not really possible ( yet )",
    "we study a small part , using periodic boundary conditions to mimic _",
    "ad infinitum_.    the computational demand for our large scale cosmological @xmath0-body simulation is enormous , and instead of running on a single supercomputer , we opted for running concurrently on two widely separated supercomputers .",
    "we demonstrate that that it can be efficient to run high - performance production simulations on a computational grid .",
    "the future of large - scale scientific computing infrastructures aims at distributing resources rather than concentrating supercomputers locally @xcite",
    ". the higher cost effectiveness of distributing resources can be efficient for those applications in which the compute time scales steeper than the communication , or when the problem can be decomposed in domains @xcite .    instead of starting the simulation on one location and",
    "switch half way to another computer to continue the calculation , we run the simulation on two supercomputers concurrently .",
    "one of our computers is located in amsterdam ( the netherlands ) and the other is in tokyo ( japan ) .",
    "the challenge is to efficiently use a large number of processors separated by half the planet without running in the inter - processor and intercontinental communication bottlenecks .",
    "if we can run successfully among two supercomputers separated by half the planet , we can be confident about upscaling our virtual organization to include more than two supercomputers .",
    "the management , political and technical issues of coupling the computers with an uncongested network and to schedule the resources proves to be extremely challenging , and one can wonder if it is worth the effort . the preparations for our calculations lasted about a year .",
    "however , running on a single supercomputer would have required its entire capacity for several months , which would probably not have been granted .",
    "acquiring relatively little time on a large number of supercomputers proves to be considerably easier . with the expertise obtained in running on two supercomputers we can now extend to more sites without much additional overhead",
    "of course , the political and technical issues of coupling the computers with an efficient network and to schedule the resources remain challenging , but many of these aspects will become easier once high - performance grid computing becomes mainstream .",
    "additional complications arise by the required hybrid parallelization strategies , the diversity in topologies , scheduling , load balancing and the complications introduced by the different hardware architectures in particular if , as in our case , the code is machine dependent .",
    "one of the complications we encountered is related to the network topology and internal hardware setup .",
    "neither supercomputer is directly connected to the optical network , but communication is realized via a special node that is connected to the outside world with a 10gbe optical switch in amsterdam and neterion nic in tokyo . upon every communication",
    "step each processor : identifies the particles which need to be communicated , packs them and sends the particles to the supercomputer s internal communication node , which subsequently sends the entire package of particles to the communication node outside the firewall .",
    "this package is subsequently transmitted to the distant computer 9,400 km away , as the bird flies .",
    "the data however , travels @xmath15 km as the network links criss - crosses the atlantic ocean , the usa and the pacific ocean . in fig.[fig : network ] we illustrate the network topology . with the speed of light in fiber this distance",
    "is covered in 0.138 seconds , resulting in a round - trip time of 0.277 seconds .",
    "the network communication was realized by configuring two virtual lans  ( vlan ) to create two paths connecting both supercomputers .",
    "one vlan was used for the production traffic and runtime synchronization , while the second was configured for testing purposes and for collecting the simulation data at the pbyte storage array in amsterdam .",
    "both paths are dedicated to our experiments which prevents packet loss , packet reordering and latency changes due to congestion . in the secondary path we had to execute vlans translation in the jng network , because the chosen vlans numbers were unavailable along the entire path length .",
    "the choice of an available vlan number for the end - to - end communication should have been trivial ; but since our multi - domain light path lagged automatic configuration tools , human intervention was necessary .",
    "the time required for solving problems depends on the quick responses to email and phone calls , which is hindered by working over several time zones ; debugging link failure at layer2 in a multi - domain multi - vendor infrastructure is impractical and extremely time consuming .",
    "a self - healing or automatic setup would enormously help future experiments .",
    "the research and education networks ( ren ) provided the links for free , which is motivated by their vision to advertise and broaden their services to the scientific community . in the last few years",
    "many rens have adopted a hybrid model for their architecture , expanding it to routed ip services where users have access to dedicated light paths in which communication occurs at lower layers of the open system - interconnection reference model .",
    "we settled for a flat ethernet path , but still had to make the network reservations 3 weeks in advance of each simulation restart , and then it would generally take about a week before it was fully operational .",
    "our overall experience with the use of light paths however , is quite positive .",
    "we could not have performed our calculation without this type of network setup and certainly perceived the advantage of the unrestricted and exclusive use of the links .      while performing a cosmological @xmath0-body simulation in which the computational domain is shared by two ( or more ) computers , the positions and velocities of all particles on both machines have to be synchronized throughout the simulation .",
    "this introduces an enormous demand on the network between the computers .",
    "luckily only a small boundary layer between the two halves of the universe and the layer nearest the periodic boundary are communicated between amsterdam and tokyo .",
    "the amount of data that has to be communicated per step is then @xmath16 here @xmath0 is the number of particles , @xmath17 is the resolution of the mesh in one dimension and @xmath18 is the sampling rate , which for the production simulation @xmath19 .",
    "the first term is required for communicating the tree structure , the second term is for exchanging the particles in the border region , and the last term is for exchanging the mesh .",
    "the first term in this estimate depends slightly on redshift ( @xmath20 ) and on the opening angle in the treecode ( @xmath21 ) , but is accurate for @xmath22 .",
    "while running , dense dark - matter clumps may not be distributed evenly across the computational domain and load balancing is done by guaranteeing that the calculation time per step is the same on each computer , generating a variable boundary layer between the two computers .",
    "where initially both computer resolve exactly half the universe , in due time one of the two computers tends to deal with a larger volume .",
    "the communication within each of the supercomputers is realized by domain decomposition , using the message passing interface ( mpich @xcite ) . to warrant efficient and stable data transfer we developed the parallel socket library mpwide to facilitate the communication outside the local mpi domains ( groen etal 2009 , in preparation ) .",
    "this communication consists of data transfers between the local compute nodes and the local communication node , and for the data transfer over the light - path between the supercomputers .",
    "the socket library is included in all processes , providing an interface similar to regular mpi .",
    "a static communication topology is established at startup which uses multiple tcp connection for each intra - cluster communication path and multiple tcp connections for paths between supercomputers .",
    "the concurrent use of multiple streams is realized by running a separate thread for each stream . for the smaller runs we used 16 tcp streams , and 64 streams for the larger runs ( see tab.[tab : simulation ] ) .",
    "we adopted the treepm code greem which was initially developed by @xcite and rewritten by ishiyama etal ( in press ) to run efficiently on the selected hardware .",
    "the equations of motion are integrated in co - moving coordinates using the leap - frog scheme with a shared but variable time - step .",
    "good performance at sufficient accuracy is then achieved when the time step is at least an order of magnitude smaller than the crossing time in the densest halo @xcite .",
    "spurious relaxation effects in the high - density clumps are prevented by introducing a plummer softening to the force , which is comparable to the local inter - particle distance .",
    "since most simulation time is spent in calculating the gravitational forces between particles , we optimize this operation using the single precision x86 - 64 streaming simd extensions ( sse ) .",
    "the inverse square - root sse instruction provides the best speedup in calculating newtonian gravity .",
    "we further improve performance by minimizing ram access through the 16  xmm registers for the force operations , and by operating on pairs of two 64-bit floating point words concurrently ( nitadori & yoshikawa in preparation ) .    with these optimizations the power6 with symmetric",
    "multiprocessing is per core is about 4.2% slower than the intel based cray xt4 .",
    "this small difference in speed is achieved by adopting the x86 sse version and the power altivec architectures , but for the former we use in - line assembly whereas for the latter we adopted the intrinsic functions ( nitadori & yoshikawa in preparation ) .",
    "we measure the performance of the code using the amsterdam and tokyo machines separately with @xmath23 to @xmath24 processors and @xmath25 to @xmath26 particles .",
    "the wall clock time @xmath27 is then fitted by @xmath28 seconds , with a ten fold increase of @xmath27 for every increase of the number of particles by @xmath29 .",
    "for relatively small @xmath0 we lose scalability with respect to the number of processors for @xmath30 and @xmath31 .",
    "we are interested in structures of kpc to mpc size . to minimize the effect of the periodic boundary conditions on such dark matter distributions we selected our simulation box at @xmath32 and today @xmath33 . ] to have sides of 30mpc .",
    "the initial dark - matter distribution is generated at @xmath34 , assuming that the relation between the velocity and the potential in zeldovich approximation is the same as in the linear theory @xcite .",
    "the density field is then realized by multi - scale gaussian random fields , which is described in terms of its power spectrum and was generated using mpgrafic @xcite .",
    "we further adopted cosmological parameters which are consistent the 5-year wmap results @xcite .",
    "for clarity we opted for the nearest round values , which yields : matter ( including dark ) density @xmath35 , dark energy density @xmath36 , the slope for the scalar perturbation spectrum @xmath37 , and the amplitude of fluctuations @xmath38 . with these parameters",
    "the universe is about 13.7 billion years old and the hubble constant @xmath39km / s / mpc .",
    "we ran several realizations at different resolution ( in mass as well as spatially ) to measure the performance before we completed a simulation to @xmath32 .",
    "an overview of the performed simulations is presented in tab.[tab : simulation ] .",
    ".the performed simulations in a computational box of 30mpc starting at @xmath40 , a softening of 300pc and the opening angle in the tree - code @xmath41 for @xmath42 after which @xmath22 .",
    "the first column gives the number of particles in the simulation followed by the number of mesh - cells in one dimension .",
    "then we give the number of processors in amsterdam ( a ) and tokyo ( t ) .",
    "the next two columns give the average wall - clock time for one step ( @xmath43 ) and the time spent computing the forces ( @xmath27 ) , both in seconds .",
    "the last two columns gives the speed - up @xmath44 which is defined as the wall - clock time of a single computer as fraction of the wall - clock time of the grid of supercomputers .",
    "here we define @xmath44 for a grid where each computer has the same number of processors @xmath45 . for the last two entries",
    "@xmath44 is then not properly defined .",
    "[ tab : simulation ] [ cols=\">,>,^ , > , > , > , > \" , ]     the simulation with @xmath46 took about 10 hours to complete from @xmath34 to @xmath32 . in fig.[fig : cputimevsz ] we show the wall - clock time of this run decomposed in the time spend in calculation , communication and storing the data to the file system .",
    "about 25% of the time is spent in communication ( see tab.[tab : simulation ] ) , which is roughly constant through the simulation .",
    "we encountered a few problems between @xmath47 and @xmath48 , which are probably related to packet loss in the network transport protocol suite and resulted in an additional performance loss of a few per - cent .    during the communication",
    "the speed of data transfer averages 21.1mbit / s with peaks of about 7gbit / s and per step between 16 mb and 26 mb is transferred .",
    "for the larger runs the average throughput increases to about 208mbit / s since the size of the packages increases with @xmath0 ( see eq.[eq : commsize ] ) .",
    "the result of the simulation with @xmath46 at @xmath32 is presented in two panels in fig.[fig : cgn256qtoz0 ] .",
    "each of the two panels show the universe as it is seen by the two supercomputers , with the black parts on the right of the left panel ( left on the right image ) indicate the part of the universe that resides on the other machine .",
    "the formation of large scale structure in the universe can be studied effectively by means of simulation but requires phenomenal computer power . even with fully optimized numerical methods and approximations the wall - clock time for such simulations can exceed several months .",
    "our largest simulation with @xmath49 has been running for about 1 million cpu hours ( more than 1 month on 1024 processors ) to reach @xmath50 and we expect to spend another @xmath51  million cpu hours to reach @xmath32 .",
    "the runs in tab.[tab : simulation ] are performed on a grid of supercomputers . with our implementation",
    "the latency and throughput of the communication poses a relatively small overhead to the calculation cost , in our largest simulation the communication overhead @xmath52% .",
    "we have therewith demonstrated that large - scale structure formation simulations are excellently suited for high - performance grid computing .",
    "the adopted setup would in principle have reduced our wall - clock time by almost a factor two compared to running on a single supercomputer , and our simulations in tab.[tab : simulation ] have indeed effectively benefitted from using the grid .",
    "however , we have spend more than a year preparing and optimizing the code , and acquiring and scheduling the resources , none of which proved to be trivial . on the other hand , if we would not have opted for running on a grid it would have proven extremely difficult to perform the simulations at all since acquiring 1024 processors on a supercomputer for half - year via a normal proposal would be challenging . even acquiring half the resources on two supercomputers with the promise to switch computers half - way the simulation would be difficult . our strategy to run on a grid enabled us to secure the required cpu time on both supercomputers . during this project",
    "we all became very enthusiastic about the computational grid as a high - performance resource , despite the time we have spend with preparations .",
    "the success of our grid is in part a consequence of the realization that latency forms no bottleneck , even if the computers are separated by half the planet .",
    "we can not beat the speed of light , but bandwidth is likely to improve with time , making high - performance grid computing very attractive in the foreseeable future .",
    "much work , however , is needed in improving practical matters , like scheduling issues , network acquisition and cooperation between supercomputer centers , each of which are major bottlenecks .",
    "we seriously consider to perform our next run on a grid with many supercomputers .    based on our measurements for each of the two supercomputers and the intercontinental grid we constructed a performance model , which is composed of two main components : the calculation ( @xmath27 ) and the data transfer over the grid @xmath53 , where @xmath54 is the network latency and @xmath55 is the network throughput ( see eq.[eq : commsize ] ) .",
    "the parameter @xmath56 is introduced to correct for the inefficiencies in our code which require several transmissions ( @xmath56 ) to the other computer . in fig.[fig : prognosis ] we present the results of the performance model based on the characteristics of the adopted computers , network and software environment for @xmath57 .",
    "the speedup is limited by the bandwidth , whereas latency , for which we adopted @xmath58s with @xmath59 , poses no limiting factor to our simulations . reducing the total latency will hardly improve the performance , but improving the throughput by a factor 10 would allow us to use an order of magnitude more processors per supercomputer while still acquiring acceptable speedup .",
    "the two topologies adopted in fig.[fig : prognosis ] are selected based on an ideal setup where the supercomputers are interconnected via a ring topology .",
    "a sub - optimal star topology leads to congestion as the communication tends to go through a single site . with the optimal topology ( solid curves in fig.[fig : prognosis ] ) running on a 10 supercomputers with @xmath60 processors",
    "each results in a speed - up of a factor of @xmath61 , whereas running on 100 supercomputers the speedup would be only a factor of 10 .",
    "the best strategy for performing cold dark matter simulations using a treepm code on a grid appears to be by acquiring @xmath62 processors on @xmath63 supercomputers in a ring network topology .",
    "the main reason for adopting this strategy would be to be able to acquire the required compute resources ; it is considerably easier to obtain @xmath64 processors for an extended period ( @xmath65  year ) from a dozen supercomputer centers , than to acquire the required cpu hours on a single supercomputer .",
    "this strategy , however , would require drastic changes in the co - scheduling policy of the supercomputer centers .    with our cosmological cold dark matter simulation",
    "we make the dream of foster and kesselman @xcite comes true , our calculation benefits from having multiple widely separated computers interconnected with a high - bandwidth network , effectively operating as a single machine .",
    "we are grateful to hans blom , maxine brown , andreas burkert , halden cohn , jonathan cole , tom defanti , jan eveleth , katsuyuki hasabe , douglas heggie , wouter huisman , piet hut , mary inaba , akira kato , walter lioen , kees neggers , breanndan  nuilln , hanno pet , steven rieder , joaching stadel , huub stoffers , yoshino takeshi , thomas tam , jin tanaka , peter tavenier , mark van de sanden , ronald van der pol alan verlo and seiichi yamamoto .",
    "we are grateful to ben moore for offering us a bottle of _ pommery 1998 cuve louise brut _ for the first relevant scientific results coming from this calculation .",
    "this work was supported by nwo ( grants # 643.200.503 and # 639.073.803 ) , the ncf ( project # sh-095 - 08 ) , qoscosgrid ( eu - fp6-ist - fet contract # 033883 ) , nsf irnc , naoj , nova , lkbf and the jsps .",
    "we thank the network facilities of surfnet , masafumi ooe ; ieeaf ; wide ; northwest gigapop and the global lambda integrated facility ( glif ) gole of translight cisco on national lambdarail , translight , starlight , netherlight , t - lex , pacific and atlantic wave .",
    "the calulations were performed on the supercomputers at cray xt4 at the center for computational astrophysics of national astronomical observatory of japan and the national academic supercomputer center sara in amsterdam , the netherlands .",
    "12    natexlab#1#1    , i. , c.  kesselman , and s.  tuecke , `` the anatomy of the grid : enabling scalable virtual organizations , ''",
    "_ `` international journal of high performance computing applications '' _ 2001 , _ 15 _ , 200 .",
    "gropp , w. , e.  lusk , n.  doss , and a.  skjellum , `` a high - performance , portable implementation of the mpi message passing interface standard , '' _ parallel computing _ 1996 , _ 22 _ , 6 , 789828 .    , a.  h. , `` inflationary universe : a possible solution to the horizon and flatness problems , '' _",
    "_ 1981 , _ 23 _ , 347356 .    , r. and j.  eastwood , _ computer simulation using particles _ ,",
    "adan hilger ltd . ,",
    "bristol , uk , 1988 , 540 pp .    ,",
    "g. , a. , s.  portegies zwart , m.  bubak , and p.   sloot , _ towards distributed petascale computing _",
    ", petascale computing : algorithms and applications , by david a. bader ( ed . ) .",
    "chapman & hall / crc computational science series 2008 , 565pp .",
    ", m. and b.  marcos ( 2007 ) , `` quantification of discreteness effects in cosmological n - body simulations .",
    "ii . evolution up to shell crossing , '' _ _ _ 76 _ , 10 , 103505 .    ,",
    "e. , j.  dunkley , m.  r. nolta , c.  l. , `` five - year wilkinson microwave anisotropy probe ( wmap ) observations : cosmological interpretation , '' 2008 , _ arxiv e - prints _ _",
    "803_.    , b.  w. and s.  weinberg ( 1977 ) , `` cosmological lower bound on heavy - neutrino masses , '' _ physical review letters _",
    "_ 39 _ , 165168 .    , s. , c.  pichon , d.  aubert , d.  pogosyan , r.  teyssier , and s.  gottloeber , `` initial conditions for large cosmological simulations , '' _ _ 2008 , _ 178 _ , 179188 .",
    ", d.  n. , r.  bean , o.  dor , et la .",
    "`` three - year wilkinson microwave anisotropy probe ( wmap ) observations : implications for cosmology , '' _ _ 2007 , _ 170 _ , 377408 .    , g. , `` a new parallel n - body gravity solver : tpm , '' _",
    "_ 1995 , _ 98 _ , 355",
    ".    , k. and t.  fukushige , `` pppm and treepm methods on grape systems for cosmological n - body simulations , '' _",
    "_ 2005 , _ 57 _ , 849860 .",
    "[ [ simon - portegies - zwart ] ] simon portegies zwart + + + + + + + + + + + + + + + + + + + + +    is professor in computational astrophysics at the sterrewacht leiden in the netherlands .",
    "he received his ph.d . in astronomy at utrecht university .",
    "his principal scientific interests are high - performance computational astrophysics and the ecology of dense stallar systems .",
    "+ e-mail:spz@strw.leidenuniv.nl , + url : http://www.strw.leidenuniv.nl/@xmath66spz/          is professor of astrophysics at the national astronomical observatory of japan .",
    "he recieved his phd in astronomy at the university in tokyo .",
    "his research interests are stellar dynamics , large - scale scientific simulation and high - performance computing .",
    "+ e-mail:makino@cfca.jp , + url : http://www.artcompsci.org/@xmath66makino      is associate professor of system and network engineering at the university of amsterdam .",
    "he recieved his phd in computer science at the university of amsterdam .",
    "his research interests are optical / switched internet for data - transport in terascale escience applications .",
    "e-mail:delaat@uva.nl , + url : http://www.science.uva.nl/@xmath66delaat      is professor of physics at drexel university in philadelphia , pennsylvania , u.s.a .",
    "he received his ph.d . in astronomy from harvard university .",
    "his principal scientific interests are high - performance computation and the astrophysics of star clusters and galactic nuclei .",
    "+ e-mail:steve@physics.drexel.edu , + url : http://www.physics.drexel.edu/@xmath66steve      is professor in computer science at the university of tokyo .",
    "he received his ph.d . in phisics at the university of tokyo .",
    "his research interests are parallel and distributed system , high - performance computing and networking .",
    "+ e-mail:hiraki@is.s.u-tokyo.ac.nl , + url : http://www - hiraki.is.s.u - tokyo.ac.jp      is a postdoctoral fellow in the computational astrophysics group of portegies zwart .",
    "he received his ph.d . in astrophysics at the university of kiel , germany .",
    "his research interests are stellar dynamics and high - performance computing .",
    "+ e-mail:harfst@strw.leidenuniv.nl          is a senior researcher at the system and network engineering group at the university of amsterdam .",
    "she received her ph.d . in physics from the university of turin in italy .",
    "her research interests are modeling , programming and virtualization of networks for escience applications .",
    "+ e-mail:p.grosso@uva.nl , + url : http://www.science.uva.nl/@xmath66grosso"
  ],
  "abstract_text": [
    "<S> understanding the universe is hampered by the elusiveness of its most common constituent , cold dark matter . almost impossible to observe , </S>",
    "<S> dark matter can be studied effectively by means of simulation and there is probably no other research field where simulation has led to so much progress in the last decade . </S>",
    "<S> cosmological n - body simulations are an essential tool for evolving density perturbations in the nonlinear regime . simulating the formation of large - scale structures in the universe </S>",
    "<S> , however , is still a challenge due to the enormous dynamic range in spatial and temporal coordinates , and due to the enormous computer resources required . </S>",
    "<S> the dynamic range is generally dealt with by the hybridization of numerical techniques . </S>",
    "<S> we deal with the computational requirements by connecting two supercomputers via an optical network and make them operate as a single machine . </S>",
    "<S> this is challenging , if only for the fact that the supercomputers of our choice are separated by half the planet , as one is located in amsterdam and the other is in tokyo . </S>",
    "<S> the co - scheduling of the two computers and the gridification of the code enables us to achieve a 90% efficiency for this distributed intercontinental supercomputer . </S>",
    "<S> we conclude that running cosmological @xmath0-body simulations on a limited number of ( @xmath1 ) processors concurrently on more than 10 supercomputers in ring topology with a high - bandwidth network would provide satisfactory performance and is politicaly favorable regarding the acquisition of the resources .    </S>",
    "<S>   a&a      @xmath2 sterrewacht leiden , p.o . </S>",
    "<S> box 9513 , 2300 ra leiden , the netherlands + @xmath3 department of general and sciences , university of tokyo , tokyo 153 - 8902 , japan + @xmath4 astronomical institute `` anton pannekoek '' , university of amsterdam , amsterdam , the netherlands + @xmath5 section computational science , university of amsterdam , amsterdam , the netherlands + @xmath6 department of astronomy , school of science , university of tokyo , tokyo 113 - 8654 , japan + @xmath7 center for computational astrophysics in tokyo , japan + @xmath8 section system and network engineering science , university of amsterdam , amsterdam . </S>",
    "<S> + @xmath9 department of physics , drexel university , philadelphia , pa 19104 , usa ; + @xmath10 department of creative informatics , graduate school of information science and technology the university of tokyo . </S>",
    "<S> +    _ keywords : _ + computer applications : physical sciences and engineering : astronomy ; + computing methodologies : simulation , modeling , and visualization : distributed </S>"
  ]
}