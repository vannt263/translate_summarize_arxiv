{
  "article_text": [
    "over the past few decades , sparse matrix - vector multiplication ( spmv ) has probably been the most studied sparse blas routine because of its importance in many scientific applications .",
    "the spmv operation multiplies a sparse matrix @xmath0 of size @xmath1 by a dense vector @xmath2 of size @xmath3 and obtains a dense vector @xmath4 of size @xmath5 .",
    "its nave sequential implementation can be very simple , and can be easily parallelized by adding a few pragma directives for the compilers . but",
    "to accelerate large - scale computation , parallel spmv is still required to be hand - optimized with specific data storage formats and algorithms  @xcite .    as a result , a conflict may emerge between the requirements of spmv and other sparse matrix operations such as preconditioning operations  @xcite and sparse matrix - matrix multiplication  @xcite .",
    "the reason is that those operations commonly require matrices stored in the basic formats such as the compressed sparse row ( csr ) .",
    "therefore , when users construct a real - world application , they need to consider a cost of format conversion between the spmv - oriented formats and the basic formats .",
    "unfortunately , this conversion overhead may offset the benefits of using these specialized formats , in particular when only tens of iterations are needed in a solver .",
    "the conversion cost is mainly from the expensive structure - dependent parameter tuning of a storage format .",
    "for example , some block - based formats require finding a good 2d block size  @xcite .",
    "moreover , some hybrid formats  @xcite may need completely different partitioning parameters for distinct input matrices . to avoid the format conversion overhead ,",
    "a few algorithms have concentrated on accelerating csr - based spmv with either row block methods  @xcite or segmented sum methods  @xcite . however , each of the two types of methods has its own drawbacks .",
    "as for the row block methods , despite their good performance for regular matrices , they may provide very low performance for irregular matrices due to unavoidable load imbalance .",
    "in contrast , the segmented sum methods can achieve near perfect load balance , but suffer from high overhead due to more global synchronizations and global memory accesses .",
    "furthermore , none of the above work can avoid an overhead from preprocessing , since certain auxiliary data for the basic csr format have to be generated for better load balancing  @xcite or established primitives  @xcite .",
    "therefore , to be practical , an efficient format must satisfy two criteria : ( 1 ) it should limit format conversion cost by avoiding structure - dependent parameter tuning , and ( 2 ) it should support fast spmv for both regular and irregular matrices .    to meet these two criteria , in this paper , we have designed csr5 ( compressed sparse row 5 ) , a new format directly extending the classic csr format .",
    "the csr5 format leaves one of the three arrays of the csr format unchanged , stores the other two arrays in an in - place tile - transposed order , and adds two groups of extra auxiliary information .",
    "the format conversion from the csr to the csr5 merely needs two tuning parameters : one is hardware - dependent and the other is sparsity - dependent ( but structure - independent ) . because the added two groups of information are usually much shorter than the original three in the csr format , very limited extra space is required .",
    "furthermore , the csr5 format is simd - friendly and thus can be easily implemented on all mainstream processors with the simd units .",
    "because of the structure - independence and the simd utilization , the csr5-based spmv algorithm can bring stable high throughput for both regular and irregular matrices . in this paper",
    ", we make the following contributions :    * we propose csr5 , an efficient storage format with low conversion cost and high degree of parallelism .",
    "* we present a csr5-based spmv algorithm based on a redesigned low - overhead segmented sum algorithm . *",
    "we implement the work on four mainstream devices : cpu , nvidia gpu , amd gpu and intel xeon phi . *",
    "we evaluate the csr5 format in both isolated spmv tests and iteration - based scenarios .",
    "we compare the csr5 with 11 state - of - the - art formats and algorithms on dual - socket intel cpus , an nvidia gpu , an amd gpu and an intel xeon phi . by using 14 regular and 10 irregular matrices as a benchmark suite ,",
    "we show that the csr5 obtains comparable or better performance over the previous work for the regular matrices , and can greatly outperform the prior work for the irregular matrices . as for the 10 irregular matrices , the csr5 obtains average performance improvement of 17.6% , 28.5% , 173.0% and 293.3% ( up to 213.3% , 153.6% , 405.1% and 943.3% ) over the second best work on the four platforms , respectively .",
    "moreover , for iteration - based real - world scenarios , the csr5 format achieves higher speedups because of the fast format conversion . to the best of our knowledge ,",
    "this is the first time that a single storage format can outperform state - of - the - art work on all four modern multicore and manycore processors .",
    "the csr format for sparse matrices consists of three arrays : ( 1 ) ` row_ptr ` array which saves the start and end pointers of the nonzeros of the rows .",
    "it has size @xmath6 , where @xmath5 is the number of rows of the matrix , ( 2 ) ` col_idx ` array of size @xmath7 stores column indices of the nonzeros , where @xmath7 is the number of nonzeros of the matrix , and ( 3 ) ` val ` array of size @xmath7 stores values of the nonzeros .",
    "figure  [ fig.csr ] shows an example .",
    "-16pt        in a given sparse matrix , rows are independent from each other .",
    "therefore an spmv operation can be parallelized on decomposed row blocks .",
    "a logical processing unit is responsible for a row block and stores dot product results of the matrix rows with the vector @xmath2 to corresponding locations in the result @xmath4 . when the simd units of a physical processing unit are available , the simd reduction sum operation can be utilized for higher efficiency .",
    "these two methods are respectively known as the csr - scalar and the csr - vector algorithms , and have been implemented on cpus  @xcite and gpus  @xcite .",
    "algorithm  [ alg.scalar_spmv ] shows a parallel csr - scalar method .",
    "` y[i ] ` @xmath8 ` y[i ] ` @xmath9 ` y[i ] ` @xmath10 ` val[j ] ` @xmath11 ` x[col_idx[j ] ] `    despite the good parallelism , exploiting the scalability in modern multi - processors is not trivial for the row block methods .",
    "the performance problems mainly come from load imbalance for matrices which consist of rows with uneven lengths .",
    "specifically , if one single row of a matrix is significantly longer than the other rows , only a single core can be fully used while the other cores in the same chip may be completely idle . although various strategies , such as data streaming  @xcite , memory coalescing  @xcite , data reordering or reconstruction  @xcite , static or dynamic binning  @xcite and dynamic parallelism  @xcite , have been developed , none of those can fundamentally solve the problem of load imbalance , and thus provide relatively low spmv performance for the csr format .",
    "blelloch et al .",
    "@xcite pointed out that the segmented sum may be more attractive for the csr - based spmv , since it is simd friendly and insensitive to the sparsity structure of the input matrix , thus overcoming the shortcomings of the row block methods .",
    "segmented sum ( which is a special case of the backward segmented scan ) performs a reduction sum operation for the entries in each segment in an array .",
    "a segment has its first entry flagged as ` true ` and the other entries flagged as ` false ` .",
    "algorithm  [ alg.segsum ] lists a serial segmented sum algorithm .",
    "vectorized parallel segmented sum algorithms can be found in  @xcite .    -16pt",
    "@xmath12 @xmath13 ` in[i ] `",
    "@xmath9 ` in[i ] ` @xmath10 ` in[j ] ` @xmath14 ` in[i ] ` @xmath8    ` < idx , sum > ` @xmath9 ` < 0 , array[0 ] > ` ` array[idx ] ` @xmath15 ` sum ` ` < idx , sum > ` @xmath9 ` < i , array[i ] > ` ` sum ` @xmath9 ` sum ` @xmath10 ` array[i ] ` ` array[idx ] `",
    "@xmath15 ` sum `    in the spmv operation , the segmented sum treats each matrix row as a segment and calculates a partial sum for the entry - wise products generated in each row .",
    "the spmv operation using the segmented sum methods consists of four steps : ( 1 ) generating an auxiliary ` bit_flag ` array of size @xmath7 from the ` row_ptr ` array .",
    "an entry in ` bit_flag ` is flagged as ` true ` if its location matches the first nonzero entry of a row , otherwise it is flagged as ` false ` , ( 2 ) calculating all intermediate entries ( i.e. , entry - wise products ) to an array of size @xmath7 , ( 3 ) executing the parallel segmented sum for the array , and ( 4 ) collecting all partial sums to the result vector @xmath4 if a row is not empty .",
    "algorithm  [ alg.segsum_spmv ] lists the pseudocode .",
    "figure  [ fig.seg_sum_spmv ] illustrates an example using the matrix @xmath0 plotted in figure 1 .    ` bit_flag[row_ptr[i ] ] \\gets true ` ` product[j ] ` @xmath9 ` val[j ] ` @xmath11 ` x[col_idx[j ] ] ` ` y[k ] ` @xmath9 0 ` y[k ] ` @xmath9 ` product[row_ptr[k ] ] `    -8pt    we can see that once the heaviest workload , i.e. , step 3 , is parallelized through a fast segmented sum method described in  @xcite , nearly perfect load balance can be expected in all steps of algorithm  [ alg.segsum_spmv ] . however , in this context , the load balanced computation does not mean high performance .",
    "figure  [ fig.brief ] shows that the row block method in cusparse v6.5 can significantly outperform the segmented sum method in cudpp v2.2",
    "@xcite , while doing spmv on relatively regular matrices ( see table  [ tab.suite ] for the used benchmark suite ) .    -8pt    why is this the case ?",
    "we can see that the step 1 is a scatter operation and the step 4 is a gather operation , both from the row space of size @xmath5 .",
    "this prevents the two steps from fusing with the steps 2 and 3 in the nonzero entry space of size @xmath7 . in this case ,",
    "more global synchronizations and global memory accesses may degrade the overall performance .",
    "previous research  @xcite has found that the segmented sum may be more suitable for the coo ( coordinate storage format ) based spmv , since the fully stored row index data can convert the steps 1 and 4 to the nonzero entry space : the ` bit_flag ` array can be generated by comparison of neighbor row indices , and the partial sums in the ` product ` array can be directly saved to @xmath4 since their final locations are easily known from the row index array .",
    "further , yan et al .",
    "@xcite and tang et al .",
    "@xcite reported that some variants of the coo format can also benefit from the segmented sum .",
    "however , it is well known that accessing row indices in the coo pattern brings higher off - chip memory pressure , which is just what the csr format tries to avoid . in the following",
    ", we will show that the csr5-based spmv can utilize both the segmented sum for load balance and the compressed row data for better load / store efficiency . in this way",
    ", the csr5-based spmv can obtain up to 4x speedup ( see figure  [ fig.brief ] ) over the csr - based spmv using the segmented sum primitive  @xcite .    -16pt",
    "to achieve near - optimal load balance for matrices with any sparsity structures , we first evenly partition all nonzero entries to multiple 2d tiles of the same size .",
    "thus when executing parallel spmv operation , a compute core can consume one or more 2d tiles , and each simd lane of the core can deal with one column of a tile",
    ". then the main skeleton of the csr5 format is simply a group of 2d tiles .",
    "the csr5 format has two tuning parameters : @xmath16 and @xmath17 , where @xmath16 is a tile s width and @xmath17 is its height .",
    "in fact , the csr5 format _ only has _ these two tuning parameters .",
    "further , we need extra information to efficiently compute spmv . for each tile , we introduce a tile pointer ` tile_ptr ` and a tile descriptor ` tile_desc ` . meanwhile , the three arrays , i.e. , row pointer ` row_ptr ` , column index ` col_idx ` and value ` val ` , of the classic csr format are directly integrated .",
    "the only difference is that the ` col_idx ` data and the ` val ` data in each complete tile are in - place transposed ( i.e. , from row - major order to column - major order ) for coalesced memory access from contiguous simd lanes .",
    "if the last entries of the matrix do not fill up a complete 2d tile ( i.e. , @xmath18 ) , they just remain unchanged and discard their ` tile_desc ` .    in figure  [ fig.csr5 ] ,",
    "an example matrix @xmath0 of size @xmath19 with @xmath20 nonzero entries is stored in the csr5 format . when @xmath21 and @xmath22 , the matrix is divided into three tiles including two complete tiles of size 16 and one incomplete tile of size 2 .",
    "the arrays ` col_idx ` and ` val ` in the two complete tiles are stored in tile - level column - major order now . moreover , only the first two tiles have ` tile_desc ` , since they are complete .      because the computational power of the modern multicore or manycore processors is mainly from the simd units , we design an auto - tuning strategy for high simd utilization .",
    "first , the tile width @xmath16 is set to the size of the simd execution unit of the used processor .",
    "then an simd unit can consume a 2d tile in @xmath17 steps without any explicit synchronization , and the vector registers can be fully utilized . for the double precision spmv",
    ", we always set @xmath21 for cpus with 256-bit simd units , @xmath23 for the nvidia gpus , @xmath24 for the amd gpus , and @xmath25 for intel xeon phi with 512-bit simd units .",
    "therefore , @xmath16 can be automatically decided once the processor type used is known .",
    "the other parameter @xmath17 is decided by a slightly more complex process .",
    "for a given processor , we consider its on - chip memory strategy such as cache capacity and prefetching mechanism .",
    "if a 2d tile of size @xmath26 can empirically bring better performance than using the other sizes , the @xmath17 is simply chosen .",
    "we found that the x86 processors fall into this category .",
    "for the double precision spmv on cpus and xeon phi , we always set @xmath17 to 16 and 12 , respectively . as for gpus ,",
    "the tile height @xmath17 further depends on the sparsity of the matrix .",
    "note that the `` sparsity '' is not equal to `` sparsity structure '' .",
    "we define `` sparsity '' to be the average number of nonzero entries per row ( or @xmath7/row for short ) .",
    "in contrast , `` sparsity structure '' is much more complex because it includes 2d space layout of all nonzero entries .",
    "on gpus , we have several performance considerations on mapping the value @xmath7/row to @xmath17 .",
    "first , @xmath17 should be large enough to expose more thread - level local work and to amortize a basic cost of the segmented sum algorithm .",
    "second , it should not be too large since a larger tile potentially generates more partial sums ( i.e. , entries to store to @xmath4 ) , which bring higher pressure to last level cache write . moreover , for the matrices with large @xmath7/row , @xmath17 may need to be small .",
    "the reason is that once the whole tile is located inside a matrix row ( i.e. , only one segment is in the tile ) , the segmented sum converts to a fast reduction sum .",
    "therefore , for the @xmath7/row to @xmath17 mapping on gpus , we define three simple bounds : @xmath27 , @xmath28 and @xmath29 .",
    "the first bound @xmath27 is designed to prevent a too small @xmath17 .",
    "the second bound @xmath28 is used for preventing a too large @xmath17 .",
    "but when @xmath7/row is further larger than the third bound @xmath29 , @xmath17 is set to a small value @xmath30",
    ". then we have @xmath31    the three bounds , @xmath27 , @xmath28 and @xmath29 , and the value @xmath30 are hardware - dependent , meaning that for a given processor , they can be fixed for use . for example , to execute double precision spmv on nvidia maxwell gpus and amd gcn gpus , we always set @xmath32 and @xmath33 , respectively . as for future processors with new architectures , we can obtain the four values through some simple benchmarks during initialization , and then use them for later runs .",
    "so the parameter @xmath17 can be decided once the very basic information of a matrix and a low - level hardware are known .",
    "therefore , we can see that the parameter tuning time becomes negligible because @xmath16 and @xmath17 are easily obtained .",
    "this can save a great deal of preprocessing time .      the added tile pointer information ` tile_ptr ` stores the row index of the first matrix row in each tile , indicating the starting position for storing its partial sums to the vector @xmath4 . by introducing ` tile_ptr",
    "` , each tile can find its own starting position , allowing tiles to execute in parallel .",
    "the size of the `",
    "tile_ptr ` array is @xmath34 , where @xmath35 is the number of tiles in the matrix .",
    "for the example in figure  [ fig.csr5 ] , the first entry of tile 1 is located in the @xmath36 row of the matrix , and thus 4 is set as its tile pointer . to build the array , we binary search the index of the first nonzero entry of each tile on the ` row_ptr ` array .",
    "lines 14 in algorithm  [ alg.tile_ptr ] show this procedure .",
    "recall that an empty row has exactly the same row pointer information as its first non - empty right neighbor row ( see the second row in the matrix @xmath0 in figure  [ fig.csr ] ) .",
    "thus for the non - empty rows with an empty left neighbor , we need a specific process ( which is similar to lines 1216 in algorithm  [ alg.segsum_spmv ] ) to store their partial sums to correct positions in @xmath4 .",
    "to recognize whether the specific process is required , we give a hint to the other parts ( i.e. , tile descriptor data ) of the csr5 format and the csr5-based spmv algorithm .",
    "here we set an entry in ` tile_ptr ` to its negative value , if its corresponding tile includes any empty rows .",
    "lines 512 in algorithm  [ alg.tile_ptr ] show this operation .    `",
    "bnd ` @xmath37 ` tile_ptr`[@xmath38 @xmath9",
    "@xmath39 ` tile_ptr`[@xmath38 @xmath9 * break *    if the first tile has any empty rows , we need to store a @xmath40 ( negative zero ) for it . to record @xmath40 , here",
    "we use unsigned 32- or 64-bit integer as data type of the ` tile_ptr ` array .",
    "therefore , we have 1 bit for explicitly storing the sign and 31 or 63 bits for an index . for example , in our design , tile pointer @xmath40 is represented as a binary style ` 1000 ... 000 ' , and tile pointer @xmath41 is stored as ` 0000 ... 000 ' . to the best of our knowledge ,",
    "the index of 31 or 63 bits is completely compatible to most numerical libraries such as intel mkl .",
    "moreover , reference implementation of the recent high performance conjugate gradient ( hpcg ) benchmark  @xcite also uses 32-bit signed integer for problem dimension no more than @xmath42 and 64-bit signed integer for problem dimension larger than that .",
    "thus it is safe to save 1 bit as the empty row hint and the other 31 or 63 bits as a ` real ' row index .      only having the tile pointer is not enough for a fast spmv operation . for each tile",
    ", we also need four extra hints : ( 1 ) ` bit_flag ` of size @xmath26 , which indicates whether an entry is the first nonzero of a matrix row , ( 2 ) ` y_offset ` of size @xmath16 used to further let each column know where the starting point to store its local partial sums is , ( 3 ) ` seg_offset ` of size @xmath16 used to accelerate the local segmented sum inside a tile , and ( 4 ) ` empty_offset ` of unfixed size ( but no longer than @xmath26 ) constructed to help the partial sums to find correct locations in @xmath4 if the tile includes any empty rows .",
    "the tile descriptor ` tile_desc ` is defined to denote a combination of the above four groups of data .",
    "generating ` bit_flag ` is straightforward .",
    "the procedure is very similar to lines 35 in algorithm  [ alg.segsum_spmv ] .",
    "the main difference is that the bit flags are saved in column - major order , which matches the in - place transposed ` col_idx ` and ` val ` .",
    "additionally , the first entry of each tile s ` bit_flag ` is set to ` true ` for sealing the first segment from the top and letting 2d tiles to be independent from each other .",
    "the array ` y_offset ` of size @xmath16 is used to help the columns in each tile knowing where the starting points to store their partial sums to @xmath4 are . in other words ,",
    "each column has one entry in the array ` y_offset ` as a starting point offset for all segments in the same column .",
    "we save a row index offset ( i.e. , relative row index ) for each column in ` y_offset ` .",
    "thus for the @xmath43th column in the @xmath44th tile , by calculating ` tile_ptr[tid ] ` @xmath10 ` y_offset[i ] ` , the column knows where its own starting position in @xmath4 is .",
    "thus the columns can work in a high degree of parallelism without waiting for a synchronization .",
    "generating ` y_offset ` is simple : each column counts the number of ` true`s in its previous columns ` bit_flag ` array .",
    "consider tile 1 in figure  [ fig.csr5 ] as an example : because there are 3 ` true`s in the 1st column , the 2nd column s corresponding value in ` y_offset ` is 3 .",
    "in addition , since there are in total 4 ` true`s in the 1st , 2nd and 3rd columns ` bit_flag ` , tile 1 s ` y_offset[3]`@xmath45 .",
    "algorithm  [ alg.y_offset_n_seg_offset ] lists how to generate ` y_offset ` for a single 2d tile in an simd - friendly way .    `",
    "y_offset`[@xmath43 ] @xmath8 ` y_offset`[@xmath43 ] @xmath9 ` y_offset`[@xmath43 ]",
    "@xmath10 ` bit_flag`[@xmath43][@xmath47 ` tmp_bit`[@xmath43 ] @xmath9 ` tmp_bit`[@xmath43 ] @xmath48 ` bit_flag`[@xmath43][@xmath47 ` seg_offset`[@xmath43 ] @xmath9 @xmath49 ` tmp_bit`[@xmath43 ]    the third array ` seg_offset ` of size @xmath16 is used for accelerating a local segmented sum in the workload of each tile .",
    "the local segmented sum is an essential step that synchronizes partial sums in a 2d tile ( imagine multiple columns in the tile come from the same matrix row ) . in the previous segmented sum ( or segmented scan )",
    "method  @xcite , the local segmented sum is complex and not efficient enough .",
    "thus we prepare ` seg_offset ` as an auxiliary array to facilitate implementation of segmented sum by way of the prefix - sum scan , which is a well optimized fundamental primitive for the simd units .    to generate ` seg_offset `",
    ", we let each column search its right neighbor columns and count the number of contiguous columns without any ` true`s in their ` bit_flag ` . using tile 0 in figure  [ fig.csr5 ] as an example , its 2nd column has one and only one right neighbor column ( the 3rd column ) without any ` true`s in its ` bit_flag ` .",
    "thus the 2nd column s ` seg_offset ` value is 1 . in contrast , because the other three columns ( the 1st , 3rd and 4th ) do not have any ` all ` false ` ' right neighbors , their values in ` seg_offset ` is 0 .",
    "algorithm  [ alg.y_offset_n_seg_offset ] shows how to generate ` seg_offset ` using an simd - friendly method .",
    "algorithm  [ alg.fastsegsum ] and figure  [ fig.fastsegsam ] show the fast segmented sum using ` seg_offset ` and an inclusive prefix - sum scan .",
    "the principle of this operation is that the prefix - sum scan is essentially an increment operation .",
    "once a segment knows the distance ( i.e. , offset ) between its head and its tail , its partial sum can be deduced from its prefix - sum scan results .",
    "therefore , the more complex segmented sum operation in  @xcite can be converted to a faster prefix - sum scan operation ( line 5 ) and a few arithmetic operations ( lines 68 ) .",
    "@xmath12 ` in[i ] ` @xmath9 ` in[i+seg_offset[i ] ] ` @xmath50 ` in[i ] ` @xmath10 ` tmp[i ] `    -16pt    the last array ` empty_offset ` occurs when and only when a 2d tile includes any empty rows ( i.e. , its tile pointer is negative ) .",
    "because an empty row of a matrix has the same row pointer with its rightmost non - empty neighbor row ( recall the second row in the matrix @xmath0 in figure 1 ) , ` y_offset ` will record an incorrect offset for it .",
    "we correct for this by storing correct offsets for segments within a tile .",
    "thus the length of ` empty_offset ` is the number of segments ( i.e. , the total number of ` true`s in ` bit_flag ` ) in a tile .",
    "for example , tile 0 in figure  [ fig.csr5 ] has 4 entries in its ` empty_offset ` since its ` bit_flag ` includes 4 ` true`s .",
    "algorithm  [ alg.empty_offset ] lists the pseudocode that generates ` empty_offset ` for a tile that contains at least one empty row .",
    "@xmath12 @xmath51 @xmath52 `",
    "idx ` @xmath9 @xmath39 ` idx ` @xmath9 ` idx ` @xmath50 ` empty_offset[eid ] ` @xmath9 ` idx ` @xmath53    @xmath12 @xmath54 ` empty_offset[i ] ` @xmath9 ` empty_offset[i ] ` @xmath55",
    "@xmath56      to store the ` tile_desc ` arrays in a space - efficient way , we find upper bounds to the entries and utilize the bit - field pattern .",
    "first , since entries in ` y_offset ` store offset distances inside a 2d tile , they have an upper bound of @xmath57 .",
    "so @xmath58 bits are enough for each entry in ` y_offset ` .",
    "for example , when @xmath59 and @xmath60 , 9 bits are enough for each entry .",
    "second , since ` seg_offset ` includes offsets less than @xmath16 , @xmath61 bits are enough for an entry in this array .",
    "for example , when @xmath59 , 5 bits are enough for each entry .",
    "third , ` bit_flag ` stores @xmath17 1-bit flags for each column of a 2d tile .",
    "when @xmath60 , each column needs 16 bits .",
    "so 30 ( i.e. , @xmath62 ) bits are enough for each column in the example .",
    "therefore , for a tile , the three arrays can be stored in a compact bit - field composed of @xmath16 32-bit unsigned integers .",
    "if the above example matrix has 32-bit integer row index and 64-bit double precision values , only around @xmath63 extra space is required by the three newly added arrays .",
    "the size of ` empty_offset ` depends on the number of groups of contiguous empty rows , since we only record one offset for the rightmost non - empty row with any number of empty rows as its left neighbors .      since we in - place transposed the csr arrays `",
    "col_idx ` and ` val ` , a conversion from the csr5 to the csr is required for doing other sparse matrix operations using the csr format .",
    "this conversion is simply removing ` tile_ptr ` and ` tile_desc ` and transposing ` col_idx ` and ` val ` back to row - major order .",
    "thus the conversion can be very fast .",
    "further , since the csr5 is a superset of the csr , any entry accesses or slight changes can be done directly in the csr5 format , without any need to convert it to the csr format . additionally , some applications such as finite element methods can directly assemble sparse matrices in the csr5 format from data sources .",
    "because all computations of the information ( ` tile_ptr ` , ` tile_desc ` , ` col_idx ` and ` val ` ) of 2d tiles are independent of each other , they can execute concurrently .",
    "on gpus , we assign a bunch of threads ( i.e. , warp in nvidia gpus or wavefront in amd gpus ) for each tile . on cpus and xeon phi",
    ", we use openmp pragma for assigning the tiles to available x86 cores .",
    "furthermore , the columns inside a tile are independent of each other as well .",
    "so we assign a thread on gpu cores or an simd lane on x86 cores to each column in a tile .",
    "while running the csr5-based spmv , each column in a tile can extract information from ` bit_flag ` and label the segments in its local data to three colors : ( 1 ) _ red _ means a sub - segment unsealed from its top , ( 2 ) _ green _ means a completely sealed segment existed in the middle , and ( 3 ) _ blue _ means a sub - segment unsealed from its bottom .",
    "there is an exception that if a column is unsealed both from its top and from its bottom , it is colored to _",
    "red_.    algorithm  [ alg.csr5_spmv ] shows the pseudocode of the csr5-based spmv algorithm .",
    "figure  [ fig.csr5_spmv ] plots an example of this procedure .",
    "we can see that the green segments can directly save their partial sums to @xmath4 without any synchronization , since the indices can be calculated by using ` tile_ptr ` and ` y_offset ` .",
    "in contrast , the red and the blue sub - segments have to further add their partial sums together , since they are not complete segments .",
    "for example , the sub - segments b@xmath64 , r@xmath64 and r@xmath65 in figure  [ fig.csr5_spmv ] have contributions to the same row , thus an addition is required .",
    "this addition operation needs the fast segmented sum shown in algorithm  [ alg.fastsegsum ] and figure  [ fig.fastsegsam ] .",
    "furthermore , if a tile has any empty rows , the ` empty_offset ` array is accessed to get correct global indices in @xmath4 .",
    "-18pt    ` /*use empty_offset[y_offset[i ] ] instead of y_offset[i ] for a tile with any empty rows*/ ` ` sum ` @xmath8 @xmath66 ` sum ` @xmath9 ` sum ` @xmath10 ` val[ptr ] ` @xmath11 ` x[col_idx[ptr ] ] ` ` tmp[i-1 ] ` @xmath9 ` sum ` ` sum ` @xmath8 ` y[tile_ptr[tid ] + y_offset[i ] ] ` @xmath9 ` sum ` ` y_offset[i ] ` @xmath9 ` y_offset[i ] ` @xmath55 ` sum ` @xmath8 ` last_tmp[i ] ` @xmath9 ` sum ` ` //end of a blue sub - segment ` ` last_tmp[i ] ` @xmath9 ` last_tmp[i ] ` @xmath10 ` tmp[i ] ` ` y[tile_ptr[tid ] + y_offset[i ] ]",
    "` @xmath9 ` last_tmp[i ] `    consider the synchronization among the tiles , since the same matrix row can be influenced by multiple 2d tiles running concurrently , the first and the last segments of a tile need to store to @xmath4 by atomic add ( or a global auxiliary array used in device - level reduction , scan or segmented scan  @xcite ) . in figure",
    "[ fig.csr5_spmv ] , the atomic add operations are highlighted by arrow lines with plus signs .    for the last entries not in a complete tile ( e.g. , the last two nonzero entries of the matrix in figure  [ fig.csr5 ] ) , we execute a conventional csr - vector method after all of the complete 2d tiles have been consumed .",
    "note that even though the last tile ( i.e. , the incomplete one ) does not have ` tile_desc ` arrays , it can extract a starting position from ` tile_ptr ` .    in algorithm  [ alg.csr5_spmv ]",
    ", we can see that the main computation ( lines 521 ) only contains very basic arithmetic and logic operations that can be easily programmed on all mainstream processors with simd units . as the most complex part in our algorithm",
    ", the fast segmented sum operation ( line 22 ) only requires a prefix - sum scan , which has been well - studied and can be efficiently implemented by using cuda , opencl or x86 simd intrinsics .",
    "algorithms * dual - socket intel xeon e5 - 2667 v3 * ( haswell , 2@xmath118 cores @ 3.2 ghz , 1.64 sp tflops , 819.2 dp gflops , 64 gb gddr4 , ecc - on , 2@xmath1168.3 gb / s bandwidth ) . & ( 1 ) the csr - based spmv in intel mkl 11.2 update 1 .",
    "( 2 ) bicsb v1.2 using csb  @xcite with bitmasked register block  @xcite .",
    "( 3 ) poski v1.0.0  @xcite using oski v1.0.1h  @xcite kernels .",
    "( 4 ) the csr5-based spmv implemented by using openmp and avx2 intrinsics . * an nvidia geforce gtx 980 * ( maxwell gm204 , 2048 cuda cores @ 1.13 ghz , 4.61 sp tflops , 144.1 dp gflops , 4 gb gddr5 , 224 gb / s bandwidth , driver v344.16 ) . &",
    "( 1 ) the best csr - based spmv  @xcite from cusparse v6.5 and cusp v0.4.0  @xcite.(2 ) the best hyb  @xcite from the above two libraries.(3 ) brc  @xcite with texture cache enabled.(4 ) acsr  @xcite with texture cache enabled .",
    "( 5 ) the csr5-based spmv implemented by using cuda v6.5 . *",
    "an amd radeon r9 290x * ( gcn hawaii , 2816 radeon cores @ 1.05 ghz , 5.91 sp tflops , 739.2 dp gflops , 4 gb gddr5 ,",
    "345.6 gb / s bandwidth , driver v14.41 ) . &",
    "( 1 ) the csr - vector method  @xcite extracted from cusp v0.4.0  @xcite.(2 ) the csr - adaptive algorithm  @xcite implemented in viennacl",
    "v1.6.2  @xcite .",
    "( 3 ) the csr5-based spmv implemented by using opencl v1.2 . *",
    "an intel xeon phi 5110p * ( knights corner , 60 x86 cores @ 1.05 ghz , 2.02 sp tflops , 1.01 dp tflops , 8 gb gddr5 , ecc - on , 320 gb / s bandwidth , driver v3.4 - 1 , @xmath67os v2.6.38.8 ) . &",
    "( 1 ) the csr - based spmv in intel mkl 11.2 update 1.(2 ) the esb  @xcite with dynamic scheduling enabled.(3 ) the csr5-based spmv implemented by using openmp and mic - knc intrinsics .    -16pt",
    "we evaluate the csr5-based spmv and 11 state - of - the - art formats and algorithms on four mainstream platforms : dual - socket intel cpus , an nvidia gpu , an amd gpu and an intel xeon phi .",
    "the platforms and participating approaches are shown in table  [ tab.testbeds ] .",
    "host of the two gpus is a machine with amd a10 - 7850k apu , dual - channel ddr3 - 1600 memory and 64-bit ubuntu linux v14.04 installed .",
    "host of the xeon phi is a machine with intel xeon e5 - 2680 v2 cpu , quad - channel ddr3 - 1600 memory and 64-bit red hat enterprise linux v6.5 installed .",
    "the two gpu platforms use the g++ compiler v4.8.2 .",
    "the two intel machines always set the intel c / c++ complier 15.0.1 as default .    here",
    "we evaluate double precision spmv .",
    "so cudpp library  @xcite , clspmv  @xcite and yaspmv  @xcite are not included since they only support single precision floating point as data type .",
    "two recently published methods  @xcite are not tested since the source code is not available to us yet .",
    "we use opencl profiling scheme for timing spmv on the amd platform and record wall - clock time on the other three platforms . for all participating formats and algorithms , we evaluate spmv 10 times ( each time contains 1000 runs and records the average ) and report the best observed result .      in table",
    "[ tab.suite ] , we list 24 sparse matrices as our benchmark suite for all platforms .",
    "the first 20 matrices have been widely adopted in previous spmv research  @xcite .",
    "the other 4 matrices are chosen since they have more diverse sparsity structures .",
    "all matrices except _ dense _ are downloadable at the university of florida sparse matrix collection  @xcite .    to achieve a high degree of differentiation , we categorize the 24 matrices in table  [ tab.suite ] into two groups : ( 1 ) _ regular _ group with the upper 14 matrices , ( 2 ) _ irregular _ group with the lower 10 matrices .",
    "this classification is mainly based on the minimum , average and maximum lengths of the rows .",
    "dc2 _ is a representative of the group of irregular matrices .",
    "its longest single row contains 114k nonzero entries , i.e. , 15% nonzero entries of the whole matrix with 117k rows .",
    "this sparsity pattern challenges the design of efficient storage format and spmv algorithm .",
    "@xmath68 & @xmath7 & @xmath7 per row ( min , avg , max ) r1 & dense & 2k@xmath112k & 4.0 m & 2k , 2k , 2k r2 & protein & 36k@xmath1136k & 4.3 m & 18 , 119 , 204 r3 & fem / spheres & 83k@xmath1183k & 6.0 m & 1 , 72 , 81 r4 & fem / cantilever & 62k@xmath1162k & 4.0 m & 1 , 64 , 78 r5 & wind tunnel & 218k@xmath11218k & 11.6 m & 2 , 53 , 180 r6 & qcd & 49k@xmath1149k & 1.9 m & 39 , 39 , 39 r7 & epidemiology & 526k@xmath11526k & 2.1 m & 2 , 3 , 4 r8 & fem / harbor & 47k@xmath1147k & 2.4 m & 4 , 50 , 145 r9 & fem / ship & 141k@xmath11141k & 7.8 m & 24 , 55 , 102 r10 & economics & 207k@xmath11207k & 1.3 m & 1 , 6 , 44 r11 & fem / accelerator & 121k@xmath11121k & 2.6 m & 0 , 21 , 81 r12 & circuit & 171k@xmath11171k & 959k & 1 , 5 , 353 r13 & ga41as41h72 & 268k@xmath11268k & 18.5 m & 18 , 68 , 702 r14 & si41ge41h72 & 186k@xmath11186k & 15.0 m & 13 , 80 , 662 i1 & webbase & 1m@xmath111 m & 3.1 m & 1 , 3 , 4.7k i2 & lp & 4k@xmath111.1 m & 11.3 m & 1 , 2.6k , 56.2k i3 & circuit5 m & 5.6m@xmath115.6 m & 59.5 m & 1 , 10 , 1.29 m i4 & eu-2005 & 863k@xmath11863k & 19.2 m & 0 , 22 , 6.9k i5 & in-2004 & 1.4m@xmath111.4 m & 16.9 m & 0 , 12 , 7.8k i6 & mip1 & 66k@xmath1166k & 10.4 m & 4 , 155 , 66.4k i7 & asic_680k & 683k@xmath11683k & 3.9 m & 1 , 6 , 395k i8 & dc2 & 117k@xmath11117k & 766k & 1 , 7 , 114k i9 & fullchip & 2.9m@xmath112.9 m & 26.6 m & 1 , 9 , 2.3 m i10 & ins2 & 309k@xmath11309k & 2.8 m & 5 , 9 , 309k    -16pt      figure  [ fig.benchr14 ] shows double precision spmv performance of the 14 regular matrices on the four platforms .",
    "we can see that , on average , all participating algorithms deliver comparable performance .",
    "_ on the cpu platform _",
    ", intel mkl obtains the best performance on average and the other 3 methods behave similar .",
    "_ on the nvidia gpu _ , the csr5 delivers the highest throughput .",
    "the acsr format is slower than the others , because its binning strategy leads to non - coalesced memory access .",
    "_ on the amd gpu _ , the csr5 achieves the best performance .",
    "although the dynamic assigning in the csr - adaptive method can obtain better scalability than the csr - vector method , it still can not achieve near perfect load balance .",
    "_ on the xeon phi _ , the csr5 is slower than intel mkl and the esb format .",
    "the main reason is that the current generation of xeon phi can only issue up to 4 relatively slow threads per core ( i.e. , up to @xmath69 threads in total on the used device ) , and thus the latency of gathering entries from vector @xmath2 becomes the main bottleneck .",
    "then reordering or partitioning nonzero entries based on the column index for better cache locality behaves well in the esb - based spmv .",
    "however , in section 5.6 we will show that this strategy leads to very high preprocessing cost .    -8pt",
    "-8pt -8pt    -8pt -8pt    figure  [ fig.benchi10 ] shows double precision spmv performance of the 10 irregular matrices .",
    "we can see that the irregularity can dramatically impact spmv throughput of some approaches .",
    "_ on the cpu platform _",
    ", the row block method based intel mkl is now slower than the other methods .",
    "the csr5 outperforms the others because of better simd efficiency from the avx2 intrinsics .",
    "_ on the nvidia gpu _ , the csr5 brings the best performance because of the near perfect load balance .",
    "the other two irregularity - oriented formats , hyb and acsr , behave well but still suffer from imbalanced work decomposition .",
    "note that the acsr format is based on dynamic parallelism , a technical feature only available on recently released nvidia gpus .",
    "_ on the amd gpu _ , the csr5 greatly outperforms the other two algorithms using the row block methods . because the minimum work unit of the csr - adaptive method is one row , the method delivers degraded performance for matrices with very long rows .",
    "_ on the xeon phi _",
    ", the csr5 can greatly outperform the other two methods in particular when matrices are too irregular to expose cache locality of @xmath2 by the esb format .",
    "furthermore , since esb is designed on top of the ellpack format , it can not obtain the best performance for some irregular matrices .",
    "overall , the csr5 achieves better performance ( on the two gpu devices ) or comparable performance ( on the two x86 devices ) for the 14 regular matrices . for the 10 irregular matrices , compared to poski , acsr",
    ", csr - adaptive and esb as the second best methods , the csr5 obtains average performance gain of 17.6% , 28.5% , 173.0% and 293.3% ( up to 213.3% , 153.6% , 405.1% and 943.3% ) , respectively .",
    "[ cols=\"<,>,>,>,>,>,>,>,>,>,>\",options=\"header \" , ]      in section 3.2 , we discussed a simple auto - tuning scheme for the parameter @xmath17 on gpus .",
    "figure  [ fig.autotuning ] shows its effects ( the x axis is the matrix _ _",
    "we can see that compared to the best performance chosen from a range of @xmath70 4 to 48 , the auto - tuned @xmath17 does not have obvious performance loss . on the nvidia gpu ,",
    "the performance loss is on average -4.2% . on the amd gpu ,",
    "the value is on average -2.5% .",
    "the format conversion from the csr to the csr5 includes four steps : ( 1 ) memory allocation , ( 2 ) generating ` tile_ptr ` , ( 3 ) generating ` tile_desc ` , and ( 4 ) transposition of ` col_idx ` and ` val ` arrays .",
    "figure  [ fig.conv ] shows the cost of the four steps for the 24 matrices ( the x axis is the matrix _ _",
    "id__s ) on the four used platforms .",
    "cost of one single spmv operation is used for normalizing format conversion cost on each platform .",
    "we can see that the conversion cost can be on average as low as the overhead of a few spmv operations on the two gpus .",
    "on the two x86 platforms , the conversion time is longer ( up to cost of around 1020 spmv operations ) .",
    "the reason is that the conversion code is manually simdized using cuda or opencl on gpus , but only auto - parallelized by openmp on x86 processors .",
    "-26pt      since both the preprocessing ( i.e. , format conversion from a basic format ) time and the spmv time are important for real - world applications , we have designed an iteration - based benchmark .",
    "this benchmark measures the overall performance of a solver with @xmath3 iterations .",
    "we assume the input matrix is already stored in the csr format .",
    "so the overall cost of using the csr format for the scenarios is @xmath71 , where @xmath72 is execution time of one csr - based spmv operation . for a new format ,",
    "the overall cost is @xmath73 , where @xmath74 is preprocessing time and the @xmath75 is one spmv time using the new format .",
    "thus we can calculate speedup of a new format over the csr format in the scenarios , through @xmath76 .",
    "table  [ tab.iter ] shows the new formats preprocessing cost ( i.e. , @xmath77 ) and their speedups over the csr format in the iteration - based scenarios when @xmath78 and @xmath79 .",
    "the emboldened font in the table shows the highest positive speedups on each platform .",
    "the compared baseline is the fastest csr - based spmv implementation ( i.e. , intel mkl , nvidia cusparse / cusp , csr - vector from cusp , and intel mkl , respectively ) on each platform .",
    "we can see that because of the very low preprocessing overhead , the csr5 can further outperform the previous methods when doing 50 iterations and 500 iterations .",
    "although two gpu methods , the acsr format and the csr - adaptive approach , in general have shorter preprocessing time , they suffer from lower spmv performance and thus can not obtain the best speedups . on all platforms , the csr5 always achieves the highest overall speedups .",
    "moreover , the csr5 is the only format that obtains higher performance than the csr format when only 50 iterations are required .",
    "a great deal of work has been published on accelerating the spmv operation .",
    "the * block - based sparse matrix construction * has received most attention  @xcite because of two main reasons : ( 1 ) sparse matrices generated by some real - world problems ( e.g. , finite element discretization ) naturally have the block sub - structures , and ( 2 ) off - chip load operations may be decreased by using the block indices instead of the entry indices . however , for many matrices that do not exhibit a natural block structure , trying to extract the block information is time consuming and has limited effects . on the other hand , the * hybrid formats *  @xcite , such as hyb , have been designed for irregular matrices . however , higher kernel launch overhead and invalidated cache among kernel launches tend to decrease their overall performance .",
    "moreover , it is hard to guarantee that every sub - matrix can saturate the whole device .",
    "in addition , some relatively simple operations such as solving triangular systems become complex while the input matrix is stored in two or more separate parts .",
    "the recent * row block methods * showed good performance either for regular matrices  @xcite or for irregular matrices  @xcite , but not for both .",
    "in contrast , the csr5 can deliver higher throughput both for regular matrices and for irregular matrices .",
    "the * segmented sum methods * have been used in two recently published papers  @xcite for the spmv on either gpus or xeon phi . however , both of them need to store the matrix in coo - like formats to utilize the segmented sum .",
    "in contrast , the csr5 format saves useful row index information in a compact way , and thus can be more efficient both for the format conversion and for the spmv operation .",
    "sedaghati et al .",
    "@xcite constructed machine learning classifiers for * automatic selection of the best format * for a given sparse matrix on a target gpu .",
    "the csr5 format described in this work can further simplify such a selection process because it is insensitive to the sparsity structure of the input sparse matrix .",
    "moreover , to the best of our knowledge , the csr5 is the only format that supports high throughput * cross - platform spmv * on cpus , nvidia gpus , amd gpus and xeon phi at the same time .",
    "this advantage may simplify the development of scientific software for processors with massive on - chip parallelism .",
    "in this paper , we proposed the csr5 format for efficient cross - platform spmv on cpus , gpus and xeon phi .",
    "the format conversion from the csr to the csr5 was very fast because of the format s insensitivity to sparsity structure of the input matrix .",
    "the csr5-based spmv was implemented by a redesigned segmented sum algorithm with higher simd utilization compared to the classic methods .",
    "the experimental results showed that the csr5 delivered high throughput both in the isolated spmv tests and in the iteration - based scenarios .",
    "the authors would like to thank james avery  ( ku ) , huamin ren  ( aau ) , wenliang wang  ( boc ) , jianbin fang  ( nudt ) , joseph l. greathouse  ( amd ) , shuai che ( amd ) , ruipeng li  ( umn ) , anders logg  ( chalmers and gu ) , and our anonymous reviewers for their insightful feedback .",
    "we thank klaus birkelund jensen  ( ku ) , hans henrik happe  ( ku ) and rune kildetoft  ( ku ) for access to the intel xeon and xeon phi machines .",
    "we thank bo shen  ( inspur ) for helpful discussion about xeon phi programming .",
    "we also thank arash ashari  ( osu ) , intel mkl team , joseph l. greathouse  ( amd ) and mayank daga  ( amd ) for sharing source code , libraries or implementation details of their spmv algorithms with us .",
    "finally , we thank the ppopp 15 reviewers for their valuable suggestions and comments .",
    "a.  ashari , n.  sedaghati , j.  eisenlohr , s.  parthasarathy , and p.  sadayappan . .",
    "in _ proceedings of the international conference for high performance computing , networking , storage and analysis _ , sc 14 , pages 781792 , 2014 .",
    "a.  bulu , j.  t. fineman , m.  frigo , j.  r. gilbert , and c.  e. leiserson . .",
    "in _ proceedings of the twenty - first annual symposium on parallelism in algorithms and architectures _ , spaa 09 , pages 233244 , 2009 .",
    "w.  t. tang , r.  zhao , m.  lu , y.  liang , h.  p. huynh , x.  li , and r.  s.  m. goh . .",
    "in _ proceedings of the 13th annual ieee / acm international symposium on code generation and optimization _ , cgo 15 , pages 136145 , 2015 ."
  ],
  "abstract_text": [
    "<S> sparse matrix - vector multiplication ( spmv ) is a fundamental building block for numerous applications . in this paper </S>",
    "<S> , we propose csr5 ( compressed sparse row 5 ) , a new storage format , which offers high - throughput spmv on various platforms including cpus , gpus and xeon phi . </S>",
    "<S> first , the csr5 format is insensitive to the sparsity structure of the input matrix . </S>",
    "<S> thus the single format can support an spmv algorithm that is efficient both for regular matrices and for irregular matrices . </S>",
    "<S> furthermore , we show that the overhead of the format conversion from the csr to the csr5 can be as low as the cost of a few spmv operations . </S>",
    "<S> we compare the csr5-based spmv algorithm with 11 state - of - the - art formats and algorithms on four mainstream processors using 14 regular and 10 irregular matrices as a benchmark suite . </S>",
    "<S> for the 14 regular matrices in the suite , we achieve comparable or better performance over the previous work . </S>",
    "<S> for the 10 irregular matrices , the csr5 obtains average performance improvement of 17.6% , 28.5% , 173.0% and 293.3% ( up to 213.3% , 153.6% , 405.1% and 943.3% ) over the best existing work on dual - socket intel cpus , an nvidia gpu , an amd gpu and an intel xeon phi , respectively . for real - world applications such as a solver with only tens of iterations </S>",
    "<S> , the csr5 format can be more practical because of its low - overhead for format conversion . </S>"
  ]
}