{
  "article_text": [
    "the ising model is a simple and standard model of statistical physics .",
    "the two - dimensional ( 2d ) ising model , which was exactly solved by onsager @xcite , shows a phase transition at the critical temperature @xmath2 .",
    "the ising model has been studied by using many monte carlo techniques , including the metropolis algorithm @xcite , cluster algorithm @xcite , extended ensemble algorithm such as the wang - landau method @xcite .",
    "since the ising model is simple and its exact solution is available , it is sometimes used to test a new monte carlo algorithm .",
    "recently the speed - up of computation with graphics processing unit ( gpu ) has captured a lot of attention in many fields including computational biology and chemistry @xcite , molecular dynamics simulation of fluids @xcite , ct image reconstruction @xcite , finance @xcite and much more .",
    "gpu has been developed as a supplementary arithmetic device of image processing .",
    "gpu has many streaming multiprocessors ( sm ) .",
    "each sm is composed of eight streaming processors ( sp ) with a multi - threaded instruction unit and shared memory .",
    "gpu has realized the acceleration of image processing by using many sms in parallel .",
    "since gpu is specialized for highly parallel computation , it is designed such that many transistors are devoted to data processing .",
    "the common unified device architecture ( cuda ) released by nvidia makes it possible to make a general purpose computing on gpu .",
    "cuda allows us to implement algorithms using standard c language with cuda specific extensions .",
    "the host program launches a sequence of kernels .",
    "a kernel is organized as a hierarchy of threads .",
    "threads are grouped into blocks , and blocks are grouped into a grid .",
    "the organization of a grid is determined when the gpu `` kernel '' function is invoked as @xmath3 here , @xmath4 and @xmath5 are integer variables , which specify the number of blocks and that of threads , respectively . for simplicity ,",
    "we have used one - dimensional representation for the dimensions of @xmath4 and @xmath5 .",
    "threads in a single block will be executed on a single sm , sharing the data cache , and can synchronize and share data with threads in the same block .",
    "there are several types of memories , and there is a significant difference in the access speed of memories .",
    "the access speed of global memory , which is accessible from either all the threads or the host , is much slower than that of shared memory , which is accessible from the threads in one block .",
    "the access speed of register , which is only accessible by the thread , is nearly equal to that of shared memory .",
    "preis _ et al . _",
    "@xcite studied the 2d and three - dimensional ( 3d ) ising model by using metropolis algorithm with cuda .",
    "they used a variant of sublattice decomposition for a parallel computation on gpu .",
    "the spins on one sublattice do not interact with other spins on the same sublattice .",
    "therefore one can update all spins on a sublattice in parallel when making the metropolis simulation . as a result",
    "they were able to accelerate 60 times for the 2d ising model and 35 times for the 3d ising model compared to a current cpu core .",
    "more recently , the gpu acceleration of the multispin coding of the ising model was reported @xcite .",
    "metropolis algorithm @xcite has flexibility which allows its application to a great variety of physical systems .",
    "however , this algorithm is sometimes plagued by long autocorrelation time , named critical slowing down .",
    "the cluster update algorithms are efficient for overcoming the problem of critical slowing down .",
    "the autocorrelation time drastically decreases by using the wolff single - cluster algorithm @xcite .",
    "it is highly desirable to apply parallel computations to cluster algorithms .",
    "but only a limited number of attempts have been reported so far along this line .",
    "the message passing interface ( mpi ) parallelization technique was employed by bae _",
    "@xcite for the wolff algorithm .",
    "quite recently , kaupuzs _ et al . _",
    "@xcite studied the parallelization of the wolff algorithm by using the open multiprocessing ( openmp ) . for the 3d ising model with linear size @xmath6=1024",
    ", they reached the speed - up about 1.79 times on two processors and about 2.67 times on four processors , as compared to the serial code .    in this paper , we present the gpu calculation with cuda for the wolff single - cluster algorithm of the ising model .",
    "the rest of the paper is organized as follows . in section 2 ,",
    "we briefly describe the standard way of implementing wolff single - cluster algorithm on a cpu . in section 3",
    ", we explain the idea of quasi - block synchronization on gpu . in section 4",
    ", we show the idea and techniques of gpu calculation for the wolff single - cluster algorithm . in section 5",
    ", we compare the performance of gpu calculation with that of cpu calculation .",
    "the summary and discussion are given in section 6 .",
    "our hamiltonian is given by @xmath7 here , @xmath8 is the coupling and @xmath9 is the ising spin on the lattice site @xmath10 .",
    "the summation is taken over the nearest neighbor pairs @xmath11 .",
    "periodic boundary conditions are employed .",
    "wolff proposed a monte carlo algorithm in which only a single cluster is flipped at a time @xcite .",
    "the spin - update process of the wolff single - cluster algorithm on a cpu can be formulated as follows @xcite :    * choose a site @xmath10 randomly . *",
    "look at each of the nearest neighbors @xmath12 .",
    "if @xmath13 is parallel with @xmath14 , add @xmath13 to the cluster with probability @xmath15 , where @xmath16 is the inverse temperature @xmath17 .",
    "then , flip @xmath14 . *",
    "if all the nearest neighbors @xmath12 have been checked , look at each of the nearest neighbors @xmath18 of site @xmath12 .",
    "if @xmath19 is parallel with @xmath13 and it was not a member of the cluster before , add @xmath19 to the cluster with probability @xmath15 . then , flip @xmath13 . * for all the added spins @xmath20 s , repeat step ( iii ) until no more new bonds are created . *",
    "go to ( i ) .",
    "slightly different ways of formulation are possible , for example , for the timing of the flip of spins .",
    "to check the condition that no more new bonds are created in ( iv ) , we may use two variables ` ic ` and ` in ` , where ` ic ` is the total number of members of cluster , and ` in ` is the total number of sites which have been already checked .",
    "we repeat the step ( iii ) while the condition @xmath21 is satisfied .",
    "we note that the check of site @xmath12 is done sequentially on a standard cpu .",
    "in parallel computing , the best performance is obtained if calculation on each thread is done independently . at the same time , barrier synchronization is needed at a certain point .",
    "cuda provides a barrier synchronization function ` syncthreads ( ) ` .",
    "when a kernel function calls ` syncthreads ( ) ` , all threads in a block will be held at the calling location until everyone else in the block reaches the location .",
    "but cuda is not equipped with an inter - block synchronization function .",
    "since the synchronization for different blocks is required in our gpu calculation of cluster algorithm , we here propose an idea of a quasi - block synchronization .    in cuda ,",
    "the organization of a grid is determined by two special parameters provided during kernel launch as mentioned in section 1 .",
    "they are the size of the grid in terms of numbers of blocks , @xmath4 , and the size of each block in terms of numbers of threads , @xmath5 .",
    "moreover , all the threads are identified by two - level unique coordinates , called ` blockidx.x ` and ` threadidx.x ` .",
    "we note that ` blockidx.x ` is the number associated with each block within a grid , and ` threadidx.x ` refers to the label associated with a thread in a block . here",
    ", we have used one - dimensional ids .",
    "now , we present an idea of synchronization of threads across the blocks .",
    "the program of quasi - block synchronization is as follows :    ....   / *     quasi - block synchronization     * /     if(threadidx.x = = 0 ) { stopper[blockidx.x ] = n ; }       ( a )     if(threadidx.x = = 0 ) { stop = 0 ; }                      ( b )     _ _ syncthreads ( ) ;     while(stop ! = 1 ) {        if(threadidx.x",
    "= = 0 ) { stop = 1 ; }                   ( c )        _ _ syncthreads ( ) ;        if(stopper[threadidx.x ] <",
    "n ) { stop = 0 ; }           ( d )        _ _ syncthreads ( ) ;     }                       // while(stop !",
    "= 1 ) loop end ....    we use a flag variable ` stop ` , which is allocated in a shared memory . only after ` stop ` of every block becomes 1 , one gets out of the `` while '' loop of quasi - block synchronization . to check whether each block reaches this location",
    ", we use the array ` stopper [ ] ` , which is allocated in a global memory . as an initial value",
    ", we set ` stopper[blockidx.x ] ` as @xmath22 , which is smaller than @xmath23 , for all blocks .",
    "when any block reaches this location , ` stopper[blockidx.x ] ` is set as @xmath23 in ( a ) .",
    "if there are ` stopper[blockidx.x ] ` which are smaller than @xmath23 , then ` stop ` will be set as 0 for all the blocks in ( d ) .",
    "the trick is that one sets ` stopper[blockidx.x ] ` in ( a ) but one checks ` stopper[threadidx.x ] ` in ( d ) .",
    "it is natural that we choose the size of grid , @xmath4 , and the size of block , @xmath5 , as the same value . in this way",
    ", we can realize the inter - block synchronization .",
    "of course , the synchronization within a block is guaranteed by the synchronization function , ` syncthreads ( ) ` .",
    "when we execute the quasi - block synchronization consecutively , we use different values for @xmath23 in ( a ) .",
    "there is a possibility that some block reaches the next gate before some other block gets out of this routine completely .",
    "we escape from this problem by increasing @xmath23 by one . to make a loop for @xmath23 between @xmath24 and @xmath25",
    ", we slightly modify the condition ( d ) at the points of @xmath24 and @xmath25 .",
    "we here mention the choice of the size of grid and that of block .",
    "to design the organization of a grid , we take account of the size of shared memory and register , the number of processors which are executed simultaneously within a block , etc . a warp in cuda",
    "is a group of 32 threads , which is the minimum size of the data processed in simd ( single instruction multiple data ) fashion .",
    "thus , we choose @xmath4 and @xmath5 as 32 in our gpu calculation of cluster algorithm . then , the total number of threads in a grid becomes 1024 ( @xmath26 ) .",
    "since the maximum number of threads per block is 512 for nvidia geforce gtx 200 series , we could use a single block .",
    "however , it is not efficient because the number of processors executed simultaneously in a block is limited .",
    "although it is natural to choose @xmath4 and @xmath5 as the same value , we can remove this restriction for @xmath27 by imposing a condition such that    ....        if(threadidx.x < grid )           if(stopper[threadidx.x ] <",
    "n ) { stop = 0 ; }        ( d ' )        }   ....    in ( d ) .",
    "we should note that there are upper limits for the choice of @xmath4 , the total number of blocks , and @xmath5 , the total number of threads .",
    "the upper limit of @xmath5 is determined by the maximum number of threads per block , and that of @xmath4 by the number of blocks in one sm and the number of sms .",
    "the number of blocks in one sm is determined by the number of threads and the amount of registers and shared memories , and the number of sms depends on the model of gpu .",
    "we may employ other methods for inter - block synchronization .",
    "one candidate is to use atomic operations such as atomicadd ( ) provided by cuda .",
    "atomic operations are performed without interference from any other threads .",
    "inter - block synchronization is realized by checking that a flag variable in global memory is accessed from all the blocks .",
    "however , it takes time to use atomicadd ( ) when many blocks try to access to the global memory for flag variable at the same time .",
    "xiao and feng @xcite proposed two approaches for inter - block gpu synchronization .",
    "they compared the gpu lock - based synchronization using atomic function with the gpu lock - free synchronization .",
    "our algorithm is a refined version of the latter , the gpu lock - free synchronization , in the sense that the number of access to global memories are reduced .",
    "they found that the gpu lock - based synchronization takes more time than the gpu lock - free synchronization .",
    "another possible method is the master - slave method @xcite by volkov and demmel .",
    "the time of global memory accesses should be carefully checked .",
    "further study on inter - block synchronization will be required .",
    "in this section , we describe the gpu calculation for the wolff single - cluster algorithm .",
    "the sublattice decomposition can not be used for parallelization in the case of cluster algorithm .",
    "we here perform parallel computation for the newly added spins @xmath19 in the step ( iii ) of section 2 .",
    "this idea is similar to that by kaupuzs _",
    "@xcite , where these newly added spins in the wave front of the growing cluster were referred to as wave - front spins .",
    "we assign each of wave - front spins to the thread in the grid .",
    "there are several points which ensure that the parallel algorithm works correctly .",
    "we should avoid the situation where different threads try to incorporate the same spin to the cluster simultaneously . to do this , after checking one direction of nearest neighbors , we perform both thread synchronization and quasi - block synchronization .",
    "moreover , the newly added spins should be numbered . in such a numbering ,",
    "the synchronization after the process of each direction of neighbors is essential .",
    "the numbering and the update of ` ic ` , the total number of members of cluster , are done when the last direction of neighbors is checked for each spin .",
    "then , the step ( iii ) is modified as follows :    * all the processes are done for each thread representing the wave - front spin .",
    "after checking each direction of nearest neighbors , perform the synchronization .",
    "when the last direction of the neighbors is checked , sum up the total number of newly added spins , and complete the numbering of the newly added spins .",
    "we here make a remark on the implementation of parallel computation .",
    "the number of wave - front spins is represented by @xmath28 in terms of ` ic ` and ` in ` discussed in section 2 . if the thread is indexed by    ....",
    "index = blockidx.x * 32 + threadidx.x ; ....    and the kernel function is invoked on the condition that @xmath29 , we make calculations only for wave - front spins .",
    "then , the main part of gpu kernel function is designed as    ....      if ( index < ( ic - in ) ) {          \" check one direction of the neighbors of wave - front spin \"      }      \" perform synchronization \"      ...      ...      if ( index < ( ic - in ) ) {          \" check the last direction of the neighbors of wave - front spin \"      }      \" perform synchronization \"      \" flip the wave - front spin \"      \" update the numbering of the newly added spins and ic \" ....    as mentioned before , we chose the size of the grid in terms of threads as 1024 @xmath30",
    ". then , actually , the upper limit of the parallelization is 1024 . in other words ,",
    "the calculation is made in parallel for min(@xmath28,1024 ) .",
    "thus , ` in ` , the total number of sites which have been already checked , is updated as @xmath31 = @xmath32 .    in making the gpu calculation with cuda , the proper use of shared memories , the technique to avoid the warp divergence or the bank conflict when taking the summation over the shared memories , etc .",
    ", are very effective for fast computation . since the access to global memory is time - consuming , it is better to reduce the frequency of the access to global memory . for this purpose",
    "we use the following data structure ; the information on spin value and that on flag index to specify whether the site is a member of the cluster or not are put into one word .",
    "we finally note that we use a linear congruential random generator which was proposed by preis _",
    "@xcite when evaluating the transition probability @xmath15 .",
    "we have tested the performance of our code on nvidia geforce gtx 285 . for comparison",
    ", we run the code on a current cpu , intel(r ) xeon(r ) cpu w3520 @ 2.67ghz .",
    "only one core of the cpu is used . for compiler ,",
    "we have used gcc 4.2.1 with option -o3 .",
    "since the cluster size is dependent on the temperature , the computational time changes with temperature .",
    "so we compare the gpu computational time with the cpu computational time at the critical temperature , @xmath33 for the 2d ising model and @xmath34 for the 3d ising model .",
    "the average computational times per a single - cluster update at the critical temperature for the 2d and 3d ising model are tabulated in table [ tb : gpu_cpu_time_2d ] and table [ tb : gpu_cpu_time_3d ] , respectively . there , the time for only spin - update and that including the measurement of energy and magnetization are given .",
    "we show the measured time in units of micro sec .",
    "the linear system sizes ( @xmath35 ) are @xmath36 for the 2d ising model and @xmath37 for the 3d ising model .",
    "we can see from tables [ tb : gpu_cpu_time_2d ] and [ tb : gpu_cpu_time_3d ] that the acceleration of our single - cluster algorithm increases as the linear system size grows to a large size .",
    "the gpu computational speed for the 2d ising model with @xmath0 is 5.60 times as fast as the cpu computational speed , and that for the 3d ising model with @xmath1 is 7.90 times as fast as the cpu computational speed .",
    "this result indicates that the acceleration rate of our single - cluster algorithm surpasses that of the algorithm by kaupuzs _",
    "@xcite , which reached the speed - up about 2.67 times for the 3d ising model with linear size @xmath6=1024 .",
    "because of the size of memory for gpu , that is , 1 gb for nvidia geforce gtx 285 , the system size is currently limited to @xmath0 for 2d and @xmath1 for 3d . with the increase of the system size available",
    ", much better performance is expected for @xmath6=1024 in 3d .    [ cols= \"",
    "> , > , > , > , > \" , ]     we here mention the bottleneck of our method associated with the quasi - block synchronization .",
    "this does not mean that the quasi - block synchronization itself takes long time , but the latency is a problem when all blocks reach the quasi - block synchronization point .",
    "when a new spin is added to the cluster in the step ( iii ) of section 3 , our method needs to update a flag variable in global memory to check whether that site belongs to the cluster or not .",
    "since the sites of the newly added spins are chosen randomly , this task is not a coalesced memory access in global memory , and the amount of the task becomes quite different for each block .",
    "however , all blocks must wait for other blocks at the quasi - block synchronization point .",
    "thus , the latency becomes a bottleneck .",
    "if we could equalize the load of the global memory access of each block , the performance will be improved .",
    "but such a trial will be a difficult job .",
    "we here mention about the gpu occupancy , which is available by the cuda visual profiler .",
    "the value of the gpu occupancy of our algorithm is 0.25 , which is lower than 1 .",
    "it is because we fix the number of threads in a block as 32",
    ". we could bring the gpu occupancy close to 1 by increasing the number of parallelization , but in our algorithm it also causes extra computational cost due to the extra warp together with that in the process of summing up the total number of newly added spins .    after we finished the present work , we came to know that hawick _",
    "_ @xcite also studied the cuda implementation of the wolff algorithm .",
    "they used a modified connected component labelling for the assignment of the cluster . since the gpu performance of the wolff update is not so good as that of the metropolis update , they put more emphasis on the hybrid implementation of metropolis and wolff updates and the optimal choice of the ratio of both updates .",
    "it is interesting to compare their gpu implementation of the wolff single - cluster algorithm with that of ours , which will be left to a future work .",
    "we finally make a comment on synchronization . in cuda ,",
    "the function for inter - block synchronization is not natively provided .",
    "we have proposed an idea of a quasi - block synchronization with cuda , which is effective for our single - cluster algorithm .",
    "this quasi - block synchronization algorithm can be used not only for a single - cluster monte carlo algorithm but also in many fields where the synchronizations of all threads in all blocks are required .",
    "this work was supported by a grant - in - aid for scientific research from the japan society for the promotion of science .",
    "h. scherl , b. keck , m. kowarschik , j. hornegger , fast gpu - based ct reconstruction using the common unified device architecture ( cuda ) , ieee nuclear science symposium conference record 6 ( 2007 ) 4464 - 4466 ."
  ],
  "abstract_text": [
    "<S> we present the gpu calculation with the common unified device architecture ( cuda ) for the wolff single - cluster algorithm of the ising model . proposing an algorithm for a quasi - block synchronization , we realize the wolff single - cluster monte carlo simulation with cuda . we perform parallel computations for the newly added spins in the growing cluster . as a result , </S>",
    "<S> the gpu calculation speed for the two - dimensional ising model at the critical temperature with the linear size @xmath0 is 5.60 times as fast as the calculation speed on a current cpu core . for the three - dimensional ising model with the linear size @xmath1 , the gpu calculation speed is 7.90 times as fast as the cpu calculation speed </S>",
    "<S> the idea of quasi - block synchronization can be used not only in the cluster algorithm but also in many fields where the synchronization of all threads is required .    </S>",
    "<S> monte carlo simulation , cluster algorithm , ising model , parallel computing , gpu </S>"
  ]
}