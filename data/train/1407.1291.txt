{
  "article_text": [
    "of the problems that hinders widespread use of electric vehicles ( ev ) is `` range anxiety '' , users fear that their vehicle will stop in the middle of nowhere , because of the low battery level .",
    "the batteries of these plug - in hybrid electric vehicles are to be charged at home from a standard outlet or on a corporate car park .",
    "these extra electrical loads have an impact on the distribution grid such as power losses and voltage deviations .",
    "a mass domestic charging means that the vehicles are charged instantenously when they are plugged in or after a fixed start delay .",
    "this uncoordinated power consumption on a local scale can lead to grid problems@xcite .",
    "+ this fact explains increasing concerns with charging stations and it has led many researchers to work on theoretical models of the functioning of these stations . in our case , we present a model of an ev charging station , together with an allocation mechanism for its resources .",
    "the aim is to maximize station s revenue taking into consideration fluctuations of electricity price during the day .",
    "+ we named our algorithm saeds ( station automated electricity distribution system ) .",
    "it is based on the area of machine learning called reinforcement learning .",
    "the goal of saeds is to learn how good are the different decisions of the station in the different states that it could be .",
    "this is performed in the so - called `` learning phase '' . after the end of this phase",
    ", the application uses the already learned information to distribute electricity supply among the plugged - in vehicles .",
    "+ ev charging stations are connected to the electrical grid , which often means that their electricity comes from fossil - fuel power stations or nuclear power plants . in our model , it is assumed that the ev station is connected to a renewable source of energy .",
    "the fact , that the amount of electricity received from this source needs prediction , makes the model more complex .",
    "the complexity of the cosidered problem justifies the use of machine learning algorithm , instead of alternave approaches .",
    "sections ii and iii speaks about the literature we have used and about the theory behind the q - learning method , respectively .",
    "sections iv contains the details of the model we consider and the algorithm we use to solve it .",
    "section v describes the results of the computer simulation , created to test saeds .",
    "the conclusion is stated in section vi .",
    "this work is very similar to those of oneill et al . in @xcite .",
    "there , they present an online learning application for residential demand response that uses q - learning to learn the behavior of the consumers and to make optimal energy consumption decisions .",
    "we noticed that the same approach can be applied in the context of electric vehicles charging , viewing on the charging station as one big household , which wants to minimize its expenses , or equivalently , to maximize its incomes .",
    "the clients of the station , with their vehicles , play the same role as the electrical devices of the household in @xcite . whether a vehicle or a household device will be plugged in at a particular moment is determined probabilistically in both papers .",
    "another related publication ( valogianni et al . [ 6 ] ) proposes a smart charging algorithm that uses q - learning trained on real world data to learn the individual household consumption behavior .",
    "additionally , an ev charging algorithm is proposed which maximizes individual welfare and reduces the individual energy expenses .",
    "+ making the best energy consumption decisions was the purpose of several existing papers , which consider demand response systems@xcite .",
    "however , few of them use reinforcement learning",
    ". reinforcement learning has appeared in numerous books such as `` handbook of brain theory and neural networks '' by barto@xcite , and `` introduction to reinforcement learning '' by sutton and barto@xcite , which we have used because it gives detailed information on the subject and the algorithms used . the main idea behind reinforcement learning and more specifically the q - learning algorithm",
    "is described in the next section .",
    "reinforcement learning is a method used to solve problems , formulated as markov decision processes ( mdp ) .",
    "it consists of performing a set of experiences which result in positive or negative _ rewards _ in order to optimize an _ objective _ function . in these types of problems ,",
    "an active decision - making agent is confronted with an environment , in which it aims to achieve a goal , despite its lack of knowledge of the environment .. the agent s _ actions _ affect the future state of the environment , so the purpose is to make the agent develop a _ policy _ , which would point him to the action , giving the best reward at each particular stage .",
    "+ q - learning , a reinforcement learning technique was first introduced by watkins in 1989 @xcite .",
    "the algorithm makes use of a value iteration update .",
    "the value , called a q - function value , is the expected utility of making a particular decision in a particular state .",
    "thus , q is a function over pairs of states and actions of the agent , who takes the decisions .",
    "the updating of a q - value is performed by taking the old value and making a correction based on the new information learned .",
    "the algorithm is proved to be convergent under certain conditions and gives optimal policy for finite mdp .",
    "+ let s denote with * s * - a state of the system(*s * for the state following * s * ) and with * a * - an action , which the agent could do . for every state , there exists a set of possible actions .",
    "below , the actions * a * and * a * belongs to the possible sets of actions for states * s * and * s * , respectively . at each step , the agent choose an action for which the q - value , q(s , a ) , is maximized(the agent follows a decision policy derived from q ) .",
    "the algorithm is as follows :    initialize * q(s , a ) * arbitrary , for each addmisible state - action pair ( s , a ) ; for each episode : initialize * s * with some state of the system . for each step of episode : @xmath0 choose an action * a * from the possible actions in the state * s * , using policy derived from * q * @xmath0 take action * a * , observe the reward * r * and the next state * s * @xmath0 update the q - value : @xmath1\\ ] ]    s@xmath2 s    here , @xmath3 and @xmath4 are parameters , which reflects the learning rate of the algorithm . in our particular case , one day will corresponds to one episode and an hour to a step of episode",
    ". the reward @xmath5 will be the sum that the station earns at the current hour of the day .",
    "we are going to describe the details in the station s state representation , as well as the q - learning based application which controls the supply decisions of our evs station , connected to a renewable source of energy .      *",
    "discrete time @xmath6 .",
    "we take t=24 .",
    "the vehicles arrive dynamically to the station and every arriving vehicle can start the recharging process as early as the beginning of the next hour interval .",
    "the station is observed over several number of days . + * the station has @xmath7 slots and a maximum number of @xmath8 participating vehicles ( charging or waiting to be charged at any given moment ) .",
    "we can thing that the station has @xmath8 parking places in total , and at every single time maximum @xmath7 of the vehicles on them can be supplied . + * when an ev arrives at the station , its driver , which represent the client , states his type which is a function of the vehicle s state of charge ( soc ) .",
    "this function gives the amount of money that the client is ready to pay if his car s battery reaches the different states of charge ( for example 5 euros for soc=10% and 15 euros for soc=20% ) . knowing a client s type @xmath9 and his vehicle s initial soc @xmath10 , the intermediate price he is going to pay , in order for his car to have @xmath11 can be easily calculated : @xmath12 .",
    "for examples of client s type function , see figure 2 in the next section . + * other information that the station receive is the time to leave ( ttl ) of the coming vehicle , i.e. after how many hours the client will expect his vehicle to be ready . + * additionally , we assume that a renewable source of energy is connected to the station , which supplies the station with variable amount of energy @xmath13 at each particular @xmath14 ( it is unknown before the moment @xmath14 ) .",
    "the transportation costs are neglected ( the cost of transporting the energy from the renewable source to the station slots ) . moreover",
    ", the station can buy an additional amount of electricity from the grid at any specific time at price @xmath15 which also varies .",
    "+      saeds is going to learn the typical total energy demand of the station s customers in the different hours of the day . +",
    "this approach takes advantage of the fact that the number of clients and their energy consumption at a particular time of the day would be similar each day ( weekdays and weekends are not considered separately ) . + the learning approach has to be applied with the help of a big markov chain with state : @xmath16},\\ ] ] where :    1 .",
    "@xmath17 $ ] is the hour of the current day .",
    "@xmath14 is the total number of hours elapsed from the beginning of the first day of the learning phase .",
    "in fact , @xmath18 is the remainder of @xmath14 , when divided to 24",
    "the data structure users(t ) consists of 3 column vectors of size @xmath8- ttl(t ) , soc(t ) and types(t ) .",
    "+ - the times to leave are integers between 1 and @xmath19 ( the upper limit which we have set is 12 ) .",
    "+ - socs are measured in percents of the full capacity of the battery .",
    "we took 10 possible levels- 0,10,20, .. 100 .",
    "we assume that every vehicle uses battery with the same capacity .",
    "+ - for types , we have taken fixed number of functions , chosen in advance .",
    "the particular functions that we have used are described in the next section .",
    "@xmath13 is the amount of renewable energy at time @xmath14 .",
    "it is also discretized .",
    "4 .   @xmath15 is the price per unit of extra energy that the station can buy from the grid at time @xmath14 .",
    "we have a queue of customer s vehicles that is updated dynamically with time.the occurrence of ev arrivals is modeled with _ non - homogeneous poisson process _",
    "i.e. the number of arrivals in the time interval ( a , b ] , given as n(b)-n(a ) , follows a poisson distribution with associated parameter @xmath20 : @xmath21\\\\ = \\frac{e^{-\\lambda_{a , b}}.(\\lambda_{a , b})^{k}}{k ! } ,   k=0 , .. , n\\ ] ]    thus , the number of arriving customers @xmath22 .",
    "here , the rate @xmath23 , @xmath24 which is the average number of arriving vehicles during the interval of time [ t , t+1 ] , will have to be estimated by monitoring station data from the past .",
    "@xmath25 , @xmath26 and @xmath27 , i=1,2, .. ,m represent the characteristics of the i - th vehicle in the queue at time @xmath14 .",
    "+ at any given time @xmath14 , the vectors ttl , soc and types must be updated as follows : first , when new vehicles arrive , if there are vacant spots among the m places in the station , priority is given on a first - come , first - served basis .",
    "after that the online application takes an action .",
    "the space of the application s possible actions at time @xmath14 is formed from all of the vectors @xmath28 , where @xmath29 or @xmath30 .",
    "the station has @xmath7 slots in total , so each possible action vector @xmath28 , must have no more than @xmath7 nonzero components ( 10 or @xmath30 ) .",
    "in other words , at any particular hour , the station can gives the energy equivalence of 10% soc or the maximally possible energy equivalence ( this , complementing to 100% ) to @xmath7 or less electric vehicles .",
    "this means that our station allows two speed levels of charging : normal charging , which succeed to charge 10% in one hour , and fast charging , which can charge fully any vehicle , again in one hour .",
    "+ the soc vector must be updated as follows : @xmath31    @xmath32    in the latter equation , \\ { } means fractional part .",
    "in fact , with this equation we nullify the socs of the charged vehicles .",
    "times to leave decreases by 1 , as we go from time @xmath14 to @xmath33 : @xmath34 finally , the vehicles , whose ttls have become 0 , must be removed .",
    "+ additionally , at each point in time the array of users is sorted by ttl and if ties occur , by type . in other words , we assume that the array @xmath35 $ ] has non - decreasing elements for every t and for every sequence of ties @xmath36 we have @xmath37 .",
    "this detail gradually decreases the number of possible states of the system .",
    "+ in order to generate the initial socs of the newly arriving evs , we use manually setted distribution , chosen according to the history of our station . the times to leave ( ttl ) are dependent on @xmath14 ( for example when an ev comes at 17h , the probability of small deadline before the start of the evening at 20h will increase )",
    ". for generation of ttls of the newly arrived evs , we use normal distribution with different parameters for the different day hours . + for @xmath13 and @xmath15 , real data is used , whose origin is described in the data description section .",
    "the objective function ( the station s reward at any given moment ) will be : @xmath38 @xmath39\\ ] ] @xmath40\\ ] ]    we have a value function over states @xmath41 which represents the `` profitability '' of the state @xmath42 .",
    "@xmath43)\\ ] ] in the latter equation , @xmath44 is the station s state , following @xmath45 .",
    "q - learning is an on - line , reinforcement learning method , which approximates the value of the function @xmath46 .",
    "q - learning estimates the value of a q - function for each state - action pair @xmath47 and @xmath48    in summary , the markov chain will have states in the form of @xmath42 .",
    "the transitional probabilities depend on @xmath49 and are unknown .",
    "let us describe the essence of the q - learning algorithm :    * at k = 0 , initialize @xmath50 for every state - action pair @xmath47 and select initial state @xmath51 * choose @xmath52 with probability @xmath53 else let u be a random exploratory action(uniformly selected ) .",
    "* carry out action u. let the next state be @xmath54 , and the cost be @xmath55 .",
    "update the q - values as follows : @xmath56 + here , @xmath57 and @xmath58 are some probabilities chosen in advanced . *",
    "set the current state to @xmath54 , increment k and go to step 2 .",
    "users types are defined by customer s willingness to pay in order to have their vehicle charged if their initial soc is 0% .",
    "the price that the user pay is the difference between the price of the soc they want to reach and their initial soc . in the simulation ,",
    "we consider two types of users : the so called _ rich _ type of users and _ medium _ type of users .",
    "we model their types function by an utility function represented by the quadratic function .",
    "this family of functions was choosen , because users pay less money for each extra 10% level of charging .",
    "in fact , this functions belongs to the family of sigmoid functions , which are widely used in practice .",
    "@xmath59 the variable @xmath60 refers to the price of fully charging an initially empty vehicle .",
    "example curves are shown in the figure below , where @xmath61 and @xmath62          two categories of data were used - renewable energy data and pricing data .",
    "+ 2.1 renewable energy +    as explained before , we suppose that the station is connected to a wind energy generator ( wind turbine for example ) and has a solar panel . + we need data about the average wind generation from a wind energy generator at hourly slots of the day .",
    "this data can be found on _ rseau de transport dlectricit _ ( rte ) france website @xcite and is provided as an excel file .",
    "the data is about wind generation across france . for the sake of consistency , we divided all the numbers by 4058 ( the number of wind turbines in france in 2012)@xcite .",
    "+ for the solar panel generation , we assume that the ev charging station has a 10 @xmath63 solar panel which is capable of generating an average of 1000 kwh per year in france@xcite . in order to calculate the amount of electricity per hour , we assume that solar panel generation follows a normal distribution . the generation peak is assumed to occur at 1.30 p.m. + 2.2 pricing +    the rte france website also provides data about electricity prices at half hourly slots .",
    "we take into account this data in our model by taking the average price at every hour .",
    "a comparison between the income ( in euro ) of a small ev station using saeds and the income of a station making random decisions at each point in time is depicted on the graph below .",
    "the income was measured for 29 consecutive days after a training period with data for 190 days , repeated 40 times .",
    "the results shows that the income increases by 81 percents , which is a good result , in spite of the fact that the purely random strategy can be easily beaten with the set problem formulation .",
    "however , such a percentage is an evidence for a high learning effect . as a result of previous executions of the program ,",
    "we have received slightly lower results , in the range of 40 to 80 percents of income increasing .",
    "parameter values that were used to obtain these results are : + @xmath64 , @xmath65 and the 2-type functions already described(rich and medium ) . for r(t ) and p(t)- for simplicity , we took only two possible values- high and low . for @xmath3 and @xmath66(parameters of the learning process ) were taken linear by @xmath14 functions .",
    "we tried to apply an already existing approach @xcite of learning an optimal decision policy , but in the context of electric vehicles charging .",
    "there are several similarities between the problem , formulated in @xcite and the study formulated in our paper , which portended good final results . after implementing the model on a computer",
    ", it was observed that our algorithm outperformed the most trivial one ( uniform distribution over each possible decision , on every step ) .",
    "the increase of the station s incomes in the range 40 - 80% shows that using this learning approach is meaningful .",
    "we can conclude that the proposed learning scheme performs at a satisfying level .",
    "nevertheless , the model can be adjusted in order to mimic reality more closely .",
    "probably , some of the numerical parameters in the algorithm can be adjusted more precisely , in order to receive bigger increase of the incomes . however , the utility of the proposed learning scheme is obvious .",
    "k.clement-nyns , e.haesen , j.driesen , `` the impact of charging plug - in hybrid electric vehicles on a residential distribution grid '' , power systems , ieee transactions on , vol.25 , issue : 1 , dec .",
    "d. oneill , m. levorato , a.j .",
    "goldsmith and u. mitra , `` residential demand response using reinforcement learning '' ieee smartgridcomm , oct .",
    "2010 , gaithersburg , maryland , usa .",
    "s. borenstein , m. jaske , and a. rosenfeld,``dynamic pricing , advanced metering , and demand response in electricity markets , '' uc berkeley : center for the study of energy markets , oct . 2002 .",
    "[ online ] .",
    "available : http://www.escholarship.org/uc/item/11w8d6m4 a. g. barto , reinforcement learning , in handbook of brain theory and neural networks , m.a .",
    "arbib ( ed . ) , cambridge : mit press , 1994 .",
    "konstantina valogianni , wolfgang ketter , john collins , `` smart charging of electric vehicles using reinforcement learning '' , aaai publications , workshops at the twenty - seventh aaai conference on artificial intelligence .",
    "planetoscope , information about wind generation and solar generation in france : http://www.planetoscope.com/eolienne/804-production-d electricite-eolienne-en-france.html and http://www.planetoscope.com solaire/4-production - d - electricite - solaire - photovoltaique - en - france.html"
  ],
  "abstract_text": [
    "<S> this paper presents an online reinforcement learning based application which increases the revenue of one particular electric vehicles ( ev ) station , connected to a renewable source of energy . </S>",
    "<S> moreover , the proposed application adapts to changes in the trends of the station s average number of customers and their types . </S>",
    "<S> most of the parameters in the model are simulated stochastically and the algorithm used is the q - learning algorithm . a computer simulation was implemented which demonstrates and confirms the utility of the model .    </S>",
    "<S> reinforcement learning , electric vehicles , charging stations , renewable energy , q - learning </S>"
  ]
}