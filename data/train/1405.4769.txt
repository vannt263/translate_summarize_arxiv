{
  "article_text": [
    "we live in an epoch at which several theories have been built to explain the observed accelerated expansion of the universe , see ref .",
    "@xcite for a review .",
    "the discovery of this late - time acceleration of the universe by @xcite has led people to introduce a new ingredient to total matter density in the universe : the dark energy .",
    "such a component has raised severe , and still unsolved , theoretical problems which have led the community to search for alternative approaches to explain this late - time acceleration , namely modified gravity models and inhomogeneous models .",
    "the first approach simply postulates that general relativity is accurate up to a typical scale and it needs to be modified at larger scales ; this modification could then lead to an observed acceleration .",
    "the second approach studies the effects of the large scale structures on the observed luminosity of distant supernovae of type ia ( snia ) .",
    "huge experimental efforts have been made to better understand the expansion of the universe , see for instance @xcite,@xcite @xcite , @xcite , @xcite for details on the surveys of euclid , des and pau .",
    "all of these experiments are planned to collect an impressive amount of data for different observables to reduce the statistical errors on the cosmological parameters .",
    "once the data from the different experiments are collected , then we need to be able to extract the largest amount of information on the cosmological parameters by using the smallest number of assumptions possible .",
    "this paper looks towards this direction : we use the principal component analysis ( pca ) in order to decorrelate the parameters of interest and get unbiased constraints .",
    "another advantage of the pca is that there is no need to specify the cosmology ; i.e. it is a model - independent approach , as the pca will give us a set of functions that better describes the data @xcite , @xcite , @xcite , @xcite , @xcite , @xcite .",
    "more details on the procedure of the pca and the relevant sets of equations for each approach are also presented in sec .",
    "[ sec : background ] and in the appendix .    in this paper in particular , we will compare different forms of the pca and analyze their advantages and disadvantages .",
    "the four different cases we will consider are    1 .   the deceleration parameter @xmath1 , 2 .   the de equation of state ( eos ) parameter @xmath0 , 3 .   the hubble parameter @xmath2 and 4 .   the luminosity distance @xmath8 .",
    "these quantities are the most phenomenologically interesting ones as they are connected directly with the physical properties of the de fluid ( @xmath0 ) , the expansion rate of the universe ( @xmath2 and @xmath1 ) and finally the measurements themselves ( @xmath8 ) .",
    "we then analyze the four mentioned parameterizations with three different mock snia data based on different cosmologies :    1 .   the model with @xmath9 ; 2 .   the @xmath10cdm model with @xmath11 ; 3 .",
    "the hu and sawicki @xmath7  @xcite and nesseris et al @xcite model with @xmath12 parameters .    for all the mock catalogs we assume that @xmath13 and @xmath14 , while some more details about the construction of the mocks are given in sec .",
    "[ sec : results ] .",
    "as it will be quite clear in the next few sections , the novelty of our paper lies in the following two pillars : the systematic comparison of all the methods and their subsequent testing against mock data and against each other is done for the first time in the literature and second , we present several new analytical expressions for all four different forms of piecewise - constant methods .",
    "our analysis will be immensely useful with the upcoming surveys that will collect a plethora of new data that will have to be analyzed in a systematic fashion and their cosmological information extracted .    finally , the paper is organized as follows : in sec .",
    "[ sec : theory ] we briefly review the background equations and the models we consider in this analysis , in sec .",
    "[ sec : background ] we briefly review the pca and we present several novel results for all four forms of the pca ( see also the appendix ) . in sec .  [ sec : results ] we present the results we found by applying the four different forms with the three mock catalogs ; finally in sec .  [ sec : conclusions ] we summarize our conclusions , listing the advantages and disadvantages of all the forms of the pca used in this paper .",
    "in this section we briefly review the equations for the , @xmath10cdm and the hu and sawicki @xmath7 models .",
    "the @xmath10cdm model assumes a linear expansion in terms of the scale factor of the dark energy equation of state , such that w(a)=w_0+w_a(1-a ) .",
    "the cosmological constant model corresponds to @xmath15 .",
    "the hubble parameter , assuming a flat universe , is given by h(z)^2/h_0 ^ 2=(1+z)^3+(1-)(1+z)^3(1+w_0+w_a)e^-3 w_a z/(1+z ) .",
    "the hu and sawicki @xmath7  @xcite is given by the action @xmath16",
    "\\label{action1}\\ ] ] where @xmath17 is the lagrangian of matter and @xmath18 and the function @xmath7 is given by @xmath19 in nesseris et al @xcite it was shown that this can also be rewritten as [ hu1 ] f(r)&= & r- + + & = & r- 2(1- ) + & = & r- where @xmath20 and @xmath21 . in this form it is obvious that the hu and sawicki model can be arbitrarily close to @xmath5cdm , depending on the parameters @xmath22 and @xmath23 .",
    "the explicit modified friedmann equations can be found in ref .",
    "let us suppose that we have a function @xmath24 well defined in the range @xmath25 and @xmath26 and we want to find the best parametrization to this function given a set of data @xmath27 .",
    "we can write the function @xmath24 with many piecewise - constant values as : f(x ) = _ i = i^nf_i_i(x ) [ eq : piecewise ] where @xmath28 are constant in each interval @xmath29 and @xmath30 is the theta function , i.e. @xmath31 for @xmath32 and @xmath33 elsewhere .",
    "@xmath34 is the number of parameters that we would like to constrain given the data .",
    "however , the parameters will be in general correlated ; we can use the pca to decorrelate the parameters @xmath35 s .",
    "following ref .",
    "@xcite , we first build a diagonal matrix @xmath36 with the eigenvalues of the fisher matrix @xmath37 , which is defined as the inverse of the covariance matrix @xmath38 ( obtained directly from the chains , when performed ) . then we define a matrix @xmath39 where the matrix @xmath40 is the transpose of @xmath41 and the latter is a matrix composed by the eigenvectors of fisher matrix .",
    "we finally normalize @xmath42 such that its rows sum up to unity .",
    "the matrix @xmath42 will give the uncorrelated parameters , i.e. p_i=_j=1^m_ijf_j [ eq : pca - values ] where @xmath43 isthe total number of parameters .",
    "the variance of the parameters @xmath44 will then be ^2(p_i)=1/_i .",
    "[ eq : pca - errors ] our goal is to investigate which is the best parametrization that gives us the largest amount of information about the cosmology given a set of observations .",
    "so , in what follows we consider different parameterizations , such as : the deceleration parameter @xmath1 , the dark energy eos @xmath0 , the hubble parameter @xmath2 and the luminosity distance @xmath45 .",
    "we find the best fit and consequently the principal components ( pc ) of each parametrization and we then propagate this results to the other parameters . as an example",
    ", we find the best fit and the pc for the deceleration parameter @xmath46 and then we convert the results to all the other parameters @xmath47 , @xmath48 and @xmath49 . in this way we are able to verify which is the optimal parametrization with which we can gain most of the information hidden in the data .    to clarify the analysis , let us consider a function @xmath50 , where @xmath51 is the redshift , and it can be used to calculate the luminosity distance @xmath52 . as a first step",
    "we assume that the function @xmath50 can be approximated as piecewise - constant in redshift bins , as in eq .",
    "( [ eq : piecewise ] ) ; then we evaluate the luminosity distance @xmath3 which now will be a function of the constant @xmath53 : d_l(z ) = d_l(z ; _ 1 , _ 2 , ... , _ n ) . in this case",
    "the values of the @xmath3 at one redshift will depend on all the @xmath54 s in the previous redshifts , hence each value of @xmath55 will appear in several bins and consequently the @xmath54 s will be correlated between different bins .",
    "the reason why we used the pca is to decorrelate these parameters and to extract the maximal amount of information for the parameters .",
    "the analysis is based on the algebraic concept to find a linear transformation that it is able to diagonalize the covariance matrix .",
    "we start by presenting the results of the piecewise - constant deceleration parameter @xmath46 , derived in ref .",
    "@xcite : q(z ) = _ i=1^nq_i(z_i ) , where @xmath56 are constant in each redshift bin @xmath57 and @xmath58 is the theta function defined before .",
    "once the pc of the @xmath56 parameters are found we can derive the other observables using the definition of the deceleration parameter : 1+q(z)=. [ eq : q ] the hubble parameter and the luminosity distance are ( see appendix [ app - pcaq ] for more details ) : h_n(z ) & = & h_0 b_n ( 1+z)^1+q_n[eq : hubble - q ] + d_l , n(z)&= & ( 1+z)[eq : dl - q ] where the coefficients @xmath59 and @xmath60 are b_n & = & _ j=1^n-1(1+z_j)^q_j - q_j+1 + f_n & = & + _ j=1^n-1 and @xmath61 .    to propagate our results into the dark energy eos parameter @xmath47 ,",
    "we make use of eq  ( [ eq : q ] ) and we express the hubble parameter as e^2(z ) = h^2(z)/h^2_0 = ( 1+z)^3 + ( 1-)e^3_0^zx .",
    "[ eq : hubble - gen ] deriving the last equation we then find w_n(z ) = [ eq : w - fromq ] where @xmath62 is the matter density as a function of redshift : ( z ) = .",
    "[ eq : omega_m ] it is important to notice that eq .",
    "( [ eq : w - fromq ] ) depends on the matter density @xmath63 , for which we have to assume a specific value not derived from the data themselves ( we will come back later on this ) .",
    "we now want to apply the pca directly to the dark energy eos parameter @xmath0 .",
    "as previously done for @xmath46 , we rewrite @xmath0 as w(z ) = _ i=1^n_totw_i(z_i ) , where @xmath64 are constant in each redshift bin . using the energy - momentum conservation @xmath65 for an ideal fluid with equation of state @xmath47 , we get the equation for the de density can then be written , for @xmath51 in the nth bin , as _",
    "de(z , n)=_de(z=0 ) c_n ( 1+z)^3(1+w_n ) , where the coefficient @xmath66 is c_n = _ j=1^n-1(1+z_j)^w_j - w_j+1 .",
    "consequently , we can write the hubble parameter and the luminosity distance as ( see appendix [ app - pcaw ] for more details ) h_n(z)^2/h_0 ^ 2 & = & ( 1+z)^3+(1-)c_n ( 1+z)^3(1+w_n ) [ eq : hubble - w ] + d_l , n(z ) & = & ( 1+z)(d_n(z , z_n-1)+_i=1^n-1d_i(z_i , z_i-1 ) ) , [ eq : dl - w ] where & & d_i(z_i , z_i-1)_z_i-1^z_i= + & & - \\ { - }   and @xmath67 is a hypergeometric function @xcite .",
    "then , the deceleration parameter @xmath46 will be given by inverting eq .",
    "( [ eq : w - fromq ] ) , i.e. q(z)=12 + 32 w[qfromw ] where @xmath68 is given by eq .",
    "( [ eq : omega_m ] ) for which we use the hubble parameter in eq .",
    "( [ eq : hubble - w ] ) .",
    "in order to apply the pca to the hubble parameter @xmath2 , we write it as h_n(z)/h_0 = _",
    "i=1^n_toth_i(z_i ) , where @xmath69 are constant in each redshift bin . using the definition of the luminosity distance along with the previous equations we have d_l , n(z)= ( 1+z ) ( g_n+h_n^-1 z ) , [ eq : ref - dl - h ] where we have defined the constants @xmath70 .    in order to find the dark energy eos @xmath47 we can use eq .",
    "( [ eq : hubble - gen ] ) and we find w_n(x_i)&=&-1 + , + x_i & & z_i-1 [ wzeff ] where @xmath71 is the average redshift of the bin , i.e. @xmath72 ( see appendix [ app - pcah ] for more details ) . also in this case the dark energy eos parameter depends on the matter density @xmath63 and we need to fix it to a particular value .    to obtain the deceleration parameter @xmath46 from the measurements of hubble parameters @xmath73 , we start : = , [ eq : q - h - func ] which can be integrated in the @xmath74th bin by assuming @xmath75 constant in that bin and we find : ( 1+q_j)_z_eff , j-1^z_eff , j = _ h_j-1^h_j [ eq : q - from - h-0 ] where @xmath76 is the hubble parameter evaluated at the @xmath74 bin .",
    "so we find the deceleration parameter to be : q_j = -1 + .",
    "[ eq : q - from - h-1 ] similarly with before , see also the appendix [ app - pcah ] for more details , we note that the parameters @xmath76 are evaluated in the effective redshift @xmath71 , but the resulting parameters are evaluated at the sides of the bins @xmath77 .",
    "it is also interesting to notice that we can also evaluate the deceleration parameter by applying the definition of the derivative to the eq .",
    "( [ eq : q - h - func ] ) ; the deceleration parameter becomes simpler and we can also avoid the problem of defining the mean redshift : q_j = -1 + ( - 1 ) [ eq : q - from - h-2 ] being @xmath78 the bin width .",
    "we checked both eqs .",
    "( [ eq : q - from - h-0 ] ) and ( [ eq : q - from - h-1 ] ) and we found that the results are almost identical .",
    "however , in this work we prefer to use eq .",
    "( [ eq : q - from - h-0 ] ) to evaluate the deceleration parameter as the definition of the derivative applies only when the infinitesimal quantity @xmath78 in eq .",
    "( [ eq : q - from - h-1 ] ) is small enough .",
    "let us write the luminosity distance @xmath8 as d_l , n(z ) = _ i=1^n_totd_l , i(z_i ) , where @xmath79 are constant in each redshift bin",
    ".    similarly with before , see also the appendix [ app - pcadl ] for more details , we note that the parameters @xmath79 are evaluated in the effective redshift @xmath80 .",
    "therefore , in order to get an estimate for @xmath0 and @xmath1 in this case , we can follow the same procedure as before . using the definition of the luminosity distance at two redshifts @xmath80 and @xmath81 we have -&=&_z_eff , i-1^z_eff , i x= ( z_eff , i - z_eff , i-1 ) , + x_i & & z_i-1 from which we can estimate @xmath82 at the @xmath83th bin and finally estimate @xmath64 using eq .",
    "( [ wzeff ] ) . in this case",
    "the values @xmath64 will correspond to the @xmath71 redshift and not @xmath57 .",
    "we have successfully tested these results with numeric tests and as before , we assume that our models do not have any fast transitions .    however , in this case we can not discriminate between a constant @xmath8 and a constant distance modulus @xmath84 , since these two are connected via _ th(z)= 5 _ 10 d_l(z)+_0 [ distmod ] where @xmath45 the dimensionless luminosity distance and @xmath85 . for the same reason we can not differentiate @xmath79 from @xmath86 , since the latter is just a rescaling of the normalization , unless we fix @xmath87 . for these reasons , in what follows we will consider piecewise - constant @xmath88 and by assuming a value for @xmath87 we can later convert the best fits to @xmath3 and use the previous relations to extract the cosmology .",
    "so , let us write the distance modulus @xmath84 as ( z ) = _ i=1^n_tot_i(z_i ) , where @xmath89 are constant in each redshift bin ( see appendix  [ app - pcadl ] for more details ) . by making the distance modulus @xmath84",
    "piecewise - constant we have a diagonal covariance matrix which means that the parameters are already uncorrelated and we do not have to follow the pca approach in this case ( as the advantage to use the pca is to decorrelate the parameters , i.e. make the covariance matrix diagonal ) .    in order to extract the cosmology we can invert eq .",
    "( [ distmod ] ) to find the dimensionless luminosity distance as d_l , i=10^. [ eq : ref - dl - mu ] where @xmath90 .",
    "we show these results in the next section .    however , we should mention that since @xmath49 is discontinuous on the redshift shell boundaries , this will lead to a large @xmath91 because of the assumptions , i.e. at the shell edges the model @xmath49 is bound to have a @xmath91 which becomes too large if there is enough data .",
    "this effect is illustrated in fig .",
    "[ fig : dlvsdl ] , where we compare the luminosity distance when @xmath49 is piecewise - constant in each bin ( left ) and when @xmath46 is piecewise - constant in each bin . in the first case",
    ", the luminosity distance clearly has a discontinuity at the edge of the bins , while in the second case it is continuous .",
    "as can be seen in the plot , the difference between the best - fit value ( horizontal red line ) and the data points near the edge of the shells is quite big , thus leading to a large @xmath91 .",
    "however , despite of this limitation we decided to also include this parametrization in our analysis for several reasons .",
    "first , as can be seen in appendix [ app - pcadl ] this parametrization is equivalent to directly binning the snia .",
    "second , our goal is to use as many different parametrizations as possible in order to cover all phenomenological properties of dark energy , going from the very fundamental @xmath0 , to the expansion of the universe ( @xmath2 and @xmath0 ) to the observations themselves ( @xmath45 ) .",
    "in order to compare the different methods we created mock snia data based on an _ a priori _ known cosmologies corresponding to the , @xmath92cdm and @xmath7 models . since we are more interested in testing the methods themselves rather than worrying if the differences are due to the construction of the data , we evaluated @xmath93 distance moduli uniformly distributed in the range @xmath4 $ ] ; the distance modulus @xmath94 was estimated as its theoretical value plus a gaussian error ( that can be negative or positive ) and constant errors of @xmath95 .",
    "also , we should stress that since this is the first time this comparison appears in the literature we implement the simplest possible way to produce mock data , as we are mainly concerned with comparing the different methods and not eliminating all possible sources of error in the data .",
    "therefore , using more realistic mock snia data but also other kinds of data has been left for future work , since as mentioned before our current focus is the comparison of all the methods .    as mentioned previously , in order to apply the pca to the @xmath96 , @xmath97 , @xmath98 and @xmath99 we need to find first the best - fit , given a data set , for the parameters . for our purpose",
    ", we divide the survey into 10 equally spaced redshift bins up to @xmath100 . to determine the best fit parameters we proceed in two different ways :    * for the @xmath96 and @xmath97",
    ", we perform a monte carlo markov chain ( mcmc ) method , implemented by using the code of @xcite . in the analysis we used more than 50000 steps each for the parametrizations of @xmath1 and @xmath0 . * for the @xmath98 and @xmath99 , we simply evaluate the minimum of the @xmath91 analytically .",
    "therefore , we have four distinct piecewise - constant methods in order to fit the data : method 1 the deceleration parameter @xmath1 , method 2 ) the dark energy equation of state @xmath0 , method 3 the hubble parameter @xmath2 and method 4 the luminosity distance @xmath49 .",
    "the reason we treat the last two methods differently is that for the luminosity distance @xmath49 , we do not have to perform the pca as the covariance matrix is already diagonal , i.e. the parameters are uncorrelated , while for the hubble parameter @xmath2 we found that the pca fails for two reasons : first , because the parameters @xmath101 are highly correlated in a manner that a linear transformation , i.e. the pca , can not disentangle them , and also the fisher matrix has a highly unusual structure with several elements repeated across its rows and columns , see eq .",
    "( [ fisherh ] ) in the appendix . also , we found that due to the fact that the parameters @xmath101 are highly correlated a mcmc approach also fails since the sampler was always stuck along the degenerate lines @xmath102 and far away from the minimum .",
    "then , we use the best - fit values in each case and the formulas found in the appendix for each method , in order to calculate the  derived \" parameters .",
    "for example , in the third method for the hubble data , first we calculate the best fit and then we use the expressions to calculate the parameters @xmath46 , @xmath47 and @xmath49 . to summarize ,",
    "our methodology is as follows :    1 .",
    "find the best fits for all the methods , either with a mcmc or analytically .",
    "2 .   if needed , do the pca to diagonalize the covariance matrix ( @xmath46 and @xmath47 only ) .",
    "3 .   find the derived parameters in each case .",
    "4 .   compare the methods .    in fig .",
    "[ fig : flowchart ] we present a flowchart that illustrates and clarifies our methodology , while in fig .",
    "[ fig : pca ] we plot the pca values of the deceleration parameter @xmath46 and the equation of state @xmath47 for the 10 bins ( first and second row respectively ) , while in table [ tab : pca - q - w ] we show their corresponding values and @xmath103 errors . in fig .",
    "[ fig : pca ] we show the best - fit parameters for the hubble parameter @xmath48 and @xmath104 ( third and fourth row ) , while in table [ tab : bf - h - dl ] we show their corresponding values and @xmath103 errors . in both cases ,",
    "the different columns correspond to the different cosmologies @xmath5cdm , cpl and @xmath7 , as indicated by the labels .",
    "a limitation with some of the methods is that they require values for @xmath63 in order to get an estimate of @xmath0 , see eq .",
    "( [ eq : w - fromq ] ) or values of @xmath105 like in the fourth method where it can not be estimated by the data or at least marginalized over . in this case , since we are not supposed to know the true parameters of our cosmology we will use the planck best fits @xmath106 and @xmath107 . in all cases we took care to propagate the errors from @xmath63 and @xmath105 to the derived parameters .",
    ".pca values for @xmath46 and @xmath47 and their @xmath108 errors for three different cosmologies .",
    "note that while we report the best - fit value of the @xmath91 from the mcmc , i.e. the value at the minimum @xmath109 , the values of the parameters are the ones that result from the pca and not the best - fit ones .",
    "[ tab : pca - q - w ] [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]      is piecewise - constant in each bin ( left ) and when @xmath46 is piecewise - constant in each bin . in the first case",
    ", luminosity distance clearly has a discontinuity at the edge of the bins , while in the second case it is continuous .",
    "this discontinuity causes the large @xmath91 as mentioned in table ii .",
    ", title=\"fig : \" ]   is piecewise - constant in each bin ( left ) and when @xmath46 is piecewise - constant in each bin .",
    "in the first case , luminosity distance clearly has a discontinuity at the edge of the bins , while in the second case it is continuous .",
    "this discontinuity causes the large @xmath91 as mentioned in table ii .",
    ", title=\"fig : \" ]        cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 .",
    "in all the plots the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] + cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters",
    "( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] + cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 .",
    "in all the plots the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 .",
    "in all the plots the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] + cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ] cdm cosmology , center column to the cpl cosmology and right column to the hu & sawicki @xmath7 cosmology with @xmath110 and @xmath111 . in all the plots",
    "the red dashed line is the corresponding theoretical functions used to create the mock catalogs . for the @xmath46 and @xmath47 parameters ( first and second rows ) we show the pca values given by eqs .",
    "( [ eq : pca - values ] ) and ( [ eq : pca - errors ] ) , while for @xmath48 and @xmath49 ( third and fourth rows ) we show the best - fit values.,title=\"fig : \" ]    schematically , the difficulties of the different methods are    * piecewise - constant @xmath96 : when we want to extract information on the eos parameter @xmath47 , we need to assume a value for the matter density @xmath63 , for which we use the planck prior mentioned above ; this is because the expression of the luminosity distant eq .",
    "( [ eq : dl - q ] ) does not depend on @xmath63 , consequently we can not use the chains to estimate the best - fit value of the matter density . on the other hand ,",
    "the hubble parameter @xmath98 and the luminosity distance @xmath112 do not depend on @xmath63 .",
    "* piecewise - constant @xmath97 : @xmath63 is a free parameter and we can find the best - fit value of @xmath63 directly from the chain along with the values of @xmath97 .",
    "this value for @xmath63 will be propagated to all the other parameters , @xmath96 , @xmath98 and @xmath112 , as they all depend on the matter density @xmath63 . * piecewise - constant @xmath98 :",
    "when we want to extract information on the eos parameter @xmath47 , we need to assume a value for the matter density @xmath63 , for which we use the planck prior mentioned above .",
    "on the other hand , the deceleration parameter @xmath96 and the luminosity distance @xmath112 do not depend on @xmath63 .",
    "however , using this method to find the hubble parameter , we need to assume a value for the hubble constant @xmath105 .",
    "* piecewise - constant @xmath113 : in this case the deceleration parameter and the hubble parameter do not depend on @xmath63 , but the eos parameter does so the matter density parameter needs to be propagated . also using this method , in order to find the best fit of the quantities @xmath112 we need to use a prior for the hubble constant @xmath105 .    as can be seen in tables .",
    "[ tab : pca - q - w ] and [ tab : bf - h - dl ] ( and also from fig .",
    "[ fig : pca ] ) , the method that gives the least errors is the direct measurement of the luminosity distance @xmath104 ( which is the dimensionless luminosity distant ) , where the errors are of the order of @xmath114 in average with respect to their corresponding measurements . we should remind that we have not marginalized over the hubble constant @xmath105 , for which we assumed it as constant .",
    "this result is quite obvious since snia directly measure the distance modulus @xmath88 which is connected to the luminosity distance ; this is also reflected in table  [ tab : risk - lcdm ] where the best reconstruction of the derived parameters comes from the direct measurement of the luminosity distance .    finally , we should stress that the values shown in table [ tab : bf - h - dl ] correspond to the dimensionless luminosity distance @xmath49 , so the errors do not have any units . regarding the small value of the errors ,",
    "these can be explained by understanding how the propagation of the errors occurs .",
    "the error on @xmath49 will be given by the derivative of @xmath49 with respect to @xmath89 , see ( [ eq : dli ] ) , times the error of the best - fit distance modulus , i.e. @xmath115 , but @xmath116 is given by ( [ muerrors ] ) and ( [ eq : snapp ] ) @xmath117 , where @xmath118 for all points , so that @xmath119 and @xmath120 .      in this section",
    "we will compare the different methods and we will also study their advantages and weakness with respect to each other .",
    "let us define two quantities , called the bias and variance , @xcite and @xcite : & = & _",
    "i=1^n_tot(y(z_i)-y_real(z_i))^2 + & = & _ i=1^n_tot(y(z_i))^2 , where @xmath121 are the reconstructed parameters in each bin i.e. @xmath122 , @xmath123 are the  real \" values of the parameters , and @xmath124 is the number of points .",
    "the bias tells us how different are the reconstructed parameters from the real ones , while the variance tells us how big are the errors .",
    "then we also define the risk as the sum of the two = + , [ riskstat ] so that reconstruction methods that give results closer to the real cosmology and have smaller errors , will have a smaller value for the risk .",
    "we prefer to use the risk rather than the usual @xmath91 analysis for two reasons : first , the risk is precisely aimed at measuring the closeness of an estimated quantity to the corresponding theoretical function taking in consideration also the errors of the estimated quantity ; second , as reported in details in the appendix b , the @xmath91 analysis fails to describes the goodness of the data in this analysis . in this case",
    ", the risk better describes the goodness of the reconstructed quantities as we are not interested to find the best fit to the data but rather to find the reconstructed quantities that give the original cosmology .",
    "let us make an example to clarify the problem with the @xmath91 .",
    "imagine we have two different cases : in the first case , the reconstructed quantity is far from the real one but it has small errors , and in the second case the reconstructed quantity is very close to the real one but it has large errors .",
    "the two reconstructions might give the same risk implying that they are equally bad .",
    "if instead we use the @xmath91 , then the second case would be preferred with respect to the first one , thus giving a biased result .",
    "since our aim is to find the best reconstruction of the curve with the smallest errors , the second case is not to be preferred to the first one .    for the sake of completeness",
    ", we also evaluated the @xmath91 for all the four reconstructions for the three different cosmologies in appendix b. however , in this context , these values should be not taken seriously because the @xmath91 fails to describe the closeness of reconstructed quantities , as previously reported .",
    "the results for the risk for the three cosmologies are shown in tables [ tab : risk - lcdm ] , [ tab : risk - cpl ] and [ tab : risk - f_r ] respectively .",
    "the columns indicate the methods use to fit the data , while the rows indicate the reconstructed parameters .",
    "for example , if we want the value of the risk parameter for the luminosity distance @xmath49 and for the @xmath1 piecewise - constant method , we have to pick the element in the second column and fourth row , e.g. in table [ tab : risk - lcdm ] that value would be 0.056 .",
    "if we choose one parameter of interest ( say @xmath47 ) and ask what is the best method to reconstruct this parameter , then we find that the best method is always the luminosity distance piecewise - constant scheme , followed by @xmath47 . between the two pca methods considered in this analysis ( @xmath46 and @xmath47 )",
    ", @xmath47 is better at reconstructing both @xmath46 and @xmath47 .",
    "the problem here is that if we want to reconstruct @xmath47 from @xmath46 , then the error propagation formula is _",
    "[ eq : errwfromq ] both terms in eq .",
    "( [ eq : errwfromq ] ) are proportional to @xmath125 which is an increasing function with redshift ; however , the small value of @xmath126 washes away the information of the matter density and the only contribution comes from the @xmath96 , leading to @xmath127 .",
    "this also explains the values of the risk of about @xmath128 ( first row and second column ) in table  [ tab : risk - lcdm ] .",
    "the same discussion can be applied to the case when we have @xmath48 and we want to reconstruct @xmath47 . in this case",
    "we have _ w = .",
    "[ eq : errwfromh ] the derivative in terms of @xmath48 scales as @xmath129 , whereas the one in terms of @xmath63 scales as @xmath130 .",
    "clearly , they both contribute equally to the final errors and they are much larger than unity , with the first term being dominant at low redshifts , whereas the second term dominates at high redshifts .",
    "also , we should note that the equation used for deriving @xmath47 from @xmath48 is similar to the equation used to derive @xmath47 from @xmath46 , something which explains the magnitude of the errors ( see for instance first row and third column in table  [ tab : risk - lcdm ] .",
    "finally , it should be stressed that all methods have certain limitations .",
    "for example , as mentioned in the previous paragraph , a value for @xmath63 is required in order to get an estimate of @xmath0 , see eq .",
    "( [ eq : w - fromq ] ) or values of @xmath105 , like in the fourth method , are needed since @xmath105 can not be estimated by the data or at least marginalized over .",
    "as we are interested in making a  blind \" comparison of the methods , we are not supposed to know the true values of the parameters of our underlying real cosmology , so we used the planck best fits previously defined . in all cases we took care to propagate the errors from @xmath63 and @xmath105 to the derived parameters .",
    "as a final remark , it is interesting to notice that the errors for the four reconstructed quantities @xmath1 , @xmath0 , @xmath2 , @xmath8 increase with redshift .",
    "the reasons are the following : for the deceleration parameter @xmath1 and the eos parameter @xmath0 we performed the pc analysis and the transformation matrix @xmath42 is composed by the eigenvectors of the fisher matrix whose rows , that are the eigenvectors , have been ordered according to the corresponding eigenvalues . in practice",
    ", the first row is the eigenvector with the smallest eigenvalue and the last row is the eigenvector with the largest eigenvalue , see sec .",
    "[ sec : background ] .    for the hubble parameter @xmath2 and the luminosity distant @xmath8 the errors increase with redshift simply because the propagation formula is a linear function of the redshift .      &",
    "@xmath47 & @xmath46 & @xmath48 & @xmath49   + @xmath47 & @xmath131 & @xmath132 & @xmath133 & @xmath134   + @xmath46 & @xmath135 & @xmath136 & @xmath137 & @xmath138   + @xmath48 & @xmath139 & @xmath140 & @xmath141 & @xmath142   + @xmath49 & @xmath143 & @xmath144 & @xmath145 & @xmath146   +      & @xmath47 & @xmath46 & @xmath48 & @xmath49   + @xmath47 & @xmath147 & @xmath148 & @xmath149 & @xmath150   + @xmath46 & @xmath151 & @xmath152 & @xmath153 & @xmath154   + @xmath48 & @xmath155 & @xmath156 & @xmath157 & @xmath158   + @xmath49 & @xmath159 & @xmath160 & @xmath161 & @xmath162   +      & @xmath47 & @xmath46 & @xmath48 & @xmath49   + @xmath47 & @xmath163 & @xmath164 & @xmath165 & @xmath166   + @xmath46 & @xmath167 & @xmath168 & @xmath169 & @xmath170   + @xmath48 & @xmath171 & @xmath172 & @xmath173 & @xmath142   + @xmath49 & @xmath174 & @xmath175 & @xmath145 & @xmath146   +",
    "in this paper we compared the four different methods that can be used to analyze the type ia supernovae ( snia ) data , i.e. use different piecewise - constant functions , such as : the dark energy equation of state @xmath0 , the deceleration parameter @xmath1 , the hubble parameter @xmath2 and finally the luminosity distance @xmath8 .",
    "these four quantities cover all main aspects of the accelerating universe , i.e. the phenomenological properties of dark energy , the expansion rate ( first and second derivatives ) of the universe and the observations themselves .",
    "for the first two cases we also performed principal component analysis ( pca ) so as to decorrelate the parameters , while for the last two cases we used a set of novel analytic expressions for the best fit .",
    "we derived the equations for the pca for the two methods ( @xmath47 and @xmath46 ) , while for the other two ( @xmath48 and @xmath49 ) we used their best fits as for the former we found that the parameters are very highly correlated so that a linear transformation , i.e. the pca , can not decorrelate them and that due to the degeneracy the mcmc also fails , while for the latter we found that the covariance matrix is already diagonal .    in order to test the methods we created sets of mock snia data ( 2000 points uniformly distributed in redshift @xmath4 $ ] ) for three fiducial cosmologies , the cosmological constant model ( @xmath5cdm ) , a linear expansion of the dark energy equation of state parameter @xmath6 and the hu - sawicki @xmath7 model .",
    "then we fitted the piecewise schemes on the mock data , either with a mcmc or by using the analytic formulas , and we found the best - fit parameters , to which we applied the pca for the @xmath47 and @xmath46 methods .    in the last step of our methodology , we compared the four different forms of the pca using the risk statistic defined in eq .",
    "( [ riskstat ] ) and ranked the methods accordingly .",
    "the final results of our analysis can be seen in tables [ tab : risk - lcdm ] , [ tab : risk - cpl ] and [ tab : risk - f_r ] .",
    "these tables can help us answer the question : given a parameter of interest , what is the best piecewise - constant method to reconstruct it ?    to answer this question it is best to initially focus on the two mainstream approaches for the pca , i.e. the @xmath0 and @xmath1 piecewise - constant methods . by inspecting tables [ tab : risk - lcdm ] , [ tab : risk - cpl ] and [ tab : risk - f_r ]",
    "we see that , given a parameter of interest , i.e. moving horizontally on the tables , then the best piecewise - constant scheme is always @xmath0 .",
    "if we also take the other two parameters into account , then we see that the best piecewise - constant method overall is @xmath8 , i.e. traditional binning , due to the fact that in this case the errors on the best - fit parameters are significantly smaller than in the other cases . in general",
    ", all methods suffer from a few limitations , for example using the @xmath1 scheme implies that in order to get a constraint on @xmath0 we have to assume a value for @xmath63 , as that can not be estimated from the data alone .",
    "overall , the novelty of our analysis is twofold : we performed a systematic comparison of all the methods and subsequently tested them against mock data and against each other , for the first time in the literature and second , we presented several new analytical expressions for all four different forms of piecewise - constant methods .",
    "our analysis will be immensely useful with the upcoming surveys that will collect a plethora of new data that will have to be analyzed in a systematic fashion and their cosmological information extracted .",
    "moreover , it is rather straightforward to include other kinds of data as well and get even more stringent constraints on the parameters , but also to use more realistic mocks to see which of the methods performs the best in more realistic scenarios .",
    "we would like to thank k.  rolbiecki and the anonymous referee for useful suggestions to the manuscript .",
    "the authors acknowledge financial support from the madrid regional government ( cam ) under the program hephacos s2009/esp-1473 - 02 , from micinn under grant no . aya2009 - 13936-c06 - 06 and consolider - ingenio 2010 pau ( csd2007 - 00060 ) , as well as from the european union marie curie initial training network unilhc grant",
    "pitn - ga-2009 - 237920 .",
    "we also acknowledge the support of the spanish mineco s centro de excelencia severo ochoa programme under grant no .",
    "sev-2012 - 0249 .",
    "we start piecewise - constant the deceleration parameter as q(z ) = _ i=1^nq_i(z_i ) , where @xmath56 are constant in each redshift bin @xmath57 and @xmath58 is the theta function , i.e. @xmath176 for @xmath177 and @xmath33 elsewhere .",
    "the general expression for the deceleration parameter is 1+q(z)=. the last equation can be inverted to find the hubble parameter : ( h(z)/h_0)=_0^zdx , or h(z)/h_0",
    "= e^i(z ) i(z ) = _ 0^zdx . for @xmath178",
    "$ ] and using the fact that @xmath46 is constant in each bin , we can break the integral @xmath179 in parts as i(z)&=&_0^z_1( ...",
    ")+_z_1^z_2( ... )+ ... +_z_i-1^z ( ... ) + & = & ( 1+q_1)(1+x)|_0^z_1+(1+q_2)(1+x)|_z_1^z_2+ ...",
    "+(1+q_i)(1+x)|_z_i-1^z + & = & ( 1+q_1)(1+z_1)+(1+q_2)()+ ... +(1+q_i ) ( ) . grouping the constant terms , the hubble parameter can then be written , for @xmath51 in the nth bin , as h_n(z ) = h_0 b_n ( 1+z)^1+q_n where the coefficient @xmath59 is b_n = _ j=1^n-1(1+z_j)^q_j - q_j+1 .",
    "we can now follow a similar procedure to calculate the luminosity distance . using the definition of the luminosity distance along with the previous equations we have d_l(z)&= & ( 1+z )",
    "_ 0^z x + & = & ( 1+z ) ( _ 0^z_1( ... )+_z_1^z_2( ... )+ ... +_z_i-1^z ( ... ) ) + & = & ( 1+z ) ( + + ... + ) . collecting the constant terms , the latter can be written as d_l , n(z)=(1+z ) , where f_n = + _ j=1^n-1 , being @xmath61",
    ".      we now want to apply the pca directly to the dark energy eos parameter @xmath0 .",
    "as previously done for @xmath46 , we rewrite @xmath0 as w(z ) = _ i=1^n_totw_i(z_i ) , where @xmath64 are constant in each redshift bin @xmath57 , @xmath124 is the total number of bins and @xmath58 is the theta function , i.e. @xmath176 for @xmath177 and @xmath33 elsewhere.using the energy - momentum conservation @xmath65 for an ideal fluid with equation of state @xmath47 we get the equation _",
    "de+3(1+w ) h_de=0 , which can be solved and written in terms of the redshift @xmath51 .",
    "then , the general expression for the de density in terms of a time - dependent @xmath0 is _",
    "de(z)=_de(z=0)e^3_0^z dz , and this can be rewritten as ( ) = i(z ) , [ eq : rhode1 ] where i(z)=_0^zdx [ eq : rhode2 ] proceeding like before , for @xmath178 $ ] and using the fact that @xmath47 is constant in each bin , we can break the integral @xmath179 in parts as i(z)&=&_0^z_1( ...",
    ")+_z_1^z_2( ... )+ ... +_z_i-1^z ( ... ) + & = & ( 1+w_1)(1+x)|_0^z_1+(1+w_2)(1+x)|_z_1^z_2+ ...",
    "+(1+w_i)(1+x)|_z_i-1^z + & = & ( 1+w_1)(1+z_1)+(1+w_2)()+ ... +(1+w_i ) ( ) .",
    "grouping the constant terms , the dark energy density can then be written , for @xmath51 in the nth bin , as _",
    "de(z , n)=_de(z=0 ) c_n ( 1+z)^3(1+w_n ) , where the coefficient @xmath66 is c_n = _ j=1^n-1(1+z_j)^w_j - w_j+1 , and obviously for @xmath110 we have @xmath180 , since by definition @xmath181 .",
    "then , if we also include matter , the hubble parameter can be written as h(z , n)^2/h_0 ^ 2=(1+z)^3+(1-)c_n ( 1+z)^3(1+w_n ) . since @xmath180 we have that for @xmath182 , i.e. for the first bin or @xmath110 , @xmath183 as expected .",
    "we can now follow a similar procedure to calculate the luminosity distance . using the definition of the luminosity distance along with the previous equations we have d_l(z)&= & ( 1+z ) _ 0^z x + & = & ( 1+z ) ( _ 0^z_1( ... )+_z_1^z_2(",
    "... )+ ... +_z_i-1^z ( ... ) )",
    ". however , in this case the integrals are significantly more complicated due to the presence of the matter term .",
    "so , for the @xmath184 term we have : & & d_i(z_i , z_i-1)_z_i-1^z_i= + & & - \\ { - } ,   where @xmath67 is a hypergeometric function defined by the series _ 2f_1(a ,",
    "b;c;z)^_n=0z^n on the disk @xmath185 and by analytic continuation elsewhere ; see ref .",
    "@xcite for more details .",
    "now , if we sum up all the terms , the luminosity distance becomes : d_l , n(z)= ( 1+z)(d_n(z , z_n-1)+_i=1^n-1d_i(z_i , z_i-1 ) ) , where the last term in the parentheses is just a constant and as always we assume @xmath61 . finally , we have also checked numerically that the expressions above give the correct results .",
    "here we write explicitly the derivation of the cosmological parameters starting from the binned hubble parameter @xmath2 .",
    "let us write the hubble parameter as h(z)/h_0 = _",
    "i=1^n_toth_i(z_i ) , where @xmath69 are constant in each redshift bin @xmath57 , @xmath124 is the total number of bins and @xmath58 is the theta function , i.e. @xmath176 for @xmath177 and @xmath33 elsewhere . using the definition of the luminosity distance along with the previous equations we have d_l(z , n)&= & ( 1+z ) ( _ 0^z x ) + & = & ( 1+z ) ( _ 0^z_1(h_1 ^ -1 z)+_z_1^z_2(h_2 ^ -1 z)+ ... +_z_n-1^z(h_n^-1 z ) ) + & = & ( 1+z ) ( h_1 ^ -1 z_1+h_2 ^ -1 ( z_2-z_1)+ ...",
    "+h_n^-1(z - z_n-1 ) ) + & = & ( 1+z ) ( _ i=1^n-1z_i(h_i^-1-h_i+1 ^ -1)+h_n^-1 z ) + & = & ( 1+z ) ( g_n+h_n^-1 z ) , where we have defined the constants @xmath70 .",
    "in what follows we will focus on the case of bins with constant size , i.e. @xmath186 so that @xmath187 , but our results can easily be generalized for bins of different sizes as well as well .",
    "now , we transform the data from the distance modulus @xmath89 to @xmath188 .",
    "then , the theoretical value is @xmath189 , where @xmath190 and @xmath191 and the chi square can be written as ^2=_i=1^n()^2,[chi2f ] where the errors were found by standard error propagation @xmath192 .",
    "the advantage of this method is that the chi square of eq.([chi2f ] ) is quadratic with respect to the parameters @xmath193 and can be minimized analytically . at this point",
    "it is convenient to define the following quantities : s&= & _ i=1^n ,  s_z^2 = _ i=1^n , + s_f^2 & = & _ i=1^n ,  s_fz = _ i=1^n , and s_n&= & _ j=1 , ,  s_f_n = _ j=1 , ,  s_fz_n = _ j=1 , , + s_z_n & = & _ j=1 , ,   s_z^2_n = _ j=1 , , where @xmath194 is meant to sum over only those points in the @xmath195 bin , by which it follows that _",
    "n=1^n_tots_n = s , where @xmath124 is the number of bins .    with these definitions",
    "it is easy to minimize the @xmath91 analytically , following the methodology of refs .",
    "@xcite and @xcite .",
    "the first step is to expand the @xmath91 as follows : ^2&= & _",
    "i=1^n(f_i^2+^2 c_n^2+^2_n^2 z_i^2 - 2 f_i c_n-2f_i _",
    "n z_i+2c_n _",
    "n ^2 z_i ) + & = & s_f^2+_n=1^n_tot(^2 c_n^2s_n+^2 _ n^2 s_z^2_n-2c_n s_f_n-2_n s_fz_n+2 ^ 2 c_n _",
    "n s_z_n ) .",
    "now we can define the matrix a_nm = dz(_k=1^n_km -n _ nm ) .",
    "then , the first derivatives of the @xmath91 are : _ k12_k ^2=_k ^2 s_z^2_k - s_fz_k+c_k ^2 s_z_k+_n=1^n_tota_nk(c_n ^2 s_n - s_f_n+_n ^2 s_z_n ) , while the second derivatives , i.e. the fisher matrix evaluated at the best fit , are _ kl & & 12 ^ 2_kl^2|_min=_l _ k + & = & ^2(_kl s_z^2_k+a_kls_z_k+a_lks_z_l+_n=1^n_tota_nk a_nl s_n).[fisherh ] if we define the matrices : b_kl&=&^2(_kl",
    "s_z^2_k+a_kls_z_k+a_lks_z_l ) + d_nk&=&a_nk s_n^1/2 , then the fisher matrix can be written as = b+d^td , while the covariance matrix is = ^-1=b^-1-b^-1d^t(i+d b^-1 d^t)^-1d b^-1 , where the last equation comes from considering the inverse of a sum of matrices , see ref .",
    "@xcite for details .",
    "as mentioned earlier , in this case the @xmath91 is quadratic with respect to @xmath196 , so we can use the methodology of refs .",
    "@xcite and @xcite . clearly , in this case we can write the @xmath91 as ^2=^2_min+(-_min)_i _ ij(-_min)_j , which means that _ k=_kj(-_min)_j and that the best - fit parameters and the minimum @xmath91are _ min , j&=&-_jk^-1_k|__i=0[hbf ] + ^2_min&=&s_f^2-_ij_min , i_min , j[x2bf ] , where _ k|__i=0=-(s_fz_k+_n=1^n_tota_nks_f_n)[bibf ] as can be seen from eqs .",
    "( [ fisherh ] ) , ( [ hbf ] ) and ( [ bibf ] ) the various parameters scale differently with @xmath197 or equivalently @xmath87 , eg the the fisher matrix scales as @xmath198 , while the best fit parameters as @xmath199 . on the other hand , as seen from eq .",
    "( [ x2bf ] ) the minimum chi square @xmath109 is invariant since the contributions from @xmath200 and @xmath201 cancel out .",
    "this means that in this case the best - fit is degenerate with respect to @xmath87 and as a result we have to fix it to some value before the actual fit .",
    "finally , we can also rotate the parameters to a basis where they are not correlated with each other , as in ref .",
    "@xcite . to do so",
    "we define a new variable @xmath202 , where @xmath203 can be found by decomposing the inverse fisher matrix @xmath204 by using cholesky decomposition is given by @xmath205 $ ] .",
    "this works both symbolically and numerically . ]",
    "then , going to the new basis we have s_i & & d_ij(_j-_j , min)[transf1 ] + ds_1 ... ds_n&=&d d_1 d_2 ... d_n_tot + d & = & ^1/2= ^-1/2 . [ transf3 ]",
    "also , it can be easily shown that for the uncorrelated parameters @xmath206 we have ^2=^2_min+s_1 ^ 2+s_2 ^ 2+ ... +s_n_tot^2 .",
    "the fisher matrix of the original @xmath69 parameters will be given by @xmath207 , where @xmath208 is given by eq .",
    "( [ fisherh ] ) and @xmath209 is the jacobian of the transformation .    in order to find the dark energy eos parameter @xmath47 we can use eqs .",
    "( [ eq : rhode1 ] ) and ( [ eq : rhode2 ] ) .",
    "it is important to realize that the values of the @xmath210 parameters actually correspond to the average redshift in the bin , i.e. @xmath211 , so that we can evaluate eq .",
    "( [ eq : rhode1 ] ) at two different redshifts @xmath81 and @xmath80 , and subtract to get _",
    "z_eff , i-1^z_eff , iz=13 ( ) .",
    "in general we can not evaluate the left hand side of the above equation , but if we use the mean value theorem for integration , then we can write it as _ z_eff , i-1^z_eff , iz&=&_z_eff , i-1^z_eff , i z + & = & ( ) , where @xmath212 .    for example , using an evolving de equation of state like @xmath213 it is easy to calculate @xmath214 using the above formulas .",
    "using the fact that for equal sized bins we have @xmath215 and that @xmath216 then , in this case we find that x & & ( -1 + i ) dz - dz^2/6 + ... , + & & z_i-1- dz^2/6 + ... , where the first term is the redshift of the lower bin and the second is a correction .",
    "surprisingly , in this case @xmath214 does not depend on on the parameters @xmath217 and @xmath218 . for small bins or large redshifts",
    ", the last term usually is negligible and we have confirmed this with numerical tests .",
    "for example , for @xmath219 the two terms are 0.1 and -0.0016 respectively , while for larger bins like @xmath220 the terms are 0.1 and -0.027 , thus confirming our assumptions .",
    "of course , in the case of rapidly evolving equation of state these assumptions do not necessarily hold any more .",
    "this can easily be seen by considering a model of the form @xmath221 , where the second derivative @xmath222 is not necessarily small , i.e. we can not assume @xmath223 . then , the parameter @xmath214 is given by x & & dz ( i-1 ) + dz^2 ( -4)+ ... , + & & z_i-1 + dz^2 ( -4)+ ... .",
    "clearly , in this case there might be a small effect due to the cosmology .",
    "however , models with fast transitions of the equation of state seem to be disfavored by observations @xcite , so in what follows we will assume that @xmath0 may only be evolving slowly .",
    "therefore , to excellent approximation we consider that @xmath224 and the de equation of state at a bin @xmath22 will be given by w_n(x_i)&=&-1 + , + x_i & & z_i-1 .",
    "intuitively the above result can be understood as follows .",
    "the parameters @xmath225 correspond to the redshift in the middle of the bins , so taking their differences produces a result that corresponds to the sides of the bins .      as mentioned earlier in the paper",
    ", we do not bin directly the luminosity distance @xmath226 but rather the distance moduli @xmath84 : ( z ) = _ i=1^n_tot_i(z_i ) , where @xmath89 are constant in each redshift bin @xmath57 , @xmath124 is the total number of bins and @xmath58 is the theta function , i.e. @xmath176 for @xmath177 and @xmath33 elsewhere . in this case , the chi squared can be written as ^2=_i=1^n()^2 . clearly ,",
    "the @xmath91 is linear with respect the parameters @xmath113 , so in this case we can find closed - form analytical expressions for the best - fit parameters .",
    "first , we will make the following definitions s & & _",
    "i=1^n  s__i=1^n , + s_^2 & & _ i=1^n   s_n_j=1,,[eq : snapp ] + s__n & & _ j=1 , , where @xmath194 is meant to sum over only those points in the @xmath195 bin , by which it follows that _ n=1^n_tots_n&=&s , + _ n=1^n_tots__n&=&s _ , where @xmath124 is the total number of bins . with these in mind",
    "we can now find the best fit by taking the derivatives with respect to the parameters _",
    "_ n^2|_min&=&_i=1^n 2 ( ) ( - ) + & = & -2_j=1 , ( ) + & = & -2s__n+ 2_n s_n + & = & 0 , [ chi2deriv ] where we have used the fact that @xmath227 and that the first derivative should be zero at the minimum . then , eq .  ( [ chi2deriv ] )",
    "can readily be solved to yield _",
    "n=. [ bestmu ]",
    "a similar calculation reveals that the chi square and its value at the minimum are ^2 ( _ n)&=&s_^2 - 2 _",
    "k=1,_k s__k+_k=1,_k^2 s_k [ chi2dl ] + ^2_min&=&s_^2-_n=1^n_tot .",
    "the errors on the best - fit parameters can be estimated by direct error propagation , see chapter 15 of ref .",
    "@xcite , as _ _",
    "n^2&=&_i=1^n_i^2()^2 + & = & _ i=1^n _ i^2 ( ) ^2 + & = & _ i=1^n _ i^2 ( _ j=1,)^2 + & = & _ i=1^n _ i^2 ( _ j=1,_i , j ( n bin))(_k=1,_i , k ( n bin ) ) + & = & _ i=1^n _ j=1,_k=1,_i^2 _ i , j ( n bin)_i , k ( n bin ) + & = & .[muerrors ] we find that the results of eqs .",
    "( [ bestmu ] ) and ( [ muerrors ] ) are in agreement with the ones found by considering the binning of data . following a more direct approach , by calculating directly the fisher and the covariance matrices from eq .",
    "( [ chi2dl ] ) evaluated at the minimum , we get f_nk&=&12 _",
    "nk^2 ^ 2|_=(s_1,s_2, ... ,s_n_tot ) + c_nk&=&f_nk^-1=(s_1 ^ -1,s_2 ^ -1, ...",
    ",s_n_tot^-1 ) , where @xmath228 is a @xmath229 diagonal matrix .",
    "the diagonal terms of the covariance matrix are the errors @xmath230 and they are in exact agreement with eq .",
    "( [ muerrors ] ) .",
    "finally , we should note that the covariance matrix is diagonal , which means that the parameters are already uncorrelated and we do not have to follow the pca approach in this case .    in order to extract the cosmology we can invert the equation of the distance modulus and we find the dimensionless luminosity distance to be d_l , i=10^[eq : dli ] where @xmath90 .",
    "another way to test which of the four methods reconstructs the  real \" cosmologies the best , is to use a chi square : ^2&=&_i=1^n_tot where @xmath121 are the reconstructed ( best - fit or derived ) parameters in each bin i.e. @xmath231 , @xmath123 are the  real \" values of the parameters , and @xmath124 is the number of points .",
    "the results for the @xmath91 for the three cosmologies are shown in tables [ tab : chi2-lcdm ] , [ tab : chi2-cpl ] and [ tab : chi2-f_r ] respectively .",
    "the columns indicate the methods use to fit the data , while the row the reconstructed parameters .",
    "however , this method suffers from several problems .",
    "for example , let us consider the results for the model , shown in table  [ tab : chi2-lcdm ] ; as it can be seen the worst results are given when we try to reconstruct the luminosity distance , for which we have a @xmath91 of about @xmath232 .",
    "how could be this possible ?",
    "the reason is that the errors of the binned luminosity distance @xmath3 are very small , of about @xmath233 and the @xmath91 is proportional to the inverse of the square of the errors ; however this is not the only reason why the @xmath91 is extremely large .    as we can see from the fourth row in fig .",
    "[ fig : pca ] , the best fit values of the luminosity distance are far from the theoretical curve ( red dashed line in the same figure ) .",
    "it is worth mentioning that here we are trying to reconstruct functions using mock catalogs evaluated with a specific cosmology , with the hope of getting the initial cosmology at the end , in other words verifying that our reconstruction methods indeed work as advertised .",
    "the difference from the theoretical curve and the pca values make the @xmath91 explodes as the numerator of the @xmath91 will not be sufficiently small to kill the @xmath234 in the denominator .",
    "this is the opposite to what it is usually done when we deal with data , where we have a dataset and we try to find the best fit , which mean to find those curves that better describe the data within the errors . in the latter case",
    "the @xmath91 will be in general small even if the errors are extremely small .    in this work we found that the general @xmath91 fails to describe the goodness or quality of our analysis , while the risk seems to be a more suitable parameter .",
    "as a final remark , we notice that the values shown in table  [ tab : chi2-lcdm ] follow a general trend which is the opposite to the risk , i.e. , when we go to a more complicated function then the errors increase and the @xmath91 decreases ( fourth column in table  [ tab : chi2-lcdm ] from down to up ) .",
    "this effect can also be explained , as it was mentioned in the previous paragraph , by the fact that the very small or large errors affect the estimation of the @xmath91 giving artificially large or small values even when the fit is obviously quite good .      &",
    "@xmath47 & @xmath46 & @xmath48 & @xmath49   + @xmath47 & @xmath235 & @xmath236 & @xmath237 & @xmath238   + @xmath46 & @xmath239 & @xmath240 & @xmath241 & @xmath242   + @xmath48 & @xmath243 & @xmath244 & @xmath245 & @xmath246   + @xmath49 & @xmath247 & @xmath248 & @xmath249 & @xmath250   +      & @xmath47 & @xmath46 & @xmath48 & @xmath49   + @xmath47 & @xmath251 & @xmath252 & @xmath253 & @xmath254   + @xmath46 & @xmath255 & @xmath256 & @xmath257 & @xmath258   + @xmath48 & @xmath259 & @xmath260 & @xmath261 & @xmath262   + @xmath49 & @xmath263 & @xmath264 & @xmath265 & @xmath266   +      & @xmath47 & @xmath46 & @xmath48 & @xmath49   + @xmath47 & @xmath267 & @xmath268 & @xmath269 & @xmath270   + @xmath46 & @xmath271 & @xmath272 & @xmath273 & @xmath274   + @xmath48 & @xmath275 & @xmath276 & @xmath277 & @xmath278   + @xmath49 & @xmath279 & @xmath280 & @xmath281 & @xmath282   +      r.  laureijs , _ et al . _ , arxiv:0912.0914 [ astro-ph.co ] .",
    "r.  laureijs , j.  amiaux , s.  arduini , j.  -l .",
    "augueres , j.  brinchmann , r.  cole , m.  cropper and c.  dabin _ et al . _ , arxiv:1110.3193 [ astro-ph.co ] . t.  abbott _ et al . _",
    "[ dark energy survey collaboration ] , [ astro - ph/0510346 ] .",
    "n.  benitez , e.  gaztanaga , r.  miquel , f.  castander , m.  moles , m.  crocce , a.  fernandez - soto and p.  fosalba _ et al .",
    "_ , astrophys .  j.   * 691 * , 241 ( 2009 ) . [ arxiv:0807.0535 [ astro - ph ] ] .",
    "p.  mart , r.  miquel , f.  j.  castander , e.  gaztaaga , m.  eriksen and c.  snchez , [ arxiv:1402.3220 [ astro-ph.co ] ] .",
    "d.  huterer and g.  starkman , phys .",
    "lett .   *",
    "90 * , 031301 ( 2003 ) [ astro - ph/0207517 ] .",
    "r.  de putter and e.  v.  linder , astropart .",
    "* 29 * , 424 ( 2008 ) [ arxiv:0710.0373 [ astro - ph ] ] .",
    "m.  j.  mortonson , w.  hu and d.  huterer , phys .",
    "d * 81 * , 063007 ( 2010 ) [ arxiv:0912.3816 [ astro-ph.co ] ] .",
    "a.  hojjati , g.  -b .",
    "zhao , l.  pogosian , a.  silvestri , r.  crittenden and k.  koyama , phys .",
    "d * 85 * , 043508 ( 2012 ) [ arxiv:1111.3960 [ astro-ph.co ] ] .",
    "d.  huterer and a.  cooray , phys .",
    "d * 71 * , 023506 ( 2005 ) .",
    "[ astro - ph/0404062 ] .",
    "d.  sapone , e.  majerotto and s.  nesseris , arxiv:1402.2236 [ astro-ph.co ] . w.  hu and i.  sawicki , phys .",
    "d * 76 * , 064004 ( 2007 ) [ arxiv:0705.1158 [ astro - ph ] ] .",
    "s.  basilakos , s.  nesseris and l.  perivolaropoulos , phys .",
    "d * 87 * , no .",
    "12 , 123529 ( 2013 ) [ arxiv:1302.6051 [ astro-ph.co ] ] .",
    "s.  nesseris and j.  garcia - bellido , phys .",
    "d * 88 * , 063521 ( 2013 ) , [ arxiv:1306.4885 [ astro-ph.co ] ] .",
    "abramowitz , milton ; stegun , irene a. , eds . ( 1972 ) , _ handbook of mathematical functions with formulas , graphs , and mathematical tables _ , new york : dover publications , isbn 978 - 0 - 486 - 61272 - 0      d.  huterer and g.  starkman phys",
    ".  rev .",
    "lett .   * 90 * , 031301 ( 2003 ) .",
    "[ astro - ph/0207517 ] .",
    "l.  wasserman _ et al .",
    "_ [ pica group collaboration ] , [ astro - ph/0112050 ] .",
    "w.  h.  press _ et .",
    "al . _ , ` numerical recipes ' , cambridge university press ( 1994 ) .",
    "s.  nesseris and j.  garcia - bellido , jcap * 1308 * , 036 ( 2013 ) [ arxiv:1210.7652 [ astro-ph.co ] ] .",
    "h. v.  henderson and s. r.  searle siam review , vol .",
    "* 23 * , no .",
    "1 ( jan . , 1981 ) , pp . 53 - 60 . published by : society for industrial and applied mathematics .",
    "article stable url : http://www.jstor.org/stable/2029838"
  ],
  "abstract_text": [
    "<S> we compare four different methods that can be used to analyze the type ia supernovae ( snia ) data , ie to use piecewise - constant functions in terms of : the dark energy equation of state @xmath0 , the deceleration parameter @xmath1 , the hubble parameter @xmath2 and finally the luminosity distance @xmath3 . </S>",
    "<S> these four quantities cover all aspects of the accelerating universe , ie the phenomenological properties of dark energy , the expansion rate ( first and second derivatives ) of the universe and the observations themselves . </S>",
    "<S> for the first two cases we also perform principal component analysis ( pca ) so as to decorrelate the parameters , while for the last two cases we use novel analytic expressions to find the best - fit parameters . in order to test the methods we create mock snia data ( 2000 points , uniform in redshift @xmath4 $ ] ) for three fiducial cosmologies : the cosmological constant model ( @xmath5cdm ) , a linear expansion of the dark energy equation of state parameter @xmath6 and the hu - sawicki @xmath7 model . we find that if we focus on the two mainstream approaches for the pca , i.e. @xmath0 and @xmath1 , then the best piecewise - constant scheme is always @xmath0 . finally , to our knowledge </S>",
    "<S> the piecewise - constant method for @xmath2 is new in the literature , while for the rest three methods we present several new analytic expressions . </S>"
  ]
}