{
  "article_text": [
    "the main purpose of this article is to study the quasi - newton methods from the view point of dualistic geometry or in other word _ information geometry _ @xcite .",
    "let us consider the unconstrained optimization problem @xmath0 in which the function @xmath1 is twice continuously differentiable on @xmath2 .",
    "the quasi - newton method is known to be one of the most successful methods for unconstrained function optimization . in quasi - newton method",
    "a sequence @xmath3 is successively generated in a manner such that @xmath4 , where @xmath5 is a step length computed by a line search technique .",
    "the matrix @xmath6 is a positive definite matrix which is expected to approximate the hessian matrix @xmath7 .",
    "the matrix @xmath6 and the step length @xmath5 are designed such that the sequence @xmath8 converges to a local minima of the problem . for the step length , the wolfe condition ( * ? ? ?",
    "* section 3.1 ) is a standard criterion to determine the value of @xmath5 . in terms of the approximate hessian matrix , mainly there are two methods of updating @xmath6 to @xmath9 ; one is called the dfp formula and the other is called the bfgs formula .",
    "we introduce the dfp and the bfgs methods .",
    "let @xmath10 and @xmath11 be column vectors defined by @xmath12 and suppose that @xmath13 holds . in the dfp formula",
    "the approximate hessian matrix @xmath6 is updated such that @xmath14    : = b_k-\\frac{b_ks_ky_k^\\top+y_ks_k^\\top b_k}{s_k^\\top y_k }   + s_k^\\top b_ks_k \\frac{y_k y_k^\\top}{(s_k^\\top y_k)^2}+\\frac{y_ky_k^\\top}{s_k^\\top y_k}. \\end{aligned}\\ ] ] in the bfgs update formula , the matrix @xmath9 is defined by @xmath15   : = b_k-\\frac{b_ks_ks_k^\\top b_k}{s_k^\\top b_ks_k}+\\frac{y_ky_k^\\top}{s_k^\\top y_k } , \\end{aligned}\\ ] ] under the condition that @xmath16 and @xmath13 , the matrices @xmath17 $ ] and @xmath18 $ ] are also positive definite matrices .",
    "if there is no confusion , the update formulae @xmath19 $ ] and @xmath20 $ ] are written as @xmath21 $ ] and @xmath22 $ ] , respectively . in practice , the cholesky decomposition of @xmath6 is successively updated in order to compute the search direction @xmath23 efficiently @xcite .",
    "note that the equality @xmath24^{-1}=b^{bfgs}[b^{-1};y , s]\\end{aligned}\\ ] ] holds .",
    "hence , we can derive the update formulae for the inverse @xmath25 without inversion of matrix .",
    "both the dfp and the bfgs methods are derived from variational problems over the set of positive definite matrices @xcite .",
    "let @xmath26 be the set of all @xmath27 by @xmath27 symmetric positive definite matrices , and the function @xmath28 be a strictly convex function over @xmath26 defined by @xmath29 fletcher @xcite has shown that the dfp update formula is obtained as the unique solution of the constraint optimization problem , @xmath30 where @xmath31 for @xmath32 is the matrix satisfying @xmath33 and @xmath34 .",
    "the bfgs formula is also obtained as the optimal solution of @xmath35 in which @xmath36 denotes @xmath37 or equivalently @xmath38 .",
    "it will be worthwhile to point out that the function @xmath39 is identical to kullback - leibler(kl ) divergence @xcite up to an additive constant . for @xmath40 ,",
    "the kl - divergence is defined by @xmath41 which is equal to @xmath42 .",
    "the kl - divergence is regarded as a generalization of squared distance . using the kl - divergence",
    ", we can represent the update formulae as the optimal solutions of the following minimization problems , @xmath43 the kl - divergence is asymmetric , that is , @xmath44 in general .",
    "hence the above problems will provide different solutions . in the information geometry @xcite",
    ", the kl - divergence defines a geometrical structure over the space of probability densities .",
    "statistical inference such that the maximum likelihood estimator is better understood based on the geometrical intuition .",
    "originally , the kl - divergence is defined as the discrepancy measure between two multinomial normal distributions with mean zero . in this paper",
    ", we show that the information geometrical approach is useful to understand the behaviour of quasi - newton methods . on the set of positive definite matrices , @xmath26",
    ", we define the so - called bregman divergence which is an extension of the kl - divergence .",
    "the bregman divergence induces a dualistic geometrical structure on @xmath26",
    ". then we can derive new hessian update formulae based on the bregman divergence .",
    "we present a geometrical view of quasi - newton updates , and discuss the relation between the hessian update formula and the statistical inference based on the information geometry .",
    "here is the brief outline of the article . in section [ sec : elements_information_geometry ] , we introduce the elements of information geometry based on the bregman divergence , especially over the set of positive definite matrices . in section [ sec : potential - function_quasi - newton ] , an extended quasi - newton formula is derived from the bregman divergence . section [ sec : invariance ] is devoted to discuss the invariance property of the quasi - newton update formula under the group action . in section [ sec : sparse - v - quasi - newton ] , we discuss the sparse quasi - newton methods @xcite from the viewpoint of the information geometry , and point out that the sparse quasi - newton method is closely related to statistical methods such as the em - algorithm @xcite or the boosting algorithm @xcite .",
    "we conclude with a discussion and outlook in section [ sec : concluding_remarks ] .",
    "some proofs of the theorems are postponed to appendix .    throughout the paper ,",
    "we use the following notations : the set of positive real numbers are denoted as @xmath45 .",
    "let @xmath46 be the determinant of square matrix @xmath47 , and @xmath48 denotes the set of @xmath27 by @xmath27 non - degenerate real matrices .",
    "@xmath49 is the set of @xmath27 by @xmath27 non - degenerate real matrices with determinant @xmath50 , that is , @xmath51 .",
    "the set of all @xmath27 by @xmath27 real symmetric matrices is denoted as @xmath52 , and let @xmath53 be the set of @xmath27 by @xmath27 symmetric positive definite matrices . for @xmath54 , the square root of @xmath55 is denoted as @xmath56 which is defined as @xmath55 for a vector @xmath57 , @xmath58 denotes the euclidean norm . for two square matrices",
    "@xmath59 , the inner product @xmath60 is defined by @xmath61 , and @xmath62 is the frobenius norm defined by the square root of @xmath63 . throughout the paper we only deal with the inner product of symmetric matrices , and the transposition in the trace",
    "can be dropped .",
    "we introduce bregman divergences which are regarded as an extension of the kl - divergence .",
    "then we illustrate a differential geometrical structure defined from the bregman divergence over the set of positive definite matrices . in sequel sections , we will provide a geometrical interpretation of quasi - newton methods .",
    "for general bregman divergences , however , the quasi - newton update formula can not be obtained in the explicit form . in order to obtain computationally tractable update formulae",
    ", we often use a specific bregman divergence which is called the @xmath64-bregman divergence in this article .",
    "first , we define general bregman divergences , and then we introduce the @xmath64-bregman divergence as a special case of general bregman divergences",
    ". we will show the associated geometrical structure on the set of positive definite matrices .",
    "the bregman divergence @xcite is defined through the so - called potential function .",
    "below , we define the bregman divergence over the set of positive definite matrices .",
    "let @xmath65 be a continuously differentiable , strictly convex function that maps positive definite matrices to real numbers .",
    "the function @xmath66 is referred to as potential function or potential for short . given a potential @xmath66 , the bregman divergence @xmath67 is defined as @xmath68 for @xmath69 , where @xmath70 is the @xmath27 by @xmath27 matrix whose @xmath71 element is given as @xmath72 .",
    "the bregman divergence @xmath73 is non - negative and equals zero if and only if @xmath74 holds .",
    "indeed , due to the strict convexity of @xmath66 , the function @xmath75 lies above its tangents @xmath76 at @xmath77 .",
    "hence , the non - negativity of the bregman divergence @xmath73 is guaranteed .",
    "note that @xmath73 is convex in @xmath55 but not necessarily convex in @xmath77 .",
    "bregman divergences have been well studied in the fields of statistics and machine learning @xcite .",
    "[ example : kl - div ] for @xmath78 let the function @xmath66 be @xmath79 .",
    "note that @xmath75 is a strictly convex function .",
    "then , we have @xmath80 hence the corresponding bregman divergence is @xmath81 is identical to the kl - divergence on the multivariate normal distribution with mean zero @xcite .",
    "by replacing the kl - divergence in or with a bregman divergence , we will obtain another variational problem for the quasi - newton method .",
    "in general , however , update formula can not be explicitly obtained .",
    "below we define a class of bregman divergences called @xmath64-bregman divergence . in section [ sec : potential - function_quasi - newton ] , we show that the @xmath64-bregman divergence provides an explicit update formula of the quasi - newton method .",
    "we prepare some ingredients to define the @xmath64-bregman divergence .",
    "let @xmath82 be a strictly convex , decreasing , and third order continuously differentiable function .",
    "for the derivative @xmath83 , the inequality @xmath84 holds from the condition .",
    "indeed , the condition leads to @xmath85 and @xmath86 , and if @xmath87 holds for some @xmath88 , then @xmath89 holds for all @xmath90 .",
    "hence @xmath91 is affine function for @xmath90 .",
    "this contradicts the strict convexity of @xmath64 .",
    "we define the functions @xmath92 and @xmath93 such that @xmath94 since @xmath95 holds for @xmath96 , the function @xmath97 is well defined on @xmath98 .",
    "the subscript @xmath64 of @xmath99 and @xmath97 will be dropped if there is no confusion .",
    "we now are ready to present the definition of @xmath64-bregman divergence over @xmath100 .",
    "[ def : v - bregman - div_potential ] let @xmath82 be a function which is strictly convex , decreasing , and third order continuously differentiable .",
    "suppose that the functions @xmath101 and @xmath102 defined from @xmath64 satisfy the following conditions : @xmath103 and @xmath104 the bregman divergence defined from the potential @xmath105 is called @xmath64-bregman divergence , and denoted as @xmath106 . not only @xmath107 but also @xmath91 is also referred to as potential .    as shown in @xcite ,",
    "the function @xmath107 is strictly convex in @xmath54 if and only if the potential @xmath64 satisfies .",
    "the @xmath64-bregman divergence has the form of @xmath108 indeed , substituting @xmath109 into , we obtain the expression of @xmath106 .",
    "the kl - divergence @xmath110 is represented as @xmath106 with the potential @xmath111 .",
    "below we show some examples of @xmath64-bregman divergence .",
    "[ example : power - div ] for the power potential @xmath112 with @xmath113 , we have @xmath114 and @xmath115 .",
    "then , we obtain @xmath116 the kl - divergence is recovered by taking the limit of @xmath117 .",
    "[ example : bounded - div ] for @xmath118 , let us define @xmath119 . then @xmath91 is a strictly convex and decreasing function , and we obtain @xmath120 for @xmath96 . the negative - log potential , @xmath111 , is recovered by setting @xmath121 .",
    "the potential satisfies the bounding condition @xmath122 .",
    "as shown in the sequel @xcite , the bounding condition of @xmath101 will be assumed to prove the convergence property of the quasi - newton method .",
    "the space of positive definite matrices has rich geometrical and algebraic structures @xcite here we introduce dualistic geometrical structure on @xmath100 induced form the bregman divergence .",
    "see @xcite for details .",
    "we introduce two coordinate systems on @xmath100 .",
    "the @xmath123-coordinate system @xmath124 is defined as @xmath125 which is the identity function on @xmath26 .",
    "the definition of the other coordinate system requires the potential @xmath66 for the bregman divergence @xmath73 in .",
    "let us define the @xmath126-coordinate system as @xmath127 note that the matrix @xmath128 is not necessarily a positive definite matrix .",
    "indeed , for the potential @xmath129 , we have @xmath130 which is a negative definite matrix .",
    "the function @xmath126 is , however , one - to - one mapping .",
    "hence @xmath128 works as the coordinate system on @xmath26 .",
    "the inverse function of @xmath131 is expressed by the conjugate function of @xmath66 .",
    "the convex function @xmath66 has the dual representation called fenchel conjugate , which is defined as @xmath132 then , we have @xmath133 on the domain of @xmath134 ( * ? ? ?",
    "* theorem 26.5 ) . for any potential @xmath66 ,",
    "the @xmath123-coordinate system is common and only the @xmath135-coordinate system depends on the potential .    for the potential @xmath64 of the @xmath64-bregman divergence ,",
    "the @xmath126-coordinate system is denoted as @xmath136 , which is given as @xmath137 thus @xmath136 is a negative definite matrix for @xmath78 .",
    "let us define the flatness of a submanifold in @xmath100 .",
    "see @xcite for the formal definition of the flatness with terminologies of differential geometry .",
    "[ autoparallel submanifold ] let @xmath138 be a subset of @xmath100 . if @xmath138 is represented as an affine subspace in the @xmath123-coordinate , then @xmath138 is called @xmath123-autoparallel submanifold .",
    "if @xmath138 is represented as an affine subspace in the @xmath126-coordinate , then @xmath138 is called @xmath126-autoparallel submanifold . when an @xmath123-autoparallel submanifold @xmath138 is also @xmath126-autoparallel , @xmath138 is called doubly autoparallel submanifold .    for the potential @xmath105 ,",
    "the @xmath126-coordinate and the @xmath126-autoparallel is denoted as the @xmath139-coordinate and the @xmath139-autoparallel , respectively .",
    "formally , the flatness is defined from the connection on the differentiable manifold @xcite . here , we adopt a simplified definition .",
    "[ example : doubly - autoparallel - secant - cond ] let @xmath91 be the negative logarithmic function @xmath140 , then we have @xmath141 .",
    "the @xmath123-coordinate system is defined as @xmath142 , and the @xmath139-coordinate system is given as @xmath143 . for two vectors @xmath144 we define the submanifold @xmath138 which represents the secant condition such that @xmath145 suppose @xmath146 , then we see that @xmath138 is doubly autoparallel , since @xmath147 holds .",
    "that is , @xmath138 is represented as the affine subspace in both the @xmath123-coordinate system and the @xmath139-coordinate system .",
    "the projection of a matrix in @xmath26 onto an autoparallel submanifold is defined below .",
    "then , we introduce the extended pythagorean theorem .",
    "let @xmath66 be a potential , @xmath77 be a positive definite matrix .",
    "an @xmath123-autoparallel submanifold in @xmath100 is denoted as @xmath138 .",
    "the matrix @xmath148 is called @xmath126-projection of @xmath77 onto @xmath138 , when the equality @xmath149 holds .",
    "let @xmath150 be a @xmath126-autoparallel submanifold in @xmath100 .",
    "the matrix @xmath151 is called @xmath123-projection of @xmath77 onto @xmath150 when the equality @xmath152 holds .",
    "let @xmath153 be a one - dimensional @xmath126-autoparallel submanifold defined as @xmath154 when @xmath155 is the @xmath126-projection of @xmath77 onto @xmath138 , the @xmath123-autoparallel submanifold @xmath138 is orthogonal to @xmath153 at @xmath155 with respect to the inner product @xmath156 . in the @xmath123-projection",
    ", also the same picture holds by replacing @xmath123 and @xmath126 .",
    "[ theorem : extended_pythagorean_theorem ] let @xmath66 be a potential function , @xmath138 be an @xmath123-autoparallel submanifold in @xmath100 , and @xmath77 be a positive definite matrix .",
    "then , the following three statements are equivalent .",
    "( a ) : :    @xmath155 is a @xmath126-projection of    @xmath77 onto @xmath138 .",
    "( b ) : :    @xmath148 satisfies the equality    @xmath157 for any @xmath158 .",
    "( c ) : :    @xmath155 is the unique optimal solution of the problem    @xmath159    for any @xmath160 the equality @xmath161 holds .",
    "the equivalence between ( a ) and ( b ) follows the above equality .",
    "if ( b ) holds , then the non - negativity of the divergence assures that @xmath155 is an optimal solution of .",
    "the uniqueness follows the strict convexity of the divergence @xmath73 in @xmath55 .",
    "hence ( c ) holds .",
    "finally , we show that ( a ) follows ( c ) .",
    "let @xmath155 be an optimal solution of .",
    "the @xmath123-autoparallel submanifold @xmath138 is represented by @xmath162 in which @xmath163 is an @xmath27 by @xmath27 real matrix and @xmath164 for @xmath165 .",
    "the optimality condition of yields that @xmath166 with some @xmath167 .",
    "in addition , the fact that both @xmath55 and @xmath155 are included in @xmath138 leads to the equalities @xmath168 therefore , we obtain @xmath169 for any @xmath158 .",
    "this implies that @xmath155 is a @xmath135-projection of @xmath77 onto @xmath138 .",
    "the uniqueness of the @xmath126-projection onto the @xmath123-autoparallel submanifold is shown through the equivalence between ( a ) and ( b ) in theorem [ theorem : extended_pythagorean_theorem ] .",
    "the similar argument is valid for @xmath123-projection onto @xmath126-autoparallel submanifold .",
    "we show the result without proof .",
    "[ theorem : m - projection - extended_pythagorean_theorem ] let @xmath66 be a potential function , @xmath150 be a @xmath126-autoparallel submanifold in @xmath100 , and @xmath77 be a positive definite matrix .",
    "then , the following conditions ( a ) and ( b ) are equivalent .",
    "( a ) : :    @xmath155 is an @xmath123-projection of    @xmath77 onto @xmath150 .",
    "( b ) : :    @xmath151 satisfies the equality    @xmath170 for any @xmath171 .",
    "when ( a ) or ( b ) holds , @xmath155 is the unique optimal solution of the problem @xmath172    the bregman divergence @xmath173 may not be convex in @xmath55 , and hence the conditions ( a ) or ( b ) in theorem is not necessarily derived from the optimality condition of .",
    "as shown in section [ sec : introduction ] , the bfgs / dfp update formulae are derived by minimizing the kl - divergence .",
    "example [ example : doubly - autoparallel - secant - cond ] shows that the submanifold associated with the secant condition @xmath174 is doubly autoparallel with respect to the flatness defined from the potential @xmath111 .",
    "thus , we obtain the following geometrical interpretation ,    bfgs update : : :    @xmath139-projection of @xmath6 onto the    @xmath123-autoparallel submanifold @xmath138 , dfp update : : :    @xmath123-projection of @xmath6 onto the    @xmath139-autoparallel submanifold    @xmath138 .",
    "figure [ fig : dfp - bfgs - figure ] presents the geometrical view of the standard quasi - newton updates based on information geometry .",
    "we consider quasi - newton update formulae derived from variational problems with respect to bregman divergences . as shown in section [ sec : introduction ] , the standard quasi - newton updates are derived from the minimization problem of the kl - divergence . we show that bregman divergences lead extended update formulae .",
    "in addition , an explicit expression of the extended hessian update formula is presented .",
    "we consider the minimization problem of the bregman divergence instead of the kl - divergence .",
    "the extended bfgs update formula is given as the optimal solution of @xmath175 suppose that the optimal solution @xmath9 exists .",
    "then @xmath9 is the unique @xmath126-projection of @xmath6 onto the submanifold defined from the secant condition . on the other hand , as the extension of the dfp update",
    ", we consider the problem , @xmath176 instead of the minimization of @xmath177 . in the similar way",
    ", we can derive the quasi - newton methods for the approximate inverse hessian matrix @xmath25 .    in the following we focus on the extension of the bfgs method , since the same argument is valid for the extension of dfp method .",
    "a formal expression of the optimal solution is presented in the theorem below .",
    "suppose that there exists an optimal solution .",
    "then the optimal solution @xmath9 is unique and satisfies @xmath178 where @xmath179 is a column vector and @xmath134 is the fenchel conjugate function of @xmath66 .    since is a convex problem and",
    "the objective function @xmath180 is strictly convex in @xmath181 , we see that the optimal solution is unique if it exists .",
    "suppose that @xmath9 is the optimal solution of , then @xmath9 satisfies the optimality condition . according to gler , et al .",
    "@xcite , the normal vector of the affine subspace @xmath174 is characterized by the form of @xmath182 in fact for @xmath183 we have @xmath184 and thus @xmath185 is a normal vector of @xmath138 .",
    "gler , et al .",
    "@xcite have shown that the normal vector is restricted to the expression above .",
    "hence , for the optimal solution @xmath9 there exists @xmath179 such that @xmath186 and @xmath187 hold .",
    "the first equality is represented as @xmath188 .",
    "the existence of @xmath9 assures that @xmath189 , where @xmath134 is the fenchel conjugate of @xmath66 defined in .    for general bregman divergences , we do not have the explicit expression of the hessian update formula . as a special case",
    ", we consider the minimization problem of the @xmath64-bregman divergence , @xmath190 the update formula obtained by the problem above is referred to as the @xmath64-bfgs update formula .",
    "the theorem below shows an explicit expression of the @xmath64-bfgs update formula .",
    "[ theorem : v - bfgs - form ] suppose the function @xmath64 is a potential function defined in definition [ def : v - bregman - div_potential ] .",
    "let @xmath16 , and suppose @xmath13",
    ". then the problem has the unique optimal solution @xmath191 satisfying @xmath192   + \\bigg(1-\\frac{\\nu(\\det{b_{k+1}})}{\\nu(\\det{b_k})}\\bigg )    \\frac{y_ky_k^\\top}{s_k^\\top y_k}.    \\label{eqn : update - formula - v - bfgs}\\end{aligned}\\ ] ]    though the theorem is proved in @xcite , the proof is also found in appendix [ appendix : vbfgs - formula ] of the present paper as a supplementary . in the same way",
    ", we can obtain the explicit formula of the @xmath64-dfp update formula , which is the minimizer of @xmath193 subject to @xmath194 .",
    "the update formula is equivalent to the self - scaling quasi - newton update defined as @xmath195   + ( 1-\\theta_k ) \\frac{y_ky_k^\\top}{s_k^\\top y_k } , \\end{aligned}\\ ] ] where @xmath196 is a positive real number .",
    "various choices for @xmath196 have been proposed , see @xcite . a popular choice is @xmath197 . in the @xmath64-bfgs update formula",
    ", the coefficient @xmath196 is determined from the function @xmath101 .",
    "we present a practical way of computing the hessian approximation .",
    "details are shown in the sequel @xcite . in eq",
    ", the optimal solution @xmath9 appears in both sides , that is , we have only the implicit expression of @xmath9 .",
    "the numerical computation is , however , efficiently conducted as well as the standard bfgs update . to compute the matrix @xmath9 ,",
    "first we compute the determinant @xmath198 .",
    "the determinant of both sides of leads to @xmath199)}{\\nu(\\det b_{k})^{n-1}}\\cdot\\nu(\\det   b_{k+1})^{n-1}.    \\label{eqn : determinant - v - bfgs}\\end{aligned}\\ ] ] hence , by solving the nonlinear equation @xmath200)}{\\nu(\\det b_{k})^{n-1}}\\cdot\\nu(z)^{n-1},\\qquad z>0\\end{aligned}\\ ] ] we can find @xmath198 . as shown in the proof of theorem [ theorem : v - bfgs - form ] , the function @xmath201 is monotone increasing .",
    "hence the newton method is available to find the root of the above equation efficiently .",
    "once we obtain the value of @xmath198 , we can compute the hessian approximation @xmath9 by substituting @xmath198 into eq  .",
    "figure [ fig : v - bfgs - update ] shows the update algorithm of the @xmath64-bfgs formula which exploits the cholesky decomposition of the approximate hessian matrix . by maintaining the cholesky decomposition",
    ", we can easily compute the the determinant and the search direction .",
    "the convergence property of the quasi - newton method with the @xmath64-bfgs update formula is considered in @xcite .",
    "[ example : vbfgs - power - div ] we show the @xmath64-bfgs formula derived from the power potential .",
    "let @xmath91 be the power potential @xmath112 with @xmath113 .",
    "as shown in example [ example : power - div ] , we have @xmath114 . due to the equality @xmath202)=\\det(b_k)\\frac{s_k^\\top y_k}{s_k^\\top b_ks_k }   \\end{aligned}\\ ] ] and eq .",
    ", we have @xmath203 then the @xmath64-bfgs update formula is given as @xmath204   + \\bigg(1-\\left(\\frac{s_k^\\top y_k}{s_k^\\top b_ks_k}\\right)^\\rho   \\bigg )   \\frac{y_ky_k^\\top}{s_k^\\top y_k}. \\end{aligned}\\ ] ] for @xmath205 such that @xmath113 , we have @xmath206 . in the standard self - scaling update formula ,",
    "the above matrix @xmath9 with @xmath207 is used , while it is not derived from the strictly convex potential function .",
    "in this section we study the invariance of the @xmath64-bfgs update formula under the affine coordinate transformation of the optimization variable . for",
    "the minimization problem of the function @xmath208 , let us consider the variable change of @xmath57 .",
    "for a non - degenerate matrix @xmath209 , the variable change is defined by @xmath210 then the function @xmath208 is transformed to @xmath211 defined as @xmath212 then we have @xmath213 our concern is how the point sequence @xmath214 generated by the @xmath64-bfgs method is transformed by the variable change .",
    "we consider the hessian approximation matrix under the variable change .",
    "let @xmath16 be the hessian approximation computed at the @xmath215-th step of the @xmath64-bfgs update for the minimization of @xmath208 .",
    "we now define @xmath216 let @xmath217 be the hessian approximation matrix updated from @xmath218 for the function @xmath211 , where we suppose that the @xmath64-bfgs method is used for the minimization of @xmath211 .",
    "we consider the relation between @xmath9 and @xmath217 .",
    "the updated point @xmath219 is determined by @xmath220 where @xmath221 is a non - negative real number determined by a line search .",
    "then we have @xmath222 let @xmath5 be the step length for the function @xmath208 at the @xmath215-th step of the @xmath64-bfgs method . due to the equality , we see that the step length @xmath221 is identical to @xmath5 , if the line search with the same stopping rule is applied for both @xmath208 and @xmath211 . as the result",
    ", the equality @xmath223 holds under the condition @xmath224 .",
    "let @xmath225 and @xmath226 be @xmath227 then we obtain the equalities , @xmath228    we consider the condition of @xmath229 such that the equality @xmath230 holds , when @xmath231 and @xmath232 are satisfied .",
    "for such @xmath229 , the equality @xmath233 recursively holds .",
    "this implies that the point sequence obtained by the @xmath64-bfgs method is invariant under the affine transformation . in the optimization of @xmath211 by the @xmath64-bfgs method ,",
    "the matrix @xmath218 is updated to @xmath217 such that @xmath234   +   \\bigg(1-\\frac{\\nu(\\det{\\widetilde{b}_{k+1}})}{\\nu(\\det{\\widetilde{b}_k})}\\bigg )   \\frac{\\widetilde{y}_k\\widetilde{y}_k^\\top}{\\widetilde{s}_k^\\top \\widetilde{y}_k}. \\end{aligned}\\ ] ] some calculation yields that @xmath235   +   \\bigg(1-\\frac{\\nu(\\det{\\widetilde{b}_{k+1}})}{\\nu(\\det{\\widetilde{b}_k})}\\bigg )   \\frac{y_ky_k^\\top}{s_k^\\top y_k}.   \\label{eqn : t - group - action - b-}\\end{aligned}\\ ] ] the following theorem provides a sufficient condition on @xmath229 such that @xmath236 holds .",
    "suppose that @xmath237 , that is , @xmath238 .",
    "then the equality @xmath236 holds for any @xmath64-bfgs update formula .    due to the assumption @xmath238",
    ", we have @xmath239 .",
    "is equivalent with @xmath240   + \\bigg(1-\\frac{\\nu(\\det{\\widetilde{b}_{k+1}})}{\\nu(\\det{b_k})}\\bigg )   \\frac{y_ky_k^\\top}{s_k^\\top y_k}.    \\end{aligned}\\ ] ] hence , the determinant of @xmath241 yields the equality @xmath242\\big)}{\\nu(\\det{b_k})^{n-1 } } , \\end{aligned}\\ ] ] where @xmath243 is used . on the other hand",
    ", the matrix @xmath9 defined by the @xmath64-bfgs update formula also satisfies , @xmath244\\big)}{\\nu(\\det{b_k})^{n-1 } } , \\end{aligned}\\ ] ] as shown in the proof of theorem [ theorem : v - bfgs - form ] , the function @xmath201 is one to one mapping , and thus we have @xmath245 .",
    "therefore , the equality @xmath246 holds .",
    "next , we study the variable change with @xmath209 . below we assume @xmath247 without loss of generality",
    "let us define @xmath248 and @xmath249}}{\\nu(\\det{b_k})^{n-1}}. \\end{aligned}\\ ] ] in the @xmath64-bfgs update formula , the determinant of @xmath9 leads the equality @xmath250 the matrix @xmath217 satisfies the update formula , thus the determinant of both sides yields the equality @xmath251 when @xmath246 holds , eq . is represented as @xmath252    we consider the function @xmath101 which satisfies and simultaneously . for a positive number @xmath253 , let @xmath254 be the unique solution of the equation of @xmath255 , @xmath256 and @xmath257 be the set of all possible solutions of the above equation .",
    "note that @xmath258 holds for any @xmath101 since @xmath259 holds .",
    "[ theorem : invariance - determinant ] let @xmath260 be a differentiable function on @xmath98 .",
    "suppose that there exists an open subset @xmath261 satisfying @xmath262 . for the hessian approximation by the @xmath64-bfgs method ,",
    "suppose that the equality @xmath263 holds for all @xmath209 , all @xmath264 and all @xmath265 satisfying @xmath13 .",
    "then the function @xmath101 is equal to @xmath114 with some @xmath266 .",
    "note that @xmath267 holds for @xmath114 unless @xmath268 .    under the assumption , the equations and",
    "share the same solution @xmath269 for any @xmath270 and @xmath271 .",
    "let @xmath272 .",
    "for any positive @xmath273 and @xmath57 , equations and lead to @xmath274 for @xmath275 .",
    "hence we obtain @xmath276 the assumption on @xmath277 guarantees that @xmath278 holds for any infinitesimal @xmath279 .",
    "leads the following expression , @xmath280 taking the limit @xmath281 , we obtain the differential equation , @xmath282 and the solution is given as @xmath283 .",
    "as shown in example [ example : power - div ] , the function @xmath114 is derived from the power potential @xmath112 . in robust statistics ,",
    "the power potential has been applied in wide - rage of data analysis @xcite .",
    "ohara and eguchi @xcite have studied the differential geometrical structure over @xmath100 induced by the @xmath64-bregman divergence .",
    "they pointed out that the geometrical structure is invariant under @xmath284 group action .",
    "furthermore , they have showed that for the power potential @xmath112 , the @xmath139- ( @xmath123- ) projection onto @xmath123- ( @xmath139- ) autoparallel submanifold is invariant under @xmath48 group action .",
    "it turns out that only the orthogonality is kept unchanged under the group action .",
    "the other geometrical features such as angle between two tangent vectors are not preserved in general .",
    "theorem [ theorem : invariance - determinant ] indicates that the invariance of the geometrical structure on @xmath100 is inherited to the invariance of point sequences of quasi - newton methods under the affine transformation .    in summary",
    ", we obtain the following results .",
    "suppose that @xmath285 holds .",
    "let @xmath286 and @xmath287 be point sequences generated by the @xmath64-bfgs method for the functions @xmath208 and @xmath211 , respectively .",
    "suppose that the line search with the same stopping rule is used for the step length .",
    "then , for any @xmath237 the equality @xmath231 holds for all @xmath288",
    ". moreover the equality @xmath289 holds for any @xmath209 if and only if the function @xmath91 is the power potential .",
    "sparse quasi - newton method exploits the sparsity of hessian matrix in order to reduce the computation cost @xcite .",
    "the sparsity pattern of the hessian matrix at a point @xmath290 is represented by an index set @xmath291 satisfying @xmath292 when the number of entries in @xmath291 is small , the matrix @xmath293 is referred to as sparse matrix .",
    "we assume that @xmath294 holds for @xmath295 and that @xmath296 for all @xmath297 . given a sparsity pattern @xmath291 , the set of sparse matrix is defined by @xmath298",
    "clearly the submanifold @xmath299 is @xmath123-autoparallel in @xmath100 .",
    "yamashita @xcite has proposed a sparse quasi - newton method . in this section",
    "we show an extension of sparse quasi - newton method and illustrate a geometrical structure of the update formula .",
    "first , we briefly introduce the sparse quasi - newton method proposed by yamashita @xcite .",
    "suppose @xmath300 be an approximate inverse hessian matrix at the @xmath215-th step of the sparse quasi - newton method .",
    "let @xmath301 be the updated matrix of @xmath300 by the existing quasi - newton methods such as the bfgs or the dfp method for the approximate inverse hessian matrix .",
    "in the computation of @xmath301 , we need only the elements @xmath302 for @xmath295 , and thus efficient computation will be possible even if the size of the matrix is large",
    ". then , compute the sparse matrix @xmath303 satisfying the constraint @xmath304 for all @xmath305 .",
    "the calculation of @xmath306 from @xmath301 is regarded as the @xmath139-projection with respect to the kl - divergence .",
    "the sparse clique - factorization technique @xcite is available for the practical computation of the projection .",
    "see @xcite for details .    for the computation of both @xmath307 and @xmath306 in the sparse quasi - newton method , we can use bregman divergence instead of the kl - divergence .",
    "figure [ fig : extendedsparse_quasi - newton ] shows an extended sparse quasi - newton method for the approximate hessian matrix @xmath6 .",
    "figure [ fig : geometry_extendedsparse_quasi - newton ] illustrates the geometrical interpretation of the extended sparse quasi - newton updates .",
    "we have some choices in the algorithm of figure [ fig : extendedsparse_quasi - newton ] : ( i ) the bregman divergence in step 2 , ( ii ) projection in step 3 , and ( iii ) the number of @xmath229 . in the sparse quasi - newton updates presented by yamashita @xcite ,",
    "the number of iteration is set to @xmath308 ; in step 2 , the standard bfgs / dfp method for the approximate inverse hessian is used ; in step 3 the @xmath139-projection defined from the kl - divergence is computed . moreover ,",
    "the superlinear convergence has been proved , see @xcite for details . in the following , we present the geometrical interpretation of the sparse quasi - newton method .",
    "then we show a computation algorithm for the update formula derived from the @xmath64-bregman divergence .",
    "we consider the sparse quasi - newton update formula from the geometrical viewpoint .",
    "remember that @xmath138 is the set of matrices satisfying the secant condition @xmath309 below we consider two kinds of update formulae :    algorithm 1 : : :    in the algorithm in figure [ fig : extendedsparse_quasi - newton ] , the    matrix @xmath310 is defined as the    @xmath123-projection of @xmath311 onto    @xmath138 , that is , @xmath310 is    equal to @xmath312 $ ] . then    @xmath313 is defined as the    @xmath126-projection of @xmath310    onto @xmath299 .",
    "algorithm 2 : : :    in the algorithm in figure [ fig : extendedsparse_quasi - newton ] , the    matrix @xmath310 is the    @xmath126-projection of @xmath311 onto    @xmath138 , that is , @xmath310 is    given as the optimal solution of .",
    "then @xmath313 is    defined as the @xmath126-projection of    @xmath310 onto @xmath299 .",
    "the difference between algorithm 1 and algorithm 2 is the projection onto @xmath138 to obtain @xmath310 .",
    "below we show the theoretical properties for each algorithm .    in algorithm 1 , we consider how the bregman divergence @xmath314 is updated .",
    "let @xmath315 and suppose that the @xmath126-projection onto @xmath299 exists .",
    "then , the extended pythagorean theorem in section [ subsec : extended_pythagorean_theorem ] leads that @xmath316 and hence we have @xmath317 this indicates that under a mild assumption the bregman divergence @xmath314 will converge to zero and that @xmath318 will also converge to a matrix in @xmath319 .",
    "a condition on the convergence has been investigated by bauschke , et al .",
    "this update algorithm is similar to the so - called em - algorithm @xcite which is a popular algorithm in statistics and machine learning . in the em - algorithm ,",
    "the @xmath123-projection and the @xmath139-projection with @xmath111 is repeated in the probability space .",
    "then , the maximum likelihood estimator under the partial observation is computed . in the context of statistical estimation ,",
    "usually the em - algorithm is conducted when @xmath320 holds . under some assumption with @xmath320 , the point sequences @xmath321 converges to the pair of the closest point @xmath322 such that @xmath323 is the optimal solution of the optimization problem , @xmath324 see @xcite for details .",
    "we believe that to provide a simple characterization about the convergence point @xmath323 under the condition @xmath325 is an open problem .",
    "next , we investigate algorithm 2 .",
    "likewise we suppose @xmath326 .",
    "note that @xmath319 is @xmath123-autoparallel .",
    "let @xmath327 be the @xmath126-projection of @xmath328 onto the intersection @xmath319 .",
    "then the extended pythagorean theorem leads that @xmath329 and hence we have @xmath330 suppose that @xmath331 converges to @xmath332 when @xmath229 tends to infinity , then the equality @xmath333 holds as shown below . from the definition of @xmath327 and the extended pythagorean theorem , we have @xmath334 due to the continuity of the bregman divergence , for @xmath335 we have @xmath336 and",
    "hence @xmath333 holds . as the result we have @xmath337 .",
    "figure [ fig : geometry_sparse_quasi - newton - boosting ] shows the geometrical illustration of the algorithm 2 . applying theorem 8.1 of bauschke and borwein @xcite ,",
    "we see that the convergence of @xmath331 to the point @xmath327 is guaranteed under the bregman divergence associated with power potential with @xmath338 . the iterative update procedure is closely related to the boosting algorithm @xcite in which the iterative bregman projection is exploited to compute the estimator for classification problems",
    ".     will converge to @xmath339 which is the @xmath126 projection of @xmath315 . ]",
    "as argued above , it is not guaranteed that @xmath311 in algorithm 1 converges to @xmath339 , which is the @xmath126-projection of @xmath328 onto @xmath319 .",
    "on the other hand the sequence @xmath311 in algorithm 2 converges to @xmath339 under mild assumption . from the viewpoint of the least - change principle ,",
    "the sparse quasi - newton method with algorithm 2 will be preferable .",
    "fletcher @xcite has proposed the sparse update formula using @xmath327 .",
    "the update formula using the matrix @xmath327 requires the sparsity and the secant condition simultaneously , and hence , the approximate hessian can be ill - posed when @xmath340 for some @xmath341 @xcite .",
    "we consider the computation of the extended sparse quasi - newton updates . in algorithm 1 and 2 above , we need to compute the @xmath135-projection of a matrix @xmath181 onto the @xmath123-autoparallel submanifold @xmath299 consisting of sparse positive definite matrices . generally the @xmath126-projection does not have the explicit expression . here",
    ", we study only the @xmath139-projection based on the @xmath64-bregman divergence .    according to yamashita @xcite",
    ", we briefly introduce the computation of the projection onto @xmath299 , when the geometrical structure is induced from the kl - divergence . for a given matrix @xmath342 ,",
    "the projection onto @xmath299 , denoted as @xmath313 , is obtained as the optimal solution of @xmath343 some calculation yields that @xmath313 is also the optimal solution of @xmath344 let @xmath345 be @xmath346 . if the graph @xmath347 is chordal , the existence of the optimal solution is guaranteed @xcite .",
    "the inverse of the optimal solution , @xmath348 , is represented by using the sparse clique - factorization formula @xcite , and then the updated inverse hessian matrix is obtained .",
    "the sparse clique - factorization formula of @xmath348 is represented by @xmath349 in which @xmath350 are lower triangular matrices , and @xmath351 is a positive definite block - diagonal matrix consisting of @xmath352 diagonal blocks .",
    "the number of @xmath352 is determined by the the number of maximal cliques of the graph @xmath347 , and all elements of @xmath350 and @xmath351 are explicitly computed from @xmath353 .",
    "we generalize the above argument to the projection with the @xmath64-bregman divergence .    [",
    "theorem : min - det - b - chordal ] let @xmath345 be @xmath346 , and suppose that the undirected graph @xmath354 is chordal",
    ". let @xmath342 .",
    "then there exists the @xmath139-projection of @xmath310 onto @xmath299 , and the projection is the optimal solution of the following problem , @xmath355    remember that @xmath136 is defined as @xmath356 which is a negative definite matrix .",
    "it is easy to see that the mapping @xmath357 is bijection on @xmath26 .",
    "hence , the assumption on the graph @xmath354 guarantees that the problem @xmath358 has the unique optimal solution @xmath359 , and the optimal solution satisfies @xmath360 , as shown in @xcite . in terms of the objective function",
    ", we see that @xmath361 the function @xmath362 is strictly monotone decreasing for @xmath96 .",
    "indeed , @xmath363 holds .",
    "thus , the optimal solution of is identical to that of .",
    "we find that @xmath364 holds , since @xmath365 holds .",
    "for any @xmath366 , we have @xmath367 the second and third equalities follows @xmath368 for @xmath295 and @xmath369 for @xmath370 , respectively .",
    "therefore , @xmath359 is identical to the @xmath139-projection of @xmath371 onto @xmath299 .",
    "we present a practical method of computing the projection of @xmath310 onto @xmath299 .",
    "let @xmath311 and @xmath310 for @xmath372 be matrices generated by the extended sparse quasi - newton update with algorithm 2 .",
    "we show a method of computing @xmath373 and @xmath374 .",
    "suppose we have @xmath375 , then @xmath376 is obtained by solving the problem @xmath377 in the similar way of the proof of theorem [ theorem : v - bfgs - form ] , the optimal solution @xmath376 satisfies @xmath378 +   \\bigg(1-\\frac{\\nu(\\det({\\bar{h}^{(t)}})^{-1})}{\\nu(\\det({h^{(t)}})^{-1})}\\bigg )   \\frac{s_ks_k^\\top}{s_k^\\top y_k}. \\end{aligned}\\ ] ] we need only the elements @xmath379 for @xmath295 and the determinant @xmath380 .",
    "if we have the choleskey factorization or the sparse clique - factorization formula of @xmath375 , we can obtain these values by simple computation .",
    "then , the matrix @xmath381 is given as the optimal solution of @xmath382 as shown in the proof of theorem [ theorem : min - det - b - chordal ] , @xmath381 is also the optimal solution of @xmath383 let @xmath384 , then the sparse clique - factorization formula provides the factorized expression of @xmath385 based on the information of @xmath386 .",
    "the determinant of @xmath385 is easily computed by the sparse clique - factorization formula .",
    "then , we solve the the following equation , @xmath387 the newton method is available to find the unique solution @xmath388 efficiently .",
    "using the solution @xmath388 , the matrix @xmath381 is represented @xmath389 the matrix @xmath381 also has the expression of the sparse clique - factorization formula , and thus , it is available to the sequel computation .",
    "along the line of the research stared by fletcher @xcite , we considered the quasi - newton update formula based on the bregman divergences , and presented a geometrical interpretation of the hessian update formulae .",
    "we studied the invariance property of the update formulae .",
    "the sparse quasi - newton methods were also considered based on the information geometry .",
    "we show that the information geometry is useful tool not only to better understand the quasi - newton methods but also to design new update formulae .    as pointed out in section [ sec : potential - function_quasi - newton ] , the self - scaling quasi - newton method with the popular scaling parameter is out of the formulae derived from the bregman divergence .",
    "nocedal and yuan proved that the self - scaling quasi - newton method with the popular scaling parameter has some drawbacks @xcite .",
    "an interesting future work is to pursue the relation between the numerical properties and the geometrical structure behind the optimization algorithms . in the study of the interior point methods ,",
    "it has been made clear that geometrical viewpoint is useful @xcite .",
    "the geometrical viewpoint will become important to investigate algorithms for numerical computation .",
    "the authors are grateful to dr .",
    "nobuo yamashita of kyoto university for helpful comments .",
    "t.  kanamori was partially supported by grant - in - aid for young scientists ( 20700251 ) .",
    "we prove the following lemma which is useful to show the existence of the optimal solution .",
    "we define the function @xmath393 by @xmath394 , then , the is equivalent to the equation @xmath395 since the potential function satisfies @xmath396 from the definition , we have @xmath397 . in terms of the derivative of @xmath393 , we have the following inequality @xmath398 thus , @xmath393 is an increasing function on @xmath98 .",
    "moreover we have @xmath399 the above inequality implies that @xmath400 . since @xmath393 is continuous , the equation has the unique solution .",
    "[ proof of theorem [ theorem : v - bfgs - form ] ] first , we show the existence of the matrix @xmath9 satisfying .",
    "lemma [ lemma : sol_existence - nu - equation ] now shows that there exists a solution @xmath401 for the equation @xmath402)}{\\nu(\\det{b_k})^{n-1}}\\cdot\\nu(z)^{n-1}=z,\\quad z>0 .",
    "\\end{aligned}\\ ] ] by using the solution @xmath388 , we define the matrix @xmath403 such that @xmath404   + \\big(1-\\frac{\\nu(z^*)}{\\nu(\\det{b_k})}\\big)\\frac{y_ky_k^\\top}{s_k^\\top y_k } , \\end{aligned}\\ ] ] then the determinant of @xmath403 satisfies @xmath405)}{\\nu(\\det{b_k})^{n-1}}\\cdot \\nu(z^*)^{n-1}=z^ * , \\end{aligned}\\ ] ] in which the first equality comes from the formula @xmath406 and the second one follows the definition of @xmath388 . hence there exists @xmath191 satisfying .",
    "next , we show that the matrix @xmath9 in satisfies the optimality condition of . according to gler , et al .",
    "@xcite , the normal vector for the affine subspace @xmath407 is characterized by the form of @xmath408 suppose @xmath409 be an optimal solution of , then @xmath410 satisfies the optimality condition that there exists a vector @xmath179 such that @xmath411 where @xmath412 denotes the gradient of @xmath413 with respect to the variable @xmath181 . also , the optimal solution @xmath410 should satisfy the constraint @xmath414 . on the other hand , the matrix @xmath9 defined by satisfies @xmath415)^{-1 }",
    "+ \\bigg(1-\\frac{\\nu(\\det{b_k})}{\\nu(\\det{b_{k+1}})}\\bigg )   \\frac{s_ks_k^\\top}{s_k^\\top y_k}\\\\ & =   \\frac{\\nu(\\det{b_k})}{\\nu(\\det{b_{k+1}})}b^{dfp}[b_k^{-1};y_k , s_k ]   + \\bigg(1-\\frac{\\nu(\\det{b_k})}{\\nu(\\det{b_{k+1}})}\\bigg )   \\frac{s_ks_k^\\top}{s_k^\\top y_k}\\\\ \\longleftrightarrow &   \\left\\ {   \\begin{array}{l }    \\displaystyle      -\\nu(\\det{b_{k+1}})b_{k+1}^{-1}+     \\nu(\\det{b_k})b_k^{-1 }     = s_k\\lambda^\\top+\\lambda s_k^\\top,\\vspace*{2mm}\\\\    \\displaystyle      \\lambda     = \\frac{\\nu(\\det{b_k})}{s_k^\\top y_k}b_k^{-1}y_k     -\\frac{\\nu(\\det{b_{k+1}})}{2s_k^\\top y_k}s_k     -\\frac{\\nu(\\det{b_k})y_k^\\top b_k^{-1}y_k}{2(s_k^\\top y_k)^2}s_k .",
    "\\end{array }   \\right.\\end{aligned}\\ ] ] the conditions @xmath13 and @xmath16 guarantees the existence of the above vector @xmath416 .",
    "in addition , the direct computation yields that the constraint @xmath417 is satisfied .",
    "hence , @xmath9 satisfies the optimality condition . since is a strictly convex problem",
    ", @xmath9 is the unique optimal solution .",
    "a.  ohara .",
    "information geometric analysis of an interior point method for semidefinite programming . in o.e .",
    "barndorff - nielsen and e.b .",
    "vedel jensen , editors , _ geometry in present day science _ , pages 4974 .",
    "world scientific , 1999 ."
  ],
  "abstract_text": [
    "<S> we study quasi - newton methods from the viewpoint of information geometry induced associated with bregman divergences . </S>",
    "<S> fletcher has studied a variational problem which derives the approximate hessian update formula of the quasi - newton methods . </S>",
    "<S> we point out that the variational problem is identical to optimization of the kullback - leibler divergence , which is a discrepancy measure between two probability distributions . </S>",
    "<S> the kullback - leibler divergence for the multinomial normal distribution corresponds to the objective function fletcher has considered . </S>",
    "<S> we introduce the bregman divergence as an extension of the kullback - leibler divergence , and derive extended quasi - newton update formulae based on the variational problem with the bregman divergence . as well as the kullback - leibler divergence , the bregman divergence introduces the information geometrical structure on the set of positive definite matrices . from the geometrical viewpoint , we study the approximation hessian update , the invariance property of the update formulae , and the sparse quasi - newton methods . especially , we point out that the sparse quasi - newton method is closely related to statistical methods such as the em - algorithm and the boosting algorithm . </S>",
    "<S> information geometry is useful tool not only to better understand the quasi - newton methods but also to design new update formulae . </S>"
  ]
}