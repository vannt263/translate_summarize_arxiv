{
  "article_text": [
    "pattern recognition , or pattern classification , developed significantly in the 60 s and 70 s as an interdisciplinary effort .",
    "several comprehensive reviews of the field are available  @xcite .",
    "the goal of pattern classification is to separate events of different categories , e.g. , signal and background , mixed in one data sample .",
    "using event input coordinates , a classifier labels each event as consistent with either signal or background hypotheses .",
    "this labeling can be either discrete or continuous . in the discrete case , with each category represented by an integer label , the classifier gives a final decision with respect to the event category . in case of continuous labeling",
    ", the classifier gives real - valued degrees of consistency between this event and each category . in the simplest case of only two categories ,",
    "a continuous classifier produces one real number for each event with high values of this number corresponding to signal - like events and low values corresponding to background - like events .",
    "the analyst can then impose a requirement on the continuous classifier output to assign each event to one of the two categories .",
    "a primitive pattern classifier with discrete output is a binary split in one input variable .",
    "this classifier has been used by every physicist many times and is typically referred to as `` cut '' in hep jargon .",
    "more advanced classifiers are also available .",
    "linear discriminant analysis was introduced by fisher  @xcite in 1936 and became a popular tool in analysis of hep data .",
    "neural networks  @xcite , originally motivated by studies of the human brain , were adopted by hep researchers in the late 80 s and early 90 s  @xcite .",
    "the feedforward backpropagation neural net with a sigmoid activation function appears to be the most popular sophisticated classifier used in hep analysis today .",
    "the range of available classifiers is , of course , not limited to the fisher discriminant and neural net .",
    "unfortunately , classification methods broadly used by other communities are largely unknown to physicists .",
    "radial basis function networks  @xcite are seldom used in hep practice  @xcite .",
    "decision trees  @xcite introduced in the 80 s have been barely explored by physicists  @xcite .",
    "there is only one documented application of boosted decision trees  @xcite , a powerful and flexible classification tool , to analysis of hep data  @xcite .",
    "multivariate adaptive regression splines  @xcite , projection pursuit  @xcite and bump hunting  @xcite have yet to find their way to hep analysts .    to some extent",
    ", these methods can be popularized through active advertisement .",
    "however , comparison of various classifiers on the same input data and therefore an educated choice of an optimal classifier for the problem at hand are hardly possible without consistent and reliable code . in principle , implementations of all classification methods listed above are available , as either commercial or free software . in practice , one has to deal with different formats of input and output data , different programming languages , different levels of support and documentation , and sometimes with incomprehensible coding . until an average analyst finds a way to feed data to various classifiers with minimal effort , advanced pattern recognition in hep will be left to a few enthusiasts .    is an attempt to provide such consistent code for physicists .",
    "this package implements several classifiers suited for physics analysis and serves them together in a flexible framework .",
    "this note describes basic mathematical formalism behind the implemented classifiers , illustrates their performance with examples and gives general recommendations for their use .",
    "if you are planning to apply classification methods to practice , i strongly recommend that you look more deeply into the topic .",
    "the comprehensive reviews  @xcite are an excellent place to start .",
    "section  [ sec : lqda ] describes linear and quadratic discriminant analysis broadly known in the physics community as `` fisher '' .",
    "section  [ sec : nn ] covers the feedforward backpropagation neural net and the radial basis function neural net , two classifiers implemented in the stuttgart package .",
    "although does not offer tools for training neural nets and can be used only for reading trained network configurations and computing network response for input data , i felt obligated to include a brief description of the formalism for these two methods as well .",
    "section  [ sec : trees ] describes the  implementation of decision trees .",
    "section  [ sec : hunter ] describes prim , an algorithm for bump hunting .",
    "section  [ sec : ada ] describes our implementation of adaboost , a quick , powerful , and robust classifier .",
    "section  [ sec : combiner ] discusses a method for combining classifiers implemented in .",
    "a description of other tools useful for classification and included in the package can be found in section  [ sec : other ] .",
    "technical details of the c++ implementation are briefly reviewed in section  [ sec : cpp ] . finally ,",
    "application of several classifiers to a search for the radiative lepton decay @xmath0 at  is discussed in section  [ sec : lnugamma ] .",
    "the main feature of a classifier is its predictive power , i.e. , how efficiently this classifier separates events of different categories from each other . mathematical definition of the predictive power can vary from analysis to analysis . for example , in some situations one might want to minimize the fraction of misclassified events , while under other circumstances one might focus on maximizing the signal significance , @xmath1 , where @xmath2 and @xmath3 are signal and background , respectively , found in the signal region .",
    "irrespective of the exact mathematical definition , non - linear classifiers such as neural nets or boosted decision trees typically provide a better predictive power than simple robust classifiers such as binary splits or the linear fisher discriminant .    for optimization and estimation of the predictive power of a classifier ,",
    "a three - stage procedure is typically used : training , validation , and test .",
    "first , the classifier is optimized on a training data set .",
    "the classifier performance on a validation set is used to stop the training when necessary .",
    "finally , the predictive power of the classifier is estimated using test data .",
    "the three data sets must be independent of each other .",
    "non - compliance with this three - stage procedure can result in sub - optimal classifier performance and a biased estimate of the predictive power .",
    "for example , the classification error tends to decrease for the training sample even after the error reached minimum and turned over for the validation sample .",
    "this phenomenon is called `` overtraining '' .",
    "estimates of the predictive power obtained from training or validation sets are most usually over - optimistic .",
    "these considerations are less important for robust classifiers . for instance , a simple binary split in one variable does not need validation , and the efficiency of the imposed cut for the training set is usually very close to that for the test set for large data samples . for small samples or non - linear classifiers ,",
    "the three - stage routine is a necessity .",
    "other important characteristics of classifiers include interpretability , ability to deal with irrelevant inputs , stability of training and performance in high - dimensional data , ease of training , and classification response time per event .",
    "an ideal classifier offers a high predictive power and provides insight into the data structure .",
    "these two requirements are unfortunately at odds with each other .",
    "powerful classifiers such as neural nets or boosted decision trees typically work as black boxes . on the other hand ,",
    "decision trees split data into rectangular regions suitable for easy interpretation but rarely provide a competitive quality of classification .",
    "this motivated breiman to postulate his uncertainty principle  @xcite : @xmath4 where @xmath5 stands for accuracy , @xmath6 stands for simplicity , and @xmath7 is the breiman constant .",
    "an optimal classifier for every problem has to be chosen with regard to all its features .",
    "methods implemented in  can only process data sets with two event categories  signal and background .",
    "perhaps , one day i will extend these implementations to include multi - category classification  if the community shows sufficient interest for such multi - class methods .",
    "if the likelihood function for each class is a multivariate gaussian @xmath8 ;      \\ \\ \\ c=0,1;\\ ] ] the two classes can be separated by taking the log - ratio of the two likelihoods : @xmath9 above , @xmath10 is a @xmath11-dimensional vector of point coordinates , @xmath12 is a @xmath11-dimensional mean vector for class @xmath13 , @xmath14 is a @xmath15 covariance matrix for class @xmath13 with determinant @xmath16 , and @xmath17 is the total weight of events in class @xmath13 . here and below",
    "i use index 0 for background and 1 for signal . in reality , the mean vectors and covariance matrices are unknown and need to be estimated from observed data : @xmath18 where @xmath19 is the observed vector for event @xmath20 in class @xmath13 , and @xmath21 is the weight of this event : @xmath22 . signal and background can now be separated by requiring the log - ratio  ( [ eq : qda ] ) to be above a certain value .",
    "if the covariance matrices of the two classes are equal , this expression simplifies to @xmath23 where @xmath24 is the common covariance matrix usually estimated as @xmath25 formula  ( [ eq : fisher ] ) is widely known among physicists as `` the fisher discriminant '' ; in the statistics literature this method is usually referred to as `` linear discriminant analysis '' ( lda ) . if the covariance matrices are not equal , the quadratic term in eqn .",
    "( [ eq : qda ] ) gives rise to quadratic discriminant analysis ( qda ) .",
    "the three constant terms in the expression  ( [ eq : qda ] ) , often omitted by analysts , are needed for proper normalization . with these terms",
    "included , the equality @xmath26 implies that the likelihoods for the two classes at point @xmath10 are equal .",
    "the region @xmath27 is therefore populated with signal - like events and the region @xmath28 is populated with background - like events .",
    "if the signal and background densities are truly gaussian with equal covariance matrices , the linear discriminant  ( [ eq : fisher ] ) for each class has a univariate gaussian distribution with mean @xmath29 ( signal ) or @xmath30 ( background ) and standard deviation @xmath31 , where @xmath32 .",
    "the overall distribution of the discriminant  ( [ eq : fisher ] ) is therefore a sum of two gaussians with areas @xmath33 and @xmath34 .",
    "the separation , in numbers of sigmas , between the two univariate gaussians is @xmath35 .",
    "if the true signal and background densities are indeed gaussian and if the data have enough points to obtain reliable estimates of their covariance matrices , the quadratic discriminant will perform at least not worse than the linear one and usually better . in reality , the signal - background separation is often improved by keeping the quadratic term in eqn .",
    "( [ eq : qda ] ) ; however , if one of the above conditions is not satisfied , omission of the quadratic term can occasionally result in a better predictive power .",
    "the quadratic discriminant can give a substantial improvement over the linear discriminant , for instance , if the two densities are centered at the same location @xmath36 but one of the densities is more spread than the other : @xmath37 .",
    "the linear discriminant is useless in this case .",
    "the quadratic discriminant separates signal from background by drawing a 2nd - order surface @xmath38 .",
    "if the matrix @xmath39 is positive definite , this surface is elliptical , wrapped around the more compact density , as shown in fig .",
    "[ fig : qda_gauss_and_uniform ] . for pedagogical reasons ,",
    "i show in fig .",
    "[ fig : lin_gauss_and_uniform ] an attempt to classify the same data using a linear fisher discriminant .      the stuttgart neural network simulator ( snns )",
    "@xcite provides a variety of tools for classification including feedforward neural networks with backpropagation and radial basis function networks . because the snns package is well maintained and has already earned a good reputation in the world of hep ,",
    "there is no need to reimplement these methods .",
    "however , snns does not offer convenient tools for plotting its output and classifying new data with a trained network configuration .",
    "implements interfaces to the two types of networks mentioned above .",
    "the user can read a saved snns network configuration and apply it to any independently - supplied data .",
    "the format of input ascii files implemented in  is very close to that used by snns .",
    "the user can therefore switch between snns and  with minimal effort .",
    "a disadvantage of the snns package is its inability to process weighted events .",
    "all other methods implemented in  accept weighted data .",
    "a feedforward neural network consists of one input , one output and several hidden layers linked sequentially .",
    "training of a neural net consists of two stages : forward and backward propagation . at the first stage",
    ", a @xmath11-dimensional vector @xmath10 of input event coordinates @xmath40 is propagated from the input to the output layer using @xmath41 where @xmath42 is the output generated by the @xmath43th node in layer @xmath44 , @xmath45 is the number of nodes in layer @xmath44 , @xmath46 is a linear weight associated with the link between node @xmath47 in layer @xmath44 and node @xmath43 in layer @xmath48 , @xmath49 is the local field , and @xmath50 is an activation function for node @xmath43 in layer @xmath44 .",
    "the sequential order of this propagation from input to output through a predetermined set of links explains why such a network is called `` feedforward '' or `` acyclic '' .",
    "for a two - category classification problem , the output layer has only one node with real - valued output ranging from 0 to 1 .",
    "the real - valued output @xmath51 of a network with @xmath52 layers indexed from 0 to @xmath53 is used to compute the quadratic classification error for each training event @xmath54 where @xmath55 is the true class of the input vector @xmath10 , 0 for background and 1 for signal .",
    "the network weights @xmath56 are then updated by propagating the computed classification error backwards from the output to the input layer : @xmath57 where @xmath58 are weight adjustments , @xmath59 is the local gradient at node @xmath43 of layer @xmath44 , and @xmath60 is the learning rate of the network . at the output node @xmath61",
    "the local gradient is simply @xmath62    this two - pass procedure is repeated for each training event .",
    "after the pool of training events is exhausted , the neural net goes back to the first event and reprocesses the whole training sample .    for the activation @xmath63 , a sigmoid , or logistic , function",
    "is often chosen : @xmath64    the backpropagation algorithm described above and the sigmoid activation function are among the many possibilities offered by the snns package .",
    "they are emphasized here simply because they are often chosen by hep analysts .    to apply a neural net to classification , the user must choose the number and size of hidden layers and specify the learning rate @xmath60 .",
    "both selections are typically optimized by studying the network performance on training and validation data .",
    "compared to other classifiers , neural networks usually offer an excellent predictive power .",
    "their main disadvantage is the lack of a meaningful interpretation . a neural net works as a black box .",
    "its output is a non - linear function which is hard to visualize , especially in high dimensions .",
    "training of neural nets , however , is subject to certain subtleties . a neural net can get `` confused '' if it is presented with strongly correlated input variables or irrelevant variables that do not contribute to the predictive power .",
    "mixed input data , i.e. , a combination of continuous and discrete variables , can also cause training instabilities . adding extra variables to a neural net not only",
    "increases the size of the input layer but also requires an expansion of hidden layers for an adequate representation of the data complexity .",
    "in many dimensions , therefore , training of neural nets can be painfully slow .",
    "a neural net can fail even in low dimensions if it is presented with sufficiently complex data .",
    "consider , for example , separating two signal gaussians from uniform background in two dimensions , as shown in fig .",
    "[ fig : twogauss_and_uniform ] . a 2:4:2:1 fully - connected feedforward neural net with a sigmoid activation function",
    "efficiently finds one gaussian but not the other .",
    "the network performance can be , of course , improved .",
    "for example , one could build two independent networks , each optimized for one of the two gaussians , and then combine them into one common network .",
    "but to perform such an exercise , one must already have a good knowledge of the data structure .",
    "the neural net construction would be therefore unnecessary  if we already know so much about the data , we can separate signal from background `` by hand '' , i.e. , using simple and well - understood selection requirements .      in the radial basis function ( rbf )",
    "formalism  @xcite , an unknown real - valued function @xmath65 is regressed on an input space @xmath10 using @xmath66 where @xmath67 are regression centers and @xmath68 is the regression kernel .",
    "the regression coefficients @xmath69 can be estimated from observed data @xmath70 : @xmath71 where @xmath72 is an @xmath73-dimensional vector of coefficients , @xmath55 is an @xmath74-dimensional vector of the function values observed at points @xmath75 , @xmath76 is an @xmath77 matrix defined by @xmath78 , @xmath79 is an @xmath80 matrix defined by @xmath81 , and @xmath82 is a regularization parameter included to avoid overtraining .",
    "the kernel is usually chosen to be radially symmetric , @xmath83 hence the name `` radial basis function '' . the two - class recognition problem is equivalent to the regression problem with the real - valued function @xmath65 replaced by an integer - valued function @xmath65 allowed to take only two values , 0 for background and 1 for signal .",
    "three popular choices for the kernel @xmath84 have been implemented in the stuttgart package  @xcite :    * gaussian @xmath85 * multiquadratic @xmath86 * thin plate spline @xmath87    where @xmath88 is a scale parameter .",
    "the rbf formalism is represented by a 3-layer neural network .",
    "the input layer consists of @xmath11 nodes , one for each component of the @xmath11-dimensional input vector @xmath10 .",
    "@xmath73 nodes representing the regression centers compose the hidden layer .",
    "the output layer for a two - class recognition problem is a single node with real - valued output ranging from 0 to 1 .",
    "components of the input vector @xmath10 are delivered to the hidden nodes which compute distances between the input vector and the corresponding regression centers .",
    "the hidden units are then activated by applying the chosen kernel to the computed distances .",
    "the result of the activation is linearly propagated from the hidden layer to the output node .",
    "this node applies an activation function , either an identity or sigmoid transformation , to its input to produce the network output .",
    "shortcut connections between the input and output layers are allowed but not required ; such shortcuts are implemented by adding linear terms to the regression formula  ( [ eq : rbf_regression ] ) .",
    "one can also bias the output node by adding a constant term to its input .",
    "the final expression for the network output is then @xmath89 where @xmath63 is the activation function for the output node , @xmath90 is a @xmath11-dimensional vector of the linear shortcut coefficients , and @xmath7 is the bias applied to the output node .",
    "several algorithms are available for training rbf networks ; for a brief survey see a subsection on learning strategies for rbf nets in ref .",
    "@xcite . the snns implementation  @xcite minimizes quadratic error @xmath91 by using gradient descent . in this approach ,",
    "the weights @xmath69 are not computed using eqn .",
    "( [ eq : rbf_weights ] ) but adjusted to minimize the classification error on training data",
    ". the regularization parameter @xmath82 is not used , and smootheness of the regression surface is maintained by stopping the training routine when the validation error reaches minimum .",
    "a non - trivial part of the training procedure is initialization of the regression centers @xmath67 .",
    "a conservative approach is to treat every observed point as a regression center by setting @xmath92 .",
    "while this approach generally guarantees a high predictive power , it is hardly practical for large data sets . in practice",
    ", one usually selects a smaller number of regression centers and then samples the observed distribution to find an efficient initial center assignment .",
    "this sampling can be accomplished by a number of algorithms ; see a section on rbf implementation in the snns manual  @xcite .",
    "rbf nets share most flaws and advantages with the backpropagation neural network of section  [ sec : stdnn ] .",
    "the snns implementation of the rbf training procedure optimizes the rbf weights @xmath72 , the scale parameter @xmath88 for each rbf center and positions of the rbf centers @xmath67 .",
    "the rbf training mechanism is therefore quite flexible and provides a good predictive power .",
    "however , the large number of optimized parameters makes an rbf network more fragile than a backpropagation neural net of the same size .",
    "rbf networks also suffer a great deal from the `` curse of dimensionality '' . in high - dimensional data ,",
    "the performance of distance - based classifiers crucially depends on the chosen definition of distance between two points .",
    "in addition , the number of points required to adequately sample a distribution grows exponentially with the dimensionality of input space thus making every realistic data set undersampled in a high - dimensional representation .",
    "the combination of these two features makes every distance - based classifier highly unstable in a multidimensional setting .    despite these setbacks ,",
    "rbf nets should be considered as a good competitor to the backpropagation neural net in low dimensions .",
    "for example , the problem with two signal gaussians and uniform background discussed in section  [ sec : stdnn ] can be solved , as shown in fig .",
    "[ fig : rbf_twogauss_and_uniform ] , using a very simple rbf network with only 3 regression centers  one for each gaussian and one for background .",
    "if the two signal peaks were not gaussian , more regression centers would be needed .",
    "but it is clear that the rbf net is superior over the standard backpropagation neural net in this case .",
    "a decision tree recursively splits training data into rectangular regions ( nodes ) .",
    "it starts with all input data and looks at all possible binary splits in each dimension to select one with a highest figure of merit .",
    "then the tree examines each of the obtained nodes and splits it into finer nodes .",
    "this procedure is repeated until a stopping criterion is satisfied .",
    "each binary split is optimized using a certain .",
    "suppose we are solving a classification problem with only two categories , signal and background .",
    "suppose we start with a parent node with the total weight of events given by @xmath93 and split it into two daughter nodes with weights @xmath94 and @xmath95 , respectively .",
    "the parent node was labeled as either `` signal '' or `` background '' , and we assign temporary labels to the two daughter nodes ; the daughter labels can be , of course , swapped if this results in a better .",
    "let @xmath96 and @xmath97 be fractions of correctly classified and misclassified events in each node , with proper indices supplied for the daughter nodes .",
    "three popular figures of merit , @xmath98 , used in conventional classification trees are given by    * correctly classified fraction of events : @xmath99 . *",
    "negative gini index : @xmath100 .",
    "* negative cross - entropy : @xmath101 .",
    "a split is optimized to give the largest overall , @xmath102 where @xmath103 and @xmath104 are figures of merit computed for the two daughter nodes .",
    "if the initial  @xmath98 can not be improved , this node is not split and becomes `` leaf '' , or `` terminal '' .",
    "a terminal node is labeled as `` signal '' if the total weight of signal events contained in this node is not less than the total weight of background events in this node .",
    "several criteria can be used to stop training ; for a brief review see , e.g. , a section on decision trees in ref .",
    "@xcite . only one stopping criterion is implemented in : the user must specify the minimal number of events per tree node .",
    "the tree continues making new nodes until it is composed of leaves only  nodes that can not be split without a decrease in the  and nodes that can not be split because they have too few events .",
    "note that the quantities @xmath96 and @xmath105 used for the split optimization were defined with no regard to event categories .",
    "a conventional decision tree makes no distinction between signal and background and spends an equal amount of time optimizing nodes dominated by signal and background events . in hep analysis ,",
    "one most usually treats the two categories asymetrically , i.e. , one is only concerned with optimizing signal but not background .",
    "if @xmath106 and @xmath107 are weights of the signal and background components , respectively , in a given node , one can use the following asymmetric optimization criteria included in :    * signal purity : @xmath108 . *",
    "signal significance : @xmath109 . * tagging efficiency : @xmath110_+^2 $ ] .",
    "the `` + '' subscript above indicates that the expression in the brackets is used only when it is positive ; at negative values it yields zero .",
    "the  for split optimization then becomes @xmath111 similarly , if the parent  @xmath98 can not be improved , this node is labeled as `` terminal '' .",
    "the algorithm for class assignment to terminal nodes will be discussed in detail later in this section .",
    "the choice of an optimization criterion is not straightforward .",
    "consider , for example , separating the two bivariate gaussians from uniform background shown in fig .",
    "[ fig : tree_twogauss_and_uniform ] . a decision tree based on",
    "the signal significance finds a big rectangle that covers both gaussians . at the same time",
    ", a tree using the gini index puts a separate rectangle around each gaussian .",
    "although the gini - based tree yields a slightly worse ( by 4% ) signal significance , it provides more insight into the data structure .",
    "a purity - based tree makes many small terminal nodes with high signal - to - background ratios and is hardly appropriate for this problem .",
    "however , if one wants to search for small pure signal clusters , the purity criterion comes in handy , as discussed in section  [ sec : hunter ] .",
    "two popular commercial decision trees , cart  @xcite and c4.5 developed on the basis of the id3 algorithm  @xcite , are undoubtedly implemented at a higher level of sophistication .",
    "cart , for example , allows to make splits on linear combinations of input variables . to simplify the tree architecture",
    ", cart deploys a pruning algorithm after all terminal nodes have been found .",
    "suppose that a non - terminal node in a tree is connected to @xmath79 terminal nodes with optimized figures of merit @xmath112 and weights @xmath113 .",
    "the cost complexity criterion for this non - terminal node is defined as @xmath114 where @xmath115 is the cost parameter and @xmath116 is the total weight of the terminal nodes .",
    "if the cost complexity criterion is less than the  for this non - terminal node @xmath98 , the node is declared `` terminal '' and all its daughters are discarded .",
    "decision trees are not pruned in the  implementation .",
    "it is possible , however , to merge terminal signal nodes if requested by the user .",
    "the merging procedure selects a subset of terminal nodes to optimize the overall .",
    "searching through all possible combinations of terminal nodes would be cpu - exhaustive , especially for a large number of nodes ; hence , a more efficient algorithm is deployed .",
    "first , all signal terminal nodes obtained in the training phase are sorted by signal purity in descending order .",
    "the algorithm then computes the overall  for the @xmath20 first nodes in the sorted list with @xmath20 taking consecutive values from 1 to the full length of the list . the optimal combination of the terminal nodes is given by the highest  computed in this manner .",
    "it can be shown that this algorithm selects the optimal subset of the terminal nodes for any , @xmath117 , which increases with purity , @xmath118 , if the total event weight , @xmath119 , is kept constant .",
    "rigorous proof of this statement is beyond the scope of this paper .    if the user wants to have terminal signal nodes merged , the tree assigns all terminal nodes with a non - zero signal contribution to the signal category .",
    "terminal nodes that do not contribute to the overall  are rejected by the merging procedure .    in effect , the  implementation of decision trees offers two different algorithms .",
    "the first algorithm , close to that used by conventional decision trees such as cart and c4.5 , chooses a symmetric  and finds signal and background terminal nodes . in this case",
    "terminal nodes are not merged .",
    "the second algorithm , suited specifically for hep analysis , uses an asymmetric , assigns all terminal nodes with non - zero signal content to the signal category and never attempts to find regions with high background purity .",
    "terminal nodes are then merged to get rid of those that do not contribute to the overall signal .",
    "the distinction between these two algorithms is not enforced by the package .",
    "for example , the user can choose a symmetric optimization criterion and merge terminal nodes or choose an asymmetric criterion without merging .",
    "these possibilities , however , do not make much sense for most practical problems . to summarize ",
    "if you choose a symmetric  such as the correctly classified fraction , gini index or cross - entropy , do not merge terminal nodes ; if you choose an asymmetric  such as the signal purity or signal significance , make sure to merge terminal nodes .",
    "compared to neural nets , decision trees typically offer an inferior predictive power .",
    "their main strength is interpretability .",
    "rectangular regions can be easily understood in many dimensions .",
    "decision trees that allow splits on linear combinations of variables give a better predictive power but at the same time , following the breiman uncertainty principle , become harder to interpret .",
    "another disadvantage of decision trees is the lack of training stability .",
    "binary splits can not be reversed .",
    "if a tree cuts too deep into the signal region , it will never recover and it will fail to find a small subset of optimal nodes .",
    "this is especially true for trees without pruning .    due to the simplicity of the training mechanism , decision trees are less fragile than neural nets .",
    "they can easily deal with strongly correlated variables and mixed data types .",
    "the three - stage training - validation - test routine is absolutely necessary for decision trees .",
    "if the tree is constructed by maximizing the chosen  on the training set , it will most likely be overtrained . at the same time , the  computed for the validation set can be substantially larger than that for the test set .",
    "several applications of decision trees to hep analysis  @xcite gave optimistic conclusions about their predictive power .",
    "while it is entirely plausible that in some cases decision trees can compete with neural nets and even exceed their predictive power , a significantly better performance can imply that the neural net was not properly trained . the last publication in ref .",
    "@xcite is particularly alarming in this sense . according to this paper",
    ", cart gives a spectacular improvement over the neural net for the quality of the @xmath120 separation at the  detector .",
    "however , the paper says close to nothing about what neural net was used and how it was trained .",
    "it is not even clear if the neural net and cart were trained on the same set of input variables and similar data sets .",
    "about 70 input variables , both discrete and continuous , were used for cart optimization in this analysis ; some of them are strongly correlated .",
    "this is a hard problem for neural nets  processing a large training sample with dozens of input variables can be excruciatingly slow , and the presence of mixed inputs and strongly correlated variables can make the training procedure unstable . to obtain an accurate estimate of the neural net performance , one would have to get rid of the strongly correlated variables , exclude inputs that do not contribute much to the predictive power , and try several network configurations . without this exercise ,",
    "such a comparison is hardly useful .",
    "another useful tool implemented in  is prim , a bump hunting algorithm  @xcite .",
    "prim searches for rectangular regions with an optimal separation between the categories but , unlike a decision tree , it follows a more conservative approach .",
    "instead of recursively splitting the input space into finer nodes , this algorithm attempts to find one rectangular region with a globally optimized .",
    "this search is implemented in two steps :    * shrinkage . at this stage ,",
    "the bump hunter gradually reduces the size of the signal box by imposing binary splits .",
    "the most optimal split is found at each iteration by searching through all possible splits in all input variables .",
    "the rate of shrinkage is controlled by a `` peel '' parameter , the maximal fraction of events that can be peeled off the signal box with one binary split .",
    "this parameter is supplied by the user and can be used to adjust the level of conservatism .",
    "if the bump hunter can not find a binary split to improve the , shrinkage is stopped . * expansion . at this stage ,",
    "the hunter attempts to relax the bounds of the signal box to optimize the .",
    "after the signal box has been found , the hunter removes points located inside this box from the original data set and starts a new search from scratch .",
    "the bump hunting algorithm is slower and more conservative than the recursive splitting used by decision trees .",
    "it is most suitable for problems where the user wants to find a pre - determined number of signal regions .",
    "for example , if you wish to find one signal box , the bump hunter is the right tool .",
    "a decision tree can be forced to produce only one signal terminal node too  by requesting a large enough minimal size of a terminal node .",
    "but the bump hunter is more flexible . by trying various peel parameters",
    ", one can find a box close to the optimal one .",
    "an interesting application of bump hunting is exploratory search for new signatures in a multidimensional space .",
    "the sleuth algorithm  @xcite developed by the d0 collaboration searches for new signatures by splitting data into voronoi cells and identifying cells with low probabilities of observing that many events given the monte carlo expectation .",
    "a closely related problem is estimation of the goodness of fit  @xcite by comparing minimal and maximal sizes of observed event clusters to those expected from monte carlo simulation .",
    "these methods unfortunately rely on a uniformity transformation from the space of physical observables into the flattened space where voronoi cells or clusters are constructed .",
    "a uniformity transformation is not unique .",
    "the choice of this transformation can have a serious impact on the result , especially in multidimensional problems .",
    "with bump hunting , a uniformity transformation is not necessary .",
    "one simply labels monte carlo events as category 0 and observed data as category 1 , and then searches for bumps in the space of unaltered physical variables .",
    "it is surprising how good are the bump hunter and decision trees for locating even tiny clusters of events .",
    "suppose we expect 10,000 points drawn from a uniform 2d distribution within a unit square @xmath121 and observe 9910 uniformly distributed points with two tiny peaks included : a 60 event peak located at @xmath122 and a 30 event peak located at @xmath123 .",
    "the standard deviations for both peaks are 0.05 and there is no correlation between the variables .",
    "the two bumps are hardly visible on the scatter plot shown in fig .",
    "[ fig : tiny_bumps_true_signal ] : the larger bump can be seen but the smaller bump is practically lost among uniformly distributed points .",
    "we now attempt to find these tiny bumps with a decision tree and a bump hunter .",
    "the most efficient  for this task is the signal purity .",
    "the decision tree easily finds the larger bump but typically misses the smaller bump at @xmath123 .",
    "the best result is obtained with a decision tree with at least 30 events per node ; the smaller bump in this case is found as the 6th most significant terminal node and it is stretched vertically bearing little resemblance to the real bump .",
    "the bump hunter is more flexible .",
    "depending on the peel parameter and the requested minimal bump size , one can obtain a variety of bump configurations .",
    "for example , the bump hunter with at least 20 events per node and peel parameter 0.7 places the most significant bump with 20 signal and 1 background events in the vicinity of @xmath123 and locates several bumps at @xmath122 .",
    "note that the most significant bumps are not necessarily found in the order of their significance .",
    "the most significant bump marked in red is only the 11th found by the hunter .",
    "the training instability of decision trees can be alleviated by using a more flexible training algorithm .",
    "instead of imposing irreversible hard splits , one can impose `` soft '' splits . the final category",
    "is then assigned to an event using a weighted vote of all soft splits .",
    "this classifier replaces the hard decision , signal versus background , with a continuous output obtained by summation of weights from relevant splits .",
    "adaboost is one popular implementation of this approach .",
    "adaboost works by enhancing weights of misclassified events and training new splits on the reweighted data sample . at the first step ,",
    "all weights are set to initial event weights in the training sample of size @xmath74 : @xmath124 at iteration @xmath125 , the algorithm imposes a new binary split @xmath126 to minimize the misclassified fraction of events and computes the associated misclassification error : @xmath127 where @xmath128 is an indicator function equal to 1 if the indicator expression is true and 0 otherwise , @xmath129 is the true category of event @xmath20 ( -1 for background and 1 for signal ) , and the binary split @xmath126 is a function with discrete output , equal to 1 if an event is accepted by the split and -1 otherwise .",
    "the weights of misclassified events are then enhanced : @xmath130 and the weights of correctly classified events are suppressed : @xmath131 to impose splits , the algorithm loops through data dimensions cyclically  if the @xmath125th split was imposed on dimension @xmath11 , the @xmath132st split will be imposed on dimension @xmath133 , and after the last dimension has been processed , the algorithm goes back to the first dimension .",
    "this iterative process is continued as long as the misclassification error at iteration @xmath125 is less than 50% , @xmath134 , and the number of training splits requested by the user is not exceeded .",
    "after the training has been completed , a new event is classified by a weighted vote of all binary splits : @xmath135 with the beta weight of each split given by @xmath136 for a large number of binary splits @xmath137 , the function @xmath65 is practically continuous .",
    "it gives negative values for background - like events and positive values for signal - like events .",
    "formally , the adaboost algorithm described above can be derived by minimization of the exponential loss @xmath138 phenomenologically , adaboost can be understood as a clever mechanism which improves the quality of classification by enhancing poorly classified events and training new splits on these events .",
    "the trained splits are then rated by their quality : if the misclassification error for split @xmath125 is small , this split is included with a large weight @xmath139 ; if the misclassification error is barely better than 50% , this split will only have a small effect on the output .",
    "note that above i used a different convention for signal and background labeling  unlike in the rest of the paper , background is labeled as -1 , not 0 .",
    "this was done simply to preserve the notation in which adaboost is explained in statistical textbooks and to illustrate the concept of the exponential loss . to maintain consistency with other methods , the adaboost implementation in  uses the conventional labeling ( 0 for background ) and",
    "normalizes the beta weights to a unit sum .",
    "the adaboost output from eqn .",
    "( [ eq : ada ] ) is therefore shifted by 0.5 and confined between 0 and 1 .",
    "signal - like events populate the region above 0.5 and background - like events populate the region below 0.5 .",
    "adaboost has been discussed here as a method of consecutive application of binary splits .",
    "the adaboost algorithm , of course , can be as well used with any other classifiers .",
    "includes adaboost implementations with binary splits , with linear and quadratic fisher discriminants , and with decision trees .",
    "adaboost combines the best features of decision trees and neural nets  needless to say , at the expense of interpretability .",
    "the training algorithm is very robust .",
    "adaboost can easily deal with mixed discrete and continuous input variables , and it gives a high predictive power . unlike neural nets , adaboost does not require any pre - training initialization .",
    "adaboost generally takes less training time than the two neural nets discussed in this note .",
    "this advantage in the training speed becomes spectacular in high - dimensional problems .",
    "for example , the training time for adaboost with soft binary splits scales linearly with the number of dimensions , while for the standard backpropagation neural net this dependence is at best quadratic and can be much worse if more than one hidden layer is used .",
    "separation of two bivariate gaussians from uniform background by adaboost with 500 soft binary splits on each input variable is shown in fig .",
    "[ fig : ada_twogauss_and_uniform ] .",
    "another powerful tool for combining weak classifiers is bootstrap aggregating , or `` bagging ''  @xcite .",
    "also implemented in , this method will be described in more detail in a separate note submitted simultaneously .",
    "classifiers independently trained on data subsets can be combined to obtain a higher overall predictive power .",
    "methods for combining classifiers have been a subject of recent research  @xcite .",
    "one approach is to train one global classifier in the space of subclassifier outputs .",
    "for example , to discriminate signal from background , one could look at various sources of background and train a separate classifier for each data subset composed of the signal and a corresponding background source .",
    "then output values of all individual classifiers computed for the full training set that includes all sources of background could be processed by one global classifier .",
    "implements a classifier combiner using adaboost .",
    "the user supplies trained subclassifiers to the combiner , and the combiner runs the adaboost algorithm , either with soft binary splits or with fisher discriminants or with decision trees , using output values of the subclassifiers as input data .",
    "i would like to stress the distinction between adaboost and the classifier combiner described in the previous paragraph .",
    "the former combines a large number of weak classifiers by applying them sequentially .",
    "the latter combines a few powerful classifiers by training another powerful classifier in the space of their output values .",
    "a few other methods of general use for statistics analysis in hep have been implemented in .",
    "computation of data moments is sometimes necessary in analysis applications .",
    "mean , variance and kurtosis for variables and their combinations can be computed . a test of independence of two variables drawn from a joint elliptical distribution with a positive definite matrix @xmath140 .",
    "this class obviously includes multivariate gaussian distributions .",
    "] can be performed using a simple analytic formula  @xcite .",
    "bootstrap  @xcite is a convenient tool that can be used to estimate the distribution of a statistic computed for a small data set if monte carlo or other means of sample regeneration are not available .",
    "this can be done by sampling from the data set with replacement . to build one bootstrap replica",
    ", one needs to draw @xmath74 events with replacement out of the data set of size @xmath74 . to study the distribution of the statistic of interest , one typically needs to build 100 - 200 bootstrap replicas .",
    "this method can be efficiently used for data sets of size @xmath141 or larger .",
    "[ fig : bootstrap ] illustrates bootstrap application for computation of the error of a correlation coefficient estimator .",
    "two random variables , @xmath10 and @xmath55 , have a joint bivariate gaussian distribution with a covariance matrix @xmath24 : @xmath142 ; @xmath143 . the correlation coefficient between the two variables is estimated for a sample of 20 events in the usual way : @xmath144 , where @xmath145 and @xmath146 are the two means .",
    "imagine that you only have a data set with 20 events and you do not know the underlying distribution",
    ". how would compute the error associated with the estimator @xmath147 ?",
    "the usual approach in hep analysis is to assume an underlying distribution for @xmath10 and @xmath55 and study the distribution of @xmath147 based on this assumption .",
    "bootstrap offers a non - parametric and assumption - free approach  the distribution of @xmath147 is studied on bootstrap replicas . as shown in fig .",
    "[ fig : bootstrap ] , bootstrap provides an unbiased estimate of the error of @xmath147 based on 100 replicas for each data set .",
    "the methods described above have been implemented in c++ using flexible object - oriented design patterns .",
    "a detailed description of this implementation would be lengthy and is already included in the readme file distributed with the package .",
    "flexible tools have been provided for imposing selection requirements on input data  the user can choose input variables , restrict analysis to a few event categories , select events within a specified index range , and impose orthogonal cuts on input variables .",
    "the user can easily add new classifiers and new figures of merit for classifier optimization by providing new implementations to the abstract interfaces set up in the package .",
    "executables using the new classifiers and new figures of merit can be copied from existing executables with few modifications .    at the moment ,",
    "the package accepts input data only in ascii , in a format very similar to that used by snns .",
    "an optional extension to the snns format is weighted input data .",
    "external dependencies of the package include : clhep libraries  @xcite used for matrix operations , cern libraries  @xcite used for random number generation and various probability calculations , and an internal  interface for hbook or root ntuple generation .",
    "if used by researchers outside , the latter will need to be replaced with a similar interface .",
    "essentially , only two methods need to be implemented  one for booking an ntuple and another one for filling ntuple columns .",
    "because every collaboration has a set of tools for this purpose , this should hardly pose a problem .",
    "a search for the radiative leptonic decay @xmath0 is currently in progress at ; results of this analysis will be made available to the public in the near future .",
    "this analysis focuses on measuring the @xmath3 meson decay constant , @xmath148 , which has not been previously measured . while purely leptonic decays such as @xmath149 and @xmath150 offer the cleanest way of measuring this decay constant , these decays are not accessible at the present level of experimental sensitivity .",
    "the decay @xmath151 , with a branching fraction of @xmath152 predicted by the standard model , suffers from substantial background due to the presence of two or three neutrinos in the final state . due to helicity suppression ,",
    "the decays @xmath149 and @xmath150 are suppressed by factors of 225 and @xmath153 , respectively , and are therefore not observable at current luminosities .",
    "the presence of the photon removes the helicity suppression but introduces theoretical uncertainties into the calculation of the branching fraction .",
    "theoretical estimates of the radiative leptonic branching fraction vary from @xmath154 to @xmath155 .",
    "present upper limits on the branching fractions , @xmath156 and @xmath157 for the electron and muon channels , respectively , were set by cleo in 1997 using 2.5  fb@xmath158 of on - resonance data .",
    "large samples of simulated monte carlo events are used to study signal and background signatures in this analysis . to model the signal ,",
    "about 1.2 m @xmath0 signal events were generated in each channnel .",
    "large samples of generic @xmath159 , @xmath160 , @xmath161 , @xmath162 and @xmath163 monte carlo events were used to estimate background contributions .",
    "because semileptonic decays of @xmath3 are an important source of background , several exclusive semileptonic modes were simulated by monte carlo as well with a typical sample size of several hundred thousand events .",
    "these modes include : @xmath164 , @xmath165 , @xmath166 , @xmath167 , @xmath168 , @xmath169 , and @xmath170 .",
    "for the @xmath171 analysis , 224 m radiative muon monte carlo events were also included .",
    "overall , 12 ( 13 ) background sources were studied for the @xmath172 ( @xmath171 ) analysis .",
    "various preliminary requirements have been imposed to enhance the signal purity and at the same time reduce the monte carlo samples to a manageable size .",
    "these requirements include tight pid quality cuts on the signal lepton and photon candidates and relaxed pid cuts on the rest of the charged tracks detected by the tracking system and neutral clusters detected by the calorimeter , a cut on the ratio of the 2nd and 0th wolfram moments , cuts on momenta and polar angles of the photon and lepton candidates , a cut on the angle between the recoil @xmath3 candidate and the @xmath173 momentum in the center - of - mass frame , a cut on the lateral profile of the calorimeter shower used for the signal photon reconstruction , and a fiducial cut on the direction of the missing momentum .",
    "after these preliminary requirements have been imposed , eleven variables are identified to be included in the final optimization procedure : cosine between the signal lepton and photon candidates in the center - of - mass frame ( coslg ) , the inverse of the difference between the mass of the @xmath174 combination obtained from the signal photon candidate and any other photon in the event closest to the nominal @xmath174 mass and the nominal @xmath174 mass ( ipi0 ) , cosine of the angle between the recoil @xmath3 candidate and the @xmath173 momentum in the center - of - mass frame ( costheblg ) , energies of the lepton and photon candidates in the center - of - mass frame ( leptone and photone ) , the total number of identified leptons in the event ( numlepton ) , a fisher discriminant composed of the 0th and 2nd legendre moments of all tracks forming the recoil @xmath3 candidate computed with respect to the thrust axis given by the @xmath173 system ( fisher ) , magnitude of the cosine between the thrust axis of the recoil @xmath3 and the thrust axis of the @xmath173 system ( acthrust ) , the difference between the energy of the recoil @xmath3 candidate and the beam energy in the center - of - mass frame ( deltae ) , the beam - energy - constrained mass of the recoil @xmath3 candidate ( mes ) , and the difference between the missing energy and the magnitude of the missing momentum ( nuep ) .",
    "distributions of these variables for signal and background are shown in fig .",
    "[ fig : lnugamma_e_vars ] .",
    "the correlation between fisher and acthrust is 0.80 for both signal and combined background from all listed sources .",
    "costheblg and nuep show a strong negative correlation , -0.87 , for the combined background and a weaker correlation , -0.62 , for the signal .",
    "all other included variables show less significant levels of correlation .",
    "-2.0 cm    distributions of these eleven variables in the signal and combined background monte carlo samples are used by various optimization algorithms to maximize the signal significance expected in 210  fb@xmath158 of data .",
    "the training samples used for this optimization consist of roughly half a million signal and background monte carlo events in both electron and muon channels , appropriately weighted according to the integrated luminosity observed in the data .",
    "the training : validation : test ratio for the sample sizes is 2:1:1 .",
    "the exact breakdown of generated events for each channel is shown in table  i. signal monte carlo samples are weighted assuming a branching fraction of @xmath175 for each channel .",
    "authors of this analysis deploy an original cut optimization routine for separation of signal and background .",
    "four variables ( coslg , numlepton , acthrust , and mes ) are used for one - sided optimization , and the other seven variables are used for two - sided optimization .",
    "the available range for each variable is divided into intervals of preselected length . at each iteration step of the optimization procedure ,",
    "a 1d plot of the signal significance versus the cut position is constructed for each one - sided optimization variable ; for each two - sided optimization variable , a similar 2d plot is made of the signal significance versus all possible combinations of the two cut positions on both ends of the variable range .",
    "at each iteration step , the optimization algorithm finds the best variable and the best one- or two - sided cut on this variable that maximizes the signal significance .",
    "this optimal cut on one variable is then imposed and remains in effect for all next iterations of the algorithm .",
    "this cut is in effect for the optimization of all selection variables except the variable on which it has been imposed .",
    "for example , events that are rejected by a cut on numlepton are included in the optimization of a new cut on numlepton at the next iteration of the algorithm .",
    "this allows the algorithm to relax the cut and even remove it completely if it improves the signal significance .",
    "this algorithm makes 10 - 20 iterations and produces an optimized rectangular region in the 11-dimensional space .",
    "the optimized signal region is then tested on independently generated test data . note",
    "that this algorithm does not include a validation stage .",
    "[ tab : lnugamma_training ]    .generated signal and background contributions used for the training samples in the @xmath0 analysis . [ cols=\"^,^,^\",options=\"header \" , ]     i draw several conclusions from this exercise :    * the signal region does not have a well - defined optimum . using various methods",
    ", it is possible to find signal regions that are very different yet comparable in terms of the expected signal significance .",
    "* from the point of view of classification , the physics data in this analysis are quite simple .",
    "this is why simple - minded classifiers such as the bump hunter and the original method developed by the analysts are competitive with flexible classifiers such as decision trees and adaboost . * the two classifiers that require more training time , adaboost with decision trees and adaboost - based combiner of individual background subclassifiers ,",
    "provide a somewhat better performance .",
    "* adaboost with binary splits shows similar output distributions for the training and validation samples .",
    "it is so robust that the validation stage can be safely omitted .",
    "for adaboost with decision trees , the situation is dramatically different . now",
    "the use of the validation samples is crucial for finding the optimal tree node size and , even more importantly , the optimal cut on the adaboost output .",
    "this shows the distinction between using primitive classifiers ( binary splits ) and flexible classifiers ( decision trees ) as building blocks for adaboost .",
    "the second item needs a little more comment .",
    "the simplicity of the data in this analysis is not completely accidental .",
    "physicists tend to include only those variables in analysis that obviously contribute to the signal - background separation .",
    "the traditional approach is to visually inspect one - dimensional distributions of a certain variable for the signal and background components and include this variable only if the two distributions are clearly distinct . with this mindset ,",
    "one will rarely find himself in a situation where a sophisticated flexible classifier shows a significant improvement over simpler methods , e.g. , orthogonal cuts .",
    "physicists are beginning to adopt more advanced methods that search for trends , hardly visible to the human eye , in data with dozens of input variables .",
    "in such analyses , adaboost , decision trees and other powerful classifiers will prove most useful .",
    "although none of the attempted classifiers gives a significantly better performance over the original method designed by the analysts , it is possible to improve the signal significance in this analysis by a considerable amount .",
    "this can be accomplished by using a powerful technique known in the statistics literature as `` bagging '' .",
    "this method will be described in a separate note .    among the listed classifiers ,",
    "adaboost with binary splits , the decision tree and the bump hunter are the fastest .",
    "training these classifiers on a sample of 550k events takes only several minutes on a linux node with a 1.8  ghz cpu processor and 2  gb of available memory .",
    "training of the adaboost classifier built with 50 decision trees and the adaboost - based combiner of background - categorized subclassifiers requires batch job submissions and consumes 4 - 8 hours of running on allocated batch nodes .",
    "because the cpu speed and available memory vary from one batch node to another , a straightforward comparison of the training time is not feasible .",
    "the original method developed by the analysts also requires several hours of training time ; however , this training procedure is performed in a substantially different framework and its speed could be significantly limited by root output .",
    "a c++ package tailored for needs of hep analysts has been implemented and is available for free distribution to hep researchers .",
    "thanks to gregory dubois - felsmann for useful discussions and the idea to sort terminal nodes of a decision tree by signal purity .",
    "thanks to ed chen for data and documentation on the @xmath0 analysis .",
    "thanks to byron roe and frank porter for their comments on a draft of this note .",
    "w.s .  mcculloch and w.",
    "pitts , _ a logical calculus of the ideas immanent in nervous activity _ , bulletin of mathematical biophysics * 5 * , 115 - 133 ( 1943 ) ; f.  rosenblatt , _ the perceptron : a probabilistic model for information storage and organization in the brain _ , psychological review * 65 * , 386 - 408 ( 1958 ) ; j.j .",
    "hopfield , _ neural networks and physical systems with emergent collective computational abilities _ , proceedings of the national academy of sciences , usa , * 79 * , 2554 - 2558 ( 1982 ) ; d.e .  rumelhart et al . , _ learning internal representation by error propagation _ , parallel distributed processing : explorations in the microstructure of cognition * 1 * , 318 - 362 ( 1986 ) ; j.a .  anderson and e.  rosenfeld ,",
    "_ neurocomputing .",
    "foundations of research _ , mit press , 1988 .",
    "artificial neural networks in high energy physics , + http://neuralnets.web.cern.ch/neuralnets/nnwinhep.html ; c.  peterson , _ track finding with neural networks _ ,",
    "nucl . instr . and meth .",
    "* a279 * , 537 ( 1989 ) ; l.  lonnblad et al . , _ finding gluon jets with a neural trigger _ ,",
    "phys . rev .",
    "lett . * 65 * , 1321 - 1324 ( 1990 ) ; d.  cutts et al .",
    ", _ neural networks for event filtering at d0 _ , comput . phys",
    ". commun . * 57 * , 478 - 482 ( 1989 ) ; d.  cutts et al .",
    ", _ the use of neural networks in the d0 data acquisition system _ , ieee trans .",
    "sci . * 36 * , 1490 - 1493 ( 1989 ) ; b.h .",
    "denby et al . , _ neural networks for triggering _ , ieee trans .",
    "sci . * 37 * , 248 - 254 ( 1990 ) .",
    "broomhead and d.  lowe , _ multi - variable function interpolation and adaptive networks _ , complex systems * 2*(3 ) , 269 - 303 ( 1988 ) ; t.  poggio and f.  girosi , _ networks for approximation and learning _ , ieee proceedings * 78 * , 1481 - 1497 ( 1990 ) .",
    "j.  allison , _ multiquadratic radial basis functions for representing multidimensional high - energy physics data _ , comput .",
    ". commun . * 77 * , 377 - 395 ( 1993 ) ; j.c .",
    "ribes et al . , _ rbf neural net based classifier for the airix accelerator fault diagnosis _ , physics/0008030 , 2000 .      d.  bowser - chao and d.l .",
    "dzialo , _ a comparison of binary decision trees and neural networks in top quark detection _ , phys . rev . *",
    "d47 * , 1900 - 1905 ( 1993 ) ; m.  mjahed , _ multivariate decision tree designing for the classification of multi - jet topologies in @xmath176 collisions _ , nucl .",
    "instrum . and meth .",
    "* a481 * , 601 - 614 ( 2002 ) ; r.  quiller , _ decision tree technique for particle identification _ , slac - tn-03 - 019 , 2003 .",
    "y.  freund and r.e .",
    "schapire , _ a decision - theoretic generalization of on - line learning and an application to boosting _",
    ", j. of computer and system sciences * 55 * , 119 - 139 ( 1997 ) ; l.  breiman , _ arcing classifiers _ , the annals of statistics * 26 * , 801 - 849 ( 1998 ) ; r.e .",
    "schapire et al . , _ boosting the margin : a new explanation for the effectiveness of voting methods _ , the annals of statistics * 26 * , 1651 - 1686 ( 1998 ) .",
    "j.  friedman and j.  tukey , _ a projection pursuit algorithm for exploratory data analysis _ , ieee trans . on computers",
    "* c23 * , 881 - 889 ( 1974 ) ; j.  friedman and w.  stuetzle , _ projection pursuit regression _ , j. of the amer .",
    "* 76 * , 817 - 823 ( 1981 ) .",
    "institute for parallel and distributed high performance systems ( university of stuttgart ) and wilhelm - schickard institute for computer science ( university of tbingen ) , _ stuttgart neural network simulator : user manual , version 4.2 _ , http://www - ra.informatik.uni - tuebingen.de / snns/."
  ],
  "abstract_text": [
    "<S> modern analysis of high energy physics ( hep ) data needs advanced statistical tools to separate signal from background . </S>",
    "<S> a c++ package has been implemented to provide such tools for the hep community . </S>",
    "<S> the package includes linear and quadratic discriminant analysis , decision trees , bump hunting ( prim ) , boosting ( adaboost ) , bagging and random forest algorithms , and interfaces to the standard backpropagation neural net and radial basis function neural net implemented in the stuttgart neural network simulator . </S>",
    "<S> supplemental tools such as bootstrap , estimation of data moments , and a test of zero correlation between two variables with a joint elliptical distribution are also provided . </S>",
    "<S> the package offers a convenient set of tools for imposing requirements on input data and displaying output . </S>",
    "<S> integrated in the  computing environment , the package maintains a minimal set of external dependencies and therefore can be easily adapted to any other environment . </S>",
    "<S> it has been tested on many idealistic and realistic examples . </S>"
  ]
}