{
  "article_text": [
    "recent works have shown that the performance of deep neural networks ( dnn ) benefits a lot from big data and big models  @xcite .",
    "however , the training of large dnn models over big data is extremely time - consuming .",
    "to accelerate the training of dnn , parallelization frameworks like mapreduce  @xcite and parameter server  @xcite have been widely used .",
    "a typical parallel training procedure for dnn contains many iterations of the following three steps .",
    "first , each worker performs local training based on its local data by stochastic gradient decent ( sgd ) or its variants .",
    "second , the parameters of the local dnn models are communicated and aggregated to a global model , e.g. , by averaging the parameters of the local models  @xcite .",
    "third , the global model is used as the starting point of the next round of local training .",
    "we call the method that performs the aggregation by averaging model parameters _ ma _ , and call the corresponding parallel implementation of dnn _ ma - dnn_.    however , since dnn is a highly non - convex model , the loss of the global model produced by ma can not be guaranteed to be upper bounded by the average loss of the local models . in other words , ma - dnn lacks guarantee for the performance improvements of the global model over the local models , and sometimes ( especially when the local models fall into different local - convex domains ) its global model could be even worse than any local model . considering that the aggregated global model will be used as the starting point of the successive iterations of local training , the poor performance of the global model will slow down the convergence of the training process and thus hurt the performance of the final model .    to tackle this problem , we propose a novel method for parallel dnn training , which is called _ ensemble - compression ( ec - dnn)_. the key idea is to produce the global model by ensemble instead of ma .",
    "specifically , ensemble averages the outputs but not the parameters of the local models .",
    "equivalently , the global model produced by ensemble is a larger network with one additional layer with weights @xmath0 connecting to the output nodes of the local models , where @xmath1 is the number of the local models . since the widely used loss functions for dnn ( e.g. , cross entropy loss , square loss and hinge loss ) are convex with respect to the output vector of the model ,",
    "the loss of the global model produced by ensemble can be upper bounded by the average loss of the local models .",
    "empirical evidence in  @xcite even show that the ensemble model ( i.e. , the global model in our context ) is usually better than any base model ( i.e. , the local model in our context ) .",
    "please note that the ensemble will increase the model complexity : the resultant global model will have one additional layer and will be @xmath1 times wider than the local models . if the ensemble is conducted for multiple iterations and the successive local training starts directly from the global model produced by ensemble , the model size will become bigger and bigger and explode quickly ( which will lead to extraordinarily high space and time complexity ) . to avoid this situation ,",
    "we further propose adding a compression step after the ensemble , to ensure the size of the resultant global model to be the same size as those local models while preserving the accuracy of the global model .",
    "in particular , we compress the global model in a similar way to  @xcite , which labels the local training data by the global model and trains a new model ( whose size is the same as the previous local models ) based on the pseudo labels produced by the global model .",
    "the future local training is conducted by using this compressed model as its starting point . in order to alleviate the fluctuations of the training curve between compression and local training phases ( which is caused by switching the loss function with the pseudo labels to that with true labels ) , we train the compressed model by minimizing the weighted combination of the loss with the pseudo labels and the loss with the true labels , and the combination coefficient for the loss with pseudo labels will decrease to @xmath2 so as to transit to the pure local training phase . with this soft transition ,",
    "the training error curve goes down more smoothly throughout the compression and the local training .",
    "we conducted experiments on cifar-100 benchmark dataset under different settings w.r.t .",
    "the number of workers and the communication frequency .",
    "we have the following observations from the experimental results .",
    "( 1 ) for ma - dnn , the performance of the global model varied dramatically and was much worse than the local models in many cases .",
    "( 2 ) for ec - dnn , we compared the average performance of the local models at the end of the local training , the performance of the global model , and the performance of the compressed model before the pure local training started .",
    "we observe that the global model consistently outperformed the local models while the compressed model only led to minor accuracy drop and still significantly outperformed the local models .",
    "( 3 ) in terms of the end - to - end results , our proposed ec - dnn method stably achieved better test accuracy than ma - dnn in all the settings .",
    "the remaining part of this paper is organized as follows . in section  [",
    "sec : preliminary ] , we briefly introduce dnn and the parallel training of dnn . in section",
    "[ sec : ma_vs_ec ] , we compare ma and ensemble for dnn models . in section  [ sec : ec_alg ] , we propose the new parallel training method for dnn , i.e , ensemble - compression ( ec - dnn ) method",
    ". we present the experimental results in section  [ sec : exp ] , and conclude the paper in section  [ sec : discussion ] .",
    "in this section , we briefly introduce dnn models and the parallel training of dnn .",
    "deep neural networks ( dnn ) is a multi - layer non - linear model .",
    "specifically , in a @xmath3-class classification problem , assume the input @xmath4 and the output @xmath5 with @xmath6 .",
    "the space of @xmath7-layer dnn models can be defined as follows .",
    "@xmath8    where @xmath9 denotes the parameter in dnn , @xmath10 is the @xmath11-th dimension of the input @xmath12 , @xmath13 is the activation function , and @xmath14 is the pooling function ( which is set as @xmath15 except for the convolutional layer ) . for ease of reference , we denote all the parameters of a dnn model as a vector @xmath16 . then , a @xmath7-layer dnn model with weights @xmath16 is denoted as @xmath17 . in addition , we denote the output of the model @xmath18 on the input @xmath12 as @xmath19 .    the loss of the dnn model @xmath18 on the data @xmath20 is denoted as @xmath21 .",
    "the most widely used loss for dnn is the cross entropy loss ( or equivalently the log likelihood loss ) , i.e. ,    @xmath22    other widely used loss functions include the square loss , the hinge loss , etc . to learn a dnn model , back - propagation algorithm  @xcite is usually employed to minimize the loss , in which the weights are updated by sgd  @xcite .      during the dnn training ,",
    "since the gradient computation of very deep networks over large datasets is time - consuming , parallel implementations have been widely used to speed up the dnn training . in the parallel training of dnn ,",
    "suppose that there are @xmath1 workers and each of them holds a local dataset @xmath23 with size @xmath24 , @xmath25 .",
    "denote the weights of the dnn model at the iteration @xmath26 on the worker @xmath27 as @xmath28 .",
    "the communication between the workers is invoked after every @xmath29 iterations of updates for the weights , and we call @xmath29 the communication frequency . a typical parallel training procedure for dnn implements the following three steps in an iterative manner until the training curve converges .    \\1 .",
    "_ local training : _ at iteration @xmath26 , worker @xmath27 updates its local model from @xmath28 to @xmath30 using sgd , i.e. , @xmath31 , where @xmath32 is the learning rate and @xmath33 is the gradients of the local model @xmath28 on one mini batch of the local dataset @xmath34 .",
    "such local model is updated for @xmath29 iterations before the cross - machine synchronization .",
    "_ model aggregation : _ the parameters of local models @xmath35 are communicated across machines . then , a global model is produced by aggregating local models according to certain aggregation method .",
    "we denote the aggregation method as @xmath36 and the weights of the global model as @xmath37 .",
    "that is , @xmath38 , where @xmath39 .",
    "_ local model reset : _ the global model is sent back to the local workers , and set as the starting point for the next round of local training , i.e. , @xmath40 , where @xmath39 .",
    "a widely used model aggregation method is _ model average ( ma ) _",
    "( denoted as @xmath41 ) , which averages the parameters of the local models  @xcite , i.e. ,    @xmath42    we denote the parallel training method of dnn that using ma as ma - dnn for ease of reference .",
    "in this section , we first show that the performance of the global model produced by ma does not have guarantee on its performance improvement over the local models .",
    "then , we propose using ensemble to perform the model aggregation .    actually , ma ( and its variants such as admm  @xcite ) was originally proposed for convex optimization . if the model is convex w.r.t .",
    "the parameters and the loss is convex w.r.t . the model outputs , the performance of the global model produced by ma will be guaranteed to be no worse than the average performance of local models .",
    "this is because when @xmath43 is a convex model , we have ,    @xmath44    besides , when the loss is also convex w.r.t .",
    "the model outputs @xmath45 , we have ,    @xmath46    combining inequalities ( [ eqn : pdnn_model_convex ] ) and ( [ eqn : pdnn_output_convex ] ) , we can see the rationality of using ma in the context of convex optimization .",
    "however , unfortunately , dnn is a highly non - convex model due to the iteration of activation function and pooling function ( for convolutional layers ) according to the definition in eqn([eqn : dnn_1 ] ) .",
    "therefore , the above properties of ma for convex optimization does not hold for dnn , and therefore the performance of the global model produced by ma can not be guaranteed .",
    "especially , when the local models are in the neighborhoods of different local optima , the global model could be even worse than any of the local models .",
    "furthermore , considering that the global model is the start of the next round of local training , if the global model achieves poor performance , the performance of the final model could hardly be good .",
    "this analysis is actually not just theoretical , the experimental results reported in section  [ subsec : psgd ] verified it .",
    "while the dnn model itself is non - convex , we notice that all the widely used loss functions for dnn are convex w.r.t .",
    "the model outputs ( i.e. , eqn ( [ eqn : pdnn_output_convex ] ) is hold ) . in this case , if we average the output of the local models instead of their parameters , we will have the performance guarantee .",
    "therefore , we propose to _ ensemble _ the local models by averaging their outputs as follows ,    @xmath47",
    "in this section , we propose a new method for parallel training of dnn models , which uses ensemble to produce the global model . we denote it as ec - dnn for ease of reference .",
    "algorithm  [ alg : ensemble_compression ] shows the details of ec - dnn .",
    "the ec - dnn algorithm also iteratively implements local training , model aggregation , and local model reset just like other methods for parallel training of dnn .",
    "the local training of ec - dnn is the same as that of ma - dnn . for the model aggregation",
    ", ec - dnn uses ensemble and then introduces an additional compression process to convert the big global model to a small one .",
    "finally , the compressed model will be set as the starting point of local training , followed by a new local training phase . at the end of the training",
    ", the ec - dnn will output @xmath1 local models and we choose the model with the smallest training loss as the final model .",
    "we can also take the global model ( i.e. , ensemble of @xmath1 local models ) as the final model if there are enough computation and storage resources for the test . in the next three subsections",
    ", we will explain the technical details of ec - dnn .",
    "assume we conduct a cross - machine communication at the end of the iteration @xmath26 .",
    "first , the parameters of the local models , i.e. , @xmath48 , are broadcasted to each worker .",
    "then , we produce the global model by ensemble according to @xmath49 in eqn([eqn : ensemble ] ) , i.e. , averaging the outputs of the local models .",
    "the resultant global model is equivalent to adding a new layer with @xmath3 nodes to connect all the local models together , with weights of @xmath0 between the output nodes of the local models and the corresponding node in the new layer .",
    "we denote the function space of the global model produced by ensemble as @xmath50 , i.e. ,    @xmath51    where @xmath52 is the function space of the local models , i.e. , @xmath53 .",
    "if the weights of the global model are denoted as @xmath37 , we have another representation of the global model as follows : @xmath54 . from this representation",
    ", we can see that the global model is one layer deeper and @xmath1 times wider than the local models . then",
    ", if the ensemble process is conducted for many times , and after each time the global model is used as the starting point of the next round of local training , the global model will become bigger and bigger , and eventually we will suffer from the problem of model size exploding , which will lead to extraordinarily high space and time complexity .",
    "r0.54    randomly initialize @xmath55 and set @xmath56      to avoid this problem of model size exploding , we propose introducing a compression process after the ensemble process , to ensure the size of the resultant global model to be the same as those local models while preserving the model accuracy . the compression is conducted on each worker independently and in parallel in order to avoid additional communicational cost . specifically ,",
    "we perform model compression in a similar way to  @xcite .",
    "first , we construct a new dataset @xmath57 by relabeling the original dataset @xmath34 with the labels produced by the global model , i.e. , @xmath58\\}$ ] , which we call pseudo labels .",
    "then , on local worker @xmath27 , the compressed version of the global model @xmath59 is obtained by minimizing the following loss with the pseudo labels on dataset @xmath57 ,    @xmath60    where @xmath52 is the function class of @xmath7-layer dnn with the same size of the local model . on each worker",
    ", we use the local model before ensemble ( i.e. , @xmath28 ) as the initialization and minimize the above loss function in the compression process ( i.e. , eqn([eqn : compress_loss_general ] ) ) by sgd with @xmath61 steps .      when we move onto the local training after the model compression , we will switch the loss function from the one with the pseudo labels to that with the true labels .",
    "this switch may lead to fluctuations of the training process . to alleviate the fluctuations",
    ", we mix the phases of model compression and local training together by minimizing the weighted combination of the loss with the pseudo labels ( produced by the global model ) and the loss with the true labels , i.e. ,    @xmath62    where @xmath63 is the combination coefficient and will decrease to @xmath2 in order to take us to the pure local training phase at the end . with this soft transition ,",
    "the training curve goes down more smoothly throughout the processes of the compression and the local training .",
    "in addition , with this soft transition , we do not have much extra computational cost for model compression .",
    "that is , although the compression process lasts for @xmath61 steps , it happens together with the next round of local training .",
    "as a result , compared with ma - dnn , ec - dnn only requires a little more time to relabel @xmath57 by the global model , and we call the extra time _ relabeling time _ for ease of reference .",
    "the relabeling time is equal to the time for feed - forward propagation on the dataset @xmath34 multiplied by the number of workers @xmath1 . in order to further reduce the relabeling time",
    ", we can choose to only relabel a portion , instead of the full set of @xmath34 .",
    "in this section , we present our experimental results to demonstrate the effectiveness of ec - dnn .",
    "we conducted experiments on the public dataset cifar-100  @xcite , which contains 50000 training images and 10000 test images for 100-class classification .",
    "each image was normalized by subtracting the per - pixel mean computed over the whole training set .",
    "the training images were horizontally flipped but not cropped , and the test data were neither flipped nor cropped .    in our parallel architecture",
    ", we use a gpu cluster interconnected with an infiniband network .",
    "each local worker corresponds to one gpu processor .",
    "the training instances are randomly partitioned , and allocated to the workers . for each dataset , we explored the following experimental configurations . for the number of workers @xmath1",
    ", we explored @xmath64 and @xmath65 .",
    "for the communication frequency @xmath29 , we explored @xmath66 .",
    "@xmath67 and @xmath68 epochs .",
    "we use soft transition from compression to pure local training with the combination coefficient @xmath69 for @xmath70 iterations ( i.e. , @xmath71 ) .",
    "we use a 9-layer convolutional network structure with relu activation function and max - pooling  @xcite ( called nin ) in our experiments . during all of the training processes , we used random initialization , @xmath72-regularization , and the momentum trick .",
    "we measured the models by their test errors .      in this subsection",
    ", we show the performance of ma - dnn .",
    "specifically , we compare the performance of the local models and the global models produced by ma - dnn .",
    "we report the average test error of local models at end of each local training phase and the test error of the global model at each step of ma in figure  [ fig : psgd_compare ] .    from figure  [",
    "fig : psgd_compare ] , we can observe that the performance of the global model might be worse than the average performance of the local models sometimes and for some extreme case , the global model even became something close to random guess .",
    "specifically , the average error increase of the global model as compared to the local models was above 15% , and sometimes the error increase might be as large as 40% .",
    "in addition , the poor performance of the global model did not happen occasionally but occurred in many iterations of ma - dnn .",
    "specifically , under all the experimental configurations , over 40% global models achieved worse performance than the average performance of the local models .",
    "these empirical observations are consistent with our analysis in section  [ sec : ma_vs_ec ] , which indicates that the performance of the global model can not be bounded by the average performance of the local models .      in this subsection , we show the performance of ec - dnn . specifically , we compare the performance of the local models , the global model , and the compressed model .",
    "we report the average test error of the local models at the end of each local training phase , the test error of the global model and the test error of compressed model in figure  [ fig : ec_compare ] .    from figure",
    "[ fig : ec_compare ] , we can observe that the performance of the global model was consistently better than the average performance of the local models under different experimental configurations .",
    "specifically , the average improvements of the global model over the local models was about 7% .",
    "for example , for the first communication under all experimental configurations , the global model reduced the error by about 10% as compared to the local models .",
    "this is consistent with our analysis in the previous sections .    besides , after we compressed the big global model to a model with the same size of the previous local models , the error of the compressed model did not increase too much , and still outperformed the average performance of the local models",
    "specifically , the average improvement of the compressed models over the local models was around 4% .      in this subsection",
    ", we compare the performance of ma - dnn with that of ec - dnn .",
    "for ease of reference , we denote the sequential model trained on one gpu in  @xcite as s - dnn ; denote the ec - dnn and ma - dnn methods with communication frequency @xmath29 as ec - dnn(@xmath29 ) and ma - dnn(@xmath29 ) respectively ; denote the ec - dnn(@xmath29 ) and ma - dnn(@xmath29 ) methods that take the local model with the smallest training error for the final model as ec - dnn@xmath73(@xmath29 ) and ma - dnn@xmath73(@xmath29 ) respectively ; and denote the ec - dnn(@xmath29 ) and ma - dnn(@xmath29 ) methods that take the global model ( i.e. , the ensemble of local models for ec - dnn and the average parameters of local models for ma - dnn ) for the final model as ec - dnn@xmath74(@xmath29 ) and ma - dnn@xmath74(@xmath29 ) respectively",
    ".    r0.63    [ cols=\"^,^,^,^,^,^,^ \" , ]     table  [ tab : test_error ] compares the performances of the final model outputted by ma - dnn and ec - dnn",
    ". please note that we report the final performances when both ma - dnn and ec - dnn were trained for the same length of time and converged . from table",
    "[ tab : test_error ] , we have the following observations .",
    "first , we observe that each ec - dnn@xmath73 outperformed ma - dnn@xmath73 and ma - dnn@xmath74 .",
    "the best ec - dnn@xmath73 achieved test errors of 34.95% and 35.48% for @xmath64 and @xmath65 respectively . besides , each ec - dnn@xmath74 outperformed ma - dnn@xmath73 and ma - dnn@xmath74 by a larger margin",
    ". the average improvements of ec - dnn@xmath74 over ma - dnn@xmath73 and ma - dnn@xmath74 were around 5% and 10% for @xmath64 and @xmath65 respectively .",
    "furthermore , all ec - dnn@xmath73 and ec - dnn@xmath74 outperformed or achieved comparable performances with the s - dnn .    in figure  [ fig : psgd_ec_cifar ] , we plot the average test error of the local models during the training process w.r.t . the training time . for ma - dnn",
    ", we observe that the test error increased after the poor global model was set as the starting point of the next round of local training , which led to the slow - down of the convergence and unsatisfactory final performance .",
    "for example , for @xmath64 , the test errors of ma - dnn(40 ) increased around 1.5 hours and 2.5 hours .",
    "after that , even though the test error started to decrease , the decrease of the test error for ma - dnn(40 ) could not catch up with that of ec - dnn(40 ) . on the contrary ,",
    "for ec - dnn , we observe that the test error was decreased or kept after we set the compressed model as the starting point of the next round of local training . as a result , the test error smoothly deceased during the training process and thus achieved much better final performance .",
    "in this paper , we propose a new method for parallel training of dnn , called ec - dnn .",
    "compared to the traditional approach that averages the parameters of different local models ( ma - dnn ) , our proposed method uses the ensemble method to aggregate local models , which plays with the outputs instead of parameters of local models . in this way",
    ", we can guarantee that the error of the global model in ec - dnn is upper bounded by the average error of the local models and can consistently achieve better performance than ma - dnn . as for the future work ,",
    "we plan to consider other compression methods for ec - dnn .",
    "besides , we will investigate the theoretical properties of the ensemble method , compression method , and the whole ec - dnn framework ."
  ],
  "abstract_text": [
    "<S> in recent year , parallel implementations have been used to speed up the training of deep neural networks ( dnn ) . </S>",
    "<S> typically , the parameters of the local models are periodically communicated and averaged to get a global model until the training curve converges ( denoted as ma - dnn ) . however , since dnn is a highly non - convex model , the global model obtained by averaging parameters does not have guarantee on its performance improvement over the local models and might even be worse than the average performance of the local models , which leads to the slow - down of convergence and the decrease of the final performance . to tackle this problem , we propose a new parallel training method called _ ensemble - compression _ ( denoted as ec - dnn ) . </S>",
    "<S> specifically , we propose to aggregate the local models by ensemble , i.e. , the outputs of the local models are averaged instead of the parameters . </S>",
    "<S> considering that the widely used loss functions are convex to the output of the model , the performance of the global model obtained in this way is guaranteed to be at least as good as the average performance of local models . </S>",
    "<S> however , the size of the global model will increase after each ensemble and may explode after multiple rounds of ensembles . </S>",
    "<S> thus , we conduct model compression after each ensemble , to ensure the size of the global model to be the same as the local models . </S>",
    "<S> we conducted experiments on a benchmark dataset . </S>",
    "<S> the experimental results demonstrate that our proposed ec - dnn can stably achieve better performance than ma - dnn . </S>"
  ]
}