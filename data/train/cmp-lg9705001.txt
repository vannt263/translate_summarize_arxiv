{
  "article_text": [
    "grammatical acquisition proceeds on the basis of a partial genotypic specification of ( universal ) grammar ( ug ) complemented with a learning procedure enabling the child to complete this specification appropriately . the parameter setting framework of chomsky ( 1981 ) claims that learning involves fixing the values of a finite set of finite - valued parameters to select a single fully - specified grammar from within the space defined by the genotypic specification of ug .",
    "formal accounts of parameter setting have been developed for small fragments but even in these search spaces contain local maxima and subset - superset relations which may cause a learner to converge to an incorrect grammar ( clark , 1992 ; gibson and wexler , 1994 ; niyogi and berwick , 1995 ) .",
    "the solution to these problems involves defining default , unmarked initial values for ( some ) parameters and/or ordering the setting of parameters during learning .",
    "bickerton ( 1984 ) argues for the bioprogram hypothesis as an explanation for universal similarities between historically unrelated creoles , and for the rapid increase in grammatical complexity accompanying the transition from pidgin to creole languages . from the perspective of the parameters framework ,",
    "the bioprogram hypothesis claims that children are endowed genetically with a ug which , by default , specifies the stereotypical core creole grammar , with right - branching syntax and subject - verb - object order , as in saramaccan .",
    "others working within the parameters framework have proposed unmarked , default parameters ( e.g. lightfoot , 1991 ) , but the bioprogram hypothesis can be interpreted as towards one end of a continuum of proposals ranging from all parameters initially unset to all set to default values .",
    "a model of the language acquisition device ( lad ) incorporates a ug with associated parameters , a parser , and an algorithm for updating initial parameter settings on parse failure during learning .",
    "basic categorial grammar ( cg ) uses one rule of application which combines a functor category ( containing a slash ) with an argument category to form a derived category ( with one less slashed argument category ) .",
    "grammatical constraints of order and agreement are captured by only allowing directed application to adjacent matching categories . generalized categorial grammar ( gcg )",
    "extends cg with further rule schemata .",
    "the rules of fa , ba , generalized weak permutation ( p ) and backward and forward composition ( fc , bc ) are given in figure  [ rules ] ( where x , y and z are category variables , @xmath0 is a variable over slash and backslash , and @xmath1 denotes zero or more further functor arguments ) .",
    "once permutation is included , several semantically equivalent derivations for _ kim loves sandy _ become available , figure  [ p - ba ] shows the non - conventional left - branching one .",
    "composition also allows alternative non - conventional semantically equivalent ( left - branching ) derivations .",
    "x / y y @xmath2  x & @xmath3 y [ x(y ) ] ( y ) @xmath2  x(y ) +   + y x@xmath4y @xmath2  x & @xmath3 y [ x(y ) ] ( y ) @xmath2  x(y ) +   + x / y y / z @xmath2  x / z & @xmath3 y [ x(y ) ] @xmath3 z [ y(z ) ] @xmath2  @xmath3 z [ x(y(z ) ) ] +   + y@xmath4z x@xmath4y @xmath2  x@xmath4z & @xmath3 z [ y(z ) ] @xmath3 y [ x(y ) ] @xmath2  @xmath3 z [ x(y(z ) ) ] +   + ( x@xmath0y@xmath5)@xmath6y@xmath7 @xmath2(x@xmath0y@xmath7)@xmath0y@xmath8 & @xmath3 y@xmath9,y@xmath5 [ x(y@xmath8,y@xmath7 ) ] @xmath2",
    "@xmath3 y@xmath5,y@xmath9 [ x(y@xmath8,y@xmath7 ) ] +    lll kim & loves & sandy + np & ( s@xmath4np)/np & np + kim & @xmath3 y , x [ love@xmath10(x y ) ] & sandy@xmath10 + & p & + & ( s / np)@xmath4np & + & @xmath3 x , y [ love@xmath10(x y ) ] & + & + s / np & & + @xmath3 y [ love@xmath10(kim@xmath10 y ) ] & & +   + s & & + love@xmath10(kim@xmath10 sandy@xmath10 ) & &    gcg as presented is inadequate as an account of ug or of any individual grammar .",
    "in particular , the definition of atomic categories needs extending to deal with featural variation ( e.g. bouma and van noord , 1994 ) , and the rule schemata , especially composition and weak permutation , must be restricted in various parametric ways so that overgeneration is prevented for specific languages .",
    "nevertheless , gcg does represent a plausible kernel of ug ; hoffman ( 1995 , 1996 ) explores the descriptive power of a very similar system , in which generalized weak permutation is not required because functor arguments are interpreted as multisets .",
    "she demonstrates that this system can handle ( long - distance ) scrambling elegantly and generates mildly context - sensitive languages ( joshi _ et al _ , 1991 ) .",
    "the relationship between gcg as a theory of ug ( gcug ) and as a the specification of a particular grammar is captured by embedding the theory in a default inheritance hierarchy .",
    "this is represented as a lattice of typed default feature structures ( tdfss ) representing subsumption and default inheritance relationships ( lascarides _ et al _ , 1996 ; lascarides and copestake , 1996 ) .",
    "the lattice defines intensionally the set of possible categories and rule schemata via type declarations on nodes .",
    "for example , an intransitive verb might be treated as a subtype of verb , inheriting subject directionality by default from a type * gendir * ( for general direction ) . for english , * gendir * is default * right * but the node of the ( intransitive ) functor category , where the directionality of subject arguments is specified , overrides this to * left * , reflecting the fact that english is predominantly right - branching , though subjects appear to the left of the verb .",
    "a transitive verb would inherit structure from the type for intransitive verbs and an extra np argument with default directionality specified by * gendir * , and so forth .",
    "for the purposes of the evolutionary simulation described in  3 , gc(u)gs are represented as a sequence of _ p - settings _ ( where _ p _ denotes principles or parameters ) based on a flat ( ternary ) sequential encoding of such default inheritance lattices .",
    "the inheritance hierarchy provides a partial ordering on parameters , which is exploited in the learning procedure .",
    "for example , the atomic categories , * n * , * np * and * s * are each represented by a parameter encoding the presence / absence or lack of specification ( t / f/ ? ) of the category in the ( u)g . since they will be unordered in the lattice their ordering in the sequential coding",
    "is arbitrary .",
    "however , the ordering of the directional types * gendir * and * subjdir * ( with values l / r ) is significant as the latter is a more specific type",
    ". the distinctions between absolute , default or unset specifications also form part of the encoding ( a / d/ ? ) .",
    "figure  [ binencoding ] shows several equivalent and equally correct sequential encodings of the fragment of the english type system outlined above .",
    "[ cols= \" < , < , < , < , < , < \" , ]     to test whether it was memory limitations during learning or during parsing which were affecting the results , another series of runs for `` english '' was performed with either memory limitations during learning but not parsing enabled , or vice versa .",
    "memory limitations during learning are creating the bulk of the preference for a default learner , though there appears to be an additive effect . in seven of the ten",
    "runs with memory limitations only in learning , a clear preference for default learners emerged . in five of the runs with memory limitations only in parsing there appeared to be a slight preference for defaults emerging .",
    "default learners may have a fitness advantage when the number of interactions required to learn successfully is greater because they will tend to converge faster , at least to a subset language .",
    "this will tend to increase their fitness over unset learners who do not speak any language until further into the learning period .",
    "the precise linguistic environment of adaptation determines the initial values of default parameters which evolve .",
    "for example , in the runs initialized with 16 unset learning `` malagasy '' vos adults and 16 default ( svo ) learning vos adults , the learning procedure which dominated the population was a variant vos default learner in which the value for * subjdir * was reversed to reflect the position of the subject in this language . in some of these runs , the entire population evolved a default * subjdir * ` right ' setting , though some lagts always retained unset settings for the other two ordering parameters , * gendir * and * argo * , as is illustrated in figure  [ vos - subjdir ] .",
    "this suggests that if the human language faculty has evolved to be a right - branching svo default learner , then the environment of linguistic adaptation must have contained a dominant language fully compatible with this ( minimal ) grammar .      to explore the emergence and persistence of structured language , and consequently the emergence of effective learners , ( pseudo ) random",
    "initialization was used .",
    "a series of simulation runs of 500 cycles were performed with random initialization of 32 lagts p - settings for any combination of p - setting values , with a probability of 0.25 that a setting would be an absolute principle , and 0.75 a parameter with unbiased allocation for default or unset parameters and for values of all settings .",
    "all lagts were initialized to be age 1 with a critical period of 3 interaction cycles of 2000 random interactions for learning , a maximum age of 10 , and the ability to reproduce by crossover ( 0.9 probability ) and mutation ( 0.01 probability ) from 4 - 10 . in around 5% of the runs ,",
    "language(s ) emerged and persisted to the end of the run .",
    "languages with close to optimal wml scores typically came to dominate the population quite rapidly .",
    "however , sometimes sub - optimal languages were initially selected and occasionally these persisted despite the later appearance of a more optimal language , but with few speakers . typically , a minimal subset language dominated  although full and intermediate languages did appear briefly , they did not survive against less expressive subset languages with a lower mean wml .",
    "figure  [ emerg - lgs ] is a typical plot of the emergence ( and extinction ) of languages in one of these runs . in this run ,",
    "around 10 of the initial population converged on a minimal ovs language and 3 others on a vos language .",
    "the latter is more optimal with respect to wml and both are of equal expressivity so , as expected , the vos language acquired more speakers over the next few cycles .",
    "a few speakers also converged on vos - n , a more expressive but higher wml extension of vso - n - gwp - comp .",
    "neither this nor the ovs language survived beyond cycle 14 . instead a vso language emerged at cycle 10 , which has the same minimal expressivity of the vos language but a lower wml ( by virtue of placing the subject before the object ) and this language dominated rapidly and eclipsed all others by cycle 40 .    in all these runs ,",
    "the population settled on subset languages of low expressivity , whilst the percentage of absolute principles and default parameters increased relative to that of unset parameters ( mean % change from beginning to end of runs : + 4.7 , + 1.5 and -6.2 , respectively ) .",
    "so a second identical set of ten was undertaken , except that the initial population now contained two sov - v2 `` german '' speaking unset learner lagts . in seven of these runs ,",
    "the population fixed on a full sov - v2 language , in two on the intermediate subset language sov - v2-n , and in one on the minimal subset language sov - v2-n - gwp - comp .",
    "these runs suggest that if a full language defines the environment of adaptation then a population of randomly initialized lagts is more likely to converge on a ( related ) full language .",
    "thus , although the simulation does not model the development of expressivity well , it does appear that it can model the emergence of effective learning procedures for ( some ) full languages .",
    "the pattern of language emergence and extinction followed that of the previous series of runs : lower mean wml languages were selected from those that emerged during the run .",
    "however , often the initial optimal svo - v2 itself was lost before enough lagts evolved capable of learning this language . in these runs ,",
    "changes in the percentages of absolute , default or unset p - settings in the population show a marked difference : the mean number of absolute principles declined by 6.1% and unset parameters by 17.8% , so the number of default parameters rose by 23.9% on average between the beginning and end of the 10 runs .",
    "this may reflect the more complex linguistic environment in which ( incorrect ) absolute settings are more likely to handicap , rather than simply be irrelevant to , the performance of the lagt .",
    "partially ordering the updating of parameters can result in ( experimentally ) effective learners with a more complex parameter system than that studied previously",
    ". experimental comparison of the default ( svo ) learner and the unset learner suggests that the default learner is more efficient on typologically more common constituent orders .",
    "evolutionary simulation predicts that a learner with default parameters is likely to emerge , though this is dependent both on the type of language spoken and the presence of memory limitations during learning and parsing .",
    "moreover , a svo bioprogram learner is only likely to evolve if the environment contains a dominant svo language .",
    "the evolution of a bioprogram learner is a manifestation of the baldwin effect ( baldwin , 1896 )  genetic assimilation of aspects of the linguistic environment during the period of evolutionary adaptation of the language learning procedure . in the case of grammar",
    "learning this is a co - evolutionary process in which languages ( and their associated grammars ) are also undergoing selection .",
    "the wml account of parsing complexity predicts that a right - branching svo language would be a near optimal selection at a stage in grammatical development when complex rules of reordering such as extraposition , scrambling or mixed order strategies such as v1 and v2 had not evolved .",
    "briscoe ( 1997a ) reports further experiments which demonstrate language selection in the model .",
    "though , simulation can expose likely evolutionary pathways under varying conditions , these might have been blocked by accidental factors , such as genetic drift or bottlenecks , causing premature fixation of alleles in the genotype ( roughly corresponding to certain p - setting values ) .",
    "the value of the simulation is to , firstly , show that a bioprogram learner could have emerged via adaptation , and secondly , to clarify experimentally the precise conditions required for its emergence . since in many cases these conditions will include the presence of constraints ( working memory limitations , expressivity , the learning algorithm etc . ) which will remain causally manifest , further testing of any conclusions drawn must concentrate on demonstrating the accuracy of the assumptions made about such constraints .",
    "briscoe ( 1997b ) evaluates the psychological plausibility of the account of parsing and working memory .",
    "1.6em - plus 1pt plus 1pt                                      joshi , a. , vijay - shanker , k. and weir , d. ( 1991 ) ` the convergence of mildly context - sensitive grammar formalisms ' in sells , p. , shieber , s. and wasow , t. ( ed . ) , _ foundational issues in natural language processing , _ mit press , pp .  3182 .",
    "niyogi , p. and berwick , r.c .",
    "( 1995 ) ` a markov language learning model for finite parameter spaces ' , _ proceedings of the 33rd annual meeting of the association for computational linguistics , _ mit , cambridge , ma ..    rambow , o. and joshi , a. ( 1994 ) ` a processing model of free word order languages ' in c.  clifton , l.  frazier and k.  rayner ( ed . ) , _ perspectives on sentence processing , _ lawrence erlbaum , hillsdale , nj . , pp .  267301 ."
  ],
  "abstract_text": [
    "<S> a new account of parameter setting during grammatical acquisition is presented in terms of generalized categorial grammar embedded in a default inheritance hierarchy , providing a natural partial ordering on the setting of parameters . </S>",
    "<S> experiments show that several experimentally effective learners can be defined in this framework . </S>",
    "<S> evolutionary simulations suggest that a learner with default initial settings for parameters will emerge , provided that learning is memory limited and the environment of linguistic adaptation contains an appropriate language . </S>"
  ]
}