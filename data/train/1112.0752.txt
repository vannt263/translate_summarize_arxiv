{
  "article_text": [
    "let @xmath0 be an @xmath1 by @xmath1 random matrix whose entries @xmath4 , are independent real random variables of zero mean and unit variance .",
    "we will refer to the entries @xmath5 as the _ atom _ variables .    as determinant is one of the most fundamental matrix functions , it is a basic problem in the theory of random matrices to study the distribution of @xmath6 and indeed this study has a long and rich history .",
    "the earliest paper we find on the subject is a paper of szekeres and turn @xcite from 1937 , in which they studied an extremal problem . in the 1950s",
    ", there is a series of papers @xcite devoted to the computation of moments of fixed orders of @xmath6 ( see also @xcite ) .",
    "the explicit formula for higher moments get very complicated and in general not available , except in the case when the atom variables have some special distribution ( see , for instance @xcite ) .",
    "one can use the estimate for the moments and markov inequality to obtain an upper bound on @xmath7 .",
    "however , no lower bound was known for a long time . in particular , erds asked whether @xmath6 is non - zero with probability tending to one . in 1967 , komls @xcite addressed this question , proving that almost surely @xmath8 for random bernoulli matrices ( where the atom variables are iid bernoulli , taking values @xmath9 with probability @xmath10 ) .",
    "his method also works for much more general models .",
    "following @xcite , the upper bound on the probability that @xmath11 has been improved in @xcite .",
    "however , these results do not say much about the value of @xmath7 itself",
    ".    in a recent paper @xcite , tao and the second author proved that for bernoulli random matrices , with probability tending to one ( as @xmath1 tends to infinity )    @xmath12    for any function @xmath13 tending to infinity with @xmath1 .",
    "this shows that almost surely , @xmath14 is @xmath15 , but does not provide any distributional information . for related works concerning other models of random matrices , we refer to @xcite .    in @xcite ,",
    "goodman considered random gaussian matrices where the atom variables are iid standard gaussian variables .",
    "he noticed that in this case the determinant is a product of independent chi - square variables .",
    "therefore , its logarithm is the sum of independent variables and thus one expects a central limit theorem to hold .",
    "in fact , using properties of chi square distribution , it is not very hard to prove    @xmath16    in @xcite , girko stated that holds for general random matrices under the additional assumption that the fourth moment of the atom variables is 3 .",
    "twenty years later , he claimed a much stronger result which replaced the above assumption by the assumption that the atom variables have bounded @xmath17-th moment @xcite .",
    "however , there are points which are not clear in these papers and we have not found any researcher who can explain the whole proof to us . in our own attempt , we could not pass the proof of theorem 2 in @xcite",
    ". in particular , definition ( 3.7 ) of this paper requires the matrix @xmath18 to be invertible , but this assumption can easily fail .    in this paper , we provide a transparent proof for the central limit theorem of the log - determinant . the next question to consider , naturally , is the rate of convergence .",
    "we are able to obtain a rate which we believe to be near optimal .",
    "we say that a random variable @xmath19 satisfies condition * c0 * ( with positive constants @xmath20 ) if    @xmath21    for all @xmath22 .",
    ".1 in    [ theorem : main ] assume that all atom variables @xmath5 satisfy condition * c0 * with some positive constants @xmath20 .",
    "then    @xmath23    here and later , @xmath24 . in the remaining part of the paper",
    ", we will actually prove the following equivalent form ,    @xmath25    to give some feeling about , let us consider the case when @xmath5 are iid standard gaussian .",
    "for @xmath26 , let @xmath27 be the subspace generated by the first @xmath28 rows of @xmath0 .",
    "let @xmath29 denote the distance from @xmath30 to @xmath31 , where @xmath32 is the @xmath33-th row vector of @xmath0 .",
    "then , by the  base times height \" formula , we have    @xmath34    therefore ,    @xmath35    as the @xmath5 are iid standard gaussian , @xmath36 are independent chi - square random variables of degree @xmath37 . thus , the right hand side of is a sum of independent random variables . notice that @xmath38 has mean @xmath37 and variance @xmath39 and is very strongly concentrated .",
    "thus , with high probability @xmath40 is roughly @xmath41 and so it is easy to show that @xmath40 has mean close to @xmath42 and variance @xmath43 .",
    "so the variance of @xmath44 is @xmath45 . to get the precise value",
    "@xmath46 one needs to carry out some careful ( but rather routine ) calculation , which we leave as an exercise .    the reason for which we think that the rate @xmath47 might be near optimal is that ( as the reader will see though the proofs ) @xmath48 is only an asymptotic value of the variance of @xmath49 .",
    "this approximation has error term of order at least @xmath50 and since @xmath51 @xmath52 , it seems that one can not have rate of convergence better than @xmath53 .",
    "it is a quite interesting question whether one can obtain a polynomial rate by replacing @xmath54 and @xmath48 by other , relatively simple , functions of @xmath1 .",
    "our arguments rely on recent developments in random matrix theory and look quite different from those in girko s papers .",
    "in particular , we benefit from the arguments developed in @xcite .",
    "we also use talagrand s famous concentration inequality frequently to obtain most of the large deviation results needed in this paper .     for random bernoulli matrices , random gaussian matrices , and @xmath55 .",
    "we sampled 1000 matrices of size 1000 by 1000 for each ensemble . ]",
    ".2 in    * notation .",
    "* we say that an event @xmath56 holds almost surely if @xmath57 tends to one as @xmath1 tends to infinity . for an event @xmath58",
    ", we use the subscript @xmath59 to emphasize that the probability under consideration is taking according to the random vector @xmath60 . for @xmath61 , we denote by @xmath62 the unit vector @xmath63 , where all but the @xmath64-th component are zero . all standard asymptotic notation such as @xmath65 etc are used under the assumption that @xmath66 .",
    "we first make two extra assumptions about @xmath0 .",
    "we assume that the entries @xmath5 are bounded in absolute value by @xmath67 for some constant @xmath68 and @xmath0 has full rank with probability one",
    ". we will prove theorem [ theorem : main ] under these two extra assumptions . in appendix [",
    "appendix : model ] , we will explain why we can implement these assumptions without violating the generality of theorem [ theorem : main ] .",
    "[ theorem : main1 ] assume that all atom variables @xmath5 satisfy condition * c0 * and are bounded in absolute value by @xmath67 for some constant @xmath69 .",
    "assume furthermore that @xmath0 has full rank with probability one .",
    "then    @xmath23    in the first , and main , step of the proof , we prove the claim of theorem [ theorem : main1 ] but with the last @xmath70 rows being replaced by gaussian rows ( for some properly chosen constant @xmath71 ) .",
    "we remark that the replacement trick was also used in @xcite , but for an entire different reason .",
    "our reason here is that for the last few rows , lemma [ lemma : talagrand ] is not very effective .",
    "[ theorem : mainweak ] for any constant @xmath72 the following holds for any sufficiently large constant @xmath73 .",
    "let @xmath0 be an @xmath1 by @xmath1 matrix whose entries @xmath74 , are independent real random variables of zero mean , unit variance and absolute values at most @xmath75 .",
    "assume furthermore that @xmath0 has full rank with probability one and the components of the last @xmath76 rows of @xmath58 are independent standard gaussian random variables .",
    "then    @xmath77    in the second ( and simpler ) step of the proof , we carry out a replacement procedure , replacing the gaussian rows by the original rows once at a time and show that the replacement does not effect the central limit theorem .",
    "this step is motivated by the lindeberg replacement method used in @xcite .",
    "we present the verification of theorem [ theorem : main1 ] using theorem [ theorem : mainweak ] in section [ section : deduction ] . in the rest of this section , we focus on the proof of theorem [ theorem : mainweak ] .",
    "notice that in the setting of this theorem , the variables @xmath78 are no longer independent .",
    "however , with some work , we can make the rhs of into a sum of martingale differences plus a negligible error , which lays ground for an application of a central limit theorem of martingales .",
    "( in @xcite , girko also used the clt for martingales via the base times height formula , but his analysis looks very different from ours . )",
    "we are going to use the following theorem , due to machkouri and ouchti @xcite .",
    "* theorem 1)[theorem : martingale ] there exists an absolute constant @xmath79 such that the following holds .",
    "assume that @xmath80 are martingale differences with respect to the nested @xmath81-algebra @xmath82 .",
    "let @xmath83 , and @xmath84 .",
    "assume that @xmath85 with probability one for all @xmath28 , where @xmath86 is a sequence of positive real numbers .",
    "then we have    @xmath87    to make use of this theorem , we need some preparation . condition on the first @xmath28 rows @xmath88 , we can view @xmath29 as the distance from a random vector to @xmath89 .",
    "since @xmath0 has full rank with probability one , @xmath90 with probability one for all @xmath28 . the following is a direct corollary of ( * ? ? ?",
    "* lemma 43 ) .",
    "[ lemma : talagrand ] for any constant @xmath68 there is a constant @xmath91 depending on @xmath69 such that the following holds .",
    "assume that @xmath92 is a subspace of dimension @xmath93 .",
    "let @xmath94 be a random vector whose components are independent variables of zero mean and unit variance and absolute values at most @xmath67 .",
    "denote by @xmath95 the distance from @xmath94 to @xmath96 .",
    "then we have    @xmath97    and for any @xmath22    @xmath98    set    @xmath99 where @xmath71 is a sufficiently large constant ( which may depend on @xmath69 ) .",
    "we will use short hand @xmath100 to denote @xmath37 , the co - dimension of @xmath27 ( and the expected value of @xmath101 ) .",
    "we next consider each term of the right hand side of where @xmath102 .",
    "using taylor expansion , we write    @xmath103    where    @xmath104    by lemma [ lemma : talagrand ] and by choosing @xmath71 sufficiently large , we have with probability at least @xmath105 ( the probability here is with respect to the random @xmath33-th row , fixing the first @xmath28 rows arbitrarily )    @xmath106    thus , with probability at least @xmath105    @xmath107    hence , by bayes formula , the following holds with probability at least @xmath108    @xmath109 again by having @xmath71 sufficiently large .",
    "we conclude that    [ r ] with probability at least @xmath110    @xmath111    we will need three other lemmas    [ lemma : main1 ] @xmath112    [ lemma : main2 ] @xmath113    [ lemma : main3 ] for any constant @xmath114 @xmath115    theorem [ theorem : mainweak ] follows from the above four lemmas and the following trivial fact ( used repeatedly and with proper scaling )    @xmath116    the reader is invited to fill in the simple details .",
    "we will prove lemma [ lemma : main1 ] using theorem [ theorem : martingale ] .",
    "lemma [ lemma : main2 ] will be verified by the moment method and lemma [ lemma : main3 ] by elementary properties of chi - square variables .",
    "the key to the proof of lemmas [ lemma : main1 ] and [ lemma : main2 ] is an estimate on the entries of the projection matrix onto the space @xmath117 , presented in section [ section : step2 ] .",
    "denote by @xmath118 the projection matrix onto the orthogonal complement @xmath119 .",
    "a standard fact in linear algebra is    @xmath120    also , as @xmath121 is a projection , @xmath122 . comparing the traces we obtain    @xmath123",
    "we now express @xmath124 using @xmath121 ,    @xmath125    where @xmath126 are the coordinates of the vector @xmath30 and    @xmath127    by and we have @xmath128 and @xmath129    because @xmath130 and @xmath131 , and the @xmath132 are mutually independent , we can show by using a routine calculation that ( see section [ section : main2 ] )    @xmath133    where @xmath134 is the @xmath81-algebra generated by the first @xmath28 rows of @xmath0 .",
    "define    @xmath135    and    @xmath136    the reason we split @xmath137 into the sum of @xmath138 and @xmath139 is that @xmath140 and its variance can be easily computed .",
    "[ lemma : main2 ] @xmath141    to complete the proof of lemma [ lemma : main2 ] , we show that the sum of the @xmath142 is negligible    @xmath143    our main technical tool will be the following lemma .",
    "[ lemma : errorterm ] with probability @xmath144 we have @xmath145    notice that @xmath146 is uniformly bounded ( by condition * c0 * ) , it follows that with probability @xmath147 ,    @xmath148    proving .",
    "the key idea for proving lemma [ lemma : errorterm ] is to establish a good upper bound for @xmath149 . for this , we need some new tools . our main ingredient is the following delocalization result , which is a variant of a result from @xcite , asserting that with high probability all unit vectors in the orthogonal complement of a random subspace with high dimension have small infinity norm .",
    "[ lemma : infinitynorm ] for any constant @xmath68 the following holds for all sufficiently large constant @xmath73 .",
    "assume that the components of @xmath150 , where @xmath151 , are independent random variables of mean zero , variance one and bounded in absolute value by @xmath67",
    ". then with probability @xmath144 , the following holds for all unit vectors @xmath152 of the space @xmath153    @xmath154    ( of lemma [ lemma : errorterm ] assuming lemma [ lemma : infinitynorm ] ) write    @xmath155    note that    @xmath156    hence ,    @xmath157    to bound @xmath158 , note that    @xmath159    for some unit vector @xmath160 .    thus if @xmath161 , then @xmath162 , and hence by lemma [ lemma : infinitynorm ]    @xmath163    it follows that    @xmath164    completing the proof of lemma [ lemma : errorterm ] .",
    "we now focus on the infinity norm of @xmath152 and follow an argument from @xcite .",
    "( of lemma [ lemma : infinitynorm ] ) by the union bound , it suffices to show that @xmath165 with probability at least @xmath166 , where @xmath167 is the first coordinate of @xmath152 .",
    "let @xmath168 be the matrix formed by the first @xmath169 rows @xmath150 of @xmath58 .",
    "assume that @xmath170 , then    @xmath171    let @xmath172 be the first column of @xmath168 , and @xmath173 be the matrix obtained by deleting @xmath172 from @xmath168 .",
    "clearly ,    @xmath174    where @xmath175 is the vector obtained from @xmath152 by deleting @xmath167 .",
    "we next invoke the following result , which is a variant of ( * ? ? ?",
    "* lemma 4.1 ) .",
    "this lemma was proved using a method of guionet and zeitouni @xcite , based on talagrand s inequality .",
    "[ lemma : mplaw ] for any constant @xmath68 the following holds for all sufficiently large constant @xmath73 .",
    "let @xmath0 be a random matrix of size @xmath1 by @xmath1 , where the entries @xmath5 are independent random variables of mean zero , variance one and bounded in absolute value by @xmath176 .",
    "then for any @xmath177 , there exist @xmath178 singular values of @xmath179 in the interval @xmath180 $ ] , for some absolute constant @xmath181 , with probability at least @xmath166 .",
    "we can prove lemma [ lemma : mplaw ] by following the arguments in ( * ? ? ?",
    "* lemma 4.1 ) almost word by word .    by the interlacing law and lemma [ lemma : mplaw ]",
    ", we conclude that @xmath173 has @xmath182 singular values in the interval @xmath183 $ ] with probability @xmath166 .",
    "let @xmath184 be the space spanned by the left singular vectors of these singular values , and let @xmath185 be the orthogonal projection on to @xmath184 . by definition ,",
    "the spectral norm of @xmath186 is bounded ,    @xmath187    thus implies that    latexmath:[\\[\\label{eqn : lemmas:2 }     on the other hand , since the dimension of @xmath184 is @xmath182 , lemma [ lemma : talagrand ] implies that @xmath189 with probability @xmath190 .",
    "it thus follows from that    @xmath191",
    "recall from section [ section : approach ] that conditioned on any first @xmath28 rows , @xmath192 with probability @xmath110 .",
    "so , by paying an extra term of @xmath193 in probability , it suffices to justify lemma [ lemma : main1 ] for the sequence @xmath194 .    on the other hand ,",
    "the sequence @xmath195 is not a martingale difference sequence , so we slightly modify @xmath195 to @xmath196 and prove the claim for the sequence @xmath195 . in order to show that this modification has no effect whatsoever ,",
    "we first demonstrate that @xmath197 is extremely small .",
    "recall that @xmath198 . by cauchy - schwarz inequality and the assumption that @xmath132 are bounded in absolute value by @xmath199 , we have with probability one    @xmath200    thus with probability one    @xmath201 for the sequence @xmath202 , we apply theorem [ theorem : martingale ] .",
    "the key point here is that thanks to the indicator function in the definition of @xmath195 and the fact that the difference between @xmath203 and @xmath195 is negligible , @xmath202 is bounded by @xmath204 with probability one , so the conditions @xmath205 in theorem [ theorem : martingale ] are satisfied with    @xmath206    we need to estimate @xmath207 with respect to the sequence @xmath202 .",
    "however , thanks to the observations above @xmath124 and @xmath203 are very close , and so it suffices to compute these values with respect to the sequence @xmath124 .",
    "recall from that    @xmath208    also , recall from section [ section : step2 ] that with probability @xmath144 ,    @xmath209    this bound , together with and , imply that with probability one    @xmath210    which in turn implies that @xmath211 with probability @xmath212 .    using again ,",
    "because @xmath213 , we deduce that    @xmath214    with another application of , we obtain    @xmath215    it follows that    @xmath216    by the conclusion of theorem [ theorem : martingale ] and setting @xmath71 sufficiently large , we conclude    @xmath217    completing the proof of lemma [ lemma : main1 ] .",
    "our goal is to justify lemma [ lemma : main2 ] , which together with verify lemma [ lemma : main2 ] .",
    "we will show that the variance @xmath218 is small and then use chebyshev s inequality . the proof is based on a series of routine , but somewhat tedious calculations .",
    "we first show that the expectations of the @xmath138 s are zero , and so are the covariances @xmath219 by an elementary manipulation .",
    "the variances @xmath220 will be bounded from above by cauchy - schwarz inequality .",
    "we start with the formula @xmath221 .",
    "observe that    @xmath222    expand each term , using the fact that @xmath223 and @xmath224 , we have    @xmath225    and    @xmath226    as well as    @xmath227    it follows that    @xmath228    as @xmath229 , and the @xmath132 s are mutually independent with each other and with every row of index at most @xmath28 ( and in particular with @xmath230 s ) , every term in the last formula is zero , and so we infer that @xmath231 and @xmath140 , confirming .",
    "with the same reasoning , we can also infer that the covariance @xmath232 for all @xmath233 .",
    "it is thus enough to work with the diagonal terms @xmath220 .",
    "we have    @xmath234 ^ 2.\\end{aligned}\\ ] ]    after a series of cancellation , and because of condition * c0 * , we have    @xmath235\\big),\\end{aligned}\\ ] ]    where the first two rows consist of the squares of the terms appearing in @xmath138 ( after deleting several sums of zero expected value ) , and each of the following rows was obtained by expanding the product of each term with the rest in the order of their appearance .    because @xmath224 , one has @xmath236 for all @xmath237 .",
    "recall furthermore that @xmath223 and @xmath238 for all @xmath64 .",
    "we next estimate the terms under consideration one by one as follows .",
    "firstly , the sums @xmath239 , @xmath240 , @xmath241 , and @xmath242 can be bounded by @xmath243 , and so by @xmath244 .",
    "secondly , by applying cauchy - schwarz inequality if needed , one can bound the sums + @xmath245 , @xmath246 , and @xmath247 by @xmath248 , and so by @xmath249 .",
    "we bound the remaining terms as follows .    *",
    "@xmath250 .3 in * @xmath251 .3 in * @xmath252 @xmath253 .3 in * @xmath254 .",
    "putting all bounds together we have    @xmath255    where we applied lemma [ lemma : errorterm ] in the last estimate .",
    "to complete the proof , we note from the estimate of @xmath256 of section [ section : main1 ] and from lemma [ lemma : errorterm ] that @xmath257 .",
    "thus by chebyshev s inequality    @xmath258",
    "let us first consider the lower tail ; it suffices to show    @xmath259    for any constant @xmath260 .    by properties of the normal distribution ,",
    "it is easy to show that @xmath261 and @xmath262 are at least @xmath263 with probability @xmath264 , so we can omit these terms from the sum .",
    "it now suffices to show that    @xmath265    for any small constant @xmath260 .    flipping the inequality inside the probability ( by changing the sign of the rhs and swapping the denominators and numerators in the logarithms of the lhs ) and using the laplace",
    "transform trick ( based on the fact that the @xmath266 are independent ) , we see that the probability in question is at most    @xmath267    recall that @xmath36 is a chi - square random variable with degree of freedom @xmath37 , so @xmath268 .",
    "therefore , the numerator in the previous formula is @xmath269 .    because    @xmath270    the desired bound follows .",
    "the proof for the upper tail is similar ( in fact simpler as we do not need to treat the first two terms separately ) and we omit the details .",
    "our plan is to replace one by one the last @xmath271 gaussian rows of @xmath0 by vectors of components having zero mean , unit variance , and satisfying condition * c0*. our key tool here is the classical berry - eseen inequality . in order to apply this lemma , we will make a crucial use of lemma [ lemma : infinitynorm ] .",
    "* berry - esseen inequality)[lemma : b - e ] assume that @xmath272 is a unit vector . assume that @xmath273 are independent random variables of mean zero , variance one and satisfying condition * c0*. then we have    @xmath274 where @xmath181 is an absolute constant depending on the parameters appearing in .",
    "we remark that in the original setting of berry and esseen , it suffices to assume finite third moment .    in application",
    ", @xmath152 plays the role of the normal vector of the hyperplane spanned by the remaining @xmath275 rows of @xmath58 , and @xmath276 , where @xmath277 is the vector to be replaced .    for the deduction ,",
    "it is enough to show the following .",
    "[ lemma : replacement ] let @xmath0 be a random matrix with atom variables satisfying condition * c0 * and non - singular with probability one .",
    "assume furthermore that @xmath0 has at least one and at most @xmath70 gaussian rows .",
    "let @xmath278 be the random matrix obtained from @xmath0 by replacing a gaussian row vector @xmath94 of @xmath0 by a random vector @xmath279 whose coordinates are independent atom variables satisfying condition * c0 * such that the resulting matrix is non - singular with probability one .",
    "then    @xmath280 @xmath281    clearly , theorem [ theorem : main ] follows from theorem [ theorem : mainweak ] by applying lemma [ lemma : replacement ] @xmath282 times .",
    "( proof of lemma [ lemma : replacement ] ) without loss of generality , we can assume that @xmath278 is obtained from @xmath0 by replacing the last row @xmath283 .",
    "as @xmath0 is non - singular , @xmath284 .    by lemma [ lemma : infinitynorm ] , by paying an extra term of @xmath285 in probability ( which will be absorbed by the eventual bound @xmath286 )",
    ", we may also assume that the normal vector @xmath152 of @xmath287 satisfies    @xmath288    next , observe that    @xmath289    and    @xmath290    where @xmath291 and @xmath292 are the distance from @xmath283 and @xmath293 to @xmath287 respectively .    by lemma [ lemma",
    ": b - e ] , it is yielded that    @xmath294    hence    @xmath295 @xmath296    completing the proof of lemma [ lemma : replacement ] .",
    "in this section , we show that the two extra assumptions that @xmath297 and @xmath0 has full rank with probability one do not violate the generality of theorem [ theorem : main ] .          the above bound is extremally weak . by modifying the proof in @xcite ,",
    "one can actually prove tao - vu lower bound for random matrices satisfying * c0*. also , sharper bounds on the least singular value are obtained in @xcite . however , for the arguments in this section , we only need the bound on lemma [ lemma : lower ] .",
    "notice that by condition * c0 * , we have , with probability at least @xmath302 , that all entries of @xmath0 have absolute value at most @xmath67 , for some constant @xmath303 which may depend on the constants in * c0*.    we replace the variable @xmath5 by the variable @xmath304 , for all @xmath305 and let @xmath306 be the random matrix formed by @xmath307 .",
    "since with probability at least @xmath308 , @xmath309 , it is easy to show that if @xmath306 satisfies the claim of theorem [ theorem : main ] then so does @xmath0 .    while the entries of @xmath306 are bounded by @xmath310 , there is still one problem we need to address , namely that the new variables @xmath307 do not have mean 0 and variance one .",
    "we can achieve this by a simple normalization trick .",
    "first observe that by property * c0 * , taking @xmath69 sufficiently large , it is easy to show that @xmath311 has absolute value at most @xmath312 and @xmath313 , where @xmath314 is the standard deviation of @xmath315 . now define                                now we address the assumption that @xmath0 has full rank with probability one . notice that this is usually not true when the @xmath5 have discrete distribution ( such as bernoulli ) .",
    "however , we find the following simple trick that makes the assumption valid for our study .    instead of the entry @xmath5 , consider @xmath342 where @xmath343 is uniform on the interval @xmath344 $ ] and @xmath345 is very small , say @xmath346 .",
    "it is clear that the matrix @xmath306 formed by the @xmath307 has full rank with probability one . on the other hand ,",
    "it is easy to show that by brunn - minkowski inequality and hadamard s bound                    v.  l.  girko , _ the central limit theorem for random determinants _ ( russian ) ,",
    "translation in theory probab .",
    "appl .   24 ( 1979 ) , no . 4 , 729 - 740 . v.  l.  girko , _ a refinement of the central limit theorem for random determinants _ ( russian ) ,",
    "translation in theory probab .",
    "42 ( 1998 ) , no . 1 , 121 - 129"
  ],
  "abstract_text": [
    "<S> let @xmath0 be an @xmath1 by @xmath1 random matrix whose entries are independent real random variables satysfying some natural conditions . </S>",
    "<S> we show that the logarithm of @xmath2 satisfies a central limit theorem </S>",
    "<S> . more precisely ,    @xmath3 </S>"
  ]
}