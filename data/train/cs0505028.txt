{
  "article_text": [
    "hidden markov models ( hmms ) are widely used in bioinformatics @xcite , for example , in protein sequence alignment , protein family annotation @xcite and gene - finding @xcite .    when an hmm consisting of @xmath0 states is used to annotate an input sequence , its predictions crucially depend on its set of emission probabilities @xmath16 and transition probabilities",
    "this is for example the case for the state path with the highest overall probability , the so - called optimal state path or viterbi path @xcite , which is often reported as the predicted annotation of the input sequence .",
    "when a new hmm is designed , it is usually quite easy to define its states and the transitions between them as these typically closely reflect the underlying problem .",
    "however , it can be quite difficult to assign values to its emission probabilities @xmath16 and transition probabilities @xmath17 .",
    "ideally , they should be set up such that the model s predictions would perfectly reproduce the known annotation of a large and diverse set of input sequences .",
    "the question is thus how to derive the best set of transition and emission probabilities from a given training set of annotated sequences .",
    "two main scenarios have to be distinguished @xcite :    \\(1 ) if we know the optimal state paths that correspond to the known annotation of the training sequences , the transition and emission probabilities can simply be set to the respective count frequencies within these optimal state paths , i.e.  to their maximum likelihood estimators .",
    "if the training set is small or not diverse enough , pseudo - counts have to be added to avoid over - fitting .",
    "\\(2 ) if we do not know the optimal state paths of the training sequences , either because their annotation is unknown or because their annotation does not unambiguously define a state path in the hmm , we can employ an expectation maximisation ( em ) algorithm @xcite such as the baum - welch algorithm @xcite to derive the emission and transition probabilities in an iterative procedure which increases the overall log likelihood of the model in each iteration and which is guaranteed to converge at least to a local maximum .",
    "as in case ( 1 ) , pseudo - counts or dirichlet priors can be added to avoid over - fitting when the training set is small or not diverse enough .",
    "the baum - welch algorithm defines an iterative procedure in which the emission and transition probabilities in iteration @xmath18 are set to the number of times each transition and emission is _ expected _ to be used when analysing the training sequences with the set of emission and transition probabilities derived in the previous iteration @xmath19 .",
    "let @xmath20 denote the transition probability for going from state @xmath21 to state @xmath22 in iteration @xmath19 , @xmath23 the emission probability for emitting letter @xmath24 in state @xmath21 in iteration @xmath19 , @xmath25 the probability of sequence @xmath26 , and @xmath27 the @xmath28th letter in input sequence @xmath26 which has length @xmath3 .",
    "we also define @xmath29 as the sequence of letters from the beginning of sequence @xmath26 up to sequence position @xmath28 , @xmath30 .",
    "@xmath31 is defined as the sequence of letters from sequence position @xmath32 to the end of the sequence , @xmath33 .    for a given set of training sequences , @xmath34 , the expectation maximisation update for transition probability @xmath20 , @xmath35",
    ", can then be written as    @xmath36    the superfix @xmath19 on the quantities on the right hand side indicates that they are based on the transition probabilities @xmath20 and emission probabilities @xmath37 of iteration @xmath19 .",
    "@xmath38 is the so - called forward probability of the sequence up to and including sequence position @xmath28 , requiring that sequence letter @xmath27 is read by state @xmath21 .",
    "it is equal to the sum of probabilities of all state paths that finish in state @xmath21 at sequence position @xmath28 .",
    "the probability of sequence @xmath26 , @xmath25 , is therefore equal to @xmath39 .",
    "@xmath40 is the so - called backward probability of the sequence from sequence position @xmath32 to the end , given that the letter at sequence position @xmath28 , @xmath27 , is read by state @xmath21 .",
    "it is equal to the sum of probabilities of all state paths that start in state @xmath21 at sequence position @xmath28 .    for a given set of training sequences , @xmath34 , the expectation maximisation update for emission probability @xmath23 , @xmath41 ,",
    "is    @xmath42    @xmath43 is the usual delta function with @xmath44 if @xmath45 and @xmath46 if @xmath47 .",
    "as before , the superfix @xmath19 on the quantities on the right hand side indicates that they are calculated using the transition probabilities @xmath20 and emission probabilities @xmath37 of iteration @xmath19 .",
    "the forward and backward probabilities @xmath48 and @xmath49 can be calculated using the forward and backward algorithms @xcite which are introduced in the following section .",
    "the forward algorithm proposes a procedure for calculating the forward probabilities @xmath50 in an iterative way .",
    "@xmath50 is the sum of probabilities of all state paths that finish in state @xmath21 at sequence position @xmath28 .",
    "the recursion starts with the initialisation    @xmath51    where @xmath52 is the number of the start state in the hmm .",
    "the recursion proceeds towards higher sequence positions    @xmath53    and terminates with    @xmath54    where @xmath55 is the number of the end state in the hmm .",
    "the recursion can be implemented as a dynamic programming procedure which works its way through a two - dimensional matrix , starting at the start of the sequence in the @xmath52 state and finishing at the end of the sequence in the @xmath55 state of the hmm .",
    "the backward algorithm calculates the backward probabilities @xmath56 in a similar iterative way .",
    "@xmath56 is the sum of probabilities of all state paths that start in state @xmath21 at sequence position @xmath28 .",
    "opposed to the forward algorithm the backward algorithm starts at the end of the sequence in the @xmath55 state and finishes at the start of the sequence in the @xmath52 state of the hmm .",
    "the backward algorithm starts with the initialisation    @xmath57    and continues towards lower sequence positions with the recursion    @xmath58    and terminates with    @xmath59    as can be seen in the recursion steps of the forward and backward algorithms described above , the calculation of @xmath60 requires at most @xmath6 previously calculated elements @xmath61 for @xmath62 .",
    "@xmath6 is the maximum number of states that any state of the model is connected to .",
    "likewise , the calculation of @xmath56 refers to at most @xmath6 elements @xmath63 for @xmath62 .    in order to continue the calculation of the forward and backward values",
    "@xmath64 and @xmath65 for all states @xmath66 along the entire sequence , we thus only have to memorise @xmath0 elements .      unit now , the checkpointing algorithm @xcite was the most efficient way to perform baum - welch training .",
    "the basic idea of the checkpointing algorithm is to perform the forward and backward algorithm by memorising the forward and backward values only in @xmath67 columns along the sequence dimension of the dynamic programming table .",
    "the checkpointing algorithm starts with the forward algorithm , retaining only the forward values in @xmath67 columns .",
    "these columns partition the dynamic programming table into @xmath67 separate fields .",
    "the checkpointing algorithm then invokes the backward algorithm which memorises the backward values in a strip of length @xmath67 as it moves along the sequence .",
    "when the backward calculation reaches the boundary of one field , the pre - calculated forward values of the neighbouring checkpointing column are used to calculate the corresponding forward values for that field .",
    "the forward and backward values of that field are then available at the same time and are used to calculate the corresponding values for the em update .",
    "the checkpointing algorithm can be further refined by using embedded checkpoints . with an embedding level of @xmath28",
    ", the forward values in @xmath68 columns of the initial calculation are memorised , thus defining @xmath69 long fields .",
    "when the memory - sparse calculation of the backward values reaches the field in question , the forward algorithm is invoked again to calculate the forward values for @xmath68 additional columns within that field .",
    "this procedure is iterated @xmath28 times within the thus emerging fields . in the end , for each of the @xmath68-long k - sub - fields , the forward and backward values are simultaneously available and are used to calculate the corresponding values for the em update .",
    "the time complexity of this algorithm for one baum - welch iteration and a given training sequence of length @xmath3 is @xmath70 , since @xmath28 forward and @xmath71 backward algorithms have to be invoked , and the memory complexity is @xmath72 . for @xmath73 ,",
    "this amounts to a time requirement of @xmath74 and a memory requirement of @xmath7 , since @xmath75 .",
    "it is not trivial to see that the quantities @xmath35 and @xmath41 of equations  [ eq : t_update ] and [ eq : e_update ] can be calculated in an even more memory - sparse way as both , the forward and the corresponding backward probabilities are needed at the same time in order to calculate the terms @xmath76 in @xmath77 and @xmath78 in @xmath79 of equations  [ eq : t_update ] and [ eq : e_update ] .",
    "a calculation of these quantities for each sequence position using a memory - sparse implementation ( that would memorise only @xmath0 values at a time ) both for the forward and backward algorithm would require @xmath3-times more time , i.e.  significantly more time .",
    "also , an algorithm along the lines of the hirschberg algorithm @xcite can not be applied as we can not halve the dynamic programming table after the first recursion .",
    "we here propose a new algorithm to calculate the quantities @xmath35 and @xmath41 which are required for baum - welch training .",
    "our algorithm requires @xmath4 memory and @xmath5 time rather than @xmath7 memory and @xmath74 time .",
    "the trick for coming up with a memory efficient algorithm is to realise that    * @xmath77 and @xmath79 in equations  [ eq : t_update ] and [ eq : e_update ] can be interpreted as a weighted sum of probabilities of state paths that satisfy certain constraints and that * the weight of each state path is equal to the number of times that the constraint is fulfilled .    for example , @xmath77 in the numerator in equation  [ eq : t_update ] is the weighted sum of probabilities of state paths for sequence @xmath26 that contain at least one @xmath80 transition , and the weight of each such state path in the sum is the number of times this transition occurs in the state path .",
    "we now show how @xmath77 in equation  [ eq : t_update ] can be calculated in @xmath4 memory and @xmath81 time . as",
    "the superfix @xmath19 is only there to remind us that the calculation of @xmath77 is based on the transition and emission probabilities of iteration @xmath19 and as this index does not change in the calculation of @xmath82 , we discard it for simplicity sake in the following .",
    "let @xmath83 denote the weighted sum of probabilities of state paths that finish in state @xmath84 at sequence position @xmath28 of sequence @xmath26 and that contain at least one @xmath80 transition , where the weight for each state path is equal to its number of @xmath80 transitions .",
    "* theorem 1 : * the following algorithm calculates @xmath85 in @xmath4 memory and @xmath81 time .",
    "@xmath85 is the weighted sum of probabilities of all state paths for sequence @xmath26 that have at least one @xmath86 transition , where the weight for each state path is equal to its number of @xmath80 transitions .",
    "the algorithm starts with the initialisation    @xmath87    and proceeds via the following recursion @xmath88    and finishes with    @xmath89    * proof : *    \\(1 ) it is obvious that the recursion requires only @xmath4 memory as the calculation of all @xmath90 values with @xmath91 requires only access to the @xmath0 previous @xmath92 values with @xmath93 .",
    "likewise , the calculations of all @xmath94 values with @xmath95 refer only to @xmath0 elements @xmath96 with @xmath97 .",
    "we therefore have to remember only a thin `` slice '' of @xmath98 and @xmath99 values at sequence position @xmath28 in order to be able to calculate the @xmath98 and @xmath99 values for the next sequence position @xmath32 .",
    "the time requirement to calculate @xmath98 is @xmath81 : for every sequence position and for every state in the hmm , we have to sum at most @xmath6 terms in order to calculate the backward and forward terms .",
    "\\(2 ) the @xmath100 values are identical to the previously defined forward probabilities and are calculated in the same way as in the forward algorithm .",
    "\\(3 ) we now prove by induction that @xmath83 is equal to the weighted sum of probabilities of state paths that have at least one @xmath80 transition and that finish at sequence position @xmath28 in state @xmath84 , the weight of each state path being equal to its number of @xmath80 transitions .",
    "initialisation step ( sequence position @xmath101 ) : @xmath102 is true as the sum of probabilities of state paths that finish in state @xmath103 at sequence position @xmath104 and that have at least one @xmath80 transition is zero .",
    "induction step @xmath105 : we now show that if equation  [ eq : tij ] is true for sequence position @xmath28 , it is also true for @xmath32 .",
    "we have to distinguish two cases :    \\(i ) case @xmath106 : @xmath107    the first term , see right hand side of [ eq : first ] , is the sum of probabilities of state paths that finish at sequence position @xmath32 and whose last transition is from @xmath80 .",
    "the second term , see [ eq : second ] , is the sum of probabilities of state paths that finish at sequence position @xmath32 and that already have at least one @xmath80 transition .",
    "note that the term in [ eq : second ] also contains a contribution for @xmath108 .",
    "this ensures that the weight of those state path that already have at least one @xmath80 transition is correctly increased by 1 .",
    "the sum , @xmath94 , is therefore the weighted sum of probabilities of state paths that finish in sequence position @xmath32 and contain at least one @xmath80 transition .",
    "each state path s weight in the sum is equal to its number of @xmath80 transitions .",
    "\\(ii ) case @xmath109 :    @xmath110    the expression on the right hand side is the weighted sum of probabilities of state paths that finish in sequence position @xmath32 and contain at least one @xmath86 transition .    we have therefore shown that if equation  [ eq : tij ] is true for sequence position @xmath28 , it is also true for sequence position @xmath32 .",
    "this concludes the proof of theorem 1 .",
    "@xmath111    it is easy to show that @xmath112 in equation  [ eq : e_update ] can also be calculated in @xmath4 memory and @xmath81 time in a similar way as @xmath85 .",
    "let @xmath113 denote the weighted sum of probabilities of state paths that finish at sequence position @xmath28 in state @xmath84 and for which state @xmath21 reads letter @xmath24 at least once , the weight of each state path being equal to the number of times state @xmath21 reads letter @xmath24 . as in the calculation of @xmath85 , we again omit the superfix @xmath19 as the calculation of @xmath114 is again entirely based on the transition and emission probabilities of iteration @xmath19 .",
    "* theorem 2 : * @xmath112 can be calculated in @xmath4 memory and @xmath115 time using the following algorithm .",
    "@xmath112 is the weighted sum of probabilities of state paths for sequence @xmath26 that read letter @xmath24 in state @xmath21 at least once , the weight of each state path being equal to the number of times letter @xmath24 is read by state @xmath21 .    initialisation step :    @xmath116    recursion :    @xmath117    termination step :    @xmath118    * proof : * the proof is strictly analogous to the proof of theorem 1 .",
    "the above theorems have shown that @xmath85 and @xmath112 can each be calculated in @xmath4 memory and @xmath81 time .",
    "as there are @xmath1 transition parameters and @xmath2 emission parameters to be calculated in each baum - welch iteration , and as these @xmath119 values can be calculated independently , the time and memory requirements for each iteration and a set of training sequences whose sum of sequence lengths is @xmath3 using our new algorithm are    * @xmath4 memory and @xmath5 time , if all parameter estimates are calculated consecutively * @xmath120 memory and @xmath81 time , if all parameter estimates are calculated in parallel * more generally , @xmath121 memory and @xmath122 time for any @xmath123 , if @xmath124 of @xmath119 parameters are to be calculated in parallel    note that the calculation of @xmath25 is a by - product of each @xmath85 and each @xmath112 calculation , see equations  [ eq : px ] and [ eq : px2 ] , and that @xmath1 is equal to the number of free transition parameters in the hmm which is usually smaller than the number of transitions probabilities .",
    "likewise , @xmath2 is the number of free emission parameters in the hmm which may differ from the number of emission probabilities when the probabilities are parametrised .",
    "we propose the first linear - memory algorithm for baum - welch training . for a hidden markov model with @xmath0 states , @xmath1 free transition and",
    "@xmath2 free emission parameters , and an input sequence of length @xmath3 , our new algorithm requires @xmath4 memory and @xmath5 time for one baum - welch iteration as opposed to @xmath7 memory and @xmath125 time using the checkpointing algorithm @xcite , where @xmath6 is the maximum number of states that any state is connected to .",
    "our algorithm can be generalised to pair - hmms and , more generally , n - hmms that analyse n input sequences at a time in a straightforward way . in the n - hmm case ,",
    "our algorithm reduces the memory and time requirements from @xmath9 memory and @xmath126 time to @xmath10 memory and @xmath127 time .",
    "an added advantage of our new algorithm is that a reduced time requirement can be traded for an increased memory requirement and _ vice versa _ , such that for any @xmath13 , a time requirement of @xmath128 incurs a memory requirement of @xmath129 . for hmms ,",
    "our novel algorithm renders the memory requirement completely independent of the sequence length .",
    "generally , for n - hmms and all @xmath119 parameters being estimated consecutively , our novel algorithm reduces the memory requirement by a factor @xmath130 and the time requirement by a factor @xmath131 . for all hidden markov models",
    "whose number of states does not depend on the length of the input sequence , this thus amounts to a significantly reduced memory requirement and  in cases where the number of free parameters and states of the model ( i.e.  @xmath132 ) is smaller than the logarithm of sequence lengths  even to a reduced time requirement .",
    "for example , for an hmm that is used to predict human genes , the training sequences have a mean length of at least @xmath133  bp which is the average length of a human gene @xcite . using our new algorithm , the memory requirement for baum - welch training",
    "is reduced by a factor of about @xmath134 with respect to the most memory - sparse version of the checkpointing algorithm .",
    "our new algorithm makes use of the fact that the numerators and denominators of equations  [ eq : t_update ] and [ eq : e_update ] can be decomposed in a smart way that allows a very memory - sparse calculation .",
    "this calculation requires only one _ uni_-directional scan along the sequence rather than one or more _",
    "bi_-directional scans , see figure  1 .",
    "this property gives our algorithm the added advantage that it is easier to implement as it does not require programming techniques like recursive functions or checkpoints .",
    "baum - welch training is only guaranteed to converge to a _ local _ optimum .",
    "other optimisation techniques have been developed in order to find better optima .",
    "one of the most successful methods is simulated annealing ( sa ) @xcite .",
    "sa is essentially a markov chain monte carlo ( mcmc ) in which the target distribution is sequentially changed such that the distribution gets eventually trapped in a local optimum",
    ". one can give proposal steps a higher probability as they are approaching locally better points .",
    "this can increase the performance of the mcmc method , especially in higher dimensional spaces @xcite .",
    "one could base the candidate distribution on the expectations such that proposals are more likely to be made near the em updates ( calculated with our algorithm ) .",
    "there is no need to update all the parameters in one mcmc step , modifying a random subset of parameters yields also an irreducible chain .",
    "the last feature makes sa significantly faster than baum - welch updates as we need to calculate expectations only for a few parameters using sa . in that way",
    ", our algorithm could be used for highly efficient parameter training : using our algorithm to calculate the em updates in only linear space and using sa instead of the baum - welch algorithm for fast parameter space exploration .",
    "typical biological sequence analyses these days often involve complicated hidden markov models such as pair - hmms or long input sequences and we hope that our novel algorithm will make baum - welch parameter training an appealing and practicable option .",
    "other commonly employed methods in computer science and bioinformatics are stochastic context free grammars ( scfgs ) which need @xmath135 memory to analyse an input sequence of length @xmath3 with a grammar having @xmath0 non - terminal symbols @xcite . for a special type of scfgs , known as covariance models in bioinformatics ,",
    "@xmath0 is comparable to @xmath3 , hence the memory requirement is @xmath136 .",
    "this has recently been reduced to @xmath137 using a divide - and - conquer technique @xcite , which is the scfg analogue of the hirschberg algorithm for hmms @xcite . however , as the states of scfgs can generally impose long - range correlations between any pair of sequence positions , it seems that our algorithm can not be applied to scfgs in the general case .",
    "the algorithm is the result of a brainstorming session of the authors on the genome campus bus back to cambridge city centre on the evening of the 17th february 2005 .",
    "both authors contributed equally .",
    "the authors would like to thank one referee for the excellent comments .",
    "i.m .  is supported by a bksy gyrgy postdoctoral fellowship .",
    "both authors wish to thank nick goldman for inviting i.m .  to cambridge .",
    "10 [ 1][#1 ]    durbin r , eddy s , krogh a , mitchison g : * biological sequence analysis*. cambridge university press 1998 .",
    "krogh a , brown m , mian is , sjlander k , haussler d : * hidden markov models in biology : applications to protein modelling*. _ j mol biol _ 1994 , * 235*:15011531 .    eddy s : * hmmer : profile hidden markov models for biological sequence analysis ( http://hmmer.wustl.edu/ ) * 2001 .",
    "meyer i m , durbin r : * comparative ab initio prediction of gene structures using pair hmms*. _ bioinformatics _ 2002 , * 18*(10):13091318 .",
    "meyer i m , durbin r : * gene structure conservation aids similarity based gene prediction*. _ nucleic acids research _ 2004 , * 32*(2):776783 .",
    "viterbi a : * error bounds for convolutional codes and an assymptotically optimum decoding algorithm*. _ ieee trans infor theor _ 1967 , 260269 .",
    "dempster ap , laird nm , rubin db : * maximum likelihood from incomplete data via the em algorithm*. _ j roy stat soc b _ 1977 , * 39*:138 .",
    "baum le : * an equality and associated maximization technique in statistical estimation for probabilistic functions of markov processes*. _ inequalities _ 1972 , * 3*:18 .",
    "hirschberg ds : * a linear space algorithm for computing maximal common subsequences*. _ commun acm _ 1975 , * 18*:341343 .",
    "myers ew , miller w : * optimal alignments in linear space*. _ cabios _ 1988 , * 4*:1117 .",
    "grice ja , hughey r , speck d : * reduced space sequence alignment*. _ cabios _ 1997 , * 13*:4553 .",
    "tarnas c , hughey r : * reduced space hidden markov model training*. _ bioinformatics _ 1998 , * 14*(5):4001406 .",
    "wheeler , r. , hughey , r. : * optimizing reduced - space sequence analysis * , _ bioinformatics _ 2000 , * 16*(12):10821090 .    international human genome sequencing consortium : * initial sequencing and analysis of the human genome * , _ nature _ , 2001 , * 409*:860921 .    kirkpatrick s. , gelatt c.d .",
    "jr , vecchi m.p .",
    ": * optimization by simulated annealing * , _ science _ , 1983 , * 220*:671680 .",
    "roberts , g.o . , rosenthal , j.s . : * optimal scaling of discrete approximations to langevin diffusions * , _ j. r. statist",
    "b _ , 1998 , * 60*:255268 .",
    "eddy s : * a memory - efficient dynamic programming algorithm for optimal alignment of a sequence to an rna secondary structure*. _ bmc bioinformatics _ 2002 , * 3*:18 .",
    "this figure shows a pictorial description of the differences between the forward - backward algorithm ( a ) and our new algorithm ( b ) for the baum - welch training of a pair - hmm .",
    "each large rectangle corresponds to the projection of the three - dimensional dynamic programming matrix ( spanned by the two input sequences @xmath26 and @xmath138 and the states of the hmm ) onto the sequence plane .",
    "( a ) shows how the numerator in equation  [ eq : t_update ] is calculated at the pair of sequence positions indicated by the black square using the standard forward and backward algorithm .",
    "( b ) shows how our algorithm simultaneously calculates a strip of forward values @xmath139 and a strip of @xmath140 values at sequence position @xmath28 in sequence @xmath26 in order to estimate @xmath98 in equation  [ eq : t_update ] ."
  ],
  "abstract_text": [
    "<S> [ [ background ] ] background : + + + + + + + + + + +    baum - welch training is an expectation - maximisation algorithm for training the emission and transition probabilities of hidden markov models in a fully automated way . </S>",
    "<S> it can be employed as long as a training set of annotated sequences is known , and provides a rigorous way to derive parameter values which are guaranteed to be at least locally optimal . for complex hidden markov models such as pair hidden markov models and very long training sequences , even the most efficient algorithms for baum - welch training are currently too memory - consuming . </S>",
    "<S> this has so far effectively prevented the automatic parameter training of hidden markov models that are currently used for biological sequence analyses .    </S>",
    "<S> [ [ methods - and - results ] ] methods and results : + + + + + + + + + + + + + + + + + + + +    we introduce a linear space algorithm for baum - welch training . for a hidden markov model with @xmath0 states , @xmath1 free transition and @xmath2 free emission parameters , and an input sequence of length @xmath3 , our new algorithm requires @xmath4 memory and @xmath5 time for one baum - welch iteration , where @xmath6 is the maximum number of states that any state is connected to . the most memory efficient algorithm until now was the checkpointing algorithm with @xmath7 memory and @xmath8 time requirement . </S>",
    "<S> our novel algorithm thus renders the memory requirement completely independent of the length of the training sequences . more generally , for an n - hidden markov model and n input sequences of length @xmath3 </S>",
    "<S> , the memory requirement of @xmath9 is reduced to @xmath10 memory while the running time is changed from @xmath11 to @xmath12 .    </S>",
    "<S> an added advantage of our new algorithm is that a reduced time requirement can be traded for an increased memory requirement and _ vice versa _ , such that for any @xmath13 , a time requirement of @xmath14 incurs a memory requirement of @xmath15 .    </S>",
    "<S> [ [ conclusions ] ] conclusions : + + + + + + + + + + + +    for the large class of hidden markov models used for example in gene prediction , whose number of states does not scale with the length of the input sequence , our novel algorithm can thus be both faster and more memory - efficient than any of the existing algorithms .    </S>",
    "<S> [ 1995/12/01 ] </S>"
  ]
}