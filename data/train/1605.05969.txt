{
  "article_text": [
    "in this paper , we consider the following multi - block structured convex optimization model @xmath2 where the variables @xmath3 and @xmath4 are naturally partitioned into @xmath5 and @xmath6 blocks respectively , @xmath7 and @xmath8 are block matrices , @xmath9 s and @xmath10 s are some closed convex sets , @xmath11 and @xmath12 are smooth convex functions , and @xmath13 s and @xmath14 s are proper closed convex ( possibly nonsmooth ) functions .",
    "optimization problems in the form of have many emerging applications from various fields .",
    "for example , the constrained lasso ( classo ) problem that was first studied by james _",
    "@xcite as a generalization of the lasso problem , can be formulated as @xmath15 where @xmath16 , @xmath17 are the observed data , and @xmath18 , @xmath19 are the predefined data matrix and vector .",
    "many widely used statistical models can be viewed as special cases of , including the monotone curve estimation , fused lasso , generalized lasso , and so on @xcite . by partitioning the variable @xmath20 into blocks as @xmath21 where @xmath22 as well as other matrices and vectors in correspondingly , and introducing another slack variable @xmath23 , the classo problem can be transformed to @xmath24 which is in the form of .",
    "another interesting example is the extended linear - quadratic programming @xcite that can be formulated as @xmath25 where @xmath26 and @xmath27 are symmetric positive semidefinite matrices , and @xmath28 is a polyhedral set .",
    "apparently , includes quadratic programming as a special case . in general , its objective is a piece - wise linear - quadratic convex function . let @xmath29 , where @xmath30 denotes the indicator function of @xmath31",
    ". then @xmath32 where @xmath33 denotes the convex conjugate of @xmath12 . replacing @xmath34 by @xmath23 and introducing slack variable @xmath35",
    ", we can equivalently write into the form of : @xmath36 for which one can further partition the @xmath37-variable into a number of disjoint blocks .",
    "many other interesting applications in various areas can be formulated as optimization problems in the form of  , including those arising from signal processing , image processing , machine learning and statistical learning ; see @xcite and the references therein .    finally , we mention that computing a point on the central path for a generic convex programming in block variables @xmath38 : @xmath39 boils down to @xmath40 where @xmath41 and @xmath42 indicates the sum of the logarithm of all the components of @xmath43 .",
    "this model is again in the form of .",
    "our work relates to two recently very popular topics : the _ alternating direction method of multipliers _ ( admm ) for multi - block structured problems and the first - order primal - dual method for bilinear saddle - point problems .",
    "below we review the two methods and their convergence results .",
    "more complete discussion on their connections to our method will be provided after presenting our algorithm .",
    "one well - known approach for solving a linear constrained problem in the form of is the augmented lagrangian method , which iteratively updates the primal variable @xmath44 by minimizing the augmented lagrangian function in and then the multiplier @xmath45 through dual gradient ascent .",
    "however , the linear constraint couples @xmath46 and @xmath47 all together , it can be very expensive to minimize the augmented lagrangian function simultaneously with respect to all block variables . utilizing the multi - block structure of the problem",
    ", the multi - block admm updates the block variables sequentially , one at a time with the others fixed to their most recent values , followed by the update of multiplier .",
    "specifically , it performs the following updates iteratively ( by assuming the absence of the coupled functions @xmath11 and @xmath12 ) : @xmath48 where the augmented lagrangian function is defined as : @xmath49    when there are only two blocks , i.e. , @xmath50 , the update scheme in reduces to the classic 2-block admm @xcite .",
    "the convergence properties of the admm for solving 2-block separable convex problems have been studied extensively .",
    "since the 2-block admm can be viewed as a manifestation of some kind of operator splitting , its convergence follows from that of the so - called douglas - rachford operator splitting method ; see @xcite .",
    "moreover , the convergence rate of the 2-block admm has been established recently by many authors ; see e.g.  @xcite .",
    "although the multi - block admm scheme in performs very well for many instances encountered in practice ( e.g. @xcite ) , it may fail to converge for some instances if there are more than 2 block variables , i.e. , @xmath51 .",
    "in particular , an example was presented in @xcite to show that the admm may even diverge with 3 blocks of variables , when solving a linear system of equations .",
    "thus , some additional assumptions or modifications will have to be in place to ensure convergence of the multi - block admm .",
    "in fact , by incorporating some extra correction steps or changing the gauss - seidel updating rule , @xcite show that the convergence can still be achieved for the multi - block admm .",
    "moreover , if some part of the objective function is strongly convex or the objective has certain regularity property , then it can be shown that the convergence holds under various conditions ; see @xcite",
    ". using some other conditions including the error bound condition and taking small dual stepsizes , or by adding some perturbations to the original problem , authors of @xcite establish the rate of convergence results even without strong convexity .",
    "not only for the problem with linear constraint , in @xcite multi - block admm are extended to solve convex linear / quadratic conic programming problems . in a very recent work @xcite , sun , luo and ye propose a randomly permuted admm ( rp - admm ) that basically chooses a random permutation of the block indices and performs the admm update according to the order of indices in that permutation , and they show that the rp - admm converges in expectation for solving non - singular square linear system of equations .    in @xcite , the authors propose a block successive upper bound minimization method of multipliers ( bsumm ) to solve problem without @xmath23 variable .",
    "essentially , at every iteration , the bsumm replaces the nonseparable part @xmath52 by an upper - bound function and works on that modified function in an admm manner . under some error bound conditions and a diminishing dual stepsize assumption , the authors are able to show that the iterates produced by the bsumm algorithm converge to the set of primal - dual optimal solutions . along a similar direction ,",
    "cui et al .",
    "@xcite introduces a quadratic upper - bound function for the nonseparable function @xmath11 to solve 2-block problems ; they show that their algorithm has an @xmath0 convergence rate , where @xmath53 is the number of total iterations .",
    "very recently , @xcite has proposed a set of variants of the admm by adding some proximal terms into the algorithm ; the authors have managed to prove @xmath0 convergence rate for the 2-block case , and the same results applied for general multi - block case under some strong convexity assumptions .",
    "moreover , @xcite shows the convergence of the admm for 2-block problems by imposing quadratic structure on the coupled function @xmath52 and also the convergence of rp - admm for multi - block case where all separable functions vanish ( i.e. @xmath54 ) .",
    "recently , the work @xcite generalizes the first - order primal - dual method in @xcite to a randomized method for solving a class of saddle - point problems in the following form : @xmath55 where @xmath56 and @xmath57 .",
    "let @xmath58 and @xmath59 .",
    "then it is easy to see that is a saddle - point reformulation of the multi - block structured optimization problem @xmath60 which is a special case of without @xmath23 variable or the coupled function @xmath11 .    at each iteration ,",
    "the algorithm in @xcite chooses one block of @xmath37-variable uniformly at random and performs a proximal update to it , followed by another proximal update to the @xmath35-variable .",
    "more precisely , it iteratively performs the updates :    [ alg : r1st - pd ] @xmath61    where @xmath62 is a randomly selected block , and @xmath63 and @xmath64 are certain parameters .",
    "when there is only one block of @xmath37-variable , i.e. , @xmath65 , the scheme in becomes exactly the primal - dual method in @xcite .",
    "assuming the boundedness of the constraint sets @xmath66 and @xmath67 , @xcite shows that under weak convexity , @xmath0 convergence rate result of the scheme can be established by choosing appropriate parameters , and if @xmath13 s are all strongly convex , the scheme can be accelerated to have @xmath68 convergence rate by adapting the parameters .      *",
    "we propose a randomized primal - dual coordinate update algorithm to solve problems in the form of .",
    "the key feature is to introduce randomization as done in to the multi - block admm framework . unlike the random permutation scheme as previously investigated in @xcite ,",
    "we simply choose a subset of blocks of variables based on the uniform distribution .",
    "in addition , we perform a proximal update to that selected subset of variables . with appropriate proximal terms ( e.g. , the setting in ) , the selected block variables can be decoupled , and thus the updates can be done in parallel . *",
    "more general than , we can accommodate coupled terms in the objective function in our algorithm by linearizing such terms . by imposing lipschitz continuity condition on the partial gradient of the coupled functions @xmath11 and @xmath12 and using proximal terms ,",
    "we show that our method has an expected @xmath0 convergence rate for solving problem under mere convexity assumption .",
    "* we show that our algorithm includes several existing methods as special cases such as the scheme in and the proximal jacobian admm in @xcite .",
    "our result indicates that the @xmath0 convergence rate of the scheme in can be shown without assuming boundedness of the constraint sets .",
    "in addition , the same order of convergence rate of the proximal jacobian admm can be established in terms of a better measure . *",
    "furthermore , the linearization scheme allows us to deal with stochastic objective function , for instance , when the function @xmath11 is given in a form of expectation @xmath69 $ ] where @xmath70 is a random vector .",
    "as long as an unbiased estimator of the ( sub-)gradient of @xmath11 is available , we can extend our method to the stochastic problem and an expected @xmath1 convergence rate is achievable .    the rest of the paper is organized as follows . in section [ sec : prelim ] , we introduce our algorithm and present some preliminary results . in section",
    "[ sec : sublinear - rate ] , we present the sublinear convergence rate results of the proposed algorithm . depending on the multi - block structure of @xmath71 , different conditions and parameter settings are presented in subsections [ subsection : no - y ] , [ sbsec : mulxsiny ] and [ sbsec : mulxy ] , respectively . in section  [ sbsec : sto - conv ] , we present an extension of our algorithm where the objective function is assumed not to be even exactly computable , instead only some first - order stochastic approximation is available . the convergence analysis is extended to such settings accordingly .",
    "numerical results are shown in section  [ sec : numerical ] . in section  [ sec : connection ] , we discuss the connections of our algorithm to other well - known methods in the literature . finally , we conclude the paper in section  [ sec : conc - rem ] .",
    "the proofs for the technical lemmas are presented in appendix  [ sec : app - a ] , and the proofs for the main theorems are in appendix  [ sec : app - b ] .",
    "in this section , we first present some notations and then introduce our algorithm as well as some preliminary lemmas .",
    "we denote @xmath72 and @xmath73 . for any symmetric positive semidefinite matrix @xmath74",
    ", we define @xmath75 .",
    "given an integer @xmath76 , @xmath77 $ ] denotes the set @xmath78 .",
    "we use @xmath79 and @xmath80 as index sets , while @xmath79 is also used to denote the identity matrix ; we believe that the intention is evident in the context . given @xmath81 ,",
    "we denote :    * block - indexed variable : @xmath82 ; * block - indexed set : @xmath83 ; * block - indexed function : @xmath84 ; * block - indexed gradient : @xmath85 ; * block - indexed matrix : @xmath86 $ ] .",
    "our algorithm is rather general .",
    "its major ingredients are randomization in selecting block variables , linearization of the coupled functions @xmath11 and @xmath12 , and adding proximal terms .",
    "specifically , at each iteration @xmath87 , it first randomly samples a subset @xmath88 of blocks of @xmath20 , and then a subset @xmath89 of blocks of @xmath71 according to the uniform distribution over the indices .",
    "the randomized sampling rule is as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * randomization rule ( u ) : * for the given integers @xmath90 and @xmath91 , it randomly chooses index sets @xmath92 $ ] with @xmath93 and @xmath94 $ ] with @xmath95 _ uniformly _ ; i.e. , for any subsets @xmath96 $ ] and @xmath97 $ ] , the following holds @xmath98=1/\\left(\\begin{array}{c}n\\\\n\\end{array}\\right),\\cr { { \\mathrm{prob}}}\\big[j_k=\\{j_1,j_2,\\ldots , j_m\\}\\big]=1/\\left(\\begin{array}{c}m\\\\m\\end{array}\\right).\\nonumber\\end{aligned}\\ ] ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    after those subsets have been selected , it performs a prox - linear update to those selected blocks based on the augmented lagrangian function , followed by an update of the lagrangian multiplier .",
    "the details of the method are summarized in algorithm [ alg : rpdc ] below .    in algorithm",
    "[ alg : rpdc ] , @xmath99 and @xmath100 are predetermined positive semidefinite matrices with appropriate dimensions . for the selected blocks in @xmath88 and @xmath89 , instead of implementing the exact minimization of the augmented lagrangian function",
    ", we perform a block proximal gradient update . in particular , before minimization",
    ", we first linearize the coupled functions @xmath11 , @xmath12 , and add some proximal terms to it .",
    "note that one can always select all blocks , i.e. , @xmath101 $ ] and @xmath102 $ ] .",
    "empirically however , the block coordinate update method usually outperforms the full coordinate update method if the problem possesses certain structures ; see @xcite for an example .",
    "in addition , by choosing appropriate @xmath103 and @xmath104 , the problems and can be separable with respect to the selected blocks , and thus one can update the variables in parallel .",
    "let @xmath105 be the aggregated primal - dual variables and @xmath106 the primal - dual linear mapping ; namely @xmath107 and also let @xmath108 the point @xmath109 is a solution to _ if and only if _ there exists @xmath110 such that    [ sol - cond ] @xmath111    the following lemmas will be used in our subsequent analysis , whose proofs are elementary and thus are omitted here .    for any two vectors @xmath105 and @xmath112 , it holds @xmath113    for any two vectors @xmath114 and a positive semidefinite matrix @xmath115 : @xmath116    for any nonzero positive semidefinite matrix @xmath115 , it holds for any @xmath117 and @xmath118 of appropriate size that @xmath119 where @xmath120 denotes the matrix operator norm of @xmath74 .",
    "the following lemma presents a useful property of @xmath121 , which essentially follows from .",
    "[ property - on - h ] for any vectors @xmath122 , and sequence of positive numbers @xmath123 , it holds that @xmath124",
    "in this section , we establish sublinear convergence rate results of algorithm [ alg : rpdc ] for three different cases . we differentiate those cases based on whether or not @xmath71 in problem also has the multi - block structure . in the first case where @xmath71 is a multi - block variable",
    ", it requires @xmath125 where @xmath126 and @xmath127 are the cardinalities of the subsets of @xmath20 and @xmath71 selected in our algorithm respectively .",
    "since the analysis only requires weak convexity , we can ensure the condition to hold by adding _ zero _ component functions if necessary , in such a way that @xmath128 and then choosing @xmath129 .",
    "the second case is that @xmath71 is treated as a single - block variable , and this can be reflected in our algorithm by simply selecting all @xmath71-blocks every time , i.e.  @xmath130 .",
    "the third case assumes no @xmath23-variable at all .",
    "it falls into the first and second cases , and we discuss this case separately since it requires weaker conditions to guarantee the same convergence rate .    throughout our analysis , we choose the matrices @xmath103 and @xmath104 in algorithm [ alg : rpdc ] as follows : @xmath131 where @xmath132 and @xmath133 are given symmetric positive semidefinite and block diagonal matrices , and @xmath134 denotes the diagonal blocks of @xmath132 indexed by @xmath88 .",
    "note that such choice of @xmath103 and @xmath104 makes the selected block variables @xmath135 in and @xmath136 in decoupled , and thus both updates can be computed in parallel . in addition , we make the following assumptions :    [ assump1 ] for , @xmath9 s and @xmath10 s are some closed convex sets , @xmath11 and @xmath12 are smooth convex functions , and @xmath13 s and @xmath14 s are proper closed convex function .    [ assump2 ] there is at least one point @xmath137 satisfying the conditions in .    [ assump3 ] there exist constants @xmath138 and @xmath139 such that for any subset @xmath79 of @xmath140 $ ] with @xmath141 and any subset @xmath80 of @xmath142 $ ] with @xmath143 , it holds that    @xmath144    where @xmath145 keeps the blocks of @xmath146 that are indexed by @xmath79 and zero elsewhere .    before presenting the main convergence rate result ,",
    "we first establish a few key lemmas .",
    "[ lem:1step ] let @xmath147 be the sequence generated from algorithm [ alg : rpdc ] with matrices @xmath99 and @xmath100 defined as in .",
    "then the following inequalities hold @xmath148\\cr & & + { \\mathbb{e}}_{i_k}({x}^{k+1}-{x})^\\top(\\hat{{p}}-\\rho_x a^\\top a)({x}^{k+1}-{x}^k)-\\frac{l_f}{2}{\\mathbb{e}}_{i_k}\\|{x}^k-{x}^{k+1}\\|^2\\cr & \\le & \\left(1-\\frac{n}{n}\\right)\\big[f({x}^k)-f({x})+({x}^{k}-{x})^\\top(-{a}^\\top{\\lambda}^k)+ \\rho_x({x}^{k}-{x})^\\top{a}^\\top{r}^{k}\\big ] , \\label{ineq - k1-x}\\end{aligned}\\ ] ] and @xmath149\\cr & & + { \\mathbb{e}}_{j_k}({y}^{k+1}-{y})^\\top(\\hat{{q}}-\\rho_y b^\\top b)({y}^{k+1}-{y}^k)-\\frac{l_g}{2}{\\mathbb{e}}_{j_k}\\|{y}^k-{y}^{k+1}\\|^2\\cr & & - \\left(1-\\frac{m}{m}\\right)\\rho_y({y}^{k}-{y})^\\top{b}^\\top{a}({x}^{k+1}-{x}^k)\\cr & \\le & \\left(1-\\frac{m}{m}\\right)\\left[g({y}^k)-g({y})+({y}^{k}-{y})^\\top(-{b}^\\top{\\lambda}^k)+ \\rho_y({y}^{k}-{y})^\\top{b}^\\top{r}^{k}\\right ] , \\label{ineq - k1-y}\\end{aligned}\\ ] ] where @xmath150 denotes expectation over @xmath88 and conditional on all previous history",
    ".    note that for any feasible point @xmath44 ( namely , @xmath151 and @xmath152 ) , @xmath153 and @xmath154 then , using we have the following result .",
    "[ lem:1step - ba ] for any feasible point @xmath155 and integer @xmath53 , it holds @xmath156 and @xmath157    [ lem : xy - rate ] given a continuous function @xmath158 , for a random vector @xmath159 , if for any feasible point @xmath160 that may depend on @xmath161 , we have @xmath162\\le { \\mathbb{e}}[h(w)],\\ ] ] then for any @xmath163 and any optimal solution @xmath164 to we also have @xmath165\\le \\sup_{\\|{\\lambda}\\|\\le \\gamma}h(x^*,y^*,{\\lambda}).\\ ] ]    noting @xmath166 we can easily show the following lemma by the optimality of @xmath167 and the cauchy - schwarz inequality .    [",
    "lem : lb - obj ] assume @xmath167 satisfies . then for any point @xmath168 , we have @xmath169    the following lemma shows a connection between different convergence measures , and it can be simply proved by using . if both @xmath170 and @xmath161 are deterministic , it reduces to lemma 2.4 in @xcite .",
    "[ equiv - rate ] suppose that @xmath165 \\le \\epsilon.\\ ] ] then , we have @xmath171 \\leq \\epsilon,\\ ] ] where @xmath167 satisfies the optimality conditions in , and we assume @xmath172 .",
    "the convergence analysis for algorithm  [ alg : rpdc ] requires slightly different parameter settings under different structures .",
    "in fact , the underlying analysis and results also differ . to account for the differences , we present in the next three subsections the corresponding convergence results .",
    "the first one assumes there is no @xmath23 part at all ; the second case assumes a single block on the @xmath23 side ; the last one deals with the general case where the ratios @xmath173 is assumed to be equal to @xmath174 .",
    "we first consider a special case with no @xmath23-variable , namely , @xmath175 and @xmath176 in .",
    "this case has its own importance .",
    "it is a parallel block coordinate update version of the linearized augmented lagrangian method ( alm ) .",
    "[ thm : rate-3x ] assume @xmath177 and @xmath176 in .",
    "let @xmath178 be the sequence generated from algorithm [ alg : rpdc ] with @xmath179 .",
    "assume @xmath180 and @xmath181 let @xmath182 then , under assumptions  [ assump1 ] , [ assump2 ] and [ assump3 ] , we have @xmath183\\right|,\\ , {",
    "\\mathbb{e}}\\|a\\hat{x}^t - b\\|\\right\\}\\\\ & \\le & \\frac{1}{1+\\theta t}\\left[(1-\\theta)\\left(f({x}^0)-f({x}^*)+\\rho_x\\|r^0\\|^2\\right ) + \\frac{1}{2}\\|x^0-x^*\\|_{\\tilde{p}}^2+\\frac{\\max\\{(1+\\|\\lambda^*\\|)^2 , 4\\|\\lambda^*\\|^2\\}}{2\\rho_x}\\right]\\notag   \\ ] ] where @xmath184 , and @xmath185 is an arbitrary primal - dual solution .",
    "if there is no coupled function @xmath11 , indicates that we can even choose @xmath186 for all @xmath87 , i.e.  without proximal terms at all .",
    "some caution is required here : in that case when @xmath101,\\forall k$ ] , the algorithm is _ not _ the jacobian admm as discussed in @xcite since the block variables are still coupled in the augmented lagrangian function . to make it parallelizable , a proximal term is needed . then our result recovers the convergence of the proximal jacobian admm introduced in @xcite .",
    "in fact , the above theorem strengthens the convergence result in @xcite by establishing an @xmath0 rate of convergence in terms of the feasibility measure and the objective value .",
    "when the @xmath23-variable is simple to update , it could be beneficial to renew the whole of it at every iteration , such as the problem . in this subsection , we consider the case that there are multiple @xmath20-blocks but a single @xmath71-block ( or equivalently , @xmath130 ) , and we establish a sublinear convergence rate result with a different technique of dealing with the @xmath23-variable .",
    "[ thm : rate-1yw ] let @xmath178 be the sequence generated from algorithm [ alg : rpdc ] with @xmath130 and @xmath187 , where @xmath188 assume @xmath189 let @xmath190 where @xmath191 then , under assumptions  [ assump1 ] , [ assump2 ] and [ assump3 ] , we have @xmath192\\right|,\\,{\\mathbb{e}}\\|a\\hat{x}^t+b\\hat{y}^t - b\\|\\right\\}\\\\ & \\le & \\frac{1}{1+\\theta t}\\left[(1-\\theta)\\left(\\phi({x}^0,{y}^0)-\\phi({x}^*,{y}^*)+\\frac{\\rho_x}{2}\\|{r}^{0}\\|^2\\right)+\\frac{1}{2}\\|x^0-x^*\\|_{\\hat{p}}^2+\\frac{1}{2}\\|y^0-y^*\\|_{\\theta\\tilde{q}+\\rho_x b^\\top b}^2\\right.\\notag\\\\   & & \\hspace{1.5 cm } + \\left.\\frac{\\max\\{(1+\\|\\lambda^*\\|)^2 , 4\\|\\lambda^*\\|^2\\}}{2\\rho_x}\\right ] \\notag \\ ] ] where @xmath193 , and @xmath167 is an arbitrary primal - dual solution .",
    "it is easy to see that if @xmath194 , the result in theorem [ thm : rate-1yw ] becomes exactly the same as that in theorem [ thm : rate - cvx ] below . in general , they are different because the conditions in on @xmath195 and @xmath196 are different from those in .      in this subsection , we consider the most general case where both @xmath20 and @xmath71 have multi - block structure .",
    "assuming @xmath125 , we can still have the @xmath0 convergence rate .",
    "the assumption can be made without losing generality , e.g. , by adding zero components if necessary ( which is essentially equivalent to varying the probabilities of the variable selection ) .",
    "[ thm : rate - cvx ]",
    "let @xmath178 be the sequence generated from algorithm [ alg : rpdc ] with the parameters satisfying @xmath197 assume @xmath198 and @xmath199 satisfy one of the following conditions    [ matpqhat ] @xmath200    let @xmath201 then , under assumptions  [ assump1 ] , [ assump2 ] and [ assump3 ] , we have @xmath202\\right|,\\,{\\mathbb{e}}\\|a\\hat{x}^t+b\\hat{y}^t - b\\|\\right\\}\\\\ & \\le & \\frac{1}{1+\\theta t}\\left[(1-\\theta)\\left(\\phi({x}^0,{y}^0)-\\phi({x}^*,{y}^*)+\\rho_x\\|r^0\\|^2\\right ) +",
    "\\frac{1}{2}\\|x^0-x^*\\|_{\\tilde{p}}^2+\\frac{1}{2}\\|y^0-y^*\\|_{\\hat{q}}^2\\right.\\nonumber\\\\ & & \\hspace{1.5cm}\\left.+\\frac{\\max\\{(1+\\|\\lambda^*\\|)^2 , 4\\|\\lambda^*\\|^2\\}}{2\\rho_x}\\right ] \\nonumber\\end{aligned}\\ ] ] where @xmath203 , and @xmath167 is an arbitrary primal - dual solution .",
    "when @xmath50 , the two conditions in become the same . however , in general , neither of the two conditions in implies the other one . roughly speaking , for the case of @xmath204 and @xmath205 , the one in can be weaker , and for the case of @xmath206 and @xmath207 , the one in is more likely weaker .",
    "in addition , provides an explicit way to choose block diagonal @xmath195 and @xmath196 by simply setting @xmath208 and @xmath209 s to the lower bounds there .",
    "in this section , we extend our method to solve a stochastic optimization problem where the objective function involves an expectation . specifically , we assume the coupled function to be in the form of @xmath210 where @xmath70 is a random vector . for simplicity",
    "we assume @xmath175 , namely , we consider the following problem @xmath211 one can easily extend our analysis to the case where @xmath212 and @xmath12 is also stochastic .",
    "an example of is the penalized and constrained regression problem @xcite that includes as a special case .    due to the expectation form of @xmath11 ,",
    "it is natural that the exact gradient of @xmath11 is not available or very expensive to compute .",
    "instead , we assume that its stochastic gradient is readily accessible . by some slight abuse of the notation ,",
    "we denote @xmath213,\\quad h({w})=\\left[\\begin{array}{c}-{a}^\\top{\\lambda}\\\\ { a}{x}-{b}\\end{array}\\right].\\ ] ] a point @xmath214 is a solution to _ if and only if _ there exists @xmath110 such that    [ 1st - opt - s ] @xmath215    modifying algorithm [ alg : rpdc ] to , we present the stochastic primal - dual coordinate update method of multipliers , summarized in algorithm [ alg : srpdc ] , where @xmath216 is a stochastic approximation of @xmath217 .",
    "the strategy of block coordinate update with stochastic gradient information was first proposed in @xcite , which considered problems without linear constraint .",
    "we make the following assumption on the stochastic gradient @xmath218 .",
    "[ assump - error ] let @xmath219 .",
    "there exists a constant @xmath220 such that for all @xmath87 ,    [ ass - error1 ] @xmath221={0},\\label{ass - error11}\\\\ & { \\mathbb{e}}\\|{\\delta}^k\\|^2\\le\\sigma^2.\\label{ass - error12}\\end{aligned}\\ ] ]    following the proof of lemma [ lem:1step ] and also noting @xmath222={\\mathbb{e}}_{i_k}({x}^k-{x}^{k+1})^\\top{\\delta}^k,\\ ] ] we immediately have the following result .",
    "let @xmath223 be the sequence generated from algorithm [ alg : srpdc ] where @xmath99 is given in with @xmath224 .",
    "then @xmath225\\cr & & + { \\mathbb{e}}_{i_k}({x}^{k+1}-{x})^\\top\\left(\\hat{{p}}-\\rho a^\\top a+\\frac{{i}}{\\alpha_k}\\right)({x}^{k+1}-{x}^k ) -\\frac{l_f}{2}{\\mathbb{e}}_{i_k}\\|{x}^k-{x}^{k+1}\\|^2+{\\mathbb{e}}_{i_k}({x}^{k+1}-{x}^k)^\\top{\\delta}^k\\cr & \\le & \\left(1-\\frac{n}{n}\\right)\\big[f({x}^k)-f({x})+({x}^{k}-{x})^\\top(-{a}^\\top{\\lambda}^k)+ \\rho({x}^{k}-{x})^\\top{a}^\\top{r}^{k}\\big].\\end{aligned}\\ ] ]    the following theorem is a key result , from which we can choose appropriate @xmath226 to obtain the @xmath1 convergence rate .",
    "[ thm - s - vx ] let @xmath227 be the sequence generated from algorithm [ alg : srpdc ] .",
    "let @xmath228 and denote @xmath229 assume @xmath230 is nonincreasing , and    [ bd - x - s ] @xmath231    let @xmath232 then , under assumptions  [ assump1 ] , [ assump2 ] , [ assump3 ] and [ assump - error ] , we have @xmath233\\cr & \\le & ( 1-\\theta)\\alpha_0\\left[f({x}^0)-f({x}^*)\\right]+\\frac{\\alpha_0}{2}\\|{x}^{0}-{x}^*\\|_{\\hat{{p}}-\\rho a^\\top a}^2+\\frac{1}{2}\\|{x}^0-{x}^*\\|^2\\cr & & + \\left|\\frac{\\alpha_0\\beta_1}{2\\alpha_1}-\\frac{(1-\\theta)\\beta_1}{2}\\right|\\gamma^2+\\sum_{k=0}^t\\frac{\\alpha_k^2}{2}{\\mathbb{e}}\\|{\\delta}^k\\|^2.\\end{aligned}\\ ] ]    the following proposition gives sublinear convergence rate of algorithm [ alg : srpdc ] by specifying the values of its parameters .",
    "the choice of @xmath226 depends on whether we fix the total number of iterations .",
    "[ prop : rate - s ] let @xmath227 be the sequence generated from algorithm [ alg : srpdc ] with @xmath99 given in , @xmath132 satisfying , and the initial point satisfying @xmath234 and @xmath235 .",
    "let @xmath236 be @xmath237+\\frac{1}{2}\\|x^0-x^*\\|_{d_x}^2+\\frac{\\alpha_0}{2\\rho}\\max\\{(1+\\|\\lambda^*\\|)^2 , 4\\|\\lambda^*\\|^2\\},\\end{aligned}\\ ] ] where @xmath185 is a primal - dual solution , and @xmath238 .    1 .",
    "if @xmath239 for a certain @xmath240 , then for @xmath241 , @xmath242\\right|,\\ , { \\mathbb{e}}\\|a\\hat{x}^t - b\\|\\right\\}\\le \\frac{c_0}{\\theta\\alpha_0\\sqrt{t}}+\\frac{\\alpha_0(\\log t+2)\\sigma^2}{2\\theta\\sqrt{t}}.\\ ] ] 2 .   if the number of maximum number of iteration is fixed a priori , then by choosing @xmath243 with any given @xmath240 , we have @xmath244\\right|,\\ , { \\mathbb{e}}\\|a\\hat{x}^t - b\\|\\right\\}\\le\\frac{c_0}{\\theta\\alpha_0\\sqrt{t}}+\\frac{\\alpha_0\\sigma^2}{\\theta\\sqrt{t}}.\\ ] ]    when @xmath245 , we can show that and hold for @xmath241 ; see appendix [ app : bd - x - s ] .",
    "hence , the result in follows from , the convexity of @xmath246 , lemma [ equiv - rate ] with @xmath247 , and the inequalities @xmath248 when @xmath226 is a constant , the terms on the left hand side of and on the right hand side of are both zero , so they are satisfied . hence , the result in immediately follows by noting @xmath249 and @xmath250 .",
    "the sublinear convergence result of algorithm [ alg : srpdc ] can also be shown if @xmath11 is nondifferentiable convex and lipschitz continuous . indeed , if @xmath11 is lipschtiz continuous with constant @xmath251 , i.e. , @xmath252 then @xmath253 , where @xmath254 is a subgradient of @xmath11 at @xmath20 .",
    "hence , @xmath255+{\\mathbb{e}}_{i_k}({x}^k-{x}^{k+1})^\\top\\big(\\tilde{\\nabla}f({x}^k)-\\tilde{\\nabla } f({x}^{k+1})\\big)\\cr & = & \\frac{n - n}{n}(f({x})-f({x}^k))+{\\mathbb{e}}_{i_k}[f({x})-f({x}^{k+1})]+{\\mathbb{e}}_{i_k}({x}^k-{x}^{k+1})^\\top\\big(\\tilde{\\nabla}f({x}^k)-\\tilde{\\nabla } f({x}^{k+1})\\big).\\end{aligned}\\ ] ] now following the proof of lemma [ lem:1step ] , we can have a result similar to , and then through the same arguments as those in the proof of theorem [ thm - s - vx ] , we can establish sublinear convergence rate of @xmath1 .",
    "in this section , we test the proposed randomized primal - dual method on solving the nonnegativity constrained quadratic programming ( ncqp ) : @xmath256 where @xmath257 , and @xmath258 is a symmetric positive semidefinite ( psd ) matrix . there is no @xmath23-variable , and it falls into the case in theorem [ thm : rate-3x ] .",
    "we perform two experiments on a macbook pro with 4 cores .",
    "the first experiment demonstrates the parallelization performance of the proposed method , and the second one compares it to other methods .",
    "* parallelization .",
    "* this test is to illustrate the power unleashed in our new method , which is flexible in terms of parallel and distributive computing .",
    "we set @xmath259 and generate @xmath260 , where the components of @xmath261 follow the standard gaussian distribution .",
    "the matrix @xmath262 and vectors @xmath263 are also randomly generated .",
    "we treat every component of @xmath37 as one block , and at every iteration we select and update @xmath264 blocks , where @xmath264 is the number of used cores .",
    "figure [ parallel - comp ] shows the running time by using 1 , 2 , and 4 cores , where the optimal value @xmath265 is obtained by calling matlab function ` quadprog ` with tolerance @xmath266 . from the figure",
    ", we see that our proposed method achieves nearly linear speed - up .",
    "* comparison to other methods . * in this experiment , we compare the proposed method to the linearized alm and the cyclic linearized admm methods .",
    "we set @xmath267 and generate @xmath260 , where the components of @xmath268 follow standard gaussian distribution .",
    "note that @xmath27 is singular , and thus is not strongly convex .",
    "we partition the variable into 100 blocks , each with 50 components . at each iteration of our method ,",
    "we randomly select one block variable to update .",
    "figure [ random - qp ] shows the performance by the three compared methods , where one epoch is equivalent to updating 100 blocks once . from the figure",
    ", we see that our proposed method is comparable to the cyclic linearized admm and significantly better than the linearized alm .",
    "although the cyclic admm performs well on this example , in general it can diverge if the problem has more than two blocks ; see @xcite .",
    "in this section , we discuss how algorithms [ alg : rpdc ] and [ alg : srpdc ] are related to several existing methods in the literature , and we also compare their convergence results .",
    "it turns out that the proposed algorithms specialize to several known methods or their variants in the literature under various specific conditions .",
    "therefore , our convergence analysis recovers some existing results as special cases , as well as provides new convergence results for certain existing algorithms such as the jacobian proximal parallel admm and the primal - dual scheme in .",
    "the randomized proximal coordinate descent ( rpcd ) was proposed in @xcite , where smooth convex optimization problems are considered .",
    "it was then extended in @xcite to deal with nonsmooth problems that can be formulated as @xmath269 where @xmath56 . toward solving , at each iteration @xmath87 ,",
    "the rpcd method first randomly selects one block @xmath62 and then performs the update : @xmath270 where @xmath271 is the lipschitz continuity constant of the partial gradient @xmath272 . with more than one",
    "blocks selected every time , has been further extended into parallel coordinate descent in @xcite .",
    "when there is no linear constraint and no @xmath23-variable in , then algorithm [ alg : rpdc ] reduces to the scheme in if @xmath273 , i.e. , only one block is chosen , and @xmath274 , and to the parallel coordinate descent in @xcite if @xmath275 and @xmath276 .",
    "although the convergence rate results in @xcite are non - ergodic , we can easily strengthen our result to a non - ergodic one by noticing that implies nonincreasing monotonicity of the objective if algorithm [ alg : rpdc ] is applied to .      for solving the problem with a stochastic @xmath11 , @xcite proposes a stochastic block proximal gradient ( sbpg ) method , which iteratively performs the update in with @xmath277 replaced by a stochastic approximation . if @xmath11 is lipschitz differentiable , then an ergodic @xmath1 convergence rate was shown . setting @xmath278 , we reduce algorithm [ alg : srpdc ] to the sbpg method , and thus our convergence results in proposition [ prop : rate - s ] recover that in @xcite .      without coupled functions or proximal terms , algorithm [ alg : rpdc ]",
    "can be regarded as a randomized variant of the multi - block admm scheme in .",
    "while multi - block admm can diverge if the problem has three or more blocks , our result in theorem [ thm : rate-3x ] shows that @xmath0 convergence rate is guaranteed if at each iteration , one randomly selected block is updated , followed by an update to the multiplier . note that in the case of no coupled function and @xmath279 , indicates that we can choose @xmath186 , i.e.  without proximal term .",
    "hence , randomization is a key to convergence .",
    "when there are only two blocks , admm has been shown ( e.g. , @xcite ) to have an ergodic @xmath0 convergence rate .",
    "if there are no coupled functions , and both indicate that we can choose @xmath280 if @xmath194 , i.e. , all @xmath37 and @xmath23 blocks are selected . thus according to",
    ", we can set @xmath281 , in which case algorithm [ alg : rpdc ] reduces to the classic 2-block admm .",
    "hence , our results in theorems [ thm : rate-1yw ] and [ thm : rate - cvx ] both recover the ergodic @xmath0 convergence rate of admm for two - block convex optimization problems .      in @xcite , the proximal jacobian parallel admm ( prox - jadmm ) was proposed to solve the linearly constrained multi - block separable convex optimization model @xmath282 at each iteration , the prox - jadmm method performs the updates for @xmath283 in parallel : @xmath284 and then updates the multiplier by @xmath285 where @xmath286 and @xmath163 is a damping parameter . by choosing approapriate parameters ,",
    "@xcite established convergence rate of order @xmath287 based on norm square of the difference of two consecutive iterates .",
    "if there is no @xmath23-variable or the coupled function @xmath11 in , setting @xmath101 , p^k=\\mathrm{blkdiag}(\\rho_x a_1^\\top a_1+p_1,\\cdots,\\rho_x a_n^\\top a_n+p_n)-\\rho_x a^\\top a\\succeq 0,\\,\\forall k$ ] , where @xmath288 s are the same as those in , then algorithm  [ alg : rpdc ] reduces to the prox - jadmm with @xmath289 , and theorem  [ thm : rate-3x ] provides a new convergence result in terms of the objective value and the feasibility measure .      in this subsection",
    ", we show that the scheme in is a special case of algorithm [ alg : rpdc ] .",
    "let @xmath12 be the convex conjugate of @xmath290 , namely , @xmath291 .",
    "then is equivalent to the optimization problem : @xmath292 which can be further written as @xmath293    [ prop - equiv - pd ] the scheme in is equivalent to the following updates :    [ equiv - alg ] @xmath294    where @xmath295 .",
    "therefore , it is a special case of algorithm [ alg : rpdc ] applied to with the setting of @xmath296 and @xmath297 .",
    "while the sublinear convergence rate result in @xcite requires the boundedness of @xmath66 and @xmath67 , the result in theorem [ thm : rate-1yw ] indicates that the boundedness assumption can be removed if we add one proximal term to the @xmath23-update in .",
    "we have proposed a randomized primal - dual coordinate update algorithm , called rpdbu , for solving linearly constrained convex optimization with multi - block decision variables and coupled terms in the objective . by using a randomization scheme and the proximal gradient mappings ,",
    "we show a sublinear convergence rate of the rpdbu method . in particular , without any assumptions other than convexity on the objective function and without imposing any restrictions on the constraint matrices , an @xmath0 convergence rate is established .",
    "we have also extended rpdbu to solve the problem where the objective is stochastic .",
    "if a stochastic ( sub-)gradient estimator is available , then we show that by adaptively choosing the parameter @xmath226 in the added proximal term , an @xmath1 convergence rate can be established . furthermore ,",
    "if there is no coupled function @xmath11 , then we can remove the proximal term , and the algorithm reduces to a randomized multi - block admm .",
    "hence , the convergence of the original randomized multi - block admm follows as a consequence of our analysis .",
    "remark also that by taking the sampling set @xmath88 as the whole set and @xmath103 as some special matrices , our algorithm specializes to the proximal jacobian admm .",
    "finally , we pose as an open problem to decide whether or not a deterministic counterpart of the rpdbu exists , retaining similar convergence properties for solving problem .",
    "for instance , it would be interesting to know if the algorithm would still be convergent if a deterministic cyclic update rule is applied while a proper proximal term is incorporated .",
    "10    d.  boley",
    ". local linear convergence of the alternating direction method of multipliers on quadratic or linear programs .",
    ", 23(4):21832207 , 2013 .",
    "x.  cai , d.  han , and x.  yuan .",
    "the direct extension of admm for three - block separable convex minimization models is convergent when one function is strongly convex . , 2014 .",
    "a.  chambolle and t.  pock . a first - order primal - dual algorithm for convex problems with applications to imaging .",
    ", 40(1):120145 , 2011 .    c.  chen ,",
    "b.  he , y.  ye , and x.  yuan . the direct extension of admm for multi - block convex minimization problems is not necessarily convergent . , 155(1 - 2):5779 , 2016 .",
    "c.  chen , m.  li , x.  liu , and y.  ye . on the convergence of multi - block alternating direction method of multipliers and block",
    "coordinate descent method . , 2015 .",
    "c.  chen , y.  shen , and y.  you . on the convergence analysis of the alternating direction method of multipliers with three blocks . in _ abstract and applied analysis _ , volume 2013 .",
    "hindawi publishing corporation , 2013 .",
    "l.  chen , d.  sun , and k .- c .",
    "toh . an efficient inexact symmetric gauss - seidel based majorized admm for high - dimensional convex composite conic programming . , 2015 .",
    "y.  cui , x.  li , d.  sun , and k .- c .",
    "toh . on the convergence properties of a majorized admm for linearly constrained convex optimization problems with coupled objective functions . , 2015 .",
    "c.  dang and g.  lan .",
    "randomized first - order methods for saddle point optimization . , 2014 .",
    "c.  d. dang and g.  lan .",
    "stochastic block mirror descent methods for nonsmooth and stochastic optimization . , 25(2):856881 , 2015 .    w.  deng , m .- j .",
    "lai , z.  peng , and w.  yin .",
    "parallel multi - block admm with @xmath298 convergence . ,",
    "pages 125 , 2016 .",
    "w.  deng and w.  yin .",
    "on the global and linear convergence of the generalized alternating direction method of multipliers .",
    ", 66(3):889916 , 2015 .",
    "j.  eckstein and d.  bertsekas . on the douglas - rachford splitting method and the proximal point algorithm for maximal monotone operators . , 55(1 - 3):293318 , 1992 .",
    "d.  gabay and b.  mercier .",
    "a dual algorithm for the solution of nonlinear variational problems via finite element approximation .",
    ", 2(1):1740 , 1976 .",
    "x.  gao , b.  jiang , and s.  zhang .",
    "on the information - adaptive variants of the admm : an iteration complexity perspective . , 2014 .",
    "x.  gao and s.  zhang .",
    "first - order algorithms for convex optimization with nonseparate objective and coupled constraints . , 2015 .",
    "r.  glowinski and p.  le  tallec . , volume  9 .",
    "siam , 1989 .",
    "r.  glowinski and a.  marrocco .",
    "sur lapproximation , par elments finis dordre un , et la rsolution , par pnalisation - dualit dune classe de problmes de dirichlet non linaires .",
    ", 9(r2):4176 , 1975 .",
    "d.  han and x.  yuan . a note on the alternating direction method of multipliers .",
    ", 155(1):227238 , 2012 .",
    "b.  he , l.  hou , and x.  yuan . on full jacobian decomposition of the augmented lagrangian method for separable convex programming . , 25(4):22742312 , 2015 .",
    "b.  he , m.  tao , and x.  yuan . alternating direction method with gaussian back substitution for separable convex programming . , 22(2):313340 , 2012 .",
    "b.  he , m.  tao , and x.  yuan .",
    "convergence rate and iteration complexity on the alternating direction method of multipliers with a substitution procedure for separable convex programming . , 2012 .",
    "b.  he and x.  yuan . on the @xmath299 convergence rate of the douglas - rachford alternating direction method . , 50(2):700709 , 2012 .",
    "m.  hong , t .- h .",
    "chang , x.  wang , m.  razaviyayn , s.  ma , and z .- q .",
    "a block successive upper bound minimization method of multipliers for linearly constrained convex optimization . , 2014",
    ".    m.  hong and z.  luo .",
    "on the linear convergence of the alternating direction method of multipliers . , 2012 .",
    "g.  m. james , c.  paulson , and p.  rusmevichientong .",
    "the constrained lasso .",
    "technical report , university of southern california , 2013 .",
    "g.  m. james , c.  paulson , and p.  rusmevichientong . penalized and constrained regression . technical report , 2013 .",
    "m.  li , d.  sun , and k .- c",
    ". toh . a convergent 3-block semi - proximal admm for convex minimization problems with one strongly convex block .",
    ", 32(04):1550024 , 2015 .    x.  li , d.  sun , and k .- c . toh . a schur complement based semi - proximal admm for convex quadratic conic programming and extensions .",
    ", 155(1 - 2):333373 , 2016 .",
    "t.  lin , s.  ma , and s.  zhang .",
    "on the global linear convergence of the admm with multiblock variables . , 25(3):14781497 , 2015 .",
    "t.  lin , s.  ma , and s.  zhang .",
    "on the sublinear convergence rate of multi - block admm .",
    ", 3(3):251274 , 2015 .",
    "t.  lin , s.  ma , and s.  zhang .",
    "iteration complexity analysis of multi - block admm for a family of convex minimization without strong convexity . ,",
    "pages 130 , 2016 .",
    "z.  lu and l.  xiao . on the complexity analysis of randomized",
    "block - coordinate descent methods .",
    ", 152(1 - 2):615642 , 2015 .",
    "r.  d. monteiro and b.  f. svaiter .",
    "iteration - complexity of block - decomposition algorithms and the alternating direction method of multipliers .",
    ", 23(1):475507 , 2013 .",
    "y.  nesterov .",
    "efficiency of coordinate descent methods on huge - scale optimization problems .",
    ", 22(2):341362 , 2012 .",
    "y.  peng , a.  ganesh , j.  wright , w.  xu , and y.  ma .",
    "rasl : robust alignment by sparse and low - rank decomposition for linearly correlated images . , 34(11):22332246 , 2012 .",
    "z.  peng , t.  wu , y.  xu , m.  yan , and w.  yin .",
    "coordinate friendly structures , algorithms and applications .",
    ", 1(1):57119 , 2016 .",
    "p.  richtrik and m.  tak .",
    "iteration complexity of randomized block - coordinate descent methods for minimizing a composite function .",
    ", 144(1 - 2):138 , 2014 .    p.",
    "richtrik and m.  tak .",
    "parallel coordinate descent methods for big data optimization .",
    ", pages 152 , 2015 .",
    "r.  t. rockafellar .",
    "large - scale extended linear - quadratic programming and multistage optimization . in _ advances in numerical partial differential equations and optimization : proceedings of the fifth mexico - united states workshop _ , volume  2 , pages 247261 , 1991 .",
    "d.  sun , k .- c .",
    "toh , and l.  yang .",
    "a convergent 3-block semiproximal alternating direction method of multipliers for conic programming with 4-type constraints . , 25(2):882915 , 2015 .",
    "r.  sun , z .- q .",
    "luo , and y.  ye . on the expected convergence of randomly permuted admm . ,",
    "m.  tao and x.  yuan . recovering low - rank and sparse components of matrices from incomplete and noisy observations .",
    ", 21(1):5781 , 2011 .",
    "y.  wang , w.  yin , and j.  zeng . global convergence of admm in nonconvex nonsmooth optimization . , 2015 .",
    "hybrid jacobian and gauss - seidel proximal block coordinate update methods for linearly constrained convex programming .",
    ", 2016 .",
    "y.  xu and w.  yin . block stochastic gradient iteration for convex and nonconvex optimization . ,",
    "25(3):16861716 , 2015 .",
    "we give proofs of several lemmas that are used to show our main results . throughout our proofs , we define @xmath300 and @xmath301 as follows : @xmath302      we prove , and can be shown by the same arguments . by the optimality of @xmath303 , we have for any @xmath304 , @xmath305 where @xmath306 is a subgradient of @xmath307 at @xmath303 , and we have used the formula of @xmath308 given in .",
    "we compute the expectation of each term in in the following .",
    "first , we have @xmath309\\cr & = & \\frac{n - n}{n}\\left(f({x})-f({x}^k)\\right)+{\\mathbb{e}}_{i_k}\\left[f({x})-f({x}^{k+1})+\\frac{l_f}{2}\\|{x}^k-{x}^{k+1}\\|^2\\right],\\label{term1}\\end{aligned}\\ ] ] where the last inequality is from the convexity of @xmath11 and the lipschitz continuity of @xmath310 .",
    "secondly , @xmath311 for the third term of , we have @xmath312\\cr & = & \\frac{n}{n}\\ , u({x})-{\\mathbb{e}}_{i_k}[u({x}^{k+1})-u({x}^k)+u_{i_k}({x}_{i_k}^k)]\\cr & = & \\frac{n}{n}\\left[u({x})-u({x}^k)\\right]+{\\mathbb{e}}_{i_k}[u({x}^k)-u({x}^{k+1})]\\cr & = & \\frac{n - n}{n}\\left[u({x})-u({x}^k)\\right]+{\\mathbb{e}}_{i_k}[u({x})-u({x}^{k+1})],\\end{aligned}\\ ] ] where the inequality is from the convexity of @xmath307 .",
    "the expectation of the fourth term of is @xmath313 finally , we have @xmath314 where we used the formulas of @xmath315 in and .    plugging through into and",
    "recalling @xmath316 , by rearranging terms we have @xmath317\\cr & & + { \\mathbb{e}}_{i_k}\\left({x}^{k+1}-{x}\\right)^\\top\\tilde{{p}}({x}^{k+1}-{x}^k)-\\frac{l_f}{2}{\\mathbb{e}}_{i_k}\\|{x}^k-{x}^{k+1}\\|^2\\cr & \\le & \\frac{n - n}{n}\\left[f({x}^k)-f({x})+({x}^{k}-{x})^\\top(-{a}^\\top{\\lambda}^k)+ \\rho_x({x}^{k}-{x})^\\top{a}^\\top{r}^{k}\\right].\\end{aligned}\\ ] ] note @xmath318 hence , we can rewrite equivalently into . through the same arguments ,",
    "one can show , thus completing the proof .      letting @xmath319 in",
    ", we have for any @xmath45 that @xmath320\\nonumber\\\\ & \\ge & { \\mathbb{e}}\\left[\\phi(\\hat{{x}},\\hat{{y}})-\\phi({x}^*,{y}^*)+(\\hat{x}-x^*)^{\\top}(-a^{\\top}\\hat{\\lambda } ) + ( \\hat{y}-y^*)^{\\top}(-b^{\\top}\\hat{\\lambda})+(\\hat{\\lambda}-\\lambda)^{\\top}(a\\hat{x}+b\\hat{y}-b)\\right]\\nonumber \\\\ & = & { \\mathbb{e}}\\left[\\phi(\\hat{{x}},\\hat{{y}})-\\phi({x}^*,{y}^*)+\\langle\\hat{\\lambda},ax^*+by^*-b\\rangle-\\langle\\lambda , a\\hat{x}+b\\hat{y}-b\\rangle\\right]\\nonumber\\\\ & = & { \\mathbb{e}}\\left[\\phi(\\hat{{x}},\\hat{{y}})-\\phi({x}^*,{y}^*)-\\langle\\lambda , a\\hat{x}+b\\hat{y}-b\\rangle\\right ] , \\label{eq : xy - phi - star}\\end{aligned}\\ ] ] where the last equality follows from the feasibility of @xmath164 . for any @xmath163 , restricting @xmath45 in @xmath321 , we have @xmath322\\le \\sup\\limits_{\\lambda\\in{{\\mathcal{b}}}_\\gamma}h(x^*,y^*,{\\lambda}).\\ ] ] hence , letting @xmath323 in gives the desired result .",
    "we have @xmath324 and @xmath325\\cr & = : & \\frac{\\alpha_0}{\\rho}[\\psi(k)-\\psi(k+1)].\\end{aligned}\\ ] ] by elementary calculus , we have @xmath326\\\\ & = & \\frac{1}{2(k-1)(\\sqrt{k}-(1-\\theta)\\sqrt{k-1})}\\left((1-\\theta)-\\frac{\\sqrt{k}}{\\sqrt{k-1}}\\right)<0.\\end{aligned}\\ ] ] hence , @xmath327 is decreasing with respect to @xmath87 , and thus holds .",
    "when @xmath245 , becomes @xmath328 which is equivalent to @xmath329 this completes the proof .",
    "in this section , we give the technical details for showing all theorems .      taking expectation over both sides of and summing it over @xmath330 through @xmath53",
    ", we have @xmath331+(1-\\theta)\\rho_x{\\mathbb{e}}({x}^{t+1}-{x})^\\top{a}^\\top{r}^{t+1}\\cr & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\big[f({x}^{k+1})-f({x})+({x}^{k+1}-{x})^\\top(-{a}^\\top{\\lambda}^{k+1})\\big ] -\\sum_{k=0}^t\\rho_x{\\mathbb{e}}({x}^{k+1}-{x})^\\top{a}^\\top{b}({y}^{k+1}-{y}^k)\\cr & & + \\sum_{k=0}^t{\\mathbb{e}}({x}^{k+1}-{x})^\\top\\tilde{{p}}({x}^{k+1}-{x}^k)-\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2\\cr & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right],\\end{aligned}\\ ] ] where we have used @xmath332 , the condition in and the definition of @xmath300 in . similarly , taking expectation over both sides of , summing it over @xmath330 through @xmath53 , we have @xmath333 + ( 1-\\theta)\\rho_y{\\mathbb{e}}({y}^{t+1}-{y})^\\top{b}^\\top{r}^{t+1}\\cr & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\big[g({y}^{k+1})-g({y})+({y}^{k+1}-{y})^\\top(-{b}^\\top{\\lambda}^{k+1})\\big]\\cr & & + \\sum_{k=0}^t{\\mathbb{e}}({y}^{k+1}-{y})^\\top\\tilde{{q}}({y}^{k+1}-{y}^k)-\\frac{l_g}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{y}^k-{y}^{k+1}\\|^2\\cr & \\le & ( 1-\\theta)\\left[g({y}^0)-g({y})+({y}^{0}-{y})^\\top(-{b}^\\top{\\lambda}^0)+ \\rho_y({y}^{0}-{y})^\\top{b}^\\top{r}^{0}\\right]\\\\ & & + ( 1-\\theta)\\sum_{k=0}^t{\\mathbb{e}}\\rho_y({y}^{k}-{y})^\\top{b}^\\top{a}({x}^{k+1}-{x}^k).\\nonumber\\end{aligned}\\ ] ]      summing and together and using and , we have : @xmath338\\cr & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[\\phi({x}^{k+1},{y}^{k+1})-\\phi({x},{y})+({w}^{k+1}-{w})^\\top h({w}^{k+1})+\\frac{1}{\\rho}({\\lambda}^{k+1}-{\\lambda})^\\top({\\lambda}^{k+1}-{\\lambda}^k)\\right]\\cr & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right]\\cr & & + ( 1-\\theta)\\left[g({y}^0)-g({y})+({y}^{0}-{y})^\\top(-{b}^\\top{\\lambda}^0)+ \\rho_y({y}^{0}-{y})^\\top{b}^\\top{r}^{0}\\right]\\cr & & + \\sum_{k=0}^t\\rho_x{\\mathbb{e}}({x}^{k+1}-{x})^\\top{a}^\\top{b}({y}^{k+1}-{y}^k ) + ( 1-\\theta)\\sum_{k=0}^t\\rho_y{\\mathbb{e}}({y}^{k}-{y})^\\top{b}^\\top{a}({x}^{k+1}-{x}^k)\\cr & & -\\sum_{k=0}^t{\\mathbb{e}}({x}^{k+1}-{x})^\\top\\tilde{{p}}({x}^{k+1}-{x}^k)+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2\\cr & & -\\sum_{k=0}^t{\\mathbb{e}}({y}^{k+1}-{y})^\\top\\tilde{{q}}({y}^{k+1}-{y}^k)+\\frac{l_g}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{y}^k-{y}^{k+1}\\|^2,\\end{aligned}\\ ] ] where we have used @xmath339 and the definition of @xmath340 given in .    when @xmath176 and @xmath341 , reduces to @xmath342\\cr & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[f({x}^{k+1})-f({x})+({w}^{k+1}-{w})^\\top h({w}^{k+1})+\\frac{1}{\\rho}({\\lambda}^{k+1}-{\\lambda})^\\top({\\lambda}^{k+1}-{\\lambda}^k)\\right]\\cr & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right]\\cr & & -\\sum_{k=0}^t{\\mathbb{e}}({x}^{k+1}-{x})^\\top\\tilde{{p}}({x}^{k+1}-{x}^k)+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2.\\end{aligned}\\ ] ] using and noting @xmath343 , from the above inequality after cancelling terms we have @xmath344+\\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[f({x}^{k+1})-f({x})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right]\\nonumber\\\\ & & + \\frac{1}{2\\rho_x}{\\mathbb{e}}\\left[\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}\\|^2-\\|{\\lambda}^0-{\\lambda}\\|^2+\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}^t\\|^2+\\sum_{k=0}^{t-1}\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2\\right]\\cr & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right]\\nonumber\\\\ & & -\\frac{1}{2}{\\mathbb{e}}\\left[\\|x^{t+1}-x\\|_{\\tilde{p}}^2-\\|x^0-x\\|_{\\tilde{p}}^2+\\sum_{k=0}^t\\|x^{k+1}-x^k\\|_{\\tilde{p}}^2\\right]+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2.\\end{aligned}\\ ] ] for any feasible @xmath37 , we note @xmath345 and thus @xmath346 in addition , since @xmath347 and @xmath348 differ only on the index set @xmath88 , we have by recalling @xmath184 that @xmath349 plugging and into , and using leads to @xmath350+\\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[f({x}^{k+1})-f({x})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right]\\cr & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right ] + \\frac{1}{2\\rho_x}{\\mathbb{e}}\\|{\\lambda}^0-{\\lambda}\\|^2+\\frac{1}{2}\\|x^0-x\\|_{\\tilde{p}}^2.\\end{aligned}\\ ] ] the desired result follows from @xmath351 , and lemmas [ lem : xy - rate ] and [ equiv - rate ] with @xmath247 .",
    "it follows from with @xmath352 and @xmath130 that ( recall the definition of @xmath301 in ) for any @xmath353 , @xmath354 similar to , and recall the definition of @xmath355 , we have for any @xmath353 , @xmath356 where @xmath357 adding and to and using the formula of @xmath358 gives @xmath359 + { \\mathbb{e}}\\left(\\tilde{{\\lambda}}^{t+1}-{\\lambda}\\right)^\\top\\left({a}{x}^{t+1}+{b}\\tilde{{y}}^{t+1}-{b}+\\frac{1}{\\rho_x}(\\tilde{{\\lambda}}^{t+1}-{\\lambda}^t)\\right)\\nonumber\\\\ & & + { \\mathbb{e}}\\left[g(\\tilde{{y}}^{t+1})-g({y})+(\\tilde{{y}}^{t+1}-{y})^\\top(-{b}^\\top\\tilde{{\\lambda}}^{t+1})+\\theta(\\tilde{{y}}^{t+1}-{y})^\\top \\tilde{{q}}(\\tilde{{y}}^{t+1}-{y}^k)\\right]-\\frac{l_g}{2}{\\mathbb{e}}\\|\\tilde{{y}}^{t+1}-{y}^t\\|^2\\nonumber\\\\ & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[f({x}^{k+1})-f({x})+({x}^{k+1}-{x})^\\top(-{a}^\\top{\\lambda}^{k+1})\\right]\\nonumber\\\\ & & -\\sum_{k=0}^{t-1}\\rho_x{\\mathbb{e}}({x}^{k+1}-{x})^\\top{a}^\\top{b}({y}^{k+1}-{y}^k ) -\\rho_x{\\mathbb{e}}({x}^{t+1}-{x})^\\top{a}^\\top{b}(\\tilde{{y}}^{t+1}-{y}^t)\\nonumber\\\\ & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[g({y}^{k+1})-g({y})-\\frac{l_g}{2}\\|{y}^k-{y}^{k+1}\\|^2+({y}^{k+1}-{y})^\\top(-{b}^\\top{\\lambda}^{k+1})+({y}^{k+1}-{y})^\\top \\tilde{{q}}({y}^{k+1}-{y}^k)\\right]\\nonumber\\\\ & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}({\\lambda}^{k+1}-{\\lambda})^\\top\\left({r}^{k+1}+\\frac{1}{\\rho}({\\lambda}^{k+1}-{\\lambda}^k)\\right)\\nonumber\\\\ & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right]\\nonumber\\\\ & & -\\sum_{k=0}^t{\\mathbb{e}}({x}^{k+1}-{x})^\\top\\tilde{{p}}({x}^{k+1}-{x}^k)+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2.\\end{aligned}\\ ] ] by the notation in and using , can be written into @xmath360\\\\ & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[\\phi({x}^{k+1},{y}^{k+1})-\\phi({x},{y})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right ] \\nonumber\\\\ & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}({y}^{k+1}-{y})^\\top \\tilde{{q}}({y}^{k+1}-{y}^k)+\\theta{\\mathbb{e}}(\\tilde{{y}}^{t+1}-{y})^\\top \\tilde{{q}}(\\tilde{{y}}^{t+1}-{y}^t)\\nonumber\\\\ & & -\\sum_{k=0}^{t-1}\\rho_x{\\mathbb{e}}\\left(\\frac{1}{\\rho}({\\lambda}^k-{\\lambda}^{k+1})^\\top{b}({y}^{k+1}-{y}^k ) -({y}^{k+1}-{y})^\\top{b}^\\top{b}({y}^{k+1}-{y}^k)\\right)\\nonumber\\\\ & & -\\rho_x{\\mathbb{e}}\\left(\\frac{1}{\\rho_x}({\\lambda}^t-\\tilde{{\\lambda}}^{t+1})^\\top{b}(\\tilde{{y}}^{t+1}-{y}^t ) -(\\tilde{{y}}^{t+1}-{y})^\\top{b}^\\top{b}(\\tilde{{y}}^{t+1}-{y}^t)\\right)\\nonumber\\\\ & & + \\frac{\\theta}{\\rho}{\\mathbb{e}}(\\tilde{{\\lambda}}^{t+1}-{\\lambda})^\\top\\left(\\tilde{{\\lambda}}^{t+1}-{\\lambda}^t\\right ) + \\frac{\\theta}{\\rho}\\sum_{k=0}^{t-1}{\\mathbb{e}}({\\lambda}^{k+1}-{\\lambda})^\\top\\left({\\lambda}^{k+1}-{\\lambda}^k\\right)\\cr & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right]\\nonumber\\\\ & & -\\sum_{k=0}^t{\\mathbb{e}}({x}^{k+1}-{x})^\\top\\tilde{{p}}({x}^{k+1}-{x}^k)+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2\\nonumber\\\\ & & + \\frac{\\theta l_g}{2}\\sum_{k=0}^{t-1}{\\mathbb{e}}\\|{y}^k-{y}^{k+1}\\|^2+\\frac{l_g}{2}{\\mathbb{e}}\\|\\tilde{{y}}^{t+1}-{y}^t\\|^2.\\end{aligned}\\ ] ] now use to derive from the above inequality that @xmath361 \\nonumber",
    "\\\\ & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[\\phi({x}^{k+1},{y}^{k+1})-\\phi({x},{y})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right ] \\nonumber\\\\ & & + \\frac{\\theta}{2}\\left({\\mathbb{e}}\\|\\tilde{{y}}^{t+1}-{y}\\|_{\\tilde{{q}}}^2-\\|{y}^0-{y}\\|_{\\tilde{{q}}}^2\\right ) + \\frac{\\theta}{2}\\sum_{k=0}^{t-1}{\\mathbb{e}}\\|{y}^{k+1}-{y}^k\\|_{\\tilde{{q}}}^2+\\frac{\\theta}{2}{\\mathbb{e}}\\|\\tilde{{y}}^{t+1}-{y}^t\\|_{\\tilde{{q}}}^2\\nonumber\\\\ & & + \\frac{\\rho_x}{2}\\left({\\mathbb{e}}\\|\\tilde{{y}}^{t+1}-{y}\\|_{{b}^\\top{b}}^2-\\|{y}^0-{y}\\|_{{b}^\\top{b}}^2\\right ) + \\frac{\\rho_x}{2}\\sum_{k=0}^{t-1}{\\mathbb{e}}\\|{y}^{k+1}-{y}^k\\|_{{b}^\\top{b}}^2+\\frac{\\rho_x}{2}{\\mathbb{e}}\\|\\tilde{{y}}^{t+1}-{y}^t\\|_{{b}^\\top{b}}^2\\nonumber\\\\ & & -\\sum_{k=0}^{t-1}{\\mathbb{e}}\\frac{\\rho_x}{\\rho}\\left({\\lambda}^k-{\\lambda}^{k+1}\\right)^\\top{b}({y}^{k+1}-{y}^k ) -{\\mathbb{e}}({\\lambda}^t-\\tilde{{\\lambda}}^{t+1})^\\top{b}(\\tilde{{y}}^{t+1}-{y}^t)\\nonumber\\\\ & & + \\frac{\\theta}{2\\rho}\\left({\\mathbb{e}}\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}\\|^2-\\|{\\lambda}^0-{\\lambda}\\|^2\\right ) + \\frac{\\theta}{2\\rho}\\sum_{k=0}^{t-1}{\\mathbb{e}}\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2+\\frac{\\theta}{2\\rho}{\\mathbb{e}}\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}^t\\|^2\\nonumber\\\\ & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right]\\nonumber\\\\ & & -\\frac{1}{2}\\left[{\\mathbb{e}}\\|{x}^{t+1}-{x}\\|^2_{\\tilde{{p}}}-\\|{x}^0-{x}\\|^2_{\\tilde{{p } } } + \\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2_{\\tilde{{p}}}\\right]+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2\\nonumber\\\\ & & + \\frac{\\theta l_g}{2}\\sum_{k=0}^{t-1}{\\mathbb{e}}\\|{y}^k-{y}^{k+1}\\|^2+\\frac{l_g}{2}{\\mathbb{e}}\\|\\tilde{{y}}^{t+1}-{y}^t\\|^2.\\end{aligned}\\ ] ] note that for @xmath362 , @xmath363 and @xmath364    because @xmath365 and @xmath366 satisfy , we have from that @xmath367 \\nonumber \\\\ & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[\\phi({x}^{k+1},{y}^{k+1})-\\phi({x},{y})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right ] \\nonumber\\cr & \\le & ( 1-\\theta)\\left[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\right]\\cr & & + \\frac{1}{2}\\|{x}^0-{x}\\|^2_{\\tilde{{p}}}+\\frac{\\theta}{2}\\|{y}^0-{y}\\|_{\\tilde{{q}}}^2+\\frac{\\rho}{2\\theta}\\|{y}^0-{y}\\|_{{b}^\\top{b}}^2+\\frac{\\theta}{2\\rho}{\\mathbb{e}}\\|{\\lambda}^0-{\\lambda}\\|^2.\\end{aligned}\\ ] ] similar to theorem [ thm : rate - cvx ] , from the convexity of @xmath368 and , we have @xmath369\\cr & \\le & ( 1-\\theta)\\big[f({x}^0)-f({x})+({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}\\big]\\cr & & + \\frac{1}{2}\\|{x}^0-{x}\\|^2_{\\tilde{{p}}}+\\frac{\\theta}{2}\\|{y}^0-{y}\\|_{\\tilde{{q}}}^2+\\frac{\\rho}{2\\theta}\\|{y}^0-{y}\\|_{{b}^\\top{b}}^2+\\frac{\\theta}{2\\rho}{\\mathbb{e}}\\|{\\lambda}^0-{\\lambda}\\|^2.\\end{aligned}\\ ] ] noting @xmath351 and @xmath370 $ ] , and using lemmas [ lem : xy - rate ] and [ equiv - rate ] with @xmath247 , we obtain the result .      using and , applying to the cross terms , and also noting the definition of @xmath300 and @xmath301 in",
    ", we have @xmath371\\cr & & + \\sum_{k=0}^t\\rho_x{\\mathbb{e}}({x}^{k+1}-{x})^\\top{a}^\\top{b}({y}^{k+1}-{y}^k ) + ( 1-\\theta)\\sum_{k=0}^t\\rho_y{\\mathbb{e}}({y}^{k}-{y})^\\top{b}^\\top{a}({x}^{k+1}-{x}^k)\\cr & & -\\sum_{k=0}^t{\\mathbb{e}}({x}^{k+1}-{x})^\\top\\tilde{{p}}({x}^{k+1}-{x}^k)+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2\\cr & & -\\sum_{k=0}^t{\\mathbb{e}}({y}^{k+1}-{y})^\\top\\tilde{{q}}({y}^{k+1}-{y}^k)+\\frac{l_g}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{y}^k-{y}^{k+1}\\|^2\\cr & = & -\\frac{\\theta}{2\\rho}{\\mathbb{e}}\\left[\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}\\|^2-\\|{\\lambda}^0-{\\lambda}\\|^2+\\sum_{k=0}^{t-1}\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2+\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}^t\\|^2\\right]\\cr & & + \\frac{\\rho_x}{\\rho}\\sum_{k=0}^t { \\mathbb{e}}({\\lambda}^k-{\\lambda}^{k+1})^\\top{b}({y}^{k+1}-{y}^k)+\\frac{(1-\\theta)\\rho_y}{\\rho}\\sum_{k=0}^t{\\mathbb{e}}({\\lambda}^{k-1}-{\\lambda}^{k})^\\top{a}({x}^{k+1}-{x}^k)\\cr & & -\\frac{\\theta\\rho_y}{2}{\\mathbb{e}}\\left(\\|{x}^0-{x}\\|_{{a}^\\top{a}}^2-\\|{x}^{t+1}-{x}\\|_{{a}^\\top{a}}^2\\right)+\\frac{(2-\\theta)\\rho_y}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^{k+1}-{x}^k\\|_{{a}^\\top{a}}^2\\cr & & -\\frac{1}{2}{\\mathbb{e}}\\left(\\|{x}^{t+1}-{x}\\|_{\\hat{{p}}}^2-\\|{x}^0-{x}\\|_{\\hat{{p}}}^2 + \\sum_{k=0}^t\\|{x}^{k+1}-{x}^k\\|_{\\hat{{p}}}^2\\right)+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2\\cr & & -\\frac{1}{2}{\\mathbb{e}}\\left(\\|{y}^{t+1}-{y}\\|_{\\hat{{q}}}^2-\\|{y}^0-{y}\\|_{\\hat{{q}}}^2 + \\sum_{k=0}^t\\|{y}^{k+1}-{y}^k\\|_{\\hat{{q}}}^2\\right)+\\frac{l_g}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{y}^k-{y}^{k+1}\\|^2,\\end{aligned}\\ ] ] where we have used the conditions in . by young s inequality",
    ", we have that for @xmath372 , @xmath373 and for @xmath374 , @xmath375 plugging and and also noting @xmath376 , we can upper bound the right hand side of by @xmath377-\\frac{\\theta\\rho_y}{2}{\\mathbb{e}}\\left(\\|{x}^0-{x}\\|_{{a}^\\top{a}}^2-\\|{x}^{t+1}-{x}\\|_{{a}^\\top{a}}^2\\right)\\cr & & + \\left(\\frac{(1-\\theta)(2-\\theta)}{2\\theta^2}\\rho_x+\\frac{(2-\\theta)\\rho_y}{2}\\right)\\sum_{k=0}^t { \\mathbb{e}}\\|{x}^{k+1}-{x}^k\\|_{{a}^\\top{a}}^2+\\frac{(2-\\theta)\\rho_y}{2\\theta^2}\\sum_{k=0}^t{\\mathbb{e}}\\|{y}^{k+1}-{y}^k\\|_{{b}^\\top{b}}^2\\cr & & -\\frac{1}{2}{\\mathbb{e}}\\left(\\|{x}^{t+1}-{x}\\|_{\\hat{{p}}}^2-\\|{x}^0-{x}\\|_{\\hat{{p}}}^2 + \\sum_{k=0}^t\\|{x}^{k+1}-{x}^k\\|_{\\hat{{p}}}^2\\right)+\\frac{l_f}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2\\cr & & -\\frac{1}{2}{\\mathbb{e}}\\left(\\|{y}^{t+1}-{y}\\|_{\\hat{{q}}}^2-\\|{y}^0-{y}\\|_{\\hat{{q}}}^2 + \\sum_{k=0}^t\\|{y}^{k+1}-{y}^k\\|_{\\hat{{q}}}^2\\right)+\\frac{l_g}{2}\\sum_{k=0}^t{\\mathbb{e}}\\|{y}^k-{y}^{k+1}\\|^2\\cr & \\overset{\\eqref{para - mat}}\\le & \\frac{1}{2}\\left(\\|{x}^0-{x}\\|_{\\hat{{p}}-\\theta\\rho_xa^\\top a}^2+\\|{y}^0-{y}\\|_{\\hat{{q}}}^2\\right)+\\frac{\\theta}{2\\rho}{\\mathbb{e}}\\|\\lambda^0-\\lambda\\|^2.\\label{ineq - k4-w - sub2}\\end{aligned}\\ ] ] in addition , note that @xmath378 hence , if @xmath195 and @xmath196 satisfy , then also holds .    combining , and yields @xmath379\\cr & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[\\phi({x}^{k+1},{y}^{k+1})-\\phi({x},{y})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right]\\cr & \\le & ( 1-\\theta)\\left[\\phi({x}^0,{y}^0)-\\phi({x},{y})\\right]\\cr & & + ( 1-\\theta)\\left[({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}+({y}^{0}-{y})^\\top(-{b}^\\top{\\lambda}^0)+ \\rho_y({y}^{0}-{y})^\\top{b}^\\top{r}^{0}\\right]\\cr & & + \\frac{1}{2}\\left(\\|{x}^0-{x}\\|_{\\hat{{p}}-\\theta\\rho_xa^\\top a}^2+\\|{y}^0-{y}\\|_{\\hat{{q}}}^2\\right)+\\frac{\\theta}{2\\rho}{\\mathbb{e}}\\|\\lambda^0-\\lambda\\|^2 . \\ ] ] applying the convexity of @xmath368 and the properties of @xmath340 , we have @xmath380\\cr & \\overset{\\eqref{equivhw}}= & ( 1+\\theta t){\\mathbb{e}}\\left[\\phi(\\hat{{x}}^t,\\hat{{y}}^t)-\\phi({x},{y})+(\\hat{{w}}^{t+1}-{w})^\\top h(\\hat{{w}}^{t+1})\\right]\\cr & \\overset{\\eqref{prop - mas - h}}\\leq&{\\mathbb{e}}\\left[\\phi({x}^{t+1},{y}^{t+1})-\\phi({x},{y})+(\\tilde{{w}}^{t+1}-{w})^\\top h(\\tilde{{w}}^{t+1})\\right]\\cr & & + \\theta\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[\\phi({x}^{k+1},{y}^{k+1})-\\phi({x},{y})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right].\\end{aligned}\\ ] ] now combining and , we have @xmath381\\cr & \\le & ( 1-\\theta)\\left[\\phi({x}^0,{y}^0)-\\phi({x},{y})\\right]\\cr & & + ( 1-\\theta)\\left[({x}^{0}-{x})^\\top(-{a}^\\top{\\lambda}^0)+ \\rho_x({x}^{0}-{x})^\\top{a}^\\top{r}^{0}+({y}^{0}-{y})^\\top(-{b}^\\top{\\lambda}^0)+ \\rho_y({y}^{0}-{y})^\\top{b}^\\top{r}^{0}\\right]\\cr & & + \\frac{1}{2}\\left(\\|{x}^0-{x}\\|_{\\hat{{p}}-\\theta\\rho_xa^\\top a}^2+\\|{y}^0-{y}\\|_{\\hat{{q}}}^2\\right)+\\frac{\\theta}{2\\rho}{\\mathbb{e}}\\|\\lambda^0-\\lambda\\|^2 . \\ ] ]          assume @xmath382 .",
    "it holds that @xmath383\\notag\\\\ & & -\\sum_{k=0}^{t-1}\\frac{\\alpha_k\\beta_{k+1}}{2\\alpha_{k+1}}\\left[\\|{\\lambda}^{k+1}-{\\lambda}\\|^2-\\|{\\lambda}^{k}-{\\lambda}\\|^2+\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2\\right]\\nonumber\\\\ & \\le&-\\sum_{k=0}^{t-1}\\frac{\\beta_{k+1}}{2}\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2 + \\sum_{k=1}^t\\frac{(1-\\theta)\\beta_k}{2}\\|{\\lambda}^{k}-{\\lambda}^{k-1}\\|^2 \\nonumber \\\\ & & -\\sum_{k=0}^{t-1}\\frac{\\alpha_k\\beta_{k+1}}{2\\alpha_{k+1}}\\|{\\lambda}^{k+1}-{\\lambda}\\|^2 -\\sum_{k=1}^t\\frac{(1-\\theta)\\beta_k}{2}\\|{\\lambda}^{k-1}-{\\lambda}\\|^2\\nonumber\\\\ & & + \\frac{\\alpha_0\\beta_1}{2\\alpha_1}\\|{\\lambda}^0-{\\lambda}\\|^2+\\sum_{k=1}^{t-1}\\frac{\\alpha_k\\beta_{k+1}}{2\\alpha_{k+1}}\\|{\\lambda}^{k}-{\\lambda}\\|^2 + \\sum_{k=1}^t\\frac{(1-\\theta)\\beta_k}{2}\\|{\\lambda}^{k}-{\\lambda}\\|^2\\nonumber\\\\ & = & -\\sum_{k=0}^{t-1}\\frac{\\theta\\beta_{k+1}}{2}\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2 + \\left(\\frac{\\alpha_0\\beta_1}{2\\alpha_1}-\\frac{(1-\\theta)\\beta_1}{2}\\right)\\|{\\lambda}^0-{\\lambda}\\|^2 -\\left(\\frac{\\alpha_{t-1}\\beta_t}{2\\alpha_t}-\\frac{(1-\\theta)\\beta_t}{2}\\right)\\|{\\lambda}^{t}-{\\lambda}\\|^2\\nonumber\\\\ & & -\\sum_{k=1}^{t-1}\\left(\\frac{\\alpha_{k-1}\\beta_k}{2\\alpha_k } + \\frac{(1-\\theta)\\beta_{k+1}}{2}-\\frac{\\alpha_k\\beta_{k+1}}{2\\alpha_{k+1}}-\\frac{(1-\\theta)\\beta_k}{2}\\right)\\|{\\lambda}^{k}-{\\lambda}\\|^2.\\end{aligned}\\ ] ]    by the update formula of @xmath384 in , we have from that @xmath385\\cr & & + { \\mathbb{e}}\\left[\\frac{({\\lambda}^{k+1}-{\\lambda})^\\top({\\lambda}^{k+1}-{\\lambda}^{k})}{\\left(1-\\frac{(1-\\theta)\\alpha_{k+1}}{\\alpha_k } \\right)\\rho } + \\frac{(1-\\theta)\\alpha_{k+1}}{\\alpha_k}\\rho({x}^{k+1}-{x})^\\top{a}^\\top{r}^{k+1}\\right]\\cr & & + { \\mathbb{e}}({x}^{k+1}-{x})^\\top\\left(\\tilde{{p}}+\\frac{{i}}{\\alpha_k}\\right)({x}^{k+1}-{x}^k ) -\\frac{l_f}{2}{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2+{\\mathbb{e}}({x}^{k+1}-{x}^k)^\\top{\\delta}^k\\cr & \\le & ( 1-\\theta){\\mathbb{e}}\\left[f({x}^k)-f({x})+({x}^{k}-{x})^\\top(-{a}^\\top{\\lambda}^k)+({\\lambda}^{k}-{\\lambda})^\\top { r}^{k}+\\frac{({\\lambda}^{k}-{\\lambda})^\\top({\\lambda}^k-{\\lambda}^{k-1})}{\\left(1-\\frac{(1-\\theta)\\alpha_{k}}{\\alpha_{k-1}}\\right)\\rho } \\right]\\cr & & + ( 1-\\theta)\\rho{\\mathbb{e}}({x}^{k}-{x})^\\top{a}^\\top{r}^{k},\\end{aligned}\\ ] ] where similar to , we have defined @xmath386 .    multiplying @xmath226 to both sides of and using and , we have @xmath387\\cr & & + \\frac{\\alpha_k\\beta_{k+1}}{2\\alpha_{k+1}}{\\mathbb{e}}\\left[\\|{\\lambda}^{k+1}-{\\lambda}\\|^2-\\|{\\lambda}^{k}-{\\lambda}\\|^2+\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2\\right ] + { \\mathbb{e}}\\left[(1-\\theta)\\alpha_{k+1}\\rho({x}^{k+1}-{x})^\\top{a}^\\top{r}^{k+1}\\right]\\cr & & + \\frac{\\alpha_k}{2}{\\mathbb{e}}\\big[\\|{x}^{k+1}-{x}\\|_{\\tilde{{p}}}^2-\\|{x}^{k}-{x}\\|_{\\tilde{{p}}}^2+\\|{x}^{k+1}-{x}^k\\|_{\\tilde{{p}}}^2\\big ] + \\frac{1}{2}{\\mathbb{e}}\\left[\\|{x}^{k+1}-{x}\\|^2-\\|{x}^{k}-{x}\\|^2+\\|{x}^{k+1}-{x}^k\\|^2\\right]\\cr & & -\\frac{\\alpha_kl_f}{2}{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2+\\alpha_k{\\mathbb{e}}({x}^{k+1}-{x}^k)^\\top{\\delta}^k\\cr & \\le & ( 1-\\theta)\\alpha_k{\\mathbb{e}}\\left[f({x}^k)-f({x})+({w}^{k}-{w})^\\top h({w}^k)\\right]\\cr & & + \\frac{(1-\\theta)\\beta_k}{2}{\\mathbb{e}}\\left[\\|{\\lambda}^{k}-{\\lambda}\\|^2-\\|{\\lambda}^{k-1}-{\\lambda}\\|^2+\\|{\\lambda}^{k}-{\\lambda}^{k-1}\\|^2\\right]+\\alpha_k(1-\\theta)\\rho{\\mathbb{e}}({x}^{k}-{x})^\\top{a}^\\top{r}^{k}.\\end{aligned}\\ ] ]    denote @xmath388 then for @xmath389 , it is easy to see that becomes @xmath390\\cr & & + \\frac{\\alpha_t}{2\\rho}{\\mathbb{e}}\\left[\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}\\|^2-\\|{\\lambda}^{t}-{\\lambda}\\|^2+\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}^t\\|^2\\right]\\cr & & + \\frac{\\alpha_t}{2}{\\mathbb{e}}\\left[\\|{x}^{t+1}-{x}\\|_{\\tilde{{p}}}^2-\\|{x}^{t}-{x}\\|_{\\tilde{{p}}}^2+\\|{x}^{t+1}-{x}^t\\|_{\\tilde{{p}}}^2\\right ] + \\frac{1}{2}{\\mathbb{e}}\\left[\\|{x}^{t+1}-{x}\\|^2-\\|{x}^{t}-{x}\\|^2+\\|{x}^{t+1}-{x}^t\\|^2\\right]\\cr & & -\\frac{\\alpha_tl_f}{2}{\\mathbb{e}}\\|{x}^t-{x}^{t+1}\\|^2+\\alpha_t{\\mathbb{e}}({x}^{t+1}-{x}^t)^\\top{\\delta}^t\\cr & \\le & ( 1-\\theta)\\alpha_t{\\mathbb{e}}\\left[f({x}^t)-f({x})+({w}^{t}-{w})^\\top h({w}^t)\\right]\\cr & & + \\frac{(1-\\theta)\\beta_t}{2}{\\mathbb{e}}\\left[\\|{\\lambda}^{t}-{\\lambda}\\|^2-\\|{\\lambda}^{t-1}-{\\lambda}\\|^2+\\|{\\lambda}^{t}-{\\lambda}^{t-1}\\|^2\\right]+\\alpha_t(1-\\theta){\\mathbb{e}}\\rho({x}^{t}-{x})^\\top{a}^\\top{r}^{t}.\\end{aligned}\\ ] ]    by the nonincreasing monotonicity of @xmath226 , summing from @xmath330 through @xmath391 and and plugging gives @xmath392+\\theta\\alpha_{k+1}\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[f({x}^{k+1})-f({x})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right]\\nonumber\\\\ & & + \\frac{\\alpha_t}{2\\rho}{\\mathbb{e}}\\left[\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}\\|^2-\\|{\\lambda}^{t}-{\\lambda}\\|^2+\\|\\tilde{{\\lambda}}^{t+1}-{\\lambda}^t\\|^2\\right]\\nonumber\\\\ & & + \\frac{\\alpha_{t+1}}{2}{\\mathbb{e}}\\|{x}^{t+1}-{x}\\|_{\\tilde{{p}}}^2 + \\sum_{k=0}^t\\frac{\\alpha_k}{2}{\\mathbb{e}}\\|{x}^{k+1}-{x}^k\\|_{\\tilde{{p}}}^2+\\frac{1}{2}{\\mathbb{e}}\\big[\\|{x}^{t+1}-{x}\\|^2-\\|{x}^{0}-{x}\\|^2+\\sum_{k=0}^t\\|{x}^{k+1}-{x}^k\\|^2\\big]\\nonumber\\\\ & & -\\sum_{k=0}^t\\frac{\\alpha_kl_f}{2}{\\mathbb{e}}\\|{x}^k-{x}^{k+1}\\|^2+\\sum_{k=0}^t\\alpha_k{\\mathbb{e}}({x}^{k+1}-{x}^k)^\\top{\\delta}^k\\nonumber\\\\ & \\le & ( 1-\\theta)\\alpha_0{\\mathbb{e}}\\left[f({x}^0)-f({x})+({w}^{0}-{w})^\\top h({w}^0)\\right]+\\alpha_0(1-\\theta)\\rho({x}^{0}-{x})^\\top{a}^\\top{r}^{0}+\\frac{\\alpha_0}{2}\\|{x}^{0}-{x}\\|_{\\tilde{{p}}}^2\\nonumber\\\\ & & -\\sum_{k=0}^{t-1}\\frac{\\theta\\beta_{k+1}}{2}{\\mathbb{e}}\\|{\\lambda}^{k+1}-{\\lambda}^k\\|^2 + \\left(\\frac{\\alpha_0\\beta_1}{2\\alpha_1}-\\frac{(1-\\theta)\\beta_1}{2}\\right){\\mathbb{e}}\\|{\\lambda}^0-{\\lambda}\\|^2 -\\left(\\frac{\\alpha_{t-1}\\beta_t}{2\\alpha_t}-\\frac{(1-\\theta)\\beta_t}{2}\\right){\\mathbb{e}}\\|{\\lambda}^{t}-{\\lambda}\\|^2\\nonumber\\\\ & & -\\sum_{k=1}^{t-1}\\left(\\frac{\\alpha_{k-1}\\beta_k}{2\\alpha_k}+\\frac{(1-\\theta)\\beta_{k+1}}{2 } -\\frac{\\alpha_k\\beta_{k+1}}{2\\alpha_{k+1}}-\\frac{(1-\\theta)\\beta_k}{2}\\right){\\mathbb{e}}\\|{\\lambda}^{k}-{\\lambda}\\|^2 .\\end{aligned}\\ ] ] from , we have @xmath393\\ge-\\left(\\frac{\\alpha_{t-1}\\beta_t}{2\\alpha_t}-\\frac{(1-\\theta)\\beta_t}{2}\\right)\\|{\\lambda}^{t}-{\\lambda}\\|^2.\\ ] ] in addition , from young s inequality , it holds that @xmath394 hence , dropping negative terms on the right hand side of , from the convexity of @xmath368 and , we have @xmath395\\cr & & \\alpha_t{\\mathbb{e}}\\left[f({x}^{t+1})-f({x})+(\\tilde{{w}}^{t+1}-{w})^\\top h(\\tilde{{w}}^{t+1})\\right]+\\theta\\alpha_{k+1}\\sum_{k=0}^{t-1}{\\mathbb{e}}\\left[f({x}^{k+1})-f({x})+({w}^{k+1}-{w})^\\top h({w}^{k+1})\\right]\\cr & \\le & ( 1-\\theta)\\alpha_0\\left[f({x}^0)-f({x})+({w}^{0}-{w})^\\top h({w}^0)\\right ] + ( 1-\\theta)\\alpha_0\\rho({x}^{0}-{x})^\\top{a}^\\top{r}^{0}+\\frac{\\alpha_0}{2}\\|{x}^{0}-{x}\\|_{\\tilde{{p}}}^2+\\frac{1}{2}\\|{x}^0-{x}\\|^2\\cr & & + \\left(\\frac{\\alpha_0\\beta_1}{2\\alpha_1}-\\frac{(1-\\theta)\\beta_1}{2}\\right){\\mathbb{e}}\\|{\\lambda}^0-{\\lambda}\\|^2+\\sum_{k=0}^t\\frac{\\alpha_k^2}{2}{\\mathbb{e}}\\|{\\delta}^k\\|^2.\\end{aligned}\\ ] ] using lemma [ lem : xy - rate ] and the properties of @xmath340 , we derive the desired result .",
    "let @xmath396 denote the proximal mapping of @xmath397 at @xmath37 .",
    "then the update in can be written to @xmath398 define @xmath399 as that in .",
    "then @xmath400 hence , using the fact that the conjugate of @xmath401 is @xmath402 and the moreau s identity @xmath403 for any convex function @xmath397 , we have @xmath404 therefore , holds , and thus from it follows @xmath405 substituting the formula of @xmath406 into , we have for @xmath407 , @xmath408 which is exactly .",
    "hence , we complete the proof ."
  ],
  "abstract_text": [
    "<S> in this paper we propose a randomized primal - dual proximal block coordinate updating framework for a general multi - block convex optimization model with coupled objective function and linear constraints . assuming mere convexity </S>",
    "<S> , we establish its @xmath0 convergence rate in terms of the objective value and feasibility measure . </S>",
    "<S> the framework includes several existing algorithms as special cases such as a primal - dual method for bilinear saddle - point problems ( pd - s ) , the proximal jacobian admm ( prox - jadmm ) and a randomized variant of the admm method for multi - block convex optimization . </S>",
    "<S> our analysis recovers and/or strengthens the convergence properties of several existing algorithms . for example , for pd - s our result leads to the same order of convergence rate without the previously assumed boundedness condition on the constraint sets , and for prox - jadmm the new result provides convergence rate in terms of the objective value and the feasibility violation . </S>",
    "<S> it is well known that the original admm may fail to converge when the number of blocks exceeds two . </S>",
    "<S> our result shows that if an appropriate randomization procedure is invoked to select the updating blocks , then a sublinear rate of convergence in expectation can be guaranteed for multi - block admm , without assuming any strong convexity . </S>",
    "<S> the new approach is also extended to solve problems where only a stochastic approximation of the ( sub-)gradient of the objective is available , and we establish an @xmath1 convergence rate of the extended approach for solving stochastic programming .    * keywords : * primal - dual method , alternating direction method of multipliers ( admm ) , randomized algorithm , iteration complexity , first - order stochastic approximation .    </S>",
    "<S> * mathematics subject classification : * 90c25 , 95c06 , 68w20 . </S>"
  ]
}