{
  "article_text": [
    "efficient communication over channels introducing inter - symbol interference ( isi ) often requires the receiver to perform channel equalization .",
    "turbo equalization @xcite is a technique in which decoding and equalization are performed iteratively , similar to turbo - decoding of serially - concatenated convolutional codes @xcite . as depicted in figure [ fig : systemmodel1 ] , the key element of the receiver employing this method is a soft - input soft - output ( siso ) demodulator / equalizer ( from now on referred to as just an equalizer ) , accepting a priori likelihoods of coded bits from the siso decoder , and producing their a posteriori likelihoods based on the noisy received signal .",
    "the siso algorithm that computes the exact values of the a posteriori likelihoods is the bcjr algorithm @xcite .",
    "the complexity of a bcjr equalizer is proportional to the number of states in the trellis representing the modulation alphabet and the isi , and thus it is exponential in both the length of the channel impluse response ( cir ) and in the number of bits per symbol in the modulator .",
    "this can be a serious drawback in some scenarios , e.g. , transmission at a high data rate over a radio channel , where a large signal bandwidth translates to a long cir , and a high spectral efficiency translates to a large modulation alphabet .",
    "needed in such cases are alternative siso equalizers with the ability to achieve large complexity savings at a cost of small performance degradation .",
    "there have been two main trends in the design of such sisos .",
    "the first one relies on reducing the effective length of the channel impulse response , either by linear processing ( see , e.g. , @xcite ) , or interference cancellation via decision feedback .",
    "a particularly good algorithm is this category is the reduced - state bcjr ( rs - bcjr ) @xcite , which performs the cancellation of the final channel taps on a per - survivor basis .",
    "iterative decoding with rs - bcjr is very stable , thanks to the high quality of the soft outputs , but the receiver can not use the signal power contained in the cancelled part of the cir . another trend is to adapt `` hard - output '' sequential algorithms @xcite to produce soft outputs @xcite .",
    "examples in this category are the m - bcjr and t - bcjr algorithms @xcite , based on the m- and t - algorithms , and the liss algorithm @xcite based on list sequential decoding .",
    "these algorithms have no problem using the signal energy from the whole cir , and offer much more flexibility in choosing the desired complexity .",
    "however , their reliance on ignoring unpromising paths in the trellis or tree causes a bias in the soft output ( there are more explored paths with one value of a particular input bit than another ) , which negatively affects the convergence of iterative decoding .    in this paper",
    "we present a new siso equalization algorithm , inspired by both the m - bcjr and rs - bcjr , which shares many of their advantages , but few of their weaknesses .",
    "we call this algorithm the m@xmath1-bcjr algorithm , since it resembles the m - bcjr in preserving only a fixed number of trellis states with the largest forward metric . instead of deleting the excess states",
    ", however , the m@xmath1-bcjr dynamically merges them with the surviving states  a process that shares some similarity to the static state merging done on a per - survivor basis by the rs - bcjr . for the sake of simpler notation , we present the operation of all bcjr - based algorithms , including the m@xmath1-bcjr , in the probability domain . each of them , however , can be implemented in the log domain for better numerical stability .",
    "the rest of the paper is structured as follows .",
    "section 2 describes the communication system and the task of the siso equalizer and introduces the notation .",
    "section 3 reviews the structure of the bcjr , m - bcjr , and rs - bcjr algorithms , helping us to introduce the m@xmath1-bcjr in section 4 .",
    "section 5 presents simulation results , and conclusions are given in section 6 .",
    "a communication system with turbo equalization is depicted in figure [ fig : systemmodel1 ] .",
    "the information bits are first arranged into blocks and encoded with a convolutional code .",
    "the blocks of coded bits are permuted using an interleaver and mapped onto a sequence of complex symbols by the modulator .",
    "( in general , the modulator can have memory , but for simplicity we will assume a memoryless mapper . ) the channel acts as a discrete - time finite impulse response ( fir ) filter introducing isi , the output of which is further corrupted by additive white gaussian noise ( awgn ) .",
    "we assume the receiver knows the isi channel coefficients and the noise variance , and it attempts to recover the information bits by iteratively performing siso equalization and decoding .",
    "the part of the system significant from the point of view of the equalizer is shown in figure [ fig : systemmodel2 ] .",
    "let @xmath2 denote a sequence of @xmath3 bits entering the modulator , arranged into @xmath4 groups @xmath5 of @xmath6 bits .",
    "each @xmath6-tuple @xmath7 selects a complex - valued output symbol @xmath8 from a constellation of size @xmath9 to be transmitted .",
    "the sequence of symbols @xmath10 obtained at the receiver is modeled as @xmath11 where @xmath12 is the memory of the channel , @xmath13 , @xmath14 , are the channel coefficients , and @xmath15 , @xmath16 , are i.i.d .",
    "zero - mean complex - valued gaussian random variables with variance @xmath17 per complex dimension .",
    "equation ( [ eq : channel1 ] ) assumes that @xmath8 is zero outside @xmath18 .",
    "the siso equalizer for the above channel takes the received symbols @xmath19 and the a priori log - likelihood ratios @xmath20 for each bit @xmath21 , defined as @xmath22 and outputs the a posteriori l - values @xmath23 @xmath24",
    "the values actually fed to the siso decoder are extrinsic l - values , computed as @xmath25 .",
    "let @xmath26 denote the joint probability that @xmath27 was transmitted and @xmath19 was received . then ( [ eq : apost1 ] ) can be expressed as @xmath28 where the summations are performed over all @xmath27 consistent with @xmath29 .",
    "furthermore , @xmath30 where @xmath13 , @xmath14 , and @xmath17 are assumed known at the receiver and @xmath31 is obtained from @xmath32 as @xmath33 with @xmath34    since the number of paths involved in the summations of ( [ eq : apost_lambda ] ) is extrememly large for realistic values of @xmath6 and @xmath4 , a practical algorithm seeks to simplify or approximate this calcualtion .",
    "the classical algorithm for efficiently computing ( [ eq : apost_lambda ] ) by exploiting the trellis structure of the set of all paths is the bcjr algorithm @xcite . by defining the state @xmath35 at time i as the past @xmath12 input symbol @xmath6-tuples @xmath7 , @xmath36 , and a branch metric @xmath37 as @xmath38 the path metric can be factored into @xmath39 for indices outside the range @xmath40 , the variables @xmath7 are regarded as empty sequences @xmath41 with @xmath42 .    for every trellis branch @xmath43 starting in state @xmath35 , labeled by input bits @xmath7 , and ending in state @xmath44 , the bcjr algorithm computes the sum of the path metrics @xmath26 over all paths passing through this branch as @xmath45 the computation of the forward state metrics @xmath46 is performed in the _ forward recursion _ for @xmath47 : @xmath48 with the initial state value @xmath49 . similarly ,",
    "the _ backward recursion _ computes the backward state metrics @xmath50 for @xmath51 : @xmath52 with the terminal state value @xmath53 . with",
    "all @xmath54 s , @xmath55 s , and @xmath56 s computed , the summations over paths in ( [ eq : apost_lambda ] ) can be replaced by the summations over branches , @xmath57 the _ completion _ phase , in which ( [ eq : bcjr_complete ] ) is evaluated for every @xmath21 , concludes the algorithm .",
    "the complexity of the bcjr equalizer is proportional to the number of trellis states , @xmath58 .",
    "the following subsections describe the operation of the rs - bcjr @xcite and m - bcjr @xcite algorithms , which preserve the general structure of the bcjr , but instead operate on dynamically built simplified trellises with a number of states controlled via a parameter . in the original form of both algorithms , the construction of this simplified trellis occurs during the forward recursion and is based on the values of the forward state metrics , while the backward recursion and the completion phase just reuse the same trellis .",
    "the way we will describe the operation of the rs - bcjr algorithm is slightly different from the presentation in @xcite , but is in fact equivalent .",
    "let us consider two states in the trellis , @xmath59 differing only in the last @xmath60 binary @xmath6-tuples . furthermore , consider two partial paths beginning in states @xmath35 and @xmath61 and corresponding to the same partial input sequence @xmath62}=({\\bf a}_i , ... , { \\bf a}_l)$ ] .",
    "both paths are guaranteed to merge after @xmath60 time indices , and hence their partial path metrics are @xmath63 } ) = \\prod_{j = i}^{i+s - s'-1 } \\gamma(s_j , { \\bf a}_j )              \\prod_{j = i+s - s'}^{l } \\gamma(s_j , { \\bf a}_j ) , \\\\",
    "\\lambda(s'_i , { \\bf a}_{[i , l ] } ) = \\prod_{j = i}^{i+s - s'-1 } \\gamma(s'_j , { \\bf a}_j )              \\prod_{j = i+s - s'}^{l } \\gamma(s_j , { \\bf a}_j).\\end{aligned}\\ ] ] additionally , close examination of ( [ eq : gamma ] ) reveals that the difference between @xmath64 and @xmath65 for @xmath66 is not large .",
    "hence , the difference between @xmath67 and @xmath68 , for @xmath62}$ ] , is also not large .",
    "the rs - bcjr equalizer relies on the above observation and , for some predefined @xmath69 , declares states differing only in the last @xmath60 binary @xmath6-tuples indistinguishable .",
    "every such set of states is subsequently reduced to a single state , by selecting the state with the highest forward metric and _ merging _ all remaining states into it . here",
    ", we define _ merging _ of the state @xmath61 _ into _ @xmath35 as updating the forward metric @xmath70 , redirecting all trellis branches ending at @xmath61 into @xmath35 , and deleting @xmath61 from the trellis .",
    "this reduction is performed during the forward recursion , and the @xmath56 s for the paths that originate from removed states need never be computed .",
    "the trellis that results has only @xmath71 states , compared to @xmath58 in the original trellis .",
    "the same trellis is then reused in the backward recursion and the completion stage .",
    "the rs - bcjr equalizer is particularly effective when the final coefficients of the isi channel are small in magnitude .",
    "furthermore , the reduced - state trellis retains the same branch - to - state ratio ( branch density ) and has the same number of branches with @xmath72 and @xmath73 for any @xmath74 and @xmath75  properties that ensure a high quality for the soft outputs and good convergence of iterative decoding .",
    "unfortunately , the rs - bcjr algorithm can not use the signal power in the final @xmath60 channel taps , effectively reducing the minimum euclidean distance between paths .",
    "moreover , the number of surviving states can only be set to a power of @xmath9 , which could be a problem for large @xmath6 ( e.g. , for a system with 16qam modulation , equalization using 16 states could result in poor performance , while 256 states could exceed acceptable complexity ) .",
    "the m - bcjr algorithm is based on the m - algorithm @xcite , originally designed for the problem of maximum likelihood sequence estimation .",
    "the m - algorithm keeps track only of the @xmath0 most likely paths at the same depth , throwing away any excess paths . in the m - bcjr equalizer",
    "this idea is applied to the trellis states during the forward recursion . at every level @xmath74 , when all @xmath46 have been computed , the @xmath0 states with the largest forward metrics are retained , and all remaining states are deleted from the trellis ( together with all the branches that lead to or depart from them ) .",
    "the same trellis is then reused in the backward recursion and completion phase .    in @xcite",
    "it was shown that the m - bcjr algorithm performs well when the state reduction ratio @xmath76 is not very large . also , unlike the rs - bcjr algorithm",
    ", it can use the power from all the channel taps . for small @xmath0",
    ", however , the reduced trellis is very sparse , _",
    "i.e. _ , the branch - to - state ratio is much smaller than in the full trellis and there is often a disproportion between the number of branches labeled with @xmath72 and @xmath73 for any @xmath74 and @xmath75 .",
    "these factors reduce the quality of the soft outputs and the convergence performance and may require an alternative way of computing the a posteriori likelihoods ( like the bayesian estimation approach presented in @xcite ) .",
    "finally , the m - bcjr algorithm requires performing a partial sort ( finding the @xmath0 largest elements out of @xmath77 ) at every trellis section , which increases the complexity per state .",
    "in this section we demonstrate how the concept of _ state merging _ present in the rs - bcjr equalizer can be used to enhance the performance of the m - bcjr algorithm .",
    "we call the resulting algortihm the m@xmath1-bcjr algorithm .    during the forward recursion the m@xmath1-bcjr algorithm retains a maximum of @xmath0 states for any time index @xmath74 . unlike the m - bcjr algorithm , however , the excess states are not deleted , but merely merged into some of the surviving states .",
    "this means that none of the branches seen so far are deleted from the trellis , but they are just redirected into a more likely state .",
    "the forward recursion of the algorithm can be described as follows :    1 .",
    "set @xmath78 .",
    "for the initial trellis state @xmath79 , set @xmath80",
    ". also , fix the set of states surviving at depth 1 to be @xmath81 .",
    "2 .   initialize the set of surviving states at depth @xmath82 to an empty set , @xmath83 .",
    "3 .   for every state @xmath35 in the set @xmath84 , and every branch",
    "@xmath85 originating from that state , compute the metric @xmath37 , and add @xmath44 to the set @xmath86 .",
    "4 .   for every state @xmath44 in @xmath86 compute the forward state metric as a sum of @xmath87 over all branches @xmath85 visited in step 3 that end in @xmath44 . 5 .   if the number of states in @xmath86 is no more than @xmath0 , proceed to step 8 .",
    "otherwise continue with step 6 .",
    "determine the @xmath0 states in @xmath86 with the largest value of the forward state metric . remove all remaining states from @xmath86 and put them in a temporary set @xmath88 .",
    "go over all states @xmath89 in the set @xmath88 and perform the following tasks for each of them : * find a state @xmath44 in @xmath86 that differs from @xmath89 by the least number of final @xmath6-tuples @xmath90 .",
    "* redirect all branches ending in @xmath89 to @xmath44 . *",
    "add @xmath91 to the metric @xmath92 .",
    "* delete @xmath89 from the set @xmath88 .",
    "increment @xmath74 by 1 . if @xmath93 , go to step 2 .",
    "otherwise the forward recursion is finished .",
    "the merging of @xmath94 into @xmath35 in step 7 is also illustrated in figure [ fig : trellis2 ] .",
    "the backward recursion and the completion phase are subsequently performed only over states remaining in the sets @xmath84 and only over visited branches ( i.e. , branches for which the metrics @xmath56 were calculated in step 3 ) .     into a surviving state @xmath35 . ]",
    "just as for the m - bcjr , the m@xmath1-bcjr algorithm can use the power from all channel taps and offers full freedom in choosing the number of surviving states @xmath0 . at the same time , the m@xmath1-bcjr never deletes visited branches , and hence it retains the branch density of the full trellis and avoids a disproportion between the number of branches labeled with @xmath72 and @xmath73 . as a result , the soft outputs generated by the m@xmath1-bcjr equalizer ensure good convergence of the iterative receiver .",
    "complexity - wise , the algorithm requires some additional processing per state ( due to step 7 ) and some additional memory per branch ( the ending state must be remembered for each branch ) .",
    "however , if we regard the calculation of the branch metrics @xmath56 as the dominant operation , the complexities of the m - bcjr , rs - bcjr , and m@xmath1-bcjr equalizers are the same for fixed @xmath95 .",
    "to evaluate the performance of the m@xmath1-bcjr equalizer , we considered two turbo - equalization systems .",
    "both systems used a recursive , memory 5 , rate @xmath96 terminated convolutional code as an outer code . the first system used bpsk modulation and a 5-tap channel ( maximum 16 states ) , and a block of 507 information bits ( size 1024 drp @xcite interleaver ) .",
    "the second system used 16qam modulation , but only a 3-tap channel ( maximum 256 states ) , and a block of 2043 information bits ( size 4096 drp interleaver ) .",
    "the remaining parameters and the channel impulse responses are summarized in table [ tbl : scenarios ] .",
    "both systems were simulated with the m@xmath1-bcjr and rs - bcjr equalizers , for several values of @xmath0 and @xmath69 . in each case",
    "we allowed the receiver to perform 6 iterations .",
    "the bit error rates @xmath97 for a range of @xmath98 ( average energy per bit over noise spectral density ) are plotted in figure [ fig : bers ] . to better illustrate the complexity - performance tradeoffs achievable with both algorithms",
    ", we also plotted the number of states @xmath0 or @xmath71 against the @xmath98 needed to achieve certain @xmath97 ( @xmath99 for system 1 and @xmath100 for system 2 ) in figure [ fig : states ] .",
    "the simulations demonstrate the superior performance of the m@xmath1-bcjr equalizer . in scenario 1 , the m@xmath1-bcjr equalizer with @xmath101 states outperforms the rs - bcjr with @xmath102 states by 0.1 db for @xmath97 below @xmath99 .",
    "when both algorithms use @xmath103 states , the m@xmath1-bcjr equalizer offers a 0.7 db gain compared to the rs - bcjr . in scenario 2 ,",
    "the m@xmath1-bcjr with @xmath104 states achieves almost a 3 db gain over the rs - bcjr with the same number of states .",
    ".simulated turbo - equalization scenarios .",
    "[ cols=\"^,^,^\",options=\"header \" , ]     a )    b )    a )    b )",
    "we have examined the problem of complexity reduciton in turbo equalization for systems with large constellation sizes and/or long channel impulse responses .",
    "we have defined the operation of merging one state into another and used it to give an alternative interpretation of the rs - bcjr algorithm .",
    "finally we modified the m - bcjr algorithm , replacing the deletion of excess states by the merging of these states into the surviving states .",
    "the resulting algorithm , called the m@xmath1-bcjr algorithm , was shown to generate reduced - complexity trellises more suitable for siso equalization than those obtained by the rs - bcjr and m - bcjr algorithms .",
    "simulation results demonstrated very good performance for turbo - equalization systems employing the m@xmath1-bcjr , exceeding that of the rs - bcjr even with much smaller complexities .",
    "s.  benedetto , d.  divsalar , g.  montorsi , and f.  pollara , `` serial concatenation of interleaved codes : performance analysis , design , and iterative decoding , '' _",
    "ieee  trans .  inform .",
    "theory _ , vol .",
    "44 , pp .",
    "909926,may 1998 .",
    "k.  r.  narayanan , r.  v.  tamma , e.  kurtas , and x.  yang , `` performance limits of turbo equalization using m - bcjr algorithm , '' in _ 3rd international symposium on turbo codes _ , enst de bretagne , sept ."
  ],
  "abstract_text": [
    "<S> in this paper we propose a new soft - input soft - output equalization algorithm , offering very good performance / complexity tradeoffs . </S>",
    "<S> it follows the structure of the bcjr algorithm , but dynamically constructs a simplified trellis during the forward recursion . in each trellis section , only the @xmath0 states with the strongest forward metric are preserved , similar to the m - bcjr algorithm . </S>",
    "<S> unlike the m - bcjr , however , the remaining states are not deleted , but rather merged into the surviving states . </S>",
    "<S> the new algorithm compares favorably with the reduced - state bcjr algorithm , offering better performance and more flexibility , particularly for systems with higher order modulations . </S>"
  ]
}