{
  "article_text": [
    "during the last two years , the sparse approximation theory benefited from a rapid development of methods based on the linear programming .",
    "the idea was to relax a sparse recovery problem to a convex optimization problem .",
    "the convex problem can be further be rendered as a linear program , and analyzed with all available methods of linear programming .",
    "convex relaxation of sparse recovery problems can be traced back in its rudimentary form to mid - seventies ; references to its early history can be found in @xcite . with the development of fast methods of linear programming in the eighties ,",
    "the idea of convex relaxation became truly promising .",
    "it was put forward most enthusiastically and successfully by donoho and his collaborators since the late eighties , starting from the seminal paper @xcite ( see theorem  8 , attributed there to logan , and theorem  9 ) .",
    "there is extensive work being carried out , both in theory and in practice , based on the convex relaxation @xcite .",
    "to have theoretical guarantees for the convex relaxation method , one needs to show that _ the sparse approximation problem is equivalent to its convex relaxation_. proving this presents a mathematical challenge .",
    "known theoretical guarantees work only for random measurements ( e.g. random gaussian and fourier measurements ) . even when there is a theoretical guarantee , it involves intractable or very large constants , far worse than in the observed practical performances .    in this paper",
    ", we substantially improve best known theoretical guarantees for random gaussian and fourier ( and non - harmonic fourier ) measurements .",
    "for the first time , we are able to prove guarantees with reasonable constants ( although only for gaussian measurements ) . our proofs are based on methods of geometric functional analysis , such methods were recently successfully used for related problems @xcite , @xcite . as a result , our proofs are reasonably short ( and hopefully , transparent ) .    in section  [ s :",
    "relax ] , we state the sparse reconstruction problem and describe the convex relaxation method .",
    "a guarantee of its correctness is a very general _ restricted isometry condition _ on the measurement ensemble , due to candes and tao ( @xcite , see @xcite ) . under this condition ,",
    "the reconstruction problem with respect to these measurements is equivalent to its convex relaxation . in sections  [ s : fourier ] and [ s :",
    "gauss ] , we improve best known guarantees for the sparse reconstruction from random fourier ( and non - harmonic fourier ) measurements and gaussian measurements ( theorem  [ fourier rec ] and [ gaussian rec ] respectively ) .",
    "we want to reconstruct an unknown signal @xmath9 from linear measurements @xmath10 , where @xmath11 is some known @xmath12 matrix , called the _",
    "measurement matrix_. in the interesting case @xmath13 , the problem is underdetermined , and we are interested in the sparsest solution .",
    "we can state this as the optimization problem @xmath14 where @xmath15 is the number of nonzero coefficients of @xmath0 .",
    "this problem is highly non - convex .",
    "so we will consider its _ convex relaxation _ : @xmath16 where @xmath17 denotes the @xmath18 norm throughout this paper , @xmath19 .",
    "problem can be classically reformulated as the _ linear program _ @xmath20 which can be efficiently solved using general or special methods of linear programming .",
    "then the main question is :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ under what conditions on @xmath11 are problems and equivalent ? _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in this paper , we will be interested in the _ exact reconstruction _",
    ", i.e. we expect that the solutions to and are equal to each other and to @xmath0 .",
    "results for approximate reconstruction can be derived as consequences , see @xcite .    for exact reconstruction to be possible at all",
    ", one has to assume that the signal @xmath0 is @xmath2-sparse , that is @xmath21 , and that the number of measurements @xmath22 has to be at least twice the sparsity @xmath2 .",
    "our goal will be to find sufficient conditions ( guarantees ) for the exact reconstruction .",
    "the number of measurements @xmath1 should be kept as small as possible . intuitively , the number of measurements should be of the order of @xmath2 , which is the ` true ' dimension of @xmath0 , rather than the nominal dimension @xmath3 .",
    "various results that appeared over the last two years demonstrate that many natural measurement matrices @xmath11 yield exact reconstruction , with the number of measurements @xmath23 , see @xcite . in sections",
    "[ s : fourier ] and [ s : gauss ] , we improve best known estimates on @xmath24 for fourier ( and , more generally , nonharmonic fourier ) and gaussian matrices respectively .    a general sufficient condition for exact reconstruction is the _ restricted isometry condition _ on @xmath11 , due to candes and tao ( @xcite , see @xcite ) .",
    "it roughly says that the matrix @xmath11 acts as an almost isometry on all @xmath25-sparse vectors .",
    "precisely , we define the restricted isometry constant @xmath26 to be the smallest positive number such that the inequality @xmath27 holds for some number @xmath28 and for all @xmath29 and all subsets @xmath30 of size @xmath31 , where @xmath32 denotes the @xmath33 matrix that consists of the columns of @xmath11 indexed by @xmath34 . the following theorem is due to candes and tao ( @xcite , see @xcite ) .",
    "[ ct ] _ let @xmath11 be a measurement matrix whose restricted isometry constant satisfies @xmath35 let @xmath0 be an @xmath2-sparse signal . then the solution to the linear program is unique and is equal to @xmath0 .",
    "_    this theorem says that under the restricted isometry condition on the measurement matrix @xmath11 , the reconstruction problem is equivalent to its convex relaxation for all @xmath2-sparse functions @xmath0 .",
    "a problem with the use of theorem  [ ct ] is that the restricted isometry condition is usually difficult to check .",
    "indeed , the number of sets @xmath34 involved in this condition is exponential in @xmath2 . as a result",
    ", no explicit construction of a measurement matrix is presently known that obeys the restricted isometry condition .",
    "all known constructions of measurement matrices are randomized .",
    "our goal will be to reconstruct an @xmath2-sparse signal @xmath9 from its discrete fourier transform evaluated at @xmath22 points .",
    "these points will be chosen at random and uniformly in @xmath36 , forming a set @xmath37 .",
    "the discrete fourier transform @xmath38 is defined by the dft matrix @xmath39 with entries @xmath40 so , our measurement matrix @xmath11 is the submatrix of @xmath39 consisting of random rows ( with indices in @xmath37 ) . to be able to apply theorem [ ct ] ,",
    "it is enough to check that the restricted isometry condition holds for the random matrix @xmath11 with high probability .",
    "the problem is ",
    "what is the smallest number of rows @xmath1 of @xmath11 for which this holds ? with that number ,",
    "theorem [ ct ] immediately implies the following reconstruction theorem for fourier measurements :    [ fourier rec ] _ a random set @xmath41 of size @xmath1 satisfies the following with high probability .",
    "let @xmath0 be an @xmath2-sparse signal in @xmath42 .",
    "then @xmath0 can be exactly reconstructed from the values of its fourier transform on @xmath37 as a solution to the linear program @xmath43 _    the central remaining problem , what is the smallest value of @xmath1 , is still open .",
    "the best known estimate is due to candes and tao @xcite : @xmath44 the conjectured optimal estimate would be @xmath45 , which is known to hold for nonuniveral measuremets , i.e. for _ one _ sparse signal @xmath0 and for a random set @xmath37 @xcite .    in this paper , we improve on the best known bound :    [ sample size ] _ theorem [ fourier rec ] holds with @xmath46 _    the dependence on @xmath3 is thus optimal within the @xmath7 factor and the dependence on @xmath2 is optimal within the @xmath8 factor .",
    "so , our estimate is especially good for small @xmath2 , but our estimate always yields @xmath47 .",
    "our results hold for transforms more general than the discrete fourier transform .",
    "one can replace the dft matrix @xmath39 by any orthogonal matrix with entries of magnitude @xmath48 .",
    "theorems  [ fourier rec ] and [ sample size ] hold for any such matrix .    in the remainder of this section , we prove theorem [ sample size ] .",
    "let @xmath37 be a random subset of @xmath49 of size @xmath24 .",
    "recall that the measurement matrix @xmath11 that consists of the rows of @xmath39 whose indices are in @xmath37 ) . in view of theorem [ ri ]",
    ", it suffices to prove that the restricted isometry constant @xmath26 of @xmath11 satisfies @xmath50 whenever @xmath51 where @xmath52 is arbitrary , and @xmath53 is some absolute constant .",
    "let @xmath54 denote the rows of the matrix @xmath39 .",
    "dualizing we see that is equivalent to the following inequality : @xmath55 with @xmath56 . here and thereafter , for vectors @xmath57 the tensor @xmath58 is the rank - one linear operator given by @xmath59 , where @xmath60 is the canonical inner product on @xmath42 .",
    "the notation @xmath61 stands for the restriction of a vector @xmath29 on its coordinates in the set @xmath34 .",
    "the operator @xmath62 in is the identity on @xmath63 , and the norm is the operator norm for operators on @xmath64 .",
    "the orthogonality of @xmath39 can be expressed as @xmath65 .",
    "we shall re - normalize the vectors @xmath66 , letting @xmath67 now we have @xmath68 for all @xmath69 .",
    "the proof has now reduced to the following probabilistic statement , which we interpret as a law of large numbers for random operators .",
    "[ lln ] _ let @xmath70 be vectors in @xmath42 with uniformly bounded entries : @xmath71 for all @xmath69 .",
    "assume that @xmath72 let @xmath37 be a random subset of @xmath73 of size @xmath24 .",
    "then @xmath74 provided @xmath24 satisfies ( with constant @xmath53 that may depend on @xmath75 ) .",
    "_    theorem [ lln ] is proved by the techniques developed in probability in banach spaces .",
    "the general roadmap is similar ton @xcite , @xcite .",
    "we first observe that @xmath76 so the random operator whose norm we estimate in has mean zero .",
    "then the standard symmetrization ( see @xcite lemma 6.3 ) implies that the left - hand side of does not exceed @xmath77 where @xmath78 are independent symmetric @xmath79-valued random variables ; also ( jointly ) independent of @xmath37 .",
    "then the conclusion of theorem [ lln ] will be easily deduced from the following lemma .",
    "[ lemma norm small ] let @xmath80 , @xmath81 , be vectors in @xmath42 with uniformly bounded entries , @xmath71 for all @xmath69 .",
    "then @xmath82 where @xmath83    let us show how lemma [ lemma norm small ] implies theorem [ lln ] .",
    "we first condition on a choice of @xmath37 and apply lemma [ lemma norm small ] for @xmath84 , @xmath85 .",
    "then we take the expectation with respect to @xmath37 .",
    "we then use the a consequence of hlder inequality , @xmath86 and the triangle inequality . let us denote the left hand side of by @xmath87 .",
    "we obtain : @xmath88 it follows that @xmath89 , provided that @xmath90 .",
    "theorem  [ lln ] now follows from our choice of @xmath22 .",
    "hence it is only left to prove lemma [ lemma norm small ] . throughout the proof , @xmath91 and",
    "@xmath92 denote the unit ball of the norm @xmath93 on @xmath42 . to this end , we first replace bernoulli r.v.s @xmath94 by standard independent normal random variables @xmath95 , using a comparison principle ( inequality ( 4.8 ) in @xcite ) .",
    "then our problem becomes to bound the gaussian process , indexed by the union of the unit euclidean balls @xmath96 in @xmath63 for all subsets @xmath97 of @xmath73 of size at most @xmath2 .",
    "we apply dudley s inequality ( theorem 11.17 in @xcite ) , which is a general upper bound on gaussian processes .",
    "let us denote the left hand side of by @xmath98 .",
    "we obtain : @xmath99 where @xmath100 denotes the minimal number of balls of radius @xmath101 in metric @xmath102 centered in points of @xmath103 , needed to cover the set @xmath103 . the metric @xmath102 in dudley s inequality",
    "is defined by the gaussian process , and in our case it is @xmath104^\\frac{1}{2 } \\\\ & \\le \\big [ \\sum_{i=1}^k \\big ( \\ < x_i , x\\ > + \\",
    "< x_i , y\\ > \\big)^2     \\big]^\\frac{1}{2 }     \\",
    "\\max_{i \\le k } |\\ < x_i , x - y \\",
    "> | \\\\ & \\le 2 \\max_{\\substack{|t| \\le r \\\\ z \\in b_2^t } }    \\big [ \\sum_{i=1}^k \\",
    "< x_i , z\\ > ^2 \\big]^\\frac{1}{2 }",
    "\\le k } |\\ < x_i , x - y \\",
    "> | \\\\ & = 2 r \\max_{i \\le k } |\\ < x_i , x - y \\ > |,\\end{aligned}\\ ] ] where @xmath105 hence @xmath106 here @xmath107 we will use containments @xmath108 where @xmath109 denotes the unit ball of the norm @xmath110 .",
    "the second containment follows from the uniform boundedness of @xmath111",
    ". we can thus replace @xmath112 in by @xmath113 . comparing to the right hand side of",
    "we see that , in order to complete the proof of lemma [ lemma norm small ] , it suffices to show that @xmath114 with @xmath115 . to this end",
    ", we will estimate the covering numbers in this integral in two different ways . for big @xmath101",
    ", we will just use the second containment in , which allows us to replace @xmath113 by @xmath116 .",
    "[ lemma covering numbers ] let @xmath80 , @xmath81 , be vectors as in lemma [ lemma norm small ] .",
    "then for all @xmath117 we have @xmath118 where @xmath119 .",
    "we use the empirical method of maurey . fix a vector @xmath120 .",
    "define a random vector @xmath121 that takes values @xmath122 with probability @xmath123 each , @xmath124 ( all entries of that vector are zero except @xmath69-th ) .",
    "here @xmath125 , whenever @xmath126 , and @xmath127 otherwise .",
    "note that @xmath128 .",
    "let @xmath129 be independent copies of @xmath103 .",
    "using symmetrization as before , we see that @xmath130 now we condition on a choice of @xmath131 and take the expectation with respect to random signs @xmath132 . using comparison to gaussian variables as before ,",
    "we obtain @xmath133 for each @xmath69 , @xmath134 is a gaussian random variable with zero mean and with variance @xmath135 since @xmath136 . using a simple bound on the maximum of gaussian random variables ( see ( 3.13 ) in @xcite ) ,",
    "we obtain @xmath137 taking the expectation with respect to @xmath131 we obtain @xmath138 with the choice of @xmath139 made in the statement of the lemma , we conclude that @xmath140 we have shown that for every @xmath120 , there exists a @xmath141 of the form @xmath142 such that @xmath143 .",
    "each @xmath144 takes @xmath145 values , so @xmath146 takes @xmath147 values .",
    "hence @xmath116 can be covered by at @xmath147 balls of norm @xmath110 of radius @xmath101 .",
    "a standard argument shows that we can assume that these balls are centered in points of @xmath116 .",
    "this completes the proof of lemma  [ lemma covering numbers ] .    for small @xmath101",
    ", we will use a simple volumetric estimate .",
    "the diameter of @xmath148 considered as a set in @xmath42 is at most @xmath75 with respect to the norm @xmath110 ( this was stated as the last containment in ) .",
    "it follows that @xmath149 for all @xmath150 , see ( 5.7 ) in @xcite .",
    "the set @xmath113 consists of @xmath151 balls of form @xmath152 , thus @xmath153    now we combine the estimate of the covering number @xmath154 of lemma 3.6 , and the volumetric estimate , to bound the integral in . using stirling s approximation",
    ", we see that @xmath155 .",
    "= : n_1(u ) , \\\\",
    "n(u ) \\le \\frac{c_{10}}{u } \\sqrt{\\log k } \\sqrt{\\log n } = : n_2(u),\\end{gathered}\\ ] ] where @xmath157 .",
    "then we bound the integral in as @xmath158 \\\\    & \\ \\",
    "+ c_{11 } \\log(1/a ) \\sqrt{\\log k } \\sqrt{\\log n},\\end{aligned}\\ ] ] where @xmath159 .",
    "choosing @xmath160 , we conclude that the integral in is at most @xmath161 this proves , which completes the proof of lemma [ lemma norm small ] and thus of theorems  [ lln ] and [ sample size ] .",
    "our goal will be to reconstruct an @xmath2-sparse signal @xmath162 from @xmath22 gaussian measurements .",
    "these are given by @xmath163 , where @xmath11 is a @xmath12 random matrix ( ` gaussian matrix ' in the sequel ) , whose entries are independent @xmath164 random variables",
    ". the reconstruction will be achieved by solving the linear program .",
    "the problem again is to find the smallest number of measurements @xmath1 for which , with high probability , we have an exact reconstruciton of every @xmath2-sparse signal @xmath0 from its measurements @xmath165 ? it has recently been shown in @xcite that @xmath166 and was extended in @xcite to sub - gaussian measurements .",
    "this is asymptotically optimal .",
    "however , the constant factor implicit in has not been known ; previous proofs of yield unreasonably weak constants ( of order @xmath167 and higher ) .",
    "in fact , _ there has not been known any theoretical guarantees with reasonable constants for linear programming based reconstructions_. so , there is presently a gap between theoretical guarantees and good practical performance of reconstruction ( see e.g. @xcite ) . here",
    "we shall prove a first practically reasonable guarantee of the form : @xmath168 ( 1 + o(1 ) ) ,   \\\\",
    "c_1 = 6 + 4 \\sqrt{2 } \\approx 11.66 , \\ \\",
    "c_2 = 1.5 .",
    "\\notag\\end{aligned}\\ ] ]    [ gaussian rec ] _ a @xmath12 gaussian matrix @xmath11 with @xmath169 satisfies the following with probability @xmath170 let @xmath0 be an @xmath2-sparse signal in @xmath171 . then @xmath0 can be exactly reconstructed from the measurements @xmath165 as a unique solution to the linear program . _",
    "our proof of theorem [ gaussian rec ] is direct , we will not use the restricted isometry theorem [ ct ] .",
    "the first part of this argument follows a general method of @xcite .",
    "one interprets the exact reconstruction as the fact that the ( random ) kernel of @xmath11 misses the cone generated by the ( shifted ) ball of @xmath172 .",
    "then one embeds the cone in a universal set @xmath173 , which is easier to handle , and proves that the random subspace does not intersect @xmath173 .",
    "however , to obtain good constants as in , we will need to ( a ) improve the constant of embedding into @xmath173 from @xcite , and ( b ) use gordon s escape through the mesh theorem @xcite , which is tight in terms of constants . in gordon",
    "s theorem , one measures the size of a set @xmath174 in @xmath171 by its _",
    "gaussian width _",
    "@xmath175 where @xmath176 is a random vector in @xmath171 whose components are independent @xmath164 random variables ( gaussian vector ) .",
    "the following is gordon s theorem @xcite .",
    "[ escape ] _ let @xmath174 be a subset of the unit euclidean sphere @xmath177 in @xmath171 .",
    "let @xmath178 be a random @xmath179-dimensional subspace of @xmath171 , distributed uniformly in the grassmanian with respect to the haar measure .",
    "assume that @xmath180 then @xmath181 with probability at least @xmath182",
    "_    we will now prove theorem [ gaussian rec ] .",
    "first note that the function @xmath0 is the unique solution of if and only if @xmath127 is the unique solution of the problem @xmath183 @xmath178 is a @xmath179-dimensional subspace of @xmath171 . due to the rotation invariance of the gaussian random vectors",
    ", @xmath178 is distributed uniformly in the grassmanian @xmath184 of @xmath179-dimensional subspaces of @xmath171 , with respect to the haar measure .",
    "now , @xmath127 is the unique solution to if and only if @xmath127 is the unique metric projection of @xmath0 onto the subspace @xmath178 in the norm @xmath185 .",
    "this in turn is equivalent to the fact that @xmath127 is the unique contact point between the subspace @xmath178 and the ball of the norm @xmath185 centered at @xmath0 : @xmath186 ( recall that @xmath91 is the unit ball of the norm @xmath93 . )",
    "let @xmath187 be the cone in @xmath171 generated by the set @xmath188 ( the cone of a set @xmath189 is defined as @xmath190 ) .",
    "then the statement that holds for all @xmath2-sparse functions @xmath0 is clearly equivalent to @xmath191 we can represent the cone @xmath187 as follows .",
    "let @xmath192 then @xmath193 we will now bound the cone @xmath187 by a universal set , which does not depend on @xmath0 .",
    "fix a point @xmath197 .",
    "we have @xmath198 the norm @xmath199 on @xmath171 whose unit ball is @xmath173 can be computed as @xmath200 where @xmath201 , @xmath202 , for @xmath203 , @xmath204 , and @xmath205 is a non - decreasing rearrangement of the sequence @xmath206 .",
    "set @xmath207 . since @xmath208 , we have @xmath209 .",
    "hence , for any @xmath210 there exists a set @xmath211 , which consists of @xmath212 elements and such that @xmath213 .",
    "therefore , @xmath29 can be represented as @xmath214 so that @xmath215 , @xmath216 , @xmath217 , @xmath218 .",
    "set @xmath219 then the above argument shows that @xmath220    the maximum of @xmath221 over @xmath222 is attained at the extreme points of the sets @xmath223 , which have the form @xmath214 , where @xmath224 , and @xmath225 has coordinates 0 and @xmath226 with @xmath2 non - zero coordinates . notice that since @xmath227 , @xmath228 .",
    "thus , for any extreme point @xmath29 of @xmath229 , @xmath230 the second inequality follows from @xmath231 and @xmath232 .",
    "this completes the proof of the lemma .        by definition ,",
    "@xmath234 let @xmath235 be a number to be chosen later .",
    "by hlder s inequality , we have @xmath236 by the stirling s formula , @xmath237 therefore , @xmath238 now set @xmath239",
    ". then @xmath240    to deduce we define @xmath241 where the union is over all @xmath2-sparse functions @xmath0 .",
    "then is equivalent to @xmath242 lemma  [",
    "l : inclusion ] implies that @xmath243 .",
    "then by lemma  [ l : mean width ] , @xmath244 then follows gordon s theorem  [ escape ] .",
    "this completes the proof of theorem  [ gaussian rec ] ."
  ],
  "abstract_text": [
    "<S> this paper proves best known guarantees for exact reconstruction of a sparse signal @xmath0 from few non - adaptive universal linear measurements . </S>",
    "<S> we consider fourier measurements ( random sample of frequencies of @xmath0 ) and random gaussian measurements . </S>",
    "<S> the method for reconstruction that has recently gained momentum in the sparse approximation theory is to relax this highly non - convex problem to a convex problem , and then solve it as a linear program . </S>",
    "<S> what are best guarantees for the reconstruction problem to be equivalent to its convex relaxation is an open question . </S>",
    "<S> recent work shows that the number of measurements @xmath1 needed to exactly reconstruct any @xmath2-sparse signal @xmath0 of length @xmath3 from its linear measurements with convex relaxation is usually @xmath4 . </S>",
    "<S> however , known guarantees involve huge constants , in spite of very good performance of the algorithms in practice . in attempt to reconcile theory with practice , </S>",
    "<S> we prove the first guarantees for universal measurements ( i.e. which work for all sparse functions ) with reasonable constants . for gaussian measurements , @xmath5 $ ] , which is optimal up to constants . for fourier measurements , we prove the best known bound @xmath6 , which is optimal within the @xmath7 and @xmath8 factors . </S>",
    "<S> our arguments are based on the technique of geometric functional analysis and probability in banach spaces . </S>"
  ]
}