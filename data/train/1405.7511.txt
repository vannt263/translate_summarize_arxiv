{
  "article_text": [
    "suppose that we are interested in an @xmath0 by @xmath1 matrix @xmath2 , which is thought to be either exactly or approximately of low rank , but we only observe a single noisy @xmath0-by-@xmath1 matrix @xmath3 , obeying @xmath4 ; the noise matrix @xmath5 has independent , identically distributed entries with zero mean and unit variance .",
    "we choose a loss function @xmath6 and wish to recover the matrix @xmath2 with some bound on the risk @xmath7 , where @xmath8 is our estimated value of @xmath2 .",
    "for example , when choosing the square frobenius loss @xmath9 where @xmath2 and @xmath8 are @xmath0-by-@xmath1 matrices , we would like to find an estimator @xmath8 with small mean square error ( mse ) .",
    "the default technique for estimating a low rank matrix in noise is the _ truncated svd _ ( tsvd )",
    "@xcite : write @xmath10 for the singular value decomposition of the data matrix @xmath3 , where @xmath11 and @xmath12 , @xmath13 are the left and right singular vectors of @xmath3 corresponding to the singular value @xmath14 .",
    "the tsvd estimator is @xmath15 where @xmath16 , assumed known , and @xmath17 .",
    "being the best approximation of rank @xmath18 to the data in the least squares sense @xcite , and therefore the maximum likelihood estimator when @xmath5 has gaussian entries , the tsvd is arguably as ubiquitous in science and engineering as linear regression @xcite .",
    "the tsvd estimator shrinks to zero some of the data singular values , while leaving others untouched . more generally , for any specific choice of scalar nonlinearity @xmath19 , also known as a shrinker , there is a corresponding _ singular value shrinkage _",
    "estimator @xmath20 given by @xmath21    for scalar and vector denoising , univariate shrinkage rules have proved to be simple and practical denoising methods , with near - optimal performance guarantees under various performance measures @xcite . shrinkage makes sense for singular values , too : presumably , the observed singular values @xmath22 are `` inflated '' by the noise , and applying a carefully chosen shrinkage function , one can obtain a good estimate of the original signal @xmath2 .",
    "is there a simple , natural shrinkage nonlinearity for singular values ?",
    "if there is a simple answer to this question , surely it depends on the loss function @xmath23 and on specific assumptions on the signal matrix @xmath2 .    in @xcite",
    "we have performed a narrow investigation that focused on hard and soft thresholding of singular values under the frobenius loss .",
    "we adopted a simple asymptotic framework that models the situation where @xmath2 is low - rank , originally proposed by shabalin and nobel @xcite and inspired by johnstone s spiked covariance model @xcite . in this framework , the signal matrix dimensions @xmath24 and @xmath1 both to infinity , such that their ratio converges to an asymptotic aspect ratio : @xmath25 , with @xmath26 , while the column span of the signal matrix remains fixed .",
    "building on a recent probabilistic analysis of this framework @xcite we have discovered that , in this framework , there is an _ asymptotically unique admissible _ threshold for singular values , in the sense that it offers equal or better asymptotic mse to that of any other threshold , across all possible signal configurations .",
    "the main discovery reported here is that this phenomenon is in fact much more general : in this asymptotic framework , which models low - rank matrices observed in white noise , for each of a variety of loss functions , there exists a single _ asymptotically unique admissible _ shrinkage nonlinearity , in the sense that it offers equal or better asymptotic loss to that of any other shrinkage nonlinearity , across all possible signal configurations . in other words , once the loss function has been decided , in a definite asymptotic sense , there is a single rational choice of shrinkage nonlinearity .      in this paper , we develop a general method for finding the optimal shrinkage nonlinearity for a variety of loss functions .",
    "we explicitly work out the optimal shrinkage formula for the frobenius norm loss , the nuclear norm loss , and the operator norm loss :    [ [ frobenius - norm - loss . ] ] frobenius norm loss .",
    "+ + + + + + + + + + + + + + + + + + + +    the optimal nonlinearity for the frobenius norm loss is @xmath27 in the asymptotically square case @xmath28 this reduces to @xmath29    [ [ operator - norm - loss ] ] operator norm loss + + + + + + + + + + + + + + + + + +    the operator norm loss is @xmath30 . here , @xmath31 is the operator norm of @xmath32 , considered as a linear operator @xmath33 , namely , its maximal singular value .",
    "the operator norm loss has mostly been studied not in the context of matrix denoising but in the closely related context of covariance estimation @xcite .",
    "define @xmath34 then the optimal nonlinearity for operator loss is just @xmath35    [ [ nuclear - norm - loss . ] ] nuclear norm loss .",
    "+ + + + + + + + + + + + + + + + + +    the nuclear norm loss is @xmath36 , where @xmath37 is the nuclear norm of the matrix @xmath32 , namely the sum of its singular values .",
    "see @xcite and references within for discussion of the nuclear norm and more generally of schatten-@xmath38 norms as losses for matrix estimation .",
    "the optimal nonlinearity for nuclear norm loss is    @xmath39    where @xmath40 is given in .",
    "these nonlinearities are shown in figure [ shrinkers : fig ] .",
    "note that the formulas above are calibrated for the natural noise level @xmath41 ; see section [ noise : sec ] below for usage in known noise level @xmath42 or unknown noise level .    .",
    "all shrinkers are zero on @xmath43 $ ] ( not shown ) and asymptote to the identity @xmath44 as @xmath45 .",
    "curves jittered in the vertical axis to avoid overlap . ,",
    "column vectors are denoted by boldface lowercase letters , such as @xmath46 , their transpose is @xmath47 and their @xmath48-th coordinate is @xmath49 .",
    "the euclidean inner product and norm on vectors are denoted by @xmath50 and @xmath51 , respectively .",
    "matrices are denotes by uppercase letters , such as @xmath2 , their transpose is @xmath52 and their @xmath53-th entry is @xmath54 .",
    "@xmath55 denotes the space of real @xmath0-by-@xmath1 matrices , @xmath56 denotes the hilbert-schmidt inner product and @xmath57 denotes the corresponding frobenius norm on @xmath55 .",
    "@xmath58 and @xmath59 denote the nuclear norm ( sum of singular values ) and operator norm ( maximal singular value ) of @xmath2 , respectively . for simplicity",
    "we only consider @xmath60 .",
    "we denote matrix denoisers , or estimators , by @xmath61 . the symbols @xmath62 and @xmath63 denote almost sure convergence and equality of a.s .",
    "limits , respectively .",
    "we use `` fat '' svd of a matrix @xmath64 with @xmath60 , that is , when writing @xmath65",
    "we mean that @xmath66 , @xmath67 , and @xmath68 . symbols without tilde such as @xmath69",
    "are associated with left singular vectors , while symbols with tilde such as @xmath70 are associated with right singular vectors .",
    "by @xmath71 we mean the @xmath0-by-@xmath1 matrix whose main diagonal is @xmath72 , with @xmath1 implicit in the notation and inferred from context .      in the general model @xmath4 ,",
    "the noise level in the singular values of @xmath3 is @xmath73 . instead of specifying a different shrinkage rule that depends on the matrix size @xmath1",
    ", we calibrate our shrinkage rules to the `` natural '' model @xmath74 . in this convention",
    ", shrinkage rules stay the same for every value of @xmath1 , and we conveniently abuse notation by writing @xmath20 as in for any @xmath75 , keeping @xmath0 and @xmath1 implicit . to apply any denoiser @xmath8 below to data from the general model @xmath4 , use the denoiser @xmath76    throughout the text",
    ", we use @xmath20 to denote singular value shrinker calibrated for noise level @xmath77 . in section [ noise : sec ] below we provide a recipe for applying any denoiser @xmath20 calibrated for noise level @xmath41 for data in the presence of unknown noise level .      in this paper , we consider a sequence of increasingly larger denoising problems @xmath78 with @xmath79 , satisfying the following assumptions :    1 .",
    "_ invariant white noise : _ the entries of @xmath80 are i.i.d samples from a distribution with zero mean , unit variance and finite fourth moment . to simplify the formal statement of our results ,",
    "we assume that this distribution is _ orthogonally invariant _ in the sense that @xmath80 follows the same distribution as @xmath81 , for every orthogonal @xmath82 and @xmath83 .",
    "this is the case , for example , when the entries of @xmath80 are gaussian . in section [ general_noise : sec ]",
    "we revisit this restriction and discuss general ( not necessarily invariant ) white noise .",
    "fixed signal column span @xmath84 : _ let the rank @xmath85 be fixed and choose a vector @xmath86 with coordinates @xmath87 such that @xmath88 .",
    "assume that for all @xmath1 , @xmath89 is an arbitrary singular value decomposition of @xmath90 , where @xmath91 and @xmath92 .",
    "asymptotic aspect ratio @xmath93 : _ the sequence @xmath94 is such that @xmath95 . to simplify our formulas , we assume that @xmath96 .    [ [ remarks . ] ] remarks .",
    "+ + + + + + + +    * while the signal rank @xmath97 and nonzero signal singular values @xmath98 are shared by all matrices @xmath90 , the signal left and right singular vectors @xmath99 and @xmath100 are unknown and arbitrary . * the assumption that the signal singular values are non - degenerate ( @xmath101 , @xmath102 ) is not necessary for our results to hold , yet it simplifies the analysis considerably .    [ asyloss : def ] * asymptotic loss . *",
    "let @xmath103 be a family of losses , where each @xmath104 is a loss function obeying @xmath105 .",
    "let @xmath106 be a continuous nonlinearity and consider @xmath20 , the singular value shrinkage denoiser calibrated , as discussed above , for noise level @xmath77 .",
    "let @xmath107 be an increasing sequence such that @xmath108 , implicit in our notation .",
    "define the _ asymptotic loss _ of the shrinker @xmath106 ( with respect to @xmath23 ) at the signal @xmath109 by @xmath110    our results imply that the amse is well - defined as a function of the signal singular values @xmath111 .",
    "[ optshrink : def ] * optimal shrinker . *",
    "if a continuous shrinker @xmath112 satisfies @xmath113 for any other continuous shrinker @xmath106 , any @xmath114 and any @xmath115 , we say that @xmath112 is _ unique asymptotically admissible _",
    "( of simply `` optimal '' ) for the loss sequence @xmath23 .      at first glance",
    ", it seems too much to hope that optimal shrinkers in the sense of definition [ optshrink : def ] even exist .",
    "indeed , existence of an optimal shrinker for a loss family @xmath23 implies that , asymptotically , the decision - theoretic picture is extremely simple and actionable : from the asymptotic loss perspective , there is a single rational choice for shrinker .    in our current terminology , shabalin and nobel @xcite",
    "have effectively conjectured that an optimal shrinker exists for frobenius loss and provided a proof outline .",
    "the estimator they derive can be shown to be equivalent to the optimal shrinker , yet was given in a more complicated form .",
    "( in section [ fro : sec ] we visit the special case of frobenius loss in detail , and prove that is the optimal shrinker . )    our contribution in this paper is as follows .    1 .",
    "we rigorously establish the existence of an optimal shrinker for a variety of loss families , including the popular frobenius , operator and nuclear norm losses .",
    "we provide a framework for finding the optimal shrinkers for a variety of loss families including these popular losses .",
    "as discussed in section [ noise : sec ] , our framework can be applied whether the noise level @xmath42 is known or unknown .",
    "we use our framework to find simple , explicit formulas for the optimal shrinkers for frobenius , operator and nuclear norm losses .    in the related problem of covariance estimation in the spiked covariance model , in collaboration with i. johnstone we identified a similar phenomenon , namely , existence of optimal shrinkage for eigenvalues for the purpose of covariance estimation @xcite .",
    "in the `` null case '' @xmath116 , the empirical distribution of the singular values of @xmath117 famously converge , as @xmath118 , to the quarter - circle distribution if @xmath119 , or to the generalized quarter - circle if @xmath120 @xcite .",
    "this distribution is compactly supported on @xmath121 $ ] , with @xmath122 we say that the singular values of @xmath123 forms a ( generalized ) quarter circle _ bulk _ and call @xmath124 the _ bulk edge_.    expanding seminal results of @xcite and others , recently benaych - georges and nadakuditi @xcite have provided a thorough analysis of a collection of models , which includes the model as a special case . in this section",
    "we summarize some of their results regarding asymptotic behaviour of the model , which are relevant to singular value shrinkage",
    ".     drawn from the model @xmath125 , with @xmath126 and @xmath127 . left : singular values in decreasing order .",
    "right : histogram of the singular values ( note the bulk edge close to @xmath128).,height=240 ]    for @xmath129 , define @xmath130    it turns out that @xmath131 from eq . is the asymptotic location of a data singular value corresponding to a signal singular value @xmath132 , provided @xmath133 .",
    "( note that the function @xmath134 from is the inverse of @xmath131 when @xmath129 , and that @xmath135 . )",
    "similarly , @xmath136 from eq .",
    "@xmath137 from eq . ) is the cosine of the asymptotic angle between the signal left ( resp .",
    "right ) singular vector and the corresponding data left ( resp .",
    "right ) singular vector , provided that the corresponding signal singular value @xmath132 satisfies @xmath133 .    additional notation is required to state these facts formally .",
    "we rewrite the sequence of signal matrices in our asymptotic framework as @xmath138 so that @xmath139 ( resp .",
    "@xmath140 ) is the left ( resp .",
    "right ) singular vector corresponding to the singular value @xmath141 , namely , @xmath48-th column of @xmath99 ( resp .",
    "@xmath142 ) in .",
    "similarly , let @xmath143 be a corresponding sequence of observed matrices in our framework , and write @xmath144 so that @xmath145 ( resp .",
    "@xmath146 ) is the left ( resp .",
    "right ) singular vector corresponding to the singular value @xmath147 .    in this notation ,",
    "@xcite have proved :    [ y - asy : lem ] * asymptotic location of the top @xmath97 data singular values . * for @xmath148 ,",
    "@xmath149    it should be noted that the @xmath150 lower singular values of @xmath123 , converge almost surely in distribution to the same generalized quarter - circle distribution as in the null case ( figure [ bulk : fig ] ) , and moreover that @xmath151    [ inner - asy : lem ] * asymptotic angle between signal and data singular vectors .",
    "* let @xmath152 and assume that @xmath153 is nondegenerate , namely , the value @xmath141 appears only once in @xmath111 .",
    "then @xmath154 and @xmath155 if however @xmath156 , then we have @xmath157",
    "as an introduction to the more general framework developed below , we first examine the frobenius loss case , following the work of shabalin and nobel @xcite .      directly expanding the frobenius matrix norm ,",
    "we obtain :    [ mse : lem ] * frobenius loss of singular value shrinkage . * for any shrinker @xmath158 , we have @xmath159 \\label{mse1:eq } \\\\              & - &                2\\sum_{i , j=1}^r x_i\\eta(y_{n , i } )",
    "{ \\ensuremath{\\langle { \\ensuremath{\\mathbf{u}}}_{n , i}\\,,\\,{\\ensuremath{\\mathbf{v}}}_{n , j } \\rangle}}{\\ensuremath{\\langle { \\ensuremath{\\mathbf{{\\tilde{u}}}}}_{n , i}\\,,\\,{\\ensuremath{\\mathbf{{\\tilde{v}}}}}_{n , j } \\rangle } }                  \\label{mse2:eq } \\\\                  & + &   \\sum_{i = r+1}^{m_n }                  ( \\eta(y_{n , i}))^2 \\label{residual : eq }                 \\end{aligned}\\ ] ]    this implies a lower bound on frobenius loss of any singular value shrinker :    [ lower_bound_mse : cor ] for any shrinker @xmath158 , we have @xmath160 \\label{mse : eq }             -                2\\sum_{i , j=1}^r x_i\\eta(y_{n , i } )                  { \\ensuremath{\\langle { \\ensuremath{\\mathbf{u}}}_{n , i}\\,,\\,{\\ensuremath{\\mathbf{v}}}_{n , j } \\rangle}}{\\ensuremath{\\langle { \\ensuremath{\\mathbf{{\\tilde{u}}}}}_{n , i}\\,,\\,{\\ensuremath{\\mathbf{{\\tilde{v}}}}}_{n , j } \\rangle}}\\ , .",
    "\\end{aligned}\\ ] ]    as @xmath118 , this lower bound on the frobenius loss is governed by three quantities : the asymptotic location of data singular value @xmath147 , the asymptotic angle between the left signal singular vectors and left data singular vector @xmath161 , and asymptotic angle between the right signal singular vectors and right data singular vector @xmath162 ( see also @xcite ) .",
    "combining corollary [ lower_bound_mse : cor ] , lemma [ y - asy : lem ] and lemma [ inner - asy : lem ] we obtain a lower bound for the asymptotic frobenius loss :    for any continuous shrinker @xmath158 , we have @xmath163 where @xmath164 and @xmath165 .      by differentiating the asymptotic lower bound w.r.t @xmath106",
    ", we find that @xmath166 , where @xmath167 . expanding @xmath136 and @xmath137 from eqs . and , we find that @xmath168 is given by .",
    "the singular value shrinker @xmath112 , for which @xmath169 minimizes the asymptotic lower bound , thus becomes a natural candidate for the optimal shrinker for frobenius loss .",
    "indeed , by definition , for @xmath169 the limits of and are the smallest possible . it remains to show that the limit of is the smallest possible .",
    "it is clear from that a necessary condition for a shrinker @xmath106 to be successful , let alone optimal , is that it must set to zero data eigenvalues that do not correspond to signal . with in mind",
    ", we should only consider shrinkers @xmath106 for which @xmath170 for any @xmath171 :    a shrinker @xmath158 is said to _ collapse the bulk to @xmath172 _ if @xmath170 whenever @xmath173 .",
    "the following lemma gives a sufficient condition for a shrinker to achieve the lowest limit possible in the term , namely , for this term to converge to zero .",
    "[ proper : def ] assume that a continuous shrinker @xmath158 collapses the bulk to @xmath172 and satisfies @xmath174 for some @xmath175 and all @xmath176 .",
    "we say that @xmath106 is a _",
    "proper shrinker_.    [ residual : lem ] let @xmath158 be a proper shrinker",
    ". then @xmath177    assume first that @xmath178 .",
    "define @xmath179 .",
    "we may assume that @xmath180 .",
    "let @xmath181 denote the projection on the last @xmath182 coordinates .",
    "let @xmath183 and denote its singular values by @xmath184 .",
    "in other words , @xmath185 are the eigenvalues of @xmath186 and @xmath187 are the eigenvalues of @xmath188 by the cauchy interlacing theorem ( * ? ? ? * theorem 8.1.7 ) , we have @xmath189 , @xmath190 .",
    "it is therefore enough to show @xmath191 since by assumption @xmath106 collapses the bulk to @xmath172 , we have @xmath192 where @xmath193 is such that @xmath194 , @xmath176 .",
    "however , as almost surely the empirical distribution of @xmath195 converges in distribution to the marcenko - pastur density , we have @xmath196    the candidate @xmath112 is clearly a proper shrinker in the sense of definition [ proper : def ] .",
    "lemma [ residual : lem ] thus implies that @xmath112 is the optimal shrinker for frobenius loss .",
    "our main result may be summarized as follows : the basic ingredients that enabled us to find the optimal shrinker for frobenius loss allow us to find the optimal shrinker for each of a variety of loss families . for these loss families , an optimal ( proper ) shrinker exists and",
    "is given by a simple formula .",
    "to get started , let us describe the loss families to which our method applies .    * orthogonally invariant loss . * a loss @xmath6 is _ orthogonally invariant _ if for all @xmath197 we have @xmath198 , for any orthogonal @xmath199 and @xmath200 .    *",
    "regular loss . *",
    "a loss @xmath6 is _ regular _ if for some @xmath201 , all @xmath197 and all @xmath202 , @xmath203 is lipschitz continuous , in each argument , on the set @xmath204 , with lipschitz constant that may depend on @xmath132 .",
    "( here , @xmath205 is the smallest singular value of @xmath32 . )",
    "* decomposable loss family .",
    "* let @xmath206 and let @xmath207 and @xmath208 .",
    "assume that there are matrices @xmath209 , @xmath210 , such that @xmath211 in the sense that @xmath32 and @xmath212 are block - diagonal with blocks @xmath213 and @xmath214 , respectively .",
    "a loss family @xmath215 is _ sum - decomposable _ if , for all @xmath197 and @xmath216 with block diagonal structure as above , @xmath217 similarly , it is _ max - decomposable _ if @xmath218    [ [ examples . ] ] examples .",
    "+ + + + + + + + +    as primary examples , we consider loss families defined in section [ some : subsec ] : the frobenius norm loss @xmath219 , the operator norm loss @xmath220 and the nuclear norm loss @xmath221 .",
    "it is easy to check that : ( i ) any of these losses are orthogonally invariant and regular , and ( ii ) that the families @xmath219 and @xmath221 are sum - decomposable , while the family @xmath220 is max - decomposable .",
    "[ [ section ] ]    our framework for finding optimal shrinkers can now be stated as follows .",
    "[ char : thm ] * characterization of the optimal singular value shrinker . *",
    "let @xmath222 where @xmath136 and @xmath137 are given by eqs . and , and where @xmath223 and @xmath224 .",
    "assume that @xmath225 is a sum- or max- decomposable family of regular and orthogonally invariant losses .",
    "define @xmath226 and suppose that for any @xmath133 there exists a unique minimizer @xmath227 such that @xmath228 is a proper shrinker on @xmath229 .",
    "further suppose that there exists a point @xmath230 such that @xmath231 with @xmath232 define the shrinker @xmath233 where @xmath134 is defined in eq . .",
    "then for any proper shrinker @xmath106 , the asymptotic losses @xmath234 and @xmath235 exist , and @xmath236 for all @xmath114 and all @xmath115 .      before we proceed to prove theorem [ char : thm ] , we review the information it encodes about the problem at hand and its operational meaning .",
    "theorem [ char : thm ] is based on a few simple observations :    * first , if @xmath23 is a sum ( resp .",
    "max ) decomposable family of regular and orthogonally invariant losses , and if @xmath106 is a proper shrinker , then the asymptotic loss @xmath237 at @xmath109 can be written as a sum ( resp . a maximum ) over @xmath97 terms . these terms have identical functional form .",
    "when @xmath153 , these terms have the form @xmath238 , and when @xmath239 , these terms have the form @xmath240 ( resp .",
    "@xmath241 ) . as a result",
    ", one finds that the zero shrinker @xmath242 is necessarily optimal for @xmath243 . for @xmath129 , one",
    "just needs to minimize the loss of a specific @xmath128-by-@xmath128 matrix , namely the function @xmath244 from , to obtain the shrinker @xmath228 of . *",
    "second , the asymptotic loss curve @xmath245 necessarily crosses the asymptotic loss curve of the zero shrinker @xmath246 at a point @xmath247 . * finally , by concatenating the zero shrinker and the shrinker @xmath228 precisely at the point @xmath248 where their asymptotic losses cross , one obtains a shrinker which is continuous ( @xmath249 ) or possibly discontinuous ( @xmath250 ) .",
    "however , this shrinker always has a well - defined asymptotic loss .",
    "this loss dominates the asymptotic loss of any proper shrinker .    for some loss families",
    "@xmath225 , it is possible to find an explicit formula for the optimal shrinker using the following steps :    1 .",
    "write down an explicit expression for the function @xmath251 from . 2 .",
    "explicitly solve for the minimizer @xmath252 from .",
    "write down an explicit expression for the minimum @xmath253 4 .",
    "solve for the crossing point @xmath248 .",
    "5 .   compose @xmath252 with the transformation @xmath134 from to obtain an explicit form of the optimal shrinker @xmath254 from .",
    "in section [ froopnuc : sec ] we offer three examples of this process .",
    "note that even when an explicit analytic formula is not found , these steps are easily implemented numerically .",
    "this allows one to find the value of the optimal shrinker @xmath168 at a desired value @xmath255 to any desired numerical precision .    in the remainder of this section",
    "we describe a sequence of constructions and lemmas leading to the proof of theorem [ char : thm ] .",
    "let us start by considering a fixed signal matrix and noise matrix , without placing them in a sequence . to allow a gentle exposition of the main ideas , we initially make two simplifying assumptions : first , that @xmath178 , namely that @xmath2 is rank-@xmath256 , and second , that @xmath106 shrinks to zero all but the first singular values of @xmath3 , namely , @xmath257 , @xmath258 .",
    "let @xmath64 be a signal matrix and let @xmath259 be a corresponding data matrix .",
    "denote their svd by @xmath260 write @xmath261 for the @xmath0-by-@xmath1 matrix whose entries are all zeros .",
    "the basis pairs @xmath262 and @xmath263 diagonalize @xmath2 and @xmath3 in the sense that @xmath264 where @xmath265 is @xmath48-th standard basis vector in the appropriate dimension .    combining these two pairs",
    ", we are lead to the following `` common '' basis pair , which we will denote by @xmath266 : let @xmath267 denote the orthonormal basis constructed by applying the gram ",
    "schmidt process to the sequence @xmath268 , where @xmath269 is the @xmath48-th column of @xmath270 , namely the @xmath48-th left singular vector of @xmath2 , and similarly , @xmath271 is the @xmath48-th column of @xmath272 .",
    "denote by @xmath273 the matrix whose columns are @xmath274 . repeating this construction for @xmath275 and @xmath276 , let @xmath277 denote the orthonormal basis constructed by applying the gram - schmidt process to the sequence @xmath278 , where @xmath279 is the @xmath48-th column of @xmath270 , namely the @xmath48-th right singular vector of @xmath2 , and similarly , @xmath280 is the @xmath48-th column of @xmath276 .",
    "denote by @xmath281 the matrix whose columns are @xmath282 .",
    "we may assume that @xmath283 lies in the linear span of @xmath284 and @xmath285 , and choose the orientation of @xmath286 so that @xmath287 where @xmath288 and @xmath289 .",
    "similarly , we may choose the orientation of @xmath290 such that @xmath291 where @xmath292 and @xmath293 .",
    "writing now @xmath2 and @xmath20 in our new basis pair @xmath266 we get @xmath294    it is convenient to rewrite this as @xmath295 where @xmath296    thus ,",
    "if @xmath225 is a sum- or max - decomposable family of orthogonally invariant functions , we have @xmath297    we have proved :    [ block1:lem ] let @xmath298 be rank-@xmath256 and assume that @xmath299 and @xmath106 are such that @xmath257 , @xmath258 , where @xmath14 is the @xmath48-th largest singular value of @xmath3 .",
    "let @xmath225 be a sum- or max - decomposable family of orthogonally invariant functions .",
    "then @xmath300 where @xmath301    a similar yet significantly more technical proof gives a similar statement for rank-@xmath97 matrix @xmath2 with non - degenerate singular values ( see @xcite ) :    * a decomposition for the loss .",
    "* let @xmath302 be rank-@xmath97 with @xmath303 , and assume that @xmath299 and @xmath106 are such that @xmath257 , @xmath304 , where @xmath14 is the @xmath48-th largest singular value of @xmath3 .",
    "let @xmath225 be a sum- or max - decomposable family of orthogonally invariant functions .",
    "then @xmath305 if @xmath23 is sum - decomposable , or @xmath306 if @xmath23 is max - decomposable . here , @xmath307 for @xmath308 .      in section [ block : subsec ]",
    "we analyzed a single matrix and shown that , for fixed @xmath0 and @xmath1 , the loss @xmath309 decomposes to `` atomic '' units of the form @xmath310    let us now return to the sequence model @xmath311 and find the limiting value of these `` atomic '' units as @xmath118 , and discover a simple formula for the asymptotic loss @xmath237 .",
    "each of these `` atomic '' units only depend on @xmath14 , the @xmath48-th data singular value , and on @xmath312 ( resp .",
    "@xmath313 ) , the angle between the @xmath48-th left ( resp .",
    "right ) signal and data singular vectors . in the special case of frobenius norm loss",
    ", we have already encountered this phenomenon ( lemma [ mse : lem ] ) , where we have seen that these quantities converge to deterministic functions that depend on @xmath141 , the @xmath48-th signal singular value alone .    for the sequence @xmath311 , recall our notation @xmath314 from and , and define @xmath315 for @xmath308 .",
    "combining lemma [ block1:lem ] , lemma [ y - asy : lem ] and lemma [ inner - asy : lem ] we obtain :    [ block_conv : lem ] let @xmath311 be a matrix sequence in our asymptotic framework with signal singular values @xmath109 .",
    "assume that @xmath316 is a regular loss and @xmath106 is continuous at @xmath317 for some fixed @xmath318 . if @xmath319 then @xmath320 where @xmath321 is given by , while if @xmath239 then @xmath322    as a result , we now obtain the asymptotic loss @xmath323 as a deterministic function of the nonzero population principal values @xmath98",
    "[ char2:lem ] * a formula for the asymptotic loss of a proper shrinker . *",
    "assume that @xmath225 is a sum- or max- decomposable family of regular and orthogonally invariant losses .",
    "extend the definition of @xmath321 from by setting @xmath324 for @xmath325 . if @xmath158 is a proper shrinker , then @xmath326 if @xmath23 is sum - decomposable , or @xmath327 if @xmath23 is max - decomposable .",
    "consider the case @xmath178 .",
    "define @xmath328 .",
    "combining lemma [ block_conv : lem ] and lemma [ block1:lem ] , we have @xmath329 however , @xmath330 the result now follows from lemma [ residual : lem ] combined with the fact that @xmath331 is lipschitz for some @xmath332 .",
    "the final step toward the proof of theorem [ char : thm ] involves the case when the shrinker @xmath106 is given as a special concatenation of two proper shrinkers . even if the two part of @xmath106 do not match , forming a discontinuity , we may still have a formula for the asymptotic loss if the loss functions match .",
    "assume that there exist a point @xmath333 and two shrinkers , @xmath334 and @xmath335 , such that @xmath336 we say that the asymptotic loss functions of @xmath337 and @xmath338 _ cross _ at @xmath339 .",
    "[ char3:lem ] * a formula for the asymptotic loss of a concatenation of two proper shrinkers . *",
    "assume that @xmath225 is a sum- or max- decomposable family of regular and orthogonally invariant losses .",
    "extend the definition of @xmath321 from by setting @xmath324 for @xmath325 .",
    "assume that there exist two shrinkers , @xmath334 and @xmath335 , whose asymptotic loss functions cross at some point @xmath340 .",
    "define @xmath341 then @xmath235 exists and is given by if @xmath23 is sum - decomposable , or if @xmath23 is max - decomposable .",
    "[ [ proof - of - theorem - charthm . ] ] proof of theorem [ char : thm ] .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + +    consider the shrinker @xmath342 . by lemma [ block_conv : lem ]",
    ", @xmath337 dominates any other proper shrinker when @xmath325 . by assumption",
    ", there exists a point @xmath343 such that @xmath337 also dominates any proper shrinker on @xmath344 , and such that @xmath228 dominates any other proper shrinker on @xmath345 . finally , by assumption",
    ", the asymptotic loss functions of @xmath337 and @xmath228 cross at @xmath248 . by lemma [ char3:lem ]",
    ", the concatenated shrinker @xmath112 dominates any proper shrinker on @xmath346 .",
    "theorem [ char : thm ] provides a general recipe for finding optimal singular value shrinkers . to see it in action",
    ", we turn to our three primary examples , namely , the frobenius norm loss , the operator norm loss and the nuclear norm loss . in this section",
    "we find explicit formulas for the optimal singular value shrinkers in each of these losses .",
    "we will need the following lemmas regarding @xmath128-by-@xmath128 matrices ( see @xcite ) :    [ m : lem ] the eigenvalues of any @xmath128-by-@xmath128 matrix @xmath347 with trace @xmath348 and determinant @xmath349 are given by @xmath350    these are the roots of the characteristic polynomial of @xmath347 .",
    "[ sigma_dot : lem ] let @xmath351 be a @xmath128-by-@xmath128 matrix with singular values @xmath352 . define @xmath353 , @xmath354 and @xmath355 . assume that @xmath351 depends on a parameter @xmath106 and let @xmath356 , @xmath357 and @xmath358 denote the derivative of these quantities w.r.t the parameter @xmath106 .",
    "then @xmath359    by lemma [ m : lem ] we have @xmath360 and therefore @xmath361 differentiating and expanding @xmath362 we obtain the relation @xmath363 and the result follows .",
    "let @xmath364 and set @xmath365 and @xmath293 .",
    "define @xmath366 then @xmath367 and the singular values @xmath368 of @xmath351 are given by @xmath369      theorem [ char : thm ] allows us to rediscover the optimal shrinker for frobenius norm loss , which we derived from first principles in section [ fro : sec ] . to this end , observe that by we have @xmath370 to find the optimal shrinker , we solve @xmath371 for @xmath106 and use the fact that @xmath372 .",
    "we find that the minimizer of @xmath373 is @xmath374 .",
    "defining @xmath375 for @xmath376 , we find that the asymptotic loss of @xmath252 and of @xmath242 cross at @xmath250 .",
    "simplifying , where @xmath134 is given by , we find that @xmath377 is given by , and is in fact a proper shrinker . by theorem [ char : thm ] , this is an optimal ( proper ) shrinker .      by , @xmath378",
    "to find the optimal shrinker , we solve @xmath379 for @xmath106 on @xmath380 .",
    "we find that the minimizer of @xmath381 is at @xmath382 . by , the asymptotic loss of @xmath228 is given by @xmath383 , so that the asymptotic loss of @xmath252 and of @xmath242 cross at @xmath250 .",
    "simplifying , we recover that the shrinker @xmath168 in . observe that although this shrinker is discontinuous at @xmath384 , its asymptotic loss @xmath234 exists , and , by theorem [ char : thm ] , dominates any proper shrinker .",
    "[ [ remark . ] ] remark .",
    "+ + + + + + +    the optimal shrinker for operator norm loss @xmath385 simply shrinks the data singular value back to the `` original '' location of its corresponding signal singular value .",
    "a similar phenomenon has been observed in eigenvalue shrinkage for covariance estimation , see @xcite .",
    "again by @xmath386    to find the optimal shrinker , assume first that @xmath132 is such that @xmath387 . by lemma [ sigma_dot : lem ] , with @xmath388 we find that only zero of @xmath389 occurs when @xmath390 , namely at @xmath391 direct calculation using , and shows that the square of the asymptotic loss of @xmath228 is simply @xmath392 , so that the asymptotic loss of @xmath252 and of @xmath242 cross at the unique @xmath248 satisfying @xmath393 . substituting @xmath394 from , @xmath395 from and also @xmath396 and @xmath397 ,",
    "we find @xmath398 recovering the optimal shrinker .",
    "our main results have been formulated and calibrated specifically for the model @xmath399 , where the distribution of the noise matrix @xmath5 is orthogonally invariant . in this section",
    "we extend our main results to include the model @xmath400 , and consider :    1 .",
    "the setting where @xmath42 is either known but does not necessarily equal @xmath77 , or is altogether unknown .",
    "the setting where the noise matrix @xmath5 has i.i.d entries , but its distribution is not necessarily orthogonally invariant .",
    "the results below follow @xcite .",
    "consider an asymptotic framework slightly more general than the one in section [ framework : subsec ] , in which @xmath401 , with @xmath90 and @xmath80 as defined there . in this section",
    "we keep the loss family @xmath23 and the asymptotic aspect ratio @xmath93 fixed and implicit .",
    "we extend definition [ asyloss : def ] and write @xmath402    when the noise level @xmath42 is known , eq . allows us to re - calibrate any nonlinearity @xmath106 , originally calibrated for noise level @xmath77 , to a different noise level . for a nonlinearity @xmath158 , write @xmath403 we clearly have :    if @xmath112 is an optimal shrinker for @xmath311 , namely , @xmath113 for any @xmath114 , any @xmath115 and any continuous nonlinearity @xmath106 , and @xmath404 , then @xmath405 is an optimal shrinker for @xmath406 , namely @xmath407 for any @xmath114 , any @xmath115 and any continuous nonlinearity @xmath106 .",
    "when the noise level @xmath42 is unknown , we are required to estimate it .",
    "see @xcite and references therein for existing literature on this estimation problem .",
    "the method below has been proposed in @xcite .",
    "consider the following robust estimator for the parameter @xmath42 in the model @xmath4 : @xmath408 where @xmath409 is a median singular value of @xmath3 and @xmath410 is the median of the marcenko - pastur distribution , namely , the unique solution in @xmath411 to the equation @xmath412 where @xmath413 . note that the median @xmath410 is not available analytically but can easily be obtained by numerical quadrature .",
    "[ sigmahat : lem ] let @xmath404 . for the sequence @xmath406 in our asymptotic framework ,",
    "@xmath414    let @xmath406 be a sequence in our asymptotic framework and let @xmath112 be an optimal shrinker calibrated for @xmath311 .",
    "then the random sequence of shrinkers @xmath415 converges to the optimal shrinker @xmath405 : @xmath416 consequently , @xmath415 asymptotically achieves optimal performance : @xmath417    in practice , for denoising a matrix @xmath418 , assumed to satisfy @xmath419 , where @xmath2 is low - rank and @xmath5 has i.i.d entries , we have the following approximately optimal singular value shrinkage estimator : @xmath420 when @xmath42 is known , and @xmath421 when @xmath42 is unknown . here , @xmath112 is an optimal shrinker with respect to desired loss family @xmath23 in the natural scaling .",
    "our results were formally stated for the sequence of models of the form @xmath4 , where @xmath2 is a non - random matrix to be estimated , and the entries of @xmath5 are i.i.d samples from a distribution that is orthogonally invariant ( in the sense that the matrix @xmath5 follows the same distribution as @xmath422 , for any orthogonal @xmath423 and @xmath424 ) . while gaussian noise is orthogonally invariant , many common distributions , which one could consider to model white observation noise , are not .",
    "the singular values of a signal matrix @xmath2 are a very widely used measure of the complexity of @xmath2 .",
    "in particular , they capture its rank . one attractive feature of the framework we adopt is that the loss @xmath425 only depends on the signal matrix @xmath2 through its nonzero singular values @xmath111 .",
    "this allows the loss to be directly related to the complexity of the signal @xmath2 .",
    "if the distribution of @xmath5 is not orthogonally invariant , the loss no longer has this property .",
    "this point is discussed extensively in @xcite .    in general white noise , which is not necessarily orthogonally invariant , one can still allow the loss to depend on @xmath2 only through its singular values by placing a prior distribution on @xmath2 and shifting to a model where it is a random , instead of a fixed , matrix .",
    "specifically , consider an alternative asymptotic framework to the one in section [ framework : subsec ] , in which the sequence denoising problems @xmath311 satisfies the following assumptions :    1 .",
    "_ general white noise : _ the entries of @xmath80 are i.i.d samples from a distribution with zero mean , unit variance and finite fourth moment .",
    "2 .   _ fixed signal column span and uniformly distributed signal singular vectors : _ let the rank @xmath85 be fixed and choose a vector @xmath86 with coordinates @xmath87 .",
    "assume that for all @xmath1 , @xmath426 is a singular value decomposition of @xmath90 , where @xmath99 and @xmath100 are uniformly distributed random orthogonal matrices .",
    "formally , @xmath99 and @xmath100 are sampled from the haar distribution on the @xmath0-by-@xmath0 and @xmath1-by-@xmath1 orthogonal group , respectively .",
    "asymptotic aspect ratio @xmath93 : _ the sequence @xmath94 is such that @xmath95 .",
    "the second assumption above implies that @xmath90 is a `` generic '' choice of matrix with nonzero singular values @xmath111 , or equivalently , a generic choice of coordinate systems in which the linear operator corresponding to @xmath2 is expressed .",
    "the results of @xcite , which we have used , hold in this case as well .",
    "it follows that lemma [ y - asy : lem ] and lemma [ inner - asy : lem ] , and consequently all our main results , hold under this alternative framework . in short , in general white noise , all our results hold if one is willing to only specify the signal singular values , rather than the signal matrix , and consider a `` generic '' signal matrix with these singular values .",
    "we have presented a general method to show that optimal shrinkers exist for a variety of loss functions , and to find them . while in each of the three example loss functions we discussed there exists a simple , explicit formula for the optimal shrinker , that is not always the case .",
    "the minimization problem may not admit a simple analytic solution , but it is still extremely easy to solve numerically . in these cases , implementing our method would involve numerical tabulation of the optimal shrinker .",
    "note that our general method , theorem [ char : thm ] , finds a shrinker that is asymptotically unique admissible , or optimal , among _ proper _ shrinkers ( in the sense of definition [ proper : def ] ) , rather than among all continuous shrinkers .",
    "this is an artifact of our general approach , which handles a large variety of loss families in a single theorem .",
    "shrinkers that are continuous but not regular are strange objects , and indeed optimal shrinkers formally derived using our method may well be optimal among all continuous shrinkers , although proving this seems to require delicate arguments tailored to individual losses .",
    "for example , in the introductory section [ fro : sec ] we have proved that the shrinker , which was also derived using our general framework , is indeed optimal among all continuous shrinkers .",
    "finding an explicit form of the optimal shrinkers for other matrix loss families , such as other schatten-@xmath38 losses ( see @xcite and references within ) , and a generalization of our method to include ky - fan norms , both remain interesting problems for further study .",
    "this work was partially supported by nsf dms-0906812 ( arra ) .",
    "we thank iain johnstone for helpful comments .",
    "we also thank amit singer and boaz nadler for discussions stimulating this work , and santiago velasco - forero for pointing out an error in an earlier version of the manuscript .",
    "mg was partially supported by a william r. and sara hart kimball stanford graduate fellowship ."
  ],
  "abstract_text": [
    "<S> we consider recovery of low - rank matrices from noisy data by shrinkage of singular values , in which a single , univariate nonlinearity is applied to each of the empirical singular values . </S>",
    "<S> we adopt an asymptotic framework , in which the matrix size is much larger than the rank of the signal matrix to be recovered , and the signal - to - noise ratio of the low - rank piece stays constant . for a variety of loss functions , including the frobenius norm loss ( mse ) , nuclear norm loss and operator norm loss , </S>",
    "<S> we show that in this framework there is a well - defined asymptotic loss that we evaluate precisely in each case . </S>",
    "<S> in fact , each of the loss functions we study admits a _ </S>",
    "<S> unique admissible _ shrinkage nonlinearity dominating all other nonlinearities . </S>",
    "<S> we provide a general method for evaluating these optimal nonlinearities , and demonstrate it by working out explicit formulas for the optimal nonlinearities in the frobenius , nuclear and operator norm cases . </S>"
  ]
}