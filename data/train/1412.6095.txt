{
  "article_text": [
    "approximate ( or adaptive ) dynamic programming ( adp ) or reinforcement learning ( rl ) has been investigated extensively by different researchers as a powerful tool for approximating solutions to mathematically intractable problems seeking optimum , @xcite .",
    "adp has shown its great potential in aerospace applications as well , @xcite , from control of agile missiles to spacecraft rendezvous .",
    "the most popular algorithm for adp is _ value iteration _ ( _ vi _ ) , @xcite .",
    "the convergence of vi for problems subject to this study , i.e. , problems with undiscounted cost functions and continuous state and action spaces , was analyzed in @xcite for linear systems and in @xcite , @xcite , @xcite for nonlinear systems . a crucial assumption in the cited convergence proofs is _ perfect _ function approximation , i.e. , neglecting function approximation errors .",
    "however , this assumption rarely holds in nonlinear problems .",
    "the concern with the existence of the approximation errors is due to the fact that the errors _ propagate _ throughout the iterations , i.e. , their consequences may grow in future iterations . in other words , a ` resonance ' type phenomenon may happen , regardless of how small each single error term is , which would lead to unreliability of the solution .    incorporating the approximation errors in the analysis of vi , i.e. , analyzing _ approximate vi _ ( _ avi _ ) , is a challenging area and the appeared results , to the best of the knowledge of the author , are limited to @xcite . problems with _",
    "discounted _ cost functions were the subject of refs .",
    "these results however , are not extendable to undiscounted cost - functions , because , the ` forgetting ' nature of discounted problems plays a critical role in the derivations and if the discount factor converges to one , i.e. , the problem becomes undiscounted , the developed bounds go to infinity .",
    "@xcite , however , investigated avi for undiscounted cost functions and provided some interesting results . however , the utilized assumptions are relatively more restrictive and not easily verifiable , compared to this study . assuming the approximation errors can be written in a multiplicative form , instead of an additive form , i.e. , assuming @xmath0 holds uniformly for some positive constant @xmath1 , instead of assuming @xmath2 , for a function @xmath3 , where @xmath4 and @xmath5 denote the exact and the approximated functions , is one of such assumptions . moreover , the developed results require @xmath1 to be upper bounded by a term which involves the _ optimal _ value function . as for non - value iteration based approaches in which the approximation errors are not neglected ,",
    "interested readers are referred to @xcite .",
    "considering the scarcity of the available studies , the prevalence of approximation errors , and the dramatic success of value iteration in solving optimal control problems in different applications , including aerospace systems @xcite , rigorous theoretical analyses in the area are of interest to the controls community .",
    "this study is aimed at this pursuit , i.e. , contributing to the mathematical rigor of the field of intelligent control , more specifically , adp for control .",
    "this is done through developing a sufficient condition for boundedness / convergence of the iterations under the presence of the approximation errors",
    ". the sufficient condition can be easily checked for general deterministic nonlinear systems .",
    "moreover , the stability of the system operated using the approximate solution , obtained through a _",
    "finite _ number of iterations of avi , is investigated and required conditions for guaranteeing stability are derived in terms of known and calculable parameters for general systems .",
    "it should be noted that the presence of the approximation errors and also possibly immature conclusion of iterations not only lead to the deviation of the results from optimality , but also , can potentially lead to instability / unreliability of the system operated using the resulting solution , which may lead to catastrophic outcomes when utilized in aerospace systems .",
    "therefore , investigation of the stability and deriving sufficient conditions for guaranteeing stability are required . moreover ,",
    "the important concern that a neurocontroller is valid only when the state trajectory remains within the domain for which the controller is trained is addressed through finding an estimation of the _ region of attraction _ ( roa ) for the result obtained through the avi .",
    "it should be noted that in the general case , if a neurocontroller is trained for a given domain , it is _ not _ guaranteed that any state trajectory initiated from the domain remains inside the domain .",
    "if it exists the domain , then the neurocontroller becomes invalid .",
    "however , once an estimation of the roa is found any state trajectory initiated from the domain , will remain within the domain and therefore , the controller remains valid for use .",
    "finally , interested readers are referred to @xcite for some recent developments of the author regarding _ stablizing value iteration _ ,",
    "i.e. , the exact or approximate vi which is initiated from a stabilizing initial guess .",
    "such a scheme guarantees the stability of the system during _",
    "online learning_. and lemma [ lem_boundedness ] . ]",
    "the rest of this study is organized as follows .",
    "the optimal control problem is presented in section ii and the exact vi scheme is revisited in section iii .",
    "section iv presents the approximate vi , followed by the theoretical analyses in section v. afterward a famous aerospace example is numerically investigated in section vi . finally , concluding remarks are given in section vii .",
    "the nonlinear discrete - time dynamics of the system are assumed to be given by @xmath6 where @xmath7 and @xmath8 are the state and control vectors , respectively , with the dimensions of @xmath9 and @xmath10 .",
    "function @xmath11 is assumed to be smooth versus its inputs and @xmath12 .",
    "sub - index @xmath13 is the time - index and @xmath14 denotes the non - negative integers . the problem is defined as finding a _ feedback control policy _ @xmath15 , i.e. , @xmath16 , such that the cost function given below is minimized subject to dynamics ( [ dynamics ] ) and given any initial conditions @xmath17 .",
    "@xmath18 it is assumed that @xmath19 for a convex and smooth positive definite function @xmath20 and a positive definite @xmath21 real matrix @xmath22 .",
    "the set of non - negative reals is denoted with @xmath23 .",
    "let the _ cost - to - go _ or _ value function _ of a control policy @xmath24 , denoted by @xmath25 , be defined as @xmath26 in ( [ valuefunction_of_h ] ) one has @xmath27 and @xmath28 , i.e. , @xmath29 denotes the @xmath13th element on the state history initiated from @xmath17 and generated using @xmath24 .",
    "[ def1 ] an _ admissible _ control policy within a set is defined as a control policy which is a continuous function and leads to an upper bounded value function for any @xmath17 in the set .",
    "the defined admissibility is different from the usual definition , including @xcite , in the sense that the control policy is not required to asymptotically stabilize the system @xcite or to have @xmath30 besides being continuous and leading to a finite value function .",
    "however , the assumed two features ( continuity and finiteness of the value function ) lead to those ( not explicitly assumed ) characteristics , as the tail of a convergent series can be made arbitrarily small ( cf .",
    "@xcite ) .",
    "[ assum_existingadmissiblecont ] considering @xmath31 as a compact set containing the origin , there exists at least one admissible control policy for the system within @xmath32 .",
    "this assumption is made for guaranteeing that there is no state vector in @xmath32 for which the value function associated with the _ optimal _ control policy is infinite . because , otherwise , the optimal control policy will not be ` optimal ' compared with the existing admissible control policy .",
    "the value function of a policy @xmath24 satisfies @xmath33 based on eq .",
    "( [ valuefunction_of_h ] ) .",
    "optimal value function _ , denoted with @xmath34 , be defined as the value function of the optimal control policy .",
    "the optimal value function satisfies the bellman equation @xcite @xmath35 @xmath36 however , the famous _ curse of dimensionality _",
    "@xcite leads to the intractability of the approach of using bellman eq . for solving the problem in the general case",
    "the idea in adp is _ approximating _ the optimal value function for remedying the problem of curse of dimensionality .",
    "the approximation is typically done using look - up tables or function approximators , e.g. , neural networks .",
    "_ critic _ , in the adp / rl literature , is the term used for the optimal value function approximator .",
    "one selects a set , called the _ domain of interest _ , which is compact , connected , and contains the origin , within which the value function will be approximated . denoting the domain of interest with @xmath32",
    ", it needs to be selected based on the given system and its operation envelope , as the adp solution is valid only if the state trajectory entirely remains within @xmath32 .",
    "value iteration ( vi ) is one of the learning schemes for finding the optimal value function .",
    "the vi process starts with an initial guess @xmath37 and iterates through @xmath38 for @xmath39 until the iterations converge .",
    "as one of the available convergence proofs , @xcite shows that if the initial guess on @xmath37 is smooth and @xmath40 , then the vi converges monotonically to the optimal solution . utilizing the converged value function , denoted with @xmath34",
    ", the optimal control policy can be obtained using eq .",
    "( [ bellman_eq2 ] ) .",
    "vi is based on the assumption that one can _ exactly _ approximate / reconstruct the right hand side of eq .",
    "( [ vi_valueupdate ] ) . however , this is rarely the case for general nonlinear problems .",
    "parametric function approximators are used in practice for this purpose .",
    "the approximation process leads to some approximation errors . incorporating the errors , eq .",
    "( [ vi_valueupdate ] ) leads to @xmath41 function @xmath42 , in ( [ avi_1 ] ) , denotes the approximation error at the @xmath43th iteration and @xmath44 denotes the _ approximate value function _ at this iteration .",
    "it should be noted that the right hand side of eq .",
    "( [ avi_1 ] ) also contains an approximate quantity , generated from the previous iteration .",
    "a critical point is the fact that @xmath45 does _ not _ represent the error between the _ exact _ and the _ approximate _ value functions , denoted with @xmath46 and @xmath47 , respectively .",
    "the exact value function @xmath46 is based on using the exact @xmath48 in the right hand side of ( [ vi_valueupdate ] ) , while , @xmath47 is being calculated based on @xmath49 , per ( [ avi_1 ] ) .",
    "the difference between @xmath46 and @xmath47 is an approximation error which is the cumulative effect of @xmath42 s in the previous iterations .",
    "the ` per iteration ' error , denoted with @xmath45 , however , is simply the error of approximating / replacing @xmath50 with @xmath47 .",
    "also , note that when @xmath51 , the convergence of the approximate vi ( avi ) does not follow from the cited previous investigations , as mentioned in the introduction .",
    "it should be mentioned that one typically trains a control approximator ( actor ) as well at each iteration of avi , to approximate the solution to the minimization problem given by ( [ bellman_eq2 ] ) , in which @xmath34 is replaced with @xmath44 . the actor will give rise to _ another _ approximation error term in the solution process , as seen in @xcite . however , the effect of the actor s approximation error can be removed from the convergence analysis of avi , as the actor training can be postponed till after the conclusion of the value function learning through eq .",
    "( [ vi_valueupdate ] ) or ( [ avi_1 ] ) in _ offline _ learning .",
    "in other words , one can learn the optimal value function and then use the result for training the actor .",
    "the detailed algorithm is presented in @xcite .",
    "however , one might be interested in _ online _ learning , as it leads to the feature of not needing the perfect knowledge of the internal dynamics of the system @xcite . even in case of online learning the effect of the actor s approximation error",
    "can be removed from the convergence analysis , as the control will be directly calculated from the minimization of the right hand side of eq .",
    "( [ avi_1 ] ) and applied on the system .",
    "the point is , the actor s approximation accuracy does not affect the critic training , even though the actor will be updated simultaneously along with the critic in online learning . of course , once the offline or online learning is concluded , the system will be operated using the control resulting from the trained actor , hence , the stability of the system could be at risk due to the actor s approximation errors .",
    "after the convergence analysis , this concern will be investigated in this study .",
    "smooth function approximators are shown to uniformly approximate a function if the function is _ continuous _ , @xcite .",
    "otherwise , the approximation accuracy is not guaranteed to be suitable on _ new _ states which were _ not _ used in the training . on the other hand",
    ", the minimization operation in ( [ avi_1 ] ) may lead to discontinuity of the right hand side versus @xmath7 , since , the @xmath8 which minimizes the term is given by @xmath52 and hence , may change discontinuously , since @xmath53 is _ not _ a continuous function generally . therefore , an important step is analyzing the continuity of the function subject to approximation , that is , @xmath54 note that the the difference between @xmath55 and @xmath56 is the fact that the latter is the approximation of the former , i.e. , @xmath57 .",
    "let @xmath58 ( respectively , @xmath59 ) denote the set of continuous functions at point @xmath60 ( respectively , within @xmath32 ) .",
    "the following theorem establishes the desired continuity .",
    "[ thm_cont_vi ] if the approximate value iteration scheme , implemented using a continuous function approximator , is initiated using a continuous initial guess , then the function subject to approximation by the critic will be continuous at any finite iteration .    _",
    "proof _ : based on the continuity of the function approximator , one has @xmath61 . the theorem can be proved by showing that if @xmath62 then @xmath63 .",
    "let @xmath64 and @xmath65 .",
    "note that functions @xmath66 and @xmath67 are smooth , hence , continuous . since , @xmath68 the proof of continuity of @xmath69 suffices .",
    "the proof is done by showing that the directional limit of @xmath69 at any selected point is equal to its evaluation at the point , and hence , it is continuous at that point ( motivated by @xcite ) .",
    "let @xmath70 be an arbitrary point in @xmath32 .",
    "set @xmath71 select an open set @xmath72 such that @xmath70 belongs to the boundary of @xmath73 and limit @xmath74 exists . if @xmath75 , for every such @xmath73 , then @xmath76 . in this case",
    "the continuity of @xmath69 at @xmath70 follows from the continuity of its forming functions , @xcite .",
    "now assume @xmath77 , for some @xmath73 denoted with @xmath78 . from @xmath79 for the given @xmath80 , one has @xmath81 if it can be shown that , for every selected @xmath78 , one has @xmath82 then the continuity of @xmath69 at @xmath70 follows , because from ( [ cont_thm_eq4 ] ) and ( [ cont_thm_eq5 ] ) one has @xmath83 and ( [ cont_thm_eq6 ] ) leads to the continuity by definition , @xcite .    the proof that ( [ cont_thm_eq5 ] ) holds is done by contradiction .",
    "assume that for some @xmath70 and some @xmath78 one has @xmath84 inequality ( [ cont_thm_eq9 ] ) leads to @xmath85 .",
    "but , this is against ( [ cont_thm_eq1 ] ) , hence , ( [ cont_thm_eq9 ] ) can not hold .",
    "now , assume @xmath86 hence there exists some @xmath87 such that @xmath88 then , due to the continuity of both sides of ( [ cont_thm_eq7_1 ] ) at @xmath70 for the fixed @xmath89 and @xmath80 , there exists an open set @xmath90 containing @xmath70 , see fig .",
    "[ fig_alpha_gamma_sets ] , and some @xmath91 , such that @xmath92    r0.2        given @xmath93 , inequality ( [ cont_thm_eq8 ] ) implies that at points which are _ close enough _ to @xmath70 , function @xmath94 is away from @xmath95 at least by a margin of @xmath96 .",
    "but , this contradicts eq .",
    "( [ cont_thm_eq2 ] ) which , implies that @xmath97 can be made arbitrarily close to @xmath80 as @xmath7 gets close to @xmath70 within @xmath78 .",
    "the reason is , the latter , given the continuity of @xmath98 versus both @xmath7 and @xmath8 , leads to the conclusion that function @xmath94 can be made arbitrarily close to @xmath95 if @xmath7 approaches @xmath70 from a certain direction .",
    "note that sets @xmath90 and @xmath78 are not disjoint , as @xmath70 is _ within _ @xmath90 and on the _ boundary _ of @xmath78 , as shown in fig .",
    "[ fig_alpha_gamma_sets ] . hence , inequality ( [ cont_thm_eq7 ] ) also can not hold .",
    "therefore , ( [ cont_thm_eq5 ] ) holds and hence , @xmath99 .",
    "finally , the continuity of the function subject to investigation at any arbitrary @xmath100 , leads to the continuity of the function in @xmath32 .      analysis of boundedness and convergence of sequence @xmath101 resulting from the approximate vi given by eq .",
    "( [ avi_1 ] ) and its relation versus the optimal value function is presented in this subsection .",
    "define @xmath102 and @xmath103 where @xmath104 and @xmath105 as sequences of functions initiated from some @xmath106 and @xmath107 and propagated using @xmath108 @xmath109 now , assuming an upper bound for the approximation error @xmath45 the following results can be obtained .",
    "[ lem_boundedness ] let @xmath110 for some @xmath111 . if the recursive relations given by eqs .",
    "( [ avi_1 ] ) , ( [ v_upper_1 ] ) , and ( [ v_lower_1 ] ) are initialized such that @xmath112 , then , one has @xmath113 .",
    "moreover , @xmath114 and @xmath115 are , respectively , the _ greatest _ lower bound and the _ least _ upper bound of @xmath116 if @xmath117 .    _ proof _ : the lemma can be proved using mathematical induction",
    ". initially @xmath112 by assumption .",
    "let @xmath118 hold for some @xmath43 .",
    "comparing eq .",
    "( [ v_upper_1 ] ) with eq .",
    "( [ avi_1 ] ) it follows that @xmath119 , since @xmath120 and @xmath121 . therefore , one has @xmath122 .",
    "the proof of @xmath123 is similar through comparing eq .",
    "( [ v_lower_1 ] ) with eq .",
    "( [ avi_1 ] ) and using mathematical induction .",
    "proof of the last part of the lemma follows from assuming @xmath124 ( respectively , @xmath125 ) which leads to @xmath126 ( respectively , @xmath127 ) .",
    "therefore , there are no other ` tighter ' bounds for @xmath116 .",
    "@xmath128    it can be seen that functions @xmath129 and @xmath130 are the value functions at the @xmath43th iteration of _ exact vi _ for cost functions @xmath131 @xmath132 respectively , subject to dynamics ( [ dynamics ] ) , considering recursive relations ( [ v_upper_1 ] ) and ( [ v_lower_1 ] ) .",
    "the following lemma provides the sufficient conditions for their convergence to the respective optimal value functions .",
    "[ lem_exactvi_convergence ] the exact value iterations given by eqs .",
    "( [ v_upper_1 ] ) and ( [ v_lower_1 ] ) converge to the optimal value functions of cost functions ( [ cost_upper_1 ] ) and ( [ cost_lower_1 ] ) , respectively , if they are initialized by smooth functions @xmath107 and @xmath106 such that @xmath133 and @xmath134 , where @xmath111 .",
    "_ proof _ : the proof follows from @xcite , since , iterations given by ( [ v_upper_1 ] ) and ( [ v_lower_1 ] ) are exact vis . @xmath128    considering lemmas [ lem_boundedness ] and [ lem_exactvi_convergence ] the following theorem proves the boundedness of the elements of @xmath101 resulting from the approximate vi .    [ thm_boundedness ]",
    "let @xmath135 for some @xmath111 .",
    "if the approximate value iteration given by eq . ( [ avi_1 ] ) is initialized such that @xmath136 , then , the elements of sequence @xmath101 as @xmath137 are _ bounded _ by the optimal value functions of cost functions ( [ cost_upper_1 ] ) and ( [ cost_lower_1 ] ) denoted with @xmath138 and @xmath139 , respectively , in the sense that the greatest lower bound of @xmath116 converges to @xmath139 and the least upper bound of @xmath116 converges to @xmath138 as @xmath137 .",
    "_ proof _ : the proof follows from the boundedness of @xmath101 given in lemma [ lem_boundedness ] and the convergence of the bounds for smooth @xmath140 and @xmath141 which satisfy @xmath142 based on lemma [ lem_exactvi_convergence ] .",
    "@xmath128    moreover , the following result can be achieved , with the uniformness feature which will be used in stability analysis .",
    "[ thm_uniformconv_vs_c ] let @xmath135 for some @xmath111 . also , let the approximate value iteration given by eq .",
    "( [ avi_1 ] ) be initialized such that @xmath136 . as @xmath143 ,",
    "the results from the approximate value iteration ( [ avi_1 ] ) converges _ uniformly _ to the results from the exact value iterations given by ( [ vi_valueupdate ] ) corresponding to cost function ( [ costfunction ] ) in compact set @xmath32 .",
    "more specifically , the least upper bound and the greatest lower bound of @xmath116 for @xmath137 converge uniformly to the optimal value function associated with cost function ( [ costfunction ] ) as @xmath143 .",
    "_ proof _ : the proof is given in the appendix .",
    "theorem [ thm_boundedness ] proves that sequence @xmath144 is upper and lower bounded .",
    "then , theorem [ thm_uniformconv_vs_c ] proves the uniform convergence of these bounds to the desired optimal solution if @xmath143 .",
    "however , when the approximation error does not vanish , the mere fact that the sequence is upper bounded does not prove its convergence ( the elements of a sequence can be upper bounded but oscillatory ) .",
    "the established boundedness resembles the ` convergence to a neighborhood ' or interval presented in @xcite , however , besides the idea behind the analysis which is different in here , the assumptions are also different and less restrictive in this study .",
    "even though it is proved that the avi result remains bounded ( theorem [ thm_boundedness ] ) , it is not necessarily optimal , due to the presence of the approximation error .",
    "once the solution is not optimal with respect to the selected cost function , it may not even stabilize the system .",
    "therefore , stability analysis of the control resulting from the avi is non - trivial .",
    "this subsection is aimed at this pursuit .",
    "let the avi be terminated at the @xmath43th iteration , once a convergence tolerance , denoted with positive ( semi-)definite function @xmath145 , is achieved , i.e. , when @xmath146 note that if approximation errors do not exist , the convergence of vi to a finite limit function , @xcite , guarantees the satisfaction of the convergence criterion ( [ conv_criteria ] ) for a large enough , but finite @xmath43 for any given arbitrary positive definite @xmath147 , ) after a finite @xmath43 for an arbitrary @xmath147 needs _ uniform _ convergence of exact vi . while , the cited proofs provide its pointwise convergence .",
    "however , uniform convergence also can be proved , for example assuming boundedness of @xmath148 in @xmath32 , the result given in @xcite leads to the desired uniform convergence , @xcite . ] . however ,",
    "if the errors exist , an arbitrarily selected @xmath147 can be achieved only when the approximation errors are small enough , per the uniform convergence result of theorem [ thm_uniformconv_vs_c ] .",
    "once the convergence criteria is achieved , the resulting value function @xmath149 can be used for calculating the feedback control , denoted with @xmath150 through solving the minimization problem given by @xmath151 in online operation ( i.e. , on the fly ) based on the instantaneous state of the system , denoted with @xmath7 .",
    "this approach , however , leads to a considerable computational load during the online operation of the system .",
    "another approach , widely used by adp practitioners , is training another function approximator , called _ actor _ , for approximating the solution to the minimization problem given by eq .",
    "( [ avi_policy ] ) , for different states within the domain of operation . denoting the _ approximation _ of @xmath150 with @xmath152 , the approximation error of the actor , denoted with @xmath153 ,",
    "will be introduced to the process .",
    "@xmath154 the next theorem provides a sufficient condition for asymptotic stability of @xmath152 in a subset of @xmath32 which is an estimation of its region of attraction , @xcite .",
    "[ thm_stability ] let the value function be approximated using a smooth function approximator with an approximation error upper bounded by @xmath155 , for some @xmath156 .",
    "also , let the lipschitz constants of functions @xmath157 and @xmath158 , whose existence follows from the smoothness of the functions , be given by @xmath159 and @xmath160 .",
    "if the approximation error of the actor is upper bounded by @xmath161 with the equality holding only at the origin , then the control policy @xmath152 resulting from the approximate value iteration , terminated with the tolerance of @xmath162 asymptotically stabilizes the system for any initial sate selected in compact domain @xmath163 containing the origin , where @xmath164 , @xmath165 is the largest @xmath166 for which @xmath167 holds , and @xmath168 denotes vector norm .",
    "_ proof _ : the proof is given in the appendix .",
    "inequality ( [ mu_upperbound ] ) provides an upper bound for the norm of the actor s approximation error .",
    "however , it is important to note that the upper bound has to be positive definite , otherwise no non - zero approximation error can satisfy it .",
    "in other words , one needs the numerator of the right hand side of ( [ mu_upperbound ] ) to be positive for @xmath169 .",
    "therefore , it is required to have @xmath170 on the other hand , one has @xmath171 if the number of iterations of avi is large enough , where upper bounded positive definite functions @xmath172 and @xmath173 were defined in the proof of theorem [ thm_uniformconv_vs_c ] .",
    "the reason is the least upper bound and the greatest lower bound of @xmath174 , as @xmath175 , satisfy ( [ breve_v_4_0 ] ) and ( [ breve_v_5 ] ) , given in the appendix .",
    "therefore , considering inequality ( [ thm2_11 ] ) , if the critic s approximation error is small enough , leading to a small @xmath176 , inequality ( [ thm2_10_2 ] ) can always be achieved , which will then lead to a positive definite right hand side in ( [ mu_upperbound ] ) , that determines the upper bound of the actor s approximation error .",
    "note that , @xmath145 can be explicitly obtained from the results of the concluded avi , e.g. , @xmath177 .",
    "therefore , in practice , one can check the validity of inequality ( [ thm2_10_2 ] ) before training the actor and if not satisfied , will need to increase the approximation capability / richness of the critic , e.g. , by increasing the number of neurons .",
    "it is an interesting feature of the upper bound of the actor approximation error given by theorem [ thm_stability ] that it can be calculated for any general nonlinear system , because , all the parameters are either known or calculable for a given system . for example , besides checking the validity of inequality ( [ thm2_10_2 ] ) , which was discussed , the lipschitz constants @xmath178 and @xmath160 can be calculated analytically or numerically through examining the trained critic and actor .",
    "note that , in order to find the lipschitz constants @xmath178 and @xmath160 , one needs @xmath179 , unless the functions are globally lipschitz .",
    "set @xmath179 can be obtained using the pointwise values obtained for ( [ avi_policy ] ) and the trained actor . the former corresponds to @xmath180 and the latter is @xmath181 . utilizing these data , set @xmath179 , i.e.",
    ", the union of the images of @xmath32 under @xmath180 and @xmath181 , can be found ( see the next section for an example ) .",
    "another interesting feature of the given stability result is the point that it admits termination of the learning process after a finite number of iterations , through admitting the convergence criterion given by ( [ conv_criteria ] ) .",
    "this holds for both cases of having or not having the approximation errors .",
    "the orbital maneuver problem with continuous thrust simulated in @xcite is selected for numerical analyses in this study .",
    "a rigid spacecraft is orbiting around the earth .",
    "it needs to perform a maneuver to move to a given circular orbit .",
    "the regulation of the states corresponds to positioning the spacecraft in the destination orbit , with the desired velocity , to stay in the orbit after the maneuver .",
    "assuming planar motion , the non - dimensionalized displacement vector of the center of mass of the spacecraft from the center of the orbital frame positioned at the destination orbit is denoted by @xmath182^t$ ] , where real numbers @xmath183 and @xmath184 are the components of the vector in the orbital frame .",
    "the equations of motion of the spacecraft in the gravity field are given by @xcite @xmath185 @xmath186 where @xmath187 and @xmath188 denote the components of the non - dimensionalized total force applied on the spacecraft and @xmath189 .",
    "for non - dimensionalizing , a reference length , @xmath190 , and a reference time , @xmath191 , are selected .",
    "the radius of the destination orbit is selected for @xmath190 and the inverse of the angular velocity of the spacecraft orbiting in the destination orbit , i.e. , @xmath192 , is selected for @xmath191 , where @xmath193 denotes the gravitational parameter for the earth .    selecting the state vector as @xmath194^t$ ] and the control vector as @xmath195^t$ ] , the state equation of the orbital maneuver problem",
    "can be written as @xmath196 +    \\left [ \\begin{array}{cc }    0 & 0\\\\ 0 & 0\\\\ 1 & 0\\\\ 0 & 1\\end{array } \\right ] u , \\label{maneuver_cont_dynamics}\\ ] ] note that the elements of vector @xmath7 are denoted with @xmath197 , @xmath198 , as opposed to the customary notation of @xmath199 , to avoid mistaking them with the discrete time steps , i.e. , in @xmath200 used throughout the paper .    minimizing cost function @xmath201 leads both positioning the spacecraft in the destination orbit and having it orbit with the desired orbital velocity , since both the relative position and the relative velocity will be forced to converge to zero .",
    "the dynamics of the problem given by ( [ maneuver_cont_dynamics ] ) is in the continuous - time from . using the ( non - dimensionalized ) sampling time of @xmath202 the continuous - time problem",
    "is discretized to @xmath203 , \\mbox { } g : = \\delta t   \\left [ \\begin{array}{cc }    0 & 0\\\\ 0 & 0\\\\ 1 & 0\\\\ 0 & 1\\end{array } \\right].\\ ] ] @xmath204    since the system is control affine , and the utility function is quadratic in @xmath8 , the minimum of the term in the right hand side of ( [ avi_1 ] ) can be simply found by setting its gradient to zero , which leads to @xmath205 where @xmath206 , @xcite .",
    "note that eq .",
    "( [ avi_policy_simpler ] ) is implicit , as @xmath8 exists on the right hand side as well .",
    "@xcite proves that selecting any finite @xmath207 and conducting the successive approximation given by @xmath208 @xmath209 converges to the solution to eq .",
    "( [ avi_policy_simpler ] ) , if the sampling time , @xmath210 , is small enough .",
    "note that a complete set of iterations on ( [ avi_policy_succ_approx ] ) , called _ inner loop _ in @xcite , needs to be done at each single iteration of ( [ avi_1 ] ) , called _",
    "outer loop_. however , selecting a small enough sampling time , the inner loop iterations are observed to converge very quickly , @xcite .",
    "this approach is used for finding the minimum of ( [ avi_1 ] ) during the critic training and also for training the actor using eq .",
    "( [ avi_policy ] ) , which due to inevitable approximation errors , leads to ( [ avi_policy_approx ] ) .",
    "the linear - in - parameter structures @xmath211 and @xmath212 are selected for function approximation , where @xmath213 and @xmath214 are the nonlinear smooth basis functions to be selected , @xmath215 , and @xmath216 are the unknown parameters to be found .",
    "positive integers @xmath217 and @xmath218 denote the number of neurons or basis functions in the critic and the actor , respectively .",
    "it should be noted that each iteration of avi leads to a new set of weights for the critic , i.e. , the weights of the critic evolve with the iterations .",
    "therefore , they are denoted with superscript @xmath43 to relate them to their respective iterations .",
    "however , only one actor will be trained to learn the resulting control policy , denoted with @xmath181 .",
    "so , the actor s weight matrix , @xmath219 , is not iteration dependent .    denoting the vector whose elements",
    "are all the non - repeating polynomials made up through multiplying the polynomial elements of vector @xmath220 by those of vector @xmath221 with @xmath222 , the following basis functions are selected for the function approximators @xmath223^t",
    ", \\label{selected_phi_1}\\ ] ] @xmath224^t .",
    "\\label{selected_sigma_1}\\ ] ]    @xmath225 random state vectors , denoted with @xmath226 } , p \\in \\{1,2,\\ldots,500\\}$ ] , were selected from @xmath227 , for learning the value function using eq .",
    "( [ avi_1 ] ) . selecting a constant convergence tolerance of @xmath228",
    "the convergence was evaluated in a fashion similar to ( [ conv_criteria ] ) . starting with @xmath229 as the initial guess",
    ", the avi converged after @xmath230 iterations of ( [ avi_1 ] ) , each involving an inner loop over ( [ avi_policy_succ_approx ] ) which was observed to converge in less than @xmath231 iterations .",
    "each iteration of the process involves finding @xmath232 given @xmath233 using @xmath234 } ) \\approx u(x^{[p]},u^{i,[p ] } ) + { w_c^{i}}^t \\phi \\big(f(x^{[p]},u^{i,[p]})\\big ) , \\forall p \\in \\{1,2,\\ldots,500\\ } , \\label{avi_1_nn}\\ ] ] where each @xmath235}$ ] is the converged value of ( [ avi_policy_succ_approx ] ) in which , the @xmath7 is substituted with the respective sample state @xmath226}$ ] and @xmath236 is used for @xmath149 .",
    "the method of least squares , as detailed in @xcite , was used for finding @xmath232 .",
    "[ fig_weights ] shows the evolution of the elements of the critic s weight , versus the iteration index . in terms of the elapsed time ,",
    "the critic training took around @xmath237 seconds on a desktop computer with intel core i7 - 3770 , 3.40 ghz processor and 8 gb of memory , running windows 7 and matlab 2013 ( single threading ) .",
    "once the critic training is concluded , the actor training is done in one shot over the selected random states , that is , @xmath219 is found using @xmath238 } ) \\approx u^{i,[p ] }          , \\forall p \\in \\{1,2,\\ldots,500\\ } ,   \\label{avi_2_nn}\\ ] ] evaluated at @xmath239 , i.e. , the iteration at which the critic training converged .      for _ evaluation _ of the function approximation accuracy , i.e. , to quantify the approximation error @xmath240 , _ another _ set of sample states were selected .",
    "the point is , the approximation error at the sample states used in the training , @xmath226}$]s , may be very low , but , it is important to evaluate the error at other states , to evaluate the _ generalization _ accuracy of the function approximators . to this goal ,",
    "@xmath241 equidistant states were selected by gridding @xmath242 , denoted with @xmath243 } , p \\in \\{1,2,\\ldots,20000\\}$ ] .",
    "function @xmath45 is then given by @xmath244 } ) : = { w_c^{i+1}}^t \\phi(y^{[p ] } ) - \\big(u(y^{[p]},w^{i,[p ] } ) + { w_c^{i}}^t \\phi \\big(f(y^{[p]},w^{i,[p]})\\big)\\big ) , \\forall p \\in \\{1,2,\\ldots,20000\\ } , \\forall i , \\label{avi_1_nn_eval}\\ ] ] where @xmath245}$ ] is the converged value of ( [ avi_policy_succ_approx ] ) evaluated at the respective sample state @xmath243}$ ] .    having the pointwise values of function @xmath246 } ) , \\forall p , \\forall i$ ] , constant @xmath176 used in @xmath247 , can be found using @xmath248})|}{u(y^{[p]},0)}\\ ] ] which led to @xmath249 .",
    "if the @xmath176 was obtained using @xmath226}$]s , that is , at the states utilized in the training , the result would be @xmath250 , which is close to what was achieved using new set of states .",
    "this demonstrates the good generalization capability of the function approximator .",
    "note that a value less than @xmath251 is desired per the theory presented in this work , e.g. , for proof of boundedness as in theorem [ thm_boundedness ] .",
    "however , the iteration has already converged , therefore , the concern of divergence does not exist in here .",
    "but , the existing concern is the quality of the result compared with the optimal solution and the reliability of the controller .    if the assumptions of theorem [ thm_stability ] hold , asymptotic stability of the controller can be concluded .",
    "the main issue is verification of inequality ( [ mu_upperbound ] ) , for which , function @xmath153 is needed .",
    "the process of evaluation of the approximation accuracy of the critic at the new state vectors @xmath243}$ ] are done for quantifying @xmath153 as well . as for the upper bound of this error given by eq .",
    "( [ mu_upperbound ] ) , functions @xmath252},0)$ ] and @xmath253 } ) = |{w_c^{i}}^t \\phi(y^{[p ] } ) - { w_c^{i-1}}^t \\phi(y^{[p]})|$ ] evaluated at @xmath254 are used .",
    "but , the lipschitz constants @xmath178 and @xmath160 are also required , cf .",
    "( [ thm2_3 ] ) and ( [ thm2_5 ] ) given in the appendix .",
    "note that the respective functions are smooth , hence , differentiable .",
    "so , finding the maximum of their gradient with respect to @xmath8 leads to their lipschitz constants , @xcite .",
    "@xmath255 therefore @xmath256},w^{330,[p]})\\big ) g = 0.186\\ ] ] to be more accurate , @xmath160 is the maximum number between the result of the foregoing equation and @xmath257},w_a^t\\sigma(y^{[p]})\\big)\\big ) g\\ ] ] where the difference is one is evaluated at @xmath258}$]s and the other one at @xmath259})$]s .",
    "note that the former is @xmath260})$ ] and the latter is @xmath261})$ ] , for @xmath254 .",
    "but , considering the maximum norm of the actor approximation error given by @xmath262})\\| = 0.02 $ ] compared with the maximum norm of the control which was observed to be around 10 , the difference between the two evaluations of @xmath160 turned out to be negligible .    similarly , @xmath263 therefore @xmath264}}^t r \\approx    \\max_{p \\in \\{1,2,\\ldots,20000\\ } } 2\\sigma^t(y^{[p]})w_a r = 0.186\\ ] ] evaluating @xmath265 and its upper bound given by ( [ mu_upperbound ] ) , it turned out that @xmath266 never exceeds the bound .",
    "as a matter of fact , it remains smaller than @xmath267 of the upper bound .",
    "therefore , the asymptotic stability of the controller about the origin follows .    selecting the initial condition of @xmath268^t$ ] ,",
    "the system is operated using the trained neurocontroller and the resulting state trajectories are presented in fig .",
    "[ fig_states ] . for comparison purposes , the ( open loop ) optimal solution to the problem",
    "is calculated numerically , using direct method of optimization , and super - imposed with the results .",
    "it can be seen from these results that the controller has been very accurate in approximating the optimal solution . besides comparing the resulting state trajectories , the cost - to - go s also can be compared .",
    "the cost - to - go for the numerical open loop solution ( the optimal cost - to - go ) turned out to be @xmath269 , which is slightly less than the cost - to - go resulting from the close - loop controller , @xmath270 . note that the latter is the actual resulting cost - to - go using the trained neurocontroller , not the one approximated by @xmath271 .",
    "this approximation , however , is supposed to be upper and lower bounded by the optimal cost - to - go s corresponding to cost functions ( [ cost_upper_1 ] ) and ( [ cost_lower_1 ] ) , per theorem [ thm_boundedness ] . the upper and lower cost - to - go s",
    "were numerically found to be @xmath272 and @xmath273 , respectively , which confirm the analytical result given by the theorem and provide an idea of the near optimality of the avi results .",
    "considering the previous simulated initial conditions , it is seen that the state trajectory did not exit @xmath32 , as no state element ever exited the interval of @xmath274 $ ] .",
    "therefore , the control calculated by the neurocontroller was valid .",
    "however , this was not guaranteed or obvious from the given initial condition .",
    "but , per theorem [ thm_stability ] one can find an estimation of the roa for the trained neurocontroller , in order to guarantee such a desired behavior .",
    "as for finding the estimation of the roa , numerically analyzing @xmath275 it was observed that @xmath276 for the selected @xmath242 , where @xmath165 is the largest @xmath166 using which @xmath277 .",
    "but , evaluating the converged critic at the selected initial condition , one has @xmath278 , which means @xmath279 therefore , it was _ not _ guaranteed that @xmath280 . if interested to utilize the trained neurocontroller with guaranteed stability , one needs to select smaller initial conditions , such that they belong to @xmath281 .",
    "note that @xmath282 is continuous and vanishes at the origin .",
    "therefore , @xmath283 is a compact set with the origin as an _ interior _ point , @xcite .",
    "details of this conclusion are given in the proof of theorem [ thm_stability ] in the appendix .",
    "however , if controlling larger initial conditions , like the selected @xmath17 , is of interest , one needs to re - train the neurocontroller using a larger domain of interest . to this purpose ,",
    "@xmath284 was selected and the neurocontroller was retrained .",
    "note that as the training domain is expanded it is advisable to pick more random sample states as well .",
    "for this training @xmath285 random states were selected from @xmath286 , instead of @xmath225 used earlier . once the training is concluded , evaluating the critic upper bound constant @xmath176 using the discussed method , it was observed to be around @xmath287 , which is close to the critical value of @xmath251 .",
    "such a large critic approximation error led to the norm of the actor approximation error @xmath266 exceeding its upper limit by @xmath288 .",
    "therefore , not only we did nt expand @xmath281 , but also , the asymptotic stability of the origin is no longer guaranteed . note that , simulating this controller one still gets good results as in fig . [ fig_states ] , at least for that specific @xmath17 , however , such a good result is not theoretically guaranteed using the presented analyses in this study , due to the violation of ( [ mu_upperbound ] ) .",
    "the problem can be resolved by improving the approximation capability of the function approximators .",
    "an option is using multi - layer neural networks .",
    "another option is using richer basis functions .",
    "for example , instead of the basis function ( [ selected_phi_1 ] ) and ( [ selected_sigma_1 ] ) , one may select the richer sets of basis functions given by    @xmath289^t , \\label{selected_phi_2}\\ ] ]    @xmath290^t . \\label{selected_sigma_2}\\ ] ]    selecting this new set of basis functions the training was redone over @xmath286 and the critic upper bound constant , @xmath176 , turned out to be @xmath291 , with @xmath266 never exceeding @xmath292 of its upper bound .",
    "this new neurocontroller lead to @xmath293 , therefore , @xmath294 and one can be assured that the trajectory will not exit the domain on which the neurocontroller is trained . using this new neurocontroller for the given initial conditions ,",
    "it was observed that the results are extremely similar to what presented in fig .",
    "[ fig_states ] .",
    "this similarity may mean that the developed sufficient conditions for guaranteed stability and roa are still conservative and milder conditions for the approximation bounds can probably be obtained .        , and for the open loop optimal controller , denoted with @xmath295 . ]",
    "analytical investigations of the effects of the approximation errors on the quality of the result of approximate dynamic programming were conducted . it was observed through verifiable assumptions and conditions that the learning results remain bounded .",
    "once the learning is terminated after a finite number of iterations , it was shown that the stability of the result can be verified and an estimation of the domain of attraction can be obtained .",
    "the comprehensive numerical analysis of the theoretical results through a non - trivial fourth - order aerospace problem demonstrated the process of utilizing the theory in practice .",
    "these results lay the foundation for and push the state of the art in improving the mathematical rigor of the field of intelligent / bio - inspired control .",
    "_ proof of theorem [ thm_uniformconv_vs_c ] _ : let the optimal value function associated with cost function ( [ costfunction ] ) be given by @xmath148 .",
    "let @xmath172 be defined as @xmath296 where @xmath297 and @xmath298 . in other words , the summation in ( [ breve_v_1 ] )",
    "is evaluated along the ` optimal ' trajectory with respect to ( [ costfunction ] ) .",
    "one has @xmath299 where @xmath138 is optimal value function associate with cost function ( [ cost_upper_1 ] ) , otherwise , the control resulting from @xmath138 will be the optimal control for cost function ( [ costfunction ] ) .",
    "moreover , @xmath300 otherwise @xmath138 will not be the optimal value function for cost function ( [ cost_upper_1 ] ) .",
    "note that , both sides of inequality ( [ breve_v_3 ] ) include infinite sums of @xmath301 terms , but , they are evaluated along different trajectories , i.e. , the applied controls are different .",
    "the summation in the left hand side is based on the control which minimizes ( [ cost_upper_1 ] ) and the summation in the right hand side is based on the control that minimizes cost function ( [ costfunction ] ) .    by inequalities ( [ breve_v_2 ] ) and ( [ breve_v_3 ] ) , one has @xmath302 let @xmath303 .",
    "note that @xmath304 is a _",
    "finite _ constant , by assumption [ assum_existingadmissiblecont ] .",
    "therefore , the foregoing inequality leads to @xmath305 inequality ( [ breve_v_4 ] ) proves the convergence of @xmath138 to the optimal value function associated with cost function ( [ costfunction ] ) as @xmath143 .",
    "moreover , since the right hand side of ( [ breve_v_4 ] ) is independent of @xmath7 , this convergence is uniform , @xcite",
    ".    let @xmath173 be defined as @xmath306 where @xmath307 is the optimal control policy for cost function @xmath308 , i.e. , the summation in the right hand side of ( [ breve_v_4_1 ] ) is evaluated along the trajectory which is optimal with respect to @xmath308 given by ( [ cost_lower_1 ] ) . through a similar argument",
    "it can be seen that @xmath309 and @xmath310 which leads to @xmath311 defining @xmath312 a similar uniform convergence can be concluded as the right hand side of ( [ breve_v_5 ] ) will be upper bounded by the @xmath7-independent term @xmath313 .",
    "it should be noted that @xmath314 will be finite as long as @xmath111 .",
    "the reason is the finiteness of @xmath315 which leads to a finite @xmath139 , because @xmath316 .",
    "one has @xmath317 , hence , @xmath318 being finite leads to a finite @xmath319 and the finiteness of the latter leads to a finite @xmath320 when @xmath321 .",
    "finally , these uniform convergence results along with theorem [ thm_boundedness ] prove this theorem .",
    "@xmath128    _ proof of theorem [ thm_stability ] _ : the idea for the proof is using @xmath149 as a lyapunov function for the system , @xcite . from the boundedness of @xmath116 per lemma [ lem_boundedness ] and the positive definiteness of the bounds ( they are value functions of the respective _ finite - horizon _ cost functions",
    "as shown in @xcite ) for @xmath322 it follows that @xmath116 is a positive definite function .",
    "note that equation ( [ thm2_1 ] ) is based on @xmath180 , i.e. , it is independent of the actor s approximation error , see remarks at the end of section [ avi ] .",
    "so the next step is replacing @xmath180 with @xmath181 , since the system will be operated using @xmath181 . from the lipschitz continuity of @xmath325 and @xmath149 within",
    "compact sets @xmath32 and @xmath179 , which follows from their smoothness in the respective compact domains @xcite , one has @xmath326",
    "@xmath327 where @xmath179 is a compact subset of @xmath328 such that @xmath329 and @xmath330 . in other words",
    ", @xmath179 is the union of the images of @xmath32 under @xmath180 and @xmath181 . from inequalities ( [ thm2_3 ] ) and ( [ thm2_5 ] ) one has @xmath331 @xmath332 after replacing @xmath333 and @xmath334 in the right hand side of eq .",
    "( [ thm2_1 ] ) using inequalities ( [ thm2_6 ] ) and ( [ thm2_7 ] ) one has @xmath335 because @xmath336 .",
    "the asymptotic stability follows if @xmath337 with the equality holding only at @xmath338 . considering ( [ thm2_8 ] ) , condition ( [ thm2_10 ] ) holds if @xmath339 with the possible equality only at the origin .",
    "using @xmath340 and @xmath341 , which leads to @xmath342 one has @xmath343 considering ( [ thm2_9_1 ] ) , if inequality ( [ mu_upperbound ] ) holds , then ( [ thm2_9 ] ) will hold , which leads to @xmath344 . in the foregoing inequality the two sides are equal only at the origin , due to the positive definiteness of @xmath67 .",
    "hence , value function @xmath49 serves as a lyapunov function and the asymptotic stability of the system under the approximate control policy @xmath152 , within @xmath32 , follows , as long as the entire state trajectory remains inside @xmath32 . because , if it leaves @xmath32 , the control policy @xmath152 will no longer be valid , i.e. , relation ( [ thm2_1 ] ) , which is the backbone of the stability result , will no longer hold .",
    "this concern can be resolved by considering the fact that @xmath281 will be an estimation of roa for the system @xcite , per the definition of @xmath281 and inequality @xmath344 which guarantees that a state trajectory initiated within @xmath281 remains inside @xmath281 .",
    "therefore , the asymptotic stability of the control policy inside @xmath281 follows .    finally , since @xmath281 is contained in @xmath32 , it is bounded . also , the set is closed , because , it is the _ inverse image _ of a closed set , namely @xmath345 $ ] under a continuous function ( due to the continuity of the function approximator ) , @xcite .",
    "hence , @xmath281 is compact .",
    "it also contains the origin , because @xmath346 which is the consequence of its lower and upper boundedness established in lemma [ lem_boundedness ] .",
    "@xmath128        p.  j. werbos , `` approximate dynamic programming for real - time control and neural modeling , '' in _ handbook of intelligent control _ ( d.  a. white and d.  a. sofge , eds . ) , pp .  493525 , multiscience press , 1992 .",
    "g.  venayagamoorthy , r.  harley , and d.  wunsch , `` comparison of heuristic dynamic programming and dual heuristic programming adaptive critics for neurocontrol of a turbogenerator , '' _ ieee transactions on neural networks _ ,",
    "vol .  13 , pp .",
    "764773 , may 2002 .",
    "doi : 10.1109/tnn.2002.1000146 .",
    "r.  enns and j.  si , `` helicopter trimming and tracking control using direct neural dynamic programming , '' _ ieee transactions on neural networks _ ,",
    "14 , no .  4 , pp .",
    "929939 , 2003 .",
    "doi : 10.1109/tnn.2003.813839 .",
    "a.  al - tamimi , f.  lewis , and m.  abu - khalaf , `` discrete - time nonlinear hjb solution using approximate dynamic programming : convergence proof , '' _ ieee transactions on systems , man , and cybernetics , part b : cybernetics _ , vol .  38 , pp .",
    "943949 , aug 2008 .",
    "doi : 10.1109/tsmcb.2008.926614 .",
    "q.  zhao , h.  xu , and s.  jagannathan , `` optimal control of uncertain quantized linear discrete - time systems , '' _ international journal of adaptive control and signal processing _",
    "doi : 10.1002/acs.2473 .",
    "d.  liu and q.  wei , `` finite - approximation - error - based optimal control approach for discrete - time nonlinear systems , '' _ ieee transactions on cybernetics _ , vol .",
    "43 , pp .",
    "779789 , april 2013 .",
    "doi : 10.1109/tsmcb.2012.2216523 .          b.  w. james s.  mcgrew , jonathon p.  how and n.  roy , `` air - combat strategy using approximate dynamic programming , '' _ journal of guidance , control , and dynamics _",
    "33 , pp .",
    "16411654 , 2010 .",
    "doi : 10.2514/1.46815 .",
    "j.  ding and s.  n. balakrishnan , `` intelligent constrained optimal control of aerospace vehicles with model uncertainties , '' _ journal of guidance , control , and dynamics _ ,",
    "35 , pp .",
    "15821592 , 2012 .",
    "doi : 10.2514/1.54505 .",
    "f.  lewis , d.  vrabie , and k.  vamvoudakis , `` reinforcement learning and feedback control : using natural decision methods to design optimal adaptive controllers , '' _ ieee control systems magazine _ , vol .",
    "32 , pp .",
    "76105 , dec 2012 .",
    "doi : 10.1109/mcs.2012.2214134 .",
    "x.  liu and s.  balakrishnan , `` convergence analysis of adaptive critic based optimal control , '' in _ proceedings of the american control conference _ ,",
    "vol .  3 , pp .  19291933 , 2000 .",
    "doi : 10.1109/acc.2000.879538 .",
    "a.  heydari and s.  n. balakrishnan , `` finite - horizon control - constrained nonlinear optimal control using single network adaptive critics , '' _ ieee trans .",
    "neural netw .",
    "learning syst .",
    "_ , vol .  24 , no .  1 ,",
    "pp .  145157 , 2013 .",
    "doi : 10.1109/tnnls.2012.2227339 .",
    "a.  farahmand , c.  szepesvri , and r.  munos , `` error propagation for approximate policy and value iteration , '' in _ advances in neural information processing systems _",
    "( j.  lafferty , c.  williams , j.  shawe - taylor , r.  zemel , and a.  culotta , eds . ) , pp .  568576 , 2010 .",
    "k.  g. vamvoudakis and f.  l. lewis , `` online actor - critic algorithm to solve the continuous - time infinite horizon optimal control problem , '' _ automatica _ , vol .",
    "46 , no .  5 , pp .",
    "878  888 , 2010 .",
    "doi : 10.1016/j.automatica.2010.02.018 .",
    "t.  dierks and s.  jagannathan , `` online optimal control of nonlinear discrete - time systems using approximate dynamic programming , '' _ journal of control theory and applications _",
    ", vol .  9 , no .  3 , pp .",
    "361369 , 2011 .",
    "doi : 10.1007/s11768 - 011 - 0178 - 0 .",
    "k.  g. vamvoudakis , d.  vrabie , and f.  l. lewis , `` online adaptive algorithm for optimal control with integral reinforcement learning , '' _ international journal of robust and nonlinear control _",
    "24 , no .",
    "17 , pp .",
    "26862710 , 2014 .",
    "j. werbos , `` reinforcement learning and approximate dynamic programming ( rladp)-foundations , common misconceptions , and the challenges ahead , '' in _ reinforcement learning and approximate dynamic programming for feedback control _ ( f.  l. lewis and d.  liu , eds . ) , pp .  130 , john wiley & sons , 2012 .",
    "c.  park , g.  guibout , and d.  scheeres , `` solving optimal continuous thrust rendezvous problems with generating functions , '' _ journal of guidance , control , and dynamics _ , vol .  29 , no .  2 , pp .  321331 , 2006",
    "doi : 10.2514/1.14580 .",
    "a.  heydari and s.  n. balakrishnan , `` global optimality of approximate dynamic programming and its use in non - convex function minimization , '' _ applied soft computing _",
    "24 , pp .",
    "291303 , 2014 .",
    "doi : 10.1016/j.asoc.2014.07.003 ."
  ],
  "abstract_text": [
    "<S> this study is aimed at answering the famous question of how the approximation errors at each iteration of approximate dynamic programming ( adp ) affect the quality of the final results considering the fact that errors at each iteration affect the next iteration . to this goal </S>",
    "<S> , convergence of value iteration scheme of adp for deterministic nonlinear optimal control problems with undiscounted cost functions is investigated while considering the errors existing in approximating respective functions . </S>",
    "<S> the boundedness of the results around the optimal solution is obtained based on quantities which are _ known _ in a general optimal control problem and assumptions which are _ </S>",
    "<S> verifiable_. moreover , since the presence of the approximation errors leads to the deviation of the results from optimality , sufficient conditions for stability of the system operated by the result obtained after a _ finite _ number of value iterations , along with an estimation of its _ region of attraction _ , are derived in terms of a _ </S>",
    "<S> calculable _ upper bound of the control approximation error . </S>",
    "<S> finally , the process of implementation of the method on an orbital maneuver problem is investigated through which the assumptions made in the theoretical developments are verified and the sufficient conditions are applied for guaranteeing stability and near optimality . </S>"
  ]
}