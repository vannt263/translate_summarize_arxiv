{
  "article_text": [
    "exploratory data analysis regarding asymmetry in data is usually based on histograms and nonparametric density estimates , and statements such as `` this data set looks almost gaussian , but it is skewed to the right '' or `` these asset returns have heavy tails , but they are too skewed that a student-@xmath5 would make sense '' are fairly common .",
    "it is therefore natural to generalize symmetric distributions to allow for asymmetry .",
    "a prominent generalization is the skew - normal distribution [ azzalini(@xcite ) ] , which includes the gaussian as a special case . a skew - normal random variable ( rv )",
    "is defined by having the probability density function ( p.d.f . ) @xmath6 , where @xmath7 is the cumulative distribution function ( c.d.f . ) of a standard gaussian , and @xmath8 is the skew parameter .",
    "this approach to skewness has not only led to substantial research in the skew - normal case [ @xcite , @xcite ] , but the same concept has also been used for student-@xmath5 [ @xcite ] and cauchy distributions [ @xcite , @xcite ] , among others . in all these cases , a parametric manipulation of the original symmetric p.d.f . introduces skewness .    notwithstanding the huge success of this approach to model skewed data , manipulating the p.d.f . to introduce skewness seems like putting the cart before the horse : densities are skewed , because the random variable is  not the other way around . also , applied research starts with data , not with histograms",
    ".    motivated by this data - driven view on skewness , i propose a novel approach to asymmetry : lambert @xmath9 distributions emerge naturally by modeling the observable rv @xmath2 as the output of a system @xmath10 driven by random input @xmath0 with c.d.f .",
    "@xmath11 . here",
    "@xmath10 can either be a real chemical , physical , or biological system , or refer to any kind of mechanism in a broader sense . in statistical modeling",
    "such a system is simply represented by transformations of rvs .",
    "as there are no restrictions on @xmath11 , this is a very general framework that can be analyzed in detail for a particularly chosen input c.d.f .",
    "figure [ fig : lambertw_flowchart ] illustrates the methodology .",
    "for instance , consider @xmath10 being the stock market , where people buy and sell an asset according to its expected success in the future .",
    "asset returns , that is , the percentage change in price , typically exhibit negative skewness and positive excess kurtosis ",
    "so - called _ stylized facts _ [ @xcite , @xcite ] .",
    "the left panel of figure [ fig : latam_news4 ] shows daily log - returns ( in percent ) @xmath12 of an equity fund investing in latin america ( latam ) .",
    "also , these returns are clearly non - gaussian given their excess kurtosis ( @xmath13 ) and large negative skewness ( @xmath14)see table [ tab : latam_summary ] .",
    "the excess kurtosis is typically addressed by a student @xmath5-distribution , but here a kolmogorov ",
    "smirnov test still rejects @xmath15 on a @xmath16 level ( even for the estimated  @xmath17 ) , as the empirical skewness is too large .",
    "thus , to model the probabilistic properties of such data , asymmetric distributions must be used .",
    "approach to asymmetry : ( left ) an input / output system @xmath10 transforms ( solid arrows ) input @xmath18 to output @xmath19 lambert @xmath20 and herewith introduces skewness ; ( right ) inference about skewed data @xmath21 : ( 1 )  unskew observed @xmath22 to latent symmetric data @xmath23 , ( 2 ) use methods of your choice ( regression , time series models , quantile estimation , hypothesis tests , etc . ) for statistical inference based on @xmath24 , and ( 3 ) convert results back to the `` skewed world '' of @xmath22 . ]     and estimated latent news @xmath25 ; ( right ) gaussian qq plot and lambert @xmath26 qq plot of @xmath27 . ]",
    "using lambert @xmath28 rvs to model the asymmetry in asset returns is perfectly suitable not only given empirical evidence of `` almost student-@xmath5 , but a little skewed data , '' but also by a more fundamental viewpoint .",
    "price changes are commonly considered as the result of bad and good news hitting the market : bad news , negative returns ; good news , positive returns .",
    "the empirical evidence of negative skewness evokes the following question : why should news per se be negatively skewed ? or put in other words : do really bad things happen more often than really good things ?    in the lambert @xmath1 framework",
    "this news @xmath29 return relation is modeled under the assumption that the probability of getting negative news is about the same as of getting positive news , but typically people react far more drastically facing negative than positive ones . thus , news @xmath30 are symmetrically distributed , the market @xmath10 acts as an asymmetric filter , and the measurable / observable outcome is a skewed rv @xmath2/data @xmath27 .",
    "last , the right part of figure [ fig : lambertw_flowchart ] also illustrates a very pragmatic , yet useful way to exploit the lambert @xmath1 framework for ( slightly ) skewed data .",
    "if a  certain statistical procedure or model assumes a symmetric  a gaussian , as often is the case  distribution and no skewed implementation of this method is available , then instead of applying it to the skewed @xmath22 , it is advisable to work with the `` symmetrized '' data @xmath31 , make statistical inference about @xmath0 based on @xmath31 , and then transform the obtained results back to the `` skewed world '' of @xmath2 .",
    "although this is only an approximation to the truth , at least this approach takes skewness into consideration instead of  ignoring  it .",
    "275pt@ld2.3d2.3d2.3@ * latam * & & & + min & -6.064 & -5.073 & -4.985 + max & 5.660 & 7.036 & 7.268 + mean & 0.121 & 0.190 & 0.198 + median & 0.138 & 0.138 & 0.138 + st .",
    "1.468 & 1.456 & 1.457 + skewness & -0.433 & 0.000 & 0.053 + kurtosis & 1.201 & 1.100 & 1.159 + @xmath32 & 6.220 & 7.093 & 7.048 + [ 6pt ]",
    "sw & 0.000 & 0.000 & 0.000 + jb & 0.000 & 0.000 & 0.000 + [ 3pt ] ks ( @xmath33 ) & 0.028 & 0.088 & 0.102 +    section [ sec : lambertw_rv ] defines lambert @xmath1 rvs and their basic properties are studied .",
    "section  [ sec : dist_dens ] presents analytic expressions of the c.d.f .",
    "@xmath34 and p.d.f .",
    "@xmath35 , which are particular variants of their input counterparts .",
    "after studying gaussian input in section [ sec : gaussian ] , various estimators for the parameter vector of lambert @xmath28 rvs are introduced in section [ sec : estimation ] .",
    "section [ sec : simulations ] compares their finite sample properties and shows that additional estimation of the skewness parameter @xmath36 does not affect the quality of other parameter estimates .",
    "this new class of distribution functions is particularly useful for data with slightly negative skewness , thus , section [ sec : applications ] demonstrates its adequacy on an australian athletes data set and on the latam return series .",
    "in particular , section [ sec : financial_data ] shows that the input - output system ( figure  [ fig : lambertw_flowchart ] ) with student-@xmath5 input @xmath0 is a proper model for these returns .",
    "a detailed comparison of quantile estimates , which are essential to get appropriate risk measures of an asset , confirms the aptness of lambert @xmath1 distributions ( see lambert @xmath1 qq plot in figure [ fig : latam_news4 ] ) .",
    "empirical evidence for the significance of conditional heteroskedastic time series models using lambert @xmath37 innovations concludes section [ sec : financial_data ] .",
    "finally , section [ sec : tukeys_h ] establishes a direct link of this new class of distributions to the existing statistics literature , noting that the square of a rv having tukey s @xmath38 distribution [ @xcite ] has a lambert @xmath39 distribution .",
    "computations , figures and simulations were realized with the open - source statistics package r [ @xcite ] .",
    "functions used in the analysis are available as the r package http://cran.r-project.org/web/packages/lambertw/index.html[`lambertw ` ] , which provides many other methods to perform lambert @xmath1 inference in practice .",
    "the general notion of a system @xmath10 with random input and output as shown in figure [ fig : lambertw_flowchart ] translates to a variable transformation in statistical terminology .",
    "[ def : noncentral_nonscaled_lambertw ] let @xmath40 be a continuous rv with c.d.f . @xmath41 and p.d.f . @xmath42 .",
    "then @xmath43 is a _",
    "noncentral , nonscaled lambert @xmath28 _ rv with skewness parameter @xmath3 .",
    "if @xmath40 is from a parametric family @xmath44 , where @xmath45 parametrizes the @xmath46 , then @xmath47 is a noncentral , nonscaled lambert @xmath28 rv with parameter vector @xmath48 .",
    "the key of this family of rvs is @xmath3 , which can take any value on the real line .",
    "as @xmath49 is always positive , @xmath40 and @xmath47 have the same sign . for readability",
    "let @xmath50 . for @xmath51 transformation ( [ eq : noncentral_nonscale_lambertw_y ] ) reduces to the identity @xmath52 ; thus , @xmath47 possesses the exact same properties as @xmath40 . by continuity of @xmath53",
    ", one can expect for @xmath54 but close , also @xmath55 but close.=1    transformation ( [ eq : noncentral_nonscale_lambertw_y ] ) indeed describes a system @xmath10 with an asymmetry property : let @xmath56 be a symmetric zero - mean rv , then @xmath47 is a skewed version of @xmath40depending on the sign of @xmath3 . for @xmath57",
    "negative @xmath40 are amplified by the factor @xmath58 and positive @xmath40 are damped by : @xmath47 is skewed to the left .",
    "for @xmath59 the same reasoning shows that @xmath47 is a  positively .",
    "the noncentral moments @xmath60 equal @xmath61    if the moment - generating function @xmath62 exists for @xmath63 , then ( [ eq : non_contral_mom_z ] ) can be rewritten to get a more tractable formula . as @xmath64 interchanging differentiation and the integral sign yields @xmath65",
    "if @xmath66 does not exist ( e.g. , for student-@xmath5 @xmath40 ) , then ( [ eq : non_contral_mom_z ] ) must be calculated explicitly .      in a typical input / output system @xmath10 such as a  microphone / loudspeaker setting",
    ", the loudspeaker will be louder if speakers raise their voice . in this sense",
    "it is stable with respect to scaling : doubling the volume of the input doubles the volume of the loudspeakers  the signal is not affected in any other way .",
    "viewing this system as a lambert @xmath28 rv system ( where the signal is considered as a rv ) , multiplying @xmath0 by a  factor  @xmath67 , should  ceteris paribus  only affect the output @xmath2 by multiplying by  @xmath67 ; other properties , such as skewness or kurtosis , should not be altered.=-1    transformation ( [ eq : noncentral_nonscale_lambertw_y ] ) , however , does not have this scaling property of @xmath40 . hence , to allow a comparable system characterization via @xmath3 among different scalable data sets define a scaled lambert @xmath1 rv .",
    "[ def : scale_lambertw ] let @xmath68 be the unit - variance version of a continuous rv @xmath0 from a scale family @xmath69 , where @xmath45 is the parameter ( vector ) of @xmath70 and @xmath71 the standard deviation of  @xmath0 . then @xmath72 is a _ scale lambert @xmath28 _ rv with parameter vector @xmath48 .    transformation ( [ eq : scaled_lambertw_y ] ) is invariant to scaling of the input , for example , a  different measurement unit for the input does not modify the asymmetry property of the system , but just scales the output accordingly .",
    "here @xmath71 is a function of @xmath45 : for an exponentially distributed input @xmath73 , @xmath74 and @xmath75 ; an input @xmath0 having a gamma distribution with shape @xmath76 and rate @xmath77 gives @xmath78 and @xmath79 .",
    "the focus of this work lies in introducing skewness to symmetric rvs with support on @xmath80 , such as a  gaussian or student-@xmath5 .",
    "these distributions are not only scale , but also shift invariant , a property lambert @xmath28 distribution should also have for location - family input .",
    "however , transformation ( [ eq : scaled_lambertw_y ] ) is not shift - invariant .",
    "for example , consider a zero - mean and unit variance input rv @xmath81 , @xmath82 , and let @xmath83 .",
    "if @xmath84 is close to @xmath85 , then the shifted @xmath86 will be close to @xmath87 .",
    "for the corresponding @xmath88 and @xmath89 this does not hold : @xmath90 is close to @xmath85 , but @xmath89 will not be shifted by @xmath87 , but lies close to @xmath91 .",
    "[ def : location_scale_lambertw ] let @xmath0 be a rv from a location - scale family with c.d.f .",
    "@xmath92 with mean @xmath93 and standard deviation @xmath71 ; again @xmath45 parametrizes @xmath70 .",
    "let @xmath94 be the zero - mean , unit - variance version of @xmath0 .",
    "then @xmath95 is a _ location - scale lambert @xmath28 _ rv with parameter vector @xmath48 .    as before",
    ", the parameter @xmath3 regulates the closeness between @xmath0 and its skewed version @xmath2 .    for a full parametrization of a lambert @xmath28 distribution",
    "it is necessary to know @xmath48 ; viewing ( [ eq : location_scale_lambertw_y ] ) only as a transformation from @xmath0 to @xmath2 , it is more natural  and in practice more useful  to only consider @xmath93 , @xmath71 and  @xmath3 , ignoring the particular structure of @xmath0 given its parametrization by  @xmath45 . in order to distinguish these two cases in the remaining part of this work",
    "let @xmath96 .",
    "clearly , @xmath97 can be computed from @xmath98 , @xmath99 , but not necessarily vice - versa .",
    "for example , for a gaussian @xmath0 @xmath100 since @xmath101 and @xmath102 . in contrast , for a location - scale student-@xmath5 input with",
    "@xmath103where @xmath104 is the location , @xmath105 the scale and @xmath17 the degrees of freedom parameter@xmath106 : @xmath107 and @xmath108 if @xmath109 .",
    "thus , below i use either @xmath98 if the full parametrization is important or @xmath97 if it is sufficient to consider ( [ eq : location_scale_lambertw_y ] ) only as a transformation rather than a fully specified parametric distribution .",
    "[ notation : lambertw_f ] for simplicity i will refer to all  @xmath2 in definitions [ def : noncentral_nonscaled_lambertw ] , [ def : scale_lambertw ] and [ def : location_scale_lambertw ] as a _",
    "lambert @xmath28 _ rv .",
    "which one of the three transformations ( [ eq : noncentral_nonscale_lambertw_y ] ) , ( [ eq : scaled_lambertw_y ] ) or ( [ eq : location_scale_lambertw_y ] ) is used to generate @xmath2 will be clear from the type of input @xmath0 .",
    "for example , since a @xmath110 distribution does not have location or scale parameters , a lambert @xmath111 rv refers to @xmath2 in definition [ def : noncentral_nonscaled_lambertw ] ; the exponential distribution is a scale family , thus , a lambert @xmath112 rv @xmath2 is defined in definition  [ def : scale_lambertw ] ; and for gaussian input @xmath0 , the corresponding lambert @xmath113gaussian @xmath2 refers to definition [ def : location_scale_lambertw ] .",
    "transformation having @xmath114 , and a noncentral , nonscaled lambert @xmath28 transformation having @xmath115 .",
    "this is especially useful for empirical work and implementation of the methods involving lambert @xmath28 rvs . ]",
    "so far attention has been drawn to @xmath2 and its properties given @xmath0 and @xmath98 ( or @xmath97 ) .",
    "now consider the inverse problem : given @xmath2 and  @xmath98 ( or only  @xmath97 ) , what does @xmath0 look like ?",
    "this is not only interesting for a latent variable interpretation of @xmath0 , but the inverse of a transformation is essential to derive the c.d.f . of the transformed variable . before analyzing transformation ( [ eq : location_scale_lambertw_y ] ) , consider the nonlinear transformation @xmath116 [ figure [ fig : events_z_events_u ] shows @xmath117 only for @xmath118 . for positive",
    "@xmath119 the function is bijective and resembles @xmath120 very closely . for negative @xmath119",
    ", however , @xmath117 is quite different from @xmath120 : it takes on negative values , its minimum value equals @xmath121 , and  most importantly  it is nonbijective .",
    "function : transformation @xmath117 and the two inverse branches of @xmath122 for @xmath123 : principal branch ( dashed curve ) and nonprincipal branch ( dotted curve ) . ]",
    "although @xmath117 has no analytical inverse [ @xcite ] , its implicitly defined inverse function is well known in mathematics and physics .",
    "the many - valued function @xmath122 is the root of @xmath124 and is commonly denoted as the _ lambert @xmath1 function_.    generally the lambert @xmath1 function is defined for any @xmath125 . since lambert @xmath1 rvs are only defined for real - valued outcomes , in this work the domain and image of the lambert @xmath1 function is restricted to the reals . for @xmath126 no real solution exists ; for @xmath127 @xmath122",
    "is a real - valued function . if @xmath128 , there are two real solutions : the principal branch @xmath129 and the nonprincipal branch @xmath130 ; for @xmath131 only one real - valued solution exists , @xmath132 ( see figure [ fig : events_z_events_u ] ) . for a detailed review including useful properties and functional identities of @xmath122 see @xcite , @xcite and the references therein .",
    "figure [ fig : events_z_events_u ] also shows how skewness is introduced via transformation ( [ eq : location_scale_lambertw_y ] ) .",
    "symmetric input @xmath0 ( @xmath133-axis ) is mapped to asymmetric output @xmath2 ( @xmath134-axis ) due to the curvature of @xmath117 .",
    "analogously , mapping values from the @xmath134-axis to the @xmath133-axis `` unskews '' them .",
    "figure [ fig : events_z_events_u ] shows @xmath135 for @xmath136 , thus , its inverse is lambert s @xmath1 function ( @xmath137 ) .",
    "the curvature of @xmath138 depends on the skewness parameter : for @xmath51 no curvature is present [ @xmath139 ; higher @xmath3 results in more curvature , and thus more skewness in @xmath2 .",
    "it can be easily verified that @xmath140 is the inverse function of transformation ( [ eq : noncentral_nonscale_lambertw_y ] ) .",
    "hence , given @xmath2 and @xmath97 , the unobservable input @xmath0 can be recovered via @xmath141 for empirical work it is important to point out that ( [ eq : backtrafo_normalized_x ] ) does not require specific knowledge about @xmath70 or @xmath45 ; @xmath93 and @xmath71 ( and @xmath3 ) suffice .",
    "this will become especially useful for estimating the optimal inverse transformation ",
    "see section [ sec : igmm ] .",
    "the lambert @xmath1 function has two branches on the negative real line ( figure [ fig : events_z_events_u ] ) , so transformation ( [ eq : backtrafo_normalized_x ] ) is not unique .",
    "for example , consider @xmath142 and @xmath143 .",
    "the two real - valued solutions are @xmath144 and @xmath145 . assuming a stable input / output system",
    ", only the principal branch makes sense , it is more reasonable to assume that this corresponds to the close input of @xmath146 rather than the very extreme @xmath145.]denoted by @xmath147 . if the nonprincipal solution is required , @xmath148 will be used .",
    "the probability @xmath149 that the observed value @xmath150 was indeed caused by the nonprincipal solution is at most @xmath151 , since @xmath152 changes its monotonicity at @xmath153 . for gaussian @xmath0 and @xmath154a very large value given empirical evidence ",
    "this probability equals @xmath155 . for an input with student @xmath5-distribution and @xmath156 degrees of freedom @xmath157 .",
    "hence , ignoring the nonprincipal root to obtain unique latent data should not matter too much in practice .",
    "data vector @xmath22 ; parameter vector @xmath158 . input vector @xmath159",
    ".    @xmath160 .",
    "back - transform @xmath161 via the principal branch to @xmath162 .",
    "@xmath163 .",
    "algorithm [ alg : get.input ] describes the empirical version of ( [ eq : backtrafo_normalized_x ] ) .",
    "the so obtained @xmath164 is the input data @xmath159 generating the observed @xmath22 and should have c.d.f .",
    "@xmath11 . here",
    "@xmath165 does not stand for an estimate of @xmath97 , but since @xmath166 ignores the nonprincipal branch , algorithm [ alg : get.input ] need not return the `` true '' input data @xmath167even if @xmath97 is known .",
    ", as otherwise the back - transformation @xmath168 is bijective .",
    "in particular , if @xmath169for example , for scale family input @xmath170then @xmath171 , not just an approximation .",
    "see also corollary [ cor : cdf_pdf_scale_lambertw ] .",
    "] for small @xmath3 , @xmath172 will most likely equal the true @xmath167 for all @xmath173 ; for large @xmath3 some @xmath174 s might be falsely assigned to the principal  @xmath175 s , although these @xmath174 s were actually caused by nonprincipal @xmath175 s . for an estimate @xmath176 the notation @xmath177 will be used , which itself is an approximation to  @xmath159 .",
    "for ease of notation and readability let @xmath178 \\\\[-8pt ] x_{0 } & : = & u_{0 } \\sigma_x + \\mu_x , \\qquad x_{-1 } : = u_{-1 } \\sigma_x + \\mu_x . \\nonumber \\ ] ]    by definition , @xmath179    the transformation @xmath152 changes its monotonicity at @xmath180 and its inverse @xmath181 at @xmath182 .",
    "consequently , the event @xmath183 [ for @xmath184 has  to be split up into separate events in @xmath40 to derive the distribution of @xmath2 .    [ theorem : cdf_y ] the c.d.f . of a location - scale lambert @xmath28 rv @xmath2 equals ( for @xmath185 ) @xmath186 the case @xmath57 can be obtained analogously and for @xmath51 it is clear that @xmath187 .    follows by matching the events in @xmath47 with the corresponding events in @xmath40 [ @xcite ] ; see figure [ fig : events_z_events_u ] .    for @xmath188 both branches of @xmath122",
    "coincide , thus , @xmath189 .",
    "therefore , @xmath190 at @xmath191 , which implies continuity of @xmath34 at @xmath192 ; the same reasoning shows continuity at @xmath193 ( @xmath194 ) .",
    "[ theorem : pdf_y ] the p.d.f . of a location - scale lambert",
    "@xmath28 rv @xmath2 equals ( for @xmath185 ) @xmath195    again , @xmath57 can be obtained analogously , and @xmath196 .    using that @xmath197 , the first derivative of @xmath34 with respect to @xmath134 equals ( [ eq : pdf_lambertw_y ] ) .",
    "the same arguments as for @xmath34 show that  @xmath35 is continuous at @xmath198 and @xmath199 .    in general",
    ", the support of @xmath2 depends on @xmath200 if @xmath54 .",
    "however , restricting @xmath97 to the subspace @xmath201 gives the same support @xmath202 for all @xmath203 [ or @xmath204 $ ] for @xmath57 ] .",
    "of particular empirical importance are @xmath206    for ( a scale family ) @xmath207 taking values in @xmath208 and @xmath209 , the support of the corresponding ( scale ) lambert @xmath28 rv y does not depend on @xmath97 but always equals @xmath208 .    [ cor : cdf_pdf_scale_lambertw ] if  @xmath0 is a nonnegative rv taking values in @xmath208 and @xmath210 , then the inverse transformation @xmath211 is unique .",
    "hence , the c.d.f . and p.d.f .",
    "of a scale lambert @xmath28 rv @xmath2 equal @xmath212 and @xmath213    follows by setting @xmath214 in ( [ eq : define_z_u_x ] ) , ( [ eq : cdf_lambertw_y ] ) and ( [ eq : pdf_lambertw_y ] ) , and noting that the case @xmath215 does not exist since @xmath170 .",
    "rv y for different degrees of skewness . ]    for the c.d.f . and p.d.f .",
    "of a noncentral , nonscaled lambert @xmath28 rv y ( definition  [ def : noncentral_nonscaled_lambertw ] ) with @xmath0 taking values in @xmath208 set @xmath216 in ( [ eq : cdf_lambertw_scale_y ] ) and ( [ eq : pdf_lambertw_scale_y ] ) .",
    "theorems [ theorem : cdf_y ] and [ theorem : pdf_y ] demonstrate the great flexibility of the lambert  @xmath1 setting , since the closed form expressions for @xmath217 and @xmath218 hold for any well - defined input @xmath219 and @xmath220 , respectively .",
    "thus , researchers can easily create lambert @xmath1 variants of their favorite distribution @xmath70 , by simply plugging @xmath70 and @xmath221 in ( [ eq : cdf_lambertw_y ] ) and ( [ eq : pdf_lambertw_y ] ) .",
    "figure [ fig : lambertw_densities ] shows the p.d.f . and c.d.f .",
    "of the three lambert@xmath222 rvs discussed in notation  [ notation : lambertw_f ] for four degrees of skewness , @xmath223 .",
    "for @xmath51 the output  @xmath2 equals the input @xmath0 , thus , also their p.d.f.s / c.d.f.s coincide ( solid black lines ) . with increasing @xmath3 , the rv @xmath2and thus its distribution and density ",
    "become more and more skewed to the right ( since @xmath185 ) .",
    "although lambert @xmath1 rvs are defined by transformation ( [ eq : location_scale_lambertw_y ] ) , they can be also considered as a particular variant of an arbitrary @xmath70independent of this transformation .",
    "sometimes the input / output aspect might be more insightful ( e.g. , stock returns ) , whereas otherwise solely the generalized distribution suffices to analyze a given data set .",
    "especially , if the latent variable  @xmath0 does not have any suitable interpretation ( see bmi data in section  [ sec : applications ] ) , one can concentrate on the probabilistic properties of @xmath2 , ignoring the input  @xmath0 .",
    "equation ( [ eq : cdf_lambertw_y ] ) and an inspection of figure [ fig : events_z_events_u ] directly relate @xmath93 to @xmath2 .",
    "[ cor : y_median ] for a location - scale lambert @xmath1 rv @xmath2 , @xmath224 in particular , @xmath93 equals the median of @xmath2 , if @xmath0 is symmetric .",
    "the transformation @xmath138 passes through @xmath225 for all @xmath226 .",
    "furthermore , @xmath227 for all @xmath3 and all @xmath228 .",
    "therefore , @xmath229 for symmetric input @xmath230 , therefore , @xmath93 is the median of @xmath2 .",
    "corollary [ cor : y_median ] not only gives a meaningful interpretation of the parameter  @xmath93 for symmetric input , but the sample median of @xmath22 also yields a robust estimate of @xmath231 .",
    "in general , the @xmath76-quantile @xmath232 of @xmath2 satisfies @xmath233    for @xmath185 ( @xmath234 analogously ) and @xmath235 the function @xmath236 is bijective .",
    "thus , @xmath237 and by definition of the @xmath76-quantile of @xmath0 , @xmath238 where @xmath239 .    for @xmath240 and @xmath185 ,",
    "however , @xmath241 is not bijective , thus , @xmath242 can not be computed explicitly as in ( [ eq : x_alpha ] ) , but must be obtained by solving the implicit equation @xmath243 in either case , the @xmath76-quantile of @xmath2 equals @xmath244 .",
    "the results so far hold for any continuous input rv . to get a better insight consider gaussian input @xmath245 as a special case ; here @xmath246 .",
    "its moment generating function @xmath66 equals @xmath247    therefore , noncentral moments of @xmath47 can be computed explicitly [ see ( [ eq : non_central_mom_z_derivative ] ) ] by @xmath248    in particular , @xmath249 \\sigma_z^2 & = & 2^{-2 } \\bigl ( e^{2 \\gamma\\mu_u + 2 \\gamma^2 \\sigma _ u^2 } \\bigl((4 \\gamma\\sigma_u^2 + 2 \\mu_u)^2 + 4 \\sigma_u^2\\bigr )   \\bigr ) - ( \\mu_u + \\gamma\\sigma_u^2)^2 e^{2 \\gamma\\mu_u + \\gamma^2 \\sigma _ u^2}\\\\[2pt ] & = & e^{2 \\gamma\\mu_u + 2 \\gamma^2 \\sigma_u^2 } \\bigl((2 \\gamma\\sigma",
    "_ u^2 + \\mu_u)^2 + \\sigma_u^2\\bigr ) - ( \\mu_u + \\gamma\\sigma_u^2)^2 e^{2 \\gamma\\mu_u + \\gamma^2 \\sigma_u^2}.\\end{aligned}\\ ] ] as already mentioned in section [ sec : lambertw_rv ] , this is an unstable system , in the sense that a small perturbation in @xmath250 results in a completely different @xmath251 for @xmath54 .    in contrast",
    ", the central moments of a location - scale lambert @xmath252gaussian rv @xmath2 with input @xmath253 have a much simpler and stable form @xmath254 since @xmath255 . using ( [ eq : mu_y_lambertw_gaussian ] ) , the @xmath256th central moment of @xmath2 can be expressed by the @xmath256th central moment of @xmath257 , @xmath258    in particular , @xmath259 which only depends on the input variance and the skewness parameter @xmath3 .",
    "the main motive to introduce lambert @xmath1 rvs is to accurately model skewed data .",
    "the skewness coefficient of @xmath2 is defined as @xmath260 .",
    "analogously , the kurtosis equals @xmath261 and measures the thickness of tails of @xmath2 .",
    "[ lem : skew_kurt_lambertw_gaussian ] for a location - scale lambert @xmath113gaussian rv with input @xmath262 , @xmath263\\end{aligned}\\ ] ] and @xmath264    dividing the third and fourth derivative of the moment generating functions for a standard gaussian @xmath40 at @xmath265 with respect to @xmath3 by  @xmath266 gives @xmath267 the rest follows by expanding @xmath268 and @xmath269 via the binomial formula and using the above expressions .",
    "as expected , the skewness coefficient is an odd function in @xmath3 with the same sign as @xmath3 . on the contrary",
    ", @xmath270 is even . a first and second order taylor approximation around @xmath271 yields @xmath272 and @xmath273 , respectively .",
    "although @xmath3 can take any value in @xmath274 , in practice , it rarely exceeds @xmath275 in absolute value . in this interval",
    "the taylor approximation is almost indistinguishable from the true function ( figure  [ fig : skew_kurt_y]).=1    $ ] and its first order taylor approximation ( dashed line ) ; and : zoom to the interval @xmath276 $ ] . ]",
    "this first order approximation to @xmath277 offers a rule of thumb @xmath278 which can be used as a starting value for better algorithms such as igmm and mle ( see section [ sec : estimation ] ) .",
    "[ cor : limit_skew_kurt_lambertw_gaussian ] the skewness and kurtosis coefficient are unbounded for @xmath279 , that is , @xmath280    omitting @xmath281 in the denominator and @xmath282 in the numerator of the skewness coefficient can be bounded from below @xmath283 as the exponential function dominates rational functions , the first term tends to  @xmath284 , whereas the second one goes to @xmath85 for @xmath36 to @xmath284 .    in case of the kurtosis coefficient , the term @xmath285 in the numerator dominates all other terms for large @xmath3 and thus determines the asymptotic behavior of @xmath270 for @xmath36 to @xmath286 .",
    "this result shows that the lambert @xmath113gaussian distributions can be used to model a larger variety of skewed data than a skew - normal distribution , since its skewness coefficient is restricted to the interval @xmath287 [ @xcite ] .",
    "for a sample of @xmath288 independent identically distributed ( i.i.d . ) observations @xmath289 , which presumably originates from transformation ( [ eq : location_scale_lambertw_y ] ) , @xmath48 has to be estimated from the data .",
    "in addition to the commonly used maximum likelihood estimator ( mle ) for  @xmath98 , i also present a method of moments estimator for @xmath97 that builds on the input / output relation in figure [ fig : lambertw_flowchart ] .      the log - likelihood function in the i.i.d .",
    "case equals @xmath290 where @xmath291 is the p.d.f . of @xmath2see ( [ eq : pdf_lambertw_y ] ) .",
    "the mle is that @xmath292 which maximizes the log - likelihood @xmath293    since @xmath294 is a function of @xmath295 , the mle depends on the specification of the input density . in general , this multivariate , nonlinear optimization problem must be carried out by numerical methods , as the two branches of @xmath122 for @xmath296 do not allow any further simplification .    for ( scale )",
    "lambert @xmath9 with support in @xmath297 and @xmath209 , however , @xmath298 ( corollary [ cor : cdf_pdf_scale_lambertw ] ) .",
    "thus , ( [ eq : likelihood_general ] ) can be rewritten as @xmath299 where @xmath300 is the log - likelihood of the back - transformed data @xmath301 [ no @xmath302 since the inverse is unique in this case ] .",
    "note that @xmath303 only depends on @xmath304 ( and @xmath3 ) , but not necessarily on every coordinate of @xmath45 .",
    "the equivalence ( [ eq : loglikelihood_y_with_xi ] ) shows the relation between the exact mle @xmath305 based on @xmath22 and the approximate mle @xmath306 based on @xmath307 : if we would know @xmath71 and @xmath3 beforehand , then we could just back - transform @xmath22 to @xmath308 and compute @xmath309 based on @xmath310 [ maximize ( [ eq : likelihood_x.hat ] ) ] ; however , in practice , @xmath71 and @xmath3 have to be estimated from @xmath22 and this uncertainty enters the log - likelihood ( [ eq : loglikelihood_y_with_xi ] ) by the additional term @xmath311 .    for @xmath312 it can be easily shown that @xmath313 as well as @xmath314 since @xmath315 and @xmath316 . hence , @xmath317 for @xmath185 and can be thought of as a penalty for transforming @xmath22 to the `` nicer '' @xmath318 with estimated parameters : the larger @xmath3 , the bigger the penalty on the log - likelihood @xmath319 of the `` nice '' back - transformed data , since @xmath320 .      for location - scale lambert @xmath28 rvs the support of @xmath35 depends on @xmath321 and therefore violates a crucial assumption of most results related to ( asymptotic ) properties of the mle . only for",
    "@xmath51 the support of @xmath322 does not depend on @xmath98 .",
    "for @xmath323 it can be shown that the fisher information matrix @xmath324 .",
    "hence , for the symmetric gaussian case @xmath325 .",
    "simulations in section [ sec : simulations ] confirm this asymptotic result and suggest that also for the general gaussian case @xmath326 is well behaved , that is , it is @xmath327-consistent and asymptotically efficient .",
    "a theoretical analysis of the asymptotic behavior of the mle for @xmath54 is beyond the scope of this study , but simulations show that also for parameter dependent support @xmath328 is an unbiased estimator with root mean square errors comparable to the @xmath329 case .      a disadvantage of the mle is the mandatory a - priori specification of the input distribution . in practice , however , it is rarely known what kind of distribution is a good fit to the data , even more so if the data is transformed via a nonlinear transformation .",
    "thus , here i present an iterative method to estimate the optimal inverse - transformation ( [ eq : backtrafo_normalized_x ] ) by estimating @xmath97 directly , instead of estimating  @xmath98 and then computing @xmath330 .",
    "this method builds on the input / output aspect and only relies upon the specification of the theoretical skewness of @xmath0 .",
    "the proposed estimator for @xmath97 works as follows ( see below for a more detailed discussion ) :    set starting values @xmath331 . set @xmath332 ;    [ item : gamma ] assume @xmath333 and @xmath334 are known and estimate @xmath3 from @xmath335 to obtain @xmath336 ( algorithm [ alg : gamma_gmm ] ) ;    [ item : mu_sigma ] assume @xmath337 is known and get estimates @xmath338 and @xmath339 from the back - transformed data @xmath340 ( algorithm [ alg : igmm ] ) .",
    "set @xmath341 ;    iterate between ( [ item : gamma ] ) and ( [ item : mu_sigma ] ) until convergence of the sequence @xmath342 .",
    "standardized data vector @xmath161 ; theoretical skewness @xmath343.@xmath344 as in ( [ eq : gamma_gmm ] ) .    compute lower and upper bound for @xmath3 : @xmath345 and @xmath346 .",
    "@xmath347 where @xmath348 subject to @xmath349 $ ] .",
    "@xmath344 .    for a moment",
    "assume that @xmath93 and @xmath71 are known and only @xmath3 has to be estimated .",
    "since @xmath93 and @xmath71 are known , we can consider @xmath350 .",
    "a natural choice for @xmath3 is the one that results in back - transformed data @xmath351 with sample skewness equal to the theoretical skewness of @xmath40 , which equals the theoretical skewness of  @xmath0 .",
    "formally , @xmath352 where @xmath353 is a proper norm in @xmath274 , for example , @xmath354 or @xmath355 .",
    "for example , let @xmath22 be positively skewed data , @xmath356 , and the input @xmath167 causing the observed @xmath22 is assumed / known to be symmetric , thus , @xmath357 . by the nature of transformation @xmath152 , the skewness parameter @xmath3 must be also positive and the taylor approximation of @xmath277 for gaussian input [ see ( [ eq : gamma_taylor_rule ] ) ] gives a good initial estimate @xmath358 .",
    "in the same way as the mapping @xmath359 introduces skewness , the inverse transformation @xmath360 results in less positively skewed  @xmath361 due to the curvature in @xmath241 ( see figure [ fig : events_z_events_u ] ) . as the initial guess  @xmath362 rarely gives exactly symmetric input , algorithm [ alg : gamma_gmm ] searches for a @xmath3 such that the empirical skewness of @xmath361 is as close as possible to the `` true '' skewness  @xmath343 .",
    "there are natural bounds for @xmath3 to guarantee the observability of @xmath22 , for example , a @xmath3 too large makes large negative observations in @xmath22 impossible ( due to the minimum at @xmath363 ; see figure [ fig : events_z_events_u ] ) . however , since @xmath22 has actually been observed , the search space for @xmath3 must be limited to the interval @xmath364 $ ] . if there exists a @xmath365 such that @xmath366 , then algorithm [ alg : gamma_gmm ] will return @xmath367 due to the monotonically increasing curvature of @xmath152 and @xmath368 respectively ; if there is no such @xmath369 , then algorithm [ alg : gamma_gmm ] returns either the lower or upper bound of  @xmath370 , depending on whether @xmath161 is negatively or positively skewed .    this univariate minimization problem with constraints",
    "can be carried out by standard optimization algorithms .    in practice , @xmath93 and @xmath71",
    "are rarely known but also have to be estimated from the data . as @xmath22 is shifted and scaled _ ahead of _",
    "the back - transformation  @xmath147 , the initial choice of @xmath93 and @xmath71 affects the optimal choice of @xmath3 .",
    "therefore , the optimal triple @xmath371 must be obtained iteratively .",
    "algorithm [ alg : igmm ] first computes @xmath372 using @xmath333 and @xmath334 from the previous step .",
    "this normalized output can then be passed to algorithm [ alg : gamma_gmm ] to obtain an updated @xmath373 . using this new @xmath337 , one can back - transform @xmath374 to the presumably zero - mean , unit - variance input @xmath375 .",
    "herewith we can obtain a  better approximation to the `` true '' latent @xmath167 by @xmath376 .",
    "however , @xmath336and therefore @xmath377has been obtained using @xmath333 and @xmath334 which are not necessarily the most accurate estimates in light of the updated approximation @xmath378 .",
    "thus , algorithm [ alg : igmm ] computes new estimates @xmath338 and @xmath339 by the sample mean and standard deviation of @xmath379 , and starts another iteration by passing the updated normalized output @xmath380 to algorithm [ alg : gamma_gmm ] to obtain a new @xmath381 .    data vector @xmath22 ; tolerance level @xmath382 ; theoretical skewness @xmath343 .",
    "igmm parameter estimate @xmath383 .",
    "set @xmath384 .",
    "starting values : @xmath385 , where @xmath386 and @xmath387 are the sample median and standard deviation of @xmath22 , respectively .",
    "@xmath389 see ( [ eq : gamma_taylor_rule ] ) for details .",
    "@xmath390 , pass @xmath374 to algorithm [ alg : gamma_gmm ] @xmath391 , back - transform @xmath374 to @xmath392 ; compute @xmath393 , update parameters : @xmath394 and @xmath395 , [ line : set_new_theta ] @xmath396 , @xmath397 .",
    "@xmath398 .",
    "the algorithm returns the optimal @xmath399 once the estimated parameter triple does not change anymore from one iteration to the next , that is , if @xmath400 .",
    "a great advantage of the igmm estimator is that it does not require any further specification of the input except its skewness .",
    "for example , no matter if the input is normally , student-@xmath5 , laplace or uniformly distributed , the igmm estimator finds a @xmath97 that gives symmetric @xmath177 independent of the particular choice of ( symmetric ) @xmath401 .",
    "a disadvantage of igmm from a probabilistic point of view is its determination .",
    "in general , algorithm [ alg : igmm ] will lead to back - transformed data with sample skewness identical to @xmath343 and so no stochastic element remains in the nature of the estimator . depends on one or more parameters of the distribution of @xmath0 ( e.g. , gamma ) , then the igmm algorithm must be adapted to this very problem .",
    "] note that igmm does not provide an estimate of @xmath45 ( except for gaussian input ) ; if necessary , an estimate of @xmath45 must be obtained in a separate step , for example , by estimating @xmath45 from the back - transformed data @xmath177 . however , in general , @xmath309 estimated only from @xmath177 is ( slightly ) different from @xmath309 using lambert @xmath1 mle on the original data  @xmath22 : in the first case @xmath402 is assumed to be known and fixed , whereas in the second case @xmath45 and @xmath97 are estimated jointly [ see ( [ eq : loglikelihood_y_with_xi ] ) ] .",
    "the underlying input data @xmath403 can be approximated via algorithm  [ alg : get.input ] using @xmath404 .",
    "the so obtained @xmath405 may then be used to check if @xmath0 has characteristics of a known parametric distribution @xmath219 , and thus is an easy , but heuristic check if @xmath22 follows a particular lambert @xmath9 distribution .",
    "however , such a test can only serve as a rule of thumb for various reasons : ( i ) @xmath406 , thus tests are too optimistic as @xmath407 will have `` nicer '' properties regarding @xmath70 than the true @xmath167 would have ; ( ii ) ignoring the nonprincipal branch alters the sample distribution of the input  putting no observations to the far left ( or right ) : not so much of a problem for small  @xmath3 , the distribution can be truncated considerably for large @xmath3 . for gaussian input various tests",
    "are available [ jarque  bera , shapiro ",
    "wilk , among others ; see @xcite ] , for other distributions a kolmogorov ",
    "smirnov test can be used .. ]      for gaussian @xmath0 the system of equations @xmath408 has a unique solution for @xmath409 .",
    "given @xmath410 and the sample moments  @xmath411 and @xmath412 , the input parameters @xmath93 and @xmath413 can be obtained by @xmath414 hence , line [ line : set_new_theta ] of algorithm [ alg : igmm ] can be altered to @xmath415    number of observations @xmath173 ; parameter vector @xmath416 ; specification of the input distribution @xmath417 ; skewness parameter @xmath3 .",
    "random sample @xmath418 of a lambert @xmath419 rv .",
    "simulate @xmath173 samples @xmath420 .",
    "compute @xmath421 and @xmath422 given the type of lambert @xmath28 distribution ( noncentral , nonscale ; scale ; location - scale ) .",
    "@xmath425 .",
    "even though this simplification would lead to a faster estimation of @xmath97 , it is mostly of theoretical interest , as it can not be guaranteed that @xmath0 indeed is gaussian ; the more general algorithm [ alg : igmm ] should be used in practice .",
    "reported in section [ sec : simulations ] were obtained using the more general algorithm with line [ line : set_new_theta ] , not [ line : set_new_theta]b . ]",
    "although the c.d.f . , p.d.f . and moments of a lambert  @xmath1 rvs are nontrivial expressions , their simulation is straightforward ( algorithm [ alg : sim_lambertw ] ) .",
    "this section explores the finite - sample properties of estimators for @xmath426 under gaussian input @xmath427 .",
    ", thus , igmm estimates @xmath428 can be compared directly to @xmath429 . ] in particular , conventional gaussian mle ( estimation of @xmath430 and @xmath431 only ; @xmath432 ) , igmm and lambert @xmath113gaussian mle , and  for a skew competitor  the skew - normal mle are studied . whereas a comparison of accuracy and efficiency in @xmath433 does not make sense ,",
    "it is meaningful to analyze @xmath434 and @xmath435 of skew - normal versus lambert @xmath113gaussian mle .",
    "each estimator is applied to 3 kinds of simulated data sets for @xmath436 different sample sizes of @xmath437 and @xmath438 :    data is sampled from a symmetric rv @xmath439 . does additional estimation of @xmath3 affect the properties of @xmath434 or @xmath435 ?    a typical value for financial data , such as the latam returns introduced in section [ sec : introduction ] .",
    "this large value reveals the importance of the two branches of the lambert @xmath1 function .",
    "how does the skew - normal mle handle extremely skewed data [ @xmath440 ?",
    "simulations are based on @xmath441 replications .",
    "the input mean @xmath93 and standard deviation @xmath71 are chosen such that the observed rv has @xmath442 and @xmath443 for all @xmath3 .",
    "these functional relations can be obtained by ( [ eq : igmm_sigma2 ] ) and ( [ eq : igmm_mu ] ) . for igmm",
    "the tolerance level was set to @xmath444 and the euclidean norm was used .    the gaussian and skew - normal mle estimate the mean and standard deviation of @xmath2 .",
    "both lambert @xmath1 methods estimate the mean and standard deviation of the latent variable @xmath0 plus the skewness parameter  @xmath3 .",
    "thus , for a meaningful comparison the implied estimates @xmath445 and @xmath446 given by ( [ eq : mu_y_gaussian ] ) and ( [ eq : sigma2_y_gaussian ] ) are reported below .",
    "this parameter choice investigates if imposing the lambert @xmath1 framework , even though its use is superfluous , causes a  quality loss in the estimation .",
    "furthermore , critical values can be obtained for the finite sample behavior of @xmath433 under the null hypothesis of a symmetric distribution .",
    "@ld5.0d2.4d2.4d2.4d1.4d1.4d1.4@ & & & + & & & + & & & & & & & + gaussian ml&50 & 0.0000&0.0054&-0.0175&0.0000&0.9943&0.7053 + & 100 & 0.0000&0.0016&-0.0084&0.0000&0.9812&0.7410 + & 250 & 0.0000&-0.0029&-0.0009&0.0000&0.9997&0.6917 + & 1000 & 0.0000&0.0005&-0.0013&0.0000&0.9788&0.7105 + [ 3pt ] igmm & 50 & -0.0015&0.0054&-0.0060&0.4567&0.9945&0.7059 + & 100&-0.0012&0.0015&-0.0030&0.4368&0.9813&0.7405 + & 250 & 0.0001&-0.0017&-0.0009&0.4210&0.9997&0.6919 + & 1000&0.0003&0.0005&-0.0008&0.4014&0.9788&0.7102 + [ 3pt ] lambert @xmath1 ml&50 & -0.0013&0.0054&-0.0126&0.5144&0.9951&0.7210 + & 100&-0.0013&0.0016&-0.0072&0.4670&0.9813&0.7407 + & 250 & 0.0002&-0.0017&-0.0027&0.4333&0.9997&0.6922 + & 1000&0.0003&0.0005&-0.0012&0.4039&0.9788&0.7106 + [ 3pt ] skew - normal ml&50 & & 0.0052&-0.0135&&0.9928&0.7149 + & 100 & & 0.0015&-0.0073&&0.9821 & 0.7409 + & 250 & & -0.0018&-0.0027&&1.0004 & 0.6925 + & 1000 & & 0.0000&-0.0013&&0.9788 & 0.7105 +    table [ tab : n01_sim ] displays the bias and root mean square error ( rmse ) of @xmath447 .",
    "not only are all estimators unbiased , but they also have essentially equal rmse for @xmath434 and @xmath435 .",
    "it is well known that the gaussian mle of @xmath71 is only asymptotically unbiased , but for small samples it underestimates the standard deviation , whereas a method of moments estimator such as igmm does not have that problem ( see @xmath448 ) . for @xmath433",
    "the igmm estimator has slightly smaller rmse than mle for small @xmath288 ; for large @xmath288 the difference disappears .",
    "this can also be explained by an only asymptotically unbiased mle for @xmath71 , and the functional relation ( [ eq : sigma_y_gamma_sigma_x ] ) of @xmath3 , @xmath71 and @xmath431 .",
    "overall , estimating @xmath3 has _ no _ effect on the quality of the remaining parameter estimates , if the data comes from a truly ( symmetric ) gaussian distribution .",
    "a  shapiro wilk gaussianity test on the @xmath449 estimates of @xmath450 and @xmath451 gives @xmath452-values of @xmath453 and @xmath454 , respectively ( @xmath455 ) , and thus confirms the asymptotic normality of @xmath433 as stated in section [ sec : lambertw_mle ] .",
    "this choice of @xmath3 is motivated by real world data  in particular , asset returns typically exhibit slightly negative skewness [ @xmath456 .",
    "@ld5.0d2.4d2.4d2.4d1.4d1.4d1.4@ & & & + & & & + & & & & & & & + gaussian ml&50 & 0.0500&0.0057&-0.0176&0.3536&0.9952 & 0.7350 + & 100 & 0.0500&0.0016&-0.0083&0.5000&0.9826&0.7741 + & 250 & 0.0500&-0.0029&-0.0009&0.7906&0.9981&0.7095 + & 1000 & 0.0500&0.0005&-0.0014&1.5811&0.9781&0.7281 + [ 3pt ] igmm & 50 & -0.0008&0.0046&-0.0057&0.4582&0.9954&0.7410 + & 100 & -0.0008&0.0011&-0.0026&0.4389&0.9828&0.7753 + & 250 & 0.0002&-0.0019&-0.0007&0.4189&0.9982&0.7102 + & 1000 & 0.0003&0.0005&-0.0009&0.3986&0.9780&0.7276 + [ 3pt ] lambert @xmath1 ml & 50 & -0.0043&0.0052&-0.0116&0.5113&0.9961&0.7570 + & 100 & -0.0029&0.0015&-0.0062&0.4701&0.9829&0.7802 + & 250 & -0.0006&-0.0017&-0.0024&0.4282&0.9981&0.7114 + & 1000 & 0.0001&0.0005&-0.0013&0.3992&0.9781&0.7284 + [ 3pt ] skew - normal ml&50 & & 0.0073&-0.0136&&1.0011&0.7490 + & 100 & & 0.0026&-0.0067&&0.9834 & 0.7811 + & 250 & & -0.0014&-0.0025&&0.9990 & 0.7109 + & 1000 & & 0.0000&-0.0012&&0.9796 & 0.7281 +    table [ tab : typical_lambertw ] presents the effect of ignoring small asymmetry in data .",
    "gaussian mle is by definition biased for @xmath3 , but @xmath434 and @xmath435 are still good estimates . neither igmm nor lambert @xmath1 mle gives biased @xmath447 , but the rmse of  @xmath457 increases for all estimators and all sample sizes .",
    "again igmm presents smaller rmse for @xmath433 than mle for small @xmath288 , but not for large @xmath288for the same reason as in the @xmath51 case .",
    "notably , the skew - normal mle for @xmath430 and  @xmath431 is also unbiased and has the same rmse as the lambert @xmath1 and gaussian competitors , even though the true distribution is a lambert @xmath113gaussian , not a skew - normal .      in this case",
    ", the lambert @xmath1 mle should work better than the skew - normal mle , since the skewness coefficient @xmath458 lies outside the theoretically possible values of skew - normal distributions .",
    "furthermore , the nonprincipal branch of the lambert @xmath1 function becomes more important as @xmath459 , so the lambert @xmath1 mle should also outperform igmm , which ignores the nonprincipal solution .",
    "only the skew - normal mle fails to provide accurate estimates of location and scale for heavily skewed data sets ; all other estimators are practically unbiased ( table [ tab : extreme_lambertw ] ) .",
    "the rmse for @xmath435 almost doubled compared to the symmetric case , and for gaussian as well as skew - normal mle it is increasing with sample size instead of decreasing .",
    "while @xmath410 has less bias , @xmath460 has a much smaller rmse : not ignoring the nonprincipal branch more than compensates the finite sample bias in @xmath461 .",
    "surprisingly , the rmse for @xmath433 has diminished by about @xmath462 over all sample sizes compared to the symmetric case .",
    "@ld5.0d2.4d2.4d2.4d1.4d1.4d1.4@ & & & + & & & + & & & & & & & + gaussian ml&50 & -0.3000&0.0029&-0.0336&2.1213&0.9851&1.2941 + & 100 & -0.3000&0.0006&-0.0194&3.0000&0.9863&1.3957 + & 250 & -0.3000 & -0.0076&-0.0056&4.7434&1.0045&1.4499 + & 1000 & -0.3000 & 0.0002&-0.0013&9.4868&0.9883&1.4954 + [ 3pt ] igmm & 50 & -0.0076&0.0081&-0.0057&0.4374&0.9917&1.2417 + & 100 & -0.0055&0.0028&-0.0063&0.4005&0.9853&1.2440 + & 250 & -0.0032&-0.0012&-0.0056&0.3647&1.0009&1.2204 + & 1000&-0.0026&-0.0003&-0.0049&0.3197&0.9820&1.1992 + [ 3pt ] lambert @xmath1 ml&50 & 0.0180&0.0221 & 0.0266&0.3844&1.0152&1.2385 + & 100 & 0.0115&0.0131 & 0.0168&0.3241&1.0095&1.2218 + & 250 & 0.0055 & 0.0053 & 0.0054&0.2747&1.0102&1.1535 + & 1000 & 0.0000 & 0.0021&-0.0021&0.2349&0.9818&1.1383 + [ 3pt ] skew - normal ml&50 & & 0.0695&-0.0938&&1.3638&1.1965 + & 100 & & 0.0558&-0.0834&&1.3508 & 1.3182 + & 250 & & 0.0520&-0.0748&&1.4865 & 1.5577 + & 1000 & & 0.0560&-0.0704&&2.1588 & 2.4585 +      estimation of @xmath430 is unaffected by the value of @xmath3 ; the quality of @xmath435 , however , depends on @xmath3 : the larger @xmath3 , the greater the rmse of  @xmath435 . for @xmath51",
    "the lambert @xmath1 methods perform equally well as gaussian mle , whereas for nonzero @xmath3 gaussian and  to some extent  skew - normal mle have inferior qualities compared to the lambert @xmath1 alternatives . in particular",
    ", the rmse for @xmath435 increases with sample size .",
    "hence , there is no gain restricting analysis to the ( symmetric ) gaussian case , as the lambert @xmath1 framework extends this distribution to a broader class , without losing the good properties of gaussian mle . for little asymmetry in the data ,",
    "both the lambert @xmath1 and the skew - normal approach give accurate and precise estimates of location , scale and skewness .",
    "yet for heavily skewed data ( skewness greater than @xmath463 in absolute value ) , the skew - normal framework fails not only in theory , but also in practice to provide a good approximation .",
    "@lcd2.2d2.2d2.2d2.2d2.2@ & + & + @xmath464 & & & & & & + @xmath465 & 8.39 & 10.24 & 34.65 & 15.82 & 16.76 & 26.91 + @xmath466 & 6.05 & 8.16 & 35.01 & 17.52 & 19.12 & 21.45 + @xmath467 & 4.37 & 6.45 & 27.51 & 17.83 & 21.34 & 13.23 + @xmath438 & 3.58 & 4.96 & 18.43 & 17.20 & 24.58 & 6.49 + [ 3pt ] @xmath465 & 4.43 & 4.60 & 6.24 & 4.56 & 4.64 & 6.23 + @xmath466 & 4.10 & 4.44 & 6.44 & 4.46 & 4.44 & 5.78 + @xmath467 & 3.90 & 4.26 & 5.92 & 4.15 & 4.19 & 5.45 + @xmath438 & 3.58 & 4.11 & 5.91 & 3.78 & 4.04 & 5.36 +    table [ tab : count_iterations ] shows the average number of iterations the igmm algorithm needed to converge : for increasing sample size it needs less iterations ",
    "sample moments can be estimated more accurately ; more iterations are needed for larger @xmath3as the starting value for @xmath3 is based on the taylor expansion around @xmath468 and moving away from the origin makes the initial estimate @xmath469 less precise .",
    "a closer look at the two sub - tables ( top and bottom ) shows that finding the optimal @xmath3 ( algorithm [ alg : gamma_gmm ] ) becomes much more difficult for increasing @xmath3 and sample size @xmath288 than finding the optimal @xmath93 and @xmath71 given the optimal  @xmath344 ( algorithm [ alg : igmm ] ) . for @xmath271 and large @xmath288",
    "there is almost no difference between the total number of iterations ( top ) and the number of iterations in algorithm [ alg : igmm ] only ( bottom ) . for large @xmath3 , however , the total number of iterations is approximately @xmath470 times as large .",
    "the right panel shows the values for simulations of a lambert @xmath26 rv with @xmath156 degrees of freedom . for small  @xmath3 ,",
    "finding @xmath344 takes much longer than for gaussian input ; surprisingly , for large @xmath3 convergence is reached faster .",
    "this is probably a result of the constrained optimization : due to more extreme values for a @xmath5-distribution , algorithm [ alg : gamma_gmm ] often returns one of the two boundary values for @xmath471 without even starting the optimization process .",
    "given its good empirical properties , fairly general assumptions about the input variable @xmath0 , and its fast computation time , the igmm algorithm can be used as a quick lambert @xmath1 check . for a particular lambert @xmath28 distribution , the lambert @xmath419 mle gives more accurate results , especially for heavily skewed data .",
    "this section demonstrates the usefulness of the presented methodology on real world data . in the first example",
    "i analyze parts of the australian athletes data set which can be typically found in the literature on modeling skewed data [ @xcite , @xcite ] .",
    "( dots ) and back - transformed data  @xmath25 ( triangles ) ; ( right ) histogram plus density estimates . ]    220pt@ld2.3d2.3d2.3@ * bmi * & & & + min & 16.750 & 15.356 & 15.406 + max & 31.930 & 29.335 & 29.384 + mean & 21.989 & 21.735 & 21.742 + median & 21.815 & 21.815 & 21.815 + st .",
    "2.640 & 2.570 & 2.569 + skewness & 0.683 & 0.000 & 0.017 + kurtosis & 1.093 & 0.186 & 0.187 + [ 6pt ] sw & 0.035 & 0.958 & 0.959 + jb & 0.001 & 0.877 & 0.874 +    the second example reexamines the latam returns introduced in section [ sec : introduction ] .",
    "a lambert @xmath472-distribution is found to give an appropriate fit , both for the raw data as well as the standardized residuals of an auto - regressive conditional heteroskedastic time series model ( see section [ sec : non_iid_garch ] for details ) . in particular , a comparison of risk estimators ( value at risk ) demonstrates the suitability of the lambert @xmath28 distributions to model financial data.=1      figure [ fig : bmi ] shows the body mass index ( bmi ) of @xmath466 female australian athletes ( dots ) and table [ tab : summary_bmi ] lists several statistical properties ( column 1 ) .",
    "although the data appear fairly gaussian , its large positive skewness makes both tests reject normality on a @xmath16 level.=1    after 5 iterations @xmath473 , which implies @xmath474 , @xmath475 , and @xmath476 , assuming gaussian input .",
    "@ld2.3cd2.3c@ & & & & + @xmath93 & 21.742 & 0.274 & 79.494 & 0.000 + @xmath71 & 2.556 & 0.188 & 13.618 & 0.000 + [ 3pt ] @xmath3 & 0.096 & 0.039 & 2.481 & 0.013 +    the bmi data set consists of exactly @xmath477 i.i.d . samples and table [ tab : n01_sim ] lists finite sample properties of @xmath478 for this case .",
    "is clearly not @xmath479 , the location - scale invariance of lambert @xmath113gaussian rvs makes this difference to scenario 1 in the simulations [ @xmath480 irrelevant ; finite sample properties of @xmath3 do not change between @xmath481 and general @xmath253 , since in both cases @xmath93 and @xmath71 are also estimated . ]",
    "thus , if @xmath482 was gaussian , then @xmath483    plugging @xmath484 into ( [ eq : bmi_gamma_finite_sample_distribution ] ) gives @xmath485 and a corresponding @xmath452-value of @xmath486 .",
    "thus , @xmath487 is significant on a @xmath16 level , yielding an indeed positively skewed distribution for the bmi data @xmath22 .",
    "as both tests can not reject gaussianity for @xmath488 , a lambert @xmath252gaussian approach seems reasonable .",
    "table [ tab : mle_bmi ] shows that all estimates are highly significant , where standard errors are obtained by numerical evaluation of the hessian at the optimum . as not one single test",
    "can reject normality of @xmath489 ( triangles in figure  [ fig : bmi ] ) , an adequate model to capture the statistical properties of the bmi data is @xmath490    for @xmath429 the support of @xmath491 lies in the half - open interval @xmath492 . as all observations lie within these boundaries , @xmath493 is indeed a ( local ) maximum .",
    "figure [ fig : bmi ] shows the closeness of the lambert @xmath113gaussian density to the histogram and kernel density estimate , whereas the best gaussian is apparently an improper approximation .",
    "although a more detailed study of athlete type and other health indicators might explain the prevalent skewness , the lambert @xmath1 results at least support common sense : the human body has a natural physiological lower bound corresponds to a @xmath494  cm tall athlete only weighing @xmath495 kg . ] for the bmi , whereas outliers on the right tail  albeit , in principle , also having an upper bound ",
    "are more likely .      a lot of financial data , also the latam return series introduced in section [ sec : introduction ] ( table [ tab : latam_summary ] and figure [ fig : latam_news4 ] ) , display negative skewness and excess kurtosis .",
    "these so - called _ stylized facts _ are well known and typically addressed via ( generalized ) auto - regressive conditional heteroskedastic ( garch ) [ @xcite , @xcite ] or stochastic volatility ( sv ) models [ @xcite , @xcite ] . a theoretical analysis of lambert @xmath28 time series models , however , is far beyond the scope and focus of this work .",
    "for empirical evidence regarding the usefulness and significance of lambert @xmath28 distributions in garch models and possible future research directions see section [ sec : non_iid_garch ] .",
    "it is also worth noting that the lambert @xmath496 transformation ( [ eq : noncentral_nonscale_lambertw_y ] ) resembles sv models very closely , and connections between the two can be made in future work .",
    "based on the news @xmath29 return interpretation in a stock market @xmath10 , it makes sense to assume a symmetric input distribution @xmath11 for the latent news rv @xmath0 . without specifying",
    "the symmetric @xmath11 any further , the igmm algorithm gives a robust estimate for @xmath97 : here @xmath497 .",
    "column 2 of table [ tab : latam_summary ] shows that the unskewed data @xmath498here interpreted as news hitting the market  is non - gaussian , but a @xmath5-distribution can not be rejected . in consequence",
    ", @xmath2 is modeled as a lambert @xmath113location - scale @xmath5-distribution with @xmath499 , where @xmath104 is the location , @xmath105 the scale and @xmath500 the degrees of freedom parameter .",
    "table [ tab : latam_lambertw_t ] shows that all coefficients of @xmath429 are highly significant ; in particular , @xmath433 increased substantially ( in absolute value ) , as @xmath3 now solely addresses asymmetry in the data , and @xmath17 can capture excess kurtosis .",
    "thus , the prevalent negative skewness in the latam daily returns is not an artifact of large outliers in the left tail of an otherwise symmetric distribution , but a significant characteristic of the data .",
    "@ld2.3cd2.3c@ & & & & @xmath501 + @xmath104 & 0.197 & 0.037 & 5.270 & 0.000 + @xmath105 & 1.240 & 0.057 & 21.854 & 0.000 + @xmath17 & 7.047 & 2.196 & 3.208 & 0.001 + [ 3pt ] @xmath3 & -0.053 & 0.014 & -3.860 & 0.000 +    in order to check if the lambert @xmath26-distribution is indeed an appropriate model for @xmath22 , it is useful to study the back - transformed data @xmath177 ; here @xmath502 .",
    "not surprisingly , the skewness of  @xmath25 reduced to almost @xmath85 ( column 3 of table [ tab : latam_summary ] ) .",
    "as a kolmogorov ",
    "smirnov test can not reject a student @xmath5-distribution for @xmath25 , the lambert  @xmath503-distribution @xmath504 is an adequate unconditional probabilistic model for the latam returns @xmath22 .",
    "@xmath29 return @xmath22 scatter plot plus histograms ; solid @xmath505 line : @xmath51",
    ". dashed vertical and horizontal lines represent the sample mean of @xmath25 and @xmath22 , respectively . ]    the effect of news @xmath506 in the market @xmath10 is clearly shown in a scatter plot of @xmath25 versus @xmath22 .",
    "for example , consider the lower - left point @xmath507 in figure [ fig : latam_news_return ] . here",
    ", the observed negative return equals @xmath508 , but as @xmath509 , this outcome was an overreaction to bad news that was only `` worth '' @xmath510 . for location - scale lambert @xmath1",
    "rvs the skewness parameter @xmath3 is a powerful , yet easy way to characterize different markets / assets .",
    "the negative @xmath433 shows that this specific market ( system ) is exaggerating bad news , and devalues positive news .",
    "the var is a popular measure in financial statistics to estimate the potential loss for an investment in an asset over a fixed time period .",
    "that is , the maximum percentage an investor can expect to lose  with a confidence of @xmath511over a fixed time period .",
    "statistically this corresponds to the @xmath76-quantile of the distribution .",
    "the var can be obtained in various ways : the simplest are empirical and theoretical quantiles given the estimated parameter vector of a parametric distribution ( which are sufficient for comparative purposes ) .",
    "@ld2.3d2.3d2.3d1.3d1.3d1.3d1.3@ & + & + * method * & & & & & & & + empirical & -4.562 & -4.078 & -2.478 & 0.138 & 2.344 & 3.192 & 3.818 + [ 3pt ] gaussian & -3.660 & -3.294 & -2.293 & 0.121 & 2.535 & 3.535 & 3.901 + @xmath5 & -4.297 & -3.634 & -2.214 & 0.121 & 2.455 & 3.875 & 4.538 + lambert @xmath26 & -4.871 & -4.049 & -2.358 & 0.197 & 2.351 & 3.437 & 3.893 + skew-@xmath5 & -4.715 & -3.973 & -2.364 & 0.201 & 2.346 & 3.465 & 3.957 +    as expected , a gaussian distribution underestimates both the low and high quantiles , as it lacks the capability to capture excess kurtosis ( see table [ tab : latam_var ] ) .",
    "the @xmath5-distribution with @xmath512 degrees of freedom has heavier tails , but underestimates low and overestimates high quantiles : clearly an indication of the prevalent skewness in the data .",
    "the lambert @xmath26 and the skew @xmath5-distribution for the location , scale , shape and degrees of freedom parameter respectively ; function ` st.mle ` in the ` sn ` package . ] are the best approximation to the empirical quantiles : both heavy tails and negative skewness are captured ( see also the lambert @xmath26 qq plot in figure [ fig : latam_news4 ] ) .",
    "there is no clear `` winner '' between the two skewed distributions : skew-@xmath5 quantiles are closer to the empirical ones for small @xmath76 , lambert @xmath26 quantiles are closer for large @xmath76 . around the median ( @xmath513 ) both skewed distributions are far away from the true value : the reason being a high concentration of close to @xmath85 returns in financial assets , so - called `` inliers '' [ see @xcite ] .",
    "it is well known that financial return series @xmath514 typically exhibit positive auto - correlation in their squares  @xmath515 , which violates the independence assumption of the mle presented in section  [ sec : lambertw_mle ] . a standard parametric way to capture this dependence",
    "is a garch model [ @xcite , @xcite ] , which models the variance at time  @xmath5 , @xmath516 , as a function of its own past . a simple , yet very successful model for an uncorrelated @xmath514 is a @xmath517 , @xmath518 where @xmath519 is a zero - mean , unit - variance i.i.d .",
    "sequence [ for technical details see @xcite , @xcite ] .",
    "typically , @xmath520 , but also student  @xmath5- or skew @xmath5-distributions are used for more flexibility in the conditional distribution of @xmath519 given the information set @xmath521 available at time @xmath522 [ @xcite ] .",
    "@xcite also found that the standardized residuals @xmath523which can be considered as an i.i.d .",
    "sequence  still exhibit negative skewness after fitting a gaussian garch model to @xmath524 @xmath525 returns .    after fitting a student-@xmath5 @xmath517 model to the latam return series @xmath22 , the lambert @xmath26 mle fit for the standardized residuals  which are approximately i.i.d . and thus do not violate the mle assumptions  still gives a highly significant @xmath526 with a @xmath452-value of @xmath527 ( other estimates are not shown here ) .",
    "while i will not study lambert @xmath113student-@xmath5 garch models in detail , this example and the great flexibility of lambert @xmath28 distribution combined with the possibility to symmetrize skewed data suggest that lambert @xmath28 garch ( and sv ) models are a promising area of future research .    this analysis confirms previous findings that negative skewness is an important feature of asset returns .",
    "for example , optimal portfolio models based on skewed distributions lead to better suited decision rules to react to asymmetric price movements",
    ". it also shows that lambert @xmath1 distributions model the characteristics of financial returns as well as skew @xmath5-distributions , with the additional option to recover symmetric latent data , which is not possible for rvs based on a manipulation of the p.d.f . rather than a variable transformation .",
    "during the final review process , professor andrew f. siegel suggested possible connections of lambert @xmath1 distributions to tukey s @xmath528@xmath38 distribution [ @xcite ] @xmath529 where @xmath530 . here",
    "@xmath528 is the skew parameter and @xmath38 controls the tail behavior of @xmath47 .",
    "although the underlying idea to introduce skewness is the same , the specific transformations to get the skewness effects are different , and so are the properties of the transformed rvs . for @xmath531 , @xmath532 becomes symmetric .",
    "the rv @xmath47 has tukey s @xmath38 distribution and is commonly used to model heavy - tails [ @xcite , @xcite ] .",
    "equation ( [ eq : tukey_h ] ) reveals a close link of lambert @xmath419 rvs to the existing statistics literature by noting that if @xmath533 , then @xmath534 has a noncentral , nonscaled lambert @xmath535 distribution with @xmath536 .    for further important connections between the lambert @xmath1 function and tukey s @xmath38 distribution see @xcite .",
    "whereas the lambert @xmath1 function plays an important role in mathematics , physics , chemistry , biology and other fields , it has not yet been used in statistics . here",
    "i introduce it in an input / output setting to skew and `` unskew '' rvs and data , respectively .",
    "successful application to biomedical and financial data together with the great flexibility with respect to the type of input rv @xmath0 of lambert @xmath28 rvs promise a wide range of applications as well as theoretical studies for particularly chosen input distributions .",
    "last but not least , a very pragmatic advantage of the transformation - based lambert @xmath28 rvs compared to other approaches to asymmetry : data can be `` unskewed '' using lambert s @xmath1 function .",
    "i am grateful to professor wilfredo palma for giving me the opportunity to work at the department of statistics , pontificia universidad catlica de chile , santiago , where i completed important parts of this study .",
    "furthermore , i want to thank professor reinaldo arellano - valle , professor cosma shalizi , the editor professor stephen fienberg and two anonymous referees for helpful comments and suggestions on the manuscript ."
  ],
  "abstract_text": [
    "<S> originating from a system theory and an input / output point of view , i introduce a new class of generalized distributions . </S>",
    "<S> a parametric nonlinear transformation converts a random variable @xmath0 into a so - called lambert @xmath1 random variable @xmath2 , which allows a very flexible approach to model skewed data . its shape depends on the shape of @xmath0 and a skewness parameter @xmath3 . in particular , for symmetric @xmath0 and nonzero @xmath3 the output @xmath2 is skewed . </S>",
    "<S> its distribution and density function are particular variants of their input counterparts . </S>",
    "<S> maximum likelihood and method of moments estimators are presented , and simulations show that in the symmetric case additional estimation of @xmath3 does not affect the quality of other parameter estimates </S>",
    "<S> . applications in finance and biomedicine show the relevance of this class of distributions , which is particularly useful for slightly skewed data . a practical by - result of the lambert @xmath1 framework </S>",
    "<S> : data can be `` unskewed . ''    </S>",
    "<S> the @xmath4 package http://cran.r-project.org/web/packages/lambertw[`lambertw ` ] developed by the author is publicly available ( http://cran.r-project.org[cran ] ) .    . </S>"
  ]
}