{
  "article_text": [
    "in many applications , especially in medical , social or economic studies , potential covariates collected for a regression analysis are categorical , measured either on an ordinal or on a nominal scale .",
    "the usual strategy for modelling the effect of categorical covariates is to define one level as baseline and to use dummy variables for the effects of the other levels with respect to this baseline .",
    "hence , the effect of a categorical covariate is captured not by a single but by a group of regression effects . including categorical variables as covariates in regression type models",
    "can therefore easily lead to a high - dimensional vector of regression effects .",
    "moreover , since only the subset of observations with a specific level contribute information to estimation of its effect , estimated effects of rare levels will be associated with high uncertainty .",
    "many methods have been proposed to achieve sparser models by identifying regressors with non - zero effects .",
    "whereas frequentist methods , e.g.  the lasso @xcite or the elastic net @xcite rely on penalties , bayesian variable selection methods are based on specification of appropriate prior distributions , e.g.  shrinkage priors @xcite or spike and slab priors @xcite",
    ". however , variable selection methods perform selection of single regression effects and are not appropriate for a categorical covariate with more categories , as the natural grouping of the dummy variables capturing its effect is not taken into account .",
    "moreover , a sparser representation of the effect of a categorical covariate can be achieved not only by restricting all of its level effects to zero but also when some of the levels have the same effect . in this paper",
    ", we propose a bayesian approach to achieve a sparser representation of the effects of a categorical predictor , which encourages both shrinkage of non - relevant effects to zero as as well as fusion of ( almost ) identical level effects .",
    "approaches that explicitly address inclusion or exclusion of groups of regression coefficients associated to one variable are the group lasso @xcite and the bayesian group lasso @xcite .",
    "@xcite uses spike and slab priors for grouped selection of the set of all dummy variables related to a categorical predictor .",
    "whereas all these methods aim at sparsity for groups of regression coefficients , the recently proposed sparse - group lasso @xcite addresses also sparsity within groups by shrinking negligible effects to zero , however not by fusing identical level effects .    to encourage both sparsity of regression effects as well as their differences in regression models with metric predictors , @xcite proposed the fused lasso and @xcite its bayesian counterpart , the bayesian fused lasso .",
    "both methods assume some ordering of effects and shrink only effect differences of subsequent effects to zero .",
    "hence , they are not appropriate for nominal predictors where any effect difference should be subject to shrinkage .",
    "effect fusion for nominal predictors is considered only in @xcite who propose a modification of the fused lasso for anova , by gertheiss and tutz @xcite who specify different lasso - type penalties for ordinal and nominal covariates and recently in @xcite where tree - structured clustering of effects of categorical covariates is performed .",
    "we address the problem of sparsity for effects of categorical predictors from a bayesian point of view and incorporate structure in the prior on the regression effects . as",
    "the goal is to learn whether two level effects are almost equal or considerably different , we do not specify a standard independence normal prior for the level effects but explicitly model dependence in their joint precision matrix by allowing for either almost perfect or low dependence .",
    "we show that the prior can alternatively be achieved by specifying spike and slab prior distributions on all level effects and their differences and taking into account their linear dependence .",
    "spike and slab prior distributions have been applied extensively in bayesian approaches to variable selection .",
    "the mixture structure with a spike at zero and a flat slab allows for intrinsic classification of effects and effect differences as ( almost ) zero , when a coefficient is assigned to the spike and as non - zero otherwise .",
    "whereas for a categorical predictor any two level effects will be subject to fusion , it seems natural to exploit the ordering information available for an ordinal predictor by restricting fusion to adjacent categories , which is easily accomplished in our framework . generally , our proposed method is not limited to categorical predictors but can be applied to all groups of covariates    the rest of the paper is organised as follows : in section [ sec : model ] we introduce the data model and in section [ sec : prior ] the prior distribution constructed to encourage a sparse representation of covariate effects .",
    "posterior inference is discussed in section [ sec : inference ] and section [ sec : simulation ] investigates the performance of the method for simulated data .",
    "an applications of the proposed method for bayesian effect fusion is illustrated on a real data example in section [ sec : application ] and we conclude with section [ sec : conclusion ] .",
    "we consider a standard linear regression model with normal response @xmath0 and @xmath1 categorical covariates @xmath2 , @xmath3 , where covariate @xmath2 has @xmath4 ordered or unordered levels @xmath5 . to represent its effect on the response @xmath0",
    ", we define @xmath6 as the baseline category and introduce dummy variables , @xmath7 , to capture the effect of level @xmath8 with respect to the baseline category .",
    "the regression model is then given as @xmath9 where @xmath10 is the intercept , @xmath11 is the effect of level @xmath12 of covariate @xmath2 ( with respect to the reference category ) and @xmath13 is the error term .    for an @xmath14 response vector @xmath15",
    "we write the model as @xmath16 where @xmath17 is the @xmath18 design matrix for covariate @xmath2 , @xmath19 is the @xmath20 vector of the corresponding regression effects and @xmath21 the @xmath14 vector of error terms .",
    "@xmath22 denotes a vector with elements @xmath23 and @xmath24 the identity matrix .",
    "bayesian model specification is completed by assigning prior distributions to all model parameters .",
    "we assume a prior of the structure @xmath25 where @xmath26 denotes additional hyperparameters , which are specified below . we assign a flat proper prior @xmath27 to the intercept and an inverse gamma distribution @xmath28 to the error variance .",
    "the prior on the regression effects @xmath19 is specified hierarchically as @xmath29 where @xmath30 is a fixed constant , @xmath31 is a scale parameter and the matrix @xmath32 determines the structure of the prior precision matrix . to encourage effect fusion , we let @xmath32 depend on a vector @xmath33 of indicator variables @xmath34 , which are defined for each pair of level effects @xmath12 and @xmath35 subject to fusion .",
    "@xmath36 indicates that @xmath11 and @xmath37 differ considerable and hence two regression parameters are needed to capture their respective effects whereas for @xmath38 the effects are almost identical and the two level effects could be fused . to allow fusion of level effects to @xmath39 , i.e.  conventional variable selection , we include in @xmath33",
    "also indicators @xmath40 , @xmath41 .    the dimension of @xmath33 and the concrete specification of @xmath32 depend on which pairs of effects are subject to fusion .",
    "we discuss the case where fusion is completely unrestricted and hence any pair of effects might be fused in section [ sec : pri_nom ] .",
    "whereas unrestricted effect fusion will be appropriate for a nominal covariate , for an ordinal covariate information on the ordering of levels is available which suggests to fuse only adjacent categories as discussed in @xcite .",
    "we describe effect fusion taking into account restrictions that preclude direct fusion for specified pairs of effects in section [ sec : pri_restr ] . for notational convenience",
    "we define @xmath42 and drop the covariate index @xmath43 in the following .      for unrestricted effect fusion",
    ", we introduce an indicator @xmath44 for each pair of effects @xmath45 and @xmath46 ( including @xmath39 for the baseline ) and hence @xmath47 is of dimension @xmath48 .",
    "we define @xmath49 where @xmath50 is a fixed large number ( e.g. @xmath51 ) for @xmath52 and @xmath53 for @xmath54 .",
    "the structure of the prior precision matrix is then specified as @xmath55 and finally , we set @xmath56 .",
    "the structure matrix @xmath57 determines the prior precision matrix of @xmath58 up to the scale factor @xmath59 and therefore has to be symmetric and positive definite .",
    "symmetry of @xmath57 is guaranteed by definition and positive definiteness as @xmath60 if @xmath61 , see appendix [ app : pri_pd ] for a detailed proof .    to interpret the structure matrix @xmath57",
    ", we subsume in @xmath62 all indicators related to level @xmath12 .",
    "the diagonal elements @xmath63 of @xmath57 determine the partial prior precisions and the off - diagonal elements @xmath64 the partial prior correlations of the level effects : @xmath65 thus , the prior allows for either high ( if @xmath66 ) or low ( if @xmath67 ) positive prior partial correlation .",
    "further , depending on @xmath68 , @xmath69 different values of the prior precision are possible , ranging from @xmath70 ( for @xmath71 ) to @xmath72 ( for @xmath73 ) . as an example , consider a covariate with @xmath74 levels , where @xmath75 for all @xmath76 except one and @xmath51 . if @xmath77 the structure matrix is @xmath78 and the marginal prior on @xmath79 is concentrated close to zero . for @xmath80 ,    @xmath81 and hence the joint prior on @xmath82",
    "is concentrated close to @xmath83 .",
    "note that marginally the prior on off - diagonal elements of @xmath84 is a mixture of two inverse gamma distributions .",
    "the structure of the quadratic form in equation ( [ eq : quadr_form ] ) suggests a straightforward interpretation of the effect fusion prior in terms of normal priors on all effect differences @xmath85 , @xmath86 ; @xmath87 . for @xmath66 ,",
    "the effect difference @xmath88 is concentrated around zero , whereas it is more dispersed for @xmath67 .",
    "actually , as we show in appendix [ app : pri_diff ] the effect fusion prior specified above can be derived by starting from independent spike and slab priors on all effect differences @xmath88 and then correcting for the linear restrictions @xmath89 .",
    "finally , we note that from a frequentist perspective , the effect fusion prior can be interpreted as an adaptive quadratic penalty ( see equation ( [ eq : quadr_form ] ) ) , with either heavy or slight penalization of effect differences .",
    "in contrast , @xcite use a weighted @xmath90 penalty on the effect differences .      if information on the structure of the level effects is available , this information can be exploited by allowing only fusion of specific pairs of level effects .",
    "consider e.g.  an ordinal covariate where the ordering of levels suggests to allow only fusion of subsequent level effects @xmath91 and @xmath92 i.e.  restricting direct fusion of effects to adjacent categories .",
    "a restriction that e.g.  @xmath93 and @xmath94 should not be fused can be implemented in our prior in two ways : we can either fix the indicator @xmath66 or directly set the corresponding element in the prior precision matrix @xmath57 , @xmath95 . setting",
    "@xmath66 implies that effects @xmath93 and @xmath94 are still smoothed to each other and hence a soft restriction , whereas @xmath95 is a hard restriction which implies conditional independence of @xmath93 and @xmath94 .    whereas implementation of soft restrictions is straightforward , ( hard ) conditional independence restrictions require slight modifications for the structure matrix @xmath57 , the vector of indicators @xmath47 and the constant @xmath96 .",
    "we start by introducing a vector @xmath97 of indicators @xmath98 , which are defined for each effect difference @xmath88 .",
    "the elements of @xmath97 are fixed and indicate whether an effect difference is subject to fusion ( for @xmath99 ) or not ( for @xmath100 ) . deviating from unrestricted effect fusion considered in section [ sec : pri_nom ] , we define a stochastic indicator @xmath44 only for those effect differences where @xmath99 and hence the dimension of @xmath47 is @xmath101 .    to allow off - diagonal elements of the prior precision to be zero , we set @xmath102 and @xmath103 . thus @xmath64 takes the value zero if @xmath100 and @xmath104 otherwise .",
    "similarly , the diagonal elements are specified as @xmath105    as noted above , an important special case is an ordinal covariate where it is natural to restrict fusion to adjacent categories as in @xcite , i.e. @xmath106 hence , the vector of indicators @xmath47 has only @xmath107 elements and @xmath108 is a tri - diagonal matrix with elements @xmath109 in this case , the maximum value of a diagonal element @xmath110 is two and therefore we set @xmath111 .",
    "it is easy to show that this specification of @xmath108 corresponds to a random walk prior with initial value @xmath112 on the regression effects : @xmath113 due to the spike and slab structure , this prior allows for adaptive smoothing , with almost no smoothing for @xmath114 and pronounced smoothing for @xmath115 .",
    "another special case is the standard spike and slab prior used for variable selection , where only fusion of level effects to the baseline , i.e.  shrinkage of @xmath92 to zero is considered .",
    "the spike and slab prior is recovered in our framework when @xmath111 , @xmath116 and hence non - diagonal elements of @xmath108 are zero and @xmath117 .",
    "a standard choice in variable selection is to assume conditional prior independence of the elements of @xmath47 with @xmath118 , where @xmath119 is either fixed or assigned a hyperprior @xmath120 .",
    "this would in principle be possible also with our prior , however , from a computational point of view a more convenient choice is to set @xmath121 as with this choice the determinant of @xmath122 cancels out in the joint prior @xmath123 , which results as @xmath124      the hyperparameters of the effect fusion prior should be chosen to minimize the expected loss of the underlying decision problem : loss occurs if level effects which are different are fused or effects which are equal are not fused .",
    "we call the first case _ false negative _ , as a non - zero effect difference is not detected and the second _ false positive _ , as a zero effect difference is classified as non - zero . false positives and false negatives have different impacts : if an effect difference is falsely classified as positive , two parameters are included in the model though only one would be sufficient .",
    "this results in a loss of estimation efficiency .",
    "in contrast , if an effect difference is falsely classified as negative , two effects that are actually different from each other are modelled by only one parameter .",
    "this will result in biased estimation and bad prediction performance . hence",
    ", the primary goal will be to avoid false negatives .    from the representation of the prior in terms of spike and slab priors on all effect differences",
    "@xmath88 , it is evident that the conditional prior fusion probability @xmath125 depends on the slab to spike ratio @xmath50 and the parameters of the inverse gamma distribution for the slab variance @xmath126 and @xmath127 .",
    "we propose to set @xmath128 , a standard choice in variable selection ( see e.g.  @xcite ) , where the tails of spike and slab are not too thin to cause mixing problems in mcmc . for fixed @xmath129 ,",
    "the prior fusion probability @xmath130 is lower for a larger slab to spike ratio @xmath50 and for a smaller scale parameter @xmath127 .",
    "this suggests to choose a high value for @xmath50 and a small value for @xmath127 .",
    "however , shrinkage of effect differences to zero is more pronounced with a smaller scale parameter @xmath127 , also under the slab , which might hamper detection of small effect differences .",
    "we will investigate this issue in more detail in the simulation study in section [ sec : sim_hyp ] .",
    "we subsume the regression coefficients in @xmath131 and denote the collection of all parameters by @xmath132 .",
    "the goal is posterior inference for @xmath133 , which can be accomplished by sampling from the posterior distribution @xmath134 using mcmc methods .    as the model is a linear bayesian regression model with a conditionally conjugate prior",
    ", mcmc is straightforward . after choosing starting values for @xmath135 and @xmath136 mcmc proceeds by iterating between the following steps :    1 .",
    "update the prior variance matrix @xmath137 and sample the regression coefficients @xmath58 from the full conditional normal distribution @xmath138 with moments given as @xmath139 where @xmath140 is block - diagonal with first element @xmath141 ( for the intercept ) and the matrices @xmath142 2 .",
    "sample the error variance @xmath136 from the inverse gamma distribution @xmath143 with parameters @xmath144 3 .   for @xmath3 : sample the scale parameter @xmath145 from the inverse gamma distribution @xmath146 with parameters @xmath147 4 .   as both the likelihood given in equation ( [ eq : quadr_form ] ) as well as",
    "the prior @xmath148 can be factorized with respect to the indicators @xmath34 , these can be sampled independently from @xmath149 + where @xmath150      effect fusion aims at selecting an appropriate model for a categorical predictor and thus is a particular model selection problem . in a bayesian approach",
    ", model selection is usually based on posterior model probabilities and the goal is to find the model with maximum posterior model probability . slightly differing , in bayesian variable selection typically not the maximum probability model but the median probability model , i.e.  the model including all covariates that have an estimated posterior inclusion probability larger than 0.5 , is selected .    to select a model with potentially fused effects one could use the estimated fusion probabilities @xmath151 where @xmath152 is the mean of the corresponding mcmc draws , and fuse effects if @xmath153 . however , this strategy could yield a logically inconsistent model where e.g. levels @xmath35 and @xmath154 as well as @xmath12 and @xmath154 are fused but not levels @xmath35 and @xmath12 .",
    "hence , we fuse levels @xmath12 and @xmath35 only if @xmath153 and for all @xmath155 both @xmath156 and @xmath157 are either larger or smaller than @xmath158 .",
    "this strategy avoids logically inconsisent models and as levels are only fused when evidence is clear , takes into account the asymmetry in loss of false positives and false negatives .    after model selection",
    ", we estimate the dummy - coded regression coefficients of the selected model by a bayesian regression under a flat normal prior @xmath159 with @xmath160 on all effects .",
    "we now illustrate the performance of the proposed method in a simulation study and compare our method to various other approaches : the regularization approach in @xcite ( _ penalty _ ) , the bayesian lasso ( _ blasso _ ) , bayesian elastic net ( _ ben _ ) and the group lasso ( _ glasso _ ) .",
    "additionally , we include bayesian regularization via graph laplacian ( _ glap _ ) , proposed in @xcite .",
    "they also specify the prior directly on the elements of the prior precision matrix , with the goal to identify conditional independence by shrinking off - diagonal elements to zero .",
    "a list of the used r packages and related papers are given in the appendix [ app : sim_meth ] .",
    "additionally , we fit the full model l ( _ full _ ) with separate dummy variables for each level and the true model ( _ true _ ) , i.e.  the model with fused categories according to data generation .",
    "we use a set - up similar as in @xcite and compare the methods with respect to parameter estimation , predictive performance and model selection .      for the simulation study",
    "we generate 100 data sets with @xmath161 observations from the gaussian linear regression model ( [ model1 ] ) with intercept @xmath162 , error variance @xmath163 and fixed design matrix @xmath164 .",
    "we use four ordinal and four nominal predictors , where two regressors have eight and two have four categories for each type of covariate ( ordinal and nominal ) .",
    "regression effects are set to @xmath165 and @xmath166 for the ordinal and @xmath167 and @xmath168 for the nominal covariates , and @xmath169 for @xmath170 .",
    "levels of the predictors are generated with probabilities @xmath171 and @xmath172 for regressors with eight and four levels , respectively .    to perform effect fusion ,",
    "we specify a normal prior with variance @xmath173 for the intercept and the improper prior @xmath174 ( which corresponds to an inverse gamma distribution with parameters @xmath175 ) for the error variance @xmath136 . for each covariate @xmath2 ,",
    "the hyperparameters are set to @xmath176 and @xmath177 , but we investigate also different values in section [ sec : sim_hyp ] .",
    "mcmc is run for 10000 iterations after burnin of 5000 to perform model selection for each data set . models _ full _ and _ true _ and the refit of the selected model are estimated under a flat normal prior @xmath178 with @xmath173 on the regression coefficients and mcmc is run for 3000 iterations after a burnin of 1000 .",
    "the tuning parameters of the frequentist methods _ penalty _ and _ glasso _ are selected automatically via cross - validation in the corresponding r packages .",
    "for the bayesian methods , we use the default prior parameter settings in the code ( for _ _ g__lap ) and the r packages monomvn and ebglmnet and estimate the regression coefficients by the posterior means .",
    "we first compare the suggested method for bayesian effect fusion to the other approaches with respect to estimation of the regression effects .",
    "figure [ fig : sim_mse ] shows the mean squared estimation error ( mse ) defined for each covariate @xmath2 as @xmath179    obviosly , the mean of the mses ( over all 100 data sets ) are lower for bayesian effect fusion than for all other methods .",
    "bayesian effect fusion performs particularly well for covariates where all levels have an effect of zero ( variables 2 , 4 , 6 , 8) . for covariates with non - zero effects overall",
    "performance is good , but for some data sets the mse can be higher than for the full model , when levels with actually different effects are fused , see eg .",
    "covariate 5 , a nominal covariate with eight levels .",
    "the competitors _ blasso _ and _ ben _ perform very well both for covariates with zero as well as covariates with non - zero effects .",
    "_ penalty _ which is designed for effect fusion does not clearly outperform these two methods for covariates with non - zero effects but yields higher mses for covariates with no effects .",
    "glasso _ performs reasonably well for covariates with no effects but worse for covariates with non - zero effects and _ glap _ yields only slight improvements compared to the full model for covariates with no effect .",
    "we would like to remark that also model averaged estimates , which are obtained as posterior mean estimates from the first mcmc run under the effect fusion prior , perform very well with respect to parameter estimation .",
    "simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal .",
    "variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ]   simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal .",
    "variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ] +   simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal .",
    "variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ]   simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal .",
    "variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ] +   simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal .",
    "variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ]   simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal .",
    "variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ] +   simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal . variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ]   simulation study : mses for 100 simulated data sets .",
    "variable 1 to 4 are ordinal , variable 5 to 8 are nominal .",
    "variables on the right panel ( even numbers ) have no effect on the response .",
    ", title=\"fig : \" ]    to evaluate the predictive performance of bayesian effect fusion , we generate a new sample of @xmath180 observations @xmath181 from the linear regression model ( [ model1 ] ) with fixed regressors @xmath182 and the same parameters as in the simulated data sets .",
    "predictions for these new observations are computed using the estimates from each of the original data sets as @xmath183 .",
    "the mean squared prediction errors ( mspe ) defned for each data set as @xmath184 are shown in figure [ res : mse_pred ] .",
    "the predictive performance for our method is almost as good as if the true model were known and considerably better than for all competing methods in most data sets .",
    "_ blasso _ is the second best method and also _ penalty _ , _ glasso _ and _ ben _ yield slightly smaller prediction errors compared to full model .",
    "prediction errors using _ glap _ are similar to those from the full model .        finally , to evaluate and compare the performance of the methods with respect to model selection , we use the true positive rate ( tpr ) , the true negative rate ( tnr ) , the positive predictive value ( ppv ) and the negative predictive value ( npv ) , see appendix [ app : sim_measures ] for detailed definitions .",
    "if fusion is completely correct , all four values are equal to 100% but tpr and ppv are not defined for covariates where all effects are zero .",
    "for the effect fusion prior we perform model selection as described in section [ sec : model_sel ] , for the other methods we consider two level effects as identical if the posterior mean of their difference is smaller or equal to 0.01 .",
    "results reported in tables [ tab : model_res_ord ] and [ tab : model_res_nom ] show that bayesian effect fusion clearly outperforms all other methods in particular with respect to identifying categories with the same effect ( tnr ) .        as measures for correct model selection ,",
    "we use true positive rate ( tpr ) , true negative rate ( tnr ) , positive predictive value ( ppv ) and negative predictive value ( npv ) .",
    "generally , they are defines as follows : @xmath185    in our setting of level effect fusion , tp ( _ true positive _ ) is the number of correctly detected non - zero effect differences , ( tn ) _ true negative _ the number of correctly detected zero difference , fn ( _ false negative _ ) the number of zero effect difference clasisified as npn - zero and ( fp ) _ false positive _ the number of zero effect differences classified as non - zero ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a bayesian approach to obtain a sparse representation of the effect of a categorical predictor in regression type models . as the effect of a categorical predictor </S>",
    "<S> is captured by a group of level effects , sparsity can not only be achieved by excluding single irrelevant level effects but also by excluding the whole group of effects associated to a predictor or by fusing levels which have essentially the same effect on the response . to achieve this goal </S>",
    "<S> , we propose a prior which allows for almost perfect as well as almost zero dependence between level effects a priori . </S>",
    "<S> we show how this prior can be obtained by specifying spike and slab prior distributions on all effect differences associated to one categorical predictor and how restricted fusion can be implemented . </S>",
    "<S> an efficient mcmc method for posterior computation is developed . </S>",
    "<S> the performance of the proposed method is investigated on simulated data . </S>",
    "<S> finally , we illustrate its application on real data from eu - silc .    </S>",
    "<S> _ keywords _ : spike and slab prior , sparsity , nominal and ordinal predictor , regression model , mcmc , gibbs sampler </S>"
  ]
}