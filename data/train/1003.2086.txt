{
  "article_text": [
    "a recent new scientist article  @xcite deals with errors in courts due to `` bad mathematics '' , advocating the use of the so - called bayesian methods to avoid them .",
    "although most examples of resulting `` rough justice '' come from real life cases , the first `` probabilistic pitfall '' is taken from crime fiction , namely from a `` 1974 episode of the cult us television series '' _ columbo _ , in which a `` society photographer has killed his wife and disguised it as a bungled kidnapping . ''",
    "the pretended mistake happens in the concluding scene , when `` the hangdog detective [  ] induces the murderer to grab from a shelf of 12 cameras the exact one used to snap the victim before she was killed . ''",
    "according to the article author ( or to experts on which scientific journalists often rely on ) the question is that `` killer or not , anyone would have a 1 in 12 chance of picking the same camera at random .",
    "that kind of evidence would never stand up in court . ''",
    "then a sad doubt is raised , `` or would it ?",
    "in fact , such probabilistic pitfalls are not limited to crime fiction . ''    being myself not particularly fond of this kind of entertainment ( perhaps with a little exception of the columbo series , that i watch casually ) , i can not tell how much crime fiction literature and movies are affected by `` probabilistic pitfalls '' .",
    "instead , i can give firm witness that scientific practice is plenty of mistakes of the kind reported in ref.@xcite , that happen even in fields the general public would hardly suspect , like frontier physics , whose protagonists are supposed to have a skill in mathematics superior to police officers and lawyers .",
    "but it is not just a question of math skill ( complex calculations are usually done without mistakes ) , but of _ probabilistic reasoning _",
    "( what to calculate ! ) .",
    "this is a quite old story .",
    "in fact , as david hume complained 260 years ago  @xcite ,    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _   _ `` the celebrated monsieur leibniz has observed it to be a defect in the common systems of logic , that they are very copious when they explain the operations of the understanding in the forming of demonstrations , but are too concise when they treat of probabilities , and those other measures of evidence on which life and action entirely depend , and which are our guides even in most of our philosophical speculations . ''",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    it seems to me that the general situation has not improved much .",
    "yes , ` statistics ' ( a name that , meaning too much , risks to mean little ) is taught in colleges and universities to students of several fields , but distorted by the ` frequentistic approach ' , according to which one is not allowed to speak of probabilities of causes .",
    "this is , in my opinion , the original sin that gives grounds for a large number of probabilistic mistakes even by otherwise very valuable scientists and practitioners ( see e.g. chapter 1 of ref .",
    "@xcite ) .",
    "going back to the `` shambling sleuth columbo '' , being my wife and my daughter his fans , it happens we own the dvd collections of the first seven seasons .",
    "it occurred then i watched with them , not much time ago ( perhaps last winter ) , the ` incriminated ' , superb episode _ negative reaction _",
    "@xcite , one of the best performances of peter falk playing the role of the famous lieutenant .",
    "however , reading the mentioned new scientist article , i did not remember i had a ` negative reaction ' from the final scene , although i use and teach bayesian methods for a large variety of applications .",
    "did i overlook something ?",
    "i watched again the episode and i was again convinced columbo s last move was a conclusive checkmate . ) .",
    "therefore , rather than chess , the name of the game is _ poker _ , and columbo s bluff is able to induce the murderer to provide a crucial piece of evidence to finally incriminate him .",
    "] then i have invited some friends , all with physics or mathematics degree and somewhat knowledgeable of the bayesian approach , to enjoy an evening together during the recent end of year holidays in order to let them make up their minds whether columbo had good reasons to take paul galesco , magnificently impersonated by dick van dyke , in front of the court ( bayes or not , we had some fun  ) .",
    "the verdict was unanimous : columbo was fully absolved or , more precisely , there was nothing to reproach the story writer , peter s. fischer .",
    "the convivial after dinner jury also requested me to write a note on the question , possibly with a short , self - contained introduction to the ` required math ' .",
    "not only to ` defend columbo ' or , more properly , his writer , but , and more seriously , to defend the bayesian approach , and in particular its applications in forensic science .",
    "in fact , we all deemed the beginning paragraphs of the new scientist article could throw a bad light on the rest of the contents .",
    "imagine a casual reader of the article , possibly a lawyer , a judge or a student in forensic science , to which the article was virtually addressed , and who might have seen _ negative reaction_. most likely he / she considered legitimate the charges of the policemen against the photographer .",
    "the ` negative reaction ' would be that the reader would consider the rest of the article a support of dubious validity to some ` strange math ' that can never substitute the human intuition in a trial . ]",
    "not a good service to the ` bayesian cause ' .",
    "( imagine somebody trying to convince you with arguments you hardly understand and who begins asserting something you consider manifestly false . )    in the following section i introduce the basic elements of bayesian reasoning ( subsection [ ss : jl ] can be skipped on first reading ) , using a toy model as guiding example in which the analysis of ref .",
    "@xcite ( `` 1 in 12 '' , or , more precisely `` 1 in 13 '' ) holds . section [ sec : columbo_priors ]",
    "shows how such a kind of evidence would change columbo s and jury s opinion .",
    "then i discuss in section [ sec : finale ] why a similar argument does not apply to the clip in which columbo finally frames galesco , and why all witnesses of the crucial actions ( including tv watchers , with the exception of the author of ref .",
    "@xcite and perhaps a few others ) and an hypothetical court jury ( provided the scene had been properly reported ) had to be absolutely positive the photographer killed his wife ( or at least he knew who did it in his place ) .    the rest of the paper might be marginal , if you are just curious to know why i have a different opinion than ref .",
    "@xcite , although i agree on the validity of bayesian reasoning .",
    "in fact , at the end of the work , this paper is not the ` short note ' initially planned .",
    "the reason is that the past months i had many discussions on some of the questions treated here with people from several fields .",
    "i have realized once more that it is not easy to put the basic principles at work if some important issues are not well understood .",
    "people are used to solving their statistical problems with ` ad hoc ' formulae ( see appendix h ) and therefore tend to add some ` bayesian recipes ' in their formularium .",
    "it is then too high the risk that one looks at simplified methods ",
    "bayesian methods require a bit more thinking and computation that others !  that are even advertised as ` objective ' . or one just refuses to use any math , on the defense of pure intuition .",
    "( by the way , this is an important point and i will take the opportunity to comment on the apparent contradictions between intuition and formal evaluation of beliefs , defending  both , but encouraging the use of the latter , superior to the former in complex situations  see in particular appendix c ) .",
    "so , to conclude the introduction , this document offers several levels of reading :    * if you are only interested to columbo s story , you can just jump straight to section [ sec : finale ] . *",
    "if you also ( or regardless of columbo ) want to have an opportunity to learn the basic rules of bayesian inference , subsections [ ss : bayes_theorem ] , [ ss : bayes_theorem_2 ] and [ ss : many_pieces ] , based on a simple master example , have been written on the purpose",
    ". then you might appreciate the advantage of logarithmic updating ( section [ ss : jl ] ) and perhaps see how it applies to the aids example of appendix f. + * if you already know the basics of the probabilistic reasoning , but you wonder how it can be applied into real cases , then section [ sec : real_life ] should help , together with some of the appendices . *",
    "if none of the previous cases is yours ( you might even be an expert of the field ) , you can simply browse the document .",
    "perhaps some appendices or subsections might still be of your interest .",
    "* finally , there is the question of the many footnotes , which can break the pace of the reading .",
    "they are not meant to be necessarily read sequentially along with the main text and could be skipped on a first fast reading ( in fact , this document is closer to an hypertext than to a standard article . )    enjoy !",
    "let us leave aside columbo s cameras for a while and begin with a different , simpler , stereotyped situation easier to analyze .",
    "imagine there are two types of boxes , @xmath0 , that only contain white balls ( @xmath1 ) , and @xmath2 , that contain one white balls and twelve black ( incidentally , just to be precise , although the detail is absolutely irrelevant , we have to infer from columbo s words , _ `` you did nt touch any of these twelve cameras .",
    "you picked up that one '' _ , the cameras were thirteen ) .",
    "you take at random a box and extract a ball .",
    "the resulting color is _",
    "white_. you might be interested to evaluate the probability that the box is of type @xmath0 , in the sense of stating in a quantitative way how much you believe this hypothesis .",
    "in formal terms we are interested in @xmath3 , knowing that @xmath4 and @xmath5 , a problem that can be sketched as @xmath6 [ here ` @xmath7 ' stands for ` given ' , or ` conditioned by ' ; @xmath8 is the general ( ` background ' ) status of information under which this probability is assessed ; ` @xmath9 ' or ` @xmath10 ' after ` @xmath7 ' indicates that both conditions are relevant for the evaluation of the probability . ]",
    "a typical mistake at this point is to confuse @xmath3 with @xmath11 , or , more often , @xmath12 with @xmath13 , as largely discussed in ref .",
    "hence we need to learn how to turn properly @xmath11 into @xmath3 using the rules of probability theory .",
    "the ` probabilistic inversion ' @xmath14 can _ only _ and appendix h. ] be performed using the so - called _",
    "bayes theorem _ , a simple consequence of the fact that , given the _ effect _",
    "@xmath15 and some _ hypotheses _ @xmath16 concerning its possible cause , the joint probability of @xmath15 and @xmath16 , conditioned by the _ background information _ represents all we know about the hypotheses and the effect considered .",
    "writing @xmath8 in all expressions could seem a pedantry , but it is nt .",
    "for example , if we would just write @xmath17 in these formulae , instead of @xmath18 , one might be tempted to take this probability equal to one ,  because the observed event is a well established fact , that has happened and is then certain .",
    "but it is not this certainty that enters these formulae , but rather the probability ` that fact could happen ' in the light of ` everything we knew ' about it ( ` @xmath8 ' ) . ]",
    "@xmath8 , can be written as @xmath19 where ` @xmath20 ' stands for a logical ` and ' . from the second equality of the last equation we get@xmath21 that is one of the ways to express bayes theorem . ]",
    "valid if we deal with a class of incompatible hypotheses [ i.e. @xmath22 and @xmath23 .",
    "in fact , in this case a general rule of probability theory [ eq .  ( [ eq : rul6 ] ) in appendix a ] allows us to rewrite the denominator of eq .",
    "( [ eq : phi|e ] ) as @xmath24 . in this note , dealing only with two hypotheses , we prefer to reason in terms of probability ratios , as shown in eq .",
    "( [ eq : bayes_factor ] ) . ] since a similar expression holds for any other hypothesis @xmath25 , dividing member by member the two expressions we can restate the theorem in terms of the relative beliefs , that is @xmath26 the initial ratio of beliefs ( ` odds ' ) is updated by the so - called _ bayes factor _ , that depends on how likely _ each _ hypothesis can produce that effect . , the probabilities of the effects @xmath27 and @xmath28 have usually nothing to do with each other . ] introducing @xmath29 and @xmath30 , with obvious meanings , we can rewrite eq .",
    "( [ eq : bayes_factor ] ) as @xmath31 note that , if the initial odds are unitary , than the final odds are equal to the updating factor .",
    "_ bayes factors can be interpreted as odds due only to an individual piece of evidence , the two hypotheses were considered initially equally likely_. and [ fn : jaynes ] , as well as appendix h. ) ] this allows us to rewrite @xmath32 as @xmath33 , where the tilde is to remind that _ they are not properly odds _ , but rather ` _ pseudo - odds _ ' .",
    "we get then an expression in which all terms have _ virtually _ uniform meaning : @xmath34 if we have only two hypotheses , we get simply @xmath35 .",
    "if the updating factor is unitary , then the piece of evidence does not modify our opinion on the two hypotheses ( no matter how small can numerator and denominator be , as long as their ratio remains finite and unitary ! ",
    "see appendix g for an example worked out in details ) ; when @xmath36 vanishes , then hypothesis @xmath0 becomes impossible ( `` _ it is falsified _ '' ) ; if instead it is infinite ( i.e. the denominator vanishes ) , then it is the other hypothesis to be impossible .",
    "( the undefined case @xmath37 means that we have to look for other hypotheses to explain the effect . , but an absolute truth , i.e. @xmath38 , depends on which class of hypotheses is considered . stated in other words , in the realm of probabilistic inference",
    "_ falsities can be absolute , but truths are always relative_. ] )      applying the updating reasoning to our box game , the bayes factor of interest is @xmath39 as it was remarked , this number would give the required odds _ if _ the hypotheses were initially equally likely . but how strong are the initial relative beliefs on the two hypotheses ? ` unfortunately ' , we can not perform a probabilistic inversion if we are unable to assign somehow _ prior probabilities _ to the hypotheses we are interested in . and appendix h. [ fn : nopriors ] ]    indeed , in the formulation of the problem i on purpose passed over the relevant pieces of information to evaluate the prior probabilities ( it was said that `` there are two types of boxes '' , not `` there are two boxes '' ! ) .",
    "if we specify that we had @xmath40 boxes of type @xmath0 and @xmath41 of the other kind , then the initial odds are @xmath42 and the final ones will be @xmath43 from which we get ( just requiring that the probability of the two hypotheses have to sum up to one and @xmath44 are generic , complementary hypotheses we get , calling @xmath45 the bayes factor of @xmath46 versus @xmath44 and @xmath47 the initial odds to simplify the notation , the following convenient expressions to evaluate the probability of @xmath46 : @xmath48 ] ) @xmath49 if the two hypotheses were _ initially _ considered equally likely , then the evidence @xmath1 makes @xmath0 13 times more believable than @xmath2 , i.e. @xmath50 , or approximately 93% . on the other hand , if @xmath0 was _ a priori _ much less credible than @xmath2 , for example by a factor 13 , just to play with round numbers , the same evidence made @xmath0 and @xmath2 equally likely . instead",
    ", if we were initially in strong favor of @xmath0 , considering it for instance 13 times more plausible than @xmath2 , that evidence turned this factor into 169 , making us 99.4% confident  _ highly confident _ , some would even say ` practically sure ' !  that the box is of type @xmath0 .",
    "imagine now the following variant of the previous toy experiment .",
    "after the white ball is observed , you put it again in the box , shake well and make a second extraction .",
    "you get white the second time too . calling @xmath51 and",
    "@xmath52 the two observations , we have now : ) , although we are dealing now with more complex events and complex hypotheses , logical and of simpler ones . moreover , eq .  ( [ eq : seq:2 ] )",
    "is obtained from eq .",
    "( [ eq : seq:1 ] ) making use of the formula ( [ eq : joint_prob ] ) of joint probability , that gives @xmath53 and an analogous formula for @xmath2 . note also that , going from eq .",
    "( [ eq : seq:2 ] ) to eq .",
    "( [ eq : seq:3 ] ) , @xmath54 has been rewritten as @xmath55 to emphasize that the probability of a second white ball , conditioned by the box composition and the result of the first extraction , depends indeed only on the box content and not on the previous outcome ( ` extraction after re - introduction ' ) . ]",
    "@xmath56 that , using the compact notation introduced above , we can rewrite in the following enlighting forms .",
    "the first is [ eq .  ( [ eq : seq:4 ] ) ] @xmath57 that is , _ the final odds after the first inference become the initial odds of the second inference _ ( and so on , if there are several pieces of evidence ) .",
    "therefore , beginning from a situation in which @xmath0 was thirteen times more credible than @xmath2 is exactly equivalent to having started from unitary odds updated by a factor 13 due to a piece of evidence .",
    "the second form comes from eq .",
    "( [ eq : seq:3 ] ) : @xmath58 i.e. ) follows from eq .",
    "( [ eq : seq:3a ] ) because a bayes factor can be defined as the ratio of final odds over the initial odds , depending on the evidence .",
    "therefore @xmath59 ] @xmath60 _ bayes factors _ due to _ independent _ , that we have used above to turn eq .",
    "( [ eq : seq:2 ] ) into eq .",
    "( [ eq : seq:3 ] ) and that can be expressed , in general terms as @xmath61 i.e. , _ under the condition of a well precise hypothesis _ ( @xmath16 ) , the probability of the effect @xmath62 does not depend on the knowledge of whether @xmath63 has occurred or not . note that , in general , although @xmath63 and @xmath62 are independent given @xmath16 ( they are said to be _ conditionally independent _ ) , they might be otherwise _ dependent _ , i.e. @xmath64 .",
    "( going to the example of the boxes , it is rather easy to grasp , although i can not enter in details here , that , if we do not know the kind of box , the observation of @xmath51 changes our opinion about the box composition and , as a consequence , the probability of @xmath52  see the examples in appendix j ) ] pieces of evidence _ multiply_.",
    "that is , two independent pieces of evidence ( @xmath51 and @xmath52 ) are equivalent to a single piece of evidence ( ` @xmath65 ' ) , whose bayes factor is the product of the individual ones . in our case @xmath66 .    in general , if we have several hypotheses @xmath16 and several _ independent _ pieces of evidence , @xmath63 , @xmath62 ,  ,",
    "@xmath67 , indicated all together as @xmath68 , then eq .",
    "( [ eq : bayes_factor ] ) becomes @xmath69    \\times o_{i , j}(i)\\ , ,   \\label{eq : product_odds}\\end{aligned}\\ ] ] i.e. @xmath70 where @xmath71 stand for ` product ' ( analogous to @xmath72 for sums ) .      the remark that bayes factors due to independent pieces of evidence multiply together and the overall factor finally multiplies the initial odds suggests a change of variables in order to play with additive quantities .",
    "this can be done taking the logarithm of both sides of eq .",
    "( [ eq : product_odds ] ) , that then become @xmath73 & = &   \\sum_{k=1}^n \\log_{10}[\\tilde o_{i , j}(e_k , i ) ] + \\log_{10}[o_{i , j}(i)]\\ , , \\label{eq : sum_bf}\\end{aligned}\\ ] ] respectively , where the base 10 is chosen for practical convenience because , as we shall discuss later , what substantially matters are powers of ten of the odds . introducing the new symbol jl",
    ", we can rewrite eq .",
    "( [ eq : sum_bf ] ) as @xmath74 or @xmath75 where @xmath76 \\label{eq : jl_alle}\\\\ \\mbox{jl}_{i , j}(i ) & = &   \\log_{10}\\left[o_{i , j}(i ) \\right]\\label{eq : jl_0}\\\\ \\delta\\mbox{jl}_{i , j}(e_k , i ) & = &   \\log_{10}\\left[\\tilde o_{i , j}(e_k , i)\\right ]    \\label{eq : deltajl_k } \\\\   \\delta\\mbox{jl}_{i ,",
    "j}(\\mvec{e},i )   & = &   \\sum_{k=1}^n \\delta\\mbox{jl}_{i , j}(e_k , i)\\ , .",
    "\\label{eq : sum_deltajl_k}\\end{aligned}\\ ] ]    the letter ` l ' in the symbol is to remind _",
    "logarithm_. but it has also the mnemonic meaning of _ leaning _ , in the sense of ` inclination ' or ` propension ' . the ` j ' is for _ judgment_. therefore ` jl ' stands for _ judgement leaning _ , that is an inclination of the judgement , an expression i have taken the liberty to introduce , using words not already engaged in probability and statistics , because in these fields many controversies are due to different meanings attributed to the same word , or expression , by different people ( see appendices b and g for further comments ) .",
    "jl can then be visualized as the indicator of the ` justice balance ' to which @xmath77 refers stands for ` guilty ' . ]",
    "( figure [ fig : jl ] ) , that displays zero if there is no unbalance , but it could move to the positive or the negative side depending on the weight of the several arguments pro and con .",
    "the role of the evidence is to vary the jl indicator by quantities @xmath78 s equal to base 10 logarithms of the bayes factors , that have then a meaning of _ weight of evidence _ , an expression due to charles sanders peirce  @xcite ( see appendix e ) .",
    "but the judgement is rarely initially unbalanced .",
    "this the role of @xmath79 , that can be considered as a a kind of _ initial weight of evidence _ due to our prior knowledge about the hypotheses @xmath16 and @xmath25 [ and that could even be written as @xmath80 , to stress that it is related to a 0-th piece of evidence ]    to understand the rationale behind a possible uniform treatment of the prior as it would be a piece of evidence , let us start from a case in which you now _ absolutely nothing_. for example have to state your beliefs on which of my friends , dino or paolo , will first run next rome marathon .",
    "it is absolutely reasonable you assign to the two hypotheses equal probabilities , i.e. @xmath81 , or @xmath82 ( your judgement is perfectly balanced ) .",
    "this is because in brain these names are only possibly related to italian males .",
    "nothing more .",
    "( but nowadays search engines over the web allow to modify your opinion in minutes . )    as soon as you deal with _ real _ hypotheses of your interest , things get quite different .",
    "it is in fact very rare the case in which the hypotheses tell you not more than their names .",
    "it is enough you think at the hypotheses ` rain ' or ` not rain ' , the day after you read these lines in the place where you live . in general",
    "the information you have in your brain related to the hypotheses of your interest can be considered the initial piece of evidence _ you _ have , usually different from that somebody else might have ( this the role of @xmath8 in all our expressions ) .",
    "it follows that prior odds of 10 ( @xmath83 ) will influence your leaning towards one hypothesis , exactly like unitary odds ( @xmath84 ) followed by a bayes factor of 10 ( @xmath85 ) .",
    "this the reason they enter on equal foot when `` balancing arguments '' ( to use an expression  la peirce  see the appendix e ) pro and against hypotheses .",
    ".a comparison between probability , odds and judgement leanings [ cols=\"^,^,^,^,^,^,^ \" , ]     as we see from this table , and as we better understand from figure [ fig : two_normal ] , numbers large in module are in favor of @xmath44 , and very large ones are in its strong favor . instead",
    ", the numbers laying in the interval defined by the two points marked in the figure by a cross provide evidence in favor of @xmath46 .",
    "however , while individual pieces of evidence in favor of @xmath46 can only be weak ( the maximum of @xmath86jl is about 0.3 , reached around @xmath87 , namely @xmath88 , to be precise , for which @xmath86jl reaches 0.313 ) , those in favor of the alternative hypothesis can be sometimes very large .",
    "it follows then that one gets easier convinced of @xmath44 rather than of @xmath46 .",
    "we can check this by a little simulation .",
    "we choose a model , extract 50 random variables and analyze the data as if we did nt know which generator produced them , although considering @xmath46 and @xmath44 equally likely .",
    "we expect that , as we go on with the extractions , the pieces of evidence accumulate until we _ possibly _ reach a level of practical certainty .",
    "obviously , the individual pieces of evidence do not provide the same @xmath86jl , and also the sign can fluctuate , although we expect more positive contributions if the points are generated by @xmath46 and the other way around if they came from @xmath44 .",
    "therefore , as a function of the number of extractions the accumulated weight of evidence follows a kind of _ asymmetric random walk _",
    "( imagine the jl indicator fluctuating as the simulated experiment goes on , but drifting ` in average ' in one direction ) .",
    "figure [ fig : simulazioni ] shows 200 inferential stories , half per generator .",
    "we see that , in general , we get practically sure of the model after a couple of dozens of extractions .",
    "but there are also cases in which we need to wait longer before we can feel enough sure on one hypothesis .",
    "it is interesting to remark that the leaning in favor of each hypothesis grows , _ in average _ , linearly with the number of extractions .",
    "that is , a little piece of evidence , which is in average positive for @xmath46 and negative for @xmath44 , is added after each extraction . however , around the average trend , there is a large varieties of individual inferential histories .",
    "they all start at @xmath89 for @xmath90 , but in practice there are no two identical ` trajectories ' .",
    "all together they form a kind of ` fuzzy band ' , whose ` effective width ' grows also with the number of extractions , _ but not linearly_. the widths grows as the square root of @xmath91 .. we can also evaluate the _ uncertainty of prevision _",
    ", quantified by the standard deviation .",
    "we get for the two hypotheses @xmath92 & = & 0.15 \\\\",
    "\\sigma[\\delta\\mbox{jl}_{1,2}(h_1 ) ] & = & 0.24   \\\\ u_r[\\delta\\mbox{jl}_{1,2}(h_1 ) ]   & = & 1.6          \\end{array}\\right . \\hspace{0.5",
    "cm } &   & \\hspace{0.5 cm } \\left\\{\\begin{array}{rcl } \\mbox{e}[\\delta\\mbox{jl}_{1,2}(h_2)]&=&-0.38 \\\\                          \\sigma[\\delta\\mbox{jl}_{1,2}(h_2)]&=&0.97   \\\\",
    "u_r[\\delta\\mbox{jl}_{1,2}(h_2 ) ] & = & 2.6         \\end{array}\\right.\\end{aligned}\\ ] ] where also the _ relative uncertainty _",
    "@xmath93 has been reported , defined as the uncertainty divided by the absolute value of the prevision .",
    "the fact that the uncertainties are relatively large tells clearly that we _ do not expect _ that a single extraction will be sufficient to convince us of either model .",
    "but this does not mean we can not take the decision because the number of extraction _ has been _ too small .",
    "if a very large fluctuation provides a @xmath86jl of @xmath94 ( the table in this section shows that this is not very rare ) , we have already got a very strong evidence in favor of @xmath44 . repeating what has been told several time , what matters is the cumulated judgement leaning .",
    "it is irrelevant if a jl of @xmath94 comes from ten individual pieces of evidence , only from a single one , or partially from evidence and partially from prior judgement .",
    "+ when we plan to make @xmath91 extractions from a generator , probability theory allows us to calculate expected value and uncertainty of @xmath95 : @xmath96 & = & n \\times \\mbox{e}[\\delta\\mbox{jl}_{1,2}(h_i ) ] \\\\",
    "\\sigma[\\delta\\mbox{jl}_{1,2}(n , h_i ) ] & = & \\sqrt{n } \\times \\sigma[\\delta\\mbox{jl}_{1,2}(h_i)]\\\\ u_r[\\delta\\mbox{jl}_{1,2}(n , h_i ) ] & = & \\frac{1}{\\sqrt{n}}\\times u_r[\\delta\\mbox{jl}_{1,2}(h_i)]\\,.\\end{aligned}\\ ] ] in particular , for @xmath97 we get @xmath98 ( @xmath99 ) and @xmath100 ( @xmath101 ) , that explain the gross feature of the bands in figure [ fig : simulazioni ] .",
    "] this is the reason why , as @xmath91 increases , the bands tend to move away from the line @xmath84 .",
    "nevertheless , individual trajectories can exhibit very ` irregular ' that do not follow the general trend _ are not exceptions _ , being generated by the same rules that produces all of them . ]",
    "behaviors as we can also see in figure [ fig : simulazioni ] .",
    "some comments on _ likelihood _ are also in order , because the reader might have heard this term and might wonder if and how it fits in the scheme of reasoning expounded here .",
    "one of the problems with this term is that it tends to have several meanings , and then to create misunderstandings . in plane english",
    "` likelihood ' is `` 1 . the condition of being likely or probable ; probability '' , or `` 2 .",
    "something that is probable '' ; but also `` 3 .",
    "( mathematics & measurements / statistics ) the probability of a given sample being randomly drawn regarded as a function of the parameters of the population '' .",
    "technically , with reference to the example of the previous appendix , the likelihood is simply @xmath102 , where @xmath103 is fixed ( the observation ) and @xmath16 is the ` parameter ' .",
    "then it can take two values , @xmath104 and @xmath105 .",
    "if , instead of only two models we had a continuity of models , for example the family of all gaussian distributions characterized by central value @xmath106 and ` effective width ' ( standard deviation ) @xmath107 , our likelihood would be @xmath108 , i.e. @xmath109 written in this way to remember that : 1 ) a likelihood is a function of the model parameters and not of the data ; 2 ) @xmath110 _ is not _ a probability ( or a probability density function ) of @xmath106 and @xmath107 .",
    "anyway , for the rest of the discussion we stick to the very simple likelihood based on the two gaussians . that is , instead of a double infinity of possibilities , our space of parameters is made only of two points , @xmath111 and @xmath112 .",
    "thus the situation gets simpler , although the main conceptual issues remain substantially the same .    in principle",
    "there is nothing bad to give a special name to this function of the parameters .",
    "but , frankly , i had preferred statistics gurus named it after their dog or their lover , rather than call it ` likelihood . '",
    "was introduced by r. a. fisher with the object of _ avoiding _ the use of bayes theorem ''  @xcite . ]",
    "the problem is that it is very frequent to hear students , teachers and researcher explaining that the ` likelihood ' tells `` how likely the parameters are '' ( this is _ the probability of the parameters ! not the ` likelihood ' _ ) . or",
    "they would say , with reference to our example , `` it is the probability that @xmath103 comes from @xmath16 '' ( again , this expression would be the probability of @xmath16 given @xmath103 , and not the probability of @xmath103 given the models ! ) imagine if we have only @xmath46 in the game : @xmath103 comes with certainty from @xmath46 , although @xmath46 does not yield with certainty @xmath103 .",
    "is '' ( note the quote mark of ` likely ' , as in the example of footnote [ fn : james ] ) .",
    "but , fortunately we find in http://en.wikipedia.org/wiki/likelihood_function that `` this is not the same as the probability that those parameters are the right ones , given the observed sample .",
    "attempting to interpret the likelihood of a hypothesis given observed evidence as the probability of the hypothesis is a common error , with potentially disastrous real - world consequences in medicine , engineering or jurisprudence .",
    "see prosecutor s fallacy [ * ] for an example of this . ''",
    "( [ * ] see http://en.wikipedia.org/wiki/prosecutor%27s_fallacy . )",
    "+ now you might understand why i am particular upset with the name likelihood .",
    "[ fn_wiki_likelihood ] ]    several methods in ` conventional statistics ' use somehow the likelihood to decide which model or which set of parameters describes at best the data .",
    "some even use the likelihood ratio ( our bayes factor ) , or even the logarithm of it ( something equal or proportional , depending on the base , to the weight of evidence we have indicated here by jl )",
    ". the most famous method of the series is the _ maximum likelihood principle_. as it is easy to guess from its name , it states that the _ best estimates _ of the parameters are those which maximize the likelihood .",
    "all that _ seems _ reasonable and in agreement with what it has been expounded here , but it is not quite so .",
    "first , for those who support this approach , likelihoods are not just a part of the inferential tool , they are everything .",
    "priors are completely neglected , more or less because of the objections in footnote [ fn : nopriors ] .",
    "this can be acceptable , if the evidence is overwhelming , but this is not always the case .",
    "unfortunately , as it is now easy to understand , neglecting priors is mathematically equivalent to consider the alternative hypotheses equally likely ! as a consequence of this statistics miseducation ( most statistics courses in the universities all around the world only teach",
    "` conventional statistics ' and never , little , or badly probabilistic inference ) is that too many unsuspectable people fail in solving the aids problem of appendix b , or confuse the likelihood with the probability of the hypothesis , resulting in misleading scientific claims ( see also footnote [ fn_wiki_likelihood ] and ref .",
    "@xcite ) .",
    "the second difference is that , since `` there are no priors '' , the result can not have a probabilistic meaning , as it is openly recognized by the promoters of this method , who , in fact , do not admit we can talk about probabilities of causes ( but most practitioners seem not to be aware of this ` little philosophical detail ' , also because frequentistic gurus , having difficulties to explain what is the meaning of their methods , they say they are ` probabilities ' , but in quote marks![multiblock footnote omitted ] ) . as a consequence , the resulting ` error analysis ' , that in human terms means to assign different beliefs to different values of the parameters , is cumbersome . in practice",
    "the results are reasonable only if the possible values of the parameters are initially equally likely and the ` likelihood function ' has a ` kind shape ' ( for more details see chapters 1 and 12 of ref .",
    "in most cases ( and practically always in courts ) pieces of evidence are not acquired directly by the person who has to form his mind about the plausibility of a hypothesis .",
    "they are usually accounted by an intermediate person , or by a chain of individuals .",
    "let us call @xmath113 the report of the evidence @xmath15 provided in a _",
    "testimony_. the inference becomes now @xmath114 , generally different from @xmath115 .    in order to apply bayes theorem in one of its form",
    "we need first to evaluate @xmath116 .",
    "probability theory teaches us how to get it [ see eq .",
    "( [ eq : rul4 ] ) in appendix a ] : @xmath117 ( @xmath113 could be due to a true evidence or to a fake one ) .",
    "three new ingredients enter the game :    * @xmath118 , that is the probability of the evidence to be correctly reported as such . * but the testimony could also be incorrect the other way around ( it could be incorrectly reported , simply by mistake , but also it could be a ` fabricated evidence ' ) , and therefore also @xmath119 is needed . note that the probabilities to lie could be in general asymmetric , i.e. @xmath120 , as we have seen in the aids problem of appendix f , in which the response of the analysis models false witness well . * finally , since @xmath121 enters now directly , the ` nave ' bayes factor , only depending on @xmath122 , is not longer enough .    taking our usual two hypotheses , @xmath123 and @xmath124 , we get the following bayes factor based on the _ testified evidence _",
    "@xmath113 ( hereafter , in order to simplify the notation , we use the subscript ` @xmath125 ' in odds and bayes factors , instead of ` @xmath126 ' , to indicate that they are in favor of @xmath125 and against @xmath127 , as we already did in the aids example of appendix f ) : @xmath128 as expected , this formula is a bit more complicate that the bayes factor calculated taking @xmath15 for granted , which is recovered if the lie probabilities vanish @xmath129 { }                                      { \\ \\ \\",
    "\\tilde o_{h}(e , i)}\\,,\\end{aligned}\\ ] ] i.e. only when we are absolutely sure the witness does not err or lie reporting @xmath15 ( but peirce reminds us that `` absolute certainty , or an infinite chance , can never be attained by mortals ''  @xcite ) .",
    "in order to single out the effects of the new ingredients , eq .",
    "( [ eq : bf_lie ] ) can be rewritten as and @xmath130 respectively in the numerator and in the denominator , eq .",
    "( [ eq : bf_lie ] ) becomes @xmath131 then @xmath132 can be indicated as @xmath133 , @xmath121 is equal to @xmath134 and , finally , @xmath130 can be written as @xmath135 . ]",
    "@xmath136 }                               { 1 +   \\lambda(i ) \\cdot \\left [                                  \\frac { \\tilde o_{h}(e , i)}{p(e\\,|\\,h , i ) } -1                                 \\right ] } \\ , , \\label{eq : weight_et}\\end{aligned}\\ ] ] where @xmath137 under the _ condition _ can not be factorized .",
    "the effective odds @xmath138 can however be written in the following convenient forms @xmath139 although less interesting than eq .",
    "( [ eq : weight_et ] ) .",
    "[ fb : peh=0 ] ] @xmath140 and @xmath141 , i.e. _ @xmath142 positive and finite_. the parameter @xmath133 , ratio of the _ probability of fake evidence _ and the _ probability that the evidence is correctly accounted _ , can be interpreted as a kind of _ lie factor_. given the human roughly logarithmic sensibility to probability ratios , it might be useful to define , in analogy to the jl , @xmath143\\,.\\end{aligned}\\ ] ] let us make some instructive limits of eq .",
    "( [ eq : weight_et ] ) .",
    "@xmath144{}{}&\\tilde o_{h}(e , i)\\\\ \\tilde o_{h}(e_t , i ) & \\xrightarrow [ \\mbox{{\\footnotesize                           $ \\lambda(i ) \\rightarrow 1 $ } } ] { } { } & 1 \\\\ \\tilde o_{h}(e_t , i ) & \\xrightarrow [ \\mbox{{\\footnotesize                $ p(e\\,|\\,h , i)\\rightarrow 0 $ } } ] { } { } & 1\\\\ \\tilde o_{h}(e_t , i ) & \\xrightarrow [ \\mbox{{\\footnotesize                $ \\tilde o_{h}(e , i)\\rightarrow \\infty $ } } ] { } { } &              \\frac{p(e\\,|\\,h , i)}{\\lambda(i ) } + 1 - p(e\\,|\\,h , i)\\end{aligned}\\ ] ] as we have seen , the ideal case is recovered if the lie factor vanishes . instead ,",
    "if it is equal to 1 , i.e. @xmath145 , the reported evidence becomes useless .",
    "the same happens if @xmath146 vanishes [ this implies that @xmath130 vanishes too , being @xmath147 .    however , the most remarkable limit is the last one .",
    "it states that , even if @xmath142 is very high , the effective bayes factor can not exceed the inverse of the lie factor : @xmath148}\\,,\\end{aligned}\\ ] ] or , using logarithmic quantities @xmath149 } \\,.\\end{aligned}\\ ] ] at this point some numerical examples are in order ( and those who claim they can form their mind on pure intuition get all my admiration ",
    "_if _ they really can ) .",
    "let us imagine that @xmath15 would ideally provide a weight of evidence of 6 [ i.e. @xmath150 .",
    "we can study , with the help of table [ tab : jl_et ] ,      + @xmath151 & & + & @xmath152 : & 10 & @xmath153 & @xmath154 & @xmath155 & @xmath156 & @xmath157 & @xmath158 & @xmath159 + @xmath160 & & 6.00 & 6.00 & 6.00 & 6.00 & 6.00 & 6.00 & 6.00 & 6.00 + @xmath161 & & 6.00 & 6.00 & 6.00 & 6.00 & 5.99 & 5.95 & 4.96 & @xmath162 + @xmath163 & & 5.96 & 5.96 & 5.96 & 5.95 & 5.92 & 5.68 & 4.00 & @xmath164 + @xmath165 & & 5.70 & 5.70 & 5.70 & 5.68 & 5.52 & 4.92 & 3.00 & @xmath166 + @xmath94 & & 4.96 & 4.96 & 4.95 & 4.92 & 4.68 & 3.95 & 2.00 & @xmath167 + @xmath168 & & 4.00 & 4.00 & 3.99 & 3.95 & 3.70 & 2.96 & 1.04 & @xmath169 + @xmath158 & & 3.00 & 3.00 & 3.00 & 2.96 & 2.70 & 1.96 & 0.30 & @xmath170 + @xmath171 & & 2.00 & 2.00 & 2.00 & 1.95 & 1.70 & 1.00 & 0.04 & @xmath172 + @xmath157 & & 1.00 & 1.00 & 1.00 & 0.96 & 0.74 & 0.26 & 0.004 & @xmath173 +  0 & &  0 &  0 &  0 &  0 &  0 &  0 &   0 &  0 +   +   + @xmath151 & & + & @xmath152 : & 10 & @xmath153 & @xmath154 & @xmath155 & @xmath156 & @xmath157 & @xmath158 & @xmath159 + @xmath160 & & 3.00 & 3.00 & 3.00 & 3.00 & 3.00 & 3.00 & 3.00 & 3.00 + @xmath94 & & 3.00 & 3.00 & 3.00 & 3.00 & 2.99 & 2.95 & 1.96 & @xmath167 + @xmath168 & & 2.96 & 2.96 & 2.96 & 2.95 & 2.92 & 2.68 & 1.04 & @xmath169 + @xmath158 & & 2.70 & 2.70 & 2.70 & 2.68 & 2.52 & 1.93 & 0.30 & @xmath170 + @xmath171 & & 1.96 & 1.96 & 1.96 & 1.92 & 1.68 & 1.00 & 0.04 & @xmath172 + @xmath157 & & 1.00 & 1.00 & 0.99 & 0.96 & 0.74 & 0.26 & 0.004 & @xmath173 +  0 & &  0 &  0 &  0 &  0 &  0 &  0 &   0 &  0 +   +   + @xmath151 & & + & @xmath152 : & 10 & @xmath153 & @xmath154 & @xmath155 & @xmath156 & @xmath157 & @xmath158 & @xmath159 + @xmath160 & & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 + @xmath158 & & 1.00 & 1.00 & 1.00 & 1.00 & 0.99 & 0.96 & 0.26 & @xmath170 + @xmath171 & & 0.96 & 0.96 & 0.96 & 0.96 & 0.93 & 0.72 & 0.04 & @xmath172 + @xmath157 & & 0.72 & 0.72 & 0.72 & 0.70 & 0.58 & 0.23 & 0.003 & @xmath173 + @xmath174 & & 0.41 & 0.41 & 0.41 & 0.39 & 0.27 & 0.07 & @xmath175 & @xmath173 +  0 & &  0 &  0 &  0 &  0 &  0 &  0 &   0 &  0 +    how the _ weight of the reported evidence _",
    "@xmath176 depends on the other beliefs [ in this table logarithmic quantities have been used throughout , therefore @xmath177 is the base ten logarithm of the odds in favor of @xmath15 given the hypothesis @xmath125 ; the table provides , for comparisons , also @xmath176 from @xmath178 equal to 3 and 1 ] .",
    "the table exhibits the limit behaviors we have seen analytically . in particular , if we fully trust the report , i.e. @xmath179 , then @xmath176 is exactly equal to @xmath180 , as we already know .",
    "but as soon as the absolute value of the lie factor is close to @xmath181 , there is a sizeable effect .",
    "the upper bound can be the be rewritten as @xmath182\\,,\\\\ \\mbox{or}\\hspace{3cm}\\mbox { } & & \\mbox { } \\nonumber\\\\     \\delta\\mbox{jl}_h(e_t , i ) & \\le &          \\mbox{min}\\,[\\delta\\mbox{jl}_h(e , i),\\,-\\mbox{j}\\lambda(i ) ] \\,,\\end{aligned}\\ ] ] a relation valid in the region of interest when thinking about an evidence in favor of @xmath125 , i.e. @xmath183 and @xmath184 .",
    "this upper bound is very interesting .",
    "since minimum conceivable values of @xmath151 for human beings can be of the order of @xmath165 ( to perhaps @xmath185 or @xmath186 , but in many practical applications @xmath171 or @xmath158 can already be very generous ! ) , in practice the effective weights of evidence can not exceed values of about @xmath187 ( i have no strong opinion on the exact value of this limit , my main point is that _ you consider there might be such a practical human limit_. )    this observation has an important consequence in the combination of evidences , as anticipated at the end of section [ ss : agatha ]",
    ". should we give more consideration to a single strong piece of evidence , virtually weighing @xmath188 , or 10 independent weaker evidences , each having a @xmath86jl of 1 ?",
    "as it was said , in the ideal case they yield the same global leaning factor .",
    "but as soon as human fallacy ( or conspiracy ) is taken into account , and we remember that our belief is based on @xmath113 and not on @xmath15 , then we realize that @xmath189 is well above the range of jl that we can reasonably conceive .",
    "instead the weaker pieces of evidence are little affected by this doubt and when they sum up together , they really can provide a @xmath86jl of about 10 .",
    "let us go back to our toy model of section [ sec:1in13 ] and let us complicate it just a little bit , adding the possibility of incorrect testimony ( but we also simplify it using uniform priors , so that we can focus on the effect of the _ uncertain evidence _ ) .",
    "for example , imagine you do not see directly the color of the ball , but this is reported to you by a collaborator , who , however , might not tell you always the truth .",
    "we can model the possibility of a lie in following way : after each extraction he tosses a die and reports the true color only if the die gives a number smaller than 6 . using the formalism of appendix i",
    ", we have @xmath190 the resulting _ belief network _ , .",
    "since the connections between the _ nodes _ of the resulting _ network _ have usually the meaning of probabilistic links ( but also deterministic relations can be included ) , this graph is called a _",
    "belief network_. moreover , since bayes theorem is used to update the probabilities of the possible _ states _ of the nodes ( the node ` box ' , with reference to our toy model , has states @xmath0 and @xmath2 ; the node ` ball ' has states @xmath1 and @xmath191 ) , they are also called _ bayesian networks_. for more info , as well as tutorials and demos of powerful packages having also a friendly graphical user interface , i recommend visiting hugin  @xcite and netica  @xcite web sites .",
    "( my preference for hugin is mainly due to the fact that it is multi - platform and runs nicely under linux . ) for a book introducing bayesian networks in forensics , ref .",
    "@xcite is recommended . for a monumental probabilistic network on the ` * case that will never end * ' , see ref .",
    "@xcite ( if you like classic thrillers , the recent paper of the same author might be of your interest @xcite ) . ] relative to five extractions and to the corresponding five reports is shown in figure [ fig : bn ] , redrawn in a different way in figure [ fig : bn_mon_0 ] .    in this diagram",
    "the _ nodes _ are represented by ` monitors ' that provide the probability of each _ state _ of the _ variable_. the green bars mean that we are in condition of uncertainty with respect to all states of all variable .",
    "let us describe the several nodes :    * initial box compositions have probability 50% each , that was our assumption . *",
    "the probability of white and black are the same for all extractions , with white a bit more probable than black ( 14/26 versus 12/26 , that is 53.85% versus 46.15% ) .",
    "* there is also higher probability that the ` witness ' reports white , rather than black , but the difference is attenuated by the ` lie factors . ' and one for @xmath192 . for simplicity",
    "we assume here they have the same value .",
    "] in fact , calling @xmath193 and @xmath194 the reported colors we have @xmath195      all probabilities of the network have been updated ( hugin  @xcite has nicely done the job for us ) .",
    "we recognize the 93% of box @xmath0 , that we already know .",
    "we also see that the increased belief on this box makes us more confident to observe white balls in the following extractions ( after re - introduction ) .",
    "the fact that the witness could lie reduces , with respect to the previous case , our confidence on @xmath0 and on white balls in future extractions . as an exercise on what we have learned in appendix h",
    ", we can evaluate the ` effective ' bayes factor @xmath196 that takes into account the testimony .",
    "applying eq .",
    "( [ eq : weight_et ] ) we get @xmath197 }                               { 1 +   \\lambda(i ) \\cdot \\left [                                  \\frac { \\tilde o_{h}(w , i)}{p(w\\,|\\,h , i ) } -1                                 \\right ] } \\\\ & = & 13\\times \\frac{5}{17 } = 3.82\\,,\\end{aligned}\\ ] ] or @xmath198 , about a factor of two smaller than @xmath199 , that was 1.1 ( this mean we need two pieces of evidence of this kind to recover the loss of information due to the testimony ) .",
    "the network gives us also the probability that the witness has really told us the truth , i.e. @xmath200 , that is _ different _ from @xmath201 , the reason being that white was initially a bit more probable than black .",
    "the most interesting thing that comes from the result of the network is how the probabilities that the two witness lie change .",
    "first we see that they are the same , about 95% , as expected for symmetry . but",
    "the surprise is that the probability the the first witness said the truth has increased , passing from 85% to 95% .",
    "we can justify the variation because , in qualitative agreement with intuition , if we have concordant witnesses , _ we tend to believe to each of them more than what we believed individually_. once again , the result is , perhaps after an initial surprise , in qualitative agreement with intuition .",
    "the important point is that intuition is unable to get quantitative estimates .",
    "again , the message is that , once we agree on the basic assumption and we check , whenever it is possible , that the results are reasonable , it is better to rely on automatic computation of beliefs .",
    "this last information reduces the probability of @xmath0 , but does not falsify this hypothesis , as if , instead , we had _ observed _ black .",
    "obviously , it does also reduce the probability of white balls in the following extractions .",
    "the other interesting feature concerns the probability that each witness has reported the truth .",
    "our belief that the previous two witnesses really saw what they said is reduced to 83% .",
    "but , nevertheless we are more confident on the first two witnesses than on the third one , that we trust only at 76% , although the lie factor is the same for all of them .",
    "the result is again in agreement with intuition : if many witnesses state something and fewer say the opposite , _ we tend to believe the majority _",
    ", if we initially consider all witnesses equally reliable .",
    "but a bayesian network tells us also how much we have to believe the many more then the fewer .",
    "let us do , also in this case the exercise of calculating the effective bayes factor , using however the first formula in footnote [ fb : peh=0 ] : the effective odds @xmath202 can be written as @xmath203 i.e. @xmath204 } = { 13}/{61 } = 0.213 $ ] , smaller then 1 because they provide an evidence against box @xmath0 ( @xmath205 ) .",
    "it is also easy to check that the resulting probability of 75.7% of @xmath0 can be obtained summing up the three weights of evidence , two in favor of @xmath0 and two against it : @xmath206 , i.e. @xmath207 , that gives a probability of @xmath0 of 3.1/(1 + 3.1)=76% .      only in this case",
    "we become certain that the box is of the kind @xmath2 , and the game is , to say , finished .",
    "but , nevertheless , we still remain in a state on uncertainty with respect to several things .",
    "the first one is the probability of a white ball in future extractions , that , from now becomes 1/13 , i.e. 7.7% , and does not change any longer .",
    "but we also remain uncertain on whether the witnesses told us the truth , because what they said is not incompatible with the box composition .",
    "but , and again in qualitative agreement with the intuition , we trust much more whom told black ( 1.6% he lied ) than the two who told white ( 70.6% they lied ) .    another interesting way of analyzing the final network is to consider the probability of a black ball in the five extractions considered .",
    "the fourth is one , because we have seen it .",
    "the fifth is 92.3% ( @xmath209 ) because we know the box composition . but in the first two extractions the probability is smaller than it ( 70.6% ) , while in the third is higher ( 98.4% ) .",
    "that is because in the two different cases we had an evidence respectively against and in favor of them ."
  ],
  "abstract_text": [
    "<S> triggered by a recent interesting new scientist article on the too frequent incorrect use of probabilistic evidence in courts , i introduce the basic concepts of probabilistic inference with a toy model , and discuss several important issues that need to be understood in order to extend the basic reasoning to real life cases . in particular , i emphasize the often neglected point that degrees of beliefs are updated not by ` bare facts ' alone , but by all available information pertaining to them , including how they have been acquired . in this light </S>",
    "<S> i show that , contrary to what claimed in that article , there was no `` probabilistic pitfall '' in the columbo s episode pointed as example of `` bad mathematics '' yielding `` rough justice '' . </S>",
    "<S> instead , such a criticism could have a ` negative reaction ' to the article itself and to the use of bayesian reasoning in courts , as well as in all other places in which probabilities need to be assessed and decisions need to be made . anyway , besides introductory / recreational aspects , the paper touches important questions , like : role and evaluation of priors ; subjective evaluation of bayes factors ; role and limits of intuition ; ` weights of evidence ' and ` intensities of beliefs ' ( following peirce ) and ` judgments leaning ' ( here introduced ) , including their uncertainties and combinations ; role of relative frequencies to assess and express beliefs ; pitfalls due to ` standard ' statistical education ; weight of evidences mediated by testimonies . a small introduction to bayesian networks , based on the same toy model ( complicated by the possibility of incorrect testimonies ) and implemented using hugin software , </S>",
    "<S> is also provided , to stress the importance of formal , computer aided probabilistic reasoning .    </S>",
    "<S> # 1 </S>"
  ]
}