{
  "article_text": [
    "the ever - increasing amount of information that is produced in the age of big data calls for efficient techniques for storage and processing of a large number of high - dimensional signals .",
    "compact representations can be obtained with different methods depending whether the particular task requires signal reconstruction ( e.g. , image and video compression for delivery and visualization ) or the goal is to infer some information from the signals ( e.g. , in classification , regression , information retrieval , etc . ) .",
    "embeddings provide compact representations of signals for the latter tasks .",
    "formally , an embedding is a transformation that maps a set of signals in a high dimensional space to a lower dimensional space , in such a way that the geometry of the set is approximately preserved .",
    "the concept of embedding has been successfully used in the context of information retrieval @xcite , where it is usually called `` hashing '' .",
    "an important class of signal embeddings are those preserving the distances among pairs of signals . johnson and lindenstrauss @xcite famously stated that an embedding can be realized with a lipschitz mapping to approximately preserve euclidean distances with a dimension of the embedding space that only depends on the desired distortion and logarithmically in the number of signals to be embedded .",
    "random projections have been shown to implement such embedding with high probability .",
    "several extensions have later been proposed , allowing one to approximately preserve the angle between signals @xcite , control the maximum distance that is embedded @xcite , or preserve the jaccard distance @xcite .",
    "recently , some works have studied learning embeddings @xcite from training data to derive compact codes by exploiting the particular geometry of the dataset ( e.g. , signals living close to a manifold ) .",
    "in this paper we propose an approach to construct an embedding that is not based on learning and does not require a training set of data , but rather is adapted to a single reference signal .",
    "this choice maintains to some degree the universality of random projections and it is useful when the data present no particular structure .",
    "jegou et al .",
    "@xcite empirically explored a similar idea by proposing to choose hash functions with a robustness criterion , that essentially measures how far a random projection falls from the edges of a quantization interval .",
    "our work presents a rigorous analytical treatment of a binary embedding obtained from the selection of the random projections of a reference signal with largest magnitude .",
    "the analysis of the embedding provides insights on its advantages , particularly in mitigating the difficulty of low - contrast nearest neighbor problems and superior performance on classification tasks , e.g. in a neural network .",
    "[ def : embedding ] a mapping @xmath0 of metric spaces , endowed with distances @xmath1 and @xmath2 is called an embedding with distortion @xmath3 if @xmath4 for some constant @xmath5 and for all @xmath6 .",
    "a well - known binary embedding is the sign random projections @xcite where a random matrix @xmath7 made of independent and identically distributed ( i.i.d . )",
    "gaussian entries is used to compute some random projections , which are then quantized to a binary representation by keeping their sign .",
    "the hamming distance between the binary vectors approximately preserves the angle between the signals in the original space @xcite , i.e. , @xmath8 , being @xmath9 and @xmath10 the @xmath11-th row of @xmath7 .",
    "a reference signal @xmath12 is used to generate the adaptive embedding in the following way . a number @xmath13 of random projections",
    "is computed by means of an i.i.d .",
    "gaussian matrix @xmath14 as @xmath15 .",
    "the @xmath16 entries with largest magnitude are identified and their locations stored in vector @xmath17 .",
    "the @xmath16-bit resulting binary code is : @xmath18 where @xmath19 is the matrix @xmath7 restricted to the rows indexed by @xmath17 .",
    "the locations vector @xmath17 is saved as side information of the embedding and used as in whenever a new signal is to be embedded , i.e. , @xmath20 , where @xmath21 is a generic signal and @xmath22 its embedding .",
    "the first theorem we prove confirms that the hamming distance @xmath23 , i.e. the number of differing entries , between binary codes obtained with the adaptive embedding concentrates around its expected value .",
    "[ thm : expected ] let @xmath24 be a set of @xmath25 signals and @xmath6 .",
    "let @xmath14 with @xmath26 , @xmath15 and the locations @xmath17 of the @xmath27 entries in @xmath28 with largest magnitude be known .",
    "let @xmath29 and @xmath30 .",
    "then for @xmath31 , @xmath32 with @xmath33 = \\frac{1}{m}\\sum_{i=1}^m p_i \\\\ p_i & = \\frac{1}{2 } + \\frac{1}{2}\\mathrm{erf}\\left ( -y_{l_i } \\frac{{{\\ensuremath{\\bm{\\mathrm{u}}}}}^t { { \\ensuremath{\\bm{\\mathrm{v}}}}}}{\\sqrt{2}\\sigma\\vert { { \\ensuremath{\\bm{\\mathrm{u}}}}}\\vert \\sqrt{\\vert { { \\ensuremath{\\bm{\\mathrm{u}}}}}\\vert^2 \\vert { { \\ensuremath{\\bm{\\mathrm{v}}}}}\\vert^2 - ( { { \\ensuremath{\\bm{\\mathrm{u}}}}}^t { { \\ensuremath{\\bm{\\mathrm{v}}}}})^2 } } \\right ) \\end{aligned}\\ ] ]    let us consider a single measurement in the @xmath34 location @xmath35 and @xmath36 .",
    "then @xmath37 $ ] is a bivariate gaussian with zero mean and covariance @xmath38 .",
    "suppose that @xmath39 is observed to be @xmath40 , then the conditional distribution of @xmath41 given @xmath40 is @xmath42 .",
    "after quantization of the measurements , the probability of mismatching bits in position @xmath34 is @xmath43    define the following random variable @xmath44    then , @xmath45 is a poisson binomial random variable measuring the hamming distance between @xmath46 and @xmath22 .",
    ". can be readily obtained using chernoff bounds @xcite for the tails of @xmath47 .",
    "the previous theorem holds for a fixed pair of signals .",
    "it is customary to derive an asymptotic result on the number of measurements needed to provide a distortion @xmath48 around the expectation when signals are drawn from a finite set of cardinality @xmath25 .",
    "standard derivation using a union bound on eq .",
    "yields @xmath49 , which is exactly the same as classic results on non - adaptive random projections @xcite .",
    "the advantages of the proposed method are , in fact , due to the modified expected value rather than the variance .    moreover , the previous theorem supposed we knew the values of the projections of the reference signal at the locations kept as side information .",
    "this allows us to compute the exact expected value of the hamming distance in the embedded space as function of the inner product ( or correlation coefficient ) between the original signals",
    ". however , it might be useful to have some a - priori knowledge about the embedding without the need to know the reference signal .",
    "the following theorem approximately bounds the expected value of the embedding by characterizing the order statistics of @xmath50 .",
    "the @xmath51-th order statistic of a statistical sample is equal to its @xmath51th - smallest value .",
    "let us call @xmath52 the probability density function of the @xmath51-th order statistic of @xmath50 .",
    "we could then in principle compute the probability of bit mismatch as @xmath53 and then repeat the same poisson binomial argument as before .",
    "however , this is cumbersome to compute and we instead derive some bounds .    under the same assumptions as theorem 1 , and being @xmath54 the expected value of the @xmath55-th order statistic of a sample of @xmath56 of size @xmath57 : @xmath58 \\leq \\frac{1}{2}+ \\frac{1}{2}\\mathrm{erf}\\left ( \\frac{-e_{2(m_\\text{pool}-m+1);2m_\\text{pool } }   { { \\ensuremath{\\bm{\\mathrm{u}}}}}^t { { \\ensuremath{\\bm{\\mathrm{v}}}}}}{\\sqrt{2}\\sigma\\vert { { \\ensuremath{\\bm{\\mathrm{u}}}}}\\vert \\sqrt{\\vert { { \\ensuremath{\\bm{\\mathrm{u}}}}}\\vert^2 \\vert { { \\ensuremath{\\bm{\\mathrm{v}}}}}\\vert^2 - ( { { \\ensuremath{\\bm{\\mathrm{u}}}}}^t { { \\ensuremath{\\bm{\\mathrm{v}}}}})^2 } } \\right)\\end{aligned}\\ ] ]    we first notice that @xmath59 , @xmath60 so that @xmath61 = \\frac{1}{m}\\sum_{i=1}^m p_i \\leq p_{m}$ ] . then , @xmath62 \\leq g ( { \\mathbb{e}}[\\tau ] ) $ ] by applying jensen s inequality to @xmath63 , i.e. , the same gaussian tail probability as before .",
    "also notice that the convexity of @xmath64 allows us to use jensen s inequality .",
    "@xmath65 = \\tilde{e}_{(m_\\text{pool}-m+1);m_\\text{pool}}$ ] is the expected value of the @xmath66-th order statistic of a sample of size @xmath13 from a half gaussian ( since we consider @xmath50 ) .",
    "we then notice that that is equivalent @xcite to @xmath54 , i.e. , the @xmath55-th order statistic of a sample of size @xmath57 of a full gaussian with zero mean and @xmath67 variance .    as a further remark ,",
    "according to @xcite an approximation of the expected value of the desired order statistic is : @xmath68 being @xmath69 and @xmath70 the inverse cdf of a normal distribution with zero mean and variance @xmath67 .",
    "so far we considered distances between a test signal and the reference used to adapt the embedding .",
    "we will now consider what happens to the distance between any arbitrary pair of signals @xmath21 and @xmath71 .",
    "qualitatively , we can say that the curve of the expected value of the hamming distance in the embedded space as function of the original distance between @xmath21 and @xmath71 will be somewhere between the one predicted by theorem [ thm : expected ] and the one of non - adaptive sign random projections depending on how much @xmath71 is close to the reference @xmath12 .",
    "the following theorem formalizes this concept .",
    "[ thm : expected_3party ] let @xmath24 be a set of @xmath25 signals and @xmath72 .",
    "let @xmath14 with @xmath26 , @xmath15 and the locations @xmath17 of the @xmath27 entries in @xmath28 with largest magnitude be known .",
    "let @xmath29 , @xmath30 , and @xmath73 .",
    "then for @xmath31 , @xmath74 with @xmath75 = \\frac{1}{m}\\sum_{i=1}^m p_i \\\\ p_i & = \\left [ f\\left ( \\begin{bmatrix } 0\\\\   + \\infty \\end{bmatrix } \\right ) - f\\left ( \\begin{bmatrix } 0\\\\   0 \\end{bmatrix } \\right ) \\right ] \\left [ 1 - f\\left ( \\begin{bmatrix } + \\infty\\\\   0 \\end{bmatrix } \\right ) \\right ] \\nonumber \\\\ & + \\left [ f\\left ( \\begin{bmatrix } + \\infty\\\\   0 \\end{bmatrix } \\right ) - f\\left ( \\begin{bmatrix } 0\\\\   0 \\end{bmatrix } \\right ) \\right ] f\\left ( \\begin{bmatrix } + \\infty\\\\   0 \\end{bmatrix } \\right),\\end{aligned}\\ ] ] being @xmath76 the cdf of a bivariate gaussian with mean @xmath77 and covariance @xmath78 .",
    "the proof is similar to the proof of theorem [ thm : expected ] .",
    "let us consider a single measurement in the @xmath34 location @xmath35 , @xmath36 , @xmath79 .",
    "then @xmath80 $ ] is gaussian with zero mean and covariance @xmath81 .",
    "suppose that @xmath39 is observed to be @xmath40 , then the conditional distribution of @xmath82 $ ] given @xmath40 is @xmath83 \\vert y_i=\\tau_i ) \\sim \\mathcal{n}\\left ( \\mathbf{\\mu } ' , \\mathbf{\\sigma } ' \\right)$ ] with @xmath77 and covariance @xmath78 .",
    "after quantization of the measurements , the probability of mismatching bits in position @xmath34 is @xmath84    define the random variable @xmath85 as in , then @xmath45 is a poisson binomial random variable measuring the hamming distance between @xmath22 and @xmath86 .",
    "eq . can be readily obtained using chernoff bounds @xcite for the tails of @xmath47 .    the key distinction between sign random projections @xcite and",
    "the method presented in this paper is that the former provides a linear relationship between the hamming distance in the embedded space and the angle in the original space . on the other hand",
    ", the binary adaptive embedding provides a nonlinear relationship between the two distances , with the important property that the hamming distances observed with the adaptive embedding are always smaller than those observed with sign random projections .",
    "this property is at the core of the improved performance of the embedding for tasks such as binary classification , as discussed in sec .",
    "[ sec : experimental ] .",
    "[ fig:2party ] shows the hamming distance in the embedded space as function of the inner product between the test signal and the reference signal in the original space .",
    "it can be noticed how the curve of the adaptive embedding always lies below the one for the non - adaptive embedding . for a fixed value of @xmath13 increasing the umber of measurements @xmath16 will reduce the variance and move the expected value towards that of the non - adaptive embedding .",
    "viceversa , for a fixed @xmath16 increasing @xmath13 will lower the curve .",
    "[ fig:3party ] shows the hamming distance between the first test signal and the second test signal in the embedded space as function of the inner product between the first test signal and the second test signal ( @xmath87 ) and between the second test signal and the reference signal ( @xmath88 ) in the original space .",
    "notice how for decreasing @xmath88 the shape of the embedding tends to the one of a non - adaptive embedding .",
    ", @xmath89 .",
    "non - adaptive curve uses sign random projections @xcite . ]",
    "[ fig:2party ]    , @xmath89 . ]    [ fig:3party ]",
    "a measure of difficulty @xcite of a nearest neighbor search problem is the contrast @xmath90 , which is defined as the ratio between the distance of the closest false neighbor and that of the farthest true neighbor .",
    "locality sensitive hashing @xcite is able to find approximate nearest neighbors in a time @xmath91 that is sublinear in the database size @xmath25 but that degenerates to linear search as the contrast approaches 1 .",
    "the curse of dimensionality makes low contrast more probable when high - dimensional spaces are considered .",
    "the adaptive embedding presented in this paper can be used as a dimensionality reduction technique to solve approximate nearest neighbor search more efficiently in low - contrast scenarios .    in order to show this we develop an experiment in a high - dimensional space where the goal is to find the nearest neighbors of a given signal within a certain radius .",
    "true neighbors are generated as standard gaussian vectors with @xmath92 i.i.d .",
    "entries with an expected correlation coefficient equal to 0.07 to a reference that is also used as query .",
    "disturbing signals are i.i.d .",
    "gaussian with zero expected correlation .",
    "notice that they are almost orthogonal to each other but the contrast is low because the true neighbours are weakly correlated with our query .",
    "this problem is not unrealistic and , in fact , as an example , it occurs in the detection of photo - response non - uniformity artifacts from camera sensors @xcite@xcite , used to attribute a given picture to a given camera sensor . in this experiment",
    "all the signals in the database are adaptively embedded , i.e. , the locations of the @xmath16 entries of largest magnitude are identified and stored with the binary code .",
    "the random projections of the query are computed and appropriately subsampled according to the locations stored for each database signal under test .",
    "the storage requirement for each database entry is the sum of the bits needed by the binary measurements and the overhead due to the adaptively chosen locations and it amounts to @xmath93 $ ] bits . fig .",
    "[ fig : roc ] shows the receiver operating characteristic ( roc ) showing the probability of detection of a true neighbor against the probability of false alarm .",
    "we notice that the adaptive embedding provides a performance closer to the uncompressed case .",
    "two non - adaptive strategies are presented for a fair comparison . a non - adaptive method using the same storage as the adaptive method would use @xmath94 $ ] binary random projections , with the drawback of increased computational complexity in the hamming distance evaluation .",
    "the second strategy equalizes computational complexity , thus using @xmath95 non - adaptive measurements .",
    "this is advantageous in terms of storage but it performs significantly worse .",
    "finally , we compared the proposed method with the universal embedding of boufounos et al .",
    "@xcite which is a kind of adaptive embedding where the quantizer can be parametrized in order to distort the expected value of signal distances , similarly to the embedding proposed in this paper . the universal embedding bounds the maximum distance that is embedded , beyond which points become indistinguishable .",
    "it is therefore expected to have poor performance in low - contrast , low - correlation scenarios , as it appears from fig .",
    "[ fig : roc ] . , where the quantization step is @xmath96 .    , @xmath97 , @xmath98",
    "the query is used as reference signal . ]",
    "[ fig : roc ]      in this section we apply the adaptive embedding to a multiclass linear classifier in order to improve its storage and computational efficiency .",
    "linear classifiers are widely used in the context of deep neural networks , where the layers of the network are trained to disentangle the features of each class and a simple linear classification layer provides the class labels .",
    "a @xmath51-class linear classifier can be written as @xmath99 , being @xmath100 the class label , @xmath101 the weights vectors and @xmath102 a feature vector .",
    "the weights are learned during the training phase using a suitable loss function such as the hinge loss @xcite for support vector machines or the softmax cross - entropy @xcite for multinomial logistic regression which is more popular in deep neural networks . since",
    "the feature vectors may be high dimensional and the number of classes large , this operation may require significant storage space for the real - valued weights as well as computational resources to compute all the inner products .",
    "this can be overcome using an embedding such as sign random projections .",
    "after the training phase is completed , the weights are embedded in a compact binary code for each class . during predictions the feature vectors",
    "are also embedded , the hamming distance with the weights is computed and the class label corresponding to the minimum distance is selected , i.e. @xmath103 , being @xmath104 and @xmath105 . replacing sign random projections with the proposed adaptive embedding can improve the classification performance of the compressed system .",
    "overall , there are as many adaptive embeddings as the number of classes .",
    "the random projections of each @xmath101 are used to compute a different set of locations @xmath106 for each class . at test time , the feature vector is embedded @xmath51 times to generate @xmath107 ( this amounts to generating @xmath13 non - adaptive projections and then subsampling according to the corresponding @xmath106 ) .",
    "hence , the class label is given by @xmath108 , being @xmath109 and @xmath110 .",
    "the following experiment is a classification problem on the cifar-10 dataset @xcite comprising 10 classes .",
    "we implemented the same convolutional neural network architecture presented in @xcite .",
    "this network is composed of 8 convolutional layers followed by 2 fully connected layers all with relu activation units @xcite and a final linear layer .",
    "the last linear layer outputs one of the @xmath111 class labels from a @xmath112-dimensional input feature vector . after conventional training of the network",
    ", we replaced the layer weights with its embedded codes as explained above . for the adaptive method we used @xmath113 .",
    "table [ table : cnn ] shows the classification accuracy as function of the number of measurements used by the embedding .",
    "it can be noticed that the adaptive embedding allows to achieve a significant dimensionality reduction at a negligible loss in terms of classification accuracy , with respect to both sign random projections @xcite and the universal embedding @xcite .",
    "the quantization step size of the universal embedding has been optimized via cross validation to value @xmath114 .",
    ".classification accuracy [ cols=\"^,^,^,^,^\",options=\"header \" , ]",
    "this paper presented a technique to generate compact binary codes from high - dimensional signals adapting them to a reference signal .",
    "the resulting embedding displays interesting properties that allow to improve performance in classification tasks when those are performed in the reduced - dimensionality domain . future work will focus on generalizing the approach to sub - gaussian and structured sensing matrices .          m.  s. charikar , `` similarity estimation techniques from rounding algorithms , '' in _ proceedings of the thiry - fourth annual acm symposium on theory of computing _ , ser .",
    "stoc 02.1em plus 0.5em minus 0.4emnew york , ny ,",
    "usa : acm , 2002 , pp . 380388 .",
    "[ online ] .",
    "available : http://doi.acm.org/10.1145/509907.509965    l.  jacques , j.  n. laska , p.  t. boufounos , and r.  g. baraniuk , `` robust 1-bit compressive sensing via binary stable embeddings of sparse vectors , '' _ ieee trans .",
    "inf . theory _ ,",
    "59 , no .  4 , pp . 20822102 , april 2013 .",
    "a.  broder , `` on the resemblance and containment of documents , '' in _ proceedings of the compression and complexity of sequences 1997_. 1em plus 0.5em minus 0.4emwashington , dc , usa : ieee computer society , 1997 , pp .",
    "[ online ] .",
    "available : http://dl.acm.org/citation.cfm?id=829502.830043        y.  weiss , a.  torralba , and r.  fergus , `` spectral hashing , '' in _ advances in neural information processing systems 21 _ , d.  koller , d.  schuurmans , y.  bengio , and l.  bottou , eds.1em plus 0.5em minus 0.4emcurran associates , inc .",
    ", 2009 , pp",
    ". 17531760 .",
    "[ online ] .",
    "available : http://papers.nips.cc/paper/3383-spectral-hashing.pdf    k.  he , f.  wen , and j.  sun , `` k - means hashing : an affinity - preserving quantization method for learning binary compact codes , '' in _ the ieee conference on computer vision and pattern recognition ( cvpr ) _ , june 2013 .",
    "h.  jegou , l.  amsaleg , c.  schmid , and p.  gros , `` query adaptative locality sensitive hashing , '' in _ 2008 ieee international conference on acoustics , speech and signal processing _ ,",
    "march 2008 , pp .",
    "825828 .",
    "k.  beyer , j.  goldstein , r.  ramakrishnan , and u.  shaft , _ _ when is `` nearest neighbor '' meaningful?__1em plus 0.5em minus 0.4emberlin , heidelberg : springer berlin heidelberg , 1999 , pp . 217235 . [ online ] .",
    "available : http://dx.doi.org/10.1007/3-540-49257-7_15      d.  valsesia , g.  coluccia , t.  bianchi , and e.  magli , `` compressed fingerprint matching and camera identification via random projections , '' _ ieee transactions on information forensics and security _",
    "10 , no .",
    "14721485 , july 2015 ."
  ],
  "abstract_text": [
    "<S> we use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal . </S>",
    "<S> the embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced - dimensionality space .    </S>",
    "<S> binary embeddings , random projections </S>"
  ]
}