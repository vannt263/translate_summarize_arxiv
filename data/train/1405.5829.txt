{
  "article_text": [
    "the problem of collective classification is a widely studied one in the context of graph mining and social networking applications . in this problem",
    ", we have a network containing nodes and edges , which can be represented as a graph .",
    "nodes in this network may be labeled , but it is not necessary that all nodes have a label . typically , such labels may represent some properties of interest in the underlying network .",
    "this is a setting that appears in several situations in practice .",
    "some examples of such labeled networks in real scenarios are listed below :    * in a bibliographic network , nodes correspond to authors , and the edges between them correspond to co - authorship links .",
    "the labels in the bibliographic network may correspond to subject areas that experts are interested in .",
    "it is desirable to use this information in order to classify other nodes in the network . * in a biological network , the nodes correspond to the proteins .",
    "the edges may represent the possibility that the proteins may interact .",
    "the labels may correspond to properties of proteins @xcite . * in a movie - actor network , the nodes correspond to the actors .",
    "the edges correspond to the co - actor relationship between the different actors .",
    "the labels correspond to the pre - dominant genre of the movie of the actor . * in a patent network , the nodes correspond to patent assignees .",
    "the edges model the citations between the respective patents .",
    "the labels correspond to the class categories .    in such networks ,",
    "only a small fraction of the nodes may be labeled , and these labels may be used in order to determine the labels of other nodes in the network .",
    "this problem is popularly referred to as _ collective classification _ or _",
    "label propagation _",
    "@xcite , and a wide variety of methods have been proposed for this problem .",
    "the problem of data uncertainty has been widely studied in the database literature @xcite , and also presents numerous challenges in the context of network data @xcite .",
    "in many real networks , the links are uncertain in nature , and are derived with the use of a probabilistic process . in such cases , a probability value may be associated with each edge .",
    "some examples are as follows :    * in biological networks , the links are derived from probabilistic processes . in such cases , the edges have uncertainty associated with them",
    ". nevertheless , such probabilistic networks are valuable , since the probability information on the links provides important information for the mining process . *",
    "the links in many military networks are constantly changing and may be uncertain in nature .",
    "in such cases , the analysis needs to be performed with imperfect knowledge about the network .",
    "* networks in which some links have large failure probabilities are uncertain in nature .",
    "* many human interaction networks can be created from real interaction processes , and such links are often uncertain in networks .    thus , such networks can be represented as probabilistic networks , in which we have probabilities associated with the existence of links .",
    "such probabilities can be very useful for improving the effectiveness of problems such as collective classification .",
    "furthermore , these networks may also have properties associated with nodes , that are denoted by labels .",
    "recent years have seen the emergence of numerous methods for uncertain graph management @xcite and mining    @xcite , in which uncertainty is used directly as a first - class citizen . however , none of these methods address the problem of collective graph classification .",
    "one possibility is to use _ sampling _ of possible worlds on the edges in order to generate different instantiations of the underlying network .",
    "the collective classification problem can be solved on these different instantiations , and voting can be used in order to report the final class label .",
    "the major disadvantage with this approach is that the sampling process could result in a sparse or disconnected network which is not suited to the collective classification problem . in such cases ,",
    "good class labels can not be easily produced with a modest number of samples .    in this paper , we investigate the problem of collective classification in uncertain networks with a more direct use of the uncertainty information in the network .",
    "we design two algorithms for collective classification .",
    "the first algorithm uses a probabilistic approach , which explicitly accounts for the uncertainty in the links in the classification .",
    "the second algorithm works with the assumption that most of the information in the network is encoded in high - probability links , and low - probability links sometimes even degrade the quality .",
    "therefore , the algorithm uses the links with high probability in earlier iterations , and successively relaxes the constraints on the quality of the underlying links .",
    "the idea is that a greater caution in early phases of the algorithm ensures convergence to a better optimum .",
    "the contributions we make in this paper can be summarized as follows .",
    "* we introduce the problem of collective classification in uncertain graphs , where uncertainty is associated with the edges of the graph , and provide a formal definition for this problem . *",
    "we introduce two algorithms based on iterative probabilistic labeling that incorporate the uncertainty of edges in their operation .",
    "these algorithms are based on a bayes formulation , which enables them to capture correlations across different classes , leading to improved accuracy . *",
    "we perform an extensive experimental evaluation , using two real datasets from diverse domains .",
    "we evaluate our techniques using a multitude of different conditions , and input data characteristics .",
    "the results demonstrate the effectiveness of the proposed techniques and serve as guidelines for the practitioners in the field .",
    "this paper is organized as follows . in section",
    "[ sec : relwork ] we survey prior studies on collective classification and on mining uncertain networks . in section",
    "[ sec : cc ] , we formally define the problem of collective classification in uncertain networks . in section  [ sec : proposal ] , we present our model and two algorithms for collective classification .",
    "we discuss the space and time complexity of our proposal in section  [ sec : complexity ] , and we present the results of our experimental evaluation in section  [ sec : results ] .",
    "finally , we discuss the conclusions in section  [ sec : conclusions ] .",
    "the problem of node classification has been studied in the graph mining literature , and especially relational data in the context of _ label or belief propagation _ @xcite .",
    "such propagation techniques are also used as a tool for semi - supervised learning with both labeled and unlabeled examples @xcite .",
    "collective classification @xcite refers to semi - supervised learning methods that exploit the network structure and node class labels to improve the classification accuracy .",
    "these techniques are mostly based on the assumption of homophily in social networks @xcite : neighboring nodes tend to belong to the same class .",
    "a technique has been proposed in @xcite , which uses link - based similarity for node - classification in directed graphs .",
    "recently , collective classification methods have also been used in the context of blogs @xcite . in @xcite ,",
    "bilgic et al .",
    "discuss the problem of overcoming the propagation of erroneous labels by asking the user for more labels . a method for performing collective classification    of email speech acts has been proposed by carvalho et al . in @xcite , exploiting the sequential correlation of emails . in @xcite ,",
    "integrate the classification of nodes in heterogeneous networks with ranking .",
    "methods for leveraging label consistency for collective classification have been proposed in @xcite .",
    "recently , the database and data mining community has investigated the problem of uncertain data mining widely @xcite .",
    "a comprehensive review of the proposed models and algorithms can be found in @xcite .",
    "several database systems supporting uncertain data have been proposed , such as conquer @xcite , trio @xcite , mistiq @xcite , maymbs @xcite and orion @xcite .",
    "the `` possible worlds '' model , introduced by abiteboul et al .",
    "@xcite , formalizes uncertainty by defining the space of the possible instantiations of the database .",
    "instantiations must be consistent with the semantics of the data .",
    "for example , in a graph database representing moving object trajectories there may be be different configurations of the edges where each node represents a region in the space .",
    "however , an edge can not connect a pair of nodes that represent a pair of non - neighboring regions .",
    "the main advantage of the `` possible worlds '' model is that the formulations of the queries originally designed to cope with certain data can be directly applied on each possible instantiation .",
    "many different alternatives have then been propose to aggregate the results across the different instantiations .    despite its attractiveness , the number of possible worlds explodes very quickly and even their enumeration becomes intractable problem . to overcome these issues , simplifying assumptions",
    "have been introduced to leverage its simplicity : the tuple- and the attribute - uncertainty models @xcite . in the attribute - uncertainty model , the uncertain tuple",
    "is represented by means of multiple samples drawn from its probability density function ( pdf ) .",
    "in contrast , in the tuple - uncertainty model the value of the tuple is fixed but the tuple itself may not exist .",
    "similar simplifications have been considered for graph databases where nodes may or may not exist ( node - uncertainty ) and edges are associated with an existence probability ( edge - uncertainty ) .",
    "the underlying uncertainty model can then be used to generate graph instances , eventually considering additional generation rules to consider correlations across different nodes and edges . in this study",
    "we combine a bayes approach and the edge - uncertainty model .",
    "the problem of uncertain graph mining has also been investigated extensively .",
    "the most common problems studied in uncertain graph management are those of nearest neighbor query processing @xcite , reachability computation @xcite and subgraph search @xcite . in the context of uncertain graph _ mining",
    "_ , the problems commonly studied are frequent subgraph mining @xcite , reliable subgraph mining @xcite , and clustering @xcite .",
    "recently , the problem of graph classification has also been studied for the uncertain scenario @xcite , though these methods are designed for classification of many small graphs , in which labels are attached to the entire graph rather than a node in the graph .",
    "typical social and web - based scenarios use a different model of collective classification , in which the labels are attached to nodes in a single large graph .    in this work ,",
    "we study the problem of _ collective classification _ in the context of uncertain networks , where the underlying links are uncertain .",
    "uncertainty impacts negatively on the classification accuracy .",
    "first , links may connect sub - networks of very different density , causing the propagation of erroneous labels .",
    "second , the farthest distance between two nodes tends to be smaller in very noisy networks , because of the presence of a larger number of uncertain edges , which include both true and spurious edges .",
    "this reduces the effectiveness of iterative models because of the faster propagation of errors .",
    "some of our techniques , which drop uncertain links at earlier stages of the algorithm , are designed to ameliorate these effects .",
    "in this section , we formalize the problem of collective classification after introducing some definitions .",
    "an uncertain network is composed of nodes whose connections may exist with some probability .",
    "[ def : unet ] an uncertain network is denoted by @xmath0 , with node set @xmath1 , edge set @xmath2 and probability set @xmath3 . each edge @xmath4 is associated with a probability value @xmath5 .",
    "this is the probability that edge @xmath6 exists in the network .",
    "we assume that the network is undirected , though the method can easily be extended to the directed scenario .",
    "we can assume that the @xmath7 matrix @xmath3 has entries which are denoted by @xmath8 and @xmath9 .",
    "a node @xmath10 can be associated with a label , representing its membership in a class .",
    "for ease in notation , we assume that node labels are integers .",
    "[ def : label ] given a set of labels @xmath11 drawn from a set of integers @xmath12 , we denote the label of node @xmath13 by @xmath14 .",
    "if a node @xmath13 is unlabeled , the special label @xmath15 is used .",
    "we can now introduce the definition of the collective classification problem on uncertain graphs .",
    "[ problem : cc ] given an uncertain network @xmath0 and the subset of labeled nodes @xmath16 , predict the labels of nodes in @xmath17 .",
    "figure  [ fig : net1 ] shows an example of an uncertain network .",
    "nodes @xmath18 , @xmath19 , and @xmath20 are labeled _ white _ , and nodes @xmath21 , @xmath22 , and @xmath23 are labeled _ black_. the label of nodes @xmath24 and @xmath25 is unknown .",
    "the aim of collective classification is to assign labels to nodes @xmath24 and @xmath25 .",
    "in this section , we first present the algorithm for iterative probabilistic labeling .",
    "a bayes approach is used in order to perform the iterative probabilistic labeling .",
    "this method models the probabilities of the nodes belonging to different classes on the basis of the adjacency behavior of the nodes .",
    "the bayes approach can directly incorporate the edge uncertainty probabilities into the estimation process .",
    "we continue with a second algorithm that builds upon the first one , and is based on iterative edge augmentation .",
    "finally , we describe a variation of the second algorithm that is a linear combination of two classifiers .",
    "the overall approach for the labeling process uses a bayesian model for the labeling . in the rest of the paper",
    ", we refer to this algorithm as _",
    "ubayes_. given that we have an unlabeled node @xmath26 , which is adjacent to @xmath27 other nodes denoted by @xmath28 , how do we determine the label of the node @xmath26 ?",
    "it should be noted that the concept of adjacency is also uncertain , because the edges are associated with probabilities of existence .",
    "this is particularly true , when the edge probabilities are relatively small , since the individual network instantiations are likely to be much sparser and different than the probabilistic descriptions .",
    "furthermore , for each edge @xmath6 we need to estimate the probability of the node @xmath29 having a particular label value , given the current value of the label at node @xmath13 .",
    "this is done with the use of training data containing the labels and edges in the network .",
    "these labels and edges can be used to construct a bayesian model of how the labels on the nodes and edges relate to one another .",
    "the algorithm uses an iterative approach , which successively labels more nodes in different iterations .",
    "this is the set @xmath30 of nodes whose labels will not be changed any further by the algorithm .",
    "initially , the algorithm starts off by setting @xmath30 to the initial set of ( already ) labeled nodes @xmath31 .",
    "the set in @xmath30 is expanded to @xmath32 in each iteration , where @xmath33 is the set of nodes not yet labeled that are adjacent to the labeled nodes in @xmath30 . if @xmath33 is empty , either all nodes have been labeled or there is a disconnected component of the network whose nodes are not in @xmath31 .",
    "the expanded set of labeled nodes are added to the set of training nodes in order to compute the propagation probabilities on other edges .",
    "thus , the overall algorithm iteratively performs the following steps :    * estimating the bayesian probabilities of propagation from the current set of edges . *",
    "computing the probabilities of the labels of the nodes in @xmath34 . *",
    "expanding the set of the nodes in @xmath30 , by adding the set of nodes from @xmath33 , whose labels have the highest probability for a particular class .    these steps are repeated until no more nodes reachable from the set @xmath30 remain to be labeled .",
    "we then label all the remaining nodes in a single step , and terminate .",
    "the overall procedure for performing the analysis is illustrated in algorithm  [ iterative ] .",
    "it now remains to discuss how the individual steps in algorithm  [ iterative ] are performed .",
    "+    * algorithm * _ ubayes_(graph : @xmath35 +   uncertainty prob .",
    ": @xmath3 , initial labeling : @xmath31 ) ; +   + * begin * +  = @xmath36 ; + ( not termination ) * do * +   +  = compute edge propagation probabilities ; + compute node label probabilities in @xmath34 ; + expand @xmath30 with @xmath33 nodes ; +   + * end *    the two most important steps are the computation of the edge - propagation probabilities and the expansion of the node labels with the use of the bayes approach . for a given edge @xmath6 we estimate @xmath37 .",
    "this is estimated from the data in _ each iteration _ by examining the labels of nodes which have already been decided .",
    "therefore , the training process is successively refined in each iteration .",
    "therefore , the value of @xmath38 can be estimated by examining those edges for which one end point contains a label of @xmath39 . among these edges , we compute the fraction for which the other end point contains a label of @xmath40 .",
    "for example , in the network shown in figure  [ fig : net1 ] the probability @xmath41 is estimated as @xmath42 .",
    "the label of node @xmath25 is unknown , and it is not considered in the calculation . note that this is simply equal to the probability that both end points of an edge are _ black _ , if one of them is _",
    "black_. therefore , one can compute the uncertainty weighted conditional probabilities for this in the training process of each iteration .",
    "this provides an estimate for the conditional probability .",
    "we note that in some cases , the number of nodes with a label of either @xmath40 or @xmath39 may be too small for a robust estimation .",
    "the following smoothing techniques are useful in reducing the effect of ill - conditioned probabilities :    * we always add a small value @xmath43 to each probability .",
    "this is similar to laplacian smoothing and prevents any probability value from being zero , which would cause problems in a multiplicative bayes model . * in some cases",
    ", the estimation may not be possible when labels do not exist for either nodes @xmath40 or @xmath39 . in those cases",
    ", we set the probabilities to their prior values .",
    "the prior is defined as the value of @xmath44 , and is equal to the fraction of currently labeled nodes with label of @xmath40 .",
    "the prior therefore defines the default behavior in cases where the adjacency information can not be reasonably used in order to obtain a better posteriori estimation .    for an unlabeled node @xmath26 , whose neighbors @xmath45 have labels @xmath28 , we estimate its ( unnormalized ) probability by using the naive bayes rule over all the adjacent labeled neighbors .",
    "this is therefore computed as follows : @xmath46    note that the above model incorporates the uncertainty probabilities directly within the product term of the equation .",
    "we can perform the estimation for each of the different classes separately .",
    "if desired , one can normalize the probability values to sum to one .",
    "however , such a normalization is not necessary in our case , since the only purpose of the computation is to determine the highest probability value in order to assign labels .",
    "the approach mentioned above is not very effective when a large fraction of the edges are noisy .",
    "in particular , if many edges have a low probability , this can have a significant impact on the classification process .",
    "figure  [ fig : net2 ] shows an example .",
    "nodes @xmath18 , @xmath19 , are labeled _ white _ , and nodes @xmath20 , @xmath24 , @xmath25 , @xmath22 , @xmath23 and @xmath47 are labeled _",
    "black_. the label of node @xmath21 is unknown and must be assigned by the algorithm .",
    "we observe that ignoring the edges whose existence probability is lower than @xmath48 is beneficial for the correct classification of node @xmath21 .",
    "therefore , we use an iterative augmentation process in order to reduce the impact of such edges , by instead favoring the positive impact of high quality edges in the collective classification process .",
    "the idea is to _ activate _ only a subset of the edges for use on the modeling process . in other words , edges which are not activated",
    "are not used in the modeling .",
    "we call this algorithm _ ubayes+_.    we adopt a model inspired by automatic parameter selection in machine learning . note that , analogous to parameter selection , the choice of a particular subset of high quality links , corresponds to a configuration of the network , and we would like to determine an optimal configuration for our approach . in order to do this , we split the set of labeled nodes @xmath31 into two subsets : a _ training set _ denoted by @xmath49 and a _ hold out _ set denoted by @xmath50 .",
    "the ratio of the @xmath31 nodes that are assigned to the training set @xmath49 is denoted by @xmath51 , a user - defined parameter .",
    "the purpose of the hold out set is to aid optimal configuration selection by checking the precise value of the parameters at which the training model provides optimal accuracy over the set of nodes in @xmath50 .",
    "we use labels of nodes in @xmath49 for the learning process , while using labels of nodes in @xmath50 as for the evaluation of accuracy at a particular configuration of the network .",
    "( note that a label is never used for both the training and the hold out set , in order to avoid overfitting . )",
    "the idea is to pick the ratio of active edges in such a way so as to optimize the accuracy on the hold out set .",
    "this ensures that an optimal fraction of the high quality edges are used for the labeling process .",
    "we start off considering a small fraction of the high probability edges , iteratively expanding the subset of active edges by enabling some of the inactive edges with the highest probabilities .",
    "the ratio of active edges is denoted by the parameter @xmath52 .",
    "ideally , we want to activate only the edges that contribute positively to the classification of unlabeled nodes . given a configuration of active edges , we measure their goodness as the estimated accuracy on labels of nodes in @xmath50 .",
    "the value of @xmath52 that leads to the highest accuracy , denoted by @xmath53 , is used as the ratio of edges with the highest probability to activate on the uncertain network @xmath35 .",
    "the resulting network is then used as input for the iterative probabilistic labeling algorithm ( _ ubayes _ ) .    despite optimizing accuracy by selecting the best ratio of edges to be considered ,",
    "the basic model described above is not very efficient , because it requires multiple evaluations of the iterative probabilistic labeling algorithm . in particular",
    ", it requires us to vary the parameter @xmath52 and evaluate accuracy , in order to determine @xmath53 .",
    "a more efficient technique for identifying @xmath53 can be obtained by evaluating the accuracy for different values of @xmath52 on a sample of the uncertain network @xmath35 ( rather than the full network ) as follows .",
    "we generate a new uncertain network @xmath54 by sampling @xmath55 nodes from @xmath35 uniformly at random , and retaining the edges from @xmath2 and probabilities from @xmath3 referring to these sampled nodes .",
    "@xmath56 is a user - defined parameter that controls the ratio of nodes sampled from @xmath35 and it implies the size of the sampled uncertain network @xmath57 .",
    "the initial set of labeled nodes in the sampled uncertain network @xmath57 is @xmath58 .",
    "we split the set of nodes in @xmath59 into two random subsets , @xmath60 and @xmath61 , respectively .",
    "the number of nodes in @xmath60 is @xmath62 .",
    "we start off considering @xmath63 edges with the highest probabilities , expanding iteratively the subset of active edges at each iteration by increasing @xmath52 .",
    "the goodness of parameter @xmath52 is estimated as the accuracy of node labels in @xmath61 .",
    "let @xmath53 be the value of @xmath52 leading to the highest accuracy .",
    "we activate @xmath64 edges with highest probability in @xmath35 .",
    "the resulting network is then used as input for the iterative probabilistic labeling ( algorithm  [ iterative ] ) .",
    "the overall algorithm is illustrated in algorithm  [ iterative2 ] .",
    "* algorithm * _ ubayes+_(graph : @xmath35 +  uncertainty prob . : @xmath3 , initial labeling : @xmath31 , +  sampled nodes ratio : @xmath56 , train nodes ratio : @xmath51 ) ; +   + * begin * +  = @xmath65 = random sample of @xmath55 nodes from @xmath1 ; +  = @xmath66 = edges @xmath67 in @xmath2 with @xmath68 ; +  = @xmath69 ; +  = @xmath70 edges in @xmath66 with +  greatest existence probability ; + ( @xmath71 ) * do * +   +  = construct graph @xmath72 ; +  = @xmath73 ; +  = test accuracy using nodes in @xmath50 ; +  = expand edges in @xmath74 with top edges in @xmath66 ; +   +  = construct graph @xmath75 with best +  =   configuration ( corresponding to @xmath53 ) ; +  = @xmath76 ; + * end *    we note that the frequencies used to estimate conditional and prior probabilities across the different configurations in algorithm  [ iterative2 ] can be efficiently maintained in an incremental fashion .      in this section",
    "we propose a third algorithm , _",
    "ubayes+rn_. it uses an ensemble methodology in order to further improve robustness in scenarios , where some deterministic classifiers can provide good results over _ some _ subsets of nodes , but not over all the nodes .",
    "_ ubayes+rn _ is the linear combination of two classifiers : the _ ubayes+ _ algorithm and the relational neighbor ( rn ) classifier @xcite .",
    "the rn classifier is defined as follows :    @xmath77    where @xmath78 is the probability and @xmath79 .",
    "ubayes+ _ and _ rn _ algorithms are combined as follows :    @xmath80    where @xmath43 controls the influence of the rn classifier during the collective classification process .",
    "when @xmath81 then _ ubayes+rn _ degenerates to _ ubayes+ _ , while when @xmath82 the two classifiers are weighted equally .",
    "note that this is a simple linear combination .",
    "we used this combination , since it sometimes provides greater robustness in the classification process .",
    "in this section , we discuss the complexity of the proposed algorithms .    we start with _ ubayes _ , which for the computation of the initial statistics requires @xmath83 ( label priors and conditional label probabilities ) . assuming that the cardinality of the set of immediate unlabeled neighbors of nodes in @xmath30 ( remember that @xmath30 represents the set of currently labeled nodes ) is at most @xmath84 , and that the number of neighbors for a particular node is at most @xmath85 , each iteration can be decomposed as follows .",
    "the computation of new unlabeled nodes requires @xmath86 .",
    "the computation of edge propagation probabilities requires @xmath86 .",
    "the computation of node label probabilities requires @xmath86 . summing up",
    ", each iteration requires @xmath86 . assuming that all unlabeled nodes will be labeled in @xmath87 iterations , the algorithm cost is @xmath88 , where @xmath89 .",
    "space complexity is @xmath90 .    for algorithm _",
    "ubayes+ _ , the computation of the uncertain network sample @xmath91 requires @xmath92 .",
    "active edges are maintained using a priority list , whose initialization requires @xmath93 .",
    "each iteration of the iterative automatic parameter selection procedure can be decomposed as follows .",
    "algorithm  [ iterative ] ( used by _",
    "ubayes+ _ ) requires @xmath94 .",
    "testing the classification accuracy requires @xmath95 .",
    "expanding the set of active edges requires @xmath96 . summing up",
    ", each iteration requires :    @xmath97    finally , the last call to algorithm  [ iterative ] requires @xmath88 .",
    "assuming that the parameter selection procedure terminates after @xmath98 iterations , the algorithm cost is @xmath99 .",
    "simplifying , the cost is @xmath100 .",
    "the space complexity is @xmath90 .",
    "note that algorithms _",
    "ubayes+rn _ and _ ubayes+ _ have the same space and time complexity .",
    "in this section , we evaluate the proposed techniques under different settings , in terms of both accuracy and performance .",
    "we implemented all techniques in c++ using the standard template library ( stl ) and boost libraries , and ran the experiments on a linux machine equipped with an intel xeon 2.40ghz processor and 16 gb of ram .",
    "the reported times do not include the initial loading time , which was constant over all methods .",
    "the results were obtained from @xmath21 independent runs .",
    "for all experiments we report the averages and @xmath101 confidence intervals .      in our experiments",
    ", we used two data sets for which edge probabilities can be estimated , as described below .",
    "_ dblp : _ the _ dblp data set _ @xcite is the most comprehensive citation network of curated records of scientific publications in computer science . in our experiments",
    ", we consider the subset of publications from @xmath102 to @xmath103 .",
    "the data set consists of @xmath104 nodes and @xmath105 edges .",
    "nodes represent authors and edges represent co - authorship relations .",
    "the edge probability is an estimate of the probability that two authors co - authored a paper in a year selected randomly during their period of activity .",
    "for example , if a pair of authors published papers in ten different years and they both published papers for twenty years , then their edge probability is @xmath48 .",
    "( we consider the union of their periods of activity . )",
    "we used 14 class labels , that represent different research fields in computer science .",
    "the corresponding labels and their frequencies are illustrated in table  [ table : labels_dblp ] .",
    "the labels were generated by using a set of top conferences and journals in these areas , and the most frequent label in the author s publications is used as the author s label . in our data set , @xmath106 of the nodes are labeled .",
    "the rest were not labeled , because the corresponding authors did not have publications in the relevant conferences and journals .",
    "+    .node labels and label priors of the _ dblp _ dataset . [ cols=\"^,^,^\",options=\"header \" , ]     finally , we report the accuracy of the _ ubayes+rn _ algorithm when varying the parameter @xmath43 between @xmath15 and @xmath18 . recall that @xmath43 controls the influence of the _ rn _ classifier on the overall classification process . in our experiments with both data sets , the accuracy of _",
    "ubayes+rn _ was always slightly better than _ ubayes+ _ , but never more than @xmath107 .",
    "we observed nearly no difference among the different @xmath43 configurations . in the interest of space , we omit the detailed results .",
    "we next provide some real examples of labeling results obtained with _",
    "ubayes+ _ on the patent dataset .",
    "the  _ atari inc . _ \" and  _ sega enterprises , ltd _ \" companies , which belong to the hall of fame of the video game industry , were not assigned to any category .",
    "our algorithm correctly classified them as  computers & communications \" .",
    "similarly , the companies  _ north american biologicals , inc _ \" and  _ bio - chem valve , inc _ \" were correctly labeled as  drugs & medical \" , since they are both involved in drug development and pharmaceutical research .",
    "interestingly ,  _ starbucks corporation _ \" was labeled as  chemical \" . taking",
    "a close look at their patents , it turns out that a large fraction of them describe techniques for enhancing flavors and aromas that involve chemical procedures .",
    "evidently , having labels for all the nodes in the graph allows for improved query answering and data analysis in general .      in this section ,",
    "we assess running time efficiency on a variety of settings using both real and perturbed data sets .",
    "figures  [ fig : time_gamma](a ) and [ fig : time_gamma](b ) show the cpu time required by the algorithms when varying the ratio of noisy edges , for the _ dblp _ and _ patent _ data sets , respectively .",
    "note that _ sampling _ has the same time performance as _",
    "ubayes_. the _",
    "ubayes+ _ algorithm is nearly three times slower than _",
    "ubayes_. this is due to the automatic parameter tuning approach employed by the _",
    "ubayes+ _ algorithm we observe that the performance of _ wvrn-20 _ almost always _ considerably _ worse than both _",
    "ubayes _ and _ ubayes+_. the same observation is true when we vary the standard deviation of the probability of the noisy edges ( see figures  [ fig : time_theta](a ) and [ fig : time_theta](b ) ) .",
    "note that the inference in the _ wvrn _ algorithm is based on labeling relaxation , whose complexity is proportional to the size of the network and remains constant across iterations . on the contrary ,",
    "the iterative labeling that _ ubayes _ and _ ubayes+ _ use for their inference model becomes faster with each successive iteration , since it needs to visit a smaller part of the network . as the results show",
    ", the standard deviation does not affect the time performance of the algorithms .",
    "these experiments demonstrate that the two proposed algorithms effectively combine low running times with high accuracy and robustness levels .    in the final set of experiments",
    ", we evaluated the accuracy of all algorithms as a function of the time required for algorithmic execution by the baselines . since the baselines tradeoff between running time and accuracy , it is natural to include the running time in the comparison process . in this case , we removed the constraint that _ wvrn _ and _ sampling _ end their processing after a fixed amount of time or a specific number of iterations , and examined how their accuracy changes when the number of iterations ( and consequently , processing time ) increases . for reference , we also include the _ ubayes _ and _ ubayes+ _ algorithms , which execute in a fixed amount of time .",
    "the results for the _ dblp _ and _ patent _ data sets are depicted in figures  [ fig : dblp_timeacc ] and [ fig : patents_timeacc ] respectively .",
    "the graphs show that the accuracy of _ wvrn _ and _ sampling _ is slightly increasing with time , but reaches an almost stable state after the first @xmath108 iterations .",
    "( in our experiments , we stopped _ wvrn _",
    "after @xmath109 iterations in the _ dblp _ data set and @xmath110 iterations in the _ patent _ data set , and the _ sampling _ algorithm after @xmath111 iterations in the _ dblp _ data set and @xmath109 iterations in the _ patent _ dataset ) .",
    "nevertheless , the",
    "_ and _ ubayes+ _ algorithms achieve significantly better results in a much lower running time .",
    "uncertain graphs are becoming increasingly popular in a wide variety of data domains .",
    "this is due to the statistical methods used to infer many networks , such as protein interaction networks and other link - prediction based methods .",
    "consequently , the problem of collective classification has become particularly relevant for determining node properties in such networks .    in this paper",
    ", we formulate the collective classification problem for uncertain graphs , and describe effective and efficient solutions for this problem . to this effect",
    ", we describe an iterative probabilistic labeling method , based on the bayes model , that treats uncertainty on the edges of the graph as first class citizens . in the proposed approach ,",
    "the uncertainty probabilities of the links are used directly in the labeling process .",
    "furthermore , the methodology we describe allows for automatic parameter selection .",
    "we have performed an experimental evaluation of the proposed approach using diverse , real - world datasets .",
    "the results show significant advantages of using such an approach for the classification process over more conventional methods , which do not directly use uncertainty probabilities .    *",
    "acknowledgments * + part of this work was supported by the fp7 eu ip project kap ( grant agreement no .",
    "work of the second author was sponsored by the army research laboratory under cooperative agreement number w911nf-09 - 2 - 0053 .",
    "1000010000  = 1000                      a.  fuxman , e.  fazli , and r.  j. miller .",
    "conquer : efficient management of inconsistent databases . in _ proceedings of the 2005 acm",
    "sigmod international conference on management of data _ , pages 155166 .",
    "acm , 2005 .",
    "p.  agrawal , o.  benjelloun , a.  d. sarma , c.  hayworth , s.  u. nabar , t.  sugihara , and j.  widom .",
    "trio : a system for data , uncertainty , and lineage . in",
    "_ international conference on very large data bases ( vldb ) _ , pages 11511154 , 2006 .",
    "s.  singh , c.  mayfield , s.  mittal , s.  prabhakar , s.  e. hambrusch , and r.  shah .",
    "orion 2.0 : native support for uncertain data . in _",
    "acm sigmod international conference on management of data _ , pages 12391242 , 2008 .",
    "x.  zhu , z.  ghahramani , j. lafferty , semi - supervised learning using gaussian fields and harmonic functions , , 2003 . z. zou , h. gao , j. li .",
    "discovering frequent subgraphs over uncertain graph databases under probabilistic semantics . _",
    "acm sigkdd conference on knowledge discovery and data mining ( kdd ) _ , 2010 .",
    "j.  ren , s.  d. lee , x.  chen , b.  kao , r.  cheng , and d.  cheung .",
    "naive bayes classification of uncertain data . in _ data mining , 2009 .",
    "ninth ieee international conference on _ , pages 944949 .",
    "ieee , 2009 ."
  ],
  "abstract_text": [
    "<S> in many real applications that use and analyze networked data , the links in the network graph may be erroneous , or derived from probabilistic techniques . in such cases , the node classification problem can be challenging , since the unreliability of the links may affect the final results of the classification process . </S>",
    "<S> if the information about link reliability is not used explicitly , the classification accuracy in the underlying network may be affected adversely . in this paper , we focus on situations that require the analysis of the uncertainty that is present in the graph structure . </S>",
    "<S> we study the novel problem of node classification in uncertain graphs , by treating uncertainty as a first - class citizen . </S>",
    "<S> we propose two techniques based on a bayes model and automatic parameter selection , and show that the incorporation of uncertainty in the classification process as a first - class citizen is beneficial . </S>",
    "<S> we experimentally evaluate the proposed approach using different real data sets , and study the behavior of the algorithms under different conditions . </S>",
    "<S> the results demonstrate the effectiveness and efficiency of our approach .    </S>",
    "<S> = 1 </S>"
  ]
}