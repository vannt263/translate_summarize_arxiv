{
  "article_text": [
    "in this paper , we propose a learning theory which is the * synthesis of kernel and symbolic algebraic methods * , by exposing inherent dualities between them .",
    "we use this duality to combine the structure - awareness of algebraic methods with the efficiency and generality of kernels . since their invention by boser , guyon and vapnik  @xcite ,",
    "* kernel methods * have had a fundamental impact on the fields of statistics and machine learning .",
    "the major appeal of using kernel methods for learning consists in using the kernel trick , first proposed by aizerman , braverman and rozonoer  @xcite , which allows to make otherwise costly computations in the feature space implicit and thus highly efficient for a huge variety of learning tasks - see e.g.  @xcite . however , the major advantage of kernel methods is also their major drawback : since kernels implicitize feature space computations , the learnt model is implicit as well ; in most scenarios , though kernels perform excellently , it is indeed a principal open question what it is that kernels learn - _ a question to which we can provide an answer through duality with ideals_. on the other hand , * symbolic - algebraic methods * are inherently structural , as they yield explicit and compact representations of the data , as so - called ideals , with the major advantage of being directly interpretable .",
    "the seminal buchberger - mller algorithm  @xcite allows to transform one representation into a different , easier and sparser one .",
    "one major issue of the buchberger - mller algorithm is that it is numerically unstable and therefore not applicable in noisy scenarios - this has been addressed by a class of numerical algorithms surrounding the approximate vanishing ideal ( avi ) method  @xcite . while these algorithms offer attractive and explicit representations , the major issue with symbolic methods preventing broad applicability is their exponential ( or higher ) complexity , and model selection issues - _ which we can considerably reduce through dual kernelization_. in the intersection of symbolic algebra and kernel method , we propose general tools and two algorithms , ipca and avica , which simultaneously can learn generative information from the data manifold and discriminative features ; more generally , we argue that the kernel - ideal duality translates generative and discriminative tasks in the kernel world directly to discriminative and generative tasks in the algebra world , which allows to combine the advantages of either while avoiding the disadvantages of both .",
    "we therefore expect our findings to have a considerable impact on the fields of learning , statistics , algebra , and the interaction between those .",
    "we introduce the main objects we are relating through duality .",
    "we start by defining polynomial kernels , the main kernel - type objects involved in the duality presented here .",
    "later we will explain how to treat general kernels . in this paper",
    ", @xmath0 will be one of the fields @xmath1 or @xmath2 .",
    "let @xmath3 be a fixed real number . slightly different from the usual definition , we denote + by @xmath4 the homogenous polynomial kernel function + @xmath5 , and + by @xmath6 the inhomogenous polynomial kernel function + @xmath7 .    the usual convention for the kernel function is obtained after dividing by @xmath8 .",
    "since @xmath9 is chosen arbitrarily in @xmath10 , no qualitative change is introduced by our convention .",
    "it is however , as we will show , the more natural one . on the algebra side ,",
    "the main objects linked via the duality are vector spaces of polynomials .",
    "we denote by @xmath11 a vector of coordinate variables , by @xmath12_{d}$ ] the @xmath0-vector space of homogeneous polynomials of degree  @xmath13 in  @xmath14 , by @xmath12_{\\le d}$ ] the @xmath0-vector space of all ( homogeneous or inhomogeneous ) polynomials of degree at most  @xmath13 in  @xmath14 , by @xmath12 = { \\mathbb{k}}[{\\mathbf{x}}]_{0}\\oplus { \\mathbb{k}}[{\\mathbf{x}}]_{1}\\oplus \\cdots$ ] the ring of all polynomials in  @xmath14 , and by @xmath15 , for @xmath16 , the monomial @xmath17_d$ ] where @xmath18 .",
    "the dimension of @xmath12_{\\le d}$ ] is @xmath19 , and the dimension of @xmath12_{d}$ ] is @xmath20 .",
    "the dimension of @xmath12 $ ] is infinite .",
    "the ring @xmath12 $ ] is dual to the vector space of kernel decision functions in the following way .",
    "[ thm : dual ] let @xmath21 , let @xmath22_*$ ] , where @xmath23 can denote @xmath13 or @xmath24 , and let @xmath25 be generic .",
    "then we have @xmath12 _ * = { \\operatorname{span}}\\{k_*(y_i,{\\mathbf{x } } ) , 1\\le i\\le m\\}.$ ]    linear independence of up to @xmath26_*$ ] of the @xmath27 follows from genericity of @xmath28 . since @xmath29_*$ ] , this yields the claim .",
    "let @xmath21 , and let  @xmath30 denote  @xmath13 or @xmath24 .",
    "theorem  [ thm : dual ] implies that @xmath12_*$ ] is dual to the feature space of the kernel  @xmath31 . by passing to the limit",
    ", this implies that @xmath12 $ ] contains the dual of any feature space .",
    "explicitly , this is seen as follows : consider the usual feature map @xmath32 , where @xmath33 is the feature space .",
    "elementary computations , such as in section  2.1 or problems  2.6.2 - 3 of  @xcite , show that the feature map can be explicitly identified as @xmath34 with @xmath35 . by counting the number of distinct @xmath36",
    ", we see that @xmath37_*$ ] .",
    "this can be made more explicit by interpreting polynomials in @xmath12_*$ ] as elements of the dual of  @xmath33 , i.e. , as functions @xmath38 .",
    "namely , for a polynomial @xmath39_*$ ] with @xmath40 , write @xmath41 .",
    "one checks that @xmath42 for all @xmath43 .",
    "thus we can identify  @xmath33 with the dual polynomial ring @xmath12_*^\\vee$ ] .",
    "since the latter is finite dimensional , it is self dual , so there is a canonical identification @xmath44_*$ ] .",
    "we use this identification to transfer the canonical scalar product on  @xmath33 to its dual  @xmath12_*$ ] in the natural way .",
    "namely , in order to be compatible with  @xmath45 , it needs to be the unique scalar product on  @xmath46 $ ] such that , for @xmath47 , the reproducing property @xmath48 holds . to achieve this , the factor  @xmath49 occurring in  @xmath50 above",
    "must be corrected for .",
    "consequently , an explicit description of the scalar product on  @xmath12_*$ ] is given as follows : fix @xmath21 , and let @xmath51 be exponent vectors . define @xmath52 if @xmath53 , and @xmath54 and then extend this bi- or sesquilinearly to all of  @xmath12 $ ] .",
    "+ theorem  [ thm : dual ] then implies that , for @xmath55 _ * , p\\in { \\mathbb{k}}^n$ ] , the following equalities hold : @xmath56 by applying theorem  [ thm : dual ] and properties of the symmetric outer product , we get an important consequence of equation  [ eq : eval - prod ] , namely that the scalar product is multiplicatively absorbing for orthogonality .",
    "that is , letting @xmath57 $ ] such that @xmath58 for all @xmath59 , we have @xmath60 note that in the usual convention for the polynomial kernel , similar equalities are still valid , but much less concise to express .",
    "the duality above is an algebraic analogue of the theory of reproducing kernel hilbert spaces .",
    "the associated hilbert space is the space of polynomial functions @xmath61 , which can be identified with @xmath12_*$ ] by an additional dualization ; phrased in algebraic terms , by replacing the evaluation homomorphism @xmath62 with the corresponding symbolic polynomial .",
    "note that the equations  [ eq : eval - ip ] and  [ eq : eval - prod ] could also be obtained combining riesz representation , or the reproducing property of @xmath31 , with this identification , compare e.g.  section 2.2.3 in  @xcite.equality  [ eq : orthdeg ] , on the other hand , is purely obtained from algebra .",
    "the next section will also go beyond what could be reached from usual rkhs duality alone .",
    "next we show that ideals - a classical concept in algebra - are the proper objects to dualize manifolds , in the same way as the polynomial ring dualizes feature space .",
    "[ def : ideal ] an _ ideal _ is a linear subspace @xmath63 $ ] which also absorbs multiplication , that is , which satisfies @xmath64 for all @xmath39 , g\\in { \\mathcal{i}}$ ] . for @xmath21 ,",
    "we let @xmath65_{\\le d}$ ] .",
    "while an ideal is in general infinite dimensional as a @xmath0-vector space , hilbert s basis theorem says that all ideals admit a _",
    "finite _ set of additive - multiplicative generators . as for vector spaces , such sets of generators need not be unique",
    ". an important class of ideals is given as follows .",
    "[ ex : vanishing - ideal ] let @xmath66 .",
    "then the @xmath0-vector space of polynomials @xmath67 : \\text{$f(s ) = 0 $ for all $ s\\in { \\mathcal{s}}$ } \\}$ ] is an ideal . it is called the _ vanishing ideal of @xmath68_.    while @xmath68 can be in principle any subset of  @xmath69 , we will be mainly concerned with the case where  @xmath68 is a manifold . in this case , the vanishing ideal @xmath70 is the dual of the manifold @xmath68 in the following precise sense which is an analogue to theorem  [ thm : dual ] .",
    "[ thm : dual - manifold ] let @xmath71 be a manifold .",
    "it holds that @xmath72 _ * = { \\operatorname{span}}\\{k_*(s,{\\mathbf{x}}),\\ ; s\\in { \\mathcal{s}}\\}^{\\perp}. $ ]    this follows from the definition of  @xmath70 and equation  [ eq : eval - ip ]",
    ".    theorem  [ thm : dual - manifold ] relates manifolds to ideals via kernel duality .",
    "intuitively , it says that the manifold  @xmath68 corresponds to a linear subspace of feature space .",
    "this is a kernelized version of the usual algebra - geometry duality and and relates the discriminative description through decision functions @xmath73 to the generative description by  @xmath68 .      in this section",
    "we reveal a further duality between certain kernel matrices and ideals .",
    "first we introduce the following notation for kernel matrices .",
    "let @xmath74 , let @xmath75 , and let @xmath76 be the rows of  @xmath77 and  @xmath78 , resp .",
    "for a kernel @xmath79 , we denote by  @xmath80 the @xmath81-matrix which has the number @xmath82 as its entry in position @xmath83 .",
    "the concept of an interpolation space is motivated by the duality in theorem  [ thm : dual - manifold ] and is the orthogonal to an ideal .",
    "let @xmath66 , and let @xmath84 .",
    "the _ interpolation space _ of  @xmath68 is the vector space @xmath85:={\\mathcal{i}}^\\perp$ ] , where the orthogonal is taken w.r.t .",
    "the scalar product defined above . given @xmath21",
    ", we also write @xmath86_{\\le d}$ ] for @xmath87 .",
    "intuitively , the interpolation space gives a canonical basis for the space of functions on  @xmath68 , orthogonal to those vanishing on  @xmath68 .",
    "it is a fixed choice of representatives for the factor ring @xmath12/\\operatorname*{i}({\\mathcal{s}})$ ] usually defined in algebra .",
    "the results of the previous sections yield the following duality statements between the interpolation space and polynomial kernel matrices .",
    "[ thm : dual - interp ] let @xmath66 , let @xmath88 be generic , and let @xmath89 be the matrix which has the  @xmath90 as columns .",
    "let @xmath91 be generic .",
    "denote @xmath92 .",
    "then the following claims hold :    ( i ) : :    @xmath93_{\\le d } , n , d\\right)$ ] .",
    "( ii ) : :    it holds that    @xmath94_{\\le d}$ ] .",
    "equality holds if and only if    @xmath95_{\\le d}$ ] .",
    "( iii ) : :    for @xmath96 , it holds that    @xmath97    only if @xmath98 .",
    "the converse is true if and    only if    @xmath99_{\\le d}$ ] .",
    "( notice the difference between the @xmath100-tuple of variables @xmath14 and the data matrix  @xmath77 . )    for @xmath21 , we write @xmath101_{\\le d}$ ] . claim ( i ) : first assume that @xmath102 .",
    "theorem  [ thm : dual - manifold ] implies that @xmath103_{\\le d})^\\perp = { \\mathbb{k}}[{\\mathcal{s}}]_{\\le d}$ ] .",
    "in particular , this shows @xmath104 .",
    "therefore @xmath105 generic elements of the form @xmath106 will generate @xmath85_{\\le d}$ ] , and their span will have dimension @xmath107 . by interpreting the variables in  @xmath14 once more as functions @xmath108 , we view @xmath106 as a function @xmath109 .",
    "then the polynomials @xmath106 span a vector space of dimension  @xmath107 if and only if the functions @xmath110 do that as well . by substituting @xmath111 generic arguments",
    "@xmath112 , we see that the vectors @xmath113 span a vector space of dimension  @xmath107 .",
    "this is equivalent to @xmath114 , proving the statement in case @xmath102 .",
    "the general statement follows by starting with @xmath115 and removing rows and columns .",
    "claims ( ii ) and ( iii ) follow from the fact that , if @xmath111 , the vector of variables  @xmath14 in the conditions can be equivalently replaced by  @xmath77 .",
    "the case @xmath116 follows again by removing rows and columns .",
    "theorem  [ thm : dual - interp ] states that a large enough kernel matrix of type @xmath117 contains all information on  @xmath68 , assuming the kernel degree is high enough as well .",
    "algorithmically , it yields the important statement that , instead of size @xmath118_{\\le d})=o(n^d)$ ] matrices which grow exponentially in @xmath13 , we have to deal with size @xmath119_{\\le d})= o(n)$ ] matrices instead .",
    "these are matrices whose size is bounded from above by the number of data points .",
    "in fact , the effective size is usually even lower , depending on model complexity .",
    "we also note that claims  ( ii ) and  ( iii ) of theorem  [ thm : dual - interp ] make statements about functions of the form @xmath120 , which is a familiar form of decision functions . since the functions in claim  ( ii ) are part of the interpolation space of  @xmath68 , we call them `` discriminiative features '' .",
    "the functions in claim  ( iii ) vanish on  @xmath68 .",
    "therefore they carry structural information about  @xmath68 , so we will call them `` generative features '' . a basis of  @xmath70 consisting of such generative features will be called a _ kernel border basis_. these features will be identified by both algorithms we introduce in the subsequent section .",
    "similar to the ubiquity of kernels , ideals enable us to address a wide variety of learning scenarios .",
    "the general motive in ideal - manifold duality is that generative features are transformed into discriminative ones and vice versa .",
    "for instance , a strategy for estimating a discriminative function in kernel learning , e.g. , by kernel svm , will be close to estimating a descriptive feature in learning a manifold via obtaining ideal generators .",
    "conversely , estimating descriptive features such as in kernel pca will relate to obtaining discriminative features from the interpolation space .",
    "this permits us to `` dualize '' techniques and to transfer the absorption property of ideals back into the kernel world , thus yielding new model selection tools and compact representations in terms of kernel degree .",
    "the following learning approach to ideals is inspired by vapnik s statistical learning theory .",
    "we assume that there is a _ generative truth _ , modelled by an unknown algebraic manifold @xmath71 with vanishing ideal @xmath84 .",
    "the sampling process produces a discrete point set @xmath121 , where @xmath122 , with @xmath123 sampled from a ( hausdorff-)continuous density on @xmath68 and @xmath124 being i.i.d .",
    "centered noise of finite variance .",
    "basic learning tasks can be expressed in this ideal - learning framework as follows :    [ ex : dimension - reduction ] the dimension @xmath100 is large .",
    "the task is to estimate the true manifold @xmath68 , assuming that @xmath125 .",
    "[ ex : regression ] the variables @xmath14 are partitioned into dependent and independent variables .",
    "the noise acts only on the dependent variables , and the task is to estimate  @xmath68 .",
    "[ ex : classification ] the generative truth @xmath68 is assumed to have irreducible components @xmath126 .",
    "the sample  @xmath127 is given as points @xmath128 with labels @xmath129 $ ] .",
    "the task is to estimate the components  @xmath130 of  @xmath68 .",
    "[ ex : clustering ] the generative truth  @xmath68 is assumed to have irreducible components @xmath126 .",
    "the sample  @xmath127 is unlabeled , and the task is to estimate @xmath126 .",
    "measures of statistical optimality will be given after presenting our main algorithmic principle .",
    "now we describe the basic idea behind computing with kernel duals .",
    "it can be applied in all of the examples above .",
    "algorithm  [ alg : features ] , which we term ideal pca , takes data @xmath112 , sampled with noise from a manifold  @xmath68 , and returns feature functions of the form @xmath131 which are labelled either generative (= part of the interpolation space ) or discriminative (= part of the kernel border basis ) . the kernel function  @xmath132 is one of the polynomial kernels , but could be any kernel in principle .",
    "sample @xmath133 random points @xmath134 ; write those into a @xmath135 matrix @xmath78 . compute the @xmath136 kernel matrix @xmath137 with @xmath83-th entry @xmath138 . compute the singular value decomposition @xmath139 , @xmath140 ) the @xmath141 in the output are the entries of  @xmath142 ; the @xmath143 are the @xmath143 sampled above . for each feature @xmath144 , assign the label `` discriminative '' if @xmath145 , otherwise `` generative '' .",
    "also return the @xmath146 .",
    "the number @xmath133 should be chosen sufficiently large , either as @xmath147 , or @xmath148_{\\le d}$ ] , if known . in these cases ,",
    "theorem  [ thm : dual - interp ] guarantees convergence in the noiseless case ; we will see later that this also makes ipca ( alg .",
    "[ alg : features ] ) a noise consistent algorithm . in the output ,",
    "the discriminative features are expected to vary strongly when leaving the manifold  @xmath68 .",
    "therefore they describe the internal structure of the data . on the other hand ,",
    "the generative features are expected to almost vanish in a neighborhood of  @xmath68 .",
    "therefore they describe the manifold itself .",
    "the singular values  @xmath146 yield a quantitative measure .",
    "all - or some of the - features obtained from ipca ( alg .",
    "[ alg : features ] ) can be bulk evaluated in an efficient way : first compute the kernel matrix @xmath149 , then the features are obtained from the matrix @xmath150 .",
    "we remark again that it is not necessary to choose @xmath151 - as it would be in symbolic methods - due to the rank guarantee in theorem  [ thm : dual - interp ] .",
    "as already said above , ipca ( alg",
    ".  [ alg : features ] ) transfers the exact statement in theorem  [ thm : dual - interp ] to the case of noisy data .",
    "we now show that ipca does this in a beneficial way .",
    "first we want to remark that the classical concept of consistency will not be applicable here , since a manifold  @xmath68 which is a point can lead to the same observations as a line  @xmath68 if i.i.d .",
    "gaussian noise is added to the samples .",
    "therefore , no algorithm can `` converge '' to  @xmath68 in the limit of the sample size .",
    "we argue that the proper notion of consistency in the manifold setting is _ noise consistency _ :    [ def : stability ] consider the learning setup outlined in section  [ sec : kernels.learning ] .",
    "that is , let @xmath71 be manifold with ideal @xmath84 , and assume that we have noisy samples @xmath122 from  @xmath68 .",
    "we say that an estimator @xmath152 is _ noise consistent _ if @xmath153 as @xmath154 , where convergence of @xmath155 is defined as convergence of all vector spaces @xmath156 ( possibly of different order ) .    intuitively , noise consistency is the combination of stability with respect to noise , and correctness in the noise - free case .",
    "[ thm : basic - estimate ] consider the learning setup outlined in section  [ sec : kernels.learning ] .",
    "that is , let @xmath71 be a manifold with ideal @xmath84 , and assume that we have noisy samples @xmath122 from @xmath68 .",
    "ipca ( alg .",
    "[ alg : features ] ) estimates  @xmath68 noise - consistently in the following sense : assume @xmath157 is generated in degree  @xmath13 or less , let @xmath158_{\\le d}$ ] , let @xmath159_{\\le d}}$ ] be the vector space generated by the discriminative features that ipca , with inhomogenous kernel @xmath160 , outputs .",
    "let @xmath155 be the ideal generated by the orthogonal complement of @xmath159_{\\le d}}$ ] .",
    "then , @xmath161 is a noise consistent estimate for @xmath68 .    if @xmath162 , this follows directly from the eckart - young - theorem which implies that thresholded svd is a noise consistent estimator for the span of a matrix .",
    "the general statement is implied as follows : by theorem  [ thm : dual - interp ] and noise consistency of svd , @xmath159_{\\le d}}$ ] is a noise consistent estimate for @xmath85_{\\le d}$ ] .",
    "thus , by passing to the orthogonal complement , @xmath163 is a noise consistent estimate for @xmath164 .",
    "since @xmath157 is generated in degree @xmath13 , this implies that @xmath155 is a noise consistent estimate for @xmath157 , which implies the statement .      at first glance , ipca ( alg .",
    "[ alg : features ] ) may seem to be another version of kernel pca or kernel svd .",
    "however , there is one main difference : the matrix  @xmath78 is random",
    ". therefore we do not work with the kernel matrix @xmath165 , as usual , but with a matrix @xmath80 . +",
    "this enables us to look at the feature span of  @xmath77 _ from the outside _ , whereas the classical approach only looks at relations between  @xmath77 and  @xmath77 .",
    "more specifically , doing pca or svd or any method involving only @xmath165 will reveal only features _ inside the data manifold_. the manifold itself - as the important generative object - will not be identified .",
    "this shortcoming has already been noticed in  @xcite[section 3.1 : `` kernels ca nt help '' ] , where the authors conclude that such methods are not suitable for learning generative features in an algebraic setting .",
    "the matrix @xmath80 can be used to capture the _ extrinsic structure _ of the data manifold . +",
    "what happens mathematically can be exposed using a linear ( and noise - free ) example : take the kernel to be the linear scalar product @xmath166 .",
    "suppose our data @xmath112 come from a linear subspace @xmath167 .",
    "then we would like to learn discriminative features , that is , features that vary among the @xmath90 , here the principal components , and generative features , in this simple case the subspace  @xmath168 from which the data are sampled .",
    "writing  @xmath77 as the @xmath169-matrix with the @xmath90 as rows , the kernel matrix is the @xmath170-matrix @xmath171 .",
    "note that this differs from the @xmath172-matrix @xmath173 which is taken in classic pca ( for centered data ) .",
    "singular value decomposition of  @xmath174 will reveal features of  @xmath77 , such as the dimension of  @xmath168 , through the rank of  @xmath174 .",
    "however , rotating  @xmath168 together with the  @xmath90 , or embedding it into a different @xmath175 will leave  @xmath174 unchanged . therefore the generative information on  @xmath168",
    "is lost in @xmath165 since this matrix contains only information on  @xmath77 _ inside _",
    "@xmath168 . on the other hand",
    ", if a random matrix @xmath176 is taken with @xmath177 , and if @xmath178 is considered , the space  @xmath168 can easily be reconstructed from the right singular vectors .",
    "moreover , all information on @xmath165 is potentially contained as well , most easily ( but impractically ) by adding in the rows of  @xmath77 as rows of  @xmath78 .",
    "this also shows that the important part of  @xmath78 is that `` orthogonal '' to  @xmath77 , because it allow us to capture the _ extrinsic _ structure of  @xmath168 .",
    "+ the case of general kernels is analogous , if we replace the scalar products above by the kernel function .",
    "the role of the  @xmath28 , which are above a basis for @xmath69 , is played by the feature vectors @xmath179 which now span the complete feature space .",
    "the mathematical justification is given by theorem  [ thm : dual - manifold ] which shows that the manifold @xmath68 corresponds to a proper linear subspace of the complete feature space .",
    "the interpolation space is exactly orthogonal to decision functions @xmath180 with  @xmath90 a data point .",
    "therefore @xmath165 can not be used to say anything about the interpolation space . on the other hand ,",
    "theorem  [ thm : dual ] says that the whole feature space is dual to decision functions of the form @xmath181 , with @xmath28 generic / random ; so , @xmath80 is the proper object which captures both features of the interpolation space - through the part of  @xmath78 that is kernel orthogonal to @xmath77 - and the intrinsic features of the data in  @xmath77 which can be obtained through @xmath165 and are recovered e.g.  by kernel pca .      the ipca algorithm ( alg .",
    "[ alg : features ] ) provides an estimate for the interpolation space , and therefore the generative manifold  @xmath68 , as discussed above .",
    "however , there are two points where improvement is possible : ( a ) the features learnt are all of the same degree , while there may be features of different degrees . in particular , if an overly high  @xmath13 is chosen in ipca , and if  @xmath68 is for instance linear , this will not be explicitly noticed .",
    "( b ) the size of the approximate kernel border basis , i.e. , the number of generators for @xmath70 , when naively estimated as generators for the orthogonal of @xmath85_{\\le d}$ ] , grows exponentially in  @xmath13 , since @xmath26_{\\le d}$ ] does .",
    "+ in the following we address these issues simultaneously by a powering - projection - strategy applied to the kernel matrix .",
    "to address ( a ) , we increase degrees and learn features of increasing degrees step - by - step by computing entrywise - powers of the degree  @xmath182 kernel matrix , exploiting the fact that the degree  @xmath13 kernel matrix is the @xmath13-th power to address ( b ) , we use the absorption property of the ideal @xmath70 to obtain a low number of multiplicative generators , by projecting the kernel matrix onto a low rank approximation ; by theorem  [ thm : dual - interp ]  ( iii ) this corresponds to adding new elements to the kernel border basis .",
    "powering is furthermore compatible with absorption by equation  [ eq : orthdeg ] .",
    "so , after powering , only new generators in higher degree will appear , and this allows us to add them sequentially to the approximate kernel border basis .",
    "concretely , this works as follows .",
    "start with the linear kernel matrix @xmath183",
    ". then project on a smaller rank matrix @xmath184 by singular value thresholding .",
    "next , compute @xmath185 as the entrywise second power @xmath186 and project again onto a smaller rank matrix @xmath187 . in general , obtain @xmath188 , then threshold .",
    "the threshold can be chosen fixed , or according to the hilbert function of  @xmath68 , if that is known . in each step , we add features to the approximate kernel border basis which correspond to singular values under the threshold , but not exactly zero .",
    "this `` degree greedy '' strategy can be seen as a kernelization of some ideas in the avi class of algorithms  @xcite .",
    "we now describe an algorithm , which uses the power - projecting strategy , called approximate vanishing ideal component analysis ( avica ) .",
    "avica will output generative and discriminative features of various degrees , ordered by informativity . as discussed in section  [ sec : kernels.power ] , the main difference to ipca ( alg .",
    "[ alg : features ] ) lies in the fact that the `` degree greedy '' strategy collects generators for the ideal of the manifold @xmath68 with increasing degree , therefore offers a much sparser generative description of @xmath68 than ipca , while learning degree - ordered generators of the interpolation space as well .",
    "we present avica as algorithm  [ alg : avica ] ; for simplicity of reading , we first introduce notation for the projection step which is singular value thresholding :    for a matrix @xmath174 and a threshold @xmath189 , we define the _ thresholded svd _ to be @xmath190 where the concatenations @xmath191 and @xmath192 are the left and right singular matrices of the usual singular value decomposition , with singular values in @xmath127 having absolute value @xmath193 , and those in @xmath194 being @xmath195 .",
    "applied to the kernel matrix , this means , according to theorem  [ thm : dual - interp ]  ( iii ) that the features corresponding to @xmath196 are added to the approximate kernel border basis .",
    "sample random points @xmath197 ; write those in a @xmath135 matrix @xmath78 .",
    "[ alg : avica.step1 ] let @xmath198 be the all - ones @xmath199 matrix .",
    "compute the @xmath136 matrix @xmath200 .",
    "set @xmath201 set @xmath202 ( entry - wise product ) compute the @xmath189-thresholded svd + @xmath203 .",
    "for each column @xmath204 of @xmath142 , return a discriminative feature @xmath205 .",
    "for each column @xmath204 of @xmath196 with singular value that is not zero with machine precision , return a generative feature @xmath205 also return as quantum @xmath206 the corresponding singular values times @xmath8 .",
    "@xmath207 display as informativity order the generative features ascendingly by @xmath206 , the discriminative ones descendingly .    since avica computes a basis for the interpolation space in a similar way as ipca ( alg .",
    "[ alg : features ] ) , an analogous proof shows that avica is a noise consistent estimator for @xmath68 in the same sense .",
    "evaluation of the features can again be done efficiently , by storing @xmath208 as model parameters , then repeating the computations . since",
    "this is sligtly more complex than in the case of ipca , we describe this explicitly in form of algorithm  [ alg : avica_eval ] .",
    "let @xmath198 be the all - ones @xmath199 matrix .",
    "compute the @xmath136 matrix @xmath200 .",
    "set @xmath202 return @xmath209 for discriminative and @xmath210 for generative features .",
    "rows are indexed by @xmath132 , columns by @xmath211 .",
    "@xmath212      from the discussion so far , it appears that the main advantage and novel of avica is learning generative features of some data manifold .",
    "however , with a minor but crucial modification , it can be adapted for discriminative supervised learning in a natural way which will allow one - vs - all or one - vs - one discrimination which is in some sense also class - generative .",
    "namely , consider a feature @xmath213 in the kernel border basis , that is , @xmath214 for @xmath215 .",
    "we have said that such an @xmath216 is generative , as it describes @xmath68 - but it can also be viewed discriminative , distinguishing @xmath68 from the `` set of general points '' in @xmath69 . while this is an unusual view , it is the one which generalizes well to discriminative learning : the @xmath28 were chosen to span @xmath69 or the feature space ; picking them , instead , as elements of a different manifold @xmath217 will in the same way allow to distinguish @xmath68 from @xmath218 .",
    "specifically , in step  [ alg : avica.step1 ] of avica ( alg .",
    "[ alg : avica ] ) , replace random sampling from @xmath69 with random sampling in some @xmath219 , in order to learn to identify points in @xmath68 among points in @xmath218 . to learn a one - vs - all - classifier between classes @xmath220 , choose @xmath221 and @xmath222 , then use the `` generative '' features , in the kernel border basis , as class discriminative .",
    "we would like to stress that neither ipca ( alg .",
    "[ alg : features ] ) nor avica ( alg .  [ alg : avica ] ) makes a strong use of the polynomial kernel ; for a general kernel , duality with a polynomial ring @xmath223 $ ] takes the place of duality with the polynomial ring @xmath12 $ ] ; the number @xmath133 has to be taken sufficiently large for the application .",
    "again , generative and discriminative features can be both extracted with ipca and avica .",
    "the interpolation space corresponds to the usual features learnt by kernel methods , while the manifold , or ideal , yields new generative ones , depending on the kernel . for example , when taking @xmath132 to be the gauss kernel , avica will learn the _ clusters themselves _ , instead of separators , since the clusters correspond to manifolds in the gauss feature space .",
    "a well - known example used to motivate discriminative kernel classification is a @xmath224-dimensional problem , in which the classes are sampled from concentric circles .    the analogous task for generative kernel learning is learning _",
    "one _ circle from a noisy sample .",
    "figure [ fig : circle ] shows that avica does this . in each experiment , we generate @xmath225 uniform points on a circle of radius @xmath226 , centered at the origin , add @xmath227 and then run @xmath228-avica with threshold @xmath229 , which is what we expect for noise in the feature space .",
    "the width of the green region in figure [ fig : circle ] is scaled by @xmath230 , since it is proportional to length of the generator in feature space and captures the data .",
    "however , it is only there for illustrative purposes : the estimation task is to estimate the _ manifold _ , not the data , so what is important is not the width of the green region but that the black circle is in it .",
    "we tested avica on the mnist handwritten digit recognition data set , to compare it with the avi class algorithm in  @xcite .",
    "classification was done using the discriminative method described in section  [ sec : kernels.disc ] ; the union of all classes was subsampled to @xmath225 data points .",
    "the class to which a test point was assigned was chosen as the minimizer of the @xmath231-norm of the one - vs - all - generative features . as kernels",
    ", we used the inhomogenous kernel with @xmath232 , and the gauss kernel with a width of @xmath233 .",
    "thresholding in avica was done at the logarithmic mean of the singular value spectrum . for @xmath234 ,",
    "that is with purely linear features ( in which case ipca = avica ) , both methods ( polynomial and gauss ) achieved an overall misclassification rate of @xmath235 with an overall runtime in the order of seconds . increasing the degree , the size of the subsample , or varying the parameters can lead to lower misclassification rates .",
    "however , it is difficult to compare these results to those of  @xcite , since the authors do not disclose how exactly they measure runtime or choose the threshold , therefore we refrain from more detailed comparison and conclude that ipca and avica are already very fast and competitive on handwritten digits for degree @xmath182 .",
    "let us close out our theoretical discussion with two critical connections .",
    "the first is between our method and the by - now classical theory of discriminative learning with kernels . by our discussion on duality in section  [ sec : ideals ] , we have implicitly shown the following informal `` theorem '' :    [ thm : learning - duality ] generative learning with ideals is dual to discriminative learning with kernels ; discriminative learning with ideals is dual to generative learning with kernels .",
    "for example , in the classical discriminative scenario of the _ kernel support vector machine _",
    ", the standard kernel decision function is of the form @xmath236 . in the ideals setting , this is generative learning of the separating hyperplane , which is a manifold uniquely parametrized by  @xmath216 , interpreted as a polynomial and element of the _ interpolation space_. in _ kernel pca _ , generative features are learnt for the data ; by using an analogous technique in the dual , _ ipca _ learns the data manifold in a discriminative way , by separating points on the manifold ( the matrix  @xmath77 ) from points not on the manifold ( the matrix  @xmath78 ) .",
    "moreover , the _ noise consistency _ guarantees that we obtain for both ipca and avica are dual to _ generalization bounds _ that can be obtained from classical vapnik - chervonenkis theory .",
    "finally , we briefly discuss the principal strains of related work and the ideas which are rooted there .",
    "these are , in the sequence we will discuss them : kernels , manifold learning , algebra in statistics and learning , approximate symbolic methods . * kernel methods *",
    "form a broad field and have been widely studied in practical and theoretical context ; a detailed overview over the field and its history can , for example , be found in the `` further reading '' sections of  @xcite . to our knowledge",
    ", there is so far no technique or result relating kernel methods to symbolic computation .",
    "the major link to existing literature is through the kernel trick  @xcite and the reproducing kernel hilbert space duality ( see section 2.2.3 of  @xcite ) : the initial statements on algebra - kernel duality can be obtained from rkhs theory by considering polynomial functions formally as elements of the polynomial rings . *",
    "manifold learning * techniques , such as principal curves  @xcite , lle  @xcite , or the aforementioned kernel pca  @xcite have in common that they assign an embedding of the data into low dimensional space .",
    "this corresponds to learning discriminative features .",
    "our algorithms ipca and avica also learn features which generatively describe the manifold , i.e. , explicitly describe where the points lie in the high dimensional data space . as the discussion in section  [ sec : kernels.analysis ] explains in greater detail , these methods can be seen as an extension of kernel pca in the sense that they do not only learn the embedding , but also the manifold . * algebraic techniques in statistics * have been a recurring topic since the advent of algebraic statistics , for an overview see  @xcite .",
    "however , the results in algebraic statistics are not directly applicable to a learning or data related context , since the field is predominantly concerned with understanding algebraically structured models and not estimating them from data . on the other hand , there are seemingly unrelated scenarios where specific algebraic structures have explicitly been used for estimation and learning in particular scenarios , e.g.  @xcite .",
    "a learning theory built on polynomials and ideals has been outlined in the appendix of  @xcite . *",
    "approximate symbolic computation * techniques can be traced back to corless et al  @xcite who proposed the use of singular value decomposition ( svd ) for polynomial systems , and the work of stetter  @xcite who pioneered a more general numerical view .",
    "the first algorithms which use svd to estimate an approximate vanishing ideal numerically are those of heldt et al  @xcite , which uses border bases and a numerically stable variant of term orderings , and sauer  @xcite , which uses a coordinate independent degree - increasing strategy to compute homogenous bases ; both algorithms can be considered as variations on the same idea set and yield an approximate version of the exact symbolic buchberger - mller algorithm  @xcite . the homogenous variant in  @xcite has recently reappeared under the name `` vanishing components analysis ''  @xcite in the machine learning community . *",
    "avica * can be seen as a kernelization of these avi - class algorithms : it integrates the idea of the compact order ideal / border basis representation in  @xcite ( interpolation space / kernel border basis ) with the degree - greedy strategy and homogenous coordinate independence of  @xcite into our kernel based algorithm through the concept of kernel - ideal - duality .      in this paper , we have exposed an intricate duality between kernels and commutative algebra , between ideals and manifolds , between kernel methods and symbolic algebraic methods , between generative and discriminative learning .",
    "we have outlined how a statistical learning theory in this new ideal - kernel - duality setting can be obtained , and we have described how the duality can be exploited in general for learning explicit , generative structures with kernels .",
    "we have demonstrated , theoretically and competitively on real - world - data , how the duality can be used to construct a novel type of algorithm , avica , which simultaneously extracts discriminative and generative components from the data . + in the light of this , the whole field of statistical data analysis and machine learning stands open to a plethora of new methods following this conceptual regime .",
    "lt is supported by the european research council under the european union s seventh framework programme ( fp7/2007 - 2013 ) / erc grant agreement no 247029- sdmodels .",
    "this research was carried out at mfo , supported by fk s oberwolfach leibniz fellowship .",
    "mark  a. aizerman , emmanuel  m. braverman , and lev  i. rozonoer .",
    "theoretical foundations of the potential function method in pattern recognition learning . in _ automation and remote control ,",
    "_ , number  25 in automation and remote control , , pages 821837 , 1964 .",
    "bernhard  e. boser , isabelle  m. guyon , and vladimir  n. vapnik . a training algorithm for optimal margin classifiers . in _ proceedings of the 5th annual acm workshop on computational learning theory _ , pages 144152 .",
    "acm press , 1992 .",
    "franz  johannes kirly , paul von bnau , frank meinecke , duncan blythe , and klaus - robert mller .",
    "algebraic geometric comparison of probability distributions .",
    "_ journal of machine learning research _ , 130 ( mar):0 855903 , 2012 .",
    "franz  johannes kirly , paul von bnau , jan  saputra mller , duncan blythe , frank meinecke , and klaus - robert mller .",
    "regression for sets of polynomial equations .",
    "_ jmlr workshop and conference proceedings _ , 22:0 628637 , 2012 .",
    "roi livni , david lehavi , sagi schein , hila nachliely , shai shalev - shwartz , and amir globerson .",
    "vanishing component analysis . in _ proceedings of the 30th international conference on machine learning ( icml-13 ) _ , pages 597605 , 2013 ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a theory which unifies kernel learning and symbolic algebraic methods . </S>",
    "<S> we show that both worlds are inherently dual to each other , and we use this duality to combine the structure - awareness of algebraic methods with the efficiency and generality of kernels . </S>",
    "<S> the main idea lies in relating polynomial rings to feature space , and ideals to manifolds , then exploiting this generative - discriminative duality on kernel matrices . </S>",
    "<S> we illustrate this by proposing two algorithms , ipca and avica , for simultaneous manifold and feature learning , and test their accuracy on synthetic and real world data . </S>"
  ]
}