{
  "article_text": [
    "predictability in general circulation models ( gcms ) for the north pacific , from seasonal to decadal time scales , has been the subject of many recent studies , ( e.g. , @xcite , @xcite , @xcite ) , several of which focus on the role of the initial state ( e.g. , @xcite , @xcite , @xcite , @xcite ) .",
    "the north pacific exhibits prominent examples of interannual to decadal variability , such as the pacific decadal oscillation ( pdo ; * ? ? ?",
    "* ) , and the more rapidly decorrelating north pacific gyre oscillation ( npgo ; * ? ? ?",
    "* ) , both of which have been the subject of much interest .",
    "an important phenomenon of intra - annual variability in the north pacific is the reemergence of anomalies in both the sea surface temperature ( sst ) fields @xcite , as well as in the sea ice concentration ( sic ) field @xcite , where regional anomalies in these state variables vanish over a season , and reappear several months later , as made evident by high time - lagged correlations .",
    "the north pacific ( along with the north atlantic ) is a region of relative strong low - frequency variability in the global climate system @xcite . yet , in gcms it has been shown that this region shows relative lack of predictability ( less than a decade ; * ? ? ?",
    "* ) , with the community climate system model ( ccsm ) having particularly weak persistence and low predictability in the north pacific among similiar gcms @xcite .",
    "the ocean and sea ice systems show stronger low - frequency variability than the atmosphere @xcite .",
    "the internal variability exhibited in the north pacific has also been characterized as being in distinct climate regimes ( e.g. @xcite ) , where the dynamics exhibit regime transitions and metastability . as a result ,",
    "cluster based methods have been a popular approach to model regime behavior in climate settings , such as in @xcite , where hidden markov models were used to model atmospheric flows .",
    "a traditional modeling approach for the pdo has been to fit an autoregressive process @xcite , as well as models being externally forced from the tropics through enso @xcite . in @xcite , autoregressive models were successful in predicting temporal patterns corresponding to the pdo and npgo when forced with suitably modulated intermittent modes .",
    "additional flexibility can be built into regression models by allowing nonstationary , i.e. time dependent coefficients , and a successful example of this is the finite element method ( fem ) clustering procedure combined with multivariate autoregressive factor ( varx ) model framework @xcite . in this approach",
    ", the data is partitioned into a predetermined number of clusters , and model regression coefficients are estimated on each cluster , together with a cluster affiliation function that indicates which model is used at a particular time .",
    "this can further be adapted to account for external factors @xcite . while fem - varx methods can be effective at fitting the desired data , advancing the system state in the future for a prediction is dependent on being able to successfully predict the unknown cluster affiliation function .",
    "methods for using this framework in a prediction setting have been used in @xcite .",
    "another regression modeling approach was put forward in @xcite , where physics constraints were imposed on multilevel nonlinear regression models , preventing ad - hoc , finite - time blow - up , a pathological behavior previously shown to exist in such models without physics constraints in @xcite .",
    "appropriate nonlinear regression models using this strategy have been shown to have high skill for predicting the intermittent cloud patterns of tropical intra - seasonal variability @xcite .",
    "parametric models may perform well for fitting , but often have poor performance in a prediction setting , particularly in systems that exhibit distinct dynamical regimes .",
    "nonparametric models can be advantageous in systems where the underlying dynamical system is unknown or imperfect .",
    "an early example of this is an analog forecast , first introduced by @xcite , where one considers the historical record , and makes predictions based on examining state variable trajectories in the past that are similiar to the current state .",
    "analog forecasting does not make any assumptions on the underlying dynamics , and cleverly avoids model error when the underlying model is observations from nature .",
    "this has since been applied to other climate prediction scenarios , such as the southern oscillation index @xcite , the indian summer monsoon @xcite , and wind forecasting @xcite , where it was found to be particularly useful in forecasting rare events .",
    "key to the success of an analog forecasting method is the ability to identify a good historical analog to the current initial state . in climate applications ,",
    "the choice of analog is usually determined by minimizing euclidean distance between snapshots of system states , with a single analog being selected @xcite . in @xcite ,",
    "analog forecasting was extended upon in two key ways .",
    "first , the state vectors considered were in takens lagged embedding space @xcite , which captures some of the dynamics of the system , rather than a snapshot in time .",
    "second , instead of selecting a single analog determined by euclidean distance , weighted sums of analogs were considered , where weights are determined by a kernel function . in this context , a kernel is an exponentially decaying pairwise similarity measure , intuitively , playing the role of a local covariance matrix . in @xcite , kernels were introduced in the context of nonlinear spectral analysis ( nlsa ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) for decomposing high - dimensional spatiotemporal data , leading naturally to a class of low - frequency observables for prediction through kernel eigenfunctions . in @xcite ,",
    "the kernel used in the nlsa algorithm was adapted to be multivariate , allowing for multiple variables , possibly of different physical units , to be considered jointly in the analysis .",
    "the aim of this study is to develop low dimensional , data - driven models for prediction of dominant low - frequency climate variability patterns in the north pacific , combining the approaches laid out in @xcite and @xcite .",
    "we invoke a prediction approach that is purely statistical , making use only of the available historical record , possibly corrupted by model error , and the initial state itself .",
    "the approach utilizes out - of - sample extension methods @xcite to define our target observable beyond a designated training period .",
    "there are some key differences between this study and that of @xcite .",
    "first , we consider multivariate data , including sic , which is intrinsically noisier than sst , given that sic is a thresholded variable and experiences high variability in the marginal ice zone .",
    "we also make use of observational data , which is a relatively short time series compared to gcm model data .",
    "in addition to considering kernel eigenfunctions as our target observable , which we will see is well - suited for prediction , we also consider integrated sea ice anomalies , a more challenging observable uninfluenced by the data analysis algorithm .",
    "we compare performance of these kernel ensemble analog forecasting techniques to some parametric forecast models ( specifically , autoregressive and fem - varx models ) , and find the former to have higher predictive skill than the latter , which often can not even outperform the simple persistence forecast .",
    "the rest of the paper is outlined as follows . in section [ sec : methods ] , we discuss the mathematical methods used to perform kernel ensemble analog forecasting predictions , and also discuss alternative parametric forecasting methods . in section [ sec : datasets ]",
    "we describe the data sets used for our experiments , and in section [ sec : results ] we present our results .",
    "discussion and concluding remarks are in section [ sec : discussion ] .",
    "our forecasting approach is motivated by using kernels , a pairwise measure of similarity for the state vectors of interest .",
    "a simple example of such an object is a gaussian kernel : @xmath0 where @xmath1 is a state variable of interest , and @xmath2 is a scale parameter that controls the locality of the kernel .",
    "the kernels are then used to generate a weighted ensemble of analog forecasts , making use of an available historical record , rather than relying on a single analog . to extend the observable we wish to predict beyond the historical period into the future , we make use of out - of - sample extension techniques .",
    "these techniques allow one to extend a function ( observable ) to a new point @xmath3 by looking at values of the function at known points @xmath4 close to @xmath3 , where closeness will be determined by the kernels . an important part of any prediction problem is to choose a target observable that is both physically meaningful and exhibits high predictability . as demonstrated in @xcite , defining a kernel on the desired space naturally leads to a preferred class of observables that exhibit time - scale separation and good predictability .",
    "nonlinear dynamical systems generally give rise to datasets with low - dimensional nonlinear geometric structures ( e.g. , attractors ) .",
    "we therefore turn to a data analysis technique , nlsa , that allows us to extract spatio - temporal patterns from data from a high - dimensional non - linear dynamical system , such as a coupled global climate system @xcite .",
    "the standard nlsa algorithm is a nonlinear manifold generalization of singular spectrum analysis ( ssa ) @xcite , where the covariance operator is replaced by a discrete laplace - beltrami operator to account for non - linear geometry on the underlying data manifold .",
    "the eigenfunctions of this operator then form a convenient orthonormal basis on the data manifold .",
    "a key advantage of nlsa is there is no pre - processing of the data needed , such as band - pass filtering or seasonal partitioning .",
    "an important first step in the nlsa algorithm is to perform a time - lagged embedding of the spatio - temporal data as a method of inducing time - scale separation in the extracted modes .",
    "an analog forecast method is driven by the initial data , and to incorporate some of the system s dynamics into account in selecting an analog , instead of using a snapshot in time of the state as our initial condition , time - lagged embedding helps make the data more markovian .",
    "let @xmath5 be a time series sampled uniformly with time step @xmath6 , on a grid of size @xmath7 , with @xmath8 samples .",
    "we construct a lag - embedding of the data set with a window of length @xmath9 , and consider the lag - embedded time series @xmath10 the data now lies in @xmath11 , with @xmath12 the dimension of the lagged embedded space , and @xmath13 number of samples in lagged embedded space ( also called takens embedding space , or delay coordinate space ) .",
    "it has been shown that time - lagged embedding recovers the topology of the attractor of the underlying dynamical system that has been lost through partial observations @xcite .",
    "in particular , the embedding affects the non - linear geometry of the underlying data manifold @xmath14 , in such a way to allow for dynamically stable patterns with time scale separation @xcite , a desirable property that will lead to observables with high predictability .",
    "the next step is to define a kernel on the data manifold @xmath14 . rather than using a simple gaussian as in equation",
    ", the nlsa kernel makes use of phase velocities @xmath15 , which forms a vector field on the data manifold and provides additional important dynamic information .",
    "the nlsa kernel we use is @xmath16 with this kernel @xmath17 and associated matrix @xmath18 , we solve the laplacian eigenvalue problem to acquire an eigenfunction basis @xmath19 on the data - manifold @xmath14 . to do this",
    ", we construct the discrete graph laplacian by following the diffusion maps approach of @xcite , and forming the following matrices : @xmath20 @xmath21 here @xmath22 is a real parameter , typically with value 0 , @xmath23 , 1 .",
    "we note in the large data limit , as @xmath24 and @xmath25 , this discrete laplacian converges to the laplace - beltrami operator on @xmath14 for a riemannian metric that depends on the kernel @xcite .",
    "we can therefore think of the kernel as biasing the geometry of the data to reveal a class of features , and the nlsa kernel does this in such a way as to extract dynamically stable modes with time scale separation .",
    "we then solve the eigenvalue problem @xmath26 the resulting laplacian eigenfunctions @xmath27 form an orthonormal basis on the data manifold @xmath14 with respect to the weighted inner product : @xmath28    as well as forming a convenient orthonormal basis , the eigenfunctions @xmath29 give us a natural class of observables with good time - scale separation and high predictability .",
    "these eigenfunctions @xmath29 are time series , nonlinear analogs to principal components , and can be used to recreate spatio - temporal modes , similar to extended empirical orthogonal functions ( eofs ) @xcite . however , unlike eofs , the eigenfunctions @xmath29 do not measure variance , but rather measure oscillations or roughness in the abstract space @xmath14 , the underlying data manifold .",
    "the eigenvalues @xmath30 measure the dirichlet energy of the corresponding eigenfunctions , which has the interpretation of being squared wave numbers on this manifold @xcite .",
    "we now use the leading low - frequency kernel eigenfunctions as our target observables for prediction .",
    "as described in @xcite , the above nlsa algorithm can be modified to incorporate more than one time series .",
    "let @xmath31 , @xmath32 be two signals , sampled uniformly with time step @xmath6 , on ( possibly different ) @xmath33 grid points . after lag - embedding each variable with embedding windows @xmath34 to its appropriate embedding space , so @xmath35 and @xmath36 , we construct the kernel function @xmath17 by scaling the physical variables @xmath37 to be dimensionless by @xmath38    this can then be extended to any number of variables , regardless of physical units , and allows for analysis in coupled systems , such as the ocean and sea ice components of a climate model .",
    "an alternative approach to equation in extending the nlsa kernel to multiple variables with different physical units is to first normalize each variable to unit variance .",
    "however a drawback of this approach is that information about relative variability is lost as the ratios of the variances are set equal , rather than letting the dynamics of the system control the variance ratios of the different variables @xcite as in equation .",
    "now that we have established our class of target observables , namely the eigenfunctions @xmath29 in equation , we need a method for extending these observables into the future to form our predictions , for which we draw upon out - of - sample extension techniques . to be precise ,",
    "let @xmath39 be a function defined on a set @xmath40 ; @xmath39 may be vector - valued , but in our case is scalar .",
    "we wish to make a prediction of @xmath39 by extending the function to be defined on a point outside of the training set @xmath41 , by performing an out - of - sample extension , which we call @xmath42 .",
    "there are some desireable qualities we wish to have in such an extension , namely that @xmath42 is in some way well - behaved and smooth on our space , and is in consistent as the number of in - samples increases .",
    "below we discuss two such methods of out - of - sample extension , the geometric harmonics , based on the nystrm method , and laplacian pyramids .",
    "the first approach for out - of - sample extension is based on the nystrm method @xcite , recently adapted to machine learning applications @xcite , and is based on representing a function @xmath39 on @xmath41 in terms of an eigenfunction basis obtained from a spectral decomposition of a kernel . while we use the nlsa kernel ( equation [ eq : nlsa_kernel ] ) , other kernels could be used , another natural choice being a gaussian kernel . for a general kernel @xmath43 ,",
    "consider its row - sum normalized counterpart : @xmath44 these kernels have the convenient interpretation of forming discrete probability distributions in the second argument , dependent on the first argument , so @xmath45 , which will be a useful perspective later in our analog forecasting in section [ subsec : keaf ] .",
    "we then solve the eigenvalue problem @xmath46 we note the spectral decomposition of @xmath47 yields a set of real eigenvalues @xmath48 and an orthonormal set of eigenfunctions @xmath49 that form a basis for @xmath50 @xcite , and we can thus represent our function @xmath39 in terms of this basis : @xmath51 let @xmath52 be an out - of - sample , or test data , point , to which we wish to extend the function @xmath39 . if @xmath53 , the eigenfunction @xmath49 can be extended to any @xmath54 by @xmath55 this definition ensures that the out - of - sample extension is consistent when restricted to @xmath41 , meaning @xmath56 for @xmath57 .",
    "combining equations and allows @xmath39 to be assigned for any @xmath54 by evaluating the eigenfunctions @xmath49 at @xmath3 and using the projection of @xmath39 onto these eigenfunctions as weights : @xmath58 equation is called the nystrm extension , and in @xcite the extended eigenfunctions in equation are called geometric harmonics .",
    "we note this scheme becomes ill conditioned since @xmath59 as @xmath60 @xcite , so in practice there is a truncation of the sum at some level @xmath61 , usually determined by the decay of the eigenvalues . with the interpretation of the eigenvalues @xmath62 as wavenumbers on the underlying data manifold , this truncation represents removing features from the data that are highly oscillatory in this space .",
    "the geometric harmonics method is well suited for observables that have a tight bandwidth in the eigenfunction basis ( particularly the eigenfunctions themselves ) , but for observables that may require high levels of eigenfunctions in their representation , the above mentioned ill - conditioning may hamper this method .",
    "an alternative to geometric harmonics is the laplacian pyramid @xcite , which invokes a multiscale decomposition of the original function @xmath39 in its out - of - sample extension approach .",
    "a family of kernels defined at different scales is needed , and for clarity of exposition we will use a family of gaussian kernels @xmath63 ( and their row - normalized counterparts @xmath64 ) at scales @xmath61 : @xmath65 that is , @xmath66 represents the widest kernel width , and increasing @xmath61 gives finer kernels resolving more localized structures .",
    "for a function @xmath67 , the laplacian pyramid representation of @xmath39 approximates @xmath39 in a multiscale manner by @xmath68 , where the first level @xmath69 is defined by @xmath70 and we then evaluate the difference : @xmath71 we then iteratively define the @xmath61th level decomposition @xmath72 : @xmath73 iteration is continued until some prescribed error tolerance @xmath74 is met .",
    "next we extend @xmath39 to a new point @xmath75 by @xmath76 for @xmath77 , and assign @xmath39 the value @xmath78 that is , we have formed a multiscale representation of @xmath39 using weighted averages of @xmath39 for nearby inputs , where the weights are given by the scale of the kernel function . since the kernel function can accept any inputs from @xmath11 , we can define these weights for other points outside @xmath41 , and thus define @xmath39 by using weighted values of @xmath39 on @xmath41 ( known ) , where now the weights are given by the proximity of the out - of - sample @xmath79 to input points @xmath80 .",
    "the parameter choices of the initial scale @xmath2 and error tolerance @xmath81 set the scale and cut - off of the dyadic decomposition of @xmath39 in the laplacian pyramid scheme .",
    "we choose @xmath2 to be the median of the pairwise distances of our training data , and the error tolerance @xmath81 to be scaled by the norm of the observable over the training data , for example @xmath82 . in our applications below , we use a multiscale family of nlsa kernels based on equation rather than the above family of gaussian kernels .",
    "the core idea of traditional analog forecasting is to identify a suitable analog to one s current initial state from a historical record , and then make a prediction based on the trajectory of that analog in the historical record .",
    "the analog forecasting approach laid out in @xcite , which we use here , varies in a few important regards .",
    "first , the initial system state , as well as the historical record ( training data ) , is in takens embedding space , so that an analog is not determined by a snapshot in time alone , but the current state with some history ( a ` video ' ) .",
    "second , rather than using euclidean distance , as in traditional analog forecasting , the distances we use are based on a defined kernel function , which reflects a non - euclidean geometry on the underlying data manifold .",
    "the choice of geometry is influenced by the lagged embedding and choice of kernel , which we have done in a way that gives us time scale separation in the resulting eigenfunctions @xmath83 , yielding high predictability .",
    "third , rather than identify and use a single analog in the historical record , weighted sums of analogs are used , with weights determined by the kernel function .    for this last point , it is useful to view analog forecasting in the context of an expectation over an empirical probability distribution .",
    "for example , a traditional analog forecast of a new initial state @xmath3 at some time @xmath84 in the future , based on selected analog @xmath85 , can be written as @xmath86 where @xmath87 is the dirac delta function , and @xmath88 is the operator that shifts the timestamp of @xmath4 by @xmath84 . to make use of more than one analog and move to an ensemble",
    ", we let @xmath89 be a more general discrete empirical distribution , dependent on the initial condition @xmath3 , with probabilities ( weights ) determined by our kernel function . writing in this way",
    ", we simply need to define the empirical probability distribution @xmath89 to form our ensemble analog forecast .    for the geometric harmonics method",
    ", we projected @xmath39 onto an eigenfunction basis @xmath29 ( truncated at some level @xmath61 ) and performed an out - of - sample extension on each eigenfunction basis function , i.e. @xmath90 or , to write this in terms of an expectation over an empirical probability measure : @xmath91 where @xmath92 we then define our prediction for lead time @xmath84 via geometric harmonics by @xmath93 where @xmath94    similarly , the @xmath84 shifted ensemble analog forecast via laplacian pyramids is then @xmath95 where @xmath96 corresponds to the probability distribution from the kernel at scale @xmath97 .",
    "thus we have a method for forming a weighted ensemble of predictions , that is non - parametric and data - driven through the use of a historical record ( training data ) , which itself has been subject to analysis that reflects the dynamics of the high - dimensional system in the non - linear geometry on the underlying abstract data manifold @xmath14 , and produces a natural preferred class of observables to target for prediction through the kernel eigenfunctions .",
    "we wish to compare our ensemble analog prediction methods to more traditional parametric methods , namely autoregressive models of the north pacific variability @xcite , of the form @xmath98 where @xmath99 is our signal , @xmath100 is the external forcing ( possibly 0 ) , @xmath101 is the autoregressive term , and @xmath102 is the noise term , each to be estimated from the training data , and @xmath103 is a gaussian process . in the stationary case , the model coefficients @xmath104 , @xmath105 , @xmath106 are constant in time , and can be evaluated in an optimal way through ordinary least squares . in a non - stationary case , we will invoke the fem - varx framework of @xcite by clustering the training data into @xmath17 clusters , and evaluating coefficients @xmath107 , @xmath108 , @xmath109 for each cluster ( see @xcite for more details on this algorithm ) . from the algorithm we obtain a cluster identification function @xmath110 , such that @xmath111 indicates",
    "at time @xmath112 the model coefficients are @xmath113 , @xmath114 .",
    "in addition to choosing the number of clusters @xmath17 , the method also has a persistence parameter @xmath115 that governs the number of allowable switches between clusters that must be chosen prior to calculating model coefficients .",
    "these parameters are usually chosen to be optimal in the sense of the akaike information criterion ( aic ) @xcite , an information theoretic based measure for model selection which penalizes overfitting by large number of parameters .",
    "as mentioned earlier , while non - stationary autoregressive models may perform better than stationary models in fitting , an inherent difficulty in a prediction setting is the advancement of the model coefficients @xmath116 beyond the training period , which in the above framework amounts to solely advancing the cluster affiliation function @xmath110 .",
    "if we call @xmath117 the probability of the model being at cluster @xmath118 at time @xmath119 , we can view @xmath110 as determining a markov switching process on the cluster member probabilities @xmath120 which over the training period will be 1 in one entry , and 0 elsewhere at any given time .",
    "we can estimate the transition probability matrix @xmath121 of that markov process by using the optimal cluster affiliation sequence @xmath110 from the fem - varx framework ( here optimal is for the training period ) .",
    "assuming the markov hypothesis , we can estimate the stationary probability transition matrix directly from @xmath110 by : @xmath122 where @xmath123 is the number of direct transitions from state @xmath97 to state @xmath124 @xcite .",
    "this estimated transition probability matrix @xmath121 can be used to model the markov switching process in the following ways .",
    "the first method we employ is to advance the cluster member probabilities @xmath125 using the estimated probability transition matrix @xmath121 by the deterministic equation @xcite : @xmath126 where @xmath127 is the initial cluster affiliation , which is determined by which cluster center the initial point @xmath128 is closest to , and @xmath129 is either 0 or 1 .    the second method we employ is to use the estimated transition matrix @xmath121 to generate a realization of the markov switching process @xmath130 , and use this to determine the model cluster at any given time , maintaining strict model affiliation .",
    "thus @xmath131 is @xmath132 , and 0 otherwise .      to gauge the fidelity of our predictions",
    ", we will evaluate the average root - mean - square error ( rmse ) and pattern correlation ( pc ) of our predictions @xmath3 against the ground truth @xmath1 , where points in our test data set ( of length @xmath133 ) are used as intial conditions for our predictions . as a benchmark",
    ", we will compare each prediction approach to a simple persistence forecast @xmath134 .",
    "the error metrics are calculated as @xmath135 where @xmath136 @xmath137 an important note is that for data - driven observables , such as nlsa eigenfunctions or eof principal components , there is no underlying ground truth when predicting into the future .",
    "as such a ground truth for comparison needs to be defined when evaluating the error metrics , for which one can use the out - of - sample extended function @xmath138 as defined in equations and .",
    "we use model data from the community climate system model ( ccsm ) , versions 3 and 4 , for monthly sic and sst data , restricted to the north pacific , which we define as 20@xmath13965@xmath139n , 120@xmath139e110@xmath139w .",
    "ccsm3 model data is used from a 900 year control run ( experiment b30.004 ) @xcite .",
    "the sea ice component is the community sea ice model ( csim ; * ? ? ?",
    "* ) and the ocean component is the parallel ocean program ( pop ) , both of which are sampled on the same nominal @xmath140 grid .",
    "ccsm4 model data is used from a 900 year control run ( experiment b40.1850 ) , which uses the community ice code 4 model for sea ice ( cice4 ; * ? ? ?",
    "* ) and the pop2 model for the ocean component @xcite , also on a common nominal @xmath140 grid . specific differences and improvements between the two model versions can be found in @xcite .",
    "nlsa was performed on these data sets , both in single and multiple component settings , with the same embedding window of @xmath141 months for each variable , kernel scale @xmath142 , and kernel normalization @xmath143 .",
    "a 24 month embedding window was chosen to allow for dynamical memory beyond the seasonal cycle , as is used in other nlsa studies ( e.g. , @xcite ) .",
    "there are 6648 grid points for sst and 3743 for sic in this region for these models , and with an embedding window of @xmath144 , this means our lagged embedded data lies in @xmath11 with @xmath145 for sst and @xmath146 for sic . for the purposes of a perfect model experiment , where the same model run is used for both the training and test data",
    ", we split the ccsm4 control run into two 400 year sets ; years 100  499 for the training data set , and years 500  899 for out - of - sample test points .",
    "after embedding , this leaves us with @xmath147 samples in each data set . in our model error experiment , we train on 800 years of the ccsm3 control run , and use 800 years of ccsm4 for test data , giving us @xmath148 data points .",
    "in addition to low - frequency kernel eigenfunctions as observables , we also consider north pacific integrated sea ice extent anomalies as a target for prediction in section [ subsec : siea ] .",
    "this observable exhibits faster variability , on an intra - annual time - scale , and as such we reduce the embedding window from 24 months to 6 months for our kernel evaluation . using the ccsm4 model training data , we define monthly anomalies by calculating a climatology @xmath149 of monthly mean sea ice extent .",
    "let @xmath150 be gridpoints in our domain of interest ( north pacific ) , @xmath151 the mean sic , @xmath152 the grid cell area , and then the sea ice extent anomaly hat will be our target observable is defined as @xmath153      for observational data , we turn to the met office hadley centre s hadisst data set @xcite , and use monthly data for sic and sst , from years 19792012 , sampled on a 1@xmath139 latitude - longitude grid . we assign ice covered grid points an sst value of @xmath154c , and have removed a trend from the data by calculating a linear trend for each month .",
    "there are 4161 spatial grid points for sst , for a lagged embedded dimension of @xmath155 , and 3919 grid points for sic , yielding @xmath156 . for direct comparison with this observational data set ,",
    "the above ccsm model data sets have been interpolated from the native pop grid @xmath140 grid to a common 1@xmath139 latitude - longitude grid .",
    "after embedding , we are left with @xmath157 observation test data points .",
    "the eigenfunctions that arise from nlsa typically fall into one of three categories : i ) periodic modes , which capture the seasonal cycle and its higher harmonics ; ii ) low - frequency modes , characterized by a red power spectrum and a slowly decaying autocorrelation function ; and iii ) intermittent modes , which have the structure of periodic modes modulated with a low - frequency envelope .",
    "the intermittent modes are dynamically important @xcite , shifting between periods of high activity and quiessence , but carry little variance , and are thus typically missed or mixed between modes in classical ssa",
    ". examples of each of these eigenfunctions arising from a ccsm4 data set with sic and sst variables are shown in figure [ fig : ccsm4_phi_sstice ] , for years 100499 of the pre - industrial control run .",
    "the corresponding out - of - sample extension eigenfunctions , defined through the nystrm method , are shown in figure [ fig : ccsm4_osephi_sstice ] , and are computed using years 500899 of the same ccsm4 pre - industrial control run as test ( out - of - sample ) data .",
    "we use the notation @xmath158 , @xmath159 , or @xmath160 , to indicate if the nlsa mode is from sic , sst , or joint sst and sic variables , respectively .    , @xmath161 are periodic ( annual and semi - annual ) modes , characterized by a peak in the power spectrum and oscillatory autocorrelation function ; @xmath162 , @xmath163 are the two leading low - frequency modes , characterized by a red power spectrum and slowly decaying montone autocorrelation function ; @xmath164 , @xmath165 are intermittent modes , characterized by a broad peak in the power spectrum and decaying oscillatory autocorrelation function.,width=317 ]    .,width=317 ]    we perform our prediction schemes for five year time leads by applying the kernel ensemble analog forecast methods discussed in section [ subsec : keaf ] to the leading two low - frequency modes @xmath160 , @xmath166 from nlsa on north pacific , shown in figure [ fig : ccsm4_phi_sstice ] .",
    "the leading low - frequency modes extracted through nlsa can be though of as analogs to the well known pdo and npgo modes , even in the multivariate setting .",
    "we have high correlations between the leading multivariate and univariate low - frequency nlsa modes , with corr @xmath167 for our analog of the npgo mode , and corr @xmath168 for our analog of the pdo mode .    as a benchmark",
    ", we compare against the simple constant persistence forecast @xmath169 , which can perform reasonably well given the long decorrelation time of these low - frequency modes , and in fact beats paramteric autoregressive models as we will see below .",
    "we define the ground truth itself to be the out - of - sample eigenfunction calculated by equation , so by construction , our predictions by the geometric harmonics method are exact at time lag @xmath170 , whereas predictions using laplacian pyramids will have reconstruction errors at time lag @xmath170 .",
    "we first consider the perfect model setting , where the same dynamics generate the training data and test ( forecast ) data .",
    "this should give us a measure of the potential predictability of the methods .",
    "snapshots of sample prediction trajectories along with the associated ground truth out - of - sample eigenfunction are shown in figure [ fig : ccsm4_ccsm4_sstice_traj ] . in figure",
    "[ fig : ccsm4_ccsm4_sstice_error ] , we see that for the leading low - frequency mode @xmath171 ( our npgo analog ) , the ensemble based predictions perform only marginally better than persistence in the pc metric , but have improved rmse scores over longer timescales .",
    "however with the second mode @xmath172 ( our pdo analog ) , we see a more noticeable gain in predictive skill with the ensemble analog methods over persistence .",
    "if we take 0.6 as a pc threshold @xcite , below which we no longer consider the model to have predictive skill , we see an increase of about 8 months in predictive skill with the ensemble analog methods over persistence , with skillful forecasts up to 20 months lead time .",
    "we note these low - frequency modes extracted from multivariate data exhibit similiar predictability ( as measured by when the pc falls below the 0.6 threshold ) than their univariate counterparts ( @xmath173 or @xmath174 , results not shown ) .",
    ".,title=\"fig:\",width=147 ] .,title=\"fig:\",width=147 ]     ( pdo ) mode.,title=\"fig:\",width=147 ]   ( pdo ) mode.,title=\"fig:\",width=147 ]   ( pdo ) mode.,title=\"fig:\",width=147 ]   ( pdo ) mode.,title=\"fig:\",width=147 ]      to incorporate model error into our prediction experiment , we train on the ccsm3 model data , serving as our ` model ' , and then use ccsm4 model data as our test data , serving the role of our ` nature ' , and the difference in the fidelity of the two model versions represents general model error . in this experiment ,",
    "our ground truth is an out - of - sample eigenfunction trained on ccsm3 data , and extended using ccsm4 test data . for the leading @xmath171 ( npgo )",
    "mode , we see again marginal increased predictive performance in the ensemble analog predictions over persistence at short time scales in pc in figure [ fig : ccsm3_ccsm4_sstice_error ] , but at medium to long time scales this improvement has been lost ( though after the score has fallen below the 0.6 threshold )",
    ". this could be due to the increased fidelity of the ccsm4 sea ice model component over the ccsm3 counterpart , where using the less sophisticated model data for training leaves us a bit handicapped in trying to predict the more sophisticated model data @xcite .",
    "the improvement in the predictive skill of @xmath172 ( pdo ) mode over persistence is less pronounced in the presence of model error than it was in the perfect model case shown in figure [ fig : ccsm4_ccsm4_sstice_error ] . nevertheless , the kernel ensemble analog forecasts still provide a substantial improvement of skill compared to the persistence forecast , extending the pc = 0.6 threshold to 20 months .",
    "we can further examine the model error scenario by using the actual obervational data set as our nature , and ccsm4 as our model ( figure [ fig : ccsm4_hadisst_sstice_error ] ) . given the short observational record used ,",
    "far fewer prediction realizations are generated , adding to the noisiness of the error metrics .",
    "the loss of predictability is apparent , especially in the @xmath172 mode , where the kernel ensemble analog forecasts fail to beat persistence , and drop below 0.6 in pc by 10 months , half as long as the perfect model case .",
    "( pdo ) mode.,title=\"fig:\",width=147 ]   ( pdo ) mode.,title=\"fig:\",width=147 ]    ( pdo ) mode.,title=\"fig:\",width=147 ]   ( pdo ) mode.,title=\"fig:\",width=147 ]      we compare the ensemble analog predictions to standard stationary autoregressive models , as well as non - stationary models using the fem - varx framework of @xcite discussed in section [ subsec : armodel ] , for the low - frequency modes @xmath160 , @xmath166 generated from ccsm4 model ( figure [ fig : ar_ccsm4_sstice ] ) and the hadisst observation ( [ fig : ar_hadisst_sstice ] ) training data .",
    "for both sets of low - frequency modes , @xmath175 clusters was judged to be optimal by the aic as mentioned in section [ subsec : armodel]see @xcite for more details .",
    "the coefficients for each cluster are nearly the same ( autoregressive coefficent close to 1 , similar noise coefficients ) , apart from the constant forcing coefficient @xmath176 of almost equal magnitude and opposite sign , suggesting two distinct regime behaviors . in the stationary case ,",
    "the external forcing coefficient @xmath177 is very close to 0 .    in the top left panel of figures [ fig : ar_ccsm4_sstice ] and",
    "[ fig : ar_hadisst_sstice ] , we display snapshots of the leading low - frequency mode @xmath171 ( npgo ) trajectory reconstruction during the training period , for the stationary ( blue , solid ) , and non - stationary ( red , dashed ) models , along with the cluster switching function associated with the non - stationary model . in both the ccsm4 model and hadisst data sets , the non - stationary model snapshot is a better representation of the truth ( black , solid ) , and the benefit over the stationary model is more clearly seen in the ccsm4 model data , figure [ fig : ar_ccsm4_sstice ] , which has the benefit of a 400 year training period , as opposed to the shorter 16 year training period with the observational data set .    in the prediction setting , however , the non - stationary models , which are reliant on an advancement of the unknown cluster affiliation function @xmath110 beyond the training period , as discussed in section [ subsec : armodel ] , fail to outperform their stationary counterparts in the rmse and pc metrics ( bottom panels of figures [ fig : ar_ccsm4_sstice ] and [ fig : ar_hadisst_sstice ] ) .",
    "in fact , none of the proposed regression prediction models are able to outperform the simple persistence forecast in these experiments . as a measure of _ potential _ predition skill for the non - stationary models , whereby we mean that if perfect knowledge of the underlying optimal cluster switching function @xmath110 could be known over the test period , we have run the experiment of replacing the test data period with the training data set , and find exceedingly strong predictive performance , with pc between 0.7 and 0.8 for all time lags tested , up to 60 months .",
    "similar qualitative results for the second leading low - frequency mode @xmath172 ( pdo ) for each data set were found ( not shown ) .",
    "this suggests that the markov hypothesis , the basis for the predictions of @xmath110 , is not accurate , and other methods incorporating more memory are needed .",
    "( npgo ) trajectory ( black ) with reconstructed stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods : p2 = stationary , using fem - varx model coefficients from initial cluster ; k1 = stationary autoregressive ; m1 , m2 = fem - varx with predictions as described in section [ subsubsec : pred_pi ] , where m1 is deterministic evolution of the cluster affiliation @xmath125 , and m2 uses realizations of @xmath178 generated from the estimated probability transition matrix @xmath121 .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods , including p1 = persistence as a benchmark .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]   ( npgo ) trajectory ( black ) with reconstructed stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods : p2 = stationary , using fem - varx model coefficients from initial cluster ; k1 = stationary autoregressive ; m1 , m2 = fem - varx with predictions as described in section [ subsubsec : pred_pi ] , where m1 is deterministic evolution of the cluster affiliation @xmath125 , and m2 uses realizations of @xmath178 generated from the estimated probability transition matrix @xmath121 .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods , including p1 = persistence as a benchmark .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]   ( npgo ) trajectory ( black ) with reconstructed stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods : p2 = stationary , using fem - varx model coefficients from initial cluster ; k1 = stationary autoregressive ; m1 , m2 = fem - varx with predictions as described in section [ subsubsec : pred_pi ] , where m1 is deterministic evolution of the cluster affiliation @xmath125 , and m2 uses realizations of @xmath178 generated from the estimated probability transition matrix @xmath121 .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods , including p1 = persistence as a benchmark .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]   ( npgo ) trajectory ( black ) with reconstructed stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods : p2 = stationary , using fem - varx model coefficients from initial cluster ; k1 = stationary autoregressive ; m1 , m2 = fem - varx with predictions as described in section [ subsubsec : pred_pi ] , where m1 is deterministic evolution of the cluster affiliation @xmath125 , and m2 uses realizations of @xmath178 generated from the estimated probability transition matrix @xmath121 .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods , including p1 = persistence as a benchmark .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]     ( npgo ) trajectory ( black ) with stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with the corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods  see figure [ fig : ar_ccsm4_sstice ] for details of methods .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]   ( npgo ) trajectory ( black ) with stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with the corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods  see figure [ fig : ar_ccsm4_sstice ] for details of methods .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]   ( npgo ) trajectory ( black ) with stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with the corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods  see figure [ fig : ar_ccsm4_sstice ] for details of methods .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]   ( npgo ) trajectory ( black ) with stationary ( blue ) and non - stationary @xmath175 ( red ) fem - varx model trajectories , along with the corresponding model affiliation function @xmath110 for non - stationary case .",
    "top right : sample trajectories for various prediction methods  see figure [ fig : ar_ccsm4_sstice ] for details of methods .",
    "bottom panels : rmse and pc as a function of lead time for various prediction methods .",
    "the dashed black line is for potential predictive skill of non - stationary fem - varx , where predictions were ran over the training period using the known optimal model affiliation function @xmath110.,title=\"fig:\",width=147 ]      the targeted observables hereto considered for prediction have been data driven , and as such influenced by the data analysis algorithm .",
    "hence there is no objective ground truth available when predicting these modes beyond the training period on which the data analysis was performed , and while in this case the nlsa algorithm was used , other data analysis methods such as eofs would suffer the same drawback .",
    "we wish to test our prediction method on an observable that is objective , in the sense that it can be computed independently of the data analysis algorithm , for which we turn to integrated sea ice extent anomalies , as defined in equation .",
    "we can clearly compute the time series of sea ice anomalies from the out - of - sample set directly ( relative to the training set climatology ) , which will be our ground truth , and use the laplacian pyramid approach to generate our out - of - sample extension predictions .",
    "this observable does not have a tight expansion in the eigenfunction basis , so the geometric harmonics method of extension will be ill - conditioned , and thus not considered .",
    "we note that in this approach , there are reconstruction errors at time lag @xmath170 , so at very short time scales we can not outperform persistence .",
    "we consider a range of truncation levels for the number of ensemble members used , which are nearest neighbors to the out - of - sample data point , as determined by the kernel function . using all available neighbors will likely overly smooth and average out features in forward trajectories , while using too few neighbors will place to much weight on particular trajectories .",
    "indeed we find good performance using 100 ( out of total possible 4791 ) .",
    "the top left panel of figure [ fig : ccsm4_ccsm4_npica ] shows a snapshot of the true sea ice extent anomalies , respectively , together with a reconstructed out - of - sample extension using the laplacian pyramid .",
    "to be clear this is not a prediction trajectory , but rather each point in the out - of - sample extension is calculated using equation ; that is , each point is a time - lead @xmath170 reconstruction .",
    "the top right panel shows sample snapshots of prediction trajectories , restricting the ensemble size to the nearest 10 , 100 , and then all nearest neighbors .",
    "notice in particular predictions match the truth when the anomalies are close to 0 , but then may subsequently progress in the opposite sign as the truth . as our prediction metrics are averaged over initial conditions spanning all months , the difficulty the predictions have in projecting from a state of near 0 anomaly significantly hampers the ability for long - range predictability of this observable .    in the bottom two panels of figure [ fig : ccsm4_ccsm4_npica ] we have the averaged error metrics , and see year to year correlations manifesting as a dip / bump in rmse and pc in the persistence forecast that occurs after 12 months .",
    "after the first month lag time , the kernel ensemble analog forecasts overcome the reconstruction error and beat persistence in both rmse and pc , and give about a 2 month increase in prediction skill ( as measured by when the pc drops below 0.6 ) over persistence .",
    "we see the best performance restricting the ensemble size to 100 nearest neighbors ( about 2% of the total sample size ) in both the rmse and pc metrics , though this is marginal before the error metrics drop below the 0.6 threshold .    pushing the prediction strategy to an even more difficult problem , in figure [ fig : ccsm4_hadisst_npica ]",
    "we try to predict observational sea ice extent anomalies using ccsm4 model data as training data . in this scenario , without knowledge of the test data climatology , the observation sea ice extent anomalies are defined using the ccsm4 climatology . in the top panel of figure",
    "[ fig : ccsm4_hadisst_npica ] we see the strong bias as a result , where the observational record has less sea ice than the ccsm model climatology , which has been taken from a pre - industrial control run .",
    "this strongly hampers the ability to accurately predict observational sea ice extent anomalies using ccsm4 model ensemble analogs , and as a result the only predictive skill we see is from the annual cycle .",
    "we have examined a recently proposed prediction strategy employing a kernel ensemble analog forecasting scheme making use of out - of - sample extension techniques .",
    "these nonparametric , data - driven methods make no assumptions on the underlying governing dynamics or statistics .",
    "we have used these methods in conjunction with nlsa to extract low - frequency modes of variability from north pacific sst and sic data sets , both from models and observations .",
    "we find that for these low - frequency modes , the analog forecasting performs at least as well , and in many cases better than , the simple constant persistence forecast .",
    "predictive skill , as measured by pc exceeding 0.6 , can be increased by up to 3 to 6 months for low - frequency modes of variability in the north pacific .",
    "this is a strong advantage over traditional parametric regression models , which were shown to fail to beat persistence .",
    "the kernel ensemble analog forecasting methods outlined included two variations on the underlying out - of - sample extension scheme , each with its strengths and weaknesses .",
    "the geometric harmonics method , based on the nystrm method , worked well for observables that are band - limited in the eigenfunction basis , in particular the eigenfunctions themselves .",
    "however for observables not easily expressed in such a basis , the laplacian pyramid provides an alternative method based on a multiscale decomposition of the original observable .",
    "while the low - frequency eigenfunctions from nlsa were a natural preferred class of observables to target for prediction , we also studied the case of objective observables uninfluenced by the data analysis algorithm .",
    "motivated by the strong reemergence phenomena , we considered sea ice extent anomalies as our target for prediction in the north pacific . using a shorter embedding window due to faster ( seasonal ) time scale dynamics ,",
    "we obtain approximately a two month increase in predictive skill over the persistence forecast .",
    "it is also evident that when considering regional sea ice extent anomalies winds play a large role in moving ice into and out of the domain of interest , and as such additional consideration of the atmospheric component in the system could be included in the multivariate kernel function , despite having weaker low - frequency variability .",
    "an important consideration is that our prediction metrics are averaged over initial conditions ranging over all possible initial states of the system .",
    "as we saw clearly in the case of north pacific sea ice volume anomalies , these prediction strategies can have difficulty with projecting from an initial state of quiessence , and can easily predict to the wrong sign of an active state , greatly hampering predictive skill . on the other hand we would expect predictive skill to be stronger for those initial states that begin in a strongly active state , or said differently , clearly in one climate regime , as oppose to in transition between the two .",
    "future work will further explore conditional forecasting , where we either condition forecasts on the initial month , or the target month . also extending this analysis to the north atlantic ,",
    "another region of strong low - frequency variability , is a natural progression of this work .    the research of andrew majda and dimitrios giannakis is partially supported by onr muri grant 25 - 74200-f7112 .",
    "darin comeau is supported as a postdoctoral fellow through this grant .",
    "the research of dimitrios giannakis is also partially supported by onr dri grant n00014 - 14 - 0150 .",
    "collins wd , bitz cm , blackmon ml , bonan gb , bretherton cs , carton ja , chang p , doney sc , hack jj , henderson tb , et  al ( 2006 ) the community climate system model version 3 ( ccsm3 ) . journal of climate 19(11):21222143      di  lorenzo e , schneider n , cobb k , franks p , chhak k , miller a , mcwilliams j , bograd s , arango h , curchitser e , et  al ( 2008 ) north pacific gyre oscillation links ocean climate and ecosystem change . geophysical research letters 35(8 )",
    "gent pr , danabasoglu g , donner lj , holland mm , hunke ec , jayne sr , lawrence dm , neale rb , rasch pj , vertenstein m , et  al ( 2011 ) the community climate system model version 4 .",
    "journal of climate 24(19):49734991            giannakis d , majda aj ( 2013 ) nonlinear laplacian spectral analysis : capturing intermittent and low - frequency spatiotemporal patterns in high - dimensional data . statistical analysis and data mining 6(3):180194    giannakis d , majda aj ( 2014 ) data - driven methods for dynamical systems : quantifying predictability and extracting spatiotemporal patterns .",
    "mathematical and computational modeling : with applications in engineering and the natural and social sciences p 288        holland mm , bailey da , briegleb bp , light b , hunke e ( 2012 ) improved sea ice shortwave radiation physics in ccsm4 : the impact of melt ponds and aerosols on arctic sea ice*. journal of climate 25(5):14131430          horenko i ( 2011 ) on analysis of nonstationary categorical data time series : dynamical dimension reduction , model selection , and applications to computational sociology .",
    "multiscale modeling & simulation 9(4):17001726                          rayner n , parker de , horton e , folland c , alexander l , rowell d , kent e , kaplan a ( 2003 ) global analyses of sea surface temperature , sea ice , and night marine air temperature since the late nineteenth century .",
    "journal of geophysical research : atmospheres ( 19842012 ) 108(d14 )      smith r , jones p , briegleb b , bryan f , danabasoglu g , dennis j , dukowicz j , eden c , fox - kemper b , gent p , et  al ( 2010 ) the parallel ocean program ( pop ) reference manual : ocean component of the community climate system model ( ccsm ) .",
    "los alamos national laboratory , laur-10 - 01853      tietsche s , day j , guemas v , hurlin w , keeley s , matei d , msadek r , collins m , hawkins e ( 2014 ) seasonal to interannual arctic sea ice predictability in current global climate models .",
    "geophysical research letters 41(3):10351043"
  ],
  "abstract_text": [
    "<S> the north pacific exhibits patterns of low - frequency variability on the intra - annual to decadal time scales , which manifest themselves in both model data and the observational record , and prediction of such low - frequency modes of variability is of great interest to the community . while parametric models , such as stationary and non - stationary autoregressive models , possibly including external factors , may perform well in a data - fitting setting , they may perform poorly in a prediction setting . </S>",
    "<S> ensemble analog forecasting , which relies on the historical record to provide estimates of the future based on past trajectories of those states similar to the initial state of interest , provides a promising , nonparametric approach to forecasting that makes no assumptions on the underlying dynamics or its statistics . </S>",
    "<S> we apply such forecasting to low - frequency modes of variability for the north pacific sea surface temperature and sea ice concentration fields extracted through nonlinear laplacian spectral analysis . </S>",
    "<S> we find such methods may outperform parametric methods and simple persistence with increased predictive skill . </S>"
  ]
}