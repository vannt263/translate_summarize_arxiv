{
  "article_text": [
    "gaussian process quadratures @xcite are methods to numerically compute integrals of the form @xmath0 = \\int { \\mathbf{g}}({\\mathbf{x } } ) \\ , w({\\mathbf{x } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } ,    \\label{eq : gaussint}\\ ] ] where @xmath1 is a ( non - linear ) integrand function and @xmath2 is a given , typically positive , weight function such that @xmath3 . in gaussian process",
    "quadratures the function @xmath4 is approximated with a gaussian process regressor @xcite and the integral is approximated with that of the gaussian process regressor .",
    "sigma - point methods @xcite can be seen @xcite as methods which approximate the above integrals via @xmath5 where @xmath6 are some predefined weights and @xmath7 are the sigma - points ( classically called abscissas ) .",
    "typically the evaluation points and weights are selected such that when @xmath8 is a multivariate polynomial up to a certain order , the approximation is exact .",
    "a particularly useful class of methods is obtained when the weight function is selected to be a multivariate gaussian density @xmath9 . in the context of gaussian process quadratures it then turns out that the integral of the gaussian process regressor can be computed in closed form provided that the covariance function of the process is chosen to be a squared exponential @xcite ( i.e. , exponentiated quadratic ) .",
    "this kind of quadrature methods are also often referred to as bayesian or bayes ",
    "hermite quadratures .",
    "they are closely related to gauss ",
    "hermite quadratures in the sense that as gaussian quadratures can be seen to form a polynomial approximation to the integrand via point - evaluations , gaussian process quadratures use a gaussian process regression approximation instead @xcite .",
    "because gaussian process regressors can be used to approximate much larger class of functions than polynomial approximations @xcite , they can be expected to perform much better also in numerical integration .",
    "the selection of a gaussian weight function is also particularly useful in non - linear filtering and smoothing , because the equations of non - linear gaussian ( kalman ) filters and smoothers @xcite consist of gaussian integrals of the above form and linear operations on vectors and matrices .",
    "the selection of different weights and sigma - points leads to different brands of approximate filters and smoothers @xcite .",
    "for example , the multidimensional gaussian type of gauss  hermite quadrature and cubature based filters and smoothers @xcite are based on explicit numerical integration of the gaussian integrals .",
    "the unscented transform based methods as well as other sigma - point methods @xcite can also be retrospectively interpreted to belong to the class of gaussian numerical integration based methods @xcite .",
    "conversely , gaussian type of quadrature or cubature based methods can also be interpreted to be special cases of sigma - point methods . furthermore , the classical taylor series based methods @xcite and stirling s interpolation based methods @xcite can be seen as ways to approximate the integrand such that the gaussian integral becomes tractable ( cf .",
    "@xcite ) . the recent fourier ",
    "hermite series @xcite , hermite polynomial @xcite methods are also based on numerical approximation of the integrands .",
    "the aim of this article is to present new gaussian process quadrature based methods for non - linear filtering and smoothing , and to analyze their connection with sigma - point methods and multivariate numerical integration methods .",
    "we show that many sigma - point filtering and smoothing algorithms such as unscented kalman filters and smoothers , cubature kalman filters and smoothers , and gauss  hermite kalman filters and smoothers can be seen as special cases of the proposed methods with suitably chosen covariance functions .",
    "more generally , we show that many classical multivariate gaussian quadrature methods , including gauss ",
    "hermite rules @xcite , and symmetric integration formulas @xcite are special cases of the present methodology .",
    "we also discuss different criteria for selecting the sigma - point ( abscissa ) locations : exactness for multivariate polynomials up to a given order , minimum average error , and quasi - random point sets .",
    "this article is an extended version of the conference article @xcite , where we analyzed the use of gaussian process quadratures in non - linear filtering and smoothing as well as their connection to the unscented transform and gauss  hermite quadratures . in this article , we deepen and sharpen the analysis of those connections and extend our analysis to a more general class of spherically symmetric integration rules .",
    "we also analyze different sigma - point selection schemes as well as provide more extensive set of numerical experiments .",
    "non - linear gaussian ( kalman ) filters and smoothers @xcite are methods which can be used to approximate the filtering distributions @xmath10 and smoothing distributions @xmath11 of non - linear state - space models of the form @xmath12 where , for @xmath13 , @xmath14 are the hidden states , @xmath15 are the measurements , and @xmath16 and @xmath17 are the process and measurements noises , respectively .",
    "the non - linear function @xmath18 is used to model the dynamics of the system and @xmath19 models the mapping from the states to the measurements .",
    "non - linear gaussian filters ( see , e.g. , @xcite , page 98 ) are general methods to produce gaussian approximations to the filtering distributions : @xmath20 non - linear gaussian smoothers ( see , e.g. , @xcite , page 154 ) are the corresponding methods to produce approximations to the smoothing distributions : @xmath21 both gaussian filters and smoothers above can be easily generalized to state - space models with non - additive noises ( see @xcite ) , but here we only consider the additive noise case .    a general additive noise one - step moment - matching - based gaussian filter algorithm can be written in the following form .",
    "[ alg : gf ] the prediction and update steps of the non - linear gaussian ( kalman ) filter are @xcite :    * prediction : @xmath22 * update : @xmath23    the filtering is started from initial mean and covariance , @xmath24 and @xmath25 , respectively , such that @xmath26 .",
    "then the prediction and update steps are applied for @xmath27 .",
    "the result of the filter is a sequence of approximations @xmath20 the corresponding smoothing algorithm can be written in the following form .",
    "[ alg : gs ] the equations of the non - linear gaussian ( rauch  tung  striebel , rts ) smoother are the following @xcite : @xmath28 \\ ,        [ { \\mathbf{f}}({\\mathbf{x}}_k ) - { \\mathbf{m}}^-_{k+1}]^{\\mathsf{t}}\\ ,       \\\\ & \\qquad \\times        \\mathrm{n}({\\mathbf{x}}_k \\mid   { \\mathbf{m}}_{k},{\\mathbf{p}}_{k } ) \\ ,        { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}_k + { \\mathbf{q}}_k \\\\     { \\mathbf{d}}_{k+1 } & = \\int [ { \\mathbf{x}}_k - { \\mathbf{m}}_{k } ] \\ , [ { \\mathbf{f}}({\\mathbf{x}}_k ) - { \\mathbf{m}}^-_{k+1}]^{\\mathsf{t}}\\\\ & \\qquad \\times        \\mathrm{n}({\\mathbf{x}}_k \\mid   { \\mathbf{m}}_{k},{\\mathbf{p}}_{k } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}_k \\\\     { \\mathbf{g}}_{k } & = { \\mathbf{d}}_{k+1 } \\ , [ { \\mathbf{p}}^-_{k+1}]^{-1 } \\\\     { \\mathbf{m}}^s_{k } & = { \\mathbf{m}}_{k }       + { \\mathbf{g}}_k \\ , ( { \\mathbf{m}}^s_{k+1 } - { \\mathbf{m}}^-_{k+1 } ) \\\\     { \\mathbf{p}}^s_{k } & = { \\mathbf{p}}_{k }       + { \\mathbf{g}}_k \\ , ( { \\mathbf{p}}^s_{k+1 } - { \\mathbf{p}}^-_{k+1 } ) \\ , { \\mathbf{g}}_k^{\\mathsf{t}}. \\end{split } \\label{eq : gs}\\ ] ] the smoothing recursion is started from the filtering result of the last time step @xmath29 , that is , @xmath30 , @xmath31 and proceeded backwards for @xmath32 .",
    "the approximations produced by the smoother are @xmath21 both the filter and smoother above can be derived from the following gaussian moment matching `` transform '' @xcite ( the terminology comes from unscented transform ) .",
    "[ alg : moment_trans1 ] the moment matching based gaussian approximation to the joint distribution of @xmath33 and the transformed random variable @xmath34 , where @xmath35 and @xmath36 , is given by @xmath37 where @xmath38      sigma - point filtering and smoothing methods can generally be described as methods which approximate the gaussian integrals in the gaussian filtering and smoothing equations ( and in the gaussian moment matching transform ) as @xmath39 where @xmath6 are some predefined weights and @xmath7 are the sigma - points .",
    "typically , the sigma - point methods use so called _ stochastic decoupling _ which refers to the idea that we do a change of variables @xmath40 where @xmath41 .",
    "this implies that we only need to design weights @xmath6 and unit sigma - points @xmath42 for integrating against unit gaussian distributions : @xmath43 thus leading to approximations of the form @xmath44 different sigma - point methods correspond to different choices of weights @xmath6 and unit sigma - points @xmath42 .",
    "for example , the canonical unscented transform @xcite uses the following set of @xmath45 weights ( recall that @xmath46 is the dimensionality of the state ) and sigma - points : @xmath47 where @xmath48 is a design parameter in the algorithm and @xmath49 is the unit vector towards the direction of the @xmath50th coordinate axis .    note that sigma - point methods sometimes use different weights for the integrals appearing in the mean and covariance computations of gaussian filters and smoothers .",
    "however , here we will only concentrate on the methods which use the same weights for both in order to derive more direct connections between the methods .",
    "for example , the above unscented transform weights are just a special case of more general unscented transforms ( see , e.g. , @xcite ) .",
    "gaussian process quadrature @xcite is based on forming a gaussian process ( gp ) regression @xcite approximation to the integrand using pointwise evaluations and then integrating the approximation . in gp regression",
    "@xcite the purpose is to predict the value of an unknown function @xmath51 at a certain test point @xmath52 based on a finite number of training samples @xmath53 observed from it .",
    "the difference to classical regression is that instead of postulating a parametric regression function @xmath54 , where @xmath55 are the parameters , in gp regression we put a gaussian process prior with a given covariance function @xmath56 on the unknown functions @xmath57 .    in practice ,",
    "the observations are often assumed to contain noise and hence a typical model setting is : @xmath58 where the first line above means that the random function @xmath59 has a zero mean gaussian process prior with the given covariance function @xmath56 .",
    "a commonly used covariance function is the exponentiated quadratic ( also called squared exponential ) covariance function @xmath60 where @xmath61 are parameters of the covariance function ( see @xcite ) .",
    "the gp regression equations can be derived as follows .",
    "assume that we want to estimate the value of the noise - free function @xmath62 based on its gaussian process approximation @xmath57 at a test point @xmath33 given the vector of observed values @xmath63 .",
    "due to the gaussian process assumption we now get @xmath64 where @xmath65 $ ] is the joint covariance of observed points , @xmath66 is the ( co)variance of the test point , @xmath67 $ ] is the vector cross covariances with the test point .",
    "the bayesian estimate of the unknown value of @xmath57 is now given by its posterior mean , given the training data . because everything is gaussian , the posterior distribution is gaussian and hence described by the posterior mean and ( auto)covariance functions : @xmath68 & = { \\mathbf{k}}^{\\mathsf{t}}({\\mathbf{x } } ) \\ ,    ( { \\mathbf{k}}+ \\sigma^2 { \\mathbf{i}})^{-1 } \\ , { \\mathbf{o}}\\\\    \\operatorname{cov}[g_k({\\mathbf{x } } )   \\mid { \\mathbf{o } } ] & = k({\\mathbf{x}},{\\mathbf{x } } ' )    -    { \\mathbf{k}}^{\\mathsf{t}}({\\mathbf{x } } ) \\ , ( { \\mathbf{k}}+ \\sigma^2 { \\mathbf{i}})^{-1 }      { \\mathbf{k}}({\\mathbf{x } } ' ) .",
    "\\end{split } \\ ] ] these are the gaussian process regression equations in their typical form @xcite , in the special case where @xmath69 is scalar .",
    "the extension to multiple output dimensions is conceptually straightforward ( see , e.g. , @xcite ) , but construction of the covariance functions as well as the practical computational methods tend to be complicated @xcite . however , a typical easy approach to the multivariate case is to treat each of the dimensions independently .      in gaussian process quadrature",
    "@xcite the basic idea is to approximate the integral of a given function @xmath69 against a weight function @xmath2 , that is , @xmath70",
    "= \\int g({\\mathbf{x } } ) \\ , w({\\mathbf{x } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } ,    \\label{eq : gaussint_b}\\ ] ] by evaluating the function @xmath69 at a finite number of points and then by forming a gaussian process approximation @xmath59 to the function .",
    "the integral is then approximated by integrating the gaussian process approximation ( or its posterior mean ) which is conditioned on the evaluation points instead of the function itself .",
    "here we assume that @xmath69 is scalar for simplicity as we can always take a vector function elementwise .",
    "gaussian process quadratures are related to a regression interpretation of classical gaussian quadrature integration , that is , we can interpret these integration methods as orthogonal polynomial approximations of the integrand evaluated at certain finite number of points @xcite .",
    "the integral is then approximated by integrating the polynomial instead of the original function .",
    "however , the aim of gaussian process quadrature is to get a good performance in average , whereas in classical polynomial quadratures the integration rule is designed to be exact for a limited class of ( polynomial ) functions .",
    "still , these approaches are very much linked together @xcite .",
    "due to linearity of integration , the posterior mean of the integral of the gaussian process regressor is given as @xmath71 & =     \\int \\operatorname{e}\\left[g_k({\\mathbf{x } } ) \\mid { \\mathbf{o}}\\right ] \\",
    ", w({\\mathbf{x } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } , \\end{split } \\ ] ] where the `` training set '' @xmath72 now contains the values of the function @xmath69 evaluated at certain selected inputs .",
    "the posterior variance of the integral can be evaluated in an analogous manner , and it is sometimes used to optimize the evaluation points of the function @xmath73 @xcite .",
    "the posterior covariance of the approximation is @xmath74 \\\\ &",
    "\\qquad =     \\iint \\operatorname{cov}\\left[g_k({\\mathbf{x } } ) \\mid { \\mathbf{o}}\\right ] \\ , w({\\mathbf{x } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}\\ , w({\\mathbf{x } } ' ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } ' . \\end{split } \\ ] ] that is ,",
    "when we approximate the integral   with the posterior mean we have @xmath75    ( { \\mathbf{k}}+ \\sigma^2 { \\mathbf{i}})^{-1 } \\ , { \\mathbf{o } } , \\end{split } \\label{eq : gpquad0}\\ ] ] the posterior variance of the ( scalar ) integral is @xmath74 \\\\",
    "& \\qquad =     \\iint k({\\mathbf{x}},{\\mathbf{x } } ' ) \\ , w({\\mathbf{x } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}\\ , w({\\mathbf{x } } ' ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } ' \\\\    & - \\left [ \\int { \\mathbf{k}}^{\\mathsf{t}}({\\mathbf{x } } ) \\ , w({\\mathbf{x } } ) \\ ,    { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}\\ , \\right ]    ( { \\mathbf{k}}+ \\sigma^2 { \\mathbf{i}})^{-1 } \\ ,     \\left [ \\int { \\mathbf{k}}({\\mathbf{x } } ' ) \\ , w({\\mathbf{x } } ' ) \\ ,    { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } ' \\ , \\right ] \\end{split } \\label{eq : gpquadvar0}\\ ] ] in this article we are specifically interested in the case of gaussian weight function , which then reduces the integral appearing in the above expressions and to @xmath76_i    = \\int k({\\mathbf{x}},{\\mathbf{x}}_i )   \\operatorname{n}({\\mathbf{x}}\\mid { \\mathbf{m}},{\\mathbf{p } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}\\end{split}\\ ] ] it is now easy to see that when the covariance function is a squared exponential @xmath77 , this integral can be easily computed in closed form by using the computation rules for gaussian distributions .",
    "furthermore if the covariance function is a multivariate polynomial , then these integrals are given by the moments of the gaussian distributions , which are also available in closed form .",
    "in this section we start by showing how gaussian process quadratures ( gpq ) can be seen as sigma - point methods and then introduce the gaussian process transform .",
    "the gaussian process transform then enables us to construct gpq - based non - linear filters and smoothers analogously to @xcite .      in this section",
    "the aim is to shown how gaussian process quadratures ( gpq ) can be seen as sigma - point methods .",
    "the gaussian process quadrature ( or bayes ",
    "hermite / bayesian quadrature ) can be seen is a sigma - point - type of integral approximation @xmath78 where @xmath79 with the unit sigma - points @xmath42 are selected according to a predefined criterion , and the weights are determined by @xmath80_i , \\end{split } \\label{eq : gp_wi}\\ ] ] where @xmath81 $ ] is the matrix of unit sigma - point covariances and @xmath82 $ ] is the vector cross covariances . in principle , the choice of unit sigma - points above is completely free , but good choices of them are discussed in the following sections .",
    "let us first use stochastic decoupling which enables us to only consider unit - gaussian integration formulas of the form . because we can integrate vector functions element - by - element , without loss of generality we can assume that @xmath62 is single - dimensional .",
    "let us now model the function @xmath83",
    "as a gaussian process @xmath59 with a given covariance function @xmath84 and fix the training set for the gp regressor by selecting the points @xmath85 , @xmath86 , which also determines the corresponding points @xmath79 such that the training set is @xmath72 .",
    "the gp approximation to the integral now follows from : @xmath87    ( { \\mathbf{k}}+ \\sigma^2 { \\mathbf{i}})^{-1 } \\ , { \\mathbf{o } } , \\end{split}\\ ] ] which when simplified and applied to all the dimensions of @xmath8 gives the result .",
    "note that above we actually assume that the stochastically - decoupled - function @xmath83 instead of the original integrand @xmath62 has the given covariance function .",
    "the reason for this modeling choice is that it enables us to decouple the mean and covariance from the integration formula and hence is computationally beneficial .",
    "this also makes the result invariant to affine transformations of the state and it also has a property that the variability of the functions corresponds to the scale of the problem .",
    "however , on the other hand , one might argue that it is the function @xmath62 which should actually model and using the stochastically - decoupled - function is `` wrong '' .    from equation",
    "we get that the component - wise variances of the gaussian process quadrature approximation can be expressed as @xmath88    using the above integration approximations we can also define a general gaussian process transform as follows .",
    "the reason for introducing the transform is that the corresponding approximate filters and smoothers can be readily constructed in terms of the transform ( cf .",
    "@xcite ) , which we will do in the next section .",
    "[ alg : gptrans ] the gaussian process quadrature based gaussian approximation to the joint distribution of @xmath33 and the transformed random variable @xmath34 , where @xmath35 and @xmath89 , is given by @xmath90 where @xmath91 where @xmath42 is some fixed set of sigma / training points and the weights are given by equation   with some selected covariance function @xmath84 .    in this article , at least in the analytical results , we usually assume that the measurements are noise - free , that is , @xmath92 .",
    "this enables us to obtain analytically exact relationships with the classical quadrature methods .",
    "however , when using gaussian process quadratures as numerical integration method , it is often beneficial to have at least a small non - zero value for @xmath93 in .",
    "this kind of `` jitter '' stabilizes numerics and can even be sometimes used to compensate for inaccuracies in modeling .",
    "[ ex : gpt_se ] let us now consider @xmath94 and select the sigma - point locations to be the ones of unscented transform . with the squared exponential covariance function and noise - free measurements ( @xmath92 ) we then get the weights : @xmath95 an interesting property is that in the limit @xmath96 we get @xmath97 which are the unscented transform weights .",
    "we return to this relationship in section  [ sec : minka ] .      in this section",
    "we show how to construct filters and smoothers using the gaussian process quadrature approximations . because algorithm  [ alg : gptrans ] can be seen as a sigma - point method , analogously to other sigma - point filters considered , for example , in @xcite",
    ", we can now formulate the following sigma - point filter for model , which uses the the unit sigma - points @xmath42 and weights @xmath6 defined by algorithm  [ alg : gptrans ] .",
    "the filtering is started from initial mean and covariance , @xmath24 and @xmath25 , respectively , such that @xmath26 .",
    "then the following prediction and update steps are applied for @xmath27 .",
    "prediction :    1 .",
    "form the sigma points as follows : @xmath98 .",
    "2 .   propagate the sigma points through the dynamic model : @xmath99 .",
    "3 .   compute the predicted mean @xmath100 and the predicted covariance @xmath101 : @xmath102    update :    1 .",
    "form the sigma points : @xmath103 .",
    "2 .   propagate sigma points through the measurement model : @xmath104 .",
    "3 .   compute the predicted mean @xmath105 , the predicted covariance of the measurement @xmath106 , and the cross - covariance of the state and the measurement @xmath107 : @xmath108 4 .",
    "compute the filter gain @xmath109 and the filtered state mean @xmath110 and covariance @xmath111 , conditional on the measurement @xmath112 : @xmath113 , \\\\",
    "{ \\mathbf{p}}_k & = { \\mathbf{p}}^-_k - { \\mathbf{k}}_k \\ , { \\mathbf{s}}_k \\ , { \\mathbf{k}}_k^{\\mathsf{t}}. \\end{split } \\nonumber\\ ] ]    further following the line of thought in @xcite we can formulate a sigma - point smoother using the unit sigma - points and weights from algorithm  [ alg : gptrans ] .",
    "the smoothing recursion is started from the filtering result of the last time step @xmath29 , that is , @xmath30 , @xmath31 and proceeded backwards for @xmath32 as follows .    1 .",
    "form the sigma points : @xmath114 .",
    "2 .   propagate the sigma points through the dynamic model : @xmath115 .",
    "3 .   compute the predicted mean @xmath116 , the predicted covariance @xmath117 , and the cross - covariance @xmath118 : @xmath119 4 .",
    "compute the gain @xmath120 , mean @xmath121 and covariance @xmath122 as follows : @xmath123^{-1 } , \\\\    { \\mathbf{m}}^s_k & = { \\mathbf{m}}_k + { \\mathbf{g}}_k \\ ,      ( { \\mathbf{m}}^s_{k+1 } - { \\mathbf{m}}^-_{k+1 } ) , \\\\    { \\mathbf{p}}^s_k & = { \\mathbf{p}}_k + { \\mathbf{g}}_k \\ ,       ( { \\mathbf{p}}^s_{k+1 } - { \\mathbf{p}}^-_{k+1 } ) \\ , { \\mathbf{g}}_k^{\\mathsf{t}}. \\end{split } \\nonumber\\ ] ]    note that we could cope with non - additive noises in the model by using augmented forms of the above filters and smoothers as in @xcite . the fixed - point and fixed - lag smoothers",
    "can also be derived analogously as was done in the same reference .",
    "the accuracy of the gaussian process quadrature method and hence the accuracy of the filtering and smoothing methods using it is affected by    1 .",
    "the covariance function @xmath84 used and 2 .",
    "the sigma - point locations @xmath42 .",
    "once both of the above are fixed , the weights are determined by equation  . in this section",
    "we discuss certain useful choices of covariance functions as well as `` optimal '' choices of sigma - point locations for them .",
    "we also discuss the connection of the resulting methods with sigma - point methods such as unscented transforms and gauss  hermite quadratures .      in a machine learning context",
    "@xcite the default choice for a covariance function of a gaussian process is the squared exponential covariance function in equation  .",
    "what makes it convenient in gaussian process quadrature context is that the integral required for computing the weights in equation   can be evaluated in closed form ( cf .",
    "it turns out that the posterior variance can be computed in closed form as well which is useful because for a given set of sigma - points we can immediately compute the expected error in the integral approximation ( assuming that the integrand is indeed a gp )  this is possible because the variance does not depend on the observations at all .",
    "one way to determine the sigma - point locations is to select them to minimize the posterior variance of the integral approximation @xcite . in our case",
    "this corresponds to minization of the variance in equation   with respect to the points @xmath124 .",
    "although the minimization is not possible in closed form , with a moderate @xmath125 this optimization can be done numerically .",
    "figure  [ fig : gp_sigmas ] shows examples of minimum variance point sets optimized by using the broyden  fletcher  goldfarb  shanno ( bfgs ) algorithm @xcite .",
    "+    the squared exponential covariance function is not the only possible choice for a covariance function . from the machine learning context we could , for example ,",
    "choose a matrn covariance function or some of the scale - mixture - based covariance functions @xcite .",
    "in that case the weight integral becomes less trivial , but at least we always have a chance to precompute the weights using some ( other ) multivariate quadrature method .",
    "the sigma - point optimization could also be done similarly as for the squared exponential covariance function .",
    "in addition to the squared exponential covariance funtion , another useful class of covariance function are polynomial covariance functions .",
    "they correspond to linear - in - parameters regression using polynomials as the regressor functions .",
    "it turns out that also for polynomial covariance functions we can compute the weights in closed form .",
    "what is even more interesting is that the gaussian process quadratures reduce to classical numerical integration methods . in this section",
    "we show that with certain selections of symmetric evaluation points we get a classical family of spherically symmetric integration methods of mcnamee and stenger @xcite of which the unscented transform @xcite can be ( retrospectively ) seen as a special case @xcite .",
    "more detailed information on the multivariate hermite polynomials used below can be found in appendix  [ sec : fh ] .    [ the : ut3_cov ] assume that @xmath126 where @xmath127 s form a positive definite covariance matrix and @xmath128 are multivariate hermite polynomials ( see appendix  [ sec : fh ] ) .",
    "if we now select the evaluation points as in ut , then the gpq weights @xmath6 become the ut weights . furthermore , the posterior variance of the integral approximation is exactly zero .",
    "the prior @xmath129 with the above covariance is equivalent to a parametric model of the form @xmath130 where @xmath131 are zero mean gaussian random variables with the covariances @xmath132 $ ] . when the joint covariance matrix @xmath133 $ ] is non - singular , the posterior covariance of the integral being zero is equivalent to that the integral rule is exact for all functions of the form with arbitrary coefficients . clearly with the ut evaluations points ,",
    "the ut weights are the unique ones that have this property ( see , e.g. , @xcite ) and hence the result follows .",
    "note that the above result also covers the cubature transform ( ct ) , that is , the moment matching rule used in the cubature kalman filter ( ckf ) and the smoother , because the transform is a special case of ut @xcite .",
    "[ the : utn_cov ] assume that @xmath134 if we select the evaluation points according to order @xmath135 rules in @xcite , we obtain the higher order integration formulas in @xcite , which are often referred to as fifth order , seventh order , ninth order and higher order uts .",
    "the result follows analogously to the 3rd order case above .",
    "let @xmath136 and consider the gpq with ut sigma - points and the covariance function . with @xmath137 and @xmath138",
    "we then obtain the covariance matrix in .",
    "@xmath139    it also turns out that @xmath140 and finally @xmath141 which are indeed the ut weights .",
    "the multivariate gauss ",
    "hermite point sets ( see , e.g. , @xcite ) of order @xmath142 are exact for monomials of of the form @xmath143 , where @xmath144 for @xmath145 .",
    "this implies the following covariance function class .",
    "assume that @xmath146 where @xmath127 s form a positive definite covariance matrix and @xmath128 are multivariate hermite polynomials .",
    "if we now select the evaluation points to form a cartesian product of roots of hermite polynomials of order @xmath142 , then the gpq weights @xmath6 become the multivariate gauss  hermite quadrature weights . the posterior variance of the integral approximation",
    "is again exactly zero .",
    "again the result follows from the equivalence of the polynomial approximations and polynomial covariance functions together with the uniqueness of the gauss ",
    "hermite rule for exact integration of this same function class .    even when we are using polynomial covariance functions , we are by no means restricted to using the specific points sets corresponding to the classical integration rules . however , obviously , given the order of the polynomial kernel and number of sigma - points they are also minimum variance points sets and hence good choices also in average  provided that the integrand is indeed a polynomial . in any case , for an arbitrary set of sigma - points we can use equation   to give the corresponding minimum variance weights .      as discussed in @xcite , the gaussian process quadrature with squared exponential covariance function",
    "also has a strong connection with classical quadrature methods .",
    "this is because we can consider a set of damped polynomial basis functions of the form @xmath147 , which at least informally speaking can be seen to converge to a polynomial basis when @xmath96 .",
    "we can now consider a family of random functions ( gaussian processes ) of the form @xmath148 where @xmath149 .",
    "the covariance function of this class is now given as @xmath150 which is the squared exponential covariance function .",
    "based on the above , minka @xcite argued ( although did not formally prove ) that gpqs with the squared exponential covariance functions should converge to the classical quadratures .",
    "this argument is indeed backed up by our analytical example in example  [ ex : gpt_se ] where this covergence indeed happens .",
    "recall that one way to approximate the expectation of @xmath151 over a gaussian distribution @xmath152 is to use monte carlo integration . in that method",
    "we simply draw @xmath125 samples from the gaussian distribution @xmath153 and use them as sigma - points . the classical monte carlo approximation to the integral",
    "would now correspond to setting @xmath154 .",
    "alternatively , we could use these random points as sigma - points and evaluate their weights by equation  .",
    "this leads to an approximation , which is sometimes called the bayesian monte carlo approximation @xcite .    instead of sampling from the normal distribution",
    ", we can also use quasi - random points sets such as the hammersley point sets @xcite .",
    "these are points sets which are designed to give a smaller error in average than random points .",
    "the classical method would correspond to setting all weights to @xmath154 , but again , we can also use equation   to evaluate the weights for the gp quadrature .",
    "this corresponds to a `` bayesian quasi monte carlo '' approximation to the integral .",
    "some examples of hammersley point sets are shown in figure  [ fig : h_sigmas ] .",
    "+     +    the unscented transform covariance functions of orders 37 ( see theorems [ the : ut3_cov ] and [ the : utn_cov ] ) and the exponentiated quadratic ( i.e. , the squared exponential , se ) covariance function ( eq . )",
    "are illustrated in fig .",
    "[ fig : covmatrix ] .",
    "the polynomial nature of the unscented transform ( ut ) covariance function can be clearly seen in the figures  the ut covariance function as such does not have such a simple local - correlation - interpretation as the se covariance function has as the ut covariance functions simply blow up polynomially when moving away from the diagonal .",
    "the corresponding gaussian process regression results on random data are illustrated in fig .",
    "[ fig : regr ] .",
    "the polynomial nature of the unscented transform can be clearly seen in the figures .",
    "the gaussian process prediction with the unscented transform covariance function has a clear polynomial shape as expected .",
    "clearly the polynomial fit has less flexibility to explain the data than the exponentiated quadratic fit although the flexibility certainly grows with the polynomial ( and thus ut ) order .",
    "+    we use the same test case as in section  viii.a . of @xcite ,",
    "that is , the computation of the first two moments of the function @xmath155 for @xmath156 .",
    "we thus aim to approximate the following integrals : @xmath157 & = & \\int \\left ( \\sqrt{1 + { \\mathbf{x}}^{\\mathsf{t}}{\\mathbf{x } } } \\right)^p \\ , \\operatorname{n}({\\mathbf{x}}\\mid { \\mathbf{m}},{\\mathbf{p } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } , \\label{eq : ara1 } \\\\    \\mathrm{e}[y^2({\\mathbf{x } } ) ] & = & \\int \\left ( 1 + { \\mathbf{x}}^{\\mathsf{t}}{\\mathbf{x}}\\right)^p \\ , \\operatorname{n}({\\mathbf{x}}\\mid { \\mathbf{m}},{\\mathbf{p } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}. \\label{eq : ara2}\\end{aligned}\\ ] ] figure  [ fig : ct ] shows the result of using the following methods as function of the state - dimensionality :    * _ cubature : _ the 3rd order spherical cubature sigma - points ( @xmath158 points ) with the standard integration weights . * _ gpq - cubature : _ the gaussian process quadrature with se covariance function and the 3rd spherical cubature sigma - points above . * _ gpq - hammersley : _ the gaussian process quadrature with se covariance and @xmath158 hammersley points .",
    "the 3rd spherical cubature points refer to the integration rule proposed in @xcite , which was also used in the cubature kalman filter ( ckf ) in @xcite . in the rule ,",
    "the sigma - points of placed to the intersections coordinate axes with the origin - centered @xmath46-dimensional hypersphere of radius @xmath159 .",
    "the results in figure  [ fig : ct ] show that the gpq quite consistently gives a bit lower kl - divergence and hence better result than the plain cubature when the cubature points are used .",
    "when hammersley point sets are used , the results vary a bit more : with small state dimensions the results are slightly worse than with the cubature points .",
    "when @xmath160 , the hammersley results are much better in high dimensions whereas with @xmath161 the results are worse than with the cubature point sets .      in this section",
    "we compare the performance of the different methods in the following univariate non - linear growth model ( ungm ) which is often used for benchmarking non - linear estimation methods : @xmath162 where @xmath163 , @xmath164 , and @xmath165 .",
    "we generated 100 independent datesets with 500 time step each and applied the following methods to it : extended , unscented ( @xmath166 ) , and cubature filters and smoothers ( ekf / ukf / ckf / erts / urts / crts ) ; gauss  hermite filters and smoothers with 3 , 7 , and 10 points ( ghkf / ghrts ) ; gaussian process quadrature filter and smoother with unscented transform points ( gpkfu / gprtsu ) and cubature points ( gpkfc / gprtsc ) ; with hammersley point sets of sizes 3 , 7 , and 10 ( gpkfh / gprtsh ) ; and with minimum variance points sets of sizes 3 , 7 , and 10 ( gpkfo / gprtso ) .",
    "the covariance function was the exponentiated quadratic with @xmath167 and @xmath168 and the noise variance was set to @xmath169 .",
    "the rmse results together with single standard derivation bars are shown in figures  [ fig : ungm_rmse_filters ] and [ fig : ungm_rmse_smoothers ] . as can be seen in the figures , with 5 and 10 points the gaussian process quadrature based filters and smoothers have significantly lower errors than almost all the other methods  only gauss  hermite with 10 points and the cubature rts smoother come close .              in this section",
    "we evaluate the methods in the bearings only target tracking problem with a coordinated - turn dynamic model , which was also considered in section iii.a of the article @xcite .",
    "the non - linear dynamic model is @xmath170 where the state of the target is @xmath171 , and @xmath172 are the coordinates and @xmath173 are the velocities in two dimensional space . the time step size is set to @xmath174 and the covariance of the process noise @xmath175 is @xmath176 where we used @xmath177 and @xmath178 .    in the simulation setup",
    "we have four sensors measuring the angles @xmath179 between the target and the sensors .",
    "the non - linear measurement model for sensor @xmath50 can be written as @xmath180 where @xmath181 is the position of the sensor @xmath50 in two dimensions , and @xmath182 is the measurement noise .",
    "the used parameters were the same as in the article @xcite .",
    "the rmse results for the position errors are shown in figures [ fig : botm_rmse_filters ] and [ fig : botm_rmse_smoothers ] .",
    "clearly all of the sigma - point methods outperform the taylor series based methods ( ekf / eks ) .",
    "however , the performances of all the sigma - point methods are very similar : also the gaussian process quadrature methods give very similar results to the other sigma - point methods .",
    "there is a small dip in the errors at the gauss ",
    "hermite based methods as well as in the highest order hammersley gpq method , but practically the performance of all the sigma - point methods is the same .",
    "in this article we have proposed new gaussian process quadrature based non - linear kalman filtering and smoothing methods and analyzed their relationship with other sigma - point filters and smoothers .",
    "we have also discussed the selection of the evaluation points for the quadratures with respect to different criteria : exactness for multivariate polynomials up to a given order , minimum average error , and quasi - random point sets .",
    "we have shown that with suitable selections of ( polynomial ) covariance functions for the gaussian processes the filters and smoothers reduce to unscented kalman filters of different orders as well as to gauss ",
    "hermite kalman filters and smoothers . by numerical experiments",
    "we have also shown that the gaussian process quadrature rules as well as the corresponding filters and smoothers often outperform previously proposed ( polynomial ) integration rules and sigma - point filters and smoothers .",
    "fourier  hermite series ( see , e.g. , @xcite ) are orthogonal polynomial series in a hilbert space , where the inner product is defined via an expectation of a product over a gaussian distributions .",
    "these series are also inherently related to non - linear gaussian filtering as they can be seen as generalizations of statistical linearization and they also have a deep connection with unscented transforms , gaussian quadrature integration , and gaussian process regression @xcite .",
    "we can define an inner product of multivariate scalar functions @xmath183 and @xmath62 as follows : @xmath184 if we now define a norm via @xmath185 , and the corresponding distance function @xmath186 , then the functions @xmath187 form a hilbert space @xmath188 .",
    "it now turns out that the multivariate hermite polynomials form a complete orthogonal basis of the resulting hilbert space @xcite .",
    "a multivariate hermite polynomial with multi - index @xmath189 can be defined as @xmath190 which is a product of univariate hermite polynomials @xmath191 the orthogonality property can now be expressed as @xmath192 where we have denoted @xmath193 and @xmath194 means that each of the elements in the multi - indices @xmath195 and @xmath196 are equal .",
    "we will also denote the sum of indices as @xmath197 .",
    "a function @xmath62 with @xmath198 can be expanded into fourier ",
    "hermite series @xcite @xmath199 where @xmath200 are multivariate hermite polynomials and the series coefficients are given by the inner products @xmath201 .",
    "consider a gaussian process @xmath202 which has zero mean and a covariance function @xmath56 . in the same way as deterministic functions",
    ", gaussian processes can also be expanded into fourier ",
    "hermite series : @xmath203 where the coefficients are given as @xmath204 .",
    "the coefficients @xmath205 are zero mean gaussian random variables and their covariance is given as @xmath206    & = \\operatorname{e}\\left[\\langle h_{\\mathcal{i } } , g_g \\rangle \\ ,          \\langle h_{\\mathcal{j } } , g_g \\rangle\\right ] \\\\    & =     \\iint h_{\\mathcal{i}}({\\mathbf{x } } ) \\ ,     k({\\mathbf{x}},{\\mathbf{x } } ' ) \\ , h_{\\mathcal{j}}({\\mathbf{x } } ' )     \\\\ & \\qquad \\times         \\operatorname{n}({\\mathbf{x}}\\mid \\mathbf{0},{\\mathbf{i } } ) \\ ,         \\operatorname{n}({\\mathbf{x } } ' \\mid \\mathbf{0},{\\mathbf{i } } ) \\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x}}\\ , { \\mathop{}\\!\\mathrm{d}}{\\mathbf{x } } ' . \\end{split}\\ ] ] if we define constants @xmath207 $ ] then the covariance function @xmath56 can be further written as series @xmath208            m.  a. osborne , r.  garnett , s.  j. roberts , c.  hart , s.  aigrain , n.  p. gibson , and s.  aigrain , `` bayesian quadrature for ratios : now with even more bayesian quadrature , '' in _ international conference on artificial intelligence and statistics ( aistats 2012 ) _ , 2012 .",
    "m.  osborne , d.  duvenaud , r.  garnett , c.  rasmussen , s.  roberts , and z.  ghahramani , `` active learning of model evidence using bayesian quadrature , '' in _ advances in neural information processing systems 25 _ , 2012 , pp .",
    "4654 .",
    "s.  j. julier and j.  k. uhlmann , `` a general method of approximating nonlinear transformations of probability distributions , '' robotics research group , department of engineering science , university of oxford , tech .",
    "rep . , 1995 .",
    "s.  j. julier , j.  k. uhlmann , and h.  f. durrant - whyte , `` a new method for the nonlinear transformation of means and covariances in filters and estimators , '' _ ieee transactions on automatic control _ , vol .",
    "45(3 ) , pp .",
    "477482 , march 2000 .      r.  van  der  merwe , `` sigma - point kalman filters for probabilistic inference in dynamic state - space models , '' ph.d .",
    "dissertation , ogi school of science & engineering , oregon health & science university , portland , or , usa , april 2004 .",
    "m.  deisenroth , r.  turner , m.  huber , u.  hanebeck , and c.  rasmussen , `` robust filtering and smoothing with gaussian processes , '' _ ieee transactions on automatic control _ , vol .",
    "57 , no .  7 , pp . 18651871 , 2012 .",
    "s.  srkk , `` linear operators and stochastic partial differential equations in gaussian process regression , '' in _ artificial neural networks and machine learning ",
    "icann 2011_.1em plus 0.5em minus 0.4emspringer , 2011 , pp .",
    "151158 .",
    "m.  alvarez and n.  d. lawrence , `` sparse convolved gaussian processes for multi - output regression , '' in _ advances in neural information processing systems 21 _ , d.  koller , d.  schuurmans , y.  bengio , and l.  bottou , eds .",
    "1em plus 0.5em minus 0.4emthe mit press , 2009 , pp . 5764 .",
    "m.  alvarez , d.  luengo , m.  k. titsias , and n.  d. lawrence , `` efficient multioutput gaussian processes through variational inducing kernels , '' in _ proceedings of the 13th international workshop on artificial intelligence and statistics _ , y.  w. teh and m.  titterington , eds . , 2010 , pp ."
  ],
  "abstract_text": [
    "<S> this article is concerned with gaussian process quadratures , which are numerical integration methods based on gaussian process regression methods , and sigma - point methods , which are used in advanced non - linear kalman filtering and smoothing algorithms . </S>",
    "<S> we show that many sigma - point methods can be interpreted as gaussian quadrature based methods with suitably selected covariance functions . </S>",
    "<S> we show that this interpretation also extends to more general multivariate gauss  </S>",
    "<S> hermite integration methods and related spherical cubature rules . </S>",
    "<S> additionally , we discuss different criteria for selecting the sigma - point locations : exactness for multivariate polynomials up to a given order , minimum average error , and quasi - random point sets . </S>",
    "<S> the performance of the different methods is tested in numerical experiments . </S>"
  ]
}