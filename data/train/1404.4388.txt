{
  "article_text": [
    "the intent of this research is to provide decision support to a _ leader _ , an agent that wishes to determine a policy that will select actions to best control a sequential stochastic system over an infinite planning horizon , given that there is a _ follower _ , an agent that also would like to exert control over the system for its own purposes .",
    "we assume :    * at each decision epoch , each agent knows : its past and present states , its past actions , and noise corrupted observations of the other agent s past and present states .",
    "* the leader s and follower s actions are selected simultaneously at each decision epoch .",
    "* each agent s policy selects actions based on data currently available to the agent . *",
    "the follower knows the leader s policy and determines a response policy that is optimal with respect to the follower s objective . *",
    "the leader considers multiple objectives in selecting its policy .",
    "our objective is to find a set of non - dominated policies from which the leader can select its most preferred policy .",
    "such information can serve as input to a decision support system that , for example , is based on a deterministic version of multi - attribute utility theory ( keeney & raiffa , 1993 ; holloway & white , 2008 ) . in this context , the results presented in this paper generate options ( i.e. , policies ) for consideration by ( 1 ) creating multiple generations of policies and eliminating all but the non - dominated set of policies from the last generation and ( 2 ) determining value scores for each of the policies in this non - dominated set .",
    "we remark that the assumption that the follower knows the policy of the leader is a conservative assumption from the perspective of the leader and could unrealistically bias the game to the advantage of the follower .",
    "however , this bias is mollified by the fact that the leader and follower do not share the same data at each decision epoch , and hence the follower can only infer what action the leader will actually take .",
    "this assumption is also reasonable for many applications .",
    "for example , if the leader is a large governmental agency or corporation and the follower is an individual or group intent on attacking the leader , then it may be reasonable to assume that the follower will know more about the leader than the leader will know about the follower .",
    "further , the assumption that the follower knows the policy of the leader allows us to transform the markov game that we use to model leader - follower interaction into a model of sequential decision making under uncertainty and hence to take advantage of this computationally useful transformation .",
    "the motivating application of this research is the operation of a liquid egg production facility in order to maximize the supply chain s productivity while minimizing its vulnerability to the intentional insertion of a biological or chemical toxin into the food production and distribution system ( see manning , 2005 ; oryan , 1996 ; sobel , 2002 ) . for this application",
    ", we assume the leader manages the production facility and is trying to balance two objectives : ( 1 ) maximize productivity and ( 2 ) minimize vulnerability .",
    "see mohtadi & murshid ( 2009 ) for background information about this application area .",
    "although initially developed to model this application , we remark that the decision support process to be presented can model a particularly broad class of sequential game applications , when all agents are intelligent and adaptive .",
    "many of the methodological characteristics of the decision support model presented in this paper have been considered elsewhere in the decision , risk , and reliability analysis literatures .",
    "models of intelligent agents or adversaries are examined by cardoso & diniz ( 2009 ) .",
    "the single - period leader - follower game has been widely used to analyze the strategic interactions between two intelligent and adaptive agents .",
    "for example , cavusoglu et al . (",
    "2013 ) have studied the impacts of passenger profiling on airport security operations .",
    "bakir ( 2011 ) analyzed resource allocation for cargo container transportation security .",
    "other applications of the single - period leader - follower game are presented in bier ( 2007 , 2008 ) and zhuang & bier ( 2007 ) .",
    "multi - period games have been considered by wang & bier(2011 ) , who examined a two - period leader - follower repeated game , and by hausken & zhuang ( 2011 ) , who studied a multi - period game with myopic agents .",
    "application of completely observable stochastic game to overseas cargo container security can be found in bakir & kardes ( 2009 ) to capture the state dynamics over time .",
    "models that consider incomplete or uncertain information are presented and analyzed by mclay ( 2012 ) , rothschild ( 2012 ) and wang & bier ( 2011 ) .",
    "each of above methodological characteristics is intended to enhance the realism of the respective model .",
    "our model extends the existing literature on sequential games by explicitly considering the multi - period interaction of two non - myopic agents , each of whom adjust its decisions according to the other agent s decisions over an infinite planning horizon .",
    "furthermore , our model considers the case where neither agent has complete information about the other agent . by combining these characteristics into a single model , as has been done in this paper ,",
    "we believe that the modeling realism of the resulting model has been further enhanced . however , and not surprisingly , additional model realism has resulted in increased computational challenges . dealing with these challenges is the focus of much of this paper .",
    "our approach to decision support is described as follows .",
    "we begin with an initial ( i.e. , first generation ) set of possible leader policies .",
    "we then use a multi - objective genetic algorithm ( moga ) to create successive generations of leader policies .",
    "presumably , the next generation of leader policies contains , in some sense , higher quality policies than the current generation .",
    "we then determine the non - dominated set of the last generation of leader policies determined and present this set to the leader .",
    "the leader can then select the most preferred policy from this set for implementation .    mimicking the process of natural evolution ,",
    "the moga creates the next generation of policies from the current set based on the fitness measures of each of the policies in the current set .",
    "each fitness measure is related to an objective of the leader .",
    "we model the interaction between the leader and follower as a partially observed markov game ( pomg ) .",
    "our pomg is a version of the partially observed stochastic game ( posg ) where the state dynamics possess the markovian property .",
    "we assume that the follower is aware of the policy that the leader has selected and makes use of this fact in constructing the follower s policy .",
    "thus at the policy level , the game is a leader - follower ( stackelberg ) game .",
    "this assumption allows the pomg to be converted into a partially observed markov decision process ( pomdp ) .",
    "the fitness measures for each leader policy in the current generation are needed by the moga to create the next generation of leader policies from the current generation .",
    "these fitness measure are computed by a value determination procedure , given the leader policy and the follower response policy .",
    "the paper is organized as follows .",
    "we review the pertinent literature associated with the moga , posg , and pomdp in section 2 . in section 3 , we describe the moga in more detail , show how the fitness measures are computed using the pomg and the pomdp , present equilibrium conditions , and discuss the value of information .",
    "the numerical evaluation in section 4 applies this decision support procedure to a simplified liquid eggs supply chain security problem and analyses the value of information for the agents .",
    "section 5 summarizes research results and discusses future research directions .",
    "the research presented in this paper combines and extends results associated with pomdp , posg , and moga .",
    "we now review the pertinent literature in these three areas of research .",
    "the pomdp is a model of sequential decision making under uncertainty that takes into consideration noise corrupted and/or costly observations of the state of the system under control .",
    "relative to the completely observed markov decision process ( i.e. , the mdp ; see puterman , 1994 ) , the pomdp represents a more general but significantly more computationally challenging model . in seminal research , smallwood and",
    "sondik ( 1973 ) and sondik ( 1978 ) showed that under robust conditions the optimal cost function for both the finite horizon and infinite horizon expected total discounted cost criterion pomdps is piecewise linear and convex , and presented successive approximations approaches for solving these pomdps that exploited this structure .",
    "zhang ( 2010 ) revisited these structural results and convergence properties by exploiting the dual relationship between hyperplanes and points in the pomdp and related the solution of the pomdp to the minkowski sum problem in computational geometry .",
    "monahan ( 1982 ) , eagle ( 1984 ) , and white and scherer ( 1989 ) presented improved algorithms based on the structural results .",
    "detailed descriptions of other exact algorithms can be found in cheng ( 1988 ) , littman ( 1994b ) , cassandra ( 1994a ) , cassandra , littman and zhang ( 1997 ) , feng and zilberstein ( 2004 ) , lin and white ( 1998 , 2004 ) and naser - moghadasi ( 2012 ) .",
    "surveys of related solution techniques and complexity analyses for the pomdp can be found in monahan ( 1982 ) , lovejoy ( 1991 ) , white ( 1991 ) , cassandra ( 1994b ) and poupart ( 2005 ) .    in the development of approximate solution techniques for pomdps ,",
    "point - based value iteration ( pbvi ) is presented and analysed in pineau ( 2003 ) and shani ( 2012 ) .",
    "platzman ( 1977 , 1980 ) , white and scherer ( 1994 ) , littman ( 1994a ) , hauskrecht ( 1997 ) , hansen ( 1998a , 1998b ) , poupart ( 2005 ) examined finite memory policy and finite - state controllers .",
    "varakantham ( 2007 ) and poupart ( 2011 ) focused on calculating bounds on optimal pomdp solutions in order to evaluate the quality of approximate solutions .",
    "surveys of approximation methods for pomdps can be found in hauskrecht ( 2000 ) , aberdeen ( 2003 ) , and yu ( 2007 ) .",
    "approximate algorithms have proved useful for large - scale problems ( hoey , 2010 ; thomson and young , 2010 ) .",
    "the stochastic game introduced by shapley ( 1953 ) represents a multi - agent planning problem in a stochastic environment . in this",
    "setting , each player considers the consequences of its own action and the actions that its opponents or teammates may take .",
    "see raghavan and filar ( 1991 ) , filar ( 1997 ) and ummels ( 2010 ) for details .",
    "the posg is a new , relatively unexamined generalization of the stochastic game , where the states of the game are not precisely observed by the players and all players make decisions based on these partial observations . although posgs provide a robust framework for multi - agent planning , bernstein ( 2002 ) showed that posgs are computationally intractable when problem size grows .",
    "rabinovich ( 2003 ) has shown that even epsilon - optimal approximations are np - hard . as a result ,",
    "posgs with special structures that enhance computational tractability are of considerable interest .",
    "koller ( 1994 ) provided an efficient algorithm for a two - player posg with tree - like structure .",
    "mceneaney ( 2004 ) focused on a game where only one player has imperfect information .",
    "ghosh ( 2004 ) , oliehoek ( 2005 ) , and bopardikar ( 2011 ) studied a zero - sum version of the posg .",
    "emery - montemerlo ( 2004 ) approximated posgs with common payoffs by a series of bayesian games .",
    "a cooperative version of the posg , called a decentralized partially observable markov decision process ( dec - pomdp ) , has been studied by becker ( 2004 ) , bernstein ( 2005 ) , seuken ( 2007 ) , and oliehoek ( 2008 ) .",
    "a survey of the dec - pomdp can be found in oliehoek ( 2012 ) .    it is often the case for real - world planning problems that the players payoffs are neither completely aligned with others nor directly opposed .",
    "hespanha and prandini ( 2001 ) proved the existence of nash equilibrium in a two - player finite - horizon posg .",
    "hansen ( 2004 ) developed a dynamic program for general posgs by pruning very weakly dominated strategies and then showed that this dynamic programming approach can achieve optimality for cooperative settings .",
    "however , this approach is computationally infeasible for all but the smallest problems .",
    "kumar and zilberstein ( 2009 ) developed an approximate solution procedure for the posg based on hansen s work .",
    "interactive pomdps addressed in gmytrasiewicz and doshi ( 2005 ) demonstrated another framework for multi - agent planning .",
    "genetic algorithms , introduced by holland ( 1975 ) , are adaptive heuristic search techniques that mimic the process of natural evolution .",
    "a genetic algorithm represents each feasible problem solution in a population of solutions as a genome or chromosome , and begins with an initial population of feasible solutions .",
    "solutions having high measures of fitness are preferably selected during each generation to produce the next generation of solutions having improved fitness measures by applying genetic ( e.g. , mutation and crossover ) operators . after a number of generations",
    ", the population presumably evolves to optimal or near - optimal solutions .",
    "goldberg ( 1989 ) , forrest ( 1993 ) and srinivas(1994 ) present surveys of genetic algorithms and the theories .",
    "multi - objective genetic algorithms ( moga ) are designed for the simultaneous optimization of multiple , often competing objectives .",
    "usually the optimal solutions are a set of points , called the pareto - optimal set , in the sense that no improvement can be made in any objective without sacrificing the other objectives .",
    "moga pushes the pareto frontier towards the ideal optimal set of solutions as the algorithm proceeds .",
    "moga algorithms include : the vector evaluated ga ( vega ) ( schaffer , 1985 ) , the niched pareto ga ( npga ) ( horn , 1994 ) , the pareto envelope - based selection algorithms ( pesa ) ( corne , 2000 ) and the fast non - dominated sorting ga ( nsga - ii ) ( deb , 2002 ) .",
    "surveys are presented in coello ( 2000 ) and konak ( 2006 ) .",
    "mogas have been widely applied in optimization and decision making problems ( see ponnambalam , 2001 ; deb , 2001 ; ombuki , 2006 ; lin , 2008 ; bowman , 2010 ; yildirim , 2012 ) .",
    "we first present the pomg model in section 3.1 . in order to determine an optimal response policy for the follower",
    ", the pomdp is constructed by combining the pomg model with any leader s policy .",
    "the resulting pomdp is presented and examined in section 3.2 . for computational reasons , we require that the leader and follower policies be finite memory policies",
    ". however , the pomdp constructs a perfect memory follower policy . in section 3.3 ,",
    "we present an approach for determining a finite - memory approximation of a perfect memory policy . in order for the moga to determine the next generation of leader policies ,",
    "fitness measures must be calculated for each policy in the current generation of leader policies .",
    "each fitness measure is associated with an objective of the leader . in section 3.4",
    "we present an approach for determining the fitness measures , for any given leader policy and follower policy .",
    "section 3.5 shows how to use moga to generate a non - dominate set of leader policies .",
    "section 3.6 and 3.7 address equilibria and the value of information , respectively .",
    "the partially observed markov game ( pomg ) serves as the modeling basis of our decision support system design .",
    "the pomg is comprised of :    _ decision epochs : _ let @xmath0 be the set of all decision epochs when both agents select actions simultaneously .",
    "thus , the problem horizon is countable and infinite .    _ state spaces : _ let @xmath1 and @xmath2 be the state spaces of the leader and the follower , respectively .",
    "both spaces are epoch - invariant and finite . at decision epoch t ,",
    "let @xmath3 be the leader s state , @xmath4 be the follower s state , and denote @xmath5 .",
    "_ action spaces : _ let @xmath6 and @xmath7 be the epoch - invariant action spaces of the leader and the follower , both of which are finite . at decision epoch @xmath8 ,",
    "let @xmath9 be the leader s action , @xmath10 be the follower s action , and denote @xmath11 .    _",
    "observation spaces : _ let @xmath12 and @xmath13 be the observation spaces of the leader and the follower , both of which are epoch - invariant and finite",
    ". at decision epoch @xmath8 , let @xmath14 be the follower s observation of the leader s state , @xmath15 the leader s observation of the follower s state , and denote @xmath16 .",
    "systems dynamics : _ we assume the epoch - invariant probability @xmath17 is given . note that @xmath18 where @xmath19 is referred to as the state observation probability and @xmath20 is referred to as the state transition probability .",
    "_ information patterns : _ the information pattern for agent @xmath21 describes what agent @xmath22 knows and when agent @xmath22 knows it .",
    "let    1 .",
    "@xmath23 and @xmath24 2 .",
    "@xmath25 and @xmath26 3 .",
    "@xmath27 and @xmath28 4 .",
    "@xmath29 5 .",
    "@xmath30    we assume that agent @xmath22 chooses @xmath31 on the basis of @xmath32 , if agent @xmath22 has perfect memory , or on the basis of @xmath33 , if agent @xmath22 has finite memory . note that @xmath34    _ single period cost and criteria : _ let @xmath35 be the decision epoch invariant single period cost accrued by the follower at epoch @xmath8 , given @xmath36 and @xmath37 , and let @xmath38 be the decision epoch invariant single period cost accrued by the leader with respect to criterion @xmath39 at epoch @xmath8 , given @xmath36 and @xmath37 .",
    "the criteria under consideration are the concomitant expected total discounted costs over the infinite horizon .    _",
    "policies : _ a policy @xmath40 for agent @xmath22 is a mapping from what agent @xmath22 knows at epoch @xmath8 , either @xmath32 or @xmath33 , into its set of available actions , @xmath41 .",
    "policies can be random and hence described by conditional probabilities .",
    "we restrict our interest to stationary policies .",
    "stationary policies tend to be easy to implement and in many cases , e.g. , the determination of optimal follower response policies for a broad class of scalar criteria , sufficiently rich to contain an optimal policy .",
    "_ objectives : _ the follower s objective is to select a stationary policy that minimizes its criterion . the leader s objective is to optimize all criteria under consideration in some balanced manner , with this balance being determined by the leader . our objective is to provide the leader with a non - dominated set of policies from which to choose a single policy for implementation .",
    "let @xmath42 be the _ perfect _ memory best response policy to the leader policy @xmath43 .",
    "we assume that the leader policy @xmath44 and that the follower knows @xmath43 .",
    "we also assume that the follower knows @xmath45 .",
    "( we will restrict the follower s information pattern to @xmath46 below ; however , for the moment it will be convenient to assume that the follower has perfect memory . )",
    "we remark that although the follower knows the leader s policy , because the information patterns of the agents are in general different , the follower can only infer what action the leader will actually take .",
    "let @xmath47 be the follower s optimal criterion value , given @xmath45 and @xmath43",
    ". for notational simplicity , assume the dependence of @xmath43 is implicit ; hence , @xmath48 .",
    "then , according to results in ( puterman , 1994 ; chapter 6 ) , @xmath49 uniquely satisfies @xmath50 where for any @xmath51 , @xmath52(\\mathscr{i}^f(t ) ) = \\min_{a^f(t ) } h^f(\\mathscr{i}^f(t ) , a^f(t ) , v),\\ ] ] @xmath53 and where @xmath54 is the expectation operator and the minimum is over all @xmath10 .",
    "we now state our first result .",
    "[ prop1 ] assume @xmath43 is given .",
    "then for each @xmath4 there is an at most countable set of arrays @xmath55 such that :    @xmath56    where the sum is over all @xmath57 .",
    "assume @xmath51 and @xmath58 are such that @xmath59 where the sum is over all @xmath60",
    ". then straightforward analysis , following arguments in ( smallwood and sondik , 1973 ) , shows that @xmath61    where the sum is over all @xmath57 , where if @xmath62 then @xmath63 is of the form @xmath64,\\end{aligned}\\ ] ] where @xmath65 can be any element in @xmath66 for each @xmath67 and @xmath68 .",
    "then , @xmath52(\\mathscr{i}^f(t))=\\min \\{\\sum_{\\mathscr{i}^l(t,\\tau ) } \\gamma''(\\mathscr{i}^l(t,\\tau))p(\\mathscr{i}^l(t,\\tau)|i^f(t ) ) : \\gamma '' \\in \\gamma''(s^f(t))\\},\\ ] ] where @xmath69    the operator @xmath70 is a contraction operator on the banach space comprised of all functions mapping @xmath45 into the real line , having as its norm the supremum norm , and as a result , the sequence @xmath71 , where @xmath72 , converges to @xmath49 for any given @xmath73 .",
    "the above result indicates that @xmath70 preserves piecewise linearity and concavity and in the limit preserves concavity .",
    "we remark that @xmath74 usually contains many redundant vectors , where @xmath75 is redundant if @xmath76(\\mathscr{i}^f(t))$ ] is strickly less than @xmath77 for all @xmath78 . from both storage and computational perspectives",
    ", there is a value to keep the cardinality of @xmath74 as small as possible .",
    "let the operator * purge * be such that @xmath79 is the subset of @xmath74 having the smallest cardinality that satisfies @xmath80 for all @xmath78 .",
    "related discussion about the necessity and the existence of the * purge * operator can be found in lin and white ( 1998 ) .    with regard to the implications of proposition [ prop1 ] and results in ( puterman , 1994 ; chapter 6 ) , @xmath49 and hence an optimal policy",
    "can depend on @xmath45 only through @xmath81 , where @xmath82 .",
    "hence , @xmath81 is a sufficient statistic .",
    "further , @xmath49 is concave in @xmath83 . additionally , if @xmath84 is a finite set of arrays for all @xmath4 , then @xmath49 is piecewise linear .",
    "note that the dimension of @xmath81 is finite and @xmath8-invariant .",
    "note further that the finite dimensionality of @xmath83 follows directly from the finite - memory assumption imposed on @xmath43 .",
    "thus , assuming @xmath84 is a finite set of arrays and @xmath49 is described in terms of @xmath81 , @xmath49 has a finite representation .",
    "we remark that the cardinality of @xmath85 can be substantially larger than the cardinality of @xmath86 , where both @xmath86 and @xmath85 are defined in the proof of proposition [ prop1 ] .",
    "techniques for reducing the cardinality of @xmath85 can be found in white ( 1991 ) .      as noted above , an optimal policy @xmath42 for the follower that achieves the minimum in equation 1 depends on @xmath45 and",
    "hence @xmath42 is a perfect - memory policy . in order to insure that the leader criteria have finite representation ,",
    "the follower policy must also be a finite - memory policy .",
    "we determine a finite - memory ( approximate ) policy from a given perfect memory policy as follows .",
    "we note that @xmath87 is also a sufficient statistic for this problem , with @xmath88 representing the influence of data determined up through epoch @xmath89 . by ( arbitrarily ) assuming a uniform distribution over @xmath88 , we determine probabilities of the form @xmath90 .",
    "our numerical analyses indicate that these finite memory approximations of optimal perfect memory policies can be remarkably accurate , even for small @xmath91 .",
    "identifying and analyzing other approaches for determining a finite - memory policy from a perfect memory policy is a topic for future consideration . in the following context",
    ", we use @xmath92 to denote a _ finite _ memory best response policy and @xmath42 to represent a _",
    "memory best response policy of the follower .",
    "we remark that an alternative approach for directly determining a finite - memory follower policy in response to a given leader policy involves determining @xmath49 as a function of @xmath46 , rather than as a function of @xmath45 . whether or not such an approach could be useful is a topic of future research ; see platzman ( 1977 ) and white ( 1994 ) for related discussion .      let @xmath93 be the criterion value for the leader s @xmath94 criterion , given @xmath95 , a leader policy @xmath44 , and a follower policy @xmath96 . for notational simplicity ,",
    "we assume that the dependence of @xmath93 on @xmath97 is implicit ; hence , @xmath98 .",
    "then according to results in ( puterman , 1994 ; chapter 6 ) , @xmath99 uniquely satisfies @xmath100 for all @xmath95 , where @xmath101    we now show that @xmath102 is dependent on @xmath95 only though @xmath103 , where the array @xmath104 .",
    "thus , @xmath103 is a sufficient statistic .",
    "[ prop2 ] assume @xmath97 are given as two finite - memory policies .",
    "then , there is a function @xmath105 such that @xmath106 where the sum is over all @xmath107 .",
    "further , @xmath105 is the unique solution of the equation @xmath108p(z(t+1),s(t+1)|s(t),a(t ) ) \\},\\end{aligned}\\ ] ] where @xmath109 , @xmath110 is over all @xmath36 and @xmath37 , and @xmath111 is over all @xmath112 and @xmath113 .",
    "we remark that since @xmath97 is assumed given , @xmath114 is well defined .",
    "assume there is a function @xmath115 such that @xmath116 where the sum is over all @xmath107 .",
    "then it is straightforward to show that there is a function @xmath117 such that @xmath118 where the sum is over all @xmath107 , and @xmath119p(z(t+1),s(t+1)|s(t),a(t ) ) \\},\\end{aligned}\\ ] ] and where @xmath109 , @xmath110 is over all @xmath36 and @xmath37 , and @xmath111 is over all @xmath112 and @xmath113.the result follows directly from the following facts :    1 .",
    "the operator @xmath120 , where @xmath121(\\mathscr{i}^l(t ) ) = h^l_i(\\mathscr{i}^l(t ) , v)$ ] , is a contraction operator on the banach space comprised of all functions mapping @xmath95 into the real line , having as its norm the supremum norm .",
    "2 .   as a result , the sequence @xmath71 , where @xmath122 , converges to @xmath123 for any given @xmath73 .    since both @xmath43 and @xmath92 are finite - memory policies , then both @xmath57 and @xmath124 are @xmath8-invariant arrays of finite dimension , which enhances the potential computability of @xmath123.we remark that proposition [ prop2 ] holds for any given finite memory leader policy @xmath125 and follower policy @xmath126 , and hence @xmath126 is not necessarily a response policy to @xmath125 .",
    "we now summarize how fitness measures are determined for a given finite - memory leader policy :    1 .",
    "step 1 : determine a _ perfect _ memory follower response policy that achieves the minimum expected cost for the follower , using proposition [ prop1 ] .",
    "2 .   step 2 : approximate the resulting perfect - memory follower response policy by a _",
    "finite_-memory policy .",
    "3 .   step 3 : given the leader policy and the follower s approximate response policy , determine the concomitant fitness measures using proposition [ prop2 ] .",
    "we now describe how we use a multi - objective genetic algorithm ( moga ) , nsga - ii ( deb , 2002 ) , to generate policies from which the leader will choose a most preferred policy .",
    "let @xmath127 be the current population of the leader s finite memory policies , and for each @xmath128 , let @xmath129 be the finite memory follower s response policy to the leader s policy @xmath130 .",
    "further , let @xmath131 be the expected cost of the leader s @xmath94 criterion , given the policy pair @xmath132 .",
    "policy @xmath43 is said to _ dominate _",
    "policy @xmath125 if @xmath133 and there exists at least one @xmath39 such that @xmath134 where @xmath92 and @xmath126 are the follower s response policies to the leader s policy @xmath43 and @xmath125 , respectively .",
    "policy @xmath43 is said to be _ non - dominated _ if there does not exist a policy that dominated policy @xmath43 .",
    "the moga constructs the next generation of policies from the current set of policies as follows .",
    "the moga encodes each policy @xmath130 into a chromosome .",
    "a gene is an element of the chromosome vector , and an allele is a numerical value taken by a gene . in the context of our model",
    ", the chromosome is a probability mass vector over the action space , and the @xmath94 gene of the chromosome denotes the probability that action @xmath39 is selected by the leader .",
    "the moga then determines @xmath135 for each chromosome , where @xmath136 is the number of leader s objectives .",
    "a description of how @xmath99 is determined can be found in section 3.4 .",
    "the tuple @xmath137 serves as the fitness measure of this chromosome .    on the basis of @xmath137 ,",
    "the population of chromosomes are partitioned into subsets called fronts , where front @xmath138 is the set of non - dominated chromosomes , and front @xmath139 is the set of non - dominated chromosomes when the chromosomes in fronts @xmath138 through @xmath22 are removed from consideration , @xmath140 .",
    "chromosomes in front @xmath22 are given rank @xmath22 .",
    "in addition , the crowding distance of each chromosome is determined within each front . crowding distance",
    "is defined as the average euclidean distance of a chromosome to the other chromosomes in the front , based on @xmath137 as a measure of position .",
    "crowding distance is considered a measure of diversity for the policies , and for this measure , larger is considered better . the current generation of policies is sorted according to ranking and crowding distance .",
    "parents are selected from the current generation of policies , based on their ranks and crowding distances .",
    "chromosomes with higher rank and larger crowding distance are selected to generate offspring with higher probability .",
    "the selected parents form a mating pool and generate offspring using a crossover operator .",
    "for each parents pair , the crossover operator randomly exchanges a portion of genes with each other to form two new offspring .",
    "a mutation operator is also used to maintain genetic diversity from one generation to another .",
    "this operator randomly alters a certain percentage of genes in the current generation of policies .",
    "then the non - dominated sorting procedure is applied again on the current population and offspring population , the top m ( population size ) chromosomes are kept and this is the next population .",
    "the whole algorithm repeats for a certain number of iterations .",
    "we remark that there are two equilibrium conditions , one associated with each agent .",
    "with respect to the follower , assume @xmath42 is the _ perfect _ memory response policy to a given leader policy @xmath43 .",
    "then , results in proposition [ prop1 ] imply that @xmath141 for all follower policies @xmath126 and all @xmath45 , where a direct application of the results of proposition [ prop2 ] can be used to determine @xmath142 for any pair of leader - follower policies @xmath143 .    with respect to the leader",
    ", we now assume @xmath92 is a _ finite - memory approximation _ of the perfect memory follower s response policy to the given leader policy @xmath43 .",
    "let @xmath144 be the vector of criterion values for the leader s multiple objectives , given @xmath97 , and information state @xmath95 .",
    "our process of determining candidate leader policies from which the leader can select the most preferred policy is intended to determine @xmath97 pairs so that there exists no pair @xmath143 such that @xmath145 for all @xmath95 , where @xmath125 represents any leader policy and @xmath126 represents a finite - memory approximation of the perfect memory follower response policy to @xmath125 .",
    "we note that by proposition 1 and results in ( puterman , 1994 , chapter 6 ) , the follower policy in the first equilibrium condition is an optimal policy ; hence , the follower has no incentive to deviate from this policy .",
    "with respect to the second equilibrium condition , we note that all of the follower s policies are finite memory approximations of the follower s optimal perfect memory response policy to the leader s policy .",
    "further , the process of determining the leader policies does not guarantee that pairs ( @xmath146 ) will be determined that satisfy the second equilibrium condition .",
    "thus , there is no guarantee that the leader will not want to deviate from the set of resultant non - dominated leader policies .",
    "however , given a sufficient number of generations of the moga and a sufficiently large @xmath91 such that the finite memory follower policy is a good approximation to an optimal perfect memory follower policy , it is reasonable to be confident that the resultant leader will have little incentive to deviate from the set of non - dominated leader policies generated .",
    "figure [ flow ] provides an outline of the process for generating these non - dominated leader policies .",
    "we now address the question : will improved observation quality improve , or at least not degrade , the performance of agents ?",
    "with respect to the follower , assume @xmath147 , and let @xmath148 be the stochastic matrix having @xmath149 entry @xmath150 , where @xmath151 and @xmath152 .",
    "let @xmath153 be a stochastic matrix such that for all @xmath37 , there exists a third stochastic matrix @xmath154 , where @xmath155 . then by results in ( white and harrington , 1980 ) , @xmath156 , where @xmath157 is the value of the follower s criterion associated with observation matrix @xmath148 and @xmath158 is the value of the follower s criterion associated with observation matrix @xmath153 .",
    "thus , for the follower , the observation quality provided by @xmath148 is at least as good as the observation quality provided by @xmath153 and the added value of using @xmath148 , relative to @xmath153 , is the difference @xmath159 .    the determination of conditions that guarantee that improved observation quality for the leader will not degrade leader performance is a topic for future research .",
    "we remark that counterexamples exist to the general claim that better information quality always implies improved system performance ( ortiz , erera and white , 2012 ) .",
    "this section presents an example that illustrates how the results in section 3 can be used to support a defender ( the leader ) in selecting a sequence of actions to best control a simplified liquid egg production process over time , given that there is an attacker ( the follower ) who seeks to contaminate the liquid egg production process with a chemical or biological toxin .",
    "both the defender and the attacker receive updated , possibly noise corrupted , data about his / her opponent just prior to each decision epoch .",
    "the defender has two scalar criteria : ( 1 ) a measure of the system s vulnerability to an attacker , which the defender wants to minimize , and ( 2 ) a measure of the system s productivity , which the defender wants to maximize .",
    "the attacker s criterion is to maximize the expected number of packages produced by the facility that contain a sufficiently lethal dose of the toxin .",
    "the attacker s and defender s transition diagrams are presented in figures [ attacker ] and [ defender ] , respectively .",
    "two targets , @xmath160 and @xmath161 are considered .",
    "state @xmath162 is the state that the attacker is in prior to launching an attack , and includes attack team assembly , toxin manufacture , and transportation .",
    "states @xmath163 and @xmath164 are the pre - attack states in which the attacker is armed and ready to attack target 1 and target 2 , respectively . at each decision epoch",
    ", the attacker can either choose to stay in the current state , advance forward to the next state , or retreat to a prior state .",
    "we remark that the transition probability of the attacker could be affected by the defender s strategy .",
    "an attacker s error or interdiction by the defender could return the attacker back to the state @xmath162 .",
    "the defender s states include a full production low alert state ( fp ) , a low production high alert state ( lp ) and the attacked state ( att . ) .",
    "the defender in fp or lp could stay in his / her current state or switch to the other state with given probabilities .",
    "if an attack occurs , the failure of detecting an attack promptly and remain in either fp or lp will cause significant consequences for the defender .",
    "the defender can also terminate the game and shut down the facilities to clean the toxin if the attack is successfully detected .",
    "given an attack , there are two groups of outcomes : unsuccessful attacks and successful attacks .",
    "if the attack is successful , the amount of toxin delivered to the consumers is described by a cumulative distribution function .",
    "we assume that the distribution function satisfies a standard , first - order stochastic dominance assumption .",
    "we restrict our attention to deterministic defender policies since they are easy to implement .",
    "related discussion can be found in paruchuri ( 2004 ) .",
    "there are 64 defender policies and the moga can probabilistically identify the pareto efficient policies within 5 generations .",
    "the non - dominated set of policies are presented in table [ decision_support ] .",
    "the defender must then trade off productivity and vulnerability , based on his / her preferences in order to select a most preferred policy .",
    ".decision support table [ cols=\"^,^,^ \" , ]     [ decision_support ]    figure 4 compares the value of the attacker s reward , as a function of observation quality .",
    "the results are consistent with the discussion in section 3.7 ; i.e. improved observation quality of the follower does not degrade the follower s performance .",
    "a comparison of the value of the defender s rewards as a function of observation quality is a topic of future research .",
    "the contributions of the paper are as follows :    \\(1 ) we have blended the pomg , the pomdp , and the moga to identify leader policies that are candidates for a most preferred policy in an infinite horizon , sequential decision making environment where :    * there are two intelligent and adaptable agents , a leader and a follower , and each can affect the performance of the other . * at each decision epoch , each agent knows : its past and present states , its past actions , and the noise corrupted observations of the other agent s past and present states . *",
    "the leader s and follower s actions are selected simultaneously at each decision epoch . *",
    "each agent s policy selects actions based on data currently available to the agent . *",
    "the follower knows the leader s policy and determines a response policy that is optimal with respect to the follower s objective . *",
    "the leader considers multiple objectives in selecting its policy .",
    "\\(2 ) given the pomg , a leader policy , and the assumption that the follower selects its policy with complete knowledge of and in response to the policy selected by the leader , we have constructed a specially structured pomdp that leads to the determination of a perfect - memory optimal policy for the follower ( proposition 1 ) .",
    "we have shown that this pomdp has a computationally useful sufficient statistic and a value function structure described in terms of this sufficient statistic . by assuming that the leader policy is a finite - memory policy , we have shown that the sufficient statistic is finite - dimensional and that the value function has at least an approximate finite representation , thus insuring that at least a near - optimal perfect - memory policy for the follower is potentially computable .",
    "\\(3 ) we have determined a computationally tractable procedure for calculating the fitness measures for the moga , given that the policies for both agents are finite - memory policies ( proposition 2 ) .",
    "we have presented a simple procedure for finding a finite - memory approximation to a perfect - memory policy and used it to find a finite - memory policy for the follower , based on the perfect - memory policy determined through the use of proposition 1 .",
    "the concomitant results show that there is a finite - dimensional sufficient statistic for the related fitness measures .",
    "we remark that the computational tractability of procedures for determining the follower s response policy and the fitness measures for the moga is inextricably linked to the assumption that the agent policies are finite - memory policies .",
    "the output of the process described in this paper can serve as the options generation phase of an option selection process ; e.g. , a deterministic version of multi - attribute decision theory ( kenney & raiffa , 1993 ) .",
    "topics for future research include a sensitivity analysis in order to better understand the robustness of the results and a study of the value of improving the leader s quality of observations .",
    "this material is based upon work supported by the u.s .",
    "department of homeland security under grant award number 2010-st-061-fd0001 through a grant awarded by the national center for food protection and defense at the university of minnesota .",
    "the views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies , either expressed or implied , of the u.s .",
    "department of homeland security or the national center for food protection and defense .",
    "9 aberdeen , d. , a ( revised ) survey of approximate methods for solving partially observable markov decision processes , _ technical report _",
    ", research school of information science and engineering , australia national university , 2003 .",
    "bakir , n. o. , a stackelberg game model for resource allocation in cargo container security , _",
    "annals of operations research _ , * 187 * , 5 - 22 , 2011 .",
    "bakir , n. o. and kardes , k. , a stochastic game model on overseas cargo container security , _ non - published research reports _ , create center , paper 6 , 2009 .",
    "becker , r. , zilberstein , s. , lesser , v. , and goldman , c. v. , solving transition independent decentralized markov decision processes , _ journal of artificial intelligence research ( jair ) _ , * 22 * , 423 - 455 , 2004 .",
    "bernstein , d. s. , givan , r. , immerman , n. , and zilberstein , s. , the complexity of decentralized control of markov decision processes , _ mathematics of operations research _ , * 27*(4 ) , 819 - 840 , 2002 .",
    "bernstein , d. s. , hansen , e. a. , and zilberstein , s. , bounded policy iteration for decentralized pomdps , in _ proceedings of the nineteenth international joint conference on artificial intelligence ( ijcai ) _ , 1287 - 1292 , edinburgh , scotland , july 2005 .",
    "bier , v. m. , oliveros , s. , and samuelson , l. choosing what to protect : strategic defensive allocation against an unknown attacker , _ journal of public economic theory _ , * 9*(4 ) , 563 - 587 , 2007 .",
    "bier , v. m. , haphuriwat , n. , menoyo , j. , zimmerman , r. , and culpen , a. m. , optimal resource allocation for defense of targets based on differing measures of attractiveness , in _ risk analysis _ , * 28*(3 ) , 763 - 770 , 2008 .",
    "bopardikar , s. d. , and hespanha , j. p. , randomized solutions to partial information dynamic zero - sum games , in _ american control conference ( acc ) _ , san francisco , ca , june 2011 .",
    "bowman , m. , briand , l. c. , and labiche , y. , solving the class responsibility assignment problem in object - oriented analysis with multi - objective genetic algorithms , _ ieee transactions on software engineering ( tse ) _ , * 36*(6 ) , 817 - 837 , 2010 .",
    "cardoso , j.m.p , diniz , p.c , game theory models of intelligent actors in reliability analysis : an overview of the state of the art , _ game theoretic risk analysis of security threats , international series in operations research & management science _ , *",
    "128 * , 1 - 19 , 2009 .",
    "cassandra , a. r. , kaelbling , l. p. , and littman , m. l. , acting optimally in partially observable stochastic domains , in _ proceedings twelfth national conference on artificial intelligence ( aaai-94 ) _ , seattle , wa , 1023 - 1028 , 1994a .",
    "cassandra , a. r. , optimal policies for partially observable markov decision processes , _ technical report ( cs-94 - 14 ) _ , brown university , department of computer science , providence ri , 1994b .",
    "cassandra , a. r. , littman , m. l. and zhang , n. l. , incremental pruning : a simple , fast , exact method for partially observable markov decision processes , in _ proceedings thirteenth annual conference on uncertainty in artificial intelligence ( uai-97 ) _ , morgan kaufmann , san francisco , ca , 54 - 61 , 1997 .",
    "cavusoglu , h. , and kwark , y. , passenger profiling and screening for aviation security in the presence of strategic attackers , _ decision analysis _ , * 10*(1 ) , 63 - 81 , 2013 .",
    "cheng , h. t. , algorithms for partially observable markov decision processes , phd thesis , university of british columbia , vancouver , british columbia , canada , 1988 .",
    "coello , c. a. c. , an updated survey of ga - based multiobjective optimization techniques , _ acm computing survey _ , * 32*(2 ) , 109 - 143 , 2000 .",
    "corne , d. w. , knowles , j. d. and oates , m. j. , the pareto envelope - based selection algorithm for multiobjective optimization , in _ proceedings of the parallel problem solving from nature vi conference _ , * 1917 * , 839 - 848 , 2000 .",
    "deb , k. , nonlinear goal programming using multi - objective genetic algorithms , _ journal of the operational research society _ , * 52*(3 ) , 291 - 302 , 2001 .",
    "deb , k. , pratap , a. , agarwal , s. , and meyarivan , t. , a fast and elitist multiobjective genetic algorithm : nsga - ii , _ ieee transactions on evolutionary computation _ , * 6*(2 ) , 182 - 197 , 2002 . eagle , j. n. , the optimal search for a moving target when the search path is constrained , _ operations research _ , * 32*(5 ) , 1107 - 1115 , 1984 .",
    "emery - montemerlo , r. , gordon , g. , schneider , j. , and thrun , s. , approximate solutions for partially observable stochastic games with common payoffs , in _ proceedings of the third international joint conference on autonomous agents and multi - agent systems ( aamas ) _ , 136 - 143 , 2004 .",
    "feng , z. , and zilberstein , s. , region - based incremental pruning for pomdps , in _ proceedings of the twentieth conference on uncertainty in artificial intelligence ( uai-04 ) _ , morgan kaufmann , san francisco , 146 - 153 , 2004 .",
    "filar , j. , and vrieze , k. , competitive markov decision processes , springer , heidelberg , 1997 .",
    "forrest , s. , genetic algorithms : principles of natural selection applied to computation , _ science _ , * 261 * , 872 - 878 , 1993 .",
    "ghosh , m. k. , mcdonald , d. , and sinha , s. , zero - sum stochastic games with partial information , _ journal of optimization theory and applications _ , * 121*(1 ) , 99 - 118 , 2004 .",
    "gmytrasiewicz , p. j. , and doshi , p. , a framework for sequential planning in multi - agent settings , _ journal of artificial intelligence research _ , * 24 * , 49 - 79 , 2005 .",
    "goldberg , d. e. , genetic algorithms in search , optimization , and machine learning , addison - wesley : reading , ma , 1989 .",
    "hansen , e. a. , an improved policy iteration algorithm for partially observable mdps , _ advances in neural inform . processing systems _ , * 10 * ( nips-97 ) , 1015 - 1021 , mit press , cambridge , ma , 1998a .",
    "hansen , e. a. , solving pomdps by searching in policy space , in _ proceedings of uncertainty in artificial intelligence _ , * 10 * , 211 - 219 , 1998b .",
    "hansen , e. a. , bernstein , d. s. , and zilberstein , s. , dynamic programming for partially observable stochastic games , in _ proceedings of the nineteenth national conference on artificial intelligence _",
    ", 709 - 715 , san jose , california , 2004 .",
    "hausken , k. , and zhuang , j. , governments and terrorists defense and attack in a t - period game , _ decision analysis _ , * 8*(1 ) , 46 - 70 , 2011 .",
    "hauskrecht m. , value - function approximations for partially observable markov decision processes , _ journal of artificial intelligence research _ , * 13 * , 33 - 94 , 2000 .",
    "hauskrecht m. , planning and control in stochastic domains with imperfect information , phd thesis , massachusetts institute of technology , 1997 .",
    "hespanha , j. p. , and prandini , m. , nash equilibria in partial - information games on markov chains , in _ ieee conference on decision and control _ , orlando , fl , 2102 - 2107 , dec 2001 . hoey , j. , poupart , p. , von bertoldi , a. , craig , t. , boutilier , c. , and mihailidis , a. , automated handwashing assistance for persons with dementia using video and a partially observable markov decision process , _ computer vision and image understanding _ , * 114 * ( 5 ) , 503 - 519 , 2010 .",
    "holland , j. h. , adaptation in natural and artificial systems , university of michigan press : ann arbor , 1975 . reprinted in 1992 by mit press , cambridge ma .",
    "holloway , h , and white , c. c. , question selection and resolvability for imprecise multi - attribute alternative selection , _ ieee transactions on systems , man , and cybernetics _",
    ", part a , * 38*(1),162 - 169 , 2008 .",
    "horn , j. , nafpliotis , n. , and goldberg , d. e. , a niched pareto genetic algorithm for multiobjective optimization , in _ proceedings of the 1st ieee conference on evolutionary computation , ieee world congress on computational intelligence _ * 1 * , 82 - 87 , orlando , fl , june , 1994 .",
    "keeney , r. l. and raiffa , h. , decisions with multiple objectives : preferences and value trade - offs , cambridge university press , 1993 .",
    "koller , d. , meggido , n. , and von stengel , b. , fast algorithms for finding randomized strategies in game trees , in _",
    "26th annual acm symposium on the theory of computing _",
    ", 750 - 759 , 1994 .",
    "konak , a. , coit , d. w. , and smith , a. e. , multi - objective optimization using genetic algorithms : a tutorial , _ reliability engineering and system safety _ , * 91 * , 992 - 1007 , 2006 .",
    "kumar , a. , and zilberstein , s. , dynamic programming approximations for partially observable stochastic games , in _ proceedings of the twenty - second international flairs conference _ , 547 - 552 , sanibel island , florida .",
    "lin , a. z .-",
    "z.,bean , j. , and white , c. c. , genetic algorithm heuristics for finite horizon partially observed markov decision problems , _ technical report _",
    ", university of michigan , ann arbor , 1998 .",
    "lin , a. z .-",
    "z.,bean , j. , and white , c. c. , a hybrid genetic / optimization algorithm for finite horizon partially observed markov decision processes , _",
    "journal on computing _ , * 16*(1 ) , 27 - 38 , 2004 .",
    "lin , c. m. and gen , m. , multi - criteria human resource allocation for solving multistage combinatorial optimization problems using multiobjective hybrid genetic algorithm , _ expert systems with applications _",
    ", * 34*(4 ) , 2480 - 2490 , 2008 .",
    "littman , m. l. , memoryless policies : theoretical limitations and practical results , in _ proceedings of the third international conference on simulation of adaptive behavior : from animals to animats _ , 238 - 245 , 1994a .",
    "littman , m. l. , the witness algorithm : solving partially observable markov decision processes , brown university , department of computer science , _ technical report _ cs-94 - 40 , 1994b .",
    "lovejoy , w. s. , a survey of algorithmic methods for partially observed markov decision process , _ annals of operations research _ , * 28 * ( 1 ) , 47 - 65 , 1991a . manning , l. , baines , r. , and chadd , s. , deliberate contamination of the food supply chain , _ british food journal _ , * 107 * ( 4 ) , 225 - 245 , 2005 .",
    "mceneaney , w. m. , some classes of imperfect information finite state - space stochastic games with finite - dimensional solutions , _ applied mathematics and optimization _ , * 50*(2 ) , 87 - 118 , 2004 .",
    "mclay , l.,rothschild , c. , guikema , s. , robust adversarial risk analysis : a level - k approach _ decision analysis _ , * 9*(1 ) , 41 - 54 , 2012 .",
    "monahan , g. e. , a survey of partially observable markov decision processes : theory , models , and algorithms , _ management science _ , * 28 * , 1 - 16 , 1982 .",
    "mohtadi , h. and murshid , a. p. , risk analysis of chemical , biological , or radionuclear threats : implications for food security , _ risk analysis _ , * 29 * , 1317 - 1335 , 2009 .",
    "naser - moghadasi , m. , evaluating effects of two alternative filters for the incremental pruning algorithm on quality of pomdp exact solutions , _ international journal of intelligence science _ ,",
    "* 2 * , 1 - 8 , 2012 .",
    "oliehoek , f. a. , spaan , m. t. j. , and vlassis , n. , best - response play in partially observable card game , in _ proceedings of the 14th annual machine learning conference of belgium and the netherlands _ , 45 - 50 , feb . 2005 .",
    "oliehoek , f. a. , spaan , m. t. j. , and vlassis , nikos , optimal and approximate q - value functions for decentralized pomdps , _ journal of artificial intelligence research _ , * 32 * , 289 - 353 , 2008 .",
    "oliehoek , f. a.,decentralized pomdps , in : m. wiering , & m. v. otterlo ( eds . ) , _ reinforcement learning : state of the art _ , * 12 * , 471 - 503 , springer , 2012 .",
    "ombuki , b. , ross , b. j. , and hanshar , f. , multi - objective genetic algorithms for vehicle routing problem with time windows , _ applied intelligence _ , * 24 * , 17 - 30 , 2006 .",
    "ortiz , o. l. , erera , a. l. , white , c. c. , state observation accuracy and finite - memory policy performance for partially observed markov decision processes , _ technical report _ , 2012 .",
    "oryan , m. , djuretic , t. , wall , p. , nichols , g. , hennessy , t. , slutsker , l. , hedberg , c. , macdonald , k. , and osterholm , m. , an outbreak of salmonella infection from ice cream , _ new england journal of medicine _ , * 335*(11 ) , 824 - 825 , 1996 .",
    "paruchuri , p. , tambe , m. , ordonez , f. , and kraus , s. , towards a formalization of teamwork with resource constraints , in _ international joint conference on autonomous agents and multiagent systems _ , 596 - 603 , 2004 .",
    "pineau , j. , gordon , g. j. , and thrun , s. , point - based value iteration : an anytime algorithm for pomdps , in _ international joint conference on artificial intelligence _ , 1025 - 1032 , 2003 .",
    "platzman , l. k. , finite memory estimation and control of finite probabilistic systems , phd thesis , massachusetts institute of technology , cambridge , ma , 1977 .",
    "platzman , l. k. , optimal infinite - horizon undiscounted control of finite probabilistic systems , _ siam j. control optim .",
    "_ , * 18 * , 362 - 380 , 1980 .",
    "ponnambalam , s. g. , ramkumar , v. , and jawahar , n. , a multiobjective genetic algorithm for job shop scheduling , _ production planning & control : the management of operations _ , * 12*(8 ) , 764 - 774 , 2001 .",
    "poupart , p. , exploiting structure to efficiently solve large scale partially observable markov decision processes , phd thesis , department of computer science , university of toronto , 2005 .",
    "poupart , p. , kim , k. e. , and kim , d. , closing the gap : improved bounds on optimal pomdp solutions , in _ international conference on planning and scheduling ( icaps ) _ , 2011 .",
    "puterman , m. l. , markov decision processes : discrete dynamic programming , new york : j wiley & sons , 1994 .",
    "rabinovich , z. , goldman , c. v. , and rosenschein , j. s. , the complexity of multiagent systems : the price of silence , in _ proceedings of the second international joint conference on autonomous agents and multi - agent systems ( aamas ) _ , 1102 - 1103 , melbourne , australia , 2003 .",
    "raghavan , t. e. s. , and filar , j. a. , algorithms for stochastic games - a survey , _ methods and models of operations research _ , * 35 * , 437 - 472 , 1991 .",
    "rothschild , c. , mclay , l. , guikema , s. , adversarial risk analysis with incomplete information : a level - k approach , _ risk analysis _ , * 32*(7 ) , 1219 - 1231 , 2012 .",
    "schaffer , j. d. , multiple objective optimisation with vector evaluated genetic algorithm , in _ proceedings of the 1st international conference on genetic algorithms _",
    ", 93 - 100 , morgan kaufmann publishers , inc . , san mateo , 1985 .",
    "seuken , s. , and zilberstein , s. , improved memory - bounded dynamic programming for decentralized pomdps , in _ proceedings of the 23rd conference on uncertainty in artificial intelligence _ ,",
    "vancouver , canada , july , 2007 .",
    "shani , g. , pineau , j. , and kaplow , r. , a survey of point - based pomdp solvers , _",
    "aamas _ , 1 - 51 , june 2012 .",
    "shapley , l. s. , stochastic games , _ proceedings of the national academy of sciences of the u. s. a. _ , * 39 * , 1095 - 1100 , 1953 .",
    "smallwood , r. d. , and sondik , e. j. , the optimal control of partially observable markov decision processes over a finite horizon , _ operations research _ , * 21 * , 1071 - 1088 , 1973 .",
    "sobel , j. , khan , a. , and swerdlow , d. , threat of a biological terrorist attack on the us food supply : the cdc perspective , _ the lancet _",
    ", * 359*(9309 ) , 874 - 880 , 2002 .",
    "sondik , e. j. , the optimal control of partially observable markov processes over the infinite horizon : discounted costs , _ operations research _ , * 26 * , 282 - 304 , 1978 .",
    "srinivas , m. , and patnaik , l. m. , genetic algorithms : a survey , _ ieee computer _ , * 27*(6 ) , 17 - 26 , 1994 .",
    "thomson , b. , and young , s. , bayesian update of dialogue state : a pomdp framework for spoken dialogue systems , _",
    "computer speech & language _ , * 24*(4 ) , 562 - 588 , 2010 .",
    "ummels , m. , stochastic multiplayer games : theory and algorithms , phd thesis , rwth aachen university , 2010 .",
    "varakantham , p. , maheswaran , r. , gupta , t. , and tambe , m. , towards efficient computation of quality bounded solutions in pomdps : expected value approximation and dynamic disjunctive beliefs . in _",
    "twentieth international joint conference on artificial intelligence _ , 2007 .",
    "wang , c. and bier , v. m. , target - hardening decisions based on uncertain multiattribute terrorist utility , _ decision analysis _ , * 8*(4 ) , 286 - 302 , 2011 .",
    "white , c. c. , and harrington , d. p. , application of jansen s inequality to adaptive suboptimal design , _ journal of optimization theory and application _ , * 32 * , 89 - 99 , 1980 .",
    "white , c. c. , a survey of solution techniques for the partially observed markov decision process , _ annals of operations research _ , * 32 * , 215 - 230 , 1991 .",
    "white , c. c. , and scherer , w. t. , solution procedures for partially observed markov decision processes , _",
    "operations research _ , * 37 * , 791 - 797 , 1989 .",
    "white , c. c. , and scherer , w. t. , finite - memory suboptimal design for partially observed markov decision processes , _",
    "operations research _ , * 42 * , 439 - 455 , 1994 .",
    "yildirim , m. b. , and mouzon , g. , single - machine sustainable production planning to minimize total energy consumption and total completion time using a multiple objective genetic algorithm , _ ieee transactions on engineering management _ , * 59*(4 ) , 585 - 597 , 2012 .",
    "yu , h. , approximation solution methods for partially observable markov and semi - markov decision processes , phd thesis , department of electrical engineering and computer science , massachusetts institute of technology , cambridge , ma , 2007 .",
    "zhang , h. , partially observable markov decision processes : a geometric technique and analysis , _ operations research _ , * 58 * , 214 - 228 , 2010 .",
    "zhuang j. and bier , v. m. , balancing terrorism and natural disasters defensive strategy with endogenous attacker effort , _ operations research _ , * 55*(5 ) , 976 - 991 , 2007 ."
  ],
  "abstract_text": [
    "<S> the intent of this research is to generate a set of non - dominated policies from which one of two agents ( the leader ) can select a most preferred policy to control a dynamic system that is also affected by the control decisions of the other agent ( the follower ) . </S>",
    "<S> the problem is described by an infinite horizon , partially observed markov game ( pomg ) . </S>",
    "<S> the actions of the agents are selected simultaneously at each decision epoch . at each decision epoch , each agent knows : its past and present states , its past actions , and noise corrupted observations of the other agent s past and present states . </S>",
    "<S> the actions of each agent are determined by the agent s policy , which selects actions at each decision epoch based on these data .    </S>",
    "<S> the leader considers multiple objectives in selecting its policy . </S>",
    "<S> the follower considers a single objective in selecting its policy with complete knowledge of and in response to the policy selected by the leader . </S>",
    "<S> this leader - follower assumption allows the pomg to be transformed into a specially structured , partially observed markov decision process ( pomdp ) . </S>",
    "<S> this pomdp is used to determine the follower s best response policy . a multi - objective genetic algorithm ( moga ) </S>",
    "<S> is used to create the next generation of leader policies based on the fitness measures of each leader policy in the current generation . computing a fitness measure for a leader policy </S>",
    "<S> requires a value determination calculation , given the leader policy and the follower s best response policy . </S>",
    "<S> the policies from which the leader can select a most preferred policy are the non - dominated policies of the final generation of leader policies created by the moga . </S>",
    "<S> an example is presented that illustrates how these results can be used to support a manager of a liquid egg production process ( the leader ) in selecting a sequence of actions to best control this process over time , given that there is an attacker ( the follower ) who seeks to contaminate the liquid egg production process with a chemical or biological toxin . </S>"
  ]
}