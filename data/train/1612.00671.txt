{
  "article_text": [
    "machine learning ( ml ) has been a well - explored domain of research in the present decade .",
    "it is basically a method of data analysis that automatically builds models from historical data .",
    "ml uses algorithms that iteratively learn from such historical data , which in turn finds hidden insights and patterns inside the data without being explicitly programmed for the it .",
    "ml techniques have been employed in many different real world problems such as fraud detection , intrusion detection , web search engines , e - mail spam filtering , sentiment analysis , credit scoring , equipment failures prediction , pattern and image recognition , genetics and genomics , robotics ( see @xcite ) .",
    "for all such real world tasks , ml requires certain algorithms which are called ` learning algorithms ' based on which the pattern inside the historical data could be explored .",
    "based on definition of the problem and availability of data , a learning algorithm could be of two major types : supervised , or unsupervised .",
    "supervised learning trains a model in the presence of a supervisor ( technically an ` error ' term ) whereas the unsupervised algorithms do not require such an error term for training . supervised learning methods could be applied to cases of prediction and classification .",
    "this paper is more focused in the area of supervised ml .",
    "supervised ml allows access to the data labels during training and testing phases of the model .",
    "for example , there could be a problem of identifying how a student will perform in the present semester given his attendance , weekly performance , participation in the class , past records .",
    "such a problem requires huge historical student records and their performance .",
    "supervised ml tries to understand such characteristic in the data and predicts the performance of the presently considered student . consider a set of data records @xmath0 that have to be assigned to a set of predefined labels , or classes @xmath1 ( @xmath2 is usually less than @xmath3 )",
    ". the process of assigning a class label to a data record is termed as classification .",
    "classification falls into two major categories such as binary classification , multiclass classification .",
    "binary classification is one of the basic classification task where @xmath2 is 2 .",
    "for example , finding out whether the performance of the student would be ` good ' or ` bad ' ; here , ` good ' and ` bad ' are two categorical classes . however , classification of a data record where more than two classes are available ( i.e. @xmath4 ) is one of the challenging tasks and is called multiclass classification . for example , finding out whether the performance of the student would be ` excellent ' , ` good ' , ` average ' , ` below average ' , or ` poor ' . evaluating a learning model for binary classification is easier as compared to evaluation of a learning model for multiclass classification because of the following reason . in the binary classification problem ,",
    "a data record can be identified as true positive ( @xmath5 ) or true negative ( @xmath6 ) or false positive ( @xmath7 ) or false negative ( @xmath8 ) .",
    "generation of all these information during testing could be presented as a matrix called ` confusion matrix ' ( table [ tab : confusionmat ] ) .",
    "the performance measures which could be used to evaluate a learning model for binary classification are accuracy , sensitivity ( recall ) , specificity , precision , @xmath9score .",
    "these parameters could be computed using equations through to equation . for more information on these parameters ,",
    "one can see the published work of sokolova and lapalme @xcite .",
    "@xmath10 gives an overall estimate of predictive power of a model .",
    "@xmath11 gives information about positive predictive value of the model . @xmath12 and @xmath13 estimate the true positive rate and true negative rate for the testing dataset .",
    "@xmath14 is a balanced mean between @xmath15 and @xmath12 .",
    "[ tab : confusionmat ]    @xmath16    @xmath17    @xmath18    @xmath19    @xmath20    the above - mentioned performance measures are helpful for evaluating classifiers in binary classification problems .",
    "however , researchers often use accuracy for evaluating the performance of classifiers for multiclass classification tasks .",
    "this is because of the fact that the evaluation of the above performance measures are quite difficult when more than two classes are there to be considered for the problem ( e.g. multiclass classification of student performance in the semester ) .",
    "moreover , accuracy measure for multiclass classification task could be non - reliable when the dataset ( a set of data records ) is skewed more towards a particular class .",
    "for such kind of problems , a reliable evaluation of a classifier would be very much crucial . in this work , we limit our discussion to popularly used supervised learning model ( classifier ) called _ _ neural network ( nn)__. we focus our attention majorly on reliable performance evaluation of nn for real - world multiclass classification problems rather than reasoning about the obtained results .",
    "we estimate various performance measures which could be used for multiclass classification problems based on the information provided in @xcite .",
    "the rest of the paper consists of following sections .",
    "section [ sec:2 ] provides an explanation of the implemented nn and its training algorithm .",
    "section [ sec:3 ] details on the tested benchmark datasets used in this work .",
    "section [ sec:4 ] expands on the results and discussion followed by conclusion in section [ sec:5 ] .",
    "nn is a biologically inspired mathematical model which is used to approximate functions that depend on a set of inputs called ` features ' . computational processing of nn closely follows information processing inside the human brain which has a complex network of neurons .",
    "the motivation behind evaluating nn in this work is that they have high adaptation power given a better learning algorithm and their rigorous applications in many different real - life problems .",
    "moreover , there is a good amount of flexibility to tune a learning algorithm with different parameters to improve the performance of the nn . generally , nn is a layered architecture where neurons ( nodes ) are arranged in layers . in this work ,",
    "multilayered nn , specifically , a multilayer perceptron ( mlp ) , has been implemented .",
    "mlp architecture comprises of an input layer , a hidden layer , and an output layer .",
    "the input layer takes values of the features from the dataset and computes an output in the output layer .",
    "the hidden layer is responsible for transforming the input features into a set of features which could be processed by the output layer .",
    "it has been seen that the performance of the nn depends on the learning algorithm based on which it has been trained using the training dataset . the on - line learning has been made popular by researchers by the development of back - propagation algorithm that uses gradient descent strategy for minimization of error during training of the nn .",
    "it should be noted that the stochastic version of the gradient descent algorithm , called stochastic gradient descent ( sgd ) , has been used in this work where the weights of the nn are updated for each random sample from the training dataset .",
    "an nn can be trained effectively by back - propagation if a sufficiently large dataset is used during training .",
    "the training dataset refers to a set of data records with known class ( i.e. the class to which the data record belongs ) .",
    "let a data instance be represented as a pair of vectors @xmath21 , where @xmath22 is the vector of input features , and @xmath23 is the vector of target output values or classes .",
    "let us denote @xmath24th record in the dataset as @xmath25 and the class labels for this data record as a set @xmath26 ( see footnote is a set because a class can be represented as a set of multiple classes e.g. if there are three classes of data , then a data belonging to class 1 , class 2 and class 3 could be represented as \\{0,0,1 } , \\{0,1,0 } and \\{0,0,1 } respectively . ] ) .",
    "so , the @xmath24th training data instance in a training dataset can be represented as @xmath27 , where @xmath28 is the number of inputs to the neural net .",
    "let the number of neurons in the hidden layer be represented as @xmath29 and number of neurons in the output layer as @xmath30 .",
    "each neuron in one layer of the nn is connected to each neuron in its next layer with a weight value , which represents the strength of the connection .",
    "we denote these weight set as a vector @xmath31 , where @xmath32 .",
    "@xmath33 also includes a set of biases in the hidden layer and the output layer .",
    "the weights including the biases of an nn is called as the knowledge base ( kb ) of the nn .",
    "the kb of the nn is updated during training of the nn .",
    "the back - propagation based training algorithm @xcite of the nn has been presently briefly in algorithm [ alg : backprop ] followed by a set of governing equations .",
    "initialize the weights @xmath33 to small random numbers    during the forward processing , the output of nn can be obtained by multiplying weights and the input pattern instance as shown in eq . .",
    "@xmath34 where , @xmath35 is the activation function of the output unit and is usually a sigmoid function as given in eq . .",
    "@xmath36 where , @xmath37 is a non - negative constant and is set to 1 in this work .",
    "the back - propagation training algorithm attempts to minimize an error term , @xmath38 ( for supervised classification tasks ) by changing or updating the weights of the nn .",
    "the error term is basically the squared error between the net output values and the target values for the corresponding input instance ( refer eq . ) .",
    "please note that when @xmath39 is represented as a set of ones and zeros , eq . might not be suitable . in this work ,",
    "as batch learning is used , it uses a modified version of the following equation .",
    "the details have been provided in the section [ sec:4 ] .",
    "@xmath40 where , @xmath41 is the set of output units in the nn ; @xmath42 and @xmath43 are the target and output values associated with the @xmath44th output unit for the @xmath24th input instance .    for each network output unit @xmath44 , the error term @xmath45 can be computed using eq . .",
    "@xmath46 similarly , for each hidden unit @xmath47 , the error term @xmath48 can be computed using eq . .",
    "@xmath49    the weight update equation for the network are as given in eq .",
    "where , @xmath50 is the value of @xmath51th feature of the @xmath24th data record .",
    "@xmath52 and , @xmath53 in the eq .",
    ", the constant @xmath54 is the learning rate .",
    "it should be noted that , we also used a constant term in the weight update rule called ` momentum factor ' ( denoted as @xmath55 ) to the weight update rule , which makes the amount of weight update on the @xmath3th training iteration depend partially on the weight update that had occurred during the @xmath56th training iteration , which can be clearly understood from eq . .",
    "@xmath57 here @xmath58 is the weight update performed during the @xmath3th iteration ; the constant @xmath55 is usually fixed in the range @xmath59 prior to training . in our work , the learning rate , @xmath54 and the momentum factor , @xmath55 are set to 0.3 and 0.1 respectively .",
    "to evaluate the performance of multiclass classification problem with nn , following seven different real - world benchmark datasets have been used in this work .",
    "all the datasets have been obtained from uci machine learning repository @xcite .",
    "however , we briefly explain all the datasets with regard to their dimension .",
    "more details about each dataset could be obtained from @xcite .",
    "the goal of using abalone dataset is to predict abalone age through the number of rings on the shell given various descriptive attributes of the abalone .",
    "there are 4177 data instance each with 8 input features and a class label .",
    "this dataset is one of the popular medical benchmarks in ml research .",
    "the patient records have been obtained from university of wisconsin hospital , madison .",
    "there are total 699 records with 10 input features ( one is an i d which is not used for computation ) and a class label .",
    "this dataset contains protein localization sites of 336 proteins with 8 input features ( one is a sequence number and not being used for computation ) and class label .",
    "this dataset is used to identify and predict the type of glass based on 9 different manufacturing features .",
    "it has 214 records containing such information .",
    "ilpd refers to indian liver patient database contains , 583 liver patient records each has been marked as a liver patient or a non - liver patient as their type .",
    "iris dataset is a very well known benchmark dataset which contains 4 input features and a class attribute .",
    "the dataset contains total 150 instances , 50 of each type of plants such as iris setosa , iris versicolour , and iris virginica .",
    "it contains the chemical analysis results of wines grown in italy .",
    "there are 13 predicting features and a class attribute .",
    "there are total 178 instances .",
    "all the simulations in this work are carried out in matlab r2015b using a personal computer system with windows 10 operating system , quad - core processor with the equal clock rate of 1.70 ghz and main memory of 4 gb .",
    "it is important to prepare data wisely before training of the nn .",
    "the real world data obtained from uci ml repository are distributed non - uniformly and hence , they can not be used directly during training and testing of the nn .",
    "therefore , the input features were initially normalized in the range [ 0,1 ] .",
    "the normalized dataset was then partitioned into training and an independent test set in the ratio 70:30 .",
    "the process of training and testing have been repeated for 10 independent runs ( simulations ) to get the average performance of the nn and its performance deviation from the mean .",
    "the performance parameters which are evaluated for multiclass classification are described as follows . for a class @xmath60",
    ", the classifier performance can be assessed with @xmath61 , @xmath62 , @xmath63 and @xmath64 and can be calculated from counts of testing instance belonging to @xmath60 .",
    "the quality of the overall classification performance can be assessed in two different ways such as micro and macro averaging .",
    "macro averaging treats all the classes equally while micro averaging favors classes with more data instances .",
    "computation of various performance measures suitable for evaluating nn for multiclass classification problem can be obtained as follows which is a generalization of the parameters presented in table [ tab : confusionmat ] for many classes @xmath60 @xcite . for a class @xmath60 , @xmath61 , @xmath62 , @xmath63 and",
    "@xmath64 counts respectively .",
    "micro- and macro - averaging indices are represented by @xmath55 and @xmath65 respectively .    average accuracy ( @xmath10 ) could be used to evaluate average per - class effectiveness of the nn and can be computed as , @xmath66 where , @xmath2 is the number of classes .",
    "other crucial measures could be obtained from equation through to equation @xcite .",
    "@xmath67 @xmath68 @xmath69 @xmath70 @xmath71 @xmath72 @xmath73 @xmath74    apart from the above mentioned micro and macro measures , the training error ( which is mean - squared error during training , @xmath75 ) , testing error ( which is mean - squared error during training , @xmath76 ) , and the training time ( @xmath77 ) have also been noted in this work .",
    "however , it is wise to mention that the sgd does not require @xmath78 during training rather it requires the error term ( @xmath38 ) between a random sample and its prediction for updating the weights of the nn .",
    "@xmath78 has been computed as parameter to observe the average error of convergence for the model during training and testing .",
    "@xmath78 can be calculated using the following equation , @xmath79 where , @xmath3 is the number of data records considered during the process ( either training or testing ) , @xmath39 is the hypothesis or the target for @xmath24th data instance , and @xmath80 is the output of the nn for the @xmath24th data instance .",
    "the number of neurons in the hidden layer ( @xmath29 ) is one of the most important architectural parameters which directly influences performance of nn during training and capturing data for preparing a knowledge base .",
    "however , the setting of this parameter apriori has been an unsolved problem in ml research @xcite . in this work , @xmath29 has been set to 60 , 80 and 100 and the results have been noted for each of the datasets .",
    "all the obtained results have been summarized and depicted as tables for different @xmath29 values .",
    "table [ tab : res_60 ] depicts results obtained for @xmath81 .",
    "similarly , table [ tab : res_80 ] and table [ tab : res_100 ] present results obtained for @xmath82 and @xmath83 respectively .",
    "it should be noted that all the results shown in these three tables are averaged over ten(10 ) independent simulations for each of the datasets .",
    "discussion regarding the obtained results ( as depicted in table [ tab : res_60 ] , table [ tab : res_80 ] and table [ tab : res_100 ] ) in this work has been primarily based on various performance parameters rather than how the values are obtained .",
    "this work summarizes different performance parameters to use for evaluation of nn or similar classifier for multiclass classification of real - world data .",
    "it has been seen that the popularly known ` accuracy ' parameter could not be as a reliable parameter for proper evaluation of a classifier .",
    "hence , in this work , diversified datasets are being used for evaluation of the nn for the classification problem .",
    "the training performances of the nn have been approximately equal for the abalone dataset with different @xmath29 settings such as @xmath81 , @xmath82 and @xmath83 .",
    "moreover , the standard deviation in the @xmath75 is very low for all the three cases .",
    "however , it is obvious that increasing the number of the hidden neurons increases the architectural complexity of the nn and hence the training time ( @xmath77 ) .",
    "moreover , when the @xmath29 is set to as high as 100 , there might be a high probability of over - fitting training data and could not achieve the better performance than performances observed for other considered settings such as @xmath81 or @xmath82 .",
    "it is also evident from the obtained average classification accuracy .",
    "moreover , given such a high accuracy of approximately 92% , the positive predictive value that is @xmath15 does not seem to be satisfactory .",
    "similarly , all other parameters such as @xmath12 , @xmath14 are not reliable . however , the true negative rate ( @xmath13 ) is quite good for all the tree different settings .",
    "the performance results which have been obtained for the breast cancer dataset is quite better than that of abalone . with the increase in the number of hidden neurons ,",
    "the nn is able to predict the hypothesis for the test dataset with high accuracy .",
    "the error of convergence decreases with increase in @xmath29 which could be possible by capturing the features properly during the training of the nn .",
    "the performance parameters such as @xmath10 , @xmath84 , @xmath85 , and other mentioned parameters seem to be following similar property .",
    "hence , it could be assumed that predictive accuracy could be a good measure for this dataset .",
    "however , @xmath10 parameter alone could not be taken as a reliable parameter while evaluating nn during multiclass classification of the abalone dataset .",
    "the evaluated performance of the nn for e - coli dataset closely follows the discussion about the performance for the abalone dataset .",
    "unlike the abalone dataset , the predictive accuracy of the nn for the e - coli dataset dropped quite high with @xmath83 .",
    "similar cases could also be seen for other evaluated parameters for the same dataset .",
    "the performance of nn for the glass dataset is comparatively improved with the increase in @xmath29 along with other performance parameters . however , although the predictive accuracy is good , the micro- and macro - sensitivity seems to be compromised even though the input feature is transformed into a higher dimensional feature set by the increase in @xmath29 .",
    "therefore , for glass dataset , predictive accuracy might not be a suitable parameter for evaluation of nn .",
    "evaluation of the nn with regard to ilpd dataset is quite good as compared to recent literature ( see @xcite ) considering the fact that the accuracy and other parameters still follow a particular limit of deviation unlike the results obtained for abalone , e - coli and glass datasets .",
    "this means that the micro and macro parameters could be considered reliable given such a low accuracy for the dataset .",
    "the performance of the nn for iris classification is superior when @xmath82 as compared to the results obtained for other two settings .",
    "the positive predictive rate and negative predictive rates are also better as compared to those with @xmath81 and @xmath83 . however , assuming that the predictive accuracy values are low for these two mentioned settings , it could be seen that the other performance evaluation parameters still reveal reliable performance for this dataset .",
    "however , not much deviation over the results obtained for the parameters could be seen for the wine dataset where the performance for the @xmath10 and other micro and macro parameters are improving with the increase in @xmath29 .",
    "moreover , given the present setup of experimentation , one could also achieve slightly different results because of the fact that the initial weights and biases of the nn are fixed at random .",
    "if a proper weight set is fixed initially , one could possibly land up in obtaining a better results for the same settings .",
    "this argument could be supported by the fact that the gradient descent may not always guarantee a close - to - optimal weight set at the end of the nn training process .",
    "in this work , a detailed evaluation of nn classifier has been carried out for multiclass classification of real - world data .",
    "it has been seen that the use of predictive accuracy as a single parameter for evaluating an nn would not be wise given high skewness in the test data .",
    "results obtained for different types of datasets clearly show that although the accuracy is very high , there could be fair chance that the positive or negative predictive rate falls far below any reliable range . in this work",
    ", this type of property has been seen in the performance of nn for a majority of tested datasets such as abalone dataset , e - coli dataset , glass dataset .",
    "hence , it would be wise to use many different parameters for such classification problems to accurately evaluate a classifier .",
    "t. dash , s. k. nayak , and h. s. behera ,  hybrid gravitational search and particle swarm based fuzzy mlp for medical data classification , \" in computational intelligence in data mining - volume 1 .",
    "springer science + business media , 2014 , pp ."
  ],
  "abstract_text": [
    "<S> this paper presents a systematic evaluation of neural network ( nn ) for classification of real - world data . in the field of machine learning </S>",
    "<S> , it is often seen that a single parameter that is ` predictive accuracy ' is being used for evaluating the performance of a classifier model . </S>",
    "<S> however , this parameter might not be considered reliable given a dataset with very high level of skewness . to demonstrate such behavior , </S>",
    "<S> seven different types of datasets have been used to evaluate a multilayer perceptron ( mlp ) using twelve(12 ) different parameters which include micro- and macro - level estimation . in the present study , </S>",
    "<S> the most common problem of prediction called ` multiclass ' classification has been considered . </S>",
    "<S> the results that are obtained for different parameters for each of the dataset could demonstrate interesting findings to support the usability of these set of performance evaluation parameters . </S>"
  ]
}