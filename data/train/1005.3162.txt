{
  "article_text": [
    "it is very common in practice , for a model , to measure the regressors(covariates ) but for varied reasons it is sometimes impossible to have all values of the response variable .",
    "the most widely used idea is to remove of model the observations with missing data .",
    "an alternative solution is to consider empirical likelihood(el ) method which is a powerful nonparametric method for constructing confidence regions of parameters .",
    "+ to our knowledge , previous theoretical and numerical investigations in literature have focused for model with missing response only in the linear case .",
    "the el method , proposed by @xcite does not need the asymptotic variance of estimator and it outperforms the normal approximation method in term of coverage probability for linear models .",
    "@xcite develops el inferences for the mean of a response variable under regression imputation of missing responses for a linear regression model and random covariates .",
    "@xcite construct an el statistic on parameter when regressors are deterministic and @xcite if regressors are random , based on least squares(ls ) method for linear model @xmath1 .",
    "@xcite consider the general linear model @xmath2 with @xmath3 a known vector function and investigate a hypothesis test on the response variable .",
    "these last three papers impose the condition that the conditional expectation of error @xmath4 with respect to covariate @xmath5 is zero .",
    "if this hypothesis is not satisfied , in order to reconstitute the response variable the least absolute deviations(lad ) method can be used .",
    "one advantage of least absolute deviations estimation is that it does not require any moment condition on the errors to obtain asymptotic normality .",
    "+ it is well known also that one outlier may cause a large error in a least squares estimator .",
    "this occurs in the case of fatter tail distributions of the error term . on the other hand ,",
    "as @xcite and @xcite indicate it , for heavy tailed distributions the lad estimator is more efficient than ls estimator .",
    "+ concerning the lad estimator in a complete nonlinear model we can refer to following papers : @xcite shows conditions for its consistency , @xcite proves that this estimator is consistent and asymptotically normal in a dynamic nonlinear model with neither independent nor identically distributed errors .",
    "@xcite gives the convergence rate @xmath6 , where @xmath7 is a monotone positive sequence such that @xmath8 and @xmath9 for @xmath10 .",
    "it is well known that confidence regions based on the asymptotic normality could encounter large coverage errors in small sizes or if the error distribution has outliers .",
    "the lad technique was already used in censored median linear regression model with missing data : see e.g. @xcite . + for other relevant papers ( not exhaustive list ) on el method for missing data in a linear models see @xcite , @xcite , @xcite , @xcite , @xcite .",
    "+ in this paper , for a nonlinear random model @xmath11 with missing responses , some empirical likelihood ratios are constructed by using complete - case or imputed values . the nonparametric version of wilks theorem is proved for two cases : the parameters are estimated by least squares and least absolute deviations on complete data .",
    "the limiting distribution of el statistic is @xmath0 , results which can be used to construct confidence region on parameter . in order to complete data ,",
    "the value imputed of missing value of variable response is obtained by generalising @xcite idea for linear model using a semiparametric technique : the parameters regression are estimated by ls method and missing probability by a nonparametric method .",
    "we show that the empirical log - likelihood ratio on parameter based on the improved data is asymptotically chi - squared .",
    "the numerical simulations proves that el methods outperforms the normal approximation method in terms of coverage probability up to and including on the reconstituted data .",
    "if the distribution of the errors presents outliers , the lad method gives generally best results that ls method on coverage probability and on parameter estimators efficiency .",
    "in addition to that , if the expectation of error does not exist , as it is the case for cauchy distribution , the normal approximation of ls estimator can not be satisfied .",
    "on the real data we obtain also that our semiparametric method gives more precise results to reconstitute the response variable that classic parametric ls method .",
    "+ the rest of this paper is organised as follows . in section 2 we introduce model , assumptions and some notations . in section 3 the wilks theorem for el statistic and also for its approximation",
    "is given , when the parameters are estimated using ls or lad method on complete data .",
    "the ls case is developed in section 4 , a reconstituted value for the response variable is introduced and asymptotic distribution of el for response variable is obtained .",
    "section 5 illustrates by simulation results that el methods for nonlinear random model outperform the normal method and give very competitive coverage probabilities .",
    "an application to the real data is presented in section 6 .",
    "finally , section 7 contains the proofs of the lemmas and of the theorems .",
    "let us consider following random nonlinear model : @xmath12 where @xmath13 is a sequence of continuous independent random vectors with the same joint distribution as @xmath14 and @xmath15 is a @xmath16 vector of unknown regression parameters .",
    "more precisely , @xmath4 is a random variable and @xmath5 a @xmath17 random vector of covariates .",
    "let @xmath18 denote the true ( unknown ) of the parameter @xmath15 . + with regard to the random variable @xmath4 we make one of the suppositions : + * ( h1 ) * @xmath19=0 $ ] and @xmath20 < \\infty$ ] , @xmath21 .",
    "+ * ( h1bis ) * @xmath22=0 $ ] and @xmath23 have continuous density @xmath24 satisfying @xmath25 .",
    "+ these conditions are essential for the consistency and asymptotic normality of the ls , respectively lad estimator . + in following we use notation @xmath26 , @xmath27 . for a vector ,",
    "let us denote @xmath28 the euclidean norm .",
    "+ regression function @xmath29 , with @xmath30 , @xmath31 and random vector @xmath5 satisfy the conditions : + * ( h2 ) * for all @xmath32 and for @xmath33 , the function @xmath34 is twice differentiable in @xmath15 and continuous on @xmath35 .",
    "+ * ( h3 ) * @xmath36=o(n^{-1/2})$ ] for any positive sequence @xmath37 as @xmath10 .",
    "+ * ( h4 ) * @xmath38 , @xmath39 are bounded for any @xmath40 and @xmath15 in a neighborhood of @xmath18 . + sets @xmath35 and @xmath41 are compacts .",
    "this , assumptions ( h2 ) and ( h4 ) are commonly used in nonlinear modelisation and are necessary for the consistency and for the asymptotic normality of the ls or lad estimator .",
    "assume also that the model is identifiable : if @xmath42 with probability one , then @xmath43 + for model ( [ e1 ] ) , all the @xmath44 s are observed , in exchange response variable @xmath45 can be missing .",
    "let be the sequence of random variables @xmath46 defined by : @xmath47 if @xmath45 is missing and @xmath48 if @xmath45 is observed .",
    "we suppose that @xmath45 is missing at random(mar ) : @xmath49=\\ep[\\delta_i=1|\\ex_i]$ ] , for all @xmath50 .",
    "consider the selection probability function : @xmath51>0 $ ] , @xmath52 .",
    "the supposition @xmath53 is a common assumption in the literature .",
    "the parameter @xmath15 is estimated on the completely observed data by two methods : least squares(ls ) method : @xmath54 ^ 2\\ ] ] and least absolute deviations(lad ) method : @xmath55 to build the el statistic , let us consider following functions , for @xmath56 : @xmath57 \\ef(\\ex_i,\\eb),\\ ] ] @xmath58 and let be also : @xmath59 with @xmath60 either @xmath61 or @xmath62 .",
    "the two estimators are a solution of the system of equations : @xmath63 . under ( h1 ) ,",
    "respectively ( h1bis ) , we have : @xmath64=\\textbf{0}$ ] for @xmath65 , respectively @xmath66 .",
    "the joint distribution of the errors @xmath23 and of @xmath44 is unknown , but conditional mean , respectively median , of @xmath23 is zero .    the empirical likelihood for @xmath15 with complete - case data can be defined as ( see @xcite ) : @xmath67 without constraint @xmath68 , the maximum of @xmath69 is attained for @xmath70 .",
    "then , the profile empirical likelihood ratio for @xmath15 with complete - case has the form : @xmath71 the empirical log - likelihood ratio statistic evaluated at @xmath15 is : @xmath72 let @xmath73 be , a lagrange multiplier .",
    "then , @xmath74 is maximised for @xmath75^{-1 } $ ] , where @xmath76 satisfies the equation : @xmath77 thus , @xmath78 can be written : @xmath79 in order to study the asymptotic properties of the el statistic given by ( [ e7 ] ) , let us consider following matrix : @xmath80 and under assumption ( h1 ) : @xmath81 .",
    "on suppose for the first matrix : + * ( ha ) * @xmath82 is a positive definite matrix .",
    "+ let us notice that @xmath82 is fisher information matrix on complete data .",
    "we give first a classical result for a mar model , lemma that turn out to be useful in the proof of the main results .",
    "[ lemma2 ] under assumptions ( h1 ) , respectively ( h1bis ) and ( h2 ) , ( h4 ) , ( ha ) , we have : + ( i ) @xmath83 , + ( ii ) @xmath84 , + ( iii ) @xmath85 , + with @xmath86 for ls and @xmath87 for lad method .",
    "let us consider following matrix : @xmath88 following theorem gives the asymptotic distribution of the empirical log - likelihood statistic ( [ e7 ] ) , evaluated at the true value .",
    "[ theorem1 ] under assumptions ( h1 ) , respectively ( h1bis ) , ( h2 ) , ( h4 ) , ( ha ) @xmath89 .",
    "we can use theorem [ theorem1 ] to get approximate confidence region for @xmath15 or for testing the hypothesis : @xmath90 . since , by the proof of theorem [ theorem1 ] ( see appendix ) for the el statistic we have : + @xmath91 , in order to calculate numerically @xmath92 we can use the approximation : @xmath93 we state this as a corollary .",
    "[ corollaire1 ] under the same assumptions as in theorem [ theorem1 ] , the asymptotic distribution of @xmath94 is @xmath95 .",
    "thus , an asymptotic @xmath96 confidence region for @xmath15 , based on el statistic on complete data is : @xmath97 where @xmath98 is the @xmath96 quantile of the chi - squared distribution with degrees of freedom @xmath99 .",
    "it is very interesting to note that to construct the confidence region for @xmath15 it is not necessary to calculate the lagrange multiplier which intervenes in ( [ e7 ] ) , observed data are enough .",
    "+ asymptotic normality of ls and lad estimators calculated on complete data is given by the following result .",
    "[ theorem2 ] ( i ) under assumption ( h1 ) , ( h2 ) , ( h4 ) , ( ha ) we have : + @xmath100 .",
    "+ ( ii ) under assumption ( h1bis ) , ( h2 ) , ( h4 ) , ( ha ) we have : + @xmath101 .    these theorem allows to give the normal approximation based confidence region , expression which will be specified in section 5 .",
    "then , on complete data , we have the choice between four statistics ( @xmath94 for ls , @xmath94 for lad , @xmath102 , @xmath103 ) to test hypotheses or to build the asymptotic confidence region of model parameter @xmath15 .",
    "we see in sections 5 and 6 , by simulations and a model on real data , that approximated el statistics are sharply superior to normal approximation given by theorem [ theorem2 ] .",
    "if error distribution presents outliers then @xmath104 for lad method is recommended , otherwise it is better to consider @xmath104 for ls method .",
    "this last one will more be developed in the following section .",
    "the missing probabilities @xmath105 are estimated by a nonparametric method , this is going to allow to reconstitute the missing responses . on the observed and the reconstituted observations one defined a new el statistic , which also satisfies a wilk s theorem . besides , numerically , it gives very competitive results ( see sections 5 and 6 ) .",
    "following e.g. @xcite , @xcite for linear model , we shall introduce the forecast of @xmath45 , constructed by using ls estimator for parameter @xmath15 and a nonparametric estimator for probability @xmath105 : @xmath106 with @xmath107 a nonlinear estimator for @xmath105 , as in the linear regression @xcite : @xmath108 where @xmath109 is a positive sequence tending to 0 as @xmath110 and @xmath111 is a kernel function defined in @xmath112 .",
    "the bandwidth @xmath113 satisfies : + * ( h5 ) * @xmath114 and @xmath115 , as @xmath110 , with the sequence @xmath116 given in assumption ( h3 ) .",
    "+ the kernel function @xmath117 , satisfies the classical condition ( imposed also for the linear model using the ls method of @xcite ) : + * ( h6 ) * there exist positive constants @xmath118 , @xmath119 and @xmath120 such that : @xmath121 .",
    "+ concerning the selection probability function @xmath122 let us make following regularity hypothesis necessary in the study of its nonparametric estimator . +",
    "* ( h7 ) * @xmath123 has bounded partial derivatives up to order @xmath124 almost everywhere .",
    "+ conditions ( h5)-(h7 ) are usual assumptions for convergent rates of kernel estimating method .",
    "let us denote @xmath125 $ ] the mean of @xmath126 and @xmath127 $ ] error variance .",
    "+ following lemma gives the asymptotic normality for the sequence @xmath128 and other two similar results of lemma [ lemma2 ] .",
    "[ lemma3 ] under assumptions ( h1)-(h6 ) and",
    "if @xmath129 < \\infty$ ] we have : + ( i ) @xmath130 , + ( ii ) @xmath131 , + ( iii ) @xmath132 , + with @xmath133+\\ee \\cro{\\frac{\\sigma^2(\\ex)}{\\pi(\\ex)}}$ ] .",
    "using similar arguments as for theorem [ theorem1 ] we obtain the following result .",
    "we hence omit its proof .",
    "[ theorem3 ] suppose that assumptions ( h1)-(h6 ) hold , then for empirical log - likelihood for @xmath134 : @xmath135 we have : @xmath136 .",
    "this result can be used to make test of hypothesis or to construct asymptotic confidence region for the response variable .",
    "@xcite constructs a weight - corrected empirical log - likelihood ratio for @xmath134 which is also asymptotically chi - squared .",
    "+ let be now following functions constructed using the reconstituted response : @xmath137 \\ef(\\ex_i;\\eb ) , \\qquad i=1 , \\cdots , n.\\ ] ] consider also the empirical log - likelihood associated at @xmath138 : @xmath139 then , the equivalent of ( [ e7 ] ) is : @xmath140 consider following lemma needed for theorem [ theorem5 ] .",
    "[ lemma4 ] under assumptions ( h1)-(h7 ) , we have : + ( i ) @xmath141 .",
    "+ ( ii ) @xmath142 . + ( iii ) @xmath143 .",
    "following result shows that the empirical log - likelihood ratio on @xmath18 based on the reconstituted data converges to towards @xmath95 .",
    "this theorem shows in a similar way as theorem [ theorem1 ] , then the proof will be omitted .",
    "[ theorem5 ] under assumptions ( h1)-(h7 ) we have : @xmath144 .    from theorem [ theorem5 ]",
    ", one can construct an asymptotic @xmath96-level confidence region for @xmath15 using all available values for @xmath5 and the reconstituted values for @xmath126 .    in a similar way in the complete data , corollary [ corollaire1 ] ,",
    "the statistic @xmath145 may be approximated by : @xmath146 with @xmath147 .",
    "the asymptotic distribution of @xmath148 is @xmath95 .",
    "this implies that for testing hypothesis @xmath90 we can use the statistic @xmath148 with asymptotic reject region @xmath149 where @xmath98 is the @xmath96 quantile of the chi - squared distribution with degrees of freedom @xmath99 .    since the convergence rate of @xmath103 to @xmath18 can be slower than @xmath150 ( see @xcite ) , then lemma [ lemma4 ] can not be true and the analogue of the theorem [ theorem5 ] can not be consider for lad estimator .",
    "we can minimise @xmath92 and we obtain another estimator @xmath151 of @xmath18 , called the maximum empirical likelihood estimator ( mele ) . using the same arguments as used in the proof of theorem 1 in @xcite , we obtain :    [ theorem4 ] under assumptions ( h1 ) , ( h2 ) , ( h4 ) , ( ha ) and :    * @xmath152 $ ] is positive definite , * @xmath153 is continuous in a neighborhood of the true value @xmath18 , @xmath154 and @xmath155 are bounded by some integrable function in this neighborhood , * the rank of @xmath156 $ ] is @xmath99 , * @xmath157 is continuous in a neighborhood of the true value @xmath18 , @xmath158 is bounded by some integrable function in this neighborhood    then + ( i ) convergence rate of @xmath151 is @xmath159 : @xmath160 .",
    "+ ( ii ) @xmath161 , with @xmath162 , @xmath163-\\ee[\\pi(\\ex ) \\ef(\\ex;\\ebo ) \\ef^t(\\ex;\\ebo ) \\ef(\\ex;\\ebo ) \\ef^t(\\ex;\\ebo)]$ ] .",
    "in this section we use monte carlo simulation to compare empirical likelihood method with normal method . for nominal confidence level @xmath164 , using the simulated samples , we evaluated the coverage probabilities ( cp ) of the confidence regions ( cr ) given by : + - approximated empirical log - likelihood method on the completely observed data ( theorem [ theorem1 ] ) : @xmath165 , @xmath166 by ls or lad method , respectively , where @xmath98 is the @xmath96 quantile of the standard chi - square distribution with @xmath99-degrees of freedom and @xmath167 given by ( [ e19 ] ) ; + - approximated empirical log - likelihood @xmath168 , given by ( [ e20 ] ) , associated at @xmath138 on the reconstituted data : @xmath169 ; + - normal method , based on theorem [ theorem2 ] : + @xmath170 and + @xmath171 . + throughout this section , the kernel function is taken as the epanechnikov kernel @xmath172 , the bandwidth sequence @xmath173 which satisfies assumption ( h5 ) .",
    "we generate @xmath174 monte carlo random samples of size @xmath175 for @xmath176 and for errors either @xmath177 either laplace @xmath178 or cauchy @xmath179 , with @xmath180 .",
    "we consider following two cases of response probability under the mar assumption : + _ a ) _",
    "@xmath181 if @xmath182 and 0.95 elsewhere ( similar the linear case of @xcite )",
    ". the average missing rate is 0.91 and empirical mean of @xmath183 is 0.905 for @xmath176 .",
    "@xmath184 for all @xmath185 .",
    "+ case a ) unlike to case b ) excludes most of the error outliers .",
    "+ the coverage probability are estimated by the frequency of the true values @xmath18 falling into the confidence intervals in m=2000 simulations .",
    "+ we denote by @xmath186 , @xmath187 , @xmath188 , @xmath189 , @xmath190 the coverage probabilities corresponding to the confidence regions @xmath191 , @xmath192 , @xmath193 , @xmath194 , @xmath195 respectively .",
    "+ all simulations , calculations of estimations and statistical computations were performed using r language . to calculate the ls estimations on nonlinear model we used _",
    "function of package _ stats _ and on linear model _",
    "lm _ function of package _",
    "base_. to calculate the lad estimations function _ nlrq _ of package _ quantreg _ was used if @xmath196 is nonlinear and _ rq _ function of the same package for linear model .",
    "we used also _",
    "package for random generation of the laplace distribution by _",
    "rlaplace _ function . to generate normal and cauchy distribution the functions _ rnorm _ respectively _ rcauchy _ of _ stats _ package are used .",
    "let us consider following nonlinear function corresponding a two - compartment model : @xmath197 with the true values : @xmath198 and @xmath199 .",
    "+ in figures [ figure 1 ] , [ figure 2 ] , [ figure 3 ] a simulation of this model is plotted for @xmath200 , @xmath184 , @xmath201 for errors @xmath202 respectively .",
    "we represented with `` solid circle '' the reconstituted values @xmath203 and with `` triangle '' the true complete values @xmath45 .",
    "+ in table [ tableau1 ] we get the coverage probability @xmath186 , @xmath187 , @xmath188 , @xmath189 , @xmath190 in the case @xmath204 if @xmath182 and 0.95 elsewhere when @xmath200 , @xmath205 . in table [ tableau2 ]",
    "we get the same five coverage probabilities for the case @xmath206 .",
    "these two tables show that :    1 .   @xmath186 is better than @xmath187 excepting for the case @xmath207 ; 2 .",
    "coverage probabilities on the reconstituted responses @xmath208 is bigger than 0.95 and has values very closed to values of @xmath186 obtained on complete data ; 3 .",
    "@xmath189 , @xmath190 are worse than coverage rate on the complete or reconstituted data , particularly for @xmath205 or cauchy errors",
    ". for cauchy distribution we can not calculate @xmath189 since the assumptions of theorem [ theorem2](i ) , more precisely ( h1 ) , are not satisfied ; 4 .   since case a ) has tendency to eliminate the outliers of @xmath4 , the rates of normal coverage probability are bigger for a ) than for b ) .",
    "tables [ tableau3 ] and [ tableau3bis ] contain empirical mean , standard - deviation of parameter estimations @xmath209 and @xmath210 for the two cases of response probability .",
    "we deduce from these two tables :    1 .   there is no difference between the parameter estimations obtained for both cases of probability a ) and b ) , for the same @xmath175 and the same distribution of @xmath4 ; 2 .",
    "already known thing : for distributions with outliers ( laplace or cauchy here ) the lad estimators have a standard - deviation smaller than ls estimators .",
    "table [ tableau4 ] : whether it is for the probability a ) or b ) , the medians of lad estimations are more close to the true values than the medians of ls estimations , if @xmath4 have outliers .",
    "the precision of the estimators increases with @xmath175 and in a less measure with the probability @xmath211 .",
    "consider now the simple linear model : @xmath212 , with the true value of the parameter @xmath213 . in tables [ tableau1linear ] and [ tableau2linear ] we get the coverage probabilities for the two case of @xmath214 and for @xmath215 .",
    "from these two tables we make the following remarks :    1 .   in general @xmath187 performs better than @xmath186 when @xmath216 or @xmath217 ; 2 .",
    "@xmath208 is bigger than 0.95 and has very close values to @xmath186 values ; 3 .   similar of ( 3 ) for nonlinear case , except that @xmath218 ; 4 .   with regard to both cases of probability @xmath214 : there is no difference between the three coverage probabilities by estimated el methods , contrary to the nonlinear case nor between the natural coverage probabilities .    tables [ tableau3linear ] and [ tableau4linear ] : the standard - deviation of ls estimators is much bigger than for lad s when @xmath219 .",
    "that comes from the outliers presence in nonlinear case , the algorithm to find the ls estimator can not converge , thus the estimations are not available .",
    "the coverage probabilities of cr given by normal method are lower than the nominal level @xmath220 especially for small sample sizes .",
    "the simulation results show that in terms of coverage probability the empirical likelihood methods outperform the normal approximation method in particular when error distribution have outliers or its has a bigger standard - deviation . + it is interesting to note that , if the distribution of the errors presents outliers the approximated el for lad method gives generally best results than that for ls method that concerns el coverage probability or parameter estimators efficiency . then , if @xmath221 , in order to test hypothesis @xmath222 , it is recommended to take the test statistic @xmath167 given by ( [ e19 ] ) for complete data or @xmath168 , given by ( [ e20 ] ) for improved data using ls method",
    "let us emphasize that @xmath168 gives very close results to those of @xmath104 .",
    "on the other hand , if @xmath4 presents outliers , to test @xmath222 it is recommended @xmath167 given by ( [ e19 ] ) at @xmath223 .",
    "in r language consider _ chwirut1 _ data of _ nistnls _ package to model the ultrasonic response value @xmath126 function to metal distance value @xmath224 using nonlinear function : @xmath225 the realizations of @xmath226 are known for @xmath227 observations . in figure [ figure 4 ] we represented variable @xmath126=ultrasonic response function of regressor @xmath224=metal distance : `` solid circle '' for reconstituted values @xmath203 , `` triangle '' for true complete values @xmath45 .",
    "the ls parameter estimations on all 214 observations are : @xmath228 , @xmath229 , @xmath230 .",
    "by 10000 monte carlo samples , we eliminate @xmath231 , @xmath232 , @xmath233 values of @xmath126 .",
    "consider that the true value @xmath18 of @xmath15 is the one obtained on 214 observations . in table",
    "[ tableau6 ] we have the acceptance rate of hypothesis @xmath222 with respect to missing probability @xmath234 and with respect to the estimation method ( ls or lad ) on complete data using statistic @xmath167 given by ( [ e19 ] ) , on reconstituted data using statistic @xmath168 given by ( [ e20 ] ) .",
    "we observe that , even by eliminating @xmath233 observations , the obtained estimations are very close to @xmath18 . in table",
    "[ tableau7 ] we find the empirical means , standard - deviations of differences @xmath235 , @xmath236 , where : @xmath45 are the true values of @xmath126 , @xmath237 is the forecast for @xmath45 using all 214 observations by ls method , @xmath238 . for @xmath239 , @xmath240 is the reconstituted value of @xmath45 by relation ( [ yy ] ) .",
    "we deduce that the variable response is better reconstituted by ( [ yy ] ) than by ls method on all observations .",
    "we give proofs for the results in sections 3 and 4 .",
    "+ in the following , we denote by @xmath241 a generic positive finite constant not depending on @xmath175 which may take different values in different formulae or even in different parts of the same formula . + * proof of lemma [ lemma2 ] * _ ( i ) _ we apply the central limit theorem . + _",
    "( ii ) _ by the weak law of large numbers . + _ ( iii ) _ we combine lemma 3 of @xcite and results for nonlinear regression without missing data ( see e.g. @xcite ) . @xmath242 + * proof of theorem [ theorem1 ] * we take the taylor s expansion of @xmath92 given by ( [ e7 ] ) : @xmath243 . by a similar approach of @xcite ,",
    "using also lemma [ lemma2 ] , we obtain : @xmath244 . using lemma [ lemma2](iii )",
    "we obtain : @xmath245 then , returning at @xmath92 , we have : @xmath246  relation ( [ e8 ] ) can be written also : @xmath247 on the other hand , using ( [ e11 ] ) , we obtain : @xmath248 thus @xmath249 relation ( [ e12 ] ) implies also : @xmath250 .",
    "then , expression ( [ e10 ] ) of @xmath92 becomes , using relation ( [ e13 ] ) : @xmath251 by lemma [ lemma2](i ) and ( ii ) we have : @xmath252 .",
    "@xmath242 + * proof of theorem [ theorem2]*.(i ) by definition , the ls estimator is obtained as the solution of the system : @xmath253 .",
    "using lemma [ lemma2](i ) we have : @xmath254 , then @xmath255\\overset{{\\cal l } } { \\underset{n \\rightarrow \\infty}{\\longrightarrow}}{\\cal n}(0 , \\textbf{b})$ ] .",
    "this last relation implies : @xmath256=o_{\\ep}(n^{-1/2}).\\ ] ] on the other hand , using taylor expansion : + @xmath257-\\delta_i \\ef(\\ex_i;\\ebo)[y_i - f(\\ex_i;\\ebo ) ] $ ] + @xmath258 .",
    "+ @xmath259 + since @xmath260 , we obtain : @xmath261 @xmath262 , with @xmath263 , @xmath264 , @xmath265 $ ] .",
    "+ under assumptions ( h1 ) and ( h4 ) : + @xmath266 . taking into account relation ( [ e14 ] ) , we have : @xmath267 combining these last two results , we obtain : @xmath268 and claim follows .",
    "+ ( ii ) we apply @xcite .",
    "@xmath242 + * proof of lemma [ lemma3 ] * _ ( i ) _ let us consider following decomposition : + @xmath269 , with : + @xmath270 , + @xmath271 , + @xmath272",
    "@xmath273=var[\\frac{\\delta_i \\varepsilon_i}{\\pi(\\ex_i)}]+var[f(\\ex_i;\\ebo)]+2cov(f(\\ex_i;\\ebo),\\frac{\\delta_i \\varepsilon_i}{\\pi(\\ex_i)})$ ] .",
    "by ( h1 ) we have @xmath274 . on the other hand : @xmath275=\\ee\\left[\\frac{\\delta_i^2 \\varepsilon_i^2}{\\pi^2(\\ex_i)}\\right]=\\ee\\left[\\frac{\\delta_i \\varepsilon_i^2}{\\pi^2(\\ex_i)}\\right]=\\ee\\left[\\frac{\\varepsilon^2(\\ex)}{\\pi(\\ex)}\\right]$ ]",
    ". since @xmath276 are independent for different @xmath277 , then the random variables @xmath278 are also independent .",
    "then , we can apply the central limit theorem : @xmath279 .",
    "we make a limited development for @xmath280 until order 2 around @xmath18 and taking into account relation ( [ e15 ] ) , hypothesis @xmath53 and ( h4 ) : + @xmath281 . by lemma 3 of @xcite",
    "we have @xmath282 , then the claim ( i ) is proved .",
    "the proof of ( ii ) and ( iii ) is similar . @xmath242 + * proof of lemma [ lemma4 ] * _ ( i ) _ function @xmath283 can be written : @xmath284 \\ef(\\ex_i;\\eb).\\ ] ]    for the first term of the right - hand side of ( [ e16 ] ) we have : @xmath285 on the other hand , using a limited development , relation ( [ e15 ] ) and assumption ( h4 ) : @xmath286 \\ef(\\ex_i;\\ebo)$ ] + @xmath287 , with @xmath288 between @xmath18 and @xmath102 .",
    "let us consider following random variable : + @xmath289 , for @xmath15 in a @xmath150-neighborhood of @xmath18 .",
    "if we prove : @xmath290 with @xmath291 uniformly in @xmath15 , tacking into account also ( [ e15 ] ) , we obtain ( i ) .",
    "+ let us remind at first some results of the paper @xcite , on the estimators @xmath107 of @xmath105 . under assumptions",
    "( h3 ) , ( h6 ) and ( h7 ) we have uniformly over @xmath292 : @xmath293 ^ 2=o((nh_n^d)^{-1}m_n^d)+o(h_n^{2\\max(2,d-1)})+o(n^{-1/2}).\\ ] ] in the same paper , following two random variables are considered : @xmath294 , for @xmath32 , @xmath295 and @xmath296 with @xmath118 , @xmath120 positive constants defined in assumption ( h6 ) . for this last random variable",
    "we have @xmath297 .",
    "+ we now turn to study @xmath298 , which can be written , after a limited development : @xmath299 with @xmath300 between @xmath18 and @xmath15 .",
    "we denote decomposition ( [ * 1 ] ) : @xmath301 . using cauchy - schwarz inequality , relation ( [ e15 ] ) and assumption ( h4 ) ,",
    "it is easily shown that @xmath282 uniformly in @xmath15 .",
    "we study now @xmath302 .",
    "second term @xmath303 can be written : @xmath304 .",
    "the last term of @xmath303 is @xmath291 by ( [ elemma1 ] ) .",
    "for the first term of @xmath303 we have the decomposition : + @xmath305 , with + @xmath306 $ ] , + @xmath307 $ ] , + @xmath308 .",
    "+ by cauchy - schwarz inequality , assumptions ( h4 ) , ( h7 ) we have : + @xmath309 \\leq \\frac{c}{n^2 } \\sum^n_{i=1 } \\ee \\cro {   \\ee\\cro { \\sum^n_{j=1 } w^2_{nj}(\\ex_i)[\\pi(\\ex_i)-\\pi(\\ex_j)]^2 | \\ex_i   } } $ ] + @xmath310 ^ 2 | \\ex_i   } } $ ] @xmath311 for @xmath10",
    ". then @xmath312 .",
    "+ for @xmath313 : @xmath314 \\leq \\frac{c}{n^2 } \\sum^n_{i=1 } \\ee \\cro{\\sum^n_{j=1 } w^2_{nj}(\\ex_i ) } \\leq \\frac{c}{n^2 } \\sum^n_{i=1 } \\ee \\cro{\\frac{1}{c_n(\\ex_i)}}\\rightarrow 0 $ ] for @xmath10 , then @xmath315 . in a similar way : + @xmath316 \\leq \\frac{c}{n^2 } \\sum^n_{i=1 } \\cro{\\ee \\pth{\\frac{\\delta_i}{\\pi^2(\\ex_i ) } \\| \\ef(\\ex_i;\\ebo ) \\|^2 |\\ex_i } \\pth{1-\\sum^n_{j=1 } w_{nj}(\\ex_i ) } } \\rightarrow 0 $ ] .",
    "thus @xmath317 , what implies @xmath318 . + finally , for @xmath319 : @xmath320=0 $ ] and with the same arguments as for @xmath313 : @xmath321\\rightarrow 0 $ ] for @xmath322 , then @xmath323 . with all this ,",
    "relation ( [ * 0 ] ) is proved .",
    "+ the proof of ( ii ) and ( iii ) is similar .",
    "@xmath242 +    3 bai , j. , 1998 , estimation of multiple - regime regressions with least absolute deviation . , * 74 * , pp .",
    "103 - 134 .",
    "ciuperca , g. , 2010 , .",
    "estimating nonlinear regression with and without change - points by the lad - method .",
    ", doi : 10.1007/s10463 - 009 - 0256-y .",
    "kim , h.k . , choi , s.h .",
    ", 1995 , asymptotic properties of non - linear least absolute deviation estimators . , * 24 * , pp",
    ". 127 - 139 .",
    "liang h. , qin y. , zhang x. , ruppert d. , 2009 , empirical likelihood - based inferences for generalized partially linear models .",
    ", * 36*(3 ) , 433 - 443 .",
    "oberhofer , w. , 1982 , the consistency of nonlinear regression minimizing the @xmath324-norm .",
    ", * 10 * , no .",
    "316 - 319 .",
    "owen a. , 1990 , empirical likelihood ratio confidence regions , ( 1 ) , 90 - 120 .",
    "qin y. , li l. , lei q. , 2009 , empirical likelihood for linear regression models with missing responses , ( 11 ) , 1391 - 1396 .",
    "qin j. , lawless j. , 1994 , empirical likelihood and general estimating equations , ( 1 ) , 300 - 325 .",
    "seber g.a.f .",
    ", wild c.j . , 2003 , nonlinear regression , wiley series in probability and mathematical statistics , john wiley @xmath325 sons , inc . , hoboken , new jersey .",
    "sun z. , wang , q. , dai p. , 2009",
    ", model checking for partially linear models with missing responses at random . , * 100*(4 ) , 636 - 651 .",
    "sun z. , wang , q. , 2009 , checking the adequacy of a general linear model with responses missing at random . , * 139*(10 ) , 3588 - 3604 .",
    "wang , q. , rao j.n.k . , 2002 , empirical likelihood - based inference in linear models with missing data .",
    ", * 29*(3 ) , 563 - 576 .",
    "wang , q. , linton o. , hrdle w. , 2004 , semiparametric regression analysis with missing response at random . , * 99*(466 ) , 334 - 345 .",
    "wang , q. , sun z. , 2007 , estimation in partially linear models with missing responses at random .",
    ", * 98*(7 ) , 1470 - 1493 .",
    "weiss , a.a .",
    ", 1991 , estimating nonlinear dynamic models using least absolute error estimation . , * 7 * , 46 - 68 .",
    "xue l. , 2009 , empirical likelihood for linear models with missing responses , , 1353 - 1366 .",
    "xue l. , 2009 , empirical likelihood confidence intervals for response mean with data missing at random , ( 4 ) , 671 - 685 .",
    "yang y. , xue l. , cheng w. , 2009 , empirical likelihood for a partially linear model with covariate data missing at random . , * 139*(12 ) , 4143 - 4153 .",
    "zhao y. , chen f. 2008 , empirical likelihood inference for censored median regression model via nonparametric kernel estimation , ( 2 ) , 215 - 231 ."
  ],
  "abstract_text": [
    "<S> a nonlinear model with response variable missing at random is studied . in order to improve the coverage accuracy , </S>",
    "<S> the empirical likelihood ratio ( el ) method is considered . </S>",
    "<S> the asymptotic distribution of el statistic and also of its approximation is @xmath0 if the parameters are estimated using least squares(ls ) or least absolute deviation(lad ) method on complete data . </S>",
    "<S> when the response are reconstituted using a semiparametric method , the empirical log - likelihood associated on imputed data is also asymptotically @xmath0 . </S>",
    "<S> the wilk s theorem for el for parameter on response variable is also satisfied . </S>",
    "<S> it is shown via monte carlo simulations that the el methods outperform the normal approximation based method in terms of coverage probability up to and including on the reconstituted data . </S>",
    "<S> the advantages of the proposed method are exemplified on the real data . </S>",
    "<S> + _ keywords : _ random nonlinear model ; response missing at random ; empirical likelihood ; semi - parametric estimation ; </S>"
  ]
}