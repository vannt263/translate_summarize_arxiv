{
  "article_text": [
    "the vlasov equation models the evolution of a plasma in an external or self - consistent field . in its full generality ,",
    "the model consists of an advection equation in the six - dimensional phase space coupled to maxwell s equations . since analytical solutions",
    "are usually not known , the numerical simulation of these problems is of fundamental importance . due to the high dimensionality and the development of small structures",
    "the numerical solution is very challenging .",
    "there are essentially three classes of solvers that are used in simulations : particle - in - cell ( pic ) methods , eulerian solvers , and semi - lagrangian methods .",
    "the idea of particle methods ( cf .",
    "e.g. @xcite ) is to distribute a number of macro - particles in the computational domain that are evolved according to the equations of motion .",
    "eulerian as well as semi - lagrangian solvers , on the other hand , are based on a phase - space grid . in a eulerian solver ( cf . @xcite and references therein )",
    ", the spatial derivatives are approximated by a standard method ( e.g. finite volumes @xcite or discontinuous galerkin @xcite ) and the system is advanced in time using an ode solver ( e.g. runge  kutta ) .",
    "semi - lagrangian solvers ( cf .",
    "e.g. @xcite ) update the values of the grid point by evolution along characteristics .",
    "they have the advantage that they do not suffer from time step restrictions by the courant  friedrichs  lewy ( cfl ) condition .",
    "grid - based methods suffer from the curse of dimensionality , i.e. , from the fact that the number of unknows grows with the number of dimensions of the problem .",
    "for this reason , grid - based simulations of the 6d vlasov equations are rarely done .",
    "monte - carlo methods are computationally less expensive for high - dimensional problems but suffer from a numerical noise problem .    to alleviate the curse of dimensionality for grid - based solvers , several methods especially suited for high - dimensional problems",
    "have been developed in the numerical community .",
    "such methods are the sparse - grid method @xcite , tensor - based methods @xcite , and radial basis functions @xcite . in this paper , we consider the solution of the vlasov  poisson system in tensor train format @xcite which is a type of tensor - based methods with attractive numerical properties .",
    "the main concept of this method is to represent the solution as nested sums of tensor products .",
    "the compression of full - grid data to a sum of tensor products can be achieved by performing high - order singular value decompositions ( hosvd ) .",
    "the hosvd can also be used to recompress data during time - dependent simulations .",
    "furthermore , all basic numerical operations are defined in the tensor train ( tt ) format .",
    "even though small filaments evolve in the solution of the vlasov equation , it has been shown in @xcite that the data from a gyrokinetic vlasov simulation can be compressed by hosvd .",
    "tensor - based methods were introduced in the quantum chemistry community by meyer et al .",
    "@xcite and have been further developed since then . in recent years , mathematical aspects of tensor - based methods",
    "have been addressed and formalized in the tensor train @xcite as well as the hierarchical tensor @xcite format .",
    "the tensor train format has beed applied to the solution of various high - dimensional differential equations @xcite . in particular ,",
    "dolgov et al .",
    "@xcite have designed a tensor train algorithm to solve the farley ",
    "buneman instability in ionospheric plasma .",
    "the equations are similar to the vlasov ",
    "poisson equations considered in this paper .",
    "the outline of the paper is as follows . in the next section",
    ", we introduce the split - step semi - lagrangian solver for the vlasov  poisson equation .",
    "then we give a short summary of the tensor train format in ",
    "[ sec : tt ] .",
    "the tensor train semi - lagrangian method is devised in  [ sec : ttsl ] and its efficient implementation is discussed in ",
    "[ sec : implement ] .",
    "in particular , we discuss an efficient implementation of the tt hadamard product and compression of the interpolation operator .",
    "numerical results are provided in  [ sec : numerics ] and conclusions as well as future research directions are given in ",
    "[ sec : conclusions ] .",
    "the evolution of the distribution function @xmath2 of a plasma is governed by the vlasov equation @xmath3 where the force field @xmath4 is given by the lorentz force due to external and self - consistent electromagnetic fields .",
    "the self - consistent fields can be computed by maxwell s equations .",
    "if the magnetic field is small , it suffices to consider @xmath5 .",
    "then the self - consistent part of the electric field can be computed by the poisson equation @xmath6 where @xmath7 is the particle density . in this paper , we focus on such vlasov  poisson equations .",
    "the vlasov equation is a hyperbolic equation and the associated characteristics satisfy the differential system @xmath8 if the evolution of the characteristic curves is known , the distribution function at time @xmath9 can be computed from a given initial distribution @xmath10 at time @xmath11 as @xmath12 the idea of the semi - lagrangian method is to discretize the initial distribution on a mesh . in each time step , the characteristic equations are solved numerically backwards in time and the new solution at the grid points is given by the ( interpolated ) value of the previous solution at the origin of the corresponding characteristic .    for the vlasov ",
    "poisson equation , the coefficient of the @xmath13 gradient is only dependent on @xmath14 and vice versa .",
    "therefore , a split step method can be designed where ( constant ) advection along one coordinate direction is considered at a time . in this case , the equations for the characteristics can be solved exactly .",
    "this yields the split - step semi - lagrangian scheme shown in algorithm [ alg : sssl ] which was originally introduced by chen and knorr @xcite .",
    "note that the interpolation along the @xmath15-dimensional @xmath13- and @xmath14-coordinates can be split into @xmath15 separate one - dimensional interpolations .",
    "various aspects of the semi - lagrangian method for the vlasov equations have for instance been discussed in @xcite .    given @xmath16 and @xmath17 at time @xmath18 ,",
    "we compute @xmath19 at time @xmath20 as follows :    1 .",
    "solve @xmath21 on half time step : @xmath22 .",
    "2 .   solve @xmath23 on half time step : @xmath24 .",
    "3 .   compute @xmath25 and solve the poisson equation for @xmath26 .",
    "4 .   solve @xmath27 on half time step : @xmath28 .",
    "on a tensor product grid , the number of points grows exponentially in the dimension variable . in many cases ,",
    "however , the complete information  or at least an accurate approximation  can be recovered from much less data .",
    "the simplest example is a function of the form @xmath29 on a grid with @xmath30 points along dimension @xmath31 , it suffices to store the @xmath32 function values @xmath33 , @xmath34 , @xmath35 .",
    "the value at any grid point can be reconstructed from this data .",
    "generalizing from this very special case , the tensor train ( tt ) format @xcite offers the possibility of representing a multidimensional function as nested sums of such kronecker products which yield good approximations of much more complicated functions . in the tensor train format , each dimension @xmath31 is represented by a kernel @xmath36 , in which the second index runs over the grid points along the @xmath31th dimension and the first und third index take care of couplings to the @xmath37th and @xmath38th dimension , respectively .",
    "the value at the grid point @xmath39 can be reconstructed as @xmath40 the size of the _ ranks _",
    "@xmath41 , @xmath42 , depends on the structure of the function and the level of accuracy required .",
    "since @xmath43 is a scalar function , we have @xmath44 and we will sometimes omit the corresponding index .    in case",
    "we have a function represented as a @xmath45-dimensional tensor , we can use a sequence of singular value decompositions ( svd ) applied to matrifications of the tensor to find a representation in tt format to a given error tolerance or with a fixed maximum rank .",
    "the tensor train format offers not only compression of high - dimensional data but also provides simple algorithms for basic tensor - tensor and matrix - tensor operations .",
    "for instance , we can build the kernels of the sum of two tt tensors @xmath46 and @xmath47 by setting @xmath48 this operation is essentially a copying function .",
    "however , the ranks of @xmath49 and @xmath50 sum up to the ranks of @xmath51 . in a matrix - vector product ,",
    "the ranks of the matrix and the vector are even multiplied .",
    "obviously , continued application of basic operations , for instance to propagate the tensor in time , will destroy the compression . most often , however , the representation of the new tt tensor can be truncated and it is essential to continuously add rounding steps to any algorithm operating on tt tensors . since",
    "one wants to be able to truncate one rank at the time , a left - to - right sweep with qr - decompositions of the kernels is performed to orthonormalize all kernels except for the last .",
    "then , the kernels are singular - value decomposed individually in a right - to - left sweep where only the non - orthogonal kernel is touched in each iteration .",
    "algorithm [ alg : ttround ] implements the tt rounding .",
    "note that we use an _ absolute _ threshold in constrast to the rounding in @xcite .",
    "the rounding requires the computations of @xmath52 qr decompositions ( for orthonormalization ) as well as @xmath52 svd .",
    "since we are not interested in the zero blocks , it suffices to compute an economy - size decomposition in both cases , i.e. , the vectors corresponding to zero blocks are left out .",
    "computing such economy - size qr or sv decompositions for a @xmath53 matrix is of complexity @xmath54 ( cf .",
    "the complexity of a rounding step is hence @xmath55 where we have assumed all ranks to be equal to @xmath56 and @xmath57 grid points along each dimension",
    ". a more detailed description of operations in tt format can be found in @xcite .",
    "in this section , we explain how a split - step semi - lagrangian method can be designed in tensor train format .",
    "first , we derive the formulas for the example of linear interpolation in two dimensions ( i.e. 1d vlasov ) before discussing other interpolation formulas and higher dimensions .",
    "we also discuss suitable ordering of the coordinates in four and six dimensions and the effects of tt rounding .",
    "the conservation properties of the method are discussed and we propose a projection to the manifold spanned by constant mass and momentum . finally , we shortly discuss the solution of the poisson problem .",
    "consider the @xmath13-advection in two dimensions .",
    "we assume a tensor train representation of the distribution function at time @xmath18 of the form @xmath58 we now consider the displacement in @xmath13 direction by @xmath59 . to keep the presentation simple ,",
    "we derive the formulas for linear interpolation . even though not necessary for stability",
    ", we impose the cfl - like condition that @xmath60 where @xmath61 is the grid spacing along @xmath13 . on a full grid ,",
    "the distribution function at the new time step would be computed according to the formula @xmath62 where the indices are periodically shifted for periodic boundary conditions .",
    "note that the displacement can either be positive or negative . for a function in tensor train format",
    "the kernels representing @xmath19 are computed from the kernels of @xmath16 by    @xmath63    this can be interpreted as the sum of three tensor trains . in each case , the first kernel is a shifted version of the original kernel und the second kernel is scaled depending on the value of @xmath14 .",
    "hence , we can compute the advection in @xmath13 direction performing the following steps :    1 .",
    "form the three shifted kernels of @xmath64 .",
    "2 .   form three scaled @xmath65-kernels that are line - wise multiplied by @xmath66 , @xmath67 , and @xmath68 , respectively .",
    "3 .   form three tt - tensors from the resulting kernels . 4 .   add the tt - tensors and perform a rounding step .",
    "each of the tensors formed in step 1 and 2 have the same rank as the original tensor . adding the tensors will increase the rank ( by a factor three in this case ) .",
    "however , the rank can usually be reduced again by performing a rounding step .",
    "we can also write the evolution operator as a matrix @xmath49 in tensor product form .",
    "if we denote by @xmath69 the matrix with one diagonal of ones shifted by @xmath70 from the center , we have @xmath71 this can be written in tt format as a matrix with rank @xmath72 .",
    "it is obvious that the advection with respect to @xmath14 can be done in the same way by interchanging the roles of @xmath73 and @xmath74 .    in principle",
    ", we can use any other interpolator in our derivation .",
    "especially , a centered lagrange interpolator that includes @xmath75 points will result in @xmath76 tt - tensors that have to be formed by shifting the @xmath73-kernel , scaling the @xmath74-kernel , and finally adding the @xmath77 kernels . for a non - nodal interpolator , like splines",
    ", step 2 needs to be augmented . before shifting the kernel ,",
    "the values of the interpolator weights have to be computed for each column @xmath78 .",
    "similar to a eulerian solver and opposed to the usual case for semi - lagrangian solvers we have imposed the cfl - like condition .",
    "however , it is possible to relax this condition .",
    "the condition was not introduced to ensure stability but to make sure that we only have to consider @xmath76 points for a centered interpolator with @xmath75 points .",
    "we can relax the condition at the price of additional terms in the sum .",
    "for the condition @xmath79 with some @xmath80 , the number of points will be @xmath81 .",
    "since @xmath75 intervals will be used at a time , the scaled @xmath74-tensors will contain an increasing number of zeros ( as @xmath82 increases ) .",
    "this might be exploited to further improve on the efficiency .      in higher dimensions ,",
    "the 2d algorithm can be applied in essentially the same way to parts of the kernels while others are left unchanged .",
    "in particular , the advection along a spatial dimension will only depend on one ( velocity ) dimension also in 4d and 6d .",
    "then , we treat the two corresponding kernels as discussed in the previous section and keep the other two or four kernels unchanged .    for the velocity advection ,",
    "the situation becomes more involved . for simplicity ,",
    "we consider the 4d case .",
    "the generalization to 6d is straight forward .",
    "the displacement is now not only dependent on one dimension but on two .",
    "let us revisit the linear interpolation and consider the advection along @xmath83 @xmath84 the displacement @xmath85 is a function of two variables and we assume we are given its tt representation .",
    "for the linear interpolation , we need a tt representation of @xmath86 , @xmath87 , and @xmath88 .",
    "let us denote them by @xmath89 and by @xmath90 the ranks of each tt tensor",
    ". then becomes @xmath91 thus , we have to form ( for each @xmath92 ) @xmath90 scaled versions of the 2d tt tensor represented by @xmath93 and add them up .",
    "to keep the size of the rank small , it is advantageous to truncate after each addition .",
    "note that this operation can also be described as a multiplication of the tt - tensor @xmath94 by the matrix @xmath95 .",
    "this is , of course , much more expensive than step 3 of the 2d algorithm but there is no additional difficulty due to the use of the tt format .",
    "now , we turn to the question of how to compute the kernels @xmath96 . for this , we need to compute the positive and negative part of a tt tensor .",
    "this can not easily be done because the value at the various grid points is not stored explicitly in tt format . since the displacement is just a 2d ( or 3d ) object",
    ", one might as well compute its values on the full grid to perform these operations",
    ". however , the positive and negative part are non - smooth functions and the compression in tt format will generally be rather poor .",
    "this problem is not specific to linear interpolation .",
    "the important fact is that we use a different interpolation function depending on the interval into which the point is displaced .",
    "an alternative is to always use the lagrange polynomial computed on an odd number of points around the original point .",
    "as long as we impose a cfl - like condition that makes sure that we do not displace more than the grid size , we interpolate close to the center . in this case",
    ", we only have to compute polynomials of the displacement which can easily be done in tt format .",
    "however , we have to make sure the displacements stay small , i.e. , relaxing the cfl - like condition will generally result in rather poor approximations . however",
    ", the displacement in the @xmath14-advection step due to the electric field is usually small compared to the displacement in the @xmath13-advection step .",
    "therefore , relaxing the cfl - like condition for the @xmath13-advection step only will already result in a fairly efficient time stepping .",
    "the natural ordering of the coordinates is to start with the spatial coordinates and then add the velocity coordinates . on a full grid ,",
    "a reordering does not change the algorithm . in the tt representation , however , we have an explicit coupling between neighboring dimensions . hence , the compression is affected by the ordering of the variables . to illustrate this , we consider the three - variate function @xmath97 . to represent this function",
    ", we need a tt tensor with ranks @xmath98 and kernels @xmath99 if we instead reorder the variables as @xmath100 , the tt tensor representing @xmath43 has ranks @xmath101 but @xmath102 and kernels @xmath103    analyzing the splitting algorithm , we see that the @xmath104-advection step couples dimensions @xmath104 and @xmath105 and the @xmath105 advection step couples @xmath105 and @xmath106 . therefore , it is reasonable to assume that an ordering that groups the pairs @xmath107 as well as the spatial variables together will result in better compression . in four dimensions , a reordering of the coordinates as @xmath83 , @xmath108 , @xmath109 , @xmath110 satisfies all the requirements",
    "also , it simplifies the advection steps which  up to an initial orthogonalization steps  only involves two or three neighboring kernels of the tt tensor .",
    "moreover , we note that @xmath111 which is why the first and last kernels are only 2-tensors and therefore generally smaller than the inner kernels that are 3-tensors .",
    "this further improves the compression if the number of grid points along the velocity dimensions is higher than along the spatial dimensions .",
    "this situation is not uncommon in vlasov  poisson simulations .    for the six dimensional case , there is no ordering that places together all different coordinate combinations for the six advection steps .",
    "since the coupling appears to be strongest between the pairs @xmath107 , we have found the ordering @xmath83 , @xmath108 , @xmath109 , @xmath110 , @xmath112 , @xmath113 to be most efficient ( up to index shifting ) .",
    "when simulating the vlasov equation with the semi - lagrangian split - step method in tt format , we constantly compress the data for the distribution function .",
    "this adds to the numerical error of the method . in each simulation , we have to decide when to truncate the hosvd computed to recompress the data .",
    "one strategy would be to keep all the information up to round - off errors .",
    "alternatively , we could choose the drop tolerance such that the error from tt rounding is on the same order of magnitude as other numerical errors . even though the first strategy has its advantages , especially with respect to the conservation properties as discussed in the next section",
    ", this will generally become rather expensive .",
    "the reason is that the tensors have to resolve numerical errors that are generally much less smooth than the actual solution .",
    "hence , we will need considerably larger ranks in order to resolve spurious information .",
    "it is therefore recommended to choose the drop tolerance carefully to fit the numerical errors of the underlying method .",
    "however , we need an error estimator for the underlying spatial discretization in order to be able to automatically decide on a proper tolerance .    when simulating the vlasov equation over longer times , filaments evolve .",
    "this means that the distribution function is relatively well - resolved in the beginning .",
    "a simple strategy to account for this fact is to scale the tolerance @xmath114 at final time according to the time step @xmath70 as @xmath115 , where @xmath116 is the total number of time steps in the simulation .",
    "we have used this scaling in our numerical experiments .",
    "many integrals of the solution of the vlasov equation are conserved : mass , momentum , energy , and all @xmath117 norms . moreover ,",
    "the maximum and minimum value of the solution remain constant .",
    "these invariants of the mathematical function mimic the physical behaviour of the distribution function .",
    "therefore , a good numerical method should conserve these properties as accurately as possible . without rounding in the tt representation",
    ", we would  up to roundoff errors  recover the same solution as the underlying method on the full grid . hence",
    ", our method would inherit the conservation properties of the underlying full - grid method .",
    "if we perform rounding up to some drop tolerance , we fulfill the conservation laws of the full - grid method with the accuracy of the rounding .",
    "the conservation thus depends on the drop tolerance . when choosing a loose tolerance , a projection onto the manifold spanned by one or more conservation laws should be considered ( cf .",
    "the next section ) .",
    "since the svd yields a best approximation in @xmath118 sense , the @xmath118 norm is expected to be conserved to a large extent .",
    "if the tolerance is chosen too loose or fixed ranks are used , one can use a projection method ( cf .",
    "iv.4 ) ) to improve conservation . given the propagated solution , the closest solution on the manifold defined by the conserved quantities",
    "is found by minimizing the lagrange function describing this constrained minimization problem . in particular , we consider conservation of mass and momentum . the discrete mass and momentum",
    "are defined as @xmath119 where @xmath120 denotes the set of grid points . the number of points is denoted by @xmath121 .",
    "given the solution @xmath122 at point @xmath123 , obtained by the time evolution algorithm , we add a perturbation by @xmath124 for a suitably chosen lagrange multiplier @xmath125 . in the case of mass and momentum conservation ,",
    "the projection is rather simple since all the @xmath126 projections are orthogonal to each other .",
    "the projected solution is then given by @xmath127 applying this projection to the solution in tt format will increase the rank by @xmath126 since we add @xmath126 rank - one tensors to the solution .",
    "so far , we have only discussed the solution of the vlasov equation",
    ". however , we also have to solve a poisson problem in each time step .",
    "since the poisson equation only depends on the spatial variable , the dimensionality is cut into half . in our prototype implementation",
    ", we have therefore chosen to solve poisson s equation with a pseudo - spectral method on the full grid . however , a pseudo - spectral solver based on the fast fourier transform in tt format @xcite will presumably improve the efficiency of our method .",
    "while it is possible to achieve considerable data compression when using the tt format , it is less obvious if this helps in reducing the computing time .",
    "even though a much smaller amount of data needs to be handled , we have to rely on more complex algorithms . as mentioned in ",
    "[ sec : tt ] the ranks are multiplied in a matrix - vector product in tt format and we have to perform a rounding step together with each matrix - vector product . in this section , we discuss the tt matrix - vector product in more detail and explain how to efficiently implement it for the matrices appearing in our semi - lagrangian solver .",
    "let us consider the matrix - vector product @xmath128 , where all objects are in tt format .",
    "when counting arithmetic operations , we assume all ranks of the matrix to be @xmath129 and all ranks of the vector to be @xmath56 and the matrix to be quadratic . in practice , the complexity will be dominated by the maximum rank .",
    "the complexity of a direct matrix vector product , i.e. , multiplying the kernel and then performing a tt rounding on @xmath130 at the end , is @xmath131 , where the first part is due to the matrix vector product and the second due to the tt rounding .",
    "if the matrix is a ( nested sum of ) kronecker products of sparse matrices in each dimension , the complexity is reduced to @xmath132 . in this case , the complexity is clearly dominated by the rounding operation .",
    "already if the ranks are on the order 10 , the factor @xmath133 becomes significant .",
    "compared to the complexity of the sparse matrix - vector product on the full grid , @xmath134 , this might not be small for @xmath135 .",
    "however , the optimal rank of @xmath130 is usually close to @xmath136 rather than @xmath137 .",
    "therefore , the complexity can be improved if the matrix - vector product is not computed directly but approximation to a certain threshold or maximum rank size is incorporated into the matrix vector product .    as an alternative to svd - based rounding ,",
    "methods within the alternating direction framework pose the problem of finding a low - rank approximation to a tensor as an optimization problem .",
    "these methods are iterative and optimize on one ( or two ) kernels at a time while the others are frozen . the als method @xcite works with fixed ranks and has a complexity of @xmath138 for full or @xmath139 for sparse matrix kernels .",
    "however , the convergence is rather slow and there are no convergence estimates . the dmrg method or mals",
    "@xcite is an alternative that is based on the same algorithmic idea but operates on two kernels at a time . in this way",
    ", the ranks can be adapted and convergence is faster and better understood .",
    "however , the complexity is increased by a factor @xmath57 .",
    "matrix - vector products based on the dmrg method were introduced in @xcite .",
    "recently , the amen routine @xcite has been presented that allows for adaptive ranks and relatively fast convergence with complexity comparable to the als method .",
    "a standard tt representation of a matrix does not take sparsity of the one - dimensional kernels into account .",
    "the multiplication by a diagonal matrix should therefore be considered as a hadamard product , i.e. , the element - wise product of two tensors .",
    "as seen in algorithm [ alg : ttround ] , the tt - rounding algorithm proceeds in two steps :    1 .",
    "left - to - right sweep with orthogonalization of the kernels @xmath140 via qr decomposition .",
    "2 .   right - to - left sweep with svd and truncation of singular values .",
    "for the first step , we note that we can find an orthogonalized representation of both tt tensors .",
    "if we then compute the hadamard product of the kernels , the resulting tt tensor is again orthogonal .",
    "this splitting of the orthogonalization step reduces the complexity from @xmath141 to @xmath142 compared to when computing the qr decomposition of the multiplied kernel .    for the svd in the second step , let @xmath143 and @xmath144 and consider the kernel in dimension @xmath70 .",
    "since we have already truncated over the @xmath70th rank , we can assume that this rank is of the order @xmath145 and rank @xmath146 should be truncated .",
    "the multiplied kernel consists of @xmath147 blocks size @xmath145 .",
    "we now take two such blocks and truncate them .",
    "then , we add another block of size @xmath145 and truncate again .",
    "this is repeated until all @xmath147 blocks are included .",
    "the procedure is summarized in algorithm  [ alg : tthadamard ] .    in total",
    ", the complexity is reduced to the order @xmath148 , if we assume that the rank of the product is approximately @xmath136 .",
    "the reduction is about a factor @xmath149 compared to the direct method . of course",
    ", one could consider any other grouping of the constituents of the kernel .",
    "for instance , one might group more than two @xmath145-sized kernels together .      the idea presented in the previous subsection",
    "is limited to diagonal matrices , since the qr decomposition of a sparse matrix is generally non - sparse .",
    "hence , the orthogonal representation of the kernels of the matrix in tt format would be non - sparse , which would give complexities in the range of @xmath150 .",
    "however , we can essentially apply algorithm  [ alg : tthadamard ] if we have a matrix that is non - diagonal in dimension @xmath45 only since the kernels @xmath151 are not qr decomposed .",
    "we only need to replace the hadamard product along dimension @xmath45 , @xmath152 , by a sparse matrix - vector product .",
    "using rq instead of qr decompositions , we can interchange the direction of the loops and create a tt tensor that is non - orthogonal in @xmath73 . combining qr and rq decompositions",
    ", we can create an orthogonalized tt tensor with any non - orthogonal kernel . in this case",
    ", however , we have to choose the direction of the truncation step , i.e. , the truncation step will only include the initially non - orthogonalized kernel together with either the kernels with smaller or larger indices .",
    "if we do not want to truncate on one side of the non - diagonal kernel  for instance because the matrix is the identity on one side  algorithm [ alg : tthadamard ] is still applicable .",
    "as long as we order the dimensions such that the spatial coordinates are adjacent , the advection matrices in our splitting semi - lagrangian scheme have this structure .      a more flexible alternative to combine the efficient hadamard product with a sparse non",
    "- diagonal kernel is to split diagonal and off - diagonal parts . for the kronecker product of two matrices @xmath49 and @xmath50",
    ", it holds that @xmath153 where @xmath154 denote identity matrices of the corresponding size .",
    "hence , we can apply all non - diagonal kernels first followed by an application of the diagonal kernels according to algorithm [ alg : tthadamard ] .",
    "of course , this means we are applying the matrix in two steps with an error in each step if we apply intermediate rounding in contrast to the alternative discussed in the previous section . on the other hand ,",
    "the structure of the matrix is not limited .",
    "also , we do not have to explicitly form the kernels of the non - diagonal matrices since no orthogonalization is necessary .",
    "the leading - order complexity of the various variants of the matrix - vector product that were presented in this section is of the same order as the amen matrix - vector product .",
    "however , the computation does not involve any iterative method and the constant is therefore supposed to be smaller in general .",
    "indeed , we have seen in numerical experiments that our matrix - vector product is generally faster than the amen product .",
    "possibly , the computing time can be further reduced by applying the amen algorithm not to the full matrix - vector product but to the rounding steps in the hadamard - based algorithms .",
    "when we are explicitly forming the propagation matrices , we may round the matrix before computing the matrix vector product . especially when using higher order and for the velocity advections in four and six dimension where ranks of the propagation matrices become larger , rounding can reduce the complexity of the advection step .",
    "if we build the full kernels of a matrix , the rounding has a computational complexity of @xmath155 for ranks @xmath129 and @xmath57 points per dimension .",
    "however , there will be many zero entries in this matrix .",
    "for the rounding operation , an @xmath156 tt matrix kernel is treated as a tt tensor kernel of size @xmath157 .",
    "if we have an index @xmath158 such that the corresponding entries of the kernel are all zero , i.e. , @xmath159 , this dimension will always give a zero contribution .",
    "hence , we can erase these dimensions from the tt tensor representing the tt matrix . for a diagonal kernel",
    "this means that only the diagonals need to be stored and the size can be reduced from @xmath150 to @xmath57 . for a sparse matrix , we need to keep a sparsity pattern that includes the sparsity pattern of all the sparse matrices representing the kernel .",
    "this reduces the complexity to the order @xmath160 .",
    "the sparse matrices appearing in our propagation matrices represent an index shift .",
    "if we use an interpolator that involves @xmath75 points around each point , we have @xmath75 index - shifting matrices and the total sparsity pattern includes a band of @xmath75 points around the diagonal .",
    "however this band of @xmath75 points will be exactly the same for each of the @xmath57 points .",
    "since the rank coupling to the neighboring dimensions is the same for all points , it suffices to keep one copy of this band .",
    "this reduces the size of the kernel representing the sparse index - shifting matrix from @xmath150 for a full matrix representation to @xmath75 .",
    "hence , the size of the kernel representing the index - shift matrix is independent of the number of grid points .      in our experiments , we found a matrix - vector product following  [ sec : eff_mvp2 ] to be most efficient if the advection coefficient only depends on one variable , i.e. , velocity advection in two dimensions and all spatial advections",
    "essentially , this is the algorithm described in  [ sec : ttsl2d ] with some specified interpolation formula and applied to parts of an orthonomal tt - tensor . for the case where the coefficient is multivariate",
    ", we use the splitting described in  [ sec : eff_mvp3 ] . in this case",
    ", we also round the propagation matrix before computing the matrix - vector product .",
    "however , we have observed considerable loss in accuracy when using the same threshold as for the rounding of the tt tensor .",
    "therefore , the threshold is reduced by a factor 4 for the matrix rounding . when rounding the propagation matrix ,",
    "most redundancies in the matrix - vector product are already eliminated .",
    "hence , the splitting of the svd over @xmath147 in algorithm [ alg : tthadamard ] does generally not speed up the product .",
    "in this section , we present results obtained with the semi - lagrangian method in tt format for the weak and strong landau damping as well as the two stream instability . in the simulations",
    ", we use a cubic spline interpolator for all advections with univariate coefficient and a five - point lagrange interpolation for all advections with multivariate coefficients . as a reference",
    ", we compare our result to a full grid solution using cubic spline interpolation .",
    "all experiments are performed in matlab with a prototype implementation based on the tt - toolbox .",
    "the reported computing times are for an intel ivy bridge notebook processor with two cores at 3.0 ghz .",
    "the initial condition for the standard landau test case @xcite in @xmath161 dimensions is given by @xmath162 in our experiments , we choose @xmath163 and for the weak landau damping experiments we set @xmath164 .",
    "one can linearize the electric field around the maxwellian equilibrium and get the linear solution for the electric field which is a good approximation if the parameter @xmath165 is small . for the chosen parameter @xmath163 the damping rate of the electric field",
    "is @xmath166 according to the linear theory @xcite , i.e. , the electric energy is damped by a factor @xmath167 .",
    "we solve the weak landau damping problem on the domain @xmath168^d \\times [ -6,6]^d$ ] discretized with a grid of @xmath169 points along each spacial dimension and @xmath170 points along the velocity dimensions .",
    "the experiment is done in one , two and three dimensions in tt format and in one and two dimensions on the full grid with the same resolution .",
    "the tt rounding is done to the accuracy @xmath171 .",
    "figure [ fig : linlan_eenergy ] shows the electric energy as a function of time together with the envelope functions predicted by linear theory .",
    "we note that we recover the damping rate in all cases .",
    "we also see that the solution obtained in tt format is in good agreement with the solution on the full grid .",
    "especially , the spurious recurrence occurs around time 63 on both the full and the tt grid .",
    "the maximum rank combinations are given in the upper part of table [ tab : linlancompression ] together with the corresponding compression rate compared to the full - grid solution .",
    "note that the ranks are only checked after each time step .    at time @xmath172 , the @xmath173 error in the distribution function on the @xmath174 grid in 2d is about @xmath175 compared to the solution on a refined grid .. in the tensor train computations with @xmath176 for time @xmath177 , we have an error of @xmath178 in the distribution function at time @xmath172 .",
    "hence , the tt truncation error is much smaller than the numerical error on the full grid for the chosen parameters .     grid . ]",
    ".weak landau damping .",
    "compression on tt grid .",
    "grid size : @xmath179 . [ cols=\"^,^,^,^\",options=\"header \" , ]     [ tab : nonlinlantimes ]      finally , we consider the two stream instability in 4d . in 2d phase space , the initial condition is @xmath180 in our simulations , we choose the parameters @xmath181 , @xmath182 , and @xmath183 .",
    "we consider two kinds of extensions to 4d phase space @xmath184 in the first case , we have an equilibrium state in @xmath185 plane .",
    "a solution in tt format detects this simple form and the solution is represented as a tt tensor with only rank @xmath186 different from one .",
    "the second case is a tensor product of two one dimensional two stream instabilities .",
    "figure [ fig : tsi_ranks ] shows the ( inner ) ranks as a function of time for a simulation on a grid with @xmath187 points and a rounding threshold @xmath188 .",
    "we see that the compression is very good in the beginning until about time 20 .",
    "thereafter the instability grows rapidly until about time 30 . during this phase the ranks @xmath189 coupling the pairs @xmath190 and @xmath191 strongly increase .",
    "when nonlinear effects start to dominate and the electric energy flattens out , also rank @xmath192 increases for some time .",
    "finally , the ranks remain almost constant from about time 60 .",
    "figure [ fig : tsi_energy ] shows the electric field as a function of time for the tt solution as well as the full grid solution .",
    "the curves show good agreement . in the nonlinear phase",
    "they start to deviate up to 27 % .",
    "however , comparing the solution on the full grid with a solution on the same grid but with the same interpolation formulas as in the tt algorithm we see a deviation of up to 30 % . hence , the error due to tt rounding is on the scale of the numerical error .",
    "figure [ fig : twostream_phasespace ] shows the distribution function in @xmath193 plane ( integrated over @xmath194 ) at time 35 for the tt compressed as well as the full - grid solution with splines .",
    "we see that the tt solution covers the overall features of the full - grid solution but the solution is less smooth .",
    "in this paper , we have devised a semi - lagrangian vlasov ",
    "poisson solver with representation of the distribution function in tensor train format . for the efficient implementation of the advection step it is important to avoid direct matrix - vector products .",
    "instead , we propose to compress the matrix describing the interpolation step and an iterative implementation of the arising hadamard products .",
    "the method has been tested for a number of standard test cases in two to six dimensions .",
    "we have demonstrated that the solution can be compressed to a very high extent without losing essential parts of the solution when using a tensor train representation . also the computing time is considerably reduced . as expected the gains from the tensor train representation become larger with growing dimension .    in order to be able to study more complicated equations with more pronounced multidimensional effects",
    ", we plan to work on a high - performance implementation of the method .",
    "the choice of the interpolation formula and the effects of the cfl - like conditions and possible alleviations need to be studied in future work . moreover , the effect of the rounding parameter and possibilities of automatic tolerance detection need further attention .",
    "the author thanks eric sonnendrcker for bringing the tensor train framework to her attention and discussions on various aspects of this work .",
    "discussions with michel mehrenberger and marco restelli on test cases were also appreciated .",
    "s.  v. dolgov , b.  n. khoromskij , and d.  v. savostyanov . fast solution of parabolic problems in the tensor train / quantized tensor train format with initial application to the fokker - planck equation .",
    ", 34:a2718a2739 , 2012 ."
  ],
  "abstract_text": [
    "<S> in this article , we derive a semi - lagrangian scheme for the solution of the vlasov equation represented as a low - parametric tensor . </S>",
    "<S> grid - based methods for the vlasov equation have been shown to give accurate results but their use has mostly been limited to simulations in two dimensional phase space due to extensive memory requirements in higher dimensions . </S>",
    "<S> compression of the solution via high - order singular value decomposition can help in reducing the storage requirements and the tensor train ( tt ) format provides efficient basic linear algebra routines for low - rank representations of tensors . in this paper </S>",
    "<S> , we develop interpolation formulas for a semi - lagrangian solver in tt format . in order to efficiently implement the method </S>",
    "<S> , we propose a compression of the matrix representing the interpolation step and an efficient implementation of the hadamard product . </S>",
    "<S> we show numerical simulations for standard test cases in two , four and six dimensional phase space . depending on the test case , </S>",
    "<S> the memory requirements reduce by a factor @xmath0 in four and a factor @xmath1 in six dimensions compared to the full - grid method . </S>"
  ]
}