{
  "article_text": [
    "given a set of examples ( instances ) characterized by the value of attributes and the class of the example , the aim of supervised learning algorithms is to construct a classifier which is able to assign new examples to the class they belong to . to this end , a great deal of learning algorithms have been proposed .",
    "the constructed classifiers are usually compared over a variety of datasets using various evaluation measures proposed in the literature .",
    "most results are averages over a number of runs , where each run involves splitting the dataset into disjoint training and test sets , and the test set is used to estimate several evaluation measures of the classifier generated using the corresponding training set .",
    "afterwards , it is important to statistically verify the hypothesis of improved performance ( or not ) of the learning algorithm  @xcite .",
    "however , we consider that the evaluation measures of the classifier constructed by a learning algorithm should also be compared with the ones obtained by a simple baseline classifier , as it is actually done by most of the single - label learning community . this way , case any of these measures are worse , it would encourage the community to provide additional explanation of this fact .    in _ single - label learning _",
    ", each example in the dataset is associated with only one class , which can assume several values .",
    "the task is called _ binary classification _ if there are only two possible class values ( yes / no ) , and _ multi - class classification _ when the number of class values is greater than two  @xcite .    for single - label learning , a _",
    "simple baseline classifier _ is the one constructed by only taking into account the class values , _",
    "i.e. _ , it does not consider the attributes that describe the examples in the dataset . having only this information , and due to the fact that the classification of a new instance has only two possible outcomes , correct or incorrect ,",
    "the best it can do is to output a classifier that always predicts the most frequently occurring class value in the dataset .",
    "before the single - label learning community started to pay attention to this very simple baseline classifier , many evaluation measures worse than or equal to this baseline classifier had been published in the scientific literature , without special explanations .    in",
    "@xcite an experimental comparison involving 16 commonly used single - label datasets is carried out , where the error rate of the proposed algorithms are compared to the error rate of several learning systems reported in the literature . however , although the datasets used are not highly skewed , some of these reported results fail to improve the error rate of the simple baseline classifier . for example , considering two of these datasets , breast cancer and hepatitis , from the collection distributed by the university of california at irvine  @xcite , 33 and 75 error rates are compiled respectively . for the dataset hepatitis 8 out of 33 ( more than 24% ) reported error rates are worse than or equal to the simple baseline classifier , while for the dataset breast cancer the same happens for 29 out of 75 ( more than 38% ) reported error rates .",
    "as the simple single - label baseline classifier is constructed by only looking at the class values , any learning algorithm which learns from non - skewed domains , and also takes into account the dataset attribute values should be able to construct a classifier with smaller error rates .",
    "different to single - label learning , in _ multi - label learning _",
    "an example can belong to several classes simultaneously .",
    "the main difference between multi - label learning and single - label learning is that classes in multi - label learning are often correlated , and the class values in single - label learning are mutually exclusive .",
    "due to the increasing number of applications where examples are annotated with more than one class , multi - label learning has received increasing attention from the machine learning community  @xcite .",
    "however , finding a simple multi - label baseline classifier by only looking at the multi - labels is not as straightforward as in single - label , where the classification of a new instance has only two possible outcomes , correct or incorrect , and the error rate is often considered an important single objective to be achieved .",
    "this is not the case in multi - label , as the evaluation measures of a multi - label classifier should also take into account _ partially correct _ classifications . to this end , many criteria are proposed to evaluate the classification performance from different perspectives . in  @xcite ,",
    "the connection among these criteria are established , showing that some of these criteria are uncorrelated or even negatively correlated . in other words , some loss functions are essentially conflicting .",
    "thus , several multi - label evaluation measures have been proposed , highlighting different aspects of this important characteristic of multi - label learning .    motivated by the lack of simple multi - label baseline classifiers , in  @xcite we propose a simple way to construct multi - label baseline classifiers for specific multi - label evaluation measures .",
    "nevertheless , as a multi - label classifier which focuses on minimizing / maximizing one of these measures does not necessarily minimize / maximize the others , we also proposed a unique simple baseline classifier , called , which does not focus on any one of these specific measures and can be used to determine all the multi - label evaluation measure baseline values of a classifier .    although we do not claim that the proposed multi - label baseline classifier should be the one to be used by the community whenever classifiers evaluation measures are published , as other baseline classifiers could be proposed in the future , we believe that it is time to start a discussion related to this subject . aiming to motivate the community , in this work we consider published experimental results which show that , similar to the single - label research primordium , some of the published results fail to improve on the ones obtained by our simple multi - label baseline classifier .    however , unlike  @xcite , in which results reported on a dataset could also refer to the classifier generated by a learning algorithm using a slightly different dataset due to pre - processing , such as filter feature selection or other transformation , in this work we only used the results published in papers reporting experimental results of classifiers which have been constructed using publicy available _ identical _ datasets .",
    "unfortunately , this constraint leaves out a great deal of papers , such as many related to text categorization , a typical multi - label problem , as most of the publicly available text datasets are modified by the authors in different ways to obtain the final dataset from which the classifier is generated and , in most cases , this final dataset is not publicly available . on the other hand ,",
    "this constraint enables anyone to reproduce the experiments described in these papers .    as there is a lack of reviews focusing on pieces of work",
    "which report experimental results for multi - label learning , and the systematic review process can be useful to identify related publications in a wide , rigorous and replicable way  @xcite , we used this process to identify publications which report experimental results for multi - label learning .",
    "we have gathered the data used in this work from the selected publications which answer the systematic review research question and do not fulfill any of the exclusion criteria .",
    "more specifically , in this work we report on several statistics of various evaluation measure values , which were published and obtained using the 10 datasets most frequently used in the selected papers .",
    "these statistics show that 12.8% of these published results are worse than or equal to the ones obtained by our simple multi - label baseline classifier .",
    "moreover , this percentage is unevenly distributed among the datasets . in the `` worst '' dataset ,",
    "43.0% of such results were reported , and in the `` best '' one only 0.6% .",
    "however , although a simple baseline classifier was not considered in these publications , it was observed that even for very poor results no special explanations were provided in most of these publications .",
    "the remainder of this paper is organized as follows : section  [ sec : background ] briefly describes multi - label learning and the evaluation measures used in this work .",
    "section  [ sec : lgb ] explains the simple baseline classifier .",
    "the systematic review carried out to select the papers from which we have gathered the data used in this work is described in section  [ sec : systematicreview ] , and statistics of these published evaluation measure values are reported in section  [ sec : expdesign ] .",
    "section  [ sec : conclusion ] presents the conclusions and future work .",
    "let @xmath0 be a training set composed of @xmath1 examples @xmath2 , @xmath3 .",
    "each example @xmath4 is associated with a feature vector @xmath5 described by @xmath6 features @xmath7 , @xmath8 , and a subset of labels @xmath9 , where @xmath10 is the set of @xmath11 labels .",
    "table  [ tab : multilabel ] shows this representation . in this scenario ,",
    "the multi - label classification task consists of generating a classifier @xmath12 , which given an unseen instance @xmath13 , is capable of accurately predicting its subset of labels @xmath14 , _",
    "i.e. _ , @xmath15 .    .multi - label data [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab : baselines ]    [ tab : globalperformance ]    table  [ tab : globalperformance ] shows , for each dataset , the number of times that a published measure value underperforms or it is equal to the corresponding baseline value .",
    "summary information is shown in light gray cells .",
    "column # @xmath16 shows the total number of measures fulfilling this condition on a total of # @xmath17 measure values recorded for each dataset , and column % shows the percentage .",
    "similar results are shown in rows # @xmath18 , # @xmath19 and % for each measure considered .",
    "this information is shown graphically in figures  [ fig : overallperformance ] and  [ fig : overallperformancepercent ] .            as can be observed , from a total of 5,342 measure values on the 10 datasets considered in this work , 12.8% are worse than or equal to the ones provided by .",
    "moreover , these worse results are concentrated in some datasets , such as _ corel5k _ , _ mediamill_and _ enron _ , as shown in figure  [ fig : overallperformancepercent ] . on the other hand , only 4 out of 716 ( 0.6% ) of the measure values published for the _ emotions_dataset fulfill this condition .",
    "figure  [ fig : overallperformance2 ] shows information of these four datasets .",
    "nevertheless , this kind of information does not show the degree of disagreement between the evaluation measure values published and the ones provided by . to this end , we have extracted statistics from these values , as shown in figure  [ fig : overallperformance3 ] for the datasets _ corel5k _",
    ", _ mediamill _ , _ enron_and _ emotions_considering the distribution of , and measure values",
    ". it also shows , in brackets , the worst and the best value found in the publications .",
    "recall that for , the smaller the value , the better the multi - label classifier performance is , while for the others , greater values indicate better performance .",
    "in fact , this sort of statistics extraction and organization was carried out for all datasets and measures considered , and can be found at http://www.labic.icmc.usp.br/pub/mcmonard/experimentalresults/metz-generalb-supplementarymaterial .",
    "figure  [ fig : overallperformance3 ] shows that , in some cases , there is a considerable gap between the worst and the best published measure values . although this gap could be justified because different multi - label algorithms minimize different loss - functions , which in turn favors specific evaluation measures , it should be expected that special explanations are provided case these measures are worse than the ones from the simple baseline classifier .    furthermore ,",
    "considering in figure  [ fig : overallperformance3 ] the measures which are better than or equal to the ones from , it can be observed that there is little improvement in those measures for _ corel5k _ , _ mediamill_and _ enron_datasets . on the other hand ,",
    "the improvement is considerable for _",
    "emotions_.    table  [ tab : improvement ] shows , for the 10 datasets , the highest ( @xmath20 ) and the lowest ( @xmath21 ) measure values published in the 64 papers for the 8 evaluation measures considered in this work , as well as the ones from ( @xmath22 ) .",
    "light gray cells indicate that the difference between the highest and the lowest measure values is greater than or equal to 0.5 . in most cases",
    ", it can be observed that there is a very high discrepancy between the highest and the lowest published measure values .",
    "[ tab : improvement ]    regarding the multi - label algorithms used in the 64 papers , most of them follow the problem transformation approach , using state - of - the - art single - label learning algorithms as a base learner . binary relevance is the",
    "most frequently used approach .    at this point , it is worth observing that we are quite confident about the correctness ( with respect to the published results ) of the collected measure values from the 64 papers . as stated earlier in section",
    "[ sec : systematicreview ] , these values were initially double checked . after making the graphs for all datasets and measures considered in this work",
    ", we checked , once more , the worst and the best published values .    from this third inspection of the gathered data , it was observed that few papers explain and justify very poor results .",
    "however , similar to single - label learning , case the multi - label community decides to adopt a simple baseline classifier such as , or any other , we think that it will encourage the authors to provide special explanations on very poor results .",
    "the single - label community expects that in non skewed domains a simple baseline classifier , which always predicts the majority class , should do worse than classifiers constructed by a learning algorithms .",
    "however , to the best of our knowledge , the multi - label community still does not have a consolidated idea of a simple multi - label baseline classifier .",
    "aiming to raise awareness of considering a simple multi - label baseline classifier , we have carried out a systematic review of the multi - label learning literature in order to collect experimental results to contrast with the proposed simple multi - label baseline classifier .",
    "it was found that an important number of published results ( 12.8% ) are worse than or equal to the ones obtained by .",
    "in fact , for all the 10 most frequently used datasets presented in the work , results worse than or equal to the ones obtained by were found . in the extreme case , 43% of the published results for one dataset are worse than or equal to the results .",
    "although we do not claim that the proposed multi - label baseline classifier should be the one to be used by the community , we hope that this work would encourage the multi - label community to consider the idea of using a simple baseline classifier as an initial reference related to the learning power of multi - label algorithms . with the use of a baseline , built by only taking into account the label distribution information",
    ", it would be possible to identify cases where the obtained results are not reasonable enough , and give support for better explanations about these results .    as future work",
    ", we plan to increase the number of electronic databases to search for publications which answer our research question and do not fulfil any of the exclusion criteria . as the organization of the information extracted allows to answer several useful questions , such as _ which publications use algorithm a on dataset b using 10-fold cross - validation and what are the results obtained ?",
    "_ _ are there publications reporting results on datasets with cardinality greater than c and a distinct number of multi - labels greater than w ? _ , we plan to increment and further structure the gathered information making it available to the community on a web page .",
    "godbole , s. , sarawagi , s. , 2004 .",
    "discriminative methods for multi - labeled classification . in : dai , h. , srikant , r. , zhang , c. ( eds . ) , pakdd .",
    "3056 of lecture notes in computer science .",
    "springer , pp .",
    "2230 .",
    "kitchenham , b. , pretorius , r. , budgen , d. , brereton , p. , turner , m. , niazi , m. , linkman , s.  g. , 2010 .",
    "systematic literature reviews in software engineering - a tertiary study .",
    "information & software technology 52  ( 8) , 792805 .",
    "metz , j. , de  abreu , l. f.  d. , cherman , e.  a. , monard , m.  c. , 2012 . on the estimation of predictive evaluation measure baselines for multi - label learning . in : pavn , j. , duque - mndez , n.  d. , fuentes - fernndez , r. ( eds . ) , iberamia .",
    "7637 of lecture notes in computer science .",
    "springer , pp .",
    "189198 .",
    "read , j. , pfahringer , b. , holmes , g. , frank , e. , 2009 .",
    "classifier chains for multi - label classification . in : buntine ,",
    "w.  l. , grobelnik , m. , mladenic , d. , shawe - taylor , j. ( eds . ) , ecml / pkdd ( 2 ) . vol .",
    "5782 of lecture notes in computer science .",
    "springer , pp .",
    "254269 .",
    "tsoumakas , g. , dimou , a. , spyromitros , e. , mezaris , v. , kompatsiaris , i. , vlahavas , i. , 2009 .",
    "correlation - based pruning of stacked binary relevance models for multi - label learning . in : proc .",
    "ecml / pkdd 2009 workshop on learning from multi - label data ( mld09 ) ."
  ],
  "abstract_text": [
    "<S> in supervised learning , simple baseline classifiers can be constructed by only looking at the class , _ </S>",
    "<S> i.e. _ , ignoring any other information from the dataset . </S>",
    "<S> the single - label learning community frequently uses as a reference the one which always predicts the majority class . </S>",
    "<S> although a classifier might perform worse than this simple baseline classifier , this behaviour requires a special explanation . aiming to motivate the community to compare experimental results with the ones provided by a multi - label baseline classifier , calling the attention about the need of special explanations related to classifiers which perform worse than the baseline , in this work we propose the use of , a multi - label baseline classifier . </S>",
    "<S> was evaluated in contrast to results published in the literature which were carefully selected using a systematic review process . </S>",
    "<S> it was found that a considerable number of published results on 10 frequently used datasets are worse than or equal to the ones obtained by , and for one dataset it reaches up to 43% of the dataset published results . moreover , although a simple baseline classifier was not considered in these publications , it was observed that even for very poor results no special explanations were provided in most of them . </S>",
    "<S> we hope that the findings of this work would encourage the multi - label community to consider the idea of using a simple baseline classifier , such that further explanations are provided when a classifiers performs worse than a baseline . </S>"
  ]
}