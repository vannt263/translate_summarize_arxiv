{
  "article_text": [
    "let @xmath6 be a stationary ergodic sequence of random variables with mean @xmath7 and finite variance , satisfying the central limit theorem , i.e. @xmath8 where the limit variance @xmath9 is given by the formula @xmath10 for practical applications of the central limit theorem , e.g. , for calculation of confidence intervals or testing hypothesis concerning the mean , we need to estimate the variance @xmath9 .",
    "we investigate the properties of a subsampling estimator of @xmath11 .",
    "given observations @xmath12 and a block length @xmath13 , we define the overall mean and the block means as @xmath14 respectively . by the central limit theorem",
    ", @xmath15 converges in distribution to @xmath16 as @xmath17 goes to infinity , and thus @xmath18 .",
    "thus , it is natural to estimate @xmath11 by the arithmetic mean of @xmath19 , @xmath20 $ ] . replacing the unknown mean @xmath7 by the sample mean @xmath21",
    ", we obtain @xmath22 } \\sqrt{\\frac{\\pi}{2 } } \\sum_{i=1}^{[n / l ] } \\frac{|s_i(l)-l\\bar{x}_n|}{\\sqrt{l } } \\label{eq : b_n}\\ ] ] as an estimator for @xmath11 . in",
    "what follows we will always write @xmath23 $ ] for the number of full blocks of length @xmath24 in the set @xmath25 .",
    "subsampling estimators of the limit variance @xmath9 of the arithmetic mean of a stationary stochastic process have been studied by various authors under different assumptions concerning the dependence structure . @xcite",
    "investigated @xmath26-mixing processes and the estimator @xmath27 }   \\sum_{i=1}^{[n / l ] } \\left(\\frac{|s_i(l)-l\\bar{x}_n|}{\\sqrt{l}}\\right)^2\\ ] ] for @xmath9 . @xcite studied properly normalized means of @xmath28-th powers of @xmath29 as an estimator for @xmath30 in the case of @xmath31-mixing processes .",
    "they consider overlapping blocks , i.e.   @xmath32 where @xmath33 .",
    "@xcite studied the same estimator , in the case @xmath34 , for associated processes .",
    "@xcite investigated the case of weakly dependent ( in the sense of * ? ? ?",
    "* ) processes .    in this paper",
    ", we extend the above mentioned results to processes that can be represented as functionals of strongly mixing processes .",
    "we assume that @xmath6 can be written as @xmath35 for all @xmath36 , where @xmath37 is a measurable function and where @xmath38 is an @xmath26-mixing process . recall that a process @xmath38 is called @xmath26-mixing ( or strongly mixing ) if @xmath39 as @xmath40 . here",
    "@xmath41 denotes the @xmath11-field generated by the random variables @xmath42 .",
    "the coefficients @xmath43 are called mixing coefficients . in addition",
    ", we have to require that the function @xmath37 defined above is continuous in a suitable sense .",
    "define @xmath44 in what follows , we will have to make assumptions concerning the behavior of the coefficients @xmath45 , @xmath46 and @xmath47 as @xmath48 .",
    "functionals of @xmath26-mixing processes cover most of the standard examples of weakly dependent processes known in the literature , for example arma- and garch - processes from time series analysis , many markov processes and hidden markov models , the sequence of digits and remainders in continued fraction expansion , and many chaotic dynamical systems , such as expanding , piecewise monotone maps of the unit interval .",
    "details can be found , e.g. , in @xcite and in @xcite .",
    "suppose @xmath1 is a stationary process that can be expressed as a functional of an @xmath26-mixing process .",
    "suppose that @xmath49 for some @xmath50 and that the approximation coefficients @xmath51 satisfy @xmath52 and the mixing coefficients @xmath53 satisfy @xmath54 for some @xmath55 .",
    "then , for any sequence @xmath56 satisfying @xmath57 and @xmath58 , we have @xmath59 in @xmath2 .",
    "[ th : fctl - ltwo ]    @xcite study robust non - parametric tests for structural breaks in time series .",
    "they consider the model @xmath60 , where @xmath61 is a stationary stochastic process and @xmath62 is a sequence of unknown constants .",
    "the hypothesis of interest is @xmath63 to be tested against the alternative @xmath64 .",
    "investigating the asymptotic distribution of the two - sample hodges - lehmann test statistic @xmath65 dehling and fried show that under the above null hypothesis , we have @xmath66 as @xmath67 . here",
    "@xmath68 , where @xmath69 is an independent copy of @xmath5 , and @xmath70 where @xmath4 is the distribution function of @xmath5 . for statistical applications ,",
    "the limit variance @xmath71 has to be estimated",
    ". we can directly apply theorem  [ th : fctl - ltwo ] to the sequence @xmath72 , provided that it satisfies the conditions and that @xmath4 is known .",
    "the latter is generally not the case and thus @xmath4 has to be replaced by the empirical distribution function @xmath73 . in what follows",
    ", we will show that this leads to an @xmath2-consistent estimator .    denoting the empirical distribution function by @xmath74",
    ", we define the estimators @xmath75 } \\sqrt{\\frac{\\pi}{2 } } \\sum_{i=1}^{[n / l_n ] } \\frac{|t_i(l_n)-l_n\\ , \\bar{u}_n|}{\\sqrt{l_n } } , \\\\",
    "\\hat{d}_n&= & \\frac{1}{[n / l_n ] } \\sqrt{\\frac{\\pi}{2 } }    \\sum_{i=1}^{[n / l_n ] } \\frac{|\\hat{t}_i(l_n)-l_n\\ , \\tilde{u}_n|}{\\sqrt{l_n } } , \\label{eq : def - dhat}\\end{aligned}\\ ] ] where @xmath76 , @xmath77 , and @xmath78    let @xmath1 be a stationary process satisfying the assumptions of theorem  [ th : fctl - ltwo ] .",
    "in addition , assume that @xmath79 , @xmath80 and that @xmath81 is lipschitz - continuous . then ,",
    "as @xmath82 , @xmath83 and @xmath84 , we have @xmath85 in @xmath2 .",
    "[ th : edf - ltwo ]",
    "we investigate the performance of the proposed estimators and compare them to alternatives .",
    "the estimators are applied to arma(1,1 ) processes with gaussian innovations and different parameter settings , which are given in table  [ ts_coefs_tab ] . for each parameter combination ,",
    "1000 samples are drawn , and variance , bias and mean squared error of the estimates are computed .",
    "the sample size is @xmath86 throughout .",
    ".types of arma(1,1 ) time series used in the simulation .",
    "ar and ma denote the corresponding coefficents of the arma(1,1 ) model in the usual notation . [ cols=\">,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     besides comparing several estimators , another question addressed by the simulations is the applicability of carlstein s rule for choosing the block length . @xcite proved it to be optimal in terms of mean squared error for his estimator @xmath87 , cf .",
    "( [ eq : carlstein ] ) , in case of an ar(1 ) process with known autocorrelation parameter .",
    "the simulation study consists of two stages : the first part is concerned with the estimation of @xmath9 , cf .",
    "( [ eq : clt - var ] ) , the second part with the estimation of @xmath71 , cf .",
    "( [ eq : clt - var2 ] ) .",
    "[ [ part-1 ] ] part 1 + + + + + +    we compare @xmath88 , cf .",
    "( [ eq : b_n ] ) , the carlstein estimator @xmath87 , referred to as ce in the following , and the kernel - based estimator of @xcite , abbreviated by jde in the following .",
    "while the former two depend on the block length , the latter is affected by the choice of the kernel and the bandwidth @xmath89 .",
    "we use the bartlett kernel , which is a common choice due to guaranteed positive semi - definiteness .    to compute mean squared error and bias we compare the estimates to the true value of @xmath90 , not the infinite series ( [ eq : clt - var ] ) .",
    "we use the fixed block lengths 1 , 2 , 5 , 10 , 30 , 50 and 100 , and determine the data adaptive block length following carlstein as follows : an ar(1 ) process is fit to the sample ( regardless of whether it was generated as such ) using the r function arima ( ) @xcite , and the block length is computed from the estimated autocorrelation parameter . for the jde , the bandwidths 2.8 , 3.5 , 4.7 ,",
    "7.9 and 22.4 are used , corresponding to sixth to second root of @xmath91 .",
    "[ [ part-2 ] ] part 2 + + + + + +    the second part of the simulation , which deals with the estimation of @xmath71 , compares @xmath92 , cf .",
    "( [ eq : def - dhat ] ) , to the @xcite estimator , denoted by pse in the following . both depend on a block length , but contrary to @xmath92 , the pse is based on overlapping blocks .",
    "the data generation is exactly the same as in the first part ( sample size , number of repetitions and arma parameters ) , but the data evaluation is different : each sample is split into halves and the estimate of interest is computed as the mean of the two subsample estimates .",
    "this choice is motivated by the intended application to the change - point test by @xcite .",
    "the block lengths considered are 2 , 5 , 10 , 25 , 40 , 50 for @xmath92 and 5 , 10 , 25 , 40 , 45 for the pse . note",
    "that , while a block length 1 is of interest in the first simulation , since it yields a variance estimator for independent observation , it leads to data independent estimators in this situation and is hence neglected .    for computing the true value of @xmath93",
    "we make use of the fact that the population version of spearman s rho @xmath94 equals @xmath95 if @xmath96 and @xmath97 are jointly gaussian with correlation @xmath31 . this result can be traced back to @xcite ; for a newer reference see , e.g. , @xcite .",
    "hence , letting @xmath98 denote the correlation of @xmath5 and @xmath99 , we have @xmath100    , ce and jde as a function of the smoothing parameter ; computed from 1000 samples of size 500 generated from an arma(0.5 , 0.5 ) model .",
    "gray symbols correspond to the data adaptive block length , plotted at the mean block length of 16.04 .",
    "the x - axis is on log scale . ]    , ce and jde as a function of the smoothing parameter ; computed from 1000 samples of size 500 generated from an arma(0.5 , 0.5 ) model .",
    "gray symbols correspond to the data adaptive block length , plotted at the mean block length of 16.04 .",
    "the x - axis is on log scale . ]    [ [ results ] ] results + + + + + + +    as far as the comparison of the estimators is concerned , we get similar results for all arma parameter settings , which are exemplified for the @xmath9-estimators at an arma(0.5,0.5 ) process in figure [ fig : bias.mse.1 ] and for the @xmath93-estimators at an ma(0.1 ) process in figure [ fig : bias.mse.2 ] .",
    "the performance of the estimators is generally quite similar .",
    "this is also true for @xmath92 and the pse , which may seem surprising , since the pse , in contrast to @xmath92 , uses overlapping blocks . in terms of bias , @xmath88 and @xmath92 slightly outperform their competitors ( see left panels of figures [ fig : bias.mse.1 ] and [ fig : bias.mse.2 ] ) , but they show slightly bigger variances , leading to a worse mean squared error ( see right panels ) .",
    "the choice of the smoothing parameter ( block length or bandwidth ) has altogether a much larger influence than the choice of the method .",
    "we furthermore observe the following : the data adaptive block lengths , which correspond to the gray symbols in the figures , perform quite well in terms of mean squared error ( mse ) unless the ma - coefficient is strongly negative . except for a few cases , @xmath9",
    "is always underestimated if all autocorrelations of the underlying process are non - negative and overestimated if all are non - positive . in the white noise case , a block length of 1 is mse optimal , but other block lengths give very similar results . on the other hand , even for small non - zero ar- or ma - coefficients , not accounting for the dependence substantially impairs the accuracy of the estimation .    in summary ,",
    "all estimators considered show a very similar performance for good choices of the smoothing parameter  despite some considerable differences in their construction .",
    "an unfavorable choice of the block length leads to an increase in both , bias and variance , but with varying extents for the different estimators .",
    "based on our results , we suggest to use the estimators @xmath88 , @xmath92 if one is willing to trade a smaller bias for a bigger variance .",
    "the data adaptive block length proposed by @xcite seems to yield acceptable results for all block length dependent estimators even if the assumption of an ar(1 ) process is not met .",
    "we also carried out some simulations for other values of @xmath101 . while the estimators performance , for fixed block length respectively bandwidth , clearly improves with the sample size , the ranking of the estimators is little affected .",
    "therefore we focus on a fixed sample size here .     and",
    "pse as a function of the block length ; computed from 1000 samples of size 500 generated from an arma(0.5 , 0.5 ) model .",
    "gray symbols correspond to the data adaptive block length , plotted at the mean block length of 2.7 .",
    "the x - axis is on log scale . ]     and pse as a function of the block length ; computed from 1000 samples of size 500 generated from an arma(0.5 , 0.5 ) model .",
    "gray symbols correspond to the data adaptive block length , plotted at the mean block length of 2.7 .",
    "the x - axis is on log scale . ]",
    "we will first state and prove a special case of theorem  [ th : fctl - ltwo ] where the process @xmath6 is @xmath26-mixing .",
    "we need this result later on in the proof of theorem  [ th : fctl - ltwo ] .",
    "suppose @xmath1 is a stationary , @xmath26-mixing process with + @xmath102 for some @xmath103 and mixing coefficients @xmath53 satisfying @xmath54 for some @xmath55 . then , for any sequence @xmath56 satisfying @xmath57 and @xmath58 , we have @xmath104 in @xmath2 .",
    "[ prop : sm - ltwo ]    we will show that @xmath105 as @xmath40 . by definition of @xmath88 , we get @xmath106 as @xmath58 , we get by the central limit theorem @xmath107 in @xmath108 . thus , it remains to prove that @xmath109 by lemma [ lem3 ] we have @xmath110 and thus it suffices to show @xmath111 applying lemma [ lem1 ] with @xmath112 to the stationary @xmath26-mixing process @xmath113 , we obtain @xmath114 where we have used lemma [ lem1 ] with @xmath115 in the final step . as @xmath116",
    ", we have thus proved proposition  [ prop : sm - ltwo ] .",
    "defining @xmath117 we decompose @xmath6 into a part that is @xmath26-mixing and a remaining part , which is small .",
    "in addition , we define the corresponding arithmetic means @xmath118 and @xmath119 and the block sums @xmath120 furthermore , let @xmath121 for fixed @xmath122 , the process @xmath123 is @xmath26-mixing with coefficients @xmath124 .",
    "moreover , we have @xmath125 and @xmath126 .",
    "finally , one can show that @xmath127}^{\\delta/(2+\\delta ) }   + \\psi_{[j/3]}^{(1+\\delta)/(2+\\delta)}\\right ) , \\ ] ] see @xcite .",
    "observe that @xmath128 we will now bound each of the terms on the right hand side separately . to the third term , we apply the inequality @xmath129@xmath130 valid for any @xmath131 . we fix @xmath132 and choose @xmath133 sufficiently large such that for all @xmath134 @xmath135and hence by minkowski s inequality @xmath136since @xmath137 , we also have @xmath138 for all @xmath134 with @xmath133 sufficiently large .",
    "proposition  [ prop : sm - ltwo ] implies that , for any fixed @xmath122 , @xmath139 in @xmath2 as @xmath140 .",
    "hence the first term on the right hand side of ( [ eq : main - th ] ) converges to @xmath141 in @xmath2 .",
    "finally , stationarity and lemma [ lem2 ] imply @xmath142 in @xmath2 as @xmath140 .",
    "this finishes the proof of theorem  [ th : fctl - ltwo ] .",
    "as @xmath4 is lipschitz continuous , the process @xmath72 satisfies the conditions of theorem  [ th : fctl - ltwo ] , hence @xmath143 in @xmath2 by theorem  [ th : fctl - ltwo ] .",
    "thus it suffices to show that @xmath144 almost surely .",
    "we get @xmath145\\sqrt{l_n } } \\sqrt{\\frac{\\pi}{2 } } \\sum_{i=1}^{[n / l_n ] }      \\left|   t_i(l_n)-\\hat{t}_i(l_n)-l_n(\\bar{u}_n-\\tilde{u}_n ) \\right| \\\\   \\leq       \\frac{\\sqrt{\\pi/2}}{[n / l_n]\\sqrt{l_n } }   \\sum_{i=1}^{[n / l_n ] } & & \\hspace{-2em }      \\left(\\sum_{j=(i-1)l+1}^{il}|\\hat{f}_n(x_j)-f(x_j)| + \\frac{l_n}{n}\\sum_{i=1}^n|\\hat{f}_n(x_i)-f(x_i)|     \\right ) \\\\[0.5ex ]   \\leq \\ , & &   \\hspace{-1em } 2 \\sqrt{l_n}\\sup_{t\\in { { \\mathds r } } } |\\hat{f}_n(t)-f(t)| .",
    "\\end{aligned}\\ ] ] lemma [ lem : ep - lil ] implies @xmath146 almost surely , provided @xmath147 .",
    "this proves that @xmath92 is an @xmath2 consistent estimator of the limit variance @xmath148 .",
    "for ease of reference we state some results from the literature on weakly dependent processes that we need in the course of the proofs .",
    "@xcite [ lem : ep - lil ] let @xmath167 be a stationary sequence of strongly mixing random variables with @xmath79 , and let @xmath168 , @xmath159 , be a sequence of functionals with common distribution function @xmath169 , satisfying @xmath170 where @xmath171",
    ". then @xmath172 almost surely , where @xmath173 .",
    "s.  borovkova , r.  m. burton , and h.  dehling .",
    "limit theorems for functionals of mixing processes with applications to @xmath174-statistics and dimension estimation .",
    "_ transactions of the american mathematical society _ , 3530 ( 11):0 42614318 , 2001 .",
    "h.  dehling and r.  fried .",
    "asymptotic distribution of two - sample empirical @xmath174-quantiles with applications to robust tests for shifts in location .",
    "_ journal of multivariate analysis _",
    ", 1050 ( 1):0 124140 , 2012 .",
    "doi : 10.1016/j.jmva.2011.08.014 .",
    "p.  doukhan , j.  jakubowicz , and j.  r. len .",
    "variance estimation with applications . in i.  berkes , r.  bradley , h.  dehling , m.  peligrad , and r.  tichy , editors , _ dependence in probability , analysis and number theory .",
    "_ , pages 203231 .",
    "heber city , ut : kendrick press , 2010 .",
    "k.  pearson .",
    "_ mathematical contributions to the theory of evolution .",
    "xvi . on further methods of determining correlation_. drapers company research memoirs : biometric series iv .",
    "london : dulau & co. , 1907 ."
  ],
  "abstract_text": [
    "<S> we study subsampling estimators for the limit variance @xmath0 of partial sums of a stationary stochastic process @xmath1 . </S>",
    "<S> we establish @xmath2-consistency of a non - overlapping block resampling method . </S>",
    "<S> our results apply to processes that can be represented as functionals of strongly mixing processes . </S>",
    "<S> motivated by recent applications to rank tests , we also study estimators for the series @xmath3 , where @xmath4 is the distribution function of @xmath5 . </S>",
    "<S> simulations illustrate the usefulness of the proposed estimators and of a mean squared error optimal rule for the choice of the block length . </S>"
  ]
}