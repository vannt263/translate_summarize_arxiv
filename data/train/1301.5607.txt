{
  "article_text": [
    "information is about making distinctions or differences . in james gleick s book , _ the information : a history , a theory , a flood _ , he noted the focus on differences in the seventeenth century polymath , john wilkins , who was a founder of the royal society . in 1641 ,",
    "the year before newton was born , wilkins published one of the earliest books on cryptography , _ mercury or the secret and swift messenger _ , which not only pointed out the fundamental role of differences but noted that any ( finite ) set of different things could be encoded by words in a binary code .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ for in the general we must note , that whatever is capable of a competent difference , perceptible to any sense , may be a sufficient means whereby to express the cogitations .",
    "it is more convenient , indeed , that these differences should be of as great variety as the letters of the alphabet ; but it is sufficient if they be but twofold , because two alone may , with somewhat more labour and time , be well enough contrived to express all the rest .",
    "xvii , p. 69 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    wilkins explains that a five letter binary code would be sufficient to code the letters of the alphabet since @xmath0 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ thus any two letters or numbers , suppose a.b .",
    "being transposed through five places , will yield thirty two differences , and so consequently will superabundantly serve for the four and twenty letters ... .",
    "xvii , p. 69 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    as gleick noted :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ any difference meant a binary choice .",
    "any binary choice began the expressing of cogitations . here , in this arcane and anonymous treatise of 1641 , the essential idea of information theory poked to the surface of human thought , saw its shadow , and disappeared again for [ three ] hundred years . @xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in this paper , we will start afresh by deriving an information - as - distinctions notion of logical entropy @xcite from the new logic of partitions @xcite that is mathematically dual to the usual boolean logic of subsets .",
    "then the usual shannon entropy @xcite will be essentially derived from the concepts behind logical entropy as another way to measure information - as - distinctions .",
    "this treatment of the various notions of shannon entropy ( e.g. , mutual , conditional , and joint entropy ) will also explain why their interrelations can be represented using a venn diagram picture @xcite .",
    "the logic normally called `` propositional logic '' is a special case of the logic of subsets originally developed by george boole @xcite . in the boolean logic of subsets of a fixed non - empty universe set @xmath1 ,",
    "the variables in formulas refer to subsets @xmath2 and the logical operations such as the join @xmath3 , meet @xmath4 , and implication @xmath5 are interpreted as the subset operations of union @xmath6 , intersection @xmath7 , and the conditional @xmath8 .",
    "then `` propositional '' logic is the special case where @xmath9 is the one - element set whose subsets @xmath10 and @xmath11 are interpreted as the truth values @xmath12 and @xmath11 ( or false and true ) for propositions .    in subset logic ,",
    "a _ valid formula _ or _ tautology _ is a formula such as @xmath13   \\rightarrow t$ ] where for any non - empty @xmath1 , no matter what subsets of @xmath1 are substituted for the variables , the whole formula evaluates to @xmath1 by the subset operations .",
    "it is a theorem that if a formula is valid just for the special case of @xmath9 ( i.e. , as in a truth table tautology ) , then it is valid for any @xmath1 .",
    "but in today s textbook treatments of so - called `` propositional '' logic , the truth - table version of a tautology is usually given as a definition , not as a theorem in subset logic .    what is lost by restricting attention to the special case of propositional logic rather than the general case of subset logic ?",
    "at least two things are lost , and both are relevant for our development .    * firstly if it is developed as the logic of subsets , then it is natural , as boole did , to attach a quantitative measure to each subset @xmath14 of a finite universe @xmath1 , namely the normalized counting measure @xmath15 which can be interpreted as the _ logical probability _",
    "@xmath16 ( where the elements of @xmath1 are assumed equiprobable ) of randomly drawing an element from @xmath14 . *",
    "secondly , the notion of a subset ( unlike the notion of a proposition ) has a mathematical dual in the notion of a quotient set , as is evidenced by the dual interplay between subobjects ( subgroups , subrings , ... ) and quotient objects throughout abstract algebra .",
    "this duality is the `` turn - around - the - arrows '' category - theoretic duality , e.g. , between monomorphisms and epimorphisms , applied to sets @xcite .",
    "the notion of a quotient set of @xmath1 is equivalent to the notion of an equivalence relation on @xmath1 or a partition @xmath17 of @xmath1 .",
    "when boole s logic is seen as the logic of subsets ( rather than propositions ) , then the notion arises of a dual logic of partitions which has now been developed @xcite .",
    "a partition @xmath17 on a finite set @xmath1 is a set of non - empty disjoint subsets @xmath18 ( `` blocks '' of the partition ) of @xmath1 whose union is @xmath1 .",
    "the idea of information - as - distinctions is made precise by defining a _ distinction _ or _ dit _ _ of a partition _",
    "@xmath19 of @xmath1 as an ordered pair @xmath20 of elements @xmath21 that are in different blocks of the partition .",
    "the notion of `` a distinction of a partition '' plays the analogous role in partition logic as the notion of `` an element of a subset '' in subset logic .",
    "the set of distinctions of a partition @xmath22 is its _ dit set _ @xmath23 .",
    "the subsets of @xmath1 are partially ordered by inclusion with the universe set @xmath1 as the top of the order and the empty set @xmath10 as the bottom of the order .",
    "a partition @xmath19 _ refines _ a partition @xmath24 , written @xmath25 , if each block @xmath26 is contained in some block @xmath27 .",
    "the partitions of @xmath1 are partially ordered by refinement which is equivalent to the inclusion ordering of dit sets .",
    "the discrete partition @xmath28 , where the blocks are all the singletons , is the top of the order , and the indiscrete partition @xmath29 ( with just one block @xmath1 ) is the bottom .",
    "only the self - pairs @xmath30 of the diagonal @xmath31 can never be a distinction .",
    "all the possible distinctions @xmath32 are the dits of @xmath33 and no dits are distinctions of @xmath34 just as all the elements are in @xmath1 and none in @xmath10 .    in this manner",
    ", we can construct a table of analogies between subset logic and partition logic .",
    "[ c]|c||c|c| & subset logic & partition logic + ` elements ' & elements @xmath35 of @xmath14 & dits @xmath20 of @xmath22 + order & inclusion @xmath36 & refinement : @xmath37 + top of order & @xmath1 all elements & @xmath38 , all dits + bottom of order & @xmath10 no elements & @xmath39 , no dits + variables in formulas & subsets @xmath14 of @xmath1 & partitions @xmath22 on @xmath1 + operations & subset ops . &",
    "partition ops .",
    "@xcite + formula @xmath40 holds & @xmath35 element of @xmath41 & @xmath42 dit of @xmath43 + valid formula & @xmath44 , @xmath45 & @xmath46 , @xmath47 +    table of analogies between subset and partition logics    a dit set @xmath23 of a partition on @xmath1 is a subset of @xmath48 of a particular kind , namely the complement of an equivalence relation .",
    "an _ equivalence relation _ is reflexive , symmetric , and transitive .",
    "hence the complement is a subset @xmath49 that is :    1 .",
    "irreflexive ( or anti - reflexive ) , @xmath50 ; 2 .",
    "symmetric , @xmath51 implies @xmath52 ; and 3 .",
    "anti - transitive ( or co - transitive ) , if @xmath53 then for any @xmath54 , @xmath55 or @xmath56 ,    and such binary relations will be called _ partition relations _ ( also called _ apartness relations _ ) .    given any subset @xmath57 ,",
    "the _ reflexive - symmetric - transitive ( rst ) closure _ @xmath58 of the complement @xmath59 is the smallest equivalence relation containing @xmath59 , so its complement is the largest partition relation contained in @xmath14 , which is called the _ interior _",
    "@xmath60 of @xmath14 .",
    "this usage is consistent with calling the subsets that equal their rst - closures _",
    "closed subsets _ of @xmath48 ( so closed subsets = equivalence relations ) so the complements are the _ open subsets _ (= partition relations ) .",
    "however it should be noted that the rst - closure is not a topological closure since the closure of a union is not necessarily the union of the closures , so the `` open '' subsets do not form a topology on @xmath48 .",
    "the interior operation @xmath61 provides a universal way to define operations on partitions from the corresponding subset operations :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ apply the subset operation to the dit sets and then , if necessary , take the interior to obtain the dit set of the partition operation . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    given partitions @xmath17 and @xmath62 on @xmath1 , their _ join _",
    "@xmath63 is the partition whose dit set @xmath64 is the interior of @xmath65 ( since the union @xmath66 is the subset join operation ) .",
    "but the union of partition relations ( open subsets ) is a partition relation ( open subset ) so that :    @xmath67 .",
    "this gives the same join @xmath63 as the usual definition which is the partition whose blocks are the non - empty intersections @xmath68 for @xmath26 and @xmath27 .",
    "to define the _ meet _ @xmath69 of the two partitions , we apply the subset meet operation of intersection to the dit sets and then take the interior ( which is necessary in this case ) :",
    "@xmath70   $ ] .",
    "this gives the same result as the usual definition of the partition meet in the literature .",
    "defined here with refinement as the ordering relation . ]",
    "perhaps surprisingly , the other logical operations such as the implication do not seem to be defined for partitions in the literature .",
    "since the subset operation of implication is @xmath8 , we define the _ partition implication _ @xmath71 as the partition whose dit set is :    @xmath72   $ ] .",
    "is the partition that is like @xmath22 except that whenever a block @xmath26 is contained in a block @xmath27 , then @xmath18 is ` discretized ' in the sense of being replaced by all the singletons @xmath73 for @xmath74 .",
    "then it is immediate that the refinement @xmath25 holds iff @xmath75 , as we would expect from the corresponding relation , @xmath36 iff @xmath76 , in subset logic . ]",
    "the refinement partial order @xmath25 is just inclusion of dit sets , i.e. , @xmath25 iff @xmath37 . if we denote the lattice of partitions ( using the refinement ordering ) as @xmath77 , then the mapping :    @xmath78    dit set representation of partition lattice    represents the lattice of partitions as the lattice @xmath79 of open subsets ( under inclusion ) of @xmath80 .    for any finite set @xmath81 , a ( finite ) _ measure _",
    "@xmath82 is a function @xmath83 such that :    1 .",
    "@xmath84 , 2 .   for any @xmath85 , @xmath86 , and 3 .   for any disjoint subsets @xmath87 and @xmath88 , @xmath89 .",
    "any finite set @xmath81 has the _ counting measure _ @xmath90 and _ normalized counting measure _",
    "@xmath91 defined on the subsets of @xmath81 .",
    "hence for finite @xmath1 , we have the counting measure @xmath92 and the normalized counting measure @xmath93 defined on @xmath94 .",
    "boole used the normalized counting measure @xmath95 defined on the power - set boolean algebra @xmath96 to define the logical probability @xmath97 of an event @xmath2.@xcite in view of the analogy between elements in subset logic and dits in partition logic , the construction analogous to the logical probability is the normalized counting measure applied to dit sets . that is the definition of the :    @xmath98    _ logical entropy of a partition _ @xmath22 .",
    "thus the logical entropy function @xmath99 is the dit set representation composed with the normalized counting measure :    @xmath100 .",
    "logical entropy function    one immediate consequence is the inclusion - exclusion principle :    @xmath101    which provides the motivation for our definition below of @xmath102 as the `` logical mutual information '' of the partitions @xmath22 and @xmath103 .    in a random ( i.e. , equiprobable ) drawing of an element from @xmath1 , the event @xmath104",
    "occurs with the probability @xmath16 . if we take two independent ( i.e. , with replacement ) random drawings from @xmath1 , i.e. , pick a random ordered pair from @xmath48 , then @xmath105 is the probability that the pair is a distinction of @xmath22 , i.e. , that @xmath22 distinguishes .",
    "these analogies are summarized in the following table which uses the language of probability theory ( e.g. , set of outcomes , events , the occurrence of an event ) :    [ c]|c||c|c| & subset logic & partition logic + ` outcomes ' & elements @xmath35 of @xmath14 & ordered pairs @xmath106 + ` events ' & subsets @xmath14 of @xmath1 & partitions @xmath22 of @xmath1 + ` event occurs ' & @xmath107 & @xmath108 + norm .",
    "counting measure & @xmath109 & @xmath110 + interpretation & prob .",
    "event @xmath14 occurs & prob .",
    "partition @xmath22 distinguishes +    table of quantitative analogies between subset and partition logics .",
    "thus logical entropy @xmath111 is the simple quantitative measure of the distinctions of a partition @xmath22 just as the logical probability @xmath112 is the quantitative measure of the elements in a subset @xmath14 . in short , information theory is to partition logic as probability theory is to ordinary subset logic .    to generalize logical entropy from partitions to finite probability distributions ,",
    "note that :    @xmath113 .",
    "using @xmath114 , we have :    @xmath115 .",
    "an ordered pair @xmath116 for some @xmath26 is an _ indistinction _ or _ indit _ of @xmath22 where @xmath117 . hence in a random drawing of a pair from @xmath48 ,",
    "@xmath118 is the probability of drawing an indistinction , while @xmath119 is the probability of drawing a distinction .",
    "entropies will be defined both for partitions on finite sets and for finite probability distributions ( i.e. , finite random variables ) . given a random variable @xmath35 with the probability distribution @xmath120 over the @xmath121 distinct values @xmath122 , a distinction of the discrete partition on @xmath1 is just a pair @xmath123 with @xmath124 and with the probability @xmath125 . applying the previous notion to the logical entropy of a partition to this case with @xmath126 ( where @xmath127 ) , we have the :    @xmath128    _ logical entropy of a finite probability distribution _ @xmath129 .",
    "of the random variable @xmath35 but since the values of @xmath35 are irrelevant ( other than being distinct for @xmath124 ) , we can take the logical entropy @xmath130 as a function solely of the probability distribution @xmath129 of the random variable . ]    since @xmath131 , we again have the logical entropy @xmath132 as the probability @xmath133 of drawing a distinction in two independent samplings of the probability distribution @xmath129 .",
    "this is also clear from defining the product measure on the subsets @xmath57 :    @xmath134    _ product measure _ on @xmath48    then the logical entropy @xmath135 is just the product measure of the@xmath136dit set of the discrete partition on @xmath1 .",
    "there is also the obvious generalization to consider any partition @xmath22 on @xmath1 and then define for each block @xmath26 , @xmath137 . then the logical entropy @xmath138 is the product measure of the dit set of @xmath22 ( so it is still interpreted as the probability of drawing a distinction of @xmath22 ) and that is equivalent to @xmath139 .    for the uniform distribution @xmath140 , the logical entropy has its maximum value of @xmath141 .",
    "regardless of the first draw ( even for a different probability distribution over the same @xmath121 outcomes ) , the probability that the second draw is different is @xmath141 .",
    "the logical entropy has its minimum value of @xmath12 for @xmath142 so that :    @xmath143 .    an important special case is a set @xmath1 of @xmath144 equiprobable elements and a partition @xmath22 on @xmath1 with @xmath121 equal - sized blocks of @xmath145 elements each .",
    "then the number of distinctions of elements is @xmath146 which normalizes to the logical entropy of @xmath147 and which is independent of @xmath148 .",
    "thus it holds when @xmath149 and we take the elements to be the equal blocks themselves .",
    "thus for an equal - blocked partition on a set of equiprobable elements , the normalized number of distinctions of elements is the same as the normalized number of distinctions of blocks , and that quantity is the :    @xmath150    _ logical entropy of an equiprobable set of _ @xmath121 _ _",
    "elements__.      it might be noted that no averaging is involved in the interpretation of @xmath105 .",
    "it is the number of distinctions @xmath151 normalized .",
    "the definition of the logical entropy @xmath152 of a probability distribution @xmath120 is in the form of the average value of the random variable which has the value @xmath153 with the probability @xmath154 .",
    "hence the formula can be arrived at by applying the law of large numbers in the form where the finite random variable @xmath81 takes the value @xmath155 with probability @xmath154 :    @xmath156 .    at each step @xmath157 in repeated independent sampling @xmath158 of the probability distribution @xmath120 , the probability that the @xmath159 result @xmath160 was _ not _",
    "@xmath160 is @xmath161",
    "so the _ average _ probability of the result being different than it was at each place in that sequence is :    @xmath162 .    in the long run",
    ", the _ typical _ sequences will dominate where the @xmath163 outcome is sampled @xmath164 times so that we have the value @xmath165 occurring @xmath164 times :    @xmath166 .    the logical entropy @xmath167 is usually interpreted as the _ pair - drawing probability of getting distinct outcomes _ from the distribution @xmath168 .",
    "now we have a different interpretation of logical entropy as _ the average probability of being different_.      the logical entropy formula @xmath169 is the probability of getting distinct values @xmath170 in two independent samplings of the random variable @xmath35 .",
    "the complementary measure @xmath171 is the probability that the two drawings yield the same value from @xmath1 .",
    "thus @xmath172 is a measure of heterogeneity or diversity in keeping with our theme of information as distinctions , while the complementary measure @xmath173 is a measure of homogeneity or concentration .",
    "historically , the formula can be found in either form depending on the particular context . the @xmath154 s might be relative shares such as the relative share of organisms of the @xmath163 species in some population of organisms , and then the interpretation of @xmath154 as a probability arises by considering the random choice of an organism from the population .    according to i. j. good",
    ", the formula has a certain naturalness :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ if @xmath174 are the probabilities of @xmath175 mutually exclusive and exhaustive events , any statistician of this century who wanted a measure of homogeneity would have take about two seconds to suggest @xmath176 which i shall call @xmath177 .",
    "@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    as noted by bhargava and uppuluri @xcite , the formula @xmath178 was used by gini in 1912 ( @xcite reprinted in @xcite ) as a measure of mutability  or diversity .",
    "but another development of the formula ( in the complementary form ) in the early twentieth century was in cryptography .",
    "the american cryptologist , william f. friedman , devoted a 1922 book ( @xcite ) to the `` index of coincidence ''  ( i.e. , @xmath179 ) .",
    "solomon kullback ( of the kullback - leibler divergence treated later ) worked as an assistant to friedman and wrote a book on cryptology which used the index .",
    "@xcite    during world war ii , alan m. turing worked for a time in the government code and cypher school at the bletchley park facility in england .",
    "probably unaware of the earlier work , turing used @xmath180 in his cryptoanalysis work and called it the _ repeat rate _ since it is the probability of a repeat in a pair of independent draws from a population with those probabilities ( i.e. , the identification probability @xmath181 ) .",
    "polish cryptoanalyists had independently used the repeat rate in their work on the enigma @xcite .    after the war , edward h. simpson , a british statistician , proposed @xmath182 as a measure of species concentration ( the opposite of diversity ) where @xmath22 is the partition of animals or plants according to species and where each animal or plant is considered as equiprobable . and simpson gave the interpretation of this homogeneity measure as `` the probability that two individuals chosen at random and independently from the population will be found to belong to the same group.''@xcite hence @xmath183 is the probability that a random ordered pair will belong to different species , i.e. , will be distinguished by the species partition . in the biodiversity literature @xcite ,",
    "the formula is known as `` simpson s index of diversity ''  or sometimes , the _ gini - simpson index @xcite_.  however , simpson along with i. j. good worked at bletchley park during wwii , and , according to good , `` e. h. simpson and i both obtained the notion [ the repeat rate ] from turing . ''",
    "@xcite when simpson published the index in 1948 , he ( again , according to good ) did not acknowledge turing `` fearing that to acknowledge him would be regarded as a breach of security . ''",
    "@xcite    in 1945 , albert o. hirschman ( @xcite and @xcite ) suggested using @xmath184 as an index of trade concentration ( where @xmath154 is the relative share of trade in a certain commodity or with a certain partner ) .",
    "a few years later , orris herfindahl @xcite independently suggested using @xmath176 as an index of industrial concentration ( where @xmath154 is the relative share of the @xmath163 firm in an industry ) . in the industrial economics literature , the index @xmath185 is variously called the hirschman - herfindahl index , the hh index , or just the h index of concentration . if all the relative shares were equal ( i.e. , @xmath186 ) , then the identification or repeat probability is just the probability of drawing any element , i.e. , @xmath187 , so @xmath188 is the number of equal elements .",
    "this led to the `` numbers equivalent ''  interpretation of the reciprocal of the h index @xcite .",
    "in general , given an event with probability @xmath189 , the _ numbers - equivalent  interpretation _ of the event is that it is ` as if ' an element was drawn out of a set @xmath190 of @xmath191 equiprobable elements ( it is ` as if ' since @xmath192 need not be an integer ) .",
    "this interpretation will be used later in the dit - bit connection .    in view of the frequent and independent discovery and rediscovery of the formula @xmath180 or its complement @xmath193 by gini , friedman ,",
    "turing , hirschman , herfindahl , and no doubt others , i. j. good wisely advises that `` it is unjust to associate @xmath177 with any one person . ''",
    "@xcite    two elements from @xmath194 are either identical or distinct .",
    "gini @xcite introduced @xmath195 as the distance  between the @xmath163 and @xmath159 elements where @xmath196 for @xmath197 and @xmath198 . since @xmath199 , the logical entropy , i.e. , gini s index of mutability , @xmath200 , is the average logical distance between a pair of independently drawn elements .",
    "but one might generalize by allowing other distances @xmath201 for @xmath197 ( but always @xmath198 ) so that @xmath202 would be the average distance between a pair of independently drawn elements from @xmath1 . in 1982 , c. r. ( calyampudi radhakrishna ) rao introduced precisely this concept as _ quadratic entropy _ @xcite .",
    "in many domains , it is quite reasonable to move beyond the bare - bones _ logical distance _ of @xmath196 for @xmath197 ( i.e. , the complement @xmath203 of the kronecker delta ) so that rao s quadratic entropy is a useful and easily interpreted generalization of logical entropy . : @xmath204 . ]",
    "the shannon entropy will first be motivated in the usual fashion and then developed from the basic logical notion of entropy .",
    "shannon , like ralph hartley @xcite before him , starts with the question of how much `` information '' is required to single out a designated element from a set @xmath1 of equiprobable elements .",
    "this is often formulated in terms of the search @xcite for a hidden element like the answer in a twenty questions game or the sent message in a communication . but being able to always find the designated element is equivalent to being able to distinguish all elements from one another .",
    "that is , if the designated element was in a set of two or more elements that had not been distinguished from one another , then one would not be able to single out the designated element .",
    "thus `` singling out '' or `` identifying '' an element in a set is just another way to conceptualize `` distinguishing '' all the elements of the set .",
    "intuitively , one might measure `` information '' as the minimum number of yes - or - no questions in a game of twenty questions that it would take in general to _ distinguish _ all the possible `` answers '' ( or `` messages '' in the context of communications ) .",
    "this is readily seen in the simple case where @xmath205 , i.e. , the size of the set of equiprobable elements is a power of @xmath206 . then following the lead of wilkins over three centuries earlier , the @xmath207 elements could be encoded using words of length @xmath208 in a binary code such as the digits @xmath209 of binary arithmetic ( or @xmath210 in the case of wilkins ) .",
    "then an efficient or minimum set of yes - or - no questions needed to single out the hidden element is the set of @xmath208 questions :    `` is the @xmath159 digit in the binary code for the hidden element a @xmath11 ? ''    for @xmath211 .",
    "each element is distinguished from any other element by their binary codes differing in at least one digit .",
    "the information gained in finding the outcome of an equiprobable binary trial , like flipping a fair coin , is what shannon calls a _ bit _ ( derived from `` binary digit '' ) .",
    "hence the information gained in distinguishing all the elements out of @xmath207 equiprobable elements is :    @xmath212 bits    where @xmath213 is the probability of any given element ( henceforth all logs to base @xmath206 ) .",
    "this is usefully restated in terms of partitions .",
    "given two partitions @xmath17 and @xmath24 of @xmath1 , their _ join _",
    "@xmath63 is the partition of @xmath1 whose blocks are the non - empty intersections @xmath68 for @xmath26 and @xmath27 .",
    "the determination of the @xmath159 digit in the binary code for the hidden element defines a binary partition @xmath214 of @xmath1 . then to say that the answers to the @xmath208 questions above distinguish all the elements means that the join , @xmath215 , is the discrete partition on the set @xmath1 with cardinality @xmath207 .",
    "thus we could also take @xmath216 as the minimum number of binary partitions necessary to distinguish the elements ( i.e. , to single out any given element ) .    in the more general case where @xmath217 is not a power of @xmath206",
    ", we extrapolate to the definition of @xmath218 where @xmath219 as :    @xmath220    _ shannon - hartley entropy for an equiprobable set _ @xmath1 of @xmath121 elements .",
    "the definition is further extrapolated to the case where we are only given a probability @xmath189 so that we say that @xmath221 binary partitions are needed to distinguish a set of @xmath191 elements when @xmath191 is not an integer .",
    "this interpretation of the special case of @xmath207 or more generally @xmath222 equiprobable elements is extended to an arbitrary finite probability distribution @xmath120 by an averaging process . for the @xmath163 outcome ( @xmath223 ) , its probability @xmath154 is `` as if '' it were drawn from a set of @xmath224 equiprobable elements ( ignoring that @xmath224 may not be an integer for this averaging argument ) so the shannon - hartley information content of distinguishing the equiprobable elements of such a set would be @xmath225 .",
    "but that occurs with probability @xmath154 so the probabilistic average gives the usual definition of the :    @xmath226    _ shannon entropy of a finite probability distribution _ @xmath129 .    for the uniform distribution @xmath140 , the shannon entropy has it maximum value of @xmath227 while the minimum value is @xmath12 for the trivial distribution @xmath228 so that :    @xmath229",
    ".      shannon makes this averaging argument rigorous by using the law of large numbers .",
    "suppose that we have a three - letter alphabet @xmath230 where each letter was equiprobable , @xmath231 , in a multi - letter message .",
    "then a one - letter or two - letter message can not be exactly coded with a binary @xmath232 code with equiprobable @xmath12 s and @xmath11 s . but any probability can be better and better approximated by longer and longer representations in the binary number system . hence we can consider longer and longer messages of @xmath148 letters along with better and better approximations with binary codes .",
    "the long run behavior of messages @xmath158 where @xmath233 is modeled by the law of large numbers so that the letter @xmath234 will tend to occur @xmath235 times and similarly for @xmath236 and @xmath237 . such a message is called _",
    "typical_.    the probability of any one of those typical messages is :    @xmath238   ^{n}$ ]    or , in this case ,    @xmath239   ^{n}=\\left (   \\frac{1}{3}\\right )   ^{n}$ ] .    hence the number of such typical messages is @xmath240 .",
    "if each message was assigned a unique binary code , then the number of @xmath232 s in the code would have to be @xmath81 where @xmath241 or @xmath242 .",
    "hence the number of equiprobable binary questions or bits needed per letter of the messages is :    @xmath243 .",
    "this example shows the general pattern .    in the general case ,",
    "let @xmath120 be the probabilities over a @xmath121-letter alphabet @xmath244 .",
    "in an @xmath148-letter message , the probability of a particular message @xmath158 is @xmath245 where @xmath246 could be any of the symbols in the alphabet so if @xmath247 then @xmath248 .    in a _",
    "typical _ message , the @xmath163 symbol will occur @xmath164 times ( law of large numbers ) so the probability of a typical message is ( note change of indices to the letters of the alphabet ) :    @xmath249   ^{n}$ ] .    since the probability of a typical message is @xmath250 for @xmath251 , the typical messages are equiprobable .",
    "hence the number of typical messages is @xmath252   ^{n}$ ] and assigning a unique binary code to each typical message requires @xmath81 bits where @xmath253   ^{n}$ ] where :    @xmath254   ^{n}\\right\\ } = n\\log\\left [   \\pi_{k=1}^{n}p_{k}^{-p_{k}}\\right ]   $ ]    @xmath255    @xmath256 .",
    "hence the shannon entropy @xmath257 is interpreted as the limiting _",
    "average number of bits necessary per letter in the message_. in terms of distinctions , this is the _ average number of binary partitions necessary per letter to distinguish the messages_. it is this averaging result that allows us to consider `` the number of binary partitions it takes to distinguish the elements of @xmath1 '' when @xmath258 is not a power of @xmath206 since `` number '' is interpreted as `` average number . ''",
    "shannon entropy can also be defined for a partition @xmath17 on a set @xmath1 .",
    "if the elements of @xmath1 are equiprobable , then the probability that a randomly drawn element is in a block @xmath26 is @xmath259 . in a set of @xmath260",
    "equiprobable elements , it would take ( on average ) @xmath261 binary partitions to distinguish the elements . averaging over the blocks",
    ", we have the :    @xmath262    _ shannon entropy of a partition _ @xmath22 .",
    "the functional form of shannon s formula is often further `` justified '' or `` motivated '' by asserting that it is the same as the notion of entropy in statistical mechanics , and hence the name `` entropy . ''",
    "the name `` entropy '' is here to stay but the justification of the formula by reference to statistical mechanics is not quite correct .",
    "the connection between entropy in statistical mechanics and shannon s entropy is only via a numerical approximation , the stirling approximation , where if the first two terms in the stirling approximation are used , then the shannon formula is obtained .",
    "the first two terms in the stirling approximation for @xmath263 are : @xmath264 .",
    "the first three terms in the stirling approximation are : @xmath265 .",
    "if we consider a partition on a finite @xmath1 with @xmath144 , with @xmath121 blocks of size @xmath266 , then the number of ways of distributing the individuals in these @xmath121 boxes with those numbers @xmath267 in the @xmath163 box is : @xmath268 . the normalized natural log of @xmath269",
    ", @xmath270 is one form of entropy in statistical mechanics .",
    "indeed , the formula `` @xmath271 '' is engraved on boltzmann s tombstone .    the entropy formula",
    "can then be developed using the first two terms in the stirling approximation .",
    "@xmath272   $ ]    @xmath273   -\\sum _ { i}n_{i}\\left [   \\ln\\left (   n_{i}\\right )   -1\\right ]   \\right ]   $ ]    @xmath274   = \\frac{1}{n}\\left [ \\sum n_{i}\\ln\\left (   n\\right )   -\\sum n_{i}\\ln\\left (   n_{i}\\right )   \\right ]   $ ]    @xmath275    where @xmath276 ( and where the formula with logs to the base @xmath277 only differs from the usual base @xmath206 formula by a scaling factor ) .",
    "entropy @xmath278 is in fact an excellent numerical approximation to @xmath270 for large @xmath148 ( e.g. , in statistical mechanics ) .",
    "but the common claim is that shannon s entropy has the _ same functional form _ as entropy in statistical mechanics , and that is simply false .",
    "if we use a three - term stirling approximation , then we obtain an even better numerical approximation : , mackay @xcite also uses stirling s approximation to give a `` more accurate approximation '' ( using the next term in the stirling approximation ) to the entropy of statistical mechanics than the shannon entropy . ]    @xmath279    but no one would suggest using that `` more accurate '' entropy formula in information theory .",
    "shannon s formula should be justified and understood by the arguments given previously , and not by over - interpreting the approximate relationship with entropy in statistical mechanics .",
    "the basic datum is `` the '' set @xmath280 of @xmath121 elements with the equal probabilities @xmath219 . in that basic case of an equiprobable set",
    ", we can derive the dit - bit connection , and then by using a probabilistic average , we can develop the shannon entropy , expressed in terms of bits , from the logical entropy , expressed in terms of ( normalized ) dits , or vice - versa .",
    "given @xmath280 with @xmath121 equiprobable elements , the number of dits ( of the discrete partition on @xmath280 ) is @xmath281 so the normalized dit count is :    @xmath282 normalized dits .",
    "that is the dit - count or logical measure of the information is a set of @xmath121 distinct elements . or the normalized count @xmath283 . ]",
    "but we can also measure the information in the set by the number of binary partitions it takes ( on average ) to distinguish the elements , and that bit - count is :    @xmath284 bits .    by solving the dit - count and the bit - count for @xmath189 and equating",
    ", we can derive each measure in terms of the other :    @xmath285 and @xmath286    the dit - bit conversion formulas .",
    "the common thing being measured is an equiprobable @xmath280 where @xmath287 .",
    "the dit - count for @xmath280 is @xmath288 and the bit - count for @xmath280 is @xmath289 , and the bit - dit connection gives the relationship between the two counts .",
    "using this dit - bit connection between the two different ways to measure the `` information '' in @xmath280 , each entropy can be developed from the other .",
    "we start with the logical entropy of a probability distribution @xmath168 : @xmath290 .",
    "it is expressed as the probabilistic average of the dit - counts or logical entropies of the sets @xmath291 with @xmath224 equiprobable elements . , there is no necessity that @xmath292 is an integer so the dit - counts for @xmath291 are extrapolations while the bit - counts or binary partition counts for @xmath293 are already extrapolations even when @xmath121 is an integer but not a power of @xmath206 . ]",
    "but if we switch to the binary - partition bit - counts of the information content of those same sets @xmath291 of @xmath224 equiprobable elements , then the bit - counts are @xmath294 and the probabilistic average is the shannon entropy : @xmath295 . both entropies have the mathematical form :    @xmath296    and differ by using either the dit - count or bit - count to measure the information in @xmath291 .",
    "clearly the process is reversible , so one can use the dit - bit connection in reverse to develop the logical entropy @xmath130 from the shannon entropy @xmath297 .",
    "thus the two notions of entropy are simply two different ways , using distinctions ( dit - counts ) or binary partitions ( bit - counts ) , to measure the information in a probability distribution .",
    "moreover the dit - bit connection carries over to the compound notions of entropy so that the shannon notions of conditional entropy , mutual information , and joint entropy can be developed from the corresponding notions for logical entropy . since the logical notions are the values of a probability measure , the compound notions of logical entropy have the usual venn diagram relations such as the inclusion - exclusion principle .",
    "there is a well - known analogy between the `` venn diagram '' relationships for the shannon entropies and the relationships satisfied by any measure on a set ( @xcite , @xcite ) . as l. l. campbell puts it , the analogy :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ suggests the possibility that @xmath298 and @xmath299 are measures of sets , that @xmath300 is the measure of their union , that @xmath301 is the measure of their intersection , and that @xmath302 is the measure of their difference .",
    "the possibility that @xmath303 is the entropy of the `` intersection '' of two partitions is particularly interesting .",
    "this `` intersection , '' if it existed , would presumably contain the information common to the partitions @xmath304 and @xmath305.@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    all of campbell s desiderata are precisely true when :    * `` sets '' = dit sets , and * `` entropies '' = normalized counting measure of the ( dit ) sets , i.e. , the logical entropies .    since the logical entropies are the values of a measure , by developing the corresponding shannon notions from the logical ones , we have an explanation of why the shannon notions also exhibit the same venn diagram relationships .    the expository strategy is to first develop the shannon and logical compound notions of entropy separately and then to show the relationship using the dit - bit connection .",
    "given two partitions @xmath17 and @xmath62 on a finite set @xmath1 , how might one measure the new information that is provided by @xmath22 that was not already in @xmath103 ?",
    "campbell suggests associating sets with partitions so the conditional entropy would be the measure of the difference between the sets .",
    "taking the information as distinctions , we take the difference between the@xmath136dit sets , i.e. , @xmath306 , and then take the normalized counting measure of that subset of @xmath307 :    @xmath308    _ logical conditional entropy of _ @xmath22 _ _  given _ _",
    "@xmath103 .",
    "when the two partitions @xmath22 and @xmath103 are joined together in the join @xmath63 , whose blocks are the non - empty intersections @xmath68 , their information as distinctions is also joined together as sets , @xmath67 ( the `` union '' mentioned by campbell ) , which has the normalized counting measure of :    @xmath309   $ ]    _ logical entropy of a partition join _ @xmath63 .",
    "this logical entropy is interpreted as the probability that a pair of random draws from @xmath1 will yield a @xmath22-distinction _ or _ a @xmath310-distinction ( where `` or '' includes both ) .",
    "then the relationships between the logical entropy concepts can be read off the venn diagram inclusion - exclusion principle for the dit sets :    @xmath311    so that    @xmath312 .",
    "figure1.eps    figure 1 : venn diagram for subsets of @xmath48    the shaded area in the venn diagram has the dit - count measure :    @xmath313    @xmath314 .    for",
    "the corresponding definitions for random variables and their probability distributions , consider a random variable @xmath315 taking values in the product @xmath316 of finite sets with the joint probability distribution @xmath317 , and thus with the marginal distributions : @xmath318 and @xmath319 .",
    "for notational simplicity , the entropies can be considered as functions of the random variables or of their probability distributions , e.g. , @xmath320 . for the joint distribution",
    ", we have the :    @xmath321   $ ]    _ logical entropy of the joint distribution _    which is the probability that two samplings of the joint distribution will yield a pair of _ distinct _ ordered pairs @xmath322 , @xmath323 , i.e. , with an @xmath81-distinction @xmath324 _ or _ a @xmath325-distinction @xmath326 .    for the definition of the conditional entropy @xmath327",
    ", we simply take the product measure of the set of pairs @xmath315 and @xmath328 that give an @xmath81-distinction but not a @xmath325-distinction .",
    "thus given the first draw @xmath315 , we can again use a venn diagram to compute the probability that the second draw @xmath328 will have @xmath329 but @xmath330 .    to illustrate this using venn diagram reasoning ,",
    "consider the probability measure defined by @xmath317 on the subsets of @xmath316 .",
    "given the first draw @xmath315 , the probability of getting an @xmath315-distinction on the second draw is @xmath331 and the probability of getting a @xmath332-distinction is @xmath333 . a draw that is a @xmath332-distinction is , a fortiori , an @xmath322-distinction so the area @xmath334 is contained in the area @xmath335 .",
    "then the probability of getting an @xmath315-distinction that is not a @xmath332-distinction on the second draw is the difference : @xmath336 .",
    "figure2.eps    figure 2 : @xmath337    = probability of an @xmath338-distinction but not a @xmath332-distinction on @xmath316 .",
    "since the first draw @xmath315 was with probability @xmath317 , we have the following as the product measure of the subset of @xmath339   ^{2}$ ] of pairs @xmath340   $ ] that are @xmath81-distinctions but not @xmath325-distinctions :    @xmath341   $ ]    _ logical conditional entropy of _ @xmath338 _ _  given _ _ @xmath332 .",
    "then a little algebra quickly yields :    @xmath341   $ ]    @xmath342   -\\left [   1-\\sum _ { y}p\\left (   y\\right )   ^{2}\\right ]   = h\\left (   x , y\\right )   -h\\left (   y\\right ) $ ] .",
    "the summation over @xmath317 recasts the venn diagram to the set @xmath343 where the product probability measure ( for the two independent draws ) gives the logical entropies :    figure3.eps    figure 3 : @xmath344 .",
    "it might be noted that the logical conditional entropy , like the other logical entropies , is not just an average ; the conditional entropy is the product probability measure of the subset :    @xmath345   : x\\neq x^{\\prime},y = y^{\\prime}\\right\\ }   \\subseteq\\left (   x\\times y\\right )   \\times\\left (   x\\times y\\right )   $ ] .",
    "the shannon conditional entropy for partitions @xmath22 and @xmath103 is based on subset reasoning which is then averaged over a partition . given a subset @xmath346 @xmath347 ,",
    "a partition @xmath348 induces a partition of @xmath346 with the blocks @xmath349 .",
    "then @xmath350 is the probability distribution associated with that partition so it has a shannon entropy which we denote : @xmath351 .",
    "the shannon conditional entropy is then obtained by averaging over the blocks of @xmath103 :    @xmath352    _ shannon conditional entropy of _ @xmath22 _ _  given _ _",
    "@xmath103 .",
    "since the join @xmath63 is the partition whose blocks are the non - empty intersections @xmath68 ,    @xmath353 .",
    "developing the formula gives :    @xmath354   = h\\left (   \\pi \\vee\\sigma\\right )   -h\\left (   \\sigma\\right )   $ ] .",
    "thus the conditional entropy @xmath355 is interpreted as the shannon - information contained in the join @xmath63 that is not contained in @xmath103 .",
    "figure4.eps    figure 4 : @xmath356    `` venn diagram picture '' for shannon conditional entropy of partitions    given the joint distribution @xmath317 on @xmath316 , the conditional probability distribution for a specific @xmath357 is @xmath358 which has the shannon entropy : @xmath359 . then the conditional entropy is the average of these entropies :    @xmath360    _ shannon conditional entropy of _ @xmath338 _ _  given _ _ @xmath332 .    expanding as before gives @xmath361 with a similar venn diagram picture ( see below ) .",
    "now we can develop the shannon conditional entropy from the logical conditional entropy and thereby explain the venn diagram relationship .",
    "the logical conditional entropy is :    @xmath341   $ ]    where @xmath335 is the normalized dit count for the discrete partition on a set @xmath362 with @xmath363 equiprobable elements .",
    "hence that same equiprobable set requires the bit - count of @xmath364 binary partitions to distinguish its elements .",
    "similarly @xmath334 is the normalized dit count for ( the discrete partition on ) a set @xmath365 with @xmath366 equiprobable elements , so it requires @xmath367 binary partitions to make those distinctions .",
    "those binary partitions are included in the @xmath368 binary partitions ( since a @xmath332-distinction is automatically a @xmath315-distinction ) and we do nt want the @xmath332-distinctions so they are subtracted off to get : @xmath369 bits . taking the same probabilistic average ,",
    "the average number of binary partitions needed to make the @xmath338-distinctions but not the @xmath332-distinctions is :    @xmath370   = \\sum_{x , y}p\\left (   x , y\\right )   \\log\\left (   \\frac{p\\left (   y\\right ) } { p\\left (   x , y\\right )   } \\right )   = h\\left (   x|y\\right )   .$ ]    replacing the dit - counts by the bit - counts for the equiprobable sets , and taking the probabilistic average gives the same venn diagram picture for the shannon entropies .",
    "figure5.eps    figure 5 : @xmath371 .",
    "if the atom  of information is the distinction or dit , then the atomic information in a partition @xmath22 is  its dit set , @xmath372 . following again campbell s dictum about the mutual information , the information common to two partitions @xmath22 and @xmath103 would naturally be the intersection of their@xmath136dit sets :    @xmath373    _ mutual information set_.    it is an interesting and not completely trivial fact that as long as neither @xmath22 nor @xmath103 are the indiscrete partition @xmath34 ( where @xmath374 ) , then @xmath22 and @xmath103 have a distinction in common .",
    "[ non - empty dit sets intersect]given two partitions @xmath22 and @xmath103 on @xmath1 with non - empty dit sets , @xmath375 . , if every pair of elements @xmath21 is equated by one or the other of the relations , i.e. , @xmath376 , then either @xmath377 or @xmath378 . ]",
    "since @xmath22 is not the indiscrete partition , consider two elements @xmath35 and @xmath379 distinguished by @xmath22 but identified by @xmath103 [ otherwise @xmath380 .",
    "since @xmath103 is also not the indiscrete partition , there must be a third element @xmath381 not in the same block of @xmath103 as @xmath35 and @xmath379 .",
    "but since @xmath35 and @xmath379 are in different blocks of @xmath22 , the third element @xmath381 must be distinguished from one or the other or both in @xmath22 .",
    "hence @xmath382 or @xmath383 must be distinguished by both partitions and thus must be in their mutual information set @xmath384.@xmath385    the dit sets @xmath23 and their complementary indit sets (= equivalence relations ) @xmath386 are easily characterized as : @xmath387    the mutual information set can also be characterized in this manner .",
    "[ structure of mutual information sets]given partitions @xmath22 and @xmath103 with blocks @xmath388 and @xmath389 , then    @xmath390 .",
    "the union ( which is a disjoint union ) will include the pairs @xmath20 where for some @xmath26 and @xmath27 , @xmath391 and @xmath392 .",
    "since @xmath379 is in @xmath346 but not in the intersection @xmath68 , it must be in a different block of @xmath22 than @xmath18 so @xmath393 .",
    "symmetrically , @xmath394 so @xmath395 . conversely if @xmath396 then take the @xmath18 containing @xmath35 and the @xmath346 containing @xmath379",
    ". since @xmath20 is distinguished by both partitions , @xmath397 and @xmath398 so that @xmath399.@xmath385    the probability that a pair randomly chosen from @xmath48 would be distinguished by @xmath22 _ and _ @xmath103 would be given by the normalized counting measure of the mutual information set which is the :    @xmath400 @xmath401 probability that @xmath22 and @xmath103 distinguishes    _ mutual logical information of _ @xmath22 _ _  and _ _ @xmath103 .    by the inclusion - exclusion principle :    @xmath402 .",
    "normalizing , the probability that a random pair is distinguished by both partitions is given by the inclusion - exclusion principle : @xmath403    inclusion - exclusion principle for logical entropies of partitions    this can be extended after the fashion of the inclusion - exclusion principle to any number of partitions .",
    "it was previously noted that the intersection of two dit sets is not necessarily the dit set of a partition , but the interior of the intersection is the dit set @xmath404 of the partition meet @xmath69 .",
    "hence we also have the :    @xmath405    submodular inequality for logical entropies .",
    "consider again a joint distribution @xmath317 over @xmath316 for finite @xmath81 and @xmath325 .",
    "intuitively , the mutual logical information @xmath406 in the joint distribution @xmath317 would be the probability that a sampled pair @xmath315 would be a distinction of @xmath407 _ and _ a distinction of @xmath408 .",
    "that means for each probability @xmath317 , it must be multiplied by the probability of not drawing the same @xmath338 _ and _ not drawing the same @xmath332 ( e.g. , in a second independent drawing ) . in the venn diagram ,",
    "the area or probability of the drawing that @xmath338 or that @xmath332 is @xmath409 ( correcting for adding the overlap twice ) so the probability of getting neither that @xmath338 nor that @xmath332 is the complement @xmath410   + \\left [   1-p\\left (   y\\right )   \\right ] -\\left [   1-p\\left (   x , y\\right )   \\right ]   $ ] .",
    "figure6.eps    figure 6 : @xmath411   + \\left [   1-p\\left (   y\\right ) \\right ]   -\\left [   1-p\\left (   x , y\\right )   \\right ]   $ ]    = shaded area in venn diagram for @xmath316    hence we have :    @xmath412   + \\left [   1-p\\left (   y\\right )   \\right ]   -\\left [ 1-p\\left (   x , y\\right )   \\right ]   \\right ]   $ ]    _ logical mutual information in a joint probability distribution_.    the probability of two independent draws differing in _ either _ the @xmath338 _ or _",
    "the @xmath332 is just the logical entropy of the joint distribution :    @xmath413 = 1-\\sum_{x , y}p\\left (   x , y\\right )   ^{2}$ ] .    using a little algebra to expand the logical mutual information : @xmath414   + \\left [   1-{\\textstyle\\sum\\nolimits_{x , y } } p\\left (   x , y\\right )   p\\left (   y\\right )   \\right ]   -\\left [   1-{\\textstyle\\sum\\nolimits_{x , y } } p\\left (   x , y\\right )   ^{2}\\right ] \\\\ &   = h\\left (   x\\right )   + h\\left (   y\\right )   -h\\left (   x , y\\right)\\end{aligned}\\ ] ]    inclusion - exclusion principle for logical entropies of a joint distribution .",
    "figure7.eps    figure 7 : @xmath415    = shaded area in venn diagram for @xmath343 .",
    "it might be noted that the logical mutual information , like the other logical entropies , is not just an average ; the mutual information is the product probability measure of the subset :    @xmath345   : x\\neq x^{\\prime},y\\neq y^{\\prime}\\right\\ }   \\subseteq\\left (   x\\times y\\right )   \\times\\left (   x\\times y\\right )   $ ] .",
    "the usual heuristic motivation for shannon s mutual information is much like its development from the logical mutual information so we will take that approach at the outset .",
    "the logical mutual information for partitions can be expressed in the form :    @xmath416   $ ]    so if we substitute the bit - counts for the dit - counts as before , we get :    @xmath417   = \\sum_{b , c}p_{b\\cap c}\\log\\left ( \\frac{p_{b\\cap c}}{p_{b}p_{c}}\\right )   $ ]    _ shannon s mutual information for partitions_.    keeping the log s separate gives the venn diagram picture : @xmath418 \\\\ &   = h\\left (   \\pi\\right )   + h\\left (   \\sigma\\right )   -h\\left (   \\pi\\vee \\sigma\\right)\\end{aligned}\\ ] ]    inclusion - exclusion analogy for shannon entropies of partitions .      to move from partitions to probability distributions , consider again the joint distribution @xmath317 on @xmath316 . then developing the shannon mutual information from the logical mutual information amounts to replacing the block probabilities @xmath419 in the join @xmath63 by the joint probabilities @xmath317 and the probabilities in the separate partitions by the marginals ( since @xmath420 and @xmath421 ) , to obtain :    @xmath422    _ shannon mutual information in a joint probability distribution_.    then the same proof carries over to give the :    @xmath423    figure8.eps    figure 8 : inclusion - exclusion `` picture '' for shannon entropies of probability distributions .",
    "the logical mutual information formula :    @xmath412   + \\left [   1-p\\left (   y\\right )   \\right ]   -\\left [ 1-p\\left (   x , y\\right )   \\right ]   \\right ]   $ ]    develops via the dit - count to bit - count conversion to :    @xmath424   = \\sum _ { x , y}p\\left (   x , y\\right )   \\log\\left (   \\frac{p\\left (   x , y\\right )   } { p\\left ( x\\right )   p\\left (   y\\right )   } \\right )   = i\\left (   x , y\\right )   $ ] .",
    "thus the genuine venn diagram relationships for the product probability measure that gives the logical entropies carry over , via the dit - count to bit - count conversion , to give a similar venn diagram picture for the shannon entropies .",
    "two partitions @xmath22 and @xmath103 are said to be ( stochastically ) _ independent _ if for all @xmath26 and @xmath27 , @xmath425 . if @xmath22 and @xmath103 are independent , then :    @xmath426 ,    so that :    @xmath427    shannon entropy for partitions additive under independence .    in ordinary probability theory , two events @xmath428 for a sample space @xmath1 are said to be _ independent _ if @xmath429 .",
    "we have used the motivation of thinking of a partition - as - dit - set @xmath23 as an event  in a sample space @xmath48 with the probability of that event being @xmath105 , the logical entropy of the partition .",
    "the following proposition shows that this motivation extends to the notion of independence .",
    "[ independent partitions have independent dit sets]if @xmath22 and @xmath103 are ( stochastically ) independent partitions , then their dit sets @xmath23 and @xmath430 are independent as events in the sample space @xmath48 ( with equiprobable points ) .    for independent partitions @xmath22 and @xmath103 , we need to show that the probability @xmath431 of the event @xmath432 is equal to the product of the probabilities @xmath105 and @xmath433 of the events @xmath23 and @xmath434 in the sample space @xmath48 . by the assumption of stochastic independence , we have @xmath435 so that @xmath436 . by the previous structure theorem for the mutual information set : @xmath437 , where the union is disjoint so that :    @xmath438    so that :    @xmath439.@xmath385    hence the logical entropies behave like probabilities under independence ; the probability that @xmath22 _ and _ @xmath103 distinguishes , i.e. , @xmath440 , is equal to the probability @xmath441 that @xmath22 distinguishes times the probability @xmath442 that @xmath103 distinguishes :    @xmath443    logical entropy multiplicative under independence .",
    "it is sometimes convenient to think in the complementary terms of an equivalence relation `` equating '' or identifying  rather than a partition distinguishing .",
    "since @xmath105 can be interpreted as the probability that a random pair of elements from @xmath1 are distinguished by @xmath22 , i.e. , as a distinction probability , its complement @xmath444 can be interpreted as an _ identification probability _ ,",
    "i.e. , the probability that a random pair is equated by @xmath22 ( thinking of @xmath22 as an equivalence relation on @xmath1 ) . in general ,    @xmath445   \\left [   1-h\\left (   \\sigma\\right ) \\right ]   = 1-h\\left (   \\pi\\right )   -h\\left (   \\sigma\\right )   + h\\left ( \\pi\\right )   h\\left (   \\sigma\\right )   = \\left [   1-h\\left (   \\pi\\vee\\sigma\\right ) \\right ]   + \\left [   h\\left (   \\pi\\right )   h\\left (   \\sigma\\right )   -m(\\pi , \\sigma\\right ]   $ ]    which could also be rewritten as :    @xmath446   -\\left [   1-h\\left ( \\pi\\right )   \\right ]   \\left [   1-h\\left (   \\sigma\\right )   \\right ]   = m(\\pi , \\sigma)-h\\left (   \\pi\\right )   h\\left (   \\sigma\\right )   $ ] .    thus if @xmath22 and @xmath103 are independent , then the probability that the join partition @xmath63 identifies is the probability that @xmath22 identifies times the probability that @xmath103 identifies :    @xmath445   \\left [   1-h\\left (   \\sigma\\right ) \\right ]   = \\left [   1-h\\left (   \\pi\\vee\\sigma\\right )   \\right ]   $ ]    multiplicative",
    "identification probabilities under independence .",
    "a joint probability distribution @xmath317 on @xmath316 is _ independent _ if each value is the product of the marginals : @xmath447 .    for an independent distribution , the shannon mutual information    @xmath448",
    "is immediately seen to be zero so we have :    @xmath449    shannon entropies for independent @xmath317 .",
    "for the logical mutual information , independence gives : @xmath450 \\\\ &   = { \\textstyle\\sum\\nolimits_{x , y } } p\\left (   x\\right )   p\\left (   y\\right )   \\left [   1-p\\left (   x\\right )   -p\\left ( y\\right )   + p\\left (   x\\right )   p\\left (   y\\right )   \\right ] \\\\",
    "&   = { \\textstyle\\sum\\nolimits_{x } } p\\left (   x\\right )   \\left [   1-p\\left (   x\\right )   \\right ] { \\textstyle\\sum\\nolimits_{y } } p\\left (   y\\right )   \\left [   1-p\\left (   y\\right )   \\right ] \\\\ &   = h\\left (   x\\right )   h\\left (   y\\right)\\end{aligned}\\ ] ]    logical entropies for independent @xmath317 .",
    "this independence condition @xmath451 plus the inclusion - exclusion principle @xmath452 also implies that : @xmath453   \\left [   1-h\\left (   y\\right )   \\right ] &   = 1-h\\left (   x\\right )   -h\\left (   y\\right )   + h\\left (   x\\right )   h\\left ( y\\right ) \\\\ &   = 1-h\\left (   x\\right )   -h\\left (   y\\right )   + m\\left (   x , y\\right ) \\\\ &   = 1-h\\left (   x , y\\right )   \\text{.}\\ ] ]    hence under independence , the probability of drawing the same pair @xmath315 in two independent draws is equal to the probability of drawing the same @xmath338 times the probability of drawing the same @xmath332 .",
    "given two probability distributions @xmath120 and @xmath454 on the same sample space @xmath455 , we can again consider the drawing of a pair of points but where the first drawing is according to @xmath129 and the second drawing according to @xmath456 .",
    "the probability that the points are distinct would be a natural and more general notion of logical entropy that would be the :    @xmath457    _ logical _ _ cross entropy of _",
    "@xmath129 _ and _ @xmath456    which is symmetric",
    ". the logical cross entropy is the same as the logical entropy when the distributions are the same , i.e. , if @xmath458 , then @xmath459 .",
    "the notion of _ cross entropy _ in shannon entropy can be developed by applying dit - bit connection to the logical cross entropy @xmath460 to obtain :    @xmath461    which is not symmetrical due to the asymmetric role of the logarithm , although if @xmath458 , then @xmath462 .",
    "since the logical cross entropy is symmetrical , it could also be expressed as @xmath463 which develops to the shannon cross entropy @xmath464 so it might be more reasonable to use a _ symmetrized cross entropy _ :    @xmath465   $ ] .    the _ kullback - leibler divergence _ ( or _ relative entropy _ )",
    "@xmath466 is defined as a measure of the distance or divergence between the two distributions where @xmath467 .",
    "a basic result is the :    @xmath468 with equality if and only if @xmath458    _ information inequality _",
    "@xcite .    given two partitions @xmath22 and @xmath103 , the inequality @xmath469",
    "is obtained by applying the information inequality to the two distributions @xmath470 and @xmath471 on the sample space @xmath472 :    @xmath473    with equality iff independence .    in the same manner",
    ", we have for the joint distribution @xmath474 :    @xmath475    with equality iff independence .    the _ symmetrized kullback - leibler divergence _",
    "is :    @xmath476   = h_{s}\\left (   p||q\\right )   -\\left [   \\frac{h\\left (   p\\right ) + h\\left (   q\\right )   } { 2}\\right ]   $ ] .",
    "but starting afresh , one might ask : what is the natural measure of the difference or distance between two probability distributions @xmath120 and @xmath454 that would always be non - negative , and would be zero if and only if they are equal ?",
    "the ( euclidean ) distance between the two points in @xmath477 would seem to be the logical  answer ",
    "so we take that distance ( squared with a scale factor ) as the definition of the :    @xmath478 @xmath479    _ logical divergence _ ( or _ logical _ _ relative entropy _ ) . ]    which is symmetric and we trivially have :    @xmath480 with equality iff @xmath458    logical information inequality .",
    "we have component - wise :    @xmath481   -\\left [   \\frac{1}{n}-p_{i}^{2}\\right ]   -\\left [   \\frac{1}{n}-q_{i}^{2}\\right ]   $ ]    so that taking the sum for @xmath223 gives :    @xmath482   -\\frac{1}{2}\\left [   \\left (   1-{\\textstyle\\sum\\nolimits_{i } } p_{i}^{2}\\right )   + \\left (   1-{\\textstyle\\sum\\nolimits_{i } } q_{i}^{2}\\right )   \\right ] \\\\ &   = h\\left (   p\\vert q\\right )   -\\frac{h\\left (   p\\right )   + h\\left (   q\\right ) } { 2}\\text{.}\\ ] ]    logical divergence = _ jensen difference _",
    "@xcite between probability distributions .",
    "then the information inequality implies that the logical cross - entropy is greater than or equal to the average of the logical entropies :    @xmath483 with equality iff @xmath458 .",
    "the half - and - half probability distribution @xmath484 that mixes @xmath129 and @xmath456 has the logical entropy of    @xmath485   $ ]    so that :    @xmath486 with equality iff @xmath458",
    ".    mixing different @xmath129 and @xmath456 increases logical entropy .    the logical divergence can be expressed as :    @xmath487   -\\frac{1}{2}\\left [   \\left (   \\sum_{i}p_{i}\\left (   1-p_{i}\\right )   \\right )   + \\left ( \\sum_{i}q_{i}\\left (   1-q_{i}\\right )   \\right )   \\right ]   $ ]    that develops via the dit - bit connection to :    @xmath488   $ ]    @xmath489   = \\frac{1}{2}\\left [   d\\left (   p||q\\right )   + d\\left (   q||p\\right )   \\right ]   $ ]    @xmath490 .",
    "thus the logical divergence @xmath491 develops via the dit - bit connection to the symmetrized version of the kullback - leibler divergence .",
    "the following table summarizes the concepts for the shannon and logical entropies .",
    "we use the case of probability distributions rather than partitions , and we use the abbreviations @xmath492 , @xmath493 , and @xmath494 .",
    "[ c]l|c|c| & @xmath495 & @xmath496 + & @xmath497 & @xmath498 + & @xmath499 & @xmath500 + & @xmath501 & @xmath502 + & @xmath503 &  @xmath504   \\left [   1-h\\left (   y\\right )   \\right ]   $ ] + &  @xmath505 & @xmath506   $ ] + & @xmath507 & @xmath508 + & @xmath509 & @xmath510 + & @xmath511 &  @xmath512 + & @xmath513 & @xmath514   /2 $ ] + & @xmath515 & @xmath516 +      the above table shows many of the same relationships holding between the various forms of the logical and shannon entropies due ultimately to the dit - bit connection . the dit - bit connection between the two notions of entropy is based on them being two different measures of the `` amount of information - as - distinctions , '' the dit - count being the normalized count of the distinctions and the bit - count being the number of binary partitions required ( on average ) to make the distinctions .",
    "logical entropies arise naturally as the normalized counting measure for partition logic just as probabilities arise as the normalized counting measure for subset logic , where the two logics are dual to one another .",
    "all the forms of logical entropy have simple interpretations as the probabilities of distinctions .",
    "shannon entropy is a higher - level and more refined notion adapted to the theory of communications and coding where it can be interpreted as the average number of bits necessary per letter to code the messages , i.e. , the average number of binary partitions necessary per letter to distinguish the messages .",
    "ricotta , carlo and laszlo szeidl 2006 . towards a unifying approach to diversity measures : bridging the gap between the shannon entropy and rao s quadratic index .",
    "_ theoretical population biology_. 70 : 237 - 43 ."
  ],
  "abstract_text": [
    "<S> the logical basis for information theory is the newly developed logic of partitions that is dual to the usual boolean logic of subsets . </S>",
    "<S> the key concept is a `` distinction '' of a partition , an ordered pair of elements in distinct blocks of the partition . </S>",
    "<S> the logical concept of entropy based on partition logic is the normalized counting measure of the set of distinctions of a partition on a finite set  just as the usual logical notion of probability based on the boolean logic of subsets is the normalized counting measure of the subsets ( events ) . </S>",
    "<S> thus logical entropy is a measure on the set of ordered pairs , and all the compound notions of entropy ( join entropy , conditional entropy , and mutual information ) arise in the usual way from the measure ( e.g. , the inclusion - exclusion principle)just like the corresponding notions of probability . </S>",
    "<S> the usual shannon entropy of a partition is developed by replacing the normalized count of distinctions ( dits ) by the average number of binary partitions ( bits ) necessary to make all the distinctions of the partition . </S>"
  ]
}