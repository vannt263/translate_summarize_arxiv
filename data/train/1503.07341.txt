{
  "article_text": [
    "process mining is a technique that enables the automatic analysis of business processes based on event logs . instead of designing a workflow ,",
    "process mining consists in gathering the information of the tasks that take place during the workflow process and storing that data in structured formats called the event logs  @xcite .",
    "while gathering this information , it is assumed that ( 1 ) each event refers to a task in the business process , ( 2 ) each event is associated to an instance of the workflow and ( 3 ) since the events are stored by their execution time , it is assumed that they are sorted  @xcite .    during the last decade ,",
    "process mining has been growing a lot of attention in the scientific community due to its promise to provide techniques for process discovery that will lead to an increase of productivity and to the reduction of costs  @xcite .",
    "process modelling can be seen as the techniques to graphically represent a business process .",
    "this graphical representation describes dependencies between activities that need to be executed together in order to fulfil a business target  @xcite .    since in process mining the order of the events",
    "is taken into consideration , there are already many models that can be directly applied to represent the workflow .",
    "some of those models include markov chains  @xcite , petri nets  @xcite , neural networks  @xcite and bpmn  @xcite .",
    "however , markov chains and petri nets are the models that are most used in the literature of process mining  @xcite .    in this work",
    ", it is proposed an alternative representation of business process by using bayesian networks .",
    "a bayesian networks can be defined as an acyclic directed graph in which each node represents a random variable and each edge represents a direct influence from the source node to the target node ( conditional dependencies )  @xcite .",
    "they differ from markov chains , because of their cycle - free and directed structure . moreover , bayesian networks have the advantage of dealing with uncertainty differently from markov chains .",
    "in the latter , business processes are modelled as a chain of events that are observed to occur . under a bayesian network perspective",
    ", this does not apply : each task can either be _ present _ or _ absent _ in the business process .",
    "therefore , it is possible to perform special analysis that will enable the computation of the probability of some task of the business process occurring , given that we do not know which tasks have already been performed  @xcite .    with this research work ,",
    "we argue that the capabilities of bayesian networks provide a promising technique to model business processes , to perform analysis regarding risk management , cost reduction , finding irrelevant / repetitive tasks , etc .",
    "the outline of this work is as follows .",
    "section  [ sec : mc ] presents a brief summary of markov chains .",
    "section  [ sec : bn ] makes an introduction to bayesian networks .",
    "it shows how to compute probabilistic inferences and presents some learning techniques that are used to automatically learn conditional probabilities in bayesian networks .",
    "section  [ sec : bn_process ] presents how bayesian networks can be applied in the realm of process mining .",
    "this section demonstrates how one can define the structure of a bayesian network and how one can perform automatic learning .",
    "section  [ sec : case_study ] presents a case study in which we apply the proposed network .",
    "finally , section  [ sec : conclusions ] summarises the current work , presents the main conclusions achieved and some directions for future work .",
    "a markov chain is defined by a state space @xmath0 and a model that defines , for every state @xmath1 a next - state distribution over @xmath0 .",
    "more precisely , the transition model @xmath2 specifies for each pair of states @xmath3 the probability @xmath4 of going from state @xmath5 to @xmath6  @xcite .    in markov chains ,",
    "the transition probability matrix must be stochastic , that is , each row of the matrix must sum to one .",
    "matrix  [ eq : transition ] represents the transition matrix of the markov chain in figure  [ fig : markov_ex ] .",
    "@xmath7                \\label{eq : transition}\\ ] ]    suppose that one is in state @xmath8 at time @xmath9 . in order to compute the evolution of the system for @xmath10 ,",
    "one just needs to perform a matrix multiplication between the current state and the transition probability matrix .",
    "the current state @xmath8 will be encoded as vector @xmath11 $ ] .",
    "@xmath12    \\left [ \\begin{matrix } ~0.9   & 0.075 & 0.025~\\\\                                                              0.15     & 0.8    & 0.05   \\\\                                                           0.25    & 0.25   & 0.5    \\\\",
    "\\end{matrix } \\right ] = \\left [ \\begin{matrix } 0.15 & 0.8 & 0.05 \\end{matrix } \\right ]   \\label{eq : transition_ex}\\ ] ]    the calculations in formula  [ eq : transition_ex ] show that the probability from transiting from state @xmath13 is @xmath14 .",
    "the probability of transiting from @xmath15 is @xmath16 .",
    "and the probability of transiting from state @xmath17 is @xmath18 .    moreover , if one wishes to compute the probability of the sequence @xmath19 , one would need to perform the following calculations : @xmath20",
    "bayesian networks are directed acyclic graphs in which each node represents a different random variable from a specific domain and each edge represents a direct influence from the source node to the target node  @xcite .",
    "the graph represents independence relationships between variables and each node is associated with a conditional probability table ( cpt ) which specifies a distribution over the values of a node given each possible joint assignment of values of its parents .",
    "the full joint distribution of a bayesian network , where @xmath21 is the list of variables , is given by  @xcite :    @xmath22    the formula for computing classical exact inferences on bayesian networks is based on the full joint distribution ( equation  [ eq : joint ] ) .",
    "let @xmath23 be the list of observed variables and let @xmath24 be the remaining unobserved variables in the network . for some query @xmath21 , the inference is given by :    @xmath25 \\label{eq : inference}\\ ] ]    @xmath26 the summation is over all possible @xmath27 , i.e. , all possible combinations of values of the unobserved variables @xmath27 .",
    "the @xmath28 parameter , corresponds to the normalisation factor for the distribution",
    "@xmath29  @xcite .",
    "consider the bayesian network in figure  [ fig : example_bn ] .",
    "suppose that we want to determine the probability of raining given that we know that the grass is wet .        in order to perform such inference on a bayesian network , one can use equation  [ eq : inference ] in the following way : @xmath30 @xmath31 \\end{split}\\ ] ] @xmath32 = \\alpha~0.1604 = 0.3577   \\label{eq : pr_r_final}\\ ] ] given that bayesian networks are based on the nave bayes rule , one needs to normalize the final probabilities by a factor @xmath28 .",
    "this normalisation factor @xmath28 corresponds to : @xmath33 so , in order to compute @xmath28 , one also needs to compute the probability of not raining given that the grass is wet , pr ( r = f @xmath34 w = t ):",
    "@xmath35 @xmath36 \\end{split}\\ ] ] @xmath37 = \\alpha~0.288   \\label{eq : pr_nr_final}\\ ] ] going back to the normalisation factor in equation  [ eq : norm_fact ] , one can substitute @xmath38 by the result in equation  [ eq : pr_r_final ] and @xmath39 by the results in equation  [ eq : pr_nr_final ] . @xmath40 now that we have computed the normalisation factor , the final probabilities are : @xmath41 @xmath42      there are two main approaches to build a bayesian network .",
    "one is to construct the network by hand and to use the knowledge of an expert to estimate the conditional probability tables .",
    "the second is to use statistical models to automatically _ learn _ these probabilities  @xcite .    estimating the conditional probabilities by hand with the knowledge of an expert is problematic for several reasons . in some situations ,",
    "the network is so big that it is almost impossible for the expert to make a reliable assignment of the probabilities to the random variables .",
    "moreover , in many situations , the distribution of the data varies according to its application and through time .",
    "this makes it impossible for an expert to reliably estimate the probabilities associated to the random variables of the bayesian network .",
    "statistical models , on the other hand , offer a mechanism to automatically learn a model that represents the probability distribution of some population .    according to the situation that one is modelling",
    ", one can have a fully observed dataset or have an incomplete dataset ( or partially observed ) .",
    "for the scope of this work , we will only address the problem of learning in bayesian networks with a fully observed dataset and a known graphical structure .",
    "the data are considered fully observed if on each of the training instances there is a full instantiation to all the random variables of our sample space  @xcite .",
    "the maximum likelihood estimation is a statistical method that assumes that data follows a gaussian probability distribution .",
    "the mean and the variance of the probability distribution can be estimated by only knowing a partial sample of the dataset  @xcite .",
    "suppose that we have a bayesian network just like specified in figure  [ fig : learning_ex1 ] .",
    "this network is parameterized by a parameter vector @xmath43 which specifies the parameters for the conditional probability distribution of the network .",
    "the training instances regarding figure  [ fig : learning_ex1 ] consist in a tuple of the form @xmath44 , y\\left [ m \\right ]   \\rangle$ ] , where @xmath5 is an instance of the random variable @xmath21 , @xmath27 is an instance of the random variable @xmath24 and @xmath45 is the @xmath46 training example from the training dataset @xmath47 of size @xmath48 .",
    "the likelihood function is given by : @xmath49 , y\\left [ m \\right ] : \\theta ) \\label{eq : learn1}\\ ] ]    since in a bayesian network we can specify a full joint probability distribution @xmath50 , y\\left [ m \\right ] : \\theta ) $ ] by the chain rule , then , equation  [ eq : learn1 ] becomes :    @xmath51 : \\theta_x ) pr ( y\\left [ m \\right ] | x\\left [ m \\right ]   : \\theta_{y|x } ) \\label{eq : learn2}\\ ] ]    @xmath52 : \\theta_x ) \\right ) \\left ( \\prod_{m } pr ( y\\left [ m \\right ] | x\\left [ m \\right ]   : \\theta_{y|x } ) \\right ) \\label{eq : learn3}\\ ] ]    equation  [ eq : learn3 ] shows that the likelihood function can be decomposed into two separate terms .",
    "if we had @xmath53 random variables , then equation  [ eq : learn3 ] would also have @xmath53 terms .",
    "each of these terms is called a _ local likelihood function _ and can estimate how well a variable can predict its parents .",
    "moreover , one can expand the second term of equation  [ eq : learn3 ] for each instance of @xmath5 in the following way : @xmath54",
    "| x\\left [ m \\right ]   : \\theta_{y|x } ) = \\prod_{m : x\\left [ m \\right ] = x_{true } } pr ( y\\left [ m \\right ] | x\\left [ m \\right ]   : \\theta_{y|x_{true } } ) \\prod_{m : x\\left [ m \\right ] = x_{false } } pr ( y\\left [ m \\right ] | x\\left [ m \\right ]   : \\theta_{y|x_{false } } ) \\end{split } \\label{eq : learn4}\\ ] ]    going back to the simple bayesian network in figure  [ fig : learning_ex1 ] , if we analyse the first term of  equation[eq : learn4 ] , we can see that it refers to the number of instances of the training data in which @xmath55 .",
    "this gives us two sets : @xmath56 and @xmath57 .",
    "equation  [ eq : learn6 ] discriminates these instances .",
    "@xmath58 = x_{true } } pr ( y\\left [ m \\right ] | x\\left [ m \\right ]   : \\theta_{y|x_{true } } ) = \\theta_{y = true | x = true } .",
    "\\theta_{y = false | x = true } \\label{eq : learn6}\\ ] ]    then , equation  [ eq : learn6 ] becomes : @xmath59 @xmath60    from equation  [ eq : learn7 ] , we can see that the maximum likelihood estimate for a bayesian network with a known structure and fully observed data consists in simply counting how many times each of the possible assignments of @xmath21 and @xmath24 appear in the training data . in order to obtain a probability value , we normalize this score by counting the total number of instances that class @xmath21 appears .",
    "samiam - _ sensitivity , analysis , modeling , inference and more _ - is a tool that enables the graphical modeling of bayesian networks .",
    "it was developed by the _",
    "automated reasoning group _ form the university of california .",
    "samiam is composed of a graphical interface and a reasoning engine .",
    "the graphical interface provides an easy way to model bayesian networks by specifying the random variables as nodes , causal connections as edges and the respective conditional probability tables .",
    "the reasoning engine , on the other hand , can perform classical inferences over the plotted bayesian network , make parameter estimations by learning mechanisms , sensitivity analysis , etc . for",
    "the scope of this work , only the classical inference and the learning mechanisms will be necessary .",
    "examples of samiam s graphical interface are given by figures  [ fig : ex1 ] to  [ fig : ex6 ] .    in figure",
    "[ fig : ex1 ] , it is presented the bayesian network from figure  [ fig : example_bn ] under the samiam graphical interface .",
    "the marginal probabilities for each node are automatically computed as soon as the user builds the bayesian network .",
    "figure  [ fig : ex1 ] shows that : @xmath61 , @xmath62 , @xmath63 , @xmath64 , @xmath65 , @xmath66 .",
    "figure  [ fig : ex2 ] represents a graphical representation of the inference that was manually computed in equations  [ eq : pr_r_final ] and  [ eq : pr_nr_final ] .",
    "the red markers represent variables which are _",
    "observed_. that is , variables , which have occurred .",
    "they can be seen as the conditions of probabilities .",
    "for instance , in the manually computed probability in equation  [ eq : pr_r ] , the observed variable was the condition @xmath67 , that is , we are asking the probability of raining given that it was observed that the grass was wet .    for large bayesian networks ,",
    "the inference process becomes very heavy and hard to be computed manually .",
    "therefore , samiam provides an easy interface that automatically performs such heavy operations .    in process mining , event logs",
    "are usually associated with a large amount of tasks , which can be mapped into nodes of a bayesian network .",
    "consequently , for the scope of this work , we chose the capabilities of samiam to automatically compute inferences related to the probability of certain sequences of tasks occurring .",
    "this mechanism will be more detailed in section  [ sec : samiam ] of this work .",
    "probabilistic graphical models , such as bayesian networks , are usually used for probabilistic inferences , that is , asking queries to the model and receiving answers in the form of probability values .    under the realm of process mining ,",
    "bayesian networks can represent activities as nodes ( i.e. random variables ) and the edges between activities can be seen as transitions between these tasks . from this structure ,",
    "it is possible to automatically learn the conditional probability tables from a complete log of events using the maximum likelihood estimations ( section  [ sec : mle ] ) .",
    "if the log is incomplete , then a bayesian network can also automatically learn and estimate the probability tables through the usage of em clustering , just like used in the work of  @xcite , who developed a bayesian network to recommend business processes .    in the literature ,",
    "business processes that are learnt from event logs are usually represented by either markov chains or petri nets  @xcite . in this work , however , we propose another approach to model business processes using bayesian networks .",
    "the reason why we do this is concerned with the fact that bayesian networks can deal with uncertainty more easily .",
    "bayesian networks provide advantages in situations where we do not know if some task has occurred and we need to determine the probability of the process terminating or the probability of the process reaching some other task . therefore , these structures provide more insights when there are high levels of uncertainty when compared to markov chains .",
    "another advantage of bayesian networks is that they allow the direct representation of business process diagrams by capturing the direct dependencies between tasks .",
    "however , they do not allow an explicit representation of cycles , because bayesian networks are directed acyclic graphs .",
    "to represent a cycle , in a bayesian network , one would need to create many instances of the same node , which is intractable to perform inferences , since the inference problem is _ np - complete _ ( figures  [ fig : cycle_example ] and  [ fig : cycle_example_markov ] ) .    in this work , in order to eliminate cycles from the log of events",
    ", we used an heuristic that would choose the most probable transitions between nodes .",
    "for instance , suppose that there is a transition from nodes @xmath68 that occurred 900 times .",
    "suppose also that there is a transition from nodes @xmath13 that occurred 100 times .",
    "following the proposed heuristic , we would only represent the bayesian network with the transition @xmath68 .",
    "figures  [ fig : cycles_h1 ] and  [ fig : cycles_h2 ] illustrates this example .    another structure that bayesian networks can not represent directly is concerned with mutual exclusion .",
    "two events are mutually exclusive if they can not occur at the same time .",
    "bayesian networks can capture mutually exclusive events through the notions of independence by manually adding new edges to the network .",
    "for instance , consider the business process represented by the bayesian network in figure  [ fig : mutual1 ] . nodes @xmath8 and @xmath69 represent the end of the process , while node @xmath70 represents a task that begins the process . in this situation , and following the semantics of the business process , it is required that nodes @xmath8 and @xmath69 become mutually exclusive .",
    "that is , the process flow can only end in one of these nodes and not on both of them at the same time .        as one can see in figures  [ fig : mutual2 ] and  [ fig : mutual3 ] , the bayesian network can not represent this mutual exclusion .",
    "when computing bayesian inferences , all nodes depend on each other .",
    "therefore , in order to semantically represent _ node b can not occur at the same time as node c _ , one needs to add an extra edge between @xmath17 .",
    "this additional edge will create a new dependency between these nodes .",
    "one can manually configure the conditional probability table of node @xmath69 to represent this mutually exclusion : when node @xmath8 is set to @xmath71 , then the probability of occurring @xmath69 is zero and vice - versa .",
    "the mutual exclusion of the bayesian network in figure  [ fig : mutual1 ] is illustrated in figures  [ fig : mutual4 ] to  [ fig : mutual6 ] .",
    "note that , in figures  [ fig : mutual4 ] to  [ fig : mutual6 ] , the probability of node @xmath69 occurring when nothing is observed changed when compared to the bayesian network of figure  [ fig : mutual1 ] .",
    "this happened , because of the extra edge that was added in the later bayesian network , which ended up changing the configurations of the conditional probability tables and , consequently , final probability values .",
    "samiam provides an intuitive interface for constructing bayesian network .",
    "there are two modes in samiam : the query mode ( for learning and inferences ) and the edit mode ( for network structure and definition of conditional probabilities ) .",
    "when samiam is started , the edit mode appears by default .",
    "figure  [ fig : edit_mode ] describes the general edit mode interface .",
    "the interface enables the creation / removal of nodes and the creation / removal of edges between nodes . for each node",
    "created , there will be a configuration window that can be accessed when the node is _",
    "double - clicked_. in this window , one must specify a unique identifier for the node and a name to be displayed in the samiam interface .",
    "additionally , one also needs to specify which states the node can have . for the scope of this work",
    ", we will only have binary random variables , so each node will have exactly two states : one representing the occurrence of the random variable and another representing its absence .",
    "the conditional probability table can be accessed by clicking the tab _",
    "probailities_. a windows , similar to the one presented in figure  [ fig : cpt ] , will appear .        in this window , a user can manually specify the conditional probabilities of the random variable . by default",
    ", samiam fills these tables using a normal probability distribution , that is , each instance of each node has the same probability of occurring ( pr = 0.5 ) .    the buttons _ complement",
    "_ can be used to automatically assign the last probability value of the table .",
    "this takes into account the constraint that the probabilities of an event must sum to one .",
    "this way , the user can only manually specify @xmath72 entries of the table .",
    "samiam computes the remaining probability by subtracting that value with 1 : @xmath73 .",
    "the button _ normalize _ normalizes all the entries of the conditional probability table .",
    "given a log of events and a graphical structure , samiam is able to find a statistical model that can automatically estimate the conditional probability tables of the given bayesian network .",
    "this learning process can be computed using the maximum likelihood estimation ( section  [ sec : mle ] ) if the log of events is complete or using the em clustering algorithm if the log of events is incomplete  @xcite .    in the scope of this work ,",
    "since we were given a complete event log , the process of filling the conditional probability tables was given by the maximum likelihood estimation , that is , by counting the number of times each instance of the log of events was present and then by normalizing to obtain a probability value .",
    "samiam can automatically do this in the query mode . in the main samiam interface , one can select the _ query mode _ just like presented in figure  [ fig : querymode1 ] .",
    "to go into the learning menu , one needs to find the option",
    "_ em learning _",
    "( figure  [ fig : querymode2 ] ) .",
    "under the _ em learning _",
    "menu , the user is presented with a window that asks for a training file , a probability threshold , the maximum number of iterations that the algorithm should perform and if the learning algorithm should ignore entries that lead to divisions by zero .",
    "figure  [ fig : samiam_learning ] illustrates these options .        in figure",
    "[ fig : samiam_learning ] , the field _ max iterations",
    "_ corresponds to the total number of iterations that the _ em clustering _",
    "should perform in case the algorithm does not converge . for the scope of this work ,",
    "this entry is irrelevant since we are dealing with fully observed log of events .",
    "consequently , the em clustering will collapse to the maximum likelihood estimate .",
    "the field _ log - likelihood threshold _ is also used in the scope of the em clustering .",
    "this threshold specifies that the algorithm will converge when the change in the log - likelihood function falls bellow a certain threshold .",
    "it is a common practice in the literature to set this value to @xmath18  @xcite .",
    "the option _ use bias to prevent divisions by zero _ should always be used , otherwise the maximum likelihood estimate formula will try to perform a division by zero when it tries to compute the probability of an instance that does not exist in the training set .    in process mining , a training set consists in a portion of the log of events that is used to fit ( train ) a model for prediction of values . in the scope of this work ,",
    "a training set will consist of 70% randomized entries of the log of events .",
    "the format of the training file contains the names of all random variables ( nodes ) in the first line .",
    "the remaining lines of the file correspond to the instances of the nodes that are specified in the log of events . in this work ,",
    "we modeled binary random variables with the instances _ present _ to represent the occurrence of a task in the business process and _ absent _ to represent the non - existence of the task .",
    "figure  [ fig : training ] shows the log of events ( left ) and the conversion of one instance of the log of events into a training file with the samian format ( right ) .",
    "after samiam learns the conditional probability tables , it is necessary to correct some semantics of the network .",
    "more specific the inclusion of mutually exclusive relationships .",
    "for instance , figure  [ fig : cpt_learned ] presents a conditional probability table that was automatically learned by samiam .",
    "as one can see , when the node a_partlysubmitted is absent , samiam did not update the normal probability distribution , so the probabilities @xmath74 remained in the conditional probability table .",
    "this means that there were no events in the log that did not have an instance of the a_partlysubmitted node .",
    "this happens , because in process mining , the activities that are performed are usually mutually exclusive , unless a special structure is used to say the contrary . in order to correct these probabilities , such that the mutual exclusion is captured",
    ", one just needs to fill the conditional probability table just like illustrated in figure  [ fig : cpt_corrected ] . when the preceding node is _",
    "absent _ , then the posterior nodes should also become _",
    "the event log that we use in this work is taken from a dutch financial institute ] .",
    "the event log represents a loan application belonging to a global financial organization , in which a customer requests a certain amount of money .",
    "the process is composed of three different sub processes .",
    "the first letter of each task corresponds to an identifier of the sub process it belongs to .",
    "the tasks that start with letter @xmath75 correspond to states of the application .",
    "the tasks that start with letter @xmath76 correspond to offers belonging to the application . and",
    "the tasks that start with letter @xmath77 correspond to the work item belonging to the application .",
    "the general scenario is as follows .",
    "there is a webpage that enables the submission of loan applications .",
    "a customer selects a certain amount of money and then submits his request .",
    "then , the application performs some automatic tasks and checks if an application is eligible .",
    "if it is eligible , then the customer is sent an offer by mail .",
    "after this offer is received , it will be evaluated . in case of any missing information",
    ", the offer goes back to the client and is again evaluated until all the required information is gathered .",
    "a final evaluation is done to the application .",
    "finally , the application is approved and activated .",
    "the log contains @xmath78 events and @xmath79 cases .",
    "the statistics of the log of events is summarised in table  [ tab : data ] .",
    ".summary of the statistics of the loan application event log .",
    "only complete events were taken into account . [",
    "cols=\"<,^,<,^\",options=\"header \" , ]     in order to validate both approaches , we leveraged on the test set and computed the probability of each sequence occurring in a bayesian network and in a markov chain . in the end , those probabilities were weighted with the number of occurrences of each sequence in the test set .",
    "the results obtained are discriminated in table  [ tab : resfinal ] .    .",
    "[ tab : resfinal ]    table  [ tab : resfinal ] shows that the probabilities computed in a bayesian network are almost identical to the ones computed by the markov chain .",
    "individually , the probabilities of computing the sequences in the test set did not have an error percentage superior to @xmath80 , which is statistically insignificant given the total amount of data tested .",
    "moreover , the overall error percentage between the proposed bayesian network and the markov chain was around @xmath81 , which is also statistically insignificant .",
    "this means that the bayesian networks have a similar performance as a markov chain .",
    "consequently , one can conclude that bayesian networks are also good approaches to model business processes , with the advantage of being able to represent uncertainty ( computing probabilities of tasks that we do not know if occurred ) .",
    "as already mentioned , one of the capabilities of bayesian networks for process mining is their ability to deal with uncertainty .",
    "they enable the analysis of tasks that are not known to occur .",
    "for instance , for the loan application bayesian network , one can be interested in analyzing the probability of the business process ending successfully by only knowing that a couple of tasks were observed to occur . combining this ability with samiam s graphical capabilities will enable a fast analysis of business processes as well as risk management .",
    "figure  [ fig : declined ] shows the probabilities of some nodes of the loan application bayesian network , when it is only known that the application was declined , that is , the node a_declined was observed to occur . from this analysis , one can conclude that the majority of the applications that are declined have a high probability of reaching the state a_preaccept .",
    "moreover , if an application is declined , then the nodes a_activated and a_cancelled are never reached .",
    "another example is given by figure  [ fig : cancelled ] .",
    "when it is known that the application ended up in a cancelled state , then one can estimate with a @xmath82 probability that the process reached the task a_preaccept and never reached the tasks a_declined and a_activated",
    ". moreover , there is a high probability that the application was cancelled during the tasks a_accepted and a_finalized .",
    "the maximum uncertainty in the loan application business process is given when one only knows that the process was started , which happens when the task a_submitted is observed to occur . in this situation ,",
    "the proposed bayesian network estimates that there is a high probability of the process going to the task a_preaccept or being declined ( a_declined ) . if one chooses task a_preaccept , then from figure  [ fig : preaccepted ] one can conclude that there is a high probability that the process will be either accepted or finalised .",
    "in this work , we propose the usage of bayesian networks as a new approach to represent business processes automatically extracted from event logs .      experiments made over a loan application case study suggest that bayesian networks have the same performance as markov chains , so they are good models to make accurate predictions about sequences of events in the scope of process mining .    moreover , by modelling a business process through bayesian networks ,",
    "one is able to take advantage of the ability of these structures to deal with uncertainty .",
    "more specifically , bayesian networks enable the reconstruction of a flow by only taking into account partial observations in the business process . as for future work",
    ", it would be interesting to extend the capabilities of bayesian networks to learn from incomplete logs of events .",
    "one could train such network using the em clustering in order to find an approximate probability distribution for the occurrence of the tasks . moreover , together with samiam , one could try to estimate the most probable sequences of business processes using the probabilities learned from the incomplete log .",
    "bautista , a.  d. , wangikar , l. & akbar , s. m.  k. ( 2012 ) , process mining - driven optimization of a consumer loan approvals process : the bpic 2012 challenge , _ in _ ` proceedings of the 8th international workshop on business process intelligence ' .",
    "bose , j.  c. & van  der aalst , w. ( 2012 ) , process mining applied to the bpi challenge 2012 : divide and conquer while discerning resources , _ in _ ` proceedings of the 8th international workshop on business process intelligence ' .",
    "ferreira , d. , zacarias , m. , malheiros , m. & ferreira , p. ( 2007 ) , approaching process mining with sequence clustering : experiments and findings , _ in _ ` in proceedings of the 5th international conference on business process management ' .",
    "kang , c.  j. , shin , c.  k. , lee , e.  s. , kim , j.  h. & an , m.  a. ( 2012 ) , analyzing application process for a personal loan or overdraft of dutch financial institute with process mining techniques , _ in _ ` proceedings of the 8th international workshop on business process intelligence ' .",
    "rebuge , a. & ferreira , d. ( 2012 ) , ` business process analysis in healthcare environments : a methodology based on process mining ' , _ journal of information systems : management and engineering of process - aware information systems _ * 37 * ,  99116 .",
    "van  der aalst , w. & de  medeiros , a.  k. ( 2005 ) , ` process mining and security : detecting anomalous process executions and checking process conformance ' , _ journal of electronic notes in theoretical computer science _ * 121 * ,  321 .",
    "van  der aalst , w. , weijters , t. & maruster , l. ( 2004 ) , ` workflow mining : discovering process models from event logs ' , _ journal of ieee transactions on knowledge and data engineering _ * 16 * ,  1128  1142 ."
  ],
  "abstract_text": [
    "<S> process mining is a technique that performs an automatic analysis of business processes from a log of events with the promise of understanding how processes are executed in an organisation .    </S>",
    "<S> several models have been proposed to address this problem , however , here we propose a different approach to deal with uncertainty . by uncertainty , </S>",
    "<S> we mean estimating the probability of some sequence of tasks occurring in a business process , given that only a subset of tasks may be observable .    in this sense </S>",
    "<S> , this work proposes a new approach to perform process mining using bayesian networks . </S>",
    "<S> these structures can take into account the probability of a task being present or absent in the business process . </S>",
    "<S> moreover , bayesian networks are able to automatically learn these probabilities through mechanisms such as the maximum likelihood estimate and em clustering .    </S>",
    "<S> experiments made over a loan application case study suggest that bayesian networks are adequate structures for process mining and enable a deep analysis of the business process model that can be used to answer queries about that process . </S>"
  ]
}