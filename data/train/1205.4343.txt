{
  "article_text": [
    "in the standard pac model @xcite and other similar theoretical models of learning @xcite , the distribution according to which training and test points are drawn is fixed over time . however",
    ", for many tasks such as spam detection , political sentiment analysis , financial market prediction under mildly fluctuating economic conditions , or news stories , the learning environment is not stationary and there is a continuous drift of its parameters over time .",
    "there is a large body of literature devoted to the study of related problems both in the on - line and the batch learning scenarios . in the on - line scenario , the target function",
    "is typically assumed to be fixed but no distributional assumption is made , thus input points may be chosen adversarially @xcite .",
    "variants of this model where the target is allowed to change a fixed number of times have also been studied @xcite . in the batch scenario ,",
    "the case of a fixed input distribution with a drifting target was originally studied by helmbold and long @xcite .",
    "a more general scenario was introduced by bartlett @xcite where the joint distribution over the input and labels could drift over time under the assumption that the @xmath0 distance between the distributions in two consecutive time steps was bounded by @xmath1 .",
    "both generalization bounds and lower bounds have been given for this scenario @xcite .",
    "in particular , long @xcite showed that if the @xmath0 distance between two consecutive distributions is at most @xmath1 , then a generalization error of @xmath2 is achievable and barve and long @xcite proved this bound to be tight .",
    "further improvements were presented by freund and mansour @xcite under the assumption of a constant rate of change for drifting .",
    "other settings allowing arbitrary but infrequent changes of the target have also been studied @xcite .",
    "an intermediate model of drift based on a near relationship was also recently introduced and analyzed by @xcite where consecutive distributions may change arbitrarily , modulo the restriction that the region of disagreement between nearby functions would only be assigned limited distribution mass at any time .",
    "this paper deals with the analysis of learning in the presence of drifting distributions in the batch setting .",
    "we consider both the general drift model introduced by @xcite and a related drifting pac model that we will later describe .",
    "we present new generalization bounds for both models ( sections  [ sec : pac ] and [ sec : tracking ] ) . unlike the @xmath0 distance used by previous authors to measure the distance between distributions ,",
    "our bounds are based on a notion of _ discrepancy _ between distributions generalizing the definition originally introduced by @xcite in the context of domain adaptation .",
    "the @xmath0 distance used in previous analyses admits several drawbacks : in general , it can be very large , even in favorable learning scenarios ; it ignores the loss function and the hypothesis set used ; and it can not be accurately and efficiently estimated from finite samples ( see for example lower bounds on the sample complexity of testing closeness by @xcite ) .",
    "in contrast , the discrepancy takes into consideration both the loss function and the hypothesis set .",
    "the learning bounds we present in sections  [ sec : pac ] and [ sec : tracking ] are tighter than previous bounds both because they are given in terms of the discrepancy which lower bounds the @xmath0 distance , and because they are given in terms of the rademacher complexity instead of the vc - dimension . additionally , our proofs are often simpler and more concise .",
    "we also present a generalization of the standard on - line to batch conversion to the scenario of drifting distributions in terms of the discrepancy measure ( section  [ sec : online ] ) .",
    "our guarantees hold for convex combinations of the hypotheses generated by an on - line learning algorithm .",
    "these bounds lead to the definition of a natural meta - algorithm which consists of selecting the convex combination of weights in order to minimize the discrepancy - based learning bound ( section  [ sec : algorithm ] ) .",
    "we show that this optimization problem can be formulated as a simple qp and report the results of preliminary experiments demonstrating its benefits .",
    "finally we will discuss the practicality of our algorithm in some natural scenarios .",
    "in this section , we introduce some preliminary notation and key definitions , including that of the _ discrepancy _ between distributions , and describe the learning scenarios we consider .",
    "let @xmath3 denote the input space and @xmath4 the output space .",
    "we consider a loss function @xmath5 bounded by some constant @xmath6 .",
    "for any two functions @xmath7 and any distribution @xmath8 over @xmath9 , we denote by @xmath10 the expected loss of @xmath11 and by @xmath12 the expected loss of @xmath11 with respect to @xmath13 : @xmath14 \\qquad \\text{and } \\qquad { { \\cal l}}_d(h , h ' ) = \\operatorname*{\\rm e}_{x \\sim d^1 } [ l(h(x ) , h'(x))],\\ ] ] where @xmath15 is the marginal distribution over @xmath3 derived from @xmath8 .",
    "we adopt the standard definition of the empirical rademacher complexity , but we will need the following sequential definition of a rademacher complexity , which is related to that of @xcite .",
    "[ def : empiricalrademacher ] let @xmath16 be a family of functions mapping from a set @xmath17 to @xmath18 and @xmath19 a fixed sample of size @xmath20 with elements in @xmath17 .",
    "the _ empirical rademacher complexity _ of @xmath16 for the sample @xmath21 is defined by : @xmath22,\\ ] ] where @xmath23 , with @xmath24s independent uniform random variables taking values in @xmath25 .",
    "the _ rademacher complexity _ of @xmath16 is the expectation of @xmath26 over all samples @xmath19 of size @xmath20 drawn according to the product distribution @xmath27 : @xmath28.\\ ] ]    note that this coincides with the standard rademacher complexity when the distributions @xmath29 , @xmath30 $ ] , all coincide .    a key question for the analysis of learning with a drifting scenario is a measure of the difference between two distributions @xmath8 and @xmath31 .",
    "the distance used by previous authors is the @xmath0 distance .",
    "however , the @xmath0 distance is not helpful in this context since it can be large even in some rather favorable situations .",
    "moreover , the @xmath0 distance can not be accurately and efficiently estimated from finite samples and it ignores the loss function used .",
    "thus , we will adopt instead the _ discrepancy _ , which provides a measure of the dissimilarity of two distributions that takes into consideration both the loss function and the hypothesis set used , and that is suitable to the specific scenario of drifting .",
    "our definition of discrepancy is a generalization to the drifting context of the one introduced by @xcite for the analysis of domain adaptation .",
    "observe that for a fixed hypothesis @xmath32 , the quantity of interest with drifting distributions is the difference of the expected losses @xmath33 for two consecutive distributions @xmath8 and @xmath31 .",
    "a natural distance between distributions in this context is thus one based on the supremum of this quantity over all @xmath32 .    given a hypothesis set @xmath34 and a loss function @xmath35 ,",
    "the @xmath4-_discrepancy _ @xmath36 between two distributions @xmath8 and @xmath31 over @xmath9 is defined by : @xmath37    in a deterministic learning scenario with a labeling function @xmath38 , the previous definition becomes @xmath39 where @xmath40 and @xmath15 are the marginal distributions associated to @xmath8 and @xmath31 defined over @xmath3 .",
    "the target function @xmath38 is unknown and could match any hypothesis @xmath13 .",
    "this leads to the following definition @xcite .",
    "given a hypothesis set @xmath34 and a loss function @xmath35 , the _ discrepancy _ @xmath41 between two distributions @xmath8 and @xmath31 over @xmath9 is defined by : @xmath42    an important advantage of this last definition of discrepancy , in addition to those already mentioned , is that it can be accurately estimated from finite samples drawn from @xmath40 and @xmath15 when the loss is bounded and the rademacher complexity of the family of functions @xmath43 is in @xmath44 , where @xmath20 is the sample size ; in particular when @xmath45 has a finite pseudo - dimension @xcite . the discrepancy is by definition symmetric and verifies the triangle inequality for any loss function @xmath35 . in general , it does not define a _ distance _ since we may have @xmath46 for @xmath47 .",
    "however , in some cases , for example for kernel - based hypothesis sets based on a gaussian kernel , the discrepancy has been shown to be a distance @xcite .",
    "we will present our learning guarantees in terms of the @xmath4-discrepancy @xmath36 , that is the most general definition since guarantees in terms of the discrepancy @xmath41 can be straightforwardly derived from them .",
    "the advantage of the latter bounds is the fact that the discrepancy can be estimated in that case from unlabeled finite samples .",
    "we will consider two different scenarios for the analysis of learning with drifting distributions : the _ drifting pac scenario _ and the _ drifting tracking scenario_.    the drifting pac scenario is a natural extension of the pac scenario , where the objective is to select a hypothesis @xmath11 out of a hypothesis set @xmath34 with a small expected loss according to the distribution @xmath48 after receiving a sample of @xmath49 instances drawn from the product distribution @xmath50 .",
    "thus , the focus in this scenario is the performance of the hypothesis @xmath11 with respect to the environment distribution after receiving the training sample .",
    "the drifting tracking scenario we consider is based on the scenario originally introduced by @xcite for the zero - one loss and is used to measure the performance of an algorithm @xmath51 ( as opposed to any hypothesis @xmath11 ) . in that learning model , the performance of an algorithm",
    "is determined based on its average predictions at each time for a sequence of distributions .",
    "we will generalize its definition by using the notion of discrepancy and extending it to other loss functions .",
    "the following definitions are the key concepts defining this model .    for any sample @xmath52 of size @xmath20 , we denote by @xmath53 the hypothesis returned by an algorithm @xmath51 after receiving the first @xmath54 examples and by @xmath55 its loss or mistake on @xmath56 : @xmath57 . for a product distribution @xmath27 on @xmath58",
    "we denote by @xmath59 the expected mistake of @xmath51 : @xmath60 = \\operatorname*{\\rm e}_{s \\sim d}[l(h_{t-1}(x_t ) , y_t)].\\ ] ]    [ def : tracking ] let @xmath61 and let @xmath62 be the supremum of @xmath59 over all distribution sequences @xmath63 , with @xmath64 .",
    "algorithm @xmath51 is said to _",
    "@xmath65-track @xmath34 _ if there exists @xmath66 such that for @xmath67 we have @xmath68 .",
    "an analysis of the tracking scenario with the @xmath0 distance used to measure the divergence of distributions instead of the discrepancy was carried out by long @xcite and barve and long @xcite , including both upper and lower bounds for @xmath62 in terms of @xmath1 .",
    "their analysis makes use of an algorithm very similar to empirical risk minimization , which we will also use in our theoretical analysis of both scenarios .",
    "in this section , we present guarantees for the drifting pac scenario in terms of the discrepancies of @xmath29 and @xmath69 , @xmath70 $ ] , and the rademacher complexity of the hypothesis set .",
    "we start with a generalization bound in this scenario and then present a bound for the agnostic learning setting .",
    "let us emphasize that learning bounds in the drifting scenario should of course not be expected to converge to zero as a function of the sample size but depend instead on the divergence between distributions .",
    "[ th : generalization ] assume that the loss function @xmath35 is bounded by @xmath71 .",
    "let @xmath72 be a sequence of distributions and let @xmath73 .",
    "then , for any @xmath74 , with probability at least @xmath75 , the following holds for all @xmath76 : @xmath77    we denote by @xmath8 the product distribution @xmath50 .",
    "let @xmath78 be the function defined over any sample @xmath79 by @xmath80 let @xmath21 and @xmath81 be two samples differing by one labeled point , say @xmath82 in @xmath21 and @xmath83 in @xmath81 , then : @xmath84 \\leq \\frac{m}{t}.\\ ] ] thus , by mcdiarmid s inequality , the following holds : @xmath85 > { \\epsilon}\\big ] \\leq \\exp(-2t{\\epsilon}^2/m^2).\\ ] ] we now bound @xmath86 $ ] by first rewriting it , as follows : @xmath87&\\\\ & \\leq \\!\\operatorname*{\\rm e}\\!\\big [ \\sup_{h \\in h } { { \\cal l}}_{d_{t + 1}}(h ) \\!-\\ !",
    "\\frac{1}{t } \\sum_{t = 1}^t { { \\cal l}}_{d_{t}}(h ) \\big ]   \\!+\\ !",
    "\\operatorname*{\\rm e}\\!\\big[\\sup_{h \\in h }   \\frac{1}{t } \\sum_{t =      1}^t { { \\cal l}}_{d_{t}}(h ) \\!-\\ !",
    "\\frac{1}{t } \\sum_{t = 1}^t l(h(x_t ) , y_t ) \\big]\\\\ & \\leq \\!\\operatorname*{\\rm e}\\ ! \\big [ \\frac{1}{t } \\sum_{t = 1}^t \\sup_{h \\in h } \\big ( { { \\cal l}}_{d_{t + 1}}(h ) -    { { \\cal l}}_{d_{t}}(h )   \\big ) \\!+\\ !",
    "\\sup_{h \\in h }   \\frac{1}{t } \\sum_{t =      1}^t \\big ( { { \\cal l}}_{d_{t}}(h ) - l(h(x_t ) , y_t ) \\big ) \\big]\\\\ & \\leq \\frac{1}{t } \\sum_{t = 1}^t { \\mathrm{disc}}_{\\mathcal{y}}(d_t , d_{t + 1 } ) + \\operatorname*{\\rm e}\\ ! \\big [ \\sup_{h \\in h }   \\frac{1}{t } \\sum_{t =      1}^t \\big ( { { \\cal l}}_{d_{t}}(h ) - l(h(x_t ) , y_t ) \\big ) \\big].\\end{aligned}\\ ] ] it is not hard to see , using a symmetrization argument as in the non - sequential case , that the second term can be bounded by @xmath88 .    for many commonly used loss functions , the empirical rademacher complexity @xmath89 can be upper bounded in terms of that of the function class @xmath34 .",
    "in particular , for the zero - one loss it is known that @xmath90 and when @xmath35 is the @xmath91 loss for some @xmath92 , that is @xmath93 for all @xmath94 , then @xmath95 .",
    "indeed , since @xmath96 $ ] , by talagrand s contraction lemma , @xmath89 is bounded by @xmath97 with @xmath98 . furthermore , @xmath99 can be analyzed as follows : @xmath100 \\\\ \\begin{aligned } & = \\frac{1}{t } \\operatorname*{\\rm e}_{{\\boldsymbol \\sigma}}\\bigg [ \\sup_{h \\in h } \\sum_{t = 1}^t \\sigma_t h(x_t ) \\bigg ] + \\frac{1}{t}\\operatorname*{\\rm e}_{{\\boldsymbol \\sigma}}\\bigg [ \\sum_{t = 1}^t -\\sigma_t y_t \\bigg ]   = { \\widehat}{\\mathfrak{r}}_t(h ) , \\end{aligned}\\end{gathered}\\ ] ] since @xmath101   = 0 $ ] .",
    "taking the expectation of both sides yields a similar inequality for rademacher complexities .",
    "thus , in the statement of the previous theorem , @xmath89 can be replaced with @xmath102 when @xmath35 is the @xmath91 loss .",
    "observe that the bound of theorem  [ th : generalization ] is tight as a function of the divergence measure ( discrepancy ) we are using .",
    "consider for example the case where @xmath103 , then a standard rademacher complexity generalization bound holds for all @xmath76 : @xmath104 now , our generalization bound for @xmath105 includes only the additive term @xmath106 , but by definition of the discrepancy , for any @xmath107 , there exists @xmath76 such that the inequality @xmath108 holds .",
    "next , we present pac learning bounds for empirical risk minimization .",
    "let @xmath109 be a best - in class hypothesis in @xmath34 , that is one with the best expected loss . by a similar reasoning as in theorem [ th : generalization ] , we can show that with probability @xmath110 we have @xmath111 let @xmath112 be a hypothesis returned by empirical risk minimization ( erm ) . combining this inequality with the bound of theorem [ th : generalization ] while using the definition of @xmath112 and using the union bound",
    ", we obtain that with probability @xmath75 the following holds : @xmath113 this learning bound indicates a trade - off : larger values of the sample size @xmath20 guarantee smaller first and third terms ; however , as @xmath20 increases , the average discrepancy term is likely to grow as well , thereby making learning increasingly challenging .",
    "this suggests an algorithm similar to empirical risk minimization but limited to the last @xmath114 examples instead of the whole sample with @xmath115 .",
    "this algorithm was previously used in @xcite for the study of the tracking scenario .",
    "we will use it here to prove several theoretical guarantees in the pac learning model .",
    "[ prop:8 ] let @xmath116 .",
    "assume that @xmath117 is a sequence of distributions such that @xmath118 for all @xmath119 .",
    "fix @xmath120 and let @xmath112 denote the hypothesis returned by the algorithm @xmath51 that minimizes @xmath121 after receiving @xmath122 examples . then",
    ", for any @xmath123 , with probability at least @xmath75 , the following learning bound holds : @xmath124    the proof is straightforward .",
    "notice that the algorithm discards the first @xmath125 examples and considers exactly @xmath114 instances .",
    "thus , as in inequality [ agnosticineq ] , we have : @xmath126 now , we can use the triangle inequality to bound @xmath127 by @xmath128 .",
    "thus , the sum of the discrepancy terms can be bounded by @xmath129 .    to obtain the best learning guarantee",
    ", we can select @xmath114 to minimize the bound just presented .",
    "this requires the expression of the rademacher complexity in terms of @xmath114 .",
    "the following is the result obtained when using a vc - dimension upper bound of @xmath130 for the rademacher complexity .",
    "[ cor : cubicpac ] fix @xmath131 .",
    "let @xmath34 be a hypothesis set with vc - dimension @xmath132 such that for all @xmath120 , @xmath133 for some constant @xmath134 .",
    "assume that @xmath135 is a sequence of distributions such that @xmath136 for all @xmath137 .",
    "then , there exists an algorithm @xmath51 such that for any @xmath123 , the hypothesis @xmath112 it returns after receiving @xmath138^{\\frac 2 3 } ( \\frac{d}{\\delta^2})^{\\frac 1 3}$ ] instances , where @xmath139 , satisfies the following with probability at least @xmath75 : @xmath140^{2/3}(d\\delta)^{1/3 } + \\delta.\\ ] ]    fix @xmath123 . replacing @xmath141 by the upper bound @xmath142 in yields @xmath143 choosing @xmath144 to minimize the right - hand side gives exactly .",
    "when @xmath34 has finite vc - dimension @xmath132 , it is known that @xmath141 can be bounded by @xmath145 for some constant @xmath134 , by using a chaining argument @xcite .",
    "thus , the assumption of the corollary holds for many loss functions @xmath35 , when @xmath34 has finite vc - dimension .",
    "in this section , we present a simpler proof of the bounds given by @xcite for the agnostic case demonstrating that using the discrepancy as a measure of the divergence between distributions leads to tighter and more informative bounds than using the @xmath0 distance .",
    "let @xmath61 and let @xmath117 be a sequence of distributions such that @xmath118 for all @xmath137 .",
    "let @xmath146 and let @xmath112 be as in proposition [ prop:8 ] . then",
    ", @xmath147 - \\inf_h{{\\cal l}}_{d_{t+1}}(h ) \\leq 4{\\mathfrak{r}}_{m}(h_l ) + 2m\\sqrt{\\frac{\\pi}{m } } + ( m + 1)\\delta.\\ ] ]    let @xmath148 and @xmath149 .",
    "by fubini s theorem we can write : @xmath150 - \\inf_h{{\\cal l}}_{d_{t+1}}(h ) =    \\operatorname*{\\rm e}_{d'}\\big[{{\\cal l}}_{d_{t+1}}(h_t ) - \\inf_h{{\\cal l}}_{d_{t+1}}(h)\\big].\\ ] ] now , let @xmath151 , then , by , for @xmath152 , the following holds : @xmath153 < \\phi(\\beta).\\ ] ] thus , the expectation on the right - hand side of can be bounded as follows : @xmath154 \\leq 4{\\mathfrak{r}}_{m}(h_l ) + ( m + 1)\\delta + \\int_{4{\\mathfrak{r}}_{m}(h_l)+(m+1)\\delta}^\\infty \\mspace{-40mu } \\phi(\\beta ) d\\beta.\\end{gathered}\\ ] ] the last integral can be rewritten as @xmath155 using the change of variable @xmath156 .",
    "this concludes the proof .    the following corollary can be shown using the same proof as that of corollary  [ cor : cubicpac ] .",
    "fix @xmath131 .",
    "let @xmath34 be a hypothesis set with vc - dimension @xmath132 such that for all @xmath157 , @xmath158 .",
    "let @xmath135 be a sequence of distributions over @xmath9 such that @xmath136 .",
    "let @xmath159 and @xmath160^{2/3}$ ] .",
    "then , for @xmath161^{\\frac 2 3 } ( \\frac{d}{\\delta^2})^{\\frac 1 3}$ ] , the following inequality holds : @xmath162 - \\inf_h{{\\cal l}}_{d_{t+1}}(h )",
    "< k(d\\delta)^{1/3 } + \\delta.\\ ] ]    in terms of definition [ def : tracking ] , this corollary shows that algorithm @xmath51 @xmath163-tracks @xmath34 .",
    "this result is similar to a result of @xcite which states that given @xmath107 if @xmath164 then @xmath51 @xmath165-tracks @xmath34 .",
    "however , in @xcite , @xmath1 is an upper bound on the @xmath0 distance and not the discrepancy .",
    "our result provides thus a tighter and more general guarantee than that of @xcite , the latter because this result is applicable to any loss function and not only the zero - one loss , the former because our bound is based on the rademacher complexity instead of the vc - dimension and more importantly because it is based on the discrepancy , which is a finer measure of the divergence between distributions than the @xmath0 distance .",
    "indeed , for any @xmath30 $ ] , @xmath166 furthermore , when the target function @xmath38 is in @xmath34 , then the @xmath4-discrepancies can be bounded by the discrepancies @xmath167 , which , unlike the @xmath0 distance , can be accurately estimated from finite samples .",
    "it is important to emphasize that even though our analysis was based on a particular algorithm , that of `` truncated '' empirical risk minimization , the bounds obtained here can not be improved upon in the general scenario of drifting distributions , as shown by @xcite in the case of binary classification .",
    "distance and the discrepancy . in the left figure ,",
    "the @xmath0 distance is given by twice the area of the green rectangle .",
    "in the right figure , @xmath168 is equal to the area of the blue rectangle and @xmath169 is the area of the red rectangle .",
    "the two areas are equal , thus @xmath170 . ]",
    "[ fig : discrepancy ]    we now illustrate the difference between the guarantees we present and those based on the @xmath0 distance by presenting a simple example for the zero - one loss where the @xmath0 distance can be made arbitrarily close to 2 while the discrepancy is 0 . in that case ,",
    "our bounds state that the learning problem is as favorable as in the absence of any drifting , while a learning bound with the @xmath0 distance would be uninformative . consider measures @xmath171 and",
    "@xmath172 in @xmath173 where @xmath171 is uniform in the rectangle @xmath174 defined by the vertices @xmath175 and @xmath172 is uniform in the rectangle @xmath176 spanned by @xmath177 .",
    "the measures are depicted in figure [ fig : discrepancy ] .",
    "the @xmath0 distance of these probability measures is given by twice the difference of measure in the green rectangle , i.e , @xmath178 this distance goes to @xmath179 as @xmath180 . on the other hand",
    "consider the zero - one loss and the hypothesis set consisting of threshold functions on the first coordinate , i.e. @xmath181 iff @xmath182 .",
    "for any two hypotheses @xmath183 the area of disagreement of this two hypotheses is given by the stripe @xmath184 .",
    "but it is trivial to see that @xmath185 , but also @xmath186 , since this is true for any pair of hypotheses we conclude that @xmath170 .",
    "this example shows that the learning bounds we presented can be dramatically more favorable than those given in the past using the @xmath0 distance .",
    "although this may be viewed as a trivial illustrative example , the discrepancy and the @xmath0 distance can greatly differ in more complex but realistic cases .",
    "in this section , we present learning guarantees for drifting distributions in terms of the regret of an on - line learning algorithm @xmath51 .",
    "the algorithm processes a sample @xmath187 sequentially by receiving a sample point @xmath188 , generating a hypothesis @xmath189 , and incurring a loss @xmath190 , with @xmath191 .",
    "we denote by @xmath192 the regret of algorithm @xmath51 after processing @xmath49 sample points : @xmath193 the standard setting of on - line learning assumes an adversarial scenario with no distributional assumption .",
    "nevertheless , when the data is generated according to some distribution , the hypotheses returned by an on - line algorithm @xmath51 can be combined to define a hypothesis with strong learning guarantees in the distributional setting when the regret @xmath192 is in @xmath194 ( which is attainable by several regret minimization algorithms ) @xcite . here ,",
    "we extend these results to the drifting scenario and the case of a convex combination of the hypotheses generated by the algorithm .",
    "the following lemma will be needed for the proof of our main result .",
    "[ lemma:13 ] let @xmath195 be a sample drawn from the distribution @xmath196 and let @xmath197 be the sequence of hypotheses returned by an on - line algorithm sequentially processing @xmath198 .",
    "let @xmath199 be a vector of non - negative weights verifying @xmath200 .",
    "if the loss function @xmath35 is bounded by @xmath71 then , for any @xmath123 , with probability at least @xmath75 , each of the following inequalities hold : @xmath201 where @xmath202 denotes the average discrepancy @xmath203 .",
    "consider the random process : @xmath204 and let @xmath205 denote the filtration associated to the sample process .",
    "we have : @xmath206 and @xmath207 = \\operatorname*{\\rm e}_{d}[w_tl(h_t(x_t ) , y_t)|{\\mathcal{f}}_{t-1 } ] - \\operatorname*{\\rm e}_{d_t}[w_tl(h_t(x_t ) , y_t ) ] = 0\\ ] ] the second equality holds because @xmath189 is determined at time @xmath208 and @xmath209 are independent of @xmath210 .",
    "thus , by azuma - hoeffding s inequality , for any @xmath123 , with probability at least @xmath75 the following holds : @xmath211 by definition of the discrepancy , the following inequality holds for any @xmath30 $ ] : @xmath212 summing up these inequalities and using to bound @xmath213 proves the first statement .",
    "the second statement can be proven in a similar way .",
    "the following theorem is the main result of this section .",
    "[ th : onlinetobatch ] assume that @xmath35 is bounded by @xmath71 and convex with respect to its first argument .",
    "let @xmath214 be the hypotheses returned by @xmath51 when sequentially processing @xmath215 and let @xmath11 be the hypothesis defined by @xmath216 , where @xmath217 are arbitrary non - negative weights verifying @xmath218 .",
    "then , for any @xmath123 , with probability at least @xmath75 , @xmath11 satisfies each of the following learning guarantees : @xmath219 where @xmath220 , @xmath221 , and @xmath222 is the vector with all its components equal to @xmath223 .",
    "observe that when all weights are all equal to @xmath224 , the result we obtain is similar to the learning guarantee obtained in theorem  [ th : generalization ] when the rademacher complexity of @xmath225 is @xmath226 . also , if the learning scenario is i.i.d .",
    ", then the first sum of the bound vanishes and it can be seen straightforwardly that to minimize the rhs of the inequality we need to set @xmath227 , which results in the known i.i.d.guarantees for on - line to batch conversion @xcite .    since @xmath35 is convex with respect to its first argument , by jensen s inequality , we have @xmath228 .",
    "thus , by lemma  [ lemma:13 ] , for any @xmath123 , the following holds with probability at least @xmath75 : @xmath229 this proves the first statement of the theorem . to prove the second claim",
    ", we will bound the empirical error in terms of the regret . for any @xmath230",
    ", we can write using @xmath231 : @xmath232 \\!+\\ ! \\frac{1}{t } \\!\\sum_{t = 1}^t [ l(h_t(x_t ) , y_t ) \\!-\\ !",
    "l(h^*(x_t ) , y_t ) ] \\\\    & \\leq m\\| { { { \\mathbf w}}}- { { { \\mathbf u}}}_0 \\|_1 + \\frac{1}{t}\\sum_{t = 1}^t    l(h_t(x_t),y_t)-\\inf_h \\frac{1}{t } \\sum_{t = 1}^t l(h(x_t ) , y_t)\\\\   & \\leq m\\| { { { \\mathbf w}}}- { { { \\mathbf u}}}_0 \\|_1+\\frac{r_t}{t}.\\end{aligned}\\ ] ] now , by definition of the infimum , for any @xmath107 , there exists @xmath233 such that @xmath234 . for that choice of @xmath235 , in view of , with probability at least @xmath236 , the following holds : @xmath237 by the second statement of lemma  [ lemma:13 ] , for any @xmath123 , with probability at least @xmath236 , @xmath238 combining these last two inequalities , by the union bound , with probability at least @xmath75 , the following holds with @xmath239 : @xmath240 the last inequality holds for all @xmath107 , therefore also for @xmath241 by taking the limit .",
    "the results of the previous section suggest a natural algorithm based on the values of the discrepancy between distributions .",
    "let @xmath242 be the sequence of hypotheses generated by an on - line algorithm .",
    "theorem  [ th : onlinetobatch ] provides a learning guarantee for any convex combination of these hypotheses . the convex combination based on the weight vector @xmath243 minimizing the bound of theorem  [ th : onlinetobatch ] benefits from the most favorable guarantee .",
    "this leads to an algorithm for determining @xmath243 based on the following convex optimization problem : @xmath244 \\text{subject to : } & \\quad \\big ( \\sum_{t = 1}^t w_t = 1 \\big ) \\wedge ( \\forall t \\in [ 1 , t ] , w_t \\geq 0),\\nonumber\\end{aligned}\\ ] ] where @xmath245 is a regularization parameter .",
    "this is a standard qp problem that can be efficiently solved using a variety of techniques and available software .    in practice , the discrepancy values @xmath246 are not available since they require labeled samples .",
    "but , in the deterministic scenario where the labeling function @xmath38 is in @xmath34 , we have @xmath247 .",
    "thus , the discrepancy values @xmath248 can be used instead in our learning bounds and in the optimization .",
    "this also holds approximately when @xmath38 is not in @xmath34 but is close to some @xmath76 .    as shown in @xcite ,",
    "given two ( unlabeled ) samples of size @xmath249 from @xmath29 and @xmath69 , the discrepancy @xmath127 can be estimated within @xmath250 , when @xmath251 . in many realistic settings , for tasks such as spam filtering ,",
    "the distribution @xmath29 does not change within a day .",
    "this gives us the opportunity to collect an independent _ unlabeled _ sample of size @xmath249 from each distribution @xmath29 .",
    "if we choose @xmath252 , by the union bound , with high probability , all of our estimated discrepancies will be within @xmath44 of their exact counterparts @xmath248 .",
    "additionally , in many cases , the distributions @xmath29 remain unchanged over some longer periods ( cycles ) which may be known to us .",
    "this in fact typically holds for some tasks such as spam filtering , political sentiment analysis , some financial market prediction problems , and other problems . for example , in the absence of any major political event such as a debate , speech , or a prominent measure , we can expect the political sentiment to remain stable . in such scenarios , it should be even easier to collect an unlabeled sample from each distribution . more crucially , we do not need then to estimate the discrepancy for all @xmath30 $ ] but only once for each cycle .      here , we report the results of preliminary experiments demonstrating the performance of our algorithm .",
    "we tested our algorithm on synthetic data in a regression setting .",
    "the testing and training data were created as follows : instances were sampled from a two - dimensional gaussian random variables @xmath253 .",
    "the objective function at each time was given by @xmath254 .",
    "the weight vectors @xmath255 and mean vectors @xmath256 were selected as follows : @xmath257 and @xmath258 , where @xmath259 is the uniform random variable over @xmath260 ^ 2 $ ] and @xmath261 a rotation of magnitude @xmath262 distributed uniformly over @xmath263 .",
    "we used the widrow - hoff algorithm @xcite as our base on - line algorithm to determine @xmath189 .",
    "after receiving @xmath20 examples , we tested our final hypothesis on @xmath264 points taken from the same gaussian distribution @xmath265 .",
    "we ran the experiment @xmath266 times for different amounts of sample points and took the average performance of our classifier .",
    "for these experiments , we are considering the ideal situation where the discrepancy values are given .",
    "comparison of the performance of three algorithms as a function of the sample size @xmath20 .",
    "weighted stands for the algorithm described in this paper , regular for an algorithm that averages over all the hypotheses , and fixed for the algorithm that averages only over the last 100 hypotheses . ]",
    "we compared the performance of our algorithm with that of the algorithm that ( uniformly ) averages all of the hypotheses and with that of the algorithm that averages only the last 100 hypotheses generated by the perceptron algorithm .",
    "figure  [ fig : performance ] shows the results of our experiments in the first setting .",
    "observe that the error increases with the sample size . while the analysis of section  [ sec : pac ] could provide an explanation of this phenomenon in the case of the uniform averaging algorithm , in principle",
    ", it does not explain why the error also increases in the case of our algorithm .",
    "the answer to this can be found in the setting of the experiment .",
    "notice that the gaussians considered are moving their center and that the squared loss grows proportional to the radius of the smallest sphere containing the sample .",
    "thus , as the number of points increases , so does the maximum value of the loss function in the test set .",
    "nevertheless , our algorithm still outperforms the other two algorithms .",
    "it is worth noting that the accuracy of our algorithm can drastically change of course depending on the choice of the online algorithm used .",
    "we presented a theoretical analysis of the problem of learning with drifting distributions in the batch setting .",
    "our learning guarantees improve upon previous ones based on the @xmath0 distance , in some cases substantially , and our proofs are simpler and concise .",
    "these bounds benefit from the notion of discrepancy which seems to be the natural measure of the divergence between distributions in a drifting scenario .",
    "this work motivates a number of related studies , in particular a discrepancy - based analysis of the scenario introduced by @xcite and further improvements of the algorithm we presented , in particular by exploiting the specific on - line learning algorithm used .",
    "we thank yishay mansour for discussions about the topic of this paper ."
  ],
  "abstract_text": [
    "<S> we present a new analysis of the problem of learning with drifting distributions in the batch setting using the notion of discrepancy . </S>",
    "<S> we prove learning bounds based on the rademacher complexity of the hypothesis set and the discrepancy of distributions both for a drifting pac scenario and a tracking scenario . </S>",
    "<S> our bounds are always tighter and in some cases substantially improve upon previous ones based on the @xmath0 distance . </S>",
    "<S> we also present a generalization of the standard on - line to batch conversion to the drifting scenario in terms of the discrepancy and arbitrary convex combinations of hypotheses . </S>",
    "<S> we introduce a new algorithm exploiting these learning guarantees , which we show can be formulated as a simple qp . finally , we report the results of preliminary experiments demonstrating the benefits of this algorithm .    </S>",
    "<S> * keywords : * drifting environment , generalization bound , domain adaptation . </S>"
  ]
}