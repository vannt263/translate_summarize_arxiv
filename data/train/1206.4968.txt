{
  "article_text": [
    "evolution strategies ( ess ) are stochastic search algorithms for numerical optimization . in ess ,",
    "candidate solutions are sampled using a gaussian distribution parametrized by a mean vector and a covariance matrix . in state - of - the art",
    "ess , those parameters are iteratively adapted using the ranking of the candidate solutions w.r.t . the objective function",
    ". consequently , ess are invariant to applying a monotonic transformation to the objective function .",
    "adaptive es algorithms are successfully applied in practice and there is ample empirical evidence that they converge linearly towards a local optimum of the objective function on a wide class of functions . however , their theoretical analysis even on simple functions is difficult as the state of the algorithm is given by both the mean vector and the covariance matrix that have a stochastic dynamic that needs to be simultaneously controlled .",
    "their linear convergence to local optima is so far only proven for functions that are composite of a monotonic transformation with a convex quadratic function  hence function with a single optimum  for rather simple search algorithms compared to the covariance matrix adaptation evolution strategy ( cma - es ) that is considered as the state - of - the - art es @xcite . in this paper , instead of analyzing the exact stochastic dynamic of the algorithms , we consider the deterministic time continuous model underlying adaptive ess that follows from the information - geometric optimization ( igo ) setting recently introduced @xcite .",
    "the information - geometric optimization is a unified framework for randomized search algorithms . given a family of probability distributions parametrized by @xmath1 , the original objective function , @xmath2 ,",
    "is transformed to a fitness function @xmath3 defined on @xmath4 .",
    "the igo algorithm defined on @xmath4 performs a natural gradient ascent aiming at maximizing @xmath3 . for the family of gaussian distributions",
    ", the igo algorithm recovers the pure rank-@xmath0 update cma - es @xcite , for the family of bernoulli distributions , pbil @xcite is recovered .",
    "when the step - size for the gradient ascent algorithm ( that corresponds to a learning rate in cma - es and pbil ) goes to zero , we obtain an ordinary differential equation ( ode ) in @xmath5 .",
    "the set of solutions of this ode , the igo - flow , consists of continuous time models of the recovered algorithms in the limit of the population size going to infinity and the step - size ( learning rate for es or pbil ) to zero .    in this paper",
    "we analyze the convergence of the igo - flow for isotropic ess where the family of distributions is gaussian with covariance matrix equal to an overall variance times the identity .",
    "the underlying algorithms are step - size adaptive ess that resemble ess with derandomized adaptation @xcite and encompass xnes @xcite and the pure rank-@xmath0 update cma - es with only one variance parameter @xcite .",
    "previous works have proposed and analyzed continuous models of ess that are solutions of odes @xcite using the machinery of stochastic approximation @xcite .",
    "the ode variable in these studies encodes solely the mean vector of the search distribution and the overall variance is taken to be proportional to @xmath6 where @xmath7 is a smooth function with @xmath8 .",
    "consequently the model analyzed looses invariance to monotonic transformation of the objective function and scale - invariance , both being fundamental properties of virtually all ess .",
    "the technique relies on the lyapunov function approach and assumes the stability of critical points of the ode @xcite . in this paper ,",
    "our approach also relies on the stability of the critical points of the ode that we analyze by means of lyapunov functions .",
    "however one difficulty stems from the fact that when convergence occurs , the variance typically converges to zero which is at the boundary of the definition domain @xmath4 . to circumvent this difficulty we extend the standard lyapunov method to be able to study stability of boundary points .    applying the extended lyapunov s method to the igo - flow in the manifold of isotropic gaussian distributions , we derive a sufficient condition on the so - called weight function @xmath9parameter of the algorithm and usually chosen by the algorithm designer  so that the igo - flow converges to the global minimum independently of the starting point on objective functions that are composite of a monotonic function with a convex quadratic function .",
    "we will call those functions _",
    "monotonic convex - quadratic - composite _ in the sequel .",
    "we then extend this result to functions that are the composition of a monotonic transformation and a twice continuously differentiable function , called monotonic @xmath10-composite in the rest of the paper .",
    "we prove local convergence to a local optimum of the function in the sense that starting close enough from a local optimum , with a small enough variance , the igo - flow converges to this local optimum .",
    "the rest of the paper is organized as follows . in section  [ sec : esigo ]",
    "we introduce the igo - flow for the family of isotropic gaussian distributions , which we call _ es - igo_-flow . in section  [ sec : elst ] we extend the standard lyapunov s method for proving stability . in section  [ sec : conv ] we apply the extended method to the es - igo - flow and provide convergence results of the es - igo - flow on monotonic convex - quadratic - composite functions and on monotonic @xmath10-composite functions .",
    "[ [ notation . ] ] * notation . * + + + + + + + + + + +    for @xmath11 , where @xmath12 is a topological space , we let @xmath13 denote the complement of @xmath14 in @xmath12 , @xmath15 the interior of @xmath14 , @xmath16 the closure of @xmath14 , @xmath17 the boundary of @xmath14 .",
    "let @xmath18 and @xmath19 be the sets of real numbers and @xmath20-dimensional real vectors , @xmath21 and @xmath22 denote the sets of non - negative and positive real numbers , respectively",
    ". let @xmath23 represent the euclidean norm of @xmath24 .",
    "the open and closed balls in @xmath19 centered at @xmath5 with radius @xmath25 are denoted by @xmath26 and @xmath27 .",
    "let @xmath28 denote the lebesgue measure on either @xmath18 or @xmath19 .",
    "let @xmath29 and @xmath30 be the probability measures induced by the one - variate and @xmath20-variate standard normal distributions , @xmath31 and @xmath32 the probability density function induced by @xmath29 and @xmath30 w.r.t .",
    "let @xmath33 and @xmath34 represent the probability density function w.r.t .",
    "@xmath28 and the probability measure induced by the gaussian distribution @xmath35 parameterized by @xmath1 , where the mean vector @xmath36 is in @xmath19 and the covariance matrix @xmath37 is a positive definite symmetric matrix of dimension @xmath20 .",
    "we sometimes abbreviate @xmath38 and @xmath39 to @xmath40 and @xmath41 .",
    "let @xmath42 denote the vectorization operator such that @xmath43^{\\mathrm{t}}$ ] , where @xmath44 is the @xmath45-th element of @xmath46 .",
    "we use both notations : @xmath47^{\\mathrm{t}}$ ] and @xmath48 .",
    "the igo framework for continuous optimization with the family of gaussian distributions is as follows .",
    "the original objective is to minimize an objective function @xmath49 .",
    "this objective function is mapped into a function on @xmath4 .",
    "hereunder , we suppose that @xmath2 is @xmath28-measurable .",
    "let @xmath50 \\to { \\mathbb{r}}$ ] be a bounded , non - increasing weight function .",
    "we define the weighted quantile function @xcite as @xmath51 \\bigr)\\enspace.\\ ] ] the function @xmath52 is a preference weight for @xmath53 according to the @xmath34-quantile .",
    "the fitness value of @xmath54 given @xmath5 is defined as the expectation of the preference @xmath55 over @xmath56 , @xmath57 $ ] .",
    "note that since @xmath52 depends on @xmath5 so does @xmath58 . the function @xmath3 is defined on a statistical manifold @xmath59 equipped with the fisher metric @xmath60 as a riemannian metric .",
    "the fisher metric is the natural metric .",
    "it is compatible with relative entropy and with kl - divergence and is the only metric that does not depend on the chosen parametrization . using log - likelihood trick and exchanging the order of differentiation and integration ,",
    "the `` vanilla '' gradient of @xmath3 at @xmath61 can be expressed as @xmath62 $ ] .",
    "the natural gradient , that is , the gradient taken w.r.t .",
    "the fisher metric , is given by the product of the inverse of the fisher information matrix @xmath63 at @xmath5 and the vanilla gradient , namely @xmath64 .",
    "the igo ordinary differential equation is defined as @xmath65 since the right - hand side ( rhs ) of the above ode is independent of @xmath66 the igo ode is autonomous . the igo - flow is the set of solution trajectories of the above ode .    when the parameter @xmath5 encodes the mean vector and the covariance matrix of the gaussian distribution in the following way @xmath47^{\\mathrm{t}}$ ] , the product of the inverse of the fisher information matrix @xmath67 and the gradient of the log - likelihood @xmath68 can be written in an explicit form @xcite and reduces to @xmath69 the pure rank-@xmath0 update cma - es @xcite can be considered as an euler scheme for solving with a monte - carlo approximation of the integral .",
    "let @xmath70 be samples independently generated from @xmath34 .",
    "then , the quantile @xmath71 $ ] in is approximated by the number of solutions better than @xmath72 divided by @xmath73 , i.e. , @xmath74 .",
    "then @xmath75 is approximated by @xmath76 , where @xmath9 is the given weight function .",
    "the euler scheme for approximating the solutions of where the integral is approximated by monte - carlo leads to @xmath77 } } { }     \\ifthenelse{\\equal{display}{color}}{\\color{{\\color{yellow!70!black } [ ] } } } { } } } } } { { {     \\ifthenelse{\\equal{display}{display}}{{\\color{red!20!yellow}[rmed : \" \\frac{{w}(r_{i } / n)}{n } \" [ ] ] } } { }     \\ifthenelse{\\equal{display}{color}}{\\color{{\\color{red!20!yellow}[rmed : \" \\frac{{w}(r_{i } / n)}{n } \" [ ] ] } } } { } } } } \\begin{bmatrix } x_{i } - { m}^{t}\\\\ \\operatorname{vec}\\bigl((x_{i } - { m}^{t})(x_{i } - { m}^{t})^{\\mathrm{t}}- { c}^{t } \\bigr ) \\end{bmatrix } \\enspace , \\label{eq : cma - igo - alg}\\ ] ] where @xmath78 is the time discretization step - size .",
    "this equation is equivalent to the pure rank-@xmath0 update cma - es when the learning rates @xmath79 and @xmath80 , for the update of @xmath81 and @xmath82 respectively , are set to the same value @xmath78 , while they have different values in practice ( @xmath83 and @xmath84 ) .",
    "the summation on the rhs in converges to the rhs of with probability one as @xmath85 ( theorem  4 in @xcite ) .    in the following ,",
    "we study the simplified igo - flow where the covariance matrix is parameterized by only a single variance parameter @xmath86 as @xmath87 . under the parameterization @xmath88^{\\mathrm{t}}$ ] , reduces to @xmath89 { p_{\\theta}}({\\mathrm{d}x})$ ] . using the change of variable @xmath90 , the above ode reads@xmath91 and we rewrite it by part @xmath92",
    "the domain of this ode is @xmath93 .",
    "we call the _ es - igo _ ordinary differential equation .",
    "the following proposition shows that for a lipschitz continuous weight function @xmath9 , solutions of the ode exist for any initial condition @xmath94 and are unique .",
    "[ prop : uni ] suppose @xmath9 is lipschitz continuous . then the initial value problem : @xmath95 , @xmath96 , has a unique solution on @xmath97 for each @xmath98 , i.e.  there is only one solution @xmath99 to the initial value problem .",
    "we can obtain a lower bound @xmath100 and an upper bound @xmath101 for @xmath102 for each @xmath103 under a bounded @xmath9 .",
    "similarly , we can have an upper bound @xmath104 for @xmath105 .",
    "then we have that @xmath106 and @xmath107 is compact for each @xmath103 .",
    "meanwhile , @xmath108 is locally lipschitz continuous for a lipschitz continuous @xmath9 .",
    "since @xmath107 is compact , the restriction of @xmath108 into @xmath107 is lipschitz continuous . applying theorem  3.2 in @xcite that is an extension of the theorem known as picard - lindelf theorem or cauchy - lipschitz theorem",
    ", we have the existence and uniqueness of the solution on each bounded interval @xmath109 $ ] . since @xmath66 is arbitrary",
    ", we have the proposition .",
    "now that we know that solutions of the es - igo ode exist and are unique , we define the es - igo - flow as the mapping @xmath110 , which maps @xmath111 to the solution @xmath112 of with initial condition @xmath96 .",
    "note that we can extend the domain of @xmath108 from @xmath113 to @xmath114 .",
    "it is easy to see from that the value of @xmath115 at @xmath116 is @xmath117 for any @xmath118 .",
    "however , we exclude the boundary @xmath119 from the domain for reasons that will become clear in the next section . because the initial variance must be positive and the variance starting from positive region never reach the boundary in finite time , solutions @xmath120 will stay in the domain @xmath4",
    ". however , as we will see , they can converge asymptotically towards points of the boundary .     since @xmath3 is _ adaptive _ , i.e.  @xmath121 for @xmath122 in general , it is not trivial to determine whether the solutions to converge to points where @xmath123   is not adaptive and defined to be the expectation of the objective function @xmath124 over @xmath125 , convergence to the zeros of the rhs of is easily obtained .",
    "for example , see theorem  12 and its proof in @xcite , where the solution to the system of a similar ode whose rhs is the vanilla gradient of the expected objective function is derived and the convergence of the solution trajectory to the critical point of the expected function is proven . ] . even know",
    "ing that they converge to zeros of @xmath115 is not helpful at all , because we have @xmath123 for any @xmath5 with variance zero and we are actually interested in convergence to the point @xmath126 where @xmath127 is a local optimum of @xmath2 .",
    "[ rem : inv - param ] because of the invariance property of the natural gradient , the mean vector @xmath36 and the variance @xmath128 obey and under re - parameterization of the gaussian distributions .",
    "therefore , the trajectories of @xmath129 and @xmath86 are also independent of the parameterization . for instance , we obtain the same trajectories @xmath128 for any of the following parameterizations : @xmath130 , @xmath131 , and @xmath132 , although the trajectories of the parameters @xmath133 are of course different .",
    "consequently , the same convergence results for @xmath36 and @xmath128 ( see section  [ sec : conv ] ) will hold under any parameterization",
    ". parameterizations @xmath134 and @xmath135 correspond to the pure rank-@xmath0 update cma - es and the xnes with only one variance parameter .",
    "thus , the continuous model to be analyzed encompasses both algorithms .",
    "theory of stochastic approximation says that a stochastic algorithm @xmath136 follows the solution trajectories of the ode @xmath137 $ ] in the limit for @xmath78 to zero under several conditions . in our setting , @xmath5 encodes @xmath129 and @xmath86 and the noisy observation @xmath138 , where @xmath139 , @xmath140 , are predefined weights and @xmath141 is the ranking of @xmath142 . if we define @xmath143 in , then @xmath144 $ ] and the ode agrees with .",
    "therefore , can be viewed as the limit behavior of adaptive - es algorithms not only in the case @xmath145 and @xmath85 but also in the case @xmath145 and finite @xmath146 .",
    "indeed , it is possible to bound the difference between @xmath147 and the solution @xmath148 of the ode by extending lemma  1 in chapter  9 of @xcite .",
    "the details are omitted due to the space limitation .",
    "is a ( natural ) gradient of a function , the stochastic algorithm is called a stochastic gradient method .",
    "the theory of stochastic gradient method ( e.g. , @xcite ) relates the convergence of the stochastic algorithm with the zeros of @xmath149 .",
    "however , it is not applicable to our algorithm due to the reason mentioned above remark  [ rem : inv - param ] . ]",
    "when convergence occurs , the variance typically converges to zero .",
    "hence the study of the convergence of the solutions of the ode will be carried out by analyzing the stability of the points @xmath150 . however , because points with variance zero are excluded from the domain @xmath4 , we need to extend classical definitions of stability to be able to handle points located on the boundary of @xmath4 .",
    "consider the following system of differential equation @xmath151where @xmath152 is a continuous map and @xmath153 is open",
    ". then @xmath154 is called    * _ _ stable in the sense of lyapunov _ _ be a stable point . if @xmath155 or @xmath156 can be prolonged by continuity at @xmath157 as @xmath158 , then @xmath159 .",
    "that is , @xmath157 is a stationary point .",
    "however , @xmath160 does not always exist for a stable boundary point @xmath161 .",
    "for example , consider the ode : @xmath162 , @xmath163 .",
    "the domain is @xmath164 .",
    "then , @xmath165 and @xmath166 are monotonically decreasing to zero .",
    "hence , @xmath167 is globally asymptotically stable .",
    "however , @xmath168 does not exist . ] if for any @xmath169 there is @xmath170 such that @xmath171 for all @xmath103 , where @xmath172 is any solution of ; * _ locally attractive _ if there is @xmath170 such that @xmath173 for any solution @xmath172 of ; * _ globally attractive _",
    "if @xmath174 for any @xmath175 and any solution @xmath172 of ; * _ locally asymptotically stable _ if it is stable and locally attractive ; * _ globally asymptotically stable _ if it is stable and globally attractive .    we can now understand why we need to exclude points with variance zero from the domain @xmath4 .",
    "indeed , points with variance zero are points from where solutions of the ode will never move because @xmath176 .",
    "consequently , if we include points @xmath177 in @xmath4 , none of these points can be attractive as in a neighborhood we always find @xmath178 such that a solution starting in @xmath179 stays there and can not thus converge to any other point . a standard technique to prove stability is lyapunov s method that consists in finding a scalar function @xmath180 that is positive except for a candidate stable point @xmath157 with @xmath181 , and that is monotonically decreasing along any trajectory of the ode .",
    "such a function is called _ lyapunov function _ ( and is analogous to a potential function in dynamical systems ) .",
    "lyapunov s method does not require the analysis of the solutions of the ode .",
    "the standard lyapunov s stability theorem gives practical conditions to verify that a function is indeed a lyapunov function .",
    "however , because our candidate stable points are located on @xmath119 , we need to extend this standard theorem .",
    "[ lem : lyapunov - boundary ] consider the autonomous system , where @xmath182 is a map and @xmath153 is the open domain of @xmath5 .",
    "let @xmath154 be a candidate stable point .",
    "suppose that there is an @xmath183 such that + ( a1 ) : @xmath184 is continuous on @xmath185 ; + ( a2 ) : there is a continuously differentiable @xmath186 } } { }     \\ifthenelse{\\equal{display}{color}}{\\color{{\\color{yellow!70!black } [ ] } } } { } } } } } } \\to { \\mathbb{r}}$ ] such that for some strictly increasing continuous function @xmath187 satisfying @xmath188 , @xmath189 ( a3 ) : for any @xmath190 and @xmath191 such that @xmath192 , if a solution @xmath148 to starting from @xmath193 stays in @xmath194 for @xmath195 , then there is a @xmath196 and a _ compact _ set @xmath197 such that @xmath198 for @xmath199 .",
    "+ then , @xmath157 is locally asymptotically stable .",
    "if ( a1 ) and ( a2 ) hold with @xmath200 replacing @xmath185 and ( a3 ) holds with @xmath201 , then @xmath157 is globally asymptotically stable .",
    "we follow the proof of theorem  4.1 in @xcite .",
    "we have from assumptions ( a1 ) and ( a2 ) that there is @xmath202 such that @xmath157 is stable and @xmath203 for each @xmath204 .",
    "moreover , under ( a1 ) and ( a2 ) with @xmath200 replacing @xmath185 we have that @xmath203 for each @xmath205 .",
    "since @xmath206 implies @xmath207 by , it is enough to show @xmath208 .",
    "we show @xmath208 by contradiction argument .",
    "assume that @xmath209 .",
    "then , we have that for each @xmath205 ( or @xmath210 for the case of local asymptotic stability ) there are @xmath211 and @xmath212 such that @xmath213 ( @xmath214 ) and @xmath112 lies in @xmath215 for @xmath103 .",
    "note that @xmath194 is not necessarily a compact set .",
    "this is different from theorem  4.1 in @xcite . by assumption ( a3 )",
    "we have that there is a compact set @xmath216 and @xmath196 such that @xmath198 for @xmath217 .",
    "since @xmath218 is continuously differentiable and @xmath156 is continuous , @xmath219 is continuous .",
    "then , the function @xmath220 has its maximum @xmath221 on the compact @xmath216 and @xmath222 by .",
    "this leads to @xmath223 as @xmath224 .",
    "this contradicts the hypothesis that @xmath225 .",
    "hence , @xmath208 for any @xmath205 ( or @xmath210 ) .",
    "in this section we study the convergence properties of the es - igo - flow @xmath226 , where @xmath148 represents the solution to the es - igo ode with initial value @xmath96 , i.e. , @xmath227 and @xmath228 . by the definition of asymptotic stability ,",
    "the global asymptotic stability of @xmath229 implies the global convergence , that is , @xmath230 for all @xmath231 .",
    "moreover , the local asymptotic stability of @xmath229 implies the local convergence , that is , @xmath232 such that @xmath230 for all @xmath233 .",
    "we will prove convergence properties of the es - igo - flow by applying lemma  [ lem : lyapunov - boundary ] . in order to prove our result",
    "we need to make the following assumption on @xmath9 : + ( b1 ) : @xmath9 is non - increasing and lipschitz continuous with @xmath234 ; + ( b2 ) : @xmath235 ) ( z^{2}/d-1/d ) { p_{1}}({\\mathrm{d}z } ) = \\alpha > 0 $ ] .",
    "assumption ( b1 ) is not restrictive .",
    "indeed , the non - increasing and non - constant property of @xmath236 is a natural requirement and any weight setting in can be expressed , for any given population size @xmath73 , as a discretization of some lipschitz continuous weight function .",
    "assumption ( b2 ) is satisfied if and only if the variance @xmath86 diverges exponentially on a linear function .",
    "in fact , @xmath237 defined in reduces to @xmath238 ) ( z^{2}/d-1/d ) { p_{1}}({\\mathrm{d}z})$ ] when @xmath239 for @xmath240 and we have that @xmath241 and the solution is @xmath242 .",
    "then , @xmath243 as @xmath224 . assumption ( b2 ) holds , for example , if @xmath9 is convex and not linear .",
    "let @xmath244 be the set of strictly increasing functions @xmath245 that are @xmath28-measurable and @xmath10 be the set of twice continuously differentiable functions @xmath246 that are @xmath28-measurable . under ( b1 ) and ( b2 ) , we have the following main theorems .",
    "[ thm : quad ] suppose that the objective function @xmath2 is a monotonic convex - quadratic - composite function @xmath247 , where @xmath248 and @xmath249 is a convex quadratic function @xmath250 where @xmath14 is positive definite and symmetric .",
    "assume that ( b1 ) and ( b2 ) hold .",
    "then , @xmath251 is the globally asymptotically stable point of the es - igo .",
    "hence , we have the global convergence of @xmath252 to @xmath157 .",
    "since the es - igo does not explicitly utilize the function values but uses the quantile @xmath253 $ ] which is equivalent to @xmath254 $ ] , without loss of generality we assume @xmath255 .",
    "according to lemma  [ lem : lyapunov - boundary ] , it is enough to show that ( a1 ) and ( a2 ) hold with @xmath256 replacing @xmath185 and ( a3 ) holds with @xmath201 . as is mentioned in the proof of proposition  [ prop : uni ] , @xmath108 is locally lipschitz continuous for a lipschitz continuous @xmath9 .",
    "thus , ( a1 ) is satisfied under ( b1 ) .",
    "we can choose as a lyapunov candidate function @xmath257 .",
    "all the conditions on @xmath218 described in ( a2 ) are obvious except for the negativeness of @xmath258 . to show the negativeness , rewrite @xmath115 as @xmath259 .",
    "the idea is to show the ( strictly ) negative correlation between @xmath260 and @xmath261 by using an extension of the result in ( * ? ? ?",
    "* chapter  1 ) and apply the inequality @xmath262 = 0 .",
    "we use the non - increasing property of @xmath9 with @xmath234 in ( b1 ) to show the negative correlation .    to prove ( a3 ) , we require ( b2 ) .",
    "since a continuously differentiable function can be approximated by a linear function at any non - critical point @xmath263 , the natural gradient @xmath108 is approximated by that on a linear function in a small neighborhood of ( @xmath264 , 0 ) .",
    "we use the property @xmath265 = 0 $ ] to approximate @xmath108 . as is mentioned above , ( b2 ) implies @xmath266 on a linear function is positive . by using the approximation and this property",
    ", we can show that @xmath267 satisfies ( a3 ) for some @xmath268 .",
    "we have that for any initial condition @xmath269 , the search distribution @xmath34 weakly converges to the dirac measure @xmath270 concentrated at the global minimum point @xmath127 .",
    "this result is generalized to monotonic @xmath10-composite functions using a quadratic taylor approximation .",
    "however , global convergence becomes local convergence .",
    "[ thm : general ] suppose that the objective function @xmath2 is a monotonic @xmath10-composite function @xmath247 , where @xmath248 and @xmath271 has the property that @xmath272 = 0 $ ] for any @xmath273 .",
    "assume that ( b1 ) and ( b2 ) hold .",
    "let @xmath127 be a critical point of @xmath249 , i.e.  @xmath274 , with a positive definite hessian matrix @xmath14 .",
    "then , @xmath251 is a locally asymptotically stable point of the es - igo .",
    "hence , we have the local convergence of @xmath252 to @xmath157 . moreover , if @xmath263 is not a critical point of @xmath275 , for any @xmath98 , @xmath276 will never converge to @xmath277 .     as in the proof of theorem  [ thm : quad ]",
    ", we assume @xmath255 without loss of generality .",
    "the proofs of ( a1 ) and ( a3 ) carry over from theorem  [ thm : quad ] because we only use d the property @xmath265 = 0 $ ] . to show ( a2 ) , we use the taylor approximation of the objective function @xmath2 . since f is approximated by a quadratic function in a neighborhood of a critical point @xmath127 , we approximate the natural gradient by the corresponding natural gradient on the quadratic function .",
    "then , employing the same lyapunov candidate function as in the previous theorem we can show ( a2 ) .",
    "because of the approximation , we only have local asymptotic stability .",
    "the last statement of theorem  [ thm : general ] is an immediate consequence of the approximation of the natural gradient and ( b2 ) .",
    "we have that starting from a point close enough to a local minimum point @xmath127 with a sufficiently small initial variance , the search distribution weakly converges to @xmath270 .",
    "it is not guaranteed for the parameter to converge somewhere when the initial mean is not close enough to the local optimum or the initial variance is not small enough .",
    "theorem  [ thm : general ] also states that the convergence @xmath278 does not happen for @xmath263 such that @xmath279 .",
    "that is , the continuous time es - igo does not prematurely converge on a slope of the landscape of @xmath2 .",
    "in this paper we have proven the local convergence of the continuous time model associated to step - size adaptive ess towards local minima on monotonic @xmath10-composite functions . in the case of monotonic convex - quadratic - composite functions",
    "we have proven the global convergence , i.e. convergence independently of the initial condition ( provid ed the initial step - size is strictly positive ) towards the unique minimum .",
    "our analysis relies on investigating the stability of critical points associated to the underlying ode that follows from the information geometric optimization setting .",
    "we use a classical method for the analysis of stability of critical points , based on lyapunov functions .",
    "we have however extended the method to be able to handle convergence towards solutions at the boundary of the ode definition domain .",
    "we believe that our approach is general enough to handle more difficult cases like the cma - es with a more general covariance matrix .",
    "we want to emphasize that the model we have analyzed is the correct model for step - size _ adaptive _ ess as the ode encodes both the mean vector _ and _ step - size and preserves fundamental invariance properties of the algorithm .",
    "this work was partially supported by the anr-2010-cosi-002 grant ( siminole ) of the french national research agency and the anr cosinus project anr-08-cosi-007 - 12 ."
  ],
  "abstract_text": [
    "<S> the _ information - geometric optimization ( igo ) _ has been introduced as a unified framework for stochastic search algorithms . </S>",
    "<S> given a parametrized family of probability distributions on the search space , the igo turns an arbitrary optimization problem on the search space into an optimization problem on the parameter space of the probability distribution family and defines a natural gradient ascent on this space . from the natural gradients defined over the entire parameter space we obtain continuous time trajectories which are the solutions of an ordinary differential equation ( ode ) . via discretization , the igo naturally defines an iterated gradient ascent algorithm . </S>",
    "<S> depending on the chosen distribution family , the igo recovers several known algorithms such as the pure rank-@xmath0 update cma - es . </S>",
    "<S> consequently , the continuous time igo - trajectory can be viewed as an idealization of the original algorithm .    in this paper </S>",
    "<S> we study the continuous time trajectories of the igo given the family of isotropic gaussian distributions . </S>",
    "<S> these trajectories are a deterministic continuous time model of the underlying evolution strategy in the limit for population size to infinity and change rates to zero . on functions that are the composite of a monotone and a convex - quadratic function , we prove the global convergence of the solution of the ode towards the global optimum . </S>",
    "<S> we extend this result to composites of monotone and twice continuously differentiable functions and prove local convergence towards local optima . </S>"
  ]
}