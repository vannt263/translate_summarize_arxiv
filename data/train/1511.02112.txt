{
  "article_text": [
    "concentration inequalities are central in the analysis of adaptive nonparametric statistics .",
    "they lead to sharp penalized criteria for model selection @xcite , to select bandwidths and even approximation kernels for parzen s estimators in high dimension @xcite , to aggregate estimators @xcite and to properly calibrate thresholds @xcite . in the present work ,",
    "we are interested in the selection of a general kernel estimator based on a least - squares density estimation approach .",
    "the problem has been considered in @xmath0-loss by devroye and lugosi @xcite .",
    "other methods combining log - likelihood and roughness / smoothness penalties have also been proposed in @xcite .",
    "however these estimators are usually quite difficult to compute in practice .",
    "we propose here to minimize penalized least - squares criteria and obtain from them more easily computable estimators .",
    "sharp concentration inequalities for u - statistics @xcite control the variance term of the kernel estimators , whose asymptotic behavior has been precisely described , for instance in @xcite .",
    "we derive from these bounds ( see proposition  [ prop : concrisk ] ) a penalization method to select a kernel which satisfies an asymptotically optimal oracle inequality , i.e. with leading constant asymptotically equal to @xmath1 .    in the spirit of @xcite",
    ", we use an extended definition of kernels that allows to deal simultaneously with classical collections of estimators as projection estimators , weighted projection estimators , or parzen s estimators .",
    "this method can be used for example to select an optimal model in model selection ( in accordance with @xcite ) or to select an optimal bandwidth together with an optimal approximation kernel among a finite collection of parzen s estimators . in this sense , our method deals , in particular , with the same problem as that of goldenshluger and lepski @xcite and we establish in this framework that a leading constant 1 in the oracle inequality is indeed possible .    another main consequence of concentration inequalities is to prove the existence of a minimal level of penalty , under which no oracle inequalities can hold .",
    "birg and massart shed light on this phenomenon in a gaussian setting for model selection @xcite .",
    "moreover in this setting , they prove that the optimal penalty is twice the minimal one .",
    "in addition , there is a sharp phase transition in the dimension of the selected models leading to an estimate of the optimal penalty in their case ( which is known up to a multiplicative constant ) . indeed , starting from the idea that in many models the optimal penalty is twice the minimal one ( this is the slope heuristic ) , arlot and massart @xcite propose to detect the minimal penalty by the phase transition and to apply the `` @xmath2 '' rule ( this is the slope algorithm ) .",
    "they prove that this algorithm works at least in some regression settings .    in the present work",
    ", we also show that minimal penalties exist in the density estimation setting .",
    "in particular , we exhibit a sharp `` phase transition '' of the behavior of the selected estimator around this minimal penalty .",
    "the analysis of this last result is not standard however .",
    "first , the `` slope heuristic '' of @xcite only holds in particular cases as the selection of projection estimators , see also @xcite .",
    "as in the selection of a linear estimator in a regression setting @xcite , the heuristic can sometimes be corrected : for example for the selection of a bandwidth when the approximation kernel is fixed . in general",
    "since there is no simple relation between the minimal penalty and the optimal one , the slope algorithm of @xcite shall only be used with care for kernel selection .",
    "surprisingly our work reveals that the minimal penalty can be negative . in this case , minimizing an unpenalized criterion leads to oracle estimators . to our knowledge",
    ", such phenomenon has only been noticed previously in a very particular classification setting @xcite .",
    "we illustrate all of these different behaviors by means of a simulation study .    in section  [ sec : not ] , after fixing the main notation , providing some examples and defining the framework , we explain our goal , describe what we mean by an _ oracle inequality _ and state the exponential inequalities that we shall need .",
    "then we derive optimal penalties in section  [ sec : opt ] and study the problem of minimal penalties in section  [ sec : minpen ] .",
    "all of these results are illustrated for our three main examples : projection kernels , approximation kernels and weighted projection kernels . in section  [ sec : simu ] , some simulations are performed in the approximation kernel case .",
    "the main proofs are detailed in section  [ sec : proofs ] and technical results are discussed in the appendix .",
    "let @xmath3 denote i.i.d .",
    "random variables taking values in the measurable space @xmath4 , with common distribution @xmath5 .",
    "assume @xmath5 has density @xmath6 with respect to @xmath7 and @xmath6 is uniformly bounded .",
    "hence , @xmath6 belongs to @xmath8 , where , for any @xmath9 , @xmath10    moreover , @xmath11 and @xmath12 denote respectively the @xmath8-norm and the associated inner product and @xmath13 is the supremum norm .",
    "we systematically use @xmath14 and @xmath15 for @xmath16 and @xmath17 respectively , and denote @xmath18 the cardinality of the set @xmath19 . recall that @xmath20 and , for any @xmath21 , @xmath22 .",
    "let @xmath23 denote a collection of symmetric functions @xmath24 indexed by some given finite set @xmath25 such that @xmath26 a function @xmath27 satisfying these assumptions is called a _ kernel _ , in the sequel .",
    "a kernel @xmath27 is associated with an estimator @xmath28 of @xmath6 defined for any @xmath29 by @xmath30    our aim is to select a `` good '' @xmath31 in the family @xmath32 .",
    "our results are expressed in terms of a constant @xmath33 such that for all @xmath34 , @xmath35    this condition plays the same role as @xmath36 , the milder condition used in @xcite when working with @xmath0-losses . before describing the method ,",
    "let us give three examples of such estimators that are used for density estimation , and see how they can naturally be associated to some kernels .",
    "section  [ sec : proofskernels ] of the appendix gives the computations leading to the corresponding @xmath37 s .",
    "[ [ example-1-projection - estimators . ] ] example 1 : projection estimators .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    projection estimators are among the most classical density estimators . given a linear subspace @xmath38 , the projection estimator on @xmath39 is defined by @xmath40 let @xmath41 be a family of linear subspaces @xmath39 of @xmath8 .",
    "for any @xmath42 , let @xmath43 denote an orthonormal basis of @xmath39 .",
    "the projection estimator @xmath44 can be computed and is equal to @xmath45 it is therefore easy to see that it is the estimator associated to the _ projection kernel _",
    "@xmath46 defined for any @xmath47 and @xmath48 in @xmath49 by @xmath50    notice that @xmath46 actually depends on the basis @xmath43 even if @xmath44 does not . in the sequel , we always assume that some orthonormal basis @xmath43 is given with @xmath39 . given a finite collection @xmath51 of linear subspaces of @xmath8",
    ", one can choose the following constant @xmath37 in for the collection @xmath52 @xmath53    [ [ example-2-parzens - estimators . ] ] example 2 : parzen s estimators .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given a bounded symmetric integrable function @xmath54 such that @xmath55 , @xmath56 and a bandwidth @xmath57 , the parzen estimator is defined by @xmath58    it can also naturally be seen as a kernel estimator , associated to the function @xmath59 defined for any @xmath47 and @xmath48 in @xmath60 by @xmath61    we shall call the function @xmath59 an approximation or parzen kernel .",
    "+ given a finite collection of pairs @xmath62 , one can choose @xmath63 in if , @xmath64    [ [ example-3-weighted - projection - estimators . ] ] example 3 : weighted projection estimators .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    let @xmath65 denote an orthonormal system in @xmath8 and let @xmath66 denote real numbers in @xmath67 $ ] .",
    "the associated weighted kernel projection estimator of @xmath6 is defined by @xmath68 these estimators are used to derive very sharp adaptive results . in particular , pinsker s estimators",
    "are weighted kernel projection estimators ( see for example @xcite ) .",
    "when @xmath69 , we recover a classical projection estimator .",
    "a weighted projection estimator is associated to the _ weighted projection kernel _ defined for any @xmath47 and @xmath48 in @xmath49 by @xmath70 given any finite collection @xmath71 of weights , one can choose in @xmath72      the goal is to estimate @xmath6 in the best possible way using a finite collection of kernel estimators @xmath73 .",
    "in other words , the purpose is to select among @xmath73 an estimator @xmath74 from the data such that @xmath75 is as close as possible to @xmath76 .",
    "more precisely our aim is to select @xmath77 such that , with high probability , @xmath78 where @xmath79 is the leading constant and @xmath80 is usually a remaining term . in this case , @xmath81 is said to satisfy an _ oracle inequality _ , as long as @xmath82 is small compared to @xmath76 and @xmath83 is a bounded sequence .",
    "this means that the selected estimator does as well as the best estimator in the family up to some multiplicative constant . the best case one can expect is to get @xmath83 close to 1",
    "this is why , when @xmath84 , the corresponding oracle inequality is called _ asymptotically optimal_. to do so , we study minimizers of _ penalized least - squares criteria_. note that in our three examples choosing @xmath85 amounts to choosing the smoothing parameter , that is respectively to choosing @xmath86 , @xmath87 or @xmath88 .",
    "let @xmath89 denote the empirical measure , that is , for any real valued function @xmath90 , @xmath91    for any @xmath92 , let also @xmath93 + the _ least - squares contrast _ is defined , for any @xmath92 , by @xmath94 then for any given function @xmath95 , the _ least - squares penalized criterion _ is defined by @xmath96 finally the selected @xmath97 is given by any minimizer of @xmath98 , that is , @xmath99    as @xmath100 , it is equivalent to minimize @xmath101 or @xmath102 . as our goal is to select @xmath74 satisfying an oracle inequality , an ideal penalty @xmath103 should satisfy @xmath104 , i.e. criterion with @xmath105 to identify the main quantities of interest ,",
    "let us introduce some notation and develop @xmath106 .",
    "for all @xmath34 , let @xmath107}},\\qquad \\forall x\\in\\xbb\\enspace,\\end{aligned}\\ ] ] and @xmath108 } } \\right .",
    "\\right)}\\enspace.\\ ] ] because those quantities are fundamental in the sequel , let us also define @xmath109 where for @xmath110 @xmath111    denoting @xmath112 the ideal penalty is then equal to @xmath113    the main point is that by using concentration inequalities , we obtain : @xmath114    the term @xmath115 depends on @xmath6 which is unknown .",
    "fortunately , it can be easily controlled as detailed in the sequel .",
    "therefore one can hope that the choice @xmath116    is convenient . in general , this choice still depends on the unknown density @xmath6 but it can be easily estimated in a data - driven way by @xmath117    the goal of section  [ sec : opt ] is to prove this heuristic and to show that @xmath118 and @xmath119 are optimal choices for the penalty , that is , they lead to an asymptotically optimal oracle inequality .      to derive sharp oracle inequalities , we only need two fundamental concentration tools , namely a weak bernstein s inequality and the concentration bounds for degenerate u - statistics of order two .",
    "we cite them here under their most suitable form for our purpose .",
    "[ [ a - weak - bernsteins - inequality . ] ] a weak bernstein s inequality .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +       [ prop : bern ] for any bounded real valued function @xmath120 and any @xmath121 i.i.d . with distribution @xmath5 , for any @xmath122 ,",
    "@xmath123    the proof is straightforward and can be derived from either bennett s or bernstein s inequality @xcite .",
    "[ [ concentration - of - degenerate - u - statistics - of - order-2 . ] ] concentration of degenerate u - statistics of order 2 .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +       [ prop : concustat ] let @xmath124 be i.i.d .",
    "random variables defined on a polish space @xmath49 equipped with its borel @xmath125-algebra and let @xmath126 denote bounded real valued symmetric measurable functions defined on @xmath127 , such that for any @xmath128 , @xmath129 and @xmath130}}=0\\qquad \\mbox{for\\;a.e.\\;}x \\mbox { in } \\xbb\\enspace.\\ ] ] let @xmath131 be the following totally degenerate @xmath131-statistic of order @xmath132 , @xmath133    let @xmath19 be an upper bound of @xmath134 for any @xmath135 and @xmath136}},\\sup_{j , t\\in\\xbb}\\sum_{i = j+1}^{n}{{\\mathbb{e}}{\\left [ \\left .",
    "f_{i , j}(x_i , t)^2 \\right . \\right ] } } \\right .",
    "\\right)}\\\\   c^2&=\\sum_{1\\le i\\neq j\\le n}{{\\mathbb{e}}{\\left [ \\left .",
    "f_{i , j}(x_i , x_j)^2 \\right . \\right]}}\\\\   d&=\\sup_{(a , b)\\in\\sa}{{\\mathbb{e}}{\\left [ \\left . \\sum_{1\\le",
    "i < j\\le n}f_{i , j}(x_i , x_j)a_i(x_i)b_j(x_j ) \\right . \\right]}}\\enspace,\\end{aligned}\\ ] ]    where @xmath137}}\\le 1,\\;{{\\mathbb{e}}{\\left [ \\left .",
    "\\sum_{j=2}^{n}b_j(x_j)^2 \\right .",
    "\\right]}}\\le 1 \\right .",
    "\\right\\}}.$ ] + then there exists some absolute constant @xmath138 such that for any @xmath122 , with probability larger than @xmath139 , @xmath140    the present result is a simplification of theorem 3.4.8 in @xcite , which provides explicit constants for any variables defined on a polish space .",
    "it is mainly inspired by @xcite , where the result therein has been stated only for real variables .",
    "this inequality actually dates back to gin , latala and zinn @xcite .",
    "this result has been further generalized by adamczak to u - statistics of any order @xcite , though the constants are not explicit .",
    "the main aim of this section is to show that @xmath118 is a theoretical optimal penalty for kernel selection , which means that if @xmath141 is close to @xmath118 , the selected kernel @xmath77 satisfies an asymptotically optimal oracle inequality .      to express our results in a simple form",
    ", a positive constant @xmath142 is assumed to control , for any @xmath27 and @xmath143 in @xmath144 , all the following quantities .",
    "@xmath145}}&\\le & \\upsilon p\\theta_k\\enspace,\\label{eq : assor4}\\\\   \\sup_{x\\in\\xbb}~{{\\mathbb{e}}{\\left [ \\left .",
    "a_k(x , x)^2 \\right .",
    "\\right]}}&\\le & \\upsilon n \\enspace,\\label{eq : assor5}\\\\   v_k^2{:=}\\sup_{t\\in\\boule_k}p t^2&\\le & \\upsilon\\vee \\sqrt{\\upsilon p\\theta_k}\\enspace , \\label{eq : assvar}\\end{aligned}\\ ] ] where @xmath146 is the set of functions @xmath90 that can be written @xmath147 for some @xmath148 with @xmath149 .",
    "these assumptions may seem very intricate .",
    "they are actually fulfilled by our three main examples under very mild conditions ( see section  [ sec : mainexamples ] ) .      in the sequel",
    ", @xmath150 denotes a positive absolute constant whose value may change from line to line and if there are indices such as @xmath151 , it means that this is a positive function of @xmath152 and only @xmath152 whose value may change from line to line .    [ thm : optimalpenalty ] if assumptions  , , , , hold , then , for any @xmath153 , with probability larger than @xmath154 , for any @xmath155 , any minimizer @xmath77 of the penalized criterion satisfies the following inequality @xmath156    assume moreover that there exists @xmath157 , @xmath158 and @xmath159 such that for any @xmath153 , with probability larger than @xmath160 , for any @xmath161 , @xmath162 then for all @xmath163 and all @xmath153 , the following holds with probability at least @xmath164 , @xmath165    let us make some remarks .    *",
    "first , this is an oracle inequality ( see ) with leading constant @xmath83 and remaining term @xmath82 given by @xmath166 as long as * * @xmath152 is small enough for @xmath83 to be positive , * * @xmath47 is large enough for the probability to be large and * * @xmath167 is large enough for @xmath82 to be negligible .",
    "+ typically , @xmath168 and @xmath142 are bounded w.r.t . @xmath167 and @xmath47 has to be of the order of @xmath169 for the remainder to be negligible . in particular",
    ", @xmath144 may grow with @xmath167 as long as ( i ) @xmath170 remains negligible with respect to @xmath167 and ( ii ) @xmath142 does not depend on @xmath167 . *",
    "if @xmath171 , that is if @xmath172 and @xmath173 in , the estimator @xmath81 satisfies an asymptotically optimal oracle inequality i.e. @xmath174 since @xmath152 can be chosen as close to 0 as desired .",
    "take for instance , @xmath175 . *",
    "in general @xmath176 depends on the unknown @xmath6 and this last penalty can not be used in practice .",
    "fortunately , its empirical counterpart @xmath177 satisfies with @xmath178 , @xmath179 , @xmath180 and @xmath181 for any @xmath155 and in particular @xmath175 ( see in proposition  [ prop : concremainder ] ) .",
    "hence , the estimator @xmath81 selected with this choice of penalty also satisfies an asymptotically optimal oracle inequality , by the same argument . * finally , we only get an oracle inequality when @xmath182 , that is when @xmath141 is larger than @xmath183 up to some residual term .",
    "we discuss the necessity of this condition in section  [ sec : minpen ] .",
    "this section shows that theorem  [ thm : optimalpenalty ] can be applied in the examples .",
    "in addition , it provides the computation of @xmath118 in some specific cases of special interest .",
    "[ [ example-1-continued . ] ] example 1 ( continued ) .",
    "+ + + + + + + + + + + + + + + + + + + + + +     +    [ prop : assprojkern ] let @xmath184 be a collection of projection kernels .",
    "assumptions  , , , and hold for any @xmath185 , where @xmath37 is given by .",
    "in addition , assumption   is satisfied under either of the following classical assumptions ( see ( * ? ? ? * chapter 7 ) ) : @xmath186 or @xmath187    these particular projection kernels satisfy for all @xmath110 @xmath188 in particular , @xmath189 and @xmath190 .",
    "moreover , it appears that the function @xmath191 is constant in some linear spaces @xmath39 of interest ( see @xcite for more details ) .",
    "let us mention one particular case studied further on in the sequel .",
    "suppose @xmath51 is a collection of regular histogram spaces @xmath39 on @xmath49 , that is , any @xmath192 is a space of piecewise constant functions on a partition @xmath193 of @xmath49 such that @xmath194 for any @xmath195 in @xmath193 .",
    "assumption   is satisfied for this collection as soon as @xmath196 .",
    "the family @xmath197 , where @xmath198 is an orthonormal basis of @xmath39 and @xmath199    hence , @xmath200 and @xmath201 can actually be used as a penalty to ensure that the selected estimator satisfies an asymptotically optimal oracle inequality . moreover , in this example it is actually necessary to choose a penalty larger than @xmath202 to get an oracle inequality ( see @xcite or section  [ sec : minpen ] for more details ) .",
    "[ [ example-2-continued . ] ] example 2 ( continued ) .",
    "+ + + + + + + + + + + + + + + + + + + + + +     +    [ prop : assapproxkern ] let @xmath203 be a collection of approximation kernels .",
    "assumptions  , , , , and hold with @xmath63 , for any @xmath204 as soon as is satisfied .",
    "these approximation kernels satisfy , for all @xmath205 , @xmath206 therefore , the optimal penalty @xmath207 can be computed in practice and yields an asymptotically optimal selection criterion",
    ". surprisingly , the lower bound @xmath208 can be negative if @xmath209 . in this case , a minimizer of satisfies an  oracle inequality , even if this criterion is not penalized .",
    "this remarkable fact is illustrated in the simulation study in section  [ sec : simu ] .",
    "[ [ example-3-continued . ] ] example 3 ( continued ) .",
    "+ + + + + + + + + + + + + + + + + + + + + +     +    [ prop : assweiprojkern ] let @xmath210 be a collection of weighted projection kernels .",
    "assumption   is valid for @xmath185 , where @xmath37 is given by .",
    "moreover and imply , , , and .",
    "for these weighted projection kernels , for all @xmath211 @xmath212 in this case , the optimal penalty @xmath213 has to be estimated in general . however , in the following example it can still be directly computed . +",
    "let @xmath214 $ ] , let @xmath7 be the lebesgue measure .",
    "let @xmath215 and , for any @xmath216 , @xmath217 consider some odd @xmath218 and a family of weights @xmath219 such that , for any @xmath220 and any @xmath221 @xmath222 . in this case , the values of the functions of interest do not depend on @xmath47 @xmath223 in particular , this family includes pinsker s and tikhonov s weights",
    "the purpose of this section is to see whether the lower bound @xmath224 is sharp in theorem  [ thm : optimalpenalty ] .",
    "to do so we first need the following result which links @xmath225 to deterministic quantities , thanks to concentration tools .",
    "[ prop : concrisk ] assume @xmath23 is a finite collection of kernels satisfying assumptions  , , , and . for all @xmath226 , for all @xmath227 in @xmath228 $ ] , with probability larger than @xmath229",
    "@xmath230 @xmath231    moreover , for all @xmath226 and for all @xmath227 in @xmath232 , with probability larger than @xmath229 , for all @xmath34 , each of the following inequalities hold @xmath233 @xmath234    this means that not only in expectation but also with high probability can the term @xmath235 be decomposed in a bias term @xmath236 and a `` variance '' term @xmath237 . the bias term measures the capacity of the kernel @xmath27 to approximate @xmath6 whereas @xmath237 is the price to pay for replacing @xmath238 by its empirical version @xmath28 . in this sense , @xmath237 measures the complexity of the kernel @xmath27 in a way which is completely adapted to our problem of density estimation . even if it does not seem like a natural measure of complexity at first glance , note that in the previous examples , it is indeed always linked to a natural complexity . when dealing with regular histograms defined on @xmath67 $ ]",
    ", @xmath239 is the dimension of the considered space @xmath39 , whereas for approximation kernels @xmath240 is proportional to the inverse of the considered bandwidth @xmath241 .      in this section ,",
    "we assume that we are in the asymptotic regime where the number of observations @xmath242 .",
    "in particular , the asymptotic notations refers to this regime .    from now on ,",
    "the family @xmath243 may depend on @xmath167 as long as both @xmath37 and @xmath142 remain absolute constants that do not depend on it .",
    "indeed , on the previous examples , this seems a reasonable regime .",
    "since @xmath244 now depends on @xmath167 , our selected @xmath245 also depends on @xmath167 .    to prove that the lower bound @xmath246 is sharp , we need to show that the estimator chosen by minimizing with a penalty smaller than @xmath247 does not satisfy an oracle inequality",
    "this is only possible if the @xmath235 s are not of the same order and if they are larger than the remaining term @xmath248 . from an asymptotic point of view , we rewrite this thanks to proposition  [ prop : concrisk ] as for all @xmath249 , there exist @xmath250 and @xmath251 in @xmath244 such that @xmath252 where @xmath253 means that @xmath254 . more explicitly , denoting by @xmath255 a sequence only depending on @xmath167 and tending to 0 as @xmath167 tends to infinity and whose value may change from line to line , one assumes that there exists @xmath256 and @xmath257 positive constants such that for all @xmath249 , there exist @xmath250 and @xmath251 in @xmath244 such that @xmath258    we put a log - cube factor in the remaining term to allow some choices of @xmath259 and @xmath260 .    but and ( or ) are not sufficient .",
    "indeed , the following result explains what happens when the bias terms are always the leading terms .",
    "[ cor : biaisdomine ] let @xmath261 be a sequence of finite collections of kernels @xmath27 satisfying assumptions  , , , , for a positive constant @xmath142 independent of @xmath167 and such that @xmath262    for some positive constant @xmath263 .",
    "assume that there exist real numbers of any sign @xmath264 and a sequence @xmath265 of nonnegative real numbers such that , for all @xmath249 , with probability larger than @xmath266 , for all @xmath267 , @xmath268    then , with probability larger than @xmath269 , @xmath270    the proof easily follows by taking @xmath175 in , @xmath271 for instance in proposition  [ prop : concrisk ] and by using assumption   and the bounds on @xmath141 .",
    "this result shows that the estimator @xmath272 satisfies an asymptotically optimal oracle inequality when condition holds , whatever the values of @xmath273 and @xmath274 even when they are negative .",
    "this proves that the lower bound @xmath247 is not sharp in this case .",
    "therefore , we have to assume that at least one bias @xmath235 is negligible with respect to @xmath237 . actually , to conclude",
    ", we assume that this happens for @xmath251 in .",
    "[ thm : minpen ] let @xmath261 be a sequence of finite collections of kernels satisfying assumptions  , , , , , with @xmath142 not depending on @xmath167 . each @xmath244 is also assumed to satisfy and with a kernel @xmath275 in such that @xmath276    for some fixed positive constant @xmath277 .",
    "suppose that there exist @xmath278 and a sequence @xmath265 of nonnegative real numbers such that @xmath279 and such that for all @xmath249 , with probability larger than @xmath280 , for all @xmath267 , @xmath281    then , with probability larger than @xmath269 , the following holds @xmath282 @xmath283    by , under the conditions of theorem  [ thm : minpen ] , the estimator @xmath272 can not satisfy an oracle inequality , hence , the lower bound @xmath183 in theorem  [ thm : optimalpenalty ] is sharp .",
    "this shows that @xmath183 is a minimal penalty in the sense of @xcite for kernel selection .",
    "when @xmath284    the complexity @xmath285 presents a sharp phase transition when @xmath286 becomes positive . indeed , when @xmath287 it follows from that the complexity @xmath285 is asymptotically larger than @xmath288 .",
    "but on the other hand , as a consequence of theorem  [ thm : optimalpenalty ] , when @xmath138 , this complexity becomes smaller than @xmath289      [ [ example-1-continued.-1 ] ] example 1 ( continued ) .",
    "+ + + + + + + + + + + + + + + + + + + + + +    let @xmath290 be the collection of spaces of regular histograms on @xmath67 $ ] with dimensions @xmath291 and let @xmath292 be the selected space thanks to the penalized criterion .",
    "recall that , for any @xmath293 , the orthonormal basis is defined by @xmath198 and @xmath294 .",
    "assume that @xmath6 is @xmath295-hlderian , with @xmath296 $ ] with @xmath295-hlderian norm @xmath297 .",
    "it is well known ( see for instance section 1.3.3 . of @xcite ) that the bias is bounded above by @xmath298    in particular , if @xmath299 , @xmath300 thus , holds for kernel @xmath301 .",
    "moreover , if @xmath302 , @xmath303 hence , holds with @xmath304 and @xmath305 .",
    "therefore , theorem  [ thm : minpen ] and theorem  [ thm : optimalpenalty ] apply in this example . if @xmath306 , the dimension @xmath307 and @xmath308 is not consistent and does not satisfy an oracle inequality . on the other hand , if @xmath309 , @xmath310 and @xmath308 satisfies an oracle inequality which implies that , with probability larger than @xmath269 , @xmath311 by taking @xmath312 it achieves the minimax rate of convergence over the class of @xmath295-hlderian functions .    from theorem  [ thm : optimalpenalty ]",
    ", the penalty @xmath313 provides an estimator @xmath308 that achieves an asymptotically optimal oracle inequality .",
    "therefore the optimal penalty is equal to @xmath132 times the minimal one . in particular ,",
    "the slope heuristics of @xcite holds in this example , as already noticed in @xcite .    finally to illustrate corollary  [ cor : biaisdomine ] ,",
    "let us take @xmath314 and the collection of regular histograms with dimension in @xmath315 , with @xmath316 .",
    "simple calculations show that @xmath317 hence applies and the penalized estimator with penalty @xmath318 always satisfies an oracle inequality even if @xmath319 or @xmath320 .",
    "this was actually expected since it is likely to choose the largest dimension which is also the oracle choice in this case .",
    "[ [ example-2-continued.-1 ] ] example 2 ( continued ) .",
    "+ + + + + + + + + + + + + + + + + + + + + +    let @xmath321 be a fixed function , let @xmath322 denote the following grid of bandwidths @xmath323 and let @xmath324 be the selected bandwidth .",
    "assume as before that @xmath6 is a density on @xmath67 $ ] that belongs to the nikolski class @xmath325 with @xmath296 $ ] and @xmath326 .",
    "by proposition  1.5 in @xcite , if @xmath321 satisfies @xmath327 @xmath328    in particular , when @xmath329 , @xmath330    on the other hand , for @xmath331 , @xmath332    hence , and hold with kernels @xmath333 and @xmath334 .",
    "therefore , theorem  [ thm : minpen ] and theorem  [ thm : optimalpenalty ] apply in this example .",
    "if for some @xmath182 we set @xmath335 , then @xmath336 and @xmath337 is not consistent and does not satisfy an oracle inequality . on the other hand , if @xmath338 , then @xmath339    and @xmath340 satisfies an oracle inequality which implies that , with probability larger than @xmath269 , @xmath341    for @xmath342 in particular it achieves the minimax rate of convergence over the class @xmath325 .",
    "finally , if @xmath343 , @xmath337 achieves an asymptotically optimal oracle inequality , thanks to theorem  [ thm : optimalpenalty ] .",
    "the minimal penalty is therefore @xmath344    in this case , the optimal penalty @xmath345 derived from theorem  [ thm : optimalpenalty ] is not twice the minimal one , but one still has , if @xmath346 , @xmath347    even if they can be of opposite sign depending on @xmath321 .",
    "this type of nontrivial relationship between optimal and minimal penalty has already been underlined in @xcite in regression framework for selecting linear estimators .",
    "note that if one allows two kernel functions @xmath348 and @xmath349 in the family of kernels such that @xmath350 , @xmath351 and @xmath352 then there is no absolute constant multiplicative factor linking the minimal penalty and the optimal one .",
    "in this section we illustrate on simulated data theorem  [ thm : optimalpenalty ] and theorem  [ thm : minpen ] .",
    "we focus on approximation kernels only , since projection kernels have been already discussed in @xcite .",
    "we observe an @xmath353 i.i.d .",
    "sample of standard gaussian distribution . for a fixed parameter @xmath354",
    "we consider the family of kernels @xmath355    where for @xmath356 + in particular the kernel estimator with @xmath357 is the classical gaussian kernel estimator .",
    "moreover @xmath358    thus ,",
    "depending on the value of @xmath359 , the minimal penalty @xmath360 may be negative .",
    "we study the behavior of the penalized criterion @xmath361    with penalties of the form @xmath362 for different values of @xmath286 ( @xmath363 ) and @xmath359 ( @xmath364 ) . on figure",
    "[ fig : optimal ] are represented the selected estimates by the optimal penalty @xmath365 for the different values of @xmath359 and on figure  [ fig : contraste ] one sees the evolution of the different penalized criteria as a function of @xmath366 .    [ cols=\"^ \" , ]     finally figure  [ fig : saut ] shows that there is indeed in all cases a sharp phase transition around @xmath367 i.e. at the minimal penalty for the complexity of the selected estimate .",
    "the starting point to prove the oracle inequality is to notice that any minimizer @xmath77 of @xmath368 satisfies @xmath369 using the expression of the ideal penalty we find @xmath370    by proposition  [ prop : concremainder ] ( see the appendix ) , for all @xmath226 , for all @xmath152 in @xmath232 , with probability larger than @xmath371 , @xmath372    hence @xmath373 } + 2\\theta{\\left [ \\left .",
    "{ \\left \\lvert \\bayes-\\bayes_k \\right\\rvert}^2+\\frac{p\\theta_k}{n } \\right . \\right ] } +   \\square \\frac{\\upsilon x^2}{\\theta n}\\enspace.\\end{aligned}\\ ] ]    this bound holds using , and only . now by proposition  [ prop : concrisk ] applied with @xmath374 , we have for all @xmath226 , for all @xmath375 , with probability larger than @xmath376 , @xmath377    this gives the first part of the theorem .    for the second part , by the condition   on the penalty , we find for all @xmath226 , for all @xmath152 in @xmath232 , with probability larger than @xmath378 , @xmath379 by proposition  [ prop : concrisk ] applied with @xmath380",
    ", we have with probability larger than @xmath381 , @xmath382    that is @xmath383 hence , because @xmath384\\leq   ( \\delta'\\vee 1)+(4+\\delta')\\theta$ ] , we obtain the desired result .      first , let us denote for all @xmath211 @xmath385 } } , \\qquad \\zeta_k(x){:=}\\int{\\left ( \\left .",
    "k(y , x)-\\bayes_k(y ) \\right . \\right)}^2d\\mu(y)\\enspace,\\ ] ]    and @xmath386 } } \\right . \\right)}\\enspace.\\ ] ]    some easy computations then provide the following useful equality @xmath387    we need only treat the terms on the right - hand side , thanks to the probability tools of section  [ sec : tools ] . applying proposition  [ prop : bern ] , we get , for any @xmath153 , with probability larger than @xmath388 , @xmath389 one can then check the following link between @xmath390 and @xmath391 @xmath392 next , by and @xmath393 } } \\right .",
    "\\right)}^2d\\mu(x)\\\\ & \\le 4\\sup_{y\\in\\xbb}\\int k(y , x)^2d\\mu(x)\\le 4\\upsilon n\\enspace . \\end{aligned}\\ ] ] in particular , since @xmath394 , @xmath395 it follows from these computations and from that there exists an absolute constant @xmath150 such that , for any @xmath153 , with probability larger than @xmath388 , for any @xmath155 , @xmath396    we now need to control the term @xmath397 . from proposition",
    "[ prop : concustat ] , for any @xmath153 , with probability larger than @xmath398 , @xmath399 by , and cauchy - schwarz inequality , @xmath400 in addition , by , @xmath401}}\\le 16\\upsilon n\\enspace.$ ] + moreover , applying the assumption  , @xmath402}}=n^2{{\\mathbb{e}}{\\left [ \\left .",
    "a_k(x , y)^2 \\right .",
    "\\right]}}\\le n^2 \\upsilon p\\theta_k\\enspace.\\ ] ] finally , applying the cauchy - schwarz inequality and proceeding as for @xmath403 , the quantity used to define @xmath404 can be bounded above as follows : @xmath405}}\\le n\\sqrt{{{\\mathbb{e}}{\\left [ \\left .",
    "a_k(x , y)^2 \\right .",
    "\\right]}}}\\le n\\sqrt{\\upsilon p\\theta_k}\\enspace.\\ ] ] hence for any @xmath153 , with probability larger than @xmath398 , @xmath406 therefore , for all @xmath375 , @xmath407    and the first part of the result follows by choosing @xmath408 .",
    "concerning the two remaining inequalities appearing in the proposition , we begin by developing the loss . for all @xmath34 @xmath409 then , for all @xmath211 @xmath410    moreover , since @xmath411 , we find @xmath412 } } -{\\left \\lvert \\bayes_k \\right\\rvert}^2\\\\ & = \\frac{1}{n}\\sum_{i=1}^n \\int { \\left ( \\left .",
    "k(x , x_i){\\left ( \\left .",
    "\\bayes_k(x)-\\bayes(x ) \\right .",
    "\\right ) } \\right .",
    "\\right)}d\\mu(x)+p(\\bayes_k - f_{a , k})\\\\ & = \\frac{1}{n}\\sum_{i=1}^n { \\left ( \\left .",
    "f_{a , k}(x_i)-\\bayes_k(x_i ) \\right .",
    "\\right)}+p(\\bayes_k - f_{a , k})\\\\ & = ( p_n - p)(f_{a , k}-\\bayes_k)\\enspace.\\end{aligned}\\ ] ]    this expression motivates us to apply again proposition  [ prop : bern ] to this term .",
    "we find by , and cauchy - schwarz inequality @xmath413    moreover , @xmath414    thus by , for any @xmath415 , @xmath416    hence , for any @xmath155 and @xmath153 , taking @xmath417 @xmath418 by proposition  [ prop : bern ] , for all @xmath152 in @xmath232 , for all @xmath419 with probability larger than @xmath420 , @xmath421    putting together all of the above , one concludes that for all @xmath152 in @xmath232 , for all @xmath226 , with probability larger than @xmath422 @xmath423 and @xmath424 choosing , @xmath425 leads to the second part of the result .",
    "it follows from ( applied with @xmath426 and @xmath427 ) and assumption   that with probability larger than @xmath428 we have for any @xmath34 and any @xmath429 @xmath430    applying this inequality with @xmath431 and using proposition  [ prop : concrisk ] with @xmath432 and @xmath433 as a lower bound for @xmath434 and as an upper bound for @xmath435 , we obtain asymptotically that with probability larger than @xmath428 , @xmath436    by assumption  , @xmath437 and by , @xmath438    this gives .",
    "in addition , starting with the event where holds and using proposition  [ prop : concrisk ] , we also have with probability larger than @xmath428 , @xmath439    since @xmath440 , this leads to @xmath441    this leads to by .",
    "we have to show for each family @xmath23 ( see and ) that there exists a constant @xmath33 such that for all @xmath34 @xmath442    [ [ example-1-projection - kernels . ] ] example 1 : projection kernels .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    first , notice that from cauchy - schwarz inequality we have for all @xmath443 @xmath444 and by orthonormality , for any @xmath445 , @xmath446    in particular , for any @xmath29 , @xmath447 .",
    "hence , projection kernels satisfy for @xmath448 .",
    "we conclude by writing @xmath449 for @xmath450 we have @xmath451 . hence with @xmath452 , @xmath453    [ [ example-2-approximation - kernels . ] ] example 2 : approximation kernels .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    @xmath454first , @xmath455 second , since @xmath456 @xmath457    now @xmath456 and @xmath458 implies @xmath459 , hence holds with @xmath63 if one assumes that @xmath460 .",
    "[ [ example-3-weighted - projection - kernels . ] ] example 3 : weighted projection kernels .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for all @xmath211 @xmath461    from cauchy - schwarz inequality , for any @xmath443 , @xmath462 we thus find that @xmath463 verifies with @xmath464 .",
    "since @xmath465 we find the announced result which is independent of @xmath466 .      since @xmath467 , we find that only requires @xmath468 .",
    "assumption   holds : this follows from @xmath469 and @xmath470}}\\le { \\left \\lvert \\chi_{k_s } \\right\\rvert}_\\infty p\\chi_{k_s } \\le \\gamma n p\\theta_{k_s } \\enspace.\\ ] ] now for proving assumption  , we write @xmath471}}&={{\\mathbb{e}}{\\left [ \\left .",
    "k_s(x , y)^2 \\right . \\right]}}=\\int_\\xbb{{\\mathbb{e}}{\\left [ \\left .",
    "k_s(x , x)^2 \\right . \\right]}}\\bayes(x)d\\mu(x)\\\\   & \\le { \\left \\lvert \\bayes \\right\\rvert}_\\infty\\sum_{(i , j)\\in\\si^2_s}{{\\mathbb{e}}{\\left [ \\left .",
    "\\vphi_i(x)\\vphi_j(x ) \\right .",
    "\\right]}}\\int_{\\xbb}\\vphi_i(x)\\vphi_j(x)d\\mu(x)\\\\   & = { \\left \\lvert \\bayes \\right\\rvert}_\\infty p\\theta_{k_s}\\le \\upsilon   p\\theta_{k_s } \\enspace.\\end{aligned}\\ ] ] in the same way , assumption   follows from @xmath472 .",
    "suppose holds with @xmath473 so that the basis @xmath474 of @xmath475 is included in the one @xmath476 of @xmath39 .",
    "since @xmath477 we have @xmath478 hence , holds in this case .",
    "assuming implies that holds since @xmath479 finally for , for any @xmath148 , @xmath480 is the orthogonal projection of @xmath359 onto @xmath39 .",
    "therefore , @xmath481 is the unit ball in @xmath39 for the @xmath8-norm and , for any @xmath482 , @xmath483}}\\leq { \\left \\lvert \\bayes \\right\\rvert}_\\infty{\\left \\lvert t \\right\\rvert}^2\\leq { \\left \\lvert \\bayes \\right\\rvert}_\\infty\\enspace.$ ]      first , since @xmath459 @xmath484 hence , assumption   holds if @xmath485 .",
    "now , we have @xmath486 so it is sufficient to have @xmath487 ( since @xmath488 ) to ensure .",
    "moreover , for any @xmath489 and any @xmath211 , @xmath490 therefore , assumption   holds for @xmath491 . then on one hand @xmath492 and on the other hand @xmath493}}&\\le \\frac1{h}\\int_{\\xbb^2}{\\left\\lvert k{\\left ( \\left .",
    "\\frac{x - y}{h}-u \\right .",
    "\\right)}k{\\left ( \\left .",
    "\\right ) } \\right\\rvert}du~\\bayes(y)dy\\\\ & = \\int_{\\xbb^2}{\\left\\lvert k{\\left ( \\left .",
    "\\right)}k{\\left ( \\left .",
    "\\right ) } \\right\\rvert}\\bayes(x+h(v - u))du dv\\le",
    "{ \\left \\lvert",
    "\\bayes \\right\\rvert}_\\infty{\\left \\lvert k \\right\\rvert}_1 ^ 2 \\enspace.\\end{aligned}\\ ] ] therefore , @xmath494}}\\le \\sup_{(x , y)\\in\\xbb^2}{\\left\\lvert a_{k_{k , h}}(x , y ) \\right\\rvert}~\\sup_{x\\in\\xbb}~{{\\mathbb{e}}{\\left [ \\left . { \\left\\lvert a_{k_{k , h}}(x , x ) \\right\\rvert } \\right .",
    "\\right]}}\\\\ \\le { \\left ( \\left .",
    "p\\theta_{k_{k , h}}\\wedge n \\right .",
    "\\right)}{\\left \\lvert \\bayes \\right\\rvert}_\\infty{\\left \\lvert k \\right\\rvert}_1 ^ 2\\enspace,\\end{gathered}\\ ] ] and @xmath495}}\\le \\sup_{x\\in\\xbb}~{{\\mathbb{e}}{\\left [ \\left .",
    "a_{k_{k , h}}(x , x)^2 \\right .",
    "\\right]}}\\le { \\left \\lvert \\bayes \\right\\rvert}_\\infty{\\left \\lvert k",
    "\\right\\rvert}_1 ^ 2p\\theta_{k_{k , h } } \\enspace .",
    "$ ] hence assumption   and hold when @xmath496 .",
    "finally let us prove that assumption   is satisfied .",
    "let @xmath497 and @xmath148 be such that @xmath498 and @xmath499 for all @xmath500",
    ". then the following follows from cauchy - schwarz inequality @xmath501 thus for any @xmath502 @xmath503 we conclude that all the assumptions hold if @xmath504      let us define for convenience @xmath505 , so @xmath506 .",
    "then we have for these kernels : @xmath507 for all @xmath29 .",
    "moreover , denoting by @xmath508 the orthogonal projection of @xmath6 onto the linear span of @xmath65 , @xmath509 assumption   holds for this family if @xmath510 .",
    "we prove in what follows that all the remaining assumptions are valid using only and .",
    "+ first , it follows from cauchy - schwarz inequality that , for any @xmath29 , @xmath511 .",
    "assumption   is then automatically satisfied from the definition of @xmath37 @xmath512}}\\le { \\left \\lvert   \\phi \\right\\rvert}_\\infty p\\theta_{k_w } \\le \\gamma n p\\theta_{k_w } \\enspace.\\ ] ] now let @xmath513 and @xmath514 be any two vectors in @xmath67^p$ ] , we have @xmath515 hence @xmath516 and , by cauchy - schwarz inequality , for any @xmath211 , @xmath517 assumption   follows using .",
    "concerning assumptions   and , let us first notice that by orthonormality , for any @xmath445 , @xmath518    therefore , assumption   holds since @xmath519}}&=\\int_\\xbb{\\left ( \\left .",
    "\\sum_{i=1}^pw_i^2\\vphi_i(y)\\vphi_i(x ) \\right .",
    "\\right)}^2\\bayes(y)d\\mu(y)\\\\ & \\le { \\left \\lvert \\bayes \\right\\rvert}_\\infty \\sum_{1\\le i , j\\le p}w_i^2w_j^2\\vphi_i(x)\\vphi_j(x)\\int_{\\xbb}\\vphi_i(y)\\vphi_j(y)d\\mu(y)\\\\ & = { \\left \\lvert \\bayes \\right\\rvert}_\\infty \\sum_{i=1}^p w_i^4\\vphi_i(x)^2\\le { \\left \\lvert \\bayes \\right\\rvert}_\\infty \\phi(x)\\le { \\left \\lvert \\bayes \\right\\rvert}_\\infty \\gamma",
    "n \\enspace.\\end{aligned}\\ ] ]    assumption   also holds from similar computations : @xmath520}}&=\\int_\\xbb{{\\mathbb{e}}{\\left [ \\left .",
    "{ \\left ( \\left .",
    "\\sum_{i=1}^pw_i^2\\vphi_i(x)\\vphi_i(x ) \\right .",
    ". \\right]}}\\bayes(x)d\\mu(x)\\\\   & \\le { \\left \\lvert \\bayes \\right\\rvert}_\\infty\\sum_{1\\le i , j\\le p}w_i^2w_j^2{{\\mathbb{e}}{\\left [ \\left .",
    "\\vphi_i(x)\\vphi_j(x ) \\right .",
    "\\right]}}\\int_{\\xbb}\\vphi_i(x)\\vphi_j(x)d\\mu(x)\\\\   & \\le { \\left \\lvert \\bayes \\right\\rvert}_\\infty p\\theta_{k_w}\\enspace.\\end{aligned}\\ ] ]    we finish with the proof of .",
    "let us prove that @xmath521 , where @xmath522    first , notice that any @xmath523 can be written @xmath524    then , consider some @xmath525 . by definition",
    ", there exists a collection @xmath526 such that @xmath527 , and @xmath528 . if @xmath529 , @xmath530 and @xmath531 , hence @xmath532 .",
    "conversely , for @xmath532 , there exists some function @xmath148 such that @xmath533 , and @xmath534 .",
    "since @xmath65 is an orthonormal system , one can take @xmath535 . with @xmath536 ,",
    "we find @xmath537 and @xmath525 . for any @xmath538 , @xmath539 . hence @xmath540",
    "the following proposition gathers the concentration bounds of the remaining terms appearing in .    [",
    "prop : concremainder ] let @xmath23 denote a finite collection of kernels satisfying and suppose that assumptions  , and hold .",
    "then @xmath541 for any @xmath153 , with probability larger than @xmath542 , for any @xmath543 , for any @xmath155 , @xmath544 for any @xmath153 , with probability larger than @xmath388 , for any @xmath34 , @xmath545 for any @xmath153 , with probability larger than @xmath398 , for any @xmath34 , @xmath546    _ proof _ first for , notice that , by , for any @xmath155 @xmath547 then , by proposition  [ prop : bern ] , with probability larger than @xmath548 , @xmath549 since by @xmath550 @xmath551 moreover , by @xmath552 hence , for @xmath153 , with probability larger than @xmath548 @xmath553      concerning , we get by , @xmath557 , hence , for any @xmath153 we have with probability larger than @xmath554 @xmath558 for , we apply proposition  [ prop : concustat ] to obtain with probability larger than @xmath559 , for any @xmath34 , @xmath560 where @xmath561 are defined accordingly to proposition  [ prop : concustat ] .",
    "let us evaluate all these terms .",
    "first , @xmath562 by and .",
    "next , @xmath563}}\\le",
    "\\square n^2{\\left \\lvert \\bayes \\right\\rvert}_\\infty p\\theta_k\\le \\square n^2\\upsilon p\\theta_k\\enspace .",
    "$ ]      by , we consequently have @xmath565 . finally , using cauchy - schwarz inequality and proceeding as for @xmath403 , @xmath566}}\\le n\\sqrt{{{\\mathbb{e}}{\\left [ \\left .",
    "k(x , y)^2 \\right .",
    "\\right]}}}\\le n\\sqrt{\\upsilon p\\theta_k}\\enspace.\\ ] ] hence , @xmath567 which gives .",
    "e.  gin , r.  lataa , and j.  zinn . exponential and moment inequalities for @xmath131-statistics . in _ high dimensional probability , ii _ ,",
    "volume  47 of _ progr .",
    "_ , pages 1338 .",
    "birkhuser boston , 2000 .        c.  houdr and p.  reynaud - bouret .",
    "exponential inequalities , with constants , for u - statistics of order two . in _ stochastic inequalities and applications _ , volume  56 of _ progr .",
    "_ , pages 5569 .",
    "birkhuser , basel , 2003 ."
  ],
  "abstract_text": [
    "<S> we provide new general kernel selection rules thanks to penalized least - squares criteria . </S>",
    "<S> we derive optimal oracle inequalities using adequate concentration tools . </S>",
    "<S> we also investigate the problem of minimal penalty as described in @xcite . </S>"
  ]
}