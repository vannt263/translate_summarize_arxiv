{
  "article_text": [
    "inverse problems arise from many practical applications whenever one searches for unknown causes based on observation of their effects .",
    "a characteristic property of inverse problems is their ill - posedness in the sense that their solutions do not depend continuously on the data . due to errors in the measurements , in practical applications",
    "one never has the exact data ; instead only noisy data are available .",
    "therefore , how to use the noisy data to produce a stable approximate solution is an important topic .",
    "we are interested in solving nonlinear inverse problems in banach spaces which can be formulated as the nonlinear operator equation @xmath1 where @xmath2 is a nonlinear operator between two banach spaces @xmath3 and @xmath4 with domain @xmath5 .",
    "we will use the same notation @xmath6 to denote the norms of @xmath3 and @xmath4 which should be clear from the context .",
    "let @xmath7 be the only available approximate data to @xmath8 satisfying @xmath9 with a given small noise level @xmath10 . due to the ill - posedness",
    ", regularization methods should be employed to produce from @xmath7 a stable approximate solution .",
    "when both @xmath3 and @xmath4 are hilbert spaces and @xmath11 is frchet differentiable , a lot of regularization methods have been developed during the last two decades , see @xcite and the references therein . the iteratively regularized gauss ",
    "newton method is one of the well known methods and it takes the form ( @xcite ) @xmath12 where @xmath13 denotes the frchet derivative of @xmath11 at @xmath14 , @xmath15 denotes the adjoint of @xmath13 , @xmath16 is an initial guess , and @xmath17 is a sequence of positive numbers satisfying @xmath18 for some constant @xmath19 . when terminated by the discrepancy principle , the regularization property of the iteratively regularized gauss ",
    "newton method has been studied extensively , see @xcite and references therein .",
    "it is worthwhile to point out that @xmath20 is the unique minimizer of the quadratic functional @xmath21    regularization methods in hilbert spaces can produce good results when the sought solution is smooth .",
    "however , because such methods have a tendency to over - smooth solutions , they may not produce good results in applications where the sought solution has special features such as sparsity or discontinuities . in order to capture the special features , the methods in hilbert spaces",
    "must be modified by incorporating the information of some adapted penalty functionals such as the @xmath0 and the total variation ( tv ) like functionals , for which the theories in hilbert space setting are no longer applicable . on the other hand , due to their intrinsic features ,",
    "many inverse problems are more natural to formulate in banach spaces than in hilbert spaces .",
    "therefore , it is necessary to develop regularization methods to solve inverse problems in the framework of banach spaces with general penalty function .    in this paper",
    "we will extend the iteratively regularized gauss ",
    "newton method to the banach space setting .",
    "motivated by the variational formulation ( [ 4.1.1 ] ) in hilbert spaces , it is natural to use convex optimization problems to define the iterates . to this end , we take a proper , lower semi - continuous , convex function @xmath22 $ ] whose sub - differential is denoted as @xmath23 . by picking an initial guess @xmath24 and @xmath25 ,",
    "we define @xmath26 where @xmath27 , @xmath28 , and @xmath29 denotes the bregman distance induced by @xmath30 at @xmath31 in the direction @xmath32 .",
    "when @xmath33 and @xmath34 , this method has been considered in @xcite under essentially the nonlinearity condition @xmath35 with the iteration terminated by an a priori stopping rule .",
    "it turns out that ( [ 4.5.2 ] ) is difficult to verify for nonlinear inverse problems , and the restriction of @xmath30 to the special choice may prevent the method from capturing the special features of solutions .",
    "moreover , since a priori stopping rules depend crucially on the unknown source conditions , it is useless in practical applications . in this paper",
    "we will develop a convergence theory on the iteratively regularized gauss  newton method in banach spaces with general convex penalty function @xmath30 .",
    "we will propose some a posteriori stopping rules , including the discrepancy principle , to terminate the method and give detailed convergence analysis under reasonable nonlinearity conditions .",
    "this paper is organized as follows . in section [ sect2 ]",
    "we give some preliminary facts on convex analysis . in section [ sect3 ]",
    "we then formulate the iteratively regularized gauss ",
    "newton method in banach spaces and propose some a posteriori stopping rules .",
    "we show that the method is well - defined and obtain a weak convergence result . in section [ sect4 ]",
    "we derive the rates of convergence when the solution satisfies certain source conditions formulated as variational inequalities . in section [ sect5 ]",
    "we prove a strong convergence result without assuming any source conditions when @xmath4 is a hilbert spaces and @xmath30 is a @xmath36-convex function , which is useful for sparsity reconstruction and discontinuity detection .",
    "finally , in section [ sect6 ] we present some numerical experiments to test our method for parameter identification in partial differential equations .",
    "let @xmath3 be a banach space with norm @xmath37 .",
    "we use @xmath38 to denote its dual space .",
    "given @xmath39 and @xmath40 we write @xmath41 for the duality pairing . if @xmath4 is another banach space and @xmath42 is a bounded linear operator , we use @xmath43 to denote its adjoint , i.e. @xmath44 for any @xmath39 and @xmath45 .    let @xmath46 $ ] be a convex function .",
    "we use @xmath47 to denote its effective domain .",
    "we call @xmath30 proper if @xmath48 .",
    "given @xmath39 we define @xmath49 which is called the subgradient of @xmath30 at @xmath14 .",
    "it is clear that @xmath50 is convex and closed in @xmath38 for each @xmath39 .",
    "the multi - valued mapping @xmath51 is called the subdifferential of @xmath30 .",
    "it could happen that @xmath52 for some @xmath53 .",
    "we set @xmath54 for @xmath55 and @xmath56 we define @xmath57 which is called the bregman distance induced by @xmath30 at @xmath14 in the direction @xmath58 . clearly @xmath59 . by direct calculation",
    "we can see that @xmath60 for all @xmath61 , @xmath56 , and @xmath62 .",
    "a proper function @xmath22 $ ] is said to be @xmath63-convex for some @xmath64 if there is a constant @xmath65 such that for all @xmath66 and @xmath67 there holds @xmath68 it can be shown that @xmath30 is @xmath63-convex if and only if there is a constant @xmath69 such that @xmath70^{\\frac{1}{p}}\\ ] ] for all @xmath71 , @xmath72 and @xmath56 .",
    "for a proper , lower semi - continuous , convex function @xmath46 $ ] we can define its fenchel conjugate @xmath73 it is well known that @xmath74 is also proper , lower semi - continuous , and convex . if , in addition , @xmath3 is reflexive , then @xmath56 if and only if @xmath75 .",
    "when @xmath30 is @xmath63-convex satisfying ( [ pconv ] ) with @xmath64 , it follows from ( * ? ? ?",
    "* corollary 3.5.11 ) that @xmath76 , @xmath74 is frchet differentiable and its gradient @xmath77 satisfies @xmath78    many examples of @xmath63-convex functions can be provided by functions of the norms in @xmath63-convex banach spaces .",
    "we say a banach space @xmath3 is @xmath63-convex with @xmath64 if there is a positive constant @xmath79 such that @xmath80 for all @xmath81 , where @xmath82 is the modulus of convexity of @xmath3 .",
    "according to a characterization of uniform convexity of banach spaces in @xcite , it is easy to see that , for any @xmath83 , the functional @xmath84 is @xmath63-convex and its subgradient at @xmath14 is given by @xmath85 , where @xmath86 denotes the duality mapping of @xmath3 with gauge function @xmath87 which is defined for each @xmath39 by @xmath88 the sequence spaces @xmath89 , the lebesgue spaces @xmath90 , the sobolev spaces @xmath91 and the besov spaces @xmath92 with @xmath93 are the most commonly used function spaces that are @xmath94-convex ( @xcite ) .    given a proper , lower semi - continuous , @xmath63-convex function @xmath30 on @xmath3 , we can produce such new functions @xmath95 by adding any available proper , lower semi - continuous , convex functions @xmath96 to @xmath30 . in this way",
    ", we can construct non - smooth @xmath63-convex functions that can be used to detect special features of solutions when solving inverse problems .",
    "for instance , let @xmath97 , where @xmath98 is a bounded domain in @xmath99 .",
    "it is clear that the functional @xmath100 is @xmath36-convex on @xmath101 . by adding the function @xmath102 to the multiple of the above function",
    "we can obtain the @xmath36-convex function @xmath103 with small @xmath104 which is useful for sparsity recovery ( @xcite ) .",
    "similarly , we may produce on @xmath101 the @xmath36-convex function @xmath105 where @xmath106 denotes the total variation of @xmath14 over @xmath107 that is defined by ( @xcite ) @xmath108 this functional is useful for detecting the discontinuities , in particular , when the solutions are piecewise - constant ( @xcite ) .",
    "in this section we formulate the iteratively regularized gauss  newton method in the framework of banach spaces to produce a stable approximate solution of ( [ 1.1 ] ) from an available noisy data @xmath7 satisfying ( [ 1.3 ] ) . in order to capture the features of solutions , we take a proper , lower semi - continuous",
    ", @xmath63-convex function @xmath22 $ ] with @xmath64 ; we assume that @xmath30 satisfies ( [ pconv ] ) and @xmath109 .",
    "we will work under the following conditions on the nonlinear operator @xmath11 .",
    "[ a1 ]    1 .   1 .",
    "@xmath110 is a closed convex set in @xmath3 and the equation ( [ 1.1 ] ) has a solution @xmath111 ; 2 .",
    "there is @xmath112 such that for each @xmath113 there is a bounded linear operator @xmath114 such that @xmath115 where @xmath116 ; 3 .   the operator @xmath117 is properly scaled so that @xmath118 ; 4 .",
    "there exist two constants @xmath119 and @xmath120 such that @xmath121 w\\|\\le k_0\\|z - x\\| \\|f'(x ) w\\| + k_1\\|f'(x)(z - x)\\| \\|w\\|\\ ] ] for all @xmath122 and @xmath123 .",
    "it is easy to see that condition ( b ) in assumption [ a1 ] implies , for any @xmath123 , that the function @xmath124 is differentiable and @xmath125 the condition ( d ) was first formulated in @xcite . in section 6",
    "we will present several examples from the parameter identification in partial differential equations to indicate that this condition indeed can be verified for a wide range of applications . as direct consequences of ( b ) and ( d )",
    ", we have for @xmath123 that @xmath126 and @xmath127    in order to formulate the method , let @xmath128 be the characteristic function of @xmath110 and define @xmath129 since @xmath110 is closed and convex , @xmath130 is a proper , lower semi - continuous , convex function on @xmath3 .",
    "consequently , @xmath131 is a proper , lower semi - continuous , @xmath63-convex function on @xmath3 satisfying @xmath132^{\\frac{1}{p } } , \\quad \\forall z\\in \\x , x\\in d(\\p \\theta_f ) \\mbox { and } \\xi\\in \\p \\theta_f(x).\\ ] ] we pick @xmath133 and define @xmath134 , where @xmath135 denotes the fenchel conjugate of @xmath131 and is known to be frchet differentiable with gradient @xmath136 .",
    "we have @xmath137 and @xmath138 .",
    "consequently @xmath139 we use @xmath32 and @xmath31 as initial data .",
    "we then pick a sequence of positive numbers @xmath17 satisfying ( [ 1.5 ] ) and define @xmath140 successively by setting @xmath16 and letting @xmath20 be the unique minimizer of the convex minimization problem @xmath141 by the properties of @xmath131 , @xmath20 is uniquely defined and @xmath142 .    considering the practical applications , the iteration must be terminated by some a posteriori stopping rule to output an integer @xmath143 and hence @xmath144 which is used as an approximate solution of ( [ 1.1 ] ) . in this paper",
    "we will consider the following three stopping rules .",
    "[ rule1 ] let @xmath145 be a given number .",
    "we define @xmath143 to be the integer such that @xmath146    [ rule2 ] let @xmath145 be a given number . if @xmath147 we define @xmath148 ; otherwise we define @xmath149 to be the first integer such that @xmath150    [ rule3 ] let @xmath145 be a given number . if @xmath147 we define @xmath148 ; otherwise we define @xmath151 to be the first integer such that @xmath152    rule [ rule1 ] is known as the discrepancy principle and is widely used to terminate regularization methods .",
    "rule [ rule3 ] appeared first in @xcite to deal with some newton - type regularization methods in hilbert spaces .",
    "it is easy to see that rule [ rule1 ] terminates the iteration no later than rule [ rule2 ] , and rule [ rule2 ] terminates the iteration no later than rule [ rule3 ] .",
    "most of the results in this paper are true for rule [ rule1 ] except the ones in section [ sect4 ] concerning the rates of convergence under certain source conditions formulated as variational inequalities ; the convergence rates , however , can be derived when the iteration is terminated by either rule [ rule2 ] or rule [ rule3 ] .    in this section",
    "we show that the method together with any one of the above three stopping rules with @xmath145 is well - defined . to this end",
    ", we introduce the integer @xmath153 defined by @xmath154 where @xmath155 is the number conjugate to @xmath63 , i.e. @xmath156 , the number @xmath157 is chosen to satisfy @xmath158 and @xmath159 is the unique element that realizes the distance @xmath160 from @xmath32 to the closed convex set @xmath161 in @xmath38 , i.e. @xmath162 because the sequence @xmath17 satisfies ( [ 1.5 ] ) , the integer @xmath153 exists and is finite .",
    "we will show that @xmath163 for all @xmath164 and @xmath165 for the integer @xmath143 defined by any one of the above three stopping rules .",
    "for simplicity of presentation , we use the notation @xmath166 .",
    "we also use @xmath167 to denote a universal constant that is independent of @xmath168 and @xmath169 when its explicit formula is not important .",
    "[ l3.1.1 ] let @xmath3 and @xmath4 be banach spaces , let @xmath170 $ ] be a proper , lower semi - continuous , @xmath63-convex function with @xmath64 , let @xmath17 be a sequence satisfying ( [ 1.5 ] ) , and let @xmath11 satisfy assumption [ a1 ] . if @xmath171 and @xmath172 is sufficiently small , then @xmath173 and @xmath174 for all @xmath164 .",
    "moreover , @xmath165 for the integer @xmath143 defined by either rule [ rule1 ] , [ rule2 ] , or [ rule3 ] with @xmath145 .",
    "since @xmath159 implies @xmath175 , from the definition of @xmath31 and ( [ 3.29.1 ] ) it follows that @xmath176 thus @xmath177 and ( [ est1 ] ) holds . in view of the scaling condition",
    "@xmath178 we can obtain @xmath179 therefore the result holds for @xmath180 .",
    "now we assume that the estimates for @xmath181 have been proved for some @xmath182 and show that the estimates for @xmath20 are also true . by the minimizing property of @xmath20 we have @xmath183 by using the identity ( [ 4.3.1 ] ) we have",
    "@xmath184 therefore , it follows from the above inequality that @xmath185 in view of the young s inequality @xmath186 for @xmath187 , @xmath188 and @xmath189 , we have @xmath190 combining this with ( [ 2.20.1 ] ) and using the @xmath63-convexity of @xmath131 , we can obtain @xmath191 by using the fact that @xmath192 for @xmath187 and @xmath193 , we have from the above inequality that @xmath194 and @xmath195 by using @xmath196 and assumption [ a1 ] we have @xmath197 since @xmath182 , it follows from ( [ nhat ] ) that @xmath198 in view of the induction hypotheses we thus have @xmath199 combining this with ( [ 2.20.2 ] ) gives @xmath200 therefore , if @xmath201 is sufficiently small , then @xmath202 next we estimate @xmath203 . from",
    "( [ 2.20.3 ] ) and ( [ 2.20.4 ] ) it follows that @xmath204 observing that @xmath205 thus , we may use assumption [ a1 ] , ( [ 2.19.2 ] ) , and the estimates on @xmath206 and @xmath207 to derive that @xmath208 therefore , by using the induction hypothesis on @xmath209 , the fact @xmath210 , and ( [ 2.21.1 ] ) , we can obtain for sufficiently small @xmath201 that @xmath211 we therefore obtain the desired estimates ( [ est1 ] ) and ( [ est2 ] ) .",
    "finally we show that @xmath165 .",
    "we first claim that for @xmath164 there holds @xmath212 in fact , for @xmath180 this inequality follows from ( [ 1.3 ] ) and ( [ 9.15.2 ] ) , and for @xmath213 it follows from ( [ 2.20.5 ] ) , ( [ est2 ] ) and ( [ 1.5 ] ) .",
    "therefore , by using assumption [ a1 ] and the estimates ( [ est1 ] ) and ( [ est2 ] ) , we can obtain @xmath214 if @xmath215 , then @xmath216 .",
    "therefore @xmath217 in view of ( [ gamma ] ) we have for sufficiently small @xmath201 that @xmath147 .",
    "consequently @xmath148 .    in the following",
    "we assume that @xmath218 . observing from ( [ 1.5 ] ) and ( [ nhat ] )",
    "that for @xmath219 and @xmath220 there holds @xmath221 thus , from ( [ 9.15.3 ] ) we have for @xmath219 and @xmath220 that @xmath222 since @xmath223 is chosen to satisfy ( [ gamma ] ) , we have for sufficiently small @xmath201 that @xmath224 therefore , by the definition of @xmath143 we have @xmath165 .",
    "@xmath225    [ remark3.1 ] we use @xmath131 in ( [ irgn ] ) to guarantee that @xmath226 without assuming @xmath227 is an interior point of @xmath110 .",
    "if @xmath227 is an interior point of @xmath110 so that @xmath228 for a ball @xmath229 of radius @xmath112 , we can replace @xmath131 in ( [ irgn ] ) by @xmath30 and define @xmath20 to be the unique minimizer of the convex minimization problem @xmath230 the same argument in the proof of lemma [ l3.1.1 ] can be used to show that for sufficiently small @xmath231 there holds @xmath232 for all @xmath164 .",
    "therefore , the modified method is well - defined and all the results in this paper still hold .    as a byproduct of the estimates in lemma [ l3.1.1 ]",
    ", we can prove a weak convergence result of our method .",
    "[ t4.5.1 ] assume that the conditions in lemma [ l3.1.1 ] hold . assume also that @xmath3 is reflexive and @xmath11 is weakly closed .",
    "if the method ( [ irgn ] ) is terminated by either rule [ rule1 ] , [ rule2 ] , or [ rule3 ] with @xmath145 , then for any sequence @xmath233 satisfying @xmath234 with @xmath235 as @xmath236 , @xmath237 has a subsequence that converges weakly in @xmath3 to a solution of ( [ 1.1 ] ) in @xmath238 . if @xmath227 is the unique solution of ( [ 1.1 ] ) in @xmath238",
    ", then @xmath144 converges weakly in @xmath3 to @xmath227 as @xmath239 .",
    "it follows from lemma [ l3.1.1 ] that @xmath240 . since @xmath3 is reflexive",
    ", @xmath237 has a subsequence that converges weakly in @xmath3 to some @xmath241 . by using the weak lower semi - continuity of norms in banach spaces and the convexity and closedness of @xmath110",
    ", we have @xmath242 .",
    "moreover , since @xmath243 , we have @xmath244 as @xmath236 . by the weakly closedness of @xmath11 we have @xmath245 ,",
    "i.e. @xmath246 is a solution of ( [ 1.1 ] ) in @xmath238 .",
    "@xmath225    in theorem [ t4.5.1 ] we only obtain the weak convergence .",
    "the proof of strong convergence remains open in general .",
    "however , in section [ sect5 ] we will prove a strong convergence result when @xmath4 is a hilbert space and @xmath30 is a @xmath36-convex function .",
    "moreover , in some situations we are interested in the strong convergence in a banach space @xmath247 in which @xmath3 can be compactly embedded , the weak convergence in @xmath3 is already enough for the purpose .",
    "in this section we will derive rates of convergence for @xmath144 to @xmath227 under certain source conditions . in hilbert space setting , the usual source conditions are @xmath248 for some @xmath249 and @xmath250 . by the interpolation inequality it is easy to see that ( [ s1 ] ) implies @xmath251 in banach space setting , the formulation ( [ s1 ] ) for source conditions does not make sense in general . however , we may use ( [ s2 ] ) to propose the replacement of the form @xmath252 considering the @xmath63-convexity of @xmath131 , we may further modify this into the form @xmath253^{\\frac{1-\\nu}{p } } \\|t(x - x^\\dag)\\|^\\nu , \\quad \\forall x\\in \\x\\ ] ] for some @xmath249 and @xmath254 .",
    "we therefore obtain source conditions formulated as variational inequalities , whose analog have already been introduced in @xcite",
    ". we will use ( [ source ] ) as our source conditions to derive convergence rates .",
    "[ t3.1.1 ] let @xmath3 and @xmath4 be banach spaces , let @xmath170 $ ] be a proper , lower semi - continuous , @xmath63-convex function for some @xmath64 , let @xmath17 be a sequence satisfying ( [ 1.5 ] ) , and let @xmath11 satisfy assumption [ a1 ] . if the source condition ( [ source ] ) is satisfied with @xmath249 and if @xmath255 is sufficiently small , then for the integer @xmath143 determined by either rule [ rule2 ] or rule [ rule3 ] with @xmath145 there holds @xmath256 and thus @xmath257 where @xmath167 is a constant depending only on @xmath63 , @xmath258 , @xmath259 , @xmath260 and @xmath261 .",
    "we will complete the proof of theorem [ t3.1.1 ] by proving a series of lemmas .",
    "[ l3.7.1 ] under the same conditions in theorem [ t3.1.1 ] , if the source condition ( [ source ] ) holds and @xmath255 is sufficiently small , then there holds @xmath262 for all @xmath164 , where @xmath153 is the integer defined by ( [ nhat ] )",
    ".    we will use ( [ 2.20.1 ] ) . in view of the young s inequality",
    ", it follows from ( [ source ] ) that @xmath263 plugging this into ( [ 2.20.1 ] ) gives @xmath264 this inequality implies immediately that @xmath265 in view of ( [ 3.8.20 ] ) , we can obtain from ( [ 3.7.3 ] ) that @xmath266 with the help of assumption [ a1 ] we then obtain @xmath267 by employing the estimate on @xmath206 from lemma [ l3.1.1 ] , we can obtain from ( [ 3.7.4 ] ) that @xmath268 by using the young s inequality again we can derive that @xmath269 therefore if @xmath201 is sufficiently small , then we can obtain @xmath270 thus , in view of @xmath271 , if we further assume that @xmath201 is sufficiently small , then an induction argument would show that @xmath272 for all @xmath164 if we could show that this is also true for @xmath273 .",
    "observing that @xmath274^{\\frac{1-\\nu}{p } } \\|t e_0\\|^\\nu.\\end{aligned}\\ ] ] this implies that @xmath275 and consequently by the @xmath63-convexity of @xmath30 we have @xmath276 therefore @xmath277 in view of @xmath178 we can obtain @xmath278 we therefore complete the proof .",
    "@xmath225    [ l3.8.2 ] under the same conditions in theorem [ t3.1.1 ] , if @xmath255 is sufficiently small , then there holds @xmath279 for all @xmath164",
    ".    we will use ( [ 3.7.4 ] ) . in view of the estimates on @xmath206 given in lemma [ l3.1.1 ]",
    ", we can obtain from ( [ 3.7.4 ] ) that @xmath280 using the estimates on @xmath209 in lemma [ l3.7.1 ] , the fact @xmath281 and the inequality @xmath282 for @xmath187 and @xmath283 , we have @xmath284 by using the young s inequality we have @xmath285 combining the above two estimates we therefore obtain ( [ 3.8.1 ] ) for @xmath213 .",
    "it remains only to check ( [ 3.8.1 ] ) for @xmath180 . by using @xmath196 and ( [ 3.8.3 ] ) , this is obvious .",
    "@xmath225    [ l8.3.5 ] under the same conditions in theorem [ t3.1.1 ] , there exists a positive universal constant @xmath286 such that @xmath287 for all @xmath288 , where @xmath143 is the integer defined by either rule [ rule2 ] or rule [ rule3 ] with @xmath145 .    if @xmath289 we must have @xmath290 .",
    "it then follows from assumption [ a1 ] and ( [ 3.8.3 ] ) that @xmath291 this implies the desired estimate on @xmath292 .",
    "so we may assume that @xmath293 .",
    "from the definition of @xmath143 we have for @xmath294 that @xmath295 by using lemma [ l3.8.2 ] , assumption [ a1 ] , and the estimates in lemma [ l3.1.1 ] we have for all @xmath164 that @xmath296 in view of the estimate on @xmath209 in lemma [ l3.7.1 ] , it follows for @xmath164 that @xmath297 recall that @xmath165 and @xmath298 , we therefore obtain from ( [ 3.10.1 ] ) that @xmath299 thus , if @xmath201 is sufficiently small , then we can derive that @xmath300 which gives the conclusion immediately .",
    "@xmath225    finally we prove theorem [ t3.1.1 ] concerning the convergence rates of the method .",
    "_ of theorem",
    "[ t3.1.1]_. we first consider the case @xmath301 . then for @xmath302 we have from ( [ 3.7.2 ] ) that @xmath303 therefore , by using assumption [ a1 ] , the estimate on @xmath206 in lemma [ l3.1.1 ] , the inequality @xmath304 for @xmath187 and @xmath305 , we can obtain @xmath306 observing that assumption [ a1 ] and the estimate on @xmath206 in lemma [ l3.1.1 ] imply @xmath307 thus , if @xmath201 is sufficiently small , then we have @xmath308 .",
    "since @xmath143 is determined by rule [ rule2 ] or rule [ rule3 ] , we have @xmath309 we therefore obtain @xmath310 now we can take @xmath311 in ( [ 3.8.10 ] ) to obtain @xmath312 an application of lemma [ l8.3.5 ] then gives the desired rates of convergence .",
    "for the case @xmath148 , we have @xmath147 and thus @xmath313",
    ". we may use ( [ 9.15.5 ] ) to derive that @xmath314 this completes the proof . @xmath225    the similar argument can be applied to derive the rate of convergence under the general source condition @xmath315^{\\frac{1}{p } } f\\left(\\frac{\\|t(x - x^\\dag)\\|^p}{d_{\\xi^\\dag } \\theta_f(x , x^\\dag)}\\right)\\ ] ] for some index function @xmath316 with suitable properties .",
    "although theorem [ t3.1.1 ] gives the rates of convergence , it does not tell whether the method is convergent when the source condition is not known to be satisfied . in this section",
    "we will consider the situation that @xmath3 is a reflexive banach space , @xmath4 is a hilbert space , and @xmath30 is a proper , lower semi - continuous , @xmath36-convex function satisfying ( [ pconv ] ) with @xmath317 , and derive the convergence result without assuming any source condition .",
    "we will use @xmath318 to denote the inner product in @xmath4 . in this situation",
    ", @xmath20 is the unique minimizer of the convex minimization problem @xmath319 where @xmath131 is the proper , lower semi - continuous , convex function on @xmath3 defined by ( [ theta_f ] ) satisfying @xmath320^{\\frac{1}{2 } } , \\qquad \\forall z\\in \\x , \\",
    ", x\\in d(\\p \\theta_f ) \\mbox { and } \\xi\\in \\p \\theta_f(x).\\ ] ] let @xmath143 be the integer determined by either rule [ rule1 ] , rule [ rule2 ] or rule [ rule3 ] with @xmath145",
    ". we will show that @xmath321 as @xmath239 if @xmath322 where @xmath323 denotes the null space of @xmath324 and @xmath325    we will derive the convergence result in two steps . in the first step",
    ", we consider the noise - free iterative sequence @xmath326 defined by ( [ 3.14.1 ] ) with @xmath7 replaced by @xmath8 , i.e. @xmath327 is the unique minimizer of the problem @xmath328 we will show that @xmath329 as @xmath330 . in the second step , we will consider the relation between @xmath181 and @xmath331 and establish some crucial stability estimates .",
    "the definition of @xmath143 then enables us to derive the desired convergence result .    in order to achieve these two steps ,",
    "we need the following simple result which plays a crucial role in the arguments .",
    "[ l3.11.3 ] assume that @xmath3 is a banach space and @xmath4 is a hilbert space .",
    "let @xmath332 and @xmath333 be two bounded linear operators from @xmath3 to @xmath4 .",
    "for @xmath334 let @xmath335 be the minimizer of the problem @xmath336 and let @xmath337 be the minimizer of ( [ 2.15.1 ] ) with @xmath332 , @xmath8 , @xmath31 and @xmath32 replaced by @xmath333 , @xmath338 , @xmath339 and @xmath340 respectively .",
    "then there holds @xmath341 where @xmath342 .    since @xmath335 is the minimizer of ( [ 2.15.1 ] ) , we immediately have @xmath343 . by using the minimizing property of @xmath337 , we have @xmath344 recall that @xmath345 and @xmath346 combining the above three equations we can derive that @xmath347 since @xmath348 we can obtain @xmath349 in view of the fact @xmath350 , by rearranging the terms we therefore obtain the desired result . @xmath225      in this subsection we will show for the noise - free iteration @xmath326 that @xmath329 as @xmath330 if @xmath351 satisfies ( [ 3.14.10 ] ) .",
    "we first confirm this convergence result under the stronger condition @xmath352 for some @xmath353 .",
    "this is included in the following result .",
    "[ l3.14.2 ] assume that @xmath3 is a banach space , @xmath4 is a hilbert space , and @xmath22 $ ] is a proper , lower semi - continuous , @xmath36-convex function .",
    "let @xmath11 satisfy assumption [ a1 ] and let @xmath17 satisfy ( [ 1.5 ] ) .",
    "if @xmath354 for some @xmath353 and @xmath355 is sufficiently small , then for all @xmath168 there hold @xmath356    since @xmath354 , the source condition ( [ source ] ) holds with @xmath357 and @xmath358 .",
    "thus we can apply lemma [ l3.7.1 ] to obtain the estimate on @xmath359 immediately . in order to derive the estimate on @xmath360 , we use ( [ 3.8.10 ] ) which can be formulated as @xmath361 by using the estimates on @xmath362 , ( [ 1.5 ] ) and the @xmath36-convexity of @xmath131",
    ", we can obtain the desired estimate .",
    "@xmath225    in order to derive convergence under merely the condition ( [ 3.14.10 ] ) , we will use the following strategy .",
    "we first find @xmath363 and @xmath364 such that @xmath365 is sufficiently close to @xmath32 and @xmath366 , where @xmath367 denotes the range of @xmath368 .",
    "we then use @xmath339 and @xmath365 as new initial data and define @xmath369 by letting @xmath370 be the unique minimizer of the problem @xmath371 according to lemma [ l3.14.2 ] , we have @xmath372 as @xmath330 . in order to pass this convergence result to @xmath326 , we need a perturbation result on @xmath326 with respect to @xmath32 .",
    "[ l3.15.1 ] assume that @xmath3 is a banach space , @xmath4 is a hilbert space , and @xmath22 $ ] is a proper , lower semi - continuous , @xmath36-convex function",
    ". let @xmath11 satisfy assumption [ a1 ] and let @xmath17 satisfy ( [ 1.5 ] ) .",
    "if @xmath373 is sufficiently small , then for all @xmath168 there hold @xmath374    using the same argument in the proof of lemma [ l3.1.1 ] , it follows that if @xmath201 is sufficiently small then @xmath331 and @xmath375 are well - defined for all @xmath168 and there hold the estimates @xmath376 where @xmath377    in the following we will prove ( [ 3.29.2 ] ) by induction . since @xmath378 and @xmath379 , we have from ( [ 3.29.1 ] ) and the scaling condition @xmath380 that ( [ 3.29.2 ] ) holds for @xmath180 .",
    "now we assume that ( [ 3.29.2 ] ) holds for some @xmath168 and show that it also holds true for @xmath381 .",
    "let @xmath382 .",
    "then @xmath383 and @xmath384 is still a 2-convex function . by using the definition of @xmath327 ,",
    "it is easy to see that @xmath385 is the minimizer of the minimization problem @xmath386 where @xmath387 similarly , @xmath388 is the unique minimizer of the minimization problem @xmath389 where @xmath390 let @xmath391 and @xmath392 .",
    "it then follows from lemma [ l3.11.3 ] and the @xmath36-convexity of @xmath384 that @xmath393 in view of the identity @xmath394 in hilbert spaces , we can write @xmath395 therefore we can obtain @xmath396 where @xmath397 in the following we will estimate @xmath398 for @xmath399 . with the help of assumption [ a1 ] , ( [ 1.5 ] ) , ( [ 3.29.3 ] ) and the induction hypotheses , we can derive that @xmath400 and @xmath401 moreover , by writing @xmath402 we can use assumption [ a1 ] , ( [ 3.29.3 ] ) , and the induction hypotheses to derive that @xmath403 by making use of the above estimates we therefore obtain @xmath404 and @xmath405 combining these estimates on @xmath398 , @xmath406 with ( [ 3.10.2 ] ) gives @xmath407 therefore , if @xmath201 is sufficiently small , we can obtain immediately that @xmath408 in view of the condition @xmath271 , we therefore obtain the desired estimates .",
    "@xmath225    now we are ready to prove the convergence of the noise - free iteration @xmath326 .",
    "[ p3.12.1 ] let @xmath3 be a reflexive banach space and @xmath4 be a hilbert space , let @xmath30 be a proper , lower semi - continuous , @xmath36-convex function on @xmath3 .",
    "let @xmath11 satisfy assumption [ a1 ] and let @xmath17 satisfy ( [ 1.5 ] ) .",
    "if @xmath409 and @xmath355 is sufficiently small , then there hold @xmath410    let @xmath135 denote the fenchel conjugate of @xmath131 .",
    "it is known that @xmath411 , @xmath135 is frchet differentiable and its gradient @xmath136 satisfies @xmath412    let @xmath413 be sufficiently small . since @xmath3 is reflexive , we have @xmath414",
    ". therefore @xmath415 .",
    "consequently , we can choose @xmath416 such that @xmath417 and @xmath366 .",
    "we now define @xmath418",
    ". then we have @xmath419 and @xmath420 .",
    "moreover @xmath421 since @xmath422 , by taking @xmath423 to be small enough , we can guarantee that @xmath424 .",
    "we then use this @xmath339 as an initial guess to define @xmath369 as above .",
    "since the smallness of @xmath425 implies the smallness of @xmath426 , we may use lemma [ l3.15.1 ] to conclude that there is a constant @xmath427 independent of @xmath168 such that @xmath428 on the other hand , since @xmath366 , it follows from lemma [ l3.14.2 ] and ( [ 1.5 ] ) that there exists an integer @xmath429 such that @xmath430 consequently @xmath431 since @xmath423 can be arbitrarily small , we therefore obtain the convergence result . @xmath225      although we have shown in the previous subsection the convergence of the noise - free iteration @xmath326 as @xmath330 , our ultimate aim is to show that @xmath321 as @xmath239 with the integer @xmath143 defined by either rule [ rule1 ] , [ rule2 ] , or [ rule3 ] with @xmath145 .",
    "we still need some stability estimates contained in the following result .",
    "[ l3.15.10 ] assume that all the conditions with @xmath317 in lemma [ l3.1.1 ] hold , and assume also that @xmath4 is a hilbert space .",
    "if @xmath432 is sufficiently small , then for all @xmath164 there hold @xmath433 where @xmath153 is the integer defined by ( [ nhat ] ) .",
    "we first prove by induction that @xmath434 since @xmath28 , the estimates are trivial for @xmath180 .",
    "we now assume that the estimates are true for some @xmath182 and show that they are also true for @xmath381 .",
    "we will use the similar argument in the proof of lemma [ l3.15.1 ] . by the definition of @xmath20",
    ", it is easy to see that @xmath435 is the unique minimizer of the problem @xmath436 where @xmath437 recall that @xmath385 is the unique minimizer of the problem ( [ 2.15.200 ] ) with @xmath438 given by ( [ 2.15.11 ] ) . in view of lemma [ l3.11.3 ] and the @xmath36-convexity of @xmath384",
    ", we can obtain @xmath439 we can write @xmath440 therefore , it follows from ( [ 3.11.10 ] ) that @xmath441 where @xmath442 in the following we will estimate @xmath443 for @xmath399 . with the help of assumption [ a1 ] , ( [ 1.5 ] ) , ( [ 3.29.3 ] ) , the estimates in lemma [ l3.1.1 ] and the induction hypotheses , we can derive that @xmath444 and @xmath445 in order to estimate @xmath446 , we use the expressions of @xmath447 and @xmath438 to write @xmath448 + \\left[f'(x_n^\\d)-f'(x_n)\\right ] e_n^\\d.\\end{aligned}\\ ] ] by using assumption [ a1 ] , the estimates in lemma [ l3.1.1 ] , ( [ 3.29.3 ] ) and the induction hypotheses , we can derive that @xmath449 therefore @xmath450 by making use of the above estimates we therefore obtain @xmath451 combining the above estimates on @xmath443 for @xmath399 we therefore obtain from ( [ 4.13.1 ] ) that @xmath452 thus , if @xmath201 is sufficiently small , we have @xmath453 this together with @xmath454 completes the proof of ( [ 3.31.1 ] ) .    by using the estimate ( [ 3.31.1 ] )",
    "we have @xmath455 .",
    "thus , we may use ( [ 3.11.10 ] ) and ( [ 3.31.3 ] ) to obtain @xmath456 observing that assumption [ a1 ] , lemma [ l3.1.1 ] , and ( [ 3.31.1 ] ) imply @xmath457 we may use ( [ 3.31.2 ] ) , ( [ 9.17.1 ] ) and ( [ 9.17.2 ] ) to obtain @xmath458 since it is trivial for @xmath180 because @xmath28 .",
    "finally , we can use assumption [ a1 ] , lemma [ l3.1.1 ] , ( [ 3.29.3 ] ) and ( [ 3.31.1 ] ) to derive that @xmath459(x_n^\\d - x_n)\\|\\\\ & \\quad \\ , + \\|y^\\d - y - t(x_n^\\d - x_n)\\|\\\\ & \\le ( 1+c\\vep ) \\d.\\end{aligned}\\ ] ] the proof is therefore complete .",
    "@xmath225    now we are ready to prove the main convergence result .",
    "[ t5.2 ] let @xmath3 be a reflexive banach space and @xmath4 be a hilbert space , and let @xmath46 $ ] be a proper , lower semi - continuous , @xmath36-convex function .",
    "let @xmath11 satisfy assumption [ a1 ] and let @xmath17 satisfy ( [ 1.5 ] ) .",
    "assume that @xmath227 is the unique solution of ( [ 1.1 ] ) in @xmath460 .",
    "if @xmath409 and @xmath432 is sufficiently small , then for the method ( [ irgn ] ) terminated by either rule [ rule1 ] , [ rule2 ] , or [ rule3 ] with @xmath145 there holds @xmath461 as @xmath239 .",
    "we complete the proof by considering two cases .",
    "assume first that there is a sequence @xmath233 satisfying @xmath234 with @xmath235 such that @xmath462 converges to a finite integer @xmath168 as @xmath236 .",
    "we may assume that @xmath463 for all @xmath464 .",
    "by lemma [ l3.15.10 ] we have @xmath465 as @xmath236 .",
    "since the definition of @xmath466 implies @xmath467 by taking @xmath236 we can obtain @xmath468 . since @xmath227 is the unique solution of ( [ 1.1 ] ) in @xmath229 , we have @xmath469 and hence @xmath470 as @xmath236 .",
    "assume next that there is a sequence @xmath233 satisfying @xmath234 with @xmath235 such that @xmath471 as @xmath236 .",
    "by the first estimate in lemma [ l3.15.10 ] we have @xmath472 by using the definition of @xmath466 and the second estimate in lemma [ l3.15.10 ] we can obtain @xmath473 by using assumption [ a1 ] and ( [ 3.29.3 ] ) we can show that @xmath474 for all @xmath168 if @xmath201 is sufficiently small , and consequently @xmath475 since @xmath476 , it follows from theorem [ p3.12.1 ] and ( [ 1.5 ] ) that @xmath477 as @xmath236 .",
    "moreover , theorem [ p3.12.1 ] also implies that @xmath478 as @xmath236 .",
    "we therefore obtain again @xmath479 as @xmath236 .",
    "in this section we consider some examples on parameter identification in partial differential equations to illustrate that assumption [ a1](d ) can be verified for a wide range of applications .",
    "we also report some numerical experiments to test the efficiency of our method .",
    "[ e6.1 ] we first consider the identification of the parameter @xmath480 in the boundary value problem @xmath481 from an @xmath101-measurement of the state @xmath482 , where @xmath483 is a bounded domain with lipschitz boundary @xmath484 , @xmath485 and @xmath486 .",
    "we assume @xmath487 is the sought solution .",
    "this problem reduces to solving an equation of the form ( [ 1.1 ] ) if we define the nonlinear operator @xmath11 to be the parameter - to - solution mapping @xmath488 with @xmath489 being the unique solution of ( [ 8.27.1 ] ) .",
    "such @xmath11 is well - defined on @xmath490 for some positive constant @xmath491 .",
    "it is well known that @xmath11 has frchet derivative @xmath492 where @xmath493 is defined by @xmath494 which is an isomorphism uniformly in a ball @xmath495 around @xmath496 .",
    "let @xmath497 be the dual space of @xmath498 with respect to the bilinear form @xmath499 then @xmath500 extends to an isomorphism from @xmath101 to @xmath497 .",
    "since ( [ 8.27.2 ] ) implies for any @xmath501 and @xmath502 @xmath503 and since @xmath504 embeds into @xmath497 due to the restriction @xmath505 , we have @xmath506 on the other hand , observing that @xmath507 by using ( [ 8.27.2 ] ) we have @xmath508 thus , by a similar argument as above , @xmath509 therefore , if @xmath112 is small enough , we have @xmath510 for @xmath511 , which together with ( [ 8.27.3 ] ) verifies assumption [ a1](d ) .    in order to reconstruct @xmath496",
    ", we choose a @xmath36-convex function @xmath30 , an initial guess @xmath512 and @xmath513 . by adopting remark [ remark3.1 ]",
    ", we then define @xmath514 as the minimizer of the convex optimization problem @xmath515 let @xmath143 be the integer determined by either rule [ rule1 ] , [ rule2 ] , or [ rule3 ] with @xmath145",
    ". then , by theorem [ t5.2 ] , we have @xmath516 as @xmath239 .    in the following we present two numerical experiments for this example to test our method . in these computation",
    ", we always choose @xmath30 to be nonnegative with @xmath517 so that we can take @xmath518 and @xmath34 and consequently @xmath519 .      in the first numerical experiment we consider the one - dimensional problem over the interval @xmath524 with the sought solution given by @xmath525 we assume that the inhomogeneous term is @xmath526 and the boundary data are @xmath527 and @xmath528 . then @xmath529 . in our computation , instead of @xmath530 we use random noise data @xmath531 satisfying @xmath532}=\\d$ ] with noisy level @xmath533 ; we take @xmath534 and @xmath535 .",
    "the differential equations involved are solved approximately by a finite difference method by dividing @xmath536 $ ] into @xmath537 subintervals of equal length with the resulting tridiagonal system solved by the thomas algorithm .",
    "the convex optimization problems ( [ min6.2 ] ) is solved by a restart conjugate gradient method ( @xcite ) . the iteration is terminated by rule [ rule1 ] , i.e. the discrepancy principle , with @xmath538 . in figure [ fig6.2 ]",
    "we report the computational results with different choices of @xmath30 . in ( a )",
    "we report the result with @xmath520 for which the corresponding method becomes the iteratively regularized gauss ",
    "newton method in hilbert spaces .",
    "although the reconstruction tells something on the sought solution , it does not tell more information such as sparsity , discontinuities and constancy since the result is too oscillatory . in ( b )",
    "we report the result corresponding to @xmath521 with @xmath522 . since @xmath539 is non - smooth ,",
    "we replace it by @xmath540 with @xmath541 in our computation .",
    "it is clear that the sparsity of the sought solution is significantly reconstructed .",
    "the reconstruction result , however , is still oscillatory on the nonzero parts which is typical for this choice of @xmath30 . in ( c )",
    "we report the result corresponding to @xmath542 } |d c|$ ] with @xmath522 .",
    "again we replace @xmath543 } |dc|$ ] by @xmath543 } \\sqrt{|d c|^2 + \\varepsilon}$ ] with @xmath541 .",
    "the reconstruction is rather satisfactory and the notorious oscillatory effect is efficiently removed .    in the second numerical experiment",
    "we consider the two dimensional problem with @xmath544\\times[0,1]$ ] .",
    "the sought solution is @xmath545\\times [ 0.2 , 0.5],\\\\ 0 , & \\quad \\mbox{elsewhere}. \\end{array}\\right.\\ ] ] we assume that @xmath546 , @xmath547 , and the boundary condition @xmath548 .",
    "we add noise to @xmath530 to produce a noisy data @xmath531 satisfying @xmath549 with @xmath534 .",
    "we take @xmath535 and use @xmath531 to reconstruct @xmath496 by our method which is terminated by rule [ rule1 ] with @xmath538 .",
    "all partial differential equations involved are solved approximately by a finite difference method by dividing @xmath107 into @xmath550 small squares of equal size with the resulting linear system solved by the gauss  seidel method .",
    "all optimization problems are solved by a restart conjugate gradient method .",
    "we report the computational results in figure [ fig6.3 ] .",
    "in ( a ) we plot the the exact solution @xmath496 , in ( b ) we plot the computational result corresponding to @xmath520 , and in ( c ) and ( d ) we plot the computational results corresponding to @xmath551 with @xmath522 and @xmath552 respectively .",
    "we replace @xmath553 by @xmath554 with @xmath541 during computation .",
    "it is clear that the reconstruction results in ( c ) and ( d ) are much better than the one in ( b ) .",
    "moreover , the results in ( c ) and ( d ) indicate that the method is rather robust with respect to @xmath555 since the change of @xmath555 does not affect the reconstruction much .",
    "[ e3.2 ] let @xmath98 be a bounded domain with lipschitz boundary @xmath484 .",
    "consider the identification of the diffusion parameter @xmath556 in @xmath557 from the @xmath558 measurement of @xmath482 , where @xmath559 and @xmath560 are given .",
    "it is well - known that for @xmath561 bounded below by a positive constant , ( [ 4.3.10 ] ) has a unique solution @xmath562 .",
    "we assume that the sought solution @xmath563 is in @xmath564 for some @xmath565 with @xmath566 on @xmath107 for some positive constant @xmath567 .",
    "thus this inverse problem reduces to solving an equation of the form ( [ 1.1 ] ) if we define @xmath11 as @xmath568 with @xmath569 since @xmath564 embeds into @xmath570 , the operator @xmath11 is well - defined .",
    "this is the inverse groundwater filtration problem corresponding to the steady state case studied in @xcite in which it has been shown that @xmath11 is frchet differentiable and there holds @xmath571 for all @xmath572 , where @xmath573 denotes the ball in @xmath564 of radius @xmath574 around @xmath563 .",
    "we will follow the technique in @xcite to show assumption [ a1](d ) . for @xmath572 and @xmath575 we set @xmath576 recall",
    "that @xmath577 is the weak solution of the boundary value problem @xmath578 the same is true for @xmath579 .",
    "therefore @xmath580 since the operator @xmath581 defined by @xmath582 can be extended as an isomorphism @xmath583 so that @xmath584 is uniformly bounded around @xmath563 , where @xmath497 denotes the anti - dual of @xmath585 with respect to the bilinear form ( [ bilinear ] ) , from the above equation we then have @xmath586    in order to proceed further , note that for @xmath575 , @xmath587 and @xmath588 , we have @xmath589 recall the embedding @xmath590 for @xmath565 and the embedding @xmath591 for all @xmath592 . since @xmath565 implies @xmath593",
    ", we have @xmath594 therefore , for all @xmath588 , @xmath595 which implies that @xmath596 applying this inequality to estimate the two terms on the right hand side of ( [ 3.4 ] ) , we obtain @xmath597 for all @xmath598 and @xmath572 . from ( [ 3.3 ] ) it follows @xmath599 for @xmath572 by shrinking the ball @xmath573 if necessary .",
    "this together with ( [ 3.5 ] ) verifies assumption [ a1](d ) .",
    "in order to reconstruct @xmath563 , we pick an initial guess @xmath600 and take the function @xmath601 which is known to be @xmath602-convex in @xmath564 . observing that @xmath603 and thus @xmath604 . therefore , for one - dimensional problem , i.e. @xmath605 , we may take @xmath606 and define @xmath607 as the minimizer of the convex functional @xmath608 over @xmath564 . if @xmath143 denotes the integer determined by either rule [ rule1 ] , [ rule2 ] , or [ rule3 ] with @xmath145 , we have from theorem [ t5.2 ] that @xmath609 as @xmath239 . for higher dimensional problem ,",
    "i.e. @xmath610 , we have @xmath611 .",
    "we may define @xmath607 as the minimizer of the convex functional @xmath612 over @xmath564 .",
    "it then follows from theorem [ t4.5.1 ] that @xmath613 converges to @xmath563 weakly in @xmath564 .",
    "since @xmath564 can be compactly embedded into @xmath570 , we have @xmath614 as @xmath239 .      in the following",
    "we present a numerical test for the one - dimensional problem over the interval @xmath544 $ ] with boundary data @xmath615 and inhomogeneous term @xmath616 the function to be reconstructed is @xmath617 observing that @xmath618 .",
    "we add noise to @xmath619 to produce a noisy data @xmath531 satisfying @xmath620}=\\d$ ] with given noise level @xmath533 and use @xmath531 to reconstruct @xmath563 by our method in which each iterate is defined by the convex optimization problem ( [ min6.1 ] ) with @xmath606 .",
    "we take the noise level @xmath534 and the initial guess @xmath621 .",
    "we also take the sequence @xmath17 to be @xmath535 . during the computation ,",
    "all differential equations are solved approximately by the finite element method on the subspace of piecewise linear splines on a uniform grid with subinterval length @xmath622 , and the optimization problems ( [ min6.1 ] ) are solved by a restart conjugate gradient method ( @xcite ) . in figure [ f6.1 ]",
    "we report the numerical results of our method for several different values of @xmath623 $ ] with the iteration terminated by rule [ rule1 ] with @xmath538 .",
    "it shows that the method works well for these selected values of @xmath63 .",
    "moreover , by decreasing @xmath63 from @xmath36 to @xmath624 , the reconstruction result becomes better when the sought solution has corners and constant parts .",
    "however , one has to pay the price of more computational time for smaller @xmath63 .",
    "[ e3.4 ] we consider the transient case of the inverse groundwater filtration problem which identifies the material coefficient @xmath556 in @xmath625 , \\\\",
    "u=\\varphi \\qquad \\mbox { on } \\partial \\omega \\times ( 0 , t ] , \\\\ u = u_0 \\qquad \\mbox { on } \\omega \\times \\{t=0\\ } \\end{array } \\right.\\end{aligned}\\ ] ] from the @xmath626-measurement of @xmath482 , where @xmath627 is a bounded domain with lipschitz boundary , @xmath628 , @xmath629 and @xmath630 .",
    "it is well - known that ( [ 3.10 ] ) has a unique solution @xmath631 for each @xmath561 bounded from below by a positive constant .",
    "we assume that the sought solution @xmath563 is in @xmath564 with @xmath565 satisfying @xmath632 on @xmath107 .",
    "this inverse problem reduces to solving ( [ 1.1 ] ) if we define the nonlinear operator @xmath633 by @xmath634 with the same domain @xmath110 as in example [ e3.2 ] .",
    "it is known that @xmath11 is frchet differentiable , and , for @xmath635 and @xmath575 , @xmath636 satisfies @xmath637,\\\\ u'=0 \\qquad \\mbox{on } \\partial \\omega \\times ( 0 , t],\\\\ u'=0 \\qquad \\mbox{on } \\omega \\times \\{t=0\\}. \\end{array } \\right.\\end{aligned}\\ ] ] using the same notations as in ( [ 3.3.5 ] ) we have for @xmath638 that @xmath639,\\\\ w=0 \\qquad",
    "\\mbox{on } ( \\partial \\omega \\times ( 0 , t ] ) \\times ( \\omega\\times \\{t=0\\ } ) . \\end{array } \\right.\\end{aligned}\\ ] ] from the well - known facts on parabolic equations ( see @xcite ) it follows that @xmath640 for all @xmath641 and @xmath556 in a neighborhood around @xmath563 . by employing the corresponding estimates derived in example [ e3.2 ] we obtain @xmath642 from ( * ? ? ?",
    "* theorem 3.2 ) we know that @xmath643 this together with ( [ 3.11 ] ) implies assumption [ a1](c ) . therefore , our method is applicable to this example , and we can formulate the procedure to reconstruct @xmath563 similarly as is done in example [ e3.2 ] .",
    "0.3 cm * acknowledgements * q jin is partly supported by the grant de120101707 of australian research council , and m zhong is partly supported by the national natural science foundation of china ( no.11101093 ) ."
  ],
  "abstract_text": [
    "<S> in this paper we propose an extension of the iteratively regularized gauss  </S>",
    "<S> newton method to the banach space setting by defining the iterates via convex optimization problems . </S>",
    "<S> we consider some a posteriori stopping rules to terminate the iteration and present the detailed convergence analysis . </S>",
    "<S> the remarkable point is that in each convex optimization problem we allow non - smooth penalty terms including @xmath0 and total variation ( tv ) like penalty functionals . </S>",
    "<S> this enables us to reconstruct special features of solutions such as sparsity and discontinuities in practical applications . </S>",
    "<S> some numerical experiments on parameter identification in partial differential equations are reported to test the performance of our method .    </S>",
    "<S> example.eps    gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore    [ section ] [ section ] [ section ]         </S>"
  ]
}