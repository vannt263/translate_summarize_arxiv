{
  "article_text": [
    "approximate bayesian computation ( abc ) ( or likelihood - free inference ) has become increasingly prevalent in areas of the natural sciences in which likelihood functions requiring integration over a large number of complex latent states are essentially intractable .",
    "( see cornuet _",
    "et al . , _ 2008 ,",
    "beaumont , 2010 , marin _ et al .",
    "_ , 2011 and sisson and fan , 2011 for recent reviews . )",
    "the technique circumvents direct evaluation of the likelihood function by matching summary statistics calculated from the observed data with corresponding statistics computed from data simulated from the assumed data generating process . if such statistics are sufficient , the method yields an approximation to the exact posterior distribution of interest that is accurate , given an adequate number of simulations ; otherwise , _ partial _ posterior inference , reflecting the information content of the set of summary statistics only , is the outcome .",
    "the choice of statistics for use within the abc method , in addition to techniques for determining the matching criterion , are clearly of paramount importance , with much recent research having been devoted to devising ways of ensuring that the information content of the chosen set is maximized , in some sense ; e.g. * *  * * joyce and marjoram ( 2008 ) , wegmann _ et al . _",
    "( 2009 ) , blum ( 2010a ) and fearnhead and prangle ( 2012 ) .",
    "recent contributions here include those of drovandi _ et al . _",
    "( 2011 ) , drovandi and pettitt ( 2013 ) , gleim and pigorsch ( 2013 ) and creel and kristensen ( 2014 ) , in which the statistics are produced by estimating an approximating _ auxiliary _ model using both simulated and observed data .",
    "this approach mimics , in a bayesian framework , the principle underlying the frequentist methods of indirect inference ( ii ) ( gouriroux _ et al .",
    "_ 1993 , smith , 1993 , heggland and frigessi , 2004 ) and efficient method of moments ( emm ) ( gallant and tauchen , 1996 ) , using , as it does , the approximating model to produce feasible , but sub - optimal , inference about an intractable true model . whilst the price paid for the approximation in the frequentist setting is a possible reduction in efficiency ,",
    "the price paid in the bayesian case is posterior inference that is conditioned on statistics that are not sufficient for the parameters of the true model , and which amounts to only partial inference as a consequence .",
    "our paper continues in this spirit , but with particular focus given to the application of auxiliary model - based abc methods in the state space model ( ssm ) framework .",
    "we begin by demonstrating that reduction to a set of sufficient statistics of fixed dimension relative to the sample size is _ infeasible _ in finite samples in ssms .",
    "this key observation then motivates our decision to seek asymptotic sufficiency in the state space setting by using the mle of the parameters of the auxiliary model as the ( vector ) summary statistic in the abc matching criterion .",
    "we focus on two qualitatively different cases : 1 ) one in which the auxiliary model _ coincides _ with the true model , in which case asymptotic sufficiency for the true parameters is achievable via the proposed abc technique ; and 2 ) the more typical case in which the exact likelihood function is inaccessible , and the auxiliary model represents an approximation only .",
    "the first case mimics that sometimes referenced in the ii ( or emm ) literature , in which the auxiliary model ` nests ' , or is equivalent to in some well - defined sense , the true model , and full asymptotic efficiency is achieved by the frequentist methods as a consequence .",
    "investigation of this case allows us to document the maximum accuracy gains that are possible via the auxiliary model route , compared with abc techniques based on alternative summaries , without the confounding effect of the error in the approximating model .",
    "the second case gives some insight into what can be achieved in a general non - linear state space setting when the investigator is forced to adopt an inexact approximating model in the implementation of auxiliary model - based abc .",
    "we give emphasis here to non - linear models in which the state ( and possibly the observed ) is driven by a continuous time model , as this is the canonical example in which simulation from the true model is feasible ( at least via an arbitrarily fine discretization ) , whilst the likelihood function is ( typically ) unavailable and exact posterior analysis thus not achievable .",
    "we begin by considering the very concept of finite sample sufficiency in the state space context , and the usefulness of applying a typical abc approach - based on _ ad hoc _ summary statistics - in this setting . using the linear gaussian model for illustration",
    ", we demonstrate the lack of reduction to a set of sufficient statistics of fixed dimension , this result providing motivation , as noted above , for the pursuit of asymptotic sufficiency via the auxiliary model method .",
    "we then proceed to demonstrate the bayesian consistency of the auxiliary model approach , subject to the typical quasi - mle form of conditions being satisfied .",
    "we also illustrate that to the order of accuracy that is relevant in establishing the theoretical properties of an abc technique ( i.e. allowing the tolerance used in the matching of the statistics to approach zero ) , a selection criterion based on the score of the auxiliary model - evaluated at the mle computed from the observed data - yields equivalent results to a criterion based directly on the mle itself . this equivalence is shown to hold in both the exactly and over - identified cases , and independently of any ( positive definite ) weighting matrix used to define the two alternative distance measures , and",
    "implies that the proximity to asymptotic sufficiency yielded by the use of the mle in an abc algorithm will be replicated by the use of the score .",
    "given the enormous gain in speed achieved by avoiding optimization of the approximate likelihood at each replication of abc , this is an critical result from a computational perspective .",
    "the application of the proposed method in multiple parameter settings is addressed , with separate treatment of scalar ( or lower - dimensional blocks of ) parameters , via marginal , or integrated likelihood principles advocated , as a possible way of avoiding the inaccuracy that plagues abc techniques in high dimensions .",
    "( see blum , 2010b and nott _ et al .",
    "_ , 2014 ) .",
    "the results outlined in the previous paragraph are applicable to an auxiliary model - based abc method applied in any context ( subject to regularity ) and , hence , are of interest in their own right .",
    "however , our particular interest , as already noted , is in applying the auxiliary model method - and thereby exploiting these properties  - in the state space setting . for case 1 ) we choose to illustrate the approach using the linear gaussian model , whereby the exact likelihood ( and hence score ) is accessible via the kalman filter ( kf ) , and asymptotic sufficiency thus achievable . for case 2 ) we illustrate the approach via a particular choice of ( approximating ) auxiliary model for the continuous time ssm .",
    "specifically , the approximating model is formed as a discretization of the true continuous time model , with the augmented unscented kalman filter ( aukf ) ( julier _ et al . _ , 1995 , julier and uhlmann , 2004 )  used to evaluate the likelihood of that model . the general applicability ,",
    "speed and simplicity of the aukf calculations render the abc scheme computationally feasible and relatively simple to implement .",
    "this particular approach to the definition of an auxiliary model also leads to a set of summary statistics of relatively small dimension .",
    "this is in contrast , for example , with an approach based on a highly parameterized ( ` nesting ' ) approximating model ( see , for example , gleim and pigorsch , 2013 ) , in which the large number of auxiliary parameters - in principle sufficient for the parameters of the true latent diffusion model - is likely to yield a very inaccurate ( non - parametric ) estimate of the true posterior , due to the large dimension of the conditioning statistics .",
    "the equality between the number of parameters in our exact and approximating models also means that marginalization of the approximating model to produce a scalar matching criterion for each parameter of the true model is meaningful .",
    "the paper proceeds as follows . in section [ abc ]",
    "we briefly summarize the basic principles of abc as they would apply in a state space setting , including the role played by summary statistics and sufficiency .",
    "we demonstrate the lack of finite sample sufficiency reduction in an ssm , using the linear gaussian model for illustration . * *  * * in section [ aux ] , we then proceed to demonstrate the properties of abc based on the mle , score and marginal score , respectively , of a generic approximating model followed , in section [ model ] , by an outline of a computationally feasible approximation - based on the aukf - for use in the non - linear state space setting . using ( repeated samples of ) artificially generated data , the accuracy with which the proposed technique reproduces the exact posterior distribution",
    "is assessed in section [ assess ] .",
    "the abc methods are based respectively on : i ) the joint score ; ii ) the marginal score ; iii ) a ( weighted ) euclidean metric based on statistics that are sufficient for an observed autoregressive model of order one ; and iv ) the dimension - reduction technique of fearnhead and prangle ( 2012 ) , applied to this latter set of summary statistics .",
    "we conduct the assessment firstly within the context of the linear gaussian model , with the issues of sufficiency and matching that are key to accurately reproducing the true posterior distribution able to be illustrated precisely in this setting .",
    "the overall superiority of both the joint and marginal score techniques over the abc methods based on summary statistics is demonstrated numerically , as is the remarkable accuracy yielded by the marginal score technique in particular .",
    "this exercise thus forms a resounding proof - of - concept for the score - based abc method , albeit in a case where the exact score is available .",
    "we then proceed to assess performance in a particular non - linear latent diffusion model , in which the degree of accuracy of the aukf - based approximating model plays a role .",
    "a stochastic volatility for financial returns , in which the latent volatility is driven by a square root diffusion model , is adopted as the non - linear example , as the existence of known ( non - central chi - squared ) transition densities means that the exact likelihood function / posterior distribution is available , for the purpose of comparison .",
    "we apply the deterministic grid - based filtering method of ng _ et al . _",
    "( 2013 ) - suitable for this particular setting - to produce the exact comparators for our abc - based estimates of the relevant marginal posteriors , as well as the marginal posteriors associated with an euler approximation to the true model .",
    "the score methods out - perform the summary statistic methods in the great majority of cases documented .",
    "some gain in accuracy is still produced via the marginalization technique , although that gain is certainly less marked than in the linear gaussian case , in which the exact score is accessible .",
    "notably , all abc - based approximations , which exploit simulation from the exact latent diffusion model , serve as more accurate estimates ( overall ) of the exact posteriors than do the aukf and euler approximations themselves .",
    "section [ end ] concludes .",
    "the aim of abc is to produce draws from an approximation to the posterior distribution of a vector of unknowns , @xmath0 , given the @xmath1-dimensional vector of observed data @xmath2 , @xmath3 in the case where both the prior , @xmath4 , and the likelihood , @xmath5 , can be simulated .",
    "these draws are used , in turn , to approximate posterior quantities of interest , including marginal posterior moments , marginal posterior distributions and predictive distributions .",
    "the simplest ( accept / reject ) form of the algorithm ( tavar _ _  et al .",
    "_ _ 1997 , pritchard , 1999 ) proceeds as follows :    1 .",
    "simulate @xmath6 , @xmath7 , from @xmath4 2 .",
    "simulate @xmath8 , @xmath7 , from the likelihood , @xmath9 3 .",
    "select @xmath6 such that:@xmath10 where @xmath11 is a ( vector ) statistic , @xmath12 is a distance criterion of some sort , and the tolerance level @xmath13 is arbitrarily small . in practice @xmath13 may be chosen such that , for a given value of @xmath14 , a certain ( small ) proportion of draws of @xmath6 are selected .",
    "the algorithm thus samples @xmath0 and @xmath15 from the joint _ _  _ _ posterior:@xmath16 where @xmath17 is the indicator function defined on the set @xmath18 and @xmath19 clearly , when @xmath11 is sufficient and @xmath13 arbitrarily small,@xmath20 approximates the true posterior , @xmath21 , and draws from @xmath22 can be used to estimate features of the true posterior . in practice",
    "however , the complexity of the models to which abc is applied implies , almost by definition , that sufficiency is unattainable . hence , in the limit , as @xmath23 , the draws can be used only to approximate features of @xmath24    adaptations of the basic rejection scheme have involved post - sampling corrections of the draws using kernel methods ( beaumont _ et al .",
    "_ , 2002 , blum , 2010a , blum and franois , 2010 ) , or the insertion of markov chain monte carlo ( mcmc ) and/or sequential monte carlo ( smc ) steps ( marjoram _ et al . _ , 2003 ,",
    "et al . , _ 2007 , beaumont _",
    "et al . _ , 2009 ,",
    "toni _ et al . _ , 2009 and wegmann _ et al . _ , 2009 ) , to improve the accuracy with which @xmath25 is estimated , for any given number of draws .",
    "focus is also given to choosing @xmath26 and/or @xmath12 so as to render @xmath25 a closer match to @xmath21 , in some sense ; see joyce and marjoram ( 2008 ) , wegmann _ et al .",
    "_ , blum ( 2010b ) and fearnhead and prangle ( 2012 ) . in the latter vein ,",
    "drovandi _ et al . _",
    "( 2011 ) argue , in the context of a specific biological model , that the use of @xmath26 comprised of the mles of the parameters of a well - chosen approximating model , may yield posterior inference that is conditioned on a large portion of the information in the data and , hence , be close to exact inference based on @xmath21 .",
    "( see also drovandi and pettitt , 2013 , gleim and pigorsch , 2013 , and creel and kristensen , 2014 ) .",
    "it is the spirit of this approach that informs the current paper , but with our attention given to rendering the approach feasible in a _",
    "general _ state space framework that encompasses a large number of the models that are of interest to practitioners , including continuous time models .",
    "our focus then is on the application of abc in the context of a general ssm with measurement and transition distributions,@xmath27 respectively , where @xmath28 is a @xmath29-dimensional vector of static parameters , elements of which may characterize either the measurement or state relation , or both .",
    "_ _  _ _ for expositional simplicity , and without loss of generality , we consider the case where both @xmath30 and @xmath31 are scalars . in financial applications",
    "it is common that both the observed and latent processes _ _",
    "_ _ are driven by continuous time processes , with the transition distribution in ( [ state_gen ] ) being unknown ( or , at least , computationally challenging ) as a consequence .",
    "bayesian inference would then typically proceed by invoking ( euler ) discretizations for both the measurement and state processes and applying mcmc- or smc - based techniques , with such methods being tailor - made to suit the features of the particular ( discretized ) model at hand ; see giordini _ et al . _",
    "( 2011 ) for a recent review .",
    "in some models expressed initially in discrete time , it may also be the case that the conditional distribution in ( [ meas_gen ] ) is unavailable in closed form , such as when empirically relevant distributions for financial returns are adopted ( e.g. peters _ et al . , _",
    "in such cases , smc- or mcmc - based inferential methods are typically infeasible .",
    "in contrast , in all of these cases the proposed abc method _ is _ feasible , as long as simulation from the true model ( at least via an arbitrarily fine discretization , in the continuous time case ) is possible .",
    "the full set of unknowns thus constitutes the augmented vector @xmath32 where , in the case where @xmath31 evolves in continuous time , @xmath33 represents the infinite - dimensional vector comprising the continuum of unobserved states over the sample period .",
    "however , to fix ideas , we define @xmath34 where @xmath35 is the @xmath1-dimensional vector comprising the time @xmath36 states for the @xmath1 observation periods in the sample .",
    "implementation of the algorithm thus involves simulating from @xmath4 by simulating @xmath28 from the prior @xmath37 , followed by simulation of @xmath31 via the process for the state , conditional on the draw of @xmath28 , and subsequent simulation of artificial data @xmath38 conditional on the draws of @xmath28 and the state variable .",
    "crucially , the focus in this paper is on inference about @xmath28  only ; hence , only draws of @xmath28  are retained ( via the selection criterion ) and those draws used to produce an estimate of the marginal posterior , @xmath39 , and with sufficiency to be viewed as relating to @xmath28  only .",
    "hence , from this point onwards , when we reference a vector of summary statistics , @xmath40 , it is the information content of that vector with respect to @xmath28  that is of importance , and the proximity of @xmath41  to the marginal posterior of @xmath28 that is under question .",
    "we comment briefly on state inference in section [ end ] .    before outlining the proposed methodology for the model in ( [ meas_gen ] ) and ( [ state_gen ] ) in section [ aux ] we highlight the key observation that motivates our approach , namely that reduction to sufficiency in finite samples",
    "is not possible in state space settings . * *  * * we use a linear gaussian state space model to illustrate this result , as closed - form expressions are available in this case ; however , as highlighted at the end of the section , the result is , in principle , applicable to any ssm .",
    "sufficient statistics are useful for inference about a ( vector ) parameter since , being in possession of the sufficient set means that the data itself may be discarded for inference purposes . when the cardinality of the sufficient set is small relative to the sample size a significant reduction in complexity is achieved and in the case of abc , conditioning on the sufficient statistics leads to no loss of information , and the method produces a simulation - based estimate of the true posterior .",
    "the difficulty that arises is that only distributions that are members of the exponential family ( ef ) possess sufficient statistics that achieve a reduction to a fixed dimension relative to the sample size . in the context of the general ssm described by ( [ meas_gen ] ) and ( [ state_gen ] )",
    "the effective use of sufficient statistics is problematic . for any @xmath36 it is unlikely that the marginal distribution of @xmath30 will be a member of the ef , due to the vast array of non - linearities that are possible , in either the measurement or state equations , or both .",
    "moreover , even if @xmath30 _ were _ a member of the ef for each @xmath36 , to achieve a sufficiency reduction it is required that the _ joint _ distribution of @xmath42 also be in the ef . for example , even if @xmath30 were gaussian , it does not necessarily follow that the joint distribution of @xmath43 will achieve a sufficiency reduction .",
    "the most familiar example of this is when @xmath43 follows a gaussian moving average ( ma ) process and consequently only the whole sample is sufficient .",
    "even the simplest ssms generate ma - like dependence in the data .",
    "consider the linear gaussian ssm , expressed in regression form as @xmath44 where the disturbances are respectively independent @xmath45 and @xmath46 variables .",
    "in this case , the joint distribution of the vector of @xmath30 s ( which are marginally normal and members of the ef ) is @xmath47 where @xmath48 is the inverse of the signal - to - noise ( sn ) ratio and @xmath49 is the familiar toeplitz matrix associated with an autoregressive ( ar ) model of order 1 . to construct the sufficient statistics we need to evaluate @xmath50 , which appears in the quadratic form of the multivariate normal density , with the structure of @xmath51 determining the way in which sample information about the parameters is accumulated and , hence , the sufficiency reduction that is achievable .",
    "( see , for example , anderson , 1958 , chp 6 . )",
    "representing @xmath51 as @xmath52 it is straightforward to show that as the order of the approximation is increased by retaining more terms , the extent of the accumulation across successive observations is reduced , with the full sample of observations on @xmath30 ultimately being needed to attain sufficiency . given that the magnitude of @xmath53 determines how many terms in ( [ expansion ] ) are required for the approximation to be accurate , we see that the sn ratio determines how well the set of summary statistics _ that would be sufficient _ for an observed ar(1 ) process ( with @xmath54 ) , namely , @xmath55 approximates the information content of the true set of sufficient statistics , that is , the full sample . if the sn ratio is large ( i.e. @xmath53 is small ) then using the set in ( [ ar1_summ_stats ] ) as summary statistics may produce a reasonable approximation to sufficiency .",
    "however , as the sn ratio declines ( and higher powers of @xmath53 can not be ignored as a consequence ) then this set deviates further and further from sufficiency .",
    "this same qualitative problem would also characterize any ssm nested in ( [ meas_gen ] ) and ( [ state_gen ] ) , with the only difference being that , in any particular case there would not necessarily be an analytical link between the sn ratio and the lack of sufficiency associated with any finite set of statistics calculated from the observations .",
    "the quest for an accurate abc technique in a state space setting - based on an arbitrary set of statistics - is thus not well - founded and this , in turn , motivates the search for asymptotic sufficiency via the mle .",
    "the asymptotic gaussianity of the mle for the parameters of ( [ meas_gen ] ) and ( [ state_gen ] ) ( under regularity ) implies that the mle  satisfies the factorization theorem and is thereby asymptotically sufficient for the parameters of that model .",
    "( see cox and hinkley , 1974 , chp . 9 for elucidation of this matter . )",
    "denoting the log - likelihood function by * *  * * @xmath56 , maximizing * *  * * @xmath56 with respect to @xmath28 yields @xmath57 , which could , in principle , be used to define @xmath58 in an abc algorithm . for large enough @xmath1",
    "the algorithm would produce draws from the exact posterior .",
    "indeed , in arguments that mirror those adopted by gallant and tauchen ( 1996 ) and gouriroux _ et al . _",
    "( 1993 ) for the emm and ii estimators respectively , gleim and pigorsch ( 2013 ) demonstrate that if @xmath58 is chosen to be the mle of an auxiliary model that ` nests ' the true model in some well - defined way , asymptotic sufficiency will still be achieved ; see also gouriroux and monfort ( 1995 ) on this point .    of course",
    ", if the ssm in question is such that the exact likelihood is accessible , the model is likely to be tractable enough to preclude the need for treatment via abc .",
    "further , as we allude to in the introduction , the quest for asymptotic sufficiency via a ( possibly large ) nesting auxiliary model conflicts with the quest for an accurate non - parametric estimate of the posterior using the abc draws ; a point that , to our knowledge , has not been noted in the literature .",
    "hence , in practice , the appropriate goal in the abc context is to define a _ parsimonious _",
    ", analytically tractable ( and computationally efficient ) model that _ approximates _ the ( generally intractable ) data generating process in ( [ meas_gen ] ) and ( [ state_gen ] ) as well as possible , and use that model as the basis for constructing a summary statistic within an abc algorithm .",
    "if the approximating model is ` accurate enough ' as a representation of the true model , such an approach will yield , via the abc algorithm , an estimate of the posterior distribution that is conditioned on a statistic that is ` close to ' being sufficient , at least for a large enough sample .    despite the loss of full ( asymptotic ) sufficiency associated with the use of an approximating model to generate the matching statistics in an abc algorithm , we show here that bayesian consistency will still be achieved , subject to certain regularity conditions .",
    "define the choice criterion in ( [ distance ] ) as @xmath59   ^{\\prime}\\mathbf{\\omega}\\left [   \\widehat{\\mathbf{\\beta } } ( \\mathbf{y)-}\\widehat{\\mathbf{\\beta}}(\\mathbf{z}^{i}\\mathbf{)}\\right ]   } \\leq\\varepsilon,\\label{dist_mle}\\ ] ] where @xmath60 is the mle of the parameter vector @xmath61 of the auxiliary model with log - likelihood function , @xmath62 , and @xmath63 is some positive definite matrix .",
    "the quadratic function under the square root essentially mimics the criterion used in the ii technique , in which case @xmath63 would assume the sandwich form of variance - covariance estimator - appropriate for when the auxiliary model does not coincide with the true model - and optimization with respect to the parameters of the latter is the goal .",
    "that is a multiple of the size of the empirical sample . ] in bayesian analyses , in which ( [ dist_mle ] ) is used to produce abc draws , @xmath63 may also be defined as the sandwich estimator ( drovandi and pettit , 2013 , and gleim and pigorsch , 2013 ) , or simply as the inverse of the ( estimated ) variance - covariance matrix of @xmath64 , evaluated at @xmath65 ( drovandi _ _  et al . , _ _ 2011 ) .",
    "however , in common with the frequentist proof of consistency of the ii estimator , bayesian consistency - whereby the posterior for @xmath28 is degenerate as @xmath66 at the true @xmath67 - is invariant to the choice of the ( positive definite ) @xmath68 the demonstration follows directly from the same arguments used to prove consistency of the ii estimator ( see , for e.g. gouriroux and monfort , 1996 , appendix 4a.1 ) , with the following regularity conditions required :    ( a1 ) : :    lim@xmath69 , uniformly in    @xmath61 , where    @xmath70 is a deterministic limit    function .",
    "( a2 ) : :    @xmath70 has a unique maximum    with respect to @xmath71    @xmath72    @xmath73 ( a3 ) : :    the equation @xmath74 admits a unique    solution in @xmath28 , for all    @xmath28 .    under these conditions it follows that@xmath75 hence , in the abc context , in which the generic parameter @xmath28 represents a draw @xmath76 from the prior , @xmath77 , we see that as @xmath78 the choice criterion in ( [ dist_mle ] ) approaches@xmath79 ^{\\prime}\\mathbf{\\omega}\\left [   \\mathbf{b}(\\mathbf{\\phi}_{0}\\mathbf{)-b}(\\mathbf{\\phi}^{i}\\mathbf{)}\\right ]   } \\leq\\varepsilon.\\ ] ] as @xmath23 , being the relevant addition limiting condition required in the abc setting , we see that irrespective of the form of @xmath63 , the only values of @xmath76 that will be selected and , hence , be used to construct an estimate of the posterior distribution , are values such that @xmath80 given the assumption of the uniqueness of the solution of @xmath81 for @xmath82  ( or @xmath83 ) , @xmath84 if and only if @xmath85 hence , abc produces draws that produce a degenerate distribution at the true parameter @xmath83 , as required by the bayesian consistency property .",
    "once again , this is despite the fact that asymptotic sufficiency will not be achieved in the typical case in which the approximating model is in error , a result that is analogous to the frequentist finding of consistency for the ii estimator , without full ( cramer rao ) efficiency obtaining .",
    "we conclude this section by citing related work in hidden markov models ( e.g. yildirim _ et al . , _ 2013 ; dean _ et al .",
    "_ , 2014 ) , in which abc principles that avoid summarization have been advocated . specifically , the difference between the observed data and the simulated pseudo - data is operated time step by time step , as in @xmath86 .",
    "this form of abc approximation also allows for the derivation of consistency properties ( in the number of observations ) of the abc estimates .",
    "in particular , using such a distance in the algorithm allows for the approximation to converge to the genuine posterior when the tolerance @xmath13 goes to zero .",
    "one problem with this approach , however , is that the acceptance rate decreases quickly with @xmath1 , unless @xmath13 is increasing with @xmath1 .",
    "( 2014 ) provide some solutions here , but within an observation - driven model context only .",
    "finally , looking at the application of this form of abc approximation in a particle mcmc ( pmcmc ) setting , jasra _ et al . _",
    "( 2013 ) and martin _ et al . _",
    "( 2014 ) establish convergence ( to the exact posterior ) , in connection with the alive particle filter ( le gland and oudjane , 2006 ) .      with large computational gains , @xmath58 in ( [ distance ] )",
    "can be defined using the score of the auxiliary model .",
    "that is , the score vector associated with the approximating model , when evaluated at the simulated data , and with @xmath65 substituted for @xmath61 , will be closer to zero the ` closer ' is the simulated data to the true .",
    "hence , the choice criterion in ( [ distance ] ) for an abc algorithm can be based on @xmath87 , where @xmath88 yielding@xmath89 ^{\\prime}\\mathbf{\\sigma}\\left [   \\mathbf{s}(\\mathbf{z}^{i};\\widehat{\\mathbf{\\beta}}(\\mathbf{y)})\\right ]   } \\leq\\varepsilon , \\label{dist_score}\\ ] ] where @xmath90 denotes an arbitrary positive definite weighting matrix .",
    "implementation of abc via ( [ dist_score ] ) is faster ( by many orders of magnitude ) than the approach based upon @xmath91 , due to the fact that maximization of the approximating model is required only once , in order to produce @xmath92 from the observed data @xmath93 all other calculations involve simply the _ _ evaluation  _ _ of @xmath94 at the simulated data , with a numerical differentiation technique invoked to specify @xmath95    once again in line with the proof of the consistency of the relevant frequentist ( emm ) estimator , the bayesian consistency result in section [ theory ] could be re - written in terms @xmath96 , upon the addition of a differentiability condition regarding @xmath97 and the assumption that @xmath98 is the unique solution to the limiting first - order condition,@xmath99 and the convergence is uniform in @xmath61 . in brief , given that @xmath100 , as @xmath78 the choice criterion in ( [ dist_score ] ) approaches @xmath101   ^{\\prime}\\mathbf{\\sigma}\\left [ \\partial l_{\\infty}(\\mathbf{\\phi}^{i};\\mathbf{b}(\\mathbf{\\phi}_{0}\\mathbf{)})/\\partial\\mathbf{\\beta}\\right ]   } \\leq\\varepsilon.\\ ] ] as @xmath23 , irrespective of the form of @xmath102 , the only values of @xmath76 that will be selected via abc are values such that @xmath84 , which , given assumption ( a3 ) , implies @xmath103    hence , bayesian consistency is maintained through the use of the score .",
    "however , a remaining pertinent question concerns the impact on sufficiency ( or , more precisely , on _ the proximity to asymptotic sufficiency _ ) of the use of the score instead of the mle . in practical terms",
    "this question can be re - phrased as : does the selection criterion based on @xmath94  yield identical draws of @xmath28 to those yielded by the selection criterion based on @xmath64 ? if the answer is yes then , unambiguously , for large enough @xmath1 and for @xmath104 ,  the score- and mle - based abc criteria will yield equivalent estimates of the exact posterior , with the accuracy of those ( equivalent ) estimates dependent , of course , on the nature of the auxiliary model itself .    for any auxiliary model ( satisfying identification and regularity conditions ) with unknown parameter vector @xmath61 , we expand the score function in ( [ score ] ) , evaluated at @xmath65 , around the point",
    "@xmath105 ( with scaling via @xmath106 having been introduced at the outset in the definition of the score in ( [ score ] ) ) , @xmath107   = \\mathbf{d}\\left [ \\widehat{\\mathbf{\\beta}}(\\mathbf{y)}-\\widehat{\\mathbf{\\beta}}(\\mathbf{z}^{i}\\mathbf{)}\\right ]   , \\ ] ] where@xmath108 and @xmath109 denotes an ( unknown ) intermediate value between @xmath65 and @xmath105 .",
    "hence , the ( scaled ) criterion in ( [ dist_score ] ) becomes@xmath110   ^{\\prime}\\mathbf{\\sigma}\\left [   \\mathbf{s}(\\mathbf{z}^{i};\\widehat{\\mathbf{\\beta}}(\\mathbf{y)})\\right ]   } \\nonumber\\\\ & = \\sqrt{\\left [   \\widehat{\\mathbf{\\beta}}(\\mathbf{y)}-\\widehat{\\mathbf{\\beta}}(\\mathbf{z}^{i}\\mathbf{)}\\right ]   ^{\\prime}\\mathbf{d}^{\\prime}\\mathbf{\\sigma d}\\left [   \\widehat{\\mathbf{\\beta}}(\\mathbf{y)}-\\widehat{\\mathbf{\\beta}}(\\mathbf{z}^{i}\\mathbf{)}\\right ]   ^{\\prime}}\\leq\\varepsilon.\\label{new_score}\\ ] ] subject to standard conditions regarding the second derivatives of the auxiliary model , the matrix @xmath111 in ( [ d ] ) will be of full rank and as @xmath78 , @xmath112 some positive definite matrix ( given the positive definiteness of @xmath90 ) that is some function of @xmath113 hence , whilst for any @xmath114 , the presence of @xmath111 affects selection , as it is a function of the drawn value @xmath76 ( through @xmath115 ) , as @xmath23 , @xmath76 will be selected via ( [ new_score ] ) if and only if @xmath65 and @xmath105 are equal .",
    "similarly , irrespective of the form of the ( positive definite ) weighting matrix in ( [ dist_mle ] ) , the mle criterion will produce these same selections .",
    "this result pertains no matter what the dimension of @xmath61 relative to @xmath28 , i.e. no matter whether the true parameters are exactly or over - identified by the parameters of the auxiliary model .",
    "this result thus goes beyond the comparable result regarding the ii / emm estimators ( see , for e.g. gouriroux and monfort , 1996 ) , in that the equivalence is independent of the form of weighting matrix used _ and _ the form of identification that prevails .    of course in practice ,",
    "abc is implemented with @xmath116 at which point the two abc criteria will produce different draws .",
    "however , for the models entertained in this paper , preliminary investigation has assured us that the difference between the abc estimates of the posteriors yielded by the alternative criteria is negligible for small enough @xmath117  hence , we proceed to operate solely with the score - based approach as the computationally feasible method of extracting approximate asymptotic sufficiency in the state space setting .",
    "the actual selection and optimization of the tolerance level , @xmath13 , has been the subject of intense scrutiny in the recent years ( see , for example , marin _ et al_. , 2011 for a detailed survey ) .",
    "what appears to be the most fruitful path to the calibration of the tolerance is to firmly set it within the realm of non - parametric statistics ( blum and franois , 2010 ) as this provides proper convergence rates for the tolerance ( rates that differ between standard and noisy abc ; see fearnhead and prangle , 2012 ) and shows that the optimal value stays away from zero for a given sample size .",
    "in addition , the practical constraints imposed by finite computing time and the necessity to produce an abc sample of reasonable length lead us to follow the recommendations found in biau _",
    "( 2012 ) , namely to analyze the abc approximation as a k - nearest neighbour technique and to exploit this perspective to derive a practical value for the tolerance .",
    "an abc algorithm induces two forms of approximation error .",
    "firstly , and most fundamentally , the use of a vector of summary statistics @xmath118 to define the selection criterion in ( [ distance ] ) means that a simulation - based estimate of the posterior of interest is the outcome of the exercise .",
    "only if @xmath119 is sufficient for @xmath28 is @xmath120 equivalent to the exact posterior @xmath121 otherwise the exact posterior is necessarily estimated with error because of the analytical difference between the exact density and the _ partial _ posterior density @xmath122 .",
    "secondly , the partial posterior density itself , @xmath123 will be estimated with simulation error . critically , as highlighted by blum ( 2010b )",
    ", the accuracy of the simulation - based estimate of @xmath120 will be less , all other things given , the larger the dimension of @xmath119 .",
    "this ` curse of dimensionality ' obtains even when the parameter @xmath28 is a scalar , and relates solely to the dimension of @xmath124 as elaborated on further by nott _",
    "( 2014 ) , this problem is exacerbated as the dimension of @xmath28 itself increases , firstly because an increase in the dimension of @xmath28 brings with it a concurrent need for an increase in the dimension of @xmath119 and , secondly , because the need to estimate a multi - dimensional density ( for @xmath28 ) brings with it its own problems related to dimension .    as a potential solution to the inaccuracy induced by the dimensionality of the problem , nott _",
    "( 2014 ) suggest allocating ( via certain criteria ) a subset of the full set of summary statistics to each element of @xmath28 , @xmath125 @xmath126 using kernel density techniques to estimate each marginal density , @xmath127 and then using standard techniques to retrieve a more accurate estimate of the joint posterior , @xmath128 if required .",
    "however , the remaining problem associated with the ( possibly still high ) dimension of each @xmath129 , in addition to the very problem of defining an appropriate set @xmath129 for each @xmath130 , remains unresolved .",
    "( 2013 ) for further elaboration on the dimensionality issue in abc and a review of current approaches for dealing with the problem .",
    "the principle advocated in this paper is to exploit the information content in the mle of the parameters of an auxiliary model , @xmath61 , to yield ` approximate ' asymptotic sufficiency for @xmath28 . within this framework",
    ", the dimension of @xmath61 determines the dimension of @xmath119 and the curse of dimensionality thus prevails for high - dimensional @xmath131 however , in this case a solution is available , at least when the dimensions of @xmath61 and @xmath132 are equivalent and there is a one - to - one match between the elements of the two parameter vectors .",
    "this is clearly so for the two cases tackled in this paper . in the linear gaussian model investigated as case 1 )",
    "the auxiliary model coincides exactly with the true model , in which case @xmath133 in the stochastic volatility ssm investigated as case 2 ) , we produce an auxiliary model by discretizing the latent diffusion ( and evaluating the resultant likelihood via the aukf ) ; hence , @xmath134 @xmath135 and there is a natural one - to - one mapping between the parameters of the ( true ) continuous time and ( approximating ) discretized models . in both of these examples then , marginalizing the auxiliary likelihood function with respect to all parameters other than @xmath136 and then producing the score of this function with respect to @xmath136 ( as evaluated at the marginal mle from the observed data , @xmath137 ) , yields , by construction , an obvious _ scalar _",
    "statistic for use in selecting draws of @xmath130 and , hence , a method for estimating @xmath138 if the marginal posteriors only are of interest , then all @xmath29 marginals can be estimated in this way , with @xmath29 applications of @xmath139-dimensional integration required at each step within abc to produce the relevant score statistics .",
    "importantly , we do not claim here that the ` proximity ' to sufficiency ( for @xmath28 ) of the vector statistic @xmath40 , translates into an equivalent relationship between the score of the marginalized ( auxiliary ) likelihood function and the corresponding scalar parameter , nor that the associated product density is coherent with a joint probability distribution . if the joint posterior ( of the full vector @xmath28 ) is of particular interest , the sort of techniques advocated by nott et al .",
    "( 2014 ) , amongst others , can be used to yield joint inference from the estimated marginals .    in section [ assess ]",
    "we explore the benefits of marginalization , in addition to the increase in accuracy yielded by using a score - based abc method ( either joint or marginal ) rather than an abc algorithm based on a more _ ad hoc _ choice of summary statistics . however , prior to that we provide details in the following section of the form of auxiliary model advocated for the non - linear ssm case .",
    "when the ssm defined in ( [ meas_gen ] ) and ( [ state_gen ] ) is analytically intractable , an approximating model is needed to drive the score - based abc technique . in the canonical non - linear example",
    "being emphasized in the paper , in which either @xmath30 or @xmath31 ( or both ) is driven by a continuous time process , this approximation begins with the specification of a discretized version of ( [ meas_gen ] ) and ( [ state_gen ] ) , expressed generically using a regression formulation as:@xmath140 for @xmath141 , where the @xmath142 and @xmath143 are assumed to be sequences of @xmath144  random variables .",
    "this formulation is general enough to include , for example , independent random jump components in either the measurement or state equations ( subsumed under @xmath145 and @xmath146 respectively ) , but does exclude cases where the nature of the model is such that a regression formulation in discrete time is not feasible .",
    "we note here that in producing ( [ discrete_meas ] ) and ( [ discrete_state ] ) either the observation or the state variable , or both , may need to be transformed ; however , for notational simplicity we continue to use the same symbols , @xmath31 and @xmath30 , as are used to denote the variables in the true model in ( [ meas_gen ] ) and ( [ state_gen ] ) .",
    "the nature of the discretization affects the functional form of @xmath147 and @xmath148 in section [ hest ] we illustrate the method using a model in which the true model for @xmath30 is already expressed in discrete time ( i.e. there is no discretization error via the measurement process ) and in which a ( first - order ) euler process is initially used to approximate a square root diffusion model for the state .    the log - likelihood function associated with the approximate model in ( [ discrete_meas ] ) and ( [ discrete_state ] ) is defined by @xmath149 where @xmath150 only if the approximate model is linear and gaussian or the state variable is discrete on a finite support , are the components used to define ( [ approx_like ] ) available in closed form .",
    "given the nature of the problem we are tackling here , namely one in which ( [ discrete_meas ] ) and ( [ discrete_state ] ) are produced as discrete approximations to a continuous time model ( or , indeed , one in which ( [ discrete_meas ] ) and ( [ discrete_state ] ) represent an initial discrete - time formulation that is non - linear and/or non - gaussian ) , it is reasonable to assume that ( [ approx_like ] ) can not be evaluated exactly .",
    "whilst several methods ( see , e.g. simon , 2006 ) are available to approximate @xmath151 including , indeed , simulation - based methods such as particle filtering and smc , the fact that this computation is to be _ embedded _ within the abc algorithm makes it essential that the technique is both fast and numerically stable . _ _  _ _ the aukf satisfies these criteria and , hence , is our method of choice for this illustration .    in brief , the unscented kalman filter ( ukf ) is based on the theory of unscented transformations , which is a method for calculating the moments of a non - linear transformation of a random variable .",
    "it involves selecting a set of points on the support of the random variable , called _ sigma points _ , according to a predetermined and deterministic criterion .",
    "these sigma points yield , in turn , a ` cloud ' of transformed points through the non - linear function , which are then used to produce approximations to the moments of the state variable @xmath31 used in implementing the kf , via simple weighted sums of the transformed sigma points .",
    "( see haykin , 2001 , chapter 7 , for an excellent introduction . )",
    "the _ _ augmented v__ersion of the ukf  - the aukf - generalizes the filter to the case where the state and measurement errors are non - additive , applying the principles of unscented transformations to the _ augmented _ state vector @xmath152 the computational burden of the filter is thus minimal - comprising the calculation of updated sigma points for the time varying state @xmath31 at each @xmath36 , the computation of the relevant means and variances ( for both @xmath31 and @xmath30 ) using simple weighted sums , and the use of the usual kf up - dating equations .",
    "details of the specification of the sigma points , plus the steps involved in estimating ( [ approx_like ] ) are provided in the appendix .    in implementing the score - based abc method in a non - linear continuous time setting , there are two aspects of the approximation used therein to consider : 1 ) the accuracy of the discretization ; and 2 ) the accuracy with which the likelihood function of the approximate ( discretized ) model is evaluated via the aukf and , hence , the accuracy of the resultant estimate of the mle . in addressing the first aspect ,",
    "one has access to the existing literature in which various discretized versions of continuous time models are derived , to different orders of accuracy . with regard to the accuracy of the aukf evaluation of the likelihood function",
    ", we make reference to relevant results ( see , e.g. haykin , 2001 ) that document the higher - order accuracy ( relative , say , to the extended kf ) of the aukf - based estimates of the mean and variance of the filtered and predictive ( for both state and observed ) distributions",
    ". we also advocate here using any transformation of the measurement ( or state ) equation that renders the gaussian approximations invoked by the aukf more likely to be accurate .",
    "however , beyond that , the accuracy of the gaussian assumption embedded in the aukf - based likelihood estimate is case - specific . on a first - order euler discretization only . ]",
    "we now undertake a numerical exercise in which the accuracy of the score - based methods of abc ( both joint and marginal ) is compared with that of abc methods based on a set of summary statistics that are chosen without reference to a model .",
    "we conduct this exercise firstly within the context of the linear gaussian model , in which case the exact score is accessible via the kf .",
    "the results here thus provide evidence on two points of interest : 1 ) whether or not accuracy can be increased by accessing the asymptotic sufficiency of the ( exact ) mle in an abc treatment of a state space model ( compared to the use of other summary statistics ) ; and 2 ) whether or not the curse of dimensionality can be obviated via marginalization , or integration , of the exact likelihood .    in section [ hest ]",
    "we then assess accuracy in the typical setting in which an approximating model is used to generate the score .",
    "the square root volatility model is used as the example , with the approximate score produced by evaluating the likelihood function of a discretized version of that model using the aukf . whilst recognizing that the results relate to one particular model and approximation thereof ,",
    "they do , nevertheless , serve to illustrate that score - based methods can dominate the summary statistic - based techniques ( overall ) , even when the approximating model used is not particularly accurate .        in this section",
    "we simulate a sample of size @xmath153 from the linear gaussian ( lg ) model in ( [ measlg ] ) and ( [ statelg ] ) , based on the parameter settings : @xmath154 , @xmath155 and @xmath156 , with the three - dimensional parameter @xmath157 to be estimated . the value of the measurement error variance , @xmath158 , is set in order to fix the sn ratio .",
    "we compare the performance of the ( joint and marginal ) score - based techniques with that of more conventional abc methods based on summary statistics that may be deemed to be a sensible choice in this setting . given the relationship between the lg model and an observable ar(1 ) process , it seems sensible to propose a set of summary statistics that are sufficient for the latter , as given in ( [ ar1_summ_stats ] ) .",
    "two forms of distances are used .",
    "firstly , we apply the conventional euclidean distance , with each summary statistic also weighted by the inverse of the variance of the values of the statistic across the abc draws .",
    "that is , we define @xmath159^{1/2}\\label{euclid}\\ ] ] for abc iteration @xmath160 , where @xmath161 is the variance ( across @xmath162 ) of the @xmath163 , and @xmath164 is the observed value of the @xmath165 statistic .",
    "secondly , we use a distance measure proposed in fearnhead and prangle ( 2012 ) which , as made explicit in blum _",
    "( 2013 ) , is a form of dimension reduction method .",
    "we explain this briefly as follows .",
    "given the vector of observations @xmath43 , the set of summary statistics in ( [ ar1_summ_stats ] ) are used to produce an estimate of @xmath166 , @xmath167 which , in turn , is used as the summary statistic in a subsequent abc algorithm .",
    "the steps of the procedure ( as modified for this context ) described for selection of the scalar parameter @xmath130 , @xmath167 are as follows :    1 .",
    "simulate @xmath168 , @xmath7 , from @xmath169 and , subsequently , simulate @xmath170 from ( [ vol ] ) using the exact transitions , and pseudo data , @xmath115 using the conditional gaussian form of @xmath171 2 .   for @xmath115 , @xmath7 , calculate @xmath172   ^{\\prime}\\label{fp_stats}\\ ] ] 3 .",
    "define @xmath173 @xmath174{cccc}1 & 1 & \\cdots & 1\\\\ \\mathbf{s}^{1 } & \\mathbf{s}^{2 } & \\cdots & \\mathbf{s}^{r}\\end{array } \\right ]   ^{\\prime}$ ] and @xmath175+\\mathbf{e}=\\mathbf{x}\\left [ \\begin{array } [ c]{c}\\alpha\\\\ \\mathbf{\\beta}\\end{array } \\right ]   + \\mathbf{e,}\\ ] ] where @xmath176   $ ] and @xmath61 is of dimension @xmath177 4 .",
    "use ols to estimate @xmath178 $ ] * *  * * as @xmath179=\\widehat{\\alpha}+\\left [ \\begin{array } [ c]{cccc}\\mathbf{s}^{1 } & \\mathbf{s}^{2 } & \\cdots & \\mathbf{s}^{r}\\end{array } \\right ]   ^{\\prime}\\widehat{\\mathbf{\\beta}}$ ] 5 .",
    "define:@xmath180 where @xmath181 denotes the vector of summary statistics in ( [ fp_stats ] ) calculated from the vector of observed returns , and use:@xmath182 as the selection criterion for @xmath130 at each iteration @xmath162 .",
    "the joint score - based method uses the distance measure in ( [ dist_score ] ) , but with the score in this case computed from the _ exact _ model , evaluated using the kf .",
    "the weighting matrix @xmath90 is set equal to the hessian - based estimate of the covariance matrix of the ( joint )  mle estimator of @xmath183 , evaluated at the mle computed from the observed data , @xmath184 the marginal score - based method for estimating the marginal posterior for the @xmath185 element of @xmath28 , @xmath130 , @xmath167 is based on the distance@xmath186 where@xmath187 and @xmath188 is produced by integrating ( numerically ) the exact likelihood function ( evaluated via the kf ) with respect to all parameters other than @xmath130 , and taking the logarithm .",
    "we produce results that compare the performance of the four different methods : the joint score - based abc ( ` abc - joint score ' in all figures ) ; the marginal score - based abc ( ` abc - marg score ' ) ; the summary statistic - based abc using the euclidean metric in ( [ euclid ] ) ( ` abc - summ stats ' ) ; and the approach of fearnhead and prangle ( 2012 ) based on the metric in ( [ fp ] ) ( ` abc - fp ' ) .",
    "marginal density estimates are produced initially for a single run of abc , based on 50,000 replications of the accept / reject algorithm detailed in section 2.1 , and with @xmath13 defined as the 5th percentile of the 50,000 draws .",
    "the true data is generated from a process in which the sn ratio is high ( i.e. @xmath189   /\\sigma_{e}^{2}=20 $ ] ) , with the true marginal posteriors computed by normalizing the likelihood function evaluated using the kf ( and multiplied by a uniform prior ) , then marginalizing using deterministic integration over a very fine grid for @xmath190 the score - based methods also use the exact likelihood function to compute the score . all three sets of marginal posteriors , for @xmath191 , @xmath192 and @xmath193 , are produced in figure [ fig_lg ] , panels a , b and c respectively .",
    "we then summarize the results for 100 replications of abc , using box plots , in figures 2 to 4 .",
    "we produce estimates of the @xmath194 , @xmath195 , @xmath196 , @xmath197 and @xmath198 percentiles of each true ( kf - based ) posterior density , with the exact percentile represented by the horizontal dotted line . the short - hand notation used to denote the form of abc method corresponds to that used in figure 1 .",
    "lg_tri_paper_graph__1.pdf[fig_lg ]    figure [ fig_lg ] , panels a to c , highlight graphically  - for a single abc run - the performance of the four abc methods in reproducing the true marginal posteriors .",
    "the two most notable features of all three plots are : i ) the remarkable accuracy of the marginal score approach ; and ii ) the very poor performance of the summary statistic approach based on the euclidean metric .",
    "these features are replicated across the 100 runs of abc , as evidenced by figures [ bp_rho_high ] to [ bp_sig_high ] .",
    "for all three parameters , the marginal score approach produces percentile estimates that are extremely accurate in terms of both mean location and spread . for the parameter @xmath191",
    "the joint score approach is the next most accurate technique , followed by the fp method . for the parameter @xmath193",
    "there is little to choose between the latter two methods , both of which produce quite accurate estimates of the true percentiles , although both being still less accurate than the marginal score technique . for @xmath192",
    "the fp method tends to outperform the joint score approach , but with neither method competing with the accuracy yielded by the marginal score .",
    "for all three parameters , the summary statistic approach based on the euclidean metric produces the poorest results overall , despite the high sn ratio .",
    "k_box_hisn_newsv_5p__2.pdf[bp_rho_high ]    d_box_hisn_newsv_5p__3.pdf[bp_d_high ]    s_box_hisn_newsv_5p__4.pdf[bp_sig_high ]    further results ( not documented here , for reasons of space ) demonstrate that decreasing the sn ratio has no qualitative effective on the performance of the score - based results , including the marked accuracy of the marginal score method .",
    "this robustness of the score methods to the sn ratio is to be anticipated , give that the likelihood function for the full state space model has been used to generate the matching statistics .",
    "the impact of the change in the sn ratio on the summary statistic - based methods is not uniform , with there certainly being no clear tendency for the results the worsen .",
    "this suggests that the accuracy with which @xmath199 itself is estimated ( and , hence , the issue of dimensionality ) , rather than the relationship between @xmath199 and @xmath200 ( and , hence the ` closeness ' of @xmath119 to sufficiency ) , remains a dominant influence on final accuracy , no matter what the sn value .",
    "given the critical role played by volatility in asset pricing , portfolio management and the calculation of risk measures , a large segment of the empirical finance literature has been devoted to the construction and analysis of volatility models .",
    "three decades of empirical studies have demonstrated that the _ constant _ volatility feature of a geometric brownian motion process for an asset price is inconsistent with both the observed time - variation in return volatility and the non - gaussian characteristics of empirical distributions of returns ; see bollerslev , chou and kroner ( 1992 ) for a review .",
    "empirical regularities documented in the option pricing literature , most notably implied volatility ` smiles ' , are also viewed as evidence that asset prices deviate from the geometric brownian motion assumption underlying the black and scholes ( 1973 ) option price ; see , for example , bakshi _ et al . _",
    "( 1997 ) and lim _ et al . _",
    "( 2005 ) , or garcia _ et al . _",
    "( 2010 ) for a recent review .    in response to these",
    "now well - established empirical findings , many alternative time - varying volatility models have been proposed , with continuous time stochastic volatility ( sv ) models - often augmented by random jump processes - being particularly prominent of late .",
    "this focus on the latter form of models is due , in part , to the availability of ( semi- ) closed - form option prices , with variants of the sv model of heston ( 1993 ) becoming the workhorse of the empirical option pricing literature ( e.g. eraker , 2004 , forbes _",
    "_ 2007 , broadie _ et al .",
    "_ 2007 , johannes _ et al .",
    "_ 2009 ) , and mcmc and particle filtering techniques the typical numerical methods of choice .",
    "it is of interest , therefore , to assess the performance of the proposed abc method in the context of this form of model .",
    "we adopt here the simplest version of heston ` square root ' sv model , given by:@xmath201 where @xmath202 denotes the ( demeaned ) logarithmic return on an asset price over period ( day say ) @xmath36 , @xmath203 , @xmath204 is a standard wiener process , and the restriction @xmath205 ensures the positivity of the stochastic variance @xmath206 @xmath207 in our previous generic notation ) . for @xmath208 , @xmath206 is mean reverting and as @xmath209 the variance approaches a steady state gamma distribution , with @xmath210=\\delta/\\alpha$ ] and @xmath211 the transition density for @xmath206 , conditional on @xmath212 , is@xmath213 where @xmath214 , @xmath215 , @xmath216 , @xmath217 , and @xmath218 is the modified bessel function of the first kind of order @xmath219 the conditional distribution function is non - central chi - square , @xmath220 , with @xmath221 degrees of freedom and non - centrality parameter @xmath222 . the discrete - time model for @xmath202 in ( [ return ] )",
    "can be viewed as a discretized version of a diffusion process for returns ( or returns can be viewed as being _",
    "inherently _ discretely observed ) whilst we retain the diffusion model for the latent variance .",
    "that is , we eschew the discretization of the variance process that would typically be used , with simulation of @xmath206 in step 2 of the abc algorithm occurring exactly , through the treatment of the @xmath220 random variable as a composition of central @xmath223 and @xmath224 variables .    for the purpose of this illustration we set the parameters in ( [ vol ] ) to values that produce simulated values of both @xmath202 and @xmath206 that match the characteristics of ( respectively ) daily returns and daily values of realized volatility ( constructed from 5 minute returns ) for the s&p500 stock index over the 2003 - 2004 period ,",
    "namely@xmath225 this relatively calm period in the stock market is deliberately chosen as a reference point , as the inclusion of price and volatility jumps , and/or a non - gaussian conditional distribution in the model would be an empirical necessity for any more volatile period , such as that witnessed during the recent 2008/2009 financial crisis .    in order to implement the score - based abc method , using the aukf algorithm to evaluate an auxiliary likelihood ,",
    "we invoke the following discretization , based on ( an exact ) transformation of the measurement equation and an euler approximation of the state equation,@xmath226 where @xmath146 is treated as a truncated gaussian variable with lower bound,@xmath227 directing the reader to the appendix for the detailed outline of the aukf approach , we note that sigma points that span the support of @xmath145 are defined by calculating @xmath228 and @xmath229 using deterministic integration and the closed form of @xmath230 with the specification of @xmath231 adopted for convenience .",
    "those for @xmath206 , @xmath232 are defined as:@xmath233 where @xmath234 and @xmath235 respectively denote the mean and variance of the relevant distribution of @xmath206 ( marginal , filtered or predictive , depending on the particular step in the aukf algorithm ) .",
    "the sigma points for ( [ v_trunc ] ) are then defined using the mean and variance of the truncated normal distribution , with the value of @xmath212 in ( [ v_trunc ] ) represented using the relevant sigma point for @xmath212 , and ( with reference to the appendix ) @xmath236 specified .",
    "in order to evaluate the accuracy of the estimate of the posterior produced using the abc method , we produce the exact joint posterior distribution for @xmath157 via the deterministic non - linear filtering method of ng _ et al . _",
    "( 2013 ) . in brief",
    ", this method represents the recursive filtering and prediction distributions used to define the likelihood function as the numerical solutions of integrals defined over the support of @xmath145 in ( [ trans ] ) , with deterministic integration used to evaluate the relevant integrals , and the _ exact _ transitions in ( [ non - central ] ) used in the specification of the filtering and up - dating steps .",
    "whilst lacking the general applicability of the abc - based method proposed here , this deterministic filtering method is ideal for the particular model used in this illustration , and can be viewed as producing a very accurate estimate of the exact density , without any of the simulation error that would be associated with an mcmc - based comparator , for instance .",
    "we refer the reader to ng _ et al .",
    "_ for more details of the technique ; see also kitagawa ( 1987 ) . in the current",
    "setting , in which @xmath145 is specifed parametrically , the known form of the distribution of @xmath145 is used directly in the evaluation of the relevant integrals .",
    "we refer the reader to section 2.2 . of that paper for a full description of the algorithm .",
    "preliminary experimentation with the number of grid points used in the deterministic integration was undertaken in order to ensure that the resulting estimate of the likelihood function / posterior stabilized , with 100 grid points underlying the final results documented here . ]",
    "the likelihood function , evaluated via this method , is then multiplied by a uniform prior that imposes the restrictions : @xmath237 @xmath238 , @xmath239 and @xmath205 .",
    "the three marginal posteriors are then produced via deterministic numerical integration ( over the parameter space ) , with a very fine grid on @xmath28 being used to ensure accuracy .",
    "we compare the auxiliary model - based abc technique with abc approaches based on summary statistics , as discussed in the linear gaussian context in section [ lg ] . for want of a better choice we use the vector of statistics given attention therein , as well as the two alternative distance measures described there .",
    "we also compute marginal posterior densities estimated using the aukf - based approximation ( of the likelihood ) itself . that is , using the aukf to evaluate the likelihood function associated with the discretized model in ( [ trans ] ) and ( [ v_state ] ) and normalizing ( using a uniform prior ) produces an approximation of the posterior which can , in principle , be invoked as an approximation in its own right , independently of its subsequent use as a score generator with an abc algorithm . finally , we compute the marginal posteriors ( again , based on a uniform prior ) using the likelihood function of the euler approximation in ( [ trans ] ) and ( [ v_state ] ) evaluated using the ng",
    "_ et al . _",
    "( 2013 ) filtering method , with gaussian transitions used in the filtering and up - dating steps .",
    "when normalized , this density can be viewed as the quantity that a typical mcmc scheme ( as based on the equivalent prior ) would be targeting , given that the tractability of gaussian approximations to the transitions would typically be exploited in structuring an mcmc algorithm .    as a concluding note on computational matters , we re - iterate that the time taken to evaluate the aukf - based approximate likelihood function at any point in the parameter space is roughly comparable to that required for kf evaluation , thus rendering it a feasible method to be inserted within the abc algorithm .",
    "in contrast , evaluation of the euler - based likelihood via the ng _ et al . _ ( 2013 ) technique , whilst producing , in the main , ( as will be seen in the following section ) a more accurate estimate of the exact posterior than the aukf method , is many orders of magnitude slower and , hence , simply infeasible as a score generator within abc .      in order to abstract initially from the impact of dimensionality on the abc methods , we first report results for each single parameter of the heston model , keeping the remaining two parameters fixed at their true values .",
    "three abc - based estimates of the relevant exact ( univariate ) posterior , invoking a uniform prior , are produced in this instance .",
    "three matching statistics are used , respectively : 1 ) the ( uni - dimensional ) auxiliary score based on the approximating model ( abc - score ) ; 2 ) the summary statistics in ( [ ar1_summ_stats ] ) , matched via the euclidean distance measure in ( [ euclid ] ) ( abc - summ stats ) ; and 3 ) the summary statistics in ( [ ar1_summ_stats ] ) , matched via the fp distance measure in ( [ fp ] ) ( abc - fp ) .",
    "we produce representative posterior ( estimates ) in each case , to give some visual idea of the accuracy ( or otherwise ) that is achievable via the abc methods .",
    "we then summarize accuracy by reporting the average ( over the 100 runs ) of the root mean squared error ( rmse ) of each abc - based estimate of the exact posterior for a given parameter , computed as:@xmath240 where @xmath241 is the ordinate of the abc density estimate and @xmath242 the ordinate of the exact posterior density , at the @xmath243 grid - point used to produce the plots .",
    "all single parameter results are documented in panel a of table 1 .",
    "we also tabulate there , as benchmarks of a sort , the rmses associated with the ( one - off ) aukf- and euler - based approximations of each univariate density .",
    "sq_uni_paper_graph__5.pdf[sq_fig ]    figure [ sq_fig ] , panel a reproduces the exact posterior of ( the single unknown parameter ) @xmath191 , the posteriors associated with the aukf- and euler - based approximations , and the three abc - based estimates . as is clear , the aukf - based approximation is reasonably inaccurate , in terms of replicating the location and shape of the exact posterior - an observation that is interesting in its own right , given the potential for such a simple and computationally efficient approximation method to be used to evaluate likelihood functions ( and posterior distributions ) in non - linear state space models such as the one under consideration .",
    "however , once the approximation is embedded within an abc scheme , in the manner described in section [ model ] , the situation is altogether different , with the ( pink ) dotted line ( denoted by ` abc - score ' in the key ) providing a remarkably accurate estimate of the exact posterior , using only 50,000 replications of the simplest rejection - based abc algorithm , and fifteen minutes of computing time on a desktop computer .",
    "it is worth noting that the abc - based estimate is also more accurate in this case than the euler approximation , where we highlight , once again , that production of the latter still requires the application of the much more computationally burdensome non - linear filtering method .",
    "most notably , the abc method based on the summary statistics , combined using a euclidean distance measure , performs very badly , although the dimensional reduction technique of fearnhead and prangle ( 2012 ) , applied to this same set of summary statistics , yields a reasonable estimate of the exact posterior in this instance .",
    "comparable graphs are produced for the single parameters @xmath192 and @xmath193 in panels b and c respectively of figure [ sq_fig ] , with the remaining pairs of parameters ( @xmath191 and @xmath193 , and @xmath191 and @xmath192 respectively ) held fixed at their true values . in the case of @xmath192 ,",
    "the score - based method arguably provides the best representation of the shape of the exact posterior , despite being slightly inaccurate in terms of location .",
    "the fearnhead and prangle ( 2012 ) method also provides a reasonable estimate , whilst the summary statistic approach using the euclidean distance , once again performs very poorly . for the parameter @xmath193 ,",
    "_ only _ the score based method yields a density with a well - defined shape , with the two summary statistic - based techniques essentially producing uniform densities that reflect little more than the restricted support imposed on the parameter draws .",
    "interestingly , the euler approximation itself provides a quite poor representation of the exact marginal , a result which has not , as far as we know , been remarked upon in the literature , given that a typical mcmc scheme ( as noted above ) would in fact be targeting the euler density itself , as the best representation of the true model - the exact posterior remaining unaccessed due to the difficulty of devising an effective mcmc scheme which uses the exact transitions . for neither @xmath192 nor @xmath193 is the aukf approximation _ itself _ particularly accurate , despite the fact that it respects the non - linearity in the true state space model .",
    "the rmse results recorded in panel a of table 1 confirm the qualitative nature of the single - run graphical results . for @xmath192 and @xmath193 ,",
    "all three abc - based estimates are seen to produce lower rmse values ( sometimes an order of magnitude lower ) than the aukf approximation , indicating that _ any _ of the abc procedures would yield gains over the use of the unscented filtering method itself . for @xmath191 ,",
    "the aukf approximation is better than that of the summary statistic - based abc estimate , but in part because the latter is so poor . for @xmath191 and @xmath193 ,",
    "the score - based abc method is the most accurate , and is most notably _ very _ precise for the case of the persistence parameter @xmath191 . for the parameter @xmath192 the fp method is the most accurate according to this measure although , as indicated by the nature of the graphs in panel b of figure [ sq_fig ] , this result does tend to understate the ability of the score - based method to capture the basic shape of the exact posterior . for @xmath191 and @xmath192 ,",
    "the ( euclidean ) summary - statistic method is an order of magnitude more inaccurate than the other two abc methods , whilst also exhibiting no ability to identify the shape of the true posterior in the case of @xmath244 once again as is consistent with the graphs in figure [ sq_fig ] , the euler approximation for @xmath191 is reasonably accurate , but not as accurate as the score - based abc estimate . for @xmath192 and @xmath245 the euler approximation , whilst being more accurate that the aukf approximation , is dominated by all three abc estimates .",
    "[ table1 ]    table 1 : rmse of an estimated marginal and the exact marginal : average rmse value over multiple runs of abc using 50,000 replications .",
    "` score ' refers to the abc method based on the score of the aukf model ; ` ss ' refers to the abc method based on a euclidean distance for the summary statistics in ( [ ar1_summ_stats ] ) ; ` fp ' refers to the fearnhead and prangle abc method , based on the summary statistics in ( [ ar1_summ_stats ] ) . for the single parameter case ,",
    "the ( single ) score method is documented in the row denoted by ` abc - marginal score ' , whilst in the multi - parameter case , there are results for both the joint and marginal score methods .",
    "the rmse of the aufk and euler approximations ( computed once only , using the observed data ) are recorded as benchmarks , in the top two rows of each panel . for the single and dual parameter cases ,",
    "100 runs of abc were used to produce the results , whilst for the three parameter case , 50 runs were used . the smallest rmse figure in each column",
    "is highlighted in bold .",
    "[ c]lllc|l|l|ll|lll & & & & + & & & & + & @xmath191 & @xmath192 & @xmath193 & @xmath191 & @xmath193 & @xmath191 & @xmath192 & @xmath191 & @xmath192 & @xmath193 + approximate density & & & & & & & & & & + & & & & & & & & & & + aukf & & & 0.0935 & 0.0529 & 0.0370 & 0.0185 & 0.0798 & 0.0201 & 0.0862 & 0.0287 + euler & & 0.0434 & 0.0664 & 0.0072 & 0.0308 & 0.0175 & 0.0818 & 0.0120 & 0.0459 & 0.0242 + abc - joint score & & & - & 0.0054 & * 0.0217 * & 0.0101 & 0.0441 & * 0.0063 * & * 0.0124 * & * 0.0166 * + abc - marginal score & & & * 0.0392 * & * 0.0045 * & 0.0219 & * 0.0048 * & 0.0480 & 0.0085 & 0.0381 & 0.0167 + abc - ss & & & 0.0427 & 0.0310 & 0.0234 & 0.0119 & * 0.0312 * & 0.0109 & 0.0389 & 0.0170 + abc - fp & & & 0.0431 & 0.0145 & 0.0233 & 0.0093 & 0.0358 & 0.0124 & 0.0407 & 0.0168 + & & & & & & & & & & +    in panels b and c respectively of table 1 , we record all rmse results for the case when two , then all three parameters are unknown , with a view to gauging the relative performance of the abc methods when multiple matches are required . in these multiple parameter cases , a preliminary run of abc , based on uniform priors",
    "defined over the domains defined above , has been used to determine the high mass region of the joint posterior .",
    "this information has been used to further truncate the priors in a subsequent abc run , and final marginal posterior estimates then produced .",
    "the results recorded in panels b and c highlight that when two parameters are unknown ( either @xmath191 and @xmath193 or @xmath191 and @xmath192 ) , the score - based abc method produces the most accurate density estimates in three of the four cases .",
    "marginalization produces an improvement in accuracy for @xmath246 for the other two parameters marginalization does not yield an increase in accuracy ; however , the differences between the joint and marginal score estimates are minimal . only in one case ( as pertains to @xmath192 ) does an abc method based on summary statistics outperform the score - based methods . in all four cases ,",
    "the aukf estimate is inferior to all other comparators , and the euler approximation also inferior to all abc - based estimates in three of the four cases .",
    "as is seen in panel d , when all three parameters are to be estimated , the score - based abc estimates remain the most accurate , with the joint score method superior overall and yielding notably improvements in accuracy over both the aukf and euler approximations .",
    "this paper has explored the application of approximate bayesian computation in the state space setting .",
    "certain fundamental results have been established , namely the lack of reduction to finite sample sufficiency and the bayesian consistency of the auxiliary model - based method .",
    "the ( limiting ) equivalence of abc estimates produced by the use of both the maximum likelihood and score - based summary statistics has also been demonstrated .",
    "the idea of tackling the dimensionality issue that plagues the application of abc in high dimensional problems via an integrated likelihood approach has been proposed .",
    "the approach has been shown to work extremely well in the case in which the auxiliary model is exact , and to yield some benefits otherwise . however , a much more comprehensive analysis of different non - linear settings ( and auxiliary models ) would be required for a definitive conclusion to be drawn about the trade - off between the gain to be had from marginalization and the loss that may stem from integrating over an _ inaccurate _ auxiliary model .",
    "indeed , the most important challenge that remains , as is common to the related frequentist techniques of indirect inference and efficient methods of moments , is the specification of a computationally efficient and accurate approximating model . given the additional need for parsimony , in order to minimize the number of statistics used in the matching exercise , the principle of aiming for a large nesting model , with a view to attaining full asymptotic sufficiency , is not an attractive one .",
    "we have illustrated the use of one simple approximation approach based on the unscented kalman filter .",
    "the relative success of this approach in the particular example considered , certainly in comparison with methods based on other more _ ad hoc _ choices of summary statistics , augers well for the success of score - based methods in the non - linear setting .",
    "further exploration of approximation methods in other non - linear state space models is the subject of on - going research .",
    "( see also creel and kristensen , 2014 , for some contributions on this front . )    finally , we note that despite the focus of this paper being on inference about the static parameters in the state space model , there is nothing to preclude marginal inference on the states being conducted , at a second stage .",
    "specifically , conditional on the ( accepted ) draws used to estimate @xmath39 , existing filtering and smoothing methods ( including the recent methods that exploit abc at the filtering / smoothing level ; see , for example , jasra _ et al . , _ 2010 ,",
    "calvet and czellar , 2014 , martin _ et al . , _ 2014 ) could be used to yield draws of the states , and ( marginal ) smoothed posteriors for the states produced via the usual averaging arguments . with the asymptotic properties of both approaches established ( under relevant conditions ) , of particular interest would be a comparison of both the finite sample accuracy and computational burden of the abc - pmcmc method developed martin _",
    "( 2014 ) , with that of the method proposed herein , in which @xmath39 is targeted more directly via the score - based approach .",
    "cornuet , j - m .",
    ",  santos , f. , beaumont , m.a . ,",
    "robert , c.p . ,",
    "marin , j - m .",
    ", balding , d.j . ,",
    "guillemard , t. and estoup , a. 2008 . inferring population history with diy abc : a user - friendly approach to approximate bayesian computation , _ bioinformatics _ 24 , 2713 - 2719 .",
    "fearnhead , p , prangle , d. 2012 .",
    "constructing summary statistics for approximate bayesian computation : semi - automatic approximate bayesian computation . _ journal of the royal statistical society , series b. _ 74 : 419474 .",
    "forbes c.s . , martin , g.m . and wright j. 2007 .",
    "inference for a class of stochastic volatility models using option and spot prices : application of a bivariate kalman filter , _ econometric reviews , special issue on bayesian dynamic econometrics _ , _ _",
    "_ _ 26 : 387 - 418 .",
    "julier , s.j . , uhlmann , j.k . and durrant - whyte , h.f . 2000 . a new method for the nonlinear transformation of means and covariances in filters and estimators , ieee _ transactions on automatic control _ 45 , 477 - 481 .",
    "le gland , f. and oudjane , n. 2006 .",
    "a sequential particle algorithm that keeps the particle system alive . in _",
    "stochastic hybrid systems : theory and safety critical applications",
    "_ ( h. blom and j. lygeros , eds ) , lecture notes in control and information sciences 337 , 351389 , springer : berlin .",
    "ng , j. , forbes , c , s . ,",
    "martin , g.m . and mccabe , b.p.m . 2013 . non - parametric estimation of forecast  distributions in non - gaussian , non - linear state space models , _ international journal of forecasting _ 29 , 411430          pritchard , j.k . ,",
    "seilstad , m.t .",
    ", perez - lezaun , a. and feldman , m.w .",
    "population growth of human y chromosomes : a study of y chromosome microsatellites , _ molecular biology and evolution _",
    "16 1791 - 1798 .",
    "toni , t. , welch , d. , strelkowa , n. , ipsen , a. and stumpf , m.p.h . 2009 .",
    "approximate bayesian computation scheme for parameter inference and model selection in dynamical systems , _ jrss ( interface ) _ 6 , 187 - 202 .            given the assumed invariance ( over time ) of both @xmath247 and @xmath145 in ( [ discrete_meas ] ) and ( [ discrete_state ] ) , the sigma points are determined as:@xmath248 and @xmath249 respectively , and propagated at each @xmath36 through the relevant non - linear transformations , @xmath147 and @xmath148 the values @xmath250 , @xmath251 , @xmath252 and @xmath253 are chosen according to the assumed distribution of @xmath254 and @xmath146 , with a gaussian assumption for both variables yielding values of @xmath255 as being ` optimal ' .",
    "different choices of these values are used to reflect higher - order distributional information and thereby improve the accuracy with which the mean and variance of the non - linear transformations are estimated ; see julier _",
    "et al . _ ( 2000 ) and ponomareva and date ( 2010 ) for more details .",
    "restricted supports are also managed via appropriate truncation of the sigma points .",
    "the same principles are applied to produce the mean and variance of the time varying state @xmath31 , except that the sigma points need to be recalculated at each time @xmath36 to reflect the up - dated mean and variance of @xmath31 as each new value of @xmath30 is realized",
    ".      1 .   use the ( assumed ) marginal mean and variance of @xmath31 , along with the invariant mean and variance of @xmath146 and @xmath145 respectively , to create the @xmath256 matrix of augmented sigma points for @xmath257 , @xmath258 , as follows .",
    "define : @xmath259{c}e(x_{t})\\\\ e(v_{t})\\\\ e(e_{t } ) \\end{array } \\right ]   \\text { , } p_{a0}=\\left [ \\begin{array } [ c]{ccc}var(x_{t } ) & 0 & 0\\\\ 0 & var(v_{t } ) & 0\\\\ 0 & 0 & var(e_{t } ) \\end{array } \\right ]   , \\text { } \\label{e_var}\\ ] ] and @xmath260 as the @xmath185 column of the cholesky decomposition ( say ) of @xmath261 given the diagonal form of @xmath262 ( in this case ) , we have@xmath263{c}\\sqrt{var(x_{t})}\\\\ 0\\\\ 0 \\end{array } \\right ]   ; \\text { } \\sqrt{p_{a0}}_{2}=\\left [ \\begin{array } [ c]{c}0\\\\ \\sqrt{var(v_{t})}\\\\ 0 \\end{array } \\right ]   ; \\text { } \\sqrt{p_{a0}}_{1}=\\left [ \\begin{array } [ c]{c}0\\\\ 0\\\\ \\sqrt{var(e_{t})}\\end{array } \\right ]   .\\ ] ] the seven columns of @xmath258 are then generated by@xmath264 where @xmath265 , @xmath266 and @xmath267 , and the corresponding notation is used for @xmath268 , @xmath269 2 .",
    "propagate the @xmath257 sigma points through the transition equation as @xmath270 and estimate the predictive mean and variance of @xmath271 as:@xmath272 where @xmath273 denotes the @xmath274 element of the @xmath275 vector @xmath276 and @xmath277 the associated weight , determined as an appropriate function of the @xmath278 and @xmath279 see ponomareva and date ( 2010 ) .",
    "3 .   produce a new matrix of sigma points , @xmath280 for @xmath281 generated by@xmath282 using the updated formulae for the mean and variance of @xmath31 from ( [ pred_e ] ) and ( [ pred_var ] ) respectively , in the calculation of @xmath283 and @xmath284 .",
    "4 .   propagate the @xmath281 sigma points through the measurement equation as @xmath285 and estimate the predictive mean and variance of @xmath286 as:@xmath287 where @xmath288 denotes the @xmath274 element of the @xmath275 vector @xmath289 and @xmath277 is as defined in step 3 .",
    "estimate the first component of the likelihood function , @xmath290 , as a gaussian distribution with mean and variance as given in ( [ e_y ] ) and ( [ var_y ] ) respectively .",
    "6 .   given observation @xmath286 produce the up - dated filtered mean and variance of @xmath31 via the usual kf up - dating equations:@xmath291 where:@xmath292 and the @xmath273 , @xmath293 are as computed in step 3",
    "continue as for steps 2 to 6 , with the obvious up - dating of the time periods and the associated indexing of the random variables and sigma points , and with the likelihood function in evaluated as the product of the components produced in each implementation of step 5 , and the log - likelihood in ( [ approx_like ] ) produced accordingly ."
  ],
  "abstract_text": [
    "<S> a new approach to inference in state space models is proposed , based on approximate bayesian computation ( abc ) . </S>",
    "<S> abc avoids evaluation of the likelihood function by matching observed summary statistics with statistics computed from data simulated from the true process ; exact inference being feasible only if the statistics are sufficient . with finite sample sufficiency unattainable in the state space setting , we seek asymptotic sufficiency via the maximum likelihood estimator ( mle ) of the parameters of an auxiliary model . we prove that this auxiliary model - based approach achieves bayesian consistency , and that - in a precise limiting sense - the proximity to ( asymptotic ) sufficiency yielded by the mle is replicated by the score . in multiple parameter settings a separate treatment of scalar parameters , based on integrated likelihood techniques , </S>",
    "<S> is advocated as a way of avoiding the curse of dimensionality . </S>",
    "<S> some attention is given to a structure in which the state variable is driven by a continuous time process , with exact inference typically infeasible in this case as a result of intractable transitions . </S>",
    "<S> the abc method is demonstrated using the unscented kalman filter as a fast and simple way of producing an approximation in this setting , with a stochastic volatility model for financial returns used for illustration .    _ </S>",
    "<S> keywords : _ likelihood - free methods , latent diffusion models , linear gaussian state space models , asymptotic sufficiency , unscented kalman filter , stochastic volatility .    </S>",
    "<S> _ jel classification : _ </S>",
    "<S> c11 , c22 , c58 </S>"
  ]
}