{
  "article_text": [
    "state estimation is a key issue in nonlinear systems control and diagnosis .",
    "algorithms that achieve this task are called observers . these algorithms attempt to reconstruct the evolution of the state vector by using the only measured ( and generally noisy ) quantities . as far as nonlinear systems are concerned ,",
    "many observation techniques have been developed during the last four decades .",
    "this includes high - gain observers @xcite , sliding - modes observers @xcite , moving - horizon observers ( mhos ) @xcite and naturally , the widely used extended - kalman - filter ( ekf ) . excellent reviews of nonlinear observer design techniques can be found in @xcite and @xcite .",
    "+   + amongst all possible observer design alternatives , mho technique has witnessed an increasing interest these last years because of its ability to handle constraints and to fully exploit precise and generally nonlinear models of the dynamic processes under study .",
    "this observer requires on - line solution of non convex optimization problems in which the cost function is the integral output prediction error while the decision variable is the set of unknown quantities to be recovered ( state and unknown parameter vectors ) .",
    "+ despite encouraging recent advances on the issue of real - time computation of mhos ( see @xcite and the references therein for a recent survey on the topics ) , the highly demanding on - line computation involved may question the feasibility of the algorithm when system needing high sampling rate are involved or when the use of highly involved optimization software is prohibited by real - life context . when the combination of such obstacles and the absence of mathematical structure that renders impossible the use of analytical observers , something has to be done in order to achieve the estimation task . +   + the ideas proposed in this paper",
    "follow a suggestion made by @xcite aiming at identifying off - line the relationship between the sequence of ( input / output ) measurements and the corresponding initial state at the beginning of the moving observation window . by doing so , the cumbersome on - line optimization step involved in moving - horizon observers @xcite ( mhos ) can be avoided .",
    "concrete implementation of such an idea using neural networks ( nns ) identification structure has been proposed in @xcite . as pointed out in @xcite , nns structures , like all standard nonlinear approximators offer high approximation capabilities at the price of non convex optimization schemes which generally suffer from convergence and computation time issues . in the present paper ,",
    "a novel nonlinear identification scheme is proposed , which after a suitable change in the decision variables , can be solved using constrained quadratic programming .",
    "such a feature is crucial if this optimization problem lies in the inner loop of a whole static game optimization formulation ( such as the one proposed in @xcite ) that may be needed to assess the convergence of the resulting mho .",
    "more precisely , the solution of the static game is needed to compute the set of approximator parameters that achieves sufficiently small maximum error over all possible initial states ( and initial estimation error ) .",
    "the price to pay in order to gain computational efficiency is a theoretical restriction of the class of situations that can be addressed as the proposed identification structure possesses no universal approximation property .",
    "for this reason , the comparison between the two identification schemes is problem - dependent .",
    "+   + it is important to underline that in the scheme proposed in @xcite a set of non convex optimization problems are solved off - line ( using different initial conditions and initial state estimation errors ) in order to built the learning data for the identification step .",
    "instead , the scheme of the present paper avoids the use of such non convex optimization by focusing on the model - based state / output relationship that can be discovered using system simulation providing the learning data followed by an identification step using specific class of nonlinear structures .",
    "this so identified function gives the output - related guess of the state which can then be amended by a model related term in a rather trivial way in order to recover a kind of classical measurement / model confidence trade - off . by doing so",
    ", one can avoid the risk of local minima that may corrupt the quality of the learning data .",
    "+   + the paper is organized as follows .",
    "first , section [ secprobform ] defines the state estimation - related identification problem .",
    "section [ secrecall ] describes the proposed nonlinear identification setting and gives a bound on the state estimation error .",
    "the way the learning data used in the identification scheme is built is shown in section [ seclearning ] .",
    "illustrative examples are given in section [ secexamples ] while section [ secconc ] summarizes the contribution of the paper and suggests hints for further investigations .",
    "let us consider a nonlinear system given by : @xmath0 where @xmath1 , @xmath2 and @xmath3 represent the state , the measured input and the measured output vectors respectively .",
    "the integer @xmath4 refers to the sampling instant @xmath5 for some sampling period @xmath6 .",
    "regardless of the algorithm that may be used to reconstruct the state vector using the measured quantities , the implicit assumption that underlines the possibility of state reconstruction is that there is an integer @xmath7 and a map @xmath8 such that the following approximation holds : @xmath9 where @xmath10 is the regressor built up with the past measured quantities according to : @xmath11 where : @xmath12 and @xmath13 .",
    "note that in the absence of measurement noise , the implicit definition of the map @xmath8 involved in ( [ basicfunction ] ) is given by the solution of the following optimization problem : @xmath14 where @xmath15 [ resp .",
    "@xmath16 refers to the model - based predicted value at instant @xmath17 of the state [ resp .",
    "output ] when the state at instant @xmath18 is equal to @xmath19 and when the sequence of controls defined by @xmath20 is applied on the time interval @xmath21 $ ] .",
    "+ it results that if one obtains through off - line computations a good approximation of the map @xmath8 involved in ( [ basicfunction ] ) , then the measurement - related part of the mhos would be obtained without on - line optimization .",
    "this is obviously a static identification problem .",
    "more precisely , since this identification is to be obtained for each component of the state vector one needs to reconstruct , the basic generic problem one has to solve is the one consisting of finding a nonlinear map @xmath22 that links a scalar quantity @xmath23 ( a component of the state vector ) to a regression vector @xmath24 , namely : @xmath25    note that , as suggested in @xcite the cost function involved in ( [ ygtf2 ] ) may contain a regularization term @xmath26 where @xmath27 stands for the predicted value of the decision variable based on the state space model and the last estimate .",
    "this enables a regularization of the estimation process and enhance more stability of the iterates .",
    "such regularization can be decoupled from the identification process by introducing afterward the following final estimation which represents a trade - off between the measurement related estimation and the model related estimation based on the previous solution : @xmath28 where @xmath29 is the measurement - based identified part while @xmath27 is the model predicted part based on the past value of the estimation . in the sequel ,",
    "we focus on @xmath29 term which corresponds to the choice @xmath30 in ( [ defdelambda ] ) .",
    "identification of nonlinear relationships is an open issue .",
    "many frameworks have been proposed including neural networks , wiener - hammerstein , volterra series based formulations @xcite to cite but few possibilities .",
    "while offering high approximation capabilities , nonlinear approximators need non convex optimization in order to compute the approximator parameters .",
    "such optimization problems suffer from computation time issue and the presence of local minima that may prevent the solver from reaching the global minimizers . in this paper , a nonlinear approximator",
    "is proposed that can be computed through constrained qp formulation at the price of lesser universality .",
    "+ more precisely , in this paper , attention is focused on nonlinear maps @xmath22 that takes the following form : @xmath31 the existence of @xmath32 is guaranteed by the strict monotonicity of @xmath33 .",
    "note that putting together ( [ jnhbgv ] ) and ( [ defstructf ] ) enables the identification problem to be re - formulated as the one of finding :    * the vector @xmath34 and * the monotonic increasing function @xmath33 such that the following approximation holds : @xmath35    note that ( [ defstructf ] ) is a nonlinear parameterization as one has to find both @xmath32 and @xmath36 which operate nonlinearly .",
    "moreover , while this structure is adapted to the derivation of efficiently solvable constrained qp problems , it is not universal in the sense that any function @xmath37 can not necessarily be represented using the structure ( [ defstructf ] ) .",
    "only the class of functions @xmath22 for which there exists a linear combination of the components of @xmath24 that maps to @xmath24 through a monotonic function is eligible .",
    "this property is impossible to check a priori , but the efficiency of the qp problem solvers makes it easy to check even for very rich parameterization . if the residual is still too high , then the system is surely out of this class since failure can not result from local minima as it is the case in standard nonlinear approximators computation .",
    "the general form of the l.h.s of ( [ gammar ] ) that is used hereafter is the one given by : @xmath38\\cdot \\mu=:\\bigl[b(\\eta(r))\\bigr]\\cdot \\mu=\\sum_{j=1}^{n_b}\\mu_j\\bigl[b^{(j)}(\\eta(r))\\bigr ] \\label{plokrd } \\end{aligned}\\ ] ] where @xmath39 and @xmath40 are the minimum and maximum values of @xmath23 over the learning data ( see section [ seclearning ] ) while @xmath41 is a function basis that is hereafter defined according to : @xmath42 where the number of functions in the set is @xmath43 while the functions @xmath44 and @xmath45 are defined by : @xmath46 the coefficients @xmath47 are given by @xmath48 .",
    "note that many other function basis can be used although the author s experience suggests that the function basis proposed above is rather appropriate given the monotonicity character of the targeted nonlinear map .",
    "+ now by combining ( [ gammar])-([plokrd ] ) , it follows that the unknowns @xmath36 and @xmath49 may be obtained by solving the following least squares problem : @xmath50 where @xmath51 is the learning data including a large number of instantiations of the pairs @xmath52 .",
    "note however that the least squares minimization invoked in ( [ defdelsopt ] ) has to be done taking into account the following constraints : + 1 ) @xmath53 * must be strictly increasing*. this leads to the following inequality constraint on the parameter vector @xmath49 : @xmath54\\quad\\bigl[\\dfrac{db}{d\\eta}(\\eta)\\bigr]\\mu\\ge \\varepsilon \\label{contr1 } \\end{aligned}\\ ] ] for some a priori chosen lower bound of the derivative @xmath55 .",
    "+ 2 ) the following * normalization integral constraint has to be satisfied * in order to avoid trivial zero values trivial solution ( @xmath56 , @xmath57 ) : @xmath58\\cdot \\mu=\\dfrac{1}{2}\\bigl[r_{min}+r_{max}\\bigr ] \\label{contr2 } \\end{aligned}\\ ] ]    note that the use of @xmath59 in the r.h.s of ( [ contr2 ] ) is arbitrary since the solution of ( [ defdelsopt ] ) is defined up to a multiplicative gain . note",
    "however that the r.h.s of ( [ contr2 ] ) is inspired by the particular case where a linear function fits the learning data . in this case , @xmath33 can be taken to be the identity map and in this case , the integral has to equal the mean value of @xmath23 .",
    "+ the two sets of constraints ( [ contr1 ] ) and ( [ contr2 ] ) are affine in the decision variable @xmath49 . therefore by considering a sequence of values @xmath60 , these constraints can be approximated by the following matrix inequalities : @xmath61\\mu\\le b_{ineq}\\in \\mathbb{r}^q\\quad;\\quad [ a_{eq}]\\mu = b_{eq}\\in \\mathbb{r } \\label{matrixineq } \\end{aligned}\\ ] ] to summarize , the quadratic function ( [ defdelsopt ] ) in the decision variables @xmath36 and @xmath49 is minimized under the linear constraints ( [ matrixineq ] ) to obtain the optimal parameters @xmath62 and @xmath63 .",
    "+ note that there is no loss in generality in taking @xmath33 strictly increasing since it suffices to take @xmath36 of opposite sign to make ( [ gammar ] ) valid for a strictly decreasing map .",
    "+   + the following result on the estimation error is straightforward :    * if * let @xmath64 be a subset of the state space to which belong all state of interest .",
    "the following conditions are satisfied : + 1 ) the admissible control sequences are bounded ( i.e. @xmath65 ) and lead for any @xmath66 to a bounded sequence of output ( @xmath67 ) + 2 ) there is an upper bound @xmath68 on the identification residual : @xmath69 3 ) for any initial state @xmath70 , the combined effect of noise and model mismatches on the regressor @xmath24 is bounded according to : @xmath71 where @xmath72 denotes the real measurement matrix ( which is not obtained by simulation ) that may be obtained on the real system starting from @xmath70 and under the control sequence @xmath73 .",
    "+   + * then * the estimation error is of the form : @xmath74    proof .",
    "first of all , note that assumption 2 ) implies that all nominal measurement regressors @xmath24 of interest belong to some compact set @xmath75 . using the assumptions ( [ defdevarepsilon])-([defdegamma ] ) ,",
    "one clearly has : @xmath76 where @xmath77 and where @xmath78 is the maximum value of the continuous ( by construction ) map @xmath79 over the compact set @xmath80 .",
    "@xmath81    note that the upper bound @xmath82 involved in ( [ defdevarepsilon ] ) can be used as a cost function @xmath83 to be minimized in the decision variable @xmath84 which are the parameters of the approximator .",
    "this obviously results in a static game in which the identification step appears in the inner loop .",
    "this strengthens the relevance of having easy to solve identification problem through the constrained qp formulation .",
    "note also that the value of @xmath82 reflects to which extent the map linking the regressor @xmath24 and the initial state is far from the set of maps described by the structure ( [ defstructf ] ) .",
    "this is because no identification error can be affected to the optimization process as the underlying problem is a quadratic programming one .",
    "assume without loss of generality that one is focused on the identification problem associated to the estimation of @xmath85 for some @xmath86 .",
    "let us also assume that the set of relevant values of the state vector @xmath19 is contained in some hypercube , namely : @xmath87\\end{aligned}\\ ] ] the learning data set @xmath51 involved in the definition of the least squares problem ( [ defdelsopt ] ) is obtained through the following steps : + 1 ) first , a set of initial states @xmath88 is chosen .",
    "this can be obtained using uniform grid on each interval @xmath89 $ ] of possible values of the @xmath90-th component of the state .",
    "+ 2 ) a set of @xmath91 control profiles @xmath92 is also generated .",
    "each profile defines a control sequence over @xmath93 sampling periods .",
    "+ 3 ) for each pair @xmath94 , the system is simulated over @xmath95 sampling periods in order to generate the corresponding state profiles : @xmath96 4 ) the data described above enables , for each pair @xmath97 to obtain :    * two sequences @xmath98 , @xmath99 that defines @xmath100 according to ( [ defdez ] ) . * the corresponding value of @xmath101 according to @xmath102    finally , the learning data set @xmath51 involved in the definition of the least squares problem ( [ defdelsopt ] ) is given by : @xmath103 which is obviously a discrete set of cardinality @xmath104 that is given by : @xmath105",
    "in this section , illustrative examples are proposed in order to give concrete instantiations of the different steps of the proposed framework .",
    "let us consider the famous van der pol oscillator which is governed by : @xmath106 where @xmath107 is a measurement noise assumed here to be white , gaussian with variance @xmath108 .",
    "typical behavior of the resulting noisy measurement is shown in figure [ typical_noise ] ( left subplot ) together with typical corresponding level of the noise ( right subplot ) .",
    "the basic sampling period is taken equal to @xmath109 .",
    "the bounds on the state components leading to the definition of the subset @xmath110 are given by @xmath111\\times [ -5,+5 ] $ ] which obviously _ enhances _ the nonlinear character of the resulting identification problem ( as @xmath112 is not negligible when compared to 1 in the expression of @xmath113 ) .",
    "the learning data has been defined using a uniform grid containing @xmath114 initial states . for each state , the system is simulated during @xmath115 sampling period ( @xmath116 ) which , according to ( [ defdene ] ) leads to a learning data @xmath51 of cardinality @xmath117 .    )",
    "is generated with a variance @xmath108,scaledwidth=60.0% ]    note that since there are two states @xmath118 and @xmath119 to be estimated , two identification problems are solved and two nonlinear maps @xmath120 and @xmath121 are obtained for the reconstruction of these two states according to @xmath122 , @xmath123 the identification parameters @xmath124 and @xmath125 are used which leads to an observation horizon of @xmath126 and a functional basis containing @xmath127 elements .   + the quality of the resulting matching between the identified and the _ true _ values are shown in the upper subplots of figure [ diagonals ] ( left plots ) .",
    "note that the identified values are obtained using an intentionally noised simulation data which enables one to inject the noise already in the identification process resolving the trade - off at this early stage .",
    "the lower subplots of figure [ diagonals ] ( left ) shows the gradient of the resulting nonlinear functions @xmath128 and @xmath129 that are involved in ( [ defstructf ] ) for the two components of the state respectively .",
    "one may note the highly nonlinear character of the map @xmath128 in particular .",
    "an estimation scenario is shown in figure [ diagonals ] ( right plots ) where the initial state of the system and the observer are respectively given by @xmath130 and @xmath131 and where a new generation of the measurement noise is used ( different from the ones used to construct the learning data set ) .",
    "note that the correction of the observer begins only when data is obtained that covers the observer horizon length .",
    "this means that the first correction occurs at @xmath132 .",
    "it is worth emphasizing here that the observer design is done using only the output measurement related correction in order to concentrate on the contribution of the present paper .",
    "it goes without saying that a balanced estimation in which the output - related estimation and the state equation related estimation can be implemented following or more generally the standard ideas of @xcite and the references therein .    .",
    "the upper plots compares the estimated states based on the identified maps @xmath120 and @xmath121 while the lower plots show the gradient of the corresponding monotonic functions @xmath128 and @xmath129 ( right ) : typical results of the state estimation using a purely output - related estimation .",
    "the observer begins once the first @xmath124 first measurements are acquired ( first correction at @xmath133 ) .",
    "this estimation is obtained while a gaussian measurement noise @xmath107 of variance @xmath108 is injected ( see figure [ typical_noise ] for a typical behavior of the noise),title=\"fig:\",scaledwidth=49.0% ] .",
    "the upper plots compares the estimated states based on the identified maps @xmath120 and @xmath121 while the lower plots show the gradient of the corresponding monotonic functions @xmath128 and @xmath129 ( right ) : typical results of the state estimation using a purely output - related estimation .",
    "the observer begins once the first @xmath124 first measurements are acquired ( first correction at @xmath133 ) .",
    "this estimation is obtained while a gaussian measurement noise @xmath107 of variance @xmath108 is injected ( see figure [ typical_noise ] for a typical behavior of the noise),title=\"fig:\",scaledwidth=49.0% ]      let us consider the problem of estimating the state of a dynamic model describing the escherichia coli strain .",
    "many knowledge - based model derivation attempts have been investigated in order to better understand the mechanisms that underline the evolution of the population @xcite or to develop model - based state observers @xcite .",
    "the dynamic model that is commonly used in deriving dynamic state estimation involves the e. coli strain @xmath134 that grows on the limiting substrate @xmath135 while yielding a final intracellular product : the @xmath136-galactosidae @xmath137 .",
    "the model is given by : @xmath138 where @xmath49 is the growth rate that is modelled using classical monod - type relation such as @xmath139 where @xmath140 is the maximum specific growth rate for the cell growth ( in @xmath141 ) .",
    "@xmath142 is the half saturation constant ; @xmath143 and @xmath144 are constants involved in the arrhenius - type death kinetic that depends on @xmath137 .",
    "@xmath145 is a maintenance rate that describes the energy required for normal upkeep and repair . @xmath146 , @xmath147 [ used in the measurement equation ( [ defdel ] ) below ] and @xmath148 are identified coefficients .",
    "@xmath149 stands for the arabinose inducer that is assumed to be constant ( no degradation ) .",
    "the output measurement vector is given by @xmath150 where @xmath36 is the light produced by the bioluminescence that is linked to the state variables by the following expressions : @xmath151 while @xmath152 and @xmath153 are measurement noises that are taken here white , gaussian and of variances @xmath154 and @xmath155 respectively .",
    "the values of the model parameters used in the sequel can be found in @xcite .",
    "+ for this example , the framework described above can be used to construct a reduced observer .",
    "more precisely , it is shown hereafter that there is a satisfactory solution to the underlined identification problem for any learning set that is constructed using an admissible set of initial conditions of the form @xmath156\\times [ 0,0.3]$ ] with the following parameters @xmath157 .",
    "this means that for each initial value @xmath158 of the e. coli strain , @xmath159 simulations of the system with different initial states ( sharing all the same value @xmath158 and different values of @xmath160 and @xmath161 ) is simulated during @xmath162 time units ( @xmath163 ) hence generating a learning set of cardinality @xmath104 given by @xmath164 .",
    "note that two identification problems are to be defined and solved using the following definition of the quantities @xmath165 and @xmath166 to be identified : @xmath167 note that if @xmath165 and @xmath166 are well estimated then @xmath168 can also be well estimated since @xmath134 is assumed to be measured ( reduced observer ) .",
    "note also that since only @xmath168 is involved in the system equation , @xmath135 is necessarily estimated through @xmath168 .",
    "+ the identification results are shown on figure [ figecolix0 ] for two different values of @xmath169 .",
    "one can appreciate that a good match between the estimated @xmath170 and the simulated @xmath171 for @xmath172 is obtained over the @xmath173 learning set while using a rather economic parametrization ( @xmath174 and @xmath125 ) .",
    "this clearly shows that the proposed methodology enables us to perform the estimation scheme provided that the initial value of the state component @xmath134 is measured at the beginning of the batch .     and @xmath175 for two different values of the e. coli strain @xmath169.,title=\"fig : \" ]   + ( a ) identification results for @xmath176        and @xmath175 for two different values of the e. coli strain @xmath169.,title=\"fig : \" ]   + ( b ) identification results for @xmath177    .,scaledwidth=90.0% ]       .,scaledwidth=90.0% ]    figure [ estimation5 ] shows typical behavior of the output - based state estimation that used the two nonlinear maps identified above .",
    "note that the noises @xmath152 and @xmath153 that affect the measured signals used in the construction of the regressor are given by @xmath178 and @xmath179 .",
    "this leads to a noise level that can be observed in figure [ noiseecoli ] .",
    "in this paper , a nonlinear approximator has been proposed for a class of nonlinear relationships and has been applied in the context of moving - horizon observer design .",
    "the proposed scheme offers the advantage of requiring only a constrained qp problem solution and can therefore be efficiently integrated in the inner loop of a global scheme aiming at optimizing the approximator parameters .   + a potential research line is to investigate a systematic computation of the optimal triplet @xmath180 defining the nonlinear approximator .",
    "indeed , a convenient ( _ optimal _ ) choice of these parameters is intimately linked to the noise level as well as the uncertainty structure . a good knowledge of the latter is crucial to obtain pertinent choice of these parameters which had been found in this paper by trial and error approach .   +   + another research track concerns the use of sparse identification techniques in order to derive low dimensional parameter vector .",
    "this can be greatly facilitated by the availability of the lagrange multipliers of the qp problem that underline the identification step ."
  ],
  "abstract_text": [
    "<S> in this paper , a new nonlinear identification framework is proposed to address the issue of off - line computation of moving - horizon observer estimate . </S>",
    "<S> the proposed structure merges the advantages of nonlinear approximators with the efficient computation of constrained quadratic programming problems . </S>",
    "<S> a bound on the estimation error is proposed and the efficiency of the resulting scheme is illustrated using two state estimation examples . </S>"
  ]
}