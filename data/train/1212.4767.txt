{
  "article_text": [
    "sampling distributions over high - dimensional state - spaces is a notoriously difficult problem , and a lot of effort has been devoted to developing methods to do this effectively @xcite .",
    "if a given target distribution is known pointwise , then markov chain monte carlo is a general method which prescribes a markov chain having this target distribution as its invariant distribution .",
    "examples include metropolis - hastings schemes , developed first by @xcite and later generalized for arbitrary proposals by @xcite .",
    "it is well known that metropolis - hastings schemes developed for finite - dimensional systems degenerate in the limit of inference on infinite - dimensional spaces and a lot of work has been devoted to examining the actual rate with which they degenerate @xcite .",
    "indeed this analysis has led to improved performance of chains in finite dimensions through arguments about optimal scaling of the acceptance probability .",
    "recently , it has been shown in a series of works by @xcite that it is possible to overcome this limitation by defining the metropolis - hastings scheme in function space , thus confronting the issue of high dimensions head on .",
    "this is particularly useful , for example , in the case where the system is a finite - dimensional approximation of a pde .",
    "in such a case , the convergence properties of the function - space scheme are independent of mesh refinement which increases the dimension of the finite - dimensional approximating system .",
    "however , it may ( and often does in practice ) happen that , even though the scheme is defined on function - space and is hence independent of mesh refinement , the integrated autocorrelation may be quite large for certain degrees of freedom , in particular those which have small effect on the likelihood .",
    "hence , there is clearly a paradox in that the degrees of freedom which are not informed by the data , and hence are governed by the prior , are the rate limiting factor in the convergence of mcmc .",
    "but , at the same time , more is known _ a priori _ about such degrees of freedom .",
    "we aim to maintain the positive aspect of being defined in function space while removing the deficit of mixing poorly in directions which are better known a priori",
    ". this will be accomplished by appropriate rescaling based on curvature which leverages a priori known information .",
    "inspired by the works @xcite we will focus on sampling distributions @xmath0 over some hilbert space @xmath1 which have density with respect to a gaussian reference measure @xmath2 of the following form @xmath3 \\mu_0(du ) , \\label{density}\\ ] ] where @xmath4 , and @xmath5 \\mu_0(du)$ ] .",
    "for simplicity , we let @xmath6 and we will further assume that @xmath7 is a compact operator .",
    "this function arises as the negative log likelihood of @xmath8 given a realization of an observation @xmath9 , where @xmath10 independent of @xmath11 . the posterior distribution on @xmath11 , conditioned on the particular observation @xmath12 , is given by .",
    "furthermore , we will assume that @xmath13 for some trace - class operator @xmath14 .",
    "the rest of this paper will be organized as follows . in section",
    "[ back ] , we will provide a review of necessary concepts , including the recently developed function - space mcmc method pcn @xcite which will serve as a starting point for this work . in section [ proposal ]",
    "we illustrate the remaining decorrelation issues pertaining to proposal chains of the pcn form . in section [ operator ]",
    "we introduce a class of constant operator rescaled proposals which address the issues highlighted in section [ proposal ] and yet still fit into the function - space sampler framework , hence taking full advantage of its benefits . in section [ numerics ] ,",
    "we investigate the performance of these algorithms computationally against standard pcn for the inverse heat equation and inversion of navier - stokes equation .",
    "finally , we provide conclusions and possible future directions in section [ conclusion ] .",
    "in this section we give some background on markov chain monte carlo ( mcmc ) methods , and in particular the metropolis - hastings ( mh ) variants .",
    "we then introduce the preconditioned crank - nicolson ( pcn ) proposal that yields an mh algorithm which is well - defined on function spaces for a wide range of posterior distributions of the form , for example those arising from the bayesian interpretation of the solution to an inverse problem with gaussian prior and observational noise .",
    "finally , we will describe the way that correlations enter into approximations based on the resulting set of samples and define the effective sample size , which will be relevant in the derivation of the new methods which follow .",
    "mcmc methods aim to sample from a target distribution @xmath0 over @xmath1 by designing a markov transition kernel @xmath15 such that @xmath0 is invariant under the action of @xmath15 @xmath16 with shorthand ( @xmath17 ) , where the integral on the lefthand side is with respect to @xmath18 .",
    "the condition known as _ detailed balance _ between a transition kernel @xmath15 and a probability distribution @xmath0 says that @xmath19 integrating with respect to @xmath11 , one can see that detailed balance implies @xmath17 .",
    "metropolis - hastings methods prescribe an accept - reject move based on proposals from an arbitrary transition kernel @xmath20 in order to define a kernel @xmath15 such that detailed balance with an arbitrary probability distribution is guaranteed . in order to make sense of mh in infinite dimensions ,",
    "first define the measures @xcite @xmath21 provided @xmath22 , where @xmath23 is denoting absolutely continuity when comparing measures , one can define the mh kernel as follows .",
    "given current state @xmath24 , a proposal is drawn @xmath25 , and then accepted with probability @xmath26 the resulting chain has transition kernel @xmath15 given by @xmath27 so , we have that @xmath28 \\right \\}\\end{aligned}\\ ] ] which one can see satisfies the necessary symmetry condition . under an additional assumption of geometric ergodicity , one has that @xmath29 for any @xmath30 @xcite .",
    "therefore , after a sufficiently long equilibration period , or _",
    "burn - in _ , one has a prescription for generating samples approximately distributed according to @xmath0 , from which integrals with respect to @xmath0 can be approximated .",
    "these samples are correlated , however , and this will be revisited below .",
    "since there is great freedom in choice of proposal kernel @xmath20 , one can imagine that there is great discrepancy in the performance of the algorithms resulting from different choices .",
    "it is popular to choose symmetric kernels , such as the classical random walk algorithm , in which the difference between the current and proposed states is taken to be a centered gaussian distribution , e.g. @xmath31 where @xmath14 is the covariance of the prior @xmath2 . for this proposal ,",
    "the absolute continuity condition @xmath32 is violated , so the mh algorithm is defined only in finite dimensions @xcite and degenerates under mesh refinement . however , a small modification to the classical random walk proposal @xmath33 yields an algorithm which satisfies @xmath22 since @xmath20 and @xmath2 are in detailed balance .",
    "this algorithm is therefore defined on function space , and it is referred to as pcn in @xcite .",
    "the key point here is that the posterior has a density with respect to the infinite - dimensional prior gaussian measure @xmath2 and so designing a proposal such as the one above which is reversible with respect to this measure leads to the necessary condition @xmath22 .",
    "the condition that @xmath22 for algorithms on function space ensures that the methods are well - behaved with respect to mesh refinement ; only such methods are considered in this paper ( see @xcite for more details on that point ) .",
    "the choice of proposal @xmath20 which satisfies detailed balance with the prior @xmath2 is natural and elegant because it leads to acceptance probability @xmath34 which depends only on the ratio of the likelihood evaluated at its arguments @xmath35      let @xmath36 , and suppose we would like to estimate the integral @xmath37 by @xmath38 samples @xmath39 , where @xmath40 .",
    "denoting this estimate by @xmath41 , we have @xmath42 then @xmath43 , so the estimator is without bias .",
    "furthermore , we denote the variance of @xmath44 by @xmath45.\\ ] ] so , we also have that @xmath46.\\ ] ] we restrict attention to the case @xmath47 , for ease of exposition only .",
    "assume also that the samples are correlated in such a way that @xmath48 for example , such samples arise from a sample path of a homogeneous markov chain , such as in mcmc . then @xmath49 \\\\ & = & ( 1/n^2 ) \\left ( \\sum_{n=1}^n r_n r_n + \\sum_{n , m=1 , n \\neq m}^n r_n r_m \\right ) \\\\ & \\approx & ( 1/n ) { \\mathbb{v}}(r ) ( 1 + 2 \\sum_{n=1}^n \\rho_n ) \\\\ & = & \\frac{(1 + 2\\theta ) } { n } { \\mathbb{v}}(r ) ,   \\end{array}\\end{aligned}\\ ] ] where we assumed @xmath50 for all @xmath51 with @xmath52 , and we denoted @xmath53 .",
    "if the @xmath54 are independent then @xmath55 and we recover constant prefactor @xmath56 to @xmath57 .",
    "otherwise , the constant prefactor reduces the effective number of samples with respect to the case of independent samples .",
    "it is therefore informative to define the _ effective sample size _ by @xmath58 .",
    "phrased another way , one needs @xmath59 samples in order to get another approximately _ independent _ sample .",
    "hence , @xmath60 and @xmath61 are referred to as the autocorrelation function and the integrated autocorrelation , respectively .",
    "the general case is dealt with similarly , working elementwise . in this case , the effective sample size is given by the minimum over effective sample sizes of individual degrees of freedom , strictly speaking . from the above discussion , it is clear that the performance of an mh algorithm in approximating integrals with respect to a distribution is limited by the correlation between the samples it generates .",
    "the rest of the paper will concern this point about correlations .",
    "in this section , we explore some drawbacks of the function - space sampling algorithm defined in the previous section . we will first motivate the need for modification with a simple one dimensional example .",
    "then , we describe how this same idea lifts into higher dimensions .      , and fast decorrelation in the informed chain @xmath62 .",
    "left plot shows autocorrelations , as compared also to the analytical autocorrelation of @xmath63 .",
    "right plot shows an interval of the evolution over which @xmath63 has barely decorrelated , but @xmath62 has clearly decorrelated within a small sub - interval . ,",
    "title=\"fig:\",scaledwidth=50.0% ] , and fast decorrelation in the informed chain @xmath62 . left plot shows autocorrelations , as compared also to the analytical autocorrelation of @xmath63 .",
    "right plot shows an interval of the evolution over which @xmath63 has barely decorrelated , but @xmath62 has clearly decorrelated within a small sub - interval .",
    ", title=\"fig:\",scaledwidth=50.0% ]    as an example , consider the following markov chain @xmath64 where @xmath65 i.i.d . for all @xmath66 and @xmath67 .",
    "so , @xmath68 for all @xmath66 , and @xmath69 therefore , @xmath70 , and @xmath71 notice also that this is independent of @xmath72 .",
    "now , we run an experiment .",
    "let @xmath73 , with @xmath74 and @xmath75 .",
    "consider the following metropolis - hastings chain @xmath62 with proposal given by @xmath63 : @xmath76 the above chain samples the posterior distribution with observation @xmath77 and prior @xmath78 .",
    "we will denote the chains by @xmath79 and @xmath80 figure [ mixing ] illustrates the decorrelation time and evolution of the proposal chain , @xmath63 , and the informed chain @xmath62 for a given choice of @xmath81 which yields reasonable acceptance probability .",
    "the speedup is astonishing , and indeed counterintuitive : one only accepts or rejects moves from the proposal chain , so intuition may suggest that the informed chain would have larger autocorrelation .",
    "but actually , if the distribution is dominated by the likelihood , then the integrated autocorrelation is determined by the accept / reject , and is indeed _ smaller _ than that of the proposal . on the other hand ,",
    "if the distribution is dominated by the prior , then the integrated autocorrelation is dictated by the proposal chain and is then increased by the acceptance probability .",
    "if the state - space were two dimensional , with an observation only in the first dimension , then for a given choice of scalar @xmath81 the chain will mix like @xmath62 in the observed component and like @xmath63 in the unobserved component .",
    "the same idea extends to higher dimensions , motivating the use of direction - dependent proposal step - sizes .     with @xmath74 ( left ) and over @xmath82 with @xmath83",
    "fixed ( right ) , for proposals which are reversible with respect to the prior ( blue ) and the posterior ( red dashed ) .",
    "also shown at the bottom is the lag 1 autocorrelation of the proposal chain , @xmath84 , in black dash - dotted.,title=\"fig:\",width=188,height=188 ]   with @xmath74 ( left ) and over @xmath82 with @xmath83 fixed ( right ) , for proposals which are reversible with respect to the prior ( blue ) and the posterior ( red dashed ) .",
    "also shown at the bottom is the lag 1 autocorrelation of the proposal chain , @xmath84 , in black dash - dotted.,title=\"fig:\",width=204,height=188 ] +   with @xmath74 ( left ) and over @xmath82 with @xmath83 fixed ( right ) , for proposals which are reversible with respect to the prior ( blue ) and the posterior ( red dashed ) .",
    "also shown at the bottom is the lag 1 autocorrelation of the proposal chain , @xmath84 , in black dash - dotted.,title=\"fig:\",width=188,height=188 ]   with @xmath74 ( left ) and over @xmath82 with @xmath83 fixed ( right ) , for proposals which are reversible with respect to the prior ( blue ) and the posterior ( red dashed ) .",
    "also shown at the bottom is the lag 1 autocorrelation of the proposal chain , @xmath84 , in black dash - dotted.,title=\"fig:\",width=188,height=188 ] +    we now digress momentarily to explain this counterintuitive phenomenon in more detail for the simple analytically solvable example above .",
    "this is crucial to understanding the methods which will be developed in subsequent sections . in the above example",
    ", the posterior distribution is given by @xmath85 , where @xmath86 and @xmath87 .",
    "assuming that @xmath88 , we know from eqs .",
    "[ example1d ] that the correlation between subsequent steps is given by the following analytically known , yet horrendous , integral @xmath89 & = & \\frac{1}{2\\pi c\\sqrt{c } }   \\int_{{\\mathbb{r}}^2 } [ z^*-a)(z - a ) \\alpha(z , z^ * ) + \\\\ & & ( z - a)^2 ( 1-\\alpha(z , z^ * ) ) ] e^{-[\\frac{(z - a)^2}{2c } + \\frac{1}{2}w^2 ] } dz dw , \\end{array } \\label{lag1_exp}\\ ] ] where @xmath90 .",
    "it is instructive to also consider in eq .",
    "the alternative proposal which keeps the posterior invariant : @xmath91 in which case @xmath92 in eq . and the acceptance is modified accordingly .",
    "indeed , it turns out that @xmath93 \\leq \\sqrt{1-\\beta^2}$ ] , with equality in the limit of @xmath94 .",
    "see also @xcite .",
    "we compute this integral using a riemann sum and plot the corresponding values of @xmath95 and @xmath93 $ ] for varying @xmath81 with @xmath82 fixed , and varying @xmath82 with @xmath81 fixed in fig .",
    "[ analytical ] . in the top left panel",
    ", we see that for proposal with @xmath74 the acceptance decreases from 1 to 0 as @xmath81 ranges from 0 to 1 . in the bottom left panel below this",
    "we can see that the minimum lag 1 autocorrelation corresponds to the acceptance probability @xmath96 ; c.f . the well - known optimal scaling results of @xcite . for the proposal ,",
    "the acceptance is 1 for all @xmath81 .",
    "it is clear from the bottom left plot that in this example we should use with @xmath97 and do independence sampling on the known posterior .",
    "this obvious result in this idealistic example is instructive , however .",
    "it is not possible to do independence sampling on the posterior for non - gaussian posteriors ( or we would be done ) and even independence sampling on the gaussian approximation to the posterior may lead to unacceptably small acceptance probability .",
    "indeed we can see that the prior proposal can perform as well as or better than the posterior proposal for appropriate @xmath98 , a step - size that is probably much too large in a gaussian proposal whenever the target is non - gaussian .",
    "this makes it clear that the crucial thing is to choose the appropriate scale of the proposal steps , rather than naively trying to match the target with the proposal ( although this principle does hold for the case presented above in which the proposal keeps the target posterior invariant , and in general this does lead to reasonably effective proposals ) . from the top right panel",
    ", we can see that the acceptance probability for the posterior proposal is again 1 for all @xmath82 for a fixed @xmath83 , while the autocorrelation ( bottom right ) also remains approximately @xmath84 ( which is quite large in this case ) .",
    "however , as this choice of @xmath81 is tuned to the prior proposal for @xmath74 , we see that the acceptance probability for prior proposal increases from @xmath99 to almost 1 as @xmath82 ranges from @xmath100 to 5 . the corresponding lag 1 autocorrelation ranges from @xmath101 to approximately @xmath84 as @xmath82 increases and the posterior approaches the prior .",
    "recall that in one dimension if the distribution is determined by the likelihood then the integrated autocorrelation is close to 1 , and if the distribution is determined by the prior then the integrated autocorrelation is dictated by the proposal chain .",
    "suppose that some of the space is dominated by the prior , and some is dominated by the likelihood .",
    "if the observations have small variance , then the step - size @xmath81 in the chain will have to be small in order to accommodate a reasonable acceptance probability .",
    "but , the part of the space which is dominated by the prior will have @xmath102 autocorrelation , as dictated by the proposal chain .",
    "now consider the bayesian inverse problem on a hilbert space @xmath1 .",
    "let @xmath103 denote an orthonormal basis for the space @xmath1 .",
    "let @xmath104 project onto a finite basis @xmath105 and @xmath106 .",
    "suppose we wish to sample from the posterior distribution @xmath0 with prior distribution @xmath2 , where @xmath107 for some @xmath108 and some distance function @xmath109 between probability measures ( for example hellinger metric or kullback - leibler distance ) .",
    "thus @xmath104 projects onto the `` important part '' of the space , in the sense that the observations only inform the projection of the posterior distribution on this part of the space .",
    "suppose we use metropolis - hastings to sample from posterior @xmath0 with prior @xmath110 .",
    "let @xmath111 , so that the pcn chain @xmath112 leaves the prior invariant . without loss of generality , we represent this chain in the basis in which @xmath14 is diagonal , so the proposal for each degree of freedom is an independent version of the above and clearly has decorrelation time @xmath113 .",
    "the distribution @xmath114 on @xmath115 , so when we impose accept / reject using the above chain as proposal , we are sampling from @xmath116 with decorrelation time @xmath117 where @xmath118 is the expected acceptance probability .",
    "but , we may as well sample independently from the known distribution @xmath119 in this part of the space .    following the reasoning above",
    ", one can consider modifying the above proposal chain to sample from the pcn chain on @xmath120 and sample independently from the prior on @xmath115 .",
    "this corresponds to setting the chain above to @xmath121 our first and simplest variant of weighted proposal follows directly from this . in the next section we will consider a more elaborate extension of this proposal using curvature information",
    "inspired by the discussion in the preceding section , we let @xmath122 be an _ operator _ weighting different directions differently , and define the proposal chain to be @xmath123 with @xmath124 .",
    "notice that this proposal preserves the prior in the sense that @xmath125 .",
    "this form of proposal for @xmath126 constant in @xmath66 can be derived as a generalization of the pcn proposal from @xcite , for some general pre - conditioner @xmath127 in their notation .",
    "recall the form of distribution we consider , given by and , with prior of the form @xmath128 . for simplicity of exposition",
    ", we will consider standard normal @xmath129 priors , without any loss of generality . in the general case ,",
    "a change of variables @xmath130 is imposed to normalize the prior , such that a chain of the form keeps the prior invariant .",
    "the samples of the resulting mh chain are then transformed back as @xmath131 for inference . for the moment",
    ", we revert to the case in which @xmath132 in order to motivate the form of the proposals . in this case",
    "we have @xmath133 , where @xmath134 is lebesgue measure , and after transformation @xmath135 =   \\phi(u)+ \\frac{1}{2}|u|^2 + k(y ) , \\label{logpost}\\ ] ] with @xmath136\\ } d\\lambda$ ] .",
    "at a given point @xmath137 the local curvature is given by the hessian of this functional , which can be approximated by the positive definite operator @xmath138 where @xmath139 denotes the derivative of @xmath140 with respect to @xmath11 evaluated at @xmath137 .",
    ", which is close to @xmath141 for example if the distribution is concentrated close to the data or if @xmath140 is close to quadratic at @xmath137 . ]",
    "if @xmath142 are eigenpairs associated with this operator , then :    * if @xmath143 , then the distribution is dominated by the data likelihood in the direction @xmath144 ; * if @xmath145 , then the distribution is dominated by the prior in the direction @xmath144 .",
    "the direction of the largest curvature at a given point will determine the smallest scale of the probability distribution at that point . if @xmath146 is the largest eigenvalue of , then the smallest local scale of the density will be @xmath147 .",
    "this scale will directly determine the step - size necessary to obtain a reasonable acceptance probability within the mh algorithm .",
    "but , such a small step - size is only necessary in the direction @xmath148 .",
    "so , rather than prescribing this as the step - size in the remaining directions , we would like to _ rescale _ them according to the operator given in .",
    "there is no log - density with respect to lebesgue measure in function space , but the above operator is still well - defined and indeed its minimizer gives the point of maximum probability over small sets @xcite .",
    "this functional will be the basis for choosing @xmath122 in .",
    "based on the above intuition , one can devise a whole hierarchy of effective such operators . in the simplest instance , one can allow for @xmath122 constant , either during the duration of the algorithm , after a transient adaptive phase , or even for some lag - time where it is only updated every so often .",
    "in this manuscript we consider only constant @xmath149 , and we consider three simple levels of complexity . the rigorous theory underlying the algorithms",
    "will be deferred to future work , although we note that the case of constant @xmath149 is easier to handle theoretically , since one does not need to consider the conditions for ergodicity of adaptive mcmc @xcite .",
    "the first level of complexity consists of letting @xmath150 the resulting method corresponds to the original pcn proposal , and we denote this method by o.    the second level of complexity consists of letting @xmath151 where @xmath152 , @xmath153 is the kronecker delta function , and @xmath154 $ ] and @xmath155 $ ] are chosen during a transient adaptive phase , either together or one at a time , yielding .",
    "the simple idea is that the curvature is constant and prescribed by the prior for sufficiently high frequencies , and this has already been motivated by the discussion in the previous section .",
    "the method resulting from this proposal will be denoted by c.    the third level of complexity consists of utilizing a relaxed version of the hessian for an arbitrary given point close to a mode of the distribution .",
    "let @xmath156 where @xmath81 is the scalar tuning parameter as in the above cases , @xmath157 $ ] is another tuning parameter , and @xmath137 is any point .",
    "the method resulting from this proposal will be denoted by h. we consider a low rank approximation of the resulting operator , since it is assumed to be compact . notice that for this choice of @xmath149 , the covariance of the search direction is given by @xmath158 for linear @xmath140 we let @xmath159 , and the right - hand side of the above approaches the covariance of the posterior distribution as @xmath160 , giving exactly the correct rescaling for the search direction while still keeping the prior invariant .",
    "the linear case is rather special , since we could just do independence sampling of the posterior , but it provides a good benchmark . in practice ,",
    "adaptation of @xmath81 in this case leads to small , but non - zero , @xmath81 . for nonlinear @xmath140",
    "we choose @xmath161 , and the scaling of the search direction is commensurate with the local curvature except with increased weight on the log - likelihood component , given by the first term of .",
    "the second term is a perturbation which is larger for the ( changing ) directions dictated by the hessian of the log - likelihood and approaches zero for directions which are dictated by the prior , in which the curvature is constant .",
    "provided @xmath162 is chosen small enough , and the curvature does not change too much , the step - size of a given move will not exceed the scale of the distribution .",
    "furthermore , the step - size will interpolate appropriately between the direction of the smallest scale and the directions which are determined by the prior , corresponding to scale 1 .",
    "we note that a whole array of different proposals are possible by choosing different @xmath122 , and there may be better options since an exhaustive search has not been carried out .",
    "investigation into various other proposals which incorporate the appropriate ( re)scaling in different ways , however , suggests that their performance is similar @xcite .",
    "both methods c and h can be considered as splitting methods in which part of the space is targeted using the acceptance probability and independence sampling is done on the complement .",
    "c splits the spectral space into a finite inner radius in which pcn is done , and its infinite - dimensional complement where independence sampling is done .",
    "h more delicately splits the space into the finite dominant eigenspace of the hessian of the likelihood @xmath163 and its complement , and then furthermore considers an appropriate rescaling of the dominant eigenspace .",
    "notice that the complement space absorbs the infinite - dimensionality .",
    "such proposals which separate the space allow any general form of proposal on the finite - dimensional target subspace and maintain the property that @xmath22 as long as the proposal is reversible with respect to the prior on the infinite - dimensional complement subspace .",
    "for example , a simple pcn proposal with another choice of @xmath81 can be used on this subspace .",
    "this idea appears in the forthcoming works @xcite .",
    "note that one could instead incorporate this curvature information by devising a chain which keeps a gaussian approximation to the posterior invariant , similarly to .",
    "for example , one can use the following proposal which preserves @xmath164 : @xmath165 with @xmath166 and @xmath167 .",
    "the curvature information can be incorporated for example by letting @xmath168,\\ ] ] or some close by point , and @xmath169 . in this case",
    ", it would be sensible to revert to the case of scalar @xmath122 since the relevant scaling is already intrinsic in the form of the gaussian .",
    "empirical statistics can also be incorporated in this way .",
    "if @xmath170 is genuinely low - rank then equivalence @xmath22 is immediate .",
    "if not , one must look more closely .",
    "however , in this case if one imposes a low - rank approximation , then equivalence is again immediate based on the discussion above . the results in sec .",
    "[ principle ] , and in particular those presented in fig .",
    "[ analytical ] illustrate that this strategy should not be considered as preferable to using a proposal of the form for appropriate choice of @xmath122 .",
    "these variations do however warrant further investigation .",
    "we limit the current study to the case of proposals of the form because ( i ) these methods are defined on function space , following immediately from the reversibility with respect to the prior and the framework of @xcite , ( ii ) the acceptance probability takes the simple and elegant form of which is easy to implement , and ( iii ) as described in sec .",
    "[ proposal ] , it is not clear that a more elaborate kernel such as the one following from the proposal would yield better results .",
    "in this section , we will investigate the performance of the mh methods introduced in the previous section on two prototypical examples .",
    "first , we briefly explore the inverse heat equation on the circle as an illustrative example .",
    "next , we look in more depth at the more complicated problem of the inverse navier - stokes equation on the 2d torus .",
    "consider the one dimensional heat equation @xmath171 we are interested in the inverse problem of recovering @xmath172 from noisy observations of @xmath173 : @xmath174 where @xmath175 , @xmath176 , @xmath177 with @xmath178 , and @xmath179 we let @xmath180 , and @xmath181 .",
    "the problem may be solved explicitly in the fourier sine basis , and the coefficients of the mean @xmath182 and variance @xmath183 of the posterior distribution can be represented in terms of the coefficients of the data @xmath184 and the prior @xmath185 : @xmath186 the posterior distribution is strongly concentrated on the @xmath187 mode , in the sense that @xmath188 and @xmath189 for all @xmath190 , and the discrepancy between @xmath191 and @xmath14 will necessitate a very small scalar @xmath81 . for the case of scalar weighting",
    "o , all modes will decorrelate slowly except for @xmath187 .",
    "[ comp_op ] for a comparison between scalar weighting o and curvature - based weighting h. in this case , we will omit the intermediate proposal which simply proposes independent samples outside a certain radius in the spectral domain .     and",
    "@xmath192 , in the case of o ( left ) , and h ( right).,title=\"fig:\",scaledwidth=50.0% ]   and @xmath192 , in the case of o ( left ) , and h ( right).,title=\"fig:\",scaledwidth=50.0% ]      in this section , we consider the inverse problem of determining the initial condition of navier - stokes equation on the two dimensional torus given noisy observations .",
    "this problem is relevant to data assimilation applications in oceanography and meteorology .",
    "consider the 2d navier - stokes equation on the torus @xmath193 with periodic boundary conditions : @xmath194 here @xmath195 is a time - dependent vector field representing the velocity , @xmath196 is a time - dependent scalar field representing the pressure , @xmath197 is a vector field representing the forcing ( which we assume to be time independent for simplicity ) , and @xmath198 is the viscosity .",
    "we are interested in the inverse problem of determining the initial velocity field @xmath11 from pointwise measurements of the velocity field at later times .",
    "this is a model for the situation in weather forecasting where observations of the atmosphere are used to improve the initial condition used for forecasting . for simplicity",
    "we assume that the initial velocity field is divergence free and integrates to zero over @xmath199 , noting that this property will be preserved in time .",
    "define @xmath200 and @xmath1 as the closure of @xmath201 with respect to the @xmath202 norm .",
    "we define @xmath203 to be the leray - helmholtz orthogonal projector ( see @xcite ) .",
    "given @xmath204 , define @xmath205 .",
    "then an orthonormal basis for @xmath1 is given by @xmath206 , where @xmath207 for @xmath208 .",
    "thus for @xmath209 we may write @xmath210 where , since @xmath11 is a real - valued function , we have the reality constraint @xmath211 . using the fourier decomposition of @xmath11",
    ", we define the fractional sobolev spaces @xmath212 with the norm @xmath213 , where @xmath214 . if @xmath215 , the stokes operator , then @xmath216 .",
    "we assume that @xmath217 for some @xmath218 .",
    "let @xmath219 , for @xmath220 , and let @xmath221 be the set of pointwise values of the velocity field given by @xmath222 .",
    "note that each @xmath223 depends on @xmath11 and define @xmath224 by @xmath225 .",
    "we let @xmath226 be a set of random variables in @xmath227 which perturbs the points @xmath228 to generate the observations @xmath229 in @xmath227 given by @xmath230 we let @xmath231 denote the accumulated data up to time @xmath232 , with similar notation for @xmath233 , and define @xmath234 by @xmath235 .",
    "we now solve the inverse problem of finding @xmath11 from @xmath236 .",
    "we assume that the prior distribution on @xmath11 is a gaussian @xmath237 , with the property that @xmath238 and that the observational noise @xmath239 is i.i.d .  in @xmath240 ,",
    "independent of @xmath11 , with @xmath241 distributed according to a gaussian measure @xmath242 .",
    "we let @xmath243 noting that if @xmath8 , then @xmath244 almost surely for all @xmath245 ; in particular @xmath209 .",
    "thus @xmath238 as required .",
    "the forcing in @xmath246 is taken to be @xmath247 , where @xmath248 and @xmath249 with @xmath250 the canonical skew - symmetric matrix , @xmath251 and @xmath252 .",
    "furthermore , we let the viscosity @xmath253 , and the interval between observations @xmath254 , where @xmath255 is the timestep of the numerical scheme , and @xmath256 so that the total time interval is @xmath257 . in this regime , the nonlinearity is mild and the attractor is a fixed point , similar to the linear case . nonetheless , the dynamics are sufficiently nonlinear that the optimal proposal used in the previous section does not work .",
    "we take @xmath258 evaluated at the gridpoints , such that the observations include all numerically resolved , and hence observable , wavenumbers in the system .",
    "the observational noise standard deviation is taken to be @xmath259 .",
    "the truth and the mean vorticity initial conditions and solutions at the end of the observation window are given in fig .",
    "[ truth_mean ] .",
    "the truth ( top panels ) clearly resembles a draw from a distribution with mean given by the bottom panels .",
    "[ thebs ] shows the diagonal of the rescaling operator @xmath149 ( i.e. for @xmath260 ) for c , given by ( 4.4 ) and h , given by ( 4.5 ) . for o",
    "the operator is given by the identity , i.e. all one along the diagonal .",
    "some autocorrelation functions ( approximated with one chain of @xmath261 each ) are shown in fig .",
    "[ autos ] . also given for comparison is the exponential fit to the ( acceptance lagged ) autocorrelation function for the proposal chain for o , i.e. @xmath262 .",
    "this differs for each , because @xmath81 and @xmath118 differ .",
    "in particular , for o @xmath263 and @xmath264 ( approximated by the sample mean ) , for c @xmath265 and @xmath266 , and for h @xmath267 and @xmath268 .",
    "presumably this accounts for the slight discrepancy in the autocorrelation of mode @xmath269 , given in the top left panel . for the mode @xmath270 ( top right ) ,",
    "the difference becomes more pronounced between h and the other two . in the bottom left it is clear that the mode @xmath271 decorrelates very quickly for h , while it decorrelates approximately like the proposal chain for both other methods .",
    "the mode @xmath272 ( bottom right ) is independently sampled for c , so has decorrelation time @xmath273 .",
    "it decorrelates almost as fast for h , and is decorrelating with the proposal chain for o.    figure [ conv ] shows the relative error in the mean ( left ) and variance ( right ) with respect to the converged values as a function of sample number for small sample sizes .",
    "the accumulated benefit of h is very apparent here , yielding almost an order of magnitude improvement .",
    "for each of the proposals , we run @xmath274 chains of length @xmath261 , both for the benchmark mean and variance used in fig .",
    "[ conv ] , and to compare the so - called potential scale reduction factor ( psrf ) convergence diagnostic @xcite . in fig .",
    "[ psrf ] we plot the psrf for each method on the same color scale from @xmath275 $ ] .",
    "although it can not be seen for o(left ) and c(middle ) since the values of most modes are saturated on this scale , every mode for every method is clearly converged according to the psrf , which should be smaller than 1.1 and eventually converges to 1 .",
    "we can see for o that the low frequencies , where the mean and uncertainty are concentrated , converge faster ( as indicated in [ autos ] ) , and other modes converge more or less uniformly slowly .",
    "c clearly outperforms o in the high frequencies outside the truncation radius ( where the mean and uncertainty are negligible ) and is comparable for the smallest frequencies , but may even be worse than o for the intermediate frequencies .",
    "this may be explained by the different @xmath81 and @xmath118 .",
    "again , h quite clearly outperforms both of the other two methods .",
    "note that these results are for particular parameter choices , and methods of choosing the parameters @xmath81 , @xmath276 , @xmath162 , and may be sensitive to their choice .              for c , given by and h , given by .,title=\"fig:\",scaledwidth=50.0% ]   for c , given by and h , given by .,title=\"fig:\",scaledwidth=50.0% ]    .",
    "this differs for each , because @xmath81 and @xmath118 differ.,title=\"fig:\",scaledwidth=50.0% ] .",
    "this differs for each , because @xmath81 and @xmath118 differ.,title=\"fig:\",scaledwidth=50.0% ] .",
    "this differs for each , because @xmath81 and @xmath118 differ.,title=\"fig:\",scaledwidth=50.0% ] .",
    "this differs for each , because @xmath81 and @xmath118 differ.,title=\"fig:\",scaledwidth=50.0% ]",
    "this manuscript introduces and investigates a new class of proposals for function - space mcmc , giving rise to a new class of algorithms .",
    "these proposals involve an operator which appropriately rescales the proposal step - sizes in different directions in order to decrease autocorrelation and hence improve convergence rate . for the curvature - inspired rescaling h introduced here , there are two design parameters which need to be hand - chosen .",
    "but , it seems to be effective to choose @xmath162 sufficiently small and then allow @xmath81 to be chosen during a transient adaptive phase , as in the other methods .",
    "it should be possible to achieve the same result with one design parameter .",
    "furthermore , there exists a whole range of more sophisticated models between those investigated here and the more sophisticated but expensive rmhmc @xcite and stochastic newton @xcite .",
    "this gives rise to exciting opportunities for future investigation .",
    "it will be of great interest to establish rigorous theoretical results , such as geometric ergodicity , for these methods .",
    "+   + * acknowledgements * the author would like to thank epsrc and onr for financial support .",
    "i also thank the mathematics institute and centre for scientific computing at warwick university , as well as the institute for computational and experimental research in mathematics and the center for computation and visualization at brown university for supplying valuable computation time .",
    "i would like to extend thanks to tiangang cui , youssef marzouk , and gareth roberts for interesting discussions related to these results , and also to the referee for a careful read and insightful suggestions for improvement .",
    "finally , i thank andrew stuart for interesting discussions related to these results and for valuable input in the revision of this manuscript .",
    "j.  martin , l.  wilcox , c.  burstedde , o.  ghattas , a stochastic newton mcmc method for large - scale statistical inverse problems with application to seismic inversion , siam journal on scientific computing 34  ( 3 ) ( 2012 ) 14601487 .",
    "m.  dashti , k.j.h .",
    "law , a.m.  stuart and j.  voss , map estimators and posterior consistency in bayesian nonparametric inverse problems . accepted for publication in inverse problems ( 2013 ) ."
  ],
  "abstract_text": [
    "<S> inverse problems lend themselves naturally to a bayesian formulation , in which the quantity of interest is a posterior distribution of state and/or parameters given some uncertain observations . for the common case in which the forward operator is smoothing , </S>",
    "<S> then the inverse problem is ill - posed . </S>",
    "<S> well - posedness is imposed via regularisation in the form of a prior , which is often gaussian . under quite general conditions </S>",
    "<S> , it can be shown that the posterior is absolutely continuous with respect to the prior and it may be well - defined on function space in terms of its density with respect to the prior . in this case , by constructing a proposal for which the prior is invariant , one can define metropolis - hastings schemes for mcmc which are well - defined on function space ( @xcite ) , and hence do not degenerate as the dimension of the underlying quantity of interest increases to infinity , e.g. under mesh refinement when approximating pde in finite - dimensions . however , in practice , despite the attractive theoretical properties of the currently available schemes , they may still suffer from long correlation times , particularly if the data is very informative about some of the unknown parameters . </S>",
    "<S> in fact , in this case it may be the directions of the posterior which coincide with the ( already known ) prior which decorrelate the slowest . </S>",
    "<S> the information incorporated into the posterior through the data is often contained within some finite - dimensional subspace , in an appropriate basis , perhaps even one defined by eigenfunctions of the prior . </S>",
    "<S> we aim to exploit this fact and improve the mixing time of function - space mcmc by careful rescaling of the proposal . to this end , we introduce two new basic methods of increasing complexity , involving ( i ) characteristic function truncation of high frequencies and ( ii ) hessian information to interpolate between low and high frequencies . the second , more sophisticated version , bears some similarities with recent methods which exploit local curvature information , for example rmhmc @xcite , and stochastic newton @xcite . </S>",
    "<S> these ideas are illustrated with numerical experiments on the bayesian inversion of the heat equation and navier - stokes equation , given noisy observations . </S>"
  ]
}