{
  "article_text": [
    "hidden markov models ( hmm s ) are a workhorse of modern statistics and machine learning , with applications ranging from speech recognition to biological sequence alignment , to pattern classification @xcite .",
    "an hmm defines the joint distribution over a sequence of states @xmath0 , @xmath1 , and observations @xmath2 , whereby the states form a markov chain and the observations are conditionally independent given the sequence of states . in formulae we have @xmath3= p_1(s_1)\\prod_{i=2}^{t}p_i(s_{i}|s_{i-1 } ) \\prod_{i=1}^{t}q_i(y_i|s_i)\\ , . \\ ] ] the most fundamental algorithmic task related to hmm s is arguably the problem of inferring the sequence of states @xmath4 from the observations .",
    "the conditional distribution of the state sequence given the observations is , by bayes theorem , @xmath5 = \\frac{1}{z(\\us ) }   p_0(s_0)\\prod_{i=2}^{t}p_i(s_{i}|s_{i-1})\\prod_{i=1}^{t}q_i(y_i|s_i)\\ , , \\label{eq : conditional } \\ ] ] where @xmath6 $ ] can be thought as a normalization constant .",
    "the state sequence can then be estimated by the sequence of most likely states ( symbol maximum a posteriori probability -map- estimation ) @xmath7 \\right\\}\\ , . \\ ] ] this reduces the inference problem to the problem of computing marginals of @xmath8 $ ] .    from a statistical physics point of view @xcite , the conditional distribution ( [ eq : conditional ] )",
    "can be regarded as the boltzmann distribution of a one dimensional system with variables @xmath9 and energy function @xmath10 at temperature @xmath11 .",
    "the sequence of observations thus act as a quenched external field .",
    "as suggested by this analogy , the marginals of @xmath8 $ ] can be computed efficiently using a transfer matrix algorithm . in the present context",
    "this is also known as the bahl - cocke - jelinek - raviv ( bcjr ) algorithm @xcite .",
    "the bcjr algorithm has complexity that is linear in the sequence length and quadratic in the number of states @xmath12 .",
    "more precisely , the complexity in @xmath12 is the same as multiplying an @xmath13 matrix times an @xmath12 vector .",
    "while this is easy for simple models with a few states , it becomes intractable for complex models .",
    "a simple mechanism leading to state space explosion is the presence of memory in the underlying markov chain , or the dependence of each observation on multiple states . in all of these cases ,",
    "the model can be reduced to a standard hmm via state space augmentation , but the augmented state space becomes exponential in the memory length .",
    "this leads to severe limitations on the tractable memory length .",
    "this paper proposes several new algorithms for addressing this problem .",
    "our basic intuition is that , when the memory length gets large , the transfer matrix can be accurately approximated using mean field ideas .",
    "we study the proposed method on a concrete model used in dna pyrosequencing . in this case , one is interested in inferring the underlying dna sequence from an absorption signal that carries traces of the base type at several positions .",
    "the effective memory length scales roughly as the square root of the sequence length , thus making plain transfer matrix impractical .",
    "the paper is organized as follows .",
    "the next section will define the concrete model we study , and section [ section_pyrosequencing ] describes the connection with dna pyrosequencing to motivate it .",
    "section [ section_algorithms ] describes the transfer matrix algorithm and several low complexity approximation schemes . after describing a few bounds in [ section_discussion ] ,",
    "numerical and analytical results are collected in section [ section_results ] .",
    "consider a sequence of @xmath14 positive integers @xmath15 .",
    "each entry @xmath16 of this sequence is generated randomly and independently from the others with probability distribution @xmath17 .",
    "this distribution has finite support , i.e. we introduce a positive integer @xmath18 such that @xmath19 if @xmath20 .",
    "this sequence is observed through a non - recursive linear filter , i.e. each observation does not depend on any previous observation .",
    "that is , we observe the sequence @xmath21 defined by @xmath22 where @xmath23 is what we call the memory function and @xmath24 is a gaussian random variable with mean @xmath25 and variance @xmath26 which is drawn independently for each position @xmath27 .",
    "the memory function also has a finite support .",
    "we introduce an integer @xmath28 which represents the total memory length , i.e. we assume @xmath29 when @xmath30",
    ". therefore the sum on the right hand side of eq .",
    "( [ eq_y_a_def ] ) effectively starts at @xmath31 .",
    "there is no restriction on the sign of @xmath32 .",
    "the relationship between the sequences @xmath33 and @xmath34 can be described by a factor graph representation . this is a bipartite graph including one _ function node _ for every @xmath35 in @xmath34 and one _ variable node _ for every @xmath16 in @xmath33 . except for the first @xmath28",
    ", every function node is connected to exactly @xmath36 variable nodes by as many edges .",
    "a schematical representation is presented in fig .",
    "[ fig_factor_graph ] .",
    "( 1500,550)(0,0 ) ( 200,150 ) ( 400,150 ) ( 600,150 ) ( 800,150 ) ( 1000,150 ) ( 1200,150 )    ( 575,125)(0,-1)10 ( 575,115)(1,0)650 ( 1225,125)(0,-1)10 ( 900,40)(0,1)60 ( 850,0)@xmath36 ( 1190,515)@xmath27    ( 175,450 )    ' '' ''    ( 200,450)(-2,-1)100 ( 200,450)(-4,-3)100 ( 200,450)(-2,-3)100 ( 200,450)(0,-1)300    ( 375,450 )    ' '' ''    ( 400,450)(-2,-1)300 ( 400,450)(-4,-3)300 ( 400,450)(-2,-3)200 ( 400,450)(0,-1)300    ( 575,450 )    ' '' ''    ( 600,450)(-2,-1)500 ( 600,450)(-4,-3)400 ( 600,450)(-2,-3)200 ( 600,450)(0,-1)300    ( 775,450 )    ' '' ''    ( 800,450)(-2,-1)600 ( 800,450)(-4,-3)400 ( 800,450)(-2,-3)200 ( 800,450)(0,-1)300    ( 975,450 )    ' '' ''    ( 1000,450)(-2,-1)600 ( 1000,450)(-4,-3)400 ( 1000,450)(-2,-3)200 ( 1000,450)(0,-1)300    ( 1175,450 )    ' '' ''    ( 1200,450)(-2,-1)600 ( 1200,450)(-4,-3)400 ( 1200,450)(-2,-3)200 ( 1200,450)(0,-1)300    ( 800,150)(2,1)500 ( 1000,150)(4,3)300 ( 1200,150)(2,3)100    ( 1000,150)(2,1)300 ( 1200,150)(4,3)100    ( 1200,150)(2,1)100    ( 100,150 )  ( 100,475 )  ( 1350,150 ) ",
    "( 1350,475 ) ",
    "our goal is to develop an efficient algorithm to recover the sequence @xmath37 from the observed noisy sequence @xmath38 .",
    "+ the sequence @xmath39 is a chain of i.i.d .",
    "variables therefore it can be regarded as zero - order markov chain . the output @xmath35 is observed from the underlying state @xmath16 as well as the all the states preceding @xmath16 .",
    "this problem can therefore be thought of as a variable length higher - order hidden markov model ( hmm ) where the hidden states are i.i.d .",
    "and the observations depend on all previous hidden states ( @xcite ) .",
    "we will denote by @xmath40^n$ ] an estimate of @xmath39 . using bayes rule , the posterior probability of @xmath41 knowing @xmath34 is @xmath42 = \\frac { \\prob \\left [ \\underline{y } | \\underline{x } \\right ] \\mathbb{p } \\left [ \\underline{x } \\right]}{\\mathbb{p } \\left [ \\underline{y } \\right ] } \\ , .",
    "\\label{eq_posterior_prob}\\ ] ] we use the maximum a posteriori ( map ) method @xcite to produce the estimation .",
    "the probability @xmath43 $ ] is known as the _ likelihood function _ and takes the form @xmath44 = \\prod_{a=1}^t \\psi_a ( x_{a - n } , \\ldots ,    x_a ) \\ , , \\label{eq_likelihood_function}\\ ] ] where , if the index of @xmath45 is such that @xmath46 then we set @xmath47 .",
    "this sets the boundary condition for small @xmath27 . for @xmath48 ,",
    "the variables @xmath49 are not present in any equation and are considered free .",
    "the probability @xmath50 in eq .",
    "( [ eq_likelihood_function ] ) is the density of @xmath51 defined in eq .",
    "( [ eq_y_a_def ] ) and is written @xmath52 = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 } } \\exp \\left [ - \\frac{1}{2 \\sigma^2 } \\left (    y_a -   \\sum_{i = a - n}^a \\alpha(i , a ) x_i \\right)^2 \\right ] \\ , , \\label{eq_psi}\\ ] ] where @xmath26 is the variance of @xmath24 . the other terms in eq .",
    "( [ eq_posterior_prob ] ) are @xmath53 = \\prod_{a=1}^t \\beta ( x_a ) \\,,\\ ] ] which is the prior distribution , and @xmath54   = \\sum_{\\underline{x } }   \\prob \\left [ \\underline{x } \\right ]   \\prod_{a=1}^t \\psi_a ( x_{a - n } , \\ldots ,    x_a )   = \\mathcal{n } \\,,\\ ] ] which is a normalization constant . in fine , we construct the marginal distribution @xmath55 \\,,\\ ] ] which yields the decoded sequence as @xmath56 which is the maximum likelihood estimate .",
    "we use here a symbol map decoding , with the hope of minimizing the error for each single @xmath49 , instead of a block map decoding which would be to minimize the error over the sequence as a whole .    the direct computation using this method",
    "entails a summation over @xmath57 terms which rapidly becomes unpractical when @xmath18 and/or @xmath14 grow . in section [ section_algorithms ]",
    "we introduce four separate algorithms with various levels of approximation to overcome this limitation . +      in this section we give some details about the sequences we use in our numerical simulations . to generate the integer sequence @xmath39",
    "we use several different probability distributions @xmath58 .",
    "the details of these will be described in subsection [ subsection_beta_distributions ] .",
    "there are also several @xmath59 functions we will be using , these are described in subsection [ subsection_alpha_functions ] .",
    "consider a sequence of i.i.d .",
    "bernoulli variables taken in @xmath60 with success probability @xmath61 .",
    "we then construct the integer sequence @xmath39 by counting the number of repetitions of @xmath25s or @xmath62s in this bernoulli sequence .",
    "for instance , if the bernoulli sequence is @xmath63 this will correspond to @xmath64 .",
    "we can generate this sequence directly using the distribution @xmath65 we will give more details on this particular distribution in section [ section_pyrosequencing ] .",
    "the sequence @xmath66 is then generated using eq .",
    "( [ eq_y_a_def ] ) .",
    "the distribution @xmath67 does not admit a finite support but it decays rapidly as @xmath68 , thus we can still introduce an effective cutoff parameter @xmath18 .",
    "the uniform distribution , called @xmath71 , is @xmath72 +      here we will give the expressions we use in our different test cases for the function @xmath59 defined in eq .",
    "( [ eq_y_a_def ] ) .",
    "+ the first expression we use for the function @xmath59 is @xmath73 if there is a @xmath74 such that @xmath75 , otherwise @xmath76 .",
    "furthermore , we have @xmath77 $ ] . the derivation of this expression , as was the expression of @xmath67 in the previous section ,",
    "will be described in more detail in section [ section_pyrosequencing ] .",
    "the value of @xmath78 effectively tunes the memory length , i.e. @xmath79 .",
    "we define two other functions that will be useful for test cases , as were eqs .",
    "( [ eq_beta_t ] ) and ( [ eq_beta_u ] ) , @xmath80 for @xmath81 and are equal to zero otherwise .",
    "these functions are useful as they are non - zero in the range @xmath82 $ ] only and thus we have a better control over the precision of our approximation since we decide of the value of @xmath28 for eq .",
    "( [ eq_y_a_def ] ) .",
    "when there is no subscript to @xmath59 it can take any of these three values .",
    "the original inspiration of this work comes from the sequence - by - synthesis technique , called pyrosequencing , originally introduced in @xcite and described in @xcite with a full review in @xcite which gives a concrete and precise description of the method and its history .",
    "it is one way of sequencing dna strands by a repeated set of chemical tests . since its introduction",
    ", it has become an industry standard for low cost high efficiency sequencing .",
    "this paper being , in the end , not directly connected to the technique we will simply give an introduction to its fundamentals .",
    "a more detailed review of the usage of our approach on experimental pyrosequencing data is planned as a future publication .    in pyrosequencing ,",
    "the initial solution contains many copies of the same strand , which is called the base sequence , and a set of enzymes that will catalyze and react with the by product of the main reaction to emit light .",
    "tests are done by incorporating a repeated cycle of the @xmath83 base type nucleotides into the solution .",
    "when a nucleotide is introduced it will react with the dna sequences if the first available position of the base sequence is its complementary base ( that is bases @xmath84 and @xmath85 on one side and bases @xmath86 and @xmath87 on the other ) .",
    "the reaction will happen for as long as this same base is repeated .",
    "furthermore , the reaction will produce a readable response ( a pulse of light ) that is proportional to the total number of repetitions that were encountered , this is called a homopolymeric ( hp ) sub - sequence .",
    "finally , all positions on the dna strands that reacted will now be obstructed to subsequent tests and thus freeing the next available base in the base sequence for reaction . as an example ,",
    "[ fig_pyrosequencing ] shows a series of cycles applied to the sequence @xmath88 .    .",
    "the cycles follow the order @xmath89 .",
    "a `` @xmath90 '' sign means no reaction occurred , every sequence of `` @xmath91 '' signs means a reaction occurred and its amplitude was multiplied by as many times . ]    when a test is positive , the chemical reaction is incomplete , that means only a fraction of all the strand copies react .",
    "we call the average of this fraction the incorporation rate @xmath92 $ ]",
    ". furthermore , it means there is a fraction @xmath93 of all the copies that did not react to this test and that a fraction @xmath78 of this fraction will react only at the next cycle , and so on for each test .",
    "finally , it results that the responses will depend on this incomplete incorporation and which dependency can be simulated by the use of a memory function . in full rigour",
    ", there is an additional parameter called the non - specific incorporation rate @xcite .",
    "it measures the average fraction of strands that react when the test is _",
    "negative_. we do not take this element into account in our model since its value is usually very small .",
    "an original mathematical description was introduced in @xcite as a biochemical model .",
    "using kinetic considerations , it investigates the differential equations describing the single pulse due to a single incorporation as well as a succession of pulses linked to as many incorporations .",
    "the model that is developed gives an effective description of pyrosequencing without approximation .",
    "if we denote by @xmath94 the hp sub - sequences and @xmath95 the response sequence , then in fact this work can be considered a slight adaptation of pyrosequencing in which we consider base sequences to be made up of only two base types .",
    "this model being binary , we naturally call them @xmath25 and @xmath62 .",
    "this also explains the inspiration for the definition of the distribution @xmath67 in eq .",
    "( [ equation_prob_a_priori ] ) if we take @xmath96 . in this case",
    ", we usually take the value of this cutoff parameter to be @xmath97 since the probability for @xmath98 being bigger than this value of @xmath18 is @xmath99 .",
    "we keep the same definition for the incorporation rate @xmath78 .",
    "the sequence of incomplete reactions can thus be plotted onto a directed acyclic graph ( dag ) which can look like fig .",
    "[ fig_pyro_diag ] ( when the first tested base is represented by @xmath25 ) .     and",
    "this is also the first test performed.,title=\"fig : \" ] ( -100,-10)@xmath100 ( -80,5)@xmath101    for each test in our two base sequence , we have a fraction @xmath78 which reacts and presents the other base in the next hp sub - sequence for the following test and a fraction @xmath93 which does not react and thus will not react with the subsequent test .",
    "we then count the vertices on the graph to obtain @xmath59 .",
    "for instance , in fig .",
    "[ fig_pyro_diag ] we have singled out the position of @xmath101 . its value is obtained by adding the lengths of all the direct paths that lead from the starting point to its position .",
    "that is , there are @xmath102 possible paths and they all have the same length of @xmath103 and therefore @xmath104 . by generalizing this to any position we obtain @xmath105 expressed in eq .",
    "( [ eq_real_alpha ] ) and by adding noise the pyrosequencing equivalent response @xmath35 of test @xmath27 is expressed in eq .",
    "( [ eq_y_a_def ] ) .     for @xmath106 , @xmath107 and @xmath108 ( from left to right ) , and @xmath109 .",
    "on the right : @xmath105 for @xmath107 , @xmath108 and @xmath110 ( from left to right ) and @xmath111 . in both figures : the solid lines are the envelopes of @xmath112 and the dotted lines show the alternating behavior of @xmath112 between zero and non - zero values.,title=\"fig : \" ]   for @xmath106 , @xmath107 and @xmath108 ( from left to right ) , and @xmath109 . on the right : @xmath105 for @xmath107 , @xmath108 and @xmath110 ( from left to right ) and @xmath111 . in both figures",
    ": the solid lines are the envelopes of @xmath112 and the dotted lines show the alternating behavior of @xmath112 between zero and non - zero values.,title=\"fig : \" ] ( -100,-10)@xmath113 ( -315,-10)@xmath113 ( -440,100 )    the value of the memory @xmath28 introduced in section [ section_model ] is linked to the graph of this function @xmath112 .",
    "we define @xmath28 as the smallest integer so as to keep a certain percentage of the total weight of @xmath112 between @xmath114 and @xmath14 .",
    "the weight being here the sum of all values of @xmath115 for @xmath116 $ ] . in practice",
    ", we keep at minimum @xmath117 of the total weight . furthermore , as seen in fig .",
    "[ fig_alpha ] , the graph of @xmath112 is such that we assume for all @xmath27 and all @xmath118 that we have @xmath119 , indeed , as @xmath27 grows , the graph of @xmath112 widens and thus @xmath28 is defined for the worst possible case .",
    "it is also shown in fig .",
    "[ fig_alpha ] how the peak widens as @xmath78 decreases .",
    "in section [ section_model ] we introduced the general model which we adopt . in this section",
    "we will introduce four different algorithms to estimate the sequence while achieving the best compromise between precision and complexity .      the first algorithm we introduce",
    "will be referred to as the _ transfer matrix algorithm _ ( tm ) . +",
    "this method relies on the combination of two iterative expressions of the position @xmath27 , one for each direction forward and backward , respectively @xmath120 and @xmath121 , and which we will refer to as the constrained partition functions .",
    "both are indexed by the sequence @xmath122^n$ ] such that @xmath123 these two functions can then be combined to write the exact marginal of @xmath124 with respect to the distribution ( [ eq_posterior_prob ] ) as @xmath125 therefore , the marginal distribution of variable @xmath49 again with respect to the probability distribution defined in eq .",
    "( [ eq_posterior_prob ] ) is then @xmath126 where @xmath127 is a normalization constant .",
    "this algorithm corresponds to a reordering of the model defined in section [ section_model ] .",
    "it has complexity of order @xmath128 which is huge in most of the regimes we are interested in .",
    "because of this , we introduce in subsequent sections a set of approximations to reduce this complexity .      in this section",
    "we introduce two algorithms that emerge from the same approximation to the tm algorithm of section [ subsection_algo_tm ] .",
    "they rely on a first order expansion of the constrained partition functions .",
    "+ we assume the constrained partition functions @xmath129 ( where the @xmath130 is either @xmath131 or @xmath132 ) defined in eqs .",
    "( [ eq_tm_forwards ] ) and ( [ eq_tm_backwards ] ) factorize approximately @xmath133 where the @xmath134 are functions of a single variable .",
    "the new iterative procedures are one variable nationalizations of the functions in eqs .",
    "( [ eq_tm_forwards ] ) and ( [ eq_tm_backwards ] ) and where the functions @xmath135 on the right hand side are replaced by the approximation in eq .",
    "( [ eq_first_order_approx ] ) . these can be written , for @xmath136 $ ] , as @xmath137 where the @xmath138 are normalization constants and where @xmath139^n$ ] ( resp .",
    "@xmath140^n$ ] ) is the set of all possible values of @xmath141 ( resp .",
    "@xmath142 ) . furthermore , since they were not estimated in any previous step , @xmath143 and @xmath144 are both set to @xmath62 prior to the computation of @xmath145 and @xmath146 which are the first values to be computed at step @xmath27 in their respective directions .",
    "these values are then reinjected into the subsequent calculations at step @xmath27 by setting @xmath147 and @xmath148 . to initiate the backwards iteration",
    ", we define @xmath149 for @xmath150 to account for the free boundary conditions .",
    "finally , we have an approximation of the marginal of @xmath49 as @xmath151 where @xmath127 is a normalization constant .",
    "+ the sums in eqs .",
    "( [ eq_first_order_forwards ] ) and ( [ eq_first_order_backwards ] ) are over @xmath152 terms and thus , this algorithm , as such , has no benefit over the tm algorithm described in section [ subsection_algo_tm ] .",
    "we therefore introduce another couple of approximations to this algorithm .",
    "when possible , though , we will wish to compare these approximations to this algorithm which we will refer to as tm.1a .",
    "the first of these two approaches will be called _ first order approximation with monte carlo _ ( tm.1a.mc ) since it relies on random sampling from iteratively computed distributions .",
    "+ at step @xmath27 of the procedure described in ( [ eq_first_order_forwards ] ) , we start by estimating @xmath145 knowing that we have computed the values of @xmath153 for all @xmath154 and in particular the values of @xmath155 which are probability distributions over the variables @xmath45",
    ". we can therefore use the importance sampling technique . by using these distributions ,",
    "we generate @xmath156 independent random samples of @xmath28 independent variables @xmath157 which we use to compute the estimation @xmath158 where @xmath127 is a normalization constant and where we use the same set of samples for all @xmath159 $ ] .",
    "other positions @xmath160 are each computed in the same manner with a new sampling for each one .",
    "+ the backwards iteration is hereby discarded .",
    "indeed , the backwards iteration of eq .",
    "( [ eq_first_order_backwards ] ) will result in a large number of _ false positives_. this happens because the @xmath161 become very peeked about a mean that is not the correct @xmath45 due to the repercussion of early errors in subsequent iterations .",
    "it is therefore necessary to perform the initial sums over a very large number of samples which defeats the purpose of this algorithm .",
    "furthermore , empirical tests show that under the first order approximation , very little information is actually gained by using the backwards algorithm .",
    "+ when all iterations have been computed , we simply equate the @xmath162 to the marginals , i.e. @xmath163 $ ] and @xmath159 $ ] @xmath164    the complexity is @xmath165 which takes of the order of a second to decode a full chain for typical parameter values .",
    "+      the second first order approximation will be referred to as _",
    "first order approximation with gauss _ ( tm.1a.g ) .",
    "we make the assumption that the variable @xmath166 present in @xmath167 at step @xmath27 ( eq . [ eq_psi ] ) can be approximated with a gaussian random variable .",
    "this is possible since the variables @xmath168 are independently drawn from their respective distributions under the approximation of eq .",
    "( [ eq_first_order_approx ] ) and we assume @xmath28 is _",
    "large_.    the complete derivation can be found in appendix [ appendix_gauss ] , but if we write the mean and variance of @xmath169 as respectively @xmath170 and @xmath171 , the iterated marginal distribution of @xmath45 for @xmath172 at step @xmath27 can be written as @xmath173 \\ , , \\label{eq_marginal_gauss}\\ ] ] where @xmath127 is a normalization constant and a similar expression for @xmath174 since @xmath175 does not exist and is replaced by the prior of @xmath49 : @xmath176 .",
    "the final iteration for @xmath177 returns the complete set of marginals @xmath178 for all @xmath113 .",
    "we make the same assumption on the backwards iteration as in section [ subsubsection_mc ] .",
    "+      our final algorithm , which will be called _ second order approximation with gauss _ ( tm.2a.g ) is similar to the algorithm described in section [ subsection_algo_gauss ] but introduces a different factorized approximation .",
    "+ we introduce a second order approximation , which is similar to the factorized expression in eq .",
    "( [ eq_first_order_approx ] ) , i.e. @xmath179 \\ , , \\label{eq_second_order_approx}\\ ] ] where the functions @xmath180 are very small .",
    "this expression introduces two point correlations in the expression of the iterative marginal .",
    "there is one drawback to the expression in eq .",
    "( [ eq_second_order_approx ] ) and which is that it is required that the functions @xmath181 and numerically the control of such structures is very difficult .",
    "we thus make the assumption that the factor graph is in fact one - dimensional .",
    "that is , each variable is connected to exactly two nodes , except for the extremities . by introducing this approximation we can take advantage of a decomposition property for the joint probability of an arbitrary number of variables taken on a tree graph @xcite .",
    "thus , at any step @xmath27 , for any number of successive variables taken between positions @xmath113 and @xmath182 , as a relationship between the joint probability and the marginals , we have @xmath183 where the @xmath184 and @xmath185 are true marginals of the approximation @xmath186 .",
    "+ by setting interactions only between closest neighbors and taking inspiration from section [ subsection_algo_gauss ] , we consider the variable @xmath187 at step @xmath27 to be gaussian and we assume it has mean and variance respectively @xmath188 and @xmath189 .",
    "it then comes that the main expression for the two point marginal at step @xmath27 for the couple @xmath190 can be written @xmath191 ^ 2     \\right ] } .",
    "\\label{eq_z_i_ip1}\\ ] ] the details of this algorithm can be found in appendix [ appendix_two_point ] .",
    "in this section we describe how we test the behavior of our different approximated algorithms when it is impossible to compare them to the tm algorithm .",
    "this is done by introducing approximations on the probability of error and on the average number of errors that occur when performing estimation .",
    "we start by studying the probability of error in the limit of a memoryless channel and then we discuss another approximation for a channel with memory .      in this subsection , we assume that the channel is without memory , i.e. that the parameter @xmath28 introduced in section [ section_model ] is zero and thus that eq .",
    "( [ eq_y_a_def ] ) reduces to @xmath192 where @xmath51 is the random variable with mean @xmath25 and variance @xmath26 defined in eq .",
    "( [ eq_y_a_def ] ) .",
    "the probability of error for the single variable is then @xmath193 \\ , , \\label{eq_perr_memoryless}\\ ] ] where @xmath194 is the indicator function .",
    "errors thus happen when the expression @xmath195 in ( [ eq_perr_memoryless ] ) is maximized by a value @xmath196 . if the distribution @xmath58 is uniform this occurs if @xmath197 in general , for other expressions of @xmath58 , these events provide a lower bound on the probability we seek .",
    "these lead to the very general expression for the probability of error as @xmath198 \\prob \\left [ \\eta >",
    "\\frac{1}{2 } \\right ] + \\prob [ x_0 = c ] \\prob \\left [ \\eta < - \\frac{1}{2 } \\right ] + \\sum_{x=2}^{c-1 } \\prob [ x_0 = x ] \\prob \\left [ |\\eta | > \\frac{1}{2 } \\right ] \\,,\\ ] ] where , @xmath199 being normal , we have @xmath200 = \\prob \\left [ \\eta < - \\frac{1}{2 } \\right ] = \\frac{1}{2 } \\prob \\left [ |\\eta | > \\frac{1}{2 } \\right ] = q \\left ( \\frac{1}{2 \\sigma } \\right ) \\,,\\ ] ] where @xmath201 = \\frac{1}{\\sqrt{2 \\pi}}\\int_{x}^{+ \\infty } { \\rm d } \\nu ~ e^{-\\frac{1}{2 } \\nu^2 } \\ , .",
    "\\label{eq_q_func}\\ ] ] therefore , the probability of error can finally be expressed as @xmath202 where @xmath58 is one of the distributions introduced in section [ subsection_beta_distributions ] , i.e. depending on which distribution is being studied we will use one of the following expressions @xmath203 when in @xmath67 and @xmath204 we have @xmath205 .",
    "let us now derive a lower bound on the probability of errors in the case of channel with non - zero memory .    to do this , consider the probability density defined in ( [ eq_posterior_prob ] ) and write it as follows @xmath206   = \\frac{1}{z } \\left (   \\prod_{a=1}^t \\beta(x_a ) \\right ) ~ e^ { - \\frac{1}{2 \\sigma^2 }    h_{\\underline{a } }   ( \\underline{x } )    } \\,,\\ ] ] where @xmath207 is a normalization constant and @xmath208 ^ 2 \\,,\\ ] ] where all the parameters are the same as in ( [ eq_y_a_def ] ) .    using these notations , we can write the block map probability of error as @xmath209 where @xmath210 is the prior and takes the form @xmath211 if we consider the geometrical distribution @xmath67 with @xmath96 , @xmath212 if we consider the truncated distribution @xmath204 and @xmath213 if we consider the uniform distribution .",
    "for any @xmath214 a vector of @xmath14 strictly positive integers the probability @xmath215 is a lower bound of the right hand side of eq .",
    "( [ eq_prob_err_part_func ] ) . in order to estimate this probability",
    "we write @xmath216 ^ 2 + 2 \\sum_{a=1}^t \\eta_a \\sum_{i=1}^a \\alpha(i , a ) ( a_i - x_i )   \\,,\\\\ b_g ( \\underline{x } ) - b_g ( \\underline{a } ) & = & 2 \\sigma^2 \\ln(2 ) \\sum_{a=1}^t ( x_a - a_a)\\ , , \\\\ b_u ( \\underline{x } ) - b_u ( \\underline{a } ) & = & 0 \\,,\\end{aligned}\\ ] ] and if we define @xmath217 ^ 2 \\ , , \\\\",
    "\\eta_{\\ua , \\ux } & = & 2 \\sum_{a=1}^t \\eta_a \\sum_{i=1}^a \\alpha(i , a ) ( a_i - x_i ) \\ , , \\\\",
    "b_{\\ua , \\ux } & = & b ( \\underline{x } ) - b ( \\underline{a } ) \\,,\\end{aligned}\\ ] ] we obtain that @xmath218 furthermore , @xmath219 is a gaussian random variable of mean @xmath25 and variance @xmath220 , thus , using the same function @xmath221 as in ( [ eq_q_func ] ) , we have @xmath222    this expression is a function of both @xmath223 and @xmath39 and thus is still impractical both analytically and numerically .",
    "we thus introduce the notation @xmath224 which differs from @xmath33 at position @xmath113 only where it takes value @xmath225 , i.e. @xmath226 .",
    "that is , we have @xmath227 where the expression of @xmath228 will be given in the following .",
    "we use the previous expression and eq .",
    "( [ p_for_sigma ] ) to write a lower bound on the probability of error @xmath229 where @xmath230 is the expectation over the distribution of the vectors @xmath33 .",
    "we can also write a lower bound of the expectation of the number of errors as @xmath231    the expression of @xmath232 varies according to the prior distribution @xmath58 , we separate the results accordingly . for the geometrical distribution , we have @xmath233 which is maximized by @xmath234 if @xmath235 and @xmath236",
    "if @xmath237 thus we define the function of a single integer @xmath238 + we can therefore estimate the lower bounds when considering the geometrical distribution as @xmath239 + in the case of the uniform distribution we have , again using @xmath240 as a function of a single integer , @xmath241 where each @xmath240 in eq .",
    "( [ eq_uniform_num_errors ] ) is in fact solely a function of @xmath113 and all have the same expression , thus we define @xmath242 and thus @xmath243   \\ , , \\\\",
    "\\e_u \\ {    \\ # \\textrm{errors }    \\ } & \\geq &    \\sum_{i=1}^t r ( i ) \\,.\\end{aligned}\\ ] ] + finally , the expressions of @xmath244 and @xmath245 for the truncated distribution are the same as for the geometrical distribution and @xmath246 \\,,\\end{aligned}\\ ] ] and @xmath247 +",
    "in this section we study the behavior of our various algorithms according to the parameters used .",
    "we will separate this section into three separate subsections according to the parameters we use to control our tests .    in general , the behavior of the algorithms is the same if we are to consider either @xmath67 or @xmath204 , thus we will usually only present the results for one of the two except in section [ subsection_tvar ] where we emphasize the similarities .",
    "there are in total six algorithms we compare .",
    "these were all defined in section [ section_algorithms ] except for the one referred to as tm.1a.f which is the first order approximation of section [ subsection_algo_mc ] with the backwards iteration discarded .      in this subsection",
    "we vary the noise parameter @xmath248 .",
    "we have eight figures ( [ figs_t100_f3_v3_l4_c15_n1000_svar_pcd]-[figs_t100_f3_v2_l3_c15_n1000_svar_numerr ] ) that range over the set of distributions @xmath58 and functions @xmath59 .",
    "for each couple @xmath249 we plot the probability of error @xmath250 ( figs .",
    "[ figs_t100_f3_v3_l4_c15_n1000_svar_pcd ] , [ figs_t100_f3_v2_l4_c15_n1000_svar_pcd ] , [ figs_t100_f3_v3_l3_c15_n1000_svar_pcd ] and [ figs_t100_f3_v2_l3_c15_n1000_svar_pcd ] ) and the average number of errors @xmath251 ( figs .",
    "[ figs_t100_f3_v3_l4_c15_n1000_svar_numerr ] , [ figs_t100_f3_v2_l4_c15_n1000_svar_numerr ] , [ figs_t100_f3_v3_l3_c15_n1000_svar_numerr ] and [ figs_t100_f3_v2_l3_c15_n1000_svar_numerr ] ) over a set of independent samples .",
    "we recall that the probability of error is the probability that at least one error occurs during the estimation process .",
    "we emphasize more on the use @xmath252 and @xmath253 since they allow us to control the value of the memory length @xmath28 and thus allows us to use tm .",
    "the probability of error varies from @xmath25 to @xmath62 as @xmath248 grows from @xmath25 .",
    "the rate of increase is dependent on the couple @xmath249 .",
    "the average number of errors varies from @xmath25 to a value that depends on the distribution @xmath58 .    ) for various algorithms vs @xmath248 .",
    "the full line is the analytical lower bound .",
    "the figure on the bottom is a blowup of the one on the top for @xmath248 small . with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ] ) for various algorithms vs @xmath248 .",
    "the full line is the analytical lower bound .",
    "the figure on the bottom is a blowup of the one on the top for @xmath248 small . with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]    ) for various algorithms vs @xmath248 .",
    "the full line is the analytical lower bound .",
    "the figure on the bottom is a blowup of the one on the top for @xmath248 small . with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ] ) for various algorithms vs @xmath248 .",
    "the full line is the analytical lower bound .",
    "the figure on the bottom is a blowup of the one on the top for @xmath248 small . with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]     with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]     with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]     with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]     with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   with @xmath254 , @xmath255 , @xmath97 , using @xmath204 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]     with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]     with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   with @xmath254 , @xmath255 , @xmath97 , using @xmath71 and @xmath253 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]    there is one expected and obvious trend we can take out of the figures [ figs_t100_f3_v3_l4_c15_n1000_svar_pcd ] to [ figs_t100_f3_v2_l3_c15_n1000_svar_numerr ] and which is that the tm algorithm always , and usually very notably so , outperforms all the other algorithms .",
    "indeed the tm algorithm is an exact implementation of bit map decoding . by definition",
    ", it minimizes the probability of error over variables .",
    "also , in general , we can see that the worst performing algorithm is , unsurprisingly , tm.1a.g though it is at par with the other algorithms for @xmath248 small .    more specifically , in fig .",
    "[ figs_t100_f3_v3_l4_c15_n1000_svar_pcd ] , we see for larger @xmath248 that tm.1a and tm.2a.g perform similarly and better than tm.1a.f and tm.1a.mc which also perform the same . all",
    "perform very well for @xmath258 . in fig .",
    "[ figs_t100_f3_v3_l4_c15_n1000_svar_numerr ] we see the same threshold of @xmath259 below which there is virtually no errors . above this value",
    "we see that tm.1a and tm.2a.g remain very close , though the former slightly outperforms the latter for @xmath260 . both tm.1a.f and",
    "tm.1a.mc perform very similarly and are outperformed by tm.1a.g for @xmath261 though this could be linked to the way algorithms respond to higher values of @xmath248 .",
    "for very large values of @xmath248 , the limit value of half the total length ( i.e. @xmath262 in the current example ) is explained by the fact that when estimation is impossible the algorithms always return the estimate @xmath62 for all positions which is the maximum of the prior distributions @xmath204 . in the present case",
    "the probability that @xmath237 is very close to @xmath263 thus the probability @xmath263 of ending up with the correct value .    in the case of @xmath264 ,",
    "i.e. figs .",
    "[ figs_t100_f3_v2_l4_c15_n1000_svar_pcd ] and [ figs_t100_f3_v2_l4_c15_n1000_svar_numerr ] , we see that tm.2a.g performs very well compared to all the other algorithms which all perform very similarly , except for tm .",
    "the limit value in fig . [ figs_t100_f3_v2_l4_c15_n1000_svar_numerr ] is the worst case scenario of randomly falling on the correct value .",
    "it is the total length times the complementary probability of randomly picking a value and therefore is @xmath265 with the current parameters .    by setting the @xmath59 function to @xmath253 in figs .",
    "[ figs_t100_f3_v3_l3_c15_n1000_svar_pcd ] to [ figs_t100_f3_v2_l3_c15_n1000_svar_numerr ] , we observe a similar behavior independently of the distribution @xmath58 . besides tm and tm.1a.g which behave according to the trends described previously ,",
    "all algorithms perform very similarly .",
    "the only major difference is in the limit value for the average number of errors which is a function of @xmath58 .    ) for various algorithms and versus the noise @xmath248 and the incorporation rate @xmath78 .",
    "parameters are @xmath266 , @xmath97 , @xmath267 , using @xmath67 and @xmath112 average over @xmath256 samples and @xmath257 for tm.1a.mc . ]    , @xmath268 , @xmath269 and @xmath270 from left to right for tm.1a.mc , tm.1a.g and tm.2a.g versus the noise @xmath248 and the incorporation rate @xmath78 . parameters are @xmath266 , @xmath97 , @xmath267 , using @xmath67 and @xmath112 average over @xmath256 samples and @xmath257 for tm.1a.mc . ]    finally , in the case of @xmath112 described in eq .",
    "( [ eq_real_alpha ] ) and the distribution @xmath67 described in ( [ equation_prob_a_priori ] ) with @xmath271 , we have very similar performances for the three algorithms tm.1a.mc , tm.1a.g and tm.2a.g .",
    "the algorithms tm , tm.1a and tm.1a.f can not be used with these parameters .",
    "indeed , we have a relatively large memory @xmath267 , which is kept the same for all values of @xmath78 , and therefore tm does not fit in our computer memories and tm.1a and tm.1a.f would take several thousand years to compute a single sample on the computers used .",
    "these results are shown in fig .",
    "[ figs_multiplot ] . in this figure",
    "we show the interpolated contour lines of the probability of correct decoding @xmath272 as a function of both @xmath248 and the incorporation rate @xmath78 .",
    "we show in more detail in fig .",
    "[ figs_pcd_contour_t100_v1_l1_pvar_svar ] how performance increases with @xmath78 and how similar performance is for the three algorithms .",
    "it also shows , in this case for smaller values of @xmath78 , that tm.1a.mc has a slight advantage over the two others .",
    "this advantage for tm.1a.mc with @xmath112 will be confirmed in [ subsection_tvar ] .     for the various algorithms . with @xmath273 when using @xmath253 and @xmath274 when using @xmath252 , @xmath254 , @xmath255 , @xmath97 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   for the various algorithms . with @xmath273 when using @xmath253 and @xmath274 when using @xmath252 , @xmath254 , @xmath255 , @xmath97 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   for the various algorithms . with @xmath273 when using @xmath253 and @xmath274 when using @xmath252 , @xmath254 , @xmath255 , @xmath97 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]   for the various algorithms . with @xmath273 when using @xmath253 and @xmath274 when using @xmath252 , @xmath254 , @xmath255 , @xmath97 , average over @xmath256 samples and @xmath257 for tm.1a.mc.,title=\"fig : \" ]    finally , in fig .",
    "[ figs_mean_time_t100_f3_v2_l4_c15_n1000_svar ] we show the average computation time of a single sample according to the algorithm versus the the probability of error at a certain @xmath248 .",
    "the differences are huge , thus the log scale , and overall , tm.2a.g seems to give the best performance - time trade - off for @xmath254 and @xmath255 .",
    "+      in this subsection , we consider the memory length @xmath28 as being the control parameter .",
    "we will be setting @xmath248 large so as to never have a completely decoded chain and thus we will be comparing the average number of errors only .    ) for various algorithms vs memory length @xmath28 . with @xmath254 , @xmath275 , @xmath276 ,",
    "average over @xmath256 samples and @xmath257 for tm.1a.mc .",
    ", title=\"fig : \" ] ) for various algorithms vs memory length @xmath28 . with @xmath254 , @xmath275 , @xmath276 ,",
    "average over @xmath256 samples and @xmath257 for tm.1a.mc .",
    ", title=\"fig : \" ]    we first consider the limit case where the cutoff parameter is @xmath276",
    ". this will enable us to compare the results between all algorithms , even the ones exponential in @xmath28 , though only for relatively small values of the parameter .",
    "the average number of errors for @xmath252 are shown in fig . [ figs_t100_s1_c2_fvar ] .",
    "there are very little differences between the algorithms when we consider @xmath253 , thus we do not show these figures .    as in the previous subsection , tm outperforms all other algorithms quite well .",
    "of the other algorithms , tm.1a returns the smallest number of errors though it seems to be caught up by tm.2a.g for larger values of @xmath28 . for both distributions",
    "@xmath71 and @xmath204 , tm.1a.f and tm.1a.mc are very similar though there is a slight advantage to tm.1a.f .",
    "finally , tm.1a.g performs very much like tm.1a.mc in the case where the distribution @xmath204 is used . on the other hand",
    ", it performs quite poorly , compared to tm.1a.mc , when @xmath71 is considered .    .",
    "with @xmath254 , @xmath275 , @xmath276 , using @xmath71 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc . ]    in fig .",
    "[ figs_time_t100_v2_l4_s1_c2_fvar ] we show the various times per sample for the different algorithms for @xmath71 and @xmath252 .",
    "the times are exactly the same for the other possible combinations of @xmath58 and @xmath59 .",
    "this figure shows why we limit ourselves to @xmath277 as an upper bound for tm , tm.1a and tm.1a.f .    ) for various algorithms vs memory length @xmath28 . the full line is the analytical lower bound ( almost always equal to zero in the upper two figures ) . with @xmath254 , @xmath278 , @xmath97 ,",
    "average over @xmath256 samples and @xmath257 for tm.1a.mc for all figures .",
    ", title=\"fig : \" ] ) for various algorithms vs memory length @xmath28 .",
    "the full line is the analytical lower bound ( almost always equal to zero in the upper two figures ) . with @xmath254 , @xmath278 , @xmath97 ,",
    "average over @xmath256 samples and @xmath257 for tm.1a.mc for all figures .",
    ", title=\"fig : \" ] ) for various algorithms vs memory length @xmath28 .",
    "the full line is the analytical lower bound ( almost always equal to zero in the upper two figures ) . with @xmath254 , @xmath278 , @xmath97 , average over @xmath256 samples and @xmath257 for tm.1a.mc for all figures .",
    ", title=\"fig : \" ] ) for various algorithms vs memory length @xmath28 .",
    "the full line is the analytical lower bound ( almost always equal to zero in the upper two figures ) . with @xmath254 , @xmath278 , @xmath97 ,",
    "average over @xmath256 samples and @xmath257 for tm.1a.mc for all figures .",
    ", title=\"fig : \" ]    we now consider the case where @xmath97 in fig .",
    "[ figs_t100_s0.2_fvar ] .",
    "we show all four combinations of @xmath71 and @xmath204 with @xmath252 and @xmath253 for the algorithms tm.1a.mc , tm.1a.g and tm.2a.g .    in all cases ,",
    "tm.2a.g outperforms the two others and performs better as @xmath28 increases . in both cases where we consider the function @xmath252 it performs much better than the two others . with @xmath264 , tm.1a.mc and",
    "tm.1a.g perform about the same , in all other combinations tm.1a.mc performs better . in general , these two algorithms reach a plateau value relatively quickly .    .",
    "with @xmath254 , @xmath278 , @xmath97 , using @xmath71 and @xmath252 , average over @xmath256 samples and @xmath257 for tm.1a.mc . ]    in fig .",
    "[ figs_time_t100_v2_l4_s0.2_c15_fvar ] we have again shown the computation times for the various algorithms . even though tm.2a.g systematically outperforms the two others in fig .",
    "[ figs_t100_s0.2_fvar ] , its computation time grows sub - exponentially with @xmath28 , though it does become large .",
    "+      in this subsection we study the influence of the total length @xmath14 .",
    "( @xmath279 ) in a chain of total length @xmath280 for various algorithms . with @xmath111 , @xmath267 , @xmath97 , @xmath281 , using @xmath204 and @xmath112 , average over @xmath256 samples and @xmath257 for tm.1a.mc . ]    the first figure in this subsection , fig .",
    "[ figs_p0.99_v3_l1_f13_pvd ] , shows the probability of correctly decoding each position @xmath27 ( @xmath279 ) in a chain of total length @xmath14 .",
    "we can see that for @xmath27 bigger than a certain threshold close to @xmath282 , @xmath279 rapidly decreases to a value close to @xmath283 .",
    "this can be explained by looking at the graph of @xmath112 in fig .",
    "[ fig_alpha ] where we can see that , for @xmath111 and @xmath110 , the maximum of @xmath105 is no longer for @xmath284 .    ) for various algorithms vs full chain length @xmath14 .",
    "the full line is the analytical lower bound . with @xmath111 , @xmath267 , @xmath97 , @xmath281 , using @xmath204 and @xmath112 , average over @xmath256 samples and @xmath257 for tm.1a.mc . ]     with @xmath285 , @xmath267 , @xmath97 , @xmath281 , using @xmath67 and @xmath112 , average over @xmath256 samples and @xmath257 for tm.1a.mc . ]    in fig .",
    "[ figs_p0.99_v3_l1_f13_tvar ] we show the probability of error @xmath250 for various algorithms with @xmath204 and @xmath111 as a function of the total length @xmath14 . in fig .",
    "[ figs_p0.999_v1_l1_f13_pcd_vs_t ] we show the same thing with @xmath67 and @xmath285 .",
    "they confirm what was shown in fig .",
    "[ figs_p0.99_v3_l1_f13_pvd ] . in this particular regime ,",
    "i.e. with @xmath112 , tm.1a.mc slightly outperforms tm.1a.g and tm.2a.g .",
    "furthermore , the two latter algorithms seem to behave exactly in the same way to variations of @xmath14 when @xmath112 is used .    ) vs @xmath14 for tm.1a.mc . with @xmath111 , @xmath267 , @xmath97 , @xmath281 , using @xmath112 , average over @xmath256 samples and @xmath257 . ]    finally , in fig .",
    "[ figs_p0.999_v1_l1_f13_pcd_vs_t ] we see that @xmath250 never reaches zero .",
    "this behavior is detailed in fig .",
    "[ figs_p0.99_l1_f13_numerr ] where we see that for very small @xmath14 the number of errors is non vanishing when @xmath67 is used while it is when @xmath204 is used .",
    "the reason for this is that when the chain is generated using @xmath67 then @xmath286 with positive probability : the decoder fails in these cases . this is the only notable difference when using @xmath67 instead of @xmath204 .    ) for tm.1a.mc vs full chain length @xmath14 for various values of the incorporation rate @xmath78 . with @xmath267 , @xmath97 , @xmath281 , using @xmath204 and @xmath112 , average over @xmath256 samples and @xmath257 . ]    in fig .",
    "[ figs_a2_v3_l1_f13_tvar_pvar ] we plot the probability of error as a function of @xmath14 for various values of the incorporation rate @xmath78 for @xmath204 and @xmath112 . as @xmath78 increases",
    ", the performance does increase as well .",
    "+      in this final section , we use parameter values inspired by real data . these values ( used in @xcite )",
    "are obtained from @xcite which presents pyrosequencing data generated from two sets of tests and where a maximum likelihood sequence detection ( mlsd ) algorithm is developed .",
    "these sets are obtained from two separate sequencing systems , the first set of tests is obtained on a pyrosequencing psq96ma system for a set of @xmath102 templates ranging from @xmath287 to @xmath288 base sub - sequences . the second test run consisted of @xmath289 data sets extracted using a @xmath290 genome sequencer 20 ( gs20 ) system .",
    "these two machines are examples of two separate generations of sequencing systems having different precisions . the mathematical model which was developed to apply the mlsd algorithm was one of the inspirations for this work to which we applied an additional set of approximations and which we generalized .",
    "they estimate that the incorporation rate is @xmath291 for the psq96ma system and @xmath292 for the gs20 , while the read error introduces a gaussian noise with a standard deviation of @xmath293 in both cases . in our approach , we have considered a modified base sequence composed of a two letter alphabet in place of the @xmath83 letters that compose dna strands .",
    "the output being read as the number of repetitions of a single base in both cases ( hp sub - sequences ) , the main difference in both approaches is seen in the modification of the memory function @xmath59 ( [ eq_real_alpha ] ) which has a simplified expression from that of the similar relation defined in @xcite . though we estimate that this consideration does not modify significantly the read length and therefore that the behavior of our @xmath294-base model will be similar to that of the real @xmath83-base sequences .",
    "one thing that might lack is the non - specific incorporation rate that involves false positives .",
    "this difference can result in errorless read lengths that are higher than in experimental data , though its estimated value smaller than @xmath295 will result in only a slight increase .",
    "these issues will be further addressed in a future communication , here we will limit ourselves to presenting a few results on the behavior of our model using the parameters introduced at the beginning of this paragraph .     ( @xmath279 ) in a chain of total length @xmath280 for various algorithms . with @xmath291 , ( @xmath296 ) ,",
    "@xmath97 , @xmath297 , using @xmath67 and @xmath112 , average over @xmath256 samples and @xmath257 for tm.1a.mc . ]     with @xmath292 , ( @xmath298 ) . ]    in figure [ figs_p0.9955_s0.08_t300_i500_n1000 ] we have plotted the probability of correctly decoding each position of a chain of length @xmath280 for @xmath291 for three algorithms .",
    "this would emulate extracting sequences using the psq96ma system . as already mentioned in previous sections , tm.1a.mc performs better than the gauss algorithms with these parameters for higher values of the position @xmath27 within the chain . in figure [ figs_p0.9987_s0.08_t900_i500_n1000 ]",
    "we plot the same probability for a chain of total length @xmath299 with @xmath292 and which , in this case , would emulate extraction on a gs20 system . in @xcite ,",
    "the mlsd algorithm was capable of correctly reading @xmath300 base sub - sequences out of @xmath301 and @xmath302 out of @xmath303 of the longest two templates ran on the psq96ma and all bases for templates of length @xmath304 on the gs20 . by comparing these results to our own",
    ", we can say that they are at least consistent .",
    "indeed , in figure [ figs_p0.9955_s0.08_t300_i500_n1000 ] we have @xmath305 for values of @xmath306 .",
    "we can say this since the presented algorithms do not depend directly on the total length @xmath14 but only on the position @xmath27 .",
    "furthermore , for a total length of @xmath280 the average number of errors encountered during the decoding process of the tm.1a.mc algorithm is @xmath307 over @xmath256 samples .",
    "similar results are also true concerning data presented in figure [ figs_p0.9987_s0.08_t900_i500_n1000 ] although all we can say in comparison to @xcite is that we do indeed decode correctly chains of total length @xmath304 .",
    "also , they predict that correct decoding is possible for sequences of length greater than @xmath308 which we easily obtain ( we have an average number of errors of @xmath309 in decoding chains of length @xmath299 with tm.1a.mc ) .    we conclude that our data is in good concordance with results obtained experimentally in @xcite , though we emphasize again that direct testing of our algorithms still needs to be performed .",
    "we defined three low complexity algorithms based on the original high complexity transfer matrix algorithm ( tm ) described in [ subsection_algo_tm ] .",
    "of these three , two are first order approximations : the monte carlo algorithm ( tm.1a.mc ) and the gauss algorithm ( tm.1a.g ) described respectively in [ subsubsection_mc ] and [ subsection_algo_gauss ] . the final algorithm is a second order approximation that is based on the gauss algorithm ( tm.2a.g ) and is described in [ subsection_algo_two_point ] .",
    "the performances of these algorithms were studied in the previous section [ section_results ] where we show that the second order approximation with gauss ( tm.2a.g ) is the best performing algorithm when the memory @xmath28 is small and the first order approximation with monte carlo ( tm.1a.mc ) performs best when the memory @xmath28 is large .",
    "+ besides its direct application to hidden markov models of higher order , the description introduced in this paper has yet to be tested on real pyrosequencing data .",
    "the context being different and the variations that need to be brought to our approach being quite substantial , this issue will be addressed directly in a future communication though preliminary comparison to similar work is promising .",
    "as stated in section [ subsection_algo_gauss ] , we assume that the variable @xmath166 present in @xmath167 at step @xmath27 ( eq . [ eq_psi ] ) can be approximated with a gaussian random variable .",
    "there are two steps for each iteration to derive the marginal distribution of @xmath310 using this method .",
    "we suppose the iterative process of the transfer matrix has brought us to position @xmath27 .",
    "then , the first step is to calculate the mean and variance of the variable @xmath311 under the gaussian approximation .",
    "these can be written , respectively , as @xmath312 where @xmath313 and @xmath314 are the expectation and variance calculated at position @xmath113 using the distributions @xmath315 which where obtained at the previous iteration .    we can then define the gaussian variable @xmath316 such that its probability density function is @xmath317 , \\ ] ] and finally , the probability of having value @xmath49 at step @xmath27 is @xmath318 ^ 2 \\right ] , \\label{eq_integral_marg_first_step}\\ ] ] which yields @xmath319 ^ 2   \\right ] , \\label{equation_gauss_z_distribution}\\ ] ] where @xmath127 is a normalization constant .",
    "we then keep @xmath320 for the next step and for subsequent iterations .",
    "+ the second step is then to notice that for all @xmath321 we can rewrite eq .",
    "( [ eq_first_order_forwards ] ) as @xmath322 , \\ ] ] where @xmath127 is a normalization factor",
    ".    then we define step and position dependent mean and variance of the variables @xmath323 as @xmath324 where we artificially set @xmath325 and the same for the variance . from these and a similar expression",
    "( [ eq_integral_marg_first_step ] ) we recover eq .",
    "( [ eq_marginal_gauss ] ) .    for subsequent iterations ,",
    "we keep @xmath326 and @xmath327 .",
    "as stated in section [ subsection_algo_two_point ] we consider two point interactions only over closest neighbors . since we can reconstruct one - point marginals as marginals of the two - point marginals , we need only compute the latter .",
    "we use a similar method to the one used in section [ subsection_algo_gauss ] through approximating our sum variable @xmath328 with a gaussian random variable with mean and variance which are expressed in the following two step procedure .",
    "we start by calculating @xmath329 and @xmath330 , respectively mean and variance of @xmath331 : @xmath332 \\ , , \\label{eq_mu_z_tp}\\\\ \\sigma_{x_{a-1,a}}^2 = \\var ( x_{a-1,a } ) & = & \\sum _ {   k=1 } ^{a-2 } \\alpha(k , a)^2 ~ \\var^{(a-1 ) } ( x_k | x_{a-1 } ) + \\nonumber \\\\ & & + 2 \\sum _ {",
    "1 \\leq k < k ' < a-1 } \\alpha(k , a ) \\alpha(k',a ) ~ \\operatorname{cov}^{(a-1 ) } ( x_k x_{k ' } |x_{a-1 } )   \\,,\\end{aligned}\\ ] ] where @xmath333 - \\e^{(a-1 ) } [ x_{k } | x_{a-1}]^2 , \\\\ \\operatorname{cov}^{(a-1 ) } ( x_k x_{k ' } |x_{a-1 } )   & = & e^{(a-1 ) } [ x_{k } x_{k ' } | x_{a-1 } ] - \\e^{(a-1 ) } [ x_{k } | x_{a-1 } ] \\e^{(a-1 ) } [ x_{k ' } | x_{a-1 } ] \\,,\\end{aligned}\\ ] ] and which yield @xmath334 ^ 2     \\right ] } \\ , .",
    "\\label{eq_z_am1_a}\\ ] ]    once these values obtained , we define @xmath335 and @xmath336 which are injected into the second step of the procedure which is to compute @xmath337 for @xmath338 . again , we approximate the sum variable @xmath339 with a gaussian random variable with mean and variance respectively @xmath340 \\ , , \\\\",
    "\\label{eq_mean_two_point } \\sigma_{x_{i , i+1}}^2 = v_a + \\var ( x_{i , i+1 } ) & = & v_a + \\sum _ {   \\substack { k=1 \\\\ k \\neq i , i+1 } } ^{a-1 } \\alpha(k , a)^2 ~ \\var^{(a-1 ) } ( x_k | x_{i } , x_{i+1 } ) + \\nonumber \\\\   & & + 2 \\sum _ {   \\substack { 1 \\leq k < k ' \\leq a \\\\",
    "k , k ' \\neq i , i+1 } } \\alpha(k , a ) \\alpha(k',a ) ~ \\operatorname{cov}^{(a-1 ) } ( x_k x_{k ' } |x_{i } , x_{i+1 } )   \\ , , \\label{eq_var_two_point}\\end{aligned}\\ ] ] which enable us to compute @xmath341 ^ 2     \\right ] } \\ , , \\label{eq_z_i_ip1_appendix}\\ ] ] which is the expression in ( [ eq_z_i_ip1 ] ) . +",
    "in writing these equations ( [ eq_mu_z_tp ] - [ eq_z_i_ip1_appendix ] ) we used several expressions that we need to give more detail to .",
    "first of all , in the situation where a distribution can be decomposed on one dimensional factor graph , we can express the conditional probabilities as @xmath342 where @xmath343 expresses probabilities in general .",
    "we can then use the previous expression to compute the necessary expectations for eqs .",
    "( [ eq_z_am1_a ] ) and ( [ eq_z_i_ip1_appendix ] ) , in all following cases @xmath344 and @xmath345 when necessary , @xmath346 & = & \\sum_{x_k = 1}^c x_k ~ \\nu^{(a ) } ( x_k | x_i ) \\ , , \\end{aligned}\\ ] ] @xmath347 & = & \\e^{(a ) } [ x_k | x_i ] \\e^{(a ) } [ x_{k ' } | x_i ] ~~~~~~~~~~~~~~~~~~~~~~~~~\\textrm { if } k <",
    "i < k '   \\ , , \\\\   & = & \\sum_{x_k , x_{k ' } = 1}^c x_k x_{k ' }   ~ \\nu^{(a ) } ( x_i | x_k ) \\nu^{(a ) } ( x_k | x_{k ' } ) ~~~~~~ \\textrm { if } i < k\\ , , \\\\   & = & \\sum_{x_k , x_{k ' } = 1}^c x_k x_{k ' }   ~ \\nu^{(a ) } ( x_i | x_{k ' } ) \\nu^{(a ) } ( x_{k ' } | x_{k } ) ~~~~~ \\textrm { otherwise}\\ , , \\end{aligned}\\ ] ] @xmath348 & = & \\e^{(a ) } [ x_k | x_i ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\textrm { if } k < i\\ , , \\\\   & = & \\e^{(a ) } [ x_k | x_j ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\textrm { if } k > j , \\\\   & = & \\sum_{x_k = 1}^c x_k ~ \\frac{\\nu^{(a ) } ( x_i |",
    "x_k ) \\nu^{(a ) } ( x_k | x_{j})}{\\nu^{(a ) } ( x_i |",
    "x_j ) } ~~~~~~ \\textrm { otherwise}\\ , ,    \\end{aligned}\\ ] ] @xmath349 & = &   \\e^{(a ) } [ x_k x_{k ' } | x_i ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\textrm { if } k'<i\\ , , \\\\   &",
    "= &   \\e^{(a ) } [ x_k x_{k ' } | x_j ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\textrm { if } k > j\\ , , \\\\   & = & \\e^{(a ) } [ x_k | x_i ] \\e^{(a ) } [ x_{k ' } | x_i , x_j   ] ~~~~~~~~~~~~~~~~~~~~~~~~~ \\textrm { if } k < i < k'<j\\ , , \\\\   & = &   \\e^{(a ) } [ x_k | x_i , x_j   ] \\e^{(a ) } [ x_{k ' } | x_j ] ~~~~~~~~~~~~~~~~~~~~~~~~~ \\textrm { if } i < k < j < k'\\ , , \\\\   & = &   \\e^{(a ) } [ x_k | x_i , x_j   ] \\e^{(a ) } [ x_{k ' } | x_i , x_j   ] ~~~~~~~~~~~~~~~~~~~~~ \\textrm { if } k < i < j < k'\\ , , \\\\   & = & \\sum_{x_k , x_{k ' } = 1}^c x_k x_{k ' }   ~   \\frac{\\nu^{(a ) } ( x_i | x_{k } ) \\nu^{(a ) } ( x_k | x_{k ' } )   \\nu^{(a ) } ( x_{k ' } | x_{j } ) } { \\nu^{(a ) } ( x_i | x_{j } ) } \\\\   & & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\textrm {",
    "if } i < k < k'<j \\ , .",
    "\\nonumber\\end{aligned}\\ ] ] + finally , in most cases , the values of @xmath337 obtained numerically , though they do contain the information we are looking for , are not a distribution .",
    "that is , more often than not do we get @xmath350 . to overcome this",
    "we introduce a new set of _ real _ distributions @xmath351 such that the kullback - leibler divergences , regarded as distances , between the @xmath352 and the @xmath353 are minimized .",
    "that is , we wish to minimize the quantity @xmath354 given the constraints @xmath355 that is , by using lagrange multipliers , we wish to minimize @xmath356 \\,,\\ ] ] which , after differentiation , results in @xmath357 for all @xmath113 and all couples @xmath358 .",
    "we define @xmath359 , hence we have @xmath360 furthermore @xmath361 where the left hand side of eqs .",
    "( [ eq_two_point_gamma_im1_i ] ) and ( [ eq_two_point_gamma_ip1_i ] ) are equal by the constraint ( [ eq_two_point_lagrange_constraint ] ) and thus by equating the right hand sides we have @xmath362 we initiate the procedure by setting all values of @xmath363 in the right hand side to the value @xmath62 .",
    "the values obtained on the left hand side are then reinjected into the right hand side iteratively until the values of the @xmath363 of all @xmath113 and @xmath45 are equal on both sides of the equation . to prevent the appearance of static cycles",
    ", we produce the update at each step by randomly choosing the order in which we take the functions @xmath364 for all @xmath113 .",
    "y.  wang , l.  zhou , j.  feng , j.  wang , and z .- q .",
    "liu , `` mining complex time - series data by learning markovian models , '' _ data mining , 2006 .",
    "sixth international conference on _ , pp .  11361140 , dec .",
    "2006 .",
    "h.  eltoukhy and a.  el  gamal , `` modeling and base - calling for dna sequencing - by - synthesis , '' _ acoustics , speech and signal processing , 2006 .",
    "icassp 2006 proceedings .",
    "2006 ieee international conference on _ , vol .  2 ,",
    "ii  ii , 2006 .              a.  svantesson , p.  o. westermark , j.  h. kotaleski , b.  gharizadeh , a.  lansner , and p.  nyrn , `` a mathematical model of the pyrosequencing reaction system , '' _ biophysical chemistry _ ,",
    "110 , no .  1 - 2 , pp .  129  145 , 2004"
  ],
  "abstract_text": [
    "<S> inferring the sequence of states from observations is one of the most fundamental problems in hidden markov models . in statistical physics language , </S>",
    "<S> this problem is equivalent to computing the marginals of a one - dimensional model with a random external field . </S>",
    "<S> while this task can be accomplished through transfer matrix methods , it becomes quickly intractable when the underlying state space is large .    </S>",
    "<S> this paper develops several low - complexity approximate algorithms to address this inference problem when the state space becomes large . </S>",
    "<S> the new algorithms are based on various mean - field approximations of the transfer matrix . </S>",
    "<S> their performances are studied in detail on a simple realistic model for dna pyrosequencing . </S>"
  ]
}