{
  "article_text": [
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  jeffreys s development of the bayes factor resembles an experimental design for which one studies where the likelihood functions overlap , how they differ , and in what way the difference can be apparent from the data . \" _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the discussion on harold jeffreys s default bayes factor hypothesis tests written by alexander ly , josine verhagen , and eric - jan wagenmakers is both a worthwhile survey and an updating reinterpretation _ cum _ explanation of harold jeffreys  views on testing .",
    "the historical aspects of the paper offer little grip for critical discussion as they stand true to the unravelling of the testing perspective in harold jeffreys _ theory of probability _ ( top ) , as a worthy complement to our earlier exegesis @xcite .",
    "i also agree with the focus chosen therein on the construction of a default solution for the bayes factor , as this issue is both central to jeffreys thinking and to the defence of the  bayesian choice \" in hypothesis testing .",
    "my own discussion is therefore mostly written in the    the plan of the paper is as follows : in section [ sec : og ] , i discuss both the presentation made by the authors and the argumentation coming from jeffreys about using bayes factors .",
    "the next section presents further arguments against the use of the bayes factor , while section 4 introduces the alternative of a mixture representation put forward by @xcite .",
    "section 5 is a short conclusion .",
    "ly et al . ( 2015 ) starts with a short historical entry on jeffreys work and career , which includes four of his principles , quoted verbatim from the paper :    1 .   ",
    "scientific progress depends primarily on induction \" ; 2 .",
    " in order to formalize induction one requires a logic of partial belief \" [ thus enters the bayesian paradigm ] ; 3 .   ",
    "scientific hypotheses can be assigned prior plausibility in accordance with their complexity \" [ a.k.a .",
    ", occam s razor ] ; 4 .",
    " classical  fisherian `` @xmath0-values are inadequate for the purpose of hypothesis testing '' .",
    "while i agree with those principles on a general basis , the third principle remains too vague for my own taste and opens a pandora box about the meanings of what is simple and what is penalty .",
    "( i have had the same difficulty with the call to occam s razor principle in other papers like @xcite and @xcite . )",
    "it is all very clear to follow such a rule for a one - parameter distribution like the normal @xmath1 distribution , but much less so with  a \" model involving hundreds of parameters and latent variables .",
    "i do not think harold jeffreys envisioned at all a general setting of comparing multiple models , in particular because the @xmath2 partition of the probability mass has very little to suggest in terms of extensions , with too many potential alternatives .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  is it of the slightest use to reject a hypothesis until we have some idea of what to put in its place ? \"",
    "h. jeffreys , top ( p.390 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    i obviously support very much the above quote from jeffreys top , as indeed rejecting a null hypothesis does not sound as an ideal ultimate goal for statistical inference , but i am also less than convinced about the argument that testing should be separated from estimation ( p.5 ) , even though i recognise the need for defining a separate prior and parameter .",
    "my central difficulty stands with the issue of picking a prior probability of a model , when prior opinions about different models are at best qualitative and at worst missing .",
    "for instance , when invoking occam s razor @xcite , there is no constructive way of deriving a prior probability @xmath3 for model @xmath4 .    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  the priors do not represent substantive knowledge of the parameters within the model \" h. jeffreys , top ( p.13 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    a very relevant point made by the authors in this discussion of top is that harold jeffreys only considered embedded or nested hypotheses , a fact that allows for some common parameters between models and hence some form of reference prior , as argued in @xcite .",
    "in jeffreys top setting , it nonetheless seems mathematically delicate to precise the notion of  common \" parameters , in particular to call for the same ( improper ) prior on both parameter sets , as discussed in @xcite .",
    "however , the most sensitive issue is , from my perspective , the derivation of a reference prior on the parameter of interest , which is both fixed under the null and perspective , the derivation of a reference prior on the parameter of interest , which is both fixed under the null and unknown under the alternative in top .",
    "this state of affairs leads to the unfortunate impossibility of utilising improper priors in most testing settings .",
    "harold jeffreys thus tried to calibrate the corresponding proper prior by imposing asymptotic consistency under the alternative and by introducing the notion of  exact \" indeterminacy under  completely uninformative \" data .",
    "unfortunately , this is not a well - defined concept . that both predictives take the same values for such",
    "completely uninformative \" data thus sounds more like a post - hoc justification than a way of truly calibrating the bayes factor : that any sample with too small a size is  completely uninformative \" is for instance unconvincing .",
    "( why should nt one pick the simplest model by default ? ) further , to impose for the bayes factor to be one for _ all _ samples with too small a sample size sounds mathematically impossible to achieve in full generality , although two specific cases are provided in top and reproduced in the current paper .",
    "the reconstruction of jeffreys derivation of his reference prior on pp.10 - 12 of the authors discussion is quite illuminating of those difficulties ( while also praiseworthy for its clarity ) .",
    "it also shows that the very notion of  common \" parameter can not be made into a precise mathematical concept .",
    "for instance , if model @xmath4 corresponds to the linear regression with a single covariate @xmath5 and model @xmath6 to the linear regression with an additional covariate @xmath7 except for using the same symbols , there is plenty of room for arguing against the fact that @xmath8 is  common \" to both models .",
    "we certainly expect @xmath9 to shrink as we introduce a secondary explanatory variable , while the variability of the observable around the regression function should diminish .",
    "a further mathematical difficulty with a nested model is that a prior @xmath10 on the parameters of the embedding model tells us nothing on the embedded model since @xmath11 is not defined in a unique manner @xcite .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  the objective comparison between @xmath12 and @xmath13 is then to keep all aspects the same @xmath14 . \" _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    in the normal example , the authors recall and follow the proposal of harold jeffreys to use an improper prior @xmath15 on the nuisance parameter and argue in his defence the quote above .",
    "i find their argument weak in that , if we use an improper prior for @xmath16 , the marginal likelihood on the data as given in ( 9)which should not be indexed by @xmath17 or @xmath18 since both are integrated out  is no longer a probability density and i do not follow the argument that one should use the same measure with the same constant both on @xmath18 alone  for the nested hypothesis  and on the @xmath18 part of @xmath19for the nesting hypothesis .",
    "indeed , we are considering two separate parameter spaces with different dimensions and hence necessarily unrelated measures . once again",
    ", this quote thus sounds more like wishful thinking than like a genuine justification .",
    "similarly , assumptions of independence between @xmath20 and @xmath18 are not relevant for @xmath18-finite measures ( even though @xcite would object to this statement ) .",
    "note that the authors later point out that the posterior on @xmath18 varies between models despite using the same data ( which somewhat argues that the parameter @xmath18 is far from common to both models ! ) . from jeffreys s perspective , the [ testing ] cauchy prior on @xmath17 is only useful for the testing part and would thus have to be replaced with another [ estimation ] prior once the model has been selected [ by looking at the data ] .",
    "this may thus end up as a back - firing argument about the ( far from unique ) default choice .",
    "incidentally , i fail to understand in top the relevance of separating ( 10 ) and @xmath21 from the general case as this former event happens with probability zero , making jeffreys argument at best an approximation to the limiting case of ( 11 ) .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  using bayes theorem , these priors can then be updated to posteriors conditioned on the data that were actually observed . \" _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the re - derivation of jeffreys conclusion that a cauchy prior should be used on @xmath20 highlights the issue that this choice only proceeds from an imperative of fat tails in the prior , without in the least solving the calibration of the cauchy scale , which has no particular reason to be set to @xmath22 .",
    "the choice thus involves arbitrariness to a rather high degree .",
    "( given the now - available modern computing tools , it would be nice to see the impact of this scale @xmath23 on the numerical value of the bayes factor . ) and the above choice may also proceed from a  hidden agenda \" , namely to achieve a bayes factor that solely depends on the @xmath24 statistic . but this does not sound like a such compelling reason , given that the @xmath24 statistic is not sufficient in this setting .    in a separately interesting way",
    ", the authors mention the savage - dickey ratio ( p.17 ) as a computational technique to represent the bayes factor for nested models , without necessarily perceiving the mathematical difficulty with this ratio that @xcite exposed a few years ago . for instance , in the psychology example processed in the paper , the test is between @xmath25 and @xmath26 ; however , if i set @xmath27 under the alternative prior , which should not matter [ from a measure - theoretic perspective where the density is uniquely defined almost everywhere ] , the savage - dickey representation of the bayes factor returns zero , instead of 9.18 ! the potential for trouble is even clearer in the one - sided case illustrated on figure 2 , since the prior density is uniformly zero before @xmath25 and can be anything , including zero at @xmath25 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  in general , the fact that different priors result in different bayes factors should not come as a surprise . \" _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the second example detailed in the paper is the test for a zero gaussian correlation .",
    "this is a sort of  ideal case \" in that the parameter of interest is between -1 and 1 , hence makes the choice of a uniform u(-1,1 ) easy or easier to argue .",
    "furthermore , the setting is also  ideal \" in that the bayes factor simplifies down to a marginal over the sample correlation @xmath28 by itself , under the usual jeffreys priors on means and variances .",
    "so we have here a second case where the frequentist statistic behind the frequentist test[ing procedure ] is also the single ( and insufficient ) part of the data used in the bayesian test[ing procedure ] . once again , we thus are in a setting where bayesian and frequentist answers are in one - to - one correspondence ( at least for a fixed sample size ) and where the bayes factor allows for a closed form through hypergeometric functions , even in the one - sided case .",
    "( this is a result obtained by the authors , not by harold jeffreys who , as the proper physicist he was , obtained approximations that are remarkably accurate ! )    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  the bayes factor ( ... ) balances the tension between parsimony and goodness of fit , ( ... ) against overfitting the data . \" _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    _ in fine _ , i liked very much this re - reading of jeffreys approach to bayesian testing , maybe the more because i now consider we should move away from this approach as discussed below .",
    "however , i am not certain the discussion will help in convincing psychologists to adopt bayes factors for assessing their experiments as it may instead frighten them away . and as it does not bring an answer to the vexing issue of the relevance of point null hypotheses .",
    "but the paper constitutes a lucid and innovative treatment of the major advance represented by jeffreys formalisation of bayesian testing .",
    "in this section , i extrapolate on some difficulties i have with the bayes factor , as discussed in more depth in @xcite .    the natural bayesian decision - theoretic approach to decide between two models",
    "is to use a binary @xmath29 loss function and to derive the posterior probabilities of the model .",
    "however , since this approach depends on the choice of unnatural prior weights , harold jeffreys advocates in top its replacement with bayes factors that eliminate this dependence . unfortunately , while indicative of the respective supports brought by the data through their comparison with 1 , bayes factors escape a direct connection with the posterior distribution , for the very reason they eliminate the prior weights .",
    "therefore , they lack the direct scaling associated with a posterior probability and a loss function .",
    "this implies that they face a subsequent and delicate interpretation ( or calibration ) and explains why top does not contain a section on decision making using bayes factors , instead providing in the appendix a logarithmic scale of strength that is purely qualitative .",
    "note that posterior probabilities face similar difficulties , in that they suffer from the unavoidable tendency to interpret them as @xmath0-values and to scale them against the 5%  reference value when _ de facto _ they only report through a marginal likelihood ratio the respective strengths of fitting the data to both models .",
    "( this difficulty is not to be confused with the divergence in the [ frequentist versus epistemic ] interpretations of the probability statement , as discussed in @xcite and the ensuing comments . )    at a different semantic level , the long - going [ or long - winded ] criticism on the bayesian approach , namely the dependence on a subjective prior measure applies here twofold : first in the linear impact of the prior weights of the models under comparison and second in the lasting impact of the prior modelling on the parameter spaces of both models under comparison .",
    "( we stress as in @xcite a rather overlooked feature answering such criticisms ( see , e.g. , @xcite ) , including the lindley - jeffreys ( @xcite ) paradox , namely the overall consistency of bayes factors . )",
    "however , the resulting discontinuity in the use of improper priors is a feature i have always been uncomfortable with , since those improper priors are not justified @xcite in most testing situations , leading to many alternative if _ ad hoc _ solutions @xcite , where data is either used twice @xcite or split in artificial ways @xcite .    as pointed out in the above ,",
    "the posterior probability is more often associated with a binary ( _ accept _ vs.  _ reject _ ) outcome that is more suited for immediate decision ( if any ) than for model evaluation , in connection with the rudimentary loss function behind it , while the bayes factor is a smoother form of comparison that should not _ in fine _ conclude with a ( hard ) decision .",
    "given the current abuse of @xmath0-values and significance tests @xcite , we should advocate more strongly this stand . in conjunction with this point , note further that returning a posterior probability or a bayes factor offers no window into the uncertainty associated with the decision itself , unless one proceeds through additional and most likely costly simulation experiments .    from a computational perspective ,",
    "let me recall there is no universally acknowledged approach to compute marginal likelihoods and bayes factors @xcite , while some approaches are notoriously misleading @xcite .",
    "in a recent work about the validation of abc model selection @xcite , we also pointed out the variability of the numerical estimates and _ in fine _ the utter dependence of both posterior probabilities and bayes factors on conditioning statistics , which in turn undermines their validity for model assessment .",
    "the alternative to testing via bayes factors , as proposed in @xcite , to which the reader is referred to for details , constitutes a paradigm shift in the bayesian processing of hypothesis testing and of model selection in that it reformulates both the question and the answer into a new framework that accounts for uncertainty and returns a posterior distribution instead of a single number or a decision . as demonstrated in @xcite , this shift offers convergent and naturally interpretable solution , while encompassing a more extended use of improper priors . the central idea to the approach is to adopt a simple representation of the testing problem as a two - component mixture estimation problem where the weights are formally equal to @xmath30 or @xmath22 and to estimate those weights as in a regular mixture estimation framework .",
    "this approach is inspired from the consistency results of @xcite on estimated overfitting mixtures , i.e. , mixture models where the data is actually issued from a mixture distribution with a smaller number of components .",
    "more formally , given two statistical models , @xmath31 @xcite define the ( arithmetic ) encompassing mixture model @xmath32 with a mixture weight @xmath33 , meaning that each element of the iid sample associated with the model comparison is considered as generated according to @xmath34 . while this new and artificial model contains or encompasses both @xmath35 and @xmath36 as two special cases , that is , when @xmath37 and @xmath38 , a standard bayesian analysis of the above mixture provides an estimate of the weight @xmath39 , relying on a prior distribution @xmath40 with support the entire @xmath41 interval , e.g. , a beta @xmath42 distribution .",
    "this means that such a standard processing of the model will create a posterior distribution on the weight @xmath39 , given the data , which location on the unit interval will induce evidence ( and strength of evidence ) in favour of one model versus the other .",
    "for instance , when this posterior is concentrated near zero , the data supports more strongly @xmath36 than @xmath35 .",
    "hence , this alternative paradigm does not return a value in the binary set @xmath43 as a more traditional decision strategy or a test would do .",
    "thus , the mixture representation is quite distinct from making a choice between both models ( or hypotheses ) or even from computing a posterior probability of @xmath35 or @xmath36 .",
    "inference on the mixture representation bypasses the testing difficulties produced in the previous section in that there is no decision involved .",
    "i thus consider it has the potential of a more natural and easier to implement approach to testing , while not expanding on the total number of parameters when compared with the original approach ( as found in top ) .",
    "@xcite argue in favour of this shift from several perspectives , ranging from inferential to computational , and i once again refer to this paper for further implementation details , consistency proof , and examples .    while the encompassing mixture model @xmath34 intrinsically differs from the true model , given that the weight @xmath39 can take any value in @xmath41 , the production of a posterior distribution on the weight @xmath39 must be deemed to be a positive feature of this completely different approach to testing in that it bypasses the vexing issue of setting artificial prior probabilities on the different model indices and that it measures the proximity of the data to both models by a probability to allocate _ each _ datapoint to those models . furthermore , the mixture model @xmath34 allows for the use of partially improper priors since both components may then enjoy common parameters , as for instance location and scale parameters .",
    "this feature implies that using the _ same _ reference measure on the nuisance parameters of both models is then absolutely valid . estimating a mixture model by mcmc tools is well - established @xcite and bypasses the difficulty in computing the bayes factor . at last ,",
    "an approach by mixture modelling is quite easily calibrated by solutions like the parametric bootstrap , which that there is no decision involved .",
    "i thus consider this novel formalism has the potential of a better approach to testing , while not expanding on the number of parameters when compared with the original approach ( as found in top ) .",
    "@xcite argue in favour of this shift from several perspectives , from inferential to computational , and i once again refer to this paper for further details .    while the encompassing model @xmath34 intrinsically differs from the real model , given that the weight @xmath39 can take any value in @xmath41 , the production of a posterior distribution on the weight @xmath39 is a positive feature of this approach in that it bypasses the vexing issue of setting artificial prior probabilities on model indices and measures the proximity of the data to the models . furthermore , @xmath34 allows for the use of partially improper priors since both components may then enjoy common parameters , as for instance location and scale parameters .",
    "this implies that using the same reference measure on the nuisance parameters of both models is then completely valid .",
    "at last , the approach by mixture modelling is quite easily calibrated by solutions like the parametric bootstrap , which provides a reference posterior of @xmath39 under each of the models under comparison .    from a practical perspective  even though it involves a paradigm shift from the current practice of referring to a gold standard , like @xmath44 , the implementation of the principle of @xcite means estimating the mixture model by a computational tool like mcmc and exploiting the resulting posterior distribution on @xmath39 in the same way any posterior is to be interpreted . rather than advocating hard decision bounds associated with such a posterior @xmath45 , as in an alternative @xmath0-value with similar drawbacks @xcite ,",
    "it is more natural to contemplate the concentration of this distribution near the boundaries , @xmath30 and @xmath22 , in absolute terms and relative to the concentration of a posterior associated with a sample from either model . for a sample size that is large enough",
    ", this concentration should be clear enough to conclude in favour of one model .",
    "figure [ fig : nono ] illustrates this approach , for normal samples of sizes ranging from @xmath46 to @xmath47 , when opposing the point null @xmath48 model ( @xmath36 ) to the alternative @xmath49 model ( @xmath35 ) , under a proper @xmath50 prior . as can be inferred from the left panel , the posterior estimates of @xmath39 , whether posterior means or posterior medians , concentrate faster with @xmath51 on the relevant boundary , that is , close to zero and in favour of @xmath35 , than the exact posterior probability ( associated with a prior weight of @xmath52 on both models , and hence allows us to conclude more quickly about the evidence in favour of the null model .",
    "as seen from the right panel , the impact of the hyper - parameter value in the prior modelling @xmath53 remains moderate .    ,",
    "and of the posterior probabilities of model @xmath54 _ ( blue ) _ evaluated over 100 replicas of @xmath55 datasets with sample sizes @xmath56 ; _ ( right ) _ evolution across sample sizes of the averages of the posterior means and posterior medians of @xmath39 , and of the posterior probabilities @xmath57 , where @xmath35 stands for the @xmath49 model .",
    "each posterior estimation of @xmath39 is based on @xmath58 metropolis - hastings iterations .",
    "_ [ source : @xcite , with permission.]_,title=\"fig : \" ] , and of the posterior probabilities of model @xmath54 _ ( blue ) _ evaluated over 100 replicas of @xmath55 datasets with sample sizes @xmath56 ; _ ( right ) _ evolution across sample sizes of the averages of the posterior means and posterior medians of @xmath39 , and of the posterior probabilities @xmath57 , where @xmath35 stands for the @xmath49 model .",
    "each posterior estimation of @xmath39 is based on @xmath58 metropolis - hastings iterations .",
    "_ [ source : @xcite , with permission.]_,title=\"fig : \" ]",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  in induction there is no harm in being occasionally wrong ; it is inevitable that we shall be . \"",
    "h. jeffreys , top ( p.302 ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    as a genuine pioneer in the field , harold jeffreys set a well - defined track , namely the bayes factor , for conducting bayesian testing and by extension model selection , a track that has become the norm in bayesian analysis , while incorporating the fundamental aspect of reference priors and highly limited prior information .",
    "however , i see this solution as a child of its time , namely , as impacted by the on - going formalisation of testing by other pioneers like jerzy neyman or egon pearson . returning a single quantity for the comparison of two models fits naturally in decision making , but i strongly feel in favour of the alternative route that bayesian model comparison should abstain from automated and hard decision making . looking at the marginal likelihood of a model as evidence makes it harder to refrain from setting decision bounds when compared with returning a posterior distribution on @xmath39 or an associated predictive quantity , as further discussed in @xcite .",
    "different perspectives on this issue of constructing reference testing solutions are obviously welcome , from the incorporation of testing into the pc priors and baseline models of @xcite to the non - local tests of @xcite , and i would most gladly welcome exchanges on such perspectives .",
    "i am quite grateful to kerrie mengersen ( qut ) and joris mulder ( tilburg university ) for helpful comments and suggestions on this paper .",
    "discussions with the authors of the paper during a visit to amsterdam and their kind welcome are also warmly acknowledged ."
  ],
  "abstract_text": [
    "<S> this note is a discussion commenting on the paper by ly et al . on  </S>",
    "<S> harold jeffreys s default bayes factor hypothesis tests : explanation , extension , and application in psychology \" and on the perceived shortcomings of the classical bayesian approach to testing , while reporting on an alternative approach advanced by @xcite as a solution to this quintessential inference problem . </S>"
  ]
}