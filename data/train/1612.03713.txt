{
  "article_text": [
    "machine learning is an area of mathematical statistics that uses computer driven statistical learning techniques to find patterns in known empirical data with the intent of applying these _ learned _ patterns on new data .",
    "when one analyzes input data with the goal of predicting or estimating a specific output , this is called _ supervised learning_. an example would be using a financial firm s accounting reports to determine it s credit rating .",
    "a machine learning algorithm would be trained on the historical financial data of those firms with known credit ratings and then be used either to predict a new rating for a firm as new data arrives or to predict the rating of a firm with no past credit rating history .",
    "typically , one has @xmath0-features along with @xmath1-observation _ inputs _ of these @xmath0-dimensional feature vectors and a response called the _ supervising output _ which is measured on the same @xmath1-observation inputs . when a new set of observations is given , one would like to predict or estimate the specific output . a support vector machine ( svm ) @xcite is a supervised learning method that has been applied successfully to a very wide range of binary classification problems ( the supervised output is binary ) such as text classification ( noun or verb ) , medical risk ( heart disease , no heart disease ) , homeland security ( potential risk , not a risk ) , etc .",
    "all machine learning models must necessarily deal with big data issues such as the storage and fast retrieval of large amounts ( ~ petabytes ) of data which becomes increasingly difficult as the feature space and number of training observations of these features grow exponentially . _ _ furthermore , many popular machine learning techniques use a multi - layered or non - linear approach that leads to highly complex calculations resulting in excessive runtime speeds .",
    "it will be shown that quantum support vector machines based on coherent states may begin to address these issues .",
    "the tensor product of coherent states allows an efficient representation of high dimensional feature spaces .",
    "quantum state overlap measurements allow for the calculation of various non - linear svm kernel functions indicating a substantial runtime improvement over classical algorithms .",
    "a quantum version of a support vector classifier was given in @xcite where the authors provided a qubit representation of feature space that was adaptable for simple polynomial type kernels . yet",
    ", popular nonlinear kernels such as those of the exponential or hyperbolic tangent types are not easily amenable to their qubit representation . here",
    ", we propose using both canonical and generalized coherent states to rapidly calculate these nonlinear kernel functions on high dimensional feature spaces .",
    "a recent review of quantum machine learning techniques may be found in @xcite .",
    "a short review of svms and a recent quantum version of a linear svm are described below . for more details , see @xcite .",
    "consider @xmath1 observations of a @xmath0-dimensional feature vector @xmath2 .",
    "suppose that these observations live in a @xmath0-dimensional vector space and are separable into two classes by a @xmath3 dimensional hyperplane .",
    "for a 2-dimensional feature space , this hyperplane is simply a line as the solid black line in figure 1 whereas in 3 dimensions , this hyperplane will be a flat 2-dimensional plane .",
    "consider a @xmath0-dimensional weight vector @xmath4 and a bias parameter @xmath5 .",
    "a @xmath3 dimensional hyperplane is defined by the equation    @xmath6    where @xmath7 is the normal vector to the hyperplane . since",
    "our observations were assumed to be separable into two classes , each observation @xmath8 satisfies either    @xmath9    or @xmath10 defining the supervised output as    @xmath11,\\ ] ]    the two classes can be combined as    @xmath12\\geq0.\\ ] ]    the goal here is to find the optimal margin hyperplane ( the best choice of @xmath7 and @xmath5 ) based on the @xmath1 training observations @xmath13 such that when a new test data point @xmath14 needs to be analyzed , it will be correctly classified .",
    "this optimal margin hyperplane will then act as a decision boundary for all new data points .",
    "the perpendicular distance @xmath15 from the separating hyperplane to a particular training point @xmath13 is given by    @xmath12=m^{i}\\ ] ]    each training point will have a specific margin distance @xmath15 for a given set of hyperplane parameters @xmath7 and @xmath5 .",
    "the optimality problem is to find the optimal hyperplane parameters that gives the single biggest margin distance @xmath16 for all training points simultaneously . in figure 1 , the margin distance @xmath16 is the distance between the solid black line , the decision boundary @xmath17 , and the other two parallel lines that define the maximal margin region of width @xmath18 .",
    "it is often the case that this problem has no perfect solution .",
    "therefore , rather than searching for a perfect decision boundary , one can look for a hyperplane that separates _ most _ of the training data where a _ few feature vectors fall in the margin region or on the wrong side of the decision boundary_. these few observations are called _ support vectors _ and are assigned error terms ( or slack variables ) @xmath19 that are used to violate the margin width @xmath16 .",
    "the sum of these error terms are bounded , i.e. @xmath20 .    as @xmath7 is a normal vector to the hyperplane",
    ", one has @xmath21 .",
    "therefore , maximizing @xmath16 is the same as minimizing the norm of @xmath7 ,    @xmath22\\geq(1-\\epsilon_{i } ) & , \\forall i\\\\ \\\\ \\sum_{i=1}^{n}\\epsilon_{i}\\leq k & \\epsilon_{i}\\geq0 \\end{cases}\\ ] ]    which may be rewritten as @xmath23\\\\ \\textrm{subject to}\\\\ y^{i}\\left[\\mathbf{w\\cdot x^{\\mathrm{\\mathit{i}}}}+b\\right]\\geq(1-\\epsilon_{i}),\\epsilon_{i}\\geq0,\\forall i \\end{array}\\ ] ] where @xmath24 controls the effect of the error terms coming from the support vectors .",
    "if @xmath24 is very high , very few errors will be accepted by the optimizer .",
    "@xmath25 reduces to the completely separable case .",
    "one may use a lagrange multiplier method to solve this problem .",
    "the lagrangian is given by @xmath26-(1-\\epsilon_{i})\\right\\ } -\\sum_{i=1}^{n}\\mu^{i}\\epsilon_{i } \\end{array}\\ ] ] with the optimality conditions given by the following minimizations , @xmath27 the respective optimality conditions are @xmath28 note that the lagrange multipliers must be positive , i.e. @xmath29 .",
    "furthermore , only those lagrange multipliers that exactly satisfy @xmath30-(1-\\epsilon_{i})\\right\\ } = 0\\\\ \\\\",
    "\\mu^{i}\\epsilon_{i}=0 \\end{array}\\ ] ] can have strictly nonzero values ( the krush - kuhn - tucker conditions , see @xcite ) .    by substituting the solutions ( 11 ) into the lagrangian ( 9 ) ,",
    "one obtains the _ dual lagrangian _    @xmath31    maximizing the dual lagrangian with constraints @xmath32 and @xmath33 is often an easier problem than the minimization ( 8) given above .",
    "in the dual problem , the lagrange multipliers @xmath34 are solved for and provide the optimal solution via the relation @xmath35 .",
    "the solution for the optimal hyperplane that solves the binary classification problem is @xmath36 it is useful to introduce the concept of a _ kernel _ that can be used to link support vector classifiers to the svm technique below .",
    "a kernel @xmath37 is a type of similarity measure between two observations and in the simple linear case described here , it is given by @xmath38 this polynomial type kernel may be seen as a square symmetric matrix with components given by ( 15 ) .",
    "the dual lagrangian and the optimal hyperplane may be written as    @xmath39    and    @xmath40    there are @xmath41 dot products to calculate in ( 17 ) similar to a square symmetric matrix where each dot products takes @xmath42 time to calculate . finding the optimal @xmath34 takes @xmath43 time .",
    "the convergence to an optimality error of @xmath44 is through @xmath45 iterations as shown in @xcite .",
    "therefore classically , the dual problem takes a computational time of @xmath46",
    ". this can be improved by a quantum approach given in @xcite briefly described as follows .",
    "consider the following quantum representation of the observation feature vector @xmath8 using the base-2 _ bit string configuration _ introduced in @xcite where @xmath47    @xmath48    using quantum parallelism , consider the following superposition state of all the training data joined via a tensor product to an auxiliary hilbert space of computational basis states , @xmath49 , where    @xmath50    the density operator associated with this state is given by @xmath51 taking a partial trace over @xmath52 gives    @xmath53    from equation ( 18 ) , it is clear that    @xmath54    and therefore @xmath55 where @xmath56 according to @xcite , by using this method to calculate the kernel matrix and furthermore turning the optimization problem into a quantum matrix inversion problem , the runtime for their quantum support vector classifier method becomes ~@xmath57 .",
    "consider the classification problem illustrated in figure 2 .",
    "visually , one sees a distinct boundary on the left side of the figure yet it is clearly not linear . neither of the previous two methods will find a satisfactory decision hyperplane .",
    "however , a non - linear decision boundary appears to be possible .",
    "consider a set @xmath58 of square integrable functions that map the finite dimensional feature space into an infinite dimensional space .",
    "the equation in this new space , analogous to ( 1 ) , is given by @xmath59 with an optimal classification hyperplane of @xmath60 ( the right hand side of figure 2 ) .",
    "one would like to define an inner product kernel @xmath61 such that ( 26 ) once again reduces to the form given in ( 17 ) , @xmath62 clearly , specifying the kernel is sufficient to find the optimal classification boundary .",
    "these methods are often referred to as kernel methods @xcite as one does not need the explicit mapping @xmath63 itself as the kernel alone defines the solution in equation ( 28 ) ( `` the kernel trick '' or more formally the representer theorem @xcite ) .",
    "the form of ( 27 ) is possible using mercer s theorem from functional analysis @xcite which comes down to a spectral decomposition of a continuous symmetric kernel using a eigenvalue - eigenfunction expansion ( see section iv below ) , @xmath64 where the special case of @xmath65 has been used in ( 27 ) .",
    "some popular kernels in the svm literature are    _ polynomial of degree d : @xmath66 _    _ radial kernels ( gaussian ) : @xmath67 _    _ radial kernels ( ornstein uhlenbeck ) : _    _ @xmath68 _    _ sigmoidal ( two - layer perceptron ) : @xmath69\\ ] ] _        by using these types of kernels , one is looking for a hyperplane in a _ higher dimensional feature space .",
    "_ this _ decision boundary _ hyperplane in the higher dimensional space , as in the right side of figure 2 , results in a _ nonlinear decision boundary _ in the original feature space .",
    "analogous to the evaluation issues of ( 17 ) , one of the most significant limitations of classical algorithms using non - linear kernels is that the kernel function has to be evaluated for all pairs of input feature vectors @xmath8 and @xmath70 which themselves may be of substantially high dimension .",
    "this can lead to computationally excessive times during training and during the prediction process for new data points .",
    "in fact , classical methods such as sparse kernel methods @xcite have been developed to deal with this issue but they are mostly heuristic methods used to increase runtime speeds",
    ". rather , quantum methods analogous to the linear kernel methods described in ( 18)-(24 ) would be highly desirable and beneficial to using nonlinear svms in a fast and efficient manner .",
    "we now demonstrate how this is possible using coherent states .",
    "a feature space representation using canonical coherent states may be acheived as follows .",
    "consider the @xmath0-dimensional tensor product of canonical coherent states @xmath71 @xmath72 and their overlap @xmath73 in a feature space with @xmath74 and @xmath75 , this becomes @xmath76 which is the popular radial kernel form of svms .",
    "several simple measurements @xcite may produce this overlap function in computation times faster than the time needed on a classical computer . consider figure 3 which has been adapted from @xcite .",
    "the annihilation operator is related to one of two coherent states @xmath77 or @xmath78 .",
    "this unknown state enters the 50 - 50 beam splitter along with a vacuum field whereupon the output states undergo a coherent displacement given by @xmath79\\ ] ] this displacement operator acting on the vacuum state ( indicated by @xmath80 in figure 3 ) creates two coherent states @xmath81 and @xmath82 that have operators @xmath83 and @xmath84 related to the incident fields via a hadamard gate , @xmath85 for the joint measurement performed by the two detectors , one has the following four outcomes with projections given by @xmath86 @xmath87 @xmath88 and @xmath89 along with the corresponding povm , @xmath90    following @xcite , consider the following explicit representation , using normal ordering , of the coherent state projection operator , @xmath91:,\\,i=1,2.\\ ] ] oone can show that @xmath92 is given by , @xmath93:\\left.|0\\right\\rangle = \\\\",
    "\\left\\langle 0|\\right.:\\mathbf{exp}\\left[-\\left(\\frac{\\hat{a}^{\\dagger}+\\hat{v}}{\\sqrt{2}}-\\frac{\\alpha_{1}^{*}}{\\sqrt{2}}\\right)\\left(\\frac{\\hat{a}+\\hat{v}}{\\sqrt{2}}-\\frac{\\alpha_{1}}{\\sqrt{2}}\\right)\\right]:\\left.|0\\right\\rangle   \\end{array}\\ ] ] which simplifies to @xmath94:\\doteq:\\hat{r}_{1}:\\ ] ] where we have defined the operator @xmath95 $ ] . following in this manner",
    ", it can be verified that the povm ( 44 ) may now be written as in @xcite ,    @xmath96    @xmath97    @xmath98    and @xmath99 finally , the non zero probabilities of measuring one of the states @xmath77 or @xmath78 are given by @xmath100 @xmath101 and @xmath102 which provides the necessary evaluation of the radial kernel for our quantum svm .",
    "how has this method improved the classical runtime ?",
    "classically , there are @xmath41 exponential pairs to evaluate where each exponential of a dot product takes @xmath103 time to calculate .",
    "finding the optimal @xmath34 takes @xmath43 time while the convergence to an optimality error of @xmath44 is through @xmath45 iterations @xcite leading to a total classical computational time of @xmath104 .",
    "the povm probabilities ( 52)-(54 ) converge after @xmath16 repeated measurements . for the case of @xmath105 , which is common in the big data arena ,",
    "the calculation of the radial kernel via the povm set - up becomes largely independent of @xmath0 ( due to the tensor product hilbert space representation of the @xmath0-dimensional feature vector in the quantum domain ) .",
    "the classical time has been effectively reduced to @xmath106 . for data sets where @xmath107 , this is a substantial improvement .",
    "the @xmath108 time coming from the @xmath41 pairs of feature vectors may be further reduced by generalizing this methodology to multiple coherent states .",
    "the two state experiment above was generalized to four coherent states in @xcite where the povm to unambiguously identify one of the possible non - orthogonal states ( unambiguous state discrimination or usd ) are    @xmath109    note that this formula is only an extension of @xmath110 and @xmath111 above as neither @xmath112 or @xmath113 provides a usd result .",
    "therefore , the authors of @xcite add an inconclusive usd measurement @xmath114 to complete the povm set .",
    "( 55 ) has a natural extension for an arbitrary number of coherent states . from this generalization",
    ", we postulate that the @xmath108 pair calculation time may be reduced by the parallel measurement of several pairs of coherent states but a further study will be needed to get an estimate of the reduction in runtime .",
    "finally , the @xmath43 time coming from finding the optimal @xmath34 may also be reduced as follows . in @xcite ,",
    "a least squares method is introduced to solve the quadratic programming problem of svms .",
    "rebentrost et al .",
    "@xcite take this least squares method and propose an approximate quantum least squares method for their polynomial based kernel svm using a quantum matrix inversion algorithm .",
    "their method is largely independent of the type of kernel and therefore the quantum radial kernel calculation described here may be combined with their method to substantially reduce the @xmath43 runtime of the classical quadratic programming problem ( @xcite reduce their polynomial kernel problem to @xmath115 ) .",
    "as the structure of svms comes down to the kernel function , a more precise description of its structure will lead naturally to a relation with other types of coherent states rather than just the canonical ones used above . for more details , see @xcite .",
    "let @xmath116 be a hilbert space of real square integrable functions @xmath117 defined on a set @xmath118 .",
    "mercer kernel _ is symmetric kernel @xmath119 , @xmath120 with the following positive semi - definite property , @xmath121    _ mercer s theorem : _ for a mercer kernel @xmath122 , there exists a set of orthonormal functions @xmath123 @xmath124 such that @xmath125    _ reproducing kernel hilbert space _ ( rkhs ) : let @xmath116 be a hilbert space of real functions @xmath126 defined on a set @xmath118 .",
    "@xmath116 is called a reproducing kernel hilbert space with an inner product @xmath127 if there exists a kernel function @xmath119 with the following properties , @xmath128 ( if @xmath129 is an index then @xmath122 may be seen as a function of @xmath130 ) and the _ reproducing property @xmath131 _ property ( 59 ) can be used with mercer type kernels to induce a rkhs .",
    "the complex version of these kernels are often referred to as bergman kernels @xcite which produce rkhs of square integrable holomorphic functions .",
    "all the standard kernels used in svms satisfy mercer s theorem @xcite and may be used to create rkhss .",
    "it is also possible to create new kernels from existing ones .",
    "suppose one has two kernels @xmath132 and @xmath133 .",
    "one can add and multiply these kernels to produce a new kernel @xmath134 , i.e. @xmath135 or @xmath136",
    "in order for the quantum svm method to go beyond radial kernels , one needs to analyze generalized coherent states and their relation to rkhss .",
    "the generalization of canonical coherent states has historically proceeded along three ( not necessarily equivalent ) lines of thought which happen to result in equivalent definitions for canonical states .",
    "one generalization initiated by barut and girardello @xcite follows the path of creating generalized coherent states as eigenstates of a specific operator from a lie algebra .",
    "another group theoretical generalization begun independently by perelomov @xcite and gilmore @xcite considers a generalized displacement operator acting on a vacuum state . here",
    ", we wish to consider a third approach started by klauder and gazeau @xcite based on the following definition .    _",
    "definition : _ coherent states @xmath137 are _ wave - packets _ that are superpositions of eigenstates @xmath138 of a self - adjoint operator and square integrable functions @xmath139 , such that @xmath140 , where the states are normalized @xmath141 with a resolution of identity given by @xmath142 .    this generalization has a natural structure of an underlying rkhs .",
    "a resolution of identity requirement leads to the existence of a povm .",
    "let @xmath143 be a self - adjoint operator with a discrete spectrum @xmath144 and normalized eigenstates @xmath145 that form an orthonormal basis in a separable complex hilbert space @xmath116 , @xmath146 consider a measure space @xmath147 and the hilbert space @xmath148 of all square - integrable functions @xmath149 on the set @xmath118 ( @xmath150 ) , @xmath151 with orthonormal basis functions @xmath152 @xmath153 furthermore , assume that the eigenstates @xmath145 are in a one - to - one correspondence with these orthonormal basis functions .",
    "generalized coherent states may be defined by @xcite @xmath154 with a normalization restriction of @xmath155 the resolution of identity in @xmath116 is @xmath156    for any @xmath157 , one can associate an element @xmath158 as @xmath159 note that there is a natural isomorphism between the hilbert space @xmath116 and @xmath148 because of the one to one correspondence of basis functions explicitly given by the special case of ( 69 ) , @xmath160 using the resolution of identity ( 68 ) in ( 69 ) , one has @xmath161 which may be written as @xmath162 or @xmath163 one may create a rkhs @xmath164 composed of elements given by ( 69 ) spanned by basis functions ( 70 ) . by defining a kernel as @xmath165 equation ( 73 )",
    "becomes identical to the kernel reproducing property ( 60 ) ,    @xmath166    making @xmath164 a rkhs . for standard canonical coherent states",
    ", one has @xmath167 , @xmath168 , @xmath169 , @xmath170 , @xmath171 , @xmath172 , with the reproducing kernel given by @xmath173 . by using the kernel property ( 62 ) ,",
    "one has another kernel given by @xmath174 .    a few examples of well known generalized coherent states that can be derived using the above methodology are as follows . in @xcite ,",
    "the authors use a 1 + 1 anti de - sitter space constant negative curvature metric @xmath175 $ ] measure space and produce a reproducing kernel given by @xmath176 @xmath177 generalized coherent states using a basis of pschl - teller states @xcite may be shown to produce a modified bessel function ( @xmath178 ) kernel given by    @xmath179    several authors @xcite have investigated oscillating hermite polynomial gaussian type wave functions that lead to a reproducing kernel given by @xmath180l_{n}(\\left|\\alpha-\\beta\\right|^{2 } ) \\end{array}\\ ] ] where @xmath181 are laguerre polynomials .",
    "the nonlinear optical properties of confining pschl - teller potentials have been studied in @xcite .",
    "experimental realizations of laguerre type generalized coherent states using the behavior of a beamsplitter with a specific geometry are explored in @xcite .",
    "they also appear in the realization of a quantum oscillator consisting of a trapped ion in @xcite.the realization of generalized coherent states have also been investigated in photonic lattices @xcite .    for future work , each of these experiments needs to be analyzed in a similar manner to that of section iii in order to fully get an idea of the quantum runtime efficiency .",
    "these recent experimental realizations indicate that the quantum svm approach to calculating reproducing kernels from generalized coherent states may provide a fast and viable alternative to classical computations .",
    "in this paper , the potential of using generalized coherent states as a calculational tool for quantum svms was demonstrated .",
    "the key connecting thread was the rkhs concept used in svms .",
    "such reproducing kernels naturally arise in the quantum state overlap of canonical and generalized coherent states .",
    "it was shown that canonical coherent states reproduce the popular radial kernels of svms wherein povm measurements of overlap functions substantially reduce the computational times of such kernels , especially in high dimensional feature spaces found in big data sets . the use of reproducing kernels not usually used in classical algorithms due to their complexity , such as those from anti - de sitter space coherent states and bessel function kernels from pschl - teller coherent states are now conceivable using the coherent state driven quantum svm approach .",
    "the realization of generalized coherent states via experiments in quantum optics indicate the near term feasibility of this approach ."
  ],
  "abstract_text": [
    "<S> the support vector machine ( svm ) is a popular machine learning classification method which produces a nonlinear decision boundary in a feature space by constructing linear boundaries in a transformed hilbert space . </S>",
    "<S> it is well known that these algorithms when executed on a classical computer do not scale well with the size of the feature space both in terms of data points and dimensionality . </S>",
    "<S> one of the most significant limitations of classical algorithms using non - linear kernels is that the kernel function has to be evaluated for all pairs of input feature vectors which themselves may be of substantially high dimension . </S>",
    "<S> this can lead to computationally excessive times during training and during the prediction process for a new data point . here , we propose using both canonical and generalized coherent states to rapidly calculate specific nonlinear kernel functions </S>",
    "<S> . the key link will be the reproducing kernel hilbert space ( rkhs ) property for svms that naturally arise from canonical and generalized coherent states . </S>",
    "<S> specifically , we discuss the fast evaluation of radial kernels through a positive operator valued measure ( povm ) on a quantum optical system based on canonical coherent states . a similar procedure may also lead to fast calculations of kernels not usually used in classical algorithms such as those arising from generalized coherent states . </S>"
  ]
}