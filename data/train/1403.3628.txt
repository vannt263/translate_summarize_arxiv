{
  "article_text": [
    "brain computer interfaces ( bci ) are systems that help disabled people communicating with their environment through the use of brain signals @xcite . at the present time ,",
    "one of the most prominent bci is based on electroencephalography ( eeg ) because of its low - cost , portability and its non - invasiveness . among eeg",
    "based bci , a paradigm of interest is the one based on event - related potentials ( erp ) which are responses of the brain to some external stimuli . in this context ,",
    "the innermost part of a bci is the pattern recognition stage which has to correctly recognize presence of these erps .",
    "however , eeg signals are blurred due to the diffusion of the skull and the skin  @xcite . furthermore , eeg recordings are highly contaminated by noise of biological , instrumental and environmental origins . for addressing these issues ,",
    "advanced signal processing and machine learning techniques have been employed to learn erp patterns from training eeg signals leading to robust systems able to recognize the presence of these events  @xcite .",
    "note that while some erps are used for generating bci commands , some others can be used for improving bci efficiency .",
    "indeed , recent studies have also tried to develop algorithms for automated recognition of error - related potentials @xcite .",
    "these potentials are responses elicited when a subject commits an error in a bci task or observes an error @xcite and thus they can help in correcting errors or in providing feedbacks to bci user s .    in this context of automated recognition of event - related potentials for bci systems , reducing the number of eeg sensors is of primary importance since it reduces the implementation cost of the bci by minimizing the number of eeg sensor , and speeding up experimental setup and calibration time . for this purpose",
    ", some studies have proposed to choose relevant sensors according to prior knowledge of brain functions .",
    "for instance , sensors located above the motor cortex region are preferred for motor imagery tasks and while for visual event related potential ( erp ) , sensors located on the visual cortex are favored @xcite .",
    "recent works have focused on automatic sensor selection adapted to the specificity of a subject  @xcite .",
    "for instance , rakotomamonjy et al .",
    "@xcite performed a recursive backward sensor selection using cross - validation classification performances as an elimination criterion .",
    "another approach for exploring subset sensors have been proposed by @xcite , it consists in using a genetic algorithm for sensor selection coupled with an artificial neural networks for prediction . those methods has been proven efficient but computationally demanding . a quicker way is to estimate the relevance of the sensors in terms of signal to noise ratio ( snr )  @xcite and to keep the most relevant ones .",
    "note that this approach does not optimize a discrimination criterion , although the final aim is a classification task .",
    "recently , van gerven et al .",
    "@xcite proposed a graceful approach for embedding sensor selection into a discriminative framework .",
    "they performed sensor selection and learn a decision function by solving a unique optimization problem . in their framework , a logistic regression classifier is learned and the group - lasso regularization , also known as @xmath0 mixed - norm , is used to promote sensor selection .",
    "they have also investigated the use of this groupwise regularization for frequency band selection and their applications to transfer learning .",
    "the same idea has been explored by tomioka et al .",
    "@xcite which also considered groupwise regularization for classifying eeg signals . in this work , we go beyond these studies by providing an in - depth study of the use of mixed - norms for sensor selection in a single subject setting and by discussing the utility of mixed - norms when learning decision functions for multiple subjects simultaneously .",
    "our first contribution addresses the problem of robust sensor selection embedded into a discriminative framework .",
    "we broaden the analysis of van gerven et al .",
    "@xcite by considering regularizers which forms are @xmath1 mixed - norms , with @xmath2 , as well as adaptive mixed - norms , so as to promote sparsity among group of features or sensors .",
    "in addition to providing a sparse and accurate sensor selection , mixed - norm regularization has several advantages .",
    "first , sensor selection is cast into an elegant discriminative framework , using for instance a large margin paradigm , which does not require any additional hyper - parameter to be optimized .",
    "secondly , since sensor selection is jointly learned with the classifier by optimizing an `` all - in - one '' problem , selected sensors are directed to the goal of discriminating relevant eeg patterns .",
    "hence , mixed - norm regularization helps locating sensors which are relevant for an optimal classification performance .",
    "a common drawback of all the aforementioned sensor selection techniques is that selected set of sensors may vary , more or less substantially , from subject to subject .",
    "this variability , is due partly to subject specific differences and partly to acquisition noise and limited number of training examples .",
    "in such a case , selecting a robust subset of sensors may become a complex problem . addressing this issue",
    "is the point of our second contribution .",
    "we propose a multi - task learning ( mtl ) framework that helps in learning robust classifiers able to cope with the scarcity of learning examples .",
    "mtl is one way of achieving inductive transfer between tasks .",
    "the goal of inductive transfer is to leverage additional sources of information to improve the performance of learning on the current task .",
    "the main hypothesis underlying mtl is that tasks are related in some ways . in most cases ,",
    "this relatedness is translated into a prior knowledge , _",
    "e.g _ a regularization term , that a machine learning algorithm can take advantage of .",
    "for instance , regularization terms may promote similarity between all the tasks  @xcite , or enforce classifier parameters to lie in a low dimensional linear subspace  @xcite , or to jointly select the relevant features  @xcite .",
    "mtl has been proven efficient for motor imagery in  @xcite where several classifiers were learned simultaneously from several bci subject datasets .",
    "our second contribution is thus focused on the problem of performing sensor selection and learning robust classifiers through the use of an mtl mixed - norm regularization framework .",
    "we propose a novel regularizer promoting sensor selection and similarity between classifiers . by doing so ,",
    "our goal is then to yield sensor selection and robust classifiers that are able to overcome the data scarcity problem by sharing information between the different classifiers to be learned .",
    "the paper is organized as follows .",
    "the first part of the paper presents the discriminative framework and the different regularization terms we have considered for channel selection and multi - task learning .",
    "the second part is devoted to the description of the datasets , the preprocessing steps applied to each of them and the results achieved in terms of performances and sensor selection . in order to promote reproducible research , the code needed for generating the results in this paper is available of the author s website .",
    "in this section , we introduce our mixed - norm regularization framework that can be used to perform sensor selection in a single task or in a transfer learning setting .      typically in bci problems ,",
    "one wants to learn a classifier that is able to predict the class of some eeg trials , from a set of learning examples .",
    "we denoted as @xmath3 the learning set such that @xmath4 is a trial and @xmath5 is its corresponding class , usually related to the absence or presence of an event - related potential . in most cases ,",
    "a trial @xmath6 is extracted from a multidimensional signal and thus is characterized by @xmath7 features for each of the @xmath8 sensors , leading to a dimensionality @xmath9 .",
    "our aim is to learn , for a single subject , a linear classifier @xmath10 that will predict the class of a trial @xmath11 , by looking at the sign of the function @xmath12 defined as : @xmath13 with @xmath14 the normal vector to the separating hyperplane and @xmath15 a bias term .",
    "parameters of this function are learned by solving the optimization problem : @xmath16 where @xmath17 is a loss function that measures the discrepancy between actual and predicted labels , @xmath18 a regularization term that expresses some prior knowledge about the learning problem and @xmath19 a parameter that balances both terms . in this work ,",
    "we choose @xmath17 to be the squared hinge loss @xmath20 , thus promoting a large margin classifier .",
    "we now discuss different regularization terms that may be used for single task learning along with their significances in terms of channel selection .",
    "[ [ sec : ell_2 ] ] @xmath21 norm + + + + + + + + + + + + + + + + + + + + + + + + +    the first regularization term that comes to mind is the standard squared @xmath21 norm regularization : @xmath22 where @xmath23 is the euclidean norm .",
    "this is the common regularization term used for svms and it will be considered in our experiments as the baseline approach . intuitively , this regularizer tends to downweight the amplitude of each component of @xmath24 leading to a better control of the margin width of our large - margin classifier and thus it helps in reducing overfitting .",
    "[ [ sec : ell_1-norm ] ] @xmath25 norm + + + + + + + + + + + + + + + + + + + + + + + + +    when only few of the features are discriminative for a classification task , a common way to select the relevant ones is to use an @xmath25 norm of the form @xmath26 as a regularizer @xcite . owing to its mathematical properties ( non - differentiability at @xmath27 ) , unlike the @xmath21 norm , this regularization term promotes sparsity , which means that at optimality of problem ( [ eq : disc_framework ] ) , some components of @xmath24 are exactly @xmath27 . in a bayesian framework ,",
    "the @xmath25 norm is related to the use of prior on @xmath24 that forces its component to vanish @xcite .",
    "this is typically obtained by means of laplacian prior over the weight .",
    "however , @xmath25 norm ignores the structure of the features ( which may be grouped by sensors ) since each component of @xmath24 is treated independently to the others yielding thus to feature selection but not to sensor selection .    [",
    "[ sec : ell_1-ell_2-mixed ] ] @xmath1 mixed - norm + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a way to take into account the fact that features are structured , is to use a mixed - norm that will group them and regularize them together . here",
    ", we consider mixed - norm of the form @xmath28 with @xmath29 and @xmath30 being a partition of the set @xmath31 . intuitively , this @xmath1 mixed - norm can be interpreted as an @xmath25 norm applied to the vector containing the @xmath32 norm of each group of features .",
    "it promotes sparsity on each @xmath33 norm and consequently on the @xmath33 components as well . for our bci problem ,",
    "a natural choice for @xmath30 is to group the features by sensors yielding thus to @xmath8 groups ( one per sensor ) of @xmath7 features as reported in figure [ fig : groupfeat ] .",
    "note that unlike the @xmath0 norm as used by van gerven et al .",
    "@xcite and tomioka et al .",
    "@xcite , the use of an inner @xmath32 norm leads to more flexibility as it spans from the @xmath34 ( equivalent to the @xmath25-norm and leading thus to unstructured feature selection ) to the @xmath0 which strongly ties together the components of a group .",
    "examples of the use of @xmath32 norm and mixed - norm regularizations in other biomedical contexts can be found for instance in @xcite .",
    "[ [ sec : adaptive - ell_1-ell_2 ] ] adaptive @xmath1 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the @xmath25 and @xmath1 norms described above , are well - known to lead to grouped feature selection .",
    "however , they are also known , to lead to poor statistical properties ( at least when used with a square loss function )  @xcite .",
    "for instance , they are known to have consistency issue in the sense that , even with an arbitrarily large number of training examples , these norms may be unable to select the true subset of features . in practice",
    ", this means that when used in equation ( [ eq : disc_framework ] ) , the optimal weight vector @xmath24 will tend to over - estimate the number of relevant sensors .",
    "these issues can be addressed by considering an adaptive @xmath1 mixed - norm of the form @xcite : @xmath35 where the weights @xmath36 are selected so as to enhance the sparsity pattern of @xmath24 . in our experiments ,",
    "we obtain them by first solving the @xmath1 problem with @xmath37 , which outputs an optimal parameter @xmath38 , and by finally defining @xmath39 .",
    "then , solving the weighted @xmath1 problem yields an optimal solution with increased sparsity pattern compared to @xmath38 since the @xmath36 augments the penalization of groups with norm @xmath40 smaller than @xmath41 .",
    "+      let us now discuss how problem ( [ eq : disc_framework ] ) is solved when one of these regularizers is in play .",
    "using the @xmath21 norm regularization makes the problem differentiable .",
    "hence a first or second - order descent based algorithm can be considered  @xcite .",
    "because the other regularizers are not differentiable , we have deployed an algorithm @xcite tailored for minimizing objective function of the form @xmath42 with @xmath43 a smooth and differentiable convex function with lipschitz constant @xmath44 and @xmath45 a continuous and convex non - differentiable function having a simple proximal operator , _",
    "i.e. _ a closed - form or an easy - to - compute solution of the problem : @xmath46 such an algorithm , known as forward - backward splitting @xcite  is simply based on the following iterative approach , @xmath47 with @xmath48 being a stepsize in the gradient descent .",
    "this algorithm can be easily derived by considering , instead of directly minimizing @xmath49 , an iterative scheme which at each iteration replace @xmath43 with a quadratic approximation of @xmath50 in the neighborhood of @xmath51 .",
    "hence , @xmath52 is the minimizer of  : @xmath53 which closed - form is given in equation ( [ eq : proximal ] ) .",
    "this algorithm is known to converge towards a minimizer of @xmath49 under some weak conditions on the stepsize @xcite , which is satisfied by choosing for instance @xmath54 .",
    "we can note that the algorithm defined in equation ( [ eq : proximal ] ) has the same flavor as a projected gradient algorithm which first , takes a gradient step , and then `` projects '' back the solution owing to the proximal operator .",
    "more details can also be found in @xcite .     for our problem ( [ eq : disc_framework ] ) , we choose @xmath55 to be the squared hinge loss and @xmath56 the non - smooth regularizer .",
    "the square hinge loss is indeed gradient lipschitz with a constant @xmath44 being @xmath57 .",
    "proof of this statement is available in appendix [ sec : append_lip ] .",
    "proximal operators of the @xmath25 and the @xmath0 regularization term can be easily shown to be the soft - thresholding and the block - soft thresholding operator @xcite .",
    "the general @xmath1 norm does not admit a closed - form solution , but its proximal operator can be simply computed by means of an iterative algorithm  @xcite .",
    "more details on these proximal operators are also available in appendix [ sec : append_prox ] .",
    "we now address the problem of channel selection in cases where training examples for several subjects are at our disposal .",
    "we have claimed that in such a situation , it would be benefitial to learn the decision functions related to all subjects simultaneously , while inducing selected channels to be alike for all subjects , as well as inducing decision function parameters to be related in some sense .",
    "these two hypotheses make reasonable sense since brain regions related to the appearance of a given erp are expected to be somewhat location - invariant across subjects . for solving this problem",
    ", we apply a machine learning paradigm , known as multi - task learning , where in our case , each task is related to the decision function of a given subject and where the regularizer should reflect the above - described prior knowledge on the problem .",
    "given @xmath58 subjects , the resulting optimization problem boils down to be @xmath59 with @xmath60 being the training examples related to each task @xmath61 , @xmath62 being the classifier parameters for task @xmath63 and @xmath64\\in\\dbr^{d \\times m}$ ] being a matrix concatenating all vectors @xmath65 .",
    "note that the multi - task learning framework applied to single eeg trial classification have already been investigated by van gerven et al . @xcite .",
    "the main contribution we bring compared to their works is the use of regularizer that explicitly induces all subject classifiers to be similar to an average one , in addition to a regularizer that enforces selected channels to be the same for all subjects .",
    "the intuition behind this point is : we believe that since the classification tasks we are dealing with , are similar for all subjects and all related to the same bci paradigm , selected channels and classifier parameters should not differ that much from subject to subject .",
    "we also think that inducing task parameters to be similar may be more important than enforcing selected channels to be similar when the number of training examples is small since it helps in reducing overfitting . for this purpose ,",
    "we have proposed a novel regularization term of the form  : @xmath66 where @xmath67 is the average classifier across tasks and @xmath68 contains non - overlapping groups of components from matrix @xmath69 .",
    "the first term in equation ( [ eq : regterm_mtl ] ) is a mixed - norm term that promotes group regularization . in this work",
    ", we defined groups in @xmath68 based on the sensors , which means that all the features across subject related to a given sensor are in the same group @xmath70 , leading to @xmath8 groups of @xmath71 feature , as depicted in figure [ fig : groupfeat ] .",
    "the second term is a similarity promoting term as introduced in evgeniou et al .",
    "it can be interpreted as a term enforcing the minimization of the classifier s parameter variance . in other words",
    ", it promotes classifiers to be similar to the average one , and it helps improving performances when the number of learning examples for each task is limited , by reducing over - fitting . note that @xmath72 and @xmath73 respectively control the sparsity induced by the first term and the similarity induced by the second one .",
    "hence , when setting @xmath74 , the regularizer given in equation ( [ eq : regterm_mtl ] ) boils down to be similar to the one used by van gerven et al .",
    "note that in practice @xmath72 and @xmath73 are selected by means of a nested cross - validation which aims at classification accuracy .",
    "thus , it may occur that classifier similarity is preferred over sensor selection leading to robust classifiers which still use most of the sensors .",
    "similarly to the single task optimization framework given in equation ( [ eq : disc_framework ] ) , the objective function for problem can be expressed as a sum of gradient lipschitz continuous term @xmath75 and a non - differentiable term @xmath76 having a closed - form proximal operator ( see appendix [ sec : append_lip2 ] ) .",
    "hence , we have again considered a forward - backward splitting algorithm which iterates are given in equation ( [ eq : proximal ] ) .",
    "we now present how these novel approaches perform on different bci problems . before delving into the details of the results , we introduce the simulated and real datasets .",
    "we have first evaluated the proposed approaches on a simple simulated p300 dataset generated as follows .",
    "a p300 wave is extracted using the grand average of a single subject data from the epfl dataset described in the following .",
    "we generate @xmath77 simulated examples with 8 discriminative channels containing the p300 out of 16 channels for positive examples .",
    "a gaussian noise of standard deviation @xmath78 is added to all signals making the dataset more realistic .",
    "@xmath79 of these examples have been used for training .",
    "the first real p300 dataset we used is the epfl dataset , based on eight subjects performing p300 related tasks  @xcite .",
    "the subjects were asked to focus on one of the 3@xmath802=6 images on the screen while the one of the images is flashed at random .",
    "the eeg signals were acquired from 32 channels , sampled at @xmath81 hz and 4 recording sessions per subject have been realized .",
    "signals are pre - processed exactly according to the steps described in @xcite : a @xmath82 $ ]  hz bandpass butterworth filter of order 3 is applied to all signals followed by a downsampling .",
    "hence , for each trial ( training example ) , we have 8 time - sample features per channel corresponding to a 1000 ms time - window after stimulus , which leads to @xmath83 features for all channels ( 32@xmath808=256 features ) . on the overall , the training set of a given subject is composed of about 3000 trials .",
    "another p300 dataset , recorded by the neuroimaging laboratory of universidad autnoma metropolitana ( uam , mexico ) @xcite , has also been utilized .",
    "the data have been obtained from 30 subjects performing p300 spelling tasks on a 6@xmath806 virtual keyboard .",
    "signals are recorded over 10 channels leading thus to a very challenging dataset for sensor selection , as there are just few sensors left to select . for this dataset",
    ", we only use the first 3 sessions in order to have the same number of trials for all subjects ( @xmath844000 samples ) .",
    "the eeg signals have been pre - processed according to the following steps  : a @xmath85 $ ] hz chebychef bandpass filter of order 5 is first applied followed by a decimation , resulting in a post - stimulus time - window of 31 samples per channels .",
    "hence , each trial is composed of @xmath86 ( 10@xmath8031 ) features .",
    "we have also studied the effectiveness of our methods on an error related potential ( errp ) dataset that has been recorded in the gipsa lab .",
    "the subjects were asked to memorize the position of 2 to 9 digits and to remind the position of one of these digits , operation has been repeated @xmath87 times for each subject .",
    "the signal following the visualization of the result ( correct / error on the memorized position ) was recorded from 31 electrodes and sampled at @xmath88 hz . similarly to jrad et al .",
    "@xcite , a @xmath89 $ ]  hz butterworth filter of order 4 and a downsampling has been applied to all channel signals .",
    "finally , a time window of 1000ms is considered as a trial ( training example ) with a dimensionality of @xmath90 .",
    "we have compared several regularizers that induce feature / channel selection embedded in the learning algorithm , in a single subject learning setting as defined in equation ( [ eq : disc_framework ] ) .",
    "the performance measure commonly used in bci competitions @xcite is the area under the roc curve ( auc ) .",
    "this measure is an estimate of the probability for a positive class to have a higher score than a negative class .",
    "it makes particularly sense to use auc when evaluating a p300 speller as the letter in the keyboard is usually chosen by comparing score returned by the classifier for every column or line .",
    "in addition , auc does not depend on the proportion of positive / negative examples in the data which makes it more robust than classification error rate .",
    "our baseline algorithm is an svm , which uses an @xmath21 regularizer and thus does not perform any selection . using an @xmath25 regularizer yields a classifier which embeds feature selection , denoted as svm-1 in the sequel .",
    "three mixed - norm regularizers inducing sensor selection have also been considered : an @xmath0 denoted as gsvm-2 , and @xmath1 referred as gsvm - q , with @xmath91 being selected in the set @xmath92 ) by a nested cross - validation stage , and adaptive @xmath1 norm , with @xmath93 denoted as gsvm - a .",
    "for the multi - task learning setting , two mtl methods were compared to two baseline approaches which use all features , namely a method that treats each tasks separately by learning one svm per task ( svm ) , and a method denoted as svm - full , which on the contrary learns an unique svm from all subject datasets .",
    "the two mtl methods are respectively a mtl as described in equation ( [ eq : mtlframework ] ) , denoted as mgsvm-2s and the same mtl but without similarity - promoting regularization term , which actually means that we set @xmath74 , indicated as mgsvm-2 . for these approaches ,",
    "performances are evaluated as the average auc of the decision functions over all the subjects .",
    "the experimental setup is described in the following . for each subject , the dataset is randomly split into a training set of @xmath94 trials and a test set containing the rest of the trials .",
    "the regularization parameter @xmath19 has been selected from a log - spaced grid ( @xmath95 $ ] ) according to a nested @xmath96-fold cross - validation step on the training set .",
    "when necessary , the selection of @xmath91 is also included in this cv procedure .",
    "finally , the selected value of @xmath19 is used to learn a classifier on the training examples and performances are evaluated on the independent test set .",
    "we run this procedure 10 times for every subject and report average performances .",
    "a wilcoxon signed - rank test , which takes ties into account is used to evaluate the statistical difference of the mean performances of all methods compared to the baseline svm .",
    "we believe that such a test is more appropriate for comparing methods than merely looking at the standard deviation due to the high inter - subject variability in bci problems .",
    "we now present the results we achieved on the above - described datasets .",
    ".performance results on the simulated dataset : the average performance in auc ( in @xmath97 ) , the average percent of selected sensors ( sel ) and the f - measure of the selected channels ( in @xmath98 ) . best results for each performance measure are in bold .",
    "the p - value refers to the one of a wilcoxon signrank test with svm as a baseline . [ cols=\"<,^,^,^,^\",options=\"header \" , ]     results for these datasets are reported in table [ tab : perfresults ] . for the epfl dataset , all methods achieve performances that are not statistically different . however , we note that gsvm-2 leads to sensor selection ( 80% of sensor selected ) while gsvm - a yields to classifiers that , on average , use @xmath99 of the sensors at the cost of a slight loss in performances ( 1.5% auc ) .    results for the uam dataset follow the same trend in term of sensor selection but we also observe that the mixed - norm regularizers yield to increased performances .",
    "gsvm-2 performs statistically better than svm although most of the sensors ( 9 out of 10 ) have been kept in the model .",
    "this shows that even if few channels have been removed , the group - regularization improves performances by bringing sensor prior knowledge to the problem .",
    "we also notice that gsvm - a performance is statistically equivalent to the baseline svm one while using only half of the sensors and gsvm - p consistently gives similar results to gsvm-2 .    to summarize , concerning the performances of the different mixed - norm regularization , we outline that on one hand , gsvm-2 is at worst , equivalent to the baseline svm while achieving sensor selection and on the other hand gsvm - a yields to a substantial channel selection at the expense of a slight loss of performances .",
    "a visualization of the electrodes selected by can be seen in figure [ fig : sel_sensor_epfl ] for the epfl dataset and in figure [ fig : sel_sensor_uam ] for the uam dataset .",
    "interestingly , we observe that for the epfl dataset , the selected channels are highly dependent on the subject .",
    "the most recurring ones are : fc1 c3 t7 cp5 p3 po3 po4 pz and the electrodes located above visual cortex o1,oz and o2 .",
    "we see sensors from the occipital area that are known to be relevant  @xcite for p300 recognition , but sensors such as t7 and c3 , from other brain regions are also frequently selected .",
    "these results are however consistent with those presented in the recent literature  @xcite .",
    "the uam dataset uses only 10 electrodes that are already known to perform well in p300 recognition problem , but we can see from figure [ fig : sel_sensor_uam ] that the adaptive mixed - norm regularizer further selects some sensors that are essentially located in the occipital region .",
    "note that despite the good average performances reported in table [ tab : perfresults ] , some subjects in this dataset achieve very poor performances , of about 50 % of auc , regardless of the considered method .",
    "selected channels for one of these subjects ( subject 25 ) are depicted in figure [ fig : sel_sensor_uam ] and interestingly , they strongly differ from those of other subjects providing rationales for the poor auc .",
    "we have also investigated the impact of sparsity on the overall performance of the classifiers .",
    "to this aim , we have plotted the average performance of the different classifiers as a function of the number of selected sensors .",
    "these plots are depicted in figure  [ fig : perf_svs_sparsity ] for the epfl dataset and on figure  [ fig : perf_svs_sparsity_uam ] for the uam dataset .",
    "for both datasets , gsvm - a frequently achieves a better auc for a given level of sparsity . for most of the subjects , gsvm - a performs as well as svm but using far less sensors .",
    "a rationale may be that , in addition to selecting the relevant sensors , gsvm - a may provide a better estimation of the classifier parameters leading to better performances for a fixed number of sensors . as a summary , we suggest thus the use of an adaptive mixed - norm regularizer instead of an @xmath0 mixed - norm as in van gerven et al .",
    "@xcite when sparsity and channel selection is of primary importance .",
    "the errp dataset differs from the others as its number of examples is small ( 72 examples per subject ) .",
    "the same experimental protocol as above has been used for evaluating the methods but only 57 examples out of 72 have been retained for validation / training .",
    "classification performances are reported on table [ tab : perfresults ] . for this dataset",
    ", the best performance is achieved by gsvm-2 but the wilcoxon test shows that all methods are actually statistically equivalent .",
    "interestingly , many channels of this dataset seem to be irrelevant for the classification task .",
    "indeed , gsvm-2 selects only 30% of them while gsvm - a uses only 7% of the channels at the cost of 10% auc loss .",
    "we believe that this loss is essentially caused by the aggressive regularization of gsvm - a and the difficulty to select the regularization parameter @xmath19 using only a subset of the 57 training examples .",
    "channels selected by gsvm-2 can be visualized on figure [ fig : sel_sensor_erp ] . despite the high variance in terms of selected sensors , probably due to the small number of examples , sensors in the central area",
    "seem to be the most selected one , which is consistent with previous results in errp  @xcite .",
    "we now evaluate the impact of the approach we proposed in equation ( [ eq : mtlframework ] ) and ( [ eq : regterm_mtl ] ) on the p300 datasets .",
    "we expect that since multi - task learning allows to transfer some information between the different classification tasks , it will help in leveraging classification performances especially when the number of available training examples is small .",
    "note that the errp dataset has not been tested in this mtl framework , because the above - described results suggest an important variance in the selected channels for all subjects .",
    "hence , we believe that this learning problem does not fit into the prior knowledge considered through equation ( [ eq : regterm_mtl ] ) .    we have followed the same experimental protocol as for the single task learning except that training and test sets have been formed as follows .",
    "we first create training and test examples for a given subject by randomly splitting all examples of that subject , and then gather all subject s training / test sets to form the multi - task learning training / test sets .",
    "hence , all the subjects are equally represented in these sets .",
    "a @xmath96-fold nested cross - validation method is performed in order to automatically select the regularization terms ( @xmath72 and @xmath73 ) .",
    "[ fig : perf_mtl_epfl ]         performances of the different methods have been evaluated for increasing number of training examples per subject and are reported in figure [ fig : mtl_epfl ] .",
    "we can first see that for the epfl dataset , mgsvm-2 and yield a slight but consistent improvement over the single - task classifiers ( svm - full being a single classifier trained on all subject s examples and svm being the average performances of subject - specific classifiers ) .",
    "the poor performances of the svm - full approach is probably due to the high inter - subject variability in this dataset , which includes impaired patients .    for the uam dataset ,",
    "results are quite different since the svm - full and shows a significant improvement over the single - task learning .",
    "we also note that , when only the joint channel selection regularizer is in play ( ) , multi - task learning leads to poorer performance than the svm - full for a number of trials lower than @xmath100 .",
    "we justify this by the difficulty of achieving appropriate channel selection based only on few training examples , as confirmed by the performance of gsvm-2 . from figure",
    "[ fig : aucperfmtl ] , we can see that the good performance of is the outcome of performance improvement of about 10  achieved on some subjects that perform poorly .",
    "more importantly , while performances of these subjects are significantly increased , those that performs well still achieve good auc scores .",
    "in addition , we emphasize that these improvements are essentially due to the similarity - inducing regularizer .    for both datasets ,",
    "the mtl approach is consistently better than those of other single - task approaches thanks to the regularization parameters @xmath72 and @xmath73 that can adapt to the inter - subject similarity ( weak similarity for epfl and strong similarity for uam ) .",
    "these are interesting results showing that multi - task learning can be a way to handle the problem related to some subjects that achieve poor performances .",
    "moreover , results also indicate that multi - task learning is useful for drastically shortening the calibration time . for instance , for the uam dataset",
    ", 80% auc was achieved using only 100 training examples ( less than 1 minute of training example recordings ) .",
    "note that the validation procedure tends to maximize performances , and does not lead to sparse classifiers for mtl approaches . as shown in figures [ fig : sel_sensor_epfl ] and [ fig : sel_sensor_uam ] , the relevant sensors are quite different between subjects thus a joint sensor selection can lead to a slight loss of performances , hence the tendency of the cross - validation procedure to select non - sparse classifiers .",
    "in this work , we have investigated the use of mixed - norm regularizers for discriminating event - related potentials in bci .",
    "we have extended the discriminative framework of van gerven et al .",
    "@xcite by studying general mixed - norms and proposed the use of the adaptive mixed - norms as sparsity - inducing regularizers .",
    "this discriminative framework has been broadened to the multi - task learning framework where classifiers related to different subjects are jointly trained . for this framework ,",
    "we have introduced a novel regularizer that induces channel selection and classifier similarities .",
    "the different proposed approaches were tested on three different datasets involving a substantial number of subjects .",
    "results from these experiments have highlighted that the @xmath0 regularizer has been proven interesting for improving classification performance whereas adaptive mixed - norm is the regularizer to be considered when sensor selection is the primary objective .",
    "regarding the multi - task learning framework , our most interesting finding is that this learning framework allows , by learning more robust classifiers , significant performance improvement on some subjects that perform poorly in a single - task learning context .    in future work",
    ", we plan to investigate a different grouping of the features , such as temporal groups .",
    "this kind of group regularization could be for instance used in conjunction with the sensors group in order to promote both feature selection and temporal selection in the classifier .",
    "while the resulting problem is still convex , its resolution poses some issues so that a dedicated solver would be necessary .",
    "another research direction would be to investigate the use of asymmetrical mtl .",
    "this could prove handy when a poorly - performing subject will negatively influence the other subject performances in mtl while improving his own performances . in this case",
    "one would like that subject classifier to be similar to the other s classifier without impacting their classifiers .",
    "36 natexlab#1#1url # 1`#1`urlprefix    alamgir , m. , grosse - wentrup , m. , altun , y. , 2010 .",
    "multi - task learning for brain - computer interfaces . in : ai & statistics .",
    "argyriou , a. , evgeniou , t. , pontil , m. , 2008 .",
    "convex multi - task feature learning .",
    "machine learning 73  ( 3 ) , 243272 .",
    "bach , f. , 2008 .",
    "consistency of the group lasso and multiple kernel learning .",
    "journal of machine learning research 9 , 11791225 .",
    "bach , f. , jenatton , r. , mairal , j. , obozinski , g. , 2011 .",
    "convex optimization with sparsity - inducing norms . in : sra , s. , nowozin , s. , wright .",
    ", s.  j. ( eds . ) , optimization for machine learning . mit press , pp . 1953 .",
    "beck , a. , teboulle , m. , 2009 . a fast iterative shrinkage - thresholding algorithm for linear inverse problems .",
    "siam journal on imaging sciences 2 , 183202 .",
    "bertsekas , d. , 1999 .",
    "nonlinear programming .",
    "athena scientific .",
    "blankertz , b. , lemm , s. , treder , m. , haufe , s. , mller , k. , 2010 .",
    "single - trial analysis and classification of erp components  a tutorial .",
    "neuroimage 56  ( 2 ) , 814825 .",
    "http://dx.doi.org/10.1016/j.neuroimage.2010.06.048    blankertz , b. , mueller , k .-",
    ", krusienski , d. , schalk , g. , wolpaw , j. , schloegl , a. , pfurtscheller , g. , del r.  millan , j. , schroeder , m. , birbaumer , n. , 2006 .",
    "the bci competition iii : validating alternative approaches to actual bci problems .",
    "ieee transactions on neural systems and rehabilitation engineering 14  ( 2 ) , 153159 .",
    "buttfield , a. , ferrez , p. , millan , j. , 2006 . towards a robust bci : error potentials and online learning .",
    "neural systems and rehabilitation engineering , ieee transactions on 14  ( 2 ) , 164168 .",
    "cecotti , h. , rivet , b. , congedo , m. , jutten , c. , bertrand , o. , maby , e. , mattout , j. , 2011 . a robust sensor - selection method for p300 brain  computer interfaces .",
    "journal of neural engineering .",
    "chapelle , o. , 2007 .",
    "training a support vector machine in the primal .",
    "neural comput .",
    "19  ( 5 ) , 11551178 .    combettes , p. , pesquet , j. , 2011 .",
    "proximal splitting methods in signal processing .",
    "fixed - point algorithms for inverse problems in science and engineering , 185212 .",
    "dehaene , s. , posner , m. , tucker , d. , 1994 .",
    "localization of a neural system for error detection and compensation .",
    "psychological science 5  ( 5 ) , 303305 .",
    "dornhege , g. , milln , j. , hinterberger , t. , mcfarland , d. , mller , k. , 2007 . toward brain - computer interfacing .",
    "vol .  74 .",
    "mit press cambridge , ma .",
    "evgeniou , t. , pontil , m. , 2004 .",
    "regularized multi - task learning . in : proceedings of the tenth conference on knowledge discovery and data mining .",
    "falkenstein , m. , hohnsbein , j. , hoormann , j. , blanke , l. , 1991 .",
    "effects of crossmodal divided attention on late erp components .",
    "ii . error processing in choice reaction tasks .",
    "electroencephalography and clinical neurophysiology 78  ( 6 ) , 447455 .",
    "ferrez , p. , milln , j. , 2007 .",
    "error - related eeg potentials in brain - computer interfaces . towards brain - computer interfacing .",
    "mit press , cambridge , massachusetts .",
    "gouy - pailler , c. , congedo , m. , brunner , c. , jutten , c. , pfurtscheller , g. , 2010 .",
    "nonstationary brain source separation for multiclass motor imagery .",
    "ieee trans . on biomedical engineering 57  ( 2 ) , 469478 .",
    "hoffman , u. , yazdani , a. , vesin , j. , ebrahimi , t. , 2008 .",
    "bayesian feature selection applied in a p300 brain  computer interface . in",
    ": 16th european signal processing conference .",
    "hoffmann , u. , vesin , j. , ebrahimi , t. , diserens , k. , 2008 .",
    "an efficient p300-based brain - computer interface for disabled subjects .",
    "journal of neuroscience methods 167  ( 1 ) , 115125 .",
    "jrad , n. , congedo , m. , phlypo , r. , rousseau , s. , flamary , r. , yger , f. , rakotomamonjy , a. , 2011 .",
    "sw - svm : sensor weighting support vector machines for eeg - based brain  computer interfaces .",
    "journal of neural engineering 8 , 056004 .",
    "krusienski , d. , sellers , e. , mcfarland , d. , vaughan , t. , wolpaw , j. , 2008 . towards enhanced p300 speller performances .",
    "journal of neuroscience methods 167  ( 1 ) , 1521 .",
    "lal , t. , schroder , m. , hinterberger , t. , weston , j. , bogdan , m. , birbaumer , n. , scholkopf , b. , 2004 .",
    "support vector channel selection in bci .",
    "biomedical engineering , ieee transactions on 51  ( 6 ) , 10031010 .",
    "ledesma - ramirez , c. , bojorges valdez , e. , yez suarez , o. , saavedra , c. , bougrain , l. , gentiletti , g.  g. , 2010 . an open - access p300 speller database . in : fourth international brain - computer interface meeting",
    "liu , a. , hao , t. , gao , z. , su , y. , yang , z. , 2013 .",
    "non - negative mixed - norm convex optimization for mitotic cell detection in phase contrast microscopy .",
    "computational and mathematical methods in medicine 2013 , 110 .",
    "mller - gerking , j. , pfurtscheller , g. , flyvbjerg , h. , 1999 .",
    "designing optimal spatial filters for single - trial eeg classification in a movement task .",
    "110 , 787798 .",
    "nunez , p.  l. , srinivasan , r. , 2006 .",
    "electric fields of the brain , 2nd edition .",
    "new york : oxford univ press .",
    "rahimi , a. , xu , j. , wang , l. , 2013 .",
    "@xmath101 norm regularization in volumetric imaging of cardiac current sources .",
    "computational and mathematical methods in medicine 2013 , 110 .",
    "akotomamonjy , a. , flamary , r. , gasso , g. , canu , s. , 2011 .",
    "lp - lq penalty for sparse linear and sparse multiple kernel multi - task learning , .",
    "ieee transactions on neural networks 22  ( 8) , 13071320 .",
    "rakotomamonjy , a. , guigue , v. , 2008 .",
    "bci competition iii : dataset ii - ensemble of svms for bci p300 speller .",
    "ieee trans .",
    "biomedical engineering 55  ( 3 ) , 11471154 .",
    "rivet , b. , cecotti , h. , phlypo , r. , bertrand , o. , maby , e. , mattout , j. , 2010 .",
    "eeg sensor selection by sparse spatial filtering in p300 speller brain - computer interface .",
    "in : engineering in medicine and biology society ( embc ) , 2010 annual international conference of the ieee .",
    "ieee , pp .",
    "53795382 .",
    "salimi - khorshidi , g. , nasrabadi , a. , golpayegani , m. , 2008 .",
    "fusion of classic p300 detection methods inferences in a framework of fuzzy labels .",
    "artificial intelligence in medicine 44  ( 3 ) , 247259 .",
    "tomioka , r. , mller , k. , 2010 . a regularized discriminative framework for eeg analysis with application to brain - computer interface .",
    "neuroimage 49  ( 1 ) , 415432 .",
    "van gerven , m. , hesse , c. , jensen , o. , heskes , t. , 2009 . interpreting single trial data using groupwise regularisation .",
    "neuroimage 46  ( 3 ) , 665676 .    yang , j. , singh , h. , hines , e.  l. , schlaghecken , f. , iliescu , d.  d. , leeson , m.  s. , stocks , n.  g. , 2012 .",
    "channel selection and classification of electroencephalogram signals : an artificial neural network and genetic algorithm - based approach .",
    "artificial intelligence in medicine 55",
    "( 2 ) , 117  126 .",
    "zou , h. , 2006 .",
    "the adaptive lasso and its oracle properties .",
    "journal of the american statistical association 101  ( 476 ) , 14181429 .",
    "given the training examples @xmath102 , the squared hinge loss is written as : @xmath103 and its gradient is : @xmath104 the squared hinge loss is gradient lipschitz if there exists a constant @xmath44 such that : @xmath105 the proof essentially relies on showing that @xmath106 is lipschitz itself _",
    "i.e _ there exists @xmath107 such that @xmath108 now let us consider different situations . for a given @xmath109 and @xmath110 , if @xmath111 and @xmath112 , then the left hand side is equal to @xmath27 and any @xmath113 would satisfy the inequality . if @xmath111 and @xmath114 , then the left hand side ( lhs ) is @xmath115 a similar reasoning yields to the same bound when @xmath116 @xmath111 and @xmath114 and @xmath114 .",
    "thus , @xmath106 is lipschitz with a constant @xmath117 .",
    "now , we can conclude the proof by stating that @xmath118 is lipschitz as it is a sum of lipschitz function and the related constant is @xmath119 .      for the multi - task learning problem ,",
    "we want to prove that the function @xmath120 is gradient lipschitz , @xmath121 being the square hinge loss . from the above results , it is easy to show that the first term is gradient lipschitz as the sum of gradient lipschitz functions .      this term can be expressed as @xmath123 where @xmath124 $ ] is the vector of all classifier parameters and",
    "@xmath125 is the hessian matrix of the similarity regularizer of the form @xmath126 with @xmath127 the identity matrix and @xmath128 a block matrix with @xmath128 a @xmath129-diagonal matrix where each block is an identity matrix @xmath127 with appropriate circular shift .",
    "@xmath128 is thus a @xmath129 row - shifted version of @xmath127 .",
    "once we have this formulation , we can use the fact that a function @xmath10 is gradient lipschitz of constant @xmath44 if the largest eigenvalue of its hessian is bounded by @xmath44 on its domain @xcite .",
    "hence , since we have @xmath130 the hessian matrix of the similarity term @xmath131 has consequently bounded eigenvalues .",
    "this concludes the proof that the function @xmath132 is gradient lipschitz continuous .",
    "the proximal operator of the @xmath0 norm is defined as : @xmath135 the minimization problem can be decomposed into several ones since the indices @xmath70 are separable .",
    "hence , we can just focus on the problem @xmath136 which minimizer is @xmath137"
  ],
  "abstract_text": [
    "<S> this work investigates the use of mixed - norm regularization for sensor selection in event - related potential ( erp ) based brain - computer interfaces ( bci ) . </S>",
    "<S> the classification problem is cast as a discriminative optimization framework where sensor selection is induced through the use of mixed - norms . </S>",
    "<S> this framework is extended to the multi - task learning situation where several similar classification tasks related to different subjects are learned simultaneously . in this case </S>",
    "<S> , multi - task learning helps in leveraging data scarcity issue yielding to more robust classifiers . </S>",
    "<S> for this purpose , we have introduced a regularizer that induces both sensor selection and classifier similarities . </S>",
    "<S> the different regularization approaches are compared on three erp datasets showing the interest of mixed - norm regularization in terms of sensor selection . </S>",
    "<S> the multi - task approaches are evaluated when a small number of learning examples are available yielding to significant performance improvements especially for subjects performing poorly .    </S>",
    "<S> brain computer interface , support vector machines , sensor selection , eeg , sparse methods , event related potential , mixed norm . </S>"
  ]
}