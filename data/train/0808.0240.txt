{
  "article_text": [
    "the main goals of reactor neutrino experiments are to detect @xmath3 oscillation and precisely measure the mixing angle of neutrino oscillation @xmath4 .",
    "the experiment is designed to detect reactor @xmath5 s via the inverse @xmath6-decay reaction    @xmath7 .",
    "the signature is a delayed coincidence between @xmath8 and the neutron captured signals . in the paper , only three important sources of backgrounds are taken into account and they are the uncorrelated background from natural radioactivity and the correlated backgrounds from fast neutrons and @xmath0he/@xmath1li .",
    "the backgrounds like the neutrino events consist of two signals , a fast signal and a delay signal .",
    "it is vital to separate neutrino events from backgrounds accurately in the reactor neutrino experiments .",
    "the selection of the neutrino events based on the cuts is a methods that the event space is divided into two regions by a hyper - cuboid based on the cuts , and the events inside the hyper - cuboid , called the signal region , are regarded as neutrino events and the events outside the hyper - cuboid are regarded as backgrounds . in fact , the backgrounds in the signal region could nt be rejected by the method .",
    "the bayesian neural networks ( bnn)@xcite is an algorithm of the neural networks trained by bayesian statistics . it is not only a non - linear function as neural networks , but also controls model complexity . so its flexibility makes it possible to discover more general relationships in data than the traditional statistical methods and its preferring simple models make it possible to solve the over - fitting problem better than the general neural networks@xcite .",
    "bnn has been used to particle identification and event reconstruction in the experiments of the high energy physics , such as ref.@xcite . in this paper",
    ", bnn will be applied to discriminate the neutrino events from the background events in the signal region in the reactor neutrino experiments .",
    "the idea of bayesian neural networks is to regard the process of training a neural network as a bayesian inference .",
    "bayes theorem is used to assign a posterior density to each point , @xmath9 , in the parameter space of the neural networks .",
    "each point @xmath9 denotes a neural network . in the method of the bayesian neural network ,",
    "one performs a weighted average over all points in the parameter space of the neural network , that is , all neural networks .",
    "the methods make use of training data @xmath10 , where t@xmath11 is the known label associated with data @xmath12 .",
    "@xmath13 , if there are @xmath14 classes in the problems of classification ; @xmath12 has @xmath15 components if there are @xmath15 factors on which the classification is influenced .",
    "that is the set of data @xmath16which corresponds to the set of target @xmath17.the posterior density assigned to the point @xmath9 , that is , to a neural network , is given by bayes theorem    @xmath18    where data @xmath19 do not depend on @xmath9 , so @xmath20 .",
    "we need the likelihood @xmath21 and the prior density @xmath22 , in order to assign the posterior density @xmath23to a neural network defined by the point @xmath9 .",
    "@xmath24 is called evidence and plays the role of a normalizing constant , so we ignore the evidence . that is ,    @xmath25    we consider a class of neural networks defined by the function    @xmath26}\\ ] ]    where    @xmath27    .",
    "the neural networks have @xmath15 inputs , a single hidden layer of @xmath28 hidden nodes and a single output . in the particular bayesian neural networks described here , each neural network has the same structure .",
    "the parameter @xmath29 and @xmath30 are called the weights and @xmath31 and @xmath32 are called the biases . both sets of parameters",
    "are generally referred to collectively as the weights of the bayesian neural networks , @xmath9 .",
    "@xmath33 is the probability that an event , @xmath34 , belongs to the signal .",
    "so the likelihood of @xmath35 training events is    @xmath36    where it has been assumed that the events are independent with each other .",
    "we get the likelihood , meanwhile we need the prior to compute the posterior density . but the choice of prior is not obvious",
    ". however , experience suggests a reasonable class is the priors of gaussian class centered at zero , which prefers smaller rather than larger weights , because smaller weights yield smoother fits to data . in the paper , a gaussian prior",
    "is specified for each weight using the bayesian neural networks package of radford neal .",
    "however , the variance for weights belonging to a given group(either input - to - hidden weights(@xmath29 ) , hidden -biases(@xmath31 ) , hidden - to - output weights(@xmath30 ) or output - biases(@xmath32 ) ) is chosen to be the same : @xmath37 , @xmath38 , @xmath39 , @xmath40 , respectively .",
    "however , since we do nt know , a priori , what these variances should be , their values are allowed to vary over a large range , while favoring small variances .",
    "this is done by assigning each variance a gamma prior    @xmath41    where @xmath42 , and with the mean @xmath43 and shape parameter @xmath44 set to some fixed plausible values .",
    "the gamma prior is referred to as a hyperprior and the parameter of the hyperprior is called a hyperparameter .",
    "then , the posterior density , @xmath23 , is gotten according to eqs .",
    "( 2),(5 ) and the prior of gaussian distribution . given an event with data @xmath45 , an estimate of the probability that it belongs to the signal",
    "is given by the weighted average    @xmath46    currently , the only way to perform the high dimensional integral in eq .",
    "( 7 ) is to sample the density @xmath23 with the markov chain marlo carlo ( mcmc ) method@xcite . in the mcmc method ,",
    "one steps through the @xmath9 parameter space in such a way that points are visited with a probability proportional to the posterior density , @xmath23 .",
    "points where @xmath23 is large will be visited more often than points where @xmath23 is small .",
    "( 7 ) approximates the integral using the average    @xmath47    where @xmath48 is the number of points @xmath9 sampled from @xmath23 .",
    "each point @xmath9 corresponds to a different neural network with the same structure .",
    "so the average is an average over neural networks , and the probability of the data @xmath45 belongs to the signal .",
    "the average is closer to the real value of @xmath49 , when @xmath48 is sufficiently large .",
    "in the paper , a toy detector is designed to simulate central detectors in the reactor neutrino experiments , such as daya bay experiment@xcite and double chooz experiment@xcite , with cern geant4 package@xcite .",
    "the toy detector consists of three regions , and they are the gd - doped liquid scintillator(gd - ls from now on ) , the normal liquid scintillator(ls from now on ) and the oil buffer , respectively . the toy detector of cylindrical shape like the detector modules of daya bay experiment and double chooz experiment is designed in the paper . the diameter of the gd - ls region is 2.4 meter , and its height is 2.6 meter . the thickness of the ls region is 0.35 meter , and the thickness of the oil part is 0.40 meter . in the paper , the gd - ls and ls are the same as the scintillator adopted by the proposal of the chooz experiment@xcite .",
    "the 8-inch photomultiplier tubes ( pmt from now on ) are mounted on the inside the oil region of the detector .",
    "a total of 366 pmts are arranged in 8 rings of 30 pmts on the lateral surface of the oil region , and in 5 rings of 24 , 18 , 12 , 6 , 3 pmts on the top and bottom caps .",
    "the response of the neutrino and background events deposited in the toy detector is simulated with geant4 .",
    "although the physical properties of the scintillator and the oil ( their optical attenuation length , refractive index and so on ) are wave - length dependent , only averages@xcite ( such as the optical attenuation length of gd - ls with a uniform value is 8 meter and the one of ls is 20 meter ) are used in the detector simulation .",
    "the program could nt simulate the real detector response , but this wo nt affect the result of the comparison between bnn and the method based on the cuts .    according to the anti - neutrino interaction in the detector of the reactor neutrino experiments@xcite , the neutrino events are uniformly generated throughout gd - ls region ( see fig .",
    "the uncorrelated background events are generated in such a way that the fast signal energies are generated on the base of the energy distribute of the natural radioactivity in the proposal of the day bay experiment@xcite , the energies for the neutron events of the single signal are regarded as the delay signal energies , the delay times are uniformly generated from 2 @xmath43s to 100 @xmath43s and the positions of the fast signal and the delay signal are uniformly generated throughout gd - ls region .",
    "the fast neutron events are uniformly generated throughout gd - ls region and their energy are uniformly generated from 0 mev to 50 mev , therein the events of two signals are regarded as the fast neutron backgrounds .",
    "since the behavior of @xmath0he/@xmath1li decay in the detector could nt be simulated by the geant4 package , @xmath0he/@xmath1li events are generated in such a way that the fast signal energies are generated on the base of the energy distribute of @xmath0he/@xmath1li in the proposal of the day bay experiment@xcite , and the other physical quantities are from fast neutron events in the paper .",
    "the task of the event reconstruction in the reactor neutrino experiments is to reconstruct the energy and the vertex of a signal .",
    "the maximum likelihood method ( mld ) is a standard algorithm of the event reconstruction in the reactor neutrino experiments .",
    "the likelihood is defined as the joint poisson probability of observing a measured distribution of photoelectrons over the all pmts for given ( @xmath50 ) coordinates in the detector .",
    "the ref.@xcite for the work of the chooz experiment shows the method of the reconstruction in detail .    in the paper ,",
    "the event reconstruction with the mld are performed in the similar way with the chooz experiment@xcite , but the detector is different from the detector of the chooz experiment , so compared to ref.@xcite , there are some different points in the paper :    \\(1 ) the detector in the paper consists of three regions , so the path length from a signal vertex to the pmts consist of three parts , and they are the path length in gd - ls region , the one in ls region , and the one in oil region , respectively .",
    "\\(2 ) considered that not all pmts in the detector can receive photoelectrons when a electron is deposited in the detector , the @xmath51 equation is modified in the paper and different from the one in the chooz experiment , that is , @xmath52 , where @xmath53 is the number of photoelectrons received by the j - th pmt and @xmath54 is the expected one for the j - th pmt@xcite .",
    "\\(3 ) @xmath55 and the coordinates of the charge center of gravity for the all visible photoelectrons from a signal are regarded as the starting values for the fit parameters(@xmath50 ) , where @xmath56 is the total numbers of the visible photoelectrons from a signal and @xmath57 is the proportionality constant of the energy @xmath58 , that is , @xmath59 .",
    "@xmath57 is obtained through fitting @xmath56 s of the 1 mev electron events , and is @xmath60 in the paper .",
    "the fast and delay signals of a event in the toy detector are reconstructed using mld , respectively .",
    "the selections of neutrino events are as follows :    \\(1 ) positron energy : 1.3 mev < @xmath61 < 8 mev ;    \\(2 ) neutron energy : 6 mev < @xmath62 < 10 mev ;    \\(3 ) neutron delay : 2 @xmath43s < @xmath63@xmath64 < 100 @xmath43s ;    \\(4 ) relative positron - neutron distance : @xmath65 < 100 cm .    a hyper - cuboid in the event space",
    "is defined by the selection , and the inside is the signal region and the outside is the background region .",
    "39000 events of neutrino are generated in the signal region .",
    "11000 events each of uncorrelated background , fast neutrons and @xmath0he/@xmath1li are generated in the signal region , respectively .",
    "the energies of the fast signal and the delay signal(@xmath61 , @xmath62 ) , the delay time of the delay signal(@xmath63@xmath64 ) and the distance between the fast signal and the delay signal ( @xmath65 ) are used as inputs to all neural networks , which have the same structure . in the paper ,",
    "all the networks have the input layer of four inputs , the single hidden layer of nine nodes and the output layer of a single output which is just the probability that an event belongs to the neutrino event . a markov chain of neural networks is generated using the bayesian neural networks package of radford neal , with a training sample consisting of the neutrino events and the backgrounds .",
    "one thousand iterations , of twenty mcmc steps each , are used .",
    "the neural network parameters are stored after each iteration , since the correlation between adjacent steps is very high .",
    "that is , the points in neural network parameter space are saved to lessen the correlation after twenty steps here .",
    "it is also necessary to discard the initial part of the markov chain because the correlation between the initial point of the chain and the points of the part is very high .",
    "the initial three hundred iterations are discarded here .",
    "3000 events each of the neutrino and the three backgrounds are used to test the identification capability of the trained bnn . in the paper , the bnns are trained by the different training samples , which consist of the neutrino events and three backgrounds at different rates , since the different identification efficiencies are obtained with those bnns .",
    "the results of the identification with those bnns are listed in tab . 1 .",
    "1 , the neutrino discrimination increases from 82.6% to 91.2% with the increase of the neutrino rate from one second to nine fourteenth in the training sample using bnn in signal region . however , the background discriminations decrease with the decrease of the background rate in the training sample . the uncorrelated background discrimination decrease from 88.2% to 73.6% with the decrease of its rate from one sixth to one fourteenth in the training sample .",
    "the fast neutron background discrimination decreases from 48.5% to 37.6% with the decreases its rate from one sixth to one seventh in the training sample .",
    "the @xmath0he/@xmath1li background discrimination decreases from 51.8% to 39.9% with the decrease of its rate from one sixth to one seventh in the training sample . as a result ,",
    "the most neutrino events and uncorrelated background events in the signal region can be identified with bnn , and the part events each of the fast neutron and @xmath0he/@xmath1li backgrounds in the signal region can be identified with bnn . the different signal to noise ratios in signal region",
    "are obtained with bnns trained by the training samples consisting of neutrino events and background events at different rates in the reactor neutrino experiments . in a word ,",
    "the signal to noise ratio in signal region can be enhanced with bnn in the reactor neutrino experiments .",
    "this work is supported by the national natural science foundation of china ( nsfc ) under the contract no .",
    "10605014 .",
    "p. c. bhat and h. b. prosper _ beyesian neural networks_. in : l. lyons and m. k. unel ed .",
    "_ proceedings of statistical problems in particle physics , astrophysics and cosmology , oxford ,",
    "uk 12 - 15 , september 2005_. london : imperial college press .",
    "151 - 154                      .the different identification efficiencies are obtained with the bnns trained by the different training samples , which consist of the neutrino and three backgrounds at different rates .",
    "the term after @xmath66 is the statistical error of the identification efficiencies .",
    "the 3000 events each of the uncorrelated background , fast neutron and @xmath0he/@xmath1li are regarded as the test sample . [ cols=\"^,^,^,^,^,^ \" , ]"
  ],
  "abstract_text": [
    "<S> a toy detector has been designed to simulate central detectors in reactor neutrino experiments in the paper . </S>",
    "<S> the samples of neutrino events and three major backgrounds from the monte - carlo simulation of the toy detector are generated in the signal region . </S>",
    "<S> the bayesian neural networks(bnn ) are applied to separate neutrino events from backgrounds in reactor neutrino experiments . as a result </S>",
    "<S> , the most neutrino events and uncorrelated background events in the signal region can be identified with bnn , and the part events each of the fast neutron and @xmath0he/@xmath1li backgrounds in the signal region can be identified with bnn . </S>",
    "<S> then , the signal to noise ratio in the signal region is enhanced with bnn . </S>",
    "<S> the neutrino discrimination increases with the increase of the neutrino rate in the training sample . </S>",
    "<S> however , the background discriminations decrease with the decrease of the background rate in the training sample .    </S>",
    "<S> @xmath2department of physics , nankai university , tianjin 300071 , people s republic of china    bayesian neural networks , neutrino oscillation , identification    pacs numbers : 07.05.mh , 29.85.fj , 14.60.pq </S>"
  ]
}