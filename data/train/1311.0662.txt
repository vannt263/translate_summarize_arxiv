{
  "article_text": [
    "the increasing interest in analysis of high - dimensional data has necessitated the development of parsimonious multivariate models with reliable , computationally efficient estimation procedures .",
    "for example , sparse gaussian graphical models @xcite have drawn significant interest and several computationally efficient estimation procedures have been developed @xcite .",
    "gaussian graphical models with symmetry @xcite form another example , though no efficient estimation procedures have yet been developed .",
    "here we describe and exploit a method which provides linear estimating equations when applied to any exponential family , in particular to any gaussian graphical model with symmetry .",
    "in contrast to the maximum likelihood estimator , which often requires iterative methods , this _ score matching estimator _ is computationally efficient for such families and therefore has potential for initial model screening .",
    "even when the maximum likelihood estimator is desired , it must be computed iteratively and the score matching estimator may provide a useful initial value for the iterations .",
    "consider a random quantity taking values in an open subset @xmath0 of @xmath1 which we consider equipped with the standard inner product @xmath2 , the associated norm @xmath3 , and the canonical basis .    throughout the paper",
    ", @xmath4 denotes a class of distributions over @xmath0 with twice continuously differentiable densities with respect to the lebesgue measure on @xmath0 .",
    "the general developments below are equally valid for @xmath0 being a riemannian manifold with associated geometric measure @xcite , but as our main focus is the multivariate gaussian distribution we shall refrain from working at this level of generality .",
    "we use @xmath5 for the gradient and @xmath6 for the laplace operator on @xmath0 so that @xmath7      a _ scoring rule _",
    "@xmath8 is a real - valued function which quantifies the accuracy of a predictive distribution @xmath9 upon observing the realized value @xmath10 .",
    "it is ( strictly ) _ proper _ if the expected value @xmath11 is ( uniquely ) minimized over @xmath4 at @xmath12 .",
    "two scoring rules are _ equivalent _ if they differ by a positive scalar multiple and a function of @xmath13 alone .",
    "every proper scoring rule induces a _ divergence _ @xcite : @xmath14 the divergences of equivalent scoring rules are proportional .",
    "a much used scoring rule is the _ log score _",
    "@xmath15 , where @xmath16 is the density of @xmath17 @xcite ; the corresponding divergence is then the kullback  leibler divergence .    given an independent sample @xmath18 with empirical distribution @xmath19 and unknown distribution @xmath17 , the _ optimal score estimator _ @xcite @xmath20 of @xmath17 is determined as the minimizer of the empirical score @xmath21 the first two expressions are well - defined even when @xmath22 whereas the latter may not be .",
    "@xcite show that for a parametrized family @xmath23 with @xmath24 being an open subset of @xmath25 ,",
    "this minimization gives rise to an _ unbiased estimating equation _",
    "@xcite @xmath26 where @xmath27 is the vector of derivatives of @xmath28 w.r.t .",
    "solutions to such equations are also known as m - estimators @xcite and these are typically consistent and asymptotically normal although not necessarily efficient .",
    "if @xmath15 is the log score , the equation is the likelihood equation and the corresponding estimator is the maximum likelihood estimator ( mle ) .",
    "suppose the density @xmath16 of @xmath30 is twice continuously differentiable and satisfies the regularity assumptions : @xmath31 as well as @xmath32 then , using integration by parts , @xcite showed that the divergence function @xmath33 where @xmath34 is the density of @xmath35 , is induced by the scoring rule @xmath36 this scoring rule can be shown to be proper @xcite .",
    "the _ score matching estimator _ ( sme ) is the optimal score estimator for this scoring rule .",
    "note that the sme is not invariant under transformations of @xmath13 , nor under change of base measure .",
    "hence care must be taken when choosing the representation of the data to ensure the resulting estimator is suitable .",
    "as indicated in @xcite , the sme is particularly simple when @xmath4 is an exponential family with densities @xmath37 where @xmath38 here @xmath39 is the canonical sufficient statistic , @xmath40 is a @xmath41-dimensional vector space , @xmath42 an inner product on @xmath40 , and @xmath43 is the ( convex ) canonical parameter space @xmath44 let @xmath45 be the linear map from @xmath40 to @xmath1 determined by the equation @xmath46 for all @xmath47",
    ". then we have @xmath48 further we get @xmath49 where , similarly , @xmath50 is given by @xmath51 .",
    "we assume that the regularity conditions and are satisfied ; thus in particular both @xmath52 and @xmath53 are finite .",
    "let @xmath54 be the transpose of @xmath45 given by @xmath55 for all @xmath47 and @xmath56 .",
    "we furthermore assume that the linear map @xmath57 from @xmath40 to @xmath40 is invertible so the corresponding quadratic form @xmath58 is positive definite , i.e.  it is non - zero unless @xmath59 .",
    "the objective function @xmath60 becomes @xmath61 the last two terms depend on @xmath13 only and will henceforth be ignored : this yields an equivalent scoring rule and does not alter the sme .",
    "the objective function @xmath62 depends quadratically on @xmath29 and the minimizer of @xmath62 is unique if and only if the quadratic form on @xmath40 @xmath63 is positive definite , i.e.  if @xmath64 for @xmath65 implies @xmath59 .",
    "the sme is the minimizer of @xmath66 over @xmath24 , leading to the linear estimating equation for @xmath29 @xmath67 thus , provided @xmath68 is invertible , the score estimation equation has the unique solution @xmath69 in the case @xmath70 this is equivalent to the _ variational estimator _ ( eq . 1.9 ) of @xcite and to eq",
    ". 34 of @xcite . by taking the inner product of with the minimizer",
    "@xmath71 we further obtain @xmath72 inserting this relation into the expression for @xmath73 yields a linear dependence of the minimal value of @xmath62 on @xmath74 with the simple expression @xmath75 where we have ignored the terms depending on @xmath13 alone .",
    "as mentioned earlier , smes are m - estimators and thus typically consistent , as also claimed in corollary 3 of @xcite .",
    "however , the argument in @xcite is incomplete since the convergence of @xmath76 to its expectation does not imply that the minimizer @xmath77 converges to @xmath29 without additional conditions on @xmath62 .",
    "general consistency results are established under suitable regularity conditions in @xcite , but for an exponential family we can establish consistency directly .",
    "assume @xmath78 is an exponential family as above and @xmath79 is a sample of size @xmath80 from @xmath81 .",
    "then the sme @xmath74 is asymptotically consistent for @xmath29 .",
    "[ prop : consistency ]    each term in the last sum of has expectation @xmath82 and both integrals on the right - hand side are finite by our assumptions . using integration by parts on the second term yields",
    "@xmath83 since the boundary terms vanish by . using that @xmath84 and substituting the expression for @xmath85 on the right - hand side yields @xmath86 which then gives @xmath87 thus , by the law of large numbers @xmath88 in probability as well as with probability one",
    "similarly , for the first sum in we get @xmath89 which is invertible by assumption .",
    "hence the estimate @xmath90 converges to @xmath29 and is thus consistent for @xmath29 , as desired .    under a few additional assumptions",
    ", we can also show that the sme is asymptotically normally distributed .",
    "[ prop : normality]assume @xmath78 is an exponential family as above and @xmath79 is a sample of size @xmath80 from @xmath81 .",
    "assume further that all of @xmath91 , @xmath92 , and @xmath93 are finite .",
    "then @xmath94 converges in distribution to a normal distribution on @xmath40 with mean zero .    from",
    "we get @xmath95 where @xmath96 as in the proof of proposition  [ prop : consistency ] , we conclude that @xmath97 converges in probability to its expectation @xmath98 , which is invertible by assumption .",
    "the @xmath99 term of the sum in has expectation zero and finite variance and hence , by the central limit theorem , the normalized sum converges in distribution to a normal distribution on @xmath40 with mean zero . by slutsky s theorem , so does @xmath94 , as desired .    the asymptotic inverse covariance of the sme is @xmath100 where @xmath101 is the _ godambe information _",
    "@xcite @xmath102 here @xmath103 see for example ( * ? ? ?",
    "* section 9.2 ) or ( * ? ? ? * theorem 5.21 ) .",
    "note that , as we have shown above , @xmath97 is a consistent estimator of @xmath98 and @xmath104 can be estimated consistently by the corresponding empirical covariance .    for finite @xmath80",
    "it is possible that @xmath105 . even in this case",
    "@xmath106 itself may be useful : the value of @xmath107 can be very quickly computed and used for model screening , or @xmath106 can be used as a starting value for iterative estimation methods .",
    "consistency ensures that @xmath108 for @xmath80 sufficiently large .    as noted earlier ,",
    "the score matching equation is not invariant under data reparametrization , nor under change of base measure .",
    "we could in principle use the estimating equation after reducing to the sufficient statistic @xmath109 , which has density @xmath110 where @xmath111 and where @xmath112 is the density of the product of measures @xmath113 transformed by the sufficient statistic .",
    "then the sme becomes @xmath114 , which coincides with both @xcite s exact estimator @xcite and the maximum plausibility estimator @xcite for this case .",
    "the exact estimator is known to be consistent and efficient @xcite .",
    "unfortunately , the form of @xmath115 is often intractable and the exact estimator is often more difficult to calculate than the mle .",
    "we have chosen not to reduce by sufficiency : our sme may lose statistical efficiency , but it gains computational speed from the simplicity of its estimating equations .",
    "we now consider gaussian models with linear structure in the concentration matrix @xcite , exploiting that they are special instances of the exponential families discussed above .",
    "let @xmath40 be a @xmath41-dimensional subspace of @xmath116 , the symmetric @xmath117 matrices equipped with the trace inner product @xmath118 and associated _ frobenius _ norm @xmath119 .",
    "consider the family of gaussian densities @xmath120 which clearly has the form with @xmath121 , @xmath122 , @xmath123 , @xmath70 , and @xmath124 , where @xmath125 is the orthogonal projection onto @xmath40 in @xmath116 .",
    "the canonical parameter space is @xmath126 , where @xmath127 is the set of positive definite symmetric @xmath117 matrices , @xcite .",
    "the maps discussed above become @xmath128 where @xmath129 is the @xmath117 identity matrix . for simplicity",
    "we assume in the following that @xmath130 ; this can always be achieved for non - empty @xmath24 by choosing a suitable basis for @xmath131",
    ". then we have @xmath132 so the laplacian becomes @xmath133 .    to see that holds ,",
    "note that for any @xmath134 we have @xmath135 so @xmath136 .",
    "furthermore we have @xmath137 and finally @xmath138 in particular we get @xmath139 where @xmath140 is the _ jordan product",
    "_ @xcite of the symmetric matrices @xmath141 and @xmath142 . the estimating equation now specializes to @xmath143 where we have let @xmath144 be the scaled wishart matrix of sums of squares and products . the expression for @xmath62 becomes simply @xmath145 which can be evaluated very quickly .",
    "we next verify that @xmath146 satisfies the assumptions for consistency and asymptotic normality . for consistency we must satisfy the regularity conditions and .",
    "it is obvious that both @xmath146 and @xmath147 tend to zero as @xmath148 , and furthermore @xmath149    for asymptotic normality we note that @xmath150 since @xmath70 .",
    "furthermore @xmath151 .",
    "finally , @xmath152\\\\ & \\leq { e_{\\theta } } [ \\{\\operatorname{tr}(k{\\circ}xx{^{\\top}})\\}^2]\\\\ & = { e_{\\theta}}\\{(x{^{\\top}}k x)^2\\},\\end{aligned}\\ ] ] which is the fourth moment of a normal distribution and hence finite .",
    "thus all the conditions for consistency and asymptotic normality are satisfied .",
    "we refrain from calculating a specific expression for the godambe information .",
    "consider the special case where @xmath40 is closed under the jordan product , i.e.  it forms a jordan subalgebra of @xmath116 , or equivalently @xmath153 is closed under inversion .",
    "such hypotheses are exactly those which are linear in both the covariance and inverse covariance @xcite . in particular",
    ", @xmath40 contains all models which are determined by invariance under a subgroup of the general linear group @xcite .",
    "we shall show that the mle and the sme coincide for these models .",
    "first we need a lemma .",
    "[ lem : jordanproj ] if @xmath40 is a jordan subalgebra of @xmath116 then for any @xmath154 and @xmath155 we have @xmath156 .",
    "let @xmath157 .",
    "clearly we have @xmath158 since @xmath40 is closed under the jordan product .",
    "further , for any @xmath134 we have @xmath159 where the last equality follows because @xmath160 .",
    "thus @xmath161 is orthogonal to @xmath40 and the lemma follows .",
    "we now obtain the desired result .",
    "if the subspace @xmath40 is a jordan subalgebra then the sme is equal to the mle .",
    "furthermore , if @xmath162 is invertible we have @xmath163    the family is a full and canonical exponential family with @xmath162 as the canonical sufficient statistic , and hence the likelihood equation becomes @xmath164 this implies @xmath165 .    as @xmath40 is a jordan subalgebra we have @xmath166 . using lemma  [ lem : jordanproj ] , the score matching equation reduces to @xmath167 whence we get @xmath168 .",
    "this completes the proof .",
    "having observed a sample @xmath169 , the score matching equation has a unique solution if and only if any of the following equivalent conditions hold :    1 .",
    "the quadratic form @xmath170 is positive definite on @xmath40 2 .",
    "@xmath171 is the only element of @xmath40 which maps all @xmath172 to zero 3 .",
    "the kernel of the linear map @xmath173 is trivial .",
    "we say _ the sme exists _ if has a unique solution , ignoring the fact that @xmath174 may not be positive definite .",
    "we have the following relation between existence of the sme and the mle .",
    "[ prop : sme2mle ] consider a gaussian linear concentration model and let @xmath175 be an empirical covariance matrix as above .",
    "if the sme for @xmath175 exists , then the mle for @xmath175 also exists .",
    "we proceed by assuming that the mle for @xmath175 does not exist and showing that the sme does not exist either .",
    "if the mle does not exist , the convex level set of the likelihood function @xmath176 must be unbounded . here",
    "@xmath177 is proportional to the logarithm of the likelihood function @xmath178 and we have assumed without loss of generality that @xmath166 .",
    "this implies ( * ? ? ?",
    "* theorem 5.5 ) that there is an @xmath179 such that @xmath180 for all @xmath181 , i.e. @xmath182 or equivalently that @xmath183 but if @xmath184 are the positive eigenvalues of @xmath141 we have @xmath185 and for large @xmath186 this grows slower with @xmath186 than any line through the origin with positive slope .",
    "thus we must have @xmath187 .",
    "hence , since @xmath188 and thus @xmath189 are positive semidefinite , we have @xmath190 and thus @xmath191 .",
    "thus the third condition for the existence of the sme does not hold .    by choosing an orthogonal basis @xmath192 for @xmath40",
    ", we can write the the matrix for the quadratic form @xmath193 as @xmath194 where @xmath195 hence @xmath193 is positive definite if and only if @xmath196 .",
    "this determinant is a polynomial in @xmath13 and hence it either holds that @xmath197 for all @xmath13 or @xmath198 except for a set of lebesgue measure zero @xcite . in other words , either the sme exists with probability one , or else it never exists .",
    "this is in contrast to the mle , which can exist with some probability strictly between zero and one @xcite .",
    "we shall say that the linear space @xmath40 is _",
    "@xmath80-estimable _ if the sme exists with probability one , or equivalently , if there is an @xmath199 such that @xmath198 .    for @xmath200",
    "it is well - known that @xmath175 is positive definite with probability one and hence @xmath201 is positive definite and any @xmath40 is @xmath80-estimable .",
    "we may thus without loss of generality assume @xmath202 in the following .",
    "as many high - dimensional data sets have @xmath80 much less than @xmath203 , this case is highly relevant .",
    "let @xmath204 and @xmath205 denote the @xmath206 triangular number .",
    "let @xmath40 be a linear subspace of @xmath116 . if @xmath207 then @xmath40 is not @xmath80-estimable .",
    "[ prop : notestimable ]    let @xmath208 and let @xmath209 be the space of symmetric matrices that send all vectors in @xmath210 to zero . since @xmath211 with probability one , we have @xmath212 .",
    "noticing that the quadratic form @xmath193 is positive definite over @xmath40 if and only if @xmath213 , and that @xmath214 we see that @xmath193 is not positive definite if @xmath215 .",
    "unfortunately the converse is not always true and it may happen that @xmath40 is not @xmath80-estimable even if @xmath216 .",
    "in particular we note that @xmath217 yields a counterexample .",
    "we have @xmath218 and @xmath219 so if we consider a single observation , we have @xmath220 .",
    "letting @xmath221 be our single observation , the corresponding quadratic form @xmath222 is @xmath223 direct computation shows that @xmath222 is singular since it has the zero eigenvector @xmath224 this particular @xmath40 , investigated by @xcite , is an example of a jordan subalgebra and we conclude  as @xcite  that the mle also fails to exist .",
    "gaussian graphical models @xcite are special instances of linear concentration models .",
    "for example , in the model given by the four - cycle ( figure  [ fig : fourcycle ] ) ,    [ node/.style = circle , fill = black , inner sep=0pt , minimum size=5mm , text = white , font= * * * * , edge/.style = draw = black , line width=2pt ] ( 1 ) at ( -1,1 ) 1 ; ( 2 ) at ( 1,1 ) 2 ; ( 4 ) at ( -1,-1 ) 3 ; ( 3 ) at ( 1,-1 ) 4 ; ( 1)(2 ) ; ( 2)(3 ) ; ( 3)(4 ) ; ( 4)(1 ) ;    the mle can only be calculated with iterative methods . if @xmath225 , the mle may or may not exist , whereas for @xmath226 the mle always exists @xcite .",
    "for @xmath225 we have @xmath227 , and since @xmath228 , proposition  [ prop : notestimable ] yields that the sme does not exist . for @xmath226 and above both the sme and mle exist with probability one , the sme being a solution to a system of eight linear equations .    in the following we list a number of facts about @xmath80-estimability which may assist in determining whether a given subspace @xmath40 is @xmath80-estimable . in particular , we show that a subspace of an @xmath80-estimable space is @xmath80-estimable , and that a change of coordinate system does not affect @xmath80-estimability .",
    "[ lem : subspace ] if @xmath40 is @xmath80-estimable and @xmath229 , then @xmath230 is @xmath80-estimable . if @xmath40 is @xmath80-estimable with @xmath231 , then @xmath40 is @xmath232-estimable .",
    "the first statement follows since @xmath233 .",
    "if we let @xmath234 we have @xmath235 and the second statement follows since @xmath236 .",
    "[ lem : gl ] if @xmath237 is invertible , then @xmath40 is @xmath80-estimable if and only if @xmath238 is @xmath80-estimable .",
    "this follows as @xmath239 .",
    "we next identify @xmath80-estimability with the ability to transform @xmath40 into what we call _ standard form_. this condition may be easier to check in some situations .",
    "we first identify @xmath240 with @xmath241 and @xmath242 via @xmath243 denote by @xmath244 the subspace of @xmath116 with @xmath245 .",
    "we then have @xmath246 , @xmath247 , and @xmath248 .",
    "we say that @xmath40 has _",
    "@xmath80-standard form _ if @xmath249 for some linear function @xmath250 .",
    "note that if @xmath40 has @xmath80-standard form then we have @xmath251    [ lem : standard ] if @xmath40 has @xmath80-standard form then @xmath40 is @xmath80-estimable .",
    "if @xmath40 has @xmath80-standard form we can choose @xmath169 as the first @xmath80 standard basis vectors @xmath252 of @xmath131 .",
    "then for any @xmath253 of the form we have @xmath254 for all @xmath255 if and only if @xmath256 and @xmath257 .",
    "hence we must have @xmath171 , so the quadratic form @xmath258 is positive definite .",
    "[ cor : estimability]if @xmath259 and for some @xmath237 , @xmath260 has @xmath80-standard form , then @xmath40 is @xmath80-estimable .",
    "this follows by combining lemma  [ lem : subspace ] , lemma  [ lem : gl ] , and lemma  [ lem : standard ] .",
    "we also note that the converse to corollary  [ cor : estimability ] holds , as shown in the following lemma .",
    "[ lem : estimability]if @xmath40 is @xmath80-estimable then there exists @xmath261 and @xmath262 such that @xmath260 has @xmath80-standard form .",
    "let @xmath80 be the smallest integer @xmath263 such that @xmath40 is @xmath263-estimable .",
    "then there exist orthogonal vectors @xmath264 such that @xmath265 has full rank .",
    "let @xmath141 denote the transformation to a coordinate system where @xmath266 are the first @xmath80 basis vectors .",
    "in this coordinate system we have @xmath267 where @xmath268 with @xmath269 and @xmath270 .",
    "since @xmath40 is @xmath80-estimable , this map is injective and thus we must have @xmath271 for some linear function @xmath272 . thus in this basis @xmath259 where @xmath230 has standard form .    finally we show that all non - trivial @xmath40 with @xmath273 and @xmath274 are @xmath80-estimable .",
    "[ prop : np1 ] suppose that @xmath40 is a linear subspace of @xmath116 with @xmath275 and that @xmath216 with @xmath276 .",
    "then @xmath40 is @xmath80-estimable .",
    "for contradiction , assume @xmath277 and that @xmath40 is not @xmath80-estimable for @xmath278 .",
    "thus for _ any _ @xmath266 , the map @xmath279 has rank less than @xmath41 .",
    "assume now that @xmath280 are orthonormal so that @xmath281 form an orthonormal basis for @xmath131 .",
    "in this basis we have @xmath282 and thus @xmath283 where @xmath284 with @xmath285 and @xmath286 .",
    "since @xmath40 is not @xmath80-estimable , there must be an @xmath154 with @xmath287 , which implies that @xmath288 unless @xmath289 .",
    "we may thus assume that @xmath290 in the original basis , we have @xmath291 . since @xmath281 were arbitrary , we have shown that for any vector @xmath292 of length one , @xmath293 . since @xmath40 is a linear subspace",
    "we conclude that any matrix @xmath253 of the form @xmath294 is in @xmath40 , and hence @xmath295 .",
    "this implies @xmath296 , which is a contradiction .",
    "we conclude that @xmath40 is @xmath80-estimable .",
    "notice that even when @xmath40 is @xmath80-estimable , the estimated concentration matrix may not be positive definite if @xmath40 is not a jordan subalgebra .",
    "all we can say is that the estimate will be positive definite for sufficiently large @xmath80 by the consistency result in proposition  [ prop : consistency ] .",
    "if @xmath174 is not positive definite and the estimate of @xmath253 itself is of interest , it may be necessary to calculate the mle @xmath297 : the latter exists and is positive definite whenever @xmath174 exists . in any case ,",
    "lack of positive definiteness of @xmath174 indicates that the estimate may not be reliable and that there could be too few observations to justify the use of a model of such complexity .",
    "gaussian graphical models with symmetries @xcite are linear concentration models generated by a coloured graph .",
    "more precisely , we let @xmath298 denote a _ coloured graph _ where @xmath299 is a partition of a finite vertex set @xmath300 into _ vertex colour classes _ and @xmath301 a partition of an edge set @xmath302 into _ edge colour classes_. such a graph determines a linear concentration model with @xmath303 being the set of symmetric @xmath117 matrices @xmath253 with entries @xmath304 whenever @xmath305 and @xmath306 are not neighbours in @xmath307 , any two off - diagonal elements being identical if the corresponding edges are in the same colour class , and any two diagonal elements identical if the corresponding vertices are in the same colour class .",
    "let @xmath308 for @xmath309 denote the @xmath310 diagonal matrix with @xmath311 if @xmath312 and 0 otherwise .",
    "similarly , for each edge colour class @xmath313 we let @xmath308 be the @xmath310 symmetric matrix with @xmath314 if @xmath315 and 0 otherwise .",
    "then @xmath316 form an orthogonal basis for @xmath40 .",
    "the likelihood equations @xcite become @xmath317 which are non - linear in @xmath253 and must be solved by iterative methods in most cases .",
    "one motivation for introducing these models was the potential reduction in the number of parameters of the corresponding uncoloured graphical model .",
    "this increases the stability of estimates and allows estimators to exist for a smaller number of observations .",
    "the last issue was considered in detail by @xcite for specific examples . as",
    "an aside we note that the models determined by the coloured graphs 11 , 14 and 17 in ( * ? ? ?",
    "* table 2 ) are supermodels of the jordan linear concentration model and hence we can confirm @xcite s conjecture that in these cases , the mle does not exist for @xmath318 .",
    "we note that our proposition  [ prop : notestimable ] implies that the sme does not exist if @xmath319 and believe that the condition @xmath320 is sufficient to ensure @xmath80-estimability for this particular class of linear concentration models , but have not been able to show this except for the case of @xmath278 , cf .  proposition  [ prop : np1 ] .",
    "however , none of the examples in @xcite or @xcite provide counterexamples to this conjecture .",
    "note that the mle may well exist even if the sme does not exist ; see for example the earlier discussion of the four - cycle .",
    "if our conjecture is correct , proposition  [ prop : sme2mle ] implies that @xmath320 is also sufficient for the existence of the mle and hence provides a simple method of checking for this .",
    "the linear score matching equations for graphical gaussian models with symmetries are @xmath321 which should be compared to ; they have a strong similarity with the yule ",
    "walker equations for estimating the parameters of autoregressive processes in a time series , as also noted by @xcite .    indeed ,",
    "a circular autoregressive process of order @xmath322 is an example of a coloured graphical model with symmetry determined by the cyclic permutation group , as displayed in figure  [ fig : circularar ] . in this case",
    "walker equations are exactly equivalent to the score matching equations .",
    "[ node/.style = circle , fill = s1,inner sep=0pt , minimum size=5 mm , edge1/.style = draw = s2,line width=2pt , edge2/.style = draw = s3,line width=2pt , loosely dashed ] ( 1 ) at ( canvas polar cs : angle=90,radius=2 cm ) ; ( 2 ) at ( canvas polar cs : angle=141,radius=2 cm ) ; ( 3 ) at ( canvas polar cs : angle=192,radius=2 cm ) ; ( 4 ) at ( canvas polar cs : angle=243,radius=2 cm ) ; ( 5 ) at ( canvas polar cs : angle=294,radius=2 cm ) ; ( 6 ) at ( canvas polar cs : angle=345,radius=2 cm ) ; ( 7 ) at ( canvas polar cs : angle=39,radius=2 cm ) ; ( 1)(2 ) ; ( 2)(3 ) ; ( 3)(4 ) ; ( 4)(5 ) ; ( 5)(6 ) ; ( 6)(7 ) ; ( 7)(1 ) ; ( 1)(3 ) ; ( 2)(4 ) ; ( 3)(5 ) ; ( 4)(6 ) ; ( 5)(7 ) ; ( 6)(1 ) ; ( 7)(2 ) ;      model selection in gaussian graphical models with symmetry is problematic as the number of possible models is enormous .",
    "this affects both stepwise methods , as used in @xcite , and lattice based methods @xcite , as used in @xcite .",
    "the computational efficiency of the sme allows rapid screening of a large number of potential models as the minimized objective function indicating the model fit @xmath323 is particularly simple to calculate .",
    "note that this minimum can be calculated even though @xmath174 may not be positive definite ; in particular a time consuming check of positive definiteness can then be avoided . in this case",
    "the minimum may overestimate the model fit as it corresponds to the minimum of @xmath62 over the entire space @xmath40 rather than over @xmath43 .    to prevent overfitting a penalty for the number of parameters @xmath324 should be added to @xmath62 to give the objective function @xmath325 the scalar multiple @xmath326 for the penalty can , for example , be determined by a method such as cross - validation .",
    "such rapid model screening may be useful to identify a small set of plausible models to be considered by more sophisticated search procedures .",
    "we briefly describe some numerical experiments with the sme for gaussian graphical models with and without symmetries .",
    "these indicate that the sme provides an extremely fast estimate which is reasonably accurate for large @xmath80 .",
    "[ node1/.style = circle , fill = s2,inner sep=0pt , minimum size=5 mm , node2/.style = circle , fill = s1,inner sep=0pt , minimum size=5 mm , node3/.style = circle , fill = s3,inner sep=0pt , minimum size=5 mm , edge3/.style = draw = s4,line width=2pt , edge2/.style = draw = s5 , line width=2pt , dash pattern = on 10pt off 7pt , edge1/.style = draw = s6,line width=2pt , dash pattern = on 3pt off 10pt ] ( algebra ) at ( 0,0 ) ; ( vectors ) at ( -4.5,1.5 ) 1 ; ( analysis ) at ( 4.5,1.5 ) 1 ; ( mechanics ) at ( -4.5,-1.5 ) 2 ; ( statistics ) at ( 4.5,-1.5 ) 2 ; ( mechanics)(vectors ) ; ( analysis)(statistics ) ; ( mechanics)(algebra ) ; ( statistics)(algebra ) ; ( vectors)(algebra ) ; ( analysis)(algebra ) ;    first we consider the * mathmarks * dataset @xcite included in * grc * @xcite . following the analysis in @xcite , we use three vertex colour classes and three edge colour classes as shown in figure  [ fig : mathgraph ] .",
    "we vary the number of observations @xmath80 from 4 to 88 and compute both the sme and the mle for each @xmath80 .",
    "figure  [ fig : converge ] shows how the sme approximates the mle as @xmath80 grows .",
    "the sme appears to provide a computationally efficient estimator with good accuracy for large @xmath80 .",
    "+ [ thick , no markers , color = s2 ] table scaledfrobenius.txt ;    next we shall give an example of the sme identifying a non - decomposable graphical gaussian model .",
    "we first simulated data from a square lattice model with @xmath327 vertices : the concentration matrix for our model is symmetric with upper - triangular entries @xmath328 for @xmath329 .",
    "the previously mentioned four - cycle ( figure  [ fig : fourcycle ] ) is such a model with @xmath330 .",
    "we then used the sme to conduct a rapid model search over uncoloured graphs @xmath331 on @xmath203 vertices .",
    "we now describe our model search method . we began by initializing the graph to the best - fitting tree via kruskal s algorithm @xcite using squared correlations as weights .",
    "this initialization is very fast , and if we were only searching over trees , it corresponds to the maximum likelihood estimate @xcite . in our case",
    "we are searching among all undirected graphs , but the tree step provides a computationally efficient starting position for the search .",
    "next we conducted a greedy `` forward search '' and successively add edges to @xmath331 . to ensure our method",
    "is scalable for large @xmath203 , we considered adding edges in order of decreasing squared correlations and we terminated the forward search after attempting to add an edge that fails to improve the objective function .",
    "finally , we conducted a greedy `` backward search '' by successively removing existing edges from @xmath331 .    before running this algorithm",
    "we identified a suitable penalty @xmath326 .",
    "we considered the change in the objective function @xmath62 after adding the single extra edge which most improved the objective to the true model . by simulating several thousand samples over the grid with @xmath332 and @xmath333",
    ", we found that the expected change in @xmath62 was approximately proportional to @xmath334 .",
    "we chose to proceed with @xmath335 .",
    "we quantified the accuracy of the fitted model @xmath174 by considering the number of missing edges ( @xmath336 but the true @xmath337 ) and extra edges ( @xmath338 but the true @xmath339 ) in figure  [ fig : nmissingextra ] .",
    "+ [ color = s1 ] table[x = nmp , y = nextra16 ] ; + [ color = s2 ] table[x = nmp , y = nextra64 ] ; + [ color = s3 ] table[x = nmp , y = nextra256 ] ; + [ dashed , color = s1 ] table[x = nmp , y = nmiss16 ] ; + [ dashed , color = s2 ] table[x = nmp , y = nmiss64 ] ; + [ dashed , color = s3 ] table[x = nmp , y = nmiss256 ] ;    finally , we visually inspect how the score matching estimate becomes more accurate with increasing @xmath80 in figure  [ fig : sparsityimages ] .",
    "we see that the estimate is unstable at @xmath340 , but for @xmath341 the sme correctly identifies the majority of the true non - zero entries with few extra edges .",
    "note that for @xmath342 and @xmath341 the model is correctly identified .    in 1,5,10    in 16,64,256    0.33        +    in the case of searching for uncoloured graphical models as above",
    ", the sme may be seen as alternative to the graphical lasso algorithm @xcite .",
    "it is difficult to directly compare the accuracy of the sme to the graphical lasso due to the unspecified regularization parameter in the latter algorithm .",
    "the graphical lasso estimate is extremely sensitive to the precise value of this regularization parameter : by fine - tuning the parameter for each @xmath80 and @xmath203 we were able to achieve results equal to or better than those of the sme , however the range of values which yields accurate estimates is narrow and highly dependent on @xmath80 and @xmath203 .",
    "by contrast the sme seems relatively robust against small changes in @xmath326 .",
    "we also suspect that the sme scales better for large @xmath203 than the graphical lasso .",
    "using a implementation of the sme written in @xmath343 we were able to consider models up to @xmath344 ( @xmath345 ) before running out of computer memory : even at this large @xmath203 each estimate of the sme could be completed within ten seconds , and with @xmath341 our rudimentary search procedure correctly identified the model .",
    "we attempted to test the graphical lasso at such @xmath203 using the r package * glasso * @xcite , however the r environment ran out of memory while attempting to load the sample covariance matrix .",
    "finally , we should emphasize that , in contrast to the graphical lasso , the sme can be used for graphical model with symmetries .",
    "thus the sme may be useful for model screening over such models , though this would require the development of computationally efficient model search strategies .",
    "it is outside the scope of the present article to study such strategies in any detail .",
    "score matching is an efficient method of parameter estimation for distributions with intractable normalization constants .",
    "it is particularly suitable to parameter estimation within an exponential family , where the score estimating equations are linear and yield a consistent estimate .",
    "the ready availability of highly optimized algorithms for linear equations means that the sme can be computed quickly and with a small memory footprint , even when the number of parameters is very large .",
    "the method seems particularly promising for rapid model screening and it would be well worthwhile to further investigate the optimal form of the penalty for model complexity , i.e.  the coefficient @xmath326 in , in particular how it should depend on @xmath80 to ensure consistent identification of the model along the lines in @xcite , see also the discussion in @xcite",
    ". it would be an advantage to have a simple sufficient condition for the existence of the sme  and hence also the mle  in gaussian graphical models ( with or without symmetries ) so that a model search could be automatically restricted to models of sufficiently limited complexity .",
    "we believe the condition in proposition  [ prop : notestimable ] is necessary and sufficient for the sme to exist in this case and thus sufficient for existence of the mle .",
    "unfortunately we only been able to show this in general for @xmath346 .",
    "we hope to return to these and other questions in the future ."
  ],
  "abstract_text": [
    "<S> in many families of distributions , maximum likelihood estimation is intractable because the normalization constant for the density which enters into the likelihood function is not easily available . </S>",
    "<S> the score matching estimator @xcite provides an alternative where this normalization constant is not required . </S>",
    "<S> the corresponding estimating equations become linear for an exponential family @xcite . </S>",
    "<S> the score matching estimator is shown to be consistent and asymptotically normally distributed for such models , although not necessarily efficient . </S>",
    "<S> gaussian linear concentration models are examples of such families . for linear concentration models that are also linear in the covariance @xcite </S>",
    "<S> we show that the score matching estimator is identical to the maximum likelihood estimator , hence in such cases it is also efficient . </S>",
    "<S> gaussian graphical models and graphical models with symmetries @xcite form particularly interesting subclasses of linear concentration models and we investigate the potential use of the score matching estimator for this case . </S>",
    "<S> + ams subject classifications : 62h12 ; 62f10 .    </S>",
    "<S> gaussian graphical models , jordan algebra models , score matching , scoring rules , symmetry </S>"
  ]
}