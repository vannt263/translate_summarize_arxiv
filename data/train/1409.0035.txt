{
  "article_text": [
    "closeness centrality is a structural measure of the importance of a node in a network , which is based on the ensemble of its distances to all other nodes .",
    "it captures the basic intuition that the closer a node is to all other nodes , the more important it is .",
    "structural centrality in the context of social graphs was first considered in 1948 by bavelas  @xcite .",
    "the classic definition measures the closeness centrality of a node as the inverse of the average distance from it and was proposed by bavelas  @xcite , beauchamp  @xcite , and sabidussi  @xcite . on a graph @xmath0 with @xmath1 nodes , the centrality of @xmath2 is formally defined by @xmath3 where @xmath4 is the shortest - path distance between @xmath2 and @xmath5 in @xmath6 .",
    "this textbook definition is also referred to as _",
    "bavelas closeness centrality _ or as the _ sabidussi index _  @xcite .    the classic closeness centrality of a node @xmath2 can be computed exactly using a single - source shortest paths computation ( such as dijkstra s algorithm ) . in general , however , we are interested not only in the centrality of a particular node , but rather in the set of all centrality values .",
    "this is the case when centrality values are used to obtain a relative ranking of the nodes . beyond that , the distribution of centralities captures important characteristics of a social network , such as its _ centralization _  @xcite .",
    "when we would like to perform many centrality queries ( in particular when we are interested in centrality values for all nodes ) on graphs with billions of edges , such as large social networks and web crawl graphs , the exact algorithms do not scale .",
    "instead , we are looking for scalable computation of approximate values , with small relative error .",
    "the node with maximum classic closeness centrality is known as the 1-median of the network . a near - linear - time algorithm for finding an approximate 1-median",
    "was proposed by indyk and thorup @xcite .",
    "their algorithm samples @xmath7 nodes at random and performs dijkstra s algorithm from each sampled node .",
    "they show that the node with minimum sum of distances to sampled nodes is with high probability an approximate 1-median of the network .",
    "the same sampling approach was also used to estimate the centrality values of all nodes  @xcite and to identify the top @xmath7 centralities  @xcite .",
    "when the distance distribution is heavy - tailed , however , the sample average is a very poor estimator of the average distance : the few very distant nodes that dominate the average distance are likely to be all excluded from the sample @xmath8 , resulting in a large expected error for almost all nodes .",
    "we present the first near - linear - time algorithm for estimating , with a small relative error , the classic closeness centralities of all nodes .",
    "our algorithm provides probabilistic guarantees that hold for all instances and for all nodes .",
    "computationally , our algorithm selects a small uniform sample  @xmath8 of @xmath7 nodes and performs single - source shortest paths computation from each sampled node .",
    "we provide a high - level description , illustrated in figure [ epsh : fig ] , of how we use this information to estimate centralities of all nodes .    from the single - source computations ,",
    "we know the distances from nodes in @xmath8 to all other nodes and therefore the exact value of @xmath9 for each @xmath10 , but we need to estimate the centrality of other nodes .",
    "as we mentioned , a natural way to use this information is _ sampling _ @xcite : estimate the centrality of a node @xmath2 using the sample average @xmath11 .",
    "as we argued , however , the expected relative error can be very large when the distribution of distances from the node @xmath2 to all other nodes is skewed",
    ".    a second basic approach , which we propose here , is _ pivoting _ , which builds on techniques from approximate shortest - paths algorithms @xcite .",
    "we define the _ pivot _ @xmath12 of a node @xmath2 as the node in the sample which is closest to @xmath2 .",
    "we can then estimate the centrality of @xmath2 by that of its pivot , @xmath13 , which we computed exactly . by the triangle inequality ,",
    "the value of  @xmath14 is within  @xmath15 of @xmath13 .",
    "a large error , however , can be realized even on natural instances : the centrality of the center node in a star graph would be estimated with an error of almost @xmath16 , using average distance of approximately 2 instead of 1 .",
    "if we use the _ pivoting upper bound _  @xmath17 as our estimator , we obtain an estimate that is about three times the value of the true average .",
    "we can show , however , that this is just about the worst case : on all instances and nodes @xmath2 , the pivoting upper bound estimate is , with high probability , not much less than @xmath14 or much more than three times the value , that is , the estimate is within a factor of @xmath18 of the actual value . since the argument is both simple and illuminating , we sketch it here . when the sample has size @xmath7 , it is likely that the distance between @xmath2 and its pivot @xmath19 is one of the  @xmath20 closest distances from @xmath2",
    "actually , with very high probability ,  @xmath21 is one of the @xmath22 closest distances to @xmath2 .",
    "since @xmath14 is the average value of a set of values such that  @xmath23 of them are at least as large as @xmath21 , we obtain that @xmath24 we next apply the triangle inequality to obtain @xmath25 finally , we combine   and   to obtain that our estimate  @xmath26 is not likely to be much larger than @xmath27 .    therefore , the pivoting estimator has a bounded error with high probability , regardless of the distribution of distances , a property we could not get with the sampling estimator .",
    "neither method , sampling or pivoting , however , is satisfactory to us , since we are interested in a _ small relative _ error , for _ all _ nodes , on all instances , and with ( probabilistic ) guarantees .",
    "our key algorithmic insight is to carefully combine the sampling and pivoting approaches . when estimating centrality for a node @xmath2 , we apply the pivoting estimate only to nodes @xmath5 that are `` far '' from @xmath2 , that is , nodes that have distance @xmath4 much larger than the distance to the pivot @xmath19 .",
    "the sampling approach is applied to the remaining `` closer '' nodes . by doing so",
    ", our hybrid approach obtains an estimate with a small relative error with high confidence , something that was not possible when using only one of the methods in isolation .",
    "moreover , the computation needed by our hybrid algorithm is essentially the same as with the basic approaches : @xmath7 single - source shortest paths computation for a small value of @xmath7 .",
    "our hybrid estimator is presented and analyzed in section  [ hybrid : sec ] .",
    "the estimator is applicable to points in a general metric space and is therefore presented in this context .",
    "an efficient algorithm which computes the hybrid centrality estimate for all nodes in an undirected graphs is presented in section  [ computecc : sec ] .",
    "the effectiveness of our hybrid estimate in practice depends on setting a threshold correctly between pivoting and sampling .",
    "our analysis sets a threshold with which we obtain guarantees with respect to worst - case instances , i.e. , for any network structure and distances distribution of a node . in our implementation",
    ", we experiment with different settings .",
    "we also propose a novel _ adaptive _ approach , which estimates the error for several ( or effectively all relevant ) choices of threshold values , on a node per node basis .",
    "the sweet spot estimate which has the smallest estimated error is then used .",
    "our error estimator for each threshold setting and our adaptive approach are detailed in section  [ adaptive : sec ] .    in applications , we are often interested in measuring centrality with respect to a particular topic or property which has a different presence at each node .",
    "nodes can also intrinsically be heterogeneous , with different activity or importance levels .",
    "these situations are modeled by an assignment of weights @xmath28 to nodes .",
    "accordingly , one can naturally define _ weighted _ classic closeness centrality of a node @xmath29 as @xmath30 in section [ weighted : sec ] , we present and analyze an extension of our algorithm designed for approximating weighted centralities . the approach is based on weighted sampling of nodes , which , for any weighting @xmath31 , ensures a good approximations ( small relative error ) of equation  .",
    "the handling of weighted nodes is supported with almost no cost to scalability or accuracy when compared to unweighted instances .    in section [",
    "directed : sec ] we consider directed networks .",
    "when the graph is strongly connected , meaning that all nodes can reach all other nodes , it is often natural to consider closeness centrality with respect to _ round - trip _ distances .",
    "the round - trip distance between two nodes is defined as the sum @xmath32 of the shortest - paths distances .",
    "we show that a small modification of our hybrid algorithm , which requires both forward and reverse single - source shortest - paths computations from each sampled node , approximates round - trip centralities for all nodes with a small relative error .",
    "this follows because our hybrid estimator and its analysis apply in any metric space , and round - trip distances are a metric .",
    "when the graph is not strongly connected , however , classic closeness centrality is not well defined : all nodes that have one or more unreachable nodes have centrality value of @xmath33 .",
    "we may also want to separately consider inbound or outbound centralities , based on outbound distances from a node or inbound distances to a node , since these can be very different on directed graphs . proposed modification of classic centrality to directed graphs are based on a combination of the average distance within the outbound or inbound reachability sets of a node , as well as on the cardinalities of these sets @xcite .",
    "we therefore consider scalable estimation of these quantities , proposing a sampling - based solution which provides good estimates when the distance distribution is not too skewed .",
    "section  [ sec : related ] briefly describes other relevant related work , including other important centrality measures .",
    "the results of our experimental evaluation are provided in section [ experiments : sec ] , demonstrating the scalability and accuracy of our algorithms on benchmark networks with up to tens of millions of nodes .",
    "we present our hybrid centrality estimator , which applies for a set @xmath34 of @xmath35 points in a metric space .",
    "we use parameters @xmath7 and @xmath36 , whose setting determines a tradeoff between computation and approximation quality .",
    "we sample @xmath7 points uniformly at random from @xmath34 to obtain a set @xmath8 .",
    "we then obtain the distances @xmath37 from each point @xmath38 to all points @xmath39 .",
    "the estimators we consider are applied to this set of @xmath40 computed distances .    specifically , we consider estimators @xmath41 $ ] for @xmath39 of the sum  @xmath42 .",
    "we then estimate the centrality of @xmath43 as  ( the inverse of ) @xmath44 \\gets \\hat{s}[j]/(n-1)$ ] .    for points",
    "@xmath45 , we can compute the exact value of @xmath46 , since the exact distances @xmath47 are available to all @xmath29 . for @xmath48",
    "we are interested in estimating @xmath46 .",
    "we define the _ pivot _ of @xmath43 ( closest node in the sample ) : @xmath49 and the distance @xmath50 to the pivot .    in the introduction we discussed three basic estimators : the _ sample average _ @xmath51 the _ pivot _ estimator , @xmath52 , and the _ pivoting upper bound _",
    "@xmath53 we argued that neither one can provide a small relative error with high probability .",
    "the hybrid estimate @xmath41 $ ] for a point @xmath54 is obtained as follows ( efficient computation is discussed in the next section ) .",
    "we first compute the pivot @xmath55 and its distance @xmath56 .",
    "we then partition the points @xmath57 to three parts @xmath58 , @xmath59 , and @xmath60 , where the placement of a node @xmath29 is determined according to its distance @xmath61 from the pivot @xmath55 .    * the points @xmath58 ( @xmath62 stands for `` low '' ) have distance at most  @xmath63 from @xmath55 .",
    "the sum of distances to these points is estimated using the sum of distances to the sampled points which are in @xmath58 . since these points are a uniform sample from @xmath58 , we compute the _ effective _ sampling probability  @xmath64 , and divide the sum by @xmath65 to obtain an unbiased estimate . * the set @xmath59 ( `` high in @xmath8 '' ) includes sampled points  @xmath38 that have distance greater than @xmath63 from the pivot  @xmath55 .",
    "the distances from @xmath2 to these points are accounted for exactly .",
    "* the set @xmath66 ( `` high '' ) are the points that are not sampled whose distance to the pivot @xmath55 is greater than  @xmath63 .",
    "the sum of distances to these points is estimated by the exact sum of their distances to @xmath55 .",
    "the estimate @xmath41 $ ] for @xmath46 is thus @xmath67= \\sum_{\\mathclap{i\\in h(j ) } } d_{c(j ) i } + \\sum_{\\mathclap{i \\in { \\mathit{hc}}(j ) } } d_{ji } + \\frac{|l(j)|}{|l(j)\\cap c| } \\sum_{\\mathrlap{i \\in l(j)\\cap c } } d_{ji}.\\ ] ]    since @xmath68 , the denominator satisfies @xmath69 and thus the estimator is well defined .",
    "it is easy to verify that the estimate @xmath41 $ ] for all points @xmath43 can be computed from the @xmath40 distances we collected .",
    "we now analyse the quality of the hybrid estimator and show that the estimate @xmath41 $ ] has a small relative error for any point @xmath43 :    [ hybrid : thm ] using @xmath70 , the hybrid estimator   has a normalized root mean square error ( nrmse ) of @xmath71 . using  @xmath72 ,",
    "when applying the estimator to all points in @xmath34 , we get a maximum relative error of @xmath71 with high probability .",
    "we consider the error we obtain by using @xmath41 $ ] instead of @xmath73 $ ] for a point @xmath54 .",
    "error can be accumulated on accounting for distances to @xmath60 or to @xmath58 .",
    "the first set , @xmath60 , includes all non - sample points that have distance greater than @xmath63 from @xmath55 . the accumulated error on the sum",
    "is bounded by @xmath74 for each point in @xmath60 .",
    "since the distance from @xmath43 to a point in @xmath75 is at least @xmath76 the relative error on all of @xmath60 is at most @xmath77 = 1 / ( 1/\\epsilon-1 ) = \\epsilon/(1-\\epsilon)$ ] .",
    "we now turn to @xmath58 , where we use a sampling estimator : we estimate the sum of distances to points in @xmath58 using the sum of distances to sample points that are in @xmath58 .",
    "the sample points constitute a random sample of @xmath58 , which includes each point in  @xmath58 with probability @xmath78 .",
    "we compute the variance of estimating @xmath79 using the estimate @xmath80 .",
    "consider the ratio of the variance to the square of the sum .",
    "the ratio is maximized when the set @xmath58 includes all points ( otherwise the contribution of @xmath60 increases the denominator but not the numerator ) .",
    "therefore , since we are upper bounding the error , we can assume that the set @xmath58 contains all points .",
    "the points in @xmath58 are of distance at most @xmath81 from  @xmath43 .",
    "we first consider the total contribution to the centrality of the set of points @xmath82 that are of distance smaller than @xmath56 from @xmath43 . since @xmath56 is the distance to the pivot , the expected number of such points is not more than @xmath83 .",
    "their expected total relative contribution to @xmath84 is at most their relative fraction , which in expectation is @xmath85 .",
    "moreover , for an integer @xmath86 , the probability of there being more than @xmath87 such points is the probability that all @xmath7 sampled points selected among the @xmath88 farthest points from @xmath43 , which is at most @xmath89 .",
    "so the contribution of points @xmath82 to centrality ( and to the variance ) is also well concentrated .",
    "we now consider the contribution to variance of points that have distance between @xmath56 and @xmath90 .",
    "for convenience we use @xmath91 and @xmath92 . repeating the same argument as before , since we are computing an upper bound we can assume that this set contains all points . given the sum of distances of these points , the `` worst case '' for variance is when all distances are at one of the extremes ; we thus further assume that the distance of each point is either @xmath93 or @xmath94 .",
    "the variance contribution of a point is @xmath95 times its distance squared .",
    "we now define  @xmath96 $ ] to be the fraction of points are of distance @xmath93 ; the remaining have distance @xmath94 .",
    "the sum of distances is @xmath97 and the variance is @xmath98 we now consider the maximum over choices of @xmath99 and @xmath100 of the ratio of the variance to the square of the mean , which is @xmath101 } \\frac{1}{k } \\frac{x + ( 1-x)s^2}{(x+(1-x)s)^2}.\\ ] ] this is maximized at @xmath102 .",
    "the maximum is @xmath103 .",
    "this means that the coefficient of variation ( cv ) is about @xmath104 .",
    "balancing the sampling cv with the pivoting relative error of @xmath36 we obtain @xmath105 .    in our implementation",
    ", we worked with parameter settings of  @xmath106 .",
    "this setting means that the relative error on the pivoting component is at most @xmath107 .",
    "we can typically expect it to be much smaller , however .",
    "first , because distances in @xmath60 can be much larger than @xmath63 .",
    "second , the estimates of different points are typically not `` one sided '' ( the estimate is one sided when the pivot happens to be on or close to the shortest path from @xmath43 to most other points ) , so errors can cancel out . for the sampling component ,",
    "the analysis was with respect to a worst - case distance distribution , where all values lie at the extremes of the range , but in practice we can expect an error of @xmath108 . moreover ,",
    "when the population variance of @xmath58 is small , we can expect a smaller relative error .",
    "in section [ adaptive : sec ] we propose adaptive error estimation , which for each point @xmath43 , uses the sampled distances @xmath37 to obtain a tighter estimate on the actual error .",
    "we now consider closeness centrality on undirected graphs , with a focus on efficient computation , both in terms of running time and the ( run - time ) storage we use , specifically , we would like to compute estimates @xmath109 $ ] of @xmath110 for all nodes @xmath111 .",
    "all the estimators we consider , the basic sampling and pivoting estimates and the hybrid estimate are applied to a set of ( at most ) @xmath112 sampled distances . to compute these distances",
    ", we can first sample a set @xmath8 of @xmath7 nodes uniformly at random and then run dijkstra s single - source shortest path algorithm from each node @xmath10 to compute the distances @xmath113 from @xmath5 to all other nodes .",
    "the computation of the estimates @xmath109 $ ] given these distances is linear .",
    "the issue with this approach is a run - time storage of @xmath114 .",
    "we first observe that both the basic sampling and the basic pivoting estimates can be computed using only @xmath115 run - time storage per node . with sampling , we accumulate , for each node  @xmath2 , the sum of distances from the nodes in @xmath8 .",
    "we initialize the sum to @xmath33 for all @xmath2 and then when running dijkstra from @xmath10 , we add @xmath113 to each scanned node @xmath2 .",
    "the additional run - time storage used here is the state of dijkstra and @xmath115 additional storage per node .",
    "with pivoting , we initialize @xmath116 for all nodes . when running dijkstra from @xmath5 , we accumulate the sum of distances as  @xmath117 .",
    "we also update @xmath118 when a node  @xmath2 is scanned .",
    "when @xmath119 is updated , we also update the pivot  @xmath120 . finally , for each node @xmath2 , we estimate @xmath117 by the precomputed @xmath121 .",
    "the pseudocode provided as algorithm [ bavelasu : alg ] computes the hybrid estimates for all nodes using @xmath115 additional storage per node .",
    "to do so with only @xmath115 storage , we use an additional run of dijkstra : for each node @xmath111 , we first compute its pivot @xmath19 and the distance @xmath122 .",
    "this can be done with a single run of dijkstra s algorithm having all sampled nodes as sources .",
    "we then run dijkstra s algorithm from each sampled node  @xmath10 . for the sampled nodes",
    "@xmath10 , the sum @xmath123 is computed exactly ; for such cases , we have @xmath124=s(u)$ ] . for the nodes",
    "@xmath125 we compute an estimate @xmath109 $ ] .",
    "the computation of the estimate is based on identifying the three components of the partition of @xmath126 into @xmath127 , which is determined according to distances from the pivot  @xmath19 .",
    "the pivot mapping computed in the additional run is used to determine this classification .",
    "the contributions to the sum estimates @xmath109 $ ] are computed during the single - source shortest paths computations from @xmath8 .",
    "in particular , the contribution to @xmath109 $ ] of sampled nodes @xmath128 are computed when we run dijkstra from @xmath5 .",
    "the contribution of @xmath129 is computed when we run dijkstra from the pivot @xmath19 of @xmath2 .",
    "when running dijkstra from a sampled node @xmath10 and visiting  @xmath2 , we need to determine whether @xmath5 is in @xmath130 or @xmath131 in order to compute its contribution .",
    "if @xmath132 , we increase @xmath109 $ ] by @xmath113 .",
    "if @xmath133 , we would like to increase @xmath109 $ ] by @xmath134 $ ] . at that point , however , @xmath135 $ ] , which depends on @xmath136 and @xmath137 , may not be available .",
    "we therefore add @xmath113 to @xmath138 $ ] , which tracks the sum of distances to nodes in @xmath139 .",
    "we also increment @xmath140 $ ] , which tracks the cardinality @xmath137 .",
    "when the @xmath7 dijkstra runs terminate , we can compute @xmath135 $ ] and increase  @xmath109 $ ] by @xmath138/p[v]$ ] .    deciding whether @xmath5 is in @xmath130 or @xmath131",
    "can sometimes be done only after the pivot @xmath19 was visited by the dijkstra run from @xmath5 .",
    "if @xmath141 then from the triangle inequality  @xmath142 and we can determine that @xmath132 .",
    "similarly , if @xmath143 we can determine that @xmath133 .",
    "otherwise , we can classify @xmath5 only after we visit @xmath19 and know the distance  @xmath144 . in this case , the accounting of @xmath5 to @xmath109 $ ] is postponed : we place the pair @xmath145 in list@xmath146 $ ] . each time a sampled node @xmath147 is visited by @xmath5",
    ", we process the list list@xmath148 $ ] and for each entry  @xmath149 we use @xmath150 to classify @xmath2 and accordingly increase @xmath109 $ ] or  @xmath138 $ ] .",
    "list@xmath148 $ ] is then deleted .",
    "the accounting for @xmath129 is done when running dijkstra from the pivot @xmath19 . during dijkstra from @xmath5 ,",
    "we record information on each node @xmath2 for which @xmath151 .",
    "the threshold values @xmath152 are recorded in increasing order in the thresh array , as nodes are visited .",
    "the set of nodes with pivot @xmath5 and a threshold value is recorded in the entry of nodes which corresponds to the threshold value .",
    "the sum of distances from @xmath5 to all nodes in  @xmath153 with distances that are between entries in the thresh array is computed in the corresponding entries of the bin array .",
    "after dijkstra s algorithm from @xmath5 is completed , we process these arrays in reverse , computing for each node @xmath2 such that @xmath151 the contribution of @xmath129 to the estimate @xmath109 $ ] .",
    "this algorithm performs @xmath154 runs of dijkstra s algorithm and uses running storage that is linear in the number of nodes  ( does not depend on @xmath7 ) .",
    "this means the algorithm has very little computation overhead over the basic estimators .",
    "network @xmath6 , integer @xmath155 , @xmath156 select uniformly at random @xmath7 nodes @xmath157 @xmath158\\gets \\arg\\min_{i=1,\\ldots , k } d_{c_i v}$ ] @xmath159 \\gets d_{v , c_{c[v]}}$ ] @xmath109\\gets 0 $ ] ; @xmath138 \\gets 0 $ ] ; @xmath140 \\gets 0 $ ] ; @xmath160 \\gets 0 $ ] ; @xmath161 \\gets 0 $ ] ; @xmath162 \\gets 0",
    "$ ] ; @xmath163 ; @xmath164 ; thresh@xmath165\\gets 0 $ ] run dijkstra from the sampled node @xmath166 @xmath167 @xmath168\\gets \\hat{s}[c_i]+d$ ] @xmath169 $ ] @xmath170 \\gets i$ ] ; @xmath171 \\gets d$ ] @xmath172 \\overset{+}{\\gets } z.d$ ] @xmath173 \\overset{+}{\\gets } ( z.d - d)^2 $ ] @xmath174 \\overset{+}{\\gets } z.d$ ] ; @xmath175 \\overset{+}{\\gets } 1 $ ] ; @xmath176 \\overset{+}{\\gets } z.d^2 $ ] delete @xmath177 $ ] @xmath178 \\overset{+}{\\gets } d$ ] ; @xmath179 \\overset{+}{\\gets } 1 $ ] @xmath160 \\overset{+}{\\gets } d^2 $ ] @xmath180 ; @xmath181 @xmath182 \\gets \\text{\\sc list}[c[u ] ] \\cup \\{z\\}$ ]    @xmath183\\gets \\text{\\sc nodes}[t ] \\cup \\ { u\\ } $ ] @xmath184 ; @xmath185 \\gets d/\\epsilon$ ] ; @xmath183\\gets \\ { u\\}$ ] ; @xmath186 \\gets 0 $ ] ; @xmath187\\gets0 $ ]    @xmath188 @xmath189 \\ , { \\overset{+}{\\gets}}\\ , d$ ] ; @xmath190 \\ , { \\overset{+}{\\gets}}\\ , 1 $ ]    @xmath191 ; @xmath192 @xmath193 $ ] @xmath194 $ ] @xmath195 \\gets \\text{\\sc tailsum}$ ] @xmath196 \\gets",
    "\\text{\\sc tailnum}$ ] @xmath197 @xmath198 -k + \\text{\\sc lcnum}[u]$ ] ; @xmath199 @xmath200}{\\text{\\sc lnum}}$ ] @xmath124 \\gets",
    "\\text{\\sc hsum}[u ] + \\text{\\sc hcsum}[u ] + \\text{\\sc lcsum}[u]/p$ ] @xmath201 \\gets \\frac{1}{\\text{\\sc",
    "lcnum}[u]}(\\frac{\\text{\\sc lcsumsq}[u]}{\\text{\\sc lcnum}[u]}- \\big(\\frac{\\text{\\sc lcsum}[u]}{\\text{\\sc lcnum}[u]}\\big)^2 ) \\text{\\sc lnum}[u ] + \\frac{\\text{\\sc hcsumsqerr}[u]}{\\text{\\sc hcnum}}\\text{\\sc hnum}[u]$ ]",
    "algorithm  [ bavelasu : alg ] also computes , for each node @xmath2 , an estimate on the error of our estimate @xmath109 $ ] .",
    "this estimate is _ adaptive _ , that is , it depends on the input .",
    "this is in contrast to the error bounds in theorem [ hybrid : thm ] , which are with respect to _ worst - case _ instances and , if used , will typically grossly overestimate the actual error and provide weak and pessimistic confidence bounds . we explain how these adaptive estimates are computed .",
    "we also propose _ adaptive error minimization _ as algorithm [ bavelaserr : alg ] : instead of working with a fixed value of @xmath36 , as in algorithm [ bavelasu : alg ] , the new algorithm chooses the estimate that has the smallest estimated error .      in algorithm",
    "[ bavelasu : alg ] , error estimates are computed separately for each of the two components : one from the pivoting on the `` distant '' nodes @xmath129 , and one from the sampling , on the `` closer '' nodes  @xmath130 .    the pivoting error is estimated by considering distant sampled nodes , that is , nodes in @xmath131 .",
    "these nodes are treated as a representative sample of @xmath129 . for these nodes",
    ", we take the average of the squared difference between the distance of the node from @xmath2 and its distance from the pivot @xmath19 : @xmath202 note that for nodes in @xmath131 , both these distances are available from the single - source shortest - paths computations we performed . finally , to obtain an estimate on the contribution of the pivoting component to the squared error of @xmath109 $ ] , we multiply by the magnitude @xmath203 of the set @xmath129 , which we know exactly . in cases when there are not enough or no samples ( when @xmath131 is empty ) , we instead compute the average squared difference over a `` suffix '' of the farthest nodes in @xmath8 .",
    "the sampling error applies to the remaining `` closer '' nodes  @xmath130 and depends on the distribution of distances in @xmath130 , that is , on the population variance of @xmath130 , and on the sample size from this group , which is @xmath204 .",
    "we first estimate the population variance of the set of distances from @xmath2 to the set of nodes @xmath130 .",
    "this is estimated using the sample variance of the uniform sample  @xmath204 , as @xmath205 we then divide the estimated population variance by the number of samples @xmath206 ( variable lcnum in the pseudocode ) to estimate the variance of the average of @xmath206 samples from the population . to estimate the variance contribution of the sampling component to the sum estimate @xmath109 $ ] , we multiply by  @xmath136  ( variable lnum in the pseudocode ) .",
    "the combined square error of  @xmath109 $ ] is estimated by summing these two components : @xmath207      in order to get the most mileage from the @xmath7 single source shortest paths computations we performed , we would like to adaptively select the best `` threshold '' between pivoting and sampling , rather than work with a fixed value .    for a node @xmath111 and a threshold value @xmath208 let @xmath209 the set @xmath210 contains all non - sampled nodes with distance from @xmath19 greater than @xmath208 , the set @xmath211 contains all sampled nodes with distance from @xmath19 greater than @xmath208 , and the set @xmath212 contains all nodes with distance from @xmath19 at most @xmath208 .",
    "we can then define an estimator with respect to a threshold @xmath208 , as in equation : @xmath213    in algorithm [ bavelasu : alg ] we used the threshold value @xmath214 for a node @xmath2 . here",
    "we choose @xmath215 adaptively so as to balance the estimated error of the first and third summands .",
    "one way to achieve this is to apply algorithm [ bavelasu : alg ] simultaneously with several choices of @xmath36 .",
    "then , for each node , we take the value with the smallest estimated error .",
    "we propose here algorithm [ bavelaserr : alg ] , which maintains @xmath216 state per node but looks for the threshold sweet spot while covering the full range between pure pivoting and pure sampling .",
    "algorithm [ bavelaserr : alg ] computes estimates and corresponding error estimates as in algorithm [ bavelasu : alg ] .",
    "the estimates , however , are computed for @xmath7 values of the threshold @xmath215 which correspond to the distances from @xmath19 to each of the other sampled nodes . from these @xmath7 estimates ,",
    "the algorithm selects the one which minimizes the estimated error .",
    "the reason for considering only these @xmath7 threshold values ( for each pivot ) is that they represent all the possible assignments of sampled nodes to @xmath130 or @xmath131 .",
    "finally , we note that the run - time storage we use depends linearly in the sets of threshold values and therefore it can be advantageous , when run - time storage is constrained , to reduce the size further .",
    "one way to do this is , for example , to only use values of @xmath215 which correspond to discretized distances .",
    "select a set @xmath157 of sampled nodes , uniformly at random ; for @xmath217 , use @xmath218 \\gets j$ ] .",
    "@xmath159\\gets \\infty$ ] @xmath219\\gets 0 $ ] @xmath220 ; @xmath221 @xmath222 @xmath223\\gets 0 $ ] @xmath224\\gets i$ ] run dijkstra s algorithm from @xmath166 @xmath225 @xmath226 $ ] @xmath227 ; @xmath228\\gets j$ ] ; @xmath229 \\gets d$ ] @xmath230 \\gets vvisited$ ] @xmath231 \\gets distsumvisited$ ] @xmath159 \\gets d$ ] , @xmath158\\gets i$ ] @xmath232 \\gets d$ ] @xmath233 ; @xmath234 after dijkstra ends : @xmath235 \\gets vvisited-\\text{\\sc tailnum}[j , cvisited]$ ] @xmath236 \\gets distsumvisited-\\text{\\sc tailsum}[j , cvisited]$ ] @xmath168 \\gets distsumvisited + \\sum_{j=1}^k \\delta[i , j]$ ] @xmath237\\gets 0 $ ] ; @xmath238 ; @xmath239 $ ] ; @xmath240 - \\delta[c(v),i])^2 $ ] @xmath109 \\gets \\hat{s}[c[v]]$ ] ; @xmath241 \\gets \\text{\\sc hcsumsqerr } \\cdot ( n-1-k)/k$ ] @xmath242 $ ] @xmath243 ^ 2 $ ] @xmath244 $ ] @xmath245 @xmath246 ; @xmath247 @xmath248 $ ] @xmath249 $ ] @xmath250 $ ] @xmath251-\\delta[c(v),\\pi[c(v),i]])^2 $ ] @xmath252 @xmath253 @xmath254 @xmath109 \\gets \\text{\\sc est}$ ] ; @xmath255 \\gets \\text{\\sc esterr}$ ]",
    "we now consider weighted classic closeness centrality with respect to node weights @xmath256 , as defined in equation  .",
    "we limit our attention to estimating the denominator @xmath257 since the numerator @xmath258 can be efficiently computed exactly for all nodes by computing the sum @xmath259 once and , for each node @xmath43 , subtracting the weight of the node @xmath43 itself from the total .",
    "we show how to modify algorithm [ bavelasu : alg ] to compute estimates for @xmath260 for all nodes .",
    "we will also argue that the proof of theorem  [ hybrid : thm ] goes through with minor modifications , that is , we obtain a small relative error with high probability .",
    "if the node weights are in @xmath261 , the modification is straightforward .",
    "we obtain our sample @xmath8 only from nodes @xmath29 with weight  @xmath262 and account only for these nodes in our estimate of @xmath263 .",
    "we now provide details on the modification needed to handle general weights @xmath31 .",
    "the first component is the node sampling .",
    "we apply a weighted sampling algorithm ; in particular , we use  @xmath264 stream sampling @xcite , which is a weighted version of reservoir sampling @xcite .",
    "we obtain a sample of exactly @xmath7 nodes so that the inclusion probability of each node is proportional to its weight .",
    "more precisely , @xmath264 computes a threshold value  @xmath265 ( which depends on @xmath7 and on the distribution of  @xmath31 values ) .",
    "a node @xmath2 is sampled with probability @xmath266 .",
    "these sampling probabilities are pps ( probability proportional to size ) , but with @xmath264 we obtain a sample of size exactly  @xmath7 ( whereas independent pps only guarantees an expected size of @xmath7 ) . for each sampled node",
    "we define its _ adjusted weight _",
    "@xmath267 , where @xmath265 is the varopt threshold .",
    "the weighted algorithm is very similar to algorithm  [ bavelasu : alg ] , but requires the modification stated as algorithm [ weighted : alg ] .",
    "the contributions to @xmath124 $ ] of nodes @xmath2 that are in @xmath268 $ ] ( accounted for in the tail sums computed in the @xmath269 array ) or in @xmath270 $ ] are multiplied by @xmath271 . for nodes in @xmath130 , we compute the inverse probability estimate with respect to the inclusion probability @xmath266 . we divide the contribution , which is @xmath272 , by the inclusion probability , obtaining @xmath273 .",
    "our error estimates can also be easily modified to work with weighted centralities . instead of the cardinality of each set",
    ", we use the total @xmath31 weight of the set ; instead of a sum of distances , we use the @xmath31-weighted sum .",
    "@xmath168 \\overset{+}{\\gets}\\beta(u)d_{c_i u}$ ] @xmath124 \\overset{+}{\\gets } \\beta(c_i ) d_{c_i u}$ ] @xmath124 \\overset{+}{\\gets } \\hat{\\beta}(c_i ) d_{c_i u}$ ] @xmath274 \\overset{+}{\\gets } d_{c_i u}^2 ( \\tau-\\beta(c_i))\\tau$ ]    @xmath189 \\overset{+}{\\gets } \\beta(u ) d_{c_i u}$ ] ; @xmath190 \\overset{+}{\\gets } \\beta(u)$ ]    the analysis of the approximation quality of @xmath275 in algorithm  [ weighted : alg ] carries over to the weighted algorithm .",
    "in fact , the skewness of  @xmath31 can only improve estimation quality : intuitively , the sample would contain in expectation more than @xmath276 fraction of the total  @xmath31 weight , since heavier items are more likely to be sampled .",
    "for a strongly connected directed graph , it is natural to consider the round - trip distances @xmath277 , and _ round - trip centrality _ values computed with respect to these round - trip distances .",
    "since round - trip distances are a metric , the hybrid estimator   applies , as does theorem [ hybrid : thm ] , which provides the strong guarantees on approximation quality .",
    "moreover , a simple modification of the algorithms we presented for undirected graphs applies to estimation of round - trip centralities in strongly connected directed graphs .",
    "we choose a uniform random sample of @xmath7 nodes , as we did in the undirected case .",
    "then , for each sampled node @xmath10 , we perform two single - source shortest paths computations , to compute the forward and a backward distances to all other nodes . then for each node @xmath278 , we compute the sum @xmath279 of these distances .",
    "we sort the nodes @xmath2 by increasing @xmath280 .",
    "we then use the sorted order and round - trip distances the same way we used the dijkstra order in the undirected version of the algorithm .      as mentioned in the introduction , for general (",
    "not necessarily strongly connected ) directed graphs , we may also be interested in separating _",
    "outbound _ or _ inbound centralities_. in particular , we are interested in the average distance from a particular node @xmath2 to all nodes it can reach ( outbound centrality ) or from nodes that can reach @xmath2 ( inbound centrality ) , as well as in the cardinalities of these sets .",
    "the size of the outbound reachability set of @xmath2 is @xmath281 = \\left| \\{u\\in v\\setminus\\{v\\ } \\mid v \\leadsto u \\ } \\right|,\\ ] ] where @xmath282 indicates that @xmath5 is reachable from @xmath2 .",
    "similarly , the size of the inbound reachability set of @xmath2 is @xmath283 = \\left| \\{u\\in v\\setminus\\{v\\ } \\mid u \\leadsto v \\ } \\right|.\\ ] ] accordingly , we define the total distance to the outbound reachability set of @xmath2 as @xmath284 = \\sum_{\\mathclap{u \\mid v \\leadsto u } } d_{vu},\\ ] ] and the total distance to the inbound reachability set of @xmath2 as @xmath285 = \\sum_{\\mathclap{u \\mid u \\leadsto v } } d_{uv}.\\ ] ] the outbound and inbound centralities are accordingly defined as the ( inverse of the ) ratios@xmath286/\\overrightarrow{r}[v]$ ] and @xmath287/\\overleftarrow{r}[v]$ ] .    unfortunately , the hybrid estimator , and even the special case of the pivoting estimator , do not work well with direction .",
    "this is because directed distances are not a metric ( they are not symmetric ) . intuitively , distances from the pivot ( closest sampled node ) can be much larger than distances from the node for which we estimate centrality .    sampling can be used with direction , but , when naively applied , will not provide relative error guarantees even when the distance distribution is not skewed .",
    "the reason is that it is not enough to use all distances from a small sample of nodes . for sampling to work ,",
    "we need to obtain a sample of a certain size from the reachability set of each node .",
    "some nodes , however , may reach few or no nodes from this sample .",
    "therefore the sample provides very little information ( or none at all ) for estimating the centrality of these nodes .",
    "@xmath163 , @xmath288\\gets \\textbf{false}$ ] ; @xmath289\\gets 0 $ ] ; @xmath290}\\gets 0 $ ] ; @xmath291\\gets 0 $ ] @xmath292 ; @xmath293\\gets \\textbf{true}$ ] perform pruned dijkstra from @xmath5 on @xmath294 prune dijkstra at @xmath2 @xmath291 \\overset{+}{\\gets } d_{vu}$ ] @xmath295 \\overset{+}{\\gets } 1 $ ] @xmath296\\gets t$ ] @xmath296\\gets t-1 $ ] @xmath297 \\gets 0 $ ] @xmath298 \\gets \\text{\\sc distsum}[v]/\\text{\\sc count}[v]$ ] @xmath299 \\gets",
    "\\text{\\sc count}[v]$ ] @xmath300 \\gets 1+\\frac{(k-1)(n-2)}{t[v]-1}$ ]    we extend the basic sampling approach to directed graphs using an algorithm of cohen  @xcite that efficiently computes for each node a uniform sample of size @xmath7 from its reachability set  ( for outbound centrality ) or from nodes that can reach it ( for inbound centrality ) .",
    "we modify the algorithm so that respective distances are computed as well .",
    "( we apply dijkstra s algorithm instead of generic graph searches . )",
    "this algorithm also computes @xmath40 distinct distances , but does so adaptively , so that they are not all from the same set of sources .",
    "the same algorithm also provides approximate cardinalities of these sets  @xcite .",
    "this means that , when the distance distribution is not too skewed , we can obtain good estimates of the average distance to reachable nodes ( or from nodes our node is reachable from ) .",
    "algorithm  [ directed : alg ] contains pseudocode for estimating outbound average distance ( @xmath301 ) and reachability ( @xmath302 ) for all nodes . by applying the same algorithm on @xmath6 instead of the reverse graph  @xmath294 ,",
    "we can obtain estimates for the inbound quantities .",
    "the algorithm computes for each node a uniform random sample of size @xmath7 from its reachability set .",
    "it does so by running dijkstra s algorithm from each node @xmath5 in random order , adding  @xmath5 to the sample of all nodes it reaches . since these searches are pruned at nodes whose samples already have @xmath7 nodes , no node is scanned more than @xmath7 times during the entire computation .",
    "the total cost is thus comparable to @xmath7 full ( unpruned ) dijkstra computations .",
    "this algorithm does not offer worst - case guarantees .",
    "however , on realistic instances , where centrality is in the order of the median distance , it performs well .",
    "@xmath289\\gets 0 $ ] ; @xmath303\\gets 0 $ ] ; @xmath291\\gets 0 $ ] @xmath304>0 \\}$ ] @xmath305 \\gets \\text{\\sc",
    "rand()}/\\beta[v]$ ] perform pruned dijkstra from @xmath5 on @xmath294 prune dijkstra at @xmath2 @xmath295 \\overset{+}{\\gets } 1 $ ] @xmath291 \\overset{+}{\\gets } \\beta[u ] d_{vu}$ ] @xmath306 \\overset{+}{\\gets}\\beta[u]$ ] @xmath296= r[u]$ ] @xmath299 \\gets 0 $ ] ; @xmath109 \\gets 0 $ ] @xmath299 \\gets \\text{\\sc bcount}[v]$ ] ; @xmath109 \\gets \\text{\\sc distsum}[v]$ ] @xmath307 \\gets \\frac{\\text{\\sc distsum}}{t[v]}$ ] ; @xmath299 \\gets \\frac{k-1}{t[v]}$ ]    the algorithm applies a bottom-@xmath7 variant  @xcite of the reachability estimation algorithm of cohen  @xcite and also computes distances .",
    "the cardinality estimator is unbiased with coefficient of variation  ( cv ) at most @xmath308  @xcite .",
    "the quality of the average distance estimates depends on the distribution of distances and we evaluate it experimentally .    [ cols= \" < , < , > , > , > , > , > , > , > , > , > , > , > \" , ]      we now consider centrality on arbitrary directed graphs",
    ". table  [ tab : directed ] gives the results obtained by algorithm  [ directed : alg ] . once again , we use  @xmath309 and evaluate the algorithm with 1000 random queries .",
    "the `` exact '' column shows the estimated time for computing all @xmath99 outbound centralities using dijkstra computations .",
    "we then show the average relative error ( over the 1000 random queries ) and the total running time to compute all @xmath99 centralities using algorithm  [ directed : alg ] .",
    "although this algorithm has no theoretical guarantees , its average relative error is consistently below 6% in practice .",
    "moreover , it is quite practical , taking less than three minutes even on a graph with almost 200 million edges .",
    "we presented a comprehensive solution to the problem of approximating , within a small relative error , the classic closeness centrality of all nodes in a network .",
    "we proposed the first near - linear - time algorithm with theoretical guarantees and provide a scalable implementation .",
    "our experimental analysis demonstrates the effectiveness of our solution .",
    "our basic design and analysis apply in any metric space : given the set of distances from a small random sample of the nodes to all other nodes , we can estimate , for each node , its average distance to all other nodes , with a small relative error .",
    "we therefore expect our estimators to have further applications .",
    "p.  boldi , m.  rosa , m.  santini , and s.  vigna .",
    "layered label propagation : a multiresolution coordinate - free ordering for compressing social networks . in _ proceedings of the 20th international conference on world wide web _ , pp . 587596 .",
    "2011 .",
    "m.  holtgrewe , p.  sanders , and c.  schulz .",
    "engineering a scalable high quality graph partitioner . in _ 24th international parallel and distributed processing symposium ( ipdps10 ) _ , pp .",
    "ieee computer society , 2010 .",
    "j.  leskovec , d.  huttenlocher , and j.  kleinberg .",
    "predicting positive and negative links in online social networks . in _ proceedings of the 19th international conference on world wide web _ , pp .",
    "acm , 2010 .",
    "j.  leskovec , j.  kleinberg , and c.  faloutsos .",
    "graphs over time : densification laws , shrinking diameters and possible explanations . in _ proceedings of the eleventh acm sigkdd international conference on knowledge discovery in data mining _ , pp",
    ". 177187 .",
    "acm , 2005 .",
    "a.  mislove , m.  marcon , k.  p. gummadi , p.  druschel , and b.  bhattacharjee .",
    "measurement and analysis of online social networks . in _ proceedings of the 7th acm sigcomm conference on internet measurement _ , pp .",
    "2942 . 2007",
    "j.  yang and j.  leskovec . defining and evaluating network communities based on ground - truth . in _ proceedings of the acm sigkdd workshop on mining data semantics _ , mds 12 , pp .",
    "3:13:8 , new york , ny , usa , 2012 ."
  ],
  "abstract_text": [
    "<S> closeness centrality , first considered by bavelas  ( 1948 ) , is an importance measure of a node in a network which is based on the distances from the node to all other nodes . </S>",
    "<S> the classic definition , proposed by bavelas  ( 1950 ) , beauchamp  ( 1965 ) , and sabidussi  ( 1966 ) , is  ( the inverse of ) the average distance to all other nodes .    </S>",
    "<S> we propose the first highly scalable ( near linear - time processing and linear space overhead ) algorithm for estimating , within a small relative error , the classic closeness centralities of all nodes in the graph . </S>",
    "<S> our algorithm applies to undirected graphs , as well as for centrality computed with respect to round - trip distances in directed graphs .    for directed graphs </S>",
    "<S> , we also propose an efficient algorithm that approximates generalizations of classic closeness centrality to outbound and inbound centralities . </S>",
    "<S> although it does not provide worst - case theoretical approximation guarantees , it is designed to perform well on real networks .    </S>",
    "<S> we perform extensive experiments on large networks , demonstrating high scalability and accuracy . </S>"
  ]
}