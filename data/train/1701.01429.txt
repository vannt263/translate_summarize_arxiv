{
  "article_text": [
    "this paper concerns the design of a reduced basis function approach to mitigate the impact of the `` curse of dimensionality '' which appears when we deal with multidimensional interpolation , in particular , when we price financial derivatives employing multivariable models , like garch , or whose price may depend on multiple assets which follow different stochastic processes .",
    "since the market prices change almost constantly , thousands of derivative prices have to be recomputed very fast , hence numerical techniques that allow fast evaluation of multidimensional models are of high interest .",
    "one way to compute numerical approximations of the value of multidimensional functions is to compute the function value in a prespecified set of nodes and perform polynomial interpolation .",
    "unfortunately , for a high number of dimensions , the memory requirements for storing the interpolant , and possibly its evaluation time , grow drastically , something commonly referred as the `` curse of dimensionality '' .",
    "a common approach to reduce this curse is to search the dimensions in where more interpolation nodes would reduce most the interpolation error .",
    "the method that we propose here approaches , someway , the problem from the other side .",
    "instead on focusing on the construction of the interpolation polynomial , what we propose is to obtain , from the interpolant , a new smaller polynomial which gives similar accuracy as the original one .",
    "the techniques that we are going to propose can be exported to other kinds of polynomials but , in order to fix ideas , we are going to work with chebyshev polynomials .",
    "the properties of chebyshev polynomials , @xcite , enable us to use time - competitive and accurate enough techniques for computing polynomial coefficients , evaluation and differentiation .    in first place",
    ", we will see how to build the interpolant and we will design a very efficient evaluation algorithm which allows to compute the polynomial value for different values of each of the parameters simultaneously , something that will be referred as _",
    "tensorial valuation_. afterwards , for a fixed interpolant , we will propose a reduced basis approach employing a hierarchical orthonormalization procedure along each one of the dimensions .",
    "this procedure rewrites the interpolant in function of a set of orthonormal function basis which are ordered hierarchically depending on the amount of information of the interpolant that they posses .",
    "afterwards , fixed a tolerance level for the error with respect to the interpolant , we retain the minimum number of functions in the basis such that this tolerance is fulfilled . furthermore , since the function basis are written in function of chebyshev polynomials , the evaluation algorithm previously designed can be adapted to the new approximation .",
    "the result is a polynomial which numerically approximates the multidimensional function requiring much less memory capacity and computational valuation time .",
    "the paper is organized as follows . in section [ rbfaovpi ] ,",
    "a fast tensorial chebyshev polynomial interpolation is developed .",
    "the precision is obtained increasing the number of interpolation nodes , which leads to the storage - cost problem ( `` curse of dimensionality '' ) .",
    "this curse is reduced in section [ rbfaovrba ] , where , focusing on retaining the valuation algorithms , a reduced basis approximation method is constructed upon the polynomial obtained in section [ rbfaovpi ] .",
    "the result will be a low storage cost polynomial which can be evaluated for several different parameter values at the same time . in section [ rbfaovnr ]",
    "we perform numerical experiments with the different techniques developed over a multidimensional model employed in pricing financial derivatives .",
    "all the algorithms presented in this work have been implemented in matlab vr2010a .",
    "all the numerical experiments have been realized in a personal computer with an intel(r ) core(tm ) i3 cpu , 540 @ 3.07ghz , memory ram of 4,00 gb and a 64-bits operative system .",
    "we propose a chebyshev polynomial interpolation procedure for multidimensional models . the interpolation will be done using chebyshev polynomials and nodes in the intervals where the parameters are defined .    [ ch1defchebypol ]",
    "let us define @xmath0 where @xmath1 .",
    "it is well known , @xcite , that this function is a polynomial of degree @xmath2 , called the chebyshev polynomial of degree n.    [ ch1defchebynodes ] let @xmath3 .",
    "the @xmath4 chebyshev nodes @xmath5 in interval @xmath6 $ ] correspond to the extrema of @xmath7 and they are given by : @xmath8 , \\quad k=0,1, ...",
    ",n.\\ ] ]    we also define the @xmath4 chebyshev nodes @xmath9 in interval @xmath10 $ ] , where @xmath11    here we present just the definitions that will be needed for the proposed method .",
    "the practical computation of the coefficients of the interpolant is postponed to subsection [ ch1cotip ] .",
    "[ ch1defpolinternvariable ] let @xmath12 and @xmath13 be a continuous function defined in @xmath14 , \\quad j=1,2, ... ,n$ ] .    for @xmath15 , we define the function @xmath16^n$ ] as @xmath17 where @xmath18 , \\\\ & j=1,2, ... ,n .",
    "\\end{aligned}\\ ] ]    for @xmath19 , we define @xmath20    for @xmath21 , let @xmath22 be the @xmath23 chebyshev nodes in @xmath24 $ ] and @xmath25 be the @xmath23 chebyshev nodes in @xmath26 $ ] .",
    "we use the notation @xmath27 and @xmath28 .",
    "let @xmath29 be the n - dimensional interpolant of function @xmath30 at the chebyshev nodes @xmath31 , i.e. the polynomial which satisfies @xmath32    polynomial @xmath29 will be given by @xmath33^{n},\\ ] ] where @xmath34    let @xmath35 $ ] and suppose that we need a numerical approximation for several different values in each of the variables .",
    "let @xmath36 be a set of values such that for @xmath37 , @xmath38 , \\",
    "1\\leq k \\leq q_j \\right\\}\\ ] ] where we note that the number of points in @xmath39 is @xmath40",
    ".    the numerical approximation will be computed with the polynomial @xmath29 , where the relation between @xmath41 and @xmath42 is given by formula ( [ ch1camcoordmult ] ) .",
    "the algorithms presented in subsection [ ch1tedip ] allow us to evaluate the interpolation polynomial in a set of points like @xmath39 very fast , something that from now on will be referred as _",
    "tensorial valuation_. this will be achieved through a suitable defined multidimensional array operation and the employment of efficient algorithms described in subsection [ ch1tedip ] .",
    "* one variable *    let @xmath43 be a continuous function defined in @xmath44 $ ] and suppose that we want to compute the chebyshev interpolant @xmath45.\\ ] ]    it must hold that @xmath46 where @xmath47 are the chebyshev nodes in @xmath48 $ ] and @xmath24 $ ] .",
    "there are several efficient algorithms that allow us to obtain the coefficients @xmath49 . for one variable",
    ", we employ the algorithm presented in @xcite .",
    "_ algorithm c1v : _",
    "construct @xmath50^t.\\ ] ]    \\2 .",
    "compute @xmath51    \\3 . @xmath52",
    "* several variables *    let @xmath13 and @xmath53 be as given in definition [ ch1defpolinternvariable ] and suppose that we want to construct the interpolant @xmath33^{n}.\\ ] ]    [ ch1defpermutation ] let @xmath54 be an array of dimension @xmath55 .",
    "we denote the vector @xmath56 where @xmath57 .",
    "let @xmath58 be an array of dimension @xmath59 .",
    "we define the permutation operator @xmath60 such that if : @xmath61 we have that @xmath62 and @xmath63    suppose that we have already computed the function value at the chebyshev nodes , i.e. , @xmath64 @xmath65 which are stored in an array @xmath66 such that @xmath67    the coefficients @xmath68 of the interpolant are obtained through the following algorithm .",
    "_ algorithm cnv : _",
    "@xmath69 .    \\2 .",
    "for i=1 to @xmath2    2.1 .",
    "@xmath70 .    2.2 .",
    "for @xmath71 to @xmath72 , for @xmath73 to @xmath74 , ... , for @xmath75 to @xmath76 @xmath77    2.3 .",
    "@xmath78 .",
    "@xmath79 .",
    "we remark that the fft routine in matlab admits multidimensional valuation so that , step 2.2 of the previous algorithm can be efficiently computed without using loops .",
    "[ ch1defmultidimmatrixproduct ]    let @xmath54 and @xmath58 be two arrays , @xmath80 and @xmath81 respectively , and such that @xmath82 .",
    "we define the tensorial array operation @xmath83 , as the array @xmath84 given by : @xmath85 where @xmath86 is the usual product of matrix times a vector and @xmath60 is the permutation operator introduced in definition [ ch1defpermutation ] .",
    "it is easy to check that @xmath87 .",
    "concerning the implementation in matlab of the tensorial array operation , @xmath88\\right),\\ ] ] where _ permute _ is a standard procedure implemented in matlab .",
    "the algorithm _ multiprod _ , implemented by paolo de leva and available in mathworks ( see @xcite ) , makes the required tensorial operation simultaneously in all variables in a very efficient way .",
    "suppose now that we have a polynomial @xmath89    we want to evaluate the polynomial in a finite set of points @xmath39 , which was defined in ( [ defconjtheta ] ) .    for computational reasons , we impose that @xmath90 by default , _ multiprod _ algorithm does not recognize arrays of @xmath91-dimension and collapses to @xmath92-dimension .",
    "since @xmath93 consists of _ multiprod _ and a _ permutation _ ,",
    "if @xmath94 , a wrong dimension will be permuted in the valuation algorithm .",
    "the evaluation algorithm has two steps .",
    "* 1 . evaluate the chebyshev polynomials : *    we use the recurrence property of chebyshev polynomials : @xmath95 that with the number @xmath96 of interpolation points involved in the option pricing problem works fairly well .    from the definition of @xmath39 ( see ( [ defconjtheta ] ) ) , the possible values of each variable are a finite number .",
    "we will employ the notation @xmath97 to denote the corresponding values in @xmath39 after the change of variables ( [ ch1camcoordmult ] ) . using the recurrence property , we compute @xmath98 and we store each result in a two dimensional array .    *",
    "2 . evaluate the rest of the polynomial .",
    "*    the evaluation of the polynomial @xmath29 for the whole set of points @xmath39 can be done at once using the tensorial array operation .",
    "the polynomial coefficients are stored in a @xmath99-dimensional array @xmath54 .",
    "@xmath100 and we compute @xmath101 ... \\right)\\otimes\\boldsymbol{t}(\\eta_n),\\ ] ] where the result will be an @xmath102-dimensional array which contains the evaluation of the interpolant in all the points of set @xmath39 .",
    "we remark that the previous definition must not be seen as a product with the usual properties .",
    "the order of the parenthesis has to be strictly followed in order to be consistent with the dimensions .",
    "_ polynomial differentiation : _    sometimes , we may also need to compute and approximation to the derivative of the multidimensional function .",
    "for example , if we want to find the values of the parameters of the model that approximate best to a given a set of data , in the sense of a least square minimization .    note that if @xmath103 $ ] and we have interpolated function @xmath43 @xmath104 where @xmath105,\\ ] ] we can approximate , if function @xmath106 is regular enough ( see @xcite ) , @xmath107 where ( see ( * ? ? ?",
    "* ( 2.4.22 ) ) ) for @xmath108 : @xmath109    this implies that the coefficients of the derivatives of the polynomials need to be computed each time or stored in memory .",
    "both options do not fit with the objective of this work .",
    "if the coefficients are computed each time they are needed , that increases the total time cost . on the other hand , there is a memory storage problem in the polynomial interpolation technique . to store the coefficients of the derivative means to almost double the memory requirements of the method .    for these reasons , we prefer to employ , if possible , a fast computing way to approximate the derivative , which does not require any more memory storage .",
    "we approximate the derivative by finite differences as follows @xmath110 where @xmath111 .",
    "as it has been seen , all the algorithms developed in subsections [ ch1cotip ] and [ ch1tedip ] are general enough .",
    "the only thing that we need to know is how many variables the interpolated function has and everything is straightforward .",
    "the objective of this section is to build a new polynomial which gives comparable accuracy as the interpolant built in the previous section , but which has less memory requirements .",
    "suppose we are given a high degree polynomial @xmath112 .",
    "the objective is to construct from it a smaller polynomial , as we will see in memory terms but not in degree , which globally values as well as the original one .",
    "the method we are going to develop could be exported to other kinds of polynomials , but since our evaluation algorithms are designed for the chebyshev ones , the construction is focused to take advantage of their properties . it is also general enough to be applied to any n - variables polynomial .",
    "suppose we have a polynomial @xmath113\\in\\mathbb{n}^n,\\ ] ] where @xmath96 denotes the degree in each one of the variables .",
    "our objective is , given a set of points @xmath114 and @xmath115 , to construct a polynomial @xmath116 from @xmath117 , such that @xmath118 where polynomial @xmath116 has the smallest size ( in memory terms ) compatible with ( [ ch1forhierproc ] ) .",
    "although another set of points could be chosen , since we are continuing the work of the previous section , i.e. @xmath119 , the interpolation polynomial of a certain function @xmath120 , the natural set of points @xmath121 will be the set of points used in the construction of the interpolation polynomial , i.e. , the chebyshev nodes @xmath122 .",
    "our approach will be to use a set of orthonormal function basis that will be hierarchically chosen .",
    "all polynomials that appear in the procedure we are going to construct must be written in function of chebyshev polynomials .",
    "hence , it is natural to employ the weighted norm associated with them and to exploit all the related properties which will simplify enormously all the calculus involved .",
    "[ ch1defprodesccheby ] given two functions @xmath123 and @xmath124 , where @xmath125^{n}$ ] , we define the weighted scalar product @xmath126 as : @xmath127 we denote by @xmath128 the norm induced by this scalar product .    the following result is well known @xcite",
    ".    [ ch1lemprodesccheby ] the chebyshev polynomials , @xmath129 , are orthogonal with respect to the scalar product @xmath126 .",
    "furthermore , if @xmath130 is a polynomial of degree less or equal to @xmath131 then @xmath132 where @xmath133 are the chebyshev nodes in [ -1,1 ] .",
    "( gauss - lobato - chebyshev cuadrature )    let @xmath134 and @xmath135 be two functions such that @xmath136^{n - j+1}$ ] .",
    "we define the function @xmath137 for simplicity in the notation , we denote @xmath138 .",
    "the algorithm of the hierarchical gram - schmidt procedure has @xmath139 steps if the polynomial @xmath140 has @xmath2 variables .",
    "* hierarchical orthonormalization procedure : *    let us consider @xmath140",
    ". for simplicity assume that @xmath141 $ ] .",
    "let us consider also a set of points @xmath142^{n}$ ] .",
    "* step 1 : *    let @xmath143 denote the @xmath144 chebyshev nodes in @xmath24 $ ] and define @xmath145 .",
    "it is easy to check that we can rewrite @xmath117 as : @xmath146 where @xmath147 is a @xmath148-degree polynomial such that for @xmath149 it holds that : @xmath150    for @xmath151 , we set @xmath152 and @xmath153 and compute : @xmath154    we define @xmath155 and @xmath156 .    for @xmath157 , we set @xmath158 and @xmath159 and compute @xmath160    we define @xmath161 and @xmath162 .",
    "we proceed iteratively , so that we eventually obtain a set of orthonormal polynomials @xmath163 such that @xmath164 where @xmath165 .",
    "we approximate now @xmath166 where @xmath167 is the first index such that @xmath168    let us observe that the first polynomials @xmath169 were those that , in the sense of ( [ ch1sentidorecorte ] ) , had more `` information '' about @xmath117 . indeed , usually with very few terms ( depending on the variable ) , a good approximation to the original polynomial can be achieved .",
    "furthermore , the amount of storage required is considerably reduced .",
    "* step 2 : *    each of the @xmath170 is a @xmath139 variable polynomial , and we can proceed the same way as we did in * step 1*.    _ for each of the @xmath171 _ , let @xmath172 be the @xmath173 chebyshev nodes in @xmath10 $ ] and @xmath174 where @xmath175 .    for @xmath176 , we set @xmath177 and @xmath178 and compute : @xmath179    we define @xmath180 and @xmath181 .",
    "now , for @xmath182 , we set @xmath183 and @xmath184 .",
    "again we compute @xmath185    we define @xmath186 and @xmath187 .",
    "we proceed iteratively , at the end we will obtain @xmath188 where @xmath189 .",
    "we will approximate @xmath140 by @xmath190 where @xmath191 is the first index such that @xmath192    we proceed iteratively until completing * step n-1 * where we stop .",
    "we will have arrived to a new polynomial that can be written @xmath193 @xmath194    note that @xmath195 is the number of function basis used in each variable .",
    "note also that , in general , the degree of @xmath116 is still @xmath196 .",
    "we remark that the last value of @xmath197 is @xmath198 because the last variable remains untouched .",
    "an improved result ( in memory terms ) can be obtained if the variables of polynomial @xmath117 are reordered before the reduced basis procedure and @xmath198 is the smallest among the @xmath199 .",
    "visually , we can check the big memory saving .",
    "the following picture shows an example of the first three steps of the algorithm if @xmath200 .",
    "[ ch1truncacion3etapa ]  mark the function basis discarded ( equivalent to memory savings ) in each step ( step 1-red , step 2-green and step 3-yellow ) of the hierarchical orthonormalization procedure.,title=\"fig:\",width=14,height=264 ]    the tree represents the orthonormal decomposition in each of the variables and the colored @xmath201 the function basis that are discarded in each step : 3 function basis ( step 1-red ) , 2 function basis ( step 2-green ) and 1 function basis ( step 3-yellow ) .",
    "the proportion of the tree that has been discarded represents approximately the memory savings with respect to the original polynomial @xmath202 obtained by our method .",
    "the last step is to adequate the algorithms for _ tensorial valuation _ to @xmath116 .",
    "polynomial @xmath116 is rewritten for tensorial evaluation as @xmath203 where we denote @xmath204 .",
    "each polynomial is written in function of the chebyshev polynomials : @xmath205 and @xmath116 is kept in memory storing the coefficient of the previous polynomials in @xmath2 different multidimensional arrays .",
    "suppose that we want to evaluate the polynomial in the finite set of points @xmath39 ( see ( [ defconjtheta ] ) ) .",
    "we employ the notation @xmath97 , whose values are obtained from set @xmath39 after the change of variable given by ( [ ch1camcoordmult ] ) .",
    "the evaluation of polynomials @xmath206 , ... , @xmath207 , @xmath208 when they are given by ( [ ch1polinqstore ] ) can be done efficiently using the first algorithm from subsection [ ch1tedip ] .    thus",
    ", suppose that we have evaluated them and stored the results in arrays : @xmath209    [ defch1esptensarrop ] let @xmath54 , @xmath58 be two arrays such that @xmath210 and @xmath211 .",
    "we define the special tensorial array operation @xmath212 as : @xmath213 where @xmath214 denotes the usual product of matrix times a vector .    from definition [ defch1esptensarrop ] , observe that in ( [ ch1esptensarrop ] ) we are using the usual matrix times vector multiplication .",
    "it is straightforward that @xmath215 .",
    "_ multiprod _ command is employed again to implement the special tensorial array operation .",
    "the _ tensorial valuation _ for the reduced basis polynomial can be written as : @xmath216 where again the order fixed by the parenthesis must be strictly followed in order to be consistent with the dimensions of the arrays .",
    "the result will be a @xmath102-dimensional array which contains the evaluation of the polynomial with all the possible combinations of the given values to each of the variables .",
    "although in the numerical experiments we will see that the results are quite good in the sense of reduction of memory requirements and computational time cost , we must point that the procedure presented could be improved .",
    "we remark that @xmath116 is not optimal in various senses .",
    "first of all , the hierarchical criteria to select the function basis is not necessarily optimal .",
    "for example , there might exist a combination of several function basis that give a less overall error than the ones chosen by our criterium .",
    "the selection criterium that we employ is fast because , when we have to order hierarchically the function basis in each step , we only need to reevaluate the function basis that have not already been ordered .",
    "another factor that could be improved is the criterium for truncation .",
    "we can orthonormally decompose the whole polynomial hierarchically @xmath217 and notice that we can independently truncate one branch of the tree or another .",
    "for example : @xmath218 or @xmath219    our procedure does not maximize memory savings over the whole polynomial . a method which maximizes the memory savings versus the deterioration of the error",
    "when we truncate function basis of one or other variable could be designed .",
    "we are now going to apply the techniques developed in sections [ rbfaovpi ] and [ rbfaovrba ] to a multidimensional model employed in option valuation .",
    "the outline of the section is as follows .",
    "first , we will make a brief introduction about financial options and pricing models .",
    "afterwards , we will build an interpolation polynomial of a particular pricing function and apply the reduced basis approach .",
    "performance analysis when we employ both numerical approximations will be performed .",
    "an european call option is a financial instrument that gives the buyer the right , but not the obligation , to buy a stock or asset , at a fixed future date ( maturity ) , and at a fixed price ( strike or exercise price ) .",
    "the seller will have the obligation , if the buyer exercises his / her right , to sell the stock at the exercise price .",
    "the stock is usually modelled as an stochastic process and empirical analysis show that the volatility of the process does not remain constant in time .",
    "arch models ( autoregressive conditional heterodastic ) introduced by engle in @xcite are a kind of stochastic processes in which recent past gives information about future variance .",
    "several arch models have been proposed along the years , trying to capture some of the empirically observed stock properties .",
    "the objective of this work is not to give a deep review of the arch literature , and we refer to @xcite , @xcite , @xcite , @xcite and references therein .",
    "nevertheless , we point that arch models are broadly used in option valuation .",
    "@xcite , @xcite , @xcite , @xcite and @xcite are just a few examples where option prices are obtained through arch models .    in the present work",
    ", we are going to apply the techniques that we have developed to price options with the ngarch(1,1 ) model . for pricing options , the dynamics of the stock in the risk - free measure @xmath220 of the ngarch(1,1 ) model @xcite ,",
    "@xcite are , @xmath221 where @xmath222 denotes the stock price , @xmath223 is the variance of the stock , @xmath224 is the risk - free rate , @xmath225 are the garch model parameters and @xmath226 is a normally distributed random variable with mean 0 and variance 1 .      for this model ,",
    "there is no known closed form solution and several numerical methods can be employed , being monte - carlo based methods ( @xcite , @xcite ) , lattice methods ( @xcite , @xcite ) , finite elements ( @xcite ) or spectral methods ( @xcite ) some of them .",
    "the principal drawback of garch models is their computational cost .",
    "it will depend on the numerical method employed , but all the ones mentioned ( monte - carlo , lattice , spectral ... ) require several seconds to compute option prices and several minutes to estimate parameter values .",
    "this can result in an unpractical procedure , since option prices change almost continuously .    in this work ,",
    "the numerical method employed for computing the option prices in the chebyshev nodes will be the spectral method developed in @xcite and will be referred as b - f method .",
    "this numerical method gives enough precision with few grid points , and the employment of fft techniques makes it a low - time consuming method .    fixed an enough precision for the b - f method , we assume for the rest of the work that the option price obtained with this method is the reference option price . the construction of the interpolating polynomial and the error analysis will be carried referencing to the values obtained with it .",
    "first , we fix the intervals in which the interpolant of the option price will be constructed . the ngarch(1,1 )",
    "model is linear in the relation @xmath228 , so strike is fixed at @xmath229 .",
    "the rest of variables are defined as follows : @xmath230 , \\ \\ \\ \\ \\ \\ \\   & & \\beta_0 \\in [ 0 , \\ 2\\cdotp10^{-6 } ] ,   \\\\ & h_0 \\in [ 0\\ldotp25\\cdotp10^{-4 } , \\ 2\\ldotp25\\cdotp10^{-4 } ] , \\ & & \\beta_1 \\in [ 0\\ldotp60 , \\ 0\\ldotp95 ] , \\\\ & s_0 \\in [ 0\\ldotp75 , \\",
    "1\\ldotp20 ] , \\ \\ & & \\beta_2 \\in [ 0\\ldotp02 , \\ 0\\ldotp25 ] , \\\\ & r \\in [ 0\\ldotp02 , \\",
    "0\\ldotp085 ] , \\ \\ & & ( \\lambda+\\theta ) \\in [ 0\\ldotp20 , \\ 2 ] .",
    "\\end{aligned}\\ ] ] where these intervals are chosen because they cover usual parameter values of the model observed in the literature .",
    "we are going to carry out a standard error analysis doubling the number of nodes @xmath232 .",
    "we remark that the number of interpolation points of variable @xmath233 is @xmath234 and the storage cost of the polynomials is @xmath235 .",
    "once fixed @xmath96 , we compute the chebyshev nodes @xmath236 with formula ( [ ch1forchbnodes ] ) .",
    "we compute the function values @xmath237 with b - f method and construct @xmath238 with the algorithms developed in subsection [ ch1cotip ] .",
    "independently , we have to build a control sample which allows us to measure how well the interpolation polynomial prices options in the domain @xmath239 .",
    "we have chosen a sample uniformly distributed on @xmath239 .",
    "this set of equally spaced points will be used to build a control sample .",
    "points that correspond to @xmath244 are not included because they always correspond to chebyshev nodes .",
    "@xmath245 will denote the set of option prices for all the possible combinations of values in sets @xmath246 , i.e. @xmath247 computed with b - f method .          .",
    "[ ch3tablei3i6i12 ] storage cost of @xmath251 , computational cost of evaluating @xmath255 with @xmath251 and the mean square error committed by the interpolation polynomial @xmath251 when evaluating the contract prices of @xmath255 .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     if we compare with table [ artconsisterrorin ] , we can check that the errors committed in the prediction are of the same magnitude with all the numerical methods .",
    "we also remark that the experiment that we have realized can be seen as a consistency analysis .",
    "although the parameters obtained with each of the polynomials are slightly different ( see table [ artconsist ] ) , the prices obtained in the prediction are fairly closed to the exact ones .",
    "therefore , the reduced basis method can be successfully applied to estimate model parameters / predict option prices .",
    "furthermore , while we need several minutes to estimate / predict with b - f or @xmath256 , we can do the same computations in a few seconds with polynomials @xmath257 , @xmath258 or @xmath259 .",
    "the experiment was repeated several times with different values for @xmath260 of the sv model . obviously , in the estimation we obtained different values for the parameters and the errors were slightly different , but the behaviour between the different errors ( @xmath261 vs b - f , @xmath257 vs @xmath261 , ... ) remained the same ."
  ],
  "abstract_text": [
    "<S> this paper concerns the design of a reduced basis function approach to mitigate the impact of the `` curse of dimensionality '' which appears when we deal with multidimensional interpolation , in particular , when we price financial derivatives employing multivariable models , like garch , or whose price may depend on multiple assets which follow different stochastic processes . in this kind of problems , </S>",
    "<S> multidimensional models appear and , very often , explicit formulas are not available . </S>",
    "<S> numerical approximations of the value of the functions for different parameter values must be computed and one approach is to perform polynomial interpolation . for a high number of dimensions , the memory requirements for storing the interpolant , and possibly the evaluation time , grows drastically . employing information from the interpolant , the technique that we propose builds a set of orthonormal polynomials that are employed to construct a new polynomial which gives similar accuracy as the interpolant but where the memory requirements ( and the evaluation time ) are greatly reduced .    </S>",
    "<S> * keywords : * derivative pricing , multidimensional interpolation , chebyshev polynomials , reduced basis functions . </S>"
  ]
}