{
  "article_text": [
    "end - to - end speech recognition with neural networks has been a goal for the machine learning and speech processing communities  @xcite . in the past ,",
    "the best speech recognition systems have used many complex modeling techniques for improving accuracy : for example the use of hand - crafted feature representations , speaker or environment adaptation with feature or affine transformations , and context - dependent ( cd ) phonetic models with decision tree clustering  @xcite to name a few . for automatic speech recognition ,",
    "the goal is to minimize the word error rate .",
    "therefore it is a natural choice to use words as units for acoustic modeling and estimate word probabilities .",
    "while some attempts had been made to model words directly , in particular for isolated word recognition with very limited vocabularies  @xcite , the dominant approach is to model clustered cd sub - word units instead .    on the other hand ,",
    "these clustered units were a necessity when data were limited , but may now be a sub - optimal choice .",
    "recently , the amount of user - uploaded captions for public youtube videos has grown dramatically .",
    "using powerful neural network models with large amounts of training data can allow us to directly model words and greatly simplify an automatic speech recognition system .",
    "it was previously found that the combination of a lstm rnn model s  @xcite memorization capacity and the ability of ctc loss  @xcite to learn an alignment between acoustic input and label sequences allows a neural network that can recognize whole words to be trained  @xcite .",
    "although training data sparsity was an issue , bi - directional lstm rnn models , with a large output vocabulary could be trained , i.e. up to 90,000 words , and obtained respectable accuracy without doing any speech decoding , however this was still far from the sub - word phone - based recognizer . in this paper",
    ", we show that these techniques coupled with a larger amount of acoustic training data enable us to build a neural speech recognizer ( nsr ) that can be trained end - to - end to recognize words directly without needing to decode .",
    "there have been many different approaches to end - to - end neural network models for speech recognition .",
    "@xcite use an encoder - decoder model of the conditional probability of the full word output sequence given the input sequence .",
    "however due to the limited capacity , they extend it with a dynamic attention scheme that makes the attention vector a function of the internal state of the encoder - decoder which is then added to the encoder rnn .",
    "they compare this with explicitly increasing the capacity of the rnn . in both cases , there is still a gap in performance with the conventional hybrid neural network phone - based hmm recognizer . instead of word outputs , letters can be generated directly in a grapheme - based neural network recognizer  @xcite ; while good results are obtained for a large vocabulary task , they are not quite comparable to the phone - based baseline system .",
    "here , we describe the techniques that we used for building the nsr : a single neural network model capable of accurate speech recognition with no search or decoding involved .",
    "the nsr model has a deep lstm rnn architecture built by stacking multiple lstm layers . since the bidirectional rnn models  @xcite have better accuracy and our application is offline speech recognition , we use two lstm layers at each depth  one operating in the forward and another operating in the backward direction in time over the input sequence .",
    "both these layers are connected to both previous forward and backward layers .",
    "we train the nsr model with the ctc loss criterion  @xcite which is a sequence alignment / labeling technique with a softmax output layer that has an additional unit for the _ blank _ label used to represent outputting no label at a given time .",
    "the output label probabilities from the network define a probability distribution over all possible labelings of input sequences including the blank labels .",
    "the network is trained to optimize the total probability of correct labelings for training data as estimated using the network outputs and forward - backward algorithm .",
    "the correct labelings for an input sequence are defined as the set of all possible labelings of the input with the target labels in the correct sequence order possibly with repetitions and with blank labels permitted between labels .",
    "the ctc loss can be efficiently and easily computed using finite state transducers ( fsts ) as described in  @xcite @xmath0 where @xmath1 is the input sequence of acoustic frames , @xmath2 is the input label sequence ( e.g. a sequence of words for the nsr model ) , @xmath3 is the lattice encoding all possible alignments of @xmath1 with @xmath2 which allows label repetitions possibly interleaved with _",
    "blank _ labels .",
    "the probability for correct labelings @xmath4 can be computed using the forward - backward algorithm .",
    "the gradient of the loss function w.r.t .",
    "input activations @xmath5 of the softmax output layer for a training example can be computed as follows : @xmath6 where @xmath7 is the softmax activation for a label @xmath8 at time step @xmath9 , and @xmath10 represents the lattice states aligned with label @xmath8 at time @xmath9 , @xmath11 is the forward variable representing the summed probability of all paths in the lattice @xmath3 starting in the initial state at time @xmath12 and ending in state @xmath10 at time @xmath9 , @xmath13 is the backward variable starting in state @xmath10 of the lattice at time @xmath9 and going to a final state .",
    "the nsr model has a final softmax predicting word posteriors with the number of outputs equaling the vocabulary size .",
    "modeling words directly can be problematic due to data sparsity , but we use a large amount of acoustic training data to alleviate it .",
    "we experiment with both written and spoken vocabulary . the vocabulary obtained from the training data transcripts",
    "is mapped to the spoken forms to reduce the data sparsity further and limit label ambiguity for the spoken vocabulary experiments . for written - to - spoken domain mapping a fst verbalization model is used  @xcite . for example ,  104 `` is converted to ' ' one hundred four  and  one oh four  . given all possible verbalizations for an entity , the one that aligns best with acoustic training data is chosen .    the nsr model is essentially an all - neural network speech recognizer that does not require any beam search type of decoding .",
    "the network takes as input mel - spaced log filterbank features .",
    "the word posterior probabilities output from the model can be simply used to get the recognized word sequence .",
    "since this word sequence is in spoken domain for the spoken vocabulary model , to get the written forms , we also create a simple lattice by enumerating the alternate words and blank label at each time step , and rescore this lattice with a written - domain word language model ( lm ) by fst composition after composing it with the verbalizer fst . for the written vocabulary model",
    ", we directly compose the lattice with the language model to assess the importance of language model rescoring for accuracy .",
    "we train the models in a distributed manner using asynchronous stochastic gradient descent ( asgd ) with a large number of machines  @xcite .",
    "we found the word acoustic models performed better when initialized using the parameters from hidden states of phone models  the output layer weights are randomly initialized and the weights in the initial networks are randomly initialized with a uniform ( -0.04 , 0.04 ) distribution . for training stability ,",
    "we clip the activations of memory cells to [ -50 , 50 ] , and the gradients to [ -1 , 1 ] range .",
    "we implemented an optimized native tensorflow cpu kernel ( multi_lstm_op ) for multi - layer lstm rnn forward pass and gradient calculations .",
    "the multi_lstm_op allows us to parallelize computations across lstm layers using pipelining and the resulting speed - up decreases the parameter staleness in asynchronous updates and improves accuracy .",
    "youtube is a video sharing website with over a billion users . to improve accessibility ,",
    "google has functionality to caption youtube videos using automatic speech recognition technology  @xcite .",
    "while generated caption quality can vary , and are generally no better than human created ones , they can be produced at scale . on the whole , users have found them helpful : google received a technology breakthrough award from the us national association of the deaf in 2015 for automatic captioning on youtube . for this work",
    ", we evaluate our models on videos sampled from google preferred channels on youtube  @xcite .",
    "the test set is comprised of 296 videos from 13 categories , with each video averaging 5 minutes in length .",
    "the total test set duration is roughly 25 hours and 250,000 words  @xcite . as the bulk of our training data",
    "is not supervised , an important question is how valuable this type of the data is for training acoustic models . in all our experiments ,",
    "we keep our language model constant and use a 5-gram model with 30 m n - grams over a vocabulary of 500,000 words .",
    "training large , accurate neural network models for speech recognition requires abundant data .",
    "while others have used read speech corpora  @xcite or unsupervised methods  @xcite to gather thousands or even tens of thousands of hours of labeled training data , we apply an approach first described in @xcite but now scaled up to build a training set of over 125,000 hours .",
    "this `` islands of confidence '' filtering , allows us to use user - uploaded captions for labels , by selecting only audio segments in a video where the user uploaded caption matches the transcript produced by an asr system constrained to be more likely to produce n - grams found in the uploaded caption . of the approximately 500,000 hours of video available with english captions , a quarter remained after filtering .",
    "the initial acoustic model was trained on 650 hours of supervised training data that comes from youtube , google videos , and broadcast news described in  @xcite .",
    "the acoustic model is a 3-state hmm with 6400 cd triphone states .",
    "this system gave us a 29.0% word error rate on the google preferred test set as shown in table  [ baseline125000 ] . by training with a sequence - level state - mbr criterion and using a two - pass adapted decoding setup , the best we could do was 24.0% with a 650 hour training set .",
    "contrast this with adding more semi - supervised training data : at 5000 hours , we reduced the error rate to 21.2% for the same model size .",
    "since we have more data available , and models that can capture longer temporal context , we show results for single - state cd phone units  @xcite ; this gives a 4% relative improvement over the 3-state triphone models .",
    "this type of model improves with the amount of training data and there is little difference between ce and ctc training criteria .",
    ".bidirectional - lstm acoustic models trained on data sets of varying sizes . [ cols=\"<,<,<,>,^\",options=\"header \" , ]",
    "we presented our neural speech recognizer : an end - to - end all - neural large vocabulary continuous speech recognizer that forgoes the use of a pronunciation lexicon and a decoder .",
    "mining 125,000 hours of training data using public captions allows us to train a large and powerful bi - directional lstm rnn model for speech recognition with a ctc loss that predicts words .",
    "the neural speech recognizer can model a written vocabulary of 100k words including numeric entities .",
    "unlike many end - to - end systems that compromise accuracy for system simplicity , our final system performs better than a well - trained , conventional context - dependent phone - based system achieving a 13.4% word error rate on a difficult youtube video transcription task ."
  ],
  "abstract_text": [
    "<S> we present results that show it is possible to build a competitive , greatly simplified , large vocabulary continuous speech recognition system with whole words as acoustic units . </S>",
    "<S> we model the output vocabulary of about 100,000 words directly using deep bi - directional lstm rnns with ctc loss . </S>",
    "<S> the model is trained on 125,000 hours of semi - supervised acoustic training data , which enables us to alleviate the data sparsity problem for word models . </S>",
    "<S> we show that the ctc word models work very well as an end - to - end all - neural speech recognition model without the use of traditional context - dependent sub - word phone units that require a pronunciation lexicon , and without any language model removing the need to decode . </S>",
    "<S> we demonstrate that the ctc word models perform better than a strong , more complex , state - of - the - art baseline with sub - word units . </S>"
  ]
}