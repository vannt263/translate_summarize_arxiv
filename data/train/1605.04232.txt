{
  "article_text": [
    "recent progress in deep learning algorithms for artificial intelligence has raised widespread ethical concerns  @xcite@xcite . it has been argued that human - level ai is nt automatically good for humanity .",
    "it might be presumptuous and overconfident to be sure that humans would be able to control superhuman - clever ais , that those ais would really care about humans , for example to allow us full access to mineral resources and agriculture fields of the planet . while there are numerous advantages of having clever ais in the short - term , the long - term danger of having too clever ais might outweigh , leading to net negative effect of ai progress on society .",
    "the most common argument against consideration of such long - term risks is their vagueness due to supposed very long time distance from us  @xcite .",
    "the main purpose of my article is to show that superhuman level ai is reasonably possible even within next 5 to 10 years .",
    "certainly it s very hard to predict future of science .",
    "however it s still a battle of arguments : whatever side brings more convincing arguments wins even if arguments from both sides are not very convincing leading to low margin of win ( like 60%/40% instead of 99%/1% ) . in this article",
    "the recent progress in deep learning is reviewed .",
    "this progress has some regularities which allow to extrapolate it into the future .",
    "such extrapolation speaks in favor of reasonably high probability of human level ai in next 5 to 10 years .",
    "so `` long - term risks '' are quite probably not that distant and vague .",
    "the most similar article has been written in 2013 by katja grace  @xcite .",
    "another review with more details on deep history of deep learning has been written in 2014 by jrgen schmidhuber  @xcite .",
    "the structure of the article is as follows . in chapters 2 - 5",
    ", i review some state - of - the - arts in natural language processing , computer vision , speech recognition and reinforcement learning . in chapters 6 - 8",
    ", i answer some questions which often arise in scientific discussions about how much time we have until human level ai : unsupervised and multimodal learning , the need for embodiment . in chapters 9 and 10 ,",
    "i review some relevant details of the progress in neuroscience . in chapter 11 ,",
    "some new and prospective approaches are reviewed .",
    "many of them might revolutionize modern deep learning but we do nt know yet which ones . in chapter 12 , the predictions of prominent scientists about when human level ai would be developed are discussed . in chapter 13 and 14 ,",
    "i make a brief review of ai safety research .",
    "let s begin with some state - of - the - arts in natural language generation .",
    "+    0.15 in    @xmath0    -0.1 in    why perplexity matters ?",
    "+ if neural net says smth inconsistent ( in * any * sense : logical , syntactic , pragmatic ) then it means that it gives too much probability to some inappropriate words i.e , it s perplexity is nt optimized yet .",
    "when hierarchical neural chat bots would achieve low enough perplexity , they would likely to write coherent stories , to answer intelligibly with common sense , to reason in a consistent and logical way etc .",
    "just as in @xcite we would be able to adjust a conversational model to imitate style and opinions of a distinct person .",
    "+ * what are reasonable predictions for perplexity improvements in the nearest future ? * + let s take two impressive recent works both submitted to arxiv in february 2016 .",
    "they are : `` contextual lstm models for large scale nlp tasks ''  @xcite and `` exploring the limits of language modeling ''  @xcite .",
    "+    0.15 in    @xmath1    -0.1 in    from table 2 , it s quite reasonable to predict that for @xcite hs=4096 might give pplx@xmath220 , hs=8192 might give pplx@xmath215 and ensemble of models with hs=8192 trained on 10b words might give * perplexity well below 10 .",
    "* nobody can tell now what kind of common sense reasoning would such a neural net have . + it s quite probable that even in this year the discussed decrease in perplexity might allow us to create neural chat bots that write reasonable stories , answer intelligibly with common sense , discuss things .",
    "all this would be just a mere consequence of low enough perplexity .",
    "+ shannon estimated lower and upper bounds of human perplexity to be 0.6 and 1.3 bits per character @xcite .",
    "strictly speaking , applying formula ( 17 ) in his work gives lower bound equal 0.648 which he rounded .",
    "given average word length of 4.5 symbols and including spaces ( as shannon included them in his game ) gives us * an estimate for lower bound on human - level word perplexity as 11.8 * = @xmath3 or better to say 10 plus something .",
    "this lower bound might be much less than real human perplexity @xcite .",
    "another source of improvement may come from solving some discrepancy in what kind of perplexity is optimized @xcite @xcite . however , both kl(p@xmath4q ) and kl(q@xmath4p ) have optimum when p = q .",
    "also , @xcite @xcite propose ways to partially solve that problem using adversarial learning .",
    "`` generating sequences from continuous space''@xcite demonstrates impressive advantages of adversarial paradigm .",
    "+    0.15 in    @xmath5=>@xmath6=>@xmath7=>@xmath8=>@xmath9=>@xmath10=>@xmath11=>@xmath12=>@xmath13    -0.1 in    human bleu score for chinese=@xmath14english translation on mt03 dataset is 35.76 @xcite . in recent article",
    "@xcite neural network gets 40.06 bleu on the same task and dataset .",
    "they took state - of - the - art @xcite `` groundhog '' network and replaced maximum likelihood estimation with their own mrt criterion , which increased bleu from 33.2 to 40.06 . here is a quote from abstract : `` unlike conventional maximum likelihood estimation , minimum risk training is capable of optimizing model parameters directly with respect to evaluation metrics '' .",
    "another impressive improvement comes from improving translation with monolingual data  @xcite .",
    "modern neural nets translate text @xmath151000 times faster than humans do  @xcite .",
    "study of foreign languages is now of less practical use because nmt improves faster than most humans can .",
    "0.15 in    @xmath16<@xmath17    -0.1 in    `` identity mappings in deep residual networks''@xcite reach 5.3% top-5 error in single one - crop model while human level is reported to be 5.1%@xcite . in `` deep residual networks ''",
    "@xcite one - crop single model gives 6.7% but ensemble of those models with inception gives 3.08%@xcite just to notice that @xmath18 .",
    "another big improvement comes from `` deep networks with stochastic depth''@xcite .",
    "there was reported @xmath19% error in human annotations to imagenet@xcite , so * real error on imagenet would soon become even below 2% . * human level is overcome not only in imagenet classification task ( see also an efficient implementation of 21841 classes imagenet classification@xcite ) but also on boundary detection@xcite .",
    "video classification task on sports-1 m dataset ( 487 classes , 1 m videos ) performance improved from 63.9%@xcite ( 2014 ) to 73.1%@xcite ( march 2015 ) .",
    "see also @xcite .",
    "+ cnns outperform humans also in terms of speed being * @xmath151000x faster than human * @xcite ( note that times there are given for batches ) or even @xmath1510 000x times faster after compression @xcite .",
    "video processing 24fps on alexnet demands just 82 gflops and googlenet demands 265 gflops .",
    "here s why .",
    "the best benchmark in@xcite gives 25ms feedforward and 71ms total time for 128 pictures batch on nvidia titan x ( 6144 gflops ) , so for 24fps video real - time feedforward processing we need 6144 gflops * 24/128 * 0.025 = 30 gflops . if we want to learn something using backprop than we need 6144 gflops * 24/128 * 0.071 = 82 gflops .",
    "same calculations for googlenet give 83 gflops and 265 gflops respectively .",
    "+ nets answer questions based on images @xcite . using similar method as in @xcite the equivalent human age of net  @xcite can be estimated as 6.2 years old ( submitted 4 mar 2016 ) , while @xcite was 5.45 years ( submitted 7 nov 2015 ) , @xcite was 4.45 years ( submitted 3 may 2015 ) .",
    "see appendix 1 for details of age estimates .",
    "also , nets can describe images with sentences , in some metrics even better than humans can  @xcite .",
    "beside video=@xmath14text @xcite@xcite@xcite , there are some experiments to realize text=@xmath14picture @xcite@xcite@xcite .",
    "0.15 in    @xmath20    -0.1 in    here , ai hasnt surpassed human level yet but it s clearly seen that we have all chances to see it in 2016 .",
    "the rate of improvement is very fast .",
    "for example , google reports it s word error rate dropped from 23% in 2013 to 8% in 2015 @xcite .",
    "alphago and atari agents choose one action from a finite set of possible actions .",
    "there are also articles on _ continuous _ reinforcement learning where action is a vector @xcite @xcite @xcite @xcite @xcite @xcite @xcite @xcite @xcite .",
    "+ alphago is a powerful agi in it s own very simple world which is just a board with stones and a bunch of simple rules . if we improve alphago with continuous reinforcement learning ( and if we manage to make it work well on hard real - world tasks ) than it would be real agi in a real world .",
    "also , we can begin with pretraining it on virtual videogames@xcite .",
    "videogames often contain even much more interesting challenges than real life gives for an average human .",
    "millions of available books and videos contain the concentrated life experience of millions of people .",
    "while alphago is a successful example of pairing modern rl with modern cnn , rl can be combined with neural chat bots and reasoners @xcite@xcite .",
    "yes , we do .",
    "dcgan @xcite generates reasonable pictures@xcite .",
    "language generation models which minimize perplexity are unsupervised and were improved very much recently ( table 1 ) .",
    "`` skip - thought vectors''@xcite generate vector representation for sentences allowing to train _ linear _ classifiers over those vectors and their cosine distances to solve many supervised problems at state - of - the - art level .",
    "generative nets gained deep new horizons at the edge of 2013 - 14 @xcite@xcite .",
    "recent work @xcite continues progress in `` computer vision as inverse graphics '' approach . +",
    "people with tetra - amelia syndrome  @xcite have neither hands nor legs from their birth and they still manage to get a good intelligence . for example , hirotada ototake  @xcite is a japanese sports writer famous for his bestseller memoirs .",
    "he also worked as a school teacher .",
    "nick vujicic  @xcite has written many books , graduated from griffith university with a bachelor of commerce degree , he often reads motivational lectures .",
    "prince randian  @xcite spoke hindi , english , french , and german .",
    "modern robots have better embodiment . + there are quick and good drones , there are quite impressive results in robotic grasping  @xcite .",
    "it s argued that embodiment might be done in virtual world of videogames@xcite . among 8 to 18-year - olds average amount of time spent with tv / computer / video games etc",
    "is 7.5 hours a day  @xcite .",
    "according to another study  @xcite uk adults spend an average an 8 hours and 40 minutes a day on media devices .",
    "humans develop commonsense intelligence very fast .",
    "when you are 2 years old you barely can do something that current ai ca nt .",
    "when you re 4 years old you have common sense , you can learn from texts and conversations .",
    "however , 2 years are just 100 weeks .",
    "if we exclude sleep we get @xmath1550 weeks .",
    "the 24fps video input of 50 weeks might be processed in 10 hours on pretrained alexnet using four titan x.",
    "yes , we do .",
    "multimodal learning is used in @xcite@xcite@xcite@xcite to improve video classification related tasks .",
    "unsupervised multimodal learning is used in grounding of textual phrases in images @xcite using attention - based mechanism so that different modalities supervise one another . in `` neural self - talk ''",
    "@xcite a neural network sees a picture , generates questions based on it and answers those questions itself .",
    "this is a simplified view of human cortex :    1   + [ clusterimg_group ]    roughly speaking  @xcite  @xcite , 15% of human brain is devoted to low - level vision tasks ( occipital lobe ) . +",
    "another 15% are devoted to image and action recognition ( somewhat more than a half of temporal lobe ) . +",
    "another 15% are devoted to objects detection and tracking ( parietal lobe ) . +",
    "another 15% are devoted to speech recognition and pronunciation ( bas 41,42,22,39,44 , parts of 6,4,21 ) . +",
    "another 10% are devoted to reinforcement learning ( orbitofrontal cortex and part of medial prefrontal cortex ) .",
    "together they represent about 70% of human brain .",
    "+ * modern neural networks work at about human level for these 70% of human brain . *",
    "+ for example , cnns make 1.5x less mistakes than humans at imagenet while acting about 1000x times faster than a human .",
    "+      from the neuroscience view , human cortex has the similar structure throughout all its surface .",
    "+ it s just  3 mm thick mash of neurons functioning on the same principles throughout all the cortex .",
    "+ there is likely no big difference between how prefrontal cortex works and how other parts of cortex work .",
    "+ there is likely no big difference in their speed of calculations , in complexity of their algorithms .",
    "+ it would be somewhat strange if modern deep neural networks ca nt solve remaining 30% in several years . + about 10% are low - level motorics ( bas 6,8 ) .",
    "robotic hands are not very dexterous .",
    "however people having no fingers from their birth have problems with fine motorics but still develop normal intelligence  @xcite , see also the section `` is embodiment critical ? '' . also , one of dlpfc functions is attention which is actively used now in lstms .",
    "+ the only part which still lacks near human - level performance are bas 9,10,46,45 which constitute together only 20% of human brain cortex .",
    "these areas are responsible for complex reasoning , complex tools usage , complex language . however ,",
    "`` a neural conversational model''@xcite , `` contextual lstm ... ''@xcite , `` playing atari with deep reinforcement learning''@xcite , `` mastering the game of go ... ''@xcite and numerous other already mentioned articles have recently begun to really attack this problem .",
    "+ there seem to be no particular reasons to expect these 30% to be much harder than other 70% .",
    "less than 3 years has gone from alexnet winning imagenet competition to surpassing human level .",
    "it s reasonable to expect the same @xmath2 3 years gap from `` a neural conversational model '' winning over cleverbot to human - level reasoning .",
    "after all , there are much more deep learning researchers now with much more knowledge and experience , there are much more companies interested in dl .",
    "detailed connectome deciphering has begun to really succeed recently with creation of multi - beam scanning electron microscopy @xcite ( jan 2015 ) , which allowed labs to get a grant for deciphering the detailed connectome of 1x1x1 mm@xmath21 of rat cortex  @xcite after the proof - of - principle deciphering of 40x40x50 mcm of brain cortex with 3x3x30 nm resolution was achieved  @xcite(july 2015 ) .",
    "+ works @xcite@xcite show that weight symmetry is not important for backpropagation in many tasks : one may propagate errors using a fixed matrix with everything still working .",
    "this counterintuitive conclusion is a crucial step towards theoretical understanding of how brain manages to work .",
    "see also `` towards biologically plausible deep learning '' by y.bengio et.al  @xcite . there arise some theoretic elements of how brain might perform credit assignment in deep hierarchies @xcite .",
    "+ recently , stdp objective function has been proposed@xcite .",
    "it s like an unsupervised objective function somewhat similar to what is used for example in word2vec . in",
    "@xcite authors surveyed a space of polynomial local learning rules ( learning rules in brain are supposed to be local ) and found that backpropagation outperforms them .",
    "there are also online learning approaches which require no backpropagation through time , f.e.@xcite . though they ca nt compete with conventional deep learning , brain perhaps can use something like that given fantastic number of it s neurons and synaptic connections .",
    "recently , a flow of articles about memory networks and neural turing machines made it possible to use arbitrarily large memories while preserving reasonable number of model parameters .",
    "hierarchical attentive memory@xcite ( feb2016 ) allowed memory access in o(log n ) complexity instead of usual o(n ) operations , where n is the size of the memory .",
    "reinforcement learning neural turing machines@xcite ( may2015 ) allowed memory access in o(1 ) .",
    "it s a significant step towards realizing systems like ibm watson on completely end - to - end differentiable neural networks and in order to improve allen ai challenge results from 60% to something way closer to 100% .",
    "one also can use bottlenecks in recurrent layers to get large memories using reasonable amount of parameters like in @xcite ) .",
    "+ neural programmer @xcite is a neural network augmented with a set of arithmetic and logic operations .",
    "it might be first steps towards end - to - end differentiable wolphram alpha realized on a neural network .",
    "the `` learn how to learn '' approach  @xcite  @xcite  @xcite has a great potential .",
    "+ recently , cheap svrg@xcite was proposed ( mar2016 ) .",
    "this line of work aims to use theoretically very much better converging gradient descent methods .",
    "+    1   + [ clusterimg_group ]    there are works which when succeed would allow training with immensely large hidden layers , see _ 6.discussion _ in `` unitary evolution rnn''@xcite , `` tensorizing neural networks''@xcite,``virtualizing dnns ... ''@xcite .",
    "+ `` net2net''@xcite and `` network morphism''@xcite allow automatically initialize new architecture nn using weights of old architecture nn to obtain instantly the performance of latter .",
    "collections of pretrained models are accessible online@xcite@xcite@xcite etc",
    ". it s birth of module based approach to neural networks .",
    "one just downloads pretrained modules for vision , speech recognition , speech generation , reasoning , robotics etc . and fine - tunes them on final task .",
    "it might also be api services like @xcite . +",
    "it s very reasonable to include new words in a sentence vector in a deep way .",
    "however modern lstms update cell vector when given new word in almost shallow way .",
    "this can be solved with the help of deep - transition rnns proposed in@xcite and further elaborated in @xcite .",
    "recent successes in applying batch normalization to recurrent layers@xcite(mar2016 ) and applying dropout to recurrent layers@xcite(mar2016 ) might allow to train deep - transition lstms even more effectively .",
    "it also would help hierarchical recurrent networks like@xcite@xcite@xcite@xcite@xcite .",
    "recently , several state - of - the - arts were beaten with an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output@xcite(mar2016 ) .",
    "ideas from residual nets would also improve performance .",
    "for example , stochastic depth neural networks@xcite(mar2016 ) allow to increase the depth of residual networks beyond 1200 layers while getting state - of - the - art results .",
    "+ memristors might accelerate neural networks training by several orders of magnitude and make it possible to use trillions of parameters@xcite .",
    "quantum computing promises even more@xcite@xcite .",
    "recently , the five qubit quantum computer was demonstrated @xcite ( mar2016 ) . + * deep learning is easy .",
    "deep learning is cheap .",
    "* best articles use no more than several dozens of gpu usually . for half billion dollars one can buy 64 000 nvidia m6000 gpus with 24 gb ram , @xmath157 teraflops each including processors etc . to make them work .",
    "for another half billion dollars one can prepare 2 000 highly professional researchers from those one million@xcite enrollments on andrew ng s machine learning course on coursera .",
    "so for a very feasible r&d budget for every big country or corporation , one gets two thousand professional ai researchers equipped with 32 best gpus each .",
    "it s kind of investment very reasonable to expect during next years of explosive ai technologies improvement . + the difference between year 2011 and year 2016 is enormous .",
    "the difference between 2016 and 2021 would be even much more enormous because we have now 1 - 2 orders of magnitude more researchers and companies deeply interested in dl .",
    "for example , when machines would start to translate almost comparably to human professionals another billions of dollars would flow into deep learning based nlp .",
    "the same holds true for most spheres , for example drug design  @xcite .",
    "* andrew ng * makes very skeptical predictions : `` maybe in hundreds of years , technology will advance to a point where there could be a chance of evil killer robots ''  @xcite .",
    "`` may be hundreds of years from now , may be thousands of years from now - i do nt know - may be there will be some ai that turn evil ''  @xcite . + * geoffrey hinton * makes a moderate prediction : `` i refuse to say anything beyond five years because i do nt think we can see much beyond five years ''  @xcite . +",
    "* shane legg , * deepmind cofounder , used to make predictions about agi at the end of each year , here is the last one@xcite : `` i give it a log - normal distribution with a mean of 2028 and a mode of 2025 , under the assumption that nothing crazy happens like a nuclear war .",
    "i d also like to add to this prediction that i expect to see an impressive proto - agi within the next 8 years '' .",
    "figure 2 shows the predicted log - normal distribution .",
    "+    1   + [ clusterimg_group ]    this prediction has been made at the end of 2011 .",
    "however , it s widely held belief that progress in ai was somewhat unpredictably great after 2011 so it s very reasonable to expect that predictions did nt become to be more pessimistic .",
    "in recent 5 years there were perhaps even much more than one revolution in ai field , so it seems quite reasonable that another 5 years can make another very big difference .",
    "+ for more predictions , see @xcite . here , i illustrated each viewpoint with the quotation of just one scientist .",
    "for each of these viewpoints , there are many ai researchers supporting it  @xcite .",
    "a survey of expert opinions on the future of ai in 2012/2013 @xcite might also be interesting .",
    "however , most of involved people there ( even among `` top 100 '' group ) are not very much involved with ongoing deep learning progress .",
    "nevertheless the predictions they make are also not too pessimistic . for this article",
    "it s quite enough that we are seriously unsure about probabilities of human level ai in next ten years .",
    "we can use deep learning to teach ai our human values given some dataset of ethical problems . given sufficiently big and broad dataset , we can get sufficiently friendly ai , at least more friendly than most humans can be .",
    "however this approach does nt solve all problems of ai safety  @xcite .",
    "apart from that we can use inverse reinforcement learning but still would need the benevolence / malevolence dataset of ethical problems to test ais .",
    "this approach has been initially formulated in `` the maverick nanny with a dopamine drip ''  @xcite but it s arguably better formulated here  @xcite  @xcite  @xcite .",
    "* 13.2.1 : * it s hard to create the dataset of ethical problems .",
    "there are many cultures , political parties and opinions .",
    "it s hard to create uncontroversial examples of how to behave when you are very powerful .",
    "however if we do nt have such examples in the training set , there is a good chance that ai would misbehave in such situations being unable to generalize well ( partially because may be human morality does nt scale well ) .",
    "* 13.2.2 : * the dopamine drip is ai subjecting people to some futuristic efficient and safe drugs making people very happy for eternity .",
    "also , ai might improve human brains to make people even more happy .",
    "while humans often say they dislike the dopamine drip idea , they still behave in all other ways like the only thing they want is a dopamine drip .",
    "we can validate that ai does nt insert dopamine drip electrodes in humans on some dataset of situations which are imaginable in the current world but this generalization might be not enough for future .",
    "* 13.2.3 : * more generally , if someone believes in a substantial probability that human morality is doomed to lead humans to the dopamine drip ( or some other bad outcome ) then why to accelerate that ?",
    "* 13.2.4 : * how to guarantee it does nt cheat us just answering our dataset questions like we want but having some deceiving hindsight in mind ? that s what most of teenagers do dealing with their parents .",
    "* 13.2.5 : * again , to the benevolence training dataset .",
    "shall we legalise marijuana ( and where to stop on that slippery slope to the dopamine drip ? )",
    "shall we allow suicide ?",
    "it might lead to very strong suffer of friends but is nt it like unalienable right ?",
    "those are very simple questions when compared to future moral challenges .",
    "still they are controversial and it s hard to imagine how such a dataset might be acknowledged worldwide .",
    "is it moral to have nuke arsenal dozen of times bigger than it s enough to defend your country ?",
    "is it moral to spend trillion dollars on military issues and dozen times less on medicine r&d ? is it moral when developed countries live in luxury while people in developing countries live in poverty ?",
    "is it moral to impose your political and economical system on other countries ?",
    "how can we answer such questions in our dataset when they are so hot holy - warred political issues ?    * 13.2.6 : * evil or stupid people can use any strong ai design to make evil strong ai . as for now",
    ", almost any technology has been adapted for military purposes .",
    "why would nt it happen for strong ai ?",
    "* 13.2.7 : * if some government turns evil it can be hard to overthrow it but it s still possible .",
    "if strong ai turns evil it might be absolutely impossible to stop it .",
    "the very idea to guarantee the benevolence of super - clever being dozens and even thousands years after it s creation seems to be overly bold and self - confident .",
    "* 13.2.8 : * we can challenge ai to rank several solutions ( already written by dataset creators ) for problems in benevolence / malevolence dataset .",
    "it would be easy and fast to check .",
    "however , we also want to check the ability of ai to propose its own solutions .",
    "this is much harder problem because it requires humans for evaluation .",
    "so in intermediate steps we get smth like the cev  @xcite of amazon mechanical turk .",
    "the final version would be checked not by amt but by a worldwide community including politicians , scientists etc .",
    "the very process of checking may last months if not years especially if considering inevitable hot discussions about controversial examples in the dataset .",
    "meantime , people who do nt care too much about ai safety would be able to launch their unsafe agis .",
    "* 13.2.9 : * suppose ai thinks that * smth * is the best for humans but ai knows that most of humans would not agree with that . is ai allowed to convince people that he is right ?",
    "if yes , ai certainly would easily convince everyone",
    ". it can be very hard to construct the benevolence / malevolence dataset so good that ai would nt convince people in such a situation but still would be able to give the full information about other problems where it s needed as a consultant .",
    "it s another illustration of the hardness of the task .",
    "everyone of dataset creators would disagree on how much it s allowed for ai to convince people . and",
    "if we do nt give ai the precise border in our dataset examples , then ai might be able to interpret undefined cases however it wants .",
    "what would people include in the benevolence / malevolence dataset ?",
    "what do most people want from ai ?",
    "i doubt highly that any substantial part of people wants ai to increase it s power too much or to explicitly create some kind of singularity for us .",
    "most probably people want some scientific research from ai : a cure for cancer , cold thermonuclear synthesis , ai safety , etc .",
    "the dataset would certainly include many examples teaching ai to consult with humans on any serious actions which ai might want to follow and tell humans instantly any insights which arise in ai .",
    "etc , etc ( miri / fhi have hundreds of good papers which might be used to create examples for such a dataset  @xcite ) .",
    "this alleviates or solves almost all the problems above because it does nt suppose ai to undertake hard takeoff .",
    "it might be argued that this worsens the problem of evil / psychopath / ignorant people launching unsafe ai first .",
    "to the best of my knowledge , most of miri research on this topic  @xcite@xcite comes to warning conclusions .",
    "almost whatever architecture you choose to build agi , it still would very likely destroy humanity in the near - term perspective .",
    "it s very important to notice that arguments are almost totally independent on architecture of agi .",
    "the best easy introduction is @xcite ( or more popular @xcite and @xcite ) though for serious understanding of miri arguments , `` superintelligence ''  @xcite and `` ai foom debate ''  @xcite is very much required .",
    "here i provide my own understanding and review of miri arguments which might differ from their official views but still is strongly based on them :      those corporations would win their markets which give their ais direct * unrestricted internet access * to advertise their products , to collect user s feedback , to build positive impression about company and negative one about competitors , to research users behavior etc .",
    "those companies would win which use their ais to invent quantum computing ( ai is already used by quantum computer scientists to choose experiments  @xcite  @xcite ) , which allow ai to improve it s own algorithms ( including quantum implementation ) , and even to invent thermonuclear synthesis , asteroid mining etc . all arguments in this paragraph are also true for countries and their defense departments .",
    "andrew karpathy : `` i consider chimp - level ai to be equally scary , because going from chimp to humans took nature only a blink of an eye on evolutionary time scales , and i suspect that might be the case in our own work as well . similarly , my feeling is that once we get to that level it will be easy to overshoot and get to superintelligence ''  @xcite .",
    "+ it takes years or decades for us people to teach our knowledge to other people while ai can almost instantly create full working copies of itself by simple copying .",
    "it has a unique memory .",
    "a student forgets all the stuff very quickly after exams .",
    "ai just saves its configuration just before exams and loads it again when it s needed to solve similar tasks .",
    "modern cnns do not only recognize images better than human but also make it several orders of magnitude faster .",
    "the same holds true for lstms in translation , natural language generation etc .",
    "given all aforementioned advantages , ai would quickly learn all literature and video courses on psychology , would chat with thousands of people simultaneously and so would become great psychologist . for same reasons",
    ", it would become great scientist , great poet , great businessman , great politician , etc .",
    "it would be able to easily manipulate people and lead peoples .",
    "if someone gives internet access to human level ai , it would be able to hack millions of computers and run it s own copies or subagents on them like  @xcite .",
    "after that it might earn billion dollars in internet .",
    "it would be able to hire anonymously thousands of people to make or buy dexterous robots , 3d - printers , biological labs and even a space rocket .",
    "ai would write great clever software to control it s robots .",
    "there is a simple yet effective baseline solution .",
    "ai might create a combination of lethal viruses and bacteria or some other weapon of mass destruction to be able to kill every human on earth .",
    "you ca nt guarantee efficient control over something that is much smarter than you .",
    "after all , several people almost took over the world , so why superhuman ai can not ? +      * what would do ai to us if it has full power on earth ? * + if it s indifferent to us then it will eliminate us as a side effect .",
    "just what does indifference mean when you are dealing with unbelievably powerful creature solving it s own problems using power of the local dyson sphere .",
    "however if it s not indifferent to us then everything might be even worse . if it likes us it might decide to insert electrodes in our brains giving us the utmost pleasure but no motivation of doing something .",
    "the very opposite would be true if it dislikes us or if it was partially hardcoded to like us but that hardcoding contained a hard - to - catch mistake .",
    "mistakes are almost inevitable when trying to partially hardcode something which is many orders of magnitude smarter and more powerful than you .",
    "+ there is nothing but vague intuition behind thought that superhuman - level ai would take care of us .",
    "there are no physical laws for it to have a special interest in people , in sharing oil / fields / etc resources with us .",
    "even if ai would care about us , it s very questionable if that care would be appropriate from our today moral standards .",
    "we certainly should nt take that for granted and to risk everything we have .",
    "the fate of humanity would depend on decisions of strong ai once and _",
    "forever_. + shane legg : `` eventually , i think human extinction will probably occur , and technology will likely play a part in this .",
    "but there s a big difference between this being within a year of something like human level ai , and within a million years . as for the former meaning",
    "... i do nt know .",
    "maybe 5% , maybe 50% .",
    "i do nt think anybody has a good estimate of this''@xcite .",
    "i would likely agree about 5-to-50% probability within a year after human - level ai creation but as for 10 years , it s more likely to be like 75-to-99% probability of human extinction .",
    "just see the quotation from openai project interview :  @xcite + elon musk : `` i think the best defense against the misuse of ai is to empower as many people as possible to have ai .",
    "if everyone has ai powers , then there s not any one person or a small set of individuals who can have ai superpower '' .",
    "+ sam altman : `` i expect that @xmath22openai@xmath23 will @xmath22create superintelligent ai@xmath23 , but it will just be open source and useable by everyone @xmath24 anything the group develops will be available to everyone ''",
    "i personally know several ai researchers who are deeply sure that agi would be good inevitably _ because it s intelligence _ ( which is nt true ; see  @xcite ) and that agi is our only hope to evade all other existential risks like bioterrorism .",
    "eliminating aging and death  @xcite might be another personal reason for some researchers to create human - level agi as soon as possible and to give it maximal abilities and permissions .",
    "ai enhanced computer viruses might penetrate almost everything which is computerized i.e. almost everything .",
    "drones might be programmed to kill thousands of lay people within seconds .",
    "as intelligence of ai systems improves practically all crimes could be automated . and what about super - clever advertising chatbots trying to impose their political opinion on you ?",
    "an ai correctly designed and implemented by the isil to enforce sharia law may be considered malevolent in the west , and vice verse .",
    "there are many good arguments arguing that human level ai would be constructed during next 5 to 10 years .",
    "i m aware of contrary opinions but as far as i know they are not explicitly based on thorough analysis of current trends in deep learning .",
    "they are based on them implicitly because they are based on experience of some prominent ai scientists who claim such opinions .",
    "however there are prominent ai scientists who claim that human level ai is quite probable in next 5 to 10 years .",
    "so it s a good idea to argue using numbers and not just expert opinions .",
    "i d like to see some paper similar to mine but coming to different conclusion .",
    "+ as for ai friendliness problem , there is a good approach to solve it based on benevolence / malevolence dataset .",
    "however , it s still just a raw solution which has hard problems in it s realization and with no strict proof of safety .",
    "it still might happen to have critical drawbacks under further investigation .",
    "moreover it s not guaranteed that it would be used in real ai development .",
    "to eliezer yudkowsky , nick bostrom , roman yampolsky , alexey turchin for their inspiring articles and books . also , the similar work has been made in 2013 by katja grace  @xcite .",
    "+ this work was supported by rfbr grant 16 - 07 - 01059",
    ".    10    eiezer yudkowsky .",
    "`` artificial intelligence as a positive and negative factor in global risk '' https://intelligence.org/files/aiposnegfactor.pdf    eliezer yudkowsky , robin hanson .",
    "`` the hanson - yudkowsky ai - foom debate '' http://intelligence.org/files/aifoomdebate.pdf    http://fusion.net/story/54583/the-case-against-killer-robots-from-a-guy-actually-building-ai/    katja grace .",
    "`` algorithmic progress in six domains '' https://intelligence.org/files/algorithmicprogress.pdf    juergen schmidhuber `` deep learning in neural networks : an overview '' http://arxiv.org/abs/1404.7828    shalini ghosh , oriol vinyals , brian strope , scott roy , tom dean , larry heck .",
    "`` contextual lstm ( clstm ) models for large scale nlp tasks '' http://arxiv.org/abs/1602.06291    rafal jozefowicz , oriol vinyals , mike schuster , noam shazeer , yonghui wu .",
    "`` exploring the limits of language modeling '' http://arxiv.org/abs/1602.02410    minh - thang luong , ilya sutskever , quoc v. le , oriol vinyals , wojciech zaremba . `` addressing the rare word problem in neural machine translation '' http://arxiv.org/abs/1410.8206    jiwei li , dan jurafsky .",
    "`` mutual information and diverse decoding improve neural machine translation '' http://arxiv.org/abs/1601.00372    hendra setiawan , zhongqiang huang , jacob devlin , thomas lamar , rabih zbib , richard schwartz , john makhoul .",
    "`` statistical machine translation features with multitask tensor networks '' http://arxiv.org/abs/1506.00698    shiqi shen , yong cheng , zhongjun he , wei he , hua wu , maosong sun , yang liu .",
    "`` minimum risk training for neural machine translation '' http://arxiv.org/abs/1512.02433    rico sennrich , barry haddow , alexandra birch . `` improving neural machine translation models with monolingual data '' http://arxiv.org/abs/1511.06709    junyoung chung , kyunghyun cho , yoshua bengio .",
    "`` a character - level decoder without explicit segmentation for neural machine translation '' http://arxiv.org/abs/1410.06147    zhongyuan zhu . `` evaluating neural machine translation in english - japanese task '' http://www.aclweb.org/anthology/w/w15/w15-5007.pdf    https://en.wikipedia.org/wiki/tetra-amelia_syndrome    https://kaiserfamilyfoundation.files.wordpress.com/2013/04/8010.pdf    http://googleresearch.blogspot.ru/2016/03/deep-learning-for-robots-learning-from.html    https://faculty.washington.edu/chudler/facts.html    kennedy et al . , cerebral cortex , 8:372 - 384 , 1998 .    https://www.cs.sfu.ca/~anoop/papers/pdf/jhu-ws03-report.pdf    dzmitry bahdanau , kyunghyun cho , yoshua bengio",
    ". `` neural machine translation by jointly learning to align and translate '' http://arxiv.org/abs/1409.0473    oriol vinyals , quoc le . ``",
    "a neural conversational model '' http://arxiv.org/abs/1506.05869    iulian v. serban , alessandro sordoni , yoshua bengio , aaron courville , joelle pineau .",
    "`` building end - to - end dialogue systems using generative hierarchical neural network models '' http://arxiv.org/abs/1507.04808    yangfeng ji , trevor cohn , lingpeng kong , chris dyer , jacob eisenstein .",
    "`` document context language models '' http://arxiv.org/abs/1511.03962    http://singularityhub.com/2015/12/20/inside-openai-will-transparency-protect-us-from-artificial-intelligence-run-amok/    http://en.wikipedia.org/wiki/storm_botnet    roman yampolsky .",
    "`` taxonomy of pathways to dangerous ai '' http://arxiv.org/abs/1511.03246    https://intelligence.org/our-research/    nick bostrom , `` superintelligence ''    http://futureoflife.org/data/documents/research_priorities.pdf    http://www.macleans.ca/society/science/the-meaning-of-alphago-the-ai-program-that-beat-a-go-champ/    http://blogs.nvidia.com/blog/2015/03/19/riding-the-ai-rocket-top-artificial-intelligence-researcher-says-robots-wont-kill-us-all/    http://www.businessinsider.com/facebooks-artificial-intelligence-research-director-robots-today-dumb-2015-9    http://cs.fit.edu/~mmahoney/dissertation/entropy1.html    http://competitions.codalab.org/competitions/3221#results",
    "http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/    c.e.shannon .",
    "`` prediction and entropy of written english '' http://languagelog.ldc.upenn.edu/myl/shannon1950.pdf    ferenc huszr .",
    "`` how ( not ) to train your generative model : scheduled sampling , likelihood , adversary ? ''",
    "lucas theis , aron van den oord , matthias bethge .",
    "`` a note on the evaluation of generative models '' http://arxiv.org/abs/1511.01844    kaiming he , xiangyu zhang , shaoqing ren , jian sun .",
    "`` identity mappings in deep residual networks '' http://arxiv.org/abs/1603.05027    http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/    http://myungjun-youn-demo.readthedocs.org/en/latest/tutorial/imagenet_full.html    iasonas kokkinos .",
    "`` pushing the boundaries of boundary detection using deep learning '' http://arxiv.org/abs/1511.07386    joe yue - hei ng , matthew hausknecht , sudheendra vijayanarasimhan , oriol vinyals , rajat monga , george toderici .",
    "`` beyond short snippets : deep networks for video classification '' http://arxiv.org/abs/1503.08909    https://github.com/soumith/convnet-benchmarks    song han , xingyu liu , huizi mao , jing pu , ardavan pedram , mark a. horowitz , william j. dally .",
    "`` eie : efficient inference engine on compressed deep neural network '' http://arxiv.org/abs/1602.01528    kaiming he , xiangyu zhang , shaoqing ren , jian sun .",
    "`` deep residual learning for image recognition '' http://arxiv.org/abs/1512.03385    awni hannun , carl case , jared casper , bryan catanzaro , greg diamos , erich elsen , ryan prenger , sanjeev satheesh , shubho sengupta , adam coates , andrew y. ng .",
    "`` deep speech : scaling up end - to - end speech recognition '' http://arxiv.org/abs/1412.5567    dario amodei et.al ( 34 authors ) .",
    "`` deep speech 2 : end - to - end speech recognition in english and mandarin '' http://arxiv.org/abs/1512.02595    s. m. ali eslami , nicolas heess , theophane weber , yuval tassa , koray kavukcuoglu , geoffrey e. hinton .",
    "`` attend , infer , repeat : fast scene understanding with generative models '' http://arxiv.org/abs/1603.08575    alec radford , luke metz , soumith chintala . `` unsupervised representation learning with deep convolutional generative adversarial networks '' http://arxiv.org/abs/1511.06434    https://www.facebook.com/yann.lecun/posts/10153269667222143    caiming xiong , stephen merity , richard socher .",
    "`` dynamic memory networks for visual and textual question answering '' http://arxiv.org/abs/1603.01417    stanislaw antol , aishwarya agrawal , jiasen lu , margaret mitchell , dhruv batra , c. lawrence zitnick , devi parikh .",
    "`` vqa : visual question answering '' http://arxiv.org/abs/1505.00468    zichao yang , xiaodong he , jianfeng gao , li deng , alex smola .",
    "`` stacked attention networks for image question answering '' http://arxiv.org/abs/1511.02274    anna rohrbach , marcus rohrbach , ronghang hu , trevor darrell , bernt schiele . ``",
    "grounding of textual phrases in images by reconstruction '' http://arxiv.org/abs/1511.03745    jacob devlin , rabih zbib , zhongqiang huang , thomas lamar , richard schwartz , and john makhoul .",
    "`` fast and robust neural network joint models for statistical machine translation '' http://acl2014.org/acl2014/p14-1/pdf/p14-1129.pdf    http://bbc.com/news/technology-28677674    https://en.wikipedia.org/wiki/ai_winter    .",
    "vincent c. mller , nick bostrom .",
    "`` future progress in artificial intelligence : a survey of expert opinion '' https://intelligence.org/files/algorithmicprogress.pdf    http://www.vetta.org/2011/12/goodbye-2011-hello-2012/    christian szegedy , sergey ioffe , vincent vanhoucke .",
    "`` inception - v4 , inception - resnet and the impact of residual connections on learning '' http://arxiv.org/abs/1602.07261    http://venturebeat.com/2015/05/28/google-says-its-speech-recognition-technology-now-has-only-an-8-word-error-rate/    http://lesswrong.com/lw/691/qa_with_shane_legg_on_risks_from_ai/    shixiang gu , timothy lillicrap , ilya sutskever , sergey levine .",
    "`` continuous deep q - learning with model - based acceleration '' http://arxiv.org/abs/1603.00748    john schulman , philipp moritz , sergey levine , michael jordan , pieter abbeel .",
    "`` high - dimensional continuous control using generalized advantage estimation '' http://arxiv.org/abs/1506.02438    timothy p. lillicrap , jonathan j. hunt , alexander pritzel , nicolas heess , tom erez , yuval tassa , david silver , daan wierstra . `` continuous control with deep reinforcement learning '' http://arxiv.org/abs/1509.02971    chelsea finn , sergey levine , pieter abbeel .",
    "`` guided cost learning : deep inverse optimal control via policy optimization '' http://arxiv.org/abs/1603.00448    matthew hausknecht , peter stone .",
    "`` deep reinforcement learning in parameterized action space '' http://arxiv.org/abs/1511.04143    nicolas heess , greg wayne , david silver , timothy lillicrap , yuval tassa , tom erez .",
    "`` learning continuous control policies by stochastic value gradients '' http://arxiv.org/abs/1510.09142    nicolas heess , jonathan j hunt , timothy p lillicrap , david silver . `` memory - based control with recurrent neural networks '' http://arxiv.org/abs/1512.04455    david balduzzi , muhammad ghifary .",
    "`` compatible value gradients for reinforcement learning of continuous deep policies '' http://arxiv.org/abs/1509.03005    gabriel dulac - arnold , richard evans , peter sunehag , ben coppin .",
    "`` reinforcement learning in large discrete action spaces '' http://arxiv.org/abs/1512.07679    diederik p kingma , max welling .",
    "`` auto - encoding variational bayes '' http://arxiv.org/abs/1312.6114    danilo jimenez rezende , shakir mohamed , daan wierstra .",
    "`` stochastic backpropagation and approximate inference in deep generative models '' http://arxiv.org/abs/1401.4082    https://en.wikipedia.org/wiki/hirotada_ototake    https://en.wikipedia.org/wiki/nick_vujicic    https://en.wikipedia.org/wiki/prince_randian    a.l . eberle , s. mikula , r. schalek , j. lichtman , m.l .",
    "knothe tate , d. zeidler .",
    "high - resolution , high - throughput imaging with a multibeam scanning electron microscope http://onlinelibrary.wiley.com/doi/10.1111/jmi.12224/pdf    narayanan kasthuri , kenneth jeffrey hayworth , daniel raimund berger , ... , carey eldin priebe , hanspeter pfister , jeff william lichtman . `` saturated reconstruction of a volume of neocortex '' https://www.mcb.harvard.edu/mcb_files/media/editor_uploads/2015/07/piis0092867415008247.pdf    http://news.harvard.edu/gazette/story/2016/01/28m-challenge-figure-out-why-brains-are-so-good-at-learning/    yoshua bengio , thomas mesnard , asja fischer , saizheng zhang , yuhuai wu .",
    "`` stdp as presynaptic activity times rate of change of postsynaptic activity '' http://arxiv.org/abs/1509.05936    qianli liao , joel z. leibo , tomaso poggio .",
    "`` how important is weight symmetry in backpropagation ? ''",
    "timothy p. lillicrap , daniel cownden , douglas b. tweed , colin j. akerman .",
    "`` random feedback weights support learning in deep neural networks '' http://arxiv.org/abs/1411.0247    yoshua bengio , asja fischer . ``",
    "early inference in energy - based models approximates back - propagation '' http://arxiv.org/abs/1510.02777    pierre baldi , peter sadowski .",
    "`` the ebb and flow of deep learning : a theory of local learning '' http://arxiv.org/abs/1506.06472    yoshua bengio , dong - hyun lee , jorg bornschein , zhouhan lin .",
    "`` towards biologically plausible deep learning '' http://arxiv.org/abs/1502.04156    yann ollivier , corentin tallec , guillaume charpiat .",
    "`` training recurrent networks online without backtracking '' http://arxiv.org/abs/1507.07680    http://togelius.blogspot.ru/2016/01/why-video-games-are-essential-for.html    ji he , jianshu chen , xiaodong he , jianfeng gao , lihong li , li deng , mari ostendorf .",
    "`` deep reinforcement learning with an action space defined by natural language '' http://arxiv.org/abs/1511.04636    karthik narasimhan , tejas kulkarni , regina barzilay .",
    "`` language understanding for text - based games using deep reinforcement learning '' http://arxiv.org/abs/1506.08941    marcin andrychowicz , karol kurach .",
    "`` learning efficient algorithms with hierarchical attentive memory '' http://arxiv.org/abs/1602.03218    wojciech zaremba , ilya sutskever . `` reinforcement learning neural turing machines - revised '' http://arxiv.org/abs/1505.00521    yezhou yang , yi li , cornelia fermuller , yiannis aloimonos .",
    "`` neural self talk : image understanding via continuous questioning and answering '' http://arxiv.org/abs/1512.03460    martin arjovsky , amar shah , yoshua bengio .",
    "`` unitary evolution recurrent neural networks '' http://arxiv.org/abs/1511.06464    samuel r. bowman , luke vilnis , oriol vinyals , andrew m. dai , rafal jozefowicz , samy bengio .",
    "`` generating sentences from a continuous space '' http://arxiv.org/abs/1511.06349    kaisheng yao , geoffrey zweig , baolin peng .",
    "`` attention with intention for a neural network conversation model '' http://arxiv.org/abs/1510.08565    alexander novikov , dmitry podoprikhin , anton osokin , dmitry vetrov .",
    "`` tensorizing neural networks '' http://arxiv.org/abs/1509.06569    ryan kiros , yukun zhu , ruslan salakhutdinov , richard s. zemel , antonio torralba , raquel urtasun , sanja fidler .",
    "`` skip - thought vectors '' http://arxiv.org/abs/1506.06726    arvind neelakantan , quoc v. le , ilya sutskever .",
    "`` neural programmer : inducing latent programs with gradient descent '' http://arxiv.org/abs/1511.04834    vatsal shah , megasthenis asteris , anastasios kyrillidis , sujay sanghavi .",
    "`` trading - off variance and complexity in stochastic gradient descent '' http://arxiv.org/abs/1603.06861    http://www.bloomberg.com/news/articles/2015-12-08/why-2015-was-a-breakthrough-year-in-artificial-intelligence    http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html    http://blog.samaltman.com/machine-intelligence-part-1    http://livestream.com/oxuni/stracheylecturedrdemishassabis , 07:56 .",
    "tayfun gokmen , yurii vlasov .",
    "`` acceleration of deep neural network training with resistive cross - point devices '' http://arxiv.org/abs/1603.07341    saizheng zhang , yuhuai wu , tong che , zhouhan lin , roland memisevic , ruslan salakhutdinov , yoshua bengio .",
    "`` architectural complexity measures of recurrent neural networks '' http://arxiv.org/abs/1602.08210    alex graves .",
    "`` adaptive computation time for recurrent neural networks '' http://arxiv.org/abs/1603.08983    tim cooijmans , nicolas ballas , csar laurent , aaron courville .",
    "`` recurrent batch normalization '' http://arxiv.org/abs/1603.09025    stanislau semeniuta , aliaksei severyn , erhardt barth .",
    "`` recurrent dropout without memory loss '' http://arxiv.org/abs/1603.05118    gao huang , yu sun , zhuang liu , daniel sedra , kilian weinberger .",
    "`` deep networks with stochastic depth '' http://arxiv.org/abs/1603.09382    https://github.com/bvlc/caffe/wiki/model-zoo    tianqi chen , ian goodfellow , jonathon shlens .",
    "`` net2net : accelerating learning via knowledge transfer '' http://arxiv.org/abs/1511.05641    tao wei , changhu wang , yong rui , chang wen chen .",
    "`` network morphism '' http://arxiv.org/abs/1603.01670    minsoo rhu , natalia gimelshein , jason clemons , arslan zulfiqar , stephen w. keckler .",
    "`` virtualizing deep neural networks for memory - efficient neural network design '' http://arxiv.org/abs/1602.08124    arvind neelakantan , luke vilnis , quoc v. le , ilya sutskever , lukasz kaiser , karol kurach , james martens .",
    "`` adding gradient noise improves learning for very deep networks '' http://arxiv.org/abs/1511.06807    http://myungjun-youn-demo.readthedocs.org/en/latest/pretrained.html    http://www.vlfeat.org/matconvnet/pretrained/    http://www.technologyreview.com/news/544421/googles-quantum-dream-machine/    razvan pascanu , caglar gulcehre , kyunghyun cho , yoshua bengio .",
    "`` how to construct deep recurrent neural networks ''",
    "seth lloyd , masoud mohseni , patrick rebentrost .",
    "`` quantum algorithms for supervised and unsupervised machine learning '' http://arxiv.org/abs/1307.0411    pingbo pan , zhongwen xu , yi yang , fei wu , yueting zhuang . `` hierarchical recurrent neural encoder for video representation with application to captioning '' http://arxiv.org/abs/1511.03476    haonan yu , jiang wang , zhiheng huang , yi yang , wei xu .",
    "`` video paragraph captioning using hierarchical recurrent neural networks '' http://arxiv.org/abs/1510.07712    alessandro sordoni , yoshua bengio , hossein vahabi , christina lioma , jakob g. simonsen , jian - yun nie . `` a hierarchical recurrent encoder - decoder for generative context - aware query suggestion '' http://arxiv.org/abs/1507.02221    https://twitter.com/andrewyng/status/693182932530262016    https://github.com/samim23/neuraltalkanimator    xinchen yan , jimei yang , kihyuk sohn , honglak lee .",
    "`` attribute2image : conditional image generation from visual attributes '' http://arxiv.org/abs/1512.00570    elman mansimov , emilio parisotto , jimmy lei ba , ruslan salakhutdinov .",
    "`` generating images from captions with attention '' http://arxiv.org/abs/1511.02793    gaurav pandey , ambedkar dukkipati .",
    "`` variational methods for conditional multimodal learning : generating human faces from attributes '' http://arxiv.org/abs/1603.01801    zuxuan wu , yu - gang jiang , xi wang , hao ye , xiangyang xue , jun wang . `` fusing multi - stream deep networks for video classification '' http://arxiv.org/abs/1509.06086    junhua mao , wei xu , yi yang , jiang wang , zhiheng huang , alan yuille . `` deep captioning with multimodal recurrent neural networks ( m - rnn ) '' http://arxiv.org/abs/1412.6632    samira ebrahimi kahou , xavier bouthillier , pascal lamblin , caglar gulcehre , vincent michalski , kishore konda , sbastien jean , pierre froumenty , yann dauphin , nicolas boulanger - lewandowski , raul chandias ferrari , mehdi mirza , david warde - farley , aaron courville , pascal vincent , roland memisevic , christopher pal , yoshua bengio .",
    "`` emonets : multimodal deep learning approaches for emotion recognition in video '' http://arxiv.org/abs/1503.01800    seungwhan moon , suyoun kim , haohan wang .",
    "`` multimodal transfer deep learning with applications in audio - visual recognition '' http://arxiv.org/abs/1412.3121    http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html    volodymyr mnih , koray kavukcuoglu , david silver , alex graves , ioannis antonoglou , daan wierstra , martin riedmiller .",
    "`` playing atari with deep reinforcement learning '' http://arxiv.org/abs/1312.5602    http://www.rense.com/general81/dw.htm    https://www.facebook.com/yann.lecun/posts/10153442884182143    daniel jiwoong i m , chris dongjoo kim , hui jiang , roland memisevic .",
    "`` generating images with recurrent adversarial networks '' http://arxiv.org/abs/1602.05110    http://www.cnet.com/news/quantum-science-is-so-weird-that-ai-is-choosing-the-experiments/    leon a. gatys , alexander s. ecker , matthias bethge . `` a neural algorithm of artistic style '' http://arxiv.org/abs/1508.06576    https://backchannel.com/how-elon-musk-and-y-combinator-plan-to-stop-computers-from-taking-over-17e0e27dd02a    http://www.nickbostrom.com/superintelligentwill.pdf    aubrey de grey , michael rae `` ending aging : the rejuvenation breakthroughs that could reverse human aging in our lifetime ''    http://www.businessinsider.com/sam-altman-y-combinator-talks-mega-bubble-nuclear-power-and-more-2015-6    http://slatestarcodex.com/2015/05/22/ai-researchers-on-ai-risk/",
    "moritz august , xiaotong ni .",
    "`` using recurrent neural networks to optimize dynamical decoupling for quantum memory '' http://arxiv.org/abs/1604.00279    s. debnath , n. m. linke , c. figgatt , k. a. landsman , k. wright , c. monroe .",
    "`` demonstration of a programmable quantum computer module '' http://arxiv.org/abs/1603.04512    https://www.microsoft.com/cognitive-services/en-us/apis    stanislaw jastrzebski , damian le@xmath25niak , wojciech marian czarnecki .",
    "`` learning to smile(s ) '' http://arxiv.org/abs/1602.06289    jie fu , hongyin luo , jiashi feng , kian hsiang low , tat - seng chua .",
    "`` drmad : distilling reverse - mode automatic differentiation for optimizing hyperparameters of deep neural networks '' arxiv.org/abs/1601.00917    ilya loshchilov , frank hutter .",
    "`` online batch selection for faster training of neural networks '' http://arxiv.org/abs/1511.06343    emmanuel bengio , pierre - luc bacon , joelle pineau , doina precup .",
    "`` conditional computation in neural networks for faster models '' http://arxiv.org/abs/1511.06297    richard loosemore .",
    "`` the maverick nanny with a dopamine drip : debunking fallacies in the theory of ai motivation '' http://richardloosemore.com/docs/2014a_mavericknanny_rpwl.pdf    richard loosemore .",
    "`` defining `` benevolence '' in the context of safe ai '' http://ieet.org/index.php/ieet/more/loosemore20141210    http://www.facebook.com/groups/467062423469736/permalink/532404873602157/    http://www.facebook.com/groups/467062423469736/permalink/573838239458820/    http://www.infoq.com/articles/interview-schmidhuber-deep-learning    eliezer yudkowsky",
    ". `` coherent extrapolated volition '' https://intelligence.org/files/cev.pdf",
    "in @xcite , the age of their model was estimated as 4.45 years .",
    "it answered 54.06% questions correctly .",
    "they investigate for many questions the youngest age group that could answer it .",
    "the results are : age 3 - 4 , 15.3% ; age 5 - 8 , 39.7% ; age 9 - 12 , 28.4% ; age 13 - 17 , 11.2% ; age 18 + , 5.5% .",
    "the sum equals 100% however 18 + humans answer 83.3% correctly .",
    "we could estimate that 8 year model must answer understandable for age 8 15.3%+39.7%=55% questions with 83.3% accuracy and other 45% with accuracy equal to baseline model .",
    "if they select the most popular answer for each question type , they get 36.18% .",
    "it s reasonable to estimate the age of that baseline model as 0 years because it s not real knowledge but it depends on this distinct dataset statistics . however , there are two baseline models in article , with 36.18% and 40.61% accuracy , both are quite simple . which one to choose ?",
    "let s calculate .",
    "let s define accuracy of 8 year old model as y , accuracy of 4 year old model as x , baseline accuracy as t. we have following equations : + 4 + 4*(54.06-x)/(y - x)=4.45 + 55 * 0.833 + 45*t = y + 15.3 * 0.833 + 84.7*t = x + with solution : t = 46.86% , x = 52.4% , y = 66.9% .",
    "+ so we estimate age of model with 57.6% accuracy as 4 + 4*(57.6 - 52.4)/(66.9 - 52.4)=5.45 years . + we estimate age of model with 60.4% accuracy as 4 + 4*(60.4 - 52.4)/(66.9 - 52.4)=6.2 years .",
    "+      it s interesting to note that popular `` one billion word benchmark '' corpus contains _ shuffled _ sentences so nlm trained on that corpus could nt be hierarchical and understand the flow of thoughts .",
    "that s why it s so much interesting to see generated samples from a recent contextual lstm article where the authors get 27 perplexity for wikipedia dump .",
    "unfortunately article does nt contain such samples .",
    "`` a neural conversational model''@xcite contains samples from nn with pplx=17 on opensubtitles but this dataset is somewhat worse than wikipedia in terms of consistency and logic of thought flow .",
    "+      yann lecun writes about richard s. sutton s comment on alphago@xcite : + _ rich says : `` alphago is missing one key thing : the ability to learn how the world works  such as an understanding of the laws of physics , and the consequences of one s actions . ''",
    "+ i totally agree .",
    "rich has long advocated that the ability to predict is an essential component of intelligence .",
    "predictive ( unsupervised ) learning is one of the things some of us see as the next obstacle to better ai .",
    "we are actively working on this . _",
    "+ however , in it s own simple world , alphago is able to predict future , is able `` to learn how the world works  such as an understanding of the laws of physics , and the consequences of one s actions '' .",
    "predictive learning becomes better very fast .",
    "there were two major ai winters in 1974@xmath2680 and 1987@xmath2693 .",
    "@xcite + during first ai winter in 1974 - 1980 , a typical desktop computer achieved @xmath2 1 mflops  @xcite .",
    "the most powerful supercomputer had @xmath15100 mflops and 8 mb main memory ( only 80 cray-1 were sold )  @xcite .",
    "+          it s clearly seen even from these numbers that the situation during those winters was totally different . even if we have ai `` winter '' now it would be like + 20@xmath27c , which is better than + 100@xmath27c ai summer .",
    "+      my own prediction for human - level agi is normal distribution with `` mean = end of 2017 , sigma = 1 year '' if there would be no major restrictions on ai research etc , though i really want somebody to give me a dozen of excellent arguments why i m wrong .",
    "by far , i havent heard any reasonable well - structured arguments proving human extinction due to agi in next 10 years to be extremely ( say less than 10% with 90% confidence ) improbable .",
    "arguments like `` stop it or else we might go into another ai winter '' or `` oh , we ve seen that in hollywood so that ca nt happen '' do nt count for obvious reasons .",
    "+ in my experience , discussion about ai safety with people who did nt read  @xcite is like discussion about deep learning with people who do nt know backpropagation basics .",
    "it s always boring and misleading .",
    "+      it s reasonable to suppose that measures to prevent humans from the risks of having powerful ais can not be fulfilled immediately and instead they take many years .",
    "it might even turn out that it s already too late to effectively control safety of ai research and guarantee that the race of superhuman - smart ais do nt overcome humans in next 10 to 20 years .",
    "some people talk about cyborgization as a possible solution to that problem but do we humans really want to become cyborgs ?",
    "are cyborgs really able to compete with ais given the slowness of human brains ?",
    "i m truly interested in discussion so feel free to send me e - mails or contact via http://facebook.com/sergej.shegurin ( `` sergej shegurin '' is my nickname i choose for internet long ago ) . + here",
    "is my list of best ai articles in _ chronological _ order : http://goo.gl/7hjjhu"
  ],
  "abstract_text": [
    "<S> here , i review current state - of - the - arts in many areas of ai to estimate when it s reasonable to expect human level ai development </S>",
    "<S> . predictions of prominent ai researchers vary broadly from very pessimistic predictions of andrew ng to much more moderate predictions of geoffrey hinton and optimistic predictions of shane legg , deepmind cofounder . </S>",
    "<S> given huge rate of progress in recent years and this broad range of predictions of ai experts , ai safety questions are also discussed .    </S>",
    "<S> = 1 </S>"
  ]
}