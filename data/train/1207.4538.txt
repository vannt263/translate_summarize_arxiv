{
  "article_text": [
    "the focus of this paper is on the following structured minimization @xmath1 where @xmath2 is a continuously differentiable ( may be nonconvex ) function that is bounded below ; @xmath3 denotes the @xmath0-norm of a vector ; parameter @xmath4 is used to trade off both terms for minimization . due to its structure , problem ( [ probtype ] ) covers a wide range of apparently related formulations in different scientific fields including linear inverse problem , signal / image processing , compressive sensing , and machine learning .",
    "a popular special case of model ( [ probtype ] ) is the @xmath0-norm regularized least square problem @xmath5 where @xmath6 @xmath7 is a linear operator , and @xmath8 is an observation . model ( [ onenorm ] ) mainly appears in compressive sensing  an emerging methodology in digital signal processing , and has attracted intensive research activities over the past years @xcite .",
    "compressive sensing is based on the fact that if the original signal is sparse or approximately sparse in some orthogonal basis , an exact restoration can be produced via solving problem ( [ onenorm ] ) .",
    "another prevalent case of ( [ probtype ] ) that has been achieved much interest in machine learning is the linear and logistic regression . given the training date @xmath9^\\top\\in\\mathbb{r}^{m\\times n}$ ] and class labels @xmath10 .",
    "a linear classifier is a hyperplane @xmath11 , where @xmath12 is a set of weights and @xmath13 is the intercept .",
    "a frequently used model is the @xmath14-loss support vector machine @xmath15 because of the `` max '' operation , the @xmath14-loos function is continuous , but not differentiable . based on the conditional probability , another popular model is the logistic regression @xmath16 obviously , the logistic loss function is twice differentiable .",
    "although the models of these problems have similar structures , they may be very different from real - data point of view .",
    "for example , in compressive sensing , the length of measurement @xmath17 is much smaller than the length of original signal @xmath7 and the encoding matrix @xmath18 is dense .",
    "however , in machine learning , the numbers of instance @xmath17 and features @xmath19 are both large and the data @xmath18 is very sparse .      since the @xmath0-regularized term is non - differentiable when @xmath20 contains values of zero , the use of the standard unconstrained smooth optimization tools are generally precluded . in the past decades ,",
    "a wide variety of approaches has been proposed , analyzed , and implemented in compressive sensing and machine learning literatures .",
    "this includes a variety of algorithms for special cases where @xmath21 has a specific functional form such as the least square ( [ onenorm ] ) , the square loss ( [ svm ] ) and the logistic loss ( [ logmodel ] ) . in the following ,",
    "we briefly review some of them in each literature .",
    "the first popular approach falls into the coordinate descent method . at the current iterate @xmath22 , the simple coordinate descent method updates one component at a time to generate @xmath23 , @xmath24 , such that @xmath25 , @xmath26 , and solves a one - dimensional subproblem @xmath27 where @xmath28 is defined as the @xmath29-th column of an identity matrix",
    "clearly , the objective function has one variable , and one non - differentiable point at @xmath30 . to solve the logistic regression model ( [ logmodel ] ) ,",
    "bbr @xcite solves the sub - problem approximately by the use of trust region method with newton step ; cdn @xcite improves bbr s performance by applying a one - dimensional newton method and a line search technique . instead of cyclically updating one component at each time , the stochastic coordinate descent method @xcite randomly selects the working components to attain better performance ; the block coordinate gradient descent algorithm  cgd @xcite is based on the approximated convex quadratic model for @xmath31 , and selects the working variables with some rules .",
    "the second type of approach is to transform model ( [ probtype ] ) into an equivalent box - constrained optimization problem by variable splitting .",
    "let @xmath32 with @xmath33 and @xmath34 .",
    "then , model ( [ probtype ] ) can be reformulated equivalently as @xmath35 the objective function and constraints are smooth , and therefore , it can be solved by any standard box - constrained optimization technique . however , an obvious drawback of this approach is that it doubles the number of variables .",
    "gpsr @xcite solves ( [ subbound ] ) , and subsequently solves ( [ onenorm ] ) , by using barzilai - borwein gradient method @xcite with an efficient nonmonotone line search @xcite .",
    "it is actually an application of the well - known spectral projection gradient @xcite in compressive sensing .",
    "trust region newton algorithm  tron @xcite minimizes ( [ subbound ] ) , then solves the logistic regression model ( [ logmodel ] ) , and exhibits powerful vitality by a series of comparisons . to solve ( [ svm ] ) and ( [ logmodel ] ) , the interior - point algorithm @xcite",
    "forms a sequence of unconstrained approximations by appending a ` barrier ' function to the objective function ( [ subbound ] ) which ensures that @xmath36 and @xmath37 remain sufficiently positive . moreover , truncated newton steps and preconditioned conjugate gradient iterations are used to produce the search direction .",
    "the third type of method is to approximate the @xmath0-regularized term with a differentiable function .",
    "the simple approach replaces the @xmath0-norm with a sum of multi - quadric functions @xmath38 where @xmath39 is a small positive scalar .",
    "this function is twice - differentiable and @xmath40 .",
    "subsequently , several smooth unconstrained optimization approaches can be applied , based on this approximation .",
    "however , the performance of these algorithms is much influenced by the parameter values , and the condition number of the corresponding hessian matrix becomes larger as @xmath39 decreases .",
    "the nesterov s smoothing technique @xcite is to construct smooth functions to approximate any general convex nonsmooth function .",
    "based on this technique , nesta @xcite solves problem ( [ onenorm ] ) by using first - order gradient information .",
    "the fourth type of approach falls into the subgradient - based newton - type algorithm .",
    "the important attempt in this class is from andrew and gao @xcite , who extend the well - known limited memory bfgs method @xcite to solve @xmath0-regularized logistic regression model ( [ logmodel ] ) , and propose an orthant - wise limited memory quasi - newton method  owl - qn . at each iteration",
    ", this method computes a search direction over an orthant containing the previous point . the subspace bfgs method  subbfgs @xcite involves an inner iteration approach to find the descent quasi - newton direction and a subgradient wolfe - condtions to determine the stepsize which ensures that the objective functions are decreasing .",
    "this method enjoys global convergence and is capable of solving general nonsmooth convex minimization problems .    finally , to solve model ( [ onenorm ] ) , besides gpsr and nesta , there are other numerous specially designed solvers . by an operator splitting technique , hale , yin and zhang derive the iterative shrinkage / thresholding fixed - point continuation algorithm ( fpc ) @xcite . by combining the interior - point algorithm in @xcite , fpc",
    "is also extended to solve large - scale @xmath0-regularized logistic regression in @xcite .",
    "twist @xcite and fista @xcite speed up the performance of ist and have virtually the same complexity but with better convergence properties .",
    "another closely related method is the sparse reconstruction algorithm sparsa @xcite , which is to minimize non - smooth convex problem with separable structures .",
    "spgl1 @xcite solves the lasso model ( [ onenorm ] ) by the spectral gradient projection method with an efficient euclidean projection on @xmath0-norm ball .",
    "the alternating directions method  yall1 @xcite , investigates @xmath0-norm problems from either the primal or the dual forms and solves @xmath0-regularized problems with different types .",
    "all the reviewed algorithms differ in various aspects such as the convergence speed , ease of implementation , and practical applicability .",
    "moreover , there is no enough evidence to verify that which algorithm outperforms the others under all scenarios .",
    "although much progress has been achieved in solving the problem ( [ probtype ] ) , these algorithms mainly deal with the case where @xmath31 is a convex function even a least square . in this paper , unlike all the reviewed algorithms , we propose a barzilai - borwein gradient algorithm for solving @xmath0-regularized nonsmooth minimization problems . at each iteration , we approximate @xmath31 locally by a convex quadratic model , where the hessian is replaced by the multiplies of a spectral coefficient with an identity matrix . the search direction is determined by minimizing the quadratic model and taking full use of the @xmath0-norm structure .",
    "we show that the generated direction is descent which guarantees that there exists a positive stepsize along the direction . in our algorithm",
    ", we adopt the nonmonotone line search of grippo , lampariello , and lucidi @xcite , which allows the function values to increase occasionally in some iteration but decrease in the whole iterative process .",
    "the attractive property of the nonmonotone line search is that it saves much number of function evaluations which should be the main computational burden in large dataset .",
    "the method is easily performed , where only the value of objective function and the gradient of the smooth term are needed at each iteration .",
    "we show that each cluster of the iterates generated by this algorithm is a stationary point of @xmath41 . in this paper , although we mainly consider the @xmath0-regularizer , the @xmath14-norm regularization problem and the matrix trace norm problems can also be readily included in our framework .",
    "thus , this broaden the capability of the algorithm .",
    "we implement the algorithm to solve problem ( [ probtype ] ) where @xmath31 is a nonconvex smooth function from cuter library to show its efficiency .",
    "moreover , we also run the algorithm to solve @xmath0-regularized least square , and do performance comparisons with the state - of - the - art algorithms nesta , cgd , twist , fpc and gpsr .",
    "the comparisons results show that the proposed algorithm is effective , comparable , and promising .",
    "we organize the rest of this paper as follows . in section [ algorithmsec ] ,",
    "we briefly recall some preliminary results in optimization literature to motivate our work , construct the search direction , and present the steps of our algorithm along with some remarks . in section",
    "[ theory ] , we establish the global convergence theorem under some mild conditions . in section [ secexten ] , we show that how to extend the algorithm to solve @xmath14-norm and matrix trace norm minimization problems . in section [ expnoncov ] ,",
    "we present experiments to show the efficiency of the algorithm in solving the @xmath0-regularized nonconvex problem and least square problem .",
    "finally , we conclude our paper in section [ concludsec ] .",
    "first , consider the minimization of the smooth function without the @xmath0-norm regularization @xmath42 the basic idea of newton s method for this problem is to iteratively use the quadratic approximation @xmath43 to the objective function @xmath21 at the current iterate @xmath22 and to minimize the approximation @xmath43 .",
    "let @xmath44 be twice continuously differentiable , and its hessian @xmath45 be positive definite .",
    "function @xmath31 at the current @xmath22 is modeled by the quadratic approximation @xmath43 , @xmath46 where @xmath47 .",
    "minimizing @xmath48 yields @xmath49 which is newton s formula and @xmath50 is the so - called newton s direction .",
    "for the positive definite quadratic function , newton s method can reach the minimizer with one iteration . however , when the starting point is far away from the solution , it is not sure that @xmath51 is positive definite and newton s direction @xmath52 is a descent direction .",
    "let the quadratic model of @xmath31 at @xmath53 be @xmath54 finding the derivative yields @xmath55 setting @xmath56 , @xmath57 , and @xmath58 we get @xmath59 for various practical problems , the computing efforts of the hessian matrices are very expensive , or the evaluation of the hessian is difficult ; even the hessian is not available analytically .",
    "these lead to the quasi - newton method which generates a series of hessian approximations by the use of the gradient , and at the same time maintains a fast rate of convergence . instead of computing the hessian @xmath60",
    ", quasi - newton method constructs the hessian approximation @xmath61 , where the sequence @xmath62 possesses positive definiteness and satisfies @xmath63 in general , such @xmath64 will be produced by updating @xmath65 with some typical and popular formulae such as bfgs , dfp , and sr1 .",
    "unfortunately , the standard quasi - newton algorithm , or even its limited memory versions , does nt scale well enough to train very large - scale models involving millions of variables and training instances , which are commonly encountered , for example , in natural language processing .",
    "the main computational burden of newton - type algorithm is the storage of a large matrix at per - iteration , which may be out of the memory capability for a pc .",
    "it should be develop a matrix - free algorithm to deal with large - scale problems but also belongs to the quasi - newton framework .",
    "for this purpose , it would like to furthermore simplify the approximation hessian @xmath65 as a diagonal matrix with positive components , i.e. , @xmath66 with an identity matrix @xmath67 and @xmath68 .",
    "then , the quasi - newton condition changes to the form @xmath69 multiplying both sides by @xmath70 , gives @xmath71 similarly , multiplying both sides by @xmath72 , yields @xmath73 observing both formulae , it indicates that if @xmath74 , the matrix @xmath75 is positive definite , which ensures that the search direction @xmath76 is descent at current point .",
    "the formulae ( [ alpha1 ] ) and ( [ alpha2 ] ) were firstly developed by barzilai and borwein @xcite for the quadratic case of @xmath31 .",
    "this method essentially consists the steepest descent method , and adopts the choice of ( [ alpha1 ] ) or ( [ alpha1 ] ) as the stepsize along a negative gradient direction .",
    "barzilai and borwein @xcite showed that the corresponding iterative algorithm is r - superlinearly convergent for the quadratic case .",
    "raydan @xcite presented a globalization strategy based on nonmonotone line search @xcite for the general non - quadratic case .",
    "other developments in barzilai and borwein gradient algorithm can be found in @xcite .      due to its simplicity and numerical efficiency",
    ", the barzilai - borwein gradient method is very effective to deal with large - scale smooth unconstrained minimization problems .",
    "however , the application of the barilai - borwein gradient algorithm to @xmath0-regularized nonsmooth optimization is problematic since the regularization is non - differentiable . in this subsection , we construct an iterative algorithm to solve the @xmath0-regularized structured nonconvex optimization problem .",
    "the algorithm can be described as the iterative form @xmath77 where @xmath78 is a stepsize , and @xmath52 is a search direction defined by minimizing a quadratic approximated model of @xmath41 .",
    "now , we turn to our attention to consider the original problem with @xmath0-regularizer . since @xmath0-term is not differentiable , hence , at current @xmath22 , objective function @xmath41 is approximated by the quadratic approximation @xmath79 , @xmath80\\triangleq q_k(d),\\end{aligned}\\ ] ] where @xmath81 is a small positive number .",
    "the term in @xmath82 $ ] can be considered as an approximate taylor expansion of @xmath83 with a small @xmath81 , and the case @xmath84 reduces the equivalent form @xmath83 .",
    "minimizing ( [ direcmodel ] ) yields @xmath85 where @xmath86 , @xmath87 , and @xmath88 denote the @xmath89-th component of @xmath22 , @xmath90 , and @xmath91 respectively .",
    "the favorable structure of ( [ subd ] ) admits the explicit solution @xmath92 hence , the search direction at current point is @xmath93.\\ ] ] where @xmath94 and @xmath95 are interpreted as componentwise and the convention @xmath96 is followed .",
    "when @xmath97 , ( [ direction ] ) reduces to @xmath98 , i.e. , the traditional barizilai - borwein gradient algorithm in smooth optimization .",
    "the key motivation for this formulation is that the optimization problem in eq .",
    "( [ subd ] ) can be easily solved by exploiting the structure of the @xmath0-norm .",
    "[ ineq ] for any real vectors @xmath99 and @xmath100 , the following function @xmath101 is non - decreasing @xmath102    note that @xmath103 hence , it reduces to prove that @xmath104 is non - decreasing for each @xmath89 .",
    "+ ( a ) . when @xmath105 and @xmath106 .",
    "it is clear that @xmath107 .",
    "+ ( b ) . when @xmath105 and @xmath108 , we have @xmath109 ( c ) .",
    "when @xmath110 and @xmath106 , we have @xmath111 ( d ) .",
    "when @xmath110 and @xmath106 , we have @xmath112 .",
    "+ it is not difficult to see that @xmath104 is non - decreasing at each case . hence , @xmath101 is non - decreasing .",
    "the following lemma shows that the direction defined by ( [ direction ] ) is descent if @xmath113 .",
    "[ descent ] suppose that @xmath68 and @xmath52 is determined by ( [ direction ] )",
    ". then @xmath114+o(\\theta ) \\quad \\theta\\in(0,h],\\ ] ] and @xmath115    by the differentiability of @xmath31 and the convexity of @xmath116 , we have that for any @xmath117 $ ] ( @xmath118 $ ] ) , @xmath119,\\end{aligned}\\ ] ] which is exactly ( [ des1 ] ) .    noting that @xmath52 is the minimizer of ( [ direcmodel ] ) and @xmath117 $ ] , from ( [ direcmodel ] ) and the convexity of @xmath116 , we have @xmath120 hence",
    ", @xmath121 the last three terms of the left side in ( [ sanbuineq ] ) can be re - organized as @xmath122\\big\\}\\nonumber\\\\ = & \\frac{\\mu}{h}\\big\\{\\|x_k+hd_k\\|_1-\\|x_k\\|_1-\\theta\\big[h\\cdot \\frac{\\|x_k+h^2d_k\\|_1 -\\|x_k\\|_1}{h^2}\\big]\\big\\}\\nonumber\\\\ \\geq & \\frac{\\mu}{h}\\big\\{\\|x_k+hd_k\\|_1-\\|x_k\\|_1-\\theta\\big[h\\cdot \\frac{\\|x_k+hd_k\\|_1 -\\|x_k\\|_1}{h}\\big]\\big\\}\\nonumber\\\\ = & \\frac{\\mu}{h}(1-\\theta)\\{\\|x_k+hd_k\\|_1-\\|x_k\\|_1 \\}\\label{imporineq},\\end{aligned}\\ ] ] where the inequality is from lemma [ ineq ] . combining ( [ sanbuineq ] ) with ( [ imporineq ] ) , it produces @xmath123 dividing both sides of ( [ y416 ] ) by @xmath124 and noting @xmath117 $ ] , we get the desirable result ( [ des2 ] ) .",
    "when the search direction is determined , a suitable stepsize along this direction should be found to determine the next iterative point . in this paper , unlike the traditional armijo line search or the wolfe - powell line search , we pay particular attention to a nonmonotone line search strategy .",
    "the traditional armijo line search requires the function value to decrease monotonically at each iteration . as a result",
    ", it may cause the sequence of iterations following the bottom of a curved narrow valley , which commonly occurs in difficult nonlinear problems . to overcome this difficultly , a credible alternative is to allow an occasional increase in the objective function at each iteration . to easy comprehension of the proposed algorithm , we briefly recall the earliest nonmonotone line search technique by grippo , lampariello , and lucidi @xcite .",
    "let @xmath125 , @xmath126 and @xmath127 be a positive integer .",
    "the nonmonotone line search is to choose the smallest nonnegative integer @xmath128 such as the stepsize @xmath129 satisfing @xmath130 where @xmath131 if @xmath132 , the above nonmonotone line search reduces to the standard armijo line search .    for the @xmath0-regularized nonsmooth problem ( [ probtype ] ) , based on lemma [ descent ] ,",
    "the inequality ( [ gll1 ] ) should be modified as @xmath133 where @xmath134 from ( [ des2 ] ) , it clear that @xmath135 whenever @xmath113 .",
    "hence , this shows that @xmath78 given by ( [ gll2 ] ) is well - defined .    in light of all derivations above",
    ", we now describe the nonmonotone barzilai - borwein gradient algorithm ( abbreviated as nbbl1 ) as follows .    ' '' ''    1.5 mm    [ alg1 ] * ( nbbl1 ) * 1.5 mm    ' '' ''    choose @xmath136 and constants @xmath4 .",
    "constants @xmath137 , @xmath126 , @xmath138 , @xmath139 $ ] and positive integer @xmath127 .",
    "set @xmath140 .",
    "+ * step 1 . *",
    "stop if @xmath141 .",
    "otherwise , continue .",
    "+ * step 2 .",
    "* compute @xmath52 via ( [ direction ] ) . + * step 3 . *",
    "compute @xmath78 via ( [ gll2 ] ) . + * step 4 .",
    "* let @xmath142 .",
    "+ * step 5 . *",
    "let @xmath143 .",
    "go to step 1 .",
    "-3 mm    1 mm    ' '' ''    1 mm    [ remark1 ] we have shown that if @xmath68 , then the generated direction is descent .",
    "however , in this case , the condition @xmath68 may fail to be fulfilled and the hereditary descent property is not guaranteed any more . to cope with this defect",
    ", we should keep the sequence @xmath144 uniformly bounded ; that is , for sufficiently small @xmath145 and sufficiently large @xmath146 , the @xmath147 is forced as @xmath148 this approach ensures that @xmath147 is bounded from zero and subsequently ensures that @xmath52 is descent at per - iteration .",
    "[ remark2 ] from lemma [ descent ] , it is clear that there exists a constant @xmath117 $ ] such that @xmath149 is a descent point in sense of ( [ des1 ] ) .",
    "hence , in practical computation , it is suggested to choose the initial stepsize as @xmath150 .",
    "this section is devoted to presenting some favorable properties of the generated direction and establishing the global convergence of algorithm [ alg1 ] subsequently .",
    "our convergence result utilizes the following assumptions .",
    "[ assu1 ] the level set @xmath151 is bounded .",
    "[ station ] suppose that @xmath68 and @xmath52 is defined by ( [ direction ] ) with @xmath139 $ ] .",
    "then @xmath22 is a stationary point of problem ( [ probtype ] ) if and only if @xmath152 .",
    "if @xmath113 , then lemma [ descent ] shows that @xmath52 is descent direction at @xmath22 , which implies that @xmath22 is not a stationary point of @xmath41 . on the other hand , if @xmath152 is the solution of ( [ subd ] ) , for any @xmath153 with @xmath154 we have @xmath155 since @xmath156 , this together with ( [ yun17 ] ) yields @xmath157-\\big[\\frac{\\mu}{h}\\|x_k+\\alpha hd\\|_1-\\frac{\\mu}{h}\\|x_k\\|_1\\big]}{\\alpha}\\big).\\\\ \\geq & \\lim_{\\alpha \\downarrow 0 } \\frac{-\\frac{\\lambda_k\\alpha^2}{2 } \\|d\\|_2 ^ 2+o(\\alpha)}{\\alpha}\\\\ = & 0,\\end{aligned}\\ ] ] where the second inequality is from lemma [ ineq ] . hence , @xmath22 is a stationary point of @xmath41 .",
    "the proof of the following lemma is similar with the theorem in @xcite .",
    "[ funclem ] let @xmath158 be an integer such that @xmath159 then the sequence @xmath160 is nonincreasing and the search direction @xmath161 satisfies @xmath162    from the definition of @xmath163 , we have @xmath164 .",
    "hence @xmath165 moreover , by ( [ gll2 ] ) , we have for all @xmath166 , @xmath167 by assumption [ assu1 ] , the sequence @xmath160 admits a limit for @xmath168 .",
    "hence , it follows that @xmath169 on the other hand , from the definition of @xmath170 in ( [ deltak ] ) and the inequality ( [ des2 ] ) , it is not difficult to deduce that @xmath171 combining ( [ alphadelk ] ) , yields @xmath172 which shows the desirable result ( [ limdkzero ] ) .    [ theorem ] let the sequence @xmath173 and @xmath174 generated by algorithm [ alg1 ] . then , there exists a subsequence @xmath175 such that @xmath176    from @xcite , it is clear that ( [ limdkzero ] ) also implies @xmath177 now , let @xmath178 be a limit point of @xmath173 , and @xmath179 be a subsequence of @xmath173 converging to @xmath178 . then by ( [ dklim ] ) either @xmath180 , which implies @xmath181 , or there exists a subsequence @xmath182 ( @xmath183 ) such that @xmath184 in this case , we assume that there exists a constant @xmath185 such that @xmath186 since @xmath78 is the first value for satisfying ( [ gll2 ] ) , it follows from step 3 in algorithm [ alg1 ] that there exists an index @xmath187 such that , for all @xmath188 and @xmath189 , @xmath190 since @xmath31 is continuous differentiable , by the mean - value theorem on @xmath31 , we can find there exists a constant @xmath191 , such that @xmath192 by combining with ( [ great ] ) , we have @xmath193 since @xmath150 and @xmath194 in ( [ dkalph ] ) , we have @xmath195 as @xmath196 .",
    "it follows from lemma [ ineq ] that @xmath197 subtracting both sides of ( [ datlakk2 ] ) by @xmath170 and noting the definition of @xmath170 , it is clear that @xmath198\\nonumber\\\\ > & -(1-\\delta)\\delta_k\\nonumber\\\\ \\geq & ( 1-\\delta)\\frac{\\lambda_{(\\min)}}{2}\\|d_k\\|_2 ^ 2\\label{datlakk3}.\\end{aligned}\\ ] ] taking the limit as @xmath189 , @xmath196 in the both sides of ( [ datlakk3 ] ) and using the smoothness of @xmath31 , we obtain @xmath199 which implies @xmath200 as @xmath189 , @xmath196 .",
    "this yields a contradiction because ( [ neq ] ) indicates that @xmath201 is bounded .",
    "in this section , we show that our algorithm can be readily extended to solve @xmath14-norm and matrix trace norm minimization problems in machine learning ; thus , broaden the applicable range of our approach significantly .",
    "firstly , we consider the @xmath14-regularization problem @xmath202 it is not difficult to deduce that , the search direction @xmath52 is determined by minimizing @xmath203 from @xcite , the explicit solution is @xmath204 i.e. , @xmath205.\\ ] ]    now , we consider the matrix trace norm minimization problem @xmath206 where the functional @xmath207 is the trace norm of matrix @xmath208 , which is defined as the sum of its singular values .",
    "that is , assume that @xmath208 has @xmath209 positive singular values of @xmath210 , then @xmath211 .",
    "the matrix trace norm is alternatively known as the schatten @xmath0-norm , ky fan norm , and nuclear norm @xcite .",
    "such problem has been received much attention because it is closely related to the affine rank minimization problem , which has appeared in many control applications including controller design , realization theory and model reduction .    as it has been done in the previous sections",
    ", we can readily reformulate ( [ direcmodel ] ) as the following quadratic model to determine the search direction , @xmath212 to get the exact solution of ( [ dkmatrix ] ) , we now consider the singular value decomposition ( svd ) of a matrix @xmath213 with rank @xmath209 , @xmath214 where @xmath215 and @xmath216 are @xmath217 and @xmath218 matrices respectively with orthonormal columns , and the singular value @xmath219 is positive .",
    "for each @xmath220 , we let @xmath221_+),\\ ] ] where @xmath82_+=\\max\\{0,\\cdot\\}$ ] .",
    "it is shown that @xmath222 obeys the following nuclear norm minimization problem @xcite , i.e. , @xmath223 comparing ( [ dkmatrix ] ) to ( [ nclear ] ) , we deduce that @xmath224_+\\big),\\ ] ] or , equivalently , @xmath225.\\ ] ] subsequently , it is easily to derive the nonmonotone barzilai and borwein gradient algorithmic framework for solving @xmath14-norm and matrix trace norm regularization problems .",
    "in this section , we present numerical results to illustrate the feasibility and efficiency of nbbl1 .",
    "we partition our experiments into three classes based on different types of @xmath31 . in the first class",
    ", we perform our algorithm to solve @xmath0-regularized nonconvex problem . in the second class ,",
    "we test our algorithm to solve @xmath0-regularized least squares which mainly appear in compressive sensing . in the third class ,",
    "we compare some state - of - the - art algorithms in compressive sensing to show the efficiency of our algorithm .",
    "all experiments are performed under windows xp and matlab 7.8 ( 2009a ) running on a lenovo laptop with an intel atom cpu at 1.6 ghz and 1 gb of memory .",
    "our first test is performed on a set of the nonconvex unconstrained problems from the cuter @xcite library .",
    "the second - order derivatives of all the selected problems are available .",
    "since we are interested in large problems , we only consider the problems with size at least @xmath226 . for these problems",
    ", we use the dimensions that is admissible of the ",
    "double large \" installation of cuter .",
    "the algorithm stops if the norm of the search direction is small enough ; that is , @xmath227 the iterative process is also stopped if the number of iterations exceeds @xmath228 without achieving convergence .    in this experiment ,",
    "we take @xmath229 , @xmath84 , @xmath230 , @xmath231 . in the line search , we choose @xmath232 , @xmath233 , @xmath234 and @xmath235 .",
    "we test nbbl1 with different parameter values @xmath236 .",
    "the numerical results are presented in table [ results ] , which contains the name of the problem ( problem ) , the dimensions of the problem ( dim ) , the number of iterations ( iter ) , the number of function evaluations ( nf ) , the cpu time required in seconds ( time ) , the final objective function values ( fun ) , the norm of the final gradient of @xmath31 ( normg ) , and the norm of final direction ( normd ) .    from table [ results ] , we see that nbbl1 works successfully for all the test problems in each case .",
    "particularly , nbbl1 always produces great accuracy solutions within little consuming time .",
    "the proposed algorithm requires large number of iterations for some special problems , such as problems fletcher , noncvxu2 , broydn7d with parameter @xmath97 , problems fletcher and broydn7d with @xmath237 , problem fletcher with @xmath238 , and vardim with @xmath239 .",
    "however , if lower precision is permitted , the number can be decreased dramatically .",
    "the first part of table [ results ] presents the numerical results of nbbl1 for solving a smooth nonconvex minimization problem without any regularization . from the last second column in this part , we observe that the norm of the final gradient is sufficiently small",
    ". the important observation verifies that the proposed algorithm is very efficient to solve unconstrained smooth minimization problems .",
    "it is not a pleasant supervise , because our algorithm reduces to the well - known nonmonotone barzailai - borwein gradient of raydan @xcite in this case .",
    "let @xmath178 be a sparse or a nearly sparse original signal , @xmath240 ( @xmath241 ) be a linear operator , @xmath242 be a zero - mean gaussian white noise , and @xmath8 be an observation which satisfies the relationship @xmath243 recent compressive sensing results show that , under some technical conditions , the desirable signal can be reconstructed almost exactly by solving the @xmath0-regularized least square ( [ onenorm ] ) .",
    "in this subsection , we perform two classes of numerical experiments for solving ( [ onenorm ] ) by using the gaussian matrices as the encoder . in the first class",
    ", we show that our algorithm performs well to decode a sparse signal , while in the second class we do a series of experiments with different @xmath81 to choose the best one .",
    "we measure the quality of restoration @xmath244 by means of the relative error to the original signal @xmath178 ; that is @xmath245    in the first test , we use a random matrix @xmath18 with independent identically distributions gaussian entries .",
    "the @xmath246 is the additive gaussian noise of zero mean and standard deviation @xmath247 . due to the storage limitations of pc",
    ", we test a small size signal with @xmath248 , @xmath249 .",
    "the original contains randomly @xmath250 non - zero elements . besides",
    ", we also choose the noise level @xmath251 .",
    "the proposed algorithm starts at a zero point and terminates when the relative change of two successive points are sufficient small , i.e. , @xmath252 in this experiment , we take @xmath253 , @xmath254 , @xmath255 , @xmath256 . in the line search , we choose @xmath257 , @xmath233 , @xmath234 and @xmath235 .",
    "the original signal , the limited measurement , and the reconstructed signal are given in figure [ fig1 ] .",
    "and @xmath258 positive non - zero elements ; middle : the noisy measurement with length @xmath259 ; right : recovered signal by nbbl1 ( red circle ) versus original signal ( blue peaks).,title=\"fig : \" ]   and @xmath258 positive non - zero elements ; middle : the noisy measurement with length @xmath259 ; right : recovered signal by nbbl1 ( red circle ) versus original signal ( blue peaks).,title=\"fig : \" ]   and @xmath258 positive non - zero elements ; middle : the noisy measurement with length @xmath259 ; right : recovered signal by nbbl1 ( red circle ) versus original signal ( blue peaks).,title=\"fig : \" ]    comparing the left plot to the right one in figure [ fig1 ] , we clearly see that the original sparse signal is restored almost exactly .",
    "we see that all the blue peaks are circled by the red circles , which illustrates that the original signal has been found almost exactly .",
    "all together , this simple experiment shows that our algorithm performs quite well , and provides an efficient approach to recover large sparse non - negative signal .",
    "we have clearly known that the last term in the approximate quadratic model ( [ direcmodel ] ) is equivalent to @xmath83 exactly when @xmath84 .",
    "next , we provide evidence to show that other values can be potentially and dramatically better than @xmath84 .",
    "we conduct a series of experiments and compare the performance at each case . in our experiments",
    ", we set all the parameters values as the previous test except for @xmath260 .",
    "we present , in figure [ figh ] , the impact of the parameter @xmath81 values on the total number of iterations , the computing time , and the quality of the recovered signal . in each plot",
    ", the level axis denotes the values of @xmath81 from @xmath261 to @xmath262 in a log scale .     in log scale.,title=\"fig : \" ]   in log scale.,title=\"fig : \" ]   in log scale.,title=\"fig : \" ]    in figure [ figh ] , the number of iterations , the computing time and the quality of restorations are greatly influenced by the @xmath81 values .",
    "generally , as @xmath81 increases , nbbl1 always has good performance .",
    "the right plot clearly demonstrates that the relative error decreases dramatically at the very beginning and then becomes slightly after @xmath263 .",
    "however , the quality of restoration can not be improved any more after @xmath264 . on the other hand ,",
    "the left and the middle plots show that the number of iterations and the computing time slightly increase after @xmath265 .",
    "taking three plots together , these plots verify that the performance of nbbl1 is sensitive to the @xmath81 values , and the value @xmath266 $ ] may be the better choice .",
    "the third class of the experiment is to test against several state - of - the - art algorithms which are specifically designed in recent years to solve @xmath0-regularized problems in compressive sensing or linear inverse problems .",
    "it is difficult to compare each algorithm in a very fair way , because each algorithm is compiled with different parameter settings , such as the termination criterions , the staring points , or the continuation techniques .",
    "hence , as usual , in our performance comparisons , we run each code from the same initial point , use all the default parameter values , and only observe the convergence behavior of each algorithm to attain a similar accuracy solution .",
    "nesta uses nesterov s smoothing technique @xcite and gradient method @xcite to solve basis pursuit denoising problem .",
    "the current version is capable of solving @xmath0-norm regularization problems with different types including ( [ onenorm ] ) . in this experiment , we test nesta with continuation ( named nesta - ct ) for comparison , where this algorithm solves a sequence of problems ( [ onenorm ] ) by using a decreasing sequence of values of @xmath267",
    ". additionally , nesta - ct uses the intermediated solution as a warm start for the next problem . in running nesta ,",
    "all the parameters are taken as default except tolvar is set to be @xmath268 to obtain similar quality solutions with others .",
    "gpsr - bb ( gradient projections for sparse reconstruction ) @xcite reformulates the original problem ( [ onenorm ] ) as a box - constrained quadric programming problem ( [ subbound ] ) by splitting @xmath32 .",
    "figueiredo , nowak and wright use a gradient projection method with barziali - borwein steplength @xcite for its solution .",
    "moreover , the nonmonotone line search @xcite is also used to improve its performance . for the comparison with gpsr - bb",
    ", we use its continuation variant and set all parameters as default .",
    "the well - known cgd uses gradient algorithm to solve ( [ onesub ] ) in order to obtain the search direction @xmath269 in @xmath270 , where @xmath271 is a nonempty subset of @xmath272 , and choose the index subset @xmath271 a gauss - southwell rule .",
    "the iterative process @xmath142 continues until some termination critera are met , where @xmath273 with @xmath274 and the stepsize @xmath78 by using a armijo rule . in running cgd",
    ", we use the code cgd in its matlab package , and set all the parameter as default except for init=2 to start the iterative process at @xmath275 .",
    "twist is a two - step ist algorithm for solving a class of linear inverse problems .",
    "specifically , twist is designed to solve @xmath276 where @xmath18 is a linear operator , and @xmath277 is a general regularizer , which can be either the @xmath0-norm or the tv .",
    "the iteration framework of twist is @xmath278 where @xmath279 are parameters , @xmath280 and @xmath281 we use the default parameters in twist and terminate the iteration process when the relative variation of function value falls below @xmath282 .",
    "fpc is the fixed - point continuation algorithm to solve the general @xmath0-regularized minimization problem ( [ probtype ] ) , where @xmath31 is a continuous differentiable convex function . at current @xmath22 and any scalar @xmath220 ,",
    "the next iteration is produced by the so - called fixed point iteration @xmath283 where  sgn \" is a componentwise sign function . in order to obtain a good practical performance , a continuation approach is also augmented in fpc .",
    "moreover , the fpc is further modified by using barzilai - borwein stepsize ( code fpc - bb in matlab package fpc_v2 ) .",
    "the continuation and barzilai - borwein stepsize techniques make fpc - bb faster than fpc . in running of fpc - bb ,",
    "we use all the default parameter values except we set xtol = 1e-5 to stop the algorithm when the relative change between successive points is below xtol .    in this test",
    ", @xmath18 is a partial discrete cosine coefficients matrix ( dct ) , whose @xmath17 rows are chosen randomly from the @xmath284 dct matrix .",
    "such encoding matrix @xmath18 does not require storage and enables fast matrix - vector multiplications involving @xmath18 and @xmath285 .",
    "therefore , it is able to be used to test much larger size problems than using gaussian matrices . in nbbl1 , we take @xmath286 , @xmath265 , @xmath255 , @xmath256 . in the line search , we choose @xmath287 , @xmath233 , @xmath288 and @xmath235 . in this comparison , we let @xmath289 , @xmath290 .",
    "the original signal @xmath178 contains @xmath291 number of nonzero components , where floor is a matlab command used to round an element to the nearest integers towards minus infinity .",
    "moreover , the observation @xmath292 is contaminated by gaussian noise with level @xmath293 .",
    "the goal is to use each algorithm to reconstruct @xmath178 from the observation @xmath292 by solving ( [ onenorm ] ) with @xmath294 .",
    "all the tested algorithms start at @xmath275 and terminate with different stopping criterions to produce similar quality resolutions . to specifically illustrate the performance of each algorithm , we draw four figures to show their convergence behavior from the point of objective function values and relative error as the iteration numbers and computing time increase , which given in figure [ figcom ] .      +      from the top plots in figure [ figcom ] , nbbl1 usually decreases relative errors faster than nesta - ct , cgd and gpsr - bb throughout the entire iteration process , and meanwhile requires less number of iterations .",
    "the top right plot shows that twist needs less steps than nbbl1 to obtain similar level of relative error .",
    "however , twist is much slower because it has to solve a de - noising subproblem ( [ denois ] ) at each iteration .",
    "unfortunately , nbbl1 needs further improvement to challenge the well - known code fpc - bb .",
    "we now turn our attention to observe the function values behavior of each algorithm .",
    "similarly , nbbl1 is superior to nest - ct , cgd , gpsr - bb and twist from the computing time points of view .",
    "fpc - bb reaches the lowest function values at the very beginning , and then starts to increase it to meet nearly equal final values at the end . in this test",
    ", cgd appears to be much slower than the others , because it is sensitive to the choice of starting points .",
    "if cgd starts at @xmath295 with all the other settings unchanged , its performance should be significantly improved @xcite . taking everything together , from the limited numerical experiment",
    ", we conclude that nbbl1 provides an efficient approach for solving @xmath0-regularized nonsmooth problem and is competitive with or performs better than nesta - ct , gpsr - bb , cgd , twist and fpc - bb .",
    "in this paper , we proposed , analyzed , and tested a new practical algorithm to solve the separable nonsmooth minimization problem consisting of a @xmath0-norm regularized term and a continuously differentiable term .",
    "the type of the problem mainly appears in signal / image processing , compressive sensing , machine learning , and linear inverse problems .",
    "however , the problem is challenging due to the non - smoothness of the regularization term .",
    "our approach minimizes an approximal local quadratic model to determine a search direction at each iteration .",
    "the search direction reduces to the classic barzilai - borwein gradient method in the case of @xmath97 .",
    "we show that the objective function is descent along this direction providing that the initial stepsize is less than @xmath81 .",
    "we also establish the algorithm s global convergence theorem by incorporating a nonmonotone line search technique and assuming that @xmath31 is bounded below .",
    "extensive experimental results show that the proposed algorithm is an effective toll to solve @xmath0-regularized nonconvex problems from cuter library .",
    "moreover , we also run our algorithm to recover a large sparse signal from its noisy measurement , and numerical comparisons illustrate that our algorithm outperforms or is competitive with several state - of - the - art solvers which specifically designed to solve @xmath0-regularized compressive sensing problems .    unlike all the existing algorithms in this literature , our approach uses an linear model to approximate @xmath83 for computing the search direction with a small scalar @xmath81 ; that is @xmath296 although the equations may hold exactly in the case of @xmath84 , a series of numerical experiments show that @xmath266 $ ] may produce better performance with suitable experiment settings .",
    "this approach is distinctive and novel ; therefore , it is one of the important contributions of this paper . as we all know , the nonmonotone barzilai - borwein gradient algorithm of raydan @xcite is very effective for smooth unconstrained minimization , and its remarkable effectiveness in signal reconstruction problems involving @xmath0-regularized problems has not been clearly explored . hence , our approach can be considered as a modification or extension , to broaden the university of @xcite .",
    "moreover , the numerical experiments illustrate that our approach performs comparable to or even better than several state - of - the - art algorithms . surely , this is the numerical contribution of our paper .",
    "although the proposed algorithm needs further improvement to challenge the well - known code fpc - bb , the enhancement of it to deal with non - convex problems is noticeable .",
    "our algorithm is readily to solve the @xmath0-regularized logistic regression , the @xmath14-norm and matrix trace norm minimization problems in machine learning .",
    "however , we do not test them in this paper .",
    "this should be interesting for further investigations .",
    "e. cands , j. romberg , and t. tao , _ robust uncertainty principles : exact signal reconstruction from highly incomplete frequence information _ ,",
    "ieee transactions on information theory , 52 ( 2006 ) , 489 - 509 .",
    "m. figueiredo , r.d .",
    "nowak , and s.j .",
    "wright , _ gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems _ , ieee journal of selected topics in signal processing , ieee press , piscataway , nj , 2007 , 586 - 597 .",
    "s. kim , k. koh , m. lustig , s. boyd , and d. gorinevsky , _ an interior - point method for large - scale @xmath0-regularized least squares _",
    ", ieee journal of selected topics in signal processing , 1 ( 2007 ) , 606 - 617 .",
    "wright , r.d .",
    "nowak and m.a.t .",
    "figueiredo , _ sparse reconstruction by separable approximation _ , in roceedings of the international conference on acoustics , speech , and signal processing , 2008 , 3373 - 3376 .",
    "j. yu , s.v.n .",
    "vishwanathan , s. gnter , and n.n .",
    "schraudolph , _ a quasi - newton approach to nonsmooth convex optimization problems in machine learning _",
    ", journal of machine learning research , 11 ( 2010 ) , 1145 - 1200 .",
    "yuan , k.w .",
    "chang , c.j .",
    "hsieh , and c.j .",
    "lin , _ a comparison of optimization methods and software for large - scale @xmath0-regularized linear classification _ , journal of machine learning research , 11 ( 2010 ) , 3183 - 3234 ."
  ],
  "abstract_text": [
    "<S> this paper is devoted to minimizing the sum of a smooth function and a nonsmooth @xmath0-regularized term . </S>",
    "<S> this problem as a special cases includes the @xmath0-regularized convex minimization problem in signal processing , compressive sensing , machine learning , data mining , etc . </S>",
    "<S> however , the non - differentiability of the @xmath0-norm causes more challenging especially in large problems encountered in many practical applications . </S>",
    "<S> this paper proposes , analyzes , and tests a barzilai - borwein gradient algorithm . at each iteration , </S>",
    "<S> the generated search direction enjoys descent property and can be easily derived by minimizing a local approximal quadratic model and simultaneously taking the favorable structure of the @xmath0-norm . moreover , a nonmonotone line search technique is incorporated to find a suitable stepsize along this direction . </S>",
    "<S> the algorithm is easily performed , where the values of the objective function and the gradient of the smooth term are required at per - iteration . under some conditions , </S>",
    "<S> the proposed algorithm is shown to be globally convergent . </S>",
    "<S> the limited experiments by using some nonconvex unconstrained problems from cuter library with additive @xmath0-regularization illustrate that the proposed algorithm performs quite well . </S>",
    "<S> extensive experiments for @xmath0-regularized least squares problems in compressive sensing verify that our algorithm compares favorably with several state - of - the - art algorithms which are specifically designed in recent years .    </S>",
    "<S> nonsmooth optimization , nonconvex optimization , barzilai - borwein gradient algorithm , nonmonotone line search , @xmath0 regularization , compressive sensing    65l09 , 65k05 , 90c30 , 90c25 </S>"
  ]
}