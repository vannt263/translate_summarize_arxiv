{
  "article_text": [
    "to handle large - scale optimization problems , a popular strategy is to employ stochastic gradient descent ( sgd ) methods because of two advantages .",
    "first , they do not need to compute all gradients over the whole dataset in each iteration , which lowers computational cost per iteration .",
    "secondly , they only process a mini - batch of data points @xcite or even one data point @xcite in each iteration , which vastly reduces the memory storage .",
    "therefore , many researchers have extensively studied and applied various sgd methods @xcite .",
    "for instance , large - scale sgd @xcite has been substantially applied to the optimization of deep learning models @xcite .",
    "primal estimated sub - gradient solver ( pegasos ) @xcite is employed to speed up the support vector machines ( svm ) methods , which is suitable for large - scale text classification problems .",
    "-0.1 in    .",
    "magenta curve , black curve and green curve correspond to logistic loss , hinge loss and reversed gompertz loss accordingly .",
    "it can be observed that the incorrectly labeled instance `` a '' in the left panel can be regarded as the outlier of negative class , and its loss value @xmath1 is upper bounded by ramp loss , smooth ramp loss , and reversed gompertz loss ( see `` @xmath2 '' in the right panel).,scaledwidth=90.0% ]    -0.5 in    however , vanilla sgd methods suffer from the label noise problem since the noisy labels adversely affect the update of the primal variable in sgd methods .",
    "unfortunately , the label noise problems are very common in real - world applications . for instance",
    ", amazon mechanical turk ( mturk ) is a crowdsourcing internet platform that takes advantage of human intelligence to provide supervision , such as labeling different kinds of bird pictures and annotating keywords according to geoscience records .",
    "however , the quality of annotations is not always satisfactory because many workers are not sufficiently trained to label or annotate such specific data @xcite .",
    "another situation is where the data labels are automatically inferred from user online behaviors or implicit feedback .",
    "for example , the existing recommendation algorithms usually consider a user clicking on an online item ( e.g. , advertisements on youtube or ebay ) as a positive label indicating user preference , whereas users may click the item for different reasons , such as curiosity or clicking by mistake .",
    "therefore , the labels inferred from online behaviors are often noisy .",
    "the aforementioned issues lead to a challenging question  - if the majority of data labels are incorrectly annotated , can we reduce the negative effects on sgd methods caused by these noisy labels ?",
    "our high - level idea is to design a robust loss function with a threshold for sgd methods .",
    "we illustrate our idea by using a binary classification example . in the left panel of",
    ", we notice that the instance @xmath3 ( i.e. , data point `` a '' ) is incorrectly annotated with the label @xmath4 , which is opposite to its predicted label value ( + 1 ) according to the hyperplane . moreover , this instance is far away from the distribution of negative class .",
    "therefore , this instance @xmath3 with the noisy label @xmath5 can be regarded as the outlier of negative class .",
    "let the output of the classifier @xmath6 for a given @xmath7 be @xmath8 .",
    "let @xmath9 be the product of the real label and the predicted label of an instance @xmath7 ( i.e. , @xmath10 ) .",
    "then , given the outlier @xmath11 in the left panel of , we have @xmath12 . as illustrated in the right panel of figure [ robust - losses ] , with @xmath9 on the x - axis ,",
    "the gradient of hinge loss is non - zero on the @xmath13 , which will mislead the update of the primal variable @xmath14 in sgd methods .",
    "however , if the loss function has a , for example ramp loss @xcite in figure  [ robust - losses ] with a threshold @xmath15 , the gradient of ramp loss on the @xmath13 is zero , which minimizes the negative effects caused by this outlier on the update .",
    "therefore , it is reasonable to employ the loss with a threshold for sgd methods in the label noise problem .",
    "although the ramp loss is robust to outliers , it is computationally hard to optimize due to its nonsmoothness and nonconvexity @xcite .",
    "therefore , we consider to relax the ramp loss into smooth and locally strongly - convex loss . with random initialization , sgd methods can converge into a qualified local minima with a fast speed .",
    "our main contributions are summarized as follows .    1",
    ".   [ con1 ] we present a family of robust losses , which specifically benefit sgd methods to reduce the negative effects introduced by noisy labels , even under a high percentage of noisy labels .",
    "[ con2 ] we reveal that the convergence rate is @xmath0 for sgd methods using the proposed robust losses .",
    "moreover , we provide the robustness analysis on two representative robust losses .",
    "[ con3 ] comprehensive experimental results on varying scale datasets with noisy labels show that sgd methods using robust losses are obviously more robust than other baseline methods in most situations with fast convergence .",
    "first , our work is closely related to sgd methods . for example , xu proposes the averaged stochastic gradient descent ( asgd ) method @xcite to lower the testing error rate of the sgd @xcite . however , their work is based on the assumption that the data is clean , which significantly limits their applicability to the label noise problem .",
    "ghahdimi & lan introduce a randomized stochastic algorithm to solve nonconvex problems @xcite , and then generalize the accelerated gradient method to improve the convergence rate if the problem is nonconvex @xcite .",
    "however they do not focus on learning with noisy labels specifically , and do not consider strongly convex regularizer .",
    "second , our work is also related to bounded nonconvex losses for robust classification .",
    "for example , collobert et al .",
    "propose the bounded ramp loss for support vector machine ( svm ) classification problems .",
    "wang et al .",
    "further propose a robust svm based on a smooth version of ramp loss for suppressing the outliers @xcite .",
    "their models are commonly inferred by concave - convex procedure ( cccp ) @xcite .",
    "however , both of them do not consider that sgd methods suffer from the label noise problem .",
    "in other words , their works do not improve sgd methods using the smooth version of ramp loss in the label noise problem .",
    "finally , our work is highly related to noisy labels .",
    "for instance , reed & sukhbaatar focus on training deep neural networks using noisy labels @xcite .",
    "propose a probabilistic model for handling label noise problems @xcite .",
    "however , all these works are unrelated to sgd methods .",
    "moreover , they can not be used in real - time or large - scale applications due to their high computational cost .",
    "it is also demonstrated that the 0 - 1 loss function is robust for outliers . however , the 0 - 1 loss is neither convex nor differentiable , and it is intractable for real learning algorithms in practice .",
    "even though the surrogates of 0 - 1 loss is convex @xcite , they are very sensitive to outliers . to the best of our knowledge ,",
    "the problem of sgd methods for noisy labels has not yet been successfully addressed .",
    "this paper therefore studies this problem and provides an answer with theoretical analysis and empirical verification .",
    "in this section , we begin with the definition of a family of robust losses for sgd methods . under this definition ,",
    "we introduce two representative robust losses : smooth ramp loss and reversed gompertz loss .",
    "then , we reveal the convergence rate of sgd methods using robust losses , and provide the robustness analysis on two representative robust losses .",
    "let @xmath16 be the training data , where @xmath17 denotes the @xmath18th instance and @xmath19 denotes its binary label .",
    "the basic support vector machine model for classification is represented as @xmath20 where @xmath21 is the primal variable .",
    "specifically , @xmath22 where @xmath23 is the regularization parameter , @xmath24 is the regularizer and @xmath25 is a loss function .",
    "based on restricted strong convexity ( rsc ) and restricted smoothness ( rsm ) @xcite , we propose two extended definitions .",
    "we use @xmath26 to denote the euclidean norm , and @xmath27 to denote the @xmath28 dimensional euclidean ball of radius @xmath29 centered at local minima @xmath30 .",
    "and we assume that function @xmath31 and @xmath32 are continuously differentiable .",
    "[ arsc ] * ( augmented restricted strong convexity ( arsc ) ) * if there exists a constant @xmath33 such that for any @xmath34 , we have @xmath35    _ then @xmath31 satisfies augmented restricted strong convexity .",
    "_ +    [ arsm ] * ( augmented restricted smoothness ( arsm ) ) * if there exists a constant @xmath36 such that for any @xmath37 and @xmath34 , we have @xmath38    _ then @xmath32 satisfies augmented restricted smoothness . _ +      we first present the motivation and definition of a family of robust losses .",
    "take support vector machines ( svm ) with convex hinge loss as an example .",
    "sgd methods are commonly used to optimize the svm model for large - scale learning . however ,",
    "if data points with noisy labels deviate significantly from the hyperplane greatly , these mislabeled data points can be equally viewed as outliers .",
    "these outliers will severely mislead the update of the primal variable in sgd methods .",
    "therefore , it is intuitive to design a loss function with a threshold , which truncates the value that exceeds the threshold .",
    "inspired by ramp loss @xcite , we consider whether we can design a family of bounded , locally strongly - convex and smooth losses .",
    "if we combine this new loss with strongly - convex regularizer , the objective then satisfies the arsc ( i.e. , def .",
    "[ arsc ] ) and arsm ( i.e. , def .",
    "[ arsm ] ) simultaneously . here",
    ", we define a family of robust losses @xmath39 for sgd methods , where @xmath9 is the variable of loss function in the x - axis of figure [ robust - losses ] .",
    "[ def3 ] a loss function @xmath39 is robust for sgd methods if it simultaneously meets the following conditions :    1 .",
    "[ con1 ] upper bound condition - it should be bounded such that @xmath40 .",
    "[ con2 ] locally @xmath23-strongly convex condition - it should be locally @xmath23-strongly convex if there exists a constant @xmath41 such that @xmath42 is convex when @xmath43 , where @xmath44 denotes the 1 dimensional euclidean ball of radius @xmath45 centered at local minima @xmath46 .",
    "[ con3 ] smoothly decreasing condition - it should be monotonically decreasing and continuously differentiable .",
    "we explain three conditions in definition [ def3 ] .",
    "[ con1 ] ) since the upper bound can be equally viewed as the threshold , it is natural that the negative effects introduced by outliers are removed by the upper bound .",
    "[ con2 ] ) the loss function should be locally @xmath23-strongly convex . if the loss function is locally @xmath23-strongly convex and the regularizer is globally @xmath23-strongly convex ( e.g. , @xmath47 ) , the objective @xmath48 is locally strongly - convex .",
    "then , objective @xmath48 satisfies the arsc .",
    "[ con3 ] ) if the loss function is monotonically decreasing , we reasonably assume that the objective is non - increasing around some local minima , which is convenient to prove the convergence rate .",
    "if the loss function is differentiable at every point , @xmath49 satisfies the arsm when @xmath47 is used .",
    "then a family of robust losses for sgd methods can be acquired under these conditions . here , we propose two representative robust losses that perfectly satisfy the above three conditions . both of them are presented in figure [ robust - losses ] and employed through the whole paper .",
    "the first one is the smooth ramp loss , which is the smooth version of ramp loss .",
    "if we smooth the ramp loss around @xmath50 and around 1 , it is much easier to optimize and satisfy the arsm .",
    "therefore , we employ reversed sigmoid function to represent the smooth ramp loss .",
    "@xmath51 where we set the @xmath50 of ramp loss , then the parameters @xmath52 and @xmath53 of smooth ramp loss are determined by minimizing the difference between smooth ramp loss and ramp loss .",
    "the second one is the reversed gompertz loss , which is a special case of the gompertz function @xcite and we reverse the gompertz function by the y - axis . @xmath54 where the curve of this loss is controlled by parameter @xmath55 .",
    "the aforementioned losses are integrated into the svm model and sgd methods are employed to update the primal variable @xmath14 .    by employing two above robust losses",
    ", we finally summarize the robust sgd algorithm - stochastic gradient descent with robust losses in algorithm [ sgdrl ] . specifically , the generalized algorithm consists of two special cases . for stochastic gradient descent with smooth ramp loss ,",
    "the algorithm employs set  i and update i. for stochastic gradient descent with reversed gompertz loss , the algorithm employs set ii and update ii . in practical implementations , we often choose option a and also provide averaging option b.    @xmath56    * set : * @xmath57      when we apply sgd methods to svm model with proposed robust losses , it converges into the qualified local minima . according to the explanation about the three conditions in section [ losses and algorithm ] , the objective @xmath48 satisfies the arsc and @xmath49 satisfies the arsm .",
    "based on the arsc and arsm , we can analyze the convergence rate of sgd methods using robust losses .",
    "we use @xmath58 $ ] to denote the .",
    "[ convergence ] consider that @xmath48 satisfies augmented restricted strong convexity and @xmath49 satisfies augmented restricted smoothness .",
    "define @xmath30 as a local minima and @xmath59 as the parameter of augmented restricted smoothness .",
    "assume that learning rate @xmath60 is sufficient to let @xmath61 be a non - increasing update .",
    "after @xmath62 iterations , we have @xmath63}{(2\\eta - 12\\eta^2\\beta)\\cdot t}\\ ] ]      now we theoretically analyze the robustness of two representative robust losses .",
    "assume that @xmath64 is a random subset of the training data @xmath65 and @xmath6 is the decision function , according to the representer theorem , @xmath66 , where @xmath67 , @xmath68 and @xmath69 .",
    "@xmath41 is a regularizer parameter , @xmath70 is a mercer kernel and @xmath71 is a reproducing kernel hilbert space ( rkhs ) . for a family of robust losses",
    "@xmath39 , we define two functions @xmath72 and @xmath73 such that @xmath74 and @xmath75 . according to the inference in supplementary materials ,",
    "we define the weighted parameter @xmath76 as an important parameter that affects the update of the dual variable in sgd methods , where @xmath77 .",
    "moreover , @xmath76 is related to @xmath78 for l2-svm .",
    "we define @xmath79 for smooth ramp loss .",
    "therefore , the results of robustness analysis are provided in theorem [ robustness ] . due to the limit of space ,",
    "the detailed proof of theorem  [ robustness ] is provided in the supplementary materials .",
    "[ robustness ] assume that an instance @xmath78 is annotated with noisy label @xmath80 , which means @xmath81 .",
    "its corresponding weighted parameter @xmath76 for smooth ramp loss with @xmath82 is @xmath83 for reversed gompertz loss with @xmath84 is @xmath85 if @xmath86 increases , which means @xmath78 with noisy label @xmath80 becomes an outlier , then both @xmath76 will definitely decrease .",
    "it indicates that the proposed robust losses do reduce the negative effects introduced by noisy labels .",
    "in this section , we mainly perform experiments on noisy datasets to verify the convergence and robustness of sgd methods with two representative robust losses .",
    "the datasets @xcite range from small to large scale . for convenience ,",
    "we sgd with smooth ramp loss as sgd(sramp ) and sgd with reversed gompertz loss as sgd(rgomp ) respectively .",
    "all experimental datasets come from the libsvm datasets webpage @xcite .",
    "the statistics of the datasets are summarized in table [ datasets ] . among them , real - sim , covtype , mnist38 and ijcnn1 are manually split into the training set and testing set by about @xmath87 .",
    "we normalize the data by scaling each feature to @xmath88 $ ] . to generate the datasets with noisy labels ,",
    "we follow the settings in @xcite .",
    "specifically , we proportionally flip the class label of training data .",
    "for example , we randomly flip 20% of data labels from , and assume that the data has @xmath89 of noisy labels .",
    "we then repeat the same process to produce 40% and 60% of noisy labels on all datasets .    in the experiments ,",
    "the baseline methods are classified into two categories .",
    "the first category consists of sgd methods with different losses ranging from convex losses to robust nonconvex losses , which can verify the convergence and robustness of sgd methods with two representative losses for noisy labels .",
    "for example , we choose sgd with logistic loss ( sgd(log ) ) , hinge loss ( sgd(hinge ) ) and ramp loss ( sgd(ramp ) ) .",
    "we also choose asgd @xcite with logistic loss ( asgd(log ) ) and pegasos @xcite as baseline methods .",
    "for the second category , we compare proposed methods with liblinear @xcite ( we abbreviate l2-regularized l2-loss svm primal solution as libprimal and dual solution as libdual ) due to its wide popularity in large - scale machine learning .",
    "all the methods are implemented in c++ .",
    "experiments are performed on a computer with a 3.20ghz inter cpu and 8 gb main memory running on a windows 7 .",
    "the regularization parameter @xmath23 is chosen by @xmath90-fold cross validation for all methods in the range of \\{@xmath91}. for sgd methods with different losses , the number of epochs is normally set to @xmath92 for convergence comparison and the primal variable @xmath14 is initialized to @xmath93 . for liblinear , we set the bias @xmath94 to @xmath95 and the stopping tolerance @xmath96 to @xmath97 for primal solution and @xmath98 for dual solution by default . for pegasos ,",
    "the number of epochs for convergence is set to @xmath99 by default and the block size @xmath100 is set to @xmath95 for training efficiency . for sgd(sramp ) , the parameter @xmath50 is chosen by @xmath90-fold cross validation in the range of @xmath101 $ ] according to real - world datasets .",
    "therefore , the parameter @xmath82 is optimized to @xmath102 , @xmath103 or @xmath104 . for sgd(rgomp ) , the parameter @xmath84 is randomly fixed to  @xmath105 .",
    "all the experiments are repeated ten times and the results are averaged over the @xmath90 trials .",
    "methods are indicated by -in table  [ ter - table ] due to running out of memory .",
    "methods are not reported in figures [ testing error rate ] and [ variance ] due to running out of memory or too long training time .",
    ".datasets used in the experiments . [ cols=\"^,^,^,^\",options=\"header \" , ]     finally , we verify the robustness of sgd methods with two representative losses for noisy labels .",
    "figures [ testing error rate ] and [ variance ] respectively report testing error rate and variance with varying percentages of noisy labels . from figures",
    "[ testing error rate ] and [ variance ] , we have the following observations .",
    "( a ) on all datasets , sgd(sramp ) and sgd(rgomp ) obviously outperform the other baseline methods in testing error rate beyond @xmath106 of noisy labels . between @xmath107 to @xmath106 , sgd(sramp ) and",
    "sgd(rgomp ) still have comparative advantages .",
    "in particular , for a high - dimensional dataset real - sim , the advantage of sgd(sramp ) and sgd(rgomp ) is extremely obvious in the whole range of the x - axis .",
    "( b ) meanwhile , we notice that the variance of testing error rate for baseline methods ( e.g. , pegasos ) gradually increases with the growing percentage of noisy labels , but the variance of testing error rate for sgd(sramp ) and sgd(rgomp ) remains at the lowest level in the most cases .",
    "therefore , the robustness of sgd(sramp ) and sgd(rgomp ) have been validated by their testing error rate and variance .    in the most cases , the proposed sgd(sramp ) and sgd(rgomp ) outperform other baseline methods not only on datasets with varying percentage of noisy labels but also on clean datasets .",
    "for example , table [ ter - table ] demonstrates that in terms of the testing error rate with the standard deviation , sgd(sramp ) and sgd(rgomp ) outperform other baseline methods on ijcnn1 , real - sim , covtype and susy datasets without noisy labels .",
    "this paper studies sgd methods with a family of robust losses for the label noise problem . for convenience ,",
    "we mainly introduce two representative robust losses including smooth ramp loss and reversed gompertz loss .",
    "our theoretical analysis not only reveals that the convergence rate is @xmath0 for sgd methods using robust losses , but also proves the robustness of two representative robust losses .",
    "comprehensive experimental results show that , on real - world datasets with varying percentages of noisy labels , sgd methods using our proposed losses are robust enough to reduce negative effects caused by noisy labels with fast convergence . in the future , we will extend our proposed robust losses to improve the performance of sgd methods for regression problems with noisy labels .    4 cotter , a. , shamir , o. , srebro , n. , sridharan , k. : better mini - batch algorithms via accelerated gradient methods . in : advances in neural information processing systems ( nips ) ,",
    "( 2011 ) mitliagkas , i. , caramanis , c. , jain , p. : memory limited , streaming pca . in : advances in neural information processing systems ( nips ) , pp .",
    "( 2013 ) nesterov , y. : efficiency of coordinate descent methods on huge - scale optimization problems .",
    "siam journal on optimization ( siam ) 22(2 ) , 341362 ( 2012 ) agarwal , a. , foster , d. p. , hsu , d. , kakade , s. m. , rakhlin , a. : stochastic convex optimization with bandit feedback .",
    "siam journal on optimization ( siam ) 23(1 ) , 213240 ( 2013 ) hsieh , c .- j .",
    ", yu , h .- f . , dhillon , i. s. : passcode : parallel asynchronous stochastic dual co - ordinate descent . in : proceedings of the 32th international conference of machine learning ( icml ) , ( 2015 ) bottou , l. : large - scale machine learning with stochastic gradient descent . in : proceedings of the 19th international conference on computational statistics ( compstat ) , pp",
    ". 177187 .",
    "( 2010 ) le , q. v. , ngiam , j. , coates , a. , lahiri , a. , prochnow , b. , ng , a. y. : on optimization methods for deep learning . in : proceedings of the 28th international conference on machine learning ( icml ) , pp",
    ". 265272 .",
    "( 2011 ) shalev - shwartz , s. , singer , y. , srebro , n. , cotter , a. : pegasos : primal estimated sub - gradient solver for svm . mathematical programming 127(1 ) , 330 ( 2011 ) yan , y. , rosales , r. , fung , g. , schmidt , m. , hermosillo , g. , bogoni , l. , moy , l. , dy , j .-",
    "g . : modeling annotator expertise : learning when everybody knows a bit of something . in : proceedings of the 13th international conference on artificial intelligence and statistics ( aistats ) , pp",
    ". 932939 .",
    "( 2010 ) bi , w. , wang , l .- w . ,",
    "kwok , j .-",
    "tu , z .- w . : learning to predict from crowdsourced data . in : proceedings of the 13th conference on uncertainty in artificial intelligence ( uai ) , pp . 8291 .",
    "( 2014 ) collobert , r. , sinz , f. , weston , j. , bottou , l. : trading convexity for scalability . in : proceedings of the 23rd international conference on machine learning ( icml ) ,",
    "pp . 201208 .",
    "( 2006 ) yu , y .-",
    "l . , yang , m. , xu , l .- l , white , m. , schuurmans , d. : relaxed clipping : a global training method for robust regression and classification . in : advances in neural information processing systems ( nips ) , pp . 25322540 .",
    "( 2010 ) xu , w. : towards optimal one pass large scale learning with averaged stochastic gradient descent .",
    "arxiv preprint arxiv:1107.2490 .",
    "( 2011 ) ghadimi , s. , lan , g .- h . : stochastic first - and zeroth - order methods for nonconvex stochastic programming .",
    "siam journal on optimization ( siam ) 23(4 ) , 23412368 ( 2013 ) ghadimi , s. , lan , g .- h .",
    ": accelerated gradient methods for nonconvex nonlinear and stochastic programming .",
    "mathematical programming , 141 ( 2015 ) wang , l. , jia , h .-",
    "d . , li , j. : training robust support vector machine with smooth ramp loss in the primal space .",
    "neurocomputing , 71(13 ) , 30203025 ( 2008 ) reed , s. , lee , h. , anguelov , d. , szegedy , c. , erhan , d. , rabinovich , a. : training deep neural networks on noisy labels with bootstrapping .",
    "arxiv preprint arxiv:1412.6596 .",
    "( 2014 ) sukhbaatar , s. , bruna , j. , paluri , m. , bourdev , l. , fergus , r. : training convolution networks with noisy labels . in : proceedings of the international conference on learning representations ( iclr ) .",
    "( 2015 ) natarajan , n. , dhillon , i. s. , ravikumar , p. k. , tewari , a. : learning with noisy labels . in : advances in neural information processing systems ( nips ) ,",
    ". 11961204 .",
    "( 2013 ) bartlett , p. l. , jordan , m. i. , mcauliffe , j. d. : convexity , classification , and risk bounds .",
    "journal of the american statistical association 101(473 ) , 138156 ( 2006 ) agarwal , a. , negahban , s. , wainwright , m. j. : fast global convergence of gradient methods for high - dimensional statistical recovery .",
    "the annals of statistics 40(5 ) , 24522482 ( 2012 ) loh , p .-",
    "wainwright , m. j. : regularized m - estimators with nonconvexity : statistical and algorithmic theory for local optima .",
    "journal of machine learning research ( jmlr ) 16 , 559616 ( 2015 ) garg , m. l. , rao , r. b. , redmond , c. k. : maximum - likelihood estimation of the parameters of the gompertz survival function . journal of the royal statistical society .",
    "series c ( applied statistics ) 19(2 ) , 152159 ( 1970 ) chang , y .- w . ,",
    "hsieh , c .- j . , chang , k .- w . ,",
    "ringgaard , m. , lin , c .- j . :",
    "training and testing low - degree polynomial data mappings via linear svm .",
    "journal of machine learning research ( jmlr ) 11 , 14711490 ( 2010 ) chang , c .- c . ,",
    "lin , c .- j .",
    ": libsvm : a library for support vector machines .",
    "acm transactions on intelligent systems and technology 2(3 ) , 27 ( 2011 ) yang , t .- b .",
    ", mahdavi , m. , jin , r. , zhang , l .- j . , zhou , y. : multiple kernel learning from noisy labels by stochastic programming . in : proceedings of the 29th international conference on machine learning ( icml ) , pp .",
    "( 2012 ) fan , r .- e .",
    ", chang k .- w . ,",
    "hsieh , c .- j . ,",
    "wang , x .-",
    "r . , lin , c .- j .",
    ": liblinear : a library for large linear classification .",
    "journal of machine learning research ( jmlr ) 9 , 18711874 ( 2008 )"
  ],
  "abstract_text": [
    "<S> the convergence of stochastic gradient descent ( sgd ) using convex loss functions has been widely studied . </S>",
    "<S> however , vanilla sgd methods using convex losses can not perform well with noisy labels , which adversely affect the update of the primal variable in sgd methods . </S>",
    "<S> unfortunately , noisy labels are ubiquitous in real world applications such as crowdsourcing . to handle noisy labels , in this paper , we present a family of robust losses for sgd methods . </S>",
    "<S> by employing our robust losses , sgd methods successfully reduce negative effects caused by noisy labels on each update of the primal variable . </S>",
    "<S> we not only reveal that the convergence rate is @xmath0 for sgd methods using robust losses , but also provide the robustness analysis on two representative robust losses . </S>",
    "<S> comprehensive experimental results on six real - world datasets show that sgd methods using robust losses are obviously more robust than other baseline methods in most situations with fast convergence . </S>"
  ]
}