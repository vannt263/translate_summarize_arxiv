{
  "article_text": [
    "the aim of the present paper is to review the subject of stochastic approximation ( sa ) , along the way highlighting some recent statistical applications , and to explore its relationship with a recent algorithm  @xcite for estimating a mixing distribution .",
    "sa was introduced in  @xcite as an algorithmic method for finding the root of a function @xmath0 when only noisy observations on @xmath0 are available .",
    "it has since developed into an important area of systems control and optimization , with numerous applications in statistics . in section  [ s : sa ] we give a brief introduction to the sa algorithm and review three recent and innovative statistical applications .",
    "the first two @xcite strengthen the em and metropolis algorithms , respectively , and the third is a versatile monte carlo integration method , called stochastic approximation monte carlo ( samc )  @xcite , which can be applied in a variety of statistical problems . we demonstrate that combining samc with the _ energy  temperature duality _ @xcite provides a method for estimating the normalizing constant of a density .",
    "we then state a theorem providing sufficient conditions for almost sure convergence of a sa algorithm , which is used in section  [ s : mixingestimate ] to study the convergence properties of a mixing distribution estimate . for this purpose ,",
    "the necessary stability theory for ordinary differential equations ( odes ) is developed .",
    "many statistical problems involve modeling with latent , or unobserved , random variables , for example , cluster analysis  @xcite and multiple testing or estimation with high - dimensional data @xcite .",
    "the distribution of the manifest , or observed , random variables then becomes a mixture of the form @xmath1 where @xmath2 is the latent variable or parameter , and @xmath3 is an unknown mixing density with respect to the measure @xmath4 on @xmath5 .",
    "estimation of @xmath3 plays a fundamental role in many inference problems , such as an empirical bayes approach to multiple testing .    for the deconvolution problem , when @xmath6 in is of the form @xmath7 , asymptotic results for estimates of @xmath3 , including optimal rates of convergence , are known  @xcite . a nonparametric bayes approach to gaussian deconvolution",
    "is discussed in  @xcite . for estimating @xmath8 , a bayesian might assume an a priori distribution on @xmath3 , inducing a prior on @xmath9 via the map @xmath10 .",
    "consistency of the resulting estimate of @xmath9 is considered in  @xcite .    in section  [ s :",
    "mixingestimate ] , we describe a recursive algorithm of newton et al .",
    "@xcite for estimating the mixing density @xmath3 .",
    "this estimate is significantly faster to compute than the popular nonparametric bayes estimate based on a dirichlet process prior .",
    "in fact , the original motivation  @xcite for the algorithm was to approximate the computationally expensive bayes estimate .",
    "the relative efficiency of the recursive algorithm compared to mcmc methods used to compute the bayes estimate , coupled with the similarity of the resulting estimates , led quintana and newton  @xcite to suggest the former be used for _ bayesian exploratory data analysis_.    while newton s algorithm performs well in examples and simulations ( see @xcite and section  [ ss : examples ] ) , very little is known about its large - sample properties . a rather difficult proof of consistency , based on an approximate martingale representation of the kullback ",
    "leibler divergence , is given by ghosh and tokdar  @xcite when @xmath5 is finite ; see section  [ ss : newton - review ] . in section  [ ss : newtonconvsa ] , we show that newton s algorithm can be expressed as a stochastic approximation and results presented in section  [ ss : sa - thm ] are used to prove a stronger consistency theorem than in  @xcite for the case of finite  @xmath5 , where the kullback ",
    "leibler divergence serves as the lyapunov function .",
    "the numerical investigations in section  [ ss : examples ] consider two important cases when @xmath5 is finite , namely , when @xmath3  is strictly positive on @xmath5 and when @xmath11 for some @xmath12 . in the former case ,",
    "our calculations show that newton s estimate is superior , in terms of accuracy and computational efficiency , to both the nonparametric mle and the bayes estimate . for the latter case , when only a superset of the support of @xmath3 is known , the story is completely different .",
    "while newton s estimate remains considerably faster than the others , it is not nearly as accurate .",
    "we also consider the problem where the sampling density @xmath6 of is of the form @xmath13 , where @xmath3 is a mixing density or prior for @xmath14 , and @xmath15 is an additional unknown parameter .",
    "newton s algorithm is unable to handle unknown @xmath15 , and we propose a modified algorithm , called n@xmath16p , capable of recursively estimating both @xmath3 and @xmath15 .",
    "we express this algorithm as a general sa and prove consistency under suitable conditions .",
    "in section [ s : discuss ] we briefly discuss some additional theoretical and practical issues concerning newton s recursive algorithm and the n@xmath16p .",
    "consider the problem of finding the unique root @xmath15 of a function @xmath17 .",
    "if @xmath17 can be evaluated exactly for each @xmath18 and if @xmath0 is sufficiently smooth , then various numerical methods can be employed to locate @xmath15 . a  majority of these numerical procedures , including the popular newton  raphson method , are iterative by nature , starting with an initial guess @xmath19 of @xmath15 and iteratively defining a sequence @xmath20 that converges to @xmath15 as @xmath21 .",
    "now consider the situation where only noisy observations on @xmath17 are available ; that is , for any input @xmath18 one observes @xmath22 , where @xmath23 is a zero - mean random error .",
    "this problem arises in situations where @xmath17 denotes the expected value of the response when the experiment is run at setting @xmath18 .",
    "unfortunately , standard deterministic methods can not be used in this problem .    in their seminal paper , robbins and monro  @xcite proposed a _ stochastic approximation _",
    "algorithm for defining a sequence of design points @xmath20 targeting the root @xmath15 of @xmath0 in this noisy case . start with an initial guess @xmath19 . at stage @xmath24 , use the state @xmath25 as the input , observe @xmath26 , and update the guess @xmath27 .",
    "more precisely , the robbins  monro algorithm defines the sequence @xmath20 as follows : start with @xmath19 and , for @xmath24 , set @xmath28 \\\\[-8pt ] & = & x_{n-1 } + w_n \\{h(x_{n-1 } ) + \\varepsilon_n \\ } , \\nonumber\\end{aligned}\\ ] ] where @xmath29 is a sequence of i.i.d .",
    "random variables with mean zero , and the weight sequence @xmath30 satisfies @xmath31    while the sa algorithm above works in more general situations , we can develop our intuition by looking at the special case considered in  @xcite , namely , when @xmath0 is bounded , continuous and monotone decreasing . if @xmath32 , then @xmath33 and we have @xmath34 likewise , if @xmath35 , then @xmath36 .",
    "this shows that the move @xmath37 will be in the correct direction _ on average_.    some remarks on the conditions in are in order . while @xmath38 is necessary to prove convergence , an immediate consequence of this condition",
    "is that @xmath39 .",
    "clearly @xmath39 implies that the effect of the noise vanishes as @xmath40 .",
    "this , in turn , has an averaging effect on the iterates @xmath41 .",
    "on the other hand , the condition @xmath42 washes out the effect of the initial guess @xmath19 . for further details ,",
    "see @xcite .",
    "we conclude this section with three simple examples of sa to shed light on when and how the algorithm works .",
    "example  [ ex : first - ex ] , taken from @xcite , page  4 , is an important special case of the robbins  monro algorithm   which further motivates the algorithm as well as the conditions   on the sequence @xmath30 .",
    "example [ ex : t - quantile ] uses sa to find quantiles of a @xmath43-distribution , and example [ ex : eb ] illustrates a connection between sa and _ empirical bayes _ , two of robbins s greatest contributions .",
    "[ ex : first - ex]let @xmath44 be the cdf of a distribution with mean @xmath15",
    ". then estimation of @xmath15 is equivalent to solving @xmath45 where @xmath46 . if @xmath47 are i.i.d .",
    "observations from @xmath44 , then the average @xmath48 is the least squares estimate of  @xmath15 . to see that @xmath49 is actually a sa sequence ,",
    "recall the computationally efficient recursive expression for  @xmath48 : @xmath50 if we let @xmath51 , @xmath52 and @xmath53 , then is exactly of the form of , with @xmath30 satisfying .",
    "moreover , if @xmath54 , then we can write @xmath55 . with this setup , we could study the asymptotic behavior of @xmath56 using the sa analysis below ( see sections  [ ss : ode ] and [ ss : sa - thm ] ) , although the slln already guarantees @xmath57 a.s .",
    "[ ex : t - quantile]suppose we wish to find the @xmath58th quantile of the @xmath59 distribution ; that is , we want to find the solution to the equation @xmath60 , where @xmath61 is the cdf of the @xmath62 distribution . while there are numerous numerical methods available ( e.g. , newton  raphson or bijection )",
    ", we demonstrate below how sa can be used to solve this problem . making use of the well - known fact that the @xmath59 distribution is a scale - mixture of normals , we can write @xmath63 , \\quad z \\sim\\chi_\\nu^2,\\ ] ] where @xmath64 is the cdf of the @xmath65 distribution . now , for @xmath66 , the sequence @xmath67 defined as @xmath68 are noisy observations of @xmath69 .",
    "this @xmath0 is bounded , continuous and monotone decreasing so the robbins  monro theory says that the sequence @xmath20 defined as converges to the true quantile , for any initial condition @xmath19 . for illustration ,",
    "figure [ f : t - quantile ] shows the first 1000 iterations of the sequence @xmath20 for @xmath70 , @xmath71 and for three starting values @xmath72 .",
    "in example  [ ex : t - quantile ] .",
    "the dotted line is the exact 75th percentile of the @xmath73 distribution.[f : t - quantile ] ]    [ ex : eb]in section  [ s : mixingestimate ] we consider a particular recursive estimate and show that it is of the form of a general sa .",
    "it turns out that the problem there can also be expressed as an _ empirical bayes _ ( eb ) problem  @xcite . in this simple example",
    ", we demonstrate the connection between sa and eb , both of which are theories pioneered by robbins .",
    "consider the simple hierarchical model @xmath74 for @xmath75 , where the exponential rate @xmath76 is unknown .",
    "eb tries to estimate @xmath15 based on the observed data @xmath47 . here",
    "we consider a recursive estimate of @xmath15 .",
    "fix an initial guess @xmath19 of @xmath15 .",
    "assuming @xmath15 is equal to @xmath19 , the posterior mean of @xmath77 is @xmath78 , which is a good estimate of @xmath79 if @xmath19 is close to @xmath15 .",
    "iterating this procedure , we can generate a sequence @xmath80,\\ ] ] where @xmath81 is assumed to satisfy .",
    "let @xmath82 denote the quantity in brackets in and take its expectation with respect to the distribution of @xmath83 : @xmath84 then the sequence @xmath20 in is a sa targeting a solution of @xmath45 .",
    "since @xmath0 is continuous , decreasing and @xmath45 iff @xmath85 , it follows from the general theory that @xmath57 .",
    "figure [ f : eb - sa ] shows the first 250 steps of such a sequence with @xmath86 .",
    "in example  [ ex : eb ] .",
    "the dotted line is the value of @xmath15 used for data generation.[f : eb - sa ] ]    the examples above emphasize one important property that @xmath17 must satisfy , namely , that it must be easy to `` sample '' in the sense that there is a function @xmath87 and a random variable @xmath88 such that @xmath89 $ ] .",
    "another thing , which is not obvious from the examples , is that @xmath17 must have certain _ stability _ properties . in general , a sa sequence need not have a unique limit point .",
    "however , conditions can be imposed which guarantee convergence to a particular solution @xmath15 of @xmath45 , provided that @xmath15 is a _",
    "stable _ solution to the ode @xmath90 .",
    "this is discussed further in section  [ ss : ode ] .",
    "the em algorithm  @xcite has quickly become one of the most popular computational techniques for likelihood estimation in a host of standard and nonstandard statistical problems .",
    "common to all problems in which the em can be applied is a notion of `` missing data . ''",
    "consider a problem where data @xmath91 is observed and the goal is to estimate the parameter @xmath14 based on its likelihood function @xmath92 .",
    "suppose that the observed data @xmath91 is _ incomplete _ in the sense that there is a component @xmath88 which is _",
    "missing_this could be actual values which are not observed , as in the case of censored data , or it could be latent variables , as in a random effects model .",
    "let @xmath93 denote the _ complete _ data .",
    "then the likelihood function @xmath94 based on the complete data @xmath18 is related to @xmath92 according to the formula @xmath95 .",
    "the em algorithm produces a convergent sequence of estimates by iteratively filling in the missing data @xmath88 in the e - step and then maximizing the simpler complete - data likelihood function @xmath94 in the m - step .",
    "the e - step is performed by sampling the @xmath96-values from the density @xmath97 which is the predictive density of @xmath88 , given @xmath91 and @xmath14 .",
    "it is often the case that at least one of the e - step and m - step is computationally difficult , and many variations of the em have been introduced to improve the rate of convergence and/or simplify the computations . in the case where the e - step can not be done analytically , wei and tanner  @xcite suggest replacing the expectation in the e - step with a monte carlo integration .",
    "the resulting mcem algorithm comes with its own challenges , however ; for example , simulating the missing data @xmath98 , for @xmath99 , from @xmath100 could be quite expensive .",
    "delyon , lavielle and moulines @xcite propose , in the case where integration in the e - step is difficult or intractable , an alternative to the mcem using sa .    at step @xmath101 , simulate themissing data @xmath98 from the posterior distribution@xmath102 , @xmath99 .",
    "update @xmath103 using @xmath104 where @xmath30 is a sequence as in .",
    "then choose @xmath105 such that @xmath106 for all @xmath2 .",
    "compared to the mcem , the saem algorithm s use of the simulated data @xmath98 is much more efficient . at each iteration , the mcem simulates a new set of missing data from the posterior distribution and forgets the simulated data from the previous iteration . on the other hand , note that the inclusion of @xmath107 in the saem update @xmath108 implies _ all _",
    "the simulated data points contribute .",
    "it is pointed out in  @xcite that the saem performs strikingly better than the mcem in problems where maximization is much cheaper than simulation .",
    "delyon , lavielle and moulines @xcite show , using general sa results , that for a broad class of complete - data likelihoods @xmath94 and under standard regularity conditions , the saem sequence @xmath109 converges a.s . to the set of stationary points @xmath110 of the incomplete - data likelihood",
    "moreover , they prove that the only attractive stationary points are local maxima ; that is , saddle points of @xmath92 areavoided  a.s .",
    "a random walk metropolis ( rwm ) algorithm is a specific mcmc method that can be designed to sample from almost any distribution @xmath111 . in this particular case ,",
    "the proposal is @xmath112 , where @xmath113 is a symmetric density .",
    "a popular choice of @xmath113 is a @xmath114 density .",
    "it is well known that the convergence properties of monte carlo averages depend on the choice of the proposal covariance matrix @xmath115 , in the sense that it affects the rate at which the generated stochastic process explores the support of @xmath111 .",
    "trial and error methods for choosing @xmath115 can be difficult and time consuming .",
    "one possible solution would be to use the history of the process to suitably tune the proposal .",
    "these so - called adaptive algorithms come with their own difficulties , however .",
    "in particular , making use of the history destroys the markov property of the process so nonstandard results are needed in a convergence analysis .",
    "for instance , when the state space contains an atom , gilks , roberts and sahu @xcite propose an adaptive algorithm that suitably updates the proposal density only when the process returns to the atom .",
    "the resulting process is not markov , but ergodicity is proved using a regeneration argument  @xcite .    an adaptive metropolis ( am ) algorithm is presented by haario , saksman and tamminen @xcite , which uses previously visited states to update the proposal covariance matrix @xmath115 .",
    "introduce a mean @xmath4 and set @xmath116 .",
    "let @xmath30 be a deterministic sequence as in .",
    "fix a starting point @xmath117 and initial estimates @xmath118 and @xmath119 . at iteration",
    "@xmath24 draw @xmath120 from @xmath121 and set @xmath122    note that if @xmath52 , then @xmath123 and @xmath124 are the sample mean and covariance matrix , respectively , of the observations @xmath125 .",
    "the constant @xmath126 in the am is fixed and depends only on the dimension @xmath127 of the support of @xmath111 .",
    "a choice of @xmath126 which is , in some sense , optimal is @xmath128 ( @xcite , page  316 ) .",
    "it is pointed out in @xcite that the am has the advantage of starting the adaptation from the very beginning .",
    "this property allows the am algorithm to search the support of @xmath111 more effectively _ earlier _ than other adaptive algorithms .",
    "note that for the algorithm of @xcite mentioned above , the adaptation does not begin until the atom is first reached ; although the renewal times are a.s .",
    "finite , they typically have no finite upper bound .",
    "it is shown in  @xcite that , under certain conditions , the stationary distribution of the stochastic process @xmath129 is the target @xmath111 , the chain is ergodic ( even though it is no longer markovian ) , and there is almost sure convergence to @xmath130 , the mean and covariance of the target @xmath111 .",
    "this implies that , as @xmath21 , the proposal distributions in the am algorithm will be close to the `` optimal '' choice .",
    "if @xmath131 , then the am is a general sa algorithm with @xmath132 , and andrieu , moulines and priouret @xcite extend the work in  @xcite via new sa stability results .",
    "let @xmath133 be a finite or compact space with a dominating measure @xmath134 .",
    "let @xmath135 be a probability density on @xmath133 with respect to @xmath134 with possibly unknown normalizing constant @xmath136 .",
    "we wish to estimate @xmath137 , where @xmath3 is some function depending on @xmath138 or @xmath139 .",
    "for example , suppose @xmath140 is a prior and @xmath141 is the conditional density of @xmath142 given @xmath18 .",
    "then @xmath143 is the unnormalized posterior density of @xmath18 and its integral , the marginal density of @xmath142 , is needed to compute a bayes factor .",
    "the following stochastic approximation montecarlo ( samc ) method is introduced in  @xcite .",
    "let @xmath144 be a partition of @xmath133 and let @xmath145 for @xmath146 . take @xmath147 as an initial guess , and let @xmath148 be the estimate of @xmath149 at iteration @xmath150 . for notational convenience ,",
    "write @xmath151 the probability vector @xmath152 will denote the _ desired _ sampling frequency of the @xmath153 s ; that is , @xmath154  is the proportion of time we would like the chain to spend in @xmath153 .",
    "the choice of @xmath111 is flexible and does not depend on the particular partition @xmath155 .    starting with initial estimate @xmath156 , for @xmath157",
    "simulate a sample @xmath158 using a rwm algorithm with target distribution @xmath159 then set @xmath160 , where the deterministic sequence @xmath30 is as in , and @xmath161 .",
    "the normalizing constant in is generally unknown and difficult to compute .",
    "however , @xmath100 is only used at the rwm step where it is only required that the target density be known up to a proportionality constant .",
    "it turns out that , in the case where no @xmath153 are empty , the observed sampling frequency @xmath162 of @xmath153 converges to @xmath154 .",
    "this shows that @xmath162 is independent of its probability @xmath163 .",
    "consequently , the resulting chain will not get stuck in regions of high probability , as a standard metropolis chain might .",
    "the sequence @xmath109 is a general stochastic approximation and , using the convergence results of  @xcite , liang , liu and carroll @xcite show that if no @xmath153 is empty and suitable conditions are met , then @xmath164 for @xmath165 as @xmath21 , for some arbitrary constant  @xmath166 .",
    "liang , liu and carroll @xcite point out a _ lack of identifiability _ in the limit ( [ e : samc - limit ] ) ; that is , @xmath166 can not be determined from @xmath109 alone .",
    "additional information is required , such as @xmath167 for each  @xmath101 and for some known constant @xmath126 .",
    "in example  [ ex : samc ] , we apply samc to estimate the partition function in the one - dimensional ising model . in this simple situation , a closed - form expression is available , which we can use as a baseline for assessing the performance of the samc estimate .",
    "[ ex : samc]consider a one - dimensional ising model , which assumes that each of the @xmath127 particles in a system has positive or negative spin .",
    "the gibbs distribution on @xmath168 has density ( with respect to counting measure @xmath134 ) @xmath169 where @xmath170 is the temperature , and @xmath171 is the energy function defined , in this case , as @xmath172 .",
    "the partition function @xmath173 is of particular interest to physicists : the thermodynamic limit @xmath174 is used to study phase transitions  @xcite . in this simple case ,",
    "a closed - form expression for @xmath173 is available .",
    "there are other more complex systems , however , where no analytical solution is available and @xmath175 is too large to allow for nave calculation of @xmath173 .    our jumping - off point is the _ energy  temperature duality _",
    "@xcite @xmath176 , where @xmath177 is the density of states",
    ". we will apply samc to first estimate @xmath178 and then estimate @xmath173 with a plug - in : @xmath179 note here that a single estimate of @xmath180 can be used to estimate the partition function for any @xmath170 , eliminating the need for simulations at multiple temperatures .",
    "furthermore , @xmath181 is known so , by imposing this condition on the estimate @xmath182 we do not fall victim to the lack of identifiability mentioned above .",
    "figure  [ f : samc - ising ] shows the true partition function @xmath183 for @xmath184 as well as the samc estimate @xmath185 as a function of @xmath186 $ ] , on the log - scale , based on @xmath187 iterations .",
    "clearly , @xmath188 performs quite well in this example , particularly for large @xmath170 .",
    "( gray ) and samc estimate @xmath189 ( black ) in example [ ex : samc].[f : samc - ising ] ]      the asymptotic theory of odes plays an important role in the convergence analysis of a sa algorithm . after showing the connection between sa and odes",
    ", we briefly review some of the ode theory that is necessary in the sequel .",
    "recall the general sa algorithm in given by @xmath190 .",
    "assume there is a measurable function @xmath0 such that @xmath191 $ ] and rewrite this algorithm as @xmath192 define @xmath193 .",
    "then @xmath194 is a zero - mean _ martingale _ sequence and , under suitable conditions , the martingale convergence theorem guarantees that @xmath195 becomes negligible as @xmath21 , leaving us with @xmath196 but this latter `` mean trajectory '' is deterministic and essentially a finite difference equation with small step sizes . rearranging the terms gives us @xmath197 which , for large @xmath101 , can be approximated by the ode @xmath90 .",
    "it is for this reason that the study of sa algorithms is related to the asymptotic properties of solutions to odes .",
    "consider a general autonomous ode @xmath90 , where @xmath198 is a bounded and continuous , possibly nonlinear , function .",
    "a solution @xmath199 of the ode is a trajectory in @xmath200 with a given initial condition @xmath201 . unfortunately , in many cases , a closed - form expression for a solution @xmath199 is not available .",
    "for that reason , other methods are necessary for studying these solutions and , in particular , their properties as @xmath202 .",
    "imagine a physical system , such as an orbiting celestial body , whose state is being governed by the ode @xmath90 with initial condition @xmath203 .",
    "then , loosely speaking , the system is stable if choosing an alternative initial condition @xmath204 in a neighborhood of @xmath19 has little effect on the asymptotic properties of the resulting solution @xmath199 .",
    "the following definition makes this more precise .",
    "a point @xmath205 is said to be _ locally stable _ for @xmath90 if for each @xmath206 there is a @xmath207 such that if @xmath208 , then @xmath209 for all @xmath210 .",
    "if @xmath15 is locally stable and @xmath211 as @xmath202 , then @xmath15 is _ locally asymptotically stable_. if this convergence holds for all initial conditions @xmath201 , then the asymptotic stability is said to be _",
    "global_.    points @xmath15 for which stability is of interest are _ equilibrium points _ of @xmath90 .",
    "any point @xmath15 such that @xmath212 is called an equilibrium point , since the constant solution @xmath213 satisfies @xmath90 .",
    "[ ex : linearstability]let @xmath214 , where @xmath215 is a fixed @xmath216 matrix . for an initial condition @xmath203 , we can write an explicit formula for the particular solution : @xmath217 for @xmath218",
    ". suppose , for simplicity , that @xmath215 has a spectral decomposition @xmath219 , where @xmath220 is orthogonal and @xmath221 is a diagonal matrix of the eigenvalues @xmath222 of @xmath215",
    ". then the matrix exponential can be written as @xmath223 , where @xmath224 is diagonal with @xmath225th element @xmath226 . clearly , if @xmath227 , then @xmath228 as @xmath202 . therefore , if @xmath215 is negative definite , then the origin @xmath229 is globally asymptotically stable .    when explicit solutions are not available , proving asymptotic stability for a given equilibrium point will require a so - called _ lyapunov function",
    "_  @xcite .",
    "[ d : lyapunov]let @xmath205 be an equilibrium point of the ode @xmath90 with initial condition @xmath203 .",
    "a function @xmath230 is called a _ lyapunov function _ ( at @xmath15 ) if :    * @xmath231 has continuous first partial derivatives in a neighborhood of @xmath15 ; * @xmath232 with equality if and only if @xmath233 ; * the time derivative of @xmath231 along the path @xmath199 , defined as @xmath234 , is @xmath235 .",
    "a lyapunov function is said to be _ strong _ if @xmath236 implies @xmath233 .",
    "lyapunov functions are a generalization of the potential energy of a system , such as a swinging pendulum , and lyapunov s theory gives a formal extension of the stability principles of such a system .",
    "theorem  [ t : lyapunov1 ] is very powerful because it does not require an explicit formula for the solution .",
    "see  @xcite for a proof and various extensions of the lyapunov theory .",
    "[ t : lyapunov1]if there exists a ( strong ) lyapunov function in a neighborhood of an equilibrium point @xmath15 of @xmath90 , then @xmath15 is ( asymptotically ) stable .",
    "there is no general recipe for constructing a lyapunov function .",
    "in one important special case , however , a candidate lyapunov function is easy to find .",
    "suppose @xmath237 , for some positive definite , sufficiently smooth function @xmath238 .",
    "then @xmath239 is a lyapunov function since @xmath240 .",
    "[ ex : linearstability2]consider again the linear system @xmath214 from example [ ex : linearstability ] , where @xmath215 is a @xmath216 negative definite matrix . here",
    "we will derive asymptotic stability by finding a lyapunov function and applying theorem  [ t : lyapunov1 ] . in light of the previous remark , we choose @xmath241 .",
    "then @xmath242 so @xmath231 is a strong lyapunov function for @xmath214 and the origin is asymptotically stable by theorem  [ t : lyapunov1 ] .    of interest",
    "is the stronger conclusion of _ _ global__asymptotic stability .",
    "note , however , that theorem [ t : lyapunov1 ] does not tell us how far @xmath19 can be from the equilibrium in question and still get asymptotic stability .",
    "for the results that follow , we will prove the _ global _ part directly .",
    "consider , for fixed @xmath19 and @xmath30 satisfying @xmath243 , the general sa algorithm @xmath244 where @xmath245 is compact and @xmath246 is a projection of @xmath18 onto @xmath247 .",
    "the projection is necessary when boundedness of the iterates can not be established by other means .",
    "the _ truncated _ or _ projected _ algorithm is often written in the alternative form  @xcite @xmath248 where @xmath120 is the `` minimum '' @xmath96 such that @xmath249 belongs to @xmath247 .",
    "next we state the main stochastic approximation result used in the sequel , a special case of theorem  5.2.3 in  @xcite .",
    "define the filtration sequence @xmath250 .",
    "[ t : sa]for @xmath20 in with @xmath30 satisfying , assume    1 .",
    "2 .   there exists a continuous function @xmath252 and a random vector @xmath253 such that @xmath254 a.s . for each @xmath101 .",
    "@xmath255 converges a.s .    if @xmath15 is globally asymptotically stable for @xmath90 , then @xmath57 a.s",
    "let @xmath5 and @xmath133 be the parameter space and sample space , equipped with @xmath256-finite measures @xmath4 and @xmath134 , respectively .",
    "typically , @xmath5 and @xmath133 are subsets of euclidean space and @xmath134 is lebesgue or counting measure .",
    "the measure @xmath4 varies depending on the inference problem : for estimation , @xmath4 is usually lebesgue or counting measure , but for testing , @xmath4 is often something different ( see example [ ex : genes ] ) .",
    "consider the following model for pairs of random variables @xmath257 : @xmath258 where @xmath259 is a parametric family of probability densities with respect to @xmath134 on @xmath260 and @xmath3 is a probability density with respect to @xmath4 on @xmath5 . in the present case , the variables ( parameters ) @xmath261",
    "are not observed .",
    "therefore , under model , @xmath262 are i.i.d .",
    "observations from the marginal density @xmath9 in  .",
    "we call @xmath3 the mixing density ( or prior , in the bayesian context ) and the inference problem is to estimate @xmath3 based on the data observed from @xmath9",
    ". the following example gives a very important special case of this problem  the analysis of dna microarray data .",
    "[ ex : genes]a microarray is a tool that gives researchers the ability to simultaneously investigate the effects of numerous genes on the occurrence of various diseases . not all of the genes will be _ expressed_related to the disease in question  so the problem is to identify those which are .",
    "let @xmath263 represent the expression level of the @xmath225th gene , with @xmath264 indicating the gene is not expressed .",
    "after some reduction , the data @xmath265 is a measure of @xmath263 , and the model is of the form with @xmath3 being a prior density with respect to @xmath266 . consider the multiple testing problem @xmath267 the number @xmath101 of genes under investigation is often in the thousands so , with little information about @xmath263 in @xmath265 , choosing a fixed prior @xmath3 would be problematic .",
    "on the other hand , the data contain considerable information about the prior @xmath3 so the _ empirical bayes _",
    "approach  using the data to _ estimate _ the prior  has been quite successful  @xcite .    in what follows ,",
    "we focus our attention on a particular estimate of the mixing density @xmath3 .",
    "let @xmath268 be i.i.d .",
    "observations from the mixture density @xmath9 in .",
    "newton  @xcite suggests the following algorithm for estimating @xmath3 .",
    "choose a positive density @xmath269 on @xmath5 and weights @xmath270 .",
    "then for @xmath75 , compute @xmath271 where @xmath272 , and report @xmath273 as the final estimate .    in the following subsections we establish someasymptotic properties of @xmath274 as @xmath21 and we show the results of several numerical experiments that demonstrate the finite - sample accuracy of newton s estimate   in both the discrete and continuous cases .",
    "first , a few important remarks .",
    "* the update @xmath275 in is similar to a bayes estimate based on a dirichlet process prior ( dpp ) , given the information up to , and including , time @xmath276 .",
    "that is , after observing @xmath277 , a  bayesian might model @xmath3 with a dpp @xmath278 . in this case , the posterior expectation is exactly the @xmath279 in . *",
    "because @xmath274 depends on the ordering of the data and not simply on the sufficient statistic @xmath280 , it is _ not _ a posterior quantity . *",
    "the algorithm is very fast : if one evaluates on a grid of @xmath281 points @xmath282 and calculates the integral in @xmath283 using , say , a trapezoid rule , then the computational complexity is @xmath284 .      in this section ,",
    "we give a brief review of the known convergence results for newton s estimate @xmath274 in the case of a finite parameter space .",
    "the case of a compact  @xmath5 is quite different and , until very recently  @xcite , nothing was known about the convergence of @xmath274 in such problems ; see section  [ s : discuss ] .",
    "newton  @xcite , building on the work in  @xcite , states the following convergence theorem .",
    "[ t : newton]assume the following :    1 .",
    "@xmath5 is finite and @xmath4 is counting measure",
    "@xmath6 is bounded away from @xmath285 and @xmath286 .",
    "then _ surely _ there exists a density @xmath287 on @xmath5 such that @xmath288 as @xmath21 .",
    "newton  @xcite presents a proof of theorem  [ t : newton ] based on the theory of nonhomogeneous markov chains .",
    "he proves that @xmath274 represents the @xmath101-step marginal distribution of the markov chain @xmath289 given by @xmath290 where @xmath291 has density @xmath292 .",
    "however , the claim that this markov chain admits a stationary distribution is incomplete ",
    "n2 implies the chain @xmath293 is weakly ergodic but the necessary strong ergodicity property does not follow , even when @xmath5 is finite .",
    "counterexamples are given in @xcite .",
    "ghosh and tokdar  @xcite prove consistency of @xmath274 along quite different lines . for probability densities @xmath294 and @xmath295 with respect to  @xmath4 , define the kullback  liebler ( kl ) divergence , @xmath296 the following theorem is proved in  @xcite using an approximate martingale representation of @xmath297 .",
    "[ t : ghosh - tokdar]in addition to _ n1__n3 _ , assume    1 .",
    "@xmath3 is ; that is , @xmath10 is _",
    "injective_.    then @xmath298 a.s .",
    "as @xmath21 .",
    "part of the motivation for the use of the kl divergence lies in the fact that the ratio @xmath299 has a relatively simple form .",
    "more important , however , is the lyapunov property shown in the proof theorem [ t : martin - ghosh ] .",
    "sufficient conditions for gt2 in the case of finite @xmath5 are given in , for example , @xcite .",
    "san martin and quintana  @xcite also discuss the issue of identifiability in connection with the consistency of @xmath274 .      here",
    "we show that newton s algorithm is a special case of sa .",
    "first , note that if @xmath3 is viewed as a prior density , then estimating @xmath3 is an _ empirical bayes _",
    "( eb ) problem .",
    "the ratio in is nothing but the posterior distribution of @xmath14 , given @xmath300 , and assuming that the prior  @xmath3 is equal to @xmath301 .",
    "this , in fact , is exactly the approach taken in example  [ ex : eb ] to apply sa in an eb problem .",
    "let @xmath4 be counting measure and @xmath302 .",
    "we can think of @xmath273 as a vector @xmath303 in the probability simplex @xmath304 , defined as @xmath305^d\\dvtx \\sum_{i=1}^d \\varphi^i = 1 \\biggr\\}.\\ ] ] define @xmath306 with @xmath307th component @xmath308 where @xmath309 is the marginal density on @xmath133 induced by @xmath310 . then becomes @xmath311",
    "let @xmath312 be the diagonal matrix of the sampling density values and define the mapping @xmath313 to be the conditional expectation of @xmath314 , given @xmath315 : @xmath316 \\\\[-8pt ] & = & \\int_{\\mathcal{x } } \\frac{\\pi_f(x)}{\\pi_{\\varphi}(x ) } p_x \\varphi\\,d\\nu(x ) - \\varphi , \\nonumber\\end{aligned}\\ ] ] where @xmath317 is the true mixing / prior distribution . from , it is clear that @xmath3 solves the equation @xmath318 which implies ( i ) @xmath3 is an equilibrium point of the ode @xmath319 , and ( ii ) that @xmath3 is a fixed point of the map @xmath320 newton  @xcite , page 313 , recognized the importance of this map in relation to the limit of @xmath274 .",
    "also , the use of @xmath170 in  @xcite for the @xmath321-projection problem is closely related to the sa approach taken here .",
    "we have shown that can be considered as a general sa algorithm , targeting the solution @xmath322 of the equation @xmath323 in @xmath304 .",
    "therefore , the sa results of section  [ ss : sa - thm ] can be used in the convergence analysis .",
    "the following theorem is proved in appendix  [ ss : app-2].=1    [ t : martin - ghosh]assume , , and . if @xmath324 @xmath134-a.e . for each @xmath14 , then @xmath325 a.s .",
    "[ re : extension1]removal of the boundedness condition n3 on @xmath6 in theorem  [ t : martin - ghosh ] extends the consistency result of  @xcite to many important cases , such as mixtures of normal or gamma densities .",
    "[ re : boundary]theorem  [ t : martin - ghosh ] covers the _ interior _ case ( when @xmath3 is strictly positive ) as well as the _ boundary _ case ( when @xmath326 for some @xmath225 ) .",
    "the fact that @xmath327 implies @xmath328 for all @xmath101 suggests that convergence may be slow in the boundary case .",
    "here we provide numerical illustrations comparing the performance of newton s estimate with that of its competitors .",
    "we consider a location - mixture of normals ; that is , @xmath329 is a @xmath330 density .",
    "the weights are set to be @xmath331 and the initial estimate @xmath269 is taken to be a @xmath332 density .",
    "for the bayes estimate , we assume a dirichlet process prior @xmath333 in each example .    [",
    "ex : newton - discrete]in this example , we compare newton s recursive ( nr ) estimate with the nonparametric maximum likelihood ( npml ) estimate and the nonparametric bayes ( npb ) estimate . computation of nr and npml ( using the em algorithm ) is straightforward . here , in the case of finite @xmath5 , we use sequential imputation @xcite to calculate npb .",
    "take @xmath334 $ ] , and set @xmath335 in @xmath6 .",
    "we consider two different mixing distributions on @xmath5 :    a.   @xmath336 , b.   @xmath337 .",
    "we simulate 50 data sets of size @xmath338 from the models corresponding to mixing densities i , ii and computing the three estimates for each .",
    "figure  [ f : n - discrete ] shows the resulting estimates for a randomly chosen data set from each model .",
    "notice that nr does better for model i than both npml and npb .",
    "the story is different for model ii  both npml and npb are considerably better than nr .",
    "this is further illustrated in figure  [ f : kl - boxplot ] where the kl divergence @xmath339 on @xmath340 is summarized over the 50 samples .",
    "we see that nr has a slightly smaller kl number than npml and npb for model i , but they clearly dominate nr for model ii .",
    "this discrepancy is at least partially explained by remark  [ re : boundary ] ; see section  [ s : discuss ] for further discussion .",
    "we should point out , however , that both npml and npb take significantly longer to compute than nr , about 100 times longer on average .",
    "[ ex : newton - compact]we consider a one- and a two - component mixture of beta densities on @xmath341 $ ] as the true @xmath3 :    a.   @xmath342 , b.   @xmath343 .",
    "let @xmath344 be the normal sampling variance .",
    "again , computation of nr is straightforward . to compute npb , the importance sampling algorithm in  @xcite that makes use of a collapsing of the poly urn scheme",
    "figure  [ f : n - compact ] shows a typical realization of nr and npb , based on a sample of size @xmath345 from each of the corresponding marginals .",
    "note that the bayes estimate does a rather poor job here , being much too spiky in both cases .",
    "this is mainly because the posterior for @xmath3 sits on discrete distributions . on the other hand , newton",
    "s estimate has learned the general shape of @xmath3 after only 100 iterations and results in a much better estimate than npb .",
    "furthermore , on average , the computation time for nr is again about 100 times less than that of npb .",
    "for the three estimates @xmath347 in models and in example [ ex : newton - discrete].[f : kl - boxplot ] ]    suppose that the sampling distribution on @xmath133 is parametrized not only by @xmath14 but by an additional parameter  @xmath15 .",
    "an example of this is the normal distribution with mean @xmath14 and variance @xmath348 .",
    "more specifically , we replace the sampling densities @xmath6 of section  [ s : mixingestimate ] with @xmath13 where @xmath14 is the latent variable , and @xmath15 is also unknown .",
    "newton s algorithm can not be used in this situation since @xmath14 does not fully specify the sampling density .    in this section",
    "we introduce a modification of newton s algorithm to simultaneously and recursively estimate both a mixing distribution and an additional unknown parameter .",
    "this modification , called the newton@xmath346plug - in ( n@xmath346p ) , is actually quite simple  at each step we use a plug - in estimate of @xmath15 in the update  .",
    "we show that the n@xmath346p algorithm can be written as a general sa algorithm and , under certain conditions , prove its consistency .",
    "let @xmath13 be a two - parameter family of densities on @xmath133 , and consider the model @xmath349 \\\\[-8pt ] \\qquad x_{i1},\\ldots , x_{ir } & \\stackrel{\\mathrm{i.i.d.}}{\\sim } & p(\\cdot |\\theta^i,\\xi ) , \\quad i=1,\\ldots , n , \\nonumber\\end{aligned}\\ ] ] where @xmath3 is an unknown density on @xmath5 and the parameter @xmath350 is also unknown .",
    "the number of replicates @xmath351 is assumed fixed . note that is simply a _",
    "nonparametric _ random effects model .",
    "assume , for simplicity , that @xmath352 ; the more general case @xmath353 is a natural extension of what follows .",
    "let @xmath354 be a _ finite _ set and take @xmath4 to be counting measure on @xmath5 . recall that @xmath304 is the probability simplex .",
    "assume :    1 .",
    "@xmath355 , where @xmath356 is a compact and convex subset of @xmath357 .",
    "2 .   @xmath358 where @xmath359 is compact and , for each @xmath360 , the coordinates @xmath361 are bounded away from zero .",
    "the subset @xmath356 can be arbitrarily large so assumption np1 causes no difficulty in practice .",
    "assumption np2 is somewhat restrictive in that @xmath3 must be strictly positive . while np2 seems necessary to prove consistency ( see appendix [ ss : app-3 ] ) , simulations suggest that this assumption can be weakened .",
    "the n@xmath346p algorithm uses an estimate of @xmath15 at each step in newton s algorithm .",
    "we assume here that an _ unbiased _ estimate is available :    1 .",
    "there exists an unbiased estimate @xmath362 , @xmath363 , of @xmath15 with variance @xmath364 .",
    "later we will replace the unbiased estimate with a bayes estimate .",
    "this will require replacing np3 with another assumption .    at time @xmath75",
    ", we observe an @xmath365-vector @xmath366 and we compute @xmath367 .",
    "an unbiased estimate of @xmath15 based on the entire data @xmath368 would be the average @xmath369 , which has a convenient recursive expression @xmath370 , \\quad i=1,\\ldots , n.\\ ] ] more importantly , by construction , @xmath371 are i.i.d .",
    "random variables with mean @xmath15 and finite variance .",
    "it is , therefore , a consequence of the slln that @xmath372 , as defined in , converges a.s . to @xmath15 . while this result holds for any unbiased estimate @xmath170 , an unbiased estimate @xmath373 with smaller variance is preferred , since it will have better finite - sample performance .",
    "define the mapping @xmath374 with @xmath307th component @xmath375 for @xmath376 , where @xmath295 and @xmath294 denote generic elements in @xmath377 and @xmath356 , respectively , and @xmath378 is the joint density of an i.i.d .",
    "sample of size @xmath365 from @xmath379 .",
    "choose an initial estimate @xmath380 , weights @xmath381 , and an arbitrary @xmath382 . then for @xmath75 compute @xmath383   \\bigr\\ } , \\\\",
    "f_i & = & \\operatorname{proj}_{\\delta_0 }   \\bigl\\ { f_{i-1 } + w_i h(x_i;f_{i-1},\\xi_i ) \\bigr\\},\\end{aligned}\\ ] ] and produce @xmath384 as the final estimate .",
    "we claim that the n@xmath346p algorithm for estimating @xmath3 can be written as a general sa involving the true but unknown @xmath15 plus an additional perturbation .",
    "define the quantities @xmath385 , \\label{e : np - h } \\\\",
    "\\beta_n & = & \\mathbb{e}[h(x_n , f_{n-1},\\xi_n)|\\mathscr{f}_{n-1 } ] \\nonumber\\\\[-8pt]\\label{e : beta1 } \\\\[-8pt ] & & { } - \\mathbb{e}[h(x_n , f_{n-1},\\xi)|\\mathscr{f}_{n-1 } ] , \\nonumber\\end{aligned}\\ ] ] where @xmath386 , so that @xmath387 = h(f_{n-1 } ) + \\beta_n.\\ ] ] now the update @xmath388 can be written as @xmath389 where @xmath120 is the `` minimum '' @xmath96 keeping @xmath274 in @xmath377 , and @xmath390 is a martingale adapted to @xmath391 .",
    "notice that is now in a form in which theorem [ t : sa ] can be applied .",
    "we will make use of the law of iterated logarithm so define @xmath392 .",
    "the consistency properties of the n@xmath346p algorithm are summarized in the following theorem .",
    "[ t : martin - ghosh2]assume , , , .",
    "in addition , assume    1 .",
    "@xmath393 is bounded on @xmath394 .",
    "2 .   @xmath395 converges .    then @xmath396 a.s . as @xmath21",
    "we now remove the restriction to unbiased estimates of @xmath15 , focusing primarily on the use of a bayes estimate in place of the unbiased estimate .",
    "but first , let @xmath397 be any suitable estimate of @xmath15 based on only @xmath398 . then replace the n@xmath346p update @xmath275 with @xmath399 while this adaptation is more flexible with regard to the choice of estimate",
    ", this additional flexibility does not come for free .",
    "notice that the algorithm is no longer _",
    "recursive_. that is , given a new data point @xmath400 , we need more information than just the pair @xmath401 to obtain @xmath402 .",
    "[ cor : cor1]if assumptions and in theorem  [ t : martin - ghosh2 ] are replaced by    1 .   @xmath403 a.s . as @xmath21 , 2 .",
    "@xmath404 ,    then @xmath405 a.s . as @xmath406 .    typically , for bayes and ml estimates , the rate is @xmath407 .",
    "then np5@xmath408 holds if , e.g. , @xmath409 .    to illustrate the n@xmath346p and its modified version ,",
    "consider the special case where @xmath410 in is a normal density with mean @xmath14 and @xmath411 is the unknown variance .",
    "that is , @xmath412 moreover , the statistic @xmath413 is sufficient for the mean and the density @xmath414 of @xmath415 is known . therefore , @xmath416 in can be written as @xmath417 for @xmath376 , where @xmath418 is the @xmath419 density . even in this simple example , it is not obvious that the function @xmath416 in satisfies np4 .",
    "a proof of the following proposition is in appendix  [ ss : app-3 ] .",
    "[ p : normal - mix ] holds for @xmath416 in .",
    "let @xmath119 be the @xmath356 defined in the general setup .",
    "for the n@xmath346p , we choose @xmath362 to be the sample variance of @xmath18 , resulting in the recursive estimate @xmath420 for @xmath421 , take the standard noninformative prior @xmath422 . under squared - error loss , the bayes estimate of @xmath421 based on @xmath398 is @xmath423 \\\\[-8pt ] & = & \\frac{1}{i(r-1)-2 } \\sum_{k=1}^i \\sum_{j=1}^r ( x_{kj}-\\overline{x}_k)^2 .",
    "\\nonumber\\end{aligned}\\ ] ] note that @xmath424 a.s . so the conclusion of corollary [ cor : cor1 ] holds if @xmath409 .",
    "the following example compares three resulting estimates for this location mixture of normals problem : when @xmath421 is known , when is used with the n@xmath346p and when is used in the modified n@xmath346p .",
    "convergence of the iterates holds in each case by theorems [ t : martin - ghosh ] and [ t : martin - ghosh2 ] and corollary [ cor : cor1 ] .",
    "[ ex : np - example]let @xmath334 $ ] and take @xmath3 to be a @xmath425 density on @xmath5 .",
    "suppose @xmath426 , @xmath345 , @xmath427 and set @xmath428 .",
    "for each of 100 simulated data sets , the three estimates of @xmath3 are computed using newton s algorithm , the n@xmath346p and the bayes modification .",
    "each algorithm produces estimates @xmath429 and @xmath430 with which we compute @xmath431 .",
    "figure [ f : np - example ] summarizes the 100 kl divergences @xmath432 for each of the three estimates .",
    "surprisingly , little efficiency is lost when an estimate of @xmath421 is used rather than the true value . also , the n@xmath346p and the bayes modification perform comparably , with the bayes version performing perhaps slightly better on average .",
    "note that no projections onto @xmath433 $ ] or @xmath434 were necessary in this example .",
    "for the three algorithms in example [ ex : np - example].[f : np - example ] ]    in this paper , we have used general results in the area of stochastic approximation to prove a consistency theorem for a recursive estimate of a mixing distribution / prior in the case of a finite parameter space @xmath5 .",
    "it is natural to wonder if this theorem can be extended to the case where @xmath3 is an infinite - dimensional parameter on an uncountable space @xmath5 .",
    "very recently , tokdar , martin and ghosh @xcite have proved consistency of @xmath274 in the infinite - dimensional case , under mild conditions .",
    "their argument is based on the approximate martingale representation used in  @xcite but applied to the kl divergence @xmath435 between the induced marginals .",
    "again , there is a connection between their approach and the sa approach taken here , namely , @xmath436 is also a lyapunov function for the associated ode @xmath319 .",
    "in addition to convergence , there are some other interesting theoretical and practical questions to consider .",
    "first and foremost , there is the question of _ rate of convergence _ which , from a practical point of view , is much more important than convergence alone .",
    "we expect that , in general , the rate of convergence will depend on the support of @xmath269 , the weights @xmath437 and , in the case of an uncountable @xmath5 , the smoothness of @xmath3 . whatever the true rate of convergence might be , example  [ ex : newton - discrete ] ( model ii ) demonstrated that this rate is unsatisfactory when the support of @xmath3 is misspecified . for this reason , a modification of the algorithm that better handles such cases would be desirable .",
    "another question of interest goes back to the original motivation for newton s recursive algorithm . to an orthodox bayesian ,",
    "any method which performs well should be at least _",
    "approximately bayes_. stemming from the fact that the recursive estimate and the nonparametric bayes estimate , with the appropriate dirichlet process prior , agree when @xmath438 , newton et al .",
    "@xcite claim that the former should serve as a suitable approximation to the latter .",
    "our calculations in section  [ ss : examples ] disagree .",
    "in particular , we see two examples in the finite case , one where the recursive estimate is significantly better and the other where the bayes estimate is significantly better .",
    "a new question arises : if it is not an approximation of the dirichlet process prior bayes estimate , _ for what prior does the recursive estimate approximate the corresponding bayes estimate_?=1    finally , it should be pointed out that our approach to the finite mixture problem is somewhat less general than would be desirable . in particular , we are assuming that the support of @xmath3 is within a _ known _ finite set of points .",
    "in general , however , what is known is that the support of @xmath3 is contained in , say , a bounded interval . in this case ,",
    "a set of grid points @xmath439 are chosen to approximate the _ unknown _ support @xmath440 of @xmath3 .",
    "newton s algorithm will produce an estimate @xmath274 on @xmath5 in this case , but it is impossible to directly compare @xmath274 to @xmath3 since their supports @xmath5 and @xmath441 may be entirely different .",
    "there is no problem comparing the marginals , however .",
    "this leads us to the following important conjecture , closely related to the so - called @xmath321-projections in  @xcite .",
    "let @xmath442 and @xmath9 be the marginal densities corresponding to @xmath274 on @xmath5 and @xmath3 on @xmath441 , respectively . then , as @xmath443 , @xmath444 where @xmath295 ranges over all densities on @xmath5 .    despite these unanswered practical and theoretical questions , the strong performance of newton s algorithm and the n@xmath346p algorithm in certain cases and , more importantly , their computational cost - effectiveness , make them very attractive compared to the more expensive nonparametric bayes estimate or the nonparametric mle , and worthy of further investigation .",
    "to prove the theorem , we need only show that the algorithm satisfies the conditions of theorem  [ t : sa ] .",
    "first note that @xmath274 is , for each @xmath101 , a convex combination of points in the _ interior _ of @xmath304 so no projection as in   is necessary .",
    "second , the random variables @xmath253 in assumption sa2 are identically zero so sa3 is trivially satisfied .",
    "let @xmath445 be a convergent sequence in @xmath304 , where @xmath446 .",
    "the limit @xmath447 also belongs to @xmath448 so @xmath449 is well defined . to prove that @xmath450 is continuous ,",
    "we show that @xmath451 for each @xmath376 as @xmath21 .",
    "consider @xmath452 the integrand @xmath453 is nonnegative and bounded @xmath134-a.e .",
    "for each @xmath307 . then by the bounded convergence theorem we get @xmath454 but @xmath455 was arbitrary so @xmath0 is continuous .    next , note that @xmath314 is the difference of two points in @xmath456 and is thus bounded independent of @xmath18  and  @xmath101 .",
    "then sa1 holds trivially .",
    "finally , we show that @xmath3 is globally asymptotically stable for the ode @xmath319 in @xmath304 .",
    "note that @xmath457 so the trajectories lie on the connected and compact @xmath304 .",
    "let @xmath458 be the kl divergence , @xmath459 .",
    "we claimthat @xmath231 is a strong lyapunov function for @xmath319 at  @xmath3 .",
    "certainly @xmath458 is positive definite . to check the differentiability condition",
    ", we must show that @xmath458 has a well - defined gradient around @xmath3 , even when @xmath3 is on the boundary of @xmath304 .",
    "suppose , without loss of generality , that @xmath460 are positive , @xmath461 , and the remaining @xmath462 are zero . by definition , @xmath458 is constant in @xmath463 and , therefore , the partial derivatives with respect to those @xmath295 s are zero .",
    "thus , for any @xmath464 and for any @xmath295 such that @xmath465 , the gradient can be written as @xmath466 where @xmath467 and @xmath468 is a vector whose first @xmath469 coordinates are one and last @xmath470 coordinates are zero .",
    "the key point here is that the gradient of @xmath458 , for  @xmath295 restricted to the boundary which contains @xmath3 , is exactly .",
    "we can , therefore , extend the definition of @xmath471 continuously to the boundary if need be .",
    "given that @xmath471 exists on all of @xmath304 , the time derivative of @xmath231 along @xmath295 is @xmath472 it remains to show that @xmath473 iff @xmath474 .",
    "applying jensen s inequality to @xmath475 in gives @xmath476 \\\\[-8pt ] & \\leq & 1-   \\biggl ( \\int_{\\mathcal{x } } \\frac{\\pi_{\\varphi}}{\\pi_f } \\pi_f \\ , d\\nu \\biggr)^{-1 } = 0 , \\nonumber\\end{aligned}\\ ] ] where equality can hold in iff @xmath477 @xmath134-a.e .",
    "we assume the mixtures are identifiable , so this implies @xmath474 .",
    "therefore , @xmath473 iff @xmath322 , and we have shown that @xmath231 is a strong lyapunov function on @xmath304 . to prove that @xmath3 is a globally asymptotically stable point for @xmath478 , suppose that @xmath479 is a solution , with @xmath480 , that does not converge to @xmath3 . since @xmath231 is a strong lyapunov function , the sequence @xmath481 , as @xmath202 , is bounded , strictly decreasing and , thus , has a limit @xmath482 .",
    "then the trajectory @xmath479 must fall in the set @xmath483 for all @xmath218 . in the case",
    "@xmath484 , @xmath485 as @xmath486 , so the set @xmath487 is compact ( in the relative topology ) . if @xmath488 , then @xmath487 is not compact but , as shown above , @xmath489 is well defined and continuous there . in either case ,",
    "@xmath490 is continuous and bounded away from zero on @xmath487 , so @xmath491 then , for any @xmath492 , we have @xmath493 if @xmath494 , then @xmath495 , which is a contradiction . therefore , @xmath496 for all initial conditions @xmath480 , so @xmath3 is globally asymptotically stable .",
    "theorem [ t : sa ] then implies @xmath325 a.s .",
    "the proof of the theorem requires the following lemma , establishing a lipschitz - type bound on the error terms @xmath253 in .",
    "its proof follows immediately from np4 and the mean value theorem .",
    "[ l : lipschitz]under the assumptions of theorem  [ t : martin - ghosh2 ] , there exists a number @xmath497 such that @xmath498    proof of theorem [ t : martin - ghosh2 ] the map @xmath0 in has @xmath307th component @xmath499 where @xmath500 is the marginal density of @xmath18 and @xmath501 is the product measure on @xmath502 .",
    "notice that this @xmath0 , which does not depend on the estimate @xmath372 , is _ exactly the same _ as the @xmath0 in .",
    "therefore , the continuity and stability properties derived in the proof of theorem [ t : martin - ghosh ] are valid here as well .",
    "all that remains is to show that the @xmath253 s in satisfy sa3 of theorem  [ t : sa ] .    by the slln",
    ", @xmath372 belongs to @xmath356 for large enough @xmath101 so we can assume , without loss of generality , that no projection is necessary .",
    "let @xmath503 , where the @xmath504 and @xmath505 is the variance of @xmath506 . then @xmath507 , where @xmath508 is a constant independent of @xmath101 . since @xmath509 is a sum of i.i.d .",
    "random variables with mean zero and unit variance , the law of iterated logarithm states that @xmath510 now , by lemma [ l : lipschitz ] and we have @xmath511 and , therefore , @xmath512 converges a.s . by np5 .",
    "condition sa3 is satisfied , completing the proof .      to prove that the case of a location - mixture of normals with unknown variance is covered by theorem  [ t : martin - ghosh2 ]",
    ", we must show that the function @xmath416 , defined in  , satisfies np4 , that is , that the partial derivatives @xmath513 are bounded .",
    "proof of proposition [ p : normal - mix ] clearly each component @xmath514 of @xmath416 , defined in , is differentiable with respect to @xmath515 and , after simplification , @xmath516 ^ 2},\\end{aligned}\\ ] ] where ( as @xmath517 ) @xmath518 this derivative is continuous on @xmath519 and , since @xmath377 and @xmath119 are compact , we know that @xmath520 is finite for all @xmath521 and for all @xmath307 . by the mean value theorem , @xmath522 it remains to show that @xmath523 is bounded in @xmath469 .",
    "for notational simplicity , assume that @xmath295 and @xmath294 are the values for which the suprema in are attained . making a change of variables @xmath524 we can , with a slight abuse of notation , write @xmath525 ^ 2}.\\ ] ] we must show that @xmath526 is bounded as @xmath527 .",
    "assume , without loss of generality , that the @xmath14 s are arranged in ascending order : @xmath528 .",
    "factoring out , respectively , @xmath529 and @xmath530 , we can write @xmath531 note that since @xmath360 , each @xmath532 is bounded away from  0 . if @xmath533 , then the term @xmath534 dominates the numerator of the first inequality , while the denominator is bounded",
    ". similarly , if @xmath535 , then the term @xmath536 dominates the numerator in the second inequality , while the denominator is bounded .",
    "for the case @xmath537 or @xmath538 , note that @xmath539 , so the two inequalities can still be applied and a similar argument shows @xmath540 and @xmath541 are also bounded .",
    "therefore , @xmath526 is bounded for each @xmath307 and the claim follows by taking @xmath215 to be @xmath542 .",
    "the authors thank professors chuanhai liu and surya t. tokdar for numerous fruitful discussions , as well as professors jim berger and mike west , the associate editor and the two referees for their helpful comments .",
    "99 allison , d. , gadbury , g. , heo , m. , fernndez , j. , lee ,  c. , prolla , t. and weindruch , r. ( 2002 ) . a mixture model approach for the analysis of microarray gene expression data .",
    "statist . data anal . _ * 39 * 120 .",
    "newton , m. a. , quintana , f. a. and zhang , y. ( 1998 ) .",
    "nonparametric bayes methods using predictive updating.in _ practical nonparametric and semiparametric bayesian statistics _ ( d.  dey , p.  muller and d.  sinha , eds . ) 4561 .",
    "springer , new york .",
    "quintana , f. a. and newton , m. a. ( 2000 ) .",
    "computational aspects of nonparametric bayesian analysis with applications to the modeling of multiple binary sequences .",
    "_ j.  comput . graph .",
    "statist . _",
    "* 9 * 711737 ."
  ],
  "abstract_text": [
    "<S> many statistical problems involve mixture models and the need for computationally efficient methods to estimate the mixing distribution has increased dramatically in recent years . </S>",
    "<S> newton [ _ sankhy ser . a _ * 64 * ( 2002 ) 306322 ] proposed a fast recursive algorithm for estimating the mixing distribution , which we study as a special case of stochastic approximation ( sa ) . </S>",
    "<S> we begin with a review of sa , some recent statistical applications , and the theory necessary for analysis of a sa algorithm , which includes lyapunov functions and ode stability theory . </S>",
    "<S> then standard sa results are used to prove consistency of newton s estimate in the case of a finite mixture . </S>",
    "<S> we also propose a modification of newton s algorithm that allows for estimation of an additional unknown parameter in the model , and prove its consistency .    . </S>"
  ]
}