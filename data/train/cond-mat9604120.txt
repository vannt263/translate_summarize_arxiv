{
  "article_text": [
    "there is an inherent asymmetry between integration and differentiation which makes integration somewhat of an art form , and which is perhaps best exemplified by the lack of an elementary indefinite integral of the celebrated gaussian : @xmath2 the fact that such an integral does not in fact exist follows from the work of laplace @xcite .",
    "however , the gaussian integral is fundamental , finding applications in statistics , error theory and many branches of physics .",
    "in fact , anywhere one has gaussian distributions , cummulatives of these distributions will involve the above integral .",
    "only special case definite integrals of @xmath3 are known , the most famous being : @xmath4 in addition there is the series expansion @xcite : @xmath5 now in practise one can evaluate the integral accurately by numerical methods or tables , but in many cases it would be preferable to have an analytical solution , even if it were not exact , as long as the maximum error were very small and the approximation were simple .",
    "it turns out that there exists a function well known in the analysis of nonlinear partial differential equations whose derivative is very close to gaussian - the kink soliton : @xmath6 with derivative : @xmath7 where @xmath8 and @xmath9 are all real constants .",
    "the graphs of @xmath3 and @xmath10 are shown in figure ( 1 ) . in eq .",
    "( [ eq : family ] ) which give better fits when @xmath11 , but which do not have indefinite integrals as far as is known to the author . ]",
    "the kink soliton is the positive , time - independent , topological solution to the non - linear @xmath12 dimensional partial differential equation : @xmath13 where a subscript denotes partial derivative with respect to that variable .",
    "the solution to this equation is topological because the boundary conditions at @xmath14 are different . leaving the physical origin of @xmath15 behind , it is interesting to examine the series expansion of @xmath16 : @xmath17 which should be compared with eq .",
    "( [ eq : series1 ] ) for @xmath0 . here",
    "@xmath18 are the bernoulli numbers with generating function @xmath19 .",
    "we see that although the coefficients differ in each case , the powers of @xmath20 in the expansions are identical .",
    "further both @xmath10 and @xmath3 have the property that their derivatives can be re - expressed in terms of themselves and @xmath21 or powers of @xmath20 respectively .",
    "these observations shed some light on the foundations of the approximation .",
    "turning to practical issues , we are left with choosing the constants , @xmath22 to optimise the approximation of eq .",
    "( [ eq : gauss ] ) .",
    "we need three constraints to fix the three parameters .",
    "first we require that the gaussian and @xmath10 have the same symmetry axis .",
    "this requires the argument of @xmath23 to vanish at @xmath24 which immediately implies from eq .",
    "( [ eq : family ] ) that @xmath25 .    at this stage",
    "we have a choice , dependent on whether we are interested in an approximate solution for small or large @xmath20 . for large @xmath20 ,",
    "a constraint is obviously that our new approximation , @xmath21 , must give _ exactly _ the same result as eq .",
    "( [ eq : infint ] ) when differenced at infinity and the origin .",
    "this will ensure convergence of our approximation .",
    "since @xmath26 as @xmath27 , and @xmath28 , this implies from eq .",
    "( [ eq : kink ] ) that : @xmath29    finally we can impose that @xmath30 at some point , i.e , we match the derivatives",
    ". we will choose @xmath31 as the simplest .",
    "this gives : @xmath32 in fact the two are equal at another point as can be seen from figure ( 1 ) .",
    "our analytical approximation , which is very accurate for large @xmath20 , is therefore : @xmath33 where in this paper @xmath34 is understood as meaning asymptotic convergence , as @xmath27 and bounded error @xmath35 . from figures ( 1,2 )",
    "we see that the kink derivative underestimates the gaussian at small @xmath36 and overestimates it at large @xmath36",
    ".    alternatively if one is interested in @xmath37 where @xmath38 say , then this will not be good enough , since the error in our approximation is strongly confined to small @xmath20 .",
    "instead we can impose that @xmath21 must give the exact result , not at infinity , but at the end of the interval , i.e. at @xmath39 .",
    "thus we impose : @xmath40 in addition we need to match the derivatives @xmath41 at some point @xmath42 as before , and then solve the equations for @xmath43 .",
    "it is an open question which matching point yields the best results . for illustrative purposes we choose @xmath31 and again find @xmath44 , so that substituting in eq.([eq : smallx ] ) gives us a nonlinear root - finding problem for @xmath45 .",
    "the right - hand side can be found for example , from tables of the error function , @xmath46 .",
    "this yields an approximation which is exact at @xmath47 and hence a much better approximation for small @xmath20 , but which is invalid for @xmath48 .",
    "the extension to cases with variable lower limit of integration is obvious and will not be considered .",
    "one might be tempted to generalise eq .",
    "( [ eq : kink ] ) to a one - parameter family of approximations to the error function : @xmath49 which have derivative : @xmath50 however , since for @xmath51 , @xmath52 , they are not really suitable as approximations to a gaussian .",
    "rather they are skewed distributions with maxima at @xmath53 .",
    "it turns out however , that they will be useful later .    for testing our approximation we will use the @xmath21 valid for large @xmath20 , denoted @xmath54 , given by eq .",
    "( [ eq : integral ] ) .",
    "the crucial question is of course , how good is this approximation ?",
    "it turns out that it is very good in most cases , as can be seen from figures ( 3 ) and ( 4 ) . the maximum error from using @xmath54 is @xmath55 at @xmath56 .",
    "however as discussed earlier , if one is interested in the result for small @xmath20 , and @xmath57 is small , then this is not the best approximation to use . in practise",
    ", the error drops off very quickly due to the exponential nature of @xmath16 .",
    "for example , the error in estimating @xmath46 drops below @xmath58 for @xmath59 and at @xmath60 the error is @xmath61 .",
    "the error as a function of x is plotted in figure ( 4 ) .",
    "the shape of figure ( 4 ) is , in fact , rather startling because it is a very simple shape . from the graph it has a single local maximum and hence two points where the concavity changes .",
    "hence although it can not be written down explicitely in terms of elementary functions @xcite , it can be approximated very closely .",
    "several fitting shapes were tried , such as the log - normal and poisson distributions , but the best was found to be a generalised maxwell - distribution : @xmath62 for the case used in the figures , that of @xmath46 , the best parameters for reducing the maximum error ( i.e. minimising w.r.t .",
    "the sup - norm @xmath63 ) were ( see figure ( 5 ) ) : @xmath64 which reduced the _ maximum _",
    "error to @xmath65 .",
    "it is also likely that our choice of function and parameters for @xmath66 is not optimal , since formal optimisation was not used , but was based rather on a numerical investigation of the parameter space @xmath67 .",
    "further , since the required @xmath66 is a skewed gaussian with maximum at non - zero @xmath20 we can profitably employ the functions given by eq .",
    "( [ eq : derivdel ] ) , originally introduced to model the gaussian , as fits for the error . in this case",
    "our approximation becomes : @xmath68 \\label{eq : tot}\\ ] ] where @xmath69 denotes derivative w.r.t .",
    "@xmath20 . for @xmath70 and @xmath71 the error is at most @xmath72 .",
    "by suitable generalisation of the second term it is possible to increase the accuracy to the level of the generalised maxwell distribution , but for simplicity and because of its suggestiveness , we leave it in the above form .",
    "in the case of the error function we have explicitely that ( @xmath73 ) : @xmath74 where @xmath75 is the error function .",
    "similarly the complementary error function is given by : @xmath76 .",
    "a fundamental feature of a gaussian distributed random variable is that all moments above the second , such as the skewness , are zero . from this",
    "it follows that the sum of error distributed random variables is itself error distributed . a natural question to ask is how well the soliton approximation preserves this feature .    to make this more precise : given the distribution @xmath77",
    ", we may define the partition function @xmath78 because of its ubiquitous use in statistical physics . in the case where @xmath20 is a function",
    ", @xmath78 becomes a path - integral and derivative becomes functional derivative in eq .",
    "( [ eq : moment ] ) . ] via : @xmath79 from the  free energy \" @xmath80 we may now define the n - th moment , @xmath81 , of @xmath77 as : @xmath82    thus in the case of a gaussian distribution with zero mean , it is easy to show that the free energy is a quadratic function of @xmath83 .",
    "hence the only non - zero moment is the second , i.e. the variance , as claimed above .",
    "in the case of the soliton approximant we have : @xmath84 e^{j x } dx\\ ] ] which is unfortunately not known analytically , so we resort to numerical analysis . using the gaussian case as a testbed we approximated the free energy with an 8-th degree polynomial : @xmath85 for a gaussian @xmath86 . using a least - squares method , the error , i.e. the largest @xmath87 coefficient which is zero in the exact case but non - zero in the fit , was @xmath88 .",
    "each subsequent coefficient was roughly an order of magnitude smaller than the preceding one .    in the case of the soliton approximation , given by eq .",
    "( [ eq : family ] ) , the error was @xmath89 again for the cubic term , and again with roughly @xmath90 .    [ cols=\"^,^,^\",options=\"header \" , ]     table ( 1 ) shows a comparison between the coefficients of the free - energy polynomials for the terms higher than cubic for the exact gaussian and the soliton approximant @xmath10 .",
    "an interesting thing to note is that , although the accuracy is at the level one might expect , i.e. @xmath91 , the pattern of the terms is identical ; namely both the signs and the decrease in the coefficients have the same behaviour in both cases .",
    "this suggests that numerical errors will be  coherent \" , i.e. the errors one has from numerical integration of the gaussian will be of the same nature as those one obtains from the soliton approximation .",
    "this is perhaps obvious given the similarity of their power series ( see eq.s ( [ eq : series1],[eq : series2 ] ) ) but will not be true for other approximants in terms of e.g. rational functions @xcite .",
    "we leave this discussion by noting that inclusion of @xmath66 , via e.g. eq .",
    "( [ eq : differr ] ) , in the calculation of moments will reduce the above errors considerably , presumably by a factor of at least @xmath92 .",
    "let us now consider a small sample of applications .",
    "a primary example is in the theory of statistics .",
    "if we have a uniformly distributed random variable @xmath93 and we desire a random variable @xmath94 with statistics given by a distribution @xmath95 , first define the integral @xmath96 .",
    "then @xmath97 will have the same distribution as @xmath95 , where @xmath98 denotes the inverse of @xmath99 , on the interval @xmath100 $ ] .",
    "in particular if , as is often the case , we want to generate a realisation of a gaussian random distribution , @xmath101 , then with our approximation , @xmath102 ( we have dropped the error correction term @xmath66 for simplicity ) and the inverse @xmath103 , gives us our random variable .",
    "in this case if @xmath104 , then : @xmath105 which has the same form as @xmath21 with the replacement @xmath106 so that both the integral and inverse are essentially trivial .",
    "this avoids the necessity of using traditional monte carlo methods to calculate gaussian distributions .",
    "a related problem occurs in the study of structure formation from gravitational collapse from gaussian initial conditions , a standard assumption .",
    "the press - schecter formalism @xcite , gives the cummulative mass function @xmath107 , which is the number of objects ( such as galaxies ) with mass greater than @xmath108 : @xmath109 where @xmath110 and @xmath111 is the variance of the distribution .",
    "this can be estimated immediately using eq .",
    "( [ eq : erf ] ) .",
    "one place where error functions are ubiquitous is in diffusion theory , since the decaying gaussian is a solution to the standard diffusion equation . in the case where there is an extended distribution of diffusing material , situated at @xmath112 for example ,",
    "the solution is instead given by : @xmath113 where @xmath114 is the diffusion constant .",
    "indeed the error function appears any time there is a summation of the effects of a series of line sources each of which has an exponential distribution , both in finite and infinite media , as discussed in great detail in @xcite .",
    "further , the error function can be related to special values of the degenerate hypergeometric function , @xmath115 . in particular : @xmath116    our final example comes from the theory of parabolic cylinder functons , @xmath117 , which are solutions to the differential equation : @xmath118 with @xmath119 and for integer values of @xmath120 , they are related to the hermite polynomials , @xmath121 by @xmath122 .",
    "finally we may write , for the special cases of @xmath123 : @xmath124\\\\ d_{-2}(z ) & \\simeq & -e^{z^2/4 } \\sqrt{\\frac{\\pi}{2 } }   \\left[\\sqrt{\\frac{2}{\\pi } }   e^{-z^2/2 } - z ( 1 - \\tanh(\\sqrt{\\frac{2}{\\pi}}z ) ) \\right]\\\\\\end{aligned}\\ ] ]",
    "in this _ letter _ we have presented a function approximating @xmath46 to better than @xmath125 , with exponential convergence as @xmath27 .",
    "this solution is simply the kink soliton , @xmath126 and can be optimised for accuracy if the error function at small values of the argument is required .",
    "further we have found a solution with maximum error of @xmath127 by adding a generalised maxwell distribution to the kink soliton , equations ( [ eq : differr ] ) , ( [ eq : tot ] ) .",
    "future work should be aimed at finding truly optimal solutions .",
    "finally a few applications were discussed , particularly to diffusion dynamics and to the generation of gaussian random fields ."
  ],
  "abstract_text": [
    "<S> we provide analytical functions approximating @xmath0 , the basis of which is the kink soliton and which are both accurate ( error @xmath1 ) and simple . we demonstrate our results with some applications , particularly to the generation of gaussian random fields .    -45pt </S>",
    "<S> -45pt    2ex    @xmath0 and the kink soliton    _ bruce bassett _    international school for advanced studies , + via beirut 2 - 4 , 34014 trieste , italy +    * keywords * : error function , numerical approximation , kink soliton . </S>"
  ]
}