{
  "article_text": [
    "suppose we observe independent and identically distributed @xmath0-variate random variables @xmath1 with covariance matrix @xmath2 and the goal is to estimate the unknown matrix @xmath3 based on the sample @xmath4 .",
    "this covariance matrix estimation problem is of fundamental importance in multivariate analysis .",
    "a wide range of statistical methodologies , including clustering analysis , principal component analysis , linear and quadratic discriminant analysis , regression analysis , require the estimation of the covariance matrices . with advances in technology ,",
    "large high - dimensional data are now routinely collected in scientific investigations .",
    "examples include climate studies , gene expression arrays , functional magnetic resonance imaging , risk management and portfolio allocation and web search problems . in such settings ,",
    "the standard and most natural estimator , the sample covariance matrix , often performs poorly .",
    "see , for example , muirhead ( @xcite ) , johnstone ( @xcite ) , bickel and levina ( @xcite , @xcite ) and fan , fan and lv ( @xcite ) .",
    "regularization methods , originally developed in nonparametric function estimation , have recently been applied to estimate large covariance matrices .",
    "these include banding method in wu and pourahmadi ( @xcite ) and bickel and levina ( @xcite ) , tapering in furrer and bengtsson ( @xcite ) , thresholding in bickel and levina ( @xcite ) and el karoui ( @xcite ) , penalized estimation in huang et al .",
    "( @xcite ) , lam and fan ( @xcite ) and rothman et al .",
    "( @xcite ) , regularizing principal components in johnstone and lu ( @xcite ) and zou , hastie and tibshirani ( @xcite ) .",
    "asymptotic properties and convergence results have been given in several papers .",
    "in particular , bickel and levina ( @xcite , @xcite ) , el karoui ( @xcite ) and lam and fan ( @xcite ) showed consistency of their estimators in operator norm and even obtained explicit rates of convergence .",
    "however , it is not clear whether any of these rates of convergence are optimal .    despite recent progress on covariance matrix estimation",
    "there has been remarkably little fundamental theoretical study on optimal estimation . in this paper",
    ", we establish the optimal rate of convergence for estimating the covariance matrix as well as its inverse over a wide range of classes of covariance matrices .",
    "both the operator norm and frobenius norm are considered .",
    "it is shown that optimal procedures for these two norms are different and consequently matrix estimation under the operator norm is fundamentally different from vector estimation .",
    "in addition , the results also imply that the banding estimator given in bickel and levina ( @xcite ) is sub - optimal under the operator norm and the performance can be significantly improved .",
    "we begin by considering optimal estimation of the covariance matrix @xmath5 over a class of matrices that has been considered in bickel and levina ( @xcite ) . both minimax lower and upper bounds are derived",
    "we write @xmath6 if there are positive constants @xmath7 and @xmath8 independent of @xmath9 such that @xmath10",
    ". for a matrix @xmath11 its operator norm is defined as @xmath12 .",
    "we assume that @xmath13 for some constant @xmath14 . combining the results given in section [ operatornorm.sec ]",
    ", we have the following optimal rate of convergence for estimating the covariance matrix under the operator norm .    [ minimaxope ]",
    "the minimax risk of estimating the covariance matrix @xmath15 over the class @xmath16 given in ( [ paraspace ] ) satisfies @xmath17    the minimax upper bound is obtained by constructing a class of tapering estimators and by studying their risk properties .",
    "it is shown that the estimator with the optimal choice of the tapering parameter attains the optimal rate of convergence . in comparison to some existing methods in the literature , the proposed procedure does not attempt to estimate each row / column optimally as a vector .",
    "in fact , our procedure does not optimally trade bias and variance for each row / column . as a vector estimator , it has larger variance than squared bias for each row / column . in other words , it is undersmoothed as a vector .",
    "a key step in obtaining the optimal rate of convergence is the derivation of the minimax lower bound .",
    "the lower bound is established by using a testing argument , where at the core is a novel construction of a collection of least favorable multivariate normal distributions and the application of assouad s lemma and le cam s method .",
    "the technical analysis requires ideas that are quite different from those used in the more conventional function / sequence estimation problems .",
    "in addition to the asymptotic analysis , we also carry out a small simulation study to investigate the finite sample performance of the proposed estimator .",
    "the tapering estimator is easy to implement .",
    "the numerical performance of the estimator is compared with that of the banding estimator introduced in bickel and levina ( @xcite ) .",
    "the simulation study shows that the proposed estimator has good numerical performance ; it nearly uniformly outperforms the banding estimator .",
    "the paper is organized as follows . in section [ method.sec ] , after basic notation and definitions are introduced , we propose a tapering procedure for the covariance matrix estimation .",
    "section [ operatornorm.sec ] derives the optimal rate of convergence for estimation under the operator norm .",
    "the upper bound is obtained by studying the properties of the tapering estimators and the minimax lower bound is obtained by a testing argument .",
    "section [ frobeniusnorm.sec ] considers optimal estimation under the frobenius norm .",
    "the problem of estimating the inverse of a covariance matrix is treated in section [ inverse.sec ] .",
    "section [ simulation.sec ] investigates the numerical performance of our procedure by a simulation study .",
    "the technical proofs of auxiliary lemmas are given in section sec.proofs .",
    "in this section we will introduce a tapering procedure for estimating the covariance matrix @xmath2 based on a random sample of @xmath0-variate observations @xmath1 .",
    "the properties of the tapering estimators under the operator norm and frobenius norm are then studied and used to establish the minimax upper bounds in sections [ operatornorm.sec ] and [ frobeniusnorm.sec ] .    given a random sample @xmath18 from a population with covariance matrix @xmath19 , the sample covariance matrix is @xmath20 which is an unbiased estimate of @xmath5 , and the maximum likelihood estimator of @xmath5 is @xmath21 when @xmath22 s are normally distributed .",
    "these two estimators are close to each other for large @xmath9",
    ". we shall construct estimators of the covariance matrix @xmath5 by tapering the maximum likelihood estimator @xmath23 .    following bickel and levina ( @xcite ) we consider estimating the covariance matrix @xmath24 over the following parameter space : @xmath25\\\\[-8pt ] & & \\hspace*{53.8pt}\\mbox{for all } k% \\mbox { , and } \\lambda_{\\max } ( \\sigma ) \\leq m_{0 } \\biggr\\},\\nonumber\\end{aligned}\\ ] ] where @xmath26 is the maximum eigenvalue of the matrix @xmath15 , and @xmath27 , @xmath28 and @xmath29 .",
    "note that the smallest eigenvalue of any covariance matrix in the parameter space @xmath30 is allowed to be @xmath31 which is more general than the assumption in ( 5 ) of bickel and levina ( @xcite ) .",
    "the parameter @xmath32 in ( [ paraspace ] ) , which essentially specifies the rate of decay for the covariances @xmath33 as they move away from the diagonal , can be viewed as an analog of the smoothness parameter in nonparametric function estimation problems .",
    "the optimal rate of convergence for estimating @xmath5 over the parameter space @xmath34 critically depends on the value of @xmath32 .",
    "our estimators of the covariance matrix @xmath5 are constructed by tapering the maximum likelihood estimator ( [ mle ] ) as follows .      for a given even integer @xmath35 with @xmath36",
    ", we define a tapering estimator as @xmath37 where @xmath38 are the entries in the maximum likelihood estimator @xmath39 and the weights @xmath40 where @xmath41 . without loss of generality",
    "we assume that @xmath35 is even .",
    "note that the weights @xmath42 can be rewritten as @xmath43 see figure [ weight.fig ] for a plot of the weights @xmath42 as a function of @xmath44 .    .",
    "]    the tapering estimators are different from the banding estimators used in bickel and levina ( @xcite ) .",
    "it is important to note that the tapering estimator given in ( [ tapering.est ] ) can be rewritten as a sum of many small block matrices along the diagonal .",
    "this simple but important observation is very useful for our technical arguments .",
    "define the block matrices @xmath45 and set @xmath46 for all integers @xmath47 and @xmath48 .",
    "[ est ] the tapering estimator @xmath49 given in ( [ tapering.est ] ) can be written as @xmath50    it is clear that the performance of the estimator @xmath51 depends on the choice of the tapering parameter @xmath35 .",
    "the optimal choice of @xmath35 critically depends on the norm under which the estimation error is measured .",
    "we will study in the next two sections the rate of convergence of the tapering estimator under both the operator norm and frobenius norm . together with the minimax",
    "lower bounds derived in sections operatornorm.sec and [ frobeniusnorm.sec ] , the results show that a tapering estimator with the optimal choice of @xmath35 attains the optimal rate of convergence under these two norms .",
    "in this section we will establish the optimal rate of convergence under the operator norm . for @xmath52 , the matrix @xmath53-norm of a matrix @xmath11",
    "is defined by @xmath54 .",
    "the commonly used operator norm @xmath55 coincides with the matrix @xmath56-norm @xmath57 . for a symmetric matrix @xmath11",
    ", it is known that the operator norm @xmath58 is equal to the largest magnitude of eigenvalues of @xmath11 .",
    "hence it is also called the spectral norm",
    ". we will establish theorem [ minimaxope ] by deriving a minimax upper bound using the tapering estimator and a matching minimax lower bound by a careful construction of a collection of multivariate normal distributions and the application of assouad s lemma and le cam s method .",
    "we shall focus on the case @xmath59 in sections  [ sec.uppbd ] and  [ sec.lowbd ] .",
    "the case of @xmath60 , which will be discussed in section  [ sec.discussion ] , is similar and slightly easier .",
    "we derive in this section the risk upper bound for the tapering estimators defined in ( [ estimator ] ) under the operator norm . throughout the paper",
    "we denote by @xmath8 a generic positive constant which may vary from place to place but always depends only on indices @xmath32 , @xmath61 and @xmath62 of the matrix family .",
    "we shall assume that the distribution of the @xmath63 s is sub - gaussian in the sense that there is @xmath64 such that @xmath65 let @xmath66 denote the set of distributions of @xmath67 that satisfy ( [ paraspace ] ) and  ( [ subgau ] ) .",
    "[ operupper.bd.thm ] the tapering estimator @xmath49 , defined in ( [ estimator ] ) , of the covariance matrix @xmath2 with @xmath68 satisfies @xmath69 for @xmath70 , @xmath71 and some constant @xmath72 .",
    "in particular , the estimator @xmath73 with @xmath74 satisfies @xmath75    from ( [ operupbd.k ] ) it is clear that the optimal choice of @xmath35 is of order @xmath76 . the upper bound given in ( [ operupbd ] ) is thus rate optimal among the class of the tapering estimators defined in ( [ estimator ] ) .",
    "the minimax lower bound derived in section [ sec.lowbd ] shows that the estimator @xmath49 with @xmath74 is in fact rate optimal among all estimators .",
    "proof of theorem [ operupper.bd.thm ] note that @xmath39 is translation invariant and so is @xmath77 .",
    "we shall thus assume @xmath78 for the rest of the paper .",
    "write@xmath79 where @xmath80 is a higher order term ( see remark higherorder at the end of this section ) . in",
    "what follows we shall ignore this negligible term and focus on the dominating term @xmath81 .",
    "set @xmath82 and write @xmath83",
    ". let @xmath84 with @xmath42 given in ( [ wij ] ) .",
    "let @xmath85 .",
    "we then write @xmath86 .",
    "it is easy to see @xmath87^{1/2}[\\mathbb e ( x_{j}^{l } ) ^{4}]^{1/2 } \\leq\\frac{c}{n},\\hspace*{-10pt}\\end{aligned}\\ ] ] that is , @xmath88 is an unbiased estimator of @xmath33 with a variance @xmath89 .",
    "we will first show that the variance part satisfies @xmath90 and the bias part satisfies @xmath91 it then follows immediately that @xmath92 this proves ( [ operupbd.k ] ) and equation ( [ operupbd ] ) then follows . since @xmath93 , we may choose @xmath94 and the estimator @xmath95 with @xmath35 given in ( [ kope ] ) satisfies @xmath96 theorem [ operupper.bd.thm ] is then proved .",
    "we first prove the risk upper bound ( [ biasupp ] ) for the bias part .",
    "it is well known that the operator norm of a symmetric matrix @xmath97 is bounded by its @xmath98 norm , that is,@xmath99 [ see , e.g. , page 15 in golub and van loan ( @xcite ) ] .",
    "this result was used in bickel and levina ( @xcite , @xcite ) to obtain rates of convergence for their proposed procedures under the operator norm ( see discussions in section sec.discussion ) .",
    "we bound the operator norm of the bias part @xmath100 by its @xmath98 norm . since @xmath101 , we have@xmath102 where @xmath103 $ ] and is exactly @xmath104 when @xmath105 , then@xmath106 ^{2}\\leq m^{2}k^{-2\\alpha}.\\ ] ]    now we establish ( [ varupp ] ) which is relatively complicated .",
    "the key idea in the proof is to write the whole matrix as an average of matrices which are sum of a large number of small disjoint block matrices , and for each small block matrix the classical random matrix theory can be applied .",
    "the following lemma shows that the operator norm of the random matrix @xmath107 is controlled by the maximum of operator norms of @xmath0 number of @xmath108 random matrices .",
    "let @xmath109 .",
    "define@xmath110    [ estbias]let @xmath111 be defined as in ( [ estimator ] ) . then @xmath112    for each small @xmath113 random matrix with @xmath114 , we control its operator norm as follows .",
    "[ estbiasbd]there is a constant @xmath115 such that @xmath116 for all @xmath117 and @xmath47 .    with lemmas [ estbias ] and [ estbiasbd ] we are now ready to show the variance bound ( [ varupp ] ) .",
    "by lemma [ estbias ] we have @xmath118 \\\\ & \\leq&9\\bigl [ x^{2}+\\mathbb{e } \\bigl ( n_{l}^{(m ) } \\bigr ) ^{2}i \\bigl ( n_{l}^{(m)}>x \\bigr ) \\bigr].\\end{aligned}\\ ] ] note that @xmath119 , which is bounded by a constant , and @xmath120 . the cauchy ",
    "schwarz inequality then implies@xmath121 \\\\ & \\leq&c_{1 } \\bigl [ x^{2}+\\sqrt{\\mathbb{e } ( \\vert\\breve { \\sigma}% \\vert_{f}+c ) ^{4}}\\sqrt{\\mathbb{p } \\bigl ( n_{l}^{(m)}>x \\bigr ) } % \\bigr].\\end{aligned}\\ ] ] set @xmath122 .",
    "then @xmath123 is bounded by @xmath124 as @xmath125 . from lemma [ estbiasbd ]",
    "we obtain@xmath126 \\nonumber\\\\[-8pt]\\\\[-8pt ] & \\leq & c_{1 } \\biggl ( \\frac{\\log p+m}{n}\\biggr).\\nonumber\\end{aligned}\\ ] ]    [ higherorder ] in the proof of theorem [ operupper.bd.thm ] , the term @xmath80 was ignored .",
    "it is not difficult to see that this term has negligible contribution after tapering .",
    "let @xmath127 and @xmath128 .",
    "define @xmath129 similarly to lemma [ estbiasbd ] , it can be shown that@xmath130 for all @xmath131 and @xmath47 .",
    "note that @xmath132 , then@xmath133 let @xmath134 . from ( [ htail ] )",
    "we have @xmath135 by similar arguments as for ( [ varupp1 ] ) .",
    "therefore @xmath136 has a negligible contribution to the risk .",
    "theorem [ operupper.bd.thm ] in section [ sec.uppbd ] shows that the optimal tapering estimator attains the rate of convergence @xmath137 . in this section",
    "we shall show that this rate of convergence is indeed optimal among all estimators by showing that the upper bound in equation ( [ operupbd ] ) can not be improved .",
    "more specifically we shall show that the following minimax lower bound holds .",
    "[ operlower.bd.thm ] suppose @xmath138 for some constant @xmath14 .",
    "the minimax risk for estimating the covariance matrix @xmath5 over @xmath16 under the operator norm satisfies @xmath139    the basic strategy underlying the proof of theorem [ operlower.bd.thm ] is to carefully construct a finite collection of multivariate normal distributions and calculate the total variation affinity between pairs of probability measures in the collection .",
    "we shall now define a parameter space that is appropriate for the minimax lower bound argument . for given positive integers @xmath35 and @xmath140 with @xmath141 and @xmath142 ,",
    "define the @xmath143 matrix @xmath144 with @xmath145 set @xmath74 and @xmath146 .",
    "we then define the collection of @xmath147 covariance matrices as @xmath148 where @xmath149 is the @xmath143 identity matrix and @xmath150 . without loss of generality",
    "we assume that @xmath151 and @xmath152 .",
    "otherwise we replace @xmath149 in ( [ f11 ] ) by @xmath153 for @xmath154 . for @xmath155",
    "it is easy to check that @xmath156 as @xmath125 .",
    "in addition to @xmath157 we also define a collection of diagonal matrices @xmath158 where @xmath159 and @xmath160 .",
    "let @xmath161 .",
    "it is clear that @xmath162 .",
    "we shall show below separately that the minimax risks over multivariate normal distributions with covariance matrix in ( [ f11 ] ) and ( [ f12 ] ) satisfy @xmath163 and @xmath164 for some constant @xmath165 .",
    "equations ( [ operlwbda ] ) and ( [ operlwbdb ] ) together imply @xmath166 for multivariate normal distributions and this proves theorem operlower.bd.thm .",
    "we shall establish the lower bound ( [ operlwbda ] ) by using assouad s lemma in section [ sec.assouad ] and the lower bound ( operlwbdb ) by using le cam s method and a two - point argument in section [ sec.lecom ] .",
    "the key technical tool to establish equation ( [ operlwbda ] ) is assouad s lemma in assouad ( @xcite ) .",
    "it gives a lower bound for the maximum risk over the parameter set @xmath167 to the problem of estimating an arbitrary quantity @xmath168 , belonging to a metric space with metric @xmath169 .",
    "let @xmath170 be the hamming distance on @xmath171 , which counts the number of positions at which @xmath172 and @xmath173 differ",
    ". for two probability measures @xmath174 and @xmath175 with density @xmath0 and @xmath176 with respect to any common dominating measure @xmath177 , write the total variation affinity @xmath178 .",
    "assouad s lemma provides a minimax lower bound for estimating @xmath179 .",
    "[ assouad ] let @xmath167 and let @xmath180 be an estimator based on an observation from a distribution in the collection @xmath181 .",
    "then for all @xmath182 @xmath183    assouad s lemma is connected to multiple comparisons . in total",
    "there are @xmath35 comparisons",
    ". the lower bound has three factors .",
    "the first factor is basically the minimum cost of making a mistake per comparison , and the last factor is the lower bound for the total probability of making type i and type ii errors for each comparison , and @xmath184 is the expected number of mistakes one makes when @xmath185 and @xmath186 are not distinguishable from each other when @xmath187 .",
    "we now prove the lower bound ( [ operlwbda ] ) .",
    "let @xmath188 with @xmath189 .",
    "denote the joint distribution by @xmath190 .",
    "applying assouad s lemma to the parameter space @xmath191 , we have @xmath192\\\\[-8pt ] & & \\qquad\\geq \\min_{h ( \\theta,\\theta^{\\prime } ) \\geq1}\\frac { \\vert\\sigma ( \\theta ) -\\sigma ( \\theta^{\\prime } ) \\vert ^{2}}{h ( \\theta,\\theta^{\\prime } ) } \\frac{k}{2}\\min_{h ( \\theta , \\theta^{\\prime } ) = 1 } \\vert p_{\\theta}\\wedge p_{\\theta^{\\prime } } \\vert.\\nonumber\\end{aligned}\\ ] ] we shall state the bounds for the the first and third factors on the right - hand side of ( [ lwd.a ] ) in two lemmas .",
    "the proofs of these lemmas are given in section [ sec.proofs ] .",
    "[ dffbd ] let @xmath193 be defined as in ( [ f11 ] ) .",
    "then for some constant @xmath165 @xmath194    [ affbd]let @xmath195 with @xmath196 .",
    "denote the joint distribution by @xmath190 .",
    "then for some constant @xmath165 @xmath197    it then follows from lemmas [ dffbd ] and [ affbd ] together , with the fact @xmath74 , @xmath198 for some @xmath199 .",
    "we now apply le cam s method to derive the lower bound ( [ operlwbdb ] ) for the minimax risk .",
    "let @xmath200 be an observation from a distribution in the collection @xmath201 where @xmath202 .",
    "le cam s method , which is based on a two - point testing argument , gives a lower bound for the maximum estimation risk over the parameter set @xmath203 .",
    "more specifically , let @xmath204 be the loss function . define @xmath205 $ ] and @xmath206 , and denote @xmath207 .",
    "[ lecam ] let @xmath180 be an estimator of @xmath172 based on an observation from a distribution in the collection @xmath208 , then @xmath209    we refer to yu ( @xcite ) for more detailed discussions on le cam s method .    to apply le cam s method , we need to first construct a parameter set . for @xmath210 , let @xmath211 be a diagonal covariance matrix with @xmath212 , @xmath213 for @xmath214 , and let @xmath215 be the identity matrix .",
    "let @xmath216 , and denote the joint density of @xmath217 by @xmath218 , @xmath219 with @xmath220 , which can be written as follows:@xmath221 where @xmath222 , @xmath223 or @xmath224 , is the density of @xmath225 .",
    "denote by @xmath226 the joint density of @xmath227 when @xmath228 .",
    "let @xmath229 for @xmath219 and the loss function @xmath230 be the squared operator norm .",
    "it is easy to see @xmath231 for all @xmath232 .",
    "then the lower bound ( [ operlwbdb ] ) follows immediately from lemma [ lecam ] if there is a constant @xmath233 such that @xmath234    note that for any two densities @xmath235 and @xmath236 , @xmath237 , and jensen s inequality implies @xmath238 ^{2}= \\biggl ( \\int \\biggl\\vert\\frac{q_{0}-q_{1}}{q_{1 } } \\biggr\\vert q_{1}\\,d\\mu\\biggr ) ^{2}\\leq \\int\\frac { ( q_{0}-q_{1 } ) ^{2}}{q_{1}}\\,d\\mu=\\int\\frac { q_{0}^{2}}{% q_{1}}\\,d\\mu-1.\\ ] ] hence @xmath239 . to establish equation ( [ affbd2 ] ) , it thus suffices to show that @xmath240 , that is,@xmath241 we now calculate @xmath242 .",
    "for @xmath243 it is easy to see@xmath244 when @xmath245 , we have@xmath246 \\,dx_{m}^{i } \\\\ & = & [ 1- ( 1-\\sigma_{mm } ) ^{2 } ] ^{-n/2}= \\biggl ( 1-\\tau \\frac{\\log p_{1}}{n } \\biggr ) ^{-n/2}.\\end{aligned}\\ ] ] thus@xmath247\\\\[-8pt ] & & \\qquad\\leq \\frac{1}{p_{1 } } \\biggl ( 1-\\tau\\frac{\\log p_{1}}{n } \\biggr ) ^{-n/2}-\\frac{1}{p_{1 } } \\nonumber\\\\ & & \\qquad=\\exp\\biggl [ -\\log p_{1}-\\frac{n}{2}\\log\\biggl ( 1-\\tau\\frac{\\log p_{1}}{n}% \\biggr ) \\biggr ] -\\frac{1}{p_{1}}\\rightarrow0\\nonumber\\end{aligned}\\ ] ] for @xmath248 , where the last step follows from the inequality @xmath249 for @xmath250 .",
    "equation ( [ affbd3 ] ) , together with lemma [ lecam ] , now immediately implies the lower bound given in ( [ operlwbdb ] ) .",
    "[ logp ] in covariance matrix estimation literature , it is commonly assumed that @xmath251 .",
    "see , for example , bickel and levina ( @xcite ) .",
    "the lower bound given in this section implies that this assumption is necessary for estimating the covariance matrix consistently under the operator norm .",
    "theorems [ operupper.bd.thm ] and [ operlower.bd.thm ] together show that the minimax risk for estimating the covariance matrices over the distribution space @xmath16 satisfies , for @xmath252 , @xmath253 the results also show that the tapering estimator @xmath49 with tapering parameter @xmath74 attains the optimal rate of convergence @xmath137 .",
    "a few interesting points can be made on the optimal rate of convergence @xmath137 . when the dimension @xmath0 is relatively small , that is , @xmath254 , @xmath0 has no effect on the convergence rate and the rate is purely driven by the `` smoothness '' parameter @xmath32 .",
    "however , when @xmath0 is large , that is , @xmath255 , @xmath0 plays a significant role in determining the minimax rate .",
    "we should emphasize that the optimal choice of the tapering parameter @xmath256 is different from the optimal choice for estimating the rows / columns as vectors under mean squared error loss .",
    "straightforward calculation shows that in the latter case the best cutoff is @xmath257 so that the tradeoff between the squared bias and the variance is optimal . with @xmath258 ,",
    "the tapering estimator has smaller squared bias than the variance as a vector estimator of each row / column .",
    "it is also interesting to compare our results with those given in bickel and levina ( @xcite ) .",
    "a banding estimator with bandwidth @xmath259 was proposed and the rate of convergence @xmath260 was proved .",
    "it is easy to see that the banding estimator given in bickel and levina ( @xcite ) is not rate optimal .",
    "take , for example , @xmath261 and @xmath262 .",
    "their rate is @xmath263 , while the optimal rate in theorem minimaxope is @xmath264 .",
    "it is instructive to take a closer look at the motivation behind the construction of the banding estimator in bickel and levina ( @xcite ) .",
    "let the banding estimator be @xmath265 and denote @xmath266 by @xmath267 , and let @xmath268 .",
    "an important step in the proof of theorem 1 in bickel and levina ( @xcite ) is to control the operator norm by the @xmath98 norm as follows : @xmath269 note that @xmath270 \\asymp1/\\sqrt{n}$ ] , then @xmath271 .",
    "it is then expected that @xmath272 [ see bickel and levina ( @xcite ) for details ] and so@xmath273 an optimal tradeoff of @xmath35 is then @xmath274 which implies a rate of @xmath275 in theorem 1 in bickel and levina ( @xcite ) .",
    "this rate is slower than the optimal rate @xmath137 in theorem [ minimaxope ] .",
    "we have considered the parameter space @xmath276 defined in ( [ paraspace ] ) .",
    "other similar parameter spaces can also be considered .",
    "for example , in time series analysis it is often assumed the covariance @xmath277 decays at the rate @xmath278 for some @xmath27 .",
    "consider the collection of positive - definite symmetric matrices satisfying the following conditions : @xmath279\\\\[-8pt ] & = & \\bigl\\ { \\sigma \\dvtx \\vert\\sigma_{ij } \\vert\\leq m_{1 } \\vert i - j \\vert ^{-(\\alpha+1)}\\mbox { for } i\\neq j \\mbox { and } \\lambda_{\\max } ( \\sigma ) \\leq m_{0 } \\bigr\\},\\nonumber\\end{aligned}\\ ] ] where @xmath26 is the maximum eigenvalues of the matrix @xmath15 . note that @xmath280 , @xmath281 is a subset of @xmath282 as long as @xmath283 .",
    "using virtually identical arguments one can show that @xmath284 let @xmath285 denote the set of distributions of @xmath286 that satisfies ( [ subgau ] ) and  ( [ paraspace.g ] ) .",
    "[ nonpd ] both the tapering estimator proposed in this paper and banding estimator given in bickel and levina ( @xcite ) are not necessarily positive - semidefinite . a practical proposal to avoid this would be to project the estimator @xmath95 to the space of positive - semidefinite matrices under the operator norm .",
    "more specifically , one may first diagonalize @xmath95 and then replace negative eigenvalues by  @xmath31 .",
    "the resulting estimator is then positive - semidefinite .",
    "we have focused on the case @xmath93 in sections sec.uppbd and [ sec.lowbd ] .",
    "the case of @xmath60 can be handled in a similar way .",
    "the main difference is that in this case we no longer have a tapering estimator @xmath49 with @xmath74 because @xmath287 .",
    "instead the maximum likelihood estimator @xmath23 can be used directly .",
    "it is easy to show in this case @xmath288 the lower bound can also be obtained by the application of assouad s lemma and by using a parameter space that is similar to @xmath191 . to be more specific , for an integer @xmath289 ,",
    "define the @xmath143 matrix @xmath290 with @xmath291 define the collection of @xmath292 covariance matrices as @xmath293 since @xmath60 , then @xmath294 .",
    "again it is easy to check @xmath295 when @xmath150 .",
    "the following lower bound then follows from the same argument as in section [ sec.assouad ] : @xmath296 equations ( [ operupbd.mle ] ) and ( [ operlwbd.mle ] ) together yield the minimax rate of convergence for the case @xmath297 , @xmath298 this , together with equation ( [ minimax.rate ] ) , gives the optimal rate of convergence : @xmath299",
    "in addition to the operator norm , the frobenius norm is another commonly used matrix norm .",
    "the frobenius norm is used in defining the numerical rank of a matrix which is useful in many applications , such as the principle component analysis .",
    "see , for example , rudelson and vershynin ( @xcite ) . the frobenius norm has also been used in the literature for measuring the accuracy of a covariance matrix estimator .",
    "see , for example , lam and fan ( @xcite ) and ravikumar et al .",
    "( @xcite ) . in this section",
    "we consider the optimal rate of convergence for covariance matrix estimation under the frobenius norm .",
    "the frobenius norm of a matrix @xmath300 is defined as the @xmath301 vector norm of all entries in the matrix @xmath302 this is equivalent to treating the matrix @xmath11 as a vector of length @xmath303 .",
    "it is easy to see that the operator norm is bounded by the frobenius norm , that is , @xmath304 .",
    "the following theorem gives the minimax rate of convergence for estimating the covariance matrix @xmath5 under the frobenius norm based on the sample @xmath305 .",
    "[ minimaxfro]the minimax risk under the frobenius norm satisfies @xmath306\\\\[-8pt ] & \\asymp&\\min \\biggl\\ { n^{-% ( { 2\\alpha+1})/({2 ( \\alpha+1 ) } ) } , \\frac{p}{n } \\biggr\\}.\\nonumber\\end{aligned}\\ ] ]    we shall establish below separately the minimax upper bound and minimax lower bound",
    ".      we will only prove the upper bound for the distribution set @xmath307 given in ( [ paraspace.g ] ) .",
    "the proof for the parameter space @xmath16 is slightly more involved by thresholding procedures as in wavelet estimation .",
    "the minimax upper bound is derived by again considering the tapering estimator ( tapering.est ) . under the frobenius norm",
    "the risk function is separable .",
    "the risk of the tapering estimator can be bounded separately under the squared @xmath56 loss for each row / column .",
    "this method has been commonly used in nonparametric function estimation using orthogonal basis expansions . since @xmath308 for the tapering estimator ( [ tapering.est ] ) , we have @xmath309 it can be seen easily that@xmath310 \\\\ & \\equiv & r_{1}+r_{2}.\\end{aligned}\\ ] ] the assumption @xmath311 implies that @xmath312 for all @xmath313 . since @xmath314 is also uniformly bounded for all @xmath315 from assumption ( [ paraspace.g ] ) , we immediately have @xmath316 .",
    "it is easy to show that@xmath317 where @xmath318 for all @xmath315 .",
    "thus@xmath319 by choosing @xmath320 if @xmath321 , which is different from the choice of @xmath35 for the operator norm in  ( [ kope ] ) .",
    "if @xmath322 , we will choose @xmath323 , then the bias part is @xmath31 and consequently @xmath324    for the parameter space @xmath325 , under the frobenius norm the optimal tapering parameter @xmath35 is of the order @xmath326 .",
    "the rate of convergence of the tapering estimator with @xmath327 under the operator norm is @xmath328 which is slower than @xmath329 in ( [ rateoper ] ) .",
    "similarly , the optimal procedure under the operator norm is not rate optimal under the frobenius norm .",
    "therefore , the optimal choice of the tapering parameter @xmath35 critically depends on the norm under which the estimation accuracy is measured .",
    "similarly for @xmath325 , it can be shown that under the frobenius norm the banding estimator with @xmath327 is rate optimal . under the operator norm ,",
    "bickel and levina ( @xcite ) chose @xmath330 for the banding estimator which is close to @xmath326 up to a logarithmic factor of @xmath0 . on the other hand",
    ", it can be shown that for the parameter space @xmath16 no linear estimator can achieve the optimal convergence rate under the frobenius norm .",
    "it is sufficient to establish the lower bound for the parameter space @xmath325 given in ( [ paraspace.g ] ) . again",
    "the argument for @xmath16 is similar . as in the case of estimation under the operator norm",
    ", we need to construct a finite collection of multivariate normal distributions with a parameter space @xmath331 such that@xmath332 for some @xmath165 when @xmath333 .",
    "we construct @xmath334 as follows .",
    "let @xmath335 be a constant .",
    "define @xmath336 it is easy to verify that @xmath337 as @xmath338 .",
    "note that @xmath339 .    applying assouad s lemma with @xmath169 the frobenius norm and @xmath340 to the parameter space @xmath334 , we have @xmath341 note that@xmath342 ^{2}\\sum\\vert\\theta _ { ij}-\\theta_{ij}^{\\prime } \\vert^{2}}{h ( \\theta,\\theta ^{\\prime } ) } \\\\ & = & \\frac{\\tau^{2}}{p}n^{-1}.\\end{aligned}\\ ] ] it is easy to see that@xmath343    [ affbd1]let @xmath190 be the joint distribution of @xmath344 with @xmath345 .",
    "then for some constant @xmath199 we have@xmath346    we omit the proof of this lemma .",
    "it is very similar to and simpler than the proof of lemma [ affbd ] .",
    "from lemma [ affbd1 ] we have for some @xmath165@xmath347 thus@xmath348 which implies that the rate obtained in ( [ ratefro ] ) is optimal .",
    "the inverse of the covariance matrix @xmath349 is of significant interest in many statistical applications .",
    "the results and analysis given in section [ operatornorm.sec ] can be used to derive the optimal rate of convergence for estimating @xmath349 under the operator norm .    for estimating the inverse covariance matrix @xmath349 we require the minimum eigenvalue of @xmath5 to be bounded away from zero . for @xmath350",
    ", we define@xmath351 let @xmath352 denote the set of distributions of @xmath286 that satisfy ( [ paraspace ] ) , ( [ subgau ] ) and ( [ eiglw ] ) , and similarly , distributions in @xmath353 satisfy ( [ subgau ] ) , ( [ paraspace.g ] ) and ( [ eiglw ] ) .",
    "the following theorem gives the minimax rate of convergence for estimating @xmath354 .",
    "[ minimaxinverseope ] the minimax risk of estimating the inverse covariance matrix @xmath349 satisfies @xmath355 where @xmath356 denotes either @xmath357 or @xmath358 .",
    "we shall focus on the case @xmath93 .",
    "the proof for the case of @xmath60 is similar . to establish the upper bound , note that @xmath359 then@xmath360 it follows from assumption ( [ paraspace ] ) that @xmath361 .",
    "note that @xmath362 for any @xmath363 which decays faster than any polynomial of @xmath9 as shown in the proof of lemmas [ estbias ] and [ estbiasbd ] .",
    "let @xmath364 and @xmath365 be the smallest eigenvalues of @xmath366 and @xmath367 , respectively .",
    "then@xmath368 decays faster than any polynomial of @xmath9 .",
    "let @xmath369 ^{2}$ ] and @xmath370 $ ] , then @xmath371 decays faster than any polynomial of @xmath9 .",
    "therefore , @xmath372 \\\\ & \\leq & c\\min\\biggl\\ { n^{-{2\\alpha}/({2\\alpha+1})}+\\frac{\\log p}{n } , % \\frac{p}{n } \\biggr\\}.\\end{aligned}\\ ] ]    the proof of the lower bound is almost identical to that of theorem minimaxope except that here we need to show@xmath373 instead of lemma [ dffbd ] . for a positive definite matrix @xmath11 ,",
    "let @xmath374 denote the minimum eigenvalue of @xmath11 .",
    "since@xmath375 we have @xmath376 note that @xmath377 then lemma [ dffbd ] implies @xmath378 for some constant @xmath165 .",
    "we now turn to the numerical performance of the proposed tapering estimator and compare it with that of the banding estimator of bickel and levina ( @xcite ) . in the numerical study , we shall consider estimating a covariance matrix in the parameter space @xmath276 defined in ( paraspace ) . specifically , we consider the covariance matrix @xmath379 of the form @xmath380 note that this is a toeplitz matrix .",
    "but we do not assume that the structure is known and do not use the information in any estimation procedure .",
    "the banding estimator in ( [ bandestimator ] ) depends on the choice of @xmath35 .",
    "an optimal tradeoff of @xmath35 is @xmath381 as discussed in section [ sec.discussion ] .",
    "see bickel and levina ( @xcite ) .",
    "the tapering estimator ( [ estimator ] ) also depends on @xmath35 for which the optimal tradeoff is @xmath382 . in our simulation study , we choose    @ld4.0cccccccccc@ & & & & & & + & & & & & & + @xmath383 & & * bl * & * czz * & * bl * & * czz * & * bl * & * czz * & * bl * & * czz * & * bl * & * czz * + 250 & 250 & 2.781 & 2.706 & 2.291 & 2.023 & 1.762 & 1.684 & 1.618 & 1.517 & _ 1.325 _ & _ 1.507 _ + & 500 & 2.409 & 2.302 & 1.898 & 1.575 & 1.562 & 1.204 & 1.361 & 1.185 & 1.080 & 0.822 + & 1000 & 2.029 & 1.685 & 1.631 & 1.361 & 1.289 & 1.018 & 1.056 & 0.795 & 0.911 & 0.859 + & 2000 & 1.706 & 1.153 & 1.369 & 1.122 & 1.106 & 0.908 & 0.878 & 0.655 & 0.715 & 0.542 + & 3000 & 1.522 & 0.926 & 1.242 & 0.896 & 0.983 & 0.798 & 0.810 & 0.658 & 0.645 & 0.482 + [ 4pt ] 500 & 250 & 3.277 &",
    "2.914 & 2.609 & 2.097 & 1.961 & 1.788 & 1.745 & 1.610 & _ 1.392 _ & _ 1.571 _ + & 500 & 2.901 & 2.598 & 2.199 & 1.683 & 1.751 & 1.256 & 1.475 & 1.234 & 1.152 & 0.865 + & 1000 & 2.539 & 2.197 & 1.942 & 1.472 & 1.481 & 1.064 & 1.178 & 0.843 & 0.984 & 0.917 + & 2000 & 2.263 & 1.726 & 1.669 & 1.326 & 1.293 & 0.965 & 1.067 & 0.700 & 0.866 & 0.569 + & 3000 & 2.066 & 1.379 & 1.538 & 1.154 & 1.220 & 0.874 & 0.919 & 0.696 & 0.781 & 0.503 + [ 4pt ] 1000 & 250 & 3.747 & 3.086 & 2.873 & 2.223 & 2.385 & 1.842 & 1.833 & 1.694 & _ 1.449 _ & _ 1.643 _ + & 500 & 3.370 & 2.735 & 2.635 & 1.768 & 1.906 & 1.334 & 1.565 & 1.297 & 1.203 & 0.925 + & 1000 & 3.097 & 2.437 & 2.315 & 1.536 & 1.741 & 1.121 & 1.382 & 0.883 & 1.037 & 0.936 + & 2000 & 2.730 & 2.177 & 2.011 & 1.392 & 1.523 & 1.006 & 1.156 & 0.722 & 0.920 & 0.591 + & 3000 & 2.589 & 1.968 & 1.865 & 1.264 & 1.374 & 0.911 & 1.072 & 0.723 & 0.834 & 0.523 + [ 4pt ] 2000 & 250 & 4.438 & 3.177 & 3.107 & 2.300 & 2.511 & 1.956 & 1.903 & 1.744 & _ 1.484 _ & _ 1.736 _ + & 500 & 3.969 & 2.800 & 2.868 & 1.841 & 2.030 & 1.383 & 1.638 & 1.356 & 1.239 & 0.940 + & 1000 & 3.538 & 2.531 & 2.551 & 1.599 & 1.866 & 1.158 & 1.452 & 0.912 & 1.074 & 0.973 + & 2000 & 3.242 & 2.353 & 2.248 & 1.434 & 1.649 & 1.031 & 1.224 & 0.751 & 0.955 & 0.611 + & 3000 & 3.025 & 2.219 & 2.101 & 1.302 & 1.566 & 0.929 & 1.141 & 0.743 & 0.868 & 0.541 + [ 4pt ] 3000 & 250 & 4.679 & 3.219 & 3.230 & 2.358 & 2.576 & 1.995 & 1.931 & 1.797 &",
    "_ 1.494 _ & _",
    "1.776 _ + & 500 & 4.214 & 2.887 & 2.991 & 1.890 & 2.282 & 1.419 & 1.664 & 1.384 & 1.463 & 0.971 + & 1000 & 3.901 & 2.575 & 2.674 & 1.633 & 1.933 & 1.186 & 1.482 & 0.929 & 1.224 & 0.990 + & 2000 & 3.488 & 2.395 & 2.452 & 1.451 & 1.717 & 1.049 & 1.254 & 0.768 & 0.965 & 0.619 + & 3000 & 3.336 & 2.278 & 2.288 & 1.321 & 1.632 & 0.948 & 1.172 & 0.750 & 0.880 & 0.549 +    @xmath384 for the banding estimator and @xmath385 for the tapering estimator .",
    "a range of parameter values for @xmath32 , @xmath9 and @xmath0 are considered .",
    "specifically , @xmath32 ranges from @xmath386 to @xmath387 , the sample size @xmath9 ranges from @xmath388 to @xmath389 and the dimension @xmath0 goes from @xmath388 to @xmath389 .",
    "we choose the value of @xmath390 to be @xmath391 so that all matrices are nonnegative definite and their smallest eigenvalues are close to @xmath31 . table [ table ] reports the average errors under the spectral norm over @xmath392 replications for the two procedures .",
    "the cases where the tapering estimator underperforms the banding estimator are highlighted in boldface .",
    "figure [ risk - comp ] plots the ratios of the average errors of the banding estimator to the corresponding average errors of the tapering estimator for @xmath393 and @xmath387 .",
    "the case of @xmath394 is similar to the case of @xmath395 .",
    "it can be seen from table [ table ] and figure [ risk - comp ] that the tapering estimator outperforms the banding estimator in 121 out of 125 cases . for the given dimension @xmath0 , the ratio of the average error of the banding estimator to the corresponding average error of the tapering estimator tends to increase as the sample size @xmath9 increases .",
    "the tapering estimator fails to outperform the banding estimator only when @xmath396 and @xmath397 in which case the values of @xmath35 are small for both estimators .",
    "we have also carried out additional simulations for larger values of @xmath32 with the same sample sizes and dimensions .",
    "the performance of the tapering and abnding estimators are similar .",
    "this is mainly dur to the fact that the values of @xmath35 for both estimators are very small for large @xmath32 when @xmath9 and @xmath0 are only moderately large .",
    "in this section we give proofs of auxiliary lemmas stated and used in sections [ operatornorm.sec][inverse.sec ] .",
    "proof of lemma [ est ] without loss of generality we assume that @xmath398 . the set @xmath399 is contained in the set @xmath400 if and only if @xmath401 , that is , @xmath402 . note that @xmath403 , then @xmath404 .",
    "similarly , we have     the bars are ordered from left to right by the sample sizes @xmath405 to @xmath406 . ]",
    "thus we have @xmath408    proof of lemma [ estbias ] without loss of generality we assume that @xmath0 is divisible by @xmath140 . recall that @xmath409 .",
    "note that @xmath410 is empty when @xmath411 , and has at least one nonzero entry when @xmath412 .",
    "set @xmath413 and @xmath414 .",
    "it follows from ( [ estimator ] ) that @xmath415 since @xmath416 are disjoint diagonal blocks over @xmath417 , we have @xmath418\\\\[-8pt ] & \\leq & m\\max_{1-m\\leq l\\leq p } \\bigl\\vert\\delta_{l}^{(m ) } \\bigr\\vert.\\nonumber\\end{aligned}\\ ] ] since @xmath419 and @xmath420 are all sub - blocks of certain matrix @xmath420 with @xmath421 , lemma estbias now follows immediately from equations ( [ bound22 ] ) and ( estimator ) .",
    "proof of lemma [ estbiasbd ] for any @xmath113 symmetric matrix @xmath11 , we have @xmath422 let @xmath423 be a @xmath424 net of the unit sphere @xmath425 in the euclidean distance in @xmath426 .",
    "we have@xmath427 which implies @xmath428 . since we are allowed to pack@xmath429 balls of radius @xmath430 into a @xmath431 ball in @xmath426 , volume comparison yields @xmath432 that is , @xmath433",
    "thus there exist @xmath434 such that@xmath435 this one - step approximation argument is similar to the proof of proposition 4.2(ii ) in zhang and huang ( @xcite ) .",
    "let @xmath1 be i.i.d .",
    "@xmath0-vectors with @xmath436 . under the sub - gaussian assumption in ( [ subgau ] )",
    "there exists @xmath64 such that @xmath437 which implies @xmath438 for all @xmath439 and @xmath440 , then there exists @xmath115 such that @xmath441 \\mathbf{v } \\biggr\\vert > x \\biggr\\ } \\leq e^{-nx^{2}\\rho_{1}/2}\\ ] ] for all @xmath117 and @xmath440 .",
    "[ see , e.g. , chapter 2 in saulis and statuleviius ( @xcite ) . ]",
    "thus we have @xmath442    proof of lemma [ dffbd ] set @xmath443 and let @xmath444 v.\\ ] ] note that there are exactly @xmath445 number of @xmath446 such that @xmath447 , and @xmath448 .",
    "this implies@xmath449 v \\vert_{2}^{2}% } { \\vert v \\vert_{2}^{2 } } \\geq\\frac{h(\\theta,\\theta ^{\\prime } ) \\cdot ( \\tau ka ) ^{2}}{k_{h}}\\\\ & = & h(\\theta,\\theta^{\\prime } ) \\cdot \\tau^{2}k_{h}a^{2}.\\end{aligned}\\ ] ]    proof of lemma [ affbd ] when @xmath450 , we will show @xmath451 \\\\ & \\leq & n\\cdot cka^{2}\\end{aligned}\\ ] ] for some small @xmath165 , where @xmath452 is the kullback ",
    "leibler divergence and the first inequality follows from the well - known pinsker s inequality [ see , e.g. , csiszr ( @xcite ) ] .",
    "this immediately implies the @xmath453 distance between two measures is bounded away from @xmath104 , and then the lemma follows .",
    "write @xmath454 then@xmath455 let @xmath456 be the eigenvalues of @xmath457 .",
    "since @xmath458 is similar to the symmetric matrix @xmath459 , and@xmath460 then all eigenvalues @xmath456 s are real and in the interval @xmath461 $ ] , where @xmath462 .",
    "note that the taylor expansion yields@xmath463 where @xmath464 write @xmath465 , where @xmath466 and @xmath267 is a diagonal matrix .",
    "it follows from the fact that the frobenius norm of a matrix remains the same after an orthogonal transformation that @xmath467",
    "the authors would like to thank james x. hu for assistance in carrying out the simulation study in section [ simulation.sec ] .",
    "we also thank the associate editor and three referees for thorough and useful comments which have helped to improve the presentation of the paper .",
    "ravikumar , p. , wainwright , m. j. , raskutti , g. and yu , b. ( 2008 ) . high - dimensional covariance estimation by minimizing @xmath468-penalized log - determinant divergence .",
    "technical report , univ .",
    "california , berkeley ."
  ],
  "abstract_text": [
    "<S> covariance matrix plays a central role in multivariate statistical analysis . </S>",
    "<S> significant advances have been made recently on developing both theory and methodology for estimating large covariance matrices . however , a minimax theory has yet been developed . in this paper </S>",
    "<S> we establish the optimal rates of convergence for estimating the covariance matrix under both the operator norm and frobenius norm . </S>",
    "<S> it is shown that optimal procedures under the two norms are different and consequently matrix estimation under the operator norm is fundamentally different from vector estimation . </S>",
    "<S> the minimax upper bound is obtained by constructing a special class of tapering estimators and by studying their risk properties . </S>",
    "<S> a key step in obtaining the optimal rate of convergence is the derivation of the minimax lower bound . </S>",
    "<S> the technical analysis requires new ideas that are quite different from those used in the more conventional function / sequence estimation problems .    ,    and    .    </S>"
  ]
}