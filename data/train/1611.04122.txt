{
  "article_text": [
    "traditional document classification approaches , either monolingual or cross - lingual , use labeled documents to train a model , and apply the model to test documents . however , when we change the test documents language or change the label space from one domain to another , we need to re - train the classifiers .",
    "thus , traditional approaches do not scale well in popular languages , and becomes completely infeasible if we want to classify documents in a small language into any ontology of categories .",
    "cross - lingual dataless document classification ( * clddc * ) provides a way to classify documents without ( re-)training the model , and can thus easily adapt to new domains and many languages  @xcite .",
    "clddc only requires knowing the english name or a short description of each classification category , and works by embedding the label ( in english ) and the document ( in the other language ) jointly into the same semantic space .",
    "clddc generalizes monolingual dataless classification  @xcite by making use of cross - lingual explicit semantic analysis ( * clesa * )  @xcite , a generalization of the english explicit semantic analysis ( * esa * )  @xcite .",
    "however , clddc depends heavily on the availability of large enough wikipedia in the documents language , since it uses the language links in the wikipedia title space between this language and english .",
    "specifically , it has been applied to classify documents in 180 languages that have at least some wikipedia presence  @xcite .",
    "there are more than 7,000 known spoken languages , and 2,287 of them have writing systems .",
    "this means that the clddc approach can only work on limited proportion of languages in the world .    [",
    "cols=\"<,<,<,<\",options=\"header \" , ]      table  [ tab : ranking ] shows the results .",
    "`` original '' row is the result for original clddc .",
    "we compute the mean and standard deviation for the 39 swls as well as the correlation of clddc with the best bridged clddc .",
    "the correlation value is negative .",
    "according to figure  [ fig : comparison - best ] , it seems the improvement over smaller original clddc accuracies is larger than the ones with larger clddc accuracies .",
    "`` best bridge ( google ) '' row shows the results of bridged clddc results with the best bridge lwls .",
    "this is the upper - bound of all the other ranking based methods .",
    "we can also see from figure  [ fig : comparison - best ] that the result is significantly better .",
    "although the variance of the results is large , the t - test result still shows significance .",
    "`` best bridge ( panlex ) '' shows no significant improvement over original clddc .",
    "however , panlex has much more languages than wikipedia and google translate .",
    "thus , it might be still useful when there is something than no resource at all .",
    "`` linguistic '' row shows the results of bridged lwls ranked by @xmath0 in eq .",
    "( [ eq : linguistic ] ) .",
    "it is significantly better than original clddc at 0.05 level .",
    "`` wikipedia wikipedia language links '' row shows the results ranked by @xmath1 in eq .",
    "( [ eq : wikisize ] ) .",
    "@xmath1 is almost the same as @xmath2 , since for most of the languages , english has the largest language link number . `` wikipedia size '' row shows the results of bridged lwls ranked by @xmath2 in eq .",
    "( [ eq : wikisize ] ) .",
    "@xmath2 will always rank english as the bridge language .",
    "we have two interesting findings from the results .",
    "first , the ranking is not significantly better than original clddc , and worse than `` linguistic . ''",
    "this means that , bridging swls with english by mapping only words may not be a better solution compared to using cross - lingual wikipedia , even though the cross - lingual wikipedia is not good enough .",
    "second , the correlation value between `` wikipedia size '' and `` best bridge '' is higher than the correlation value between `` linguistic '' and `` best bridge . ''",
    "however , the dependent correlation test  @xcite shows this improvement is not significant .",
    "`` combination '' row shows the results of bridged lwls ranked by @xmath2 in eq .",
    "( [ eq : combination ] ) .",
    "the results show that combining the `` linguistic '' and `` wikipedia size '' features by hand shows no improvement over pure `` linguistic '' features .    `` ranksvm '' row shows the results using ranksvm .",
    "we split the swls into five folds .",
    "then we perform a five - fold cross validation to generate the results . for each validation",
    ", we use 80% of the swls as training data , where each language has 49 lwls accuracies .",
    "we use the 49 accuracies to generate @xmath3 pairs",
    ". then we use the learnt model to rank the other 20% swls .",
    "after the five - fold cross validation , we can rank all the swls based on the each learnt model .",
    "we tune the parameter of @xmath4 using a grid search in @xmath5 .",
    "the average result over 39 swls is significantly better than original clddc ( @xmath6 ) and `` majority voting '' ( @xmath7 ) .",
    "the correlation with `` best bridge '' is also significantly better than `` linguistic '' with `` best bridge . ''",
    "this means that machine learning based method is significantly better than the unsupervised voting and ranking with handcrafted similarities . by looking into the averaged weights of ranksvm in five fold cross validation , we select the five top weights with largest absolute values , which are : `` internally - headed relative clauses '' ( -0.3613 ) , `` front rounded vowels '' ( 0.1656 ) , `` absence of common consonants '' ( 0.1538 ) , `` optional double negation in svo languages '' ( -0.1402 ) , `` number of genders '' ( 0.1385 ) .",
    "in this section , we briefly survey some related work .      cross - lingual document classification has attracted more attention recently in low - resource settings . where target language training data is minimal or unavailable .",
    "it is a natural sub - topic of transfer learning  @xcite . in cross - lingual document classification",
    ", we train a classifier on labeled documents in the _ source _ language , and classify documents in the _ target _ language .",
    "existing approaches either need a parallel corpus to train word embeddings for different languages  @xcite , require labeled documents in both source and target languages  @xcite , make use of machine translation techniques to translate words  @xcite or documents  @xcite , or combine different approaches  @xcite . among the existing approaches , word translation is the cheapest way , while document translation and annotation on the target domain are the most expensive . in the middle ,",
    "parallel or comparable corpora may be used to learn a good word / document representation , which avoids document translation but can still find a correspondence between source and target languages .",
    "the strength of cross - lingual document classification is that it can be generalized to multiple languages even in the absence of resources .",
    "however , when we change the label space from one domain to another , we should perform translation again and re - train the classifiers . instead of using parallel corpus to train a classifier or train a translation model",
    ", our approach only needs the comparable corpus , wikipedia , in different languages aligned with english .",
    "then if a user can tell the name of the category , cross - lingual dataless classification can perform text classification on - the - fly . for lwls ,",
    "clddc is comparable to supervised learning method with about 100 to 200 labeled document per label  @xcite .",
    "clddc does not need training data in source language .",
    "instead , it only needs the label names and the comparable corpus , wikipedia , in different languages aligned with english . for lwls ,",
    "clddc is comparable to supervised learning method with about 100 to 200 labeled document per label  @xcite .",
    "clearly cross - lingual dataless classification is related to representation learning .",
    "our current approach is based on esa  @xcite , which is built based on wikipedia inverted index . essentially , esa is a distributional representation of document - level context of words .",
    "it regards each wikipedia page as a concept corresponding to an entity , category , or topic .",
    "then each word is represented by its related concepts .",
    "cross - lingual esa leverages the language links in wikipeida to relate pages in other languages to english .",
    "then words or texts in two different languages ( english and another ) can be mapped to the same semantic space represented by wikipedia concepts .",
    "recently , motivated by the simplicity and success of neural network based word embedding   @xcite , multi - lingual  @xcite , or cross - lingual  @xcite representation learning are also investigated .",
    "other representations such as cross - lingual topic models  @xcite or brown clusters  @xcite were also studied .",
    "similar to cross - lingual classification , these representation learning approaches need either parallel corpora  @xcite , some labeled data in the target domain  @xcite , or words being ( partially ) aligned in a dictionary @xcite .",
    "it has been shown that clesa outperforms one of popular the cross - lingual embedding approach  @xcite on two benchmark datasets for clddc  @xcite .",
    "pivot language is used to help machine translation when there is no enough resources to train a translation model from source language to target language  @xcite .",
    "for example , paul et al . ,  @xcite used 22 indo - european and asian languages to evaluate how to select a good pivot language for machine translation .",
    "they evaluated 45 features falling into eight categories .",
    "besides the language family feature , they used more translation - relevant features such as length of sentence , reordering , overlap of vocabulary , etc .",
    "they showed that the final result is mostly affected by the source - pivot and pivot - target translation performance .",
    "they also mentioned machine learning based method in the future work , but we are unaware of a follow - up paper that succeeded in doing it .    different from machine translation which needs the sentence level source - pivot and pivot - target translation , in cross - lingual classification , it is sufficient to use word dictionaries , making borrowing a bridging language more scalable to many languages and thus more practically useful .",
    "to the best of our knowledge , we have studied largest number of lwls ( 49 ) and swls ( 39 ) with largest number of linguistic features ( 196 ) .",
    "zero - shot learning  @xcite and one - shot learning  @xcite were first introduced in the computer vision community and are now recognized by the natural language processing community  @xcite .",
    "one - shot learning requires one example for training , while in zero - shot learning , the test data is different from the training data ( e.g. , a new label space ) . however , in contrast to the dataless scenario , both learning protocols require some training data .",
    "the dataless classification protocol , on the other hand , assumes no direct training data but only the label names or descriptions .",
    "compared to one - shot learning , the labels can be relatively simpler .",
    "in addition , it relies on background data from external knowledge sources ( like wikipedia ) , that is used in an unsupervised way to generate a common semantic space .",
    "we studied the problem of clddc for swls .",
    "clddc uses english labels to classify documents in other languages and is scalable to many languages and adaptive to any label space .",
    "however , if wikipedia for a language is not large enough , the performance is not acceptable . in this paper , we simply map the words in swl documents to lwl words , and perform dataless classification based on lwls .",
    "we systematically evaluate 39 swls and 49 lwls .",
    "experiments show that bridging the swls with lwls can significantly improve the classification results . moreover",
    ", learning from the existing ranking results can be generalized to other languages .",
    "leusch , g. , max , a. , crego , j.  m. ,  ney , h. 2010 .",
    "multi - pivot translation by system combination in 2010 international workshop on spoken language translation , iwslt 2010 , paris , france , december 2 - 3 , 2010 ,  299306 ."
  ],
  "abstract_text": [
    "<S> this paper presents an approach to classify documents in any language into an english topical label space , without any text categorization training data . </S>",
    "<S> the approach , cross - lingual dataless document classification ( clddc ) relies on mapping the english labels or short category description into a wikipedia - based semantic representation , and on the use of the target language wikipedia . </S>",
    "<S> consequently , performance could suffer when wikipedia in the target language is small . in this paper , we focus on languages with small wikipedias , ( small - wikipedia languages , swls ) . </S>",
    "<S> we use a word - level dictionary to convert documents in a swl to a large - wikipedia language ( lwls ) , and then perform clddc based on the lwl s wikipedia . </S>",
    "<S> this approach can be applied to _ thousands of languages _ , which can be contrasted with machine translation , which is a supervision heavy approach and can be done for about 100 languages . </S>",
    "<S> we also develop a ranking algorithm that makes use of language similarity metrics to automatically select a good lwl , and show that this significantly improves classification of swls documents , performing comparably to the best bridge possible . </S>"
  ]
}