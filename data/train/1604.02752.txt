{
  "article_text": [
    "many scientific and engineering problems  @xcite can be approximated as linear systems of the form @xmath0 where @xmath1 is the unknown input signal , @xmath2 is the matrix that characterizes the linear system , and @xmath3 is measurement noise .",
    "the goal is to estimate @xmath4 from the noisy measurements @xmath5 given @xmath6 and statistical information about @xmath7 .",
    "alternately , one could view the estimation of @xmath4 as fitting or learning a linear model for the data comprised of @xmath5 and @xmath6 .",
    "when @xmath8 , the setup   is known as compressed sensing ( cs )  @xcite ; by posing a sparsity or compressibility requirement on the signal , it is indeed possible to accurately recover @xmath4 from the ill - posed linear system  @xcite when the number of measurements @xmath9 is large enough , and the noise level is modest .",
    "however , we might need @xmath10 when the signal is dense or the noise is substantial .",
    "approximate message passing ( amp )  @xcite is an iterative framework that solves linear inverse problems by successively decoupling  @xcite matrix channel problems into scalar channel denoising problems with additive white gaussian noise ( awgn ) .",
    "amp has received considerable attention , because of its fast convergence and the state evolution ( se ) formalism  @xcite , which offers a precise characterization of the awgn denoising problem in each iteration . in the bayesian",
    "setting , amp often achieves the minimum mean squared error ( mmse )  @xcite in the limit of large linear systems .",
    "in real - world applications , a multi - processor ( mp ) version of cs could be of interest , due to either storage limitations in each individual processor node , or the need for fast computation .",
    "this paper considers multi - processor cs ( mp - cs )  @xcite , in which there are @xmath11 _ distributed nodes _",
    "( processor nodes ) and a _ fusion center_.",
    "each distributed node stores @xmath12 rows of the matrix @xmath6 , and acquires the corresponding linear measurements of the underlying signal @xmath4 . without loss of generality , we model the measurement system in distributed node @xmath13 as @xmath14 where @xmath15 is the @xmath16-th row of @xmath6 , and @xmath17 and @xmath18 are the @xmath16-th entries of @xmath5 and @xmath7 , respectively . once every @xmath17 is collected , we run distributed algorithms among the fusion center and @xmath11 distributed nodes to reconstruct the signal @xmath4 .",
    "mp versions of amp ( mp - amp ) for mp - cs have been studied in the literature  @xcite .",
    "usually , mp platforms are designed for distributed settings such as sensor networks  @xcite or large - scale  big data \" computing systems  @xcite .",
    "we reduce the communication costs of mp platforms by applying lossy compression  @xcite to the communication portion of mp - amp .    in this paper , we consider a rich design space that includes various costs , such as the number of iterations @xmath19 , aggregate coding rates @xmath20 ( defined later in  ) , and the mean squared error ( mse ) achieved by the reconstruction algorithm . in such a rich design space , reducing any cost is likely to incur an increase in other costs , and it is difficult to simultaneously minimize all the costs .",
    "han et al .",
    "@xcite reduce the communication costs , and ma et al .",
    "@xcite develop an algorithm with reduced computation ; both works  @xcite achieve a reasonable mse .",
    "however , the optimal trade - offs in this rich design space are not studied .",
    "we pose the problem of finding the best trade - offs among the individual costs @xmath21 , and @xmath22 as a multi - objective optimization problem ( mop ) , and study the properties of the pareto optimal tuples @xmath23  @xcite of this mop .",
    "( note that we do not intend to provide a practical implementation to achieve the optimal trade - offs . ) finally , we conjecture that the combined cost of computation and communication scales as @xmath24 ; these properties are verified numerically using a dynamic programming ( dp , cf .",
    "bertsekas  @xcite ) scheme from our prior work  @xcite .",
    "in the linear system  , we consider an independent and identically distributed ( i.i.d . )",
    "gaussian measurement matrix @xmath6 , i.e. , @xmath25 .",
    "the signal entries follow an i.i.d .",
    "_ bernoulli gaussian _ distribution , @xmath26 where @xmath27 is the dirac delta function and @xmath28 is called the _ sparsity rate _ of the signal .",
    "the noise entries obey @xmath29 , where @xmath30 is the noise variance .",
    "note that the results in this paper can be easily extended to priors other than  .",
    "starting from @xmath31 , the amp framework  @xcite proceeds iteratively according to @xmath32 where @xmath33 is a denoising function , @xmath34 is shorthand for the derivative of @xmath33 , and  @xmath35 for some vector  @xmath36 .",
    "the subscript @xmath37 represents the iteration index , @xmath38 denotes transpose , and @xmath39 is the measurement rate . owing to the decoupling effect  @xcite , in each amp iteration  @xcite , the vector  @xmath40 in ( [ eq : ampiter1 ] ) is statistically equivalent to the input signal @xmath41 corrupted by awgn @xmath42 generated by a source @xmath43 , @xmath44 in large systems ( @xmath45 ) , a useful property of amp  @xcite is that the noise variance @xmath46 evolves following state evolution ( se ) : @xmath47 , where the mean squared error @xmath48 $ ] , @xmath49 is expectation with respect to @xmath50 and @xmath51 , and @xmath52 is the source that generates @xmath4 .",
    "note that @xmath53}{\\kappa}$ ] , because of the all - zero initial estimate for @xmath4 .",
    "formal statements for se appear in prior work  @xcite .",
    "this paper considers the bayesian setting , in which we assume knowledge of the true prior for the signal @xmath4 .",
    "therefore , the mmse - achieving denoiser is the conditional expectation , @xmath54 $ ] , which can be easily obtained .",
    "other denoisers such as soft thresholding  @xcite yield mse s that are greater than that of the bayesian denoiser .",
    "when the true prior for @xmath4 is unavailable , parameter estimation techniques can be used .      in the sensing problem formulated in  , the measurement matrix is stored in a distributed manner in each distributed node .",
    "lossy mp - amp  @xcite iteratively solves mp - cs problems using lossily compressed messages : @xmath55 @xmath56 @xmath57 @xmath58 where @xmath59 denotes quantization , and an mp - amp iteration refers to the process from   to  .",
    "the reader might notice that the fusion center also needs to transmit the denoised signal vector @xmath60 and a scalar @xmath61 to the distributed nodes .",
    "the transmission of the scalar @xmath61 is negligible , and the fusion center may broadcast @xmath60 so that naive compression of @xmath60 , such as compression with a fixed quantizer , is sufficient . hence , we will not discuss possible lossy compression of the messages transmitted by the fusion center .",
    "assume that we quantize @xmath62 , and use @xmath63 bits to encode the quantized vector @xmath64 .",
    "the _ coding rate _ is @xmath65 .",
    "we incur a _ distortion _ ( or quantization error ) @xmath66 at iteration @xmath37 in each distributed node , and noise @xmath7 are both i.i.d .",
    ", the expected distortions are the same over all @xmath11 nodes , and can be denoted by @xmath67 for simplicity .",
    "other distortion metrics @xmath68 can also be used  @xcite . ] where @xmath69 and @xmath70 are the @xmath16-th entries of the vectors @xmath71 and @xmath72 , respectively .",
    "the rate distortion function , denoted by @xmath73 , offers the fundamental information theoretic limit on the coding rate @xmath74 for communicating a sequence up to distortion @xmath75",
    "a pivotal conclusion from rd theory is that coding rates can be greatly reduced even if @xmath75 is quite small .",
    "the function @xmath73 can be computed in various ways  @xcite , and can be achieved by an rd - optimal quantization scheme .",
    "other quantization schemes require larger coding rates to achieve the same expected distortion @xmath75 .",
    "the goal of this paper is to understand the fundamental trade - offs for mp - cs using mp - amp .",
    "hence , throughout this paper , we assume that appropriate vector quantization ( vq ) schemes  @xcite that achieve @xmath73 are applied within each mp - amp iteration , although our analysis is readily extended to practical quantizers such as scalar quantizer with entropy coding  @xcite .",
    "therefore , the signal _ at the fusion center _ before denoising can be modeled as @xmath76 where @xmath42 is the equivalent scalar channel noise   and @xmath77 is the overall quantization error whose entries follow @xmath78 . for large block sizes ,",
    "we expect the vq quantization error , @xmath77 , to resemble gaussian noise , which is independent of @xmath79 .",
    "the se for the lossy mp - amp  @xcite follows @xmath80 where @xmath46 can be estimated by @xmath81 with @xmath82 denoting the @xmath83 norm  @xcite , and @xmath84 is the variance of @xmath85 .",
    "the rigorous justification of   by extending bayati and montanari  @xcite is left for future work .",
    "denote the coding rate used to transmit @xmath86 at iteration @xmath37 by @xmath87 .",
    "the sequence of @xmath88 , where @xmath19 is the total number of mp - amp iterations , is called the _ coding rate sequence _ , and is denoted by the vector @xmath89 $ ] .",
    "given the coding rate sequence @xmath90 , the distortion @xmath67 can be evaluated with @xmath73 , and the scalar channel noise variance @xmath46 can be evaluated with  . hence , the mse for @xmath90 can be predicted ; we call it se - predicted mse .",
    "the mse at the last iteration is called the _ final mse_.",
    "following the discussion of sec .  [",
    "sec : setting ] , we can see that the lossy compression of @xmath91 , can reduce communication costs . on the other hand , the greater the savings in the coding rate sequence @xmath90 ,",
    "the worse the final mse is expected to be .",
    "if a certain level of final mse is desired under a small budget of coding rates , more iterations @xmath19 will be needed .",
    "define the aggregate coding rate @xmath20 as the sum of all the coding rates in @xmath90 , @xmath92 as mentioned above , there is a trade - off between @xmath19 , @xmath20 , and the final mse , and there is no optimal solution that minimizes them simultaneously . to deal with such trade - offs in a multi - objective optimization ( mop ) problem ,",
    "it is customary to think about the concept of _ pareto optimality _  @xcite .",
    "define the computation cost rate , @xmath93 , as the cost of computation in one mp - amp iteration , and define the communication cost rate , @xmath94 , as the cost of transmitting 1 bit for @xmath95  .",
    "we further define the _ relative cost _ as @xmath96 for notational convenience , denote by @xmath97 all of the mse values that would be provided by the pair @xmath98 for some relative cost @xmath99  , among which the smallest mse is denoted by @xmath100 .",
    "furthermore , the achievable set @xmath101 is defined as denotes the set of non - negative real numbers . ] @xmath102 i.e. , there exists an instantiation of the mp - amp algorithm that could reconstruct the signal with @xmath19 iterations and an aggregate coding rate @xmath20 , and yield a certain mse .",
    "[ def : pareto ] _ the point @xmath103 is said to dominate another point @xmath104 , denoted by @xmath105 , if and only if @xmath106 , @xmath107 , and @xmath108 .",
    "a point @xmath109 is said to be pareto optimal if and only if there does not exist @xmath110 satisfying @xmath111 .",
    "_ furthermore , let @xmath112 denote the set of all pareto optimal points , @xmath113    in words , the tuple @xmath114 is pareto optimal if no other tuple @xmath115 exists such that @xmath116 , @xmath117 , and @xmath118 .",
    "these are the tuples that belong to the boundary of @xmath101 .",
    "we extend the definition of the number of iterations @xmath19 to a probabilistic one .",
    "we assume that the number of iterations is drawn from a probability distribution @xmath119 over @xmath120 , such that @xmath121 .",
    "of course , this definition contains a deterministic @xmath122 as a special case with @xmath123 and @xmath124 for all @xmath125 .",
    "armed with this definition of pareto optimality and the probabilistic definition of the number of iterations , we have the following lemma .",
    "[ th : convex1 ] _ for a fixed noise variance @xmath126 , measurement rate @xmath127 , and @xmath11 distributed nodes in mp - amp , the achievable set @xmath63 is a convex set .",
    "_    we need to show that for any @xmath128 , @xmath129 @xmath130 and any @xmath131 , @xmath132     ( top panel ) and @xmath133 ( bottom ) are shown as functions of @xmath37 .",
    "( @xmath134 , @xmath135 , @xmath136 , and @xmath137.),width=302 ]    we use the well - known time - sharing argument ( see cover and thomas  @xcite ) .",
    "assume that @xmath128 , @xmath138 are achieved by probability distributions @xmath139 and @xmath140 , respectively .",
    "let us select all the parameters of the first tuple with probability @xmath141 and those of the second tuple with probability @xmath142 .",
    "hence , we have @xmath143 . due to the linearity of expectation",
    ", we have @xmath144 , and @xmath145 . again , due to the linearity of expectation , @xmath146 , implying that   is satisfied , and the proof is complete .",
    "[ def : funcs ] _ let the function @xmath147 be the pareto optimal rate function , which is implicitly described as @xmath148 .",
    "we further define implicit functions @xmath149 and @xmath100 in a similar way .",
    "_    the functions @xmath150 , @xmath149 , and @xmath100 are convex in their arguments .",
    "note that our proof for the convexity of the set @xmath151 might be extended to other distributed iterative learning algorithms that might use lossy compression .",
    "these discussions raise the question whether we can provide some asymptotic analysis of the achievable region .",
    "we believe that such an analysis is indeed possible in the limit of mse that approaches the mmse .",
    "define the excess mse ( @xmath152 )  @xcite , @xmath153 .",
    "consider a case where we aim to reach a very low @xmath152 .",
    "montanari  @xcite provided a graphical interpretation of the relation between the mse performance of amp in iteration @xmath37 and the statistical properties of the denoiser @xmath33 being used . in the limit of small @xmath152 ,",
    "the @xmath152 decreases by a nearly - constant multiplicative factor in each amp iteration , yielding a geometric decay of the mmse . in mp - amp , in addition to the equivalent scalar channel noise @xmath42 , we have additive quantization error @xmath77  . in order for",
    "the @xmath152 in an mp - amp system to decay geometrically , the quantization error @xmath67 must decay at least as quickly . to obtain this geometric decay in @xmath67 , recall that in the high resolution limit , the distortion - rate function typically takes the form @xmath154",
    "@xcite , where @xmath155 is some constant .",
    "we propose for @xmath87 to have the form , @xmath156 where @xmath157 and @xmath158 are constants .",
    "this rate will not yield a distortion that decays exactly geometrically , because the distribution of @xmath72 will be dependent on @xmath37 .",
    "that said , in the limit of small @xmath152 , the distribution barely changes between iterations , and so it is plausible to expect @xmath159 $ ] , where the decay rate @xmath160 is a function of the extra coding rate @xmath158 per iteration ( [ eq : speculated_rt ] ) , and the multiplicative term @xmath161 converges to 1 in the limit of large @xmath37 , because the distribution barely changes between iterations for large @xmath37 . now that we have driven down the quantization error geometrically , we conjecture that the pareto optimal emse , @xmath162 , decays at the same rate , @xmath163.\\ ] ] combining   and  , and considering the definition of @xmath20  , the total computation and communication cost is @xmath164 , which is @xmath165 .",
    "we have the following conjecture .",
    "the total computation and communication cost scales as @xmath165 .    having provided this conjecture , we back it up numerically by running our unconstrained dp scheme ( sec .  [ sec : unconstraineddp ] )  @xcite on a problem with relatively small @xmath166 in the last iteration @xmath19 .",
    "consider reconstructing a bernoulli gaussian signal   with @xmath167 .",
    "the signal is measured in an mp platform with @xmath135 distributed nodes according to  .",
    "the measurement rate is @xmath168 , and the noise variance is @xmath136 .",
    "the relative cost is @xmath137  .",
    "[ fig : rtandemse ] illustrates the optimal coding rate sequence @xmath90 and @xmath133 as functions of the iteration number @xmath37 .",
    "it is readily seen that after the first 56 iterations the coding rate seems near - linear , which confirms ( 18 ) ; and @xmath133 decays geometrically , as predicted by  .",
    "after proving that the achievable set @xmath151 is convex , we apply the unconstrained dp developed in zhu and baron  @xcite to find the pareto optimal points for various relative costs  , and illustrate the convexity of the achievable set .",
    "the unconstrained dp  @xcite finds a coding rate sequence @xmath90 over the mp - amp iterations such that the final mse is less than @xmath169 , while achieving the minimum cost @xmath170 .",
    "the cost @xmath170 for a given computation cost rate @xmath93 and communication cost rate @xmath94 is a function of the number of remaining iterations @xmath171 and the current scalar channel noise variance @xmath172  . in the basis case ,",
    "@xmath173 , the cost is @xmath174 . after solving the basis case ,",
    "we iterate back in time by decreasing @xmath37 , @xmath175 where @xmath176 is the coding rate used in the current mp - amp iteration  @xmath37 , @xmath177 is the indicator function , which is 1 if the condition @xmath178 is met , else 0 , and @xmath179 is the variance of the noise @xmath85 of the scalar channel   in the next mp - amp iteration after transmitting @xmath72 at rate @xmath176 .",
    "a discretized search space of @xmath172 and @xmath176 is utilized  @xcite .",
    "the coding rates @xmath176 that minimize the cost function @xmath180 for different @xmath37 and @xmath172 are stored in a table @xmath181 .",
    "after the unconstrained dp finishes , we obtain the coding rate sequence @xmath90 from the table @xmath181 .",
    "according to definition  [ def : pareto ] , the resulting tuple @xmath182 from the unconstrained dp in sec .",
    "[ sec : unconstraineddp ] is pareto optimal .",
    "hence , in this subsection , we run the unconstrained dp to obtain the pareto optimal points for a certain distributed linear system under various relative costs  .",
    "consider the same setting as in fig .",
    "[ fig : rtandemse ] , except that we analyze mp platforms  @xcite with a variety of relative costs  . running the unconstrained dp scheme developed in sec .",
    "[ sec : unconstraineddp ] , we obtain the optimal coding rate sequence @xmath90 that yields the lowest combined cost while helping mp - amp achieve an mse that is at most @xmath183 . in fig .  [",
    "fig : ratevsiter ] , we draw the pareto optimal surface obtained by running the unconstrained dp ; the circles on the surface are the pareto optimal points we analyzed . fig .  [",
    "fig : pareto2d_fixt ] plots the aggregate coding rate as a function of different mse with different optimal numbers of mp - amp iterations @xmath19 . fig .",
    "[ fig : pareto2d_fixmse ] plots the aggregate coding rate as a function of different @xmath19 with different optimal mse .",
    "we can see that the surface comprised of the pareto optimal points is indeed convex .    with stricter requirements on the final mse ( meaning smaller @xmath169 ) , more iterations @xmath19 and greater aggregate coding rates",
    "@xmath20   are needed .",
    "optimal coding rate sequences increase the coding rate to reduce the number of iterations when communication costs are low  @xcite ( examples are commerical cloud computing systems  @xcite , multi - processor cpus , and graphic processing units ) , whereas more iterations allow to reduce the coding rate when communication is costly  @xcite ( for example , in sensor networks  @xcite ) .",
    "the authors thank puxiao han and ruixin niu for numerous discussions about mp settings of cs and amp .",
    "we also thank yanting ma for useful suggestions .",
    "f.  krzakala , m.  mzard , f.  sausset , y.  sun , and l.  zdeborov , `` probabilistic reconstruction in compressed sensing : algorithms , phase diagrams , and threshold achieving matrices , '' , vol .",
    "2012 , no . 08 , pp .",
    "p08009 , aug .",
    "2012 .",
    "han , r.  niu , and y.  c. eldar , `` communication - efficient distributed iht , '' in _ proc .",
    "signal processing with adaptive sparse structured representations workshop ( spars ) _ , cambridge , united kingdom , july 2015 .",
    "han , j.  zhu , r.  niu , and d.  baron , `` multi - processor approximate message passing using lossy compression , '' in _ ieee int .",
    "acoustics , speech , signal process .",
    "( icassp ) _ , shanghai , china , mar . 2016 ."
  ],
  "abstract_text": [
    "<S> we consider large - scale linear inverse problems in bayesian settings . </S>",
    "<S> our general approach follows a recent line of work that applies the approximate message passing ( amp ) framework in multi - processor ( mp ) computational systems by storing and processing a subset of rows of the measurement matrix along with corresponding measurements at each mp node . in each mp - amp iteration , nodes of the mp system and its fusion center exchange lossily compressed messages pertaining to their estimates of the input . </S>",
    "<S> there is a trade - off between the physical costs of the reconstruction process including computation time , communication loads , and the reconstruction quality , and it is impossible to simultaneously minimize all the costs . </S>",
    "<S> we pose this minimization as a multi - objective optimization problem ( mop ) , and study the properties of the best trade - offs ( pareto optimality ) in this mop . </S>",
    "<S> we prove that the achievable region of this mop is convex , and conjecture how the combined cost of computation and communication scales with the desired mean squared error . </S>",
    "<S> these properties are verified numerically .    approximate message passing , distributed linear systems , multi - objective optimization , pareto optimality . </S>"
  ]
}