{
  "article_text": [
    "the short and noisy nature of tweets poses challenges in computing accurate latent tweet representations .",
    "we observe that paragraph2vec  @xcite which is good in computing document representation overfits when evaluated for tweets , mainly due to the short length of tweets . to overcome this problem we utilize additional context from twitter itself .",
    "specifically , we hypothesize that a principled usage of chronologically adjacent tweets from users twitter timelines can help in significantly improving the quality of the representation .",
    "the main challenge lies in assigning appropriate attention weights to context tweets such that semantically relevant tweets receive high weights compared to less relevant ones .",
    "consider fig  [ fig : at_fig ] , where we want to learn the representation for the tweet @xmath0 .",
    "one can see that the target tweet @xmath0 has less semantic interactions with the context tweet @xmath1 . to capture this",
    ", we propose an attention based model that assigns a variable weight to each context tweet that captures the semantic correspondence between the target tweet and the context tweet .",
    "we further augment the attention model to be user - aware so that it can do well in modeling the target tweet by exploiting the rich knowledge about the user such as the way the user writes the post , and also summarizing the topics on which the user writes .",
    "our work is closest to  @xcite where documents are modeled based on their word context as well as document stream context .",
    "we differ from their work in two ways : ( 1 ) they navely assume that all the documents in a stream have equal amount of semantic interactions and , ( 2 ) they ignore the knowledge of user ( or document author ) .",
    "we summarize our main contributions below . in summary ,",
    "our contributions are as follows .",
    "( 1 ) our work is the first to model the semantics of the tweet using the temporal context .",
    "( 2 ) we introduce a novel attention based model that learns the weights for context tweets by back - propagating semantic loss .",
    "( 3 ) we propose a novel way to learn user vector summarizing the content the user writes , which in turn helps in enriching the quality of the tweet embeddings .",
    "( 4 ) we conduct quantitative analysis to showcase the application potential of the tweet representations learned from the model and also provide some interesting findings .",
    "le et al .  @xcite adapt word2vec to learn document representations which are good in predicting the words present in the document . as seen in section  [ sec : experiments ] , for short documents like tweets , the model tends to learn poor document representations as the vector relies too much on the document content , resulting in overfitting .",
    "djuric et al .",
    "@xcite learn document representations using word context ( same as  @xcite ) along with document stream context in a hierarchical fashion .",
    "this work inspired us to learn tweet representations using user specific twitter streams .",
    "in this section we first introduce the notions of temporal context and attention , and then provide a formal problem statement .    :",
    "temporal context of a tweet @xmath0 is the set of @xmath2 tweets posted before and after @xmath0 by the same user .",
    "the value @xmath2 is a user specified parameter that defines the size of the temporal context to be considered to model a given tweet .",
    "for example , in fig  [ fig : at_fig ] we fix @xmath2 as 2 , the context tweets of @xmath0 are @xmath3 , @xmath1 , @xmath4 and @xmath5 . : an attention value is associated with a context tweet that defines the degree of semantic similarity between the context tweet and the target tweet .",
    "the more the latent semantic interactions between the tweets , the more is the attention .",
    "we denote the attention of context tweet @xmath3 as @xmath6 .",
    "for instance , in fig  [ fig : at_fig ] , the attention value of context tweet @xmath1 should be lower than that of context tweet @xmath3 with respect to target tweet @xmath0 . in fig  [ fig : at_fig ] , clearly @xmath1 is not talking about the topic ` climate change ' and so it makes sense to have a lower attention value .",
    ": let the training tweets be given in the order in which they are posted .",
    "in particular , we assume that we are given a user set @xmath7 of @xmath8 tweet sequences , with each sequence @xmath9 , containing @xmath10 tweets , @xmath11 posted by user @xmath12 .",
    "moreover , each tweet @xmath0 is a sequence of @xmath13 words , @xmath14 .",
    "the problem is to learn semantic low - dimensional representations for all the tweets in the sequences in set @xmath7 .",
    "0.5        0.5     our model ( fig  [ fig : t2v_fig ] ) learns tweet representations in a hierarchical fashion : learning from the words present in the tweet using word context model ( fig  [ fig : t2v_fig ] ( a ) ) along with the temporal tweets present in the user stream using tweet context model ( fig  [ fig : t2v_fig ] ( b ) ) .",
    "both the models will be discussed in detail below .",
    "let @xmath15 , @xmath16 and @xmath17 denote the embedding for a word @xmath18 from tweet @xmath19 , tweet @xmath19 and user @xmath12 respectively , all of which have the size ` n ' .",
    "we will discuss details about both of these models in this section .",
    "the goal of the word context model is to learn tweet representations which are good at predicting the words present in the tweet .",
    "the model has three layers .",
    "the first layer contains the word embeddings , @xmath20 , @xmath21 near the @xmath22 target word in tweet @xmath19 , which denote the word context for the word @xmath18 ( i.e. , @xmath23 ) along with the tweet embedding @xmath16 .",
    "secondly , there is a hidden layer with size equal to the number of words in the vocabulary ( @xmath24 ) .",
    "the final layer is a softmax layer which gives a well - defined probability distribution over words in the vocabulary .",
    "the input to the word context model is all pairs of word context of word @xmath18 and tweet @xmath0 in the corpus .",
    "the objective is to maximize the likelihood of the word @xmath23 occurring given its context , i.e. , @xmath25 .",
    "equation  [ eq : eq2 ] represents the forward propagation step in our 1-hidden layer feed forward model , where @xmath26 and @xmath27 denote the additional parameters of the model .",
    "@xmath28      the goal of this model is to enrich the tweet representation learned from the word context , by modeling the current tweet conditioned on its temporal context and the proposed user context .",
    "the user context makes our model user - aware by exploiting the user characteristics such as the way the user writes the post and also summarizing the topics on which the user writes .",
    "these user vectors are learned automatically from the set of tweets posted by the user through this model . as a nave solution",
    ", we can directly adopt djuric et al .",
    "@xcite s approach and apply on the twitter stream . as discussed in section  [ sec : problem ] , this assumption is too strong for social media streams .",
    "can we assign attention levels to the context tweets with respect to the tweet being modeled ? to learn the optimal values of attention ( @xmath29 ) , we introduce the attention parameters as shown in equation  [ eq : eq5 ] .",
    "the intuition is that semantic loss will be less if the weights of each of the temporal context tweets are learned accurately .",
    "the values of @xmath29 s can be computed as shown in equation  [ eq : eq6 ] .",
    "the objective of this model is to maximize the likelihood of the tweet @xmath19 posted by user @xmath30 given its temporal context ( @xmath31 ) and user context ( @xmath17 ) , which is given by @xmath32 .",
    "since the tweet space can be exponentially large , we use hierarchical softmax  @xcite instead of normal softmax to bring down the time complexity from @xmath33 ( or @xmath34 for the previous model ) to @xmath35 ( or @xmath36 ) ) .",
    "@xmath37 @xmath38 ) \\end{aligned}\\ ] ]    where the parenthesis inside the softmax function represents concatenation of all context representations ( ( @xmath39 in size ) .",
    "@xmath40 is the additional weight matrix ( of size @xmath41 ) added as parameters to the model . in practice",
    ", we observe that multiple passes ( ` epochs ' ) on the training set are required to fine tune these attention values . the overall objective function",
    "intertwining both the models in a hierarchical fashion to be maximized can be summarized as shown in equation  [ eq : eq7 ] .",
    "we use the cross - entropy as the cost function between the predicted distribution @xmath42 and target distributions @xmath0 and @xmath43 , for modeling using the temporal and word context respectively .",
    "we train the model using back - propagation  @xcite and adam  @xcite optimizer .",
    "@xmath44 + \\log \\mathbb{p}(u(k ) | t(1 ) , \\cdots , t(n_t ) ) \\label{eq : eq7}\\end{aligned}\\ ] ]",
    "in this section we discuss details of our dataset , experiment , and then present quantitative analysis of the proposed models .    [ cols=\"^,^,^,^\",options=\"header \" , ]      we use the publicly available dataset described in li et al .",
    "@xcite for all the experiments .",
    "it contains tweets pertaining to three profile attributes ( spouse , education and job ) of a user .",
    "specifically , it has a set of tweets from users twitter timelines , that talk about the attribute ( ` positive ' tweets ) and those that do not ( ` negative ' tweets ) .",
    "we randomly sample 1600 users from the dataset and use 70 - 10 - 20 ratio to construct train , validation and test splits .",
    "tweet embeddings are randomly initialized while the word embeddings are initialized with the pre - trained word vectors from pennington et al .",
    "@xcite .",
    "we consider the binary task of predicting whether a given entity mention corresponds to particular users profile attribute or not .",
    "we build our model to get the tweet vector and the entity vector by computing an average of all the tweet vectors for the entity .",
    "we tune the penalty parameter of a linear support vector machine ( svm ) on the validation set .",
    "note that we use a linear classifier so as to minimize the effect of variance of non - linear methods on the classification performance and subsequently help in interpreting the results .",
    "we compare our model with three baselines : ( 1 ) paragraph2vec  @xcite , ( 2 ) simple distance model ( sd ) : a model that assigns attention weight to the context tweet which is inversely proportional to the distance of the tweet from the target tweet , ( 3 ) hdv  @xcite , ( 4 ) ours ( user = 0 ) : our model when the user context is excluded from the temporal context , ( 5 ) ours ( user = 1 ) : our model when the user context is included in the temporal context .",
    "we empirically set @xmath45 and @xmath46 to 200 and 10 respectively for all the models . in case of sd , hdv and our models ,",
    "we try values in \\{1 , 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 } to fix the temporal context size parameter ( i.e. , @xmath2 ) which is crucial in improving the semantics of the tweet .",
    "0.3        0.29        0.29     1     0.33        0.29        0.29     1       from table  [ tab : classaccuracy ] , we see that paragraph2vec overfits the validation set , resulting in poor accuracy during testing .",
    "hdv s assumption of giving equal attention value to the temporal context also results in lower accuracy compared with our models .",
    "sd model outperforms hdv in two tasks , which substantiates our claim against hdv s nave assumption for social media . our model with user vector outperforming",
    "the baselines for education and job attribute classification , shows the need to consider the user characteristics while modelling his / her tweets .",
    "the poor results for spouse task suggest that this dataset has too many topic shifts and that the user vector turned out to be less accurate .",
    "fig  [ fig : perf_ct ] displays the f1 results for different values of @xmath2 , which is a vital parameter controlling the influence of temporal context .",
    "we observe that in some cases hdv outperforms the sd model , mainly due to the inability of the sd model to utilize the context information from farther tweets which are relevant with respect to the target tweet .",
    "our models are 19.66% , 2.27% and 2.22% better compared to the baselines for the spouse , education and job attributes respectively .",
    "we plot the attention mean across each position of the context tweet with respect to the epoch number . from fig",
    "[ fig : att_mean_u1 ] , we see that mean attention at each context position are approximately in the ballpark",
    ". mean attention weights vary for each context position , exhibiting no relation with respect to the increase in distance ( as seen in fig  [ fig : att_dist ] ) .",
    "these findings indicate the complexity of giving attention to tweets in the temporal context .",
    "initially , we see that the mean attention weights are changing drastically indicating their sub - optimality . it is interesting to see",
    "the convergence of these weights to the optimal solution is fast ( in terms of no . of epochs ) in the model which uses user context when compared to the model that does not use it .    0.3        0.3        0.3     1",
    "we proposed a model to learn generic tweet representations which have a wide range of applications in nlp and ir field .",
    "we discovered that the principled usage of the tweets in the temporal context is an important direction in enriching the representations .",
    "we also explored learning a novel user context vector to make our model user - aware while predicting the adjacent tweets . through experimental analysis",
    ", we identified the cases when modeling the user characteristics help enhance the embedding quality . in future",
    ", we plan to understand the application potential of the user vector learned through our approach .",
    "nemanja djuric , hao wu , vladan radosavljevic , mihajlo grbovic , and narayan bhamidipati .",
    "hierarchical neural language models for joint representation of streaming documents and their content .",
    ", 248255 ."
  ],
  "abstract_text": [
    "<S> in this work we propose a novel representation learning model which computes semantic representations for tweets accurately . </S>",
    "<S> our model systematically exploits the chronologically adjacent tweets ( ` context ' ) from users twitter timelines for this task . </S>",
    "<S> further , we make our model user - aware so that it can do well in modeling the target tweet by exploiting the rich knowledge about the user such as the way the user writes the post and also summarizing the topics on which the user writes . </S>",
    "<S> we empirically demonstrate that the proposed models outperform the state - of - the - art models in predicting the user profile attributes like spouse , education and job by 19.66% , 2.27% and 2.22% respectively .    </S>",
    "<S> = 6 </S>"
  ]
}