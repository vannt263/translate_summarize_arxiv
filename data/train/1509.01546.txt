{
  "article_text": [
    "the classification of unlabelled data is fundamental to many statistical and machine learning applications .",
    "such applications arise in the context of clustering and semi - supervised classification . underpinning these tasks",
    "is the assumption of a clusterable structure within the data , and importantly that this structure is relevant to the classification task .",
    "the assumption of a clusterable structure , however , begs the question of how a cluster should be defined .",
    "centroid based methods , such as the ubiquitous @xmath3-means algorithm , define clusters in reference to single points , or centers  @xcite . in the non - parametric statistical approach to clustering , clusters are associated with the modes of a probability density function from which the data are assumed to arise  ( * ? ? ?",
    "* chapter  11 ) .",
    "we consider the definition as given in the context of graph partitioning , and the relaxation given by spectral clustering .",
    "spectral clustering has gained considerable interest in recent years due to its strong performance in diverse application areas . in this context clusters",
    "are defined as strongly connected components of a graph defined over the data , wherein vertices correspond to data points and edge weights represent pairwise similarities  @xcite .",
    "the minimum cut graph problem seeks to partition a graph such that the sum of the edges connecting different components of the partition is minimised . to avoid partitions containing small sets of vertices ,",
    "a normalisation is introduced which helps to emphasise more balanced partitions .",
    "the normalisation , however , makes the problem np - hard  @xcite , and so a continuous relaxation is solved instead .",
    "the relaxed problem , known as spectral clustering , is solved by the eigenvectors of the _ graph laplacian _ matrices . we give a brief introduction to spectral clusering in section  [ sec : background ] .",
    "graph partitioning methods and spectral clustering define clusters as strongly connected components of a graph constructed from the data  @xcite .",
    "the lattermost of these definitions forms the focus of this work .",
    "graph based partitioning associates with a given set of data , a graph in which each vertex corresponds to a datum , and the undirected , weighted edges represent pairwise similarities .",
    "the minimum cut graph problem partitions the graph to minimise the sum of the edge weights connecting the different components of the partition . to avoid partitions containing mostly singletons a normalisation based on the size of the components",
    "is introduced , but this renders the problem np  hard  @xcite .",
    "a relaxation of this optimisation problem can be formulated as an eigen - problem , for which the solution is given by the eigenvectors associated with the @xmath3 smallest eigenvalues of the _ graph laplacian _",
    ", where @xmath3 is the size of the partition .",
    "clustering via the spectrum of the graph laplacian is referred to as spectral clustering , a method which has become increasingly popular in recent years due to its good performance compared with traditional methods  @xcite .",
    "utilising the relaxation given by spectral clustering allows us to formulate our problem in the context of projection pursuit .",
    "crucial to all cluster definitions is the relevance of spatial similarity of points . in multivariate",
    "data analysis , however , the presence of irrelevant or noisy features can significantly obscure the spatial structure in a data set .",
    "moreover , in very high dimensional applications the curse of dimensionality can make spatial similarities unreliable for distinguishing clusters  @xcite .",
    "dimension reduction techniques seek to mitigate the effect of irrelevant features and of the curse of dimensionality by finding low dimensional representations of a set of data which retain as much information as possible .",
    "most commonly these low dimensional representations are defined by the projection of the data into a linear subspace .",
    "information retention is crucial for the success of any subsequent tasks . for unsupervised classification",
    "this information must , therefore , be relevant in the context of cluster structure .",
    "classical dimension reduction techniques such as principal component analysis ( pca ) can not guarantee the structural relevance of the low dimensional subspace . moreover",
    "a single subspace may not suffice to distinguish all clusters , which may have their structures defined within differing subspaces .",
    "recently a number of dimension reduction methods with an explicit objective which is relevant to cluster structure have been proposed  @xcite .",
    "we discuss these briefly in section  [ sec : related ] .",
    "we consider the problem of learning the optimal subspace for the purpose of data bi - partitioning , where optimality is measured by the connectivity of the projected data , as defined in spectral graph theory .",
    "we formulate the problem in the context of _ projection pursuit _ ; a class of optimisation problems which aim to find _ interesting _ subspaces within potentially high dimensional data sets , where interestingness is captured by a predefined objective , called the _ projection index_. with very few exceptions , the optimisation of the projection index does not admit a closed form solution , and is instead numerically optimised .",
    "the projection indices considered in the proposed method are the second smallest eigenvalues of the graph laplacians , which measure the quality of a binary partition arising from the normalised minimum cut graph problem .",
    "these eigenvalues are non - smooth and non - convex , and so specialised techniques are required to optimise them .",
    "we establish conditions under which they are lipschitz and almost everywhere continuously differentiable , and discuss how to find local optima with guaranteed convergence properties . in this paper we establish an asymptotic connection between optimal univariate subspaces for bi - partitioning based on spectral graph theory , and maximum margin hyperplanes .",
    "formally , we show that as the scaling parameter defining pairwise similarities is reduced to zero , the optimal univariate subspace for bi - partitioning converges to the subspace normal to the largest margin hyperplane through the data .",
    "this establishes a theoretical connection between connectivity as measured by spectral graph theory and maximal euclidean separation .",
    "it also provides an alternative methodology for learning maximum margin clustering models , which have attracted considerable interest in recent years  @xcite .",
    "we introduce a way of modifying the similarity function which avoids focusing on outliers , and allows us to further control the balance of the induced partition .",
    "the importance of controlling this balance has been observed in the context of large margin clustering  @xcite and low density separators  @xcite .",
    "the computation cost associated with the eigen - problem underlying our projection index is quadratic in the number of data . to mitigate this computational burden",
    "we propose a data preprocessing step using micro - clusters which significantly speeds up the optimisation .",
    "we establish theoretical error bounds for this approximation method , and provide a sensitivity study which shows no degradation in clustering performance , even for a coarse approximation .",
    "the remainder of the paper is organised as follows . in section  [ sec : related ] we briefly discuss related work on dimension reduction for unsupervised data partitioning .",
    "a brief outline of spectral clustering is provided in section  [ sec : background ] .",
    "section  [ sec : method ] presents the methodology for finding optimal projections to perform binary partitions .",
    "section  [ sec : maxmargin ] describes the theoretical connection between optimal subspaces for spectral bi - partitioning and maximum margin hyperplanes . in section  [ sec : microclust ] we discuss an approximation method in which the computational speed associated with finding the optimal subspace can be significantly improved , with provable approximation error bounds .",
    "experimental results and sensitivity analyses are presented in section  [ sec : experiments ] , while section  [ sec : conclusion ] is devoted to concluding remarks .",
    "the literature on clustering high dimensional data is vast , and we will focus only on methods with an explicit dimension reduction formulation , as in projection pursuit .",
    "implicit dimension reduction methods based on learning sparse covariance matrices ( which impose an implicit low dimensional projection of the data / clusters ) , such as quadratic discriminant analysis , can be limited by the assumption that clusters are determined by their covariance matrices .",
    "projection pursuit approaches can be made more versatile by defining objectives which admit more general cluster definitions .",
    "principal component analysis and independent component analysis have been used in the context of clustering , however their objectives do not correspond exactly with those of the clustering task and the justification of their use is based more on common - sense reasoning . nonetheless , these methods have shown good empirical performance on a number of applications  @xcite .",
    "some recent approaches to projection pursuit for clustering rely on the non - parametric statistical notion clusters , i.e. , that clusters are regions of high density in a probability distribution from which the data are assumed to have arisen . @xcite proposed using as projection index the _ dip statistic _",
    "@xcite of the projected data .",
    "the dip is a measure of departure from unimodality , and so maximising the dip tends to projections which have mutlimodal marginal density , and therefore separate high density clusters .",
    "the authors establish that the dip is differentiable for any projection vector onto which the projected data are unique , and use a simple gradient ascent method to find local optima .",
    "the minimum density hyperplane approach  @xcite is posed as a projection pursuit for the univariate subspace normal to the hyperplane with minimal integrated density along it , thereby establishing regions of low density which separate the modes of the underlying probability density .",
    "the projection index in this case is the minimum of the kernel density estimate of the projected data , penalised to avoid hyperplanes which do not usefully split the data .",
    "the authors show an asymptotic connection between the hyperplane with minimal integrated density and the maximum margin hyperplane .",
    "the result we show in section  [ sec : maxmargin ] therefore establishes that the optimal subspace for bi - partitioning based on spectral connectivity is asymptotically connected with the minimum integrated density hyperplane .",
    "a number of direct approaches to maximum margin clustering have also been proposed  @xcite .",
    "these can be viewed as a projection pursuit for the subspace normal to the maximum margin hyperplane intersecting the data .",
    "the iterative support vector regression approach  @xcite uses support vector methods and so for the linear kernel explicitly learns the corresponding projection vector , @xmath4 .",
    "most similar to our work is that of  @xcite , who also proposed a method for dimension reduction based on spectral clustering .",
    "the authors show an interesting connection between optimal subspaces for spectral clustering and _ sufficient dimension reduction_. for the case of a binary partition , their objective is equivalent to one of the objectives we consider , i.e. , that of minimising the second smallest eigenvalue of the normalised laplacian ( cf .",
    "sections  [ sec : background ] and  [ sec : method ] ) .",
    "however , our methodology differs substantially from theirs .",
    "@xcite define their objective by    @xmath5    the matrix @xmath6 is the affinity matrix containing pairwise similarities of points projected into the subspace defined by @xmath7 , and @xmath8 is the diagonal degree matrix of @xmath6 , with @xmath9-th diagonal element equal to the @xmath9-th row sum of @xmath6 .",
    "further details of these objects can be found in section  [ sec : background ] .",
    "the approach used by the authors to maximise this objective alternates between using spectral clustering to determine the columns of @xmath10 , and then using a gradient ascent method to maximise @xmath11 over @xmath7 , where the dependence of this objective on the projection matrix @xmath7 is through eq .",
    "( [ eq : aij ] ) . within this gradient ascent",
    "step the matrices @xmath10 and @xmath8 are kept fixed .",
    "this process is iterated until convergence .",
    "however , the authors do not address the fact that the matrix @xmath8 is determined by @xmath6 , and therefore depends on the projection matrix @xmath7 .",
    "an ascent direction for the objective assuming a fixed @xmath8 is therefore not necessarily an ascent direction for the overall objective .",
    "despite this fact the method has shown good empirical performance on a number of problems  @xcite . in section  [ sec : method ] we derive expressions for the gradient of the overall objective , which allows us to optimise it directly .",
    "in this section we provide a brief introduction to spectral clustering , with particular attention to bi - partitioning , which underlies the focus of this work .",
    "bi - partitioning using spectral clustering has been considered previously by  @xcite , where a full clustering can be obtained by recursively inducing bi - partitions of ( subsets of ) the data . with a data sample , @xmath12 ,",
    "spectral clustering associates a graph @xmath13 , in which vertices correspond to observations , and the _ undirected _ edges assume weights equal to the pairwise _ similarity _ between observations .",
    "pairwise similarities can be determined in a number of ways , including nearest neighbours and similarity metrics . in general ,",
    "similarities are determined by the spatial relationships between points , and pairs which are closer are assigned higher similarity than those which are more distant .",
    "the information in @xmath14 can be represented by the _ adjacency _ , or affinity matrix , @xmath15 , with @xmath16 .",
    "the _ degree _ of each vertex @xmath17 is defined as , @xmath18 . the _ degree matrix _ , @xmath8 ,",
    "is then defined as the diagonal matrix with @xmath9-th diagonal element equal to @xmath19 . for a subset @xmath20",
    ", the size of @xmath21 can be defined either by the cardinality of @xmath21 , @xmath22 , or by the _ volume _ of @xmath21 , @xmath23 .",
    "the _ normalised min - cut graph problem _ for a binary partition is defined as the optimisation problem @xmath24    it has been shown  @xcite that the two normalised min - cut graph problems ( corresponding to the two definitions of size ) can be formulated in terms of the _ graph laplacian _ matrices , @xmath25 as follows . for @xmath20 define @xmath26 to be the vector with @xmath9-th entry , @xmath27 for size@xmath28 , the optimisation problem in ( [ eq : mincut ] ) can be written as , @xmath29 similarly , if size@xmath30 vol@xmath31 the problem in ( [ eq : mincut ] ) is equivalent to , @xmath32 both problems in ( [ eq : ratiocut ] ) and  ( [ eq : ncut ] ) are np - hard  @xcite , and so continuous relaxations of these , in which the discreteness condition on @xmath33 given in eq .",
    "( [ eq : discretef ] ) is removed , are solved instead  @xcite .",
    "the solutions to the relaxed problems are given by the second eigenvector of  @xmath34 and the second eigenvector of the generalised eigen equation @xmath35 respectively , the latter thus equivalently solved by @xmath36 , where @xmath37 is the second eigenvector of @xmath38 . in particular , we have @xmath39 where @xmath40 and @xmath41 are the second eigenvalues of @xmath34 and @xmath38 and @xmath42 and @xmath43 are the solutions to  ( [ eq : ratiocut ] ) and  ( [ eq : ncut ] ) respectively .    the following properties of the matrices @xmath34 and @xmath38 will be useful in establishing our proposed methodology and the associated theoretical results .",
    "these properties can be found in  ( * ? ? ?",
    "* propositions 2 and 3 ) .    1 .",
    "for any @xmath44 we have @xmath45 2 .",
    "@xmath34 and @xmath38 are symmetric and positive semi - definite .",
    "3 .   the smallest eigenvalue of @xmath34 is 0 with corresponding eigenvector @xmath46 , the constant 1 vector 4 .",
    "the smallest eigenvalue of @xmath38 is 0 with corresponding eigenvector @xmath47 .",
    "the extension of clustering via the normalised min - cut to a @xmath48-partition of the data is similar , and can be solved approximately by the first @xmath48 eigenvectors of either @xmath34 or @xmath38  @xcite .",
    "in this section we study the problem of minimising the second eigenvalue of the graph laplacian matrices of the projected data .",
    "if the projected data are split in two through spectral clustering , then the projection that minimises the second eigenvalue of the corresponding graph laplacian minimises the connectivity of the two components , as measured by spectral graph theory . note",
    "that while we discuss explicitly the minimisation of the second eigenvalue , the methodology we present in fact applies to an arbitrary eigenvalue of the graph laplacians . as a result",
    ", the method discussed herein trivially extends to the problem of determining a @xmath48-partition by minimising the sum of the @xmath48 smallest eigenvalues of the laplacians .    to begin with ,",
    "let @xmath12 be a @xmath49-dimensional data set and let @xmath50 be a _ projection matrix _",
    ", where @xmath51 is the dimension of the projection , and the columns of @xmath52 , @xmath53 , have unit norm . with this formulation it is convenient to consider a parameterisation of @xmath52 through polar coorindates as follows",
    ". let @xmath54 and for @xmath55 , the projection matrix @xmath56 is given by , @xmath57 from this we define the @xmath58 dimensional _ projected data set _ by @xmath59 @xmath60 , and we let @xmath61 ( resp .",
    "@xmath62 ) be the laplacian ( resp .",
    "normalised laplacian ) of the graph of @xmath63 .",
    "edge weights are determined by a positive function @xmath64 , in that the affinity matrix is given by @xmath65 . in the simplest case",
    "we may imagine @xmath66 being fully determined by the euclidean distance between two elements of the projected data , i.e. , @xmath67 , for some function @xmath68 .",
    "however we prefer to allow for a more general definition .",
    "we discuss this further in section  [ sec : sim ] .",
    "henceforth we will use @xmath69 to be the @xmath9-th ( smallest ) eigenvalue of its ( in all cases herein ) real symmetric matrix argument .",
    "the objectives @xmath70 and @xmath71 are , in general , non - convex and non - smooth in @xmath72 , and so specialised techniques are required to optimise them . in the following subsections",
    "we investigate their differentiability properties , and discuss how alternating between a naive gradient descent method and a descent step based on a directional derivative can be used to find locally optimal solutions .      in this subsection",
    "we explore the continuity and differentiability properties of the second eigenvalue of the graph laplacians , viewed as a function of the _ projection angle _ , @xmath73 .",
    "we will view the data set @xmath74 as a @xmath75 matrix with @xmath9-th column equal to @xmath76 , and similarly the projected data set as an @xmath77 matrix , @xmath78 , with @xmath9-th column @xmath79 .",
    "[ lem : lipschitz ] let @xmath80 and let @xmath81 be lipschitz continuous in @xmath82 for fixed @xmath83 .",
    "then @xmath84 and @xmath71 are lipschitz continuous in @xmath72 .",
    "we show the case of @xmath61 , where that of @xmath62 is similar .",
    "the result follows from the fact that @xmath85 is element - wise lipschitz as a composition of lipschitz functions ( @xmath86 is lipschitz in @xmath72 as a collection of products of lipschitz functions ) and the fact that @xmath87 where the first inequality is due to @xcite , and the second comes from schur s inequality   @xcite .",
    "rademacher s theorem therefore establishes that both objectives are almost everywhere differentiable  @xcite .",
    "this almost everywhere differentiability can also be seen by considering that simple eigenvalues of real symmetric matrices are differentiable , e.g.  @xcite , and establishing that under certain conditions on the function @xmath66 the eigenvalues of @xmath61 and @xmath62 are simple for almost all @xmath73 .",
    "@xcite have shown that the real symmetric matrices with non - simple spectrum lie in a subspace of co - dimension 2 .",
    "if we denote the space of real valued @xmath88 symmetric matrices by @xmath89 , and denote this subspace by @xmath90 , then @xmath91 is open and dense in @xmath89 .",
    "sufficient conditions on the function @xmath66 for the almost everywhere simplicity of @xmath70 ( resp .",
    "@xmath71 ) are therefore that it is continuous in @xmath92 for each @xmath93 and for all @xmath94 and @xmath10 open in @xmath95 , @xmath96 s.t .",
    "@xmath97 ( resp .",
    "@xmath98 ) . continuity of @xmath66 ensures continuity of the functions @xmath70 and @xmath71 , and therefore the openness of the preimage of @xmath91 .",
    "the latter condition ensures that for each open @xmath99 , the span of the image of @xmath10 under @xmath100 is @xmath89 .",
    "therefore , in every open @xmath101 s.t .",
    "therefore the pre - image of @xmath91 is dense in @xmath95 .",
    "+ generalised gradient based optimisation methods are the natural framework for finding the optimal subspace for spectral bi - partitioning .",
    "eigenvalue optimisation is , in general , a challenging problem due to the fact that eigenvalues are not differentiable where they coincide . the majority of approaches in the literature focus on the problems of minimising the largest eigenvalue or the sum of a predetermined number of largest eigenvalues  @xcite .",
    "both of these problems tend to lead to a coalescence of eigenvalues , making the issue of non - differentiability especially problematic . conversely the minimisation of the smallest eigenvalue tends to lead to a separation of eigenvalues , and",
    "so non - differentiability is less of a concern  @xcite .",
    "if the similarity function @xmath66 is strictly positive , then @xmath70 and @xmath71 are bounded away from zero , and hence minimising these has the same benefits as does minimising the smallest eigenvalue in general , in that the corresponding optimisation tends to separate them from other eigenvalues . despite this practical advantage , the simplicity of @xmath70 and @xmath71",
    "is not guaranteed over the entire optimisation .",
    "we discuss a way of handling points of non - differentiability in section  [ sec : optimisation ] .",
    "this approach uses the directional derivative formulation given by  @xcite , and allows us to find descent directions which also tend to lead to a decoupling of eigenvalues",
    ". +    let @xmath80 contain at least 2 distinct points and let @xmath81 be continuous for any fixed @xmath93 and continuously varying for fixed @xmath103 .",
    "then @xmath84 and @xmath71 are simple for almost all @xmath73 .",
    "again we consider the case of @xmath61 explicitly , where the case of @xmath62 is similar .",
    "it is well established that the eigenvalues of symmetric matrices are almost everywhere simple , see for example  @xcite",
    ". therefore @xmath104 open and dense in the space of real symmetric matrices in which all matrices have distinct eigenvalues .",
    "let us consider @xmath61 as a matrix valued function of @xmath73 . if @xmath74 contains only distinct points , then @xmath61 is element - wise continuous and continuously varying since @xmath66 is continuous and continuously varying .",
    "the pre - image of @xmath8 , @xmath105 , is therefore open and has full measure in @xmath95 .",
    "if @xmath74 contains repeated points , say @xmath106 , where datum @xmath9 has multiplicity @xmath107 , then observe that the second eigenvalue of @xmath61 is equal to the second eigenvalue of the @xmath108 matrix @xmath109 , where @xmath110 is diagonal with @xmath111-th entry @xmath112 and @xmath113 .",
    "this matrix is continuous and continuously varying in @xmath73 , and hence its pre - image of @xmath8 is open and has full measure in @xmath95 .",
    "global convergence of gradient based optimisation algorithms relies on the continuity of the derivatives ( where they exist ) . to establish this continuity , we first derive expressions for the derivatives of @xmath70 and @xmath71 as a function of @xmath73 .",
    "theorem 1 of @xcite provides a useful formulation of eigenvalue derivatives .",
    "if @xmath114 is a simple eigenvalue of a real symmetric matrix @xmath115 , then @xmath114 is infinitely differentiable on a neighbourhood of @xmath115 , and the differential at @xmath115 is given by @xmath116 where @xmath37 is the corresponding eigenvector .",
    "let us assume that @xmath81 is differentiable in @xmath82 for fixed @xmath117 . for brevity we temporarily drop the notational dependence on @xmath72 and denote the second eigenvalue of the laplacian by @xmath114 , and the corresponding eigenvector by @xmath37 .",
    "the derivative @xmath118 is given by the @xmath119 matrix with @xmath9-th column @xmath120 , where we consider the chain rule decomposition @xmath121 .",
    "here @xmath122 is the differential operator .",
    "since only the @xmath9-th column of @xmath52 depends on @xmath123 , and only the @xmath9-th row of @xmath92 depends on @xmath124 , this product can be simplified as @xmath125 , where @xmath126 is used to denote the @xmath9-th row of @xmath92 , while @xmath124 and @xmath123 are , as usual , the @xmath9-th columns of @xmath52 and @xmath73 respectively .",
    "we provide expressions for each of these terms below .",
    "we first consider the standard laplacian @xmath34 . by eq .",
    "( [ eigdev ] ) we have @xmath127 .",
    "now , @xmath128 and so , @xmath129    for the normalised laplacian , @xmath38 , consider first @xmath130 we will again use @xmath114 and @xmath37 to denote the second eigenvalue and corresponding eigenvector .",
    "using the fact that @xmath131 , @xmath132 since @xmath133 , we have , @xmath134 therefore , @xmath135 the component @xmath136 is simply the @xmath137 transposed data matrix , and the @xmath138 matrix , @xmath139 , is given by    @xmath140.\\ ] ]    having derived expressions for the derivatives of @xmath70 and @xmath71 , we can address their continuity properties .",
    "the components @xmath141 clearly form a continuous product in @xmath73 .",
    "the continuity of the elements @xmath142 can be reduced to addressing the continuity of the eigenvalue itself , of its associated eigenvector and a continuity assumption on the derivative of the function @xmath66 .",
    "it is well known that the eigenvalues of a matrix are continuous , while the continuity of the elements of the eigenvector come from the fact that we have assumed @xmath114 to be simple  @xcite .",
    "we provide full expressions for the derivatives of @xmath70 and @xmath71 , for the similarity function used in our experiments , in appendix a. + the eigenvalues of a real symmetric matrix can be expressed as the difference between two convex matrix functions  @xcite .",
    "if the similarity function , @xmath66 , is lipschitz continuous and differentiable we therefore have that @xmath70 and @xmath71 are directionally differentiable everywhere .",
    "@xcite describe a way of expressing the directional derivative of the sum of the @xmath3 largest eigenvalues of a matrix whose elements are continuous functions of a parameter , at a point of non - simplicity of the @xmath3-th largest eigenvalue .",
    "we will discuss the case of @xmath70 , where @xmath71 is analogous .",
    "if we denote the sum of the @xmath3 largest eigenvalues of @xmath61 by @xmath143 then , @xmath144 now suppose that @xmath73 is such that @xmath145 that is , the @xmath3-th largest eigenvalue has multiplicity @xmath146 and @xmath147 are included in the sum defining @xmath143 .",
    "then the directional derivative of @xmath143 in direction @xmath148 is given by  @xcite @xmath149 where @xmath150 , the matrix @xmath151 has @xmath152-th column equal to the eigenvector of the @xmath152-th largest eigenvalue of @xmath61 and the matrix @xmath153 has @xmath152-th column equal to the eigenvector of the @xmath154-th largest eigenvalue of @xmath61 .",
    "in addition the set @xmath155 is defined as , @xmath156 @xcite have shown that @xmath157 is the sum of the eigenvalues of + @xmath158 plus the sum of the @xmath147 largest eigenvalues of @xmath159 . therefore , the directional derivative of @xmath70 in the direction @xmath148 is given by the smallest eigenvalue of @xmath159 , where the matrix @xmath160 is constructed by any complete set of eigenvectors corresponding to the eigenvalue @xmath161 .      applying standard gradient descent methods to functions which are almost everywhere",
    "differentiable can result in convergence to sub - optimal points  @xcite .",
    "this occurs when the method for determining the gradient is applied at a point of non - differentiability and results in a direction which is not a descent .",
    "in addition , gradients close to points of non - differentiability may be poorly conditioned from a computational perspective leading to poor performance of the optimisation .",
    "+ the second eigenvalues of the graph laplacian matrices , while not differentiable everywhere , benefit from the fact that their minimisation tends to lead to a separation from other eigenvalues .",
    "thus a naive gradient descent algorithm tends to perform well .",
    "notice also that if @xmath162 with @xmath163 and @xmath164 is such that @xmath165 for some @xmath55 , then for any @xmath166 with @xmath167 we have @xmath168 , since @xmath169 is an upper bound for @xmath170 .",
    "thus even if @xmath70 is a repeated eigenvalue , a descent direction for @xmath171 is a descent direction for @xmath70 , where @xmath37 is any corresponding eigenvector .",
    "however , this property does not necessarily hold for @xmath71 since the first eigenvector of @xmath62 depends on @xmath73 , and thus the second eigenvector @xmath37 will not necessarily be orthogonal to the first eigenvector of @xmath172 . + we assume that the similarity function , @xmath66 , is lipschitz continuous and continuously differentiable in @xmath92 for each @xmath93 , and hence the laplacian matrices @xmath61 and @xmath62 are element - wise lipschitz continuous and continuously differentiable in @xmath73",
    ". these conditions are sufficient for the everywhere directional differentiability of @xmath70 and @xmath71 .",
    "our approach for finding locally minimal solutions for @xmath70 and @xmath71 can be seen as a simplification of the general method of  @xcite .",
    "our method alternates between a naive application of a standard gradient based optimisation algorithm , in which the simplicity of the second eigenvalue is assumed to hold everywhere along the optimisation path , and a descent step which ( in general ) decouples the second eigenvalue .",
    "we again discuss only @xmath70 explicitly , where @xmath71 is analogous .",
    "a description of the method is found in algorithm  [ alg : min ] .",
    "notice that upon convergence of a gradient descent algorithm which assumes the simplicity of @xmath70 , if @xmath70 is simple then the solution is a local minimum , and so the algorithm terminates . if @xmath70 is not simple , then the solution may or may not be a local minimum . as we discuss in section  [ sec : differentiability ] , if @xmath73 is such that @xmath70 is not simple , then the directional derivative of @xmath70 in direction @xmath148 is given by the smallest eigenvalue of @xmath173 , where @xmath160 is the matrix with columns corresponding to a complete set of eigenvectors for @xmath161 , and @xmath150 .",
    "if @xmath174 for all @xmath175 , then @xmath73 is a local minimum and the method terminates , otherwise @xmath176 s.t .",
    "@xmath177 , and thus @xmath148 is a descent direction for @xmath70 .",
    "it is possible to find a locally steepest descent direction by minimising @xmath178 over @xmath148 , however the added computational cost associated with this subproblem outweighs the benefit over a simply chosen unit coordinate vector .",
    "notice that the directional derivative of @xmath179 in direction @xmath148 is given by the @xmath180-th eigenvalue of @xmath173 , for @xmath181 , where @xmath146 is the multiplicty of the eigenvalue @xmath161 . therefore if there exists @xmath182 s.t .",
    "@xmath183 and is simple then @xmath184 , where @xmath184 is the @xmath119 matrix with zeros except in the @xmath185-th entry where it takes the value 1 , is a descent direction and @xmath186 s.t .",
    "@xmath187 for all @xmath188 . on the other hand if @xmath189 and is simple , then @xmath190 is such a descent direction",
    ". if no such pair @xmath93 exists , then we select @xmath93 which maximises @xmath191 and set @xmath192 if the maximum was determined by the largest eigenvalue and equal to @xmath190 otherwise .",
    "1 .   initialise @xmath73 .",
    "2 .   apply gradient based optimisation to @xmath70 assuming differentiability 3 .",
    "+   @xmath73 4 .",
    "find @xmath153 , a complete set of @xmath146 eigenvectors for eigenvalue @xmath161 .",
    "+ find @xmath150 for @xmath193 , and @xmath194 5 .",
    "@xmath195 + @xmath73 6 .",
    "@xmath196 s.t .",
    "@xmath183 and is simple + @xmath197 s.t .",
    "@xmath198 , @xmath199 , @xmath170 is simple + 2 .",
    "@xmath200 s.t .",
    "@xmath189 and is simple + @xmath197 s.t .",
    "@xmath201 , @xmath199 , @xmath170 is simple + 2 .",
    "@xmath202 9 .",
    "@xmath203 + @xmath204 s.t .",
    "@xmath205 + 4 .",
    "@xmath204 s.t .",
    "@xmath206 + 4 .",
    "it is common to define pairwise similarities of points via a decreasing function of the distance between them .",
    "that is , for a decreasing function @xmath207 , the similarity function @xmath66 may be written , @xmath208 where @xmath209 is a metric and @xmath210 is a _",
    "scaling parameter_. we have found that the projection pursuit method which we propose can be susceptible to outliers when the standard euclidean distance metric is used , especially in the case of minimising @xmath70 . in this subsection we discuss how to embed a balancing constraint into the distance function . by including this balancing mechanism the projection pursuit is steered away from projections which result in only few data being separated from the remainder of the data set .    while the normalisation of the graph cut objective , given in  ( [ eq : mincut ] ) , is extremely effective in emphasising balanced partitions in the general spectral clustering problem  @xcite",
    ", we have found that in the projection pursuit formulation a further emphasis on balance is sometimes required .",
    "this is especially the case in high dimensional applications .",
    "consider the extreme case where @xmath211 .",
    "then the projection equation , @xmath212 , is an underdetermined system of linear equations .",
    "therefore for any desired projected data set @xmath92 there exist @xmath213 s.t .",
    "@xmath214 . in other words",
    "the projected data can be made to have any distribution , up to a scaling constant .",
    "in particular we can generally find projections which induce a sufficient separation of a small group of points from the remainder of the data that the normalisation in  ( [ eq : mincut ] ) is inadequate to obtain a balanced partition .",
    "we have observed that in practice even for problems of moderate dimension this situation can occur .",
    "the importance of including a balancing constraint in the context of projection pursuit for clustering has been observed previously by  @xcite and  @xcite . emphasising balanced partitions",
    "is achieved through the use of a compact constraint set @xmath215 , which may be defined using the distribution of the projected data set @xmath92 . by defining the metric @xmath209 in such a way that distances between points extending beyond @xmath215",
    "are reduced , we increase the similarity of points outside @xmath215 with others . if @xmath92 is @xmath58 dimensional then we define @xmath215 as the rectangle @xmath216 , where each @xmath217 is an interval in @xmath218 which is defined using the distribution of the @xmath9-th component of @xmath92 .",
    "a convenient way of increasing similarties with points lying outside @xmath215 is with a transformation @xmath219 , defined as follows , = 1.5mu = 1.5mu @xmath220 = 4mu = 4mu where @xmath221 $ ] is the distance reducing parameter .",
    "each @xmath222 is linear on @xmath217 but has a smaller gradient outside @xmath217 . as a result",
    "we have @xmath223 for any @xmath224 , with strict inequality whenever either @xmath225 or @xmath226 lies outside @xmath215 .",
    "we define @xmath227 in this way so that it is continuously differentiable even at the boundaries of @xmath215 , and so does not affect the differentiability properties of the similarity function , @xmath66 .",
    "figure  [ fig : retardeddistance ] illustrates how the function @xmath227 influences distances and similarities in the univariate case .",
    "+   + ( a ) the univariate data set @xmath92 is plotted against the transformed data @xmath228 .",
    "the point at @xmath229 lies outside @xmath215 and its distance to other points , e.g. the point at @xmath230 , is smaller within @xmath228 ( vertical axis ) than in @xmath92 ( horizontal axis ) .",
    "( b ) the kernel density estimate of the transformed data @xmath228 ( - - - ) has a stronger bimodal structure , i.e. , two well defined clusters , than that of @xmath92 ( ) , which has multiple small modes caused by outliers .",
    "the connection between spectral connectivity and density based clustering has been investigated theoretically by  @xcite showing that the normalised graph cut is asymptotically related to the density on the separating surface .",
    "( c ) the affinity matrix of the data set @xmath92 has a weaker cluster structure than that of @xmath228 , shown in ( d ) .    in the context of projection pursuit",
    "it is convenient to define a full dimensional convex constraint set @xmath231 and define the univariate constraint intervals , which we now index by the corresponding projection angles , via the projection of @xmath232 onto each @xmath233 .",
    "that is , @xmath234.\\ ] ] in our implementation , we define @xmath232 to be a scaled covariance ellipsoid centered at the mean of the data .",
    "the projections of @xmath232 are thus given by intervals of the form , @xmath235,\\ ] ] where @xmath236 and @xmath237 are the mean and standard deviation of the @xmath9-th component of the projected data set @xmath63 and the parameter @xmath238 determines the width of the projected constraint interval @xmath239 .",
    "figure  [ fig : optidigitsprojections ] shows two dimensional projections of the 64 dimensional optical recognition of handwritten digits dataset .",
    "the leftmost plot shows the pca projection which is used as initialisation for the projection pursuit .",
    "the remaining plots show the projections arising from the minimisation of @xmath70 for a variety of values of @xmath240 . for @xmath241 ,",
    "i.e. , an unconstrained projection , the projection pursuit focuses only on a few data and leaves the remainder of the dataset almost unaffected by the projection .",
    "setting @xmath242 causes the projection pursuit to focus on a larger proportion of the tail of the data distribution .",
    "setting @xmath243 however allows the projection pursuit to identify the cluster structure in the data and find a projection which provides a good separation of three of the clusters in the data , i.e. , those shown as black , orange and turquoise in the top right plot .      the formulation of the optimisation problem associated with projection pursuit based on spectral connectivity places no constraints on the projection matrix , @xmath52 , except that its columns should have unit norm .",
    "a common consideration in dimension reduction methods is that the columns in the projection matrix should be orthogonal , i.e. , @xmath244 for all @xmath103 . in the context of projection pursuit",
    "it is common to generate the columns of @xmath52 individually , so that orthogonality of the columns can easily be enforced .",
    "@xcite proposes first learning @xmath245 using the original data , and then for each subsequent column the data are first projected into the null space of all the columns learnt so far .",
    "an alternative approach  @xcite , is to instead project the gradient of the objective into this null space during a gradient based optimisation .",
    "notice , however , that orthogonality in the columns of @xmath52 is not essential for the underlying problem .",
    "another common approach  @xcite is to transform the data after each column is determined in such a way that the columns learned so far are no longer  interesting \" , i.e. , have low projection index .",
    "this does not enforce orthogonality , but rather steers the projection pursuit away from the columns already learned by making them unattractive to the optimisation method .",
    "we propose a different approach which allows us to learn all of the columns of @xmath52 simultaneoulsy .",
    "this is achieved by introducing an additional term to the objective function which controls the level of orthogonality , or alternatively correlation , within the projection matrix . in particular",
    ", we consider the objective , @xmath246 or replacing @xmath70 with @xmath71 in the normalised case .",
    "this approach serves a dual purpose . in the first case",
    ", setting @xmath247 leads to approximately orthogonal projections , without the need to optimise separately over different projection vectors as is standard . alternatively , setting @xmath248 leads to approximately perfect correlation , i.e. , @xmath249 for all @xmath93 . in the latter case",
    "the resulting projection is therefore equivalent to a univariate projection .",
    "this is similar to simultaneously considering multiple initialisations , and allowing the optimisation procedure to select from them automatically .",
    "this is important as the objectives @xmath70 and @xmath71 are non - convex , and as a result applying gradient based optimisation can only guarantee convergence to a local optimum .",
    "notice also that the formulation in eq .",
    "( [ eq : orthog ] ) offers computational benefits over the alternative of optimising separately over each projection vector , since the eigenvalues / vectors computed in each function and gradient evaluation need only be computed once for each iteration over the multiple projection dimensions .",
    "in this section we establish a connection between the optimal univariate projection for spectral bi - partitioning using the standard laplacian and large margin separators . in particular , under suitable conditions ,",
    "as the scaling parameter tends to zero the optimal projection for spectral bi - partitioning converges to the vector admitting the largest margin hyperplane through the data .",
    "this establishes a theoretical connection between spectral connectedness and separability of the resulting clusters in terms of euclidean distance .",
    "large margin separators are ubiquitous in the machine learning literature , and were first introduced in the context of supervised classification via support vector machines ( svm ,  @xcite ) . in more recent years",
    "they have shown to be very useful for unsupervised partitioning in the context of maximum margin clustering as well  @xcite .",
    "our result pertains to univariate projections , and therefore the @xmath250 projection matrix is equivalently viewed as a _ projection vector _ in @xmath251 .",
    "we therefore use the notation @xmath252 , instead of @xmath253 as before .",
    "the result holds for all similarities for which the function @xmath3 , in eq .",
    "( [ eq : sim ] ) , satisfies the tail condition @xmath254 for all @xmath255 .",
    "this conditions is satisfied by functions with exponentially decaying tails , including the popular gaussian and laplace kernels .",
    "it is , however , not satisfied by those with polynomially decaying tails .",
    "the constraint set @xmath232 again plays an important role as in many cases the largest margin hyperplane through a set of data separates only a few points from the rest , making it meaningless for the purpose of clustering .",
    "we therefore prefer to restrict the hyperplane to intersect the set @xmath232 .",
    "what we in fact show in this section is that there exists a set @xmath256 satisfying @xmath257 , such that , as the scaling parameter tends to zero , the optimal projection for @xmath70 converges to the projection admitting the largest margin hyperplane that intersects @xmath258 .",
    "the distinction between the largest margin hyperplane intersecting @xmath258 and that intersecting @xmath232 is scarcely of practical relevance , but plays an important role in the theory we present in this section .",
    "it accounts for situations when the largest margin hyperplane intersecting @xmath232 lies close to its boundary and the distance between the hyperplane and the nearest point outside @xmath232 is larger than to the nearest point inside @xmath232 .",
    "aside from this very specific case , the two in fact coincide .",
    "a hyperplane is a translated subspace of co - dimension 1 , and can be parameterised by a non - zero vector @xmath259 and scalar @xmath260 as the set @xmath261 . clearly , for any @xmath262 , one has @xmath263 , and so we can assume that @xmath4 has unit norm , thus the same parameterisation by @xmath72 can be used .",
    "for a finite set of points @xmath264 , the _ margin _ of hyperplane @xmath265 w.r.t .",
    "@xmath74 is the minimal euclidean distance between @xmath265 and @xmath74 .",
    "that is , @xmath266 connections between maximal margin hyperplanes and bayes optimal hyperplanes as well as minimum density hyperplanes have been established @xcite .",
    "+   + in this section we will use the notation @xmath267 , and for a set @xmath268 and @xmath269 we write , for example , @xmath270 for @xmath271 . for scaling parameter @xmath272 and distance reduction factor @xmath273",
    "define @xmath274 , where @xmath275 is as @xmath85 from before , but with an explicit dependence on the scaling parameter and distance reducing parameter used in the similarity function .",
    "that is , @xmath276 defines the projection generating the minimal spectral connectivity of @xmath74 for a given pair  @xmath277 . +   + before proving the main result of this section , we require the following supporting results .",
    "lemma  [ thm : maxdistbound ] provides a lower bound on the second eigenvalue of the graph laplacian of a one dimensional data set in terms of the largest euclidean separation of adjacent points , with respect to a constraint set @xmath215 .",
    "this lemma also shows how we construct the set @xmath258 .",
    "lemma  [ thm : deltamargin ] uses this result to show that a projection angle @xmath55 leads to lower spectral connectivity than all projections admitting smaller maximal margin hyperplanes intersecting @xmath258 for all pairs @xmath277 sufficiently close to zero .",
    "[ thm : maxdistbound ] let @xmath278 be a non - increasing , positive function and let @xmath279 $ ] .",
    "let @xmath280 be a univariate data set and let @xmath281 $ ] for @xmath282 .",
    "suppose that @xmath283 and @xmath284 .",
    "define @xmath285 $ ] , where @xmath286 , @xmath287 .",
    "let @xmath288 .",
    "define @xmath289 to be the laplacian of the graph with vertices @xmath92 and similarities according to @xmath290 .",
    "then @xmath291 , where @xmath292 @xmath293 .",
    "we can assume that @xmath92 is sorted in increasing order , i.e. @xmath294 , since this does not affect the eigenvalues of @xmath289 .",
    "we first show that @xmath295 for all @xmath296 . to this end",
    "observe that @xmath297 for @xmath298 .    * if @xmath299 then @xmath300 @xmath301 by the definition of @xmath21 and using the above inequality , since @xmath3 is non - increasing .",
    "the case @xmath302 is similar . * if @xmath303 then @xmath304 + @xmath305 since @xmath115 is the largest margin in @xmath306 . *",
    "if none the above hold , then we lose no generality in assuming @xmath307 , @xmath308 since the case @xmath309 , @xmath310 is analogous .",
    "we must have @xmath311 and so @xmath312 . if @xmath313 then @xmath314 , a contradiction since @xmath315 and @xmath115 is the largest margin in @xmath306",
    ". therefore @xmath316 . in all @xmath317 .",
    "now , let @xmath37 be the second eigenvector of @xmath289",
    ". then @xmath163 and @xmath318 and therefore @xmath319 s.t .",
    "we thus know that there exists @xmath321 s.t .",
    "@xmath322 . by  (",
    "* proposition 1 ) , we know that @xmath323 since all consecutive pairs @xmath324 @xmath325 have similarity at least @xmath305 , by above .",
    "therefore @xmath326 as required .",
    "in the above lemma we have assumed that @xmath215 is contained within the convex hull of the points @xmath92 , however the results of this section can easily be modified to allow for cases where this does not hold .",
    "in particular , if an unconstrained large margin hyperplane is sought , then setting @xmath232 to be arbitrarily large allows for this .",
    "we have merely stated the results in the most convenient context for our practical implementation .",
    "the set @xmath306 in the above is defined in terms of the one dimensional constraint set @xmath327 $ ] .",
    "we define the full dimensional set @xmath258 along the same lines by , @xmath328.\\end{aligned}\\ ] ] here we assume that @xmath232 is contained within the convex hull of the @xmath49-dimensional data set @xmath74 .",
    "notice that since @xmath232 is convex , we have @xmath329 . in what follows we show that as @xmath330 and @xmath331 are reduced to zero the optimal projection for spectral partitioning converges to the projection admitting the largest margin hyperplane intersecting @xmath258 . if it is the case that the largest margin hyperplane intersecting @xmath232 also intersects @xmath258 , as is often the case , although this fact will not be known , then it is actually not necessary that @xmath331 tend towards zero . in such cases it only needs to satisfy @xmath332 for the corresponding values of @xmath115 and @xmath21 over all possible projections .",
    "in particular , choosing @xmath333 instead of @xmath21 is appropriate for all projections .",
    "[ thm : deltamargin ] let @xmath334 and let @xmath335 be non - increasing , positive , and satisfy @xmath336 for all @xmath255 .",
    "then for any @xmath337 there exists @xmath338 and @xmath339 s.t .",
    "@xmath340 , @xmath341 and @xmath342 + @xmath343 .",
    "let @xmath344 and @xmath345 .",
    "we assume that @xmath346 , since otherwise there is nothing to show .",
    "now , since spectral clustering solves a relaxation of the minimum normalised cut problem we have , @xmath347 the final inequality holds since for any @xmath93 s.t . @xmath348 and",
    "@xmath349 we must have @xmath350 @xmath351 .",
    "now , for any @xmath352 , let @xmath353 . by lemma [ thm : maxdistbound ] we know that @xmath354 , where @xmath355 @xmath356 . therefore , @xmath357 this gives the result .",
    "lemma  [ thm : deltamargin ] shows almost immediately that the margin admitted by the optimal projection for spectral bi - partitioning converges to the largest margin through @xmath258 as @xmath330 and @xmath331 go to zero .",
    "the main result of this section , theorem  [ thm : convergence ] , shows the stronger result that the optimal projection itself converges to the projection admitting the largest margin .",
    "[ thm : convergence ] let @xmath12 and suppose that there is a unique hyperplane , which can be parameterised by @xmath358 , intersecting @xmath258 and attaining maximal margin on @xmath74 .",
    "let @xmath359 be decreasing , positive and satisfy @xmath360 for all @xmath255 .",
    "then , @xmath361    take any @xmath255 .",
    "@xcite have shown that @xmath362 s.t . for @xmath363 , @xmath364margin@xmath365 margin@xmath366 . by lemma [ thm : deltamargin ]",
    "we know @xmath367 , @xmath368 s.t .",
    "if @xmath340 and @xmath369 then @xmath370 s.t . margin@xmath371 + margin@xmath366 , since @xmath276 is optimal for the pair @xmath277 .",
    "thus , by above , @xmath372 @xmath373 . but @xmath374 for any @xmath375 .",
    "since @xmath255 was arbitrary , we therefore have @xmath376 as @xmath377 .",
    "while the results of this section apply only for univariate projections , we have observed empirically that if a shrinking sequence of scaling parameters is employed for a multivariate projection , then the projected data tend to display large euclidean separation .",
    "this is illustrated in figure  [ fig : lmscyeast ] which shows two dimensional plots of the 72 dimensional yeast cell cycle analysis dataset .",
    "as in figure  [ fig : optidigitsprojections ] the top plots show the true clusters in the data and the bottom plots show the clustering assignments .",
    "the left plots show the result of a two dimensional projection pursuit using the proposed method . in the middle plots the first projection is learnt using one dimensional projection pursuit , and the second",
    "is set to be the direction of maximum variance within the null space of the first projection .",
    "the right plots use as the first projection the result of the iterative support vector regression method for maximum margin clustering  @xcite , and again the second projection is the direction of maximum variance within its null space .    a similar intuition which underlies the theoretical results of this section can be used to reason why this will occur in multivariate projections , in that as the scaling parameter reduces to zero the value of @xmath70 is controlled by the smallest distance between observations in different elements of the induced partition .",
    "it is however elusive how to formulate this rigorously in the presence of the constraint set @xmath232 in more than one dimension .",
    "in this section we discuss how a preprocessing of the data using _ microclusters _ can be used to significantly speed up the optimisation process .",
    "we derive theoretical bounds on the error induced by this approximation .",
    "our approach uses a result from matrix perturbation theory for diagonally dominant matrices , and therefore only applies to the standard laplacian , @xmath61 .",
    "however , we have seen empirically that a close approximation of the optimisation surface is obtained for both @xmath70 and @xmath71 .",
    "the concept of a microcluster was introduced by @xcite in the context of clustering very large data sets .",
    "microclusters are small clusters of data which can in turn be clustered to generate a clustering of the entire data set . a microcluster like approach in the context of spectral clustering has been considered by  @xcite , where the authors obtain bounds on the mis - clustering rate induced by the approximation . rather than using microclusters as an intermediate step towards determining a final clustering model , we use them to form an approximation of the optimisation surface for projection pursuit which is less computationally expensive to explore .",
    "the error bound depends on the ratio of cluster radii to scaling parameter . as such",
    ", this method does not provide a good approximation when @xmath330 is close to zero .",
    "our bounds rely on the following result from perturbation theory .",
    "[ thm : perturbbound]@xcite + let @xmath378 $ ] and @xmath379 $ ] be two symmetric positive semidefinite diagonally dominant matrices , and let @xmath380 and @xmath381 be their respective eigenvalues .",
    "if , for some @xmath382 , @xmath383 , and @xmath384 where @xmath385 , and similarly for @xmath386 , then @xmath387    an inspection of the proof of theorem [ thm : perturbbound ] reveals that @xmath388 is necessary only to ensure that the signs of @xmath389 are the same as those of @xmath390 . in the case of laplacian matrices this equivalence of signs holds by design , and so in this context the requirement that @xmath388 can be relaxed .",
    "+   + in the microcluster approach , the data set @xmath12 is replaced with @xmath48 points @xmath391 which represent the centers of a @xmath48-clustering of @xmath74 . by projecting these microcluster centers during subspace optimisation , rather than the data themselves , the computational cost associated with each eigen problem is reduced from @xmath392 to @xmath393 .",
    "if we define the radius , @xmath394 , of a cluster @xmath21 to be the greatest distance between one of its members and its center , that is , @xmath395 then we expect the approximation error to be small whenever the microcluster radii are small .",
    "the bounds on the approximation error which we present in this section are worst case and rely on standard eigenvalue bounds , and so can be pessimistic . to obtain a reasonable bound on the approximation surface , as many as @xmath396 might be needed , leading to only a threefold speed up .",
    "we have observed empirically , however , that even for @xmath397 ( and sometimes lower ) one still obtains a close approximation of the optimisation surface .",
    "this makes the projection pursuit of the order of 100 times faster .",
    "[ thm : approxbound1 ] let @xmath398 be a @xmath48-clustering of @xmath74 with centers @xmath391 , radii @xmath399 and counts @xmath400 . for @xmath334 define @xmath401 where @xmath402 is the diagonal matrix with @xmath403 and @xmath404 where @xmath405 are the projected microcluster centers and the similarities are given by @xmath406 , and @xmath407 is positive and non - increasing for @xmath408 .",
    "then , @xmath409 where @xmath410 and @xmath411 .    for brevity we temporarily drop the notational dependence on @xmath73 .",
    "let @xmath412 @xmath413 , where each @xmath414 is repeated @xmath107 times .",
    "let @xmath415 be the laplacian of the graph with vertices @xmath416 and edges given by @xmath417 .",
    "we begin by showing that @xmath418 .",
    "take @xmath419 , then , @xmath420 and so @xmath421 is positive semi - definite . in addition , it is straightforward to verify that @xmath422 @xmath423 , and hence @xmath424 is the smallest eigenvalue of @xmath421 with eigenvector @xmath425 .",
    "now , let @xmath37 be the second eigenvector of @xmath415",
    ". then @xmath426 for pairs of indices @xmath427 aligned with the same @xmath414 in @xmath416 .",
    "define @xmath428 s.t .",
    "@xmath429 where index @xmath152 is aligned with @xmath414 in @xmath430 .",
    "then @xmath431 where index @xmath432 is aligned with @xmath414 in @xmath433 for each @xmath9 .",
    "therefore @xmath434 and hence @xmath435 since @xmath46 is the smallest eigenvector of @xmath415 and so @xmath164 .",
    "similarly @xmath436 .",
    "thus @xmath437 and @xmath438 and so is a candidate for the second eigenvector of @xmath421 . in addition",
    "it is straightforward to show that @xmath439 . now , suppose by way of contradiction that @xmath440 with @xmath441 s.t . @xmath442 .",
    "then let @xmath443 where each @xmath444 is repeated @xmath107 times",
    ". then @xmath445 , @xmath446 and @xmath447 , a contradiction since @xmath37 is the second eigenvector of @xmath415 .",
    "now , let @xmath448 be such that @xmath449 and @xmath450 .",
    "we temporarily drop the notational dependence on @xmath215 . then , @xmath451 since @xmath452 contracts distances and @xmath453 and @xmath454 are the radii of @xmath455 and @xmath456 .",
    "since @xmath3 is non - increasing we therefore have , @xmath457 therefore @xmath458 now , we lose no generality by assume that @xmath74 is ordered such that for each @xmath9 the elements of cluster @xmath455 are aligned with @xmath414 in @xmath416 , since this does not affect the eigenvalues of the laplacian of @xmath459 , @xmath34 . by the design of the laplacian matrix the  @xmath17 \" of theorem  [ thm : perturbbound ] are exactly zero . for off diagonal terms",
    "@xmath460 with corresponding @xmath185 as above , consider @xmath461 theorem  [ thm : perturbbound ] thus gives the result .",
    "the above bound depends on @xmath73 via the quantity @xmath462 and for some functions @xmath3 it is difficult to remove this dependence .",
    "we consider the class of functions , parameterised by @xmath463 , and given by @xmath464 where we adopt the convention @xmath465 for any @xmath466 . for @xmath467",
    "this is equivalent to the laplace kernel , but for @xmath468 has the useful property of being differentiable at @xmath424 .",
    "we have found the choice of @xmath3 to matter little in the results of the proposed approach .",
    "the above class of functions is chosen as it allows us to obtain a uniform bound on the error induced by the above approximation .",
    "note the parameter @xmath469 is not intended as a tuning parameter , but rather we set @xmath469 close to zero to obtain a function similar to the laplace kernel , but which is differentiable at zero .",
    "[ thm : approxbound ] let the conditions of lemma  [ thm : approxbound1 ] hold , and let @xmath470 for @xmath463 .",
    "then , @xmath471    firstly , consider @xmath472 now , the function @xmath473 is non - decreasing in @xmath225 for @xmath474 , therefore by above @xmath475 secondly , consider the case @xmath476 , then @xmath477 since @xmath478 is non - increasing in @xmath225 for @xmath474 . on the other hand , if @xmath479 then , @xmath480 where the first inequality comes from the fact that @xmath479 and @xmath3 is decreasing . now , using the identity @xmath481 for @xmath482 , we have @xmath483 and so lemma  [ thm : approxbound1 ] gives the result .    for brevity we temporarily drop the notational dependence on @xmath73",
    "let @xmath484 , where each @xmath485 is repeated @xmath107 times .",
    "let @xmath415 be the laplacian of the graph with vertices @xmath416 and edges given by @xmath417 .",
    "it can be verified that @xmath418 .",
    "now , by the design of the laplacian matrix we know that all @xmath17 of theorem [ thm : perturbbound ] are exactly zero regardless of adjacency matrix . for off diagonal terms , let us assume that @xmath74 is ordered so that the columns corresponding to data assigned to cluster @xmath455 are aligned with @xmath485 as in @xmath416 . for @xmath486 s.t .",
    "@xmath487 and @xmath488 , let @xmath489 and @xmath490 .",
    "we have , @xmath491 notice that since @xmath227 contracts distances we have @xmath492 and so @xmath493 .",
    "now , first consider the case @xmath467 .",
    "we have , @xmath494 using the fact that @xmath481 for all @xmath482 we therefore have @xmath495 as required . now consider the case @xmath468 .",
    "observe that @xmath496 is increasing in @xmath226 and decreasing in @xmath225 .",
    "if @xmath497 , then we have @xmath498 alternatively , if @xmath499 , then suppose by way of contradiction that , @xmath500 now , the function @xmath501 is decreasing for @xmath502 .",
    "we have @xmath503 which is positive by assumption .",
    "this gives us @xmath504 this implies @xmath505 , a contradiction .",
    "we therefore have , @xmath506 by an analogous argument we can show , @xmath507 again using the fact that @xmath481 for @xmath482 we have , @xmath508 theorem [ thm : perturbbound ] then gives the result .",
    "tighter bounds can be derived if pairwise distances between elements from pairs of clusters are compared directly to the distances between the cluster centers , and for higher dimensional cases the additional tightness can be significant .",
    "we prefer to state the result as above due to the sole reliance on the internal cluster radii relative to scaling parameter .    to derive an expression for the derivative of @xmath509 we consider its equivalence with @xmath510 . in section  [ sec :",
    "method ] we showed that this derivative relies on the corresponding eigenvector , which we denote @xmath511 .",
    "this vector is unknown as we only solve the @xmath512 eigen - problem of @xmath513 .",
    "let us denote the second eigenvector of @xmath514 by @xmath33 .",
    "it can be verified that for @xmath515 s.t .",
    "@xmath487 we have @xmath516 .",
    "the derivative of @xmath517 can therefore be shown to be equal to @xmath518    while bounds of the above type are not verifiable for @xmath38 due to the fact that it is not diagonally dominant , a similar degree of agreement between the true and approximate eigenvalues has been observed in all cases considered . in this case",
    "the @xmath519 matrix is given by the normalised laplacian of the graph of @xmath520 with similarities given by @xmath521 .",
    "this matrix has the same structure as the original normalised laplacian , the only difference being the introduction of the factors @xmath522 .",
    "+   + figure [ pic : microclusterunnormalised ] shows ( a ) @xmath84 and ( b ) @xmath523 plotted against the single projection angle @xmath72 for the 2 dimensional s1 data set @xcite .",
    "the parameter @xmath330 was chosen using the same method as for our experiments .",
    "a complete linkage clustering was performed for 3000 microclusters ( @xmath524 of total number of data ) , as well as for 200 microclusters for comparison .",
    "the true values of @xmath84 and those based on approximations using @xmath525 microclusters are almost indistinguishable .",
    "the approximations based on @xmath526 microclusters also show a good approximation of the optimisation surface , and lie well within the bounds pertaining to the 3000 microcluster case .",
    "the same sort of agreement can be seen for @xmath523 .",
    "importantly , while the approximations based on 200 microclusters slightly underestimate the true eigenvalues , the location of the local minima , and indeed the shape of the optimisation surface , are very similar to the truth , and so optimising over this approximate surface leads to near optimal projections .",
    "we also show the absolute relative error , ( c ) and ( d ) , as described in lemma  [ thm : approxbound ] .",
    "the pessimism of the bound is clearly evident in the bottom left plot where the values of @xmath527 appear very close to zero on the scale of the theoretical bound .",
    "+   +     + true eigenvalue (  ) , bounds based on 3000 microclusters ( - - - ) , approximation using 3000 microclusters (  ) , approximation using 200 microclusters (  )    the second result uses the knowledge that the eventual partition intersects @xmath232 by aggregating the projected points lying either side each to a single point , and thereby forcing the intersection with @xmath232 . for each projection , the projected points lying below @xmath528 are replaced with a single point at @xmath528 . similarly the projected points lying above @xmath529 are replaced with a single point at @xmath529 .",
    "the similarities with the aggregate points and the projected points lying inside @xmath530 are weighted so that they are approximately equal to the sum of the similarities between the projected point and the projected points contributing to the aggregation .",
    "in fact , if @xmath467 then these sums are exact .    for @xmath55 let @xmath531 , @xmath532 and @xmath533 .",
    "define @xmath534 and @xmath535 .",
    "let @xmath536 , the set of projections lying inside @xmath530 combined with the boundaries of @xmath530 .",
    "define @xmath537 to be the diagonal matrix with @xmath538 and @xmath539 with @xmath540 then , @xmath541    let @xmath542 , where @xmath543 and @xmath529 are repeated @xmath544 and @xmath545 times respectively .",
    "let @xmath415 be the laplacian of the graph with vertices @xmath416 and edges given by @xmath417 , except those aligned with @xmath543 are decreased by a factor @xmath546 , and those aligned with @xmath529 are decreased by a factor @xmath547 . for notational brevity",
    "we again temporarily drop the notational dependence on the projection angle @xmath73 . as in the previous result",
    ", it can be verified that @xmath418 .",
    "again we assume the data are aligned appropriately , such that @xmath548 , @xmath549 , @xmath550 .",
    "firstly , if @xmath551 then @xmath552 and there is nothing to show . if @xmath553 , then consider @xmath554 observe that @xmath555 therefore , @xmath556 and , @xmath557 once again using the identity @xmath558 for @xmath559 we find @xmath560 the cases for @xmath561 and @xmath562 , @xmath563 , @xmath564 , and @xmath565 and @xmath562 all follow a similar argument . theorem [ thm : perturbbound ] is again used to give the result .",
    "the above bound depends on the ratio @xmath566 . as we mentioned at the start of this section ,",
    "this approximation is useful when we practically want to find large margin separators using spectral connectivity projection pursuit .",
    "provided @xmath331 shrinks at a rate faster than @xmath330 , the relative error induced by the approximation goes to zero , and hence the result of the previous section remains applicable .",
    "in this section we evaluate the proposed method on a large collection of benchmark datasets .",
    "we compare our approach with existing dimension reduction methods for clustering , where the final clustering result is determined using spectral clustering .",
    "in addition we consider solving our problem iteratively for a shrinking sequence of scaling parameters to find large margin separators , relying on the theoretical results presented in section  [ sec : maxmargin ] .",
    "we compare these results with the iterative support vector regression approach of  @xcite , a state - of - the - art maximum margin clustering algorithm .",
    "we compare the different methods based on two popular evaluation metrics for clustering , namely purity  @xcite and @xmath52-measure  @xcite .",
    "both metrics compare the label assignments made by a clustering algorithm with the true class labels of the data .",
    "they take values in @xmath567 $ ] with larger values indicating a better agreement between the two label sets , and hence a superior clustering result .",
    "purity is the weighted average of the largest proportion of each cluster which can be represented by a single class .",
    "@xmath52-measure is defined as the harmonic mean of measures of completeness and homogeneity .",
    "homogeneity is similar to purity , in that it measures the extent to which each cluster may be represented by a single class , but is given by the weighted average of the entropy of the class distribution within each cluster .",
    "completeness is symmetric to homogeneity , and measures the entropy of the cluster distribution within each class .",
    "@xmath52-measure therefore also captures the extent to which classes are split between clusters .",
    "we will use the following notation throughout this section :    * scp@xmath1 and sc@xmath568p@xmath1 refer to the proposed projection pursuits for minimising @xmath70 and @xmath71 respectively . *",
    "lmsc refers to the proposed approach of finding large margin separators , based on repeatedly minimising @xmath70 for a shrinking sequence of scaling parameters . *",
    "subscripts  @xmath569 \" and  @xmath570 \" indicate whether we use an orthogonal projection ( @xmath247 ) or a correlated one ( @xmath248 ) , respectively .",
    "* sc and sc@xmath568 refer to spectral clustering based on the eigen - decompositions of @xmath34 and @xmath38 respectively . *",
    "subscripts  @xmath571 \" and  @xmath572 \" indicate principal and independent component analysis projections respectively .",
    "for example , @xmath573 refers to spectral clustering using the normalised laplacian applied to the data projected into a principal component oriented subspace .",
    "* drsc abbreviates dimensionality reduction for spectral clustering , proposed by  @xcite .",
    "this existing approach applies only to the normalised laplacian . *",
    "isvr@xmath574 and isvr@xmath575 denote the iterative support vector regression approach for maximum margin clustering  @xcite , using the linear and gaussian kernels respectively .      to extend our approach to datasets containing multiple clusters , we simply recursively partition ( subsets of ) the dataset until the desired number of clusters is obtained .",
    "we prefer this approach to the alternative of directly seeking a projection which yields a full @xmath48 way partition of the dataset , i.e. , by minimising the sum of the first @xmath48 eigenvalues of the laplacians , as it is not always clear that all the clusters present in the data can be exposed using a single projection of fixed dimension . at each iteration in this recursive",
    "bi - partitioning we split the largest remaining cluster .",
    "the scaling parameter and initialisation are set for each bi - partition , given values determined by the subset of the data being partitioned .",
    "for the fixed scaling parameter approaches , scp@xmath1 and sc@xmath568p@xmath1 , we set @xmath576 , where @xmath58 is the dimension of the projection , @xmath110 is the size of the ( subset of the ) data and @xmath577 is the largest eigenvalue of the covariance matrix .",
    "the value @xmath578 captures the scale of the data , while @xmath579 accounts for the fact that distances scale roughly with the square root of the dimension .",
    "the denominator term , @xmath580 , is borrowed from kernel density methods and we have found it to work reasonably well for our applications as well . for the large margin approach , lmsc , @xmath330 is initialised at @xmath581 and decreased by a factor of two with each minimisation of @xmath70 , until convergence of the projection matrix .",
    "the initialisation of @xmath253 is via the first @xmath58 principal components . for the orthogonal projections we use a two dimensional projection , as this is the lowest dimensional space which can expose non - linear separation between clusters . for the correlated projections we provide a three dimensional initialisation , and it was found that in most cases a high quality univariate projection could be determined from this .    for the lmsc approach , because the values within the laplacian matrix approach zero , the optimisation becomes less robust , and we found that the correlated approach did not always lead to large margin separation .",
    "we believe this is as a result of the term controlling the correlation becoming too dominant relative to the decreasing eigenvalue unless very careful tuning of @xmath582 is performed .",
    "we therefore consider a univariate projection instead of the multivariate correlated approach in this case .",
    "recall that the parameter @xmath240 controls the size of the constraint set @xmath215 .",
    "it is clear that smaller values of @xmath240 will tend to lead to more balanced partitions , but a precise interpretation of the resulting cluster sizes is unavailable . at",
    "best bounds on the cluster sizes can be computed using chebyshev s inequality . rather than relying on these bounds , which may be loose and difficult to interpret in multivariate projections , we recommend applying the proposed method for a range of values of @xmath240 and selecting the solution corresponding to the largest value of @xmath240 which induces a specified balance in the partition .",
    "we define this balance to be satisfied if the smallest cluster size is at least half the average , i.e. , @xmath583 . in this way the effect of the constraint is limited while still producing the desired result .",
    "we initialise with a large value of @xmath240 and decrease by @xmath584 until the balance is met .",
    "if this balance is not met for @xmath585 , then the corresponding  unbalanced \" result is returned anyway .",
    "the parameter @xmath331 is set to @xmath586 and @xmath469 to @xmath587 .",
    "we have found these two parameters not to significantly influence the performance of the method .",
    "it is important to note , however , that that parameter @xmath469 controls the shape of the similarity function , and as a result there is an interplay between this value and the value of @xmath330 .",
    "for substantially larger values of @xmath469 we expect a smaller value of @xmath330 to be more appropriate .    for competing approaches based on spectral clustering we do the following .",
    "whenever the number of data exceeds 1000 we use the approximation method of  @xcite .",
    "following  @xcite , we set the reduced dimension to @xmath588 , where @xmath48 is the number of clusters .",
    "we compute clustering results for all values of @xmath330 in @xmath589 @xmath590 as well as for the local scaling approach of  @xcite , and report the highest performance in each case . for drsc we also considered the parameter setting used for our method , and to implement the local scalings of  @xcite these",
    "were recomputed with each iteration in the corresponding projected subspace .",
    "we also provided drsc with a warm start via pca as this improved performance over a random initialisation , and offers a more fair comparison .",
    "because of this extensive search over scaling parameters we expect competing methods to achieve very high performance whenever the corresponding dimension reduction is capable of exposing the clusters in the data well .    for the isvr maximum margin clustering method",
    ", we set the balancing parameter equal to @xmath591 as suggested by  @xcite when the cluster sizes are not balanced .",
    "we argue that the balance of the clusters will not be known in practice , and the unbalanced setting led to superior performance compared with the balanced setting in the examples considered .",
    "the isvr approach also generates only a bi - partition , and to generate multiple clusters we apply the same recursive approach as in our method .    the same two evaluation measures for binary partitions as in  @xcite , which are motivated by the ability of an algorithm to successfully distinguish at least one class from others , in that the majority of its members remain connected under the partition .",
    "these measures take on values between @xmath424 and @xmath592 , with higher values indicating a better quality partition .",
    "both measures compare the partition induced by an algorithm with the true class labels .",
    "we argue that utilising class labels for evaluation is necessary when the objectives underlying the different methods differ . in order for a binary partition to be meaningfully compared with a class distribution containing potentially many more than two classes ,",
    "the classes are merged into two super - classes , one associated with each of element of the partition .",
    "each class is assigned to the element of the partition which contains the majority of its members . from these",
    "we calculate the success ratio ( sr )  @xcite and @xmath52-measure  ( v )  @xcite of the partition and super - classes .",
    "the success ratio compares the number of successfully partitioned data with the number erroneously partitioned , while the @xmath52-measure is given by the harmonic mean of completeness and homogeneity , which relate to the entropy of the class distribution within the partition and the partition distribution within the classes .",
    "we compare the performance of the following methods for data bi - partitioning using projection pursuit / dimension reduction :    1 .",
    "scp@xmath1 : spectral connectivity projection pursuit .",
    "our method minimising @xmath70 .",
    "we set @xmath593 and @xmath594 , where @xmath595 is the @xmath596-th centile of the nearest neighbour distances in the data , and @xmath49 is the number of dimensions .",
    "the final partition is given by applying spectral clustering to @xmath597 , using the standard laplacian and with the above parameter settings . here",
    "@xmath598 is the local optimum found by our method , initialised using the first principal component of the data .",
    "sc@xmath568p@xmath1 : normalised spectral connectivity projection pursuit .",
    "our method minimising @xmath71 .",
    "we use the same parameter settings as for scp@xmath1 except we set @xmath242 since sc@xmath568p@xmath1 is less susceptible to outliers .",
    "the final partition is found by applying spectral clustering to @xmath597 , using the normalised laplacian and with the above parameter settings .",
    "lmsc : large margin spectral connectivity projection pursuit . to find large margin separators using our proposed method , we alternate between minimising @xmath70 and shrinking @xmath330 .",
    "we begin with @xmath599 and end with @xmath600 , and reduce @xmath330 by a factor of 3 each iteration . at each iteration",
    "we set @xmath601 .",
    "we found that constraining the solution even further than for scp@xmath1 led to better performance , and so we set @xmath602 .",
    "dip : projection pursuit for maximisation of the dip statistic  @xcite .",
    "we initialise , as for our methods , using the first principal component of the data .",
    "we generate three partitions in each experiment , arising from spectral clustering using the standard laplacian and with @xmath594 , spectral clustering using the normalised laplacian with @xmath594 and spectral clustering using the normalised laplacian with local scaling parameters as described by  @xcite .",
    "we report the best performance out of the three .",
    "pca : principal component analysis .",
    "as in the case of dip , we generate three partitions of the projected data and report the best performance from each experiment .",
    "drsc : dimension reduction for spectral clustering  @xcite . we found that using the scaling parameter described above resulted in poor performance , and that doubling it massively improved performance of the drsc method",
    "this is likely due to the fact that the gaussian kernel , used in drsc , has shorter tails than the function we employ , and hence a larger scaling parameter is needed .",
    "the final partition is given by normalised spectral clustering also with @xmath603 .",
    "itersvr : iterative support vector regression  @xcite .",
    "the itersvr method is a state - of - the - art algorithm for maximum margin clustering .",
    "we set the balancing parameter @xmath604 .",
    "this setting is proposed by the authors for unbalanced datasets .",
    "since the balance will not be known in general , we adopt this setting for all experiments .",
    "the final partition in this case is given by the algorithm itself .",
    "using neighbour distances to determine the scaling parameter is common in spectral clustering  @xcite .",
    "the factor @xmath605 is used as we expect distances to scale roughly with the square root of the number of dimensions .",
    "we acknowledge that additional work on selecting @xmath330 could improve performance .",
    "our simple method was chosen as it gives reasonable performance in most cases .",
    "the following benchmark datasets were used for comparison :    * optical recognition of handwritten digits ( opt .",
    "5620 8@xmath6068 compressed images of handwritten digits in @xmath607 , resulting in 64 dimensions with 10 classes .",
    "* pen based recognition of handwritten digits ( pen digits).@xmath1 10992 observations , each corresponding to a stylus trajectory ( @xmath608 coordinates ) from a handwritten digit in @xmath607 , i.e. , 10 classes .",
    "the trajectories are sampled at 8 time points , resulting in 16 dimensions . *",
    "satellite.@xmath1 6435 multispectral values from 3@xmath6063 pixel squares from satellite images , which results in 36 dimensions .",
    "there are 6 classes corresponding to different land types .",
    "* breast cancer wisconsin ( br .",
    "cancer).@xmath1 699 observations with 9 attributes relating to tumour masses .",
    "there are 2 classes corresponding to benign and malignant masses . * congressional votes ( voters).@xmath1 435 sets of 16 binary decisions on us congressional ballots .",
    "the 2 classes correspond to political party membership .",
    "* dermatology.@xmath1 366 observations corresponding to dermatology patients , each containing 34 dimensions derived from clinical and histopathological features .",
    "there are 6 classes corresponding to different skin diseases .",
    "* yeast cell cycle analysis ( yeast ) .",
    "698 yeast genes across 72 conditions ( dimensions ) .",
    "there are 5 classes corresponding to different genotypes .",
    "* synthetic control chart ( chart).@xmath1 600 simulated time series of length 60 displaying one of 6 fundamental characteristics , leading to 6 classes .",
    "* multiple feature digits ( m.f .",
    "digits).@xmath1 2000 handwritted digits in @xmath607 taken from dutch utility maps . following  @xcite we use only the 216 profile correlation features .",
    "* statlog image segmentation ( image seg.).@xmath1 2310 observations containing 19 features derived from 3@xmath6063 pixel squares from 7 outdoor images .",
    "each image constitutes a class .    before applying the clustering algorithms ,",
    "data were rescaled so that every feature had unit variance .",
    "this is a standard approach to handle situations where different features are captured on different scales and an appropriate rescaling is not obviously apparent from the context .",
    "for consistency we used this same preprocessing approach for all datasets .",
    "tables  [ tb : unnormpurity ] and  [ tb : unnormvm ] report the purity and @xmath52-measure respectively for the proposed method and spectral clustering using the standard laplacian applied to the original data , as well as their projection into pca and ica oriented subspaces .",
    "the tables report the average and standard deviation ( as subscript ) from 30 repetitions .",
    "the highest average performance for each dataset is highlighted in bold .",
    "both the orthogonal and correlated projection approaches achieve substantially higher performance than other methods in the majority of cases .",
    "there are few cases where they are not competitive with the best performing of the competing approaches , while there are mulitple examples where the proposed methods strongly outperform all others .",
    "the two versions of the proposed method are closely comparable with one another on average , with the correlated approach offering a slightly better worst case comparison .",
    "this , however , does not appear highly significant beyond sampling variation both within each dataset and with respect to the collection of datasets used for comparison .",
    "what is evident is that the added flexibility offered by multivariate projections does not result in a substantial improvement over univariate projections , which induce linear cluster boundaries .",
    "tables  [ tb : normpurity ] and  [ tb : normvm ] report the purity and @xmath52-measure respectively for the proposed approach based on minimising @xmath71 , the dimensionality reduction for spectral clustering algorithm  @xcite and spectral clustering based on the normalised laplacian applied to the original data and their pca and ica projections . again the tables show the average and standard deviation from 30 runs of each method , with the highest average performance on each dataset highlighted in bold .",
    "the proposed approach using both correlated and orthogonal projections is again competitive with all other methods in almost all cases considered . in addition both versions of the proposed approach",
    "substantially outperform the other methods in multiple examples .",
    "unlike in the case of the standard laplacian , here there is evidence that the orthogonal projection achieves better clustering results in general , outperforming the correlated approach in the majority of examples .",
    "it is important to note that the method described in section  [ sec : microclust ] does not provide a close approximation as @xmath609 . for the datasets containing more than 1000 data we use the microcluster approach for all values of @xmath330 and",
    "therefore only guarantee a large separation between the microclusters .",
    "it is arguable that this is a preferable objective as the maximum margin is not robust in the presence of noise , and it is not clear that it converges in the general setting  @xcite .",
    "microclusters have the potential to absorb some of the noise in the data , and in the event that they are of roughly equal density , maximising the margin over the microcluster centers has a similar effect to that of minimising the empirical density in a neighbourhood of the corresponding hyperplane separator .",
    "this is reminiscent of the soft - margin approach which does enjoy strong convergence properties  @xcite .",
    "in addition , since the optimisation is reinitialised for each value of @xmath330 , we are able to recompute the microclusters by performing the coarse clustering on the projected data with each iteration .",
    "this tends to lead to the margins in the microclusters being more closely related to the margins in the full dataset along the optimal projections .",
    "tables  [ tb : lmpurity ] and  [ tb : lmvmeasure ] report the average and standard deviation of the proposed lmsc as well as the iterative support vector regression approach  @xcite using both a linear and gaussian kernel for comparison . both versions of lmsc , using an orthogonal two dimensional and a one dimensional projection , outperform both versions of the iterative support vector regression in the majority of cases , with substantially higher performance in multiple examples . there is strong evidence that the two dimensional lmsc@xmath610 obtains better quality clustering results than the one dimensional alternative , showing sunstantially higher performance in the vast majority of cases considered .",
    "thus far we have compared different approaches for standard and normalised spectral clustering and for large margin clustering separately .",
    "these separate comparisons are important to understand the benefits of the proposed methods , however when considering the clustering problem abstractly it is necessary to compare all methods jointly .",
    "it is already clear that no method is uniformly superior to all others , since even within the separate comparisons no method outperformed the rest in every example .",
    "we find it important to reiterate the fact that for competing methods based on spectral clustering an extensive search over scaling parameters was performed and the best performance reported , whereas for our method only a simple data driven heuristic was used in every example .",
    "such a search is not possible in practice since the true lables will not be known , and hence the results reported for these methods likely overestimate their true expected performance in practice . what was evident , however , is that the local scaling approach of  @xcite is very effective and yielded the highest performance in roughly half the cases considered .",
    "it is clearly apparent from the performance of the various methods that the clustering problem differs vastly in difficulty across the different datasets considered . to combine the results from the different datasets we standardise them as follows . for each dataset",
    "@xmath8 we compute for each method the relative deviation from the average performance of all methods when applied to @xmath8 .",
    "that is , for each method @xmath611 we compute the relative purity , @xmath612 and similarly for @xmath52-measure .",
    "we can then compare the distributions of the relative performance measures from all datasets and for all methods .",
    "it is clear from table  [ tb : unnormpurity ] that the competing methods sc , sc@xmath613 and sc@xmath614 are not competitive with other methods in general , due to their vastly inferior performance on multiple datasets .",
    "moreover , their performance is sufficiently low to obscure the comparisons between others .",
    "these three methods are therefore omitted from this comparison .",
    "figures  [ fig : relmeasure ] and  [ fig : relvmeasure ] show boxplots of the relative performance measures for all other methods .",
    "the additional red dots indicate the mean relative performance measures for each method , and methods are ordered in decreasing order of their means . in the case of purity , all of the proposed methods outperform every method used for comparison , and except for the univariate large margin method , lmsc , the difference between the proposed methods and the methods used for comparison is substantial . in the case of @xmath52-measure ,",
    "the same is true except that in this case lmsc is outperformed on average by spectral clustering using the normalised laplacian applied to the pca projected data .",
    "notice that the most relevant comparison for lmsc is with isvr@xmath615 because of their similar objectives . in terms of both purity and @xmath52-measure ,",
    "lmsc significantly outperformed the existing large margin clustering method .    among the methods used for comparison ,",
    "it is evident that spectral clustering is capable of outperforming existing large margin clustering methods , provided an appropriate scaling parameter can be determined .",
    "of those spectral clustering variants , pca projections showed the best overall performance .",
    "while the drsc method  @xcite in some cases showed a substantial improvement over the simpler dimension reduction of pca , it did not yield consistently higher performance on the datasets considered .",
    "overall it is apparent that the proposed approach for projection pursuit based on spectral connectivity is highly competitive with existing dimension reduction methods .",
    "moreover , a simple data driven heuristic allowed us to select the important scaling parameter automotically without tuning it for each dataset , as is recommended for the drsc method  @xcite . among the variants of the proposed approaches , it is evident that while the flexibility of the multivariate projections offered higher performance on average than the corresponding univariate projections , it is only in the case of the large margin separation methods that this improvement is significant beyond the variation from the collection of datasets used for comparison .      in this subsection",
    "we investigate the sensitivity of the proposed approach to the setting of the important scaling parameter , @xmath330 .",
    "in addition we consider the effect on performance of the number of microclusters used in approximating the optimisation surface .",
    "for the former we consider the breast cancer , voters , dermatology , yeast and chart datasets as these exhibited very low variability in performance and offer more interpretable comparisons .",
    "figures  [ fig : sensitivityunnorm ] and  [ fig : sensitivitynorm ] show plots of the purity and @xmath52-measure values for @xmath330 taking values in @xmath616 , where @xmath617 is the value used in the experiments above .",
    "there is some variability in the performance for different values , with no clear pattern inditcating that a higher or lower value than the one used is better in general .",
    "importantly there are very few occurences of substantially poorer performance than that obtained with our simply chosen heuristic , and also it is clear that in the majority of cases performance could be improved from what is reported above if an appropriate tuning of @xmath330 is possible .",
    "+     +    to investigate the effect of microclusters on clustering accuracy we simulated datasets from gaussian mixtures containing 5 components ( clusters ) in 50 dimensions .",
    "this allows us to generate datasets of any desired size .",
    "for these experiments 30 sets of parameters for the gaussian mixtures were generated randomly . in the first case",
    "a single dataset of size 1000 was simulated from each set of parameters , and clustering solutions obtained for a number of microclusters , @xmath48 , ranging from 100 to 1000 , the final value therefore applying no approximation .",
    "figure  [ fig : microcluster1 ] shows the median and interquartile range of both performance measures for 10 values of @xmath48 .",
    "it is evident that aside from @xmath618 , performance is similar for all other values , and so using a small value , say @xmath619 , should be sufficient to obtain a good approximation of the underlying optimisation surface .    in the second",
    ", we fix the number of microclusters , @xmath619 , and for each set of parameters simulate datasets with between 1000 and 10 000 observations . in the most extreme case , therefore , the number of microclusters is only 2% of the total number of data . figure  [ fig : microcluster2 ] shows the corresponding performance plots , again containing the medians and interquartile ranges . even for datasets of size 10 000 ,",
    "the coarse approximation of the dataset through 200 microclusters is sufficient to obtain a high quality projection using the proposed approach .",
    "+     +      in this subsection we evaluate the algorithms on simulated data arising from mixtures of multivariate gaussian distributions .",
    "we consider cases with 5 mixture components in 10 and 200 dimensions for differing levels of component overlap . in each experiment ,",
    "the mean ( @xmath620 ) , covariance ( @xmath621 ) and mixture proportion ( @xmath622 ) of each component were generated randomly according to the following method .",
    "@xmath623^d&\\\\ \\label{eq : simulate } \\sigma_i = s^\\top s , \\",
    "j } \\sim n(0 , 1)&i = 1\\dots 5.\\\\ \\nonumber p_i \\propto u_i , u_i \\sim u[1 , 2]&\\end{aligned}\\ ] ]    here @xmath624 is the dimension and @xmath625 controls the overlap of the components . for each pair",
    "@xmath626 100 sets of parameters were generated and from the resulting distributions datasets of size 500 simulated .",
    "table  [ tb : resultsgaussian ] reports the average @xmath627 one standard deviation success ratio and @xmath52-measure of splits made by the different methods on each set of 100 experiments .",
    "the highest average performance in each case is highlighted in bold , and significantly lower performance using a standard one @xmath146-test at the 95% level is indicated by @xmath628 .",
    "tables  [ tb : gplot200 ] and  [ tb : gplot10 ] contain univariate density plots using kernel based estimates from the data projected into the optimal univariate subspace discovered by each method .",
    "we include the components of the density arising from each class to illustrate the strength of the resulting partition made by each method . in all cases",
    "the induced partition splits the projected data above / below a single point , which is indicated by the vertical lines .",
    "the datasets correspond to a single set of parameters generated by the method in  ( [ eq : simulate ] ) .",
    "when the number of degrees of freedom in the projection pursuit , that is , the dimensionality of the data , is large relative to the number of data , then it is often possible to find subspaces within which the data appear separable and yet the separation is not relevant to the class distribution in the data .",
    "this is evident in the case of the high dimensional gaussian simulation with high overlap , where in table  [ tb : gplot200 ] we see that for sc@xmath568p@xmath1 , drsc , lmsc and itersvr the projected density estimate is strongly bimodal and yet the induced partitions are no better than random allocations . in all but this most extreme case",
    "the performance of the proposed methods compare very favourably with all other methods considered . in the lower dimensional examples ,",
    "reported in table  [ tb : gplot10 ] , the modal structure of the complete dataset is more in accordance with the class distribution .",
    "the performance of the proposed methods compares favourably in general with all other methods considered .",
    "the most relevant comparisons are arguably between sc@xmath568p@xmath1 and drsc and between lmsc and itersvr , because of their similar objectives .",
    ".projection plots .",
    "gaussian simulations in 200 dimensions [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "we proposed a projection pursuit method for finding the optimal subspace in which to perform a binary partition of unlabelled data .",
    "the proposed method optimises the separability of the projected data , as measured by spectral graph theory , by minimising the second smallest eigenvalue of the graph laplacians .",
    "the lipschitz continuity and differentiability properties of this projection index with respect to the projection matrix were established , which enabled us to apply a generalised gradient descent method to find locally optimal solutions .",
    "compared with existing dimension reduction for spectral clustering , we derive expressions for the gradient of the overall objective and so find solutions within a single generalised gradient descent scheme , with guaranteed convergence to a local optimum .",
    "our experiments suggest that the proposed method substantially outperforms spectral clustering applied to the original data as well as existing dimensionality reduction methods for spectral clustering .",
    "a connection to maximal margin hyperplanes was established , showing that in the univariate case , as the scaling parameter of the similarity function is reduced towards zero , the binary partition of the projected data maximises the linear separability between the two clusters .",
    "implementing our method for a shrinking sequence of scaling parameters thus allows us to find large margin separators practically .",
    "we found that this approach outperforms state of the art methods for maximum margin clustering on a large collection of datasets .",
    "the computational cost of the proposed projection pursuit method per iteration is @xmath629 , where @xmath110 is the number of observations , and @xmath49 is the dimensionality , which can become prohibitive for large datasets . to ameliorate this an approximation method using microclusters , with provable error bounds",
    "is proposed .",
    "our sensitivity analysis , based on clustering performance , indicates that even for relatively few microclusters , the approximation of the optimisation surface is adequate for finding high quality subspaces for clustering .",
    "in the general case we may consider a set of @xmath48 microclusters with centers @xmath391 and counts @xmath400 .",
    "the derivations we provide in this appendix are valid for @xmath630 , and so apply to the exact formulation of the problem as well .",
    "let @xmath55 and let @xmath92 be the repeated projected cluster centers , @xmath631 @xmath632 , where each @xmath633 is repeated @xmath107 times . in section  [ sec : method ] we expressed @xmath634 via the chain rule decomposition @xmath635 .",
    "the compression of @xmath92 to the size @xmath48 non - repeated projected set , @xmath636 , requires a slight restructuring , as described in section  [ sec : microclust ] .",
    "+ we begin with the standard laplacian , and define @xmath402 and @xmath637 as in lemma  [ thm : approxbound1 ] .",
    "that is , @xmath402 is the diagonal matrix with @xmath9-th diagonal element equal to @xmath638 and @xmath639 .",
    "the derivative of the second eigenvalue of the laplacian of @xmath92 relies on the corresponding eigenvector , @xmath37 .",
    "however , this vector is not explicitly available as we only solve the @xmath519 eigen - problem of @xmath514 .",
    "let @xmath33 be the second eigenvector of @xmath514 .",
    "as in the proof of lemma  [ thm : approxbound1 ] if @xmath185 are such that the @xmath9-th element of @xmath92 corresponds to the @xmath152-th microcluster , then @xmath640 .",
    "the derivative of @xmath641 with respect to the @xmath9-th column of @xmath73 , and thus equivalently of the second eigenvalue of the laplacian of @xmath92 , is therefore given by @xmath642 where @xmath643 is the matrix with @xmath9-th column @xmath644 , @xmath92 is treated as a @xmath77 matrix with @xmath9-th column @xmath645 , and @xmath646 is given in eq .",
    "( [ eq : difftheta ] ) .",
    "now , the use of the constraint set @xmath530 and the associated transformation makes a further decomposition convenient .",
    "let @xmath647 .",
    "we provide expressions for the specific constraint sets used , i.e. , @xmath648 $ ] , where @xmath649 and @xmath650 is approximated by @xmath651 .",
    "for ease of exposition we assume that each @xmath652 is equal to zero , noting that no generality is lost through this simplification since the value of the eigenvalue of the laplacian is location independent .",
    "the data can therefore be centered prior to projection pursuit and the following formulation employed .",
    "we can then express the first component of  ( [ eq : deriv1 ] ) as @xmath653 , where = .9mu = .9mu = .9mu",
    "@xmath654 = 4mu = 4mu = 4mu and @xmath655 is the @xmath519 matrix with @xmath656 in the above we have used the lower case @xmath657 to denote the @xmath152-th element of the transformed projected dataset , where the upper case @xmath658 denotes the @xmath659-th element of the @xmath77 matrix with @xmath152-th column equal to @xmath657 .",
    "the benefit of this further decomposition lies in the fact that the majority of terms in the sums in  ( [ eq : deriv2 ] ) are zero .",
    "in fact , @xmath660 where for the function given in eq .",
    "( [ eq : kernel ] ) we have , @xmath661 for the normalised laplacian , the reduced @xmath519 eigenproblem has precisely the same form as the original @xmath662 problem , with the only difference being the introduction of the factors @xmath663 .",
    "in particular , the second eigenvalue of the normalised laplacian of @xmath92 is equal to the second eigenvalue of the laplacian of the graph of @xmath664 with similarities given by @xmath665 . with the derivation in section  [ sec : method ] we can see that the corresponding derivative is as for the standard laplacian , except that the coefficients @xmath666 in eq .",
    "( [ eq : coef ] ) are replaced with @xmath667 , where @xmath114 is the second eigenvalue of the normalised laplacian of @xmath664 , @xmath33 is the corresponding eigenvector and @xmath668 is the degree of the @xmath152-th element of @xmath664 .",
    "s.  ben - david , t.  lu , d.  pl , and m.  sotkov .",
    "learning low - density separators . in d.",
    "van dyk and m.  welling , editors , _ proceedings of the 12th international conference on artificial intelligence and statistics ( aistats ) _ , jmlr workshop and conference proceedings , pages 2532 , 2009 .",
    "kriegel , p.  krger , and a.  zimek .",
    "clustering high - dimensional data : a survey on subspace clustering , pattern - based clustering , and correlation clustering . _ acm transactions on knowledge discovery from data _ , 30 ( 1):0 158 , 2009 .",
    "h. narayanan , m. belkin , and p. niyogi . on the relation between low density separation , spectral clustering and graph cuts . in",
    "_ advances in neural information processing systems _ , pages 10251032 , 2006 .",
    "h.  weyl .",
    "das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen ( mit einer anwendung auf die theorie der hohlraumstrahlung ) .",
    "_ mathematische annalen _ , 710 ( 4):0 441479 , 1912 ."
  ],
  "abstract_text": [
    "<S> we study the problem of determining the optimal low dimensional projection for maximising the separability of a binary partition of an unlabelled dataset , as measured by spectral graph theory . </S>",
    "<S> this is achieved by finding projections which minimise the second eigenvalue of the laplacian matrices of the projected data , which corresponds to a non - convex , non - smooth optimisation problem . </S>",
    "<S> we show that the optimal univariate projection based on spectral connectivity converges to the vector normal to the maximum margin hyperplane through the data , as the scaling parameter is reduced to zero . </S>",
    "<S> this establishes a connection between connectivity as measured by spectral graph theory and maximal euclidean separation . </S>",
    "<S> it also allows us to apply our methodology to the problem of finding large margin linear separators . </S>",
    "<S> the computational cost associated with each eigen - problem is quadratic in the number of data . to mitigate this problem </S>",
    "<S> , we propose an approximation method using microclusters with provable approximation error bounds . </S>",
    "<S> we evaluate the performance of the proposed method on a large collection of benchmark datasets and find that it compares favourably with existing methods for projection pursuit and dimension reduction for unsupervised data partitioning .    * minimum spectral connectivity projection pursuit for unsupervised classification * +   + @xmath0 stor - i center for doctoral training , lancaster university + @xmath1 department of management science , lancaster university + @xmath2 department of mathematics and statistics , lancaster university    keywords + spectral clustering , dimension reduction , projection pursuit , eigenvalue optimisation , maximum margin clustering </S>"
  ]
}