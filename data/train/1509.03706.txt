{
  "article_text": [
    "a complex system consists of multiple interacting parts or subsystems .",
    "a prominent example is the human brain that exhibits structure spanning a hierarchy of multiple spatial and temporal scales @xcite .",
    "a series of recent papers have focused on the problem of information decomposition in complex systems @xcite .",
    "a simple version of the problem can be stated as follows : the total mutual information that a pair of _ predictor _ random variables ( rvs ) @xmath0 convey about a _ target _",
    "rv @xmath1 can have aspects of _ synergistic information _",
    "( conveyed only by the joint rv @xmath2 ) , of _ redundant _ information ( _ identically _ conveyed by both @xmath3 and @xmath4 ) , and of unique or private information ( _ exclusively _ conveyed by either @xmath3 or @xmath4 ) .",
    "is there a principled information - theoretic way of decomposing the total mutual information @xmath5 into nonnegative quantities ?    developing a principled approach to disentangling synergy and redundancy has been a long standing pursuit in neuroscience and allied fields @xcite , @xcite .",
    "however , the traditional apparatus of shannon s information theory does not furnish ready - made tools for quantifying multivariate interactions . starting with the work of williams and beer @xcite ,",
    "several workers have begun addressing these issues @xcite . for the general case of @xmath6 predictors , williams and",
    "beer @xcite proposed the partial information ( pi ) decomposition framework to specify how the total mutual information about the target is shared across the singleton predictors and their overlapping or disjoint coalitions . effecting a nonnegative decomposition",
    "has however turned out to be a surprisingly difficult problem even for the modest case of @xmath7 @xcite , @xcite .",
    "furthermore , there seems to be no clear consensus as to what is an ideal measure of redundant information .",
    "we focus on the relationship between redundant information and the more familiar information - theoretic notions of common information @xcite .",
    "we distinguish synergistic and redundant interactions that exist within a group of predictor rvs from those that exist between a group of predictor rvs and a target rv .",
    "a popular measure of the former ( symmetric ) type of interaction is the co - information @xcite .",
    "our main interest , however , lies in asymmetric measures of interaction that distinguish the target rv from the group of predictor rvs .",
    "an instance of such an interaction is when populations of retinal ganglion cells ( predictors ) interact to encode a ( target ) visual stimulus .",
    "yet another instance is when multiple genes ( predictors ) cooperatively interact within cellular pathways to specify a ( target ) phenotype @xcite . in building up to our main contribution , we review and extend existing ( symmetric ) measures of common information to capture the asymmetric nature of these interactions .    _ section organization and summary of results_. in section ii , building on the heuristic notion of embodying information using @xmath8-algebras and sample space partitions , we formalize the notions of common and private information structures .",
    "information in the technical sense of entropy hardly captures the structure of information embodied in a source .",
    "first introduced by shannon in a lesser known , short note @xcite , information structures capture the quintessence of `` information itself . ''",
    "we bridge several inter - related domains  notably , game theory , distributed control , and team decision problems to investigate the properties of such structures .",
    "surprisingly , while the ideas are not new , we are not aware of any prior work or exposition where common and private information structures have received a unified treatment .",
    "for instance , the notion of common information structures have appeared independently in at least four different early works , namely , that of shannon @xcite , gcs and krner @xcite , aumann @xcite , and hexner and ho @xcite , and more recently in @xcite , @xcite , @xcite . in the first part of ( mostly expository ) section ii",
    ", we make some of these connections explicit for a finite alphabet .    in the second part of section",
    "ii , we take a closer look at the intricate relationships between a pair of rvs . inspired by the notion of private information structures @xcite , we derive a measure of private information and show how a dual of that measure recovers a known result @xcite in the form of the minimal sufficient statistic for one variable with respect to the other . we also introduce two new measures of common information .",
    "the richness of the decomposition problem is already manifest in simple examples when common and private informational parts can not be isolated .    in section iii , we inquire if a nonnegative pi decomposition of @xmath5 can be achieved using a measure of redundancy based on the notions of common information due to gcs and krner @xcite and wyner @xcite .",
    "we answer this question in the negative . for independent predictor rvs",
    "when any nonvanishing redundancy can be attributed solely to a mechanistic dependence between the target and the predictors , we show that any common information based measure of redundancy can not induce a nonnegative pi decomposition .",
    "let @xmath9 be a fixed probability triple , where @xmath10 is the set of all possible outcomes , elements of the @xmath8-algebra @xmath11 are events and @xmath12 is a function returning an event s probability . a random variable ( rv )",
    "@xmath13 taking values in a discrete measurable space @xmath14 ( called the alphabet ) is a measurable function @xmath15 such that if @xmath16 , then @xmath17 .",
    "the @xmath8-algebra induced by @xmath13 is denoted by @xmath18 .",
    "we use `` iff '' as a shorthand for `` if and only if '' .      the heuristic notion of embodying information using @xmath8-algebras",
    "is not new @xcite , @xcite .",
    "a sense in which @xmath18 represents information is given by the following lemma ( see lemma 1.13 in @xcite ) .",
    "let @xmath19 and @xmath20 be two rvs , where @xmath21 is a standard borel space . then @xmath22 is @xmath23-measurable , or equivalently @xmath24 iff there exists a measurable mapping @xmath25 such that @xmath26 .",
    "suppose an agent does not know the `` true '' point @xmath27 but only observes an outcome @xmath28 .",
    "if for each drawn @xmath29 , he takes some decision @xmath30 , then clearly @xmath28 determines @xmath30 so that we necessarily have @xmath26 .",
    "the doob - dynkin lemma says that this is equivalent to @xmath22 being @xmath23-measurable under some reasonable assumptions on the underlying measurable spaces .    from lemma 1 , it is easy to see that @xmath31 and @xmath22 carry the `` same information '' iff @xmath32 .",
    "this notion of _ informational sameness _ ( denoted @xmath33 ) induces a partition on the set of all rvs into equivalence classes called _",
    "information elements_. we say that the rv @xmath34 is _ representative _ of the information element @xmath35 .",
    "first introduced by shannon in a ( perhaps ) lesser known , short note @xcite , information elements capture the quintessence of _ information itself _ in that all rvs within a given class can be derived from a representative rv for that class using finite state reversible encoding operations , i.e. , with 1-to-1 mappings .",
    "contrast the notion of information elements with the shannon entropy of a source @xmath13 , denoted @xmath36 .",
    "two sources @xmath31 and @xmath22 might produce information at the same entropy rate .",
    "entropies for countable alphabets can be infinite and even discontinuous . in the later sections",
    ", we shall be dealing solely with finite discrete rvs.]@xmath37 , @xmath38 , if @xmath25 is a bijection such that @xmath26 , then @xmath39 , i.e. , entropy is invariant under relabeling . ] , but not necessarily produce the `` same '' information .",
    "thus , @xmath40 , but the converse of neither implication is true .",
    "a partial order between two information elements @xmath41 and @xmath42 is defined as follows : @xmath43 iff @xmath44 or equivalently iff @xmath45 is @xmath46-measurable .",
    "we say that @xmath41 is _ larger _",
    "than @xmath42 or equivalently @xmath42 is an abstraction of @xmath41 .",
    "likewise , we write @xmath47 if @xmath48 , when @xmath41 is _ smaller _ than @xmath42",
    ". there exists a natural metric @xmath49 on the space of information elements and an associated topology induced by @xmath49 @xcite .",
    "@xmath49 is defined as follows : @xmath50 .",
    "clearly , @xmath51 iff @xmath43 and @xmath48 .",
    "the _ join _ of two information elements @xmath41 and @xmath42 is given by @xmath52 ( denoted @xmath53 ) and is called the _ joint information _ of both @xmath41 and @xmath42 .",
    "the joint rv @xmath54 is representative of the joint information .",
    "likewise , the _ meet _ is given by @xmath55 ( denoted @xmath56 ) and is called the _ common information _ of @xmath41 and @xmath42 .",
    "@xmath57 is the representative _ common rv _",
    "the entropy of both the joint and common information elements are invariant in a given equivalent class .    a finite set of information elements endowed with the partial order @xmath58 , join ( @xmath59 ) , and meet ( @xmath60 ) operations have the structure of a metric lattice which is isomorphic to a finite partition lattice @xcite , @xcite . as a simple example , the lattice structure arising out of a @xmath61 operation is the diamond lattice @xmath62 , the smallest instance of a nondistributive modular lattice .",
    "the nondistributivity is easily seen as follows : let @xmath63 where @xmath41 and @xmath42 are independent information elements .",
    "in this example , @xmath64 , whereas @xmath65 . in general however ,",
    "information lattices are neither distributive nor modular @xcite , @xcite . more important for our immediate purposes",
    "is the notion of common information as defined by shannon @xcite which arises naturally when quantifying information embodied in structure .",
    "contrast this with shannon s mutual information which does not correspond to any element in the information lattice .",
    "the modeling of information structures can also be motivated nonstochastically , i.e. , when the underlying space has no probability measure associated with it ( e.g. , see @xcite , @xcite , @xcite , @xcite ) .",
    "let @xmath66 be a measurable space , where @xmath10 is the set of possible states of nature , and elements of @xmath11 are events .",
    "one of the states @xmath27 is the `` true '' state .",
    "an event @xmath67 _ occurs _",
    "when @xmath68 .",
    "define an _",
    "uncertain variable _",
    "@xmath13 @xcite taking values in a discrete measurable space @xmath14 as the measurable function @xmath15 where @xmath69 contains all singletons .",
    "the @xmath8-algebra induced by @xmath13 is @xmath70 .",
    "@xmath13 generates a partition on @xmath10 called the _ information partition _ @xmath71 . since the alphabet @xmath72 is finite or countable , @xmath73 .",
    "the information structure @xmath74 specifies the extent to which an agent observing @xmath13 can distinguish among different states of nature .",
    "given an observation @xmath75 , an agent endowed with a partition @xmath76 only knows that the true state belongs to @xmath77 , where @xmath77 is the element of @xmath13 s partition that contains @xmath29 .",
    "given a pair of partitions @xmath78 on @xmath10 , @xmath79 is said to be _ finer _ than @xmath80 and that @xmath80 is _ coarser _ than @xmath79 if @xmath81 @xmath82 . if @xmath79 is finer than @xmath80 , then agent @xmath83 has _ more precise _ information than agent @xmath84 in that @xmath83 can distinguish between more states of nature .",
    "@xmath13 knows an event @xmath67 at @xmath29 _ if @xmath85 .",
    "@xmath67 can only be known if it occurs .",
    "the event that _ @xmath13 knows @xmath67 _ is the set @xmath86 .",
    "then , given two agents , alice observing @xmath13 and bob observing @xmath1 , @xmath87 is the event that @xmath67 is _ mutually _ known ( between alice and bob ) .",
    "we say that an event @xmath67 is _ commonly _ known ( to both alice and bob ) if it occurs , or equivalently , an _",
    "event @xmath67 is common information _",
    "iff @xmath88 , where @xmath89 is the finest common coarsening of the agents partitions as follows .",
    "an event _ @xmath67 is common knowledge at @xmath29 _",
    "iff @xmath90 , i.e. , the event that @xmath67 is common knowledge is @xmath91 .",
    "for any event @xmath67 , @xmath92 .",
    "@xmath67 is _ common information _ if @xmath93 @xcite , @xcite.]@xmath94-algebras of events instead of partitions . ] .",
    "since the @xmath8-algebra generated by @xmath95 is simply @xmath96 , or equivalently , @xmath97 , @xmath67 is common information iff @xmath98 .",
    "commonly knowing @xmath67 is a far stronger requirement than mutually knowing @xmath67 . for finite @xmath72 ,",
    "@xmath99 , the common information structure admits a representation as a graph @xmath100 with the vertex set @xmath101 and an edge connecting two vertices if the corresponding atoms @xmath102 and @xmath103 are contained in a single atom of @xmath104 or @xmath105 or of both .",
    "the connected components of @xmath100 are in one - to - one correspondence with the atoms of @xmath95 @xcite .",
    "let @xmath106 .",
    "alice observes @xmath13 which generates the information partition @xmath107 .",
    "likewise , bob observes @xmath1 which induces the partition , @xmath108 .",
    "let @xmath109 be the true state of nature .",
    "consider the event @xmath110 .",
    "both alice and bob know @xmath67 at @xmath109 , since @xmath111 and @xmath112 .",
    "the event that alice knows @xmath67 is simply the true state @xmath113 _ _ ( _ _ i.e . , @xmath114 _ _ ) _ _ , whereas for bob , @xmath115 .",
    "clearly , bob can not tell apart the true state @xmath113 _ _ ( _ _ in which alice knows @xmath67 _ _ ) _ _ from @xmath116 _ _ ( _ _ in which alice does not know @xmath67__)__. hence , @xmath67 is not commonly known to alice and bob .",
    "on the other hand , it is easy to check that the events @xmath117 and @xmath118 are common information .",
    "indeed , @xmath119 .",
    "@xmath100 has the vertex set @xmath120 and the connected components of @xmath100 correspond to the atoms @xmath117 and @xmath118 of @xmath95 .",
    "one may also seek to characterize the private information structures of the agents .",
    "let @xmath10 be a finite set of states of nature . to simplify notation ,",
    "let @xmath13 denote the agent @xmath13 as well as its information partition .",
    "let alice and bob be endowed , respectively , with information partitions @xmath13 and @xmath1 so that @xmath13 and @xmath1 are subalgebras of a @xmath121-element boolean algebra .",
    "one plausible definition of the _ private information structure of @xmath1 _ is the minimal amount of information that @xmath13 needs from @xmath1 to reconstruct the joint information @xmath122 @xcite .",
    "define @xmath123 .",
    "since @xmath124 complements @xmath13 to reconstruct @xmath122 , minimality of @xmath125 entails that @xmath126 , @xmath127 , where @xmath128 denotes the two - element algebra .",
    "witsenhausen @xcite showed that the problem of constructing elements of @xmath124 with minimal cardinality is equivalent to the chromatic number problem for a graph @xmath129 with the vertex set @xmath1 and an edge connecting vertices @xmath102 and @xmath103 iff there exists an atom @xmath130 such that @xmath131 and @xmath132 .",
    "unfortunately , since there are multiple valid minimal colorings of @xmath129 , @xmath124 is not be unique .",
    "the following example illustrates the point .",
    "consider the set , @xmath133 .",
    "let alice and bob s partitions be respectively , @xmath134 @xmath135 and @xmath136 @xmath137 .",
    "@xmath138 has the edge set @xmath139 , @xmath140 @xmath141 .",
    "two distinct minimal colorings of @xmath129 are as follows : @xmath142 so that @xmath143 and @xmath144 so that @xmath145    it is easy to see that @xmath146",
    ". hence , such a minimal coloring is not unique and consequently , @xmath147 is not unique .",
    "one would also like to characterize the information contained exclusively in either @xmath13 or @xmath1 .",
    "the _ private information structure of @xmath1 with respect to @xmath13 _ may be defined as the amount of information one needs to reconstruct @xmath1 from the common information @xmath148 .",
    "define @xmath149 , where minimality of @xmath125 entails that @xmath150 , if there exists a @xmath151 such that @xmath152 and @xmath153 , then @xmath154 . we note that , if @xmath155 , then @xmath156 and @xmath127 .",
    "hexner and ho @xcite proposed and showed that this definition does not admit a unique specification for the private information of @xmath1 with respect to @xmath13 as can be seen from the following example .",
    "consider the set , @xmath157 and the following partitions on @xmath158 @xmath159 , and @xmath160 .",
    "then we have , @xmath161 and @xmath162 .",
    "it is easy to see that each of the following subalgebras satisfies the definition , i.e. , given @xmath163 and @xmath164 , we have , @xmath165 and @xmath166 .",
    "hence , @xmath167 is not unique .",
    "_ we have the following observations .",
    "note that if @xmath168 , then @xmath169 .",
    "thus , one can find a @xmath170 such that @xmath171 . choosing @xmath172 minimal",
    ", it follows that the cardinality of the minimal algebras of @xmath167 is lower bounded by the cardinality of the minimal algebras of @xmath124 or equivalently by the chromatic number of @xmath129 . thus , @xmath13 need not use all of @xmath167 to reconstruct @xmath122 .",
    "furthermore , it is known that the lattice @xmath173 of subalgebras of a finite boolean algebra is isomorphic to a finite partition lattice @xcite .",
    "thus , in general , @xmath173 is not distributive , nor even modular . since both the structures @xmath124 and @xmath167 consists of complements in @xmath173 , nonmodularity of @xmath173 implies the nonuniqueness of the private information structures .",
    "_      we now turn to mainstream information - theoretic notions of `` common information '' ( ci ) .",
    "we introduce the remaining notation . for a discrete , finite - valued rv @xmath13",
    ", @xmath174 denotes the probability mass function ( pmf or distribution ) of @xmath13 .",
    "we abbreviate @xmath175 as @xmath176 when there is no ambiguity . for @xmath177 , the entropy @xmath36 of @xmath13",
    "can be written as @xmath178 , where @xmath179 and @xmath180 .",
    "the kullback - leibler ( kl ) divergence from @xmath181 to @xmath182 is defined as @xmath183 .",
    "@xmath184 denotes that @xmath13 is conditionally independent of @xmath125 given @xmath1 ( denoted @xmath185 ) , or equivalently , @xmath186 form a markov chain satisfying @xmath187 equivalently , @xmath188 .",
    "let @xmath189 be i.i.d .",
    "copies of the pair @xmath190 on @xmath191 .",
    "an information source generating such a ( stationary ) sequence is called a two - component discrete , memoryless source ( 2-dms ) . given @xmath192 , we say that @xmath193 @xmath194-recovers @xmath195 iff @xmath196 .    to fix ideas , consider a `` one - decoder '' network for the distributed compression of a 2-dms @xcite .",
    "the correlated streams @xmath197 and @xmath198 are encoded separately at rates @xmath199 and @xmath200 and decoded jointly by combining the two streams to @xmath194-recover @xmath201 .",
    "a remarkable consequence of the slepian - wolf theorem @xcite is that the ( minimum ) sum rate of @xmath202 is achievable .",
    "this immediately gives a coding - theoretic interpretation of shannon s mutual information ( mi ) as the maximum descriptive savings in sum rate by considering @xmath203 jointly rather than separately , i.e. , @xmath204 thus , for the one - decoder network , mi appears to be a natural measure of ci of two dependent rvs .",
    "however , other networks yield different ci measures .",
    "indeed , as pointed out in @xcite , depending upon the number of encoders and decoders and the network used for connecting them , several notions of ci can be defined .",
    "we restrict ourselves to two dependent sources and a `` two - decoder '' network when two different notions of ci due to gcs and krner @xcite and wyner @xcite are well known .",
    "each of these notions appear as solutions to asymptotic formulations of some distributed information processing task .",
    "given a sequence @xmath201 generated by a 2-dms @xmath205 , gcs and krner ( gk ) @xcite defined ci as the maximum rate of common randomness ( cr ) that two nodes , observing sequences @xmath206 and @xmath207 separately can extract without any communication , i.e. , @xmath208 where the supremum is taken over all sequences of pairs of deterministic mappings @xmath209 such that @xmath210 .",
    "the zero pattern of @xmath211 is specified by its characteristic bipartite graph @xmath212 with the vertex set @xmath213 and an edge connecting two vertices @xmath214 and @xmath215 if @xmath216 . if @xmath212 is a single connected component , we say that @xmath211 is _ indecomposable_. an _ ergodic decomposition _ of @xmath211 is defined by a unique partition of the space @xmath217 into connected components @xcite , @xcite , @xcite .",
    "given an ergodic decomposition of @xmath218 such that @xmath219 , define the rv @xmath220 as @xmath221 .",
    "for any rv @xmath222 such that @xmath223 , we have @xmath224 so that @xmath220 has the maximum range among all @xmath222 satisfying @xmath223 . in this sense , @xmath220 is the maximal common rv .",
    "gcs and krner independently proposed the notion of common information two decades following shannon s work @xcite .",
    "] of @xmath13 and @xmath1 .",
    "remarkably , gk showed that @xmath225 thus , common gk codes can not exploit any correlation beyond deterministic interdependence of the sources .",
    "@xmath226 depends solely on the zero pattern of @xmath218 and is zero for all indecomposable distributions .",
    "the following double markovity lemma ( see proof in appendix a ) is useful .    a triple of rvs @xmath227 satisfies the double markov conditions @xmath228 iff there exists a pmf @xmath229 such that @xmath230 and @xmath231 .",
    "furthermore , ( 2 ) implies @xmath232 iff @xmath233 .",
    "_ for all @xmath234 we have @xmath235 .",
    "we say that @xmath211 is _ saturable _ if @xmath236 .",
    "equivalently , @xmath211 is saturable iff there exists a pmf @xmath237 such that @xmath238 ( see lemma a1 in appendix a ) .",
    "we say that the triple @xmath227 has a _ pairwise double markov _ structure when the latter condition holds .",
    "_    the following alternative characterizations of @xmath226 follow from lemma 2 @xcite . @xmath239 where the cardinality of the alphabet @xmath240 is bounded as @xmath241 .",
    "wyner @xcite defined ci as the minimum rate of cr needed to simulate a 2-dms @xmath242 using local operations and no communication .",
    "more precisely , given access to a common uniform random string @xmath243 $ ] and independent noisy channels @xmath244 and @xmath245 such that @xmath246 @xmath194-recovers @xmath201 , the wyner ci , denoted @xmath247 , is the minimum cost ( in terms of the number of common random bits per symbol @xmath248 ) for the distributed approximate simulation of @xmath211 .",
    "@xmath247 admits an elegant single - letter characterization , @xmath249 where again @xmath241 .    a related notion of _ common entropy _",
    ", @xmath250 is useful for characterizing a zero - error version of the wyner ci @xcite .",
    "@xmath251    gray and wyner ( gw ) @xcite devised a distributed lossless source coding network for jointly encoding the 2-dms into a common part ( at rate @xmath252 ) and two private parts ( at rates @xmath199 and @xmath200 ) , and separately decoding each private part using the common part as side information .",
    "the optimal rate region @xmath253 for this `` two - decoder '' network configuration is given by , @xmath254 where @xmath255 is the set of all conditional pmfs @xmath256 s.t .",
    "@xmath241 . a trivial lower bound to @xmath253 follows from basic information - theoretic considerations @xcite , @xmath257 the different notions of ci can be viewed as extreme points for the corresponding common rate @xmath252 in the two - decoder network , i.e. , for @xmath258 , we have @xmath259    _ the different notions of ci are related as , @xmath260 , with equality iff @xmath218 is saturable , whence @xmath261 ( see lemma a2 in appendix a ) .",
    "_    _ @xmath262 is monotonically nonincreasing in the number of input arguments @xmath6 .",
    "in contrast , @xmath263 is monotonically nondecreasing in @xmath6 .",
    "it is easy to show that @xmath264 , while @xmath265 for any @xmath266 ( see lemma a3 in appendix a ) .",
    "_    witsenhausen @xcite defined a symmetric notion of private information .",
    "witsenhausen?s total private information , denoted @xmath267 , is defined as the complement of wyner s ci , @xmath268    one can define the private information of @xmath1 with respect to @xmath13 ( denoted @xmath269 ) as @xmath270 likewise , the complement of @xmath269 is defined as @xmath271 the double markov constraint ( see lemma 2 ) already hints at the structure of the minimizer @xmath222 in ( 7 ) .",
    "the following lemma ( see proof in appendix a ) shows that the minimizer in @xmath272 is a minimal sufficient statistic of @xmath1 with respect to @xmath13 .",
    "let @xmath273 denote a function @xmath274 from @xmath275 to the probability simplex @xmath276 ( the space of all distributions on @xmath72 ) that defines an equivalence relation on @xmath99 : @xmath277 then @xmath273 is a minimal sufficient statistic of @xmath1 with respect to @xmath13 .",
    "theorem 1 gives a decomposition of @xmath278 into a part that is correlated with @xmath13 ( @xmath279 ) and a part that carries no information about @xmath13 ( @xmath280 ) ( see proof in appendix a ) .",
    "for any pair of correlated rvs @xmath281 , the following hold : @xmath282    let @xmath283 , @xmath284 , where @xmath285@xmath286s and @xmath287@xmath286s having different subscripts are distinct ( but not necessarily disjoint ) subsets .",
    "let @xmath288 admit a unique decomposition into _ components _",
    "@xmath289 so that @xmath290 , and @xmath291 is a partition of @xmath99 induced by the equivalence relation in lemma 3 , i.e. , @xmath292 and @xmath293 .",
    "we also require that each component is the `` largest '' possible in the sense that for any two components @xmath294 , @xmath295 , there exists @xmath296 such that @xmath297 .",
    "the size of the component @xmath298 is defined as @xmath299 .",
    "given such a unique decomposition of @xmath288 into components @xmath289 , the following theorem gives necessary and sufficient conditions for @xmath300 achieving its minimum and maximum value ( see proof in appendix a ) .",
    "@xmath300 achieves its minimum , @xmath301 iff there exist no component with size greater than one .    on the other hand",
    ", @xmath300 achieves its maximum , @xmath302 iff @xmath211 is saturable iff each component @xmath298 is a connected component induced by the ergodic decomposition of @xmath211 .",
    "@xmath300 attains the lower bound for the following distribution @xmath303 .",
    "let @xmath304 .",
    "we write @xmath305 .",
    "given , @xmath306 , or graphically , @xmath307 .",
    "let @xmath308 .",
    "then we have @xmath309 $ ] , @xmath310 $ ] , and @xmath311 $ ] , so that @xmath312 .",
    "consequently @xmath301 .",
    "one can also easily verify that @xmath313 .",
    "the quantity @xmath272 first appeared in @xcite where it was called the dependent part of @xmath1 from @xmath13 .",
    "intuitively , @xmath272 is the rate of the information contained in @xmath1 about @xmath13 .",
    "@xmath272 also appears in @xcite , @xcite and has the following coding - theoretic interpretation in a source network with coded side information setup where @xmath13 and @xmath1 are encoded independently ( at rates @xmath314 and @xmath315 , resp . ) and a ( joint ) decoder needs to recover @xmath13 ( with small error probability ) using the rate - limited side information @xmath1 : @xmath272 is the minimum rate @xmath315 such that @xmath316 is achievable @xcite .",
    "the following example shows that even though @xmath278 admits a decomposition of the form in ( 8c ) , it might not always be possible to isolate its parts @xcite .",
    "let @xmath317 and @xmath318 .",
    "consider the perturbed uniform distribution @xmath218 with @xmath319 , where @xmath320 .",
    "if @xmath321 , @xmath322 . however , if @xmath323 , then @xmath324 .",
    "in fact , if @xmath323 , as @xmath325 , @xmath326 , while @xmath327 . thus , even when @xmath328 , one needs to transmit the entire @xmath1 ( i.e. , @xmath329 ) to convey the full information contained in @xmath1 about @xmath13 .",
    "we now briefly review some related candidate bivariate correlation measures .",
    "we highlight a duality in the optimizations in computing the various ci quantities .    starting with witsenhausen @xcite , the hirschfeld - gebelein - rnyi ( hgr ) maximal correlation",
    "@xcite has been used to obtain many impossibility results for the noninteractive simulation of joint distributions @xcite .",
    "the maximal correlation , denoted @xmath330 , is a function of @xmath331 and is defined as @xmath332\\end{aligned}\\ ] ] where @xmath333 $ ] is the expectation operator and the supremum is taken over all real - valued rvs @xmath334 and @xmath335 such that @xmath336=\\mathbb{e}[f_2(y)]=0 $ ] and @xmath337=\\mathbb{e}[f_2 ^ 2(y)]=1 $ ] .",
    "@xmath330 has the following geometric interpretation @xcite : if @xmath338 is a real separable hilbert space , then @xmath330 measures the cosine of the angle between the subspaces @xmath339=0,\\textbf { } \\mathbb{e}[f_1 ^ 2]<\\infty\\}$ ] and @xmath340=0,\\textbf { } \\mathbb{e}[f_2 ^ 2]<\\infty\\}$ ] .",
    "@xmath330 shares a number of interesting properties with @xmath341 , viz .",
    ", ( a ) nonnegativity : @xmath342 with @xmath343 iff @xmath344 , and @xmath345 iff @xmath346 , i.e. iff @xmath331 is decomposable @xcite , and ( b ) data processing : @xmath347 .    intuitively , for indecomposable distributions , if @xmath330 is near 1 , then @xmath203 have still lots in common .",
    "consider again the gk setup with node @xmath72 observing @xmath195 , node @xmath99 observing @xmath348 , where @xmath201 is generated by a 2-dms @xmath205 .",
    "now , a ( one - way ) rate - limited channel is made available from node @xmath99 to @xmath72 . then per @xcite , the maximum rate of cr extraction at rate @xmath248 ( denoted @xmath349 )",
    "is , @xmath350 we have @xmath351 by definition .",
    "hence , if @xmath352 , for indecomposable sources , not even a single bit in common can be extracted @xcite .",
    "but if @xmath353 , the first few bits of communication can `` unlock '' the common core of the 2-dms . assuming @xmath354 , the initial efficiency of cr extraction is given by @xcite @xmath355 where @xmath356",
    "alternatively , given a 2-dms @xmath205 , one can define the maximum amount of information that a rate @xmath248 description of source @xmath99 conveys about source @xmath72 , denoted @xmath357 , that admits the following single - letter characterization @xcite .",
    "@xmath358 where it suffices to restrict ourselves to @xmath359 with alphabet @xmath240 such that @xmath360 .",
    "the initial efficiency of information extraction from source @xmath99 is given by @xmath361 we have @xmath362 iff @xmath346 @xcite .",
    "interestingly , a dual of the optimization in ( 9 ) gives the well - known _ information bottleneck _ ( ib ) optimization @xcite that provides a tractable algorithm for approximating the minimal sufficient statistic of @xmath1 with respect to @xmath13 ( @xmath273 in lemma 3 ) .",
    "for some constant @xmath363 , the ib solves the nonconvex optimization problem , @xmath364 by alternating iterations amongst a set of convex distributions @xcite .",
    "since @xmath365 is neither concave nor convex in @xmath222 , computation of @xmath247 remains a difficult extremization problem in general , and simple solutions exist only for some special distributions @xcite .",
    "a symmetric measure of ci that combines features of both the gk and wyner measures can be defined by a rv @xmath222 as follows .",
    "@xmath366 where it suffices to minimize over all @xmath222 such that @xmath241 .",
    "observe that @xmath367 if @xmath218 is saturable .",
    "@xmath368 thus quantifies the minimum distance to saturability .",
    "however , @xmath368 is much harder to compute than the gk ci .",
    "more useful for our immediate purposes is the following asymmetric notion of ci for 3 rvs @xmath369 @xcite . @xmath370",
    "it is easy to see that @xmath371 retains an important monotonicity property of the original definition of gk ( see remark 4 ) in that @xmath371 is monotonically nonincreasing in the number of input @xmath372 s , i.e. , @xmath373 .    one can also define the following generalization of the wyner common entropy in ( 5 ) .",
    "@xmath374 it is easy to see that @xmath375 .",
    "@xmath376 is monotonically nondecreasing in the number of input @xmath372 s .",
    "any reasonable ci - based measure of redundancy in the pi decomposition framework must be nonincreasing in the number of predictors . in the next section ,",
    "we exclusively concentrate on @xmath371 .",
    "better understanding of @xmath371 will guide our investigation in section iii in search of an ideal measure of redundancy for pi decomposition .",
    "consider the following generalization of shannon s mi for three rvs @xmath369 , called _ co - information _ @xcite or _ interaction information _ ( with a change of sign ) @xcite .",
    "@xmath377 co - information is symmetric with respect to permutations of its input arguments and can be interpreted as the gain or loss in correlation between two rvs , when an additional rv is considered .",
    "the symmetry is evident from noting that @xmath378 . given a ground set @xmath10 of rvs , the shannon entropies form a boolean lattice consisting of all subsets of @xmath10 , ordered according to set inclusions @xcite .",
    "co - informations and entropies are mbius transform pairs with the co - informations also forming a lattice @xcite .",
    "co - information can however be negative when there is pairwise independence , as is exemplified by a simple two - input @xmath61 function , @xmath379 . bringing in additional side information @xmath1 induces artificial correlation between @xmath3 and @xmath4 when there was none to start with .",
    "intuitively , these artificial correlations are the source of synergy .",
    "indeed , co - information is widely used as a synergy - redundancy measure with positive values implying redundancy and negative values expressing synergy @xcite . however , as the following example shows , co - information confounds synergy and redundancy and is identically zero if the interactions induce synergy and redundancy in equal measure .",
    "let @xmath380 .",
    "we write @xmath381 .",
    "consider the following distribution : @xmath382 .",
    "first note that @xmath383 bits .",
    "the construction @xmath384 is such that one bit of information about @xmath1 is contained identically in both @xmath31 and @xmath22 . the other bit of information about @xmath1 is contained only in the joint rv @xmath385 .",
    "thus , @xmath386 contains equal amounts of synergistic and redundant information about @xmath1 . however , it is easy to check that @xmath387 .",
    "it is also less clear if the co - information retains its intuitive appeal for higher - order interactions ( @xmath388 predictor variables ) , when the same state of a target rv @xmath1 can have any combination of redundant , unique and ( or ) synergistic effects @xcite",
    ".    the partial information ( pi ) decomposition framework ( due to williams and beer @xcite ) offers a solution to disentangle the redundant , unique and synergistic contributions to the total mutual information that a set of @xmath6 predictor rvs convey about a target rv .",
    "consider the @xmath389 case .",
    "we use the following notation : @xmath390 and @xmath391 denote respectively , the unique information about @xmath1 that @xmath31 and @xmath22 exclusively convey ; @xmath392 is the redundant information about @xmath1 that @xmath31 and @xmath22 both convey ; @xmath393 is the synergistic information about @xmath1 that is conveyed only by the joint rv @xmath394 .",
    "the governing equations for the pi decomposition are given in ( 15 ) @xcite .",
    "@xmath395 using the chain rule of mi , ( 15a)-(15c ) implies @xmath396 from ( 15b)-(15e ) , one can easily see that the co - information is the difference between redundant and synergistic information . in particular , we have the following bounds .",
    "@xmath397 equivalently , @xmath398 when there is any pairwise independence , i.e. , when @xmath399 , or @xmath400 , or @xmath401 , and @xmath402 when @xmath369 form a markov chain in any order , i.e. , when @xmath403 , or @xmath404 or @xmath405 .",
    "the following lemma gives conditions under which @xmath406 achieves its bounds .",
    "1 .   if @xmath404 , then @xmath407 .",
    "2 .   if @xmath405 , then @xmath408 .",
    "3 .   if @xmath404 and @xmath405 , then @xmath409 @xmath410 .",
    "if @xmath403 , then @xmath411 .    the proofs follow directly from ( 15b)-(15e ) and the symmetry of co - information .    the following easy lemma gives the conditions under which the functions @xmath412 , @xmath413 and @xmath414 vanish .",
    "1 .   if @xmath400 or @xmath401 , then @xmath415 .",
    "also , @xmath416 .",
    "if @xmath404 , then @xmath417 .",
    "further , @xmath418 , @xmath419 , and @xmath420 .",
    "3 .   if the predictor variables are identical or if either @xmath404 or @xmath405 , then @xmath418 . also , if @xmath421 and @xmath422 , then @xmath418 and @xmath423 .",
    "the first part of a ) is immediate from ( 15b ) and ( 15c ) .",
    "the second part of a ) is a direct consequence of the asymmetry built in the pi decomposition by distinguishing the predictor rvs ( @xmath386 ) from the target rv ( @xmath1 ) .",
    "indeed , @xmath399 merely implies that @xmath424 ; the @xmath425 does not vanish in general .",
    "part b ) and c ) follow directly from ( 15b)-(15e ) .",
    "we visualize the pi decomposition of the total mutual information @xmath426 using a _ pi_-diagram @xcite . as detailed below , fig .",
    "1 shows the _",
    "pi_-diagrams for the `` ideal '' pi decomposition of several canonical functions , viz . , @xmath427 ( and its degenerate simplifications @xmath428 and @xmath429 ) , @xmath61 and @xmath430 @xcite .",
    "each irreducible pi atom in a _",
    "pi_-diagram represents information that is either unique , synergistic or redundant .",
    "ideally , one would like to further distinguish the redundancy induced by the function or mechanism itself ( called _ functional _ or _ mechanistic _",
    "redundancy ) from that which is already present between the predictors themselves ( called _ predictor _ redundancy ) .",
    "however , at present it is not clear how these contributions can be disentangled , except for the special case of independent predictor rvs when the entire redundancy can be attributed solely to the mechanism @xcite .    consider the @xmath427 function , @xmath431 , where @xmath1 consists of a perfect copy of @xmath31 and @xmath22 , i.e. , @xmath422 with @xmath421 .",
    "the @xmath427 function explicitly induces mechanistic redundancy and we expect that mi between the predictors completely captures this redundancy , i.e. , @xmath432 .",
    "indeed , lemma 5(c ) codifies this intuition .",
    "\\(a ) fig .",
    "1(a ) shows the ideal pi decomposition for the distribution @xmath433 with @xmath434 , where @xmath435 .",
    "we then have @xmath436 , @xmath418 and @xmath437 .",
    "\\(b ) fig .",
    "1(b ) shows the ideal pi decomposition for a simpler distribution @xmath433 with @xmath438 .",
    "now @xmath1 consists of a perfect copy of two i.i.d .",
    "clearly , @xmath439 . since @xmath418 ( vide lemma 5(c ) ) , only the unique contributions are nonzero , i.e. , @xmath440 .",
    "we call this the @xmath428 function .",
    "\\(c ) fig .",
    "1(c ) shows the ideal pi decomposition for the distribution @xmath433 with @xmath441 .",
    "this is an instance of a redundant @xmath427 mechanism with @xmath442 , where @xmath443 , so that @xmath444 .",
    "we then have @xmath445 .",
    "we call this the @xmath429 function .",
    "fig 1(d ) captures the pi decomposition of the following distribution : @xmath446 , where @xmath447 , @xmath448 .",
    "only the joint rv @xmath385 specifies information about @xmath1 , i.e. , @xmath449 whereas the singletons specify nothing , i.e. , @xmath450 , @xmath448 . neither the mechanism nor the predictors induce any redundancy since @xmath451 .",
    "@xmath61 is an instance of a purely synergistic function .",
    "1(e ) shows the ideal pi decomposition for the following distribution : @xmath452 , where the predictor inputs are @xmath453 and @xmath454 with @xmath455 i.i.d .",
    "the total mi of 4 bits is distributed equally between the four pi atoms .",
    "we call this the @xmath456 function since it is a composition of the functions @xmath429 , @xmath428 and @xmath61 .",
    "also see example 6 which gives an instance of composition of functions rdn and xor .     for some canonical examples . @xmath457 and @xmath458 denote , resp .",
    "unique information about @xmath1 , that @xmath31 and @xmath22 exclusively convey ; @xmath459 is the redundant information about @xmath1 that @xmath31 and @xmath22 both convey ; @xmath460 is the synergistic information about @xmath1 that can only be conveyed by the joint rv @xmath394 .",
    "( a ) copy ( b ) unq ( c ) rdn ( d ) xor ( e ) rdnunqxor ( f ) and ( see description in text),width=336 ]    fig 1(f ) shows the pi decomposition of the following distribution : @xmath461 , where @xmath447 , @xmath448 and @xmath433 is such that @xmath462 .",
    "the decomposition evinces both synergistic and redundant contributions to the total mi .",
    "the synergy can be explained as follows .",
    "first note that @xmath399 , but @xmath463 since @xmath464 . fixing",
    "the output @xmath1 induces correlations between the predictors @xmath31 and @xmath22 when there was none to start with .",
    "the induced correlations are the source of positive synergy .",
    "perhaps , more surprisingly , redundant information is not @xmath128 despite that @xmath399 .",
    "the redundancy can be explained by noting that if either predictor input @xmath465 or @xmath466 , then both @xmath31 and @xmath22 can exclude the possibility of @xmath467 .",
    "hence the latter is nontrivial information shared between @xmath31 and @xmath22 .",
    "this is clearer in light of the following argument that uses information structure aspects . given the support of @xmath433 , the set of possible states of nature include @xmath468 .",
    "@xmath31 generates the information partition @xmath469 .",
    "likewise , @xmath22 generates the partition , @xmath470 .",
    "let the true state of nature be @xmath471 .",
    "consider the event @xmath472 . both @xmath31 and @xmath22 know @xmath67 at @xmath471 , since @xmath473 and @xmath474 .",
    "the event that @xmath31 knows @xmath67 is @xmath475 .",
    "likewise , the event that @xmath22 knows @xmath67 is @xmath476 . clearly , the event @xmath477 is known to both @xmath31 and @xmath22 , so that @xmath467 can be ruled out with probability of agreement one .",
    "indeed , for independent @xmath31 and @xmath22 , when one can attribute the redundancy entirely to the mechanism , there is some consensus that @xmath478 and @xmath479 @xcite .",
    "independence of the predictor rvs implies a vanishing predictor redundancy but not necessarily a vanishing mechanistic redundancy ( also see second part of lemma 5(a ) ) .",
    "as one final illustrative application of this framework , we consider the decomposition of massey s directed information ( di ) @xcite into pi atoms .    for discrete - time stochastic processes @xmath480 and",
    "@xmath481 , the di from @xmath13 to @xmath1 is defined as follows .",
    "@xmath482 where @xmath483 denotes the past of @xmath13 relative to time @xmath83 .",
    "@xmath484 answers the following operational question : does consideration of the past of the process @xmath485 help in predicting the process @xmath486 better than when considering the past of @xmath486 alone ?",
    "di is a sum of conditional mutual information terms and admits an easy pi decomposition .",
    "@xmath487 where we have used ( 15d ) with @xmath488 , @xmath489 and @xmath490 .",
    "the decomposition has an intuitive appeal .",
    "conditioning on the past gets rid of the common histories or redundancies shared between @xmath491 and @xmath492 and adds in their synergy .",
    "thus , given the knowledge of the past @xmath492 , information gained from learning @xmath491 has a unique component from @xmath491 alone as well as a synergistic component that comes from the interaction of @xmath491 and @xmath492 .",
    "the colored areas in fig .",
    "2 shows this decomposition of the `` local '' di term @xmath493 into pi atoms , where @xmath494 .    , where @xmath495 , @xmath496 , @xmath497 , and @xmath498 ( see text),width=105 ]    from ( 15a)-(15c ) , it is easy to see that the three equations specifying @xmath499 and @xmath500 do not fully determine the four functions @xmath501 , @xmath390 , @xmath391 and @xmath393 . to specify a unique decomposition , one of the functions",
    "@xmath502 , @xmath414 or @xmath413 needs to be defined or a fourth equation relating @xmath406 , si , and ui .    pi decomposition researchers have focused on axiomatically deriving measures of redundant @xcite , @xcite , @xcite , synergistic @xcite and unique information @xcite , @xcite .",
    "for instance , for a general @xmath6 , any valid measure of redundancy @xmath503 must satisfy the following _ basic _ properties .",
    "let @xmath504 , where @xmath505 .",
    "* global positivity : @xmath506 . *",
    "symmetry : @xmath507 is invariant under reordering of the @xmath508 s . *",
    "self - redundancy : @xmath509 .",
    "for instance , for a a single predictor @xmath31 , the redundant information about the target @xmath1 must equal @xmath510 .",
    "* weak monotonicity : @xmath511 @xmath512 with equality if @xmath513 @xmath514 such that @xmath515 .",
    "* strong monotonicity : @xmath511 @xmath512 with equality if @xmath513 @xmath514 such that @xmath516 . for the equality condition for @xmath389 , also see lemma 4(a)-(c ) .",
    "* local positivity : for all @xmath6 , the derived pi measures are nonnegative .",
    "for instance for @xmath389 , a nonnegative pi measure for synergy requires that @xmath517 , where @xmath518 is the _ union information _ which is related to @xmath406 ( for any @xmath6 ) by the inclusion - exclusion principle @xcite . *",
    "identity : for @xmath389 , @xmath519 @xcite .",
    "the following properties capture the behavior of an ideal @xmath520 when one of the predictor or target arguments is enlarged .",
    "* target monotonicity : if @xmath521 , then @xmath522 @xmath523 .",
    "* predictor monotonicity : if @xmath524 , then @xmath525 .",
    "a similar set of monotonicity properties are desirable of an ideal @xmath413 .",
    "we consider only the @xmath389 case and write @xmath526 to explicitly specify the information about @xmath1 exclusively conveyed by @xmath31 .",
    "* target monotonicity : if @xmath521 , then @xmath527 .",
    "* predictor monotonicity : if @xmath528 , then @xmath529 . *",
    "predictor monotonicity with respect to the complement : if @xmath530 , then @xmath531 .",
    "properties * ( m ) * and * ( sm ) * ensure that any reasonable measure of redundancy is monotonically nonincreasing with the number of predictors . for a general @xmath6 ,",
    "given a measure of redundant information that satisfies * ( s ) * and * ( m ) * , only those subsets need to be considered which satisfy the ordering relation @xmath532 ( i.e. , the family of sets @xmath533 forms an antichain ) @xcite , @xcite .",
    "define a partial order @xmath534 on the set of antichains by the relation : ( @xmath535 ) @xmath534 ( @xmath533 ) iff for each @xmath536 @xmath513 @xmath537 such that @xmath538 .",
    "then , equipped with @xmath534 , the set of antichains form a lattice @xmath539 called the pi or the redundancy lattice @xcite . by virtue of * ( m ) * , for a fixed @xmath1 , @xmath507 is a monotone function with respect to @xmath534 .",
    "then , a unique decomposition of the total mutual information is accomplished by associating with each element of @xmath539 a pi measure @xmath540 which is the mbius transform of @xmath520 so that we have @xmath541 .",
    "for instance , for @xmath389 ( see ( 15a ) ) , the pi measures are @xmath542 , @xmath543 , @xmath544 , and @xmath545 .    while elegant in its formulation",
    ", the lattice construction does not by itself guarantee a _",
    "nonnegative _ decomposition of the total mutual information .",
    "the latter depends on the chosen measure of redundancy used to generate the pi decomposition . given a measure of redundant information ,",
    "some of the recurrent pathologies reported thus far include incompatibility of properties ( a ) * ( lp ) * and * ( tm ) * @xcite , @xcite , @xcite , @xcite , @xcite , ( b ) * ( lp ) * and * ( i d ) * for @xmath546 @xcite , and ( c ) * ( tm ) * and * ( i d ) * for @xmath389 , whenever there is mechanistic dependence between the target and the predictors @xcite . for a nonvanishing mechanistic dependency , * ( tm ) * and * ( i d ) *",
    "are incompatible since together they imply @xmath547 . for example , the desired decomposition of and in example 9 contradicts * ( tm)*. none of the measures of @xmath406 proposed thus far satisfies * ( tm)*. in the next section , we restrict ourselves to the bivariate case as some of the pathological features are already manifest .",
    "in this section , we dwell on the relationship between redundant information and the more familiar information - theoretic notions of common information .",
    "in particular , we seek to answer the following question : can optimization over a single rv yield a plausible measure of redundancy that satisfies @xmath548",
    "?    a simple measure of redundant information between predictors @xmath0 about a target rv @xmath1 is defined as follows @xcite .",
    "@xmath549 @xmath550 satisfies * ( gp ) * , * ( s ) * , * ( i ) * , * ( m ) * and * ( tm ) * but not * ( i d ) * @xcite .",
    "@xmath550 inherits the negative character of the original definition of gk and fails to capture any redundancy beyond a certain deterministic interdependence between the predictors .",
    "unless @xmath551 is decomposable , @xmath552 is trivially zero , even if it is the case that the predictors share nontrivial redundant information about the target @xmath1 .",
    "furthermore , @xmath550 violates @xmath548@xcite and is too restrictive in the sense that it does not capture the full informational overlap .    one can relax the constraint in ( 17 ) in a natural way by using the asymmetric notion of ci , @xmath553 introduced earlier in ( 12 ) .",
    "for consistency of naming convention , we call this @xmath554 .",
    "@xmath555 the definition has an intuitive appeal .",
    "if @xmath222 specifies the optimal redundant rv , then conditioning on any predictor @xmath372 should remove all the redundant information about @xmath1 , i.e. , @xmath556 , @xmath448 @xcite .",
    "@xmath557 remedies the degenerate nature of @xmath550 with respect to indecomposable distributions @xcite .",
    "it is also easy to see that the derived unique information measure , @xmath558 is nonnegative .",
    "@xmath559 @xmath558 readily satisfies the symmetry condition ( 15f ) since given @xmath222 such that @xmath560 and @xmath561 , we have @xmath562 , where ( a ) follows from @xmath563 and ( b ) follows from @xmath564 .    for the proofs of lemma 6 and 7 to follow , we shall use the standard facility of information diagrams ( @xmath565-diagrams ) @xcite . for finite rvs",
    ", there is a one - to - one correspondence between shannon s information measures and a signed measure @xmath566 over sets , called the @xmath565-measure .",
    "we denote the @xmath565-measure of rvs @xmath567 by @xmath566 . for a rv @xmath13",
    ", we overload notation by using @xmath13 to also label the corresponding set in the @xmath565-diagram .",
    "note that the @xmath565-diagrams in fig .",
    "3 are valid information diagrams since the sets @xmath568 intersect each other generically and the region representing the set @xmath222 splits each atom into two smaller ones .    if @xmath399 , then @xmath569 .    the atoms on which @xmath566 vanishes when the markov chains @xmath570 and @xmath571 hold and @xmath399 are shown in the generic @xmath565-diagram in fig .",
    "3(a ) ; @xmath572 which gives the result .    if @xmath403 , then @xmath573 .",
    "the atoms on which @xmath566 vanishes when the markov chains @xmath570 , @xmath571 and @xmath403 hold are shown in the @xmath565-diagram in fig .",
    "3(b ) . in general",
    ", for the atom @xmath574 , @xmath566 can be negative .",
    "however , since @xmath403 is a markov chain by assumption , we have @xmath575",
    ". then @xmath576 , which gives the desired claim .    by lemma 7",
    ", @xmath554 already violates the requirement posited in lemma 4(d ) for an ideal @xmath406 .",
    "it turns out that we can make a more precise statement under a stricter assumption , which also amounts to proving that @xmath554 violates * ( id)*.    -measure of rvs @xmath567 by @xmath566 , the atoms on which @xmath566 vanishes are marked by an asterisk ( see text),width=307 ]    [ fig_idiag_proofs ]    let @xmath421 and @xmath422 . then @xmath577 .",
    "first note that @xmath578 from lemma 2 we have that , given @xmath579 such that @xmath580 and @xmath581 , @xmath582 such that @xmath583 and @xmath584 .",
    "then @xmath585 .",
    "@xmath586 is the maximal common rv of @xmath31 and @xmath22 .",
    "thus , we have @xmath587 , with equality iff @xmath588 , or equivalently , iff @xmath394 is saturable ( see remark 3 ) .",
    "consider the gcs - krner version of @xmath554 : @xmath589 interestingly , @xmath590 satisfies * ( i d ) * in the sense that @xmath591 , or equivalently , @xmath592 .",
    "to show this , we again use lemma 2 .",
    "clearly the @xmath222 that achieves the maximum is the maximal common rv @xmath586 so that we have , @xmath593 .",
    "@xmath557 satisfies @xmath594@xmath595 @xmath596@xmath595 @xmath597@xmath595 @xmath598@xmath595 and @xmath599@xmath600 but not @xmath548@xmath600 and @xmath601 .",
    "@xmath594@xmath600 global positivity follows immediately from the nonnegativity of mutual information .",
    "@xmath596@xmath600 symmetry follows since @xmath557 is invariant under reordering of the @xmath508 s .",
    "@xmath597@xmath600 if @xmath570 , then @xmath602",
    ". then , self - redundancy follows from noting that @xmath603 .",
    "@xmath598@xmath600 we first show that @xmath604 .",
    "this follows immediately from noting that @xmath605 @xmath606 , since the constraint set for the @xmath607 is a subset of that for the @xmath425 and the objective function for the maximization is the same on both sides .    for the equality condition",
    ", we need to show that if @xmath608 , then @xmath609 .",
    "it suffices to show that if @xmath608 , then @xmath610 .",
    "this holds since if @xmath608 , then @xmath611 .",
    "@xmath599@xmath600 since * ( m ) * holds , it suffices to show the equality condition . for the latter ,",
    "we need to show that if @xmath612 or equivalently , if @xmath404 , then @xmath609 .",
    "this follows from noting that @xmath613 , where ( a ) follows from @xmath570 and ( b ) follows from @xmath404 .",
    "hence , we have @xmath614 , where ( c ) follows from * ( i)*.    @xmath548@xmath600 proof by counter - example : we show that if @xmath403 , then * ( lp ) * is violated",
    ". first note that if @xmath403 , then using the symmetry of co - information , the derived synergy measure is @xmath615 . from lemma 7 , it follows that @xmath616 so that @xmath617 .",
    "hence , there exists at least one distribution such that * ( lp ) * does not hold , which suffices to say that * ( lp ) * does not hold in general .",
    "indeed , the copy function in example 7 provides a direct counterexample , since @xmath618 and @xmath619 .",
    "not surprisingly , the derived synergy measure exactly matches the deficit in mechanistic redundancy that @xmath620 fails to capture .",
    "@xmath601@xmath600 by lemma 8 , @xmath554 violates * ( id)*.    @xmath557 satisfies @xmath621@xmath600 but not @xmath622 .    @xmath622@xmath600 we need to show that if @xmath521 , then , @xmath623 , or equivalently , @xmath624 .",
    "the latter does not hold since @xmath625 , @xmath448 , but the converse does not hold in general .",
    "hence , @xmath557 violates * ( tm)*.    @xmath621@xmath600 we need to show that if @xmath626 , then @xmath627 , or equivalently , @xmath628 .",
    "the latter holds since @xmath629 . since @xmath557 is symmetrical in the @xmath372 s , @xmath557 satisfies * ( pm)*.    @xmath558 satisfies @xmath630@xmath600 and",
    "@xmath631@xmath600 but not @xmath632 .    @xmath630@xmath600 we need to show that if @xmath521 , then @xmath633 , or equivalently , @xmath634 .",
    "the latter holds since @xmath625 , @xmath448 and @xmath635 .",
    "@xmath631@xmath600 we need to show that if @xmath636 , then @xmath637 , or equivalently , @xmath638 .",
    "the latter holds since @xmath639 .",
    "@xmath632@xmath600 we need to show that if @xmath626 , then @xmath640 , or equivalently , @xmath641 .",
    "the latter does not hold since @xmath629 , but the converse does not hold in general .      for the @xmath389 case , it is sufficient to specify any one of the functions @xmath642 , @xmath413 or @xmath414 to determine a unique decomposition of @xmath5 ( see ( 15a ) ) .",
    "information - geometric arguments have been forwarded in @xcite , @xcite to quantify redundancy .",
    "we do not repeat all the definitions in @xcite .",
    "however , for the sake of exposition , we prefer working with the unique information since geometrically , the latter shares some similarities with the mutual information which can be interpreted as a weighted distance .",
    "@xmath643    given a measurement of a predictor , say @xmath644 , unique information is defined in terms of the reverse information projection @xcite of @xmath645 on the convex closure of the set of all conditional distributions of @xmath1 for all possible outcomes of @xmath4 .",
    "@xmath646 where @xmath647 is the convex hull of @xmath648 , the family of all conditional distributions of @xmath1 given the different outcomes of @xmath22 .",
    "since the kl divergence is convex with respect to both its arguments , the minimization in ( 20 ) is well - defined .",
    "it is easy to see however , that @xmath649 violates the symmetry condition ( 15f ) unless the projection is guaranteed to be unique .",
    "uniqueness is guaranteed only when the set we are projecting onto is log - convex @xcite .",
    "in particular , ( 20 ) only gives a lower bound on the unique information so that we have , @xmath650 the symmetry is restored by considering the minimum of the projected information terms for the derived redundant information @xcite .",
    "\\tag{21}\\end{aligned}\\ ] ] @xmath652 satisfies * ( gp ) * , * ( s ) * , * ( i ) * , * ( m ) * , * ( lp ) * and * ( i d ) * but not * ( tm ) * @xcite .",
    "the following measure of unique information is proposed in @xcite .",
    "@xmath653 the derived redundant information is @xmath654 .",
    "@xmath655 satisfies * ( gp ) * , * ( s ) * , * ( i ) * , * ( m ) * , * ( lp ) * and * ( i d ) * but not * ( tm ) * @xcite .",
    "proposition 4 shows that both @xmath652 and @xmath655 satisfy * ( sm)*.    @xmath652 and @xmath655 satisfy @xmath599@xmath656    see lemma 13 and corollary 23 in @xcite .",
    "it is easy to show that @xmath657 violates * ( sm ) * ( see example imperfectrdn in @xcite ) .",
    "table 1 lists the desired properties satisfied by @xmath657 , @xmath658 , @xmath652 @xcite and @xmath655 @xcite .",
    "the following proposition from @xcite gives the conditions under which @xmath652 and @xmath655 vanish .",
    "if both @xmath403 and @xmath399 hold , then @xmath659 .",
    "see corollary 10 and lemma 21 in @xcite .    in general , the conditions for which an ideal @xmath406 vanishes are given in lemma 5(a ) .",
    "indeed , if both @xmath403 and @xmath399 hold , then from ( 15 g ) we have that @xmath660 , so that @xmath661 in general ( also see lemma 4(d ) ) .",
    "however , we have not been able to produce a counterexample to refute proposition 5 ( for an ideal @xmath406 ) .",
    "we conjecture that the conditions @xmath403 and @xmath399 are ideally sufficient for a vanishing @xmath406 .",
    "proposition 5 highlights a key difference between @xmath658 and the related measures @xmath652 and @xmath655 . by lemma 6 ,",
    "we have that @xmath658 vanishes if @xmath399 . clearly , unlike @xmath652 and @xmath655 , @xmath658 is not sensitive to the extra markov condition @xmath403 .",
    "this is most clearly evident for the and function in example 9 , where we have @xmath662 and @xmath663 .",
    "lemma 6 dictates that @xmath664 if @xmath399 and the ensuing decomposition is degenerate .",
    "thus , for independent predictor rvs , if @xmath1 is a function of @xmath31 and @xmath22 when any positive redundancy can be attributed solely to the _ common effect _",
    "@xmath1 , @xmath658 fails to capture the required decomposition ( see remark 5 ) . when the predictor rvs are not independent , a related degeneracy is associated with the violation of",
    "* ( i d ) * when @xmath658 fails to attain the mutual information between the predictor rvs ( see the copy function in example 7 ) . indeed , by lemma 8 ,",
    "@xmath423 iff @xmath665 is saturable . also by lemma 7 ,",
    "@xmath658 violates the requirement posited in lemma 4(d ) which generalizes the * ( i d ) * property .",
    "interestingly , lemma 6 also shows that any reasonable measure of redundant information can not be derived by optimization over a single rv .",
    ".desired properties of @xmath406 satisfied by the ci - based measures @xmath657 and @xmath658 , and the earlier measures @xmath652 @xcite and @xmath655 @xcite [ cols=\"<,<,<,<,<,<\",options=\"header \" , ]     we give one final example which elucidates the subtlety of the pi decomposition problem from a coding - theoretic point of view .",
    "consider the distribution in example 11 , where @xmath431 .",
    "the pi decomposition of @xmath5 in this case reduces to the decomposition of @xmath666 into redundant and unique information contributions .",
    "let @xmath667 and @xmath668 .",
    "let @xmath422 with @xmath669 .",
    "consider the distribution @xmath433 with @xmath670 , @xmath671 , @xmath672 , @xmath673 , @xmath674 , where @xmath320 and @xmath435 . if @xmath323 , as @xmath325 , we have the following ideal pi decomposition : @xmath675 , @xmath418 , @xmath676 and @xmath677 .",
    "consider again the source network with coded side information setup @xcite where predictors @xmath31 and @xmath22 are independently encoded and a joint decoder wishes to losslessly reconstruct only @xmath31 , using the coded @xmath22 as side information .",
    "it is tempting to assume that a complete description of @xmath31 is always possible by coding the side information at a rate @xmath678 and describing the remaining uncertainty about @xmath31 at rate @xmath679 .",
    "example 11 provides an interesting counterexample to this intuition .",
    "since the conditional distributions @xmath680 are different for all @xmath681 , we have @xmath682 ( see theorem 2 ) . consequently one needs to fully describe @xmath22 to ( losslessly ) recover @xmath31 , even if it is the case that @xmath683 is arbitrarily small .",
    "therefore , separating the redundant and unique information contributions from @xmath22 is not possible in this case .",
    "we first took a closer look at the varied relationships between two rvs . assuming information is embodied in @xmath8-algebras and sample space partitions , we formalized the notions of common and private information structures .",
    "we explored the subtleties involved in decomposing @xmath684 into common and private parts .",
    "the richness of the information decomposition problem is already manifest in this simple case in which common and private informational parts sometimes can not be isolated .",
    "we also inquired if a nonnegative pi decomposition of the total mutual information can be achieved using a measure of redundancy based on common information .",
    "we answered this question in the negative .",
    "in particular , we showed that for independent predictor rvs when any nonvanishing redundancy can be attributed solely to a mechanistic dependence between the target and the predictors , any common information based measure of redundancy can not induce a nonnegative pi decomposition .",
    "existing measures of synergistic @xcite and unique @xcite information use optimization over three auxiliary rvs to achieve a nonnegative decomposition .",
    "we leave as an open question if optimization over two auxiliary rvs can achieve a similar feat .",
    "also , at present it is not clear if the coding - theoretic interpretation leading up to the counterexample in example 11 calls into question the bivariate pi decomposition framework itself .",
    "more work is needed to assess its implications on the definitions of redundant and unique information .    in closing",
    ", we mention two other candidate decompositions of the total mutual information .",
    "proposed a decomposition of the total mutual information between the target and the predictors into terms that account for different coding modalities @xcite .",
    "some of the terms can , however , exceed the total mutual information @xcite .",
    "consequently , the decomposition is not nonnegative , thus severely limiting the operational interpretation of the different coding components .",
    "more recently , a decomposition of the total mutual information is proposed in @xcite based on a notion of synergistic information , @xmath685 , using maximum entropy projections on @xmath686-th order interaction spaces @xcite .",
    "the ensuing decomposition is , however , incompatible with * ( lp ) * @xcite . like @xmath687 , @xmath685 is symmetric with respect to permutations of the target and the predictor rvs which strongly hints that @xmath685 fails to capture any notion of mechanistic dependence . indeed , for the and example",
    ", @xmath685 computes to zero , and consequently * ( lp ) * is violated .",
    "in general , the quest for an operationally justified nonnegative decomposition of multivariate information remains an open problem .",
    "finally , given the subtle nature of the decomposition problem , intuition is not the best guide .",
    "thanks are due to aditya mahajan for short useful discussions over email .",
    "( see problem 16.25 , p. 392 in @xcite ; also see corollary 1 in @xcite ) .",
    "given @xmath688 such that @xmath689 and @xmath690 , it follows that @xmath691 @xmath692 .",
    "given an ergodic decomposition of @xmath331 such that @xmath693 , where the @xmath694@xmath286s and @xmath695@xmath286s having different subscripts are disjoint , define @xmath229 as @xmath696 @xmath697 @xmath698 .",
    "clearly @xmath230 . then , for any @xmath699 and for every @xmath700",
    ", @xmath701 is constant over @xmath702 which implies that @xmath703 .",
    "thus , for any @xmath704 for which @xmath705 , @xmath706 , so that @xmath231 .",
    "the converse is obvious .",
    "thus , given ( 2 ) , we get @xmath586 such that @xmath707 so that @xmath708 .      given @xmath709 , by lemma 2 , there exists a pmf @xmath710 such that @xmath230 and @xmath711 .",
    "clearly , @xmath712 since @xmath713 , where the last equality follows from @xmath711 .",
    "taking @xmath714 as @xmath715 , the claim follows .",
    "taking @xmath715 as @xmath222 , the other direction is obvious .",
    "let rv @xmath717 achieve the minimization in ( 4 ) .",
    "note the following chain of equivalences @xcite : @xmath718 @xmath719 , where ( a ) follows from @xmath720 the claim follows then from invoking lemma 2 and noting that @xmath721 , where @xmath722 is the maximal common rv .      let @xmath726 be a @xmath6-tuple of rvs ranging over finite sets @xmath727 where @xmath728 is an index set of size @xmath6 , and let @xmath729 be the set of all conditional pmfs @xmath730 s.t .",
    "@xmath731 . first note the following easy extensions .",
    "@xmath732 given @xmath733 such that ( a ) @xmath734 , we have @xmath735 , where ( b ) follows from using @xmath736 in ( a ) , and ( c ) follows from noting that @xmath737 .",
    "we then have @xmath738 where ( d ) follows since @xmath739 , @xmath740 implies @xmath741 . hence , @xmath742 . also note that for any @xmath743",
    ", @xmath744 , where ( e ) follows from ( a ) and ( f ) follows from invoking the data processing inequality after using ( a ) again , since for any @xmath745 , @xmath746 , with @xmath747 . hence @xmath724 .",
    "the claim for monotonicity of the wyner ci is immediate from noting that @xmath748 , since the constraint set for @xmath222 in the @xmath425 is a subset of that for the @xmath607 .",
    "further , for any @xmath743 , @xmath749 , whence @xmath750 follows .",
    "let @xmath751 . clearly , @xmath752 is a markov chain .",
    "@xmath753 is also a markov chain since given @xmath754 , @xmath755 @xmath756 , @xmath757 .",
    "now let @xmath758 so that @xmath759 .",
    "for some @xmath760 , let @xmath761 .",
    "then , @xmath762 @xmath763 .",
    "thus @xmath764 which implies @xmath765 .",
    "hence @xmath766 is a minimal sufficient statistic of @xmath1with respect to @xmath13 .    from ( 7 ) and lemma 3",
    ", it follows that @xmath273 is the minimizer in @xmath272 . since @xmath273 is a minimal sufficient statistic of @xmath1 with respect to @xmath13 , for any @xmath222 s.t . @xmath767 and",
    "@xmath689 , it follows that @xmath768 ( also see lemma 3.4(5 ) in @xcite ) .",
    "thus , @xmath273 achieves the maximum in @xmath300 .",
    "the decomposition @xmath769 easily follows . finally , @xmath770",
    "@xmath771 @xmath772 , because if @xmath689 then @xmath773 , so that @xmath774 .",
    "when @xmath218 lacks the structure to form components of size greater than one , it follows from lemma 3 that @xmath775 . consequently by theorem 1 , @xmath301 . for the other direction ,",
    "see corollary 3 in @xcite , where analogous bounds for @xmath272 are given .    for the second part , we prove the first equivalence .",
    "let @xmath776 . for @xmath777 if @xmath778 do not induce different conditional distributions on @xmath13 , i.e. , if @xmath779 , then we must have @xmath780 .",
    "this implies the existence of a function @xmath274 such that @xmath781 .",
    "if @xmath211 is saturable , we also have @xmath782 . from lemma 3 and remark 3",
    ", it then follows that @xmath783 and consequently @xmath302 . for the converse , first note that @xmath784 ( see remark 3 and ( 8d ) ) . demanding @xmath302 or equivalently @xmath785 implies @xmath786 when from remark 3 it follows that @xmath211 is saturable . for the second equivalence ,",
    "see theorem 4 in @xcite .",
    "this concludes the proof .        _ synergistic and redundant information_. synergistic interactions in the brain",
    "are observed at different levels of description . at the level of brain regions ,",
    "cross - modal illusions offer a powerful window into how the brain integrates information streams emanating from multiple sensory modalities @xcite .",
    "a classic example of synergistic interaction between the visual and auditory channels is the mcgurk illusion @xcite .",
    "conflicting voice and lip - movement cues can produce a percept that differs in both magnitude and quality from the sum of the two converging stimuli . at the single neuronal level , temporally and spatially",
    "coincident multimodal cues can increase the firing rate of individual multisensory neurons of the superior colliculus beyond that can be predicted by summing the unimodal responses @xcite . in the context of neural coding , a pair of spikes closely spaced in time can jointly convey more than twice the information carried by a single spike @xcite . in cortex studies , evidence of weak synergy",
    "have been been found in the somatosensory @xcite and motor @xcite and primary visual cortex @xcite .",
    "similarly , there are several studies evidencing net redundancy at the neuronal population level @xcite .",
    "often studies on the same model system have reached somewhat disparate conclusions .",
    "for instance , retinal population codes have been found to be approximately independent @xcite , synergistic @xcite , or redundant @xcite .    _",
    "unique information_. a wealth of evidence suggests that attributes such as color , motion and depth are encoded uniquely in perceptually separable channels in the primate visual system @xcite , @xcite . the failure to perceive apparent motion with isoluminant colored stimuli , dubbed as the color - motion illusion @xcite demonstrates that the color and motion pathways provide unique information with respect to each other . there is also mounting evidence in favor of two separate visual subsystems @xcite that encode the allocentric ( vision for perception ) and egocentric ( vision for action ) coordinates uniquely along the ventral and the dorsal pathways , respectively , for object identification and sensorimotor transformations",
    ".    in embodied approaches to cognition , an agent s physical interactions with the environment generates structured information and redundancies across multiple sensory modalities that facilitates cross - modal associations , learning and exploratory behavior @xcite .",
    "more recent work has focused on information decomposition in the sensorimotor loop to quantify morphological computation which is the contribution of an agent s morphology and environment to its behavior @xcite .",
    "some related decompositions have also focused on extracting system - environment boundaries supporting biological autonomy @xcite .",
    "further motivating examples for studying information decomposition in general abound in cryptography @xcite , distributed control @xcite and adversarial settings like game theory @xcite , where notions of common knowledge shared between agents are used to describe epistemic states .",
    "99 o. sporns , _ networks of the brain_. mit press , 2011 . p. l. williams and r. d. beer , `` nonnegative decomposition of multivariate information , '' _",
    "arxiv:1004.2515 _ , 2010 .",
    "n. bertschinger , j. rauh , e. olbrich , and j. jost , `` shared information  new insights and problems in decomposing information in complex systems , '' in _ proc .",
    "european conf . on complex systems 2012 ( eccs12 ) _ , 2012 , pp .",
    "251 - 269 . v. griffith and c. koch , `` quantifying synergistic mutual information , '' in _ guided self - organization : inception _ , springer , 2014 , vol .",
    "9 , emergence , complexity and computation series , pp .",
    "159 - 190 . v. griffith , e. k. chong , r. g. james , c. j. ellison , and j. p. crutchfield , `` intersection information based on common randomness , '' _ entropy _ , vol .",
    "4 , pp . 1985 - 2000 , 2014 .",
    "v. griffith , t. ho , `` quantifying redundant information in predicting a target random variable , '' _ entropy _ , vol .",
    "17 , no . 7 , pp . 4644 - 4653 , 2015 .",
    "m. harder , c. salge , and d. polani , `` bivariate measure of redundant information , '' _ physical review e _",
    "1 , p. 012130",
    "n. bertschinger , j. rauh , e. olbrich , j. jost , and n. ay , `` quantifying unique information , '' _ entropy _ , vol .",
    "4 , pp . 2161 - 2183 , 2014 .",
    "j. rauh , n. bertschinger , e. olbrich , and j. jost , `` reconsidering unique information : towards a multivariate information decomposition , '' in _ proc .",
    "inf . theory ( isit 2014 ) _ , 2014 ,",
    "2232 - 2236 .",
    "a. b. barrett , `` exploration of synergistic and redundant information sharing in static and dynamical gaussian systems , '' _ physical review e _ ,",
    "91 , no . 5 , p. 052802",
    "e. olbrich , n. bertschinger , and j. rauh , `` information decomposition and synergy , '' _ entropy _ , vol .",
    "5 , pp . 3501 - 3517 , 2015 .",
    "e. schneidman , s. still , m. j. berry and w. bialek , `` network information and connected correlations , '' _ phys",
    "23 , p. 238701",
    "t. kahle , e. olbrich , j. jost , n. ay , `` complexity measures from interaction structures , '' _ phys .",
    "2 , p. 026201",
    "j. massey , `` causality , feedback and directed information , '' in _ proc .",
    "theory applic .",
    "( isita90 ) _ , 1990 , pp .",
    "303 - 305 .",
    "s. wolf and j. wultschleger , `` zero - error information and applications in cryptography , '' in _ proc .",
    "2004 ieee inf .",
    "theory workshop _ ,",
    ". v. anantharam and v. borkar , `` common randomness and distributed control : a counterexample , '' _ systems & control letters _",
    "568 - 572 , 2007 .",
    "r. j. aumann , `` agreeing to disagree , '' _ the annals of statistics _ , pp .",
    "1236 - 1239 , 1976 .",
    "p. gcs and j. krner , `` common information is far less than mutual information , ''",
    "_ problems of control and information theory _",
    "149 - 162 , 1973 .",
    "a. d. wyner , `` the common information of two dependent random variables , '' _ ieee transactions on information theory _",
    "2 , pp . 163 - 179 , 1975 .",
    "d. slepian and j. wolf , `` noiseless coding of correlated information sources , '' _ ieee transactions on information theory _ , vol .",
    "471 - 480 , 1973 .",
    "g. r. kumar , c. t. li and a. e. gamal , `` exact common information , '' in _ proc .",
    "ieee isit _ , 2014 ,",
    "161 - 165 .",
    "r. m. gray and a. d. wyner , `` source coding for a simple network , '' _ bell system tech .",
    "1681 - 1721 , 1974 .",
    "r. ahlswede and j. krner , `` on common information and related characteristics of correlated information sources , '' in preprint .",
    "presented at the _",
    "7th prague conference on information theory _ , 1974 .",
    "i. csiszr and j. krner , _ information theory : coding theorems for discrete memoryless systems_. cambridge university press , 2011 .",
    "r. w. yeung , `` a new outlook on shannon s information measures , '' _ ieee transactions on information theory _ , vol .",
    "466 - 474 , 1991 . c. e. shannon , `` the lattice theory of information , '' _ transactions of the ire professional group on information theory _",
    ", vol . 1 , no",
    ". 1 , pp . 105 - 107 , 1953 .",
    "h. li and e. k. chong , `` information lattices and subgroup lattices : isomorphisms and approximations , '' in _ proc .",
    "45th allerton conf . on comm . , control and comput .",
    "_ , 2007 , pp .",
    "h. witsenhausen , `` the zero - error side information problem and chromatic numbers ( corresp . ) , '' _ ieee transactions on information theory _ , vol .",
    "592 - 593 , 1976 .",
    "g. hexner and y. c. ho , `` information structure : common and private ( corresp . ) , '' _ ieee transactions on information theory _ , vol .",
    "390 - 393 , 1977 .",
    "y. c. ho and c. martin , `` private information structures of a with respect to b , '' _ international journal of control _ , vol . 26 , no . 6 , pp . 973 - 980 , 1977 .",
    "i. csiszr , f. matus , `` information projections revisited , '' _ ieee transactions on information theory _ , vol .",
    "49 , no . 6 , pp . 1474 - 1490 , 2003",
    ". h. s. witsenhausen , `` values and bounds for the common information of two discrete random variables , '' _ siam journal on applied mathematics _ ,",
    "313 - 333 , 1976 .",
    "h. s. witsenhausen , `` on sequences of pairs of dependent random variables , '' _ siam journal on applied mathematics _ ,",
    "100 - 113 , 1975 .",
    "a. rnyi , `` on measures of dependence , '' _ acta mathematica academiae scientiarum hungarica _ ,",
    "441 - 451 , 1959 .",
    "s. kamath and v. anantharam , `` non - interactive simulation of joint distributions : the hirschfeld - gebelein - rnyi maximal correlation and the hypercontractivity ribbon , '' in _ proc .",
    "50th allerton conf . on comm . , control and comput . _ , 2012 , pp .",
    "1057 - 64 .",
    "r. ahlswede and i. csiszr , `` common randomness in information theory and cryptography .",
    "part ii : cr capacity , '' _ ieee transactions on information theory _ ,",
    "225 - 240 , 1998 .",
    "v. anantharam , a. gohari , s. kamath and c. nair ,  on maximal correlation , hypercontractivity , and the data processing inequality studied by erkip and cover , \" _ arxiv:1304.6133 _ , 2013 .",
    "n. tishby , f. pereira , and w. bialek , `` the information bottleneck method , '' in _ proc .",
    "37th allerton conf . on comm . , control and comput .",
    "_ , 1999 , pp .",
    "368 - 337 .",
    "d. marco and m. effros , `` on lossless coding with coded side information , '' _ ieee transactions on information theory _ , vol .",
    "55 , no . 7 , pp . 3284 - 3296 , 2009",
    ". s. kamath and v. anantharam , `` a new dual to the gcs - krner common information defined via the gray - wyner system , '' in _ proc .",
    "48th allerton conf . on comm . ,",
    "control and comput .",
    "_ , 2010 , pp",
    ". 1340 - 1346 .",
    "g. grtzer , k. m. koh , and m. makkai , `` on the lattice of subalgebras of a boolean algebra , '' _ proceedings of the american mathematical society _",
    "87 - 92 , 1972 .",
    "s. fujishige , `` polymatroidal dependence structure of a set of random variables , '' _ information and control _ , vol .",
    "55 - 72 , 1978 . a. j. bell , `` the co - information lattice , '' in _ proc .",
    "workshop on independent component analysis and blind signal separation _ , 2003 ,",
    "921 - 926 .",
    "w. j. mcgill , `` multivariate information transmission , '' _ psychometrika _ , vol .",
    "97 - 116 , 1954 .",
    "o. kallenberg , _ foundations of modern probability_. chicago : springer science & business media , 2006 . g. n. nair , `` a nonstochastic information theory for communication and state estimation , '' _ ieee transactions on automatic control _ , vol .",
    "58 , no . 6 , pp . 1497 - 1510 , 2013 .",
    "l. t. nielsen , `` common knowledge , communication , and convergence of beliefs , '' _ mathematical social sciences _ , vol .",
    "1 , pp . 1 - 14 , 1984 .",
    "p. billingsley , _ probability and measure_. john wiley & sons , 2008 .",
    "n. brenner , s. p. strong , r. koberle , w. bialek , and r. r. de r. van steveninck , `` synergy in a neural code , '' _ neural computation _ , vol .",
    "12 , no . 7 , pp . 1531 - 1552 , jul .",
    "e. schneidman , w. bialek , and m. j. berry , `` synergy , redundancy , and independence in population codes , '' _ the journal of neuroscience _ , vol .",
    "11539 - 11553 , 2003 .",
    "p. e. latham and s. nirenberg , `` synergy , redundancy , and independence in population codes , revisited , '' _ the journal of neuroscience _",
    "21 , pp . 5195 - 5206 , 2005 .",
    "b. b. averbeck , p. e. latham and a. pouget , `` neural correlations , population coding and computation , '' nature reviews neuroscience , vol . 7 , no .",
    "358 - 366 , 2006 .",
    "i. kontoyiannis , b. lucena , `` mutual information , synergy and some curious phenomena for simple channels , '' in _ proc .",
    "inf . theory ( isit 2005 ) _ , pp .",
    "1651 - 1655 .",
    "d. anastassiou , `` computational analysis of the synergy among multiple interacting genes , '' _ molecular systems biology _ , vol .",
    "3 , no . 1 , 2007 .",
    "n. s. narayanan , e. y. kimchi , m. laubach , `` redundancy and synergy of neuronal ensembles in motor cortex , '' _ j. neurosci .",
    "_ , vol . 25 , no . 17 , pp .",
    "4207 - 4216 , 2005 .",
    "g. chechik , m. j. anderson , o. bar - yosef , e. d. young , n. tishby and i. nelken , `` reduction of information redundancy in the ascending auditory pathway , '' _ neuron _ , vol .",
    "359 - 368 , 2006 .",
    "i. gat and n. tishby , `` synergy and redundancy among brain cells of behaving monkeys , '' in _ advances in neural information processing systems 11 _ , m. j. kearns , s. a. solla , and d. a. cohn , eds .",
    "mit press , pp .",
    "111 - 117 , 1999 .",
    "g. pola , a. thiele , k. p. hoffmann and s. panzeri , `` an exact method to quantify the information transmitted by different mechanisms of correlational coding , '' _ network _ , vol .",
    "35 - 60 , 2003 .",
    "d. s. reich , f. mechler and j. d. victor , `` independent and redundant information in nearby cortical neurons , '' _ science _ ,",
    "5551 , pp .",
    "2566 - 2568 , 2001 .",
    "s. nirenberg , s. m. carcieri , a. l. jacobs and p. e. latham , `` retinal ganglion cells act largely as independent encoders , '' _ nature _ , vol.411 , no . 6838 , pp .",
    "698 - 701 , 2001 .",
    "m. meister , `` multineuronal codes in retinal signaling , '' _ proceedings of the national academy of sciences _ , vol .",
    "609 - 614 , 1996 .",
    "j. l. puchalla , e. schneidman , r. a. harris and m. j. berry , `` redundancy in the population code of the retina , '' _ neuron _ , vol .",
    "46 , no . 3 , 493 - 504 , 2005 .",
    "d. m. eagleman , `` visual illusions and neurobiology , '' _ nature reviews neuroscience _ ,",
    "920 - 926 , 2001 .",
    "h. mcgurk and j. macdonald , `` hearing lips and seeing voices , '' _ nature _ , vol .",
    "746 - 748 , 1976 .",
    "m. a. meredith , j. w. nemitz and b. e. stein , `` determinants of multisensory integration in superior colliculus neurons . i. temporal factors , '' _ the journal of neuroscience _ , vol . 7 , pp .",
    "3215 - 3229 , 1987 . v. s. ramachandran and r. l. gregory , `` does colour provide an input to human motion perception ?",
    ", '' _ nature _ , vol .",
    "55 - 56 , 1978 .",
    "a. d. milner and m. a. goodale , `` two visual systems re - viewed , '' _ neuropsychologia _ , vol .",
    "774 - 785 , 2008 .",
    "m. lungarella and o. sporns , `` mapping information flow in sensorimotor networks , '' _ plos computational biology _ ,",
    "2 , no . 10 , p. e144",
    "k. ghazi - zahedi and j. rauh , `` quantifying morphological computation based on an information decomposition of the sensorimotor loop , '' in _ proc .",
    "13th european conf . on artificial life ( ecal )",
    "_ , 2015 , pp .",
    "n. bertschinger , e. olbrich , n. ay , j. jost , `` autonomy : an information theoretic perspective , '' _",
    "biosystems _ , vol .",
    "331 - 345 , 2008 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of decomposing the total mutual information conveyed by a pair of predictor random variables about a target random variable into redundant , unique and synergistic contributions . </S>",
    "<S> we focus on the relationship between `` redundant information '' and the more familiar information - theoretic notions of `` common information . '' </S>",
    "<S> our main contribution is an impossibility result . </S>",
    "<S> we show that for independent predictor random variables , any common information based measure of redundancy can not induce a nonnegative decomposition of the total mutual information . </S>",
    "<S> interestingly , this entails that any reasonable measure of redundant information can not be derived by optimization over a single random variable .    </S>",
    "<S> common and private information , synergy , redundancy , information lattice , sufficient statistic , partial information decomposition </S>"
  ]
}