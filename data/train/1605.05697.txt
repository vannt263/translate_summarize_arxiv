{
  "article_text": [
    "regression models are one of the main tools of statistical modeling and supervised machine learning . in regression models ,",
    "_ responses _ are related to _ predictors _ by a probabilistic model that is a function of model parameters that we want to estimate from the data . the most common regression model , linear regression , assumes the response follows a gaussian distribution that can take values anywhere in the real numbers",
    "however , different applications have a response of a different nature . for example",
    ", the response might take values only in the positive real numbers ( e.g. , the time between the arrivals of two buses ) , or in the non - negative integers ( e.g. , the number of thunders in a day , or the number and identity of the items chosen by someone from a catalog of items ) . in such cases , assuming that the response is gaussian might produce an inferior model to one obtained assuming a different distribution that better matches the nature of the response data . when the response takes on two values , it may be modeled as a bernoulli random variable . when it is a non - negative real number , it may be modeled as an exponential , erlang , or gamma distribution .",
    "when the response is a non - negative integer it may be modeled as a poisson , negative binomial , or binomial random variable .",
    "when the response is a category it may be modeled by a multinomial or categorical distribution .",
    "all these distributions , and more , are part of the so - called exponential family ( @xcite , @xcite , @xcite ) .",
    "the exponential family also includes distributions for random vectors with vector entries that are correlated , e.g. , as in the multivariate gaussian distribution , as well as with independent entries with different distribution types .",
    "the generalized linear model ( glm ) introduced in @xcite provides the theory to build regression models with static parameters where the response follows a distribution in the exponential family .",
    "the glm can then be seen as a generalization of linear regression .",
    "many applications of regression models , indeed those that rely on the glm , assume the parameters are static .",
    "this assumption is too restrictive when modeling time - series , which often exhibit trends , seasonal components , and local correlations .",
    "similarly , in many applications the model parameters represent variables that describe a portion of the state of the underlying system that can not be or are not directly measured , but with well - known relationships that describe their dynamics .",
    "the response can then be thought of as a noisy function of the system state and the predictors , and the goal of the regression model is to estimate the system state over time based on the observation time - series .",
    "situations where an underlying dynamical system is only observed through noisy measurements are often encountered in engineering applications , and the celebrated and widely used kalman filter , introduced in @xcite solves the corresponding regression model when the parameter dynamics ( linear with additive gaussian noise ) and the observation distribution ( gaussian with a mean that is a linear function of the state ) are simple enough .",
    "the kalman filter can be seen as an algorithm that efficiently estimates the mean and covariance matrix of the parameters in a linear regression model , where the parameters evolve in time through linear dynamics , based on the time - series of responses and predictors .",
    "furthermore , the kalman filter is an online algorithm that updates the parameter estimates in a relatively simple fashion when new observations arrive , without having to re - analyze all previous data .",
    "in addition , it is a second - order algorithm , in contrast to stochastic gradient descent ( e.g. , @xcite ) .",
    "so while the kalman filter takes more computation per observation than stochastic gradient descent , it can converge to an accurate estimate of the parameters with fewer observations , while allowing for much more flexibility in the modeling of the underlying parameter dynamics .",
    "lastly but importantly , because the kalman filter provides an estimate of the mean and covariance matrix , unlike other approaches that only focus on the mean , its results can be used to construct confidence intervals or samples of the parameters or of the model predictions  these statistics are often necessary in several applications such as in contextual multi - armed bandits ( e.g. , @xcite ) .",
    "many generalizations to the kalman filter have now been developed , e.g. , to allow for non - linear state dynamics state dynamics , or for observations that are noisy and non - linear and/or non - gaussian functions of the state as in the extended kalman filter , e.g. , see @xcite for a good overview .",
    "more recently , there has been interest in merging the ideas from kalman filtering , namely modeling dynamics in the parameters of a regression model , with the flexibility to model observations from the wide class of probability distributions that the glm allows through its treatment of the exponential family ( @xcite , @xcite , @xcite , @xcite , @xcite , @xcite ) .",
    "it turns out that approximate algorithms that are very similar to the extended kalman filter can be derived for a wide range of choices for the observation distributions . introducing and describing some of these algorithms for a fairly general class of models , including the multivariate glm , is the main focus of this paper .",
    "the derivation we follow is novel and simpler ( e.g. , it does not invoke any kalman filter theory nor uses conjugacy ) . and",
    "the form of the exponential family we use is slightly more general than that used in other references on these methods , because the nuissance parameter matrix @xmath0 included in our model is absent in other references that deal with multivariate responses .",
    "the second focus of this paper is to propose the application of these methods to the contextual multi - armed bandit problem ( @xcite , @xcite ) , and show the resulting performance through simulations .",
    "this application is novel to the best of our knowledge .",
    "it broadens the class of models that can be used to describe the rewards , separates the concept of a response and a reward , and allows for the explicit modeling of dynamics in the parameters that map the context to the rewards .",
    "section [ sec : setup ] introduces the class of models we study , and reviews the exponential family and the generalized linear model .",
    "section [ sec : algo ] describes the general online algorithm to estimate the mean and variance of the parameters , and specializes it to the multivariate exponential family , the univariate exponential family , and to several examples of commonly used distributions .",
    "we sketch the derivation of the algorithm in section [ sec : derivation ] .",
    "we apply the methods developed to the contextual multi - armed bandit problem in sections [ contextualbandits ] , and conclude in section [ conclusion ] .",
    "we assume all vector and matrix entries are real numbers .",
    "we denote vectors using boldface small caps , and assume them to be column vectors .",
    "we use small caps for scalars , and boldface large caps for matrices . if @xmath1 is a matrix , we denote its inverse by @xmath2 , and its transpose by @xmath3 .",
    "we assume we have received @xmath4 pairs of observations @xmath5 , for @xmath6 where @xmath7 is the @xmath8-th _ response _ that we want to explain based on the @xmath8-th _ predictor _ @xmath9 , for some positive integers @xmath10 , @xmath11 and @xmath12 .",
    "we denote by @xmath13 the information or history after @xmath4 observations , i.e. , @xmath13 is the set that includes the first @xmath4 responses and predictors .    we postulate a model that relates the response to the predictor via a probability distribution @xmath14 where @xmath15 are the model parameters at the @xmath8-th observation , with one parameter for each row in the predictor matrix @xmath16 @xmath17 is a @xmath10-by-@xmath10 matrix and nuisance parameter that is assumed known and that we will often omit . as we will see later , the nuisance parameter plays the role of the covariance of the observations in linear regression , and is the identity matrix in many other cases of interest .",
    "we consider regression models where the probability of the response depends on the predictors and the parameters only through the @xmath12-by-@xmath18 _ signal _ @xmath19 namely , models where @xmath20    rather than working with @xmath21 , we will typically work with its logarithm , denoted by @xmath22 which we assume to be a well - behaved function , particularly that the first and second derivatives with respect to @xmath23 exist and are finite .",
    "many of the commonly used regression models fit the model above , including univariate and multivariate linear regression with response @xmath24 logistic or binomial , categorical or multinomial , exponential , poisson , negative binomial , gamma , etc .",
    "our model also includes cases where the different entries in the response vector are conditionally independent given the signal , and follow any distribution that is a function of the signal or a subset of the signal entries .",
    "for example , we can have as many predictor vectors as entries in @xmath25 , i.e. , @xmath26 and have the @xmath27-the response entry depend only on the @xmath27-th signal entry so that @xmath28 , with @xmath29 being a different function for different response entries @xmath27 . because all the entries still depend on the parameters @xmath30 this setup allows for combining the information from different types of measurements that depend on the parameters to obtain more accurate parameter estimates .",
    "also , the predictor matrix @xmath31 may have a lot of structure , e.g. , to allow for some parameters to be shared across entries in the response vector , and others that are specific to subsets of the response .",
    "here we drop the time subscript of our vectors and matrices to avoid notational clutter , so , e.g. , @xmath32 becomes simply @xmath25 .",
    "all of the probability distributions of interest to model the response @xmath25 mentioned above , and more , can be re - arranged so that their log - likelihood has the following so - called _ natural exponential form _",
    "@xmath33 here , @xmath34 is a @xmath10-by-1 vector referred to as the natural parameter , with @xmath35 in its @xmath27-th entry .",
    "it is a function of the signal @xmath36 in our models  this will be made specific in the next section .",
    "crucially , the function @xmath37 is independent of @xmath25 and the function @xmath38 is independent of @xmath39 we assume that @xmath37 is twice differentiable with respect to its argument .",
    "the @xmath10-by-@xmath10 nuisance parameter matrix @xmath0 is assumed symmetric and known .",
    "when @xmath0 is unknown , it can be estimated through several methods that are not online , e.g. , see chapter 13 in @xcite .",
    "it can be shown , e.g. , see appendix [ sec : meanvarnef ] , that the mean and covariance matrix of @xmath25 are given by @xmath40 =   \\mathbf{\\phi } \\frac{\\partial b}{\\partial \\mathbf{\\eta } }    \\label{eq : nefmean } \\\\    \\mathbf{\\sigma_y(\\eta)}=&e[(\\mathbf{y}- \\mathbf{\\mu(\\eta)})(\\mathbf{y}- \\mathbf{\\mu(\\eta ) } ) ' ] =   \\mathbf{\\phi } \\frac{\\partial^2 b}{\\partial \\mathbf{\\eta}^2 } \\mathbf{\\phi } , \\label{eq : nefvar}\\end{aligned}\\ ] ] where @xmath41 is a column vector with @xmath42 in its @xmath27-th entry , and @xmath43 is the @xmath10-by-@xmath10 matrix with @xmath44 in its @xmath8-th row and @xmath27-th column .",
    "most of the frequently encountered univariate and multivariate distributions have the exponential form above .",
    "in addition , a union of independent random vectors that are in the natural exponential family is also in the natural exponential family .",
    "e.g. , a random vector with entries distributed according to different members of the exponential family is still in the natural exponential family with a natural parameter given by the union of the natural parameters of its entries .",
    "this will allow us to estimate shared parameters in a regression model from multiple time series of a potentially different nature .",
    "we consider models where the response is distributed according to equation [ eq : nef ] , which is a function of @xmath34 .",
    "we use the glm to relate @xmath34 to the signal @xmath45      in the glm , introduced in @xcite , we assume that the signal @xmath36 is a function of the mean @xmath46 of the observation @xmath25 , in addition to assuming equation [ eq : nef ] for the observation @xmath25 .",
    "the glm then assumes that there is a known one - to - one mapping between the natural parameter @xmath34 in equation [ eq : nef ] and the signal @xmath47 so we can view the likelihood or any statistic of @xmath25 either as a function of @xmath34 or of @xmath36 .",
    "specifically , we have @xmath48 for known functions @xmath49 @xmath50 and @xmath51 , and where the so - called _ link function _",
    "@xmath51 maps the mean @xmath46 of @xmath25 to the signal .",
    "the mean @xmath46 and covariance @xmath52 in equations [ eq : nefmean ] and [ eq : nefvar ] can then be seen to be either a function of the natural parameter @xmath34 or of the signal @xmath53 e.g. , @xmath54 here the function @xmath55 is the inverse of @xmath51 .",
    "we refer to @xmath56 as the _ response function _ ; it maps the signal to the mean of the response and plays a prominent role in our algorithms .",
    "any invertible function @xmath51 can be used as the link function in a glm , but one that often makes sense , and where the mathematics to learn the model simplifies , is the _",
    "canonical link _ that results in the signal being equal to the natural parameter , i.e. , in @xmath57    previous treatments of the glm in the context of dynamic parameters consider either a univariate response with the univariate case of equation [ eq : nef ] ( e.g. , see @xcite ) , or a multivariate response with equation [ eq : nef ] but with the nuissance parameter matrix @xmath0 equal to the identity ( e.g. , see chapter 2 in @xcite ) . in this sense",
    "our treatment is a slight generalization .",
    "we assume that the parameters evolve according to @xmath58 where @xmath59 is a zero - mean random vector with known covariance matrix @xmath60 we also assume that the _ noise _ @xmath61 is uncorrelated with itself over time , and uncorrelated with the observation parameters .",
    "the known vector @xmath62 drives the parameters through the appropriately sized and known matrix @xmath63 and @xmath64 is a known @xmath11-by-@xmath11 square matrix .",
    "we also assume that at time zero the mean and variance of @xmath65 are known , i.e. , that @xmath66 the general setup above includes several special cases of interest , described next .",
    "when @xmath67 ( the identity matrix ) , @xmath68 , and @xmath69 we end up with the simple parameter dynamics @xmath70 which is the standard regression problem with static parameters .",
    "this becomes the glm when we also assume that the response is in the exponential family , with natural parameter that is a function of the signal . in this sense ,",
    "our model is a generalization of the glm where the parameters are allowed to vary over time .",
    "when @xmath67 and @xmath68 , we end up with the simple parameter dynamics @xmath71 .",
    "this allows the parameters to drift or diffuse over time in an unbiased ( zero - mean ) way , according to the noise @xmath61 .",
    "this model is appealing for a range of applications , e.g. , it could model the conversion rate of visitors to the netflix signup page as a function of their browser , country , day of week , time of day , etc .",
    "equation [ eq : pardynamics ] is central to the study of linear dynamical systems , control theory , and other related areas . specifically , it is core to the kalman filter , which also assumes a dynamical system that evolves according to equation [ eq : pardynamics ] , but with a response that is a linear function of the parameters ( the state in kalman filter parlance ) and additive gaussian noise .",
    "the kalman filter is an online algorithm that estimates the mean and variance of the system state from the noisy observations .",
    "so our setup is very related .",
    "the main difference is that we do not restrict our response to be gaussian , but rather a non - linear function of @xmath36 , itself a linear function of the state .",
    "the non - linearity and the noise characteristics of the observations follow from the choice of regression model made , e.g. , from the specific mapping between the signal and the response : choosing a gaussian distribution for the response with a mean equal to the signal yields the standard kalman filter . in this sense ,",
    "our model is a generalization of the standard kalman filter .",
    "a variant of the kalman filter known as the extended kalman filter deals with general non - linear observations , and has been applied to responses modeled through the exponential family ( @xcite , @xcite ) , yielding an algorithm that can be shown to be equivalent to ours .    based on the assumptions above ,",
    "we obtain the following factorization of the joint probability function of predictors , responses and parameters . @xmath72",
    "we seek an algorithm to compute the mean and covariance matrix of the model parameters using all the observations we have at any given time .",
    "we want this algorithm to be online , i.e. , to perform a relatively simple update to the previous mean and covariance estimates when a new observation arrives , without having to re - analyze previous observations .",
    "let @xmath73 and @xmath74 denote the mean and covariance matrix of @xmath75 first , we initialize the mean and covariance of @xmath76 to the known values @xmath77 and @xmath78 .",
    "we proceed by induction : we assume we know that @xmath79 has parameter mean and covariance matrix @xmath80 and @xmath81 , and use them and the new observation to compute @xmath82 and @xmath83 through a two stepped process suggested by the following simple relation : @xmath84 @xmath85 equation [ eq : bayes ] relates @xmath86 to its @xmath87 which predicts @xmath88 based on all previous information up to but ignoring the observation at time @xmath89 , and the log - likelihood of the latest observation @xmath90 .",
    "we compute the mean and covariance of the prior @xmath91 via @xmath92 this equation is exact and does not require assuming any functional form for @xmath93 it follows fairly directly from equation [ eq : pardynamics ] , e.g. , see appendix [ sec : meanvarpredictionapp ] for a derivation of these and other equations in this section . when the parameter dynamics are non - linear , the mean and covariance of @xmath91 can be approximated through expressions identical to equations [ eq : meanpred ] and [ eq : varpred ] by suitably re - defining the matrices that appear in them , e.g. , by linearizing the parameter dynamics around @xmath94 or via numerical simulation as in the so - called unscented kalman filter ( @xcite , @xcite and @xcite ) .    note",
    "that when @xmath64 is the identity matrix , equation [ eq : varpred ] shows that the variance of the parameter estimates always increases in the prediction step , unless @xmath95 in addition , because the system input @xmath62 is deterministic , it does not contribute to the covariance matrix .    when the predictor @xmath96 becomes known",
    ", we can use equations [ eq : meanpred ] and [ eq : varpred ] to determine the mean @xmath97 and covariance matrix @xmath98 of the signal @xmath99 given @xmath100 and @xmath96 : @xmath101",
    "lastly , the covariance matrix between the signal and the parameters is given by @xmath102 .",
    "the latter follows from the fact that signal is a linear function of the parameters , with @xmath103 as the weights .",
    "now we update the estimates from the prediction step to incorporate the new observation , obtaining the mean @xmath73 and covariance @xmath74 of the posterior @xmath75 we first compute the following matrices @xmath104^{-1 } + \\mathbf{\\omega_t } , \\text { and } \\label{eq : qmatrix }   \\mathbf{a_t } = & \\mathbf{r_tx_tq_t^{-1}}.\\end{aligned}\\ ] ] here @xmath105 is the @xmath12-by-@xmath12 hessian matrix of the log - likelihood , evaluated at the predicted value of the signal @xmath106 as we will see , in many models of interest , this matrix is the negative of the variance of @xmath107 evaluated at the predicted signal value @xmath106 the matrix @xmath108 then grows with the expected variance of the predicted signal response @xmath98 , but decreases when the expected variance of the response increases .",
    "we then compute the covariance @xmath74 via : @xmath109 computing the inverse of @xmath108 starting from equation [ eq : qmatrix ] can often be numerically unstable , e.g. , because the determinant of @xmath105 can be very small in magnitude .",
    "a more robust way to compute @xmath110 is via @xmath111.\\end{aligned}\\ ] ] this expression follows directly from equations [ eq : varupdate2 ] and [ eq : qmatrix ] after applying the kailath variant of the woodbury identity ( e.g. , see @xcite ) .",
    "we finally compute the mean of the parameters by :    @xmath112    our main algorithm proceeds by executing the prediction and estimation steps for each arriving observation , namely evaluating equations [ eq : meanpred ] , [ eq : varpred ] , [ eq : varupdate2 ] and [ eq : meanupdate ] with every new observation .",
    "equations [ eq : meanupdate ] and [ eq : varupdate2 ] are approximate , and follow from ( 1 ) a second - order taylor expansion of @xmath90 around @xmath113 , and ( 2 ) assuming the prior @xmath91 is gaussian with mean and covariance given by @xmath113 and @xmath114 .",
    "a sketch of the argument is described in section [ sec : derivation ] . because the two assumptions we make are exact in the case of linear regression with a gaussian prior for @xmath115 equations [ eq : meanupdate ] and [ eq : varupdate2 ] are exact in that case and correspond to the standard kalman filter equations .",
    "west _ et al . _",
    "( @xcite ) make the different approximation that the prior @xmath79 is conjugate to the likelihood @xmath116 obtaining a slightly different algorithm that has only been developed for the univariate response scenario .",
    "many regression models involve a scalar signal @xmath117 and a scalar response @xmath118 where the predictor is now simply a vector @xmath119 .",
    "this is the situation for the most commonly encountered regression models , such as linear , logistic , poisson or exponential .",
    "the matrices @xmath120 @xmath121 and @xmath108 then also become scalars , so the update equations [ eq : meanupdate ] and [ eq : varupdate2 ] simplify to @xmath122 where the predicted signal is @xmath123 .",
    "the result is very appealing because no matrix inverses need to be computed .",
    "here we consider models where the response is in the exponential family of equation [ eq : nef ] , and where the natural parameter @xmath34 is related to the signal via equations [ eq : glm1 ] and [ eq : glm2 ] .",
    "the gradient in these models can be shown to be given by @xmath124 so the gradient is always proportional to the error @xmath125 , the difference between the response and its mean according to the model at the given signal .",
    "the covariance matrix @xmath126 is in general a function of the signal @xmath127 but we drop that dependence in our notation below to reduce clutter .    the hessian @xmath128 is then obtained by differentiating equation [ eq : gradientglm ] with respect to the signal once more , resulting in @xmath129 =   \\frac{\\partial}{\\partial \\mathbf{\\lambda_t}}\\biggr [ \\frac{\\partial   \\mathbf{h(\\lambda_t})'}{\\partial \\mathbf{\\lambda_t } }    \\mathbf{\\sigma_{y_t}^{-1 } } \\biggr ]   \\big(\\mathbf{y_t}-\\mathbf{h(\\lambda_t)}\\big ) \\nonumber \\\\    - \\frac{\\partial   \\mathbf{h(\\lambda_t})'}{\\partial \\mathbf{\\lambda_t } }    \\mathbf{\\sigma_{y_t}^{-1 } } \\frac{\\partial   \\mathbf{h(\\lambda_t})}{\\partial \\mathbf{\\lambda_t}}. \\label{eq : hessianglm }   \\end{aligned}\\ ] ] evaluating equations [ eq : gradientglm ] and [ eq : hessianglm ] for a given choice of the likelihood and link function ( which determines the response function @xmath130 ) at the predicted signal @xmath131 , and plugging the resulting expressions into equations [ eq : varupdate2 ] and [ eq : meanupdate ] completes the algorithm .      when the canonical link is used , the natural parameter is equal to the signal , so @xmath132 , and @xmath133 .",
    "equations [ eq : gradientglm ] and [ eq : hessianglm ] simplify to @xmath134 so the gradient is proportional to the error , as before , and the hessian is proportional to the negative of the covariance of @xmath135 evaluating the gradient and hessian above at the predicted signal @xmath97 , and plugging in the resulting expressions into equations [ eq : varupdate2 ] and [ eq : meanupdate ] yields the update equations @xmath136 \\mathbf{x_t'r_t }   \\label{eq : meanupdatecanonicalnefvar2 } \\\\ \\mathbf{m_t } = &   \\mathbf{a_t}+ \\mathbf{c_tx_t } \\mathbf{\\phi_t^{-1 } } \\big(\\mathbf{y_t}-\\mathbf{h(f_t)}\\big ) ,    \\text { where } \\label{eq : meanupdatecanonicalnef}\\\\   \\mathbf{e_t}= & \\mathbf{\\phi_t^{-1 } \\sigma_{y_t}(f_t ) \\phi_t^{-1}}.\\end{aligned}\\ ] ] equation [ eq : meanupdatecanonicalnefvar2 ] is the numerically stable analog of equation [ eq : varupdate2 ] that avoids inverses of potentially close - to - singular matrices .    * multivariate linear regression * is one of many examples that falls in this class of models .",
    "there , @xmath137 and @xmath107 can be shown to be in the natural exponential family ( e.g. , see equation [ eq : gaussiannef ] in the appendix ) , with @xmath138 and @xmath139 and covariance matrix equal to @xmath140 which in this case is independent of the signal .",
    "so the equations above yield the standard kalman filter equations .",
    "@xmath141 other distributions in the natural exponential family , e.g. , the multinomial , have variances that are a function of the signal  linear regression is the exception .    *",
    "the univariate signal case * covers the majority of applications encountered in practice . here",
    "the signal @xmath142 , the response @xmath143 , and the nuisance parameter @xmath144 are all scalars , and the predictor @xmath119 is a vector , so the update equations become : @xmath145 with @xmath146 being the variance of the response evaluated at the predicted signal @xmath147 . equation [ eq : varupdatecanonicalnefunivariate ] shows that the effect of the new observation is to reduce the covariance of the parameters by an amount proportional to @xmath148 , and a gain that gets smaller when there is more variance in the predicted signal , as captured by @xmath149 and larger when the response is expected to have a higher variance @xmath146 .",
    "many common regression models fall in this category .",
    "* univariate linear regression * with @xmath150 and @xmath151 this is already in natural exponential form with @xmath152 , and with @xmath142 playing the role of the natural parameter ( i.e. , the canonical link was used to map the mean of the response to the signal ) . substituting @xmath153 and @xmath154 in equations [ eq : varupdatecanonicalnefunivariate ] and [ eq : meanupdatecanonicalnefunivariate ]",
    "yields the univariate kalman filter .",
    "@xmath155    in * poisson regression *",
    "@xmath143 is a positive integer that follows a poisson distribution with mean @xmath156 .",
    "the likelihood is @xmath157 which is again in natural exponential form with @xmath142 as the natural parameter , and with @xmath158 .",
    "the variance of a poisson random variable is equal to its mean , so equations [ eq : varupdatecanonicalnefunivariate ] and [ eq : meanupdatecanonicalnefunivariate ] become @xmath159    in * exponential regression * @xmath143 is a non - negative real number that follows an exponential distribution with mean @xmath160 , so @xmath161 with mean @xmath162 and variance @xmath163 .",
    "note that here @xmath164 , so unlike other models here , the update in the mean is negatively proportional to the error , namely : @xmath165    in * logistic regression * the response is a bernoulli random variable .",
    "it takes on the value 1 with probability @xmath166 and the value 0 with probability @xmath167 .",
    "so @xmath168 is the response function , and its inverse @xmath169 is the link function .",
    "the likelihood becomes @xmath170 this is again in the natural exponential family with @xmath158 , and variance @xmath171 .",
    "the last equation above implies that @xmath172 is indeed the canonical link .",
    "so equations [ eq : varupdatecanonicalnefunivariate ] and [ eq : meanupdatecanonicalnefunivariate ] become @xmath173",
    "we start from equation [ eq : bayes ] , and view the likelihood as a function of the model parameters , namely @xmath174 .",
    "we then approximate @xmath175 about @xmath176 via the second - order taylor expansion : @xmath177 because the signal is given by @xmath178 we have that @xmath179    we also make the second approximation that @xmath180 .",
    "this approximation is what is needed to make the mathematics below work out , but could be justified in that the gaussian distribution is the continuous distribution that has maximum entropy given a mean and covariance matrix , and these are the only known statistics of @xmath181 .",
    "using the two approximations in equation [ eq : bayes ] results in @xmath182 being proportional to @xmath183 where @xmath184 equation [ eq : derivation2b ] follows from completing squares , e.g. , see appendix [ sec : completingsquares ] , and the proportional sign indicates that terms independent of @xmath185 were dropped .",
    "the result shows that under our approximations , @xmath186 to finish the argument , we substitute the expressions in equation [ eq : derivation1 ] into equation [ eq : derivation3 ] , and apply the woodbury matrix inversion formula to the expression for @xmath74 in equation [ eq : derivation3 ] to finally get the update equations [ eq : varupdate2 ] and [ eq : meanupdate ] .",
    "the models we discuss here can be and have been applied to a wide range of situations to model , analyze and forecast univariate and multivariate time series .",
    "e.g. , see chapter 14 in @xcite or @xcite for a range of examples . here",
    "we apply the models discussed to the contextual multi - armed bandits scenario , where so far only univariate time series modeled through a linear or logistic regression have been considered . in the latter case ,",
    "the only treatment known to us approximates the covariance matrix as diagonal .",
    "the models we have discussed enable explore / exploit algorithms for contextual multi - armed bandit scenarios where the reward depends on a multivariate response vector distributed according to the exponential family , and where the true parameters of the different arms are dynamic .",
    "we hope this broadens the situations where contextual multi - armed bandit approaches can be helpful .",
    "the standard setup involves a player interacting with a slot machine with @xmath187 arms over multiple rounds .",
    "every time an arm is played a reward gets generated .",
    "different plays of the same arm generate different rewards , i.e. , the reward is a random variable .",
    "different arms have different and unknown reward distributions , which are a function of an observed context . at every time step",
    ", the player must use the observed context for each arm and all the history of the game to decide which arm to pull and then collect the reward .",
    "we seek algorithms that the player can use to decide what arm to play at every round in order to maximize the sum of the rewards received .",
    "these algorithms build statistical models to predict the reward for each arm based on the context , and decide how to balance exploring arms about which little is known with the exploitation of arms that have been explored enough to be predictable .",
    "the exploration / exploitation trade - off requires having a handle on the uncertainty of the predicted reward , so the models used need to predict at least the mean and variance of the reward for each arm .",
    "real applications such as personalized news recommendations or digital advertising often have tight temporal and computational constraints per round , so the methods to update the statistical models with every outcome need to be online .",
    "popular and useful model choices describe the univariate reward for each arm as a linear function of the context plus gaussian noise ( i.e. , through a linear regression , e.g. , see @xcite ) , or through a logistic regression ( @xcite ) . in the latter case ,",
    "the algorithm that updates the model based on new observations uses a diagonal approximation of the parameter covariance matrix . in all these references , model parameters are assumed static ( although their estimates change with every observation ) . in the non - contextual multi - armed bandit problem ,",
    "recent efforts have tried to generalize the distributions for the rewards to the univariate exponential family @xcite , and as far as we know this is the first treatment for the contextual case .",
    "we consider the following scenario .",
    "the parameters of all arms are compiled in the single parameter vector @xmath185 that , unlike other settings , is allowed to change over time according to equation [ eq : pardynamics ] .",
    "some entries in @xmath185 correspond to parameters for a single arm , and others are parameters shared across multiple or all arms .",
    "we describe the model parameters via @xmath188 where @xmath100 is the history of contexts and responses seen up to and including round @xmath189 . at the start of round @xmath89",
    ", we observe the context matrix @xmath190 for each arm @xmath191 , and combine this information with our knowledge of @xmath192 to decide which arm to play .",
    "denote the arm played by @xmath193 , and its corresponding context matrix simply by @xmath194 to make it consistent with the notation in the rest of this paper . playing arm @xmath193 results in a response @xmath107 with a distribution in the ( possibly multivariate ) exponential family that depends on the context @xmath195 the relation between the response and the context",
    "is given by the dynamic glm in section [ sec : dynglm ] , so the mean of @xmath107 is a function of the signal @xmath196 the response is used to update our estimates of the model parameters @xmath197 , according to the algorithm described in section [ sec : dynglm ] , to be used in round @xmath198 .",
    "we assume the reward @xmath199 received in round @xmath89 is a known deterministic function of the response , e.g. , a linear combination of the entries in @xmath107 .",
    "if we knew the actual model parameters , the optimal strategy to maximize the rewards collected throughout the game would be to play the arm @xmath200 with the highest average reward , i.e. , @xmath201.$ ] we define the regret @xmath202 - e[f(\\mathbf{y_t})|\\mathbf{\\lambda_t}=\\mathbf{x'_t}\\mathbf{\\theta_t}],$ ] i.e. , the difference between the means of the rewards of the optimal arm and the arm played given the context and the model parameters .",
    "unlike the more standard contextual setup , ours allow for the explicit modeling of parameter dynamics .",
    "it also broadens the choice of probability distribution to use for the response or reward to more naturally match the model choice to the nature and dimensionality of the reward data .",
    "e.g. , we can use a poisson regression when the reward is a positive integer , or have a response with multiple entries each with a different distribution , use all response entries to update our parameter estimates , and then define the reward to be a single entry in the response .",
    "a contextual multi - armed bandit algorithm uses the knowledge of the parameters at each round and the context to decide which arm to play .",
    "the widely used upper confidence bound ( ucb ) approach constructs an upper bound on the reward for each arm using the mean and covariance of the parameter estimates at every round , and selects @xmath193 as the arm with the highest upper bound , e.g. , see @xcite .",
    "another approach that has gained recent popularity ( @xcite , @xcite ) is the so - called thompson sampling introduced in @xcite , where arm @xmath191 is selected at round @xmath89 with a probability that it is optimal given the current distribution @xmath203 for the model parameters .",
    "it is only recently that asymptotic bounds for its performance have been developed both for the contextual ( @xcite , for the linear regression case only ) and the non - contextual ( @xcite ) case .",
    "the studies mentioned have found thompson sampling to perform at pair or better relative to other approaches , and to be more robust than ucb when there is a delay in observing the rewards .",
    "thompson sampling is also very easy to implement . in one variant",
    "we sample a parameter value @xmath204 from the distribution @xmath205 and let @xmath206.$ ] in another variant , rather than sampling the model parameters once for all arms from @xmath205 we generate independent samples from the same distribution , the sample for arm @xmath191 denoted by @xmath207 , and then let @xmath208.$ ] the latter approach is found in @xcite for the linear regression case to have a total regret that asymptotically scales with the number of model parameters rather than with its square as in the first variant , so we use the second variant in our simulations .",
    "our goal here is to demonstrate how our online regression models work in the contextual bandits case when the observations are multivariate and not gaussian , and when the model parameters are allowed to be dynamic .",
    "the goal is not to compare different contextual bandit algorithms , so we only focus on thompson sampling .",
    "the model we simulate is inspired by the problem of optimizing the netflix sign - up experience .",
    "each arm corresponds to a variant of the sign - up pages that a visitor experiences  a combination of text displayed , supporting images , language chosen , etc .",
    "the context corresponds to the visitor s type of device and/or browser , the day of week , time of day , country where the request originated , etc .",
    "some of these predictors are continuous , such as the time of day , and others are categorical , such as the day of the week . the goal is maximizing signups by choosing the sign - up variant the is most likely to lead to a conversion given the context .",
    "we also observe other related outcomes associated to each visitor , such as the time spent on the sign - up experience , and whether they provide their email before signing up .",
    "we assume that these other observations are also related to the model parameters ( though possibly with different context vectors ) , and use them to improve our parameter estimates .",
    "so our response is multivariate , even if the reward is based on a single entry of the response vector .",
    "lastly , we want to let the model parameters drift over time , because we know that different aspects of the netflix product are relevant over time , e.g. , different videos in our streaming catalog will be the most compelling in a month than today .",
    "denote the response by @xmath209',$ ] and the reward by @xmath210 we model @xmath211 through a logistic regression , with a probability of taking the value 1 of @xmath212 and variance @xmath213 .",
    "the two other response entries do not affect the reward , but we use them to improve our estimates of the model parameters .",
    "we model @xmath214 as a linear regression with mean @xmath215 and variance @xmath216 , and @xmath217 through another logistic regression , with mean @xmath218 and variance @xmath219 .",
    "the signal is @xmath220'=\\mathbf{x'_t}\\mathbf{\\theta_t}.$ ] we assume that the entries of the response are independent of each other conditioned on the signal , so the nuisance parameter matrix @xmath221 is diagonal and time - independent , with the vector @xmath222'$ ] as its diagonal , and the covariance matrix of the response @xmath223 is diagonal with the vector @xmath224'$ ] as its diagonal .",
    "the context matrix @xmath225 for arm @xmath191 has one row for each model parameter entry and one column per response entry .",
    "some rows correspond to parameters shared by all arms , and others to parameters corresponding to a single arm . to construct @xmath226",
    "we simulate continuous and categorical predictors that we sample at every round .",
    "we let @xmath227 play the role of the continuous predictors , and sample each column from a zero - mean gaussian with covariance @xmath228 .",
    "the diagonal entries in @xmath228 are sampled independently from an exponential distribution with rate of 1 , and the off - diagonal entries all have a correlation of @xmath229 .",
    "we let the categorical predictor @xmath230 be a sample from a uniform categorical distribution with @xmath231 entries , i.e. , all entries in @xmath232 are zero except for one that is set to 1 .",
    "we also let @xmath233 be an indicator vector that specifies that arm @xmath191 is being evaluated .",
    "it has @xmath234 entries that are all zero except for its @xmath191-th entry which is set to 1 .",
    "letting @xmath235 be a column vector with @xmath236 entries , all set to 1 , we define the context matrix for arm @xmath191 as @xmath237 here @xmath238 denotes the kronecker product between two vectors or matrices .",
    "the first @xmath234 rows of @xmath226 simply specify what arm is being evaluated , the next @xmath239 rows correspond to the continuous predictors , followed by @xmath231 rows for the categorical predictors .",
    "the next @xmath240 rows @xmath241 are the interaction terms between the continuous predictors and the arm ( only rows corresponding to the arm @xmath191 are non - zero ) , and the last @xmath242 rows are the interaction terms between the categorical predictor and the arm chosen ( all these rows are zero except one that is set to 1 ) .",
    "the number of rows and model parameters is then @xmath243 .",
    "we let @xmath244 and @xmath245 .",
    "note that without the interaction terms , the optimal arm would be independent of the context .",
    "we set the model parameter dynamics to @xmath246 where @xmath247 @xmath248 has diagonal entries that are independent exponential random variables with rate @xmath249 , and a correlation coefficient of 0.2 for its off - diagonal entries .",
    "we sample a different matrix @xmath248 at every round .",
    "we assume the first visitor arrives at @xmath250 , and start the game by sampling @xmath251 from a zero - mean gaussian with diagonal covariance matrix .",
    "the diagonal entries are independent samples from an exponential distribution with rate equal to 1 .",
    "we initialize the mean and covariance estimates of @xmath251 as @xmath252 and @xmath253 , where @xmath254 is the identity matrix .    at round @xmath89 , starting from the mean @xmath80 and covariance @xmath81 estimates of the parameters , we compute the mean @xmath113 and covariance @xmath114 of the parameters .",
    "we then sample one value of the model parameters for each arm from the resulting prior distribution , and construct the context matrices @xmath226 for each arm .",
    "we use the context matrices and the parameter samples to choose @xmath193 ( which defines @xmath96 ) based on thompson sampling , we play @xmath193 and observe the response to obtain the round s reward , and update the parameter estimates to obtain @xmath73 and covariance @xmath74 and start the next round .    , but using @xmath255 rather than @xmath249 to increase the diffusion rate of the model parameters.,title=\"fig:\",width=264,height=264 ] , but using @xmath255 rather than @xmath249 to increase the diffusion rate of the model parameters.,title=\"fig:\",width=264,height=264 ]    figure [ fig : onerealization ] shows the result of one simulation with 2000 rounds and 10 arms labeled a through j. the left plot shows the optimal arm ( that with the highest predicted reward @xmath256 based on the actual parameters @xmath88 ) in blue and the arm played , selected via thompson sampling and the parameter estimates , in orange .",
    "it is evident from the spread of the orange dots across arms that , as expected , there was more exploration at the start of the game .",
    "the spread of the blue dots shows that the interaction terms between the context and the arm result in different arms being optimal in different rounds .",
    "the right plot shows the fraction of rounds when the optimal arm was not chosen through the first @xmath89 rounds for all values of @xmath89 in the simulation .",
    "it drops under 0.4 from close to 1.0 at the start .",
    "the blue line on the same figure shows the cumulative average regret rate per round , which is the sum of regrets per round divided by the number of rounds .",
    "the regret per round is simply @xmath257 , both evaluated using the actual parameters @xmath258 the yellow line shows the cumulative random regret that would have resulted from choosing any arm with equal probability , independently of the model parameter estimates or the context .",
    "we then repeated the full simulation 30 times and averaged the resulting timeseries across runs , for different scenarios with a different number of arms .",
    "figure [ fig : avgregrets ] shows the results . as expected , the probability of error , the regret and the random regret all increase with a larger number of arms .",
    "but the increased regret rate is quite mild , and continues to drop as more rounds are played .",
    "the benefit of the contextual bandit algorithm relative to uniformly at random choosing an arm is the difference between the random regret rate and the regret rate , and it increases nicely as the number of arms increases .",
    "we expect our approach to fall apart when the parameters drift so quickly over time that the information in the observations is not enough to keep the covariance of the model parameters from growing .",
    "we explored this by increasing the parameter diffusion rate by changing @xmath259 from @xmath260 to @xmath261 the results are shown in figure [ fig : avgregrets3 ] : although all metrics worsen , the regret rate still decreases nicely over time despite the large parameter fluctuations over time .",
    "we described a framework to easily obtain online algorithms that approximately estimate the mean and covariance matrix of the model parameters for a wide range of multivariate regression models where the model parameters change over time .",
    "although our derivation is novel , these algorithms have been well known within a subset of the time - series community for at least a decade , but to the best of the author s knowledge , are not well known within the broader machine learning and statistical community , where we think these tools can be helpful .",
    "we also propose using the algorithms in the contextual multi - armed bandit problem , where the approach here allows for dynamic parameters and a wider range of reward distributions .",
    "the methods we discuss here correspond to the so - called filtering problem in the kalman filter and related literature .",
    "there are other related algorithms that solve the so - called smoothing problem , i.e. , that estimate the parameters at any point in the past using all the observations .",
    "the latter have been useful for time - series analysis , but seem less obviously useful in machine learning applications ( though they are well known for the standard kalman filter , e.g. , see @xcite or @xcite ) , and so are not covered here .",
    "also , in situations where the parameter dynamics are non - linear , or where higher moments of the parameter estimates are desired , there are good alternative simulation - based approaches , e.g. , that rely on ideas from importance sampling and particle filters , that may be better choices than the methods described here .",
    "the best overviews of the full suite of methods that we know of are @xcite , @xcite and @xcite .",
    "we thank devesh parekh and dave hubbard for the initial discussions that triggered this research , stephen boyd and george c. verghese for the suggestion to relate this to kalman filters , and to justin basilico , roelof van zwol , and vijay bharadwaj for useful feedback on this paper .",
    "10    s.  agrawal and n.  goyal .",
    "further optimal regret bounds for thompson sampling . in _ proceedings of the sixteenth international conference on artificial intelligence and statistics _ , pages 99107 , 2013 .",
    "s.  agrawal and n.  goyal .",
    "thompson sampling for contextual bandits with linear payoffs . in _ proceedings of the 30th international conference on machine learning ( icml-13 ) _ , pages 127135 , 2013 .",
    "l.  bottou .",
    "large - scale machine learning with stochastic gradient descent . in _ proceedings of compstat2010",
    "_ , pages 177186 .",
    "springer , 2010 .",
    "o.  chapelle and l.  li .",
    "an empirical evaluation of thompson sampling . in _ advances in neural information processing systems _ , pages 22492257 , 2011 .",
    "j.  durbin and s.  j. koopman .",
    "time series analysis of non - gaussian observations based on state space models from both classical and bayesian perspectives . ,",
    "62(1):356 , 2000 .    j.  durbin and s.  j. koopman . .",
    "number  38 .",
    "oxford university press , 2012 .",
    "l.  fahrmeir .",
    "posterior mode estimation by extended kalman filtering for multivariate dynamic generalized linear models .",
    ", 87(418):501509 , 1992 .    j.  harrison and m.  west .",
    "springer , 1999 .",
    "r.  e. kalman . a new approach to linear filtering and prediction problems .",
    ", 82(1):3545 , 1960 .",
    "b.  m. klein . .",
    "phd thesis , citeseer , 2003 .",
    "n.  korda , e.  kaufmann , and r.  munos .",
    "thompson sampling for 1-dimensional exponential family bandits . in _ advances in neural information processing systems _ ,",
    "pages 14481456 , 2013 .",
    "l.  li , w.  chu , j.  langford , and r.  e. schapire .",
    "a contextual - bandit approach to personalized news article recommendation . in _ proceedings of the 19th international conference on world wide web _ ,",
    "pages 661670 .",
    "acm , 2010 .",
    "t.  minka . from hidden markov models to linear dynamical systems .",
    "technical report , citeseer , 1999 .",
    "c.  n. morris . natural exponential families with quadratic variance functions . , pages 6580 , 1982 .",
    "c.  n. morris .",
    "natural exponential families with quadratic variance functions : statistical theory . , pages 515529 , 1983 .",
    "k.  murphy .",
    "filtering , smoothing and the junction tree algorithm . , 1999 .",
    "j.  a. nelder and r.  baker .",
    "generalized linear models . , 1972 .",
    "k.  b. petersen , m.  s. pedersen , et  al .",
    "the matrix cookbook .",
    ", 7:15 , 2008 .",
    "d.  simon . .",
    "john wiley & sons , 2006 .    w.  r. thompson . on the likelihood that one unknown probability exceeds another in view of the evidence of two samples .",
    ", pages 285294 , 1933 .",
    "m.  j. wainwright and m.  i. jordan .",
    "graphical models , exponential families , and variational inference . , 1(1 - 2):1305 , 2008 .",
    "m.  west , p.  j. harrison , and h.  s. migon .",
    "dynamic generalized linear models and bayesian forecasting .",
    ", 80(389):7383 , 1985 .",
    "assuming that @xmath263 and using equation [ eq : pardynamics ] for the parameter dynamics , we have that @xmath264 = & e\\big [ \\mathbf{g_t\\theta_{t-1 } } + \\mathbf{b_t u_{t-1}}+\\mathbf{\\omega_t}|d_{t-1}\\big ] \\nonumber \\\\ = &    \\mathbf{g_t m_{t-1 } } + \\mathbf{b_t u_{t-1 } } = \\mathbf{a_t},\\end{aligned}\\ ] ] resulting in equation [ eq : meanpred ] .",
    "here we used the assumption that @xmath265 = \\mathbf{0}$ ] .",
    "the covariance matrix @xmath114 of @xmath262 in equation [ eq : varpred ] is found as follows : @xmath266 \\nonumber \\\\ = & e\\big[\\big(\\mathbf{g_t}(\\mathbf{\\theta_{t-1 } }   -\\mathbf{m_{t-1}})+ \\mathbf{\\omega_t}\\big)\\big(\\mathbf{g_t}(\\mathbf{\\theta_{t-1 } }   -\\mathbf{m_{t-1}})+ \\mathbf{\\omega_t}\\big ) '     |d_{t-1}\\big ] \\nonumber \\\\ = & \\mathbf{g_tc_{t-1}g_t'}+\\mathbf{w_t}.\\end{aligned}\\ ] ] here , we used the assumption that the noise vector @xmath61 is uncorrelated with the parameter @xmath267    the mean and covariance @xmath268 $ ] of the signal are derived as follows : @xmath269 = & \\mathbf{x_t'}e[\\mathbf{\\theta_t}|d_{t-1 } ] =   \\mathbf{x_t'a_t}=\\mathbf{f_t}. \\nonumber \\\\",
    "\\mathbf{\\omega_t}= & e\\big[\\mathbf{x_t'}(\\mathbf{\\theta_t - a_t})(\\mathbf{\\theta_t - a_t } ) ' \\mathbf{x_t } | d_{t-1},\\mathbf{x_t } \\big ] \\nonumber \\\\ = & \\mathbf{x_t'r_tx_t } .\\end{aligned}\\ ] ] so @xmath270 .",
    "lastly , the covariance @xmath271 $ ] between the signal and the parameters at time @xmath89 given @xmath100 and the predictors @xmath96 is given by @xmath272 = \\mathbf{x_t'r_t}.\\end{aligned}\\ ] ]",
    "let @xmath25 be a random vector with @xmath10 entries distributed according to the exponential form @xmath273 where @xmath274 is a sufficient statistic for @xmath25 , @xmath34 is the natural parameter vector , and @xmath0 is a symmetric @xmath10-by-@xmath10 matrix and a nuisance parameter .",
    "note that equation [ eq : nef ] is more restrictive , because it implicitly assumes that @xmath275 .",
    "for example , if @xmath276 with known covariance matrix @xmath277 but unknown mean , we have that @xmath278 is @xmath279 so @xmath25 in in the exponential family with @xmath280 , @xmath46 is the natural parameter @xmath281 , and the sufficient statistic is @xmath282 . when the covariance matrix is unknown , the sufficient statistic expands to include @xmath283 and @xmath34 is a function of both @xmath46 and @xmath284 .    if the covariance matrix is not known , then we can instead define the sufficient statistic to be @xmath285,$ ] where @xmath286 for any matrix @xmath1 is a column vector resulting from stacking all the columns in @xmath1 , and re - arrange equation [ eq : gaussiannef ] to have the natural exponential form in equation [ eq : ef ] , now with natural parameter @xmath34 being both a function of @xmath46 and @xmath140 with @xmath287 is proportional to : @xmath288}_{\\mathbf{\\eta}'}\\underbrace{[\\mathbf{y ' } \\",
    "vec(\\mathbf{yy'})]'}_{\\mathbf{t(y ) } } - \\underbrace { \\big(\\frac{1}{2 } \\mathbf{\\mu}'\\mathbf{\\sigma^{-1 } } \\mathbf{\\mu }   + \\frac{1}{2}\\log(|\\sigma|)\\big)}_{b(\\mathbf{\\eta } ) } , \\label{eq : gaussiannef2}\\end{aligned}\\ ] ] so in this case @xmath289 and the natural parameter becomes a function of @xmath284 as well as of the mean @xmath46 .      the function @xmath290 $ ] can be shown to equal @xmath291 .",
    "@xmath292 the last equality follows because the integrand in the first term of the second equation is also a probability distribution in the natural exponential family with parameter @xmath293 , so the integral equals 1 . taking the first derivative of @xmath294 and evaluating it at @xmath295 yields @xmath296 = \\frac{\\partial b(\\mathbf{\\eta , \\phi})}{\\partial \\mathbf{\\eta}}. \\label{eq : mgfmean}\\end{aligned}\\ ] ] similarly , the second derivative of @xmath294 at @xmath295 yields @xmath297   \\mathbf{\\phi^{-1 } } =   \\frac{\\partial b(\\mathbf{\\eta , \\phi})}{\\partial \\mathbf{\\eta } } \\biggr ( \\frac{\\partial b(\\mathbf{\\eta , \\phi})}{\\partial \\mathbf{\\eta}}\\biggr ) ' +   \\frac{\\partial^2 b(\\mathbf{\\eta , \\phi})}{\\partial \\mathbf{\\eta^2 } } , \\text { so }   \\nonumber   \\\\   \\mathbf{\\phi } \\frac{\\partial^2 b(\\mathbf{\\eta , \\phi})}{\\partial \\mathbf{\\eta^2 } }   \\mathbf{\\phi } = e\\biggr[\\biggr(\\mathbf{t(y)-e[\\mathbf{t(y)}]}\\biggr)\\biggr(\\mathbf{t(y)-e[\\mathbf{t(y)}]}\\biggr)'\\biggr ] .",
    "\\label{eq : mgfvar}\\end{aligned}\\ ] ] setting @xmath298 above yields equations [ eq : nefmean ] and [ eq : nefvar ] .",
    "the derivation of equation [ eq : derivation2 ] required turning the expression @xmath299 where @xmath300 @xmath301 ( which is symmetric and positive definite ) and @xmath302 into the so - called perfect square expression @xmath303 for @xmath304 .",
    "we have @xmath305 so we need @xmath306 this implies that @xmath307"
  ],
  "abstract_text": [
    "<S> we study the problem of estimating the parameters of a regression model from a set of observations , each consisting of a response and a predictor . </S>",
    "<S> the response is assumed to be related to the predictor via a regression model of unknown parameters . </S>",
    "<S> often , in such models the parameters to be estimated are assumed to be constant . </S>",
    "<S> here we consider the more general scenario where the parameters are allowed to evolve over time , a more natural assumption for many applications . </S>",
    "<S> we model these dynamics via a linear update equation with additive noise that is often used in a wide range of engineering applications , particularly in the well - known and widely used kalman filter ( where the system state it seeks to estimate maps to the parameter values here ) . </S>",
    "<S> we derive an approximate algorithm to estimate both the mean and the variance of the parameter estimates in an online fashion for a generic regression model . </S>",
    "<S> this algorithm turns out to be equivalent to the extended kalman filter . </S>",
    "<S> we specialize our algorithm to the multivariate exponential family distribution to obtain a generalization of the generalized linear model ( glm ) . because the common regression models encountered in practice such as logistic , exponential and multinomial all have observations modeled through an exponential family distribution , our results are used to easily obtain algorithms for online mean and variance parameter estimation for all these regression models in the context of time - dependent parameters . </S>",
    "<S> lastly , we propose to use these algorithms in the contextual multi - armed bandit scenario , where so far model parameters are assumed static and observations univariate and gaussian or bernoulli . both of these restrictions can be relaxed using the algorithms described here , which we combine with thompson sampling to show the resulting performance on a simulation . </S>"
  ]
}