{
  "article_text": [
    "statisticians are nowadays frequently confronted with massive data sets from various frontiers of scientific research .",
    "fields such as genomics , neuroscience , finance and earth sciences have different concerns on their subject matters , but nevertheless share a common theme : they rely heavily on extracting useful information from massive data and the number of covariates @xmath0 can be huge in comparison with the sample size @xmath1 .",
    "in such a situation , the parameters are identifiable only when the number of the predictors that are relevant to the response is small , namely , the vector of regression coefficients is sparse .",
    "this sparsity assumption has a nice interpretation that only a limited number of variables have a prediction power on the response . to explore the sparsity ,",
    "variable selection techniques are needed .    over the last ten years",
    ", there has been many exciting developments in statistics and machine learning on variable selection techniques for ultrahigh dimensional feature space .",
    "they can basically be classified into two classes : penalized likelihood and screening .",
    "penalized likelihood techniques are well known in statistics : bridge regression ( @xcite ) , lasso ( @xcite ) , scad or other folded concave regularization methods ( @xcite ) , and dantzig selector ( @xcite ) , among others . these techniques select variables and estimate parameters simultaneously by solving a high - dimensional optimization problem .",
    "see @xcite and @xcite for an overview of the field . despite the fact that various efficient algorithms have been proposed ( @xcite ) ,",
    "statisticians and machine learners still face huge computational challenges when the number of variables is in tens of thousands of dimensions or higher .",
    "this is particularly the case as we are entering the era of `` big data '' in which both sample size and dimensionality are large .    with this background ,",
    "@xcite propose a two - scale approach , called iterative sure independence screening ( isis ) , which screens and selects variables iteratively .",
    "the approach is further developed by @xcite in the context of generalized linear models .",
    "theoretical properties of sure independence screening for generalized linear models have been thoroughly studied by @xcite .",
    "other marginal screening methods include tilting methods ( @xcite ) , generalized correlation screening ( @xcite ) , nonparametric screening ( @xcite ) , and robust rank correlation based screening ( @xcite ) , among others .",
    "the merits of screening include expediences in distributed computation and implementation . by ranking marginal utility such as marginal correlation with the response ,",
    "variables with weak marginal utilities are screened out by a simple thresholding .",
    "the simple marginal screening faces a number of challenges . as pointed out in @xcite , it can screen out those hidden signature variables : those who have a big impact on response but are weakly correlated with the response",
    ". it can have large false positives too , namely recruiting those variables who have strong marginal utilities but are conditionally independent with the response given other variables . @xcite and",
    "@xcite use a residual based approach to circumvent the problem but the idea of conditional screening has never been formally developed .",
    "conditional marginal screening is a natural extension of simple independent screening . in many applications ,",
    "researchers know from previous investigations that certain variables @xmath2 are responsible for the outcomes .",
    "this knowledge should be taken into account when applying a variable selection technique in order not to remove these predictors from the model and to improve the selection process .",
    "conditional screening recruits additional variables to strengthen the prediction power of @xmath2 , via ranking conditional marginal utility of each variable in presence of @xmath2 . in absence of such a prior knowledge",
    ", one can take those variables that survive the screening and selection as in @xcite .",
    "conditional screening has several advantages .",
    "first of all , it makes it possible to recover the hidden significant variables .",
    "this can be seen by considering the following linear regression model @xmath3 with @xmath4 .",
    "the marginal covariance between @xmath5 and @xmath6 is given by @xmath7 where @xmath8 is equal to 0 , except for its @xmath9th element which equals to 1 .",
    "this shows that the marginal covariance between @xmath5 and @xmath6 is zero if @xmath10 , where @xmath11 is the @xmath12 element of @xmath13 , with @xmath14 . yet",
    ", @xmath15 can be far away from zero .",
    "in other words , under the conditions listed above , @xmath5 is a hidden signature variable . to demonstrate that , let us consider the case in which @xmath16 , with true regression coefficients @xmath17 , and all variables follow the standard normal distribution with equal correlation 0.5 , and @xmath18 follows the standard normal distribution . by design , @xmath19 is a hidden signature variable , which is marginally uncorrelated with the response @xmath6 .",
    "based on a random sample of size @xmath20 from the model , we fit marginal regression and obtain the marginal estimates @xmath21 .",
    "the magnitudes of these estimates are summarized by their averages over three groups : indices 1 to 5 ( denoted by @xmath22 ) , 6 and indices 7 to 2000 .",
    "clearly , the magnitude on the first group should be the largest , followed by the third group .",
    "figure  [ fig1a ] depicts the distributions of those marginal magnitudes based on 10000 simulations .",
    "clearly variable @xmath19 can not be selected by marginal screening .",
    "+    adapting the conditional screening approach gives a very different result .",
    "conditioning upon the first five variables , conditional correlation between @xmath19 and @xmath6 has a large magnitude . with the same simulated data as in the above example",
    ", the regression coefficient @xmath23 of @xmath5 in the joint model with the first five variables is computed .",
    "this measures the conditional contribution of variable @xmath5 in presence of the first five variables .",
    "again , the magnitudes @xmath24 are summarized into two values : @xmath25 and the average of @xmath26 .",
    "the distributions of those over 10000 simulations are also depicted in figure  [ fig1b ] . clearly , the variable @xmath19 has higher marginal contributions than others .",
    "that is , conditioning helps recruiting the hidden signature variable .",
    "furthermore , conditioning is fairly robust to extra elements . to demonstrate",
    "that , we have repeated the previous experiment with conditioning on five more randomly chosen features .",
    "the distribution of the magnitudes are given in figure  [ fig1c ] .",
    "it is seen that the important hidden variable again has a large magnitude .",
    "the benefits of conditioning are observed even if the conditioned variables are not in the active set . to demonstrate that , the regression coefficient @xmath23 of @xmath5 has been computed while conditioning on five randomly chosen inactive variables .",
    "that is , contribution of variable @xmath5 is calculated in the presence of these five randomly chosen inactive variables .",
    "the magnitudes of @xmath27 are summarized in three groups : the average of the first five important variables , i.e. @xmath28 , @xmath25 and the average of @xmath26 .",
    "the distributions for these variables over 10000 simulations are given in figure  [ fig1d ] .",
    "it is observed that the magnitude of the hidden signature variable increases significantly and hence it will surely not be missed during the screening . in other words , conditioning can help to recruit the important variables , even when the conditional set is not ideally chosen .",
    "+    secondly , conditional screening helps for reducing the number of false negatives .",
    "marginal screening can fail when there are covariates in the non - active set that are highly correlated with active variables . to appreciate this ,",
    "consider the linear model ( [ eq1 ] ) again with sparse regression coefficients @xmath29 , equi - correlation 0.9 among all covariates except @xmath30 , which is independent of the rest of the covariates .",
    "this setting gives @xmath31 in this case , marginal utilities for all nonactive variables are higher than that for the active variable @xmath30 .",
    "a summary similar to figure  [ fig1 ] is shown in the upper left panel of figure  [ fig2 ] .",
    "therefore , based on sis ( sure independence screening ) in fan and lv ( 2008 ) , the active variable @xmath30 has the least priority to be included . by using the conditional screening approach in which the covariate @xmath32 is conditioned upon ( used in the joint fit ) ,",
    "marginal utilities of the spurious variables are significantly reduced .",
    "the distributions of the average of the magnitude of the conditional fitted coefficients @xmath33 and @xmath34 are shown in the middle panel of figure  [ fig2 ] . clearly , the nonactive variables are significantly demoted by conditioning . to observe effects of conditioning on extra variables and randomly chosen variables , a similar experiment to the first case is also done .",
    "figure  [ fig2c ] depicts the distribution of the conditioned marginal fits when five extra variables are conditioned on .",
    "the contributions of variables @xmath5 in the presence of ten randomly chosen variables are given in figure  [ fig2d ] .",
    "it is seen that , the relative magnitude of the hidden active variable @xmath30 is considerably larger and hence it is more likely that it is recruited during screening .",
    "finally , as shown by @xcite and @xcite , for a given threshold of marginal utility , the size of the selected variables depends on the correlation among covariates , as measured by the largest eigenvalue of @xmath35 : @xmath36 .",
    "the larger the quantity , the more variables have to be selected in order to have a sure screening property . by using conditional screening",
    ", the relevant quantity now becomes @xmath37 , where @xmath2 refers to the @xmath38 covariates that we will condition upon and @xmath39 is the rest of the variables .",
    "conditioning helps reducing correlation among covariates @xmath39 .",
    "this is particularly the case when covariates @xmath40 share some common factors , as in many biological ( e.g. treatment effects ) and financial studies ( e.g. market risk factors ) . to illustrate the benefits we consider the case where @xmath40 is given by equally correlated normal random variables .",
    "simple calculations yield that @xmath41 where @xmath42 is the common correlation and @xmath43 . as @xmath40 has a normal distribution ,",
    "the conditional covariance matrix can be calculated easily and it can be shown that @xmath44 note that when @xmath45 , the formula reduces to the unconditional one .",
    "it is clear that conditioning helps reducing the correlation among the variables . to quantify the degree of de - correlation , figure  [ fig3 ] depicts the ratio @xmath46 as a function of @xmath42 for various choices of @xmath38 when @xmath47 .",
    "the reduction is dramatic , in particular when @xmath42 is large or @xmath38 is large .",
    "the benefits of conditioning are clearly evidenced .        in this paper",
    ", we propose the conditional screening technique and formally establish the conditions under which it has a sure screening property .",
    "we also give an upper bound for the number of selected variables for each given threshold value .",
    "two data - driven methods for choosing the thresholding parameter are proposed to facilitate the practical use of the conditional screening technique .",
    "the rest of the paper is organized as follows . in section 2",
    ", we introduce the conditional sure independence screening procedure .",
    "the sure independence screening property and the uniform convergence of the conditional marginal maximum likelihood estimator are presented in section 3 . in section 4 ,",
    "two approaches are proposed to choose the thresholding parameter for csis .",
    "finally , we examine the performance of our procedure in section 5 on simulated and real data .",
    "the details of the proofs are deferred to the appendix .",
    "generalized linear models assume that the conditional probability density of the random variable @xmath6 given @xmath48 belongs to an exponential family @xmath49 where @xmath50 and @xmath51 are specific known functions in the canonical parameter @xmath52 .",
    "note that we ignore the dispersion parameter @xmath53 , since the interest only focuses on estimation of the mean regression function .",
    "however , it is easy to include a dispersion parameter @xmath53 . under model ( [ eq3 ] )",
    ", we have the regression function @xmath54 the canonical parameter is further parameterized as @xmath55 namely the canonical link is used in modeling the mean regression function .",
    "well known distributions in this exponential family include the normal , binomial , poisson , and gamma distributions .    in the ultrahigh dimensional sparse linear model , we assume that the true parameter @xmath56 is sparse .",
    "namely , the set @xmath57 is small .",
    "our aim is to estimate the set @xmath58 and coefficient vector @xmath59 , as well as predicting the outcome @xmath6 .",
    "this is a more challenging task than just predicting @xmath6 as in many machine learning problems . when the dimensionality is ultrahigh , one often employs a screening technique first to reduce the model size .",
    "it is particularly effective in distributed computation for dealing with `` big data '' .",
    "conditional screening assumes that there is a set of variables @xmath2 that are known to be related to the response @xmath6 and we wish to recruit additional variables from the rest of variables , given by @xmath39 , to better explain the response variable @xmath6 . for simplicity of notation , we assume without loss of generality that @xmath60 is the set of first @xmath38 variables and @xmath61 is the remaining set of @xmath43 variables .",
    "we will use the notation @xmath62 and similar notation for @xmath2 and @xmath39 .    assume without loss of generality that the covariates have been standardized so that @xmath63    given a random sample @xmath64 from the generalized linear model ( [ eq3 ] ) with the canonical link",
    ", the conditional maximum marginal likelihood estimator @xmath65 for @xmath66 is defined as the minimizer of the ( negative ) marginal log - likelihood @xmath67 where @xmath68 and @xmath69 is the empirical measure .",
    "denote from now on by @xmath70 the last element of @xmath71 .",
    "it measures the strength of the conditional contribution of @xmath5 given @xmath2 . in the above notation",
    ", we assume that the intercept is used and is incorporated in the vector @xmath2 .",
    "conditional marginal screening based on the estimated marginal magnitude is to keep the variables @xmath72 for a given thresholding parameter @xmath73 .",
    "namely , we recruit variables with large additional contribution given @xmath2 .",
    "this method will be referred to as conditional sure independence screening ( csis ) .",
    "it depends , however , on the scale of @xmath74 and @xmath75 to be defined in section 3.1 .",
    "a scale - free method is to use the likelihood reduction of the variable @xmath5 given @xmath2 , which is equivalent to computing @xmath76 after ignoring the common constant @xmath77 .",
    "the smaller @xmath78 , the more the variable @xmath5 contributes in presence of @xmath2 .",
    "this leads to an alternative method based on the likelihood ratio statistics : recruit additional variables according to @xmath79 where @xmath80 is a thresholding parameter .",
    "this method will be referred to as conditional maximum likelihood ratio screening ( cmlr ) .",
    "we emphasize that , the set of variables @xmath2 does not necessarily have to contain active variables .",
    "conditional screening only makes use of the fact that the effects of important variables are more visible in the presence of @xmath2 and the correlations of variables are weakened upon conditioning .",
    "this is commonly the case in many applications such as finance and biostatistics , where the variables share some common factors .",
    "it gives hidden signature variables a chance to survive .",
    "in fact , it was demonstrated in the introduction that conditioning can be beneficial even if the set @xmath2 is chosen randomly .",
    "our theoretical study gives a formal justifications of the iterated method proposed in fan and lv ( 2008 ) and fan _ et .",
    "in order to prove the sure screening property of our method , we first need some properties on the population level .",
    "let @xmath81 , @xmath82 , and @xmath83 with the expectation taken under the true model .",
    "then , @xmath84 is the population version of @xmath85 . to establish the sure screening property , we need to show that the marginal regression coefficient @xmath86 , the last component of @xmath84 , provides useful probes for the variables in the joint model @xmath87 and",
    "its sample version @xmath88 is uniformly close to the population counterpart @xmath89 .",
    "therefore , the vector of marginal fitted regression coefficients @xmath85 is useful for finding the variables in @xmath87 .",
    "since we are fitting @xmath90 marginal regressions , that is we are using only @xmath91 out of the @xmath0 original predictors , we need to introduce model misspecifications .",
    "thus , we do not expect that the marginal regression coefficient @xmath86 is equal to the joint regression parameter @xmath92 .",
    "however , we hope that when the joint regression coefficient @xmath93 exceeds a certain threshold , @xmath94 exceeds another threshold in most cases .",
    "therefore , the marginal conditional regression coefficients provide useful probes for the joint regression .    by ( [ eq8 ] ) , the marginal regression coefficients @xmath95",
    "satisfy the score equation @xmath96 where the second equality follows from the fact that @xmath97 . without using the additional variable @xmath5 ,",
    "the baseline parameter is given by @xmath98 and satisfies the equation @xmath99 we assume that the problems at marginal level are fully identifiable , namely , the solutions @xmath100 and @xmath95 are unique .",
    "to understand the conditional contribution , we introduce the concept of the conditional linear expectation .",
    "we use the notation @xmath101 which is the best linearly fitted regression within the class of linear functions .",
    "similarly , we use the notation @xmath74 to denote the best linear regression fit of @xmath5 by using @xmath2 .",
    "then , equation ( [ eq11 ] ) can be more intuitively expressed as @xmath102 note that the conditioning in this paper is really a conditioning linear fit and the conditional expectation is really the conditional linear expectation .",
    "this facilitates the implementation of the conditional ( linear ) screening in high - dimensional , but adds some technical challenges in the proof .",
    "let us examine the implication marginal signal , i.e. @xmath103 .",
    "when @xmath104 , by ( [ eq9 ] ) , the first @xmath38 components of @xmath95 , denoted by @xmath105 , should be equal to @xmath100 by uniqueness of equation ( [ eq11 ] ) . then , equation ( [ eq9 ] ) on the component @xmath5 entails @xmath106 using ( [ eq13 ] ) , the above condition can be more comprehensively expressed as @xmath107 this proves the necessary condition of the following theorem .",
    "[ thm1 ] for @xmath108 , the marginal regression parameters @xmath109 if and only if @xmath110 .    proof of the sufficient part is given in appendix [ app thm1 ] . in order to have the sure screening property at the population level of equation ( [ eq8 ] )",
    ", the important variables @xmath111 should be conditionally correlated with the response , where @xmath112 . moreover ,",
    "if @xmath5 ( with @xmath113 ) is conditionally correlated with the response , the regression coefficient @xmath114 is non - vanishing .",
    "the sure screening property of conditional mle ( cmle ) , given by equation , will be guaranteed if the minimum marginal signal strength is stronger than the estimation error .",
    "this will be shown in theorem [ thm2 ] and requires condition [ cond1 ] .",
    "the details of the proof are relegated to appendix [ app thm2 ] .",
    "[ cond1 ]    1 .   for @xmath115 ,",
    "there exists a positive constant @xmath116 and @xmath117 such that @xmath118 .",
    "2 .   let @xmath119 be the random variable defined by @xmath120 then , @xmath121 uniformly in @xmath66 .",
    "note that , by strict convexity of @xmath122 , @xmath123 almost surely .",
    "when we are dealing with linear models , i.e. @xmath124 , then @xmath125 and condition  [ cond1](ii ) requires that @xmath126 is bounded uniformly , which is automatically satisfied by the normalization condition @xmath127 .",
    "[ thm2 ] if condition [ cond1 ] holds , then there exists a @xmath128 such that @xmath129      in this section , we prove the uniform convergence of the conditional marginal maximum likelihood estimator and the sure screening property of the conditional sure independence screening method .",
    "in addition we provide an upper bound on the size of the set of selected variables @xmath130 .",
    "since the log - likelihood of a generalized linear model with the canonical link is concave , @xmath131 has a unique minimizer over @xmath132 at an interior point @xmath84 , where @xmath133 is the set over which the marginal likelihood is maximized . to obtain the uniform convergence result at the sample level ,",
    "a few more conditions on the conditional marginal likelihood are needed .",
    "[ cond2 ]    1 .   for the fisher information @xmath134 , its operator norm ,",
    "@xmath135 is bounded , where @xmath136 and @xmath137 is the euclidian norm .",
    "2 .   there exists some positive constants @xmath138 and @xmath139 such that for sufficiently large @xmath140 @xmath141 and that @xmath142 3 .",
    "the second derivative of @xmath122 is continuous and positive .",
    "there exists an @xmath143 such that for all @xmath66 : @xmath144 where @xmath145 is the indicator function and @xmath146 is an arbitrarily large constant such that for a given @xmath147 in @xmath148 , the function @xmath149 is lipschitz for all @xmath150 in @xmath151 with @xmath152 .",
    "4 .   for all @xmath132 , we have @xmath153 for some positive @xmath154 , bounded from below uniformly over @xmath66 .",
    "the first three conditions given in condition [ cond2 ] are satisfied for almost all of the commonly used generalized linear models .",
    "examples include linear regression , logistic regression , and poisson regression .",
    "the first part of condition [ cond2](ii ) puts an exponential bound on the tails of @xmath5 .    in the following theorem ,",
    "the uniform convergence of our conditional marginal maximum likelihood estimator is stated as well as the sure screening property of the procedure .",
    "the proof of this theorem is deferred to appendix [ app thm3 ] .",
    "[ thm3 ] suppose that condition [ cond2 ] holds .",
    "let @xmath155 , with @xmath146 given in condition [ cond2 ] .    1 .   if @xmath156 , then for any @xmath157 , there exists a positive constant @xmath158 such that @xmath159 where @xmath160 .",
    "2 .   if in addition , condition [ cond1 ] holds , then by taking @xmath161 with @xmath162",
    ", we have @xmath163 for some constant @xmath164 , where @xmath165 the size of the set of nonsparse elements .    note that the sure screening property , stated in the second conclusion of theorem 3 , depends only on the size @xmath166 of the set of nonsparse elements and not on the dimensionality @xmath90 or @xmath0 .",
    "this can be seen in the second conclusion above .",
    "this result is understandable since we only need the elements in @xmath167 to pass the threshold , and this only requires the uniform convergence of @xmath168 over @xmath113 .",
    "the truncation parameter @xmath146 appears on both terms of the upper bound of the probability .",
    "there is a trade - off on this choice . for the bernoulli model with logistic link , @xmath169 is bounded and the optimal order for @xmath146 is @xmath170 . in this case , the conditional sure independence screening method can handle the dimensionality @xmath171 which guarantees that the upper bound in theorem  [ thm3 ] converges to zero .",
    "a similar result for unconditional screening is shown in @xcite .",
    "in particular , when the covariates are bounded , we can take @xmath172 , and when covariates are normal , we have that @xmath173 . for the normal linear model , following the same argument as in @xcite ,",
    "the optimal choice is @xmath174 where @xmath175 .",
    "then , conditional sure independence screening can handle dimensionality @xmath176 which is of order @xmath177 when @xmath173 .",
    "we have just stated the sure screening property of our csis method , that is @xmath178 .",
    "however , a good screening method does not only possess sure screening , but also retains a small set of variables after thresholding .",
    "below , we give a bound on the size of the selected set of variables , under the following additional conditions .",
    "[ cond3 ]    1 .",
    "the variance @xmath179 and @xmath180 are bounded .",
    "2 .   the minimum eigenvalue of the matrix @xmath181 $ ] is larger than a positive constant , uniformly over @xmath9 , where @xmath119 is defined in condition  [ cond1](ii ) .",
    "3 .   letting @xmath182 \\big[{\\mbox{\\bf x}}^t { \\mbox{\\boldmath $ \\beta$}}^\\star -{\\mbox{\\bf x}}_{{\\mathcal{c}}}^t { \\mbox{\\boldmath $ \\beta$}}_{{\\mathcal{c}}}^m \\big ] \\big \\},\\ ] ] it holds that @xmath183 , with @xmath184 the largest eigenvalue of @xmath185 [ { \\mbox{\\bf x}}_{{\\mathcal{d } } } -   \\operatorname{\\mathbb{e}}_l({\\mbox{\\bf x}}_{{\\mathcal{d } } } | { \\mbox{\\bf x}}_{{\\mathcal{c}}})]^t$ ] .    as noted above , for the normal linear model , @xmath124 .",
    "condition  [ cond3 ] ( ii ) requires that the minimum eigenvalue of @xmath186 be bounded away from zero . in general , by strict convexity of @xmath122 , @xmath123 almost surely .",
    "thus , condition  [ cond3](ii ) is mild .    for the linear model with @xmath187 , by ( [ eq11 ] ) , @xmath188 and",
    "hence @xmath189 since @xmath190 $ ] is linear in @xmath2 by definition .",
    "thus , condition  [ cond3](ii ) holds automatically .",
    "from the proof of theorem  [ thm4 ] , without condition  [ cond3](iii ) , theorem  [ thm4 ] below continues to hold with @xmath191 replaced by @xmath192 .",
    "[ thm4 ] under conditions [ cond2 ] and [ cond3 ] , we have for @xmath193 , there exists a @xmath194 such that @xmath195    this theorem is proved in appendix [ app thm4 ] .",
    "in the previous section , we have shown that csis has the sure screening property when the thresholding level @xmath73 is chosen such that @xmath196 . unfortunately , in practice @xmath73 , which relates to the minimum strength of marginal signals in the data , is always unknown .",
    "therefore , @xmath73 has to be estimated from the data itself .",
    "underestimating @xmath73 will result in a lot variables after screening , which leads to a large number of false positives , and similarly overestimation of @xmath73 will prevent sure screening .    in this section , we present two procedures that select a thresholding level for csis .",
    "the first approach is based on controlling the number of false positives by bounding the false discovery rate ( fdr ) .",
    "this method uses the fact that quasi - likelihood estimates for glms enjoy asymptotic normality . the second approach , that we call random decoupling , uses a resampling technique to create the null model and to measure the maximum strength of noise . in random decoupling ,",
    "we use marginal regression on the null model to obtain the marginal regression coefficients that are known to be zero .",
    "we use the maximum of these marginal coefficients of the null model as a thresholding level .",
    "it is well known that quasi - maximum likelihood estimates have an asymptotically normal distribution under general conditions ( @xcite ) .",
    "then , for covariates @xmath9 such that , @xmath104 , asymptotically it follows that @xmath197^{1/2}\\hat{\\beta}_{j}^{m}\\sim \\mathcal{n}(0,1),\\ ] ] where @xmath198 denotes the element that corresponds to @xmath199 in the information matrix @xmath200 .    using this property , we can build a thresholding technique that bounds the proportion of elements @xmath9 such that , @xmath104 . for the case , when @xmath104 for all @xmath201 , this rate is also called the false discovery rate in @xcite and is given by @xmath202 .    by choosing @xmath203 ,",
    "the expected false discovery rate is bounded above by @xmath204 , where @xmath205 is the distribution function of a standard normal random variable .",
    "this approach can also be seen as a modification of the method introduced by @xcite for the cox model . by setting @xmath206 to @xmath207 where @xmath208 is the maximum number of false positives we can tolerate , we obtain an expected false positive rate that is less than @xmath209 as the following theorem shows . the proof of this theorem is given in appendix [ app thm fdr ] .",
    "[ cond4 ]    1 .   for any @xmath9 ,",
    "let @xmath210 for @xmath211 . for a given @xmath9 , @xmath212 for some positive @xmath213 and @xmath214 and @xmath215 for some @xmath216 .",
    "2 .   for @xmath217 ,",
    "we have that @xmath218 .",
    "[ thm fdr ] under conditions [ cond2 ] , [ cond3 ] and [ cond4 ] , if we choose @xmath219 where @xmath220 and @xmath208 is the number of false positives that can be tolerated , then , for some constant @xmath221 it holds that @xmath222      random decoupling is an another procedure to select the thresholding parameter @xmath73 .",
    "it is used to create a null model , in which the data is formed by randomly permuting the rows of the last @xmath90 columns of the design matrix , while keeping the first @xmath38 columns of the design matrix intact .",
    "it is easy to see that by regressing @xmath6 on @xmath223 where the rows of the design matrix corresponding to @xmath5 ( @xmath224 ) have been randomly permuted , the obtained marginal values of @xmath225 is a statistical estimate of zero .",
    "these marginal estimates based on decoupled data measure the noise level of the estimates under the null model .",
    "let @xmath226 .",
    "if @xmath227 is used as the thresholding value , all variables will be screened out based on the permuted data , which leads to no false positives in this case .",
    "in other words , it is the minimum thresholding parameter that makes no false positives .",
    "however , this @xmath227 depends on the realization of the permutation . to stabilize the thresholding value",
    ", one can repeat this exercise @xmath228 times ( e.g. 5 or 10 times ) , resulting in the values @xmath229 @xmath230 , where @xmath231 .",
    "now , one can choose the maximum of @xmath230 , denoted by @xmath232 , as a thresholding value .",
    "a more stable choice is the @xmath233-quantile of the values in ( [ eq15 ] ) , denoted it by @xmath234 .",
    "a useful range for @xmath233 is @xmath235 $ ] .",
    "note that for @xmath236 , @xmath237 .",
    "the selected variables are then @xmath238 in our numerical implementations , we do coupling five times , i.e. @xmath239 , and take @xmath240 .",
    "a similar idea for unconditional sis appears already in @xcite for additive models .",
    "in this section , we demonstrate the performance of csis on simulated data and two empirical datasets . we compare csis versus sure independence screening and penalized least squares methods in a variety of settings .      in the simulation study",
    ", we compare the performance of the proposed csis with lasso ( @xcite ) and unconditional sis ( @xcite ) , in terms of variable screening .",
    "we vary the sample size from @xmath20 to @xmath241 for different scenarios and the number of predictors range from @xmath242 to @xmath243 .",
    "we present results with both the linear regression and the logistic regression .",
    "we evaluate different screening methods on @xmath244 simulated data sets based on the following criteria :    1 .",
    "mmms : median minimum model size of the selected models that are required to have a sure screening .",
    "the sampling variability of minimum model size ( mms ) is measured by the robust standard deviation ( rsd ) , which is defined as the associated interquartile range of mms divided by @xmath245 across 200 simulations .",
    "fp : average number of false positives across the 200 simulations , 3 .",
    "fn : average number of false negatives across 200 simulations .",
    "we consider two different methods for selecting thresholding parameters : controlling fdr and random decoupling as outlined in the previous section , and we present false negatives and false positives for each method .",
    "number of average false positives and false negatives are denoted by @xmath246 and @xmath247 for the random decoupling method and @xmath248 and @xmath249 for the fdr method . for the fdr method",
    ", we have chosen the number of tolerated false positives as @xmath250 .",
    "for the experiments with @xmath251 and @xmath252 , we do not report the corresponding results for lasso , since it is not proposed for variable screening , and the data - driven choice of regularization parameter for model selection is not necessarily optimal for variable screening .",
    "the first two simulated examples concern linear models introduced in the introduction , regarding the false positives and false negatives of unconditional sis .",
    "we report the simulation results in table  [ tab1 ] in which the column labeled `` * example 1 * '' refers to the first setting and column labeled `` * example 2 * '' referred to the second setting .",
    "these examples are designed to fail the unconditional sis .",
    "not surprisingly , sis performs poorly in sure screening the variables , and conditional sis easily resolves the problem . also , we note that csis needs only one additional variable to have sure screening , whereas lasso needs 15 additional variables .",
    "both the fdr and the random decoupling methods return no false negatives under almost all of the simulations .",
    "in other words , both of the data - driven thresholding methods ensured the sure screening property . however , they tend to be conservative , as the numbers of the false positives are high .",
    "the fdr approach has a relatively small number of false positives when used for conditional sure independent screening . for these settings ,",
    "fdr method was found to be less conservative than the random decoupling method .",
    ".the mmms , its rsd ( in parentheses ) , the `` false negative '' and `` false positive '' for the linear model with @xmath253 and @xmath242 . [ cols=\"^,^,^,^,^,^ \" , ]     ccccccc + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 300 & 215 ( 312 ) & 0.19 & 5.78 & 23.06 & 1.77 + 0.20 & 300 & 27 ( 14 ) & 73.22 & 0.02 & 109.56 & 0.00 + 0.40 & 300 & 49 ( 21 ) & 88.19 & 0.00 & 110.15 & 0.00 + 0.60 & 300 & 56 ( 20 ) & 88.17 & 0.00 & 110.00 & 0.00 + 0.80 & 300 & 68 ( 19 ) & 88.20 & 0.00 & 110.34 & 0.00 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 300 & 87 ( 173 ) & 20.15 & 1.24 & 24.03 & 1.11 + 0.20 & 300 & 19 ( 13 ) & 49.25 & 0.14 & 53.87 & 0.11 + 0.40 & 300 & 34 ( 23 ) & 67.82 & 0.17 & 61.72 & 0.31 + 0.60 & 300 & 43 ( 24 ) & 77.36 & 0.21 & 53.83 & 1.01 + 0.80 & 300 & 66 ( 55 ) & 78.33 & 0.51 & 36.16 & 3.42 +    ccccc + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 + 0.00 & 300 & 210 ( 312 ) & 20.18 & 0.08 + 0.20 & 300 & 28 ( 17 ) & 107.08 & 0.00 + 0.40 & 300 & 47 ( 24 ) & 107.82 & 0.00 + 0.60 & 300 & 60 ( 22 ) & 107.47 & 0.00 + 0.80 & 300 & 67 ( 19 ) & 107.30 & 0.00 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 + 0.00 & 300 & 83 ( 173 ) & 20.18 & 1.21 + 0.20 & 300 & 20 ( 14 ) & 45.27 & 0.20 + 0.40 & 300 & 39 ( 30 ) & 53.48 & 0.49 + 0.60 & 300 & 71 ( 87 ) & 49.47 & 1.15 + 0.80 & 300 & 402 ( 561 ) & 35.42 & 3.43 +    ccccccc + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 500 & 318 ( 7038 ) & 12.04 & 1.22 & 51.32 & 0.79 + 0.20 & 500 & 38 ( 428 ) & 32.47 & 0.57 & 68.46 & 0.38 + 0.40 & 500 & 38 ( 12 ) & 38.66 & 0.27 & 73.42 & 0.19 + 0.60 & 500 & 38 ( 12 ) & 41.99 & 0.16 & 76.11 & 0.10 + 0.80 & 500 & 35 ( 12 ) & 43.84 & 0.03 & 77.38 & 0.02 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 500 & 13 ( 354 ) & 5.96 & 0.66 & 42.51 & 0.49 + 0.20 & 500 & 15 ( 16 ) & 14.51 & 0.39 & 49.79 & 0.27 + 0.40 & 500 & 16 ( 13 ) & 19.11 & 0.24 & 51.68 & 0.22 + 0.60 & 500 & 19 ( 10 ) & 22.80 & 0.21 & 51.78 & 0.24 + 0.80 & 500 & 19 ( 10 ) & 26.39 & 0.14 & 46.49 & 0.64 +    ccccc + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 + 0.00 & 500 & 309 ( 7030 ) & 14.06 & 0.22 + 0.20 & 500 & 37 ( 255 ) & 34.10 & 0.09 + 0.40 & 500 & 35.5 ( 11 ) & 40.50 & 0.05 + 0.60 & 500 & 35.5 ( 12 ) & 42.89 & 0.03 + 0.80 & 500 & 33.5 ( 14 ) & 44.39 & 0.00 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 + 0.00 & 500 & 25 ( 892 ) & 5.96 & 0.14 + 0.20 & 500 & 13 ( 62 ) & 12.38 & 0.09 + 0.40 & 500 & 13 ( 22 ) & 14.17 & 0.08 + 0.60 & 500 & 15.5 ( 17 ) & 13.75 & 0.11 + 0.80 & 500 & 22 ( 72 ) & 9.30 & 0.28 +      in this section ,",
    "we evaluate the performance of csis under three different conditioning sets : the set consists of ( i ) only active variables , ( ii ) both active and inactive variables and ( iii ) only ( randomly chosen ) inactive variables .",
    "we consider a different correlation structure where the number of correlated variables is significantly large . for this experiment ,",
    "example 5 , we set @xmath256 and @xmath257 .",
    "we generate covariates from equation and choose the constants @xmath258 such that the correlation @xmath259 and @xmath260 among the first 2000 variables and @xmath261 .",
    "we fix @xmath262 .",
    "the following three conditioning sets are considered ( i ) @xmath263 ; ( ii ) @xmath264 and ( iii ) @xmath265\\{random choice of 4 inactive variables}. more precisely , @xmath266 consists of 3 randomly chosen variables from the first two thousand variables which are correlated and 1 randomly chosen inactive variable from the rest .",
    "note that variables 1 and 2 are active variables whereas variables 5 and 2001 are inactive .",
    "we have simulation results using both the conditional mle ( [ eq5 ] ) and conditional mlr ( [ eq6 ] ) . to save the space , we only present the results using the conditional mle for the normal model in table [ tabi1 ] and for the binomial model in table [ tabi3 ] .",
    "the results show clearly that the benefits of conditional screening are significant even when variables are wrongly chosen .",
    "csis reduces the minimum model size at least by half , and for most of the cases it uses 10 times as less variables as the unconditioning one .",
    "csis performs well even if some of the conditioned variables are inactive or even all are randomly selected inactive variables . for the worst cases ,",
    " mis - conditioning \" forced csis to recruit twice as many variables , and for most of the cases , the difference is not excessive . in all cases , csis performs significantly better than the unconditioning case .",
    "ccccccc + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 200 & 35 ( 80 ) & 98.20 & 0.28 & 20.16 & 0.63 + 0.20 & 200 & 1601 ( 812 ) & 1854.75 & 0.34 & 1537.35 & 0.51 + 0.40 & 200 & 2038 ( 267 ) & 2083.30 & 0.45 & 2010.73 & 0.63 + 0.60 & 200 & 2108 ( 470 ) & 2088.11 & 0.52 & 2010.59 & 0.73 + 0.80 & 200 & 2193 ( 663 ) & 2092.08 & 0.58 & 2010.59 & 0.83 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 200 & 6 ( 8) & 98.17 & 0.07 & 23.51 & 4.00 + 0.20 & 200 & 13 ( 47 ) & 440.33 & 0.04 & 143.85 & 3.90 + 0.40 & 200 & 75 ( 215 ) & 1001.84 & 0.03 & 336.05 & 3.67 + 0.60 & 200 & 216 ( 358 ) & 1372.48 & 0.01 & 379.81 & 3.64 + 0.80 & 200 & 423 ( 429 ) & 1518.04 & 0.00 & 234.19 & 3.79 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 200 & 6 ( 7 ) & 98.29 & 0.08 & 23.44 & 4.00 + 0.20 & 200 & 21 ( 75 ) & 565.76 & 0.03 & 212.80 & 3.75 + 0.40 & 200 & 152 ( 413 ) & 1367.95 & 0.03 & 642.06 & 3.33 + 0.60 & 200 & 443 ( 676 ) & 1766.88 & 0.01 & 830.50 & 3.12 + 0.80 & 200 & 868 ( 643 ) & 1860.01 & 0.00 & 594.86 & 3.40 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 200 & 44 ( 90 ) & 100.33 & 0.30 & 23.23 & 2.31 + 0.20 & 200 & 481 ( 687 ) & 1022.85 & 0.24 & 499.31 & 1.50 + 0.40 & 200 & 1322 ( 752 ) & 1806.40 & 0.20 & 1147.03 & 0.86 + 0.60 & 200 & 1652 ( 462 ) & 2003.43 & 0.10 & 1345.32 & 0.63 + 0.80 & 200 & 1716 ( 297 ) & 2037.08 & 0.03 & 1103.83 & 0.94 +    ccccccc + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 400 & 24 ( 59 ) & 97.39 & 0.21 & 27.29 & 0.48 + 0.20 & 400 & 1606 ( 776 ) & 1933.60 & 0.20 & 1725.60 & 0.39 + 0.40 & 400 & 2029 ( 101 ) & 2082.82 & 0.30 & 2016.35 & 0.52 + 0.60 & 400 & 2070 ( 258 ) & 2087.22 & 0.45 & 2015.59 & 0.64 + 0.80 & 400 & 2096 ( 429 ) & 2090.86 & 0.51 & 2015.07 & 0.66 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 400 & 8 ( 16 ) & 98.20 & 0.10 & 31.98 & 4.00 + 0.20 & 400 & 22 ( 75 ) & 361.04 & 0.10 & 138.73 & 3.85 + 0.40 & 400 & 107 ( 223 ) & 743.80 & 0.08 & 247.20 & 3.74 + 0.60 & 400 & 289 ( 439 ) & 1022.71 & 0.10 & 246.67 & 3.75 + 0.80 & 400 & 637 ( 528 ) & 1142.79 & 0.16 & 133.97 & 3.82 +   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 400 & 7 ( 17 ) & 98.33 & 0.11 & 31.31 & 4.00 + 0.20 & 400 & 27 ( 114 ) & 460.60 & 0.11 & 196.27 & 3.83 + 0.40 & 400 & 176 ( 429 ) & 1045.28 & 0.08 & 456.86 & 3.52 + 0.60 & 400 & 578 ( 759 ) & 1394.61 & 0.10 & 508.52 & 3.55 + 0.80 & 400 & 910 ( 673 ) & 1480.91 & 0.10 & 291.69 & 3.71",
    "+   + @xmath254 & @xmath1 & @xmath255 & @xmath246 & @xmath247 & @xmath248 & @xmath249 + 0.00 & 400 & 309 ( 919 ) & 100.00 & 0.89 & 14.83 & 2.69 + 0.20 & 400 & 777 ( 1129 ) & 529.20 & 0.66 & 149.64 & 2.12 + 0.40 & 400 & 1285 ( 1075 ) & 1087.79 & 0.56 & 333.27 & 1.96 + 0.60 & 400 & 1572 ( 977 ) & 1383.80 & 0.58 & 336.54 & 2.06 + 0.80 & 400 & 1629 ( 892 ) & 1485.02 & 0.57 & 178.37 & 2.79 +      in this section , we demonstrate how csis can be used to do variable selection with an empirical dataset .",
    "we consider the leukemia dataset which was first studied by @xcite and is available at http://www.broad.mit.edu/cgi-bin/cancer/datasets.cgi .",
    "the data come from a study of gene expression in two types of acute leukemias , acute lymphoblastic leukemia ( all ) and acute myeloid leukemia ( aml ) .",
    "gene expression levels were measured using affymetrix oligonucleotide arrays containing 7129 genes and 72 samples coming from two classes , namely 47 in class all and 25 in class aml . among these 72 samples , 38 ( 27 all and 11 aml )",
    "are set to be training samples and 34 ( 20 all and 14 aml ) are set as test samples . for this dataset we want to select the relevant genes , and based on the selected genes estimate whether the patient has all or aml .",
    "aml progresses very fast and has a poor prognosis .",
    "therefore , a consistent classification method that relies on gene expression levels would be very beneficial for the diagnosis .    in order to choose the conditioning genes ,",
    "we take a pair of genes described in @xcite that result in low test errors .",
    "first is zyxin and the second one is transcriptional activator hsnf2b .",
    "both genes have empirically high correlations for the difference between people with aml and all .    after conditioning on the aforementioned genes ,",
    "we implement our conditional selection procedure using logistic regression .",
    "using the random decoupling method , we select a single gene , tcrd ( t - cell receptor delta locus ) .",
    "although this gene has not been discovered by the all / aml studies so far , it is known to have a relation with t - cell all , a subgroup of all ( @xcite ) . by using only these three genes , we are able to obtain a training error of 0 out of 38 , and a test error of 1 out of 34 . similar studies in the past using sparse linear discriminant analysis or",
    "nearest shrunken centroids methods have obtained test errors of 1 by using more than 10 variables .",
    "we conjecture that this is due to the high correlation between the zyxin gene and others , and that this correlation masks the information contained in the tcrd gene .      in this section",
    "we illustrate the advantages of conditional sure independence screening on a factor model with financial data . from the website http://mba.tuck.dartmouth.edu/pages /faculty",
    "we obtain 30 portfolios formed with respect to their industries .",
    "the returns for each portfolio are denoted by @xmath267 ( for @xmath268 ) .",
    "the fama - french three - factor model suggests that these returns follow the following equation @xmath269 where @xmath270 is the excess return of the proxy market portfolio ( given by the difference of the one - month t - bill yield and the value weighted return of all stocks on nyse , amex and nasdaq ) , @xmath271 is the difference between the return of small and big companies ( measured by the difference of returns of two portfolios , one with companies that have small market cap and one with companies with large market cap ) and finally @xmath272 is the difference of return from value companies and growth companies .",
    "this model was first proposed by @xcite and has been extensively analyzed since then . since this seminal work ,",
    "many other factors have been considered . in our numerical example , we used screening with the permutation test to detect if other factors are necessary .",
    "besides the three factors mentioned above , we consider the momentum factor as an additional factor .",
    "this gives us 4 factors that are conditioned upon in csis .",
    "for each given industrial portfolio , we also consider the returns from the other 29 portfolios as potential prediction factors .",
    "we use daily returns data from 1/3/2002 to 12/31/2007 . for each portfolio",
    "( 30 in total ) , we first consider the marginal screening without conditioning . on average , for each portfolio , marginal screening picks 25.3 among 29 other industrial portfolios as predictors .",
    "this is mainly due to correlations between the returns of different portfolios .",
    "we next consider conditional marginal screening , in which the three fama - french factors and the momentum factor are conditioned upon .",
    "as expected , the number of the selected variables decreases significantly to an average of 4.8 .",
    "that is , about 4.8 portfolios on average can still have some potential prediction power in presence of the aforementioned four major factors .",
    "the marginal and conditional fits of the values are given in figure [ fig factors ] .",
    "the black parts indicate the variables which are not included .",
    "it is seen from these results that , conditional screening is more advantageous compared to marginal screening if few of the factors are known to be important .",
    "furthermore , when there is significant correlation between some of the factors , as shown in the introduction , marginal screening considers most of the factors as relevant . in almost all financial models ,",
    "stock returns are correlated with the return of the market portfolio .",
    "therefore , in variable selection for financial factor models with many variables , one should always consider the returns conditional on the main driving forces of the market .",
    "the necessary part has already been proven in section 3.1 . to prove the sufficient condition , we first note that condition @xmath110 is equivalent to @xmath273 as shown in section 3.1 .",
    "this and ( [ eq11 ] ) imply that @xmath274 is a solution to equation ( [ eq9 ] ) . by the uniqueness",
    ", it follows that @xmath275 , namely @xmath276 .",
    "this completes the proof .",
    "we denote the matrix @xmath277 as @xmath278 and partition it as @xmath279= \\left [ \\begin{array}{ccc } \\omega_{\\mc,\\mc } & \\omega_{\\mc , j}\\\\ \\omega_{\\mc , j}^t & \\omega_{j , j } \\end{array } \\right].\\ ] ] from the score equations , i.e. equations ( [ eq9 ] ) and ( [ eq11 ] ) , we have that @xmath280 using the definition of @xmath119 , the above equation can be written as @xmath281 by letting @xmath282 , we have that @xmath283 or equivalently @xmath284 furthermore , by ( [ eq13 ] ) , we can express @xmath285 as @xmath286 it follows from ( [ eq12 ] ) that @xmath287 using the definition of @xmath119 again , we have @xmath288 by ( [ eqa2 ] ) , we conclude that @xmath289              1 .",
    "the fisher information @xmath295\\left[\\frac{\\partial}{\\partial{\\mbox{\\boldmath $ \\beta$}}}l\\left({\\mbox{\\bf x}}^{t}{\\mbox{\\boldmath $ \\beta$}},y\\right)\\right]^{t}\\right\\ } , \\ ] ] is finite and positive definite at @xmath296 .",
    "furthermore , @xmath297 exists .",
    "2 .   the function @xmath298 is lipschitz with a positive constant @xmath299 for any @xmath147 in @xmath300 , and @xmath301 in @xmath302 with @xmath303 and @xmath304 arbitrarily large constants .",
    "furthermore , there exists a constant @xmath305 such that @xmath306\\left(1-i_{n}\\left({\\mbox{\\bf x}},y\\right)\\right)\\right|\\leq o\\left(p / n\\right),\\ ] ] where @xmath307 with constant @xmath308 defined below .",
    "the function @xmath309 is convex in @xmath147 and @xmath310\\right|\\geq v_{n}\\left\\vert { \\mbox{\\boldmath $ \\beta$}}-{\\mbox{\\boldmath $ \\beta$}}_{0}\\right\\vert ^{2},\\ ] ] for some positive constants @xmath311 , and all @xmath312 .        by lemma 1 of @xcite ,",
    "condition  [ cond2](ii ) gives the bound @xmath315 hence , we have @xmath316 using this and theorem [ thm quasi ] , letting @xmath317 , we have @xmath318 for some positive constant @xmath319 .",
    "then , by bonferroni s inequality , we obtain @xmath320 this proves the first conclusion .",
    "the second statement can be shown by considering the event @xmath321 on the event @xmath322 , by theorem [ thm2 ] , it holds that for all @xmath115 @xmath323 by letting @xmath324 , on the event @xmath325 we have the sure screening property , that is @xmath326 .",
    "the probability bound can be shown by using the first result along with bonferroni s inequality over all chosen @xmath9 , which gives @xmath327.\\ ] ] this completes the proof .",
    "the first part of the proof is similar to that of theorem 5 of @xcite .",
    "the idea of this proof is to show that @xmath328 if this holds , the size of the set @xmath329 can not exceed @xmath330 for any @xmath331 .",
    "thus on the event @xmath332 the set @xmath333 is a subset of the set @xmath329 , whose size is bounded by @xmath334 . if we take @xmath335 , we obtain that @xmath336 finally , by theorem [ thm3 ] , we obtain that @xmath337 and therefore the statement of the theorem follows .",
    "we now prove by using @xmath338 and ( [ eqa5 ] ) . by condition  [ cond3](ii ) ,",
    "the schur s complement @xmath339 is uniformly bounded from below .",
    "therefore , by ( [ eqa5 ] ) , we have @xmath340 for a positive constant @xmath341 .",
    "hence , we need only to bound the conditional covariance .    by ( [ eqa4 ] ) , ( [ eq9 ] ) and lipschitz continuity of @xmath169",
    ", we have @xmath342 \\bigr |.\\end{aligned}\\ ] ] where @xmath343 .",
    "writing the last term in the vector form , we need to bound @xmath344 from the property of the least - squares , we have @xmath345 = \\operatorname{\\mathbb{e } } [ { \\mbox{\\bf x}}_{{\\mathcal{d } } }    { \\mbox{\\bf x}}_{{\\mathcal{c}}}^t ] $ ] .",
    "thus the above expression can be written as @xmath346 { \\mbox{\\boldmath $ \\beta$}}_{{\\mathcal{d}}}^{\\star } + \\operatorname{\\mathbb{e}}\\operatorname{\\mathbb{e}}_l ( { \\mbox{\\bf x}}_{{\\mathcal{d } } } |   { \\mbox{\\bf x}}_{{\\mathcal{c } } } )   [ { \\mbox{\\bf x}}_{{\\mathcal{c}}}^{t}{\\mbox{\\boldmath $ \\beta$}}_{{\\mathcal{c}}}^\\delta + \\operatorname{\\mathbb{e}}_l({\\mbox{\\bf x}}_{{\\mathcal{d}}}^{t}|{\\mbox{\\bf x}}_{{\\mathcal{c } } } ) { \\mbox{\\boldmath $ \\beta$}}_{{\\mathcal{d}}}^ * ) ] \\| & = & \\left\\|   \\big[{\\mbox{\\boldmath $ \\sigma$}}_{{\\mathcal{d}}|{\\mathcal{c } } } \\big ] { \\mbox{\\boldmath $ \\beta$}}_{{\\mathcal{d}}}^\\star + { \\mbox{\\bf z}}\\right\\|^2,\\end{aligned}\\ ] ] recalling the definition of @xmath347 in condition [ cond3 ] .",
    "using the law of total variance , we have that @xmath348 \\bbeta_{\\md}^\\star + \\bz \\right\\|^2&=&{\\bbeta_{\\md}^\\star}^t   \\big[\\bsigma_{\\md|\\mc } \\big]^2\\bbeta_{\\md}^\\star + 2 \\bz^t   \\big[\\bsigma_{\\md|\\mc } \\big ] + \\bz^t \\bz\\\\ & \\leq&\\lambda_{\\max } \\lp \\big[\\bsigma_{\\md|\\mc } \\big ] \\rp \\lp { \\bbeta_{\\md}^\\star}^t   \\big[\\bsigma_{\\md|\\mc } \\big]\\bbeta_{\\md}^\\star\\rp + 2 \\bz^t   \\big[\\bsigma_{\\md|\\mc } \\big ] + \\bz^t \\bz \\\\ & \\leq&\\lambda_{\\max } \\lp \\big[\\bsigma_{\\md|\\mc } \\big ] \\rp \\var(\\bx^t \\bbeta^\\star)+ 2 \\bz^t   \\big[\\bsigma_{\\md|\\mc } \\big ] + \\bz^t \\bz,\\end{aligned}\\ ] ] and the last two terms are @xmath349 { \\right)}{\\right)}$ ] due to condition [ cond3 ] .",
    "therefore , we have that @xmath350 { \\right)}{\\right)},\\ ] ] and that gives us the desired result .        with the given conditions , by theorem 1 , we have @xmath276 . since @xmath2 includes the intercept term , @xmath352 .",
    "it is known that @xmath353 ( for @xmath354 ) has an asymptotically standard normal distribution ( gao et al . , 2008 , heyde , 1997 ) .",
    "then , it follows that for a @xmath221 @xmath355                fama , e.f . , and french , k.r .",
    "( 1993 ) ,  common risk factors in the returns on stocks and bonds , \"  33 , 356 .",
    "fan , j. , feng , y. , and song , r. ( 2011 ) ,  nonparametric independence screening in sparse ultra - high - dimensional additive models , \"  106 , 544557 .",
    "golub , t. , slonim , d. , tamayo , p. , huard , c. , gaasenbeek , m. , mesirov , j. , coller , h. , loh , m. , downing , j. , caligiuri , m. , bloomfield , c. , and lander , e. ( 1999 ) ,  molecular classification of cancer : class discovery and class prediction by gene expression monitoring , \"  286 , 531537 .",
    "szczepaski , t. , van der velden , v.h . ,",
    "raff , t. , jacobs , d.c . ,",
    "van wering , e.r . ,",
    "brggemann , m. , kneba , m. , and van dongen , j.j .",
    "( 2003 ) ,  comparative analysis of t - cell receptor gene rearrangements at diagnosis and relapse of t - cell acute lymphoblastic leukemia ( t - all ) shows high stability of clonal markers for monitoring of minimal residual disease and reveals the occurrence of second t - all , \"  17 , 21492156 ."
  ],
  "abstract_text": [
    "<S> independence screening is a powerful method for variable selection for ` big data ' when the number of variables is massive . </S>",
    "<S> commonly used independence screening methods are based on marginal correlations or variations of it . in many applications , researchers often have some prior knowledge that a certain set of variables is related to the response . in such a situation , </S>",
    "<S> a natural assessment on the relative importance of the other predictors is the conditional contributions of the individual predictors in presence of the known set of variables . </S>",
    "<S> this results in conditional sure independence screening ( csis ) . </S>",
    "<S> conditioning helps for reducing the false positive and the false negative rates in the variable selection process . in this paper </S>",
    "<S> , we propose and study csis in the context of generalized linear models . for ultrahigh - dimensional statistical problems , </S>",
    "<S> we give conditions under which sure screening is possible and derive an upper bound on the number of selected variables . </S>",
    "<S> we also spell out the situation under which csis yields model selection consistency . </S>",
    "<S> moreover , we provide two data - driven methods to select the thresholding parameter of conditional screening . </S>",
    "<S> the utility of the procedure is illustrated by simulation studies and analysis of two real data sets .    </S>",
    "<S> _ keywords and phrases _ : false selection rate ; generalized linear models ; sparsity ; sure screening ; variable selection . </S>"
  ]
}