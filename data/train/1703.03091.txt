{
  "article_text": [
    "deep learning methods are becoming important due to their demonstrated success at tackling complex learning problems . at the same time , increasing access to high - performance computing resources and state - of - the - art open - source libraries are making it more and more feasible for everyone to use these methods .",
    "natural language processing focuses on the interactions between human language and computers .",
    "it sits at the intersection of computer science , artificial intelligence , and computational linguistics .",
    "nlp is a way for computers to analyze , understand , and derive meaning from human language in a smart and useful way . by utilizing nlp , developers can organize and structure knowledge to perform tasks such as automatic summarization , translation , named entity recognition , relationship extraction , sentiment analysis , speech recognition , and topic segmentation .",
    "the development of nlp applications is challenging because computers traditionally require humans to communicate to them via a programming language .",
    "programming languages are precise , unambiguous and highly structured . human speech , however , is not always precise , it is often ambiguous and the linguistic structure can depend on many complex variables , including slang , regional dialects and social context .",
    "a neural network is a biologically - inspired programming paradigm which enables a computer to learn from observed data .",
    "it is composed of a large number of interconnected processing elements , neurons , working in unison to solve a problem .",
    "an ann is configured for a specific application , such as pattern recognition or data classification , through a learning process .",
    "an ann consists of three parts or layers : the input layer , a hidden layer and the output layer .",
    "convolutional neural networks are very similar to ordinary neural networks .",
    "they are also made up of neurons that have learnable weights and biases .",
    "the main difference is the number of layers .",
    "cnn are just several layers of convolutions with nonlinear activation functions applied to the results . in a traditional nn",
    "each input neuron is connected to each output neuron in the next layer .",
    "that is called a fully connected layer . in cnns , instead , convolutions are used over the input layer to compute the output .",
    "this results in local connections , where each region of the input is connected to a neuron in the output .",
    "each layer applies different filters , typically hundreds or thousands and combines their results .",
    "a key aspect of convolutional neural networks is the use of pooling layers , typically applied after the convolutional layers .",
    "pooling layers subsample their input . the most common way to perform pooling it to apply a max operation to the result of each filter .",
    "the pooling process can also be applied over a window .",
    "there are two main reasons to perform pooling .",
    "one property of pooling is that it provides a fixed size output matrix , which typically is required for classification .",
    "this allows the use of variable size sentences , and variable size filters , but always obtaining the same output dimensions to feed into a classifier .",
    "pooling also reduces the output dimensionality while keeping the most salient information .",
    "you can think of each filter as detecting a specific feature .",
    "if this feature occurs somewhere in the sentence , the result of applying the filter to that region will yield a large value , but a small value in other regions . by performing the max operation information",
    "is kept about whether or not the feature appeared in the sentence , but information is lost about where exactly it appeared . resuming , global information about locality",
    "is lost ( where in a sentence something happens ) , but local information is kept since it is captured by the filters .    during the training phase , a cnn automatically learns the values of its filters based on the task that to be performed .",
    "for example , in image classification a cnn may learn to detect edges from raw pixels in the first layer , then use the edges to detect simple shapes in the second layer , and then use these shapes to deter higher - level features , such as facial shapes in higher layers .",
    "the last layer is then a classifier that uses these high - level features .    instead of image pixels , the input to most nlp tasks are sentences or documents represented as a matrix .",
    "each row of the matrix corresponds to one token , typically a word , but it could be a character . that is , each row is vector that represents a word .",
    "typically , these vectors are word embeddings ( low - dimensional representations ) , but they could also be one - hot vectors that index the word into a vocabulary . for a 10 word sentence using a 100-dimensional embedding we would have a 10x100 matrix as our input .        in computer vision , the filters slide over local patches of an image , but in nlp filters slide over full rows of the matrix ( words ) .",
    "thus , the width of the filters is usually the same as the width of the input matrix .",
    "the height , or region size , may vary , but sliding windows over 2 - 5 words at a time is the typical size .",
    "in this paper , bitvai et al . compare the efficiency of an cnn over an ann .",
    "they consider problem of predicting the future box - office takings of movies based on reviews by movie critics and movie attributes .",
    "an artificial neural network ( ann ) is proposed for modelling text regression . in language processing ,",
    "anns were first proposed for probabilistic language modelling , followed by models of sentences and parsing inter alia .",
    "these approaches have shown strong results through automatic learning dense low - dimensional distributed representations for words and other linguistic units , which have been shown to encode important aspects of language syntax and semantics .",
    "they also develop a convolutional neural network , inspired by their breakthrough results in image processing and recent applications to language processing .",
    "past works have mainly focused on ?",
    "problems with plentiful training examples . given the large numbers of parameters , often in the millions",
    ", one would expect that such models can only be effectively learned on very large datasets .",
    "however in this paper they show that a complex deep convolution network can be trained on about a thousand training examples , although careful model design and regularisation is paramount .",
    "they consider the problem of predicting the future box - office takings of movies based on reviews by movie critics and movie attributes .",
    "their approach is based on the method and dataset of joshi et al .",
    "( 2010 ) , who presented a linear regression model over uni- , bi- , and tri - gram term frequency counts extracted from reviews , as well as movie and reviewer metadata .",
    "this problem is especially interesting , as comparatively few instances are available for training while each instance ( movie ) includes a rich array of data including the text of several critic reviews from various review sites , as well as structured data ( genre , rating , actors , etc . )",
    "inspired by joshi et al .",
    "( 2010 ) their model also operates over n - grams , 1 ?",
    "n ? 3 , and movie metadata , using an ann instead of a linear model .",
    "they use word embeddings to represent words in a low dimensional space , a convolutional network with max - pooling to represent documents in terms of n - grams , and several fully connected hidden layers to allow for learning of complex non - linear interactions .",
    "they show that including non - linearities in the model is crucial for accurate modelling , providing a relative error reduction of 40 per cent ( mae ) over the best linear model .",
    "their final contribution is a novel means of model interpretation .",
    "although it is notoriously difficult to interpret the parameters of an ann , they show a simple method of quantifying the effect of text n - grams on the prediction output .",
    "this allows for identification of the most important textual inputs , and investigation of non - linear interactions between these words and phrases in different data instances .",
    "the idea behind rnns is to make use of sequential information . in a traditional neural network",
    "all inputs ( and outputs ) are independent of each other . but for many tasks that results in a bad performance .",
    "if the next word in a sentence is going to be predicted , there is the need know which words came before it .",
    "rnns are called recurrent because they perform the same task for every element of a sequence , with the output being depended on the previous computations . another way to think about rnns",
    "is that they have a memory which captures information about what has been calculated so far .",
    "theoretically rnns can make use of information in arbitrarily long sequences , but in practice they are limited to looking back only a few steps . in figure 4",
    "we can see what a typical rnn looks like .        over the years",
    "researchers have developed more sophisticated types of rnns to deal with some of the shortcomings of the original rnn model .",
    "bidirectional rnns are based on the idea that the output at time t may not only depend on the previous elements in the sequence , but also future elements .",
    "for example , to predict a missing word in a sequence you want to look at both the left and the right context .",
    "bidirectional rnns are quite simple .",
    "they are just two rnns stacked on top of each other . the output is then computed based on the hidden state of both rnns .",
    "deep ( bidirectional ) rnns are similar to bidirectional rnns , only that we now have multiple layers per time step . in practice",
    "this gives us a higher learning capacity ( but we also need a lot of training data ) .",
    "lstms don?t have a fundamentally different architecture from rnns , but they use a different function to compute the hidden state . the memory in lstms are called cells and you can think of them as black boxes that take as input the previous state and the current input . internally these cells decide what to keep in ( and what to erase from ) memory .",
    "they then combine the previous state , the current memory , and the input .",
    "it turns out that these types of units are very efficient at capturing long - term dependencies .",
    "a recursive neural network ( rnn or rcnn ) is a deep neural network created by applying the same set of weights recursively over a structure , to produce a structured prediction over the input , or a scalar prediction on it , by traversing a given structure in topological order .",
    "rnns have been successful in learning sequence and tree structures in natural language processing , mainly phrase and sentence continuous representations based on word embedding .",
    "rnn is a general architecture to model the distributed representations of a phrase or sentence with its dependency tree .",
    "it can be regarded as semantic modelling of text sequences and handle the input sequences of varying length into a fixed - length vector .",
    "the parameters in rcnn can be learned jointly with some other nlp tasks , such as text classification .",
    "each rnn unit can model the complicated interactions of the head word and its children .",
    "combined with a specific task , rnn can capture the most useful semantic and structure information by the convolution and pooling layers .",
    "recursive neural networks , comprise a class of architecture that operates on structured inputs , and in particular , on directed acyclic graphs .",
    "a recursive neural network can be seen as a generalization of the recurrent neural network , which has a specific type of skewed tree structure .",
    "they have been applied to parsing , sentence - level sentiment analysis , and paraphrase detection .",
    "given the structural representation of a sentence , e.g. a parse tree , they recursively generate parent representations in a bottom - up fashion , by combining tokens to produce representations for phrases , eventually producing the whole sentence . the sentence - level representation ( or , alternatively , its phrases ) can then be used to make a final classification for a given input sentence .",
    "similar to how recurrent neural networks are deep in time , recursive neural networks are deep in structure , because of the repeated application of recursive connections .",
    "recently , the notions of depth in time the result of recurrent connections , and depth in space the result of stacking multiple layers on top of one another , are distinguished for recurrent neural networks . in order to combine these concepts , deep recurrent networks were proposed .",
    "they are constructed by stacking multiple recurrent layers on top of each other , which allows this extra notion of depth to be incorporated into temporal processing .",
    "empirical investigations showed that this results in a natural hierarchy for how the information is processed .          in order to capture long - distance dependencies a dependency - based convolution model ( dcnn )",
    "is proposed .",
    "dcnn consists of a convolutional layer built on top of long short - term memory ( lstm ) networks .",
    "dcnn takes slightly different forms depending on its input . for a single sentence ,",
    "the lstm network processes the sequence of word embeddings to capture long - distance dependencies within the sentence .",
    "the hidden states of the lstm are extracted to form the low - level representation , and a convolutional layer with variable - size filters and max - pooling operators follows to extract task - specific features for classification purposes . as for document modeling",
    ", dcnn first applies independent lstm networks to each subsentence .",
    "then a second lstm layer is added between the first lstm layer and the convolutional layer to encode the dependency across different sentences .",
    "dynamic k - max pooling is a generalization of the max pooling operator .",
    "the max pooling operator is a non - linear subsampling function that returns the maximum of a set of values .",
    "the operator is generalized in two respects .",
    "first , k - max pooling over a linear sequence of values returns the subsequence of k maximum values in the sequence , instead of the single maximum value .",
    "secondly , the pooling parameter k can be dynamically chosen by making k a function of other aspects of the network or the input .",
    "the convolutional layers apply one - dimensional filters across each row of features in the sentence matrix . convolving the same filter with the n - gram at every position in the sentence",
    "allows the features to be extracted independently of their position in the sentence .",
    "a convolutional layer followed by a dynamic pooling layer and a non - linearity form a feature map . like in the convolutional networks for object recognition ( lecun et al . ,",
    "1998 ) , the representation is enriched in the first layer by computing multiple feature maps with different filters applied to the input sentence .",
    "subsequent layers also have multiple feature maps computed by convolving filters with all the maps from the layer below .",
    "the weights at these layers form an order-4 tensor .",
    "the resulting architecture is dubbed a dynamic convolutional neural network .",
    "multiple layers of convolutional and dynamic pooling operations induce a structured feature graph over the input sentence .",
    "insert figure .",
    "figure 10 illustrates such a graph .",
    "small filters at higher layers can capture syntactic or semantic relations between noncontinuous phrases that are far apart in the input sentence .",
    "the feature graph induces a hierarchical structure somewhat akin to that in a syntactic parse tree .",
    "the structure is not tied to purely syntactic relations and is internal to the neural network .",
    "this model shares the same word embeddings , and s multiple columns of convolutional neural networks .",
    "the number of columns usually used is three , but it can have more or less depending on the context in which it has to be used .",
    "these columns are used to analyze different aspects of a question , i.e. , answer path , answer context , and answer type .",
    "typically this framework is combined with the learning of embeddings .",
    "the overview of this framework is shown in figure 11 .",
    "for instance , for the question when did avatar release in uk , the related nodes of the entity avatar are queried from freebase .",
    "these related nodes are regarded as candidate answers ( cq ) .",
    "then , for every candidate answer a , the model predicts a score s ( q , a ) to determine whether it is a correct answer or not .                the model architecture , shown in figure x , is a variant of the convolutional architecture of hu et al .",
    "it consists of two components : ?",
    "convolutional sentence model that summarizes the meaning of the source sentence and the target phrase ; ? matching model that compares the two representations with a multi - layer perceptron ( bengio , 2009 ) .",
    "let e be a target phrase and f be the source sentence that contains the source phrase aligning to e. first of all f and e are projected into feature vectors x and y via the convolutional sentence model , and then the matching score s(x , y ) is computed by the matching model .",
    "finally , the score is introduced into a conventional smt system as an additional feature .",
    "convolutional sentence model .",
    "as shown in figure 13 , the model takes as input the embeddings of words ( trained beforehand elsewhere ) in f and e. it then iteratively summarizes the meaning of the input through layers of convolution and pooling , until reaching a fixed length vectorial representation in the final layer . in layer-1 ,",
    "the convolution layer takes sliding windows on f and e respectively , and models all the possible compositions of neighbouring words .",
    "the convolution involves a filter to produce a new feature for each possible composition .",
    "in this paper , zhu et al . propose a recursive convolutional neural network ( rcnn ) architecture to capture syntactic and compositional - semantic representations of phrases and words .",
    "rcnn is a general architecture and can deal with k - ary parsing tree , therefore it is very suitable for dependency parsing . for each node in a given dependency tree",
    ", they first use a rcnn unit to model the interactions between it and each of its children and choose the most informative features by a pooling layer .",
    "thus , the rcnn unit can be applied recursively to get the vector representation of the whole dependency tree .",
    "the output of each rcnn unit is used as the input of the rcnn unit of its parent node , until it outputs a single fixed - length vector at root node . when applied to the re - ranking model for parsing ,",
    "rcnn improve the accuracy of base parser to make accurate parsing decisions .",
    "the experiments on two benchmark datasets show that rcnn outperforms the state - of - the - art models .",
    "the results obtained for this paper can be seen in table 1 .",
    ".results [ cols=\"<,^ \" , ]",
    "this paper presents state - of - the - art deep learning tools for natural language processing . the main contributions of this work are??an overview of cnn and its different subtypes .",
    "a get together of all the problems that have been solved using state - of - the - art cnn technologies . a general view of how cnn have been applied to different nlp problems , with results included .",
    "after the advances made in computer vision using deep learning tools , nlp has adapted some of these techniques to make major breakthroughs .",
    "however , the results , for now , are only promising .",
    "there is evidence that deep learning tools provide good solutions , but they havent provided such a big leap as the one in computer vision .",
    "one of the main problems is that cnn started being used because of the great success in cv . due to this",
    "there s a lack of a common goal .",
    "this uncertainty of what to do causes the results to be good but not as good as expected .",
    "one of the reasons could be because cnn are thought to be applied to images and not to words .",
    "however , the results and all the ... are encouraging and are an improvement over the previous state - of - the - art techniques .",
    "there s a need to define common goals and set a better use of cnn .",
    "convolutional neural networks are designed to be used on images .",
    "missing component ( 2d-3d )    speech recognition seems the area with the best results ( maybe because it s one of the areas that concerns a bigger number of people ) .",
    "try to see the model they have used and adapt it to the problem the author is trying to solve .",
    "the authors would like to thank ...        c. zhu , x. qiu , x. chen , and x. huang , _ a re - ranking model for dependency parser with recursive convolutional neural network _ , proc .",
    "53rd annu .",
    "comput . linguist .",
    "lang . process .",
    "volume 1 long pap . , pp . 1159?1168 , 2015 .",
    "t. h. nguyen and r. grishman , _ event detection and domain adaptation with convolutional neural networks _ , proc .",
    "53rd annu .",
    "comput . linguist .",
    "lang . process .",
    "( volume 2 short pap . , pp . 365?371 , 2015 .",
    "m. denil , a. demiraj , n. kalchbrenner , p. blunsom , and n. de freitas , _ modelling , visualising and summarising documents with a single convolutional neural network _ , arxiv prepr .",
    "arxiv1406.3830 , pp . 1?10 , 2014 .",
    "b. hu , z. tu , z. lu , and q. chen , _ context - dependent translation selection using convolutional neural network _ , proc .",
    "53rd annu .",
    "comput . linguist .",
    "lang . process .",
    "( volume 1 long pap .",
    "section 3 , pp . 536?541 , 2015 .",
    "f. meng , z. lu , m. wang , h. li , w. jiang , and q. liu , _ encoding source language with convolutional neural network for machine translation _ , proc .",
    "53rd annu .",
    "comput . linguist .",
    "lang . process .",
    "( volume 1 long pap . , pp . 20?30 , 2015 .",
    "d. palaz , m. magimai - doss , and r. collobert , _ analysis of cnn - based speech recognition system using raw speech as input _ ,",
    "speech commun .",
    "interspeech , vol .",
    "2015-janua , pp . 11?15 , 2015 .",
    "o. abdel - hamid , h. jiang , and g. penn _ applying convolutional neural networks concepts to hybrid nn - hmm model for speech recognition _ , department of computer science and engineering , york university , toronto , canada department of computer science , university of toronto , toronto , canada , acoust .",
    "speech signal process .",
    "( icassp ) , 2012 ieee int .",
    "conf . , pp . 4277?4280 , 2012 .",
    "p. golik , z. tuske , r. schuler , and h. ney , convolutional neural networks for acoustic modeling of raw time signal in lvcsr , proc .",
    "speech commun .",
    "interspeech , vol . 2015-janua , pp .",
    "26 - 30 , 2015 .",
    "s. thomas , s. ganapathy , g. saon , and h. soltau , ?",
    "analyzing convolutional neural networks for speech activity detection in mismatched acoustic conditions , ?",
    "icassp , ieee int .",
    "speech signal process .",
    "- proc . , pp .",
    "2519?2523 , 2014 ."
  ],
  "abstract_text": [
    "<S> convolutional neural network ( cnns ) are typically associated with computer vision . </S>",
    "<S> cnns are responsible for major breakthroughs in image classification and are the core of most computer vision systems today . </S>",
    "<S> more recently cnns have been applied to problems in natural language processing and gotten some interesting results . in this paper </S>",
    "<S> , we will try to explain the basics of cnns , its different variations and how they have been applied to nlp .    </S>",
    "<S> convolutional neural network , natural language . </S>"
  ]
}