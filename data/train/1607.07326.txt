{
  "article_text": [
    "in the recent years , online commerce outpaced the growth of traditional commerce , with a rate of growth of 15% in 2015 and accounting for $ 1.5 trillion spend in 2014 .",
    "the research work on recommender systems has consequently grown significantly during the last couples of years . as shown by key players in the online e - commerce space , such as amazon , netflix and alibaba",
    ", the product recommendation functionality is now a key driver of demand , accounting in the case of amazon for roughly @xmath0 @xcite of the overall sales .    as of now ,",
    "the state - of - the - art recommendation methods include matrix factorization techniques of the item - item and user - item matrices that differ in the choice of weighting schemes of the matrix entries , the reconstruction loss functions and their choice with regards to the use of additional item content information .",
    "the real - world recommender systems have additional constraints that inform their architecture . some of these major constraints include : scaling the recommender systems such that they can handle a huge amount of user interaction information , supporting real - time changes in recommendation @xcite and handling the cold - start problem @xcite .    in the last couple of years , a promising new class of neural probabilistic models that can generate user and product embeddings has emerged and",
    "has shown promising results .",
    "the new methods can scale to millions of items and show good improvements on the cold - start problem . in the context of product recommendation ,",
    "they were successfully applied to ad recommendations in yahoo !",
    "mail @xcite , for restaurant recommendations by opentable @xcite and in the 1st prize winners in the 2015 recsys challenge @xcite .    in this paper , we present an extension of the _ prod2vec _ algorithm initially proposed in @xcite .",
    "prod2vec _ algorithm only uses the local product co - occurrence information established by the product sequences to create distributed representations of products , but does not leverage their metadata . the authors have proposed an extension @xcite of the algorithm that takes into account the textual content information together with the sequential structure , but the approach is specific to textual metadata and the resulting architecture is hierarchical , therefore missing some of the side information terms by comparison with our method . in this work ,",
    "we make the connection with the work on recommendation using side information and propose _ meta - prod2vec _ , which is a general approach for adding categorical side - information to the _",
    "prod2vec _ model in a simple and efficient way .",
    "the usage of additional item information as side information - only , e.g. available only at training time , is motivated by real - world constraints on the number of feature values a recommender system can keep in memory for real - time scoring . in this case , using the metadata only at training time keeps the memory footprint constant ( assuming an existing recommendation system that uses item embeddings ) while improving the online performance .",
    "we show that our approach significantly improves recommendation performance on a subset of the 30music listening and playlists dataset @xcite with a low implementation and integration cost .    in section [ sec : relatedwork ]",
    "we cover previous related work and the relationship with our method .",
    "in section [ sec : proposedapproach ] we present the _ meta - prod2vec _ approach . in section [ sec : experiments ] we present the experimental setup and the results on the 30music dataset . in section [ sec : conclusion ] we summarize our findings and conclude with future directions of research .",
    "existing methods for recommender systems can roughly be categorized into collaborative filtering ( cf ) based methods , content - based ( cb ) methods and hybrid methods .",
    "cf - based methods @xcite are based on user s interaction with items , such as clicks , and do nt require domain knowledge .",
    "content - based methods make use of the user or product content profiles . in practice ,",
    "cf methods are more popular because they can discover interesting associations between products without requiring the heavy knowledge collection needed by the content - based methods .",
    "however , cf methods suffer from cold - start problem in which no or few interaction are available with niche or new items in the system . in recent years , more sophisticated methods ,",
    "namely latent factor models , have been developed to address the data sparsity problem of cf methods which we will discuss in section [ subsec : lfm ] . to further help overcoming cold - start problem , recent works focused on developing hybrid methods by combining latent factor models with content information which we will cover in section [ subsec : hybrid ] .",
    "matrix factorization ( mf ) methods @xcite became popular after their success in the netflix competition .",
    "these methods learn low - rank decompositions of a sparse user - item interaction matrix by minimizing the square loss over the reconstruction error .",
    "the dot product between the resulting user and item latent vectors is then used to perform recommendation .",
    "several modifications have been proposed to better align mf methods with the recommendation objective , for instance , bayesian personalized ranking @xcite and logistic mf @xcite . the former learns user and item latent vectors through pairwise ranking loss to emphasize the relevance - based ranking of items .",
    "the latter models the probability that a user would interact with an item by replacing the square loss in mf method with the logistic loss @xcite .",
    "one of the first methods that learns user and item latent representations through neural network was proposed in @xcite .",
    "the authors utilized restricted boltzmann machines to explain user - item interaction and perform recommendations . recently",
    ", shallow neural networks has been gaining attention thanks to the success of word embeddings in various nlp tasks , the focus being on word2vec model @xcite .",
    "an application of word2vec to the recommendation task was proposed in @xcite , called prod2vec model .",
    "it generates product embeddings from sequences of purchases and performs recommendation based on most similar products in the embedding space .",
    "our work is an extension of prod2vec and we will present its details in section [ sec : proposedapproach : prod2vec ] .",
    "many techniques have been used recently to create unified representations from latent factors and content information .",
    "one way to integrate user and item content information is to use it to estimate user and item latent factors through regression @xcite .",
    "another approach is to learn latent factors for both cf and content features , known as factorization machines @xcite .",
    "tensor factorization have been suggested as a generalization of mf for considering additional information @xcite . in this approach",
    ", user - item - content matrix is factorized in a common latent space .",
    "the authors in @xcite propose co - factorization approach where the latent user and item factors are shared between factorizations of user - item matrix and user and item content matrices .",
    "similar to @xcite , they also assigned weights to negative examples based on user - item content - based dissimilarity .",
    "graph - based models have also been used to create unified representations . in particular , in @xcite user - item interactions and side information are modeled jointly through user and item latent factors .",
    "user factors are shared by the user - item interaction component and the side information component .",
    "gunawardana et al .",
    "@xcite leans the interaction weights between user actions and various features such as user and item metadata .",
    "the authors use a unified boltzmann machines to make a prediction .",
    "in their _ prod2vec _ paper @xcite , grbovic et al . proposed the use of the _",
    "word2vec _ algorithm on sequences of product receipts coming from emails .",
    "more formally , given a set @xmath1 of sequences of products @xmath2 , @xmath3 , the objective is to find a d - dimensional real - value representation @xmath4 such that similar products are close in the resulting vector space .    the source algorithm - _ word2vec _",
    "@xcite is originally a highly scalable predictive model for learning word embeddings from text and belongs to the larger class of neural net language models @xcite .",
    "most of the work in this area is based on the distributional hypothesis @xcite , which states that words that appear in the same contexts have close if not identical meanings .",
    "a similar hypothesis can be applied in larger contexts such as online shopping , music and media consumption and has been the basis of cf methods . in the cf setup , the users of the services are used as the distributed context in which products co - occur , leading to the classical item co - occurrence approaches in cf . a further similarity between co - count based recommendation methods and _ word2vec _ has been established by omer et al . in @xcite ;",
    "the authors show that the objective of the embedding method is closely related to the decomposition of the matrix containing as entries the shifted positive pmi of the locally co - occurring items ( words ) , where pmi is the point - wise mutual information :    @xmath5    where @xmath6 and @xmath7 are items frequencies , @xmath8 is the number of times @xmath9 and @xmath10 co - occur , @xmath11 is the size of the dataset and @xmath12 is the ratio of negatives to positives .",
    "[ [ prod2vec - objective ] ] prod2vec objective + + + + + + + + + + + + + + + + + +    in @xcite the authors show that the _ word2vec _ objective ( and similarly _",
    "prod2vec _ s ) can be re - written as the optimization problem of minimizing the weighted cross entropy between the empirical and the modeled conditional distributions of context products given the target product ( more precisely , this represents the word2vec - skipgram model , which usually does better on large datasets ) .",
    "furthermore , the prediction of the conditional distribution is modeled as a softmax over the inner product between the target product and context product vectors :    @xmath13    here , @xmath14 is the cross - entropy of the empirical probability @xmath15 of seeing any product in the output space @xmath16 conditioned on the input product @xmath17 and the predicted conditional probability @xmath18 : @xmath19 where @xmath6 is the input frequency of product i and @xmath20 is the number of times the pair of products @xmath21 has been observed in the training data .    the resulting architecture for _ prod2vec _ is shown in figure [ fig : nn_prod2vec ] , where the input space of all products situated in center window is trained to predict the values of the surrounding products by using a neural network with a single hidden layer and a softmax output layer .    however , the product embeddings generated by _",
    "prod2vec _ only take into account the information of the user purchase sequence , that is the local co - occurrence information .",
    "though richer than the global co - occurrence frequency used in collaborative filtering , it does not take into account other types of item information that are available ( the items metadata ) .    for example , assuming that the inputs are sequences of categorized products , the standard _",
    "prod2vec _ embedding does not model the following interactions :    * given the current visit of the product @xmath22 with category @xmath23 , it is more likely that the next visited product @xmath24 will belong to the same category @xmath23 * given the current category @xmath23 , it is more likely that the next category is @xmath23 or one of the related categories @xmath25 ( e.g. after a swimwear sale , it is likely to observe a sunscreen sale , which belongs to an entirely different product category ) * given the current product @xmath22 , the next category is more likely to be @xmath23 or a related category @xmath25 * given the current category @xmath23 , the more likely current products visited are @xmath22 or @xmath24        as mentioned in the introduction , the same authors extended the _",
    "prod2vec _ algorithm in @xcite such that to take into account both product sequence and product text in the same time .",
    "if applying the extended method to non - textual metadata , the algorithm models , additionally to the product sequence information , the dependency between the product metadata and the product i d , but does not link the sequence of metadata and the sequence of product ids together .      as shown in the related work section , there has been extensive work on using side information for recommendation , especially in the context of hybrid methods that combine cf methods and content - based ( cb ) methods . in the case of embeddings",
    ", the closest work is the _ doc2vec _ @xcite model where the words and the paragraph are trained jointly , but only the paragraph embedding is used for the final task .",
    "we propose a similar architecture that incorporates the side information in both the input and output space of the neural net and parametrizes separately each one of the interactions between the items to be embedded and the metadata , as shown in figure [ fig : nn_metaprod2vec ] .        [ [ meta - prod2vec - objective ] ]",
    "meta - prod2vec objective + + + + + + + + + + + + + + + + + + + + + + +    the _ meta - prod2vec _ loss extends the _",
    "prod2vec _ loss by taking into account four additional interaction terms involving the items metadata :    @xmath26    where : m is the metadata space ( for example , artist ids in the case of the 30music dataset ) , @xmath27 is the regularization parameter .",
    "we list the new interaction terms below : + @xmath28 is the weighted cross - entropy between the observed conditional probability of input product ids given their metadata and the predicted conditional probability",
    ". this side - information is slightly different from the next three types because it models the item as a function of its own metadata ( same index in the sequence ) .",
    "this is because , in most cases , the item s metadata is more general than the i d and can partially explain the observation of the specific i d .",
    "+ @xmath29 is the weighted cross - entropy between the observed conditional probability of surrounding product ids given the input products metadata and the predicted conditional probability .",
    "an architecture where the normal _ word2vec _ loss is augmented with only this interaction term is very close to the _ doc2vec _ model proposed in @xcite where we replace the document i d information with a more general type of item metadata .",
    "+ @xmath30 is the weighted cross - entropy between the observed conditional probability of surrounding products metadata values given input products and the predicted conditional probability .",
    "+ @xmath31 is the weighted cross - entropy between the observed conditional probability of surrounding products metadata values given input products metadata and the predicted conditional probability .",
    "this models the sequence of observed metadata and in itself represents the _",
    "word2vec_-like embedding of the metadata .",
    "+ to summarize , @xmath32 and @xmath31 encode the loss terms coming from modeling the likelihood of the sequences of items and metadata separately , @xmath28 represents the conditional likelihood of the item i d given its metadata and @xmath29 and @xmath30 represent the cross - item interaction terms between the item ids and the metadata .",
    "in figure [ fig : matriximj ] we show the relationship between the item matrix factorized by _",
    "prod2vec _ and the one factorized by _ meta - prod2vec_.    the more general equation for _ meta - prod2vec _ introduces a separate @xmath27 for each of the four types of side - information : @xmath33 , @xmath34 , @xmath35 and @xmath36 .        in section [ sec : experiments ] we will analyze the relative importance of each type of side - information . also , in the case when multiple sources of metadata are used , each source will have its own term in the global loss and its own regularization parameter .    in terms of the softmax normalization factor",
    ", we have the option of either separate the output spaces of the items and of their metadata or not . similarly with the simplifying assumption used in _ word2vec _ , that allows each pair of co - occurring products to be predicted and fitted independently ( therefore adding an implicit mutual exclusivity constraint on the output products given an input product ) , we embed the products and their metadata in the same space , therefore allowing them to share the normalization constraint",
    ".    one of the main attractions of the _ word2vec _ algorithm is its scalability , which comes from approximating the original softmax loss over the space of all possible words with the negative sampling loss @xcite , that fits the model only on the positive co - occurrences together with a small sample of the negative examples to maximize a modified likelihood function @xmath37 :    @xmath38    and :    @xmath39    where @xmath40 probability distribution used to sample negative context examples and @xmath12 is a hyper parameter specifying the number of negative examples per positive example .",
    "the side - information loss terms @xmath28 , @xmath29 , @xmath30 , @xmath31 are computed according to the same formula , where the i , j indexes range over the respective input / output spaces .    in the case of",
    "_ meta - prod2vec _ , the impact of the decision to co - embed products and their metadata on the @xmath37 loss is that the set of potential negative examples for any positive pair ranges over the union of items and metadata values .",
    "[ [ impact - on - final - recommendation - system .- feasibility - and - engineering - concerns ] ] impact on final recommendation system .",
    "feasibility and engineering concerns + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    because of the shared embedding space , the training algorithm used for _",
    "prod2vec _ remains unchanged .",
    "the only difference is that , in the new version of the generation step of training pairs , the original pairs of items are supplemented with additional pairs that involve metadata . in terms of the online recommendation system , assuming we are augmenting a solution that already involves item embeddings , the online system does not incur any changes ( since the only time we make use of the metadata is during training ) and there is zero additional impact on the online memory footprint .",
    "the experimental section is organized as follows .",
    "first , we describe the evaluation setup , namely , the evaluation task , success metrics and the baselines . then , we report results of experiments on the 30music open dataset .",
    "we evaluate the recommendation methods on the next event prediction task .",
    "we consider time ordered sequences of user interactions with the items .",
    "we split each sequence into the consequent training , validation and test sets .",
    "we fit the embedding _",
    "prod2vec _ and _ meta - prod2vec _ models on the first ( n-2 ) elements of each user sequence and use the performance on the ( n-1)-th element to bench the hyper - parameters and we report our final by training on the first ( n-1 ) items of each sequence and predicting the nth item .",
    "we use the last item in the training sequence as the _ query item _ and we recommend the most similar products using one of the methods described below .    as mentioned in section [ sec : intro ] , due to the technical constraint of keeping a constant memory footprint , we are interested in the usefulness of item metadata only at training time",
    ". therefore we do not compare against methods where the metadata is used directly at prediction time , such as the supervised content - based embedding models proposed in @xcite where both the user and item are represented as linear combinations of the item content embeddings and @xcite , where the products are represented by the associated image content embeddings .",
    "we use the following evaluation metrics averaged over all users :    * hit ratio at k ( hr@k ) that is equal to @xmath41 if the test product appears in the top k list of recommended products .",
    "* normalized discounted cumulative gain ( ndcg@k ) favors higher ranks of the test product in the list of recommended products .    using the aforementioned metrics , we compare the following methods :    * _ bestof",
    "_ : this method retrieves the top products sorted by their popularity .",
    "this simulates the frequently encountered recommendation solution based strictly on popularity . *",
    "_ cocounts _ : the standard cf method which uses the cosine similarity of vectors of co - occurrences with other items .",
    "this method is performing particularly well in cases where the catalog of possible items is small and does not change a lot over time , therefore avoiding item cold - start problems . *",
    "_ standalone prod2vec _ : the product recommendation solution based on cosine similarities of the vectorial representation of products obtained by using _",
    "word2vec _ on product sequences .",
    "similarly with other embedding and matrix factorization solutions , the goal of the method is to address the cold - start problem . * _ standalone meta - prod2vec _ : our proposed method , which enhances _",
    "prod2vec _ with item side information and uses the resulting product embeddings to compute cosine similarities . as in _ prod2vec",
    "_ , the goal of the method is to further address cold - start problems . * _",
    "mix(prod2vec , cocounts ) _ : an ensemble method which returns the top items using a linear combination between the _ prod2vec _ and the _ cocount_-based item pair similarities .",
    "the motivation of the ensemble methods that combine embeddings and cf is to leverage the benefits of the two in cold - start and non cold - start regimes . @xmath42 * _ mix(meta - prod2vec , cocounts ) _ : an ensemble method which returns the top items using a linear combination between the _ meta - prod2vec _ and the _ cocount_-based item pair similarity .",
    "@xmath43    [ cols=\"<,^,^,^,^\",options=\"header \" , ]      we perform our evaluation on the publicly available 30music dataset @xcite that represents a collection of listening and playlists data retrieved from internet radio stations through last.fm api . on this dataset",
    ", we evaluate the recommendation methods on the task of next song prediction . for the _ meta - prod2vec _ algorithm we make use of track metadata , namely the artist information .",
    "we run our experiments on a sample of 100k user sessions of the dataset with the resulting vocabulary size of 433k songs and 67k artists .",
    "we keep the embedding dimension fixed to 50 , window size to 3 , the side information regularization parameter @xmath27 to 1 .",
    "we bench the blending factor @xmath44 in equations [ eq : mix_prod2vec],[eq : mix_metaprod2vec ] of the embedding - based similarity score with the _ cocounts_-based cosine similarity score and find that the best value @xmath45 .",
    "in addition , we vary the number of training epochs and we find the best parameter value to be 30 for _ prod2vec _ and 10 for _ meta - prod2vec_. as shown in table [ tab : reco - accuracy ] , _ meta - prod2vec _ has better performance both standalone and in the ensemble model with respect to the _",
    "prod2vec _ model ( results computed at 90% confidence levels ) .",
    "most of the gains in performance are coming from the cold - start traffic , where , as shown in figure [ fig:30music_cold - start_log_pair ] and table [ tab : reco - cold - start ] , we can see that _",
    "standalone meta - prod2vec _ outperforms by a large margin all other methods when the true pair of ( query item , next item ) have zero co - occurrences in the training set and that the _ mix(meta - prod2vec , cocounts ) _ has the best performance in the cases where the true pair has low observed counts .",
    "interestingly , we observe that the difference of performance between the ensemble models is bigger than between the standalone _ meta - prod2vec _ and _ prod2vec _ models .",
    "we explain this by the fact _",
    "standalone meta - prod2vec _ outperforms _ standalone prod2vec _ on the cold - start traffic therefore helping the overall performance of the _ cocounts _ , that by itself performs really well on head traffic . indeed , _",
    "cocounts _ memorizes frequent pairs of ( query , target ) product , while _",
    "standalone meta - prod2vec _ helps to generalize on unseen ones .",
    "these results are mirrored by similar findings covered in @xcite and motivate the newly introduced approach of wide and deep learning .",
    "we posited the relevance of each type of side information , but we want to confirm experimentally that each of the four types of side - information brings additional information .",
    "we proceed by introducing each of the types of the side information separately and to compare its performance with the original _",
    "prod2vec _ baseline .",
    "the only exception for which we test the value by leaving it out of the full _ meta - prod2vec _ model is the @xmath46 type of side - information ; in this case , the input metadata explains the output metadata and this constraint is not directly valuable in regularizing the product embeddings and needs to be introduced together with the other types of pairs that connect the products to the metadata .",
    "therefore its contribution to the model can be computed as ( 1 - degraded model performance ) and is therefore @xmath47 on hr and @xmath0 on ndcg .    in table [ tab : reco - accuracy - side - info ] , we compute the proportion of lift over the _ bestof _ baseline obtained by using each type of side information separately and we observe that each one of them account for at most 50% of the performance of the full _ meta - prod2vec _ , therefore confirming that the additional terms in our proposed model are relevant .",
    "in this paper , we introduced _",
    "meta - prod2vec _ , a new item embedding method that enhances the existing _ prod2vec _ method with item metadata at training time .",
    "this work makes a novel connection between the recent embedding - based methods and consecrated matrix factorization methods by introducing learning with side information in the context of embeddings .",
    "we analyzed separately the relative value of each type of side information and proved that each one of the four types is informative .",
    "finally , we have shown that _",
    "meta - prod2vec _ constantly outperforms _",
    "prod2vec _ on recommendation tasks both globally and in the cold - start regime , and that , when combined with a standard collaborative filtering approach , outperforms all other tested methods .",
    "these results , together with the reduced implementation cost and the fact that our method does not affect the online recommendation architecture , makes this solution attractive in cases where item embeddings are already in use .",
    "future work will extend on ways of using item metadata as side information and support of non - categorical information such as images and continuous variables .",
    "d.  agarwal and b .- c .",
    "regression - based latent factor models . in _ proceedings of the 15th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 09 , pages 1928 , new york , ny , usa , 2009 .",
    "b.  chandramouli , j.  j. levandoski , a.  eldawy , and m.  f. mokbel .",
    "streamrec : a real - time recommender system . in _ proceedings of the 2011 acm",
    "sigmod international conference on management of data _ , pages 12431246 .",
    "acm , 2011 .",
    "n.  djuric , h.  wu , v.  radosavljevic , m.  grbovic , and n.  bhamidipati . hierarchical neural language models for joint representation of streaming documents and their content . in _ proceedings of the 24th international conference on world wide web _ , pages 248255",
    ". international world wide web conferences steering committee , 2015 .",
    "y.  fang and l.  si .",
    "matrix co - factorization for recommendation with rich side information and implicit feedback . in _ proceedings of the 2nd international workshop on information heterogeneity and fusion in recommender systems _ , hetrec 11 , pages 6569 , new york , ny , usa , 2011 .",
    "m.  grbovic , v.  radosavljevic , n.  djuric , n.  bhamidipati , j.  savla , v.  bhagwan , and d.  sharp .",
    "e - commerce in your inbox : product recommendations at scale . in _ proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining _ , kdd 15 , pages 18091818 , new york , ny , usa , 2015 .",
    "a.  gunawardana and c.  meek .",
    "a unified approach to building hybrid recommender systems . in _ proceedings of the third acm conference on recommender systems",
    "_ , recsys 09 , pages 117124 , new york , ny , usa , 2009 .",
    "a.  karatzoglou , x.  amatriain , l.  baltrunas , and n.  oliver .",
    "multiverse recommendation : n - dimensional tensor factorization for context - aware collaborative filtering . in _ proceedings of the fourth acm conference on recommender systems",
    "_ , recsys 10 , pages 7986 , new york , ny , usa , 2010 .",
    "m.  kula .",
    "metadata embeddings for user and item cold - start recommendations . in _ proceedings of the 2nd workshop on new trends on content - based recommender systems co - located with 9th acm conference on recommender systems ( recsys 2015 ) , vienna , austria , september 16 - 20 , 2015 .",
    "_ , pages 1421 , 2015 .",
    "y.  li , j.  hu , c.  zhai , and y.  chen .",
    "improving one - class collaborative filtering by incorporating rich user information . in _ proceedings of the 19th acm international conference on information and knowledge management _ , cikm 10 , pages 959968 , new york , ny , usa , 2010 .",
    "t.  mikolov , i.  sutskever , k.  chen , g.  s. corrado , and j.  dean . distributed representations of words and phrases and their compositionality . in _ advances in neural information processing systems _ ,",
    "pages 31113119 , 2013 .",
    "a.  mnih and k.  kavukcuoglu . learning word embeddings efficiently with noise - contrastive estimation . in c.  burges , l.  bottou , m.  welling , z.  ghahramani , and k.  weinberger , editors , _ advances in neural information processing systems 26 _ , pages 22652273 .",
    "curran associates , inc . , 2013",
    ".    x.  ning and g.  karypis .",
    "sparse linear methods for top - n recommender systems . in _ 11th ieee international conference on data mining , icdm 2011 , vancouver , bc , canada , december 11 - 14 , 2011 _ , pages 497506 , 2011 .",
    "r.  pan , y.  zhou , b.  cao , n.  n. liu , r.  lukose , m.  scholz , and q.  yang .",
    "one - class collaborative filtering . in _ proceedings of the 2008 eighth ieee international conference on data mining _ , icdm 08 , pages 502511 , washington , dc , usa , 2008 .",
    "ieee computer society .",
    "s.  rendle , c.  freudenthaler , z.  gantner , and l.  schmidt - thieme .",
    "bpr : bayesian personalized ranking from implicit feedback . in _ proceedings of the twenty - fifth conference on uncertainty in artificial intelligence _ ,",
    "uai 09 , pages 452461 , arlington , virginia , united states , 2009 .",
    "auai press .",
    "s.  rendle , z.  gantner , c.  freudenthaler , and l.  schmidt - thieme .",
    "fast context - aware recommendations with factorization machines . in _ proceedings of the 34th international acm sigir",
    "conference on research and development in information retrieval _ , sigir 11 , pages 635644 , new york , ny , usa , 2011 .",
    "acm .",
    "r.  salakhutdinov , a.  mnih , and g.  hinton .",
    "restricted boltzmann machines for collaborative filtering . in _ proceedings of the 24th international conference on machine learning _ , icml 07 , pages 791798 , new york , ny , usa , 2007 .",
    "a.  i. schein , a.  popescul , l.  h. ungar , and d.  m. pennock . methods and metrics for cold - start recommendations . in _ proceedings of the 25th annual international acm sigir conference on research and development in information retrieval _ , pages 253260 .",
    "acm , 2002 .",
    "a.  veit , b.  kovacs , s.  bell , j.  mcauley , k.  bala , and s.  belongie .",
    "learning visual clothing style with heterogeneous dyadic co - occurrences . in _ proceedings of the ieee international conference on computer vision _ , pages 46424650 , 2015 .",
    "yang , b.  long , a.  smola , n.  sadagopan , z.  zheng , and h.  zha .",
    "like like alike : joint friendship and interest propagation in social networks . in _ proceedings of the 20th international conference on world wide web _ , www 11 , pages 537546 , new york , ny , usa , 2011 ."
  ],
  "abstract_text": [
    "<S> we propose meta - prod2vec , a novel method to compute item similarities for recommendation that leverages existing item metadata . </S>",
    "<S> such scenarios are frequently encountered in applications such as content recommendation , ad targeting and web search . </S>",
    "<S> our method leverages past user interactions with items and their attributes to compute low - dimensional embeddings of items . </S>",
    "<S> specifically , the item metadata is injected into the model as side information to regularize the item embeddings . </S>",
    "<S> we show that the new item representations lead to better performance on recommendation tasks on an open music dataset .    </S>",
    "<S> = 10000 = 10000 </S>"
  ]
}