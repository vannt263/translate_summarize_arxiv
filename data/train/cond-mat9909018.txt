{
  "article_text": [
    "the recent invention of `` turbo codes '' by berrou and glavieux @xcite is considered a major breakthrough in communications .",
    "for the first time one can communicate almost error - free for signal to noise ratios very close to the theoretical bounds of information theory .",
    "turbo codes are fastly becoming the new standard for error correcting codes in digital communications .",
    "the invention of turbo codes and their iterative decoding algorithm was empirical .",
    "there is no theoretical understanding of why they are so successfull .",
    "the decoding algorithm is thought to be an approximate algorithm .",
    "we think that turbo codes are interesting , even outside the context of communication theory , because they provide a non trivial example of a disordered system which can be studied numerically with a fast algorithm .",
    "+ in this paper we will study turbo codes and turbo decoding using the modern tools of statistical mechanics of disordered systems .",
    "one of us has already shown in the past @xcite that there is a mathematical equivalence between error correcting codes and theoretical models of spin glasses .",
    "in particular the logarithm of the probability for any given signal , conditional on the communication channel output , has the form of a spin glass hamiltonian .",
    "we will construct the hamiltonian which corresponds to the turbo codes and study its properties .",
    "this will clarify why they are so successfull .",
    "in particular we will show that there is a threshold @xmath0 such that for signal to noise ratios @xmath1 the average error probability per bit @xmath2 vanishes in the thermodynamic limit , i.e. the limit of infinitly long sequences . in @xmath2 the average is taken over a large class of turbo codes ( see later ) and over `` channel '' noise .",
    "the rate of these codes is finite .",
    "the value of the threshold has been computed for two particular turbo codes .",
    "it was found that it depends on the code .",
    "we also compare these results with numerical simulations .",
    "+ our results are typical of the statistical mechanics approach : we study only the average performance of turbo codes , not the performance of any particular one .",
    "furthermore there exist `` very few '' particular codes performing `` much worse '' than the average .",
    "+ let us first briefly remind the connection between error - correction codes and spin - glass models . in the mathematical theory of communication both the production of information and its transmission are considered as probabilistic events .",
    "a source is producing information messages according to a certain probability distribution .",
    "messages of length @xmath3 are sequencies of @xmath3 symbols or `` letters of an alphabet '' @xmath4",
    ". we will assume for simplicity a binary alphabet , i.e. @xmath5 or @xmath6 and that all symbols are equally probable .",
    "instead of @xmath7 we can equally well use ising spins @xmath8 the messages are sent through a noisy transmission channel . if a @xmath9 is sent through the transmission channel , because of the noise , the output will be a real number @xmath10 , in general different from @xmath11 . again , the statistical properties of the transmission channel are supposed to be known .",
    "let us call @xmath12 the probability for the transmission channel s output to be between @xmath13 and @xmath14 , when the input was @xmath15 .",
    "@xmath16 is supposed to be known . for reasons of simplicity , we assume that the noise is independent for any pair of bits ( `` memoryless channel '' ) , i.e. @xmath17 in the case of a memoryless channel and a gaussian noise : @xmath18 shannon calculated the channels capacity @xmath19 , i.e. the maximum information per use of the channel that can be transmitted .",
    "@xmath20 where @xmath21 is the signal power .",
    "+ under the above assumptions , communication is a statistical inference problem .",
    "given the transmission channel s output and the statistical properties of the source and of the channel , one has to infer what message was sent .",
    "in order to reduce communication errors , one may introduce ( deterministic ) redundancy into the message ( `` channel encoding '' ) and use this redundancy to infer the message sent through the channel ( `` decoding '' ) .",
    "the algorithms which transform the source outputs to redundant messages are called error - correcting codes .",
    "more precisely , instead of sending the @xmath3 original bits @xmath22 , one sends @xmath23 bits @xmath24 , @xmath25,@xmath26 , constructed in the following way @xmath27 where the `` connectivity '' matrix @xmath28 has elements zero or one . for any @xmath29 , all the @xmath30 except from one are equal to zero , i.e. the @xmath24 are equal to @xmath31 .",
    "@xmath28 defines the code , i.e. it tells from which of the @xmath32 s to construct the @xmath29th bit of the code . +",
    "this kind of codes are called parity checking codes because @xmath24 counts the parity of the minusis among the @xmath33 @xmath32 s .",
    "the ratio @xmath34 which specifies the redudancy of the code , is called the rate of the code .",
    "+ knowing the source probability , the noise probability , the code and the channel output , one has to infer the message that was sent .",
    "the quality of inference depends on the choice of the code .",
    "+ according to the famous shannon s channel encoding theorem , there exist codes which , in the limit of infinitly long messages , allow error - free communication , provided the rate of the code @xmath35 is less than the channel capacity @xmath36 .",
    "this theorem says that such `` ideal '' codes exist , but does not say how to construct them .",
    "+ we have shown that there exists a close mathematical relationship between error - correcting codes and theoretical models of disosdered systems . as we previously said ,",
    "the output of the channel is a sequence of @xmath23 real numbers @xmath37 , which are random variables , obeying the probability distribution @xmath38 .",
    "once the channel output @xmath39 is known , it is possible to compute the probability @xmath40 for any particular sequence @xmath41 to be the _ source _ output ( i.e. the information message ) . +",
    "more precisely , the equivalence between spin - glass models and error correcting codes is based on the following property .",
    "+ the probability @xmath40 for any sequence @xmath42 to be the information message , conditional on the channel output @xmath43 is given by @xmath44 where @xmath45 we recognize in this expression the hamiltonian of a p - spin spin - glass hamiltonian .",
    "the distribution of the couplings is determined by the probability @xmath46 .",
    "+ in the case when @xmath47 ( the case of a `` symmetric channel '' ) , @xmath48 and one recovers the invariance of the spin - glass hamiltonian under gauge transformations .",
    "+ `` minimum error probability decoding '' ( or med ) , which is widely used in communications @xcite , consists in choosing the most probable sequence @xmath49 .",
    "this is equivalent to finding the ground state of the above spin - glass hamiltonian .",
    "+ instead of considering the most probable instance , one may only be interested in the most probable value @xmath50 of the `` bit '' @xmath51 ( maximum a posteriori probability or map decoding ) @xcite which can be expressed in terms of the magnetization at temperature @xmath52 equal to one @xcite : @xmath53 where @xmath54 is defined by eq.([distribuzionepercodicegenerico ] ) .",
    "+ it is remarkable that @xmath55 coincides with the nishimori temperature in spin glasses @xcite .",
    "map decoding is an essential ingredient in turbo decoding ( see later ) .",
    "+ when all messages are equally probable and the transmission channel is memoryless and symmetric , the error probability is the same for all input sequences .",
    "it is enough to compute it in the case where all input bits are equal to one . in this case , the error probability per bit @xmath56 is @xmath57 and @xmath58 is the symbol sequence produced by the decoding procedure .",
    "one can derive from this a very general lower bound for @xmath56 , using the analog of the low temperature expansion .",
    "an obvious bound ( for zero temperature decoding ) is provided by the probability @xmath59 that only one bit is incorrect , i.e. @xmath60 while all other bits are correct , i.e. @xmath61 for all @xmath62 : @xmath63 where the @xmath64 denotes the set of the couplings in which @xmath65 appears . +",
    "a necessary condition for transmitting without errors is that @xmath66 with probability one .",
    "this is only possible if every spin appears in an infinite number of terms in the hamiltonian .",
    "let @xmath67 be the number of spins coupled through the coupling @xmath68 .",
    "+ the total number of spins beeing @xmath3 , a spin appears on the average in @xmath69 terms , where @xmath70 is the average of @xmath71 ( the number of spins coupled together ) and @xmath35 is the rate of the code .",
    "+ so a necessary condition for a finite rate code to achieve zero error probability , is that the average number of spins coupled together diverges in the thermodynamic limit ( @xmath72 ) .",
    "this condition is realised in derrida s random energy model @xcite which has been shown to be an ideal code @xcite ( in that case @xmath73 ) .",
    "+ we will show in the following that this is also true for the case of recursive turbo codes , while it is not true for non recursive turbo codes .",
    "convolutional codes are the building blocks of turbo codes . in this section",
    "we shall describe both non recursive and recursive convolutional codes and the corresponding spin models .",
    "the information message i.e. the source output ( before encoding ) will be denoted by : @xmath74 it is convenient to think of the source producing a symbol per unit time , i.e. in @xmath51 , @xmath75 denotes the time . for simplicity",
    "we consider a code of rate @xmath76 .",
    "the encoded message has the form : @xmath77 any hardware implementation of a convolutional encoder contains a sequence of @xmath78 memory registers .",
    "we shall call @xmath78 the range of the code .",
    "+ let s denote by @xmath79 the content of the memory registers at time @xmath80 .",
    "at each time step the content of each memory register is shifted to the right : @xmath81 moreover for convenience of notation we define @xmath82 and the following sequence of bits which we shall call the register sequence : @xmath83 the sequence of the @xmath32 s is a function of the source sequence ( which may depend on the code ) : @xmath84 when not ambiguous we shall omit in the following the functional dependence of @xmath85 upon @xmath86 . + for non recursive convolutional codes this application is extremely simple : @xmath87 the encoded message @xmath88 is easily defined in terms of the content of the register sequence : @xmath89 we shall assume hereafter that @xmath90 . +",
    "to avoid redundancy we choose @xmath78 such that either @xmath91 or @xmath92 are different from @xmath93 .",
    "+ to make eq.([encoding ] ) meaningful for @xmath94 we define @xmath95 for @xmath96 .",
    "notice however that the exact definition of @xmath97 is irrelevant in the thermodynamic limit .",
    "+ the numbers @xmath98 define the code .",
    "several conventions are used to give them in a compact form .",
    "a simple and useful one is the following .",
    "to each of the two sets of numbers @xmath99 and @xmath100 is associated a polynomial on @xmath101 @xmath102 the @xmath103 are called generating polynomials . in the same way we can associate a polynomial to the register sequence ( @xmath104 ) , to the source message ( @xmath105 ) and to each part of the encoded message ( @xmath106 )",
    ". with these definitions it is evident that the correspondence ( [ registrononricorsivo ] ) between the source and the register sequences for a non recursive convolutional code implies : @xmath107 and the encoding rule ( [ encoding ] ) becomes @xmath108 a few examples are the following :    1 .",
    "the simplest non trivial convolutional code has range @xmath6 : @xmath109 [ codicesemplice ] 2 .",
    "a simple code with range @xmath110 whose behavior will be examined in what follows : @xmath111 [ codicetipico ] 3 .",
    "the code with range @xmath112 used by berrou and collaborators to build the first example of turbo code : @xmath113 [ codicecomplicato ]    recursive convolutional codes are most easily defined in terms of the generating polynomials . the difference with non recursive codes is in the relation between the source and the register sequences . in the non recursive case",
    "it was given by eq.([registrononricorsivo ] ) or by eq.([registrononricorsivopol ] ) .",
    "in the recursive case one has : @xmath114 so that eq .",
    "( [ nonrecursivepolyn ] ) gives @xmath115 two different recursive codes can be defined by permuting the two polynomials @xmath116 and @xmath117 .",
    "+ it is easy to show that eq .",
    "( [ recursiveencoding ] ) is equivalent to @xmath118 from eq.([recursivefeedback ] ) it follows that : @xmath119 because of the last equality in eq.([recursiveregister ] ) a part of the encoded message ( in the recursive case ) is always the message itself .",
    "+ we shall now consider decoding . using the method explained in the introduction the probability distribution of the register sequence conditional to some ouput can be written as the boltzmann weight of a spin model with random couplings .",
    "the hamiltonian of this model is : @xmath120 where @xmath121 is defined in eq.([definizioneb ] ) .",
    "+ for convolutional codes the model is one dimensional with two types of couplings .",
    "the range of the interaction coincide with the range of the code .",
    "the alert reader will notice that the hamiltonian is expressed as a function of the spins of the register sequence @xmath122 , instead of the source sequence @xmath51 used in the introduction . for non recursive codes",
    "for recursive codes @xmath51 is given by eq.([recursiveregister ] ) , i.e. in this last case decoding can be thought of as the computation of an expectation value of a composite operator .",
    "however the spin hamiltonian is the same for both the recursive and not recursive codes .",
    "+ we define the decoding at arbitrary temperature @xmath124 as follows : @xmath125 where the expression for @xmath126 is given by eq.([recursiveregister ] ) or by eq.([registrononricorsivo ] ) depending whether the code is recursive or not .",
    "+ as seen in the introduction there are two widely used decoding strategies :    * maximum likelihood decoding which consists in finding the most probable sequence of bits and corresponds to the choice @xmath127 in eq.([decbeta ] ) : @xmath128 . * maximum a posteriori probability decoding which consists in finding the most probable sequence of bits and corresponds to the choice @xmath129 in eq.([decbeta ] ) : @xmath130 .",
    "this is the strategy which enters in turbo decoding .",
    "both this strategies can be implemented in a very efficient way using the transfer matrix technique .",
    "the corresponding algorithms are known in communication theory as the viterbi algorithm @xcite for the @xmath127 case and the bcjr algorithm @xcite for the @xmath131 case .",
    "the complexity of these algorithms grows like @xmath132 .",
    "+ the use of the register sequence ( i.e. of the @xmath32 variables ) makes evident the similarity between recursive and nonrecursive codes : they correspond to the same spin model .",
    "this implies e.g. that , if zero temperature decoding is adopted , the probability of transmitting a message without errors is the same with the two codes .",
    "+ in the limit @xmath133 it is possible to construct convolutional codes corresponding to spin models with infinite connectivity and couplings between an infinite number of spins .",
    "they should allow to transmit without errors when the noise is low enough . in practice , because of the growing complexity of the transfer matrix algorithm , a compromise between low @xmath78 s ( which are simpler to decode ) and high @xmath78 s ( which show better performances ) must be found .",
    "the values of @xmath78 used in practical cases are between @xmath134 and @xmath135 .",
    "+ we can write the decoding strategy in terms of the message ( i.e. of the @xmath136 variables ) without making use of the register sequence ( i.e. of the @xmath32 variables ) : @xmath137 for non recursive codes , because of eq.([registrononricorsivo ] ) , things remain unchanged .",
    "however , for recursive codes , since eq.([recursiveregister ] ) can not be inverted in a local way , we obtain a non local hamiltonian . + as a simple illustration of this observation we can consider the hamiltonian corresponding to the code [ codicesemplice ] : @xmath138 for less simple codes we define the numbers @xmath139 as follows : @xmath140 we get @xmath141 written in this form recursive codes look very different from non recursive ones with the same range . if @xmath142 is not divisible by @xmath143 the corresponding spin models have infinite connectivity and interactions with infinite range ; they are similar , in this respect , to @xmath144 non recursive codes .",
    "+ neverthless they do not behave , in general , radically better than the non recursive codes with the same range because there exists , as we have shown , a change of variables ( from @xmath136 to @xmath32 ) which makes the model local .",
    "a turbo code is defined by the choice of a convolutional code and of a permutation of @xmath3 objects .",
    "we use for the permutation the following notation : @xmath145 and we shall denote by @xmath146 the inverse permutation ( @xmath147 ) . + the basic idea is to apply the permution @xmath148 to the source sequence @xmath86 to produce a new sequence @xmath149 . obviously @xmath149 does not carry any new information because @xmath148 is known . both sequences @xmath86 and @xmath149 are the inputs to two set of registers , each one implementing a convolutional encoding . in this way the rate of the code is decreased ( i.e. greater redundancy ) .",
    "one can increase the rate by erasing some of the outputs @xcite , but we will not consider this possibility in this paper . + the properties of the system can strongly depend on the choice of the permutation .",
    "permutations `` near '' the identity give very bad codes .",
    "we shall think to a `` good '' permutation as to a random permutation . in the limit @xmath150 they are `` far '' from the identity with probability one .",
    "we shall discuss this point later in this section .",
    "+ we illustrate this idea with the example of a rate @xmath151 recursive convolutional code , defined by the constants @xmath99 and @xmath100 .",
    "the two register sequences are : @xmath152 where @xmath153 is the permuted message ( @xmath154 ) .",
    "+ the relation between the two register sequences is rather involved and nonlocal for a general choice of the permutation . moreover @xmath155 can be expressed only in terms of a large number of @xmath156 s .",
    "the identity permutation is clearly an exception since in this case @xmath157 .",
    "+ let us consider as an example the code [ codicesemplice ] : @xmath158 it is simple to show that , for a random permutation , the number of different @xmath159 s in the product on the r.h.s . of eq.([dueregistri ] ) is of order @xmath160 .",
    "+ the turbo code defined by the permutation @xmath148 and by the numbers @xmath99 and @xmath100 has rate @xmath161 and the encoded message has the following form : @xmath162 it turns out that it is convenient to write the corresponding hamiltonian as a function of both register sequences .",
    "this introduces new degrees of freedom and the hamiltonian is a function of @xmath163 instead of @xmath3 spin .",
    "the unwanted degrees of freedom are eliminated by imposing the constraint @xmath164 .",
    "this constraint can be written in terms of the @xmath32 s using eqs.([turboregistro1 ] ) and ( [ turboregistro2 ] ) .",
    "the probability distribution for the register sequences can then be written as : @xmath165 where @xmath166 is the ordinary kronecker function . in this way",
    "the probability distribution is a local function of the spin variables @xmath167 and @xmath159 .",
    "+ we shall call the code defined by eqs.([definizioneturbo1],[definizioneturbo2],[definizioneturbo3 ] ) a non recursive turbo code if @xmath168 and a recursive turbo code otherwise .",
    "recursive turbo codes are the ones usually called turbo codes in communication theory .",
    "+ the probability distribution for the recursive turbo code ( [ probabilitaturbo ] ) ca nt be written in terms of one of the two register sequences @xmath169 or @xmath170 without producing large connectivities ( see eq.([dueregistri ] ) ) . + if @xmath148 is the identity permutation then @xmath171 and the code becomes a convolutional one with the same rate ( @xmath161 ) and the same generating polynomials .",
    "we shall use the convolutional code obtained in this way as a standard comparison term for the performances of turbo codes ( see figs.([trecneargraf]-[trecgraf ] ) ) .",
    "the outcome of this comparison ( i.e. recursive turbo codes have a much lower error probability than convolutional codes ) demonstrates the importance of the choice of the permutation . + for non recursive turbo codes the two register sequences are related simply by a permutation : @xmath172 and @xmath173 so that the spin model corresponding to this type of code has a finite connectivity @xmath174 . +",
    "this finite versus infinite connectivity is the essential difference between non recursive and recursive turbo codes and explains why recursive turbo codes are so better and why they can achieve zero error probability for low enough noise .",
    "+ we now discuss decoding .",
    "there is no exact decoding algorithm for turbo codes .",
    "berrou et al .",
    "have proposed a very ingenious algorithm , called turbo decoding , which is thought to be approximate .",
    "turbo decoding is an iterative procedure . at each step of the iteration ,",
    "one considers one of the two chains , i.e. either the couplings @xmath175 and @xmath176 or @xmath175 and @xmath177 and proceeds to map decoding .",
    "the information so obtained is injected to the next step by adding appropriate external fields to the hamiltonian .",
    "the algorithm terminates if a fixed point is reached .",
    "+ in order to explain the algorithm more precisely , we introduce the following expectation values : @xmath178 & \\equiv & \\frac{1}{z } \\sum_{\\sg}\\epsilon_i(\\sg)\\exp\\left\\{\\sum_{i=1}^n b_i\\prod_{j=0}^r(\\sigma_{i - j})^{\\kappa(j;1)}+ \\sum_{i=1}^n b'_i\\prod_{j=0}^r(\\sigma_{i - j})^{\\kappa(j;2 ) } \\right\\}\\end{aligned}\\ ] ] the @xmath179 s can be computed in an efficient way by using the finite temperature transfer matrix algorithm .",
    "they are the expectation values of the operator defined by eqs.([turboregistro1 ] ) or ( [ turboregistro2 ] ) .",
    "+ then we introduce the iteration variables : @xmath180 for @xmath181 .",
    "+ in terms of these variables the iteration reads @xmath182",
    "\\label{turboiter1}\\\\ \\theta ^{(2)}_i(t+1 )   & = &   \\xi_i[\\bg^{(0),p}+\\gg^{(2)}(t),\\bg^{(2 ) } ]   \\label{turboiter2}\\\\ \\gamma",
    "^{(1)}_i(t+1 ) & = & \\arctanh\\left[\\theta ^{(2)}_{p^{-1}(i)}(t+1)\\right]-\\gamma^{(2)}_{p^{-1}(i)}(t ) -b^{(0)}_i\\label{turboiter3}\\\\ \\gamma ^{(2)}_i(t+1 ) & = & \\arctanh\\left[\\theta ^{(1)}_{p(i)}(t+1)\\right]-\\gamma^{(1)}_{p(i)}(t ) -b^{(0)}_{p(i ) } \\label{turboiter4}\\end{aligned}\\ ] ] with @xmath183 and @xmath184 . +",
    "the meaning of the previous equations is the following . the @xmath185",
    "are expectation values of a sequence of operators which can take only values @xmath186 , computed independently for every element of the sequence .",
    "the information contained in @xmath185 can therefore be represented by an `` external field '' @xmath187 such that @xmath188 . in order to avoid double counting of information one",
    "substracts the external fields of the previous iteration as shown in eqs.([turboiter3],[turboiter4 ] ) . +",
    "hopefully the iteration converges to a fixed point : @xmath189 the decoded message is obtained as follows : @xmath190 the system described by eq.([probabilitaturbo],[probabilitaturbo2 ] ) is seen in turbo decoding as the union of two one dimensional subsystem .",
    "each subsystem acts on the other one through a magnetic field ( in the non recursive case ) or through an additional coupling ( in the recursive case ) .",
    "+ to get some insight of eqs.([turboiter1]-[turboiter4 ] ) we define the free energy functionals @xmath191 and @xmath192 : @xmath193 & = & \\sum_{\\sg}\\exp\\left\\ { \\sum_{i=1}^n ( b(j^{(0)}_i)+\\gamma_i)\\prod_{j=0}^r(\\sigma_{i - j})^{\\kappa(j;1)}+ \\sum_{i=1}^n b(j^{(1)}_i)\\prod_{j=0}^r(\\sigma_{i - j})^{\\kappa(j;2)}\\right\\ } \\nonumber\\\\ \\\\ { \\cal z}^{(2)}[\\gg ] & = & \\sum_{\\sg}\\exp\\left\\ { \\sum_{i=1}^n ( b(j^{(0)}_{p(i)})+\\gamma_{p(i)})\\prod_{j=0}^r(\\sigma_{i - j})^{\\kappa(j;1)}+ \\sum_{i=1}^n b(j^{(2)}_i)\\prod_{j=0}^r(\\sigma_{i - j})^{\\kappa(j;2)}\\right\\ } \\nonumber\\\\ \\\\ f^{(m)}[\\thg ] & \\equiv & \\left.\\sum_{i=1}^n\\theta_i\\gamma_i- \\log\\left({\\cal z}^{(m)}[\\gg]\\right ) \\right|_{\\theta_i = \\frac{\\partial\\log({\\cal z}^{(m)})}{\\partial\\gamma_i } } \\end{aligned}\\ ] ] it is then simple to show that @xmath194 is a solution of the equation : @xmath195 & = & 0\\end{aligned}\\ ] ] where @xmath196 & \\equiv & f^{(1)}[\\thg]+f^{(2)}[\\thg]-f_0[\\thg]\\label{turbofreeenergy}\\\\ f_0[\\thg]&\\equiv&\\sum_{i=1}^n\\left\\{-b^{(0)}_i\\theta_i - s(\\theta_i)\\right\\}\\\\ s(x)&\\equiv & -\\left(\\frac{1+x}{2}\\right)\\log\\left(\\frac{1+x}{2}\\right)- \\left(\\frac{1-x}{2}\\right)\\log\\left(\\frac{1-x}{2}\\right)\\end{aligned}\\ ] ] eq.([turbofreeenergy ] ) is an approximation to the true free energy functional of the total system which is given by : @xmath197 & \\equiv & \\sum_{\\sg^{(1)}}\\sum_{\\sg^{(2 ) } } \\prod_{i=1}^n\\delta(\\epsilon_{p(i)}(\\sg^{(1 ) } ) , \\epsilon_{i}(\\sg^{(2)}))\\nonumber\\\\ & & \\phantom{\\sum_{\\sg^{(1)}}\\sum_{\\sg^{(2 ) } } } \\exp\\left\\{-h(\\sg^{(1)},\\sg^{(2)};\\jg^{\\mbox{out}})+ \\sum_{i=1}^n \\gamma_i\\epsilon_i(\\sg^{(1)})\\right\\}\\\\ & & { \\cal f}[\\thg;\\jg^{(0)},\\jg^{(1)},\\jg^{(2 ) } ] \\equiv   \\left.\\sum_{i=1}^n\\theta_i\\gamma_i- \\log\\left({\\cal z}[\\gg]\\right ) \\right|_{\\theta_i = \\frac{\\partial\\log({\\cal z})}{\\partial\\gamma_i } } \\end{aligned}\\ ] ] where @xmath198 is given in eq.([probabilitaturbo2 ] ) .",
    "+ it is then evident that @xmath196 =   { \\cal f}[\\thg;\\jg^{(0)},\\jg^{(1)},\\zb ] + { \\cal f}[\\thg;\\jg^{(0)},\\zb,\\jg^{(2)}]- { \\cal f}[\\thg;\\jg^{(0)},\\zb,\\zb]\\end{aligned}\\ ] ] i.e. turbo decoding neglects terms of order @xmath199 .",
    "we would like to compute the error probability per bit . as explained in the introduction , in the case of a symmetric transmission channel ,",
    "it is enough to compute the magnetization in the case of all inputs @xmath200 .",
    "the error probability per bit is given by the probability of a local magnetization being negative .",
    "+ the similarity of the hamiltonian ( [ turbohamiltonian ] ) with the hamiltonians of disordered spin systems is obvious .",
    "the disorder in the case of turbo codes has two origins .",
    "one is due to the ( random ) permutation which defines the particular code .",
    "the other is more conventional and is related to the randomness of the couplings which is due to the transmission noise . as usual in disordered systems , we can only compute the average over disorder and for that we have to introduce replicas .",
    "+ let us define the expectation value of the operator @xmath126 defined in eqs.([turboregistro1],[turboregistro2 ] ) with respect to the probability distribution given by eqs.([probabilitaturbo],[probabilitaturbo2 ] ) : @xmath201\\equiv \\sum_{\\sg^{(1)}}\\sum_{\\sg^{(2 ) } } \\epsilon_i(\\sg^{(1)})p(\\sg^{(1)},\\sg^{(2)}|\\jg^{\\mbox{out}})\\end{aligned}\\ ] ] the statistical properties of a turbo code can be derived from the probability distribution of this expectation value : @xmath202\\ ,   \\delta\\left(\\theta-\\theta_i[\\jg^{\\mbox{out}},p]\\right ) \\quad i=1,\\dots , n \\label{distribuzionesito}\\end{aligned}\\ ] ] where @xmath203 = \\prod_{n=0}^2\\prod_{i=1}^n q(j^{(n)}_i|+1)dj^{(n)}_i\\end{aligned}\\ ] ] then we define the average distribution @xmath204\\ ; \\delta\\left(\\theta-\\theta_i[\\jg^{\\mbox{out}},p]\\right)\\end{aligned}\\ ] ] where the sum runs over all possible permutations .",
    "@xmath205 is expected not to depend upon the site @xmath75 in the thermodynamic limit ( @xmath150 ) .",
    "+ the average error probability per bit is given by @xmath206 in any case @xmath207 is an upper bound for the error probability of the `` best '' code ( i.e. the one buildt with the permutation which yields the lowest error probability ) . + the replicated partition function is given by : @xmath208\\;\\sum_{\\{\\sg^{(1),a}\\}}\\sum_{\\{\\sg^{(2),a}\\ } } \\prod_{a=1}^n \\prod_{i=1}^n \\delta(\\epsilon_{p(i)}(\\sg^{(1),a}),\\epsilon_{i}(\\sg^{(2),a}))\\nonumber\\\\ & & \\phantom{\\sum_{p}w(p)\\int \\!dq[\\jg^{\\mbox{out}}]\\ ; } \\exp\\left\\{-\\sum_{a=1}^n h(\\sg^{(1),a},\\sg^{(2),a};\\jg^{\\mbox{out } } ) \\right\\}\\end{aligned}\\ ] ] the average over permutations can be done by introducing a matrix representation of the permutation @xmath209 to sum over permutations , one sums over all matrices @xmath210 or @xmath211 with the constrain @xmath212 .",
    "one may use the identity @xmath213 to write @xmath214   \\end{aligned}\\ ] ] it can be shown @xcite that the `` effective action '' which is obtained in this way describes two one - dimensional models coupled by a mean field - like interaction .",
    "+ this is easily seen by making use of the occupation densities @xcite defined below : @xmath215 the resulting replicated partition function reads @xcite : @xmath216\\;\\sum_{\\{\\sg^{(1),a}\\}}\\sum_{\\{\\sg^{(2),a}\\ } } \\prod_{\\underline{\\epsilon } } \\delta_{nc_1(\\underline{\\epsilon}),nc_2(\\underline{\\epsilon})}\\\\ & & \\phantom{\\int \\!dq[\\jg^{\\mbox{out}}]\\ ; } \\exp\\left\\{-\\sum_{a=1}^n h(\\sg^{(1),a},\\sg^{(2),a};\\jg^{\\mbox{out}})+ n\\sum_{\\underline{\\epsilon}}c_1(\\underline{\\epsilon } ) \\log c_1(\\underline{\\epsilon})\\right\\}\\nonumber\\end{aligned}\\ ] ] we briefly report here the main results of this approach for the gaussian channel described by eq.([gaussianchannel ] ) .",
    "a detailed analysis will be presented elsewhere @xcite .",
    "+ for recursive turbo codes there exists a low noise phase @xmath217 where the error probability vanishes in the thermodynamic limit ( i.e. for infinitely long sequences ) . in this phase the model",
    "is completely ordered : @xmath218 a local stability analysis yields the critical value @xmath219 such that for @xmath220 the no - error phase is destroyed by small fluctuations .",
    "clearly @xmath221 .",
    "we computed @xmath219 for the two cases listed below .",
    "+ for both the rate is @xmath222 so that the shannon noise threshold as given by eq.([gaussiancapacity ] ) is @xmath223 .",
    "error free communication can take place only for @xmath224 .    * for model [ codicesemplice ] , defined by eqs.([codicesemplice1]),([codicesemplice2 ] ) one gets @xmath225 . * for model [ codicetipico ] , defined by eqs.([codicetipico1]),([codicetipico2 ] ) one obtains @xmath226 where @xmath227 is the only real solution of the equation @xmath228 the resulting value @xmath229 is quite near to the shannon threshold .",
    "we formulated turbo codes as a spin model hamiltonian and we obtained new results using the replica method .",
    "it is well known that this method is not mathematically rigorous .",
    "so it is natural to question the validity of our results . for this purpose",
    "we have carried out numerical simulations of the following codes : the recursive turbo code corresponding to the convolutional code [ codicesemplice ] of sec.([convolutionalsection ] ) , its error probability is reported in fig.([trecneargraf ] ) ; the non recursive turbo code obtained by permuting the generating polynomials of the previous one ( see fig.([turboneargraf ] ) ) ; the recursive turbo code corresponding to the code [ codicetipico ] of the same section ( see fig.([trecgraf ] ) ) .",
    "we used the berrou et al .",
    "turbo decoding algorithm and averaged over @xmath230 to @xmath231 realizations of the disorder .",
    "+ the first conclusion is that recursive turbo codes are much better codes than non recursive ones .",
    "furthemore our results for recursive turbo codes are compatible with the existence of a threshold @xmath232 such that for @xmath233 the error probability per bit is zero , while no such threshold seems to exist for non recursive codes .",
    "this is in agreement with replica theory .",
    "zero error probability can only be achieved in the @xmath234 limit .",
    "our simulations are for @xmath235 .",
    "it would be interesting to perform a detailed study of finite size corrections , i.e. of the @xmath3 dependence of the error probability per bit .",
    "+ we now discuss the numerical value of the noise threshold @xmath232 .",
    "the first remark is that both numerically and analytically , the critical value is below shannon s bound and that it depends on the convolutional code ( i.e. on the generating polynomials ) .",
    "the second remark is that the analytical value of thresold , @xmath236 db is in very good agreement with the numerical value for the code [ codicesemplice ] . for code [ codicetipico ] @xmath237 db while one gets @xmath238 db from the simulations",
    ". it would be interesting to understand this disagreement .",
    "as we said in the previous section , @xmath239 was calculated by a local stability analysis of the ordered phase , i.e. we assumed that the transition is of second order .",
    "a possible explanation would be that the transition is second order for code [ codicesemplice ] and first order for code [ codicetipico ] .",
    "numerical results seem to support this hypothesis , as the variation of the error probability as a function of noise is much sharper in case [ codicetipico ] . but a much more careful analysis of finite size effects is necessary in order to settle this question numerically .",
    "one should also look analytically for the occurence of a first order transition . +",
    "another important issue is the breaking of replica symmetry .",
    "since turbo - decoding is thought to be an approximate algorithm , it may be not the best tool to look for replica symmetry breaking .",
    "we have started an analytical investigation of replica symmetry breaking .",
    "n.sourlas . nature * 339*(1989 ) 693 - 694 + n.sourlas , in _ statistical mechanics of neural networks _ , lecture notes in physics 368 , ed .",
    "l. garrido , springer verlag ( 1990 ) + n.sourlas , ecole normale suprieure preprint ( april 1993 ) + n.sourlas , in _ from statistical physics to statistical inference and back , _ ed .",
    "p. grassberger and j .- p .",
    "nadal , kluwer academic ( 1994 ) , page 195 ."
  ],
  "abstract_text": [
    "<S> the `` turbo codes '' , recently proposed by berrou et . </S>",
    "<S> al . </S>",
    "<S> @xcite are written as a disordered spin hamiltonian . </S>",
    "<S> it is shown that there is a threshold @xmath0 such that for signal to noise ratios @xmath1 the error probability per bit vanishes in the thermodynamic limit , i.e. the limit of infinitly long sequences . </S>",
    "<S> the value of the threshold has been computed for two particular turbo codes . </S>",
    "<S> it is found that it depends on the code . </S>",
    "<S> these results are compared with numerical simulations .    </S>",
    "<S> lptens 99/29 </S>"
  ]
}