{
  "article_text": [
    "the original linear support vector machine ( svm ) aims to find a hyperplane that separates a collection of data points .",
    "since it was proposed in @xcite , it has been widely used for binary classifications , such as texture classification @xcite , gene expression data analysis @xcite , face recognition @xcite , to name a few .",
    "mathematically , given a set of samples @xmath1 in @xmath2-dimensional space and each @xmath3 attached with a label @xmath4 , the linear svm learns a hyperplane @xmath5 from the training samples .",
    "a new data point @xmath6 can be categorized into the `` @xmath7 '' or `` @xmath8 '' class by inspecting the sign of @xmath9 .",
    "the binary - class svm ( b - svm ) has been generalized to multicategory classifications to tackle problems that have data points belonging to more than two classes .",
    "the initially proposed multi - class svm ( m - svm ) methods construct several binary classifiers , such as  one - against - all @xcite , `` one - against - one '' @xcite , and `` directed acyclic graph svm '' @xcite .",
    "these m - svms may suffer from data imbalance , namely , some classes have much fewer data points than others , which can result in inaccurate predictions .",
    "one alternative is to put all the data points together in one model , which results in the so - called `` all - together '' m - svms .",
    "the `` all - together '' m - svms train multi - classifiers by considering a single large optimization problem . an extensive comparison of different m - svms can be found in @xcite . in this paper , we consider both b - svm and m - svm .",
    "more precisely , we consider the binary - class huberized svm ( b - hsvm ) ( see below ) for b - svm and the `` all - together '' multi - class hsvm ( m - hsvm ) ( see below ) for m - svm .",
    "the advantage of hsvm over classic svm with hinge loss is the continuous differentiability of its loss function , which enables the use of the `` fastest '' first - order method : proximal gradient ( pg ) method @xcite ( see the overview in section [ sec : pg ] ) .",
    "we demonstrate that the pg method is in general much faster than existing methods for ( regularized ) b - svm and m - svm while yielding comparable prediction accuracies .",
    "in addition , extensive numerical experiments are done to compare our method to state - of - the - art ones for both b - svm and m - svm on synthetic and also benchmark datasets .",
    "statistical comparison is also performed to show the difference between the proposed method and other compared ones .",
    "b - hsvm appears to be first considered in .",
    "this modification is similar to that used in @xcite , and the extra term usually does not affect the prediction but makes pg method converge faster .",
    "] by wang _",
    "et al _ in @xcite .",
    "they demonstrate that b - hsvm can perform better than the original unregularized svm and also @xmath0-regularized svm ( i.e. , with @xmath10 ) for microarray classification .",
    "a closely related model to b - hsvm is the elastic net regularized svm @xcite @xmath11_++\\lambda_1\\|{{\\bf w}}\\|_1+\\frac{\\lambda_2}{2}\\|{{\\bf w}}\\|_2 ^ 2,\\ ] ] where @xmath12_+=\\max(0,1-t)$ ] is the hinge loss function .",
    "the @xmath0-norm regularizer has the ability to perform continuous shrinkage and automatic variable selection at the same time @xcite , and the @xmath13-norm regularizer can help to reduce the variance of the estimated coefficients and usually results in satisfactory prediction accuracy , especially in the case where there are many correlated variables .",
    "the elastic net inherits the benefits of both @xmath0 and @xmath13-norm regularizers and can perform better than either of them alone , as demonstrated in @xcite .",
    "note that uses non - differentiable hinge loss function while b - hsvm uses differentiable huberized hinge loss function .",
    "the differentiability makes b - hsvm relatively easier to solve .",
    "a path - following algorithm was proposed by wang _ et .",
    "@xcite for solving b - hsvm .",
    "their algorithm is not efficient in particular for large - scale problems , since it needs to track the disappearance of variables along a regularization path .",
    "recently , yang and zou @xcite proposed a generalized coordinate descent ( gcd ) method , which was , in most cases , about 30 times faster than the path - following algorithm .",
    "however , the gcd method needs to compute the gradient of the loss function of b - hsvm after each coordinate update , which makes the algorithm slow .",
    "m - hsvm has been considered in @xcite , which generalizes the work @xcite on b - hsvm to m - hsvm and also makes a path - following algorithm",
    ". however , their algorithm could be even worse since it also needs to track the disappearance of variables along a regularization path and m - hsvm often involves more variables than those of b - hsvm .",
    "hence , it is not suitable for large - scale problems either .",
    "similar to m - hsvm , several other models have been proposed to train multiple classifiers by solving one single large optimization problem to handle the multi - category classification , such as the @xmath0-norm regularized m - svm in @xcite and the @xmath14-norm regularized m - svm in @xcite .",
    "again , these models use the non - differentiable hinge loss function and are relatively more difficult than m - hsvm to solve .",
    "there are also methods that use binary - classifiers to handle multicategory classification problems including `` one - against - all '' @xcite , `` one - against - one '' @xcite , and `` directed acyclic graph svm '' @xcite .",
    "the work @xcite makes a thorough review of methods using binary - classifiers for multi - category classification problems and gives extensive experiments on various applications . in",
    "a follow up and more recent paper @xcite the same authors present a dynamic classifier weighting method which deals with the limitations introduced by the non - competent classifiers in the one - versus - one classification strategy . in order to dynamically weigh the outputs of the individual classifiers , they use the nearest neighbor of every class from the instance that needs to be classified .",
    "furthermore in @xcite the authors propose a new approach for building multi - class classifiers based on the notion of data - clustering in the feature space .",
    "for the derived clusters they construct one - class classifiers which can be combined to solve complex classification problems .",
    "one key advantage of our algorithm is its scalability with the training sample size , and thus it is applicable for large - scale svms . while preparing this paper , we note that some other algorithms are also carefully designed for handling the large - scale svms , including the block coordinate frank - wolfe method @xcite and the stochastic alternating direction method of multiplier @xcite . in addition ,",
    "graphics processing unit ( gpu ) computing has been utilized in @xcite to run multiple training tasks in parallel to accelerate the cross validation procedure .",
    "furthermore , different variants of svms have been proposed for specific applications such as the value - at - risk svm for stability to outliers @xcite , the structural twin svm to contain prior domain knowledge @xcite , the hierarchical svm for customer churn prediction @xcite and the ellipsoidal svm for outlier detection in wireless sensor networks @xcite .",
    "finally in @xcite a two - ellipsoid kernel decomposition is proposed for the efficient training of svms . in order to avoid the use of socp techniques , introduced by the ellipsoids ,",
    "the authors transform the data using a matrix which is determined from the sum of the classes covariances . with that transformation it is possible to use classical svm , and their method can be incorporated into existing svm libraries .",
    "we develop an efficient pg method to solve the b - hsvm @xmath15 and the `` all - together '' m - hsvm @xmath16 \\text { s.t . } & { \\mathbf{w}}{{\\bf e}}= \\mathbf{0 } , { { \\bf e}}^\\top{{\\bf b}}= 0 . \\end{array}\\ ] ] in , @xmath4 is the label of @xmath3 , and @xmath17 is the huberized hinge loss function which is continuously differentiable . in , @xmath18 is the @xmath19-th label , @xmath20 , @xmath21 if @xmath22 and @xmath23 otherwise , @xmath24 denotes the vector with all one s , and @xmath25 denotes the @xmath26-th column of @xmath27",
    ". the constraints @xmath28 and @xmath29 in are imposed to eliminate redundancy in @xmath30 and also are necessary to make the loss function @xmath31 fisher - consistent @xcite .",
    "we choose the pg methodology because it requires only first - order information and converges fast .",
    "as shown in @xcite , it is an optimal first - order method .",
    "note that the objectives in and have non - smooth terms and are not differentiable .",
    "hence , direct gradient or second - order methods such as the interior point method are not applicable .",
    "we speed up the algorithm by using a two - stage method , which detects the support of the solution at the first stage and solves a lower - dimensional problem at the second stage . for large - scale problems with sparse features ,",
    "the two - stage method can achieve more than 5-fold acceleration .",
    "we also analyze the convergence of pg method under fairly general settings and get similar results as those in @xcite .",
    "in addition , we compare the proposed method to state - of - the - art ones for b - svm and m - svm on both synthetic and benchmark datasets .",
    "extensive numerical experiments demonstrate that our method can outperform other compared ones in most cases .",
    "statistical tests are also performed to show significant differences between the compared methods .",
    "the rest of the paper is organized as follows . in section [ sec : algorithm ] , we overview the pg method and then apply it to and .",
    "in addition , assuming strong convexity , we show linear convergence of the pg method under fairly general settings .",
    "numerical results are given in section [ sec : numerical ] .",
    "finally , section [ sec : conclusion ] concludes the paper .",
    "in this section , we first give an overview of the pg method .",
    "then we apply it to and .",
    "we complete this section by showing that under strong convexity assumptions the pg method possesses linear convergence .",
    "consider convex composite optimization problems in the form of @xmath32 where @xmath33 is a convex set , @xmath34 is a differentiable convex function with lipschitz continuous gradient ( that is , there exists @xmath35 such that @xmath36 , for all @xmath37 ) , and @xmath38 is a proper closed convex function .",
    "the pg method for solving iteratively updates the solution by @xmath39 where @xmath40 & + \\frac{l_k}{2}\\|{{\\bf u}}-\\hat{{{\\bf u}}}^{k-1}\\|^2+\\xi_2({{\\bf u } } ) , \\end{array}\\ ] ] @xmath41 and @xmath42 for some nonnegative @xmath43 .",
    "the extrapolation technique can significantly accelerate the algorithm .    when @xmath44 is the lipschitz constant of @xmath45 , it can easily be shown that @xmath46 , and thus this method is a kind of majorization minimization , as illustrated in figure [ fig : pg ] . with appropriate choice of @xmath47 and @xmath44",
    ", the sequence @xmath48 converges to the optimal value @xmath49 .",
    "nesterov @xcite , and beck and teboulle @xcite showed , independently , that if @xmath50 and @xmath44 is taken as the lipschitz constant of @xmath45 , then @xmath48 converges to @xmath51 with the rate @xmath52 , namely , @xmath53 in addition , using carefully designed @xmath47 , they were able to show that the convergence rate can be increased to @xmath54 , which is the optimal rate of first - order methods for general convex problems @xcite .",
    "is a majorization approximation of @xmath55 at @xmath56 , which is an extrapolated point of @xmath57 and @xmath58 .",
    "the new iterate @xmath59 is the minimizer of @xmath60.,scaledwidth=40.0% ]      we first write the b - hsvm problem into the general form of .",
    "let @xmath61 f(b,{{\\bf w}})=\\frac{1}{n}\\sum_{i=1}^nf_i(b,{{\\bf w}}),\\\\[0.2 cm ] g(b,{{\\bf w}})=\\lambda_1\\|{{\\bf w}}\\|_1+\\frac{\\lambda_2}{2}\\|{{\\bf w}}\\|^2+\\frac{\\lambda_3}{2}b^2 .",
    "\\end{array}\\right.\\ ] ] then can be written as @xmath62 for convenience , we use the following notation in the rest of this section @xmath63    [ prop : f ] the function @xmath64 defined above is differentiable and convex , and its gradient @xmath65 is lipschitz continuous with constant @xmath66    the proof of this proposition involves standard arguments and can be found in the appendix .",
    "now , we are ready to apply pg to .",
    "define the _ proximal operator _ for a given extended real - valued convex function @xmath67 on @xmath68 by @xmath69 replacing @xmath34 and @xmath38 in with @xmath64 and @xmath70 , respectively , we obtain the update @xmath71 { { \\bf w}}^k=\\frac{1}{l_k+\\lambda_2}{\\mathcal{s}}_{\\lambda_1}\\big(l_k\\hat{{{\\bf w}}}^{k-1}-\\nabla_{{\\bf w}}f(\\hat{{{\\bf u}}}^{k-1})\\big ) , \\end{array}\\right.\\ ] ] where @xmath72 is a component - wise shrinkage operator defined by @xmath73 .    in , we dynamically update @xmath44 by @xmath74 where @xmath75 is a pre - selected constant , @xmath76 is defined in and @xmath77 is the smallest integer such that the following condition is satisfied @xmath78 \\le & f(\\hat{{{\\bf u}}}^{k-1})+\\left\\langle\\nabla f(\\hat{{{\\bf u}}}^{k-1 } ) , { \\mathrm{prox}}_{h_k}({{\\bf v}}^{k-1})-\\hat{{{\\bf u}}}^{k-1}\\right\\rangle\\\\ & \\hspace{0.2cm}+\\frac{l_k}{2}\\left\\|{\\mathrm{prox}}_{h_k}({{\\bf v}}^{k-1})-\\hat{{{\\bf u}}}^{k-1}\\right\\|^2 , \\end{array}\\ ] ] where @xmath79 , @xmath80 and @xmath81 for some weight @xmath82 .",
    "the inequality in is necessary to make the algorithm convergent ( see lemma [ lem : key ] in the appendix ) .",
    "it guarantees sufficient decrease , and the setting @xmath83 will make it satisfied . in the case where @xmath76 becomes unnecessarily large , a smaller @xmath44 can speed up the convergence . to make the overall objective non - increasing , we re - update @xmath84 by setting @xmath85 in whenever @xmath86 .",
    "_ this non - increasing monotonicity is very important , since we observe that the pg method may be numerically unstable without this modification .",
    "in addition , it significantly accelerates the pg method ; see table [ table : diffset ] below .",
    "_    algorithm [ alg : mhhsvm ] summarizes our discussion .",
    "we name it as b - pgh .",
    "sample - label pairs @xmath87 ; parameters @xmath88 .",
    "choose @xmath89 , @xmath90 ; compute @xmath76 from and choose @xmath75 and @xmath91 ; set @xmath92 .",
    "let @xmath81 for some @xmath43 ; update @xmath44 according to and @xmath84 according to ; re - update @xmath84 according to with @xmath85 ; let @xmath93 .",
    "in this section we generalize the pg method for solving multi - class classification problems .",
    "denote @xmath94 let @xmath95 then we can write as @xmath96 similar to proposition [ prop : f ] , we can show that @xmath97 is lipschitz continuous with constant @xmath98 the proof is essentially the same as that of proposition [ prop : f ] , and we do not repeat it .",
    "now we are ready to apply the pg method to or equivalently .",
    "we update @xmath99 iteratively by solving the @xmath100-subproblem @xmath101 and @xmath27-subproblem @xmath102 where @xmath44 is chosen in the same way as .",
    "problem is relatively easy and has a closed form solution .",
    "let @xmath103\\in{\\mathbb{r}}^{j\\times ( j-1)}$ ] and @xmath104 be the vector consisting of the first @xmath105 components of @xmath100 , where @xmath106 denotes the identity matrix . substituting @xmath107 to gives the solution @xmath108 hence , the update in can be explicitly written as @xmath109 where @xmath110 is defined in the above equation .",
    "problem can be further decomposed into @xmath2 independent small problems .",
    "each of them involves only one row of @xmath27 and can be written in the following form @xmath111 where @xmath112 and @xmath113 is the @xmath19-th row - vector of the matrix @xmath114 for the @xmath19-th small problem .",
    "the coexistence of the non - smooth term @xmath115 and the equality constraint @xmath116 makes a difficult optimization problem to solve .",
    "next we describe a new efficient yet simple method for solving using its dual problem , defined by @xmath117    since is strongly convex , @xmath118 is concave and continuously differentiable . hence , the solution @xmath119 of can be found by solving the single - variable equation @xmath120 .",
    "it is easy to verify that @xmath121 and @xmath122 , so the solution @xmath119 lies between @xmath123 and @xmath124 , where @xmath125 and @xmath126 respectively denote the minimum and maximum components of @xmath127 .",
    "in addition , note that @xmath128 is piece - wise linear about @xmath129 , and the breakpoints are at @xmath130 , @xmath131 .",
    "hence @xmath119 must be in @xmath132 $ ] for some @xmath133 , where @xmath134 is the sorted vector of @xmath135 in the increasing order .",
    "therefore , to solve , we first obtain @xmath134 , then search the interval that contains @xmath119 by checking the sign of @xmath136 at the breakpoints , and finally solve @xmath120 within that interval .",
    "algorithm [ alg : dual ] summarizes our method for solving .",
    "@xmath137 with @xmath127 in @xmath138-dimensional space and @xmath139 .",
    "let @xmath140\\in{\\mathbb{r}}^{2j}$ ] and sort @xmath134 as @xmath141 ; set @xmath142 .",
    "if @xmath143 , set @xmath144 ; else @xmath145 .",
    "solve @xmath120 within @xmath132 $ ] and output the solution @xmath119 .    after determining @xmath119",
    ", we can obtain the solution of by setting @xmath146 .",
    "algorithm [ alg : multihsvm ] summarizes the main steps of the pg method for efficiently solving .",
    "we name it as m - pgh .",
    "sample - label pairs @xmath87 with each @xmath147 ; parameters @xmath88 .",
    "choose @xmath148 , @xmath149 ; compute @xmath150 in and choose @xmath75 and @xmath151 ; set @xmath92 .",
    "let @xmath152 for some @xmath43 ; choose @xmath44 in the same way as ; update @xmath153 by ; set @xmath127 to the @xmath19-th row of @xmath114 ; solve by algorithm [ alg : dual ] with input @xmath154 and let @xmath119 be the output ; set the @xmath19-th row of @xmath155 to be @xmath156 ; re - update @xmath153 and @xmath155 according to and with @xmath157 ; let @xmath93 .      instead of analyzing the convergence of algorithms [ alg : mhhsvm ] and [ alg : multihsvm ] , we do the analysis of the pg method for with general @xmath34 and @xmath38 and regard algorithms [ alg : mhhsvm ] and [ alg : multihsvm ] as special cases . throughout our analysis",
    ", we assume that @xmath34 is a differentiable convex function and @xmath45 is lipschitz continuous with lipschitz constant @xmath158 .",
    "we also assume that @xmath38 is strongly convex , we only have sublinear convergence as shown in @xcite . ] with constant @xmath159 , namely , @xmath160 for any @xmath161 and @xmath162 , where @xmath163 denotes the domain of @xmath38 .",
    "similar results have been shown by nesterov @xcite and schmidt _ et al _ @xcite .",
    "however , our analysis fits to more general settings .",
    "specifically , we allow dynamical update of the lipschitz parameter @xmath44 and an acceptable interval of the parameters @xmath164 . on the other hand , @xcite either require @xmath44 to be fixed to the lipschitz constant of @xmath34 or require specific values for the @xmath164 s .",
    "in addition , neither of @xcite do the re - update step as in line 7 of algorithm [ alg : mhhsvm ] or line 13 of algorithm [ alg : multihsvm ] .",
    "we tested the pg method under different settings on synthetic datasets , generated in the same way as described in section [ sec : syn ] .",
    "our goal is to demonstrate the practical effect that each setting has on the overall performance of the pg method .",
    "table [ table : diffset ] summarizes the numerical results , which show that pg method under our settings converges significantly faster than that under the settings of @xcite .    to analyze the pg method under fairly general settings",
    ", we use the so - called kurdyka - ojasiewicz ( kl ) inequality , which has been widely applied in non - convex optimization .",
    "our results show that the kl inequality can also be applied and simplify the analysis in convex optimization .",
    "extending the discussion of the kl inequality is beyond the scope of this paper and therefore we refer the interested readers to @xcite and the references therein .    [",
    "cols=\"^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]      as in section [ sec : stat - real ] , we also performed statistical comparison of m - pgh , ova , and ddag on the benchmark datasets together with the two microarray datasets .",
    "table [ table : wilcoxon - ben ] shows their @xmath56-values and corresponding @xmath2-values of the wilcoxon signed - rank test . from the table",
    ", we see that there is no significant difference between any pair of the three methods at @xmath2-@xmath165 .",
    "however , m - pgh makes significant difference with ova at @xmath2-@xmath166 .",
    "average ranks of m - pgh , ova , and ddag according to their prediction accuracies on the 10 datasets are shown in table [ table : fried - ben ] together with the friedman statistic and @xmath2-value .",
    "again , we see that there is no significant difference among the three methods at @xmath2-@xmath165 but there is at @xmath2-@xmath166 .",
    "we further did a post - hoc test using the holm s step - down procedure as in section [ sec : stat - real ] .",
    "m - pgh was set as the control classifier .",
    "the @xmath2-@xmath167 are 0.0253 and 0.0736 respectively for ova and ddag .",
    "hence , m - pgh made significant differences with ova and ddag at @xmath2-@xmath168 .",
    "average rank & 1.4 & 2.4 & 2.2 +   +",
    "svms have been popularly used to solve a wide variety of classification problems .",
    "the original svm model uses the non - differentiable hinge loss function , which together with some regularizers like @xmath0-term makes it difficult to develop simple yet efficient algorithms .",
    "we considered the huberized hinge loss function that is a differentiable approximation of the original hinge loss .",
    "this allowed us to apply pg method to both binary - class and multi - class svms in an efficient and accurate way .",
    "in addition , we presented a two - stage algorithm that is able to solve very large - scale binary classification problems . assuming strong convexity and under fairly general assumptions",
    ", we proved the linear convergence of pg method when applied in solving composite problems in the form of , special cases of which are the binary and multi - class svms .",
    "we performed a wide range of numerical experiments on both synthetic and real datasets , demonstrating the superiority of the proposed algorithms over some state - of - the - art algorithms for both binary and multi - class svms .",
    "in particular for large - scale problems , our algorithms are significantly faster than compared methods in all cases with comparable accuracy . finally , our algorithms are more robust to the smoothing parameter @xmath169 in terms of cpu time .",
    "the authors would like to thank four anonymous referees and the associate editor for their valuable comments and suggestions that improved the quality of our paper .",
    "the first author would also like to thank professor wotao yin for his valuable discussion .",
    "first , we derive the convexity of each @xmath170 from the composition of the convex function @xmath171 and the linear transformation @xmath172 ( e.g. , see @xcite ) .",
    "thus , @xmath64 is also convex since it is the sum of @xmath173 convex functions .",
    "secondly , it is easy to verify that @xmath174 and it is lipschitz continuous , namely , @xmath175 for any @xmath176 .    using the notations in ( [ notation_uv ] ) and by the chain rule",
    ", we have @xmath177 , for @xmath178 and @xmath179 hence , for any @xmath180 , we have @xmath181 \\le&~\\hspace{-0.2cm}\\frac{1}{n}\\sum_{i=1}^n\\left\\|\\nabla f_i({{\\bf u}})-\\nabla f_i(\\hat{{{\\bf u}}})\\right\\|\\\\[0.2 cm ] = & \\frac{1}{n}\\sum_{i=1}^n\\left\\|\\left(\\phi'({{\\bf v}}_i^\\top { { \\bf u}})-\\phi'({{\\bf v}}_i^\\top \\hat{{{\\bf u}}})\\right){{\\bf v}}_i\\right\\|\\\\[0.2 cm ] \\le&\\frac{1}{n\\delta}\\sum_{i=1}^n|{{\\bf v}}_i^\\top ( { { \\bf u}}-\\hat{{{\\bf u}}})|\\|{{\\bf v}}_i\\|\\\\[0.2 cm ] \\le & \\frac{1}{n\\delta}\\sum_{i=1}^n \\|{{\\bf v}}_i\\|^2\\|{{\\bf u}}-\\hat{{{\\bf u}}}\\| , \\end{array}\\ ] ] which completes the proof .",
    "we begin our analysis with the following lemma , which can be shown through essentially the same proof of lemma 2.3 in @xcite .",
    "[ lem : key ] let @xmath182 be the sequence generated by with @xmath183 such that for all @xmath184 , @xmath185 then for all @xmath186 , it holds for any @xmath187 that @xmath188 & \\hspace{0.5cm}+l_k\\langle \\hat{{{\\bf u}}}^{k-1}-{{\\bf u } } , { { \\bf u}}^k-\\hat{{{\\bf u}}}^{k-1}\\rangle .",
    "\\end{array}\\ ] ]    we also need the following lemma , which is the kl property for strongly convex functions .",
    "[ lem : kl ] suppose @xmath55 is strongly convex with constant @xmath159",
    ". then for any @xmath189 and @xmath190 , we have @xmath191    for any @xmath190 , we have from the strong convexity of @xmath55 and the cauchy - schwarz inequality that @xmath192 which completes the proof .",
    "we first show @xmath193 . letting @xmath194 in",
    "gives @xmath195 summing up the inequality over @xmath196 , we have @xmath197 \\ge&\\frac{\\mu}{2}\\sum_{k=1}^k\\|{{\\bf u}}^{k-1}-{{\\bf u}}^k\\|^2 + \\frac{l_k}{2}\\|{{\\bf u}}^k-{{\\bf u}}^{k-1}\\|^2 , \\end{array}\\ ] ] which implies @xmath198 since @xmath199 is lower bounded .",
    "note that @xmath182 is bounded since @xmath55 is coercive and @xmath48 is upper bounded by @xmath200 .",
    "hence , @xmath182 has a limit point @xmath201 , so there is a subsequence @xmath202 converging to @xmath201 .",
    "note @xmath203 also converges to @xmath201 since @xmath198 . without loss of generality ,",
    "we assume @xmath204 .",
    "otherwise , we can take a convergent subsequence of @xmath205 . by",
    ", we have @xmath206 & \\hspace{1cm}+\\frac{l_{k_j+1}}{2}\\|{{\\bf u}}-\\hat{{{\\bf u}}}^{k_j}\\|^2+\\xi_2({{\\bf u } } ) . \\end{array}\\ ] ] letting @xmath207 in the above formula and observing @xmath208 yield @xmath209 which indicates @xmath210 .",
    "thus @xmath201 is the minimizer of @xmath55 .",
    "since @xmath48 is nonincreasing and lower bounded , it converges to @xmath211 .",
    "noting @xmath212 we have @xmath193 .",
    "next we go to show . without loss of generality ,",
    "we assume @xmath213 .",
    "otherwise , we can consider @xmath214 instead of @xmath55 .",
    "in addition , we assume @xmath215 for all @xmath216 because if @xmath217 for some @xmath218 , then @xmath219 for all @xmath220 .    for ease of description , we denote @xmath221 . letting @xmath222 in",
    ", we have @xmath223 noting @xmath224 and @xmath183 , we have for all @xmath225 , @xmath226 noting @xmath227 and using yield @xmath228 \\le & l_k\\|{{\\bf u}}^{k-1}-{{\\bf u}}^{k}\\|^2+\\frac{8l}{\\sqrt{\\mu}}\\left(\\sqrt{\\xi_k}-\\sqrt{\\xi_{k+1}}\\right)\\|{{\\bf u}}^k-{{{\\bf u}}}^{k-1}\\|\\\\[0.1 cm ] & + \\frac{8l}{\\sqrt{\\mu}}\\left(\\sqrt{\\xi_k}-\\sqrt{\\xi_{k+1}}\\right)\\|{{\\bf u}}^{k-1}-{{{\\bf u}}}^{k-2}\\|,\\end{array}\\ ] ] after rearrangements .",
    "take @xmath229 . using the inequalities @xmath230 and @xmath231 for @xmath232",
    ", we have from the above inequality that @xmath233 \\le&\\sqrt{l_k}\\|{{\\bf u}}^{k-1}-{{\\bf u}}^{k}\\|+\\frac{2l}{\\delta\\sqrt{\\mu}}\\left(\\sqrt{\\xi_k}-\\sqrt{\\xi_{k+1}}\\right)\\\\[0.1 cm ] & \\hspace{0.2cm}+\\delta\\left(\\|{{\\bf u}}^k-{{{\\bf u}}}^{k-1}\\|+\\|{{\\bf u}}^{k-1}-{{{\\bf u}}}^{k-2}\\|\\right).\\end{array}\\ ] ] summing up the above inequality over @xmath196 and noting @xmath234 yield @xmath235 \\le & \\left(\\sqrt{l}+2\\delta\\right)\\|{{\\bf u}}^{m-1}-{{\\bf u}}^m\\|+\\delta\\|{{\\bf u}}^{m-2}-{{\\bf u}}^{m-1}\\|+\\frac{2l}{\\delta\\sqrt{\\mu}}\\sqrt{\\xi_m},\\end{array}\\ ] ] which together with @xmath236 for all @xmath216 implies @xmath237 \\le & c_1 \\sqrt{\\xi_m}+c_2\\left(\\|{{\\bf u}}^{m-1}-{{\\bf u}}^m\\|+\\|{{\\bf u}}^{m-2}-{{\\bf u}}^{m-1}\\|\\right ) , \\end{array}\\ ] ] where @xmath238 denote @xmath239 and write as @xmath240 which together with gives @xmath241 = & c_3(s_{m-2}-s_m),\\text { for all } m\\ge 2 .",
    "\\end{array}\\ ] ] let @xmath242 and @xmath243",
    ". then we have @xmath244 note @xmath245 , and thus we complete the proof .",
    "l.  bottou , c.  cortes , j.s .",
    "denker , h.  drucker , i.  guyon , l.d .",
    "jackel , y.  lecun , u.a .",
    "muller , e.  sackinger , p.  simard , et  al .",
    "comparison of classifier methods : a case study in handwritten digit recognition . in _ proceedings of the 12th iapr international conference on pattern recognition _ , volume  2 , pages 7782 .",
    "ieee , 1994 .      m.p.s .",
    "brown , w.n .",
    "grundy , d.  lin , n.  cristianini , c.w .",
    "sugnet , t.s .",
    "furey , m.  ares , and d.  haussler .",
    "knowledge - based analysis of microarray gene expression data by using support vector machines .",
    ", 97(1):262 , 2000 .",
    "mikel galar , alberto fernndez , edurne barrenechea , humberto bustince , and francisco herrera .",
    "an overview of ensemble methods for binary classifiers in multi - class problems : experimental study on one - vs - one and one - vs - all schemes .",
    ", 44(8):17611776 , 2011 .",
    "golub , d.k .",
    "slonim , p.  tamayo , c.  huard , m.  gaasenbeek , j.p .",
    "mesirov , h.  coller , m.l .",
    "loh , j.r .",
    "downing , m.a .",
    "caligiuri , et  al .",
    "molecular classification of cancer : class discovery and class prediction by gene expression monitoring .",
    ", 286(5439):531537 , 1999 .",
    "b.  heisele , p.  ho , and t.  poggio .",
    "face recognition with support vector machines : global versus component - based approach . in _",
    "computer vision , 2001 .",
    "iccv 2001 .",
    "eighth ieee international conference on _ , volume  2 , pages 688694 .",
    "ieee , 2001 .",
    "j.  khan , j.s .",
    "wei , m.  ringnr , l.h .",
    "saal , m.  ladanyi , f.  westermann , f.  berthold , m.  schwab , c.r .",
    "antonescu , c.  peterson , et  al . classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks . , 7(6):673679 , 2001 .",
    "hua ouyang , niao he , long tran , and alexander gray .",
    "stochastic alternating direction method of multipliers . in _ proceedings of the 30th international conference on machine learning _ , pages 8088 , 2013 .",
    "ye , y.  chen , and x.  xie .",
    "efficient variable selection in support vector machines via the alternating direction method of multipliers . in _ proceedings of the international conference on artificial intelligence and statistics _ , 2011 ."
  ],
  "abstract_text": [
    "<S> the support vector machine ( svm ) has been used in a wide variety of classification problems . </S>",
    "<S> the original svm uses the hinge loss function , which is non - differentiable and makes the problem difficult to solve in particular for regularized svms , such as with @xmath0-regularization . </S>",
    "<S> this paper considers the huberized svm ( hsvm ) , which uses a differentiable approximation of the hinge loss function . </S>",
    "<S> we first explore the use of the proximal gradient ( pg ) method to solving binary - class hsvm ( b - hsvm ) and then generalize it to multi - class hsvm ( m - hsvm ) . under strong convexity assumptions , </S>",
    "<S> we show that our algorithm converges linearly . </S>",
    "<S> in addition , we give a finite convergence result about the support of the solution , based on which we further accelerate the algorithm by a two - stage method . </S>",
    "<S> we present extensive numerical experiments on both synthetic and real datasets which demonstrate the superiority of our methods over some state - of - the - art methods for both binary- and multi - class svms . </S>"
  ]
}