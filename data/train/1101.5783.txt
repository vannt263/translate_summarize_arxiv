{
  "article_text": [
    "supervised classification , also known as pattern recognition , is a fundamental problem in statistics , as it represents an abstraction of the decision - making problem faced by many applied practitioners .",
    "examples include a doctor making a medical diagnosis , a handwriting expert performing an authorship analysis , or an email filter deciding whether or not a message is genuine .",
    "classifiers based on nearest neighbours are perhaps the simplest and most intuitively appealing of all nonparametric classifiers . the @xmath0-nearest neighbour classifier was originally studied in the seminal works of @xcite ( later republished as @xcite ) and @xcite , but it retains its popularity today .",
    "surprisingly , it is only recently that detailed understanding of the nature of the error probabilities has emerged @xcite .",
    "arguably the most obvious defect with the @xmath0-nearest neighbour classifier is that it places equal weight on the class labels of each of the @xmath0 nearest neighbours to the point @xmath5 being classified .",
    "intuitively , one would expect improvements in terms of the misclassification rate to be possible by putting decreasing weights on the class labels of the successively more distant neighbours .",
    "the first purpose of this paper is to describe the asymptotic structure of the difference between the misclassification rate ( risk ) of a weighted nearest neighbour classifier and that of the optimal bayes classifier for classification problems with feature vectors in @xmath6 .",
    "theorem  [ thm : main ] in section  [ sec : main ] below shows that , subject to certain regularity conditions on the underlying distributions of each class and the weights , this excess risk ( or _ regret _ ) asymptotically decomposes as a sum of two dominant terms , one representing bias and the other representing variance . for simplicity of exposition , we will deal initially with binary classification problems , though we also indicate the appropriate extension to general multicategory problems .",
    "our second contribution , following on from the first , is to derive the vector of non - negative weights that is asymptotically optimal in the sense of minimising the misclassification rate ( cf .",
    "theorem  [ thm : optweights ] ) .",
    "in fact this asymptotically optimal weight vector has a relatively simple form : let @xmath7 denote the sample size and let @xmath8 denote the weight assigned to the @xmath9th nearest neighbour ( normalised so that @xmath10",
    ". then the optimal choice is to set @xmath11 ( an explicit expression for @xmath12 is given in  ( [ eq : kstar ] ) below ) and then let @xmath13 & \\mbox{for $ i=1,\\ldots , k^*$ } \\\\ 0 & \\mbox{for $ i = k^*+1,\\ldots , n$. } \\end{array } \\right.\\ ] ] thus , in the asymptotically optimal weighting scheme , only a proportion @xmath14 of the weights are positive .",
    "the maximal weight is almost @xmath15 times the average positive weight , and the discrete distribution on @xmath16 defined by the asymptotically optimal weights decreases in a concave fashion when @xmath4 , in a linear fashion when @xmath17 and in a convex fashion when @xmath18 ; see figure  [ fig : decweights ] .",
    "when @xmath1 is large , about @xmath19 of the weights are above the average positive weight .",
    "[ t][t]0 [ t][t]0.002 [ t][t]0.004 [ t][t]0.006 [ t][t]0.008 [ t][t]0.01 [ t][t]0 [ t][t]20 [ t][t]40 [ t][t]60 [ t][t]80 [ t][t]100 [ t][t]neighbour order [ t][t]optimal weight    another consequence of theorem  [ thm : optweights ] is that @xmath20 is bigger by a factor of @xmath21 than the asymptotically optimal choice of @xmath0 for traditional , unweighted @xmath0-nearest neighbour classification .",
    "it is notable that this factor , which is around 1.27 when @xmath4 and increases towards 2 for large @xmath1 , does not depend on the underlying populations .",
    "this means that there is a natural correspondence between any unweighted @xmath0-nearest neighbour classifier and one of optimally weighted form , obtained by multiplying @xmath0 by this dimension - dependent factor to obtain the number @xmath22 of positive weights for the weighted classifier , and then using the weights given in  ( [ eq : optweights ] ) with @xmath22 replacing @xmath20 .    in corollary  3",
    "we describe the asymptotic improvement in the excess risk that is attainable using the procedure described in the previous paragraph .",
    "since the rate of convergence to zero of the excess risk is @xmath23 in both cases , the improvement is in the leading constant , and again it is notable that the asymptotic improvement does not depend on the underlying populations .",
    "the improvement is relatively modest , which goes some way to explaining the continued popularity of the ( unweighted ) @xmath0-nearest neighbour classifier .",
    "nevertheless , for @xmath24 , the improvement in regret is at least 5% , though it is negligible as @xmath3 ; the greatest improvement occurs when @xmath2 , and here it is just over 8% .",
    "see figure  [ fig : asympimp ] .",
    "[ t][t]0.90 [ t][t]0.92 [ t][t]0.94 [ t][t]0.96 [ t][t]0.98 [ t][t]1.00 [ t][t]1.02 [ t][t]0 [ t][t]10 [ t][t]20 [ t][t]30 [ t][t]40 [ t][t]50 [ t][t]@xmath1 [ t][t]regret ratio    another popular way of improving the performance of a classifier is by _ bagging _ @xcite .",
    "short for ` bootstrap aggregating ' , bagging involves combining the results of many empirically simulated predictions .",
    "empirical analyses , e.g. @xcite , have reported that bagging can result in improvements over unweighted @xmath0-nearest neighbour classification . moreover , as explained by @xcite , understanding the properties of the bagged nearest neighbour classifier is also of interest because they provide insight into _ random forests _ @xcite .",
    "random forest algorithms have been some of the most successful ensemble methods for regression and classification problems , but their theoretical properties remain relatively poorly understood . when bagging the nearest neighbour classifier , we can draw resamples from the data either with- or without - replacement .",
    "we treat the ` infinite simulation ' case , where both versions take the form of a weighted nearest neighbour classifier with weights decaying approximately exponentially on successively more distant observations from the point being classified @xcite .",
    "the crucial choice is that of the resample size , or equivalently the sampling fraction , i.e. the ratio of the resample size to the original sample size . in section  [ sec : bagged ] , we describe the asymptotically optimal resample fraction ( showing in particular that it is the same for both with- and without - replacement sampling ) and compare its regret with those of the weighted and unweighted @xmath0-nearest neighbour classifiers .",
    "finally , in section  [ sec : negweights ] , we consider the problem of choosing optimal weights without the restriction that they should be non - negative .",
    "the situation here is somewhat analogous to the use of higher order kernels for classifiers based on kernel density estimates of each of the population densities . in particular ,",
    "subject to additional smoothness assumptions on the population densities , we find that powers of @xmath7 arbitrarily close to the ` parametric rate ' of @xmath25 for the excess risk are attainable .",
    "all proofs are deferred to the appendix .",
    "classification has been the subject of several book - length treatments , including @xcite , @xcite and @xcite . in particular , classifiers based on nearest neighbours form a central theme of @xcite .",
    "the review paper by @xcite contains 243 references and provides a thorough survey of the classification literature up to 2005 .",
    "more recently , @xcite have discussed the relative merits of _ plug - in _ classifiers ( a family to which weighted nearest neighbour classifiers belong ) and classifiers based on _ empirical risk minimisation _ , such as support vector machines @xcite .",
    "weighted nearest neighbour classifiers were first studied by @xcite ; see also @xcite .",
    "@xcite proved that if @xmath26 as @xmath27 and @xmath28 for some @xmath29 with @xmath30 as @xmath27 , then risk of the weighted nearest neighbour classifier converges to the risk of the bayes classifier ; see also @xcite .",
    "as mentioned above , this work attempts to study the difference between these risks more closely .",
    "weighted nearest neighbour classifiers are also related to classifiers based on kernel estimates of each of the class densities ; see for example the review by @xcite , as well as @xcite .",
    "the @xmath23 rates of convergence obtained in this paper for non - negative weights are the same as those obtained by @xcite under similar twice - differentiable conditions with second - order kernel estimators of the class densities .",
    "@xcite proved that in a certain sense this is the minimax optimal rate , though his assumptions and context are slightly different from what is studied here .",
    "further related work includes the literature on _ highest density region _ or _ level set _ estimation @xcite .",
    "@xcite and @xcite proved an analogous result for the bagged nearest neighbour classifier to the @xcite result described in the previous paragraph .",
    "more precisely , if the resample size @xmath31 used for the bagging diverges to infinity , and @xmath32 as @xmath27 , then the risk of the bagged nearest neighbour classifier converges to the bayes risk .",
    "note that this result does not depend on whether the resamples are taken with or without replacement from the training data .",
    "@xcite have recently proved a striking rate of convergence result for the bagged nearest neighbour estimate ; this is described in greater detail in section  [ sec : bagged ] .",
    "let @xmath33 be independent and identically distributed pairs taking values in @xmath34 .",
    "we suppose that @xmath35 for some @xmath36 and that @xmath37 for @xmath38 , where @xmath39 is a probability measure on @xmath6 .",
    "we write @xmath40 for the marginal distribution of @xmath41 and let @xmath42 denote the corresponding regression function . a _ classifier _",
    "@xmath43 is a borel measurable function from @xmath6 to @xmath44 , with the interpretation that the point @xmath45 is classified as belonging to class @xmath46 .",
    "the _ misclassification rate _ , or _",
    "risk _ of @xmath43 over a borel measurable set @xmath47 is defined to be @xmath48.\\ ] ] the classifier which minimises the risk over @xmath49 is the bayes classifier , given by @xmath50 its risk is @xmath51 for each @xmath52 , let @xmath53 denote a vector of weights , normalised so that @xmath54 .",
    "fix @xmath55 and an arbitrary norm @xmath56 on @xmath6 , and let @xmath57 denote a permutation of the training sample @xmath58 such that @xmath59 .",
    "we define the _ weighted nearest neighbour classifier _ to be @xmath60 we also write @xmath61 where it is necessary to emphasise the weight vector , for example when comparing different weighted nearest neighbour classifiers . our initial goal is to study the asymptotic behaviour of @xmath62,\\ ] ] where the probability is taken over the joint distribution of @xmath63 and @xmath64",
    ".    it will be convenient to define a little notation : for a smooth function @xmath65 , we write @xmath66 for its derivative at @xmath5 , and @xmath67 for its @xmath68th partial derivative at @xmath5 .",
    "analogously , we write @xmath69 for the second derivative of @xmath70 at @xmath5 , and @xmath71 for the @xmath72th element of the corresponding hessian matrix at @xmath5 .",
    "we let @xmath73 denote the closed ball of radius @xmath74 centered at @xmath5 in the norm @xmath56 , and let @xmath75 denote the @xmath1-dimensional lebesgue measure of the unit ball @xmath76 .",
    "we will make use of the following assumptions for our theoretical results :    ( a.1 ) : :    the set @xmath47 is a compact    @xmath1-dimensional manifold with boundary    @xmath77 .",
    "( a.2 ) : :    the set    @xmath78 is    non - empty .",
    "there exists an open subset @xmath79 of    @xmath6 that contains @xmath80 and    such that the following properties hold : firstly ,    @xmath81 is bounded away from zero for    @xmath82 , where @xmath83 is an open    set containing @xmath49 ; secondly the restrictions of    @xmath84 and @xmath85 to @xmath79 are    absolutely continuous with respect to lebesgue measure , with twice    continuously differentiable radon  nikodym derivatives    @xmath86 and @xmath87 respectively .",
    "( a.3 ) : :    there exists @xmath88 such that    @xmath89 .",
    "moreover , for sufficiently small @xmath90 , the ratio    @xmath91 is bounded away from    zero , uniformly for @xmath55 .",
    "( a.4 ) : :    for all @xmath92 , we have    @xmath93 , and for all    @xmath94 , we have    @xmath95 , where    @xmath96 denotes the restriction of    @xmath97 to @xmath77 .",
    "the introduction of the compact set @xmath49 finesses the problem of performing classification in the tails of the feature vector distributions .",
    "see for example ( * ? ? ?",
    "* section  3 ) for further discussion of this point and related results , as well as @xcite .",
    "@xcite and @xcite impose similar compactness assumptions for their results .",
    "the set @xmath49 may be arbitrarily large , though the larger it is , the stronger are the requirements in ( * a.2 * ) .",
    "although as stated , the assumptions on @xmath49 are quite general , little is lost by thinking of @xmath49 as a large closed euclidean ball .",
    "its role in the asymptotic expansion of theorem  [ thm : optweights ] below is that it is involved in the definition of the set @xmath80 , which represents the decision boundary of the bayes classifier .",
    "we will see that the behaviour of @xmath86 and @xmath87 on the set @xmath80 is crucial for determining the asymptotic behaviour of weighted nearest neighbour classifiers .",
    "the second part of ( * a.3 * ) asks that the ratio of the @xmath98-measure of small balls to the corresponding @xmath1-dimensional lebesgue measure is bounded away from zero .",
    "this requirement is satisfied , for instance , if @xmath84 and @xmath85 are absolutely continuous with respect to lebesgue measure , with radon  nikodym derivatives that are bounded away from zero on the open set @xmath83 .    the assumption in ( * a.4 * ) that @xmath93 for @xmath92 asks that @xmath86 and @xmath87 , weighted by the respective prior probabilities of each class , should cut at a non - zero angle along @xmath80 . in the language of differential topology",
    ", this means that 1/2 is a _ regular value _ of the function @xmath97 , and the second part of ( * a.4 * ) asks for 1/2 to be a regular value of the restriction of @xmath97 to @xmath77 .",
    "together , these two requirements ensure that @xmath80 is a @xmath99-dimensional submanifold with boundary of @xmath6 , and the boundary of @xmath80 is @xmath100 @xcite .",
    "the requirement in  ( * a.4 * ) that @xmath93 for @xmath92 is related to the well - known _ margin condition _ of , e.g. @xcite and @xcite ; when it holds ( and in the presence of the other conditions ) , there exist @xmath101 such that @xmath102 for sufficiently small @xmath103 ; see ( * ? ? ?",
    "* proposition  1 ) . a proof of this fact , which uses weyl s tube formula @xcite , is given after the proof of theorem  [ thm : main ] in the appendix . in this sense , we work in the setting of a margin condition with the power parameter equal to 1 .",
    "we now introduce some notation needed for theorem  [ thm : main ] below . for @xmath104 ,",
    "let @xmath105 denote the set of all sequences of non - negative deterministic weight vectors @xmath53 satisfying    * @xmath106 ; * @xmath107 , where @xmath108 ; note that this latter expression appears in  ( [ eq : optweights ] ) ; * @xmath109 , where @xmath110 ; * @xmath111 ; * @xmath112 .",
    "observe that @xmath113 for @xmath114 .",
    "the first and last conditions ensure that the weights are not too concentrated on a small number of points ; the second amounts to a mild moment condition on the probability distribution on @xmath16 defined by the weights .",
    "the next two conditions ensure that not too much weight ( or squared weight in the case of the latter condition ) is assigned to observations that are too far from the point being classified .",
    "although there are many requirements on the weight vectors , they are rather mild conditions when @xmath115 is small , as can be seen by considering the limiting case @xmath116 .",
    "for instance , for the unweighted @xmath0-nearest neighbour classifier with weights @xmath53 given by @xmath117 , we have that @xmath118 for small @xmath104 provided that @xmath119 .",
    "thus for the vector of @xmath0-nearest neighbour weights to belong to @xmath105 for all large @xmath7 , it is necessary that the usual conditions @xmath120 and @xmath30 for consistency are satisfied , and these conditions are almost sufficient when @xmath104 is small .",
    "the situation is similar for the bagged nearest neighbour classifier  see section  [ sec : bagged ] below .",
    "the fact that the weights are assumed to be deterministic means that they depend only on the ordering of the distances , not the raw distances themselves ( as would be the case for a classifier based on kernel density estimates of the population densities ) .",
    "such classifiers are not necessarily straightforward to implement , however : @xcite showed that even in the simple situation where @xmath4 and @xmath121 and @xmath122 cross at a single point @xmath123 , the optimal order of the bandwidth for the kernel depends on the sign of @xmath124 .    continuing with our notational definitions , let @xmath125 , and let @xmath126 where @xmath127 denotes the laplacian of @xmath97 at @xmath5 .",
    "define @xmath128 where @xmath129 denotes the natural @xmath99-dimensional volume measure that @xmath80 inherits as a subset of @xmath6 .",
    "note that @xmath130 , and @xmath131 , with equality if and only if @xmath132 is identically zero on @xmath80 . although the definitions of @xmath133 and @xmath134 are complicated , we will see after the statement of theorem  [ thm : main ] below that they are comprised of terms that have natural interpretations .",
    "[ thm : main ] assume ( * a.1 * ) , ( * a.2 * ) , ( * a.3 * ) and  ( * a.4 * ) . then for each @xmath104 , @xmath135 as @xmath27 , uniformly for @xmath118 , where @xmath136    theorem  [ thm : main ] tells us that , asymptotically , the dominant contribution to the excess risk , or _ regret _",
    ", of the weighted nearest neighbour classifier over @xmath49 , can be decomposed as a sum of two terms .",
    "the two terms , constant multiples of @xmath137 and @xmath138 respectively , represent variance and squared bias contributions to the regret .",
    "it is interesting to observe that , although the 0 - 1 classification loss function is quite different from the squared error loss often used in regression problems , we nevertheless obtain such an asymptotic decomposition .",
    "the constant multiples of the dominant variance and squared bias terms depend only on the behaviour of @xmath86 and @xmath87 ( and their first and second derivatives ) on @xmath80 , as seen from  ( [ eq : c1c2 ] ) .",
    "moreover , we can see from the expression for @xmath133 in  ( [ eq : c1c2 ] ) that the contribution to the dominant variance term in the regret will tend to be large in the following three situations : firstly , when @xmath139 is large on @xmath80 ; secondly when the @xmath129 measure of @xmath80 is large ; and thirdly when @xmath140 is small on @xmath80 . in the first two of these situations ,",
    "the probability is relatively high that a point to be classified will be close to the bayes decision boundary @xmath80 , where classification is difficult . in the latter case ,",
    "the regression function @xmath97 moves away from @xmath141 only slowly as we move away from @xmath80 , meaning that there is a relatively large region of points near @xmath80 where classification is difficult . from the expression for @xmath134 in  ( [ eq : c1c2 ] )",
    ", we see that the dominant squared bias term is also large in these situations , and also when @xmath142 is large on @xmath80 . from the proof of theorem  [ thm : main ]",
    ", it is apparent that @xmath143 is the dominant bias term for @xmath144 as an estimator of @xmath145 .",
    "indeed , by a taylor expansion , @xmath146 the two summands in the definition of @xmath147 represent asymptotic approximations to the respective summands in this approximation .",
    "consider now problem of optimising the choice of weight vectors .",
    "let @xmath148 and then define the weights @xmath149 as in  ( [ eq : optweights ] ) .",
    "the first part of theorem  [ thm : optweights ] below can be regarded as saying that the weights @xmath150 are asymptotically optimal .",
    "[ thm : optweights ] assume  * ( a.1)*(*a.4 * ) and assume also that @xmath151 .",
    "for any @xmath104 and any sequence @xmath152 , we have @xmath153 moreover , the ratio in  ( [ eq : regratio ] ) above converges to 1 if and only if both @xmath154 and @xmath155 .",
    "equivalently , this occurs if and only if both @xmath156 finally , @xmath157    now write @xmath158 for the traditional , unweighted @xmath0-nearest neighbour classifier ( or equivalently , the weighted nearest neighbour classifier with @xmath159 for @xmath160 and @xmath161 otherwise ) . another consequence of theorem  [ thm : main ] is that , provided  * ( a.1)*(*a.4 * ) hold and @xmath151 , the quantity @xmath20 defined in  ( [ eq : kstar ] ) is larger by a factor of @xmath21 ( up to an unimportant rounding error ) than the asymptotically optimal choice of @xmath162 for @xmath158 ; see also @xcite .",
    "we can therefore compare the performance of @xmath163 with that of @xmath164 .",
    "[ cor : regretratio ] assume  * ( a.1)*(*a.4 * ) and assume also that @xmath151 .",
    "then @xmath165 as @xmath27 .",
    "since the limit in  ( [ eq : regretratio ] ) does not depend on the underlying populations , we can plot it as a function of @xmath1 ; cf .",
    "figure  [ fig : asympimp ] .",
    "in fact , corollary  [ cor : regretratio ] suggests a natural correspondence between any unweighted @xmath0-nearest neighbour classifier @xmath158 and the weighted nearest neighbour classifier which we denote by @xmath166 whose weights are of the optimal form  ( [ eq : optweights ] ) , but with @xmath20 replaced with @xmath167 under the conditions of corollary  [ cor : regretratio ] , we can compare @xmath158 and @xmath166 , concluding that for each @xmath168 , @xmath169 as @xmath27 , uniformly for @xmath170 . the fact that the convergence in  ( [ eq : regretratio2 ] ) is uniform for @xmath0 in this range means that the ratio on the left - hand side of  ( [ eq : regretratio2 ] ) has the same limit if we replace @xmath0 by an estimator @xmath171 constructed from the training data @xmath172 , provided that @xmath171 lies in this range with probability tending to 1 .",
    "finally in this section , we note that the theory presented above can be extended in a natural way to multicategory classification problems , where the class labels take values in the set @xmath173 .",
    "writing @xmath174 , let @xmath175 for distinct indices @xmath176 .",
    "in addition to * ( a.1 ) * and the obvious analogues of the conditions * ( a.2 ) * , * ( a.3 ) * and  ( * a.4 * ) , we require :    ( a.5 ) : :    for each @xmath177 , the submanifolds    @xmath178 and    @xmath179 of @xmath6 are    transversal .",
    "condition  ( * a.5 * ) ensures that @xmath180 is either empty or a @xmath181-dimensional submanifold of @xmath6 @xcite . under these conditions ,",
    "the conclusion of theorem  [ thm : main ] holds , provided that the constants @xmath133 and @xmath134 are replaced with @xmath182 and @xmath183 respectively , where each term @xmath184 and @xmath185 is an integral over @xmath178 .",
    "apart from the obvious notational changes involved in converting @xmath133 and @xmath134 to @xmath184 and @xmath185 , the only other change required is to replace the constant factor @xmath186 in the definition of @xmath133 with @xmath187 where @xmath188 denotes the common value that @xmath189 and @xmath190 take at @xmath191 .",
    "this change accounts for the fact that @xmath188 is not necessarily equal to @xmath141 on @xmath178 .",
    "it follows ( provided also that @xmath192 ) that the asymptotically optimal weights are still of the form  ( [ eq : optweights ] ) , but with the ratio @xmath193 in the expression for @xmath20 in  ( [ eq : kstar ] ) replaced with @xmath194 .",
    "moreover , the conclusion of corollary  [ cor : regretratio ] and the subsequent discussion also remain true .",
    "traditionally , the bagged nearest neighbour classifier is obtained by applying the 1-nearest neighbour classifier to many resamples from the training data .",
    "the final classification is made by a majority vote on the classifications obtained from the resamples . in the most common version of bagging where the resamples are drawn with replacement , and the resample size",
    "is the same as the original sample size , bagging the nearest neighbour classifier gives no improvement over the 1-nearest neighbour classifier @xcite .",
    "this is because the nearest neighbour occurs in more than half ( in fact , roughly a proportion @xmath195 ) of the resamples .    nevertheless ,",
    "if a smaller resample size is used , then substantial improvements over the nearest neighbour classifier are possible , as has been verified empirically by @xcite .",
    "in fact , if the resample size is @xmath196 , then the ` infinite simulation ' versions of the bagged nearest neighbour classifier in the with- and without - replacement resampling cases are weighted nearest neighbour classifiers with respective weights @xmath197 and @xmath198 of course , the observations above render the resampling redundant , and we regard the weighted nearest neighbour classifiers with the weights above as defining the two versions of the bagged nearest neighbour classifier .",
    "it is convenient to let @xmath199 denote the resampling fraction .",
    "intuitively , for large @xmath7 , both versions of the bagged nearest neighbour classifier behave like the weighted nearest neighbour classifier with weights @xmath200",
    "which place a @xmath201 distribution ( conditioned on being in the set @xmath16 ) on the weights : @xmath202 the reason for this is that , in order for the @xmath9th nearest neighbour of the training data to be the nearest neighbour of the resample , the nearest @xmath203 neighbours must not appear in the resample , while the @xmath9th nearest neighbour must appear , and these events are almost independent when @xmath7 is large ; see @xcite .",
    "naturally , the parameter @xmath204 plays a crucial role in the performance of the bagged nearest neighbour classifier , and for small @xmath104 , the three vectors of weights given in  ( [ eq : bagged1 ] ) ,  ( [ eq : bagged2 ] ) and  ( [ eq : bagged3 ] ) belong to @xmath105 for all large @xmath7 if @xmath205",
    ". in the following corollary of theorem  [ thm : main ] , we write @xmath206 to denote either of the bagged nearest neighbour classifiers with weights  ( [ eq : bagged1 ] ) ,  ( [ eq : bagged2 ] ) or their approximation with weights  ( [ eq : bagged3 ] ) .    [ cor : baggednnexp ] assume  ( * a.1*)(*a.4 * ) . for every @xmath168 , @xmath207",
    "uniformly for @xmath208 , where @xmath209    this result is somewhat related to corollary  10 of @xcite . in that paper , the authors study the bagged nearest neighbour estimate @xmath210 of the regression function @xmath97 .",
    "they prove in particular that under regularity conditions ( including a lipschitz assumption on @xmath97 ) and for a suitable choice of resample size , @xmath211 = o(n^{-2/(d+2)})\\ ] ] for @xmath18 .",
    "it is known , e.g. @xcite that this is the minimax optimal rate for their problem .",
    "corollary  [ cor : baggednnexp ] may also be applied to deduce that the asymptotically optimal choice of @xmath204 in all three cases is @xmath212 thus , in an analogous fashion to section  [ sec : main ] , we can consider the performance of @xmath213 relative to that of @xmath163 .",
    "[ cor : bagged ] assume  * ( a.1)*(*a.4 * ) and assume also that @xmath151 .",
    "then @xmath214 as @xmath27",
    ".    the limiting ratio in  ( [ eq : regretratio3 ] ) is plotted as a function of @xmath1 in figure  [ fig : regretratio2 ] .",
    "the ratio is about 1.18 when @xmath4 , showing that the bagged nearest neighbour classifier has asymptotically worse performance than the @xmath0-nearest neighbour classifier in this case .",
    "the ratio is equal to 1 when @xmath17 , and is less than 1 for @xmath18 .",
    "the facts that the asymptotically optimal weights decay as illustrated in figure  [ fig : decweights ] and that the bagged nearest neighbour weights decay approximately geometrically explain why the bagged nearest neighbour classifier has almost optimal performance among weighted nearest neighbour classifiers when @xmath1 is large .",
    "similar to the discussion following corollary  [ cor : regretratio ] , based on the expressions for @xmath162 and @xmath215 , there is a natural correspondence between the unweighted @xmath0-nearest neighbour classifier @xmath216 with data driven @xmath171 , and the bagged nearest neighbour classifier @xmath217 , where @xmath218 the same limit  ( [ eq : regretratio3 ] ) holds for the regret ratio of these classifiers , again provided there exists @xmath168 such that @xmath219 .",
    "[ t][t]0.90 [ t][t]0.92 [ t][t]1.00 [ t][t]1.05 [ t][t]1.10 [ t][t]1.15 [ t][t]1.20 [ t][t]0 [ t][t]10 [ t][t]20 [ t][t]30 [ t][t]40 [ t][t]50 [ t][t]@xmath1 [ t][t]regret ratio",
    "if we allow negative weights , it is possible to choose weights satisfying @xmath220 .",
    "this means that we can eradicate the dominant squared bias term in the asymptotic expansion of theorem  [ thm : main ] .",
    "it follows that , subject to additional smoothness conditions , we can achieve faster rates of convergence with weighted nearest neighbour classifiers , as we now describe .",
    "the appropriate variant of condition  ( * a.2 * ) , which we denote by  * ( a.2)(r ) * , is as follows :    ( a.2)(r ) : :    the set    @xmath78 is    non - empty .",
    "there exists an open subset @xmath79 of    @xmath6 that contains @xmath80 and    such that the following properties hold : firstly ,    @xmath81 is bounded away from zero for    @xmath82 , where @xmath83 is an open    set containing @xmath49 ; secondly the restrictions of    @xmath84 and @xmath85 to @xmath79 are    absolutely continuous with respect to lebesgue measure , with    @xmath221-times continuously differentiable radon  nikodym    derivatives @xmath86 and @xmath87 respectively .",
    "thus condition * ( a.2)(1 ) * is identical to * ( a.2)*. note that we are still in the setting of a margin condition with power parameter equal to 1 . for",
    "non - negative integers @xmath222 and @xmath223 and @xmath224 , let @xmath225 let @xmath226 be a multi - index ( i.e. a @xmath1-tuple of non - negative integers ) .",
    "we write @xmath227 , and for @xmath228 , we write @xmath229 . now , for @xmath230 , let @xmath231 where we evaluate the integral by transforming to spherical coordinates .",
    "it is convenient here to use multi - index notation for derivatives , so we write @xmath232 . as non - standard multi - index notation ,",
    "it is also convenient for @xmath230 to write @xmath233 .",
    "now let @xmath234 so that @xmath235 .",
    "let @xmath236 for @xmath237 , define @xmath238 .",
    "we consider restrictions on the set of weight vectors analogous to those imposed on @xmath239th order kernels in kernel density estimation .",
    "specifically , we let @xmath240 denote the set of deterministic weight vectors @xmath53 satisfying    * @xmath54 , @xmath241 for @xmath242 ; * @xmath106 ; * @xmath243 ; * there exists @xmath244 such that @xmath245 and such that @xmath246 ; * @xmath111 ; * @xmath247 .    finally , we are in a position to state the analogue of theorem  [ thm : main ] for weight vectors in @xmath240 .",
    "[ thm : main2 ] assume ( * a.1 * ) , ( * a.2*)(*r * ) , ( * a.3 * ) and  ( * a.4 * ) .",
    "then for each @xmath104 , @xmath248 as @xmath27 , uniformly for @xmath249 , where @xmath250    a consequence of theorem  [ thm : main2 ] is that we can construct weighted nearest neighbour classifiers which , under conditions ( * a.1 * ) , ( * a.2*)(*r * ) , ( * a.3 * ) and  ( * a.4 * ) , and provided @xmath251 , achieve the rate of convergence @xmath252 for the regret . to illustrate this ,",
    "set @xmath253 , and in order to satisfy the restrictions on the allowable weights , consider weight vectors with @xmath161 for @xmath254 .",
    "then , by mimicking the proof of theorem  [ thm : optweights ] and seeking to minimise  ( [ eq : gamma ] ) subject to the constraints @xmath255 and @xmath256 for @xmath242 , we obtain minimising weights of the form @xmath257 the equations @xmath258 and @xmath259 for @xmath260 for weight vectors of the form  ( [ eq : negweightvec ] ) yield @xmath239 linear equations in the @xmath261 unknowns @xmath262 .",
    "although these equations can be solved directly in terms of @xmath263 say , simpler expressions are obtained by solving asymptotic approximations to these equations .",
    "in particular , since it is an elementary fact that for non - negative integers @xmath264 and @xmath265 , @xmath266 as @xmath120 , we can just deal with the dominant terms . as examples ,",
    "when @xmath267 , we find @xmath268 and when @xmath269 , we should take @xmath270 under the conditions of theorem  [ thm : main2 ] , and provided @xmath251 , these weighted nearest neighbour classifiers achieve the @xmath252 convergence rate .",
    "the choice of @xmath263 involves a trade - off between the desire to keep the remaining squared bias term @xmath271 small , and the need for it to be large enough to remain the dominant bias term .",
    "this reflects the fact that the asymptotic results of this section should be applied with some caution . besides the discomfort many practitioners might feel in using negative weights",
    ", one would anticipate that rather large sample sizes would be needed for the leading terms in the asymptotic expansion  ( [ eq : higherorderexp ] ) to dominate the error terms .",
    "this is also the reason why we do not pursue here methods such as lepski s method @xcite that adapt to an unknown smoothness level around @xmath80 .",
    "of theorem  [ thm : main ] the proof is rather lengthy , so we briefly outline the main ideas here .",
    "write @xmath272 and observe that @xmath273 \\ , dp_1(x ) \\nonumber \\\\ & \\hspace{1cm}+ \\int_{\\mathcal{r } } ( 1-\\pi)\\bigl[\\mathbb{p}\\{\\hat{c}_n^{\\mathrm{wnn}}(x ) = 1\\ } - \\mathbbm{1}_{\\{c^{\\mathrm{bayes}}(x)=1\\}}\\bigr ] \\ ,",
    "dp_2(x ) \\nonumber \\\\ & = \\int_{\\mathcal{r } } \\biggl\\{\\mathbb{p}\\biggl(\\sum_{i=1}^n w_{ni } \\mathbbm{1}_{\\{y_{(i)}=1\\ } } <",
    "\\frac{1}{2}\\biggr ) - \\mathbbm{1}_{\\{\\eta(x ) < 1/2\\}}\\biggr\\ } \\ , dp^\\circ(x).\\end{aligned}\\ ] ] for @xmath103 , let @xmath274 where @xmath275",
    ". moreover , let @xmath276 the dominant contribution to the integral in  ( [ eq : mainarg ] ) comes from @xmath277 , where @xmath278 . since the unit vector @xmath279 is orthogonal to the tangent space of @xmath80 at @xmath123 , we can decompose the integral over @xmath277 as an integral along @xmath80 and an integral in the perpendicular direction .",
    "we then apply a normal approximation to the integrand to deduce the result .",
    "this normal approximation requires asymptotic expansions to the mean and variance of the sum of independent random variables in  ( [ eq : mainarg ] ) , and these are developed in * step 1 * and * step 2 * below respectively . in order to retain the flow of the main argument",
    ", we concentrate on the dominant terms in the first five steps of the argument , simply labelling the many remainder terms as @xmath280 . the sizes of these remainder terms are controlled in * step 6 * , where we also present an additional side calculation .      by a taylor expansion , @xmath285 where we show in * step 6 * that @xmath286 uniformly for @xmath118 .",
    "writing @xmath287 , we also show in * step 6 * that for @xmath288 and @xmath289 , the restriction of the distribution of @xmath290 to a sufficiently small ball about the origin is absolutely continuous with respect of lebesgue measure , with radon  nikodym derivative given at @xmath291 by @xmath292 let @xmath293 . by examining the argument leading to  ( [ eq : aic ] )",
    ", we see that we can replace @xmath74 there with @xmath294 , to conclude that for all @xmath295 , @xmath296 it follows that @xmath297 uniformly for @xmath288 and @xmath298 .",
    "similarly @xmath299 uniformly for @xmath288 and @xmath298 .",
    "let @xmath300 , and let @xmath301 with @xmath302 ( where we introduce the comma here for clarity ) . by a taylor expansion , we have @xmath303\\mathbb{p}\\{\\mathrm{bin}(n-1,p_{\\|u\\| } )",
    "= i-1\\ } \\ , du \\nonumber \\\\ & = \\{1+o(1)\\}\\sum_{i = k_1}^{k_2 } n \\delta w_{ni}\\sum_{j=1}^d",
    "\\int_{\\|u\\| \\leq \\delta_n } \\bigl\\{\\eta_j(x)u_j^2\\bar{f}_j(x ) + \\tfrac{1}{2}\\eta_{jj}(x)u_j^2 \\bar{f}(x)\\bigr\\}\\mathbb{p}\\{\\mathrm{bin}(n-1,p_{\\|u\\| } ) <   i\\ } \\ , du,\\end{aligned}\\ ] ] uniformly for @xmath288 and @xmath118 .",
    "now , @xmath304 is decreasing in @xmath305 and is close to 1 when @xmath305 is small and close to zero when @xmath305 is large .",
    "to analyse this more precisely , note that @xmath306 as @xmath307 , uniformly for @xmath288 , so it is convenient to let @xmath308 and set @xmath309 .",
    "then there exists @xmath310 such that for @xmath311 , we have for all @xmath288 , all @xmath312 $ ] and all @xmath313",
    "that @xmath314 thus by bernstein s inequality @xcite , for each @xmath315 and for @xmath311 , @xmath316 } \\sup_{k_1 \\leq i \\leq k_2 } \\bigl[1 - \\mathbb{p}\\{\\mathrm{bin}(n-1,p_{\\|v\\|/b_n } ) <   i\\}\\bigr ] \\leq \\exp\\biggl(-\\frac{k_1}{3\\log^2 n}\\biggr ) = o(n^{-m}).\\ ] ] similarly , for @xmath311 , @xmath317 } \\sup_{k_1 \\leq i \\leq k_2 } \\mathbb{p}\\{\\mathrm{bin}(n-1,p_{\\|v\\|/b_n } ) <   i\\ } \\leq \\exp\\biggl(-\\frac{k_1}{3\\log^2 n}\\biggr ) = o(n^{-m}).\\ ] ] we deduce from ( [ eq : firstderiv ] ) , ( [ eq : secondderiv ] ) , ( [ eq : delta ] ) , ( [ eq : lower ] ) and ( [ eq : upper ] ) that @xmath318 uniformly for @xmath288 and @xmath118 . combining  ( [ eq : taylor ] ) ,  ( [ eq : r1 ] ) and  ( [ eq : biascompletion ] ) , this completes * step  1*.    * step 2 * : let @xmath319 and let @xmath320 .",
    "we claim that @xmath321 uniformly for @xmath118 . to see this , note that @xmath322 + \\sum_{i=1}^n w_{ni}^2 \\mathrm{var } \\",
    "\\eta(x_{(i ) } ) \\\\ & = \\sum_{i=1}^n w_{ni}^2 \\bigl[\\mathbb{e}\\eta(x_{(i ) } ) - \\{\\mathbb{e}\\eta(x_{(i)})\\}^2\\bigr ] .",
    "\\end{aligned}\\ ] ] but by a simplified version of the argument in * step 1 * , we have @xmath323 it follows that @xmath324 uniformly for @xmath118 .",
    "similarly , @xmath325 uniformly for @xmath118 .",
    "this completes * step 2*.    * step 3 * for @xmath326 and @xmath327 , we write @xmath328 for brevity .",
    "moreover , we write @xmath329 for the radon  nikodym derivative with respect to lebesgue measure of the restriction of @xmath330 to @xmath331 for large @xmath7 .",
    "we show that @xmath332 \\ , d(p_1 - p_2)(x ) \\nonumber \\\\ & = \\int_{\\mathcal{s } } \\int_{-\\epsilon_n}^{\\epsilon_n }",
    "\\psi(x_0^t)\\bigl[\\mathbb{p}\\{s_n(x_0^t ) < 1/2\\ } - \\mathbbm{1}_{\\{t < 0\\}}\\bigr ] \\ , dt \\ , d\\mathrm{vol}^{d-1}(x_0)\\{1 + o(1)\\},\\end{aligned}\\ ] ] uniformly for @xmath118 . recalling the definition of @xmath333 in  ( [ eq : sepseps ] ) , note that for large @xmath7 , the map @xmath334 is a diffeomorphism from @xmath335 onto @xmath331 @xcite .",
    "observe that @xmath336 moreover , for large @xmath7 and @xmath337 , we have @xmath338 .",
    "the pullback of the @xmath1-form @xmath339 is given at @xmath340 by @xmath341 where the error term is uniform in @xmath340 for @xmath326 and @xmath337 .",
    "it follows from the theory of integration on manifolds , as described in  @xcite and ( * ? ? ?",
    "* theorems  3.15 and 4.7 ) ( see also @xcite ) , that @xmath342 \\ , dp^\\circ(x ) \\nonumber \\\\ & = \\int_{\\mathcal{s}^{\\epsilon_n \\epsilon_n } } \\int_{-\\epsilon_n}^{\\epsilon_n }",
    "\\psi(x_0^t)\\bigl[\\mathbb{p}\\{s_n(x_0^t ) < 1/2\\ } - \\mathbbm{1}_{\\{t < 0\\}}\\bigr ] \\ , dt \\ , d\\mathrm{vol}^{d-1}(x_0)\\{1 + o(1)\\},\\end{aligned}\\ ] ] uniformly for @xmath118 .",
    "but @xmath343 , and this latter set has volume @xmath344 by weyl s tube formula ( * ? ? ?",
    "* theorem  4.8 ) .",
    "thus the integral over @xmath331 in  ( [ eq : fubini ] ) may be replaced with an integral over @xmath277 and , similarly , the integral over @xmath333 may be replaced with an integral over @xmath80 , without changing the order of the error term in  ( [ eq : fubini ] ) .",
    "thus  ( [ eq : rfubini ] ) holds , and this completes * step 3*.    * step 4 * : we now return to the main argument to bound the contribution to the risk  ( [ eq : mainarg ] ) from @xmath345 . in particular , we show that @xmath346 \\ ,",
    "dp^\\circ(x ) = o(n^{-m}),\\ ] ] for all @xmath295 . to see this , recall that @xmath81 is assumed to be bounded away from zero on the set @xmath347 ( for fixed @xmath103 ) , and @xmath348 is bounded away from zero for @xmath326 .",
    "hence , by  ( [ eq : containment ] ) in * step 3 * , there exists @xmath349 such that , for sufficiently small @xmath103 , @xmath350 we also claim that @xmath282 is similarly bounded away from @xmath141 uniformly for @xmath351 .",
    "in fact , we have by hoeffding s inequality that @xmath352 it follows that @xmath353 for sufficiently large @xmath7 .",
    "similarly , @xmath354 for large @xmath7 .",
    "now we may apply hoeffding s inequality again , this time to @xmath355 , to deduce that @xmath356 for each @xmath295 , using  ( [ eq : upper ] ) and  ( [ eq : lower ] ) and the fact that @xmath357 for @xmath118 .",
    "this completes * step 4*.    * step 5 * we now show that @xmath358 \\ , dt \\ , d\\mathrm{vol}^{d-1}(x_0 ) = b_1 s_n^2 + b_2t_n^2 + o(s_n^2 + t_n^2),\\ ] ] uniformly for @xmath118 , where @xmath133 and @xmath134 were defined in  ( [ eq : c1c2 ] ) . when combined with  ( [ eq : mainarg ] ) and the results of * step 3 * and * step 4 * ( in particular ,  ( [ eq : rfubini ] ) and  ( [ eq : outer ] ) ) , this will complete the proof of theorem  [ thm : main ] .    first observe that @xmath359 \\ , dt \\ , d\\mathrm{vol}^{d-1}(x_0 ) \\nonumber \\\\ & = \\int_{\\mathcal{s } } \\int_{-\\epsilon_n}^{\\epsilon_n } t\\|\\dot{\\psi}(x_0)\\|\\bigl[\\mathbb{p}\\{s_n(x_0^t ) < 1/2\\ } - \\mathbbm{1}_{\\{t < 0\\}}\\bigr ] \\ , dt \\ , d\\mathrm{vol}^{d-1}(x_0)\\{1+o(1)\\}.\\end{aligned}\\ ] ] now , @xmath355 is a sum of independent , bounded random variables , so by the non - uniform version of the berry  esseen theorem , there exists @xmath360 such that @xmath361 } \\biggl|\\mathbb{p}\\biggl(\\frac{s_n(x_0^t ) - \\mu_n(x_0^t)}{\\sigma_n(x_0^t ) } \\leq y\\biggr ) - \\phi(y)\\biggr| \\leq \\frac{c_1}{n^{1/2}(1+|y|^3)},\\ ] ] where @xmath362 denotes the standard normal distribution function .",
    "thus @xmath363 where we show in * step 6 * that latexmath:[\\[\\label{eq : r2 }      @xmath118 .",
    "finally , we can make the substitution @xmath367 to conclude that @xmath368 \\ , dt \\ , d\\mathrm{vol}^{d-1}(x_0 ) \\nonumber \\\\ & = \\frac{s_n^2}{4 } \\int_{\\mathcal{s } } \\int_{-\\infty}^\\infty u\\|\\dot{\\psi}(x_0)\\|\\biggl\\{\\phi\\biggl(-u\\|\\dot{\\eta}(x_0)\\| - \\frac{2t_n}{s_n}a(x_0)\\biggr ) -",
    "\\mathbbm{1}_{\\{u < 0\\}}\\biggr\\ } \\ , du \\ , d\\mathrm{vol}^{d-1}(x_0 ) + r_4 \\nonumber \\\\ & = b_1 s_n^2 + b_2t_n^2 + r_4,\\end{aligned}\\ ] ] where @xmath133 and @xmath134 were defined in  ( [ eq : c1c2 ] ) .",
    "we have used the fact that @xmath369 for @xmath326 in the final step of this calculation .",
    "once we have shown in * step 6 * that latexmath:[\\[\\label{eq : r4 }      * step 6 * _ to show  ( [ eq : density ] ) , which gives the radon  nikodym derivative of the restriction of the distribution of @xmath290 to a small ball about the origin _ : recall that @xmath371 , and that @xmath372 denotes @xmath1-dimensional lebesgue measure . for a borel subset @xmath373 of @xmath6 , let @xmath374 .",
    "it follows from the hypothesis * ( a.2 ) * that for @xmath288 with @xmath7 sufficiently large , and for @xmath289 , the restriction of the distribution of @xmath290 to a small ball about the origin is absolutely continuous with respect to @xmath372 .",
    "thus for @xmath288 with @xmath7 sufficiently large , for @xmath289 , for @xmath375 with @xmath305 sufficiently small , and for @xmath376 , @xmath377 as @xmath378 , by the lebesgue differentiation theorem .",
    "for the other bound , write @xmath379 and observe that @xmath380 \\\\ & \\hspace{1cm}= \\frac{1}{a_d \\delta^d}\\biggl[n\\biggl\\{\\int_{b_\\delta(x+u ) } \\bar{f}(v ) \\",
    ", dv\\biggr\\}\\binom{n-1}{i-1}p_{\\|u\\|-\\delta}^{i-1}(1-p_{\\|u\\|+\\delta})^{n - i } + o(\\delta^d)\\biggr ] \\\\ & \\hspace{1cm}\\rightarrow n\\bar{f}(x+u)\\binom{n-1}{i-1}p_{\\|u\\|}^{i-1}(1-p_{\\|u\\|})^{n - i}\\end{aligned}\\ ] ] as @xmath378 .",
    "the result therefore follows by ( * ? ? ?",
    "* theorem  3.22 ) .",
    "_ to show  ( [ eq : r1 ] ) , which bounds @xmath381 _ : we have @xmath382 \\\\ & \\hspace{0.5cm}+ \\sum_{i=1}^{k_2 } w_{ni}\\biggl[\\mathbb{e}\\{\\eta(x_{(i)})\\ } - \\eta(x ) - \\mathbb{e}\\{(x_{(i)}-x)^t\\dot{\\eta}(x)\\ } - \\frac{1}{2 } \\mathbb{e}\\{(x_{(i)}-x)^t\\ddot{\\eta}(x)(x_{(i)}-x)\\}\\biggr ] \\\\ & \\equiv r_{11 } + r_{12},\\end{aligned}\\ ] ] say .",
    "now @xmath383 , so @xmath384 to handle @xmath385 , observe that by a taylor expansion , given @xmath103 , we can find @xmath90 such that for all sufficiently large @xmath7 , all @xmath288 and all @xmath386 , we have @xmath387 for @xmath298 , let @xmath388 .",
    "further , let @xmath389 and let @xmath390 , where @xmath391 denotes the largest eigenvalue of a matrix . then for large @xmath7 and @xmath288 , latexmath:[\\[\\label{eq : r13 }    we can apply a very similar argument to that employed in * step 1 * to deduce that uniformly for @xmath298 ,",
    "@xmath393 now @xmath394 for @xmath90 sufficiently small , there exists @xmath395 such that @xmath396 .",
    "so by hoeffding s inequality , for any @xmath397 , @xmath398 \\leq ( 1+t_0)e^{-2c_2 ^ 2n\\delta^{2d } } = o(n^{-m}),\\ ] ] for every @xmath295 .",
    "moreover , using the moment bound in ( * a.3 * ) , @xmath399 as @xmath400 , uniformly for @xmath288 .",
    "therefore we can apply bennett s inequality @xcite to show that there exist @xmath401 such that for sufficiently large @xmath7 and @xmath402 and all @xmath403 , @xmath404 \\leq ( 1+c_4t^{\\rho/2})^{-c_3 n/2}.\\ ] ] we deduce from ( [ eq : esq ] ) , ( [ eq : tleqt0 ] ) and  ( [ eq : bennett ] ) that @xmath405 for all @xmath295 .",
    "this result , combined with  ( [ eq : r13dom ] ) and markov s inequality applied to the two central terms in  ( [ eq : r13 ] ) , proves  ( [ eq : r1 ] ) as required .    _ to show  ( [ eq : r2 ] ) , which bounds @xmath406 _ : observe that by * step 1 * and * step 2 * , there exist constants @xmath407 such that @xmath408 uniformly for @xmath118 .",
    "hence , @xmath409 uniformly for @xmath118 , as required .",
    "_ to show  ( [ eq : r3 ] ) , which bounds @xmath410_. let @xmath411 using the results of * step 1 * and * step 2 * , given @xmath412 sufficiently small , for large @xmath7 we have that for all @xmath118 , all @xmath326 and all @xmath413 $ ] that @xmath414 it follows that for large @xmath7 , @xmath415 we deduce that for large @xmath7 , @xmath416 this allows us to conclude  ( [ eq : r3 ] ) .      of the fact that conditions ( * a.1*)(*a.4 * ) imply the margin condition  ( [ eq : margin ] ) for the upper bound , recall from  ( [ eq : infeta ] ) that by the mean value theorem , for sufficiently small @xmath103 , @xmath420 where we may take @xmath421 , which is positive . by shrinking @xmath79 if necessary",
    ", we may assume that @xmath422 , and it follows that for small @xmath103 , @xmath423 where @xmath424 , using weyl s tube formula @xcite .",
    "for the lower bound , we construct a tube similar to @xmath425 , but contained in @xmath49 . to do this , let @xmath426 and let @xmath427 further , let @xmath428 , which is finite .",
    "again by the mean value theorem , for sufficiently small @xmath103 , @xmath429 thus , letting @xmath430 , for sufficiently small @xmath103 , @xmath431 where @xmath432 , again using weyl s tube formula .",
    "@xmath419    of theorem  [ thm : optweights ] consider any vector of non - negative weights @xmath433 that minimises the function @xmath434 defined in the statement of theorem  [ thm : main ] .",
    "since @xmath435 is symmetric in @xmath436 , while @xmath437 is increasing in @xmath9 , we see that @xmath438 is decreasing in @xmath9 .",
    "we let @xmath439 .",
    "now form the lagrangian @xmath440 then for some @xmath441 , @xmath442 for @xmath443 . by summing  ( [ eq : lagderiv ] ) from @xmath443 , and then multiplying  ( [ eq : lagderiv ] ) by @xmath437 and again summing from @xmath443",
    ", we obtain two linear equations in @xmath444 and @xmath441 , which can be solved and substituted back into  ( [ eq : lagderiv ] ) to yield @xmath445 for @xmath443 . in particular , @xmath446 is the unique minimiser of @xmath434 .",
    "the weight vector @xmath150 is asymptotically equivalent to @xmath446 in the following sense : elementary calculations reveal that @xmath447 , and moreover that @xmath448 it follows immediately that @xmath449 and therefore that  ( [ eq : regratio ] ) holds .    arguing similarly to the above , we see that the conditions @xmath154 and @xmath155 , or equivalently  ( [ eq : equivcond ] ) , are sufficient for  ( [ eq : regratio ] ) to hold . to see the necessity of these conditions ,",
    "suppose for now that for some small @xmath104 , the weight vector @xmath118 satisfies @xmath450 then , by almost the same lagrangian calculation as that above , we have @xmath451 where @xmath452 is given by @xmath453 and where @xmath454 .",
    "it follows that for small @xmath104 , and for any @xmath118 satisfying @xmath455 , we have @xmath456 a very similar argument shows that if @xmath457 then the conclusion of  ( [ eq : liminf ] ) also holds .",
    "but if  ( [ eq : regratio ] ) holds and it is not the case that both @xmath154 and @xmath155 , then either  ( [ eq : lowlimit ] ) or  ( [ eq : lowlimit2 ] ) would have to hold on a subsequence .",
    "but then we see from  ( [ eq : liminf ] ) that  ( [ eq : regratio ] ) can not hold , and this contradiction means that the conditions  ( [ eq : equivcond ] ) are necessary for  ( [ eq : regratio ] ) .        of theorem  [ thm : main2 ]",
    "let @xmath459 .",
    "we only need to show that @xmath460 uniformly for @xmath249 , because the rest of the proof is virtually identical to that of theorem  [ thm : main ] .",
    "the appropriate analogue of  ( [ eq : delta ] ) is @xmath461\\mathbb{p}\\{\\mathrm{bin}(n-1,p_{\\|u\\| } ) = i-1\\ } \\ , du \\nonumber \\\\ & = \\{1+o(1)\\}\\sum_{i = k_1}^{k_2 } \\frac{n \\delta w_{ni}}{(2r-2)!}\\sum_{j=1}^d \\sum_{s:|s|=r-1 } \\int_{\\|u\\| \\leq \\delta_n } \\bigl\\{\\eta_j(x)u_j^2 u^s\\bar{f}_{s , j}(x ) + \\tfrac{1}{2}\\eta_{jj}(x)u_j^2 u^s \\bar{f}_s(x)\\bigr\\ } \\nonumber \\\\ & \\hspace{9cm}\\times \\mathbb{p}\\{\\mathrm{bin}(n-1,p_{\\|u\\| } ) <   i\\ } \\ , du,\\end{aligned}\\ ] ] uniformly for @xmath288 and @xmath249 . combining  ( [ eq : firstderiv ] ) , ( [ eq : secondderiv ] ) , ( [ eq : delta2 ] ) , ( [ eq : lower ] ) and ( [ eq : upper ] ) , we have @xmath462 uniformly for @xmath288 and @xmath249 . combining  ( [ eq : taylor ] ) , the analogue of  ( [ eq : r1 ] ) and  ( [ eq : biascompletion2 ] ) , this proves  ( [ eq : bias2 ] ) . @xmath419    * acknowledgements * : i am very grateful to peter hall for introducing me to this topic , and to the anonymous referees and associate editor for their constructive comments , which significantly helped to improve the paper .",
    "i thank the statistical and applied mathematical sciences institute ( samsi ) in north carolina for kind hospitality when i attended the ` program on analysis of object data ' from september to november 2010 , during which time part of this research was carried out .",
    "finally , i gratefully acknowledge the support of a leverhulme research fellowship .",
    "biau , g. and devroye , l. ( 2010 ) on the layered nearest neighbour estimate , the bagged nearest neighbour estimate and the random forest method in regression and classification .",
    "_ j. mult .",
    "_ , * 101 * , 24992518 .",
    "fix , e. and hodges , j.  l. ( 1951 ) discriminatory analysis ",
    "nonparametric discrimination : consistency properties .",
    "tech . rep .",
    "4 , project no .  21 - 29 - 004 , usaf school of aviation medicine , randolph field , texas ."
  ],
  "abstract_text": [
    "<S> we derive an asymptotic expansion for the excess risk ( regret ) of a weighted nearest - neighbour classifier . </S>",
    "<S> this allows us to find the asymptotically optimal vector of non - negative weights , which has a rather simple form . </S>",
    "<S> we show that the ratio of the regret of this classifier to that of an unweighted @xmath0-nearest neighbour classifier depends asymptotically only on the dimension @xmath1 of the feature vectors , and not on the underlying populations . </S>",
    "<S> the improvement is greatest when @xmath2 , but thereafter decreases as @xmath3 . </S>",
    "<S> the popular bagged nearest neighbour classifier can also be regarded as a weighted nearest neighbour classifier , and we show that its corresponding weights are somewhat suboptimal when @xmath1 is small ( in particular , worse than those of the unweighted @xmath0-nearest neighbour classifier when @xmath4 ) , but are close to optimal when @xmath1 is large . </S>",
    "<S> finally , we argue that improvements in the rate of convergence are possible under stronger smoothness assumptions , provided we allow negative weights .    </S>",
    "<S> * key words : * bagging , classification , nearest neighbours , weighted nearest neighbour classifiers . </S>"
  ]
}