{
  "article_text": [
    "in a statistical setting , consider the nonparametric regression model @xmath0 where @xmath1 and @xmath2 are both real - valued random variables with @xmath1 uniform in @xmath3 $ ] , @xmath4<\\infty$ ] and @xmath5=0 $ ] ( see for example @xcite ) .",
    "assume , in addition , that the regression function @xmath6 is right - continuous and of bounded variation . with this respect",
    ", the jordan decomposition asserts that such a function can be written as the sum of a non - decreasing function @xmath7 and a non - increasing function @xmath8 @xmath9 viewing this latter equation as an additive model involving the increasing part and the decreasing part of @xmath6 , we propose a new estimator which combines two well - established tools in nonparametric regression : the isotonic regression related to the estimation of monotone functions , and the backfitting algorithm devoted to the estimation of additive models .",
    "the estimation of a monotone regression function dates back to the 50 s and the early work by ayer et al .",
    "@xcite . given a sample of independent and identically distributed ( i.i.d . )",
    "random couples @xmath10 following the general model ( [ eq : general - model ] ) , denoting @xmath11 the ordered sample , and @xmath12 the corresponding observations , the pool - adjacent - violators algorithm ( pava ) determines a collection of non - decreasing level sets solution to the minimization problem @xmath13 since the cone @xmath14 is a closed convex set in @xmath15 , there exists a unique solution to this minimization problem .",
    "this solution , called the isotonic regression of @xmath16 and denoted @xmath17 , is the metric projection of @xmath18 on @xmath19 with respect to the euclidean norm , that is @xmath20 correspondingly , the antitonic regression of @xmath16 is the projection of @xmath16 on the set of vectors with non - increasing coordinates , that is @xmath21 where @xmath22 . from now on , @xmath19 and @xmath23 will be called monotone cones .",
    "a major attraction of isotonic regression procedures is their simplicity .",
    "since they are nonparametric and data driven ( i.e. , they do not require the tuning of any smoothing parameter ) , these estimators have raised considerable interest since more than fifty years . a comprehensive account on the subject",
    "can be found in @xcite , statistical properties have been studied in @xcite , @xcite , @xcite , @xcite , and extensions or improvements to more general order of the pava approach can be found in @xcite , @xcite and @xcite for example .    still in nonparametric statistics , but in a multidimensional context this time , the additive models were suggested by friedman and stuetzle @xcite and popularized by hastie and tibshirani @xcite as a way to get around the so - called `` curse of dimensionality '' . in brief , this means that , in multivariate smoothing , nonparametric estimators have to consider large neighborhoods of a particular point of the space to catch observations , and hence large biases can result .",
    "the additive model assumes that the regression function can be written as the sum of smooth terms in the covariates : @xmath24    since each variable is represented separately in ( [ eq : additive - models ] ) , the additive model provides a logical extension of the standard linear regression and once an additive model is fitted , one can easily interpret the role of each variable in predicting the response .",
    "buja et al .",
    "@xcite proposed the backfitting algorithm as a practical method for fitting additive models .",
    "it consists in iterated fitting of the _ partial residuals _ from earlier steps until convergence is reached .",
    "if the current estimates are @xmath25 , then @xmath26 is updated by smoothing @xmath27 against @xmath28 .",
    "while backfitting has attracted much attention and is frequently applied , it has been somewhat difficult to analyze theoretically . nonetheless , when using linear smoothers in each direction , the convergence of the algorithm can be related to the spectrum of the individual smoothing matrices ( see , e.g. , @xcite and @xcite ) , and when all the smoothers are orthogonal projections , the whole algorithm can be replaced by a global projection operator @xcite .",
    "there exist other multivariate methods based on repeated fitting of the residuals .",
    "some of them , like l2-boosting @xcite , boosted kernel regression @xcite , iterative bias reduction @xcite , do not assume any particular structure for the regression function .",
    "the common principle of these approaches is to start out with a biased smoother or a weak learner , and then to estimate and correct the bias in an iterative manner .",
    "hence , instead of smoothing the _ partial residuals _",
    ", one smoothes the _ global residuals _",
    "@xmath29 , and then correct the previous smoother . just as for the backfitting , the convergence of these algorithms as well as the statistical properties of these estimators have mainly been studied in the case of linear smoothers .    in our situation",
    ", however , it is noteworthy that projections on convex cones are not linear operators . but considering our iterative estimator as the application of von neumann s algorithm ( see for example @xcite ) , we will show that iterating the procedure tends to reproduce the data .",
    "moreover , we manage to go further in the analysis by proving that the individual terms of the sum converge as well to identified limits .",
    "this is in fact possible thanks to a result which is rather unexpected from the statistical point of view : iterating isotonic regression in a backfitting fashion or in a boosting fashion yield the same estimators at each step .",
    "interestingly , this result stems from a property of the projections onto monotone cones which , in our case , reads as follows ( recall that @xmath17 is defined by ( [ eq : iso - y ] ) ) : @xmath30 from a more general perspective , one can see this equation as a particular case of the property of isotonicity of the projection onto convex cones . here",
    "isotonicity is considered with respect to the order induced by the cone .",
    "the idea to relate the ordering induced by a convex cone and the metric projection onto the convex cone goes back to the paper by isac and nmeth @xcite , where a convex cone in the euclidean space which admits an isotone projection onto it ( called by the authors _ isotone projection cone _ ) was characterized .",
    "thereafter , this notion was considered in the complementarity theory to provide new existence results and iterative methods @xcite .    yet , the notion of the cone in the above cited papers is used in the sense of `` closed convex pointed cone '' .",
    "confronted with the question if the monotone cones @xmath23 and @xmath19 , which are not pointed , admit or not isotonic metric projections , we shall develop in section [ sec : isotonicity - projection ] a general theory in order to apply it to this special case .",
    "this seems to be the simplest way to tackle this problem .",
    "therefore , theorem [ fo ] below is interesting by itself . by using this approach , corollary [ coriso ]",
    "states that the monotone cones @xmath23 and @xmath19 admit isotone metric projections .",
    "then , we come back to the statistical framework , the remainder of the paper being organized as follows : the definitions , the analysis and the equivalence of the iterative isotone regression estimator and the iterative isotone bias reduction estimator are detailed in section [ sec : iir ]",
    ". as iterating these algorithms tends to reproduce the data , we then explain how the procedure might be stopped in practice ( section [ sec : simulation ] ) .",
    "finally , most of the proofs are gathered in the appendix .    to conclude this introduction",
    ", we would like to make a few comments on the topics that will * not * be addressed in the present document . starting from a sample @xmath31 of i.i.d .",
    "random couples with the same distribution as a generic pair @xmath32 , our estimator takes the form @xmath33 , where @xmath34 denotes the number of iterations possibly depending on the sample size @xmath35 . in this framework ,",
    "an important aspect is to specify conditions on the regression model ( [ eq : general - model ] ) and on the sequence @xmath36 so that , for example , @xmath37\\xrightarrow[n\\to\\infty]{}0,\\ ] ] where the expectation @xmath38 $ ] is considered with respect to the sample @xmath39 and the generic variable @xmath1 .",
    "if this property is satisfied , we say that the regression function @xmath33 is consistent .",
    "it is universally consistent if this property is true for all distributions of @xmath32 with @xmath40<\\infty$ ] ( see for example @xcite ) .",
    "this important issue will not be pursued further here and will be addressed elsewhere by the authors .",
    "it turns out that the isotonicity of the projection , as defined in ( [ eq : isotonicity ] ) for the specific case of the cone @xmath19 , is in general a very strong requirement which implies the latticiality of the order induced by the convex cone .",
    "thus , the investigation of the isotone projection cones becomes part of the theory of latticially ordered euclidean and hilbert spaces .",
    "a simple finite method of projection onto isotone projection cones proposed in @xcite has become important in the effective handling of all the problems involving projection onto these cones . besides nonlinear complementarity , isotone projection cones have applications in other domains of optimization theory .",
    "the method proposed in @xcite has become important in the effective handling of the problem of map - making from relative distance information e.g. , stellar cartography , see section 5.13.2.4 in @xcite and    .... www.convexoptimization.com/wikimization/index.php/projection_on_polyhedral_cone   ....    although we shall not consider projection methods here , we stress that some of the results developed in @xcite will be useful in our proofs .",
    "let us first introduce some notations .",
    "if @xmath41 is a non - empty , closed convex set in @xmath15 , then for each @xmath42 there exists a unique nearest point @xmath43 , that is , a point with the property that @xmath44 where @xmath45 stands for the euclidean norm in @xmath15 @xcite .",
    "the mapping @xmath46 is called the nearest point mapping of @xmath15 onto @xmath41 or simply the ( metric ) projection onto @xmath41 .",
    "let @xmath41 be a closed convex cone in @xmath15 , i.e. , a closed nonempty set with @xmath47 , and @xmath48 .",
    "if @xmath49 , then @xmath41 is called a closed convex pointed cone . in order to lighten the writings , and since all sets at hand in the following will be closed and convex , we propose to call them respectively `` cone '' and `` pointed cone '' .",
    "[ ossz ] suppose that @xmath41 is a cone , denote @xmath50 the maximal subspace contained in @xmath41 , @xmath51 its orthogonal complement , and @xmath52 .",
    "then , @xmath53 is a pointed cone in @xmath51 , @xmath54 where @xmath55 stands for the orthogonal sum , and @xmath56 where @xmath57 with @xmath58 and @xmath59    by putting @xmath60 whenever @xmath61 and @xmath62 , the cone @xmath63 induces a semi - order @xmath64 in @xmath15 which is translation invariant ( i.e. @xmath60 implies @xmath65 for any @xmath66 ) and scale invariant ( i.e. @xmath60 implies @xmath67 for any @xmath68 ) .    the projection @xmath69 is said _ @xmath41-isotone _",
    "if @xmath70 implies @xmath71 if @xmath69 is @xmath41-isotone , then @xmath41 is called an _",
    "isotone projection cone_. at this point , we would like to emphasize that in @xcite , the authors investigate isotone projection properties only for pointed cones .",
    "our purpose in this section is thus to generalize their results to cones which are not necessarily pointed , hence introducing of the decomposition @xmath72 in lemma [ ossz ] , where @xmath53 is a pointed cone .",
    "[ fo ] let @xmath63 be a cone , @xmath73 with @xmath50 and @xmath74 .",
    "then , @xmath41 is an isotone projection cone if and only if the pointed cone @xmath75 is an isotone projection cone in @xmath51 .    a simple geometric characterization of the isotone projection ( pointed ) cones was given in @xcite . it uses the notion of the polar of a cone .",
    "if @xmath63 is a cone , then the set @xmath76 is called the _ polar _ of the cone @xmath41 .",
    "the set @xmath77 is obviously a cone . if the cone @xmath41 is _ generating _ in the sense that @xmath78 , then the polar @xmath77 is a pointed cone .",
    "we have the following easily verifiable result :    [ mineknev ] suppose that @xmath41 is a generating cone . using the notations introduced in the theorem [ fo ] , and denoting the polar of the pointed cone @xmath53 in the subspace @xmath79 by @xmath80",
    ", we have the relation @xmath81 where @xmath82 is the inclusion mapping of @xmath79 into @xmath83    putting together the main result in @xcite , theorem [ fo ] and lemma [ mineknev ] , we have the following conclusion :    the generating cone @xmath41 is an isotone projection cone if and only if its polar @xmath77 is a cone generated by linearly independent vectors forming mutually non - acute angles .",
    "let us focus our attention on the monotone cone @xmath84 it is readily seen that @xmath85 is a generating cone , but not a pointed cone .",
    "specifically , let @xmath86    then @xmath87 , the maximal subspace contained in @xmath85 , is of dimension one .",
    "we have also that @xmath88 is an @xmath89-dimensional pointed cone in the hyperplane @xmath51 and @xmath90 we will prove that the pointed cone @xmath53 given by ( [ subcone ] ) is an isotone projection cone in @xmath51 , hence the following result .",
    "[ coriso ] the monotone cones @xmath23 and @xmath19 admit isotone projections",
    ".    this result will be the basic ingredient to show the equality between the algorithms and presented in the next section ( see theorem [ theo : egalite - algos ] ) .",
    "in this section , we first present the algorithm called iterative isotonic regression ( in short ) which proceeds in alternating isotonic and antitonic regressions in a backfitting fashion .",
    "the connection with von neumann s algorithm and recent results in this topic will allow us to exhibit the limit of this estimator when the number of iterations goes to infinity .",
    "let us first briefly recall the statistical framework and the idea behind our algorithm .",
    "we consider the nonparametric regression model @xmath91 where the random variable @xmath1 is , for example , uniform in @xmath3 $ ] .",
    "classical hypothesis for studying the consistency of an estimator of @xmath6 are @xmath4<\\infty$ ] and @xmath5=0 $ ] , but this will not play a prominent role in the following since we will not investigate the statistical properties of .",
    "more important , we will assume that the regression function @xmath6 is right - continuous and of bounded variation .",
    "then , the jordan decomposition asserts that such a function can be written as the sum of a non - decreasing and a non - increasing function : @xmath92 .",
    "specifically , if we impose that @xmath7 and @xmath8 have singular associated stieltjes measures and , for example , that @xmath93 then the decomposition is unique . in this case , we adopt the following notation @xmath94\\hspace{1cm}r(x)=u^{\\star}(x)+b^{\\star}(x),\\ ] ] and we call this latter writing the jordan minimum variation decomposition of @xmath6 .      in a statistical setting , we shall use a sample of i.i.d .",
    "random couples @xmath10 following the model ( [ eq : modele - generaliir ] ) and try to estimate the regression function @xmath6 . viewing @xmath95 and @xmath96 as the components of an additive model involving two monotone terms , the idea of is to apply alternatively isotonic and antitonic regressions .",
    "recall that @xmath11 denotes the ordered sample , that @xmath12 are the corresponding observations and that @xmath17 ( resp .",
    "@xmath97 ) is the isotonic ( resp .",
    "antitonic ) regression of @xmath98 .    besides , if @xmath99 , the notation @xmath100 stands for the @xmath89 dimensional vector defined as @xmath101 considering two vectors @xmath102 and @xmath103 , we may write @xmath104 for their term - by - term ( or hadamard ) product , which means @xmath105 if @xmath106 , we will say that @xmath102 and @xmath103 have singular variations .",
    "we are now in a position to specify the iterative isotonic regression algorithm .",
    "\\(1 ) initialization : @xmath107 ( 2 ) cycle : for @xmath108 + @xmath109 ( 3 ) iterate ( 2 ) until a stopping condition to be specified .",
    "we prove in the appendix ( see section [ sec : identifiability ] ) that at each iteration @xmath108 , @xmath110 where @xmath111 stands for the empirical mean of @xmath16 .",
    "these equations ensure the identifiability of the decomposition @xmath112 .",
    "one might indeed consider them as the translation of conditions ( [ eq : hyp - identifiabilite ] ) in a discrete context .",
    "in other words , given @xmath113 , both vectors @xmath114 and @xmath115 are uniquely specified .",
    "figure [ fig : iir - interpolation ] illustrates the application of the algorithm on the example of the fonction @xmath6 which is drawn on the top left hand side of the figure .",
    "still on the top left hand side of the figure , we have also plotted @xmath116 points @xmath117 to obtain our sample : for each point , one has @xmath118 , where @xmath119 is a gaussian centered random variable .",
    "besides these sample points , the three other figures show the estimations @xmath113 with @xmath120 iterations .",
    "one can see that for each iteration the method fits a piecewise constant function to the data and that increasing the number of iterations leads to increase the number of level sets ( clusters ) .",
    "it also appears on this example that iterating the algorithm leads to the interpolation of the original data .",
    "this property is always true , as established by the following result .",
    "[ theo : iir - interpolation1 ] with the previous notations , one has @xmath121    in the following , @xmath16 is held fixed and , as usual , @xmath122 stands for the translated cone @xmath123 all the notations are recalled on figure [ fig : von - neumann1 ] . to understand the geometrical forces driving proposition [ theo : iir - interpolation1 ] , this figure also provides a very simple interpretation of the algorithm , as it illustrates that the sequences of vectors @xmath114 and @xmath124 might be seen as alternate projections on the cones @xmath19 and @xmath122 . in what follows",
    ", we justify this illuminating geometric interpretation in a rigorous way , and we explain its key role in the proof of the convergence as @xmath125 goes to infinity .    first , by definition , we have @xmath126 , and a moment s thought shows that @xmath127 then , coming back to the very definition of @xmath128 leads to @xmath129 in the same manner , since @xmath130 , we get @xmath131 more generally , denoting @xmath132 , this yields for all @xmath108 @xmath133    it remains to invoke theorem 4.8 in @xcite to conclude that @xmath134 { } 0,\\ ] ] which ends the proof of proposition [ theo : iir - interpolation1 ] .",
    "a few remarks are in order :    1 .",
    "the take - home message here is that iterating the algorithm leads to overfitting , which is of course not desired in practice .",
    "consequently , a stopping criterion must be applied in order to avoid this phenomenon . in this aim , we have tested several pratical rules , and this will be the object of section [ sec : simulation ] .",
    "2 .   note that proposition [ theo : iir - interpolation1 ] ensures the convergence of the sum @xmath135 but does not say anything about the convergence of its individual terms @xmath114 and @xmath115",
    ". however , corollary 4.9 in @xcite asserts that the sequences @xmath114 and @xmath115 are also convergent .",
    "the specification of these limits will be possible in our situation thanks to the introduction of another equivalent algorithm , called",
    ". this will be the topic of the next subsection .",
    "3 .   in @xcite ,",
    "mammen and yu consider isotonic functions in the multivariate additive model ( [ eq : additive - models ] ) .",
    "they rely on the analysis of dykstra s algorithm sequences @xcite to show that for fixed @xmath35 the backfitting procedure converges to the solution of @xmath136 where @xmath7 is the sum of @xmath137 vectors in @xmath19 .",
    "hence , our case is rather comparable to the one considered by these authors when @xmath138 .",
    "correspondingly , in our setting , one can see the vectors @xmath139 and @xmath140 as dykstra s sequences and consider mammen and yu s alternate projections on the polar cones of @xmath19 and @xmath23 to prove proposition [ theo : iir - interpolation1 ] .",
    "however , as we are alternating projections onto opposite cones , considering @xmath141 and @xmath124 yields the easier von neumann s type interpretation given in our proof .      in this section",
    "we propose another algorithm inspired by bias reduction techniques in regression @xcite@xcite . in a nutshell",
    ", the idea here is to work on the updated residuals @xmath142 in order to refine the estimator at each step .",
    "specifically , this algorithm takes the following form .",
    "\\(1 ) initialization : @xmath143 ( 2 ) cycle : for @xmath108 @xmath144 updating : @xmath145 ( 3 ) iterate ( 2 ) until a stopping condition to be specified .",
    "interestingly , it turns out that algorithms and coincide .",
    "this remarkable fact , which is the purpose of the next theorem , deeply relies on the property of isotonicity of the projection onto the cones we consider . as a by - product",
    ", it will allow us to make precise the individual limits of the sequences @xmath114 and @xmath115 .",
    "[ theo : egalite - algos ] with the previous notations , for all @xmath108 , @xmath146    proposition [ theo : iir - interpolation1 ] ensures that @xmath113 tends to @xmath16 when @xmath125 goes to infinity .",
    "thanks to theorem [ theo : egalite - algos ] we will go one step further and show in corollary [ theo : iir - interpolation2 ] that the individual terms @xmath114 and @xmath115 tend to some explicit limits @xmath147 and @xmath148 .",
    "these limits are simply the discrete analogous of the jordan minimum variance decomposition of a function with bounded variation ( [ eq : jordanmin ] ) for the vector @xmath16 .",
    "as will be stated below , they are indeed characterized by @xmath149 and @xmath150 the empirical means of @xmath16 and @xmath151 being the same .    before proceeding ,",
    "let us illustrate this on the example of figure [ fig : convergence - individuel ] .",
    "the left - hand side represents the vector @xmath16 and the decomposition @xmath149 .",
    "one can see that @xmath152 and conversely , @xmath153 which , in the discrete case , amounts to say that @xmath95 and @xmath96 have singular associated stieltjes measures in equation ( [ eq : jordanmin ] ) .",
    "the right - hand side of the figure illustrates that when the number of iterations @xmath125 goes to infinity , @xmath114 and @xmath115 converge respectively to @xmath147 and @xmath148 .",
    "[ theo : iir - interpolation2 ] the sequences @xmath154 and @xmath155 admit the following limits @xmath156 where @xmath151 and @xmath157 are such that @xmath149 , @xmath158 , @xmath159 and @xmath160 .    in order to lighten the notations a bit ,",
    "let us assume that the empirical mean of @xmath16 is equal to zero .",
    "consequently , all the intermediates of both algorithms have also zero mean , as well as @xmath147 and @xmath148 .",
    "this allows us to work in the hyperplane @xmath161 , that means the subspace of @xmath15 consisting in all zero mean vectors . in this hyperplane",
    ", we introduce the norm @xmath162 to quantify the variations of a vector : @xmath163    then , it is routine to check that for any vector @xmath164 and any decomposition @xmath165 with @xmath166 and @xmath167 both in @xmath161 , we have the following equivalence @xmath168    now , corollary [ cor : identifiabilite ] in section [ sec : identifiability ] ensures that for all @xmath108 , @xmath114 and @xmath115 have singular variations ( one can also see that on figure [ fig : convergence - individuel ] ) which implies that for all @xmath108 @xmath169    accordingly , we just have to justify that the limits @xmath170 and @xmath171 satisfy @xmath172 to deduce that @xmath173 , hence @xmath174 and @xmath175 .",
    "the existence of the limits implies that for all @xmath176 , @xmath177 are well - defined . then , the continuity of the norm @xmath162 leads to @xmath178 and the same relation holds for @xmath113 and @xmath16 .",
    "thus , @xmath179 from equivalence ( [ eq : equivalence ] ) , we deduce that @xmath174 and @xmath180 , where @xmath149 is the jordan minimum variance decomposition of @xmath16 .",
    "as increasing the number of iterations leads to overfit the data , iterating the procedure until convergence is not desirable .",
    "this brings up to the choice of a stopping rule which could be used in practice . viewing the latter question",
    "as a model selection issue suggests stopping criteria based for example on akaike information criterion ( aic , see @xcite ) , modified aic criterion ( aicc , see @xcite ) , bayesian information criterion ( bic , see @xcite ) and generalized cross validation ( gcv , see @xcite ) .    for a linear smoother @xmath181 , with @xmath182 the smoothing parameter and @xmath183 the smoothing matrix",
    ", these selectors can be written in the common form @xmath184 where @xmath185 denotes the residual sum of squares and @xmath186 is an increasing function of the number of degrees of freedom @xmath187 of the smoother .",
    "one usually takes @xmath188 or @xmath189 ( see @xcite , section 2.7.3 ) and according to the various criteria mentioned above @xmath190    thus , equation ( [ eq : stop - commonform ] ) leads to a tradeoff between the goodness of fit and a penalization of high model complexity .",
    "although isotonic regression is not a linear smoother , we refer to meyer and woodroofe @xcite to consider that the number of distinct values among the fitted vector provides the effective dimension of the model . taking this into account and considering that increasing the number of iterations tends to reduce the residual sum of squares but raises the complexity of the model , a natural extension for iterative isotonic regression is to replace @xmath187 by the number of sets of the fitted vector @xmath141 in ( [ eq : stop - commonform ] ) and solve @xmath191 over a grid @xmath192 of iterations .",
    "we have applied and compared these stopping rules for iterative isotonic regression through simulated data .",
    "it appears that aicc shows the best performances among the three other criteria for most investigated cases .",
    "then , for this specific stopping criterion , we have compared iterative isotonic regression with non parametric competitors , namely local polynomial regression ( package ) , and smoothing spline regression ( package , function ) . for further details on this topic",
    ", we refer the interested reader to the package dedicated to at the following address :    .... www.sites.univ-rennes2.fr/laboratoire-statistique/jegou/index.html ....    the take - home message is that can not compete with spline or local polynomial regression for smooth functions .",
    "however , when the functions have discontinuities , our estimator compares favorably with his competitors , in particular when the sample size increases .",
    "this suggests that our method could be used to locate discontinuities in a regression framework .",
    "applications arise for example in genomic where the detection of breakpoints from array comparative genomic hybridization ( array cgh ) profiles is of crucial importance to identify genes involved in cancer progression @xcite .",
    "the relation ( [ ortossz ] ) follows directly from @xmath193 it is known ( see @xcite ) that the projection @xmath194 of @xmath195 onto the cone @xmath41 is characterized by the couple of relations : @xmath196 and @xmath197 hence , we have to verify the above relations for @xmath198 instead of @xmath194 . by the relation ( [ ortossz ] ) , @xmath199 then , take an arbitrary @xmath200 represented by ( [ ortossz ] ) in the form @xmath201 with @xmath202 and @xmath203 . then , we have @xmath204 because @xmath205 is perpendicular to @xmath206 , and because of the relation similar to ( [ elso ] ) characterizing the projection of @xmath207 onto the pointed cone @xmath53 in @xmath51 . thus , relation ( [ elso ] ) holds for @xmath198 in place of @xmath208 we further have @xmath209 because @xmath210 is perpendicular to @xmath211 and because of the relation similar to ( [ masod ] ) applied to @xmath212 and its projection onto @xmath53 .",
    "the obtained relation is exactly ( [ masod ] ) for @xmath198 instead of @xmath208      take @xmath213 .",
    "then , @xmath214 is equivalent to @xmath60 .",
    "if @xmath69 is @xmath41-isotone , then @xmath60 implies by lemma [ ossz ] @xmath215 since @xmath216 , it follows that @xmath217 the obtained relation shows that @xmath218 is @xmath53-isotone , concluding the proof of the necessity of the theorem .",
    "suppose now that @xmath218 is @xmath53-isotone and take @xmath61 with @xmath60 .",
    "if @xmath219 and @xmath220 with @xmath221 , and @xmath222 , then using formula ( [ ortossz ] ) @xmath223 and hence @xmath224 , that is @xmath225 and by the @xmath53-isotonicity of @xmath218 it follows that @xmath226 hence , using formula ( [ projek ] ) we have @xmath227 that is @xmath228 , which concludes the isotonicity of @xmath69 .      it clearly suffices to prove that the monotone cone @xmath229 admits an isotone projection . for this",
    ", we have to introduce some notations .",
    "let us first take the following base in @xmath15 : @xmath230 then , an arbitrary element @xmath231 can be represented in the form @xmath232 the relation @xmath233 being equivalent with @xmath234 let us consider further the following base in @xmath51 : @xmath235 the following notation is standard in the convex geometry and ordered vector space theory : if @xmath236 is a non - empty set , then let @xmath237 the set @xmath238 is the minimal cone containing the set @xmath239 and it is called _ the cone generated by @xmath239_. denoting @xmath240 and @xmath241 , we will see next that @xmath242 since @xmath243 , we have obviously that @xmath244 comparing the vectors @xmath245 and @xmath246 we get @xmath247 by substitution of @xmath248 , the representation ( [ kifejez ] ) of @xmath8 becomes @xmath249 suppose now that @xmath233 , that is , relations ( [ ekelem ] ) hold .",
    "then , the coefficients of @xmath250 in its representation ( [ kifejez1 ] ) are non - negative .",
    "thus , we have @xmath251 in particular , if @xmath252 , then , by , we have @xmath253 . hence , by multiplying ( [ kesz ] ) scalarly by @xmath254 and by using @xmath255 ( which follows from @xmath253 and @xmath256 ) and @xmath257 ( which follows from @xmath258 and @xmath256 ) , we get @xmath259 .",
    "this reasoning shows that @xmath260 inclusion which together with ( [ benne ] ) proves ( [ ak ] ) .",
    "we consider now the vectors @xmath261 then , @xmath262 and we have @xmath263 according to the reasonings in @xcite the relations ( [ polar ] ) show that @xmath264 is the polar of @xmath53 in the subspace @xmath51 .",
    "further , we have @xmath265 by the main result in @xcite this shows that @xmath53 is an isotone projection pointed cone in @xmath266      first we prove that for all @xmath108 , @xmath267 for @xmath268 , we have @xmath269 since @xmath270 and as @xmath19 is an isotone projection cone by corollary [ coriso ] , we deduce that @xmath271 belongs to @xmath19 . in the same manner ,",
    "@xmath272 which is equivalent to @xmath273 now , as was just noticed , @xmath274 belongs to @xmath23 , which is also an isotone projection cone , so that @xmath275 is indeed in @xmath23 .",
    "we can iterate this reasoning .",
    "for example , at the next step : @xmath276 and we may go on with the same arguments as before .",
    "next , we prove the desired result by induction on @xmath125 . at the first step ( @xmath268 ) , both algorithms clearly coincide .",
    "now let us assume that it is still true at step @xmath108 , which means @xmath277 our objective is to prove that @xmath278 . for this ,",
    "let us show that @xmath279 due to the fact that @xmath280 is the best non - decreasing approximation of @xmath124 , and since @xmath281 itself is a non - decreasing sequence , we deduce that @xmath282 now , as @xmath283 is the best non - decreasing approximation of @xmath142 , it is in particular a better approximation than @xmath284 , this latter being itself non - decreasing as was just established above ( see ( [ eq : intermediaire ] ) ) .",
    "thus , we are led to @xmath285 putting all the pieces together , ( [ eq : utile1 ] ) is proved , and we get @xmath286 this indicates that @xmath283 et @xmath284 both realize the minimal distance to @xmath142 . as a consequence , @xmath287 and finally @xmath288 the same arguments may be repeated to establish that @xmath289 .",
    "details are omitted .",
    "the purpose of this section is to prove the identifiability conditions ( [ eq : identifiabilite ] ) for the algorithm , that is , for all @xmath108 ( see also figure [ fig : illustration - identifiabilite ] ) @xmath290    the two last equations rely on a characterization of metric projections which was already mentioned in the proof of lemma [ ossz ] . from equations ( [ elso ] ) and ( [ masod ] ) , we know that for any @xmath291 , the vector @xmath292 ( resp .",
    "@xmath23 ) is the isotonic ( resp .",
    "antitonic ) regression of @xmath16 if and only if @xmath293 and @xmath294    taking successively @xmath295 and @xmath296 in ( [ eq : caracterisation1bis ] ) leads to @xmath297 which proves that for all @xmath108 , @xmath298 and @xmath299 . to prove that @xmath300",
    ", we will need the following result .",
    "let us prove for example that @xmath303 we first establish that after an isotone regression , the last observation @xmath304 of a cluster @xmath305 is always lower than or equal to @xmath306 .",
    "for this , we use a proof by contradiction , arguing that a situation like the one depicted in figure [ fig : proposition - identifiabilite ] is impossible .",
    "let us denote @xmath305 the cluster with last element @xmath304 .",
    "for the sake of simplicity , @xmath306 stands for the last common value to all elements of cluster @xmath305 after the isotone regression . from the properties of the pool adjacent violators algorithm",
    "( see for example @xcite ) , it is well - known that @xmath306 satisfies the following equation @xmath307 where @xmath308 is the cardinal of @xmath305 .",
    "equivalently , the next cluster is denoted @xmath309 and the corresponding isotonic value @xmath310 .",
    "now , let us assume that @xmath311 , and denote @xmath312 so that , clearly , @xmath313 .",
    "besides , it is readily seen that @xmath314 , else @xmath304 would belong to cluster @xmath309 . since @xmath315 is the average of the @xmath316 s belonging to @xmath317 , the following inequality holds @xmath318 so that @xmath319 this latter inequality indicates that the isotone regression with the values @xmath320 would be better than the original one with @xmath321 , which is in contradiction with the very definition of the isotone regression , therefore @xmath322 .          equation ( [ eq : bir - ident1 ] ) follows immediately from proposition [ pro : intermediaire - identifiabilite ] . on the other hand , equation ( [ eq : bir - ident2 ] ) requires more attention .",
    "we prove this by induction .",
    "this is obviously true for @xmath268 by proposition [ pro : intermediaire - identifiabilite ] .",
    "let us fix @xmath108 , and assume that @xmath329 since @xmath330 and @xmath331 , we get @xmath332 and our objective is to prove that both terms on the right - hand side of this equation are equal to zero .",
    "first notice that @xmath333 and by definition of @xmath280 , @xmath334 then , proposition [ pro : intermediaire - identifiabilite ] gives @xmath335 so that @xmath336 and finally @xmath337 let us now turn to the second term on the right - hand side of equation ( [ eq : rec1 ] ) . just note that @xmath338 where @xmath339 invoking proposition [ pro : intermediaire - identifiabilite ] again , we are led to @xmath340 and the right - hand side of equation ( [ eq : pro1 ] ) is equal to zero .",
    "* remark : * one can notice that the proof of corollary [ cor : identifiabilite ] uses the fact that algorithms and coincide ( theorem [ theo : egalite - algos ] ) , but of course the proof of this theorem did not make any use of corollary [ cor : identifiabilite ] .",
    "a quick inspection shows that this last result is applied only in the demonstration of corollary [ theo : iir - interpolation2 ] ."
  ],
  "abstract_text": [
    "<S> in the present paper , we propose and analyze a novel method for estimating a univariate regression function of bounded variation . </S>",
    "<S> the underpinning idea is to combine two classical tools in nonparametric statistics , namely isotonic regression and the estimation of additive models . </S>",
    "<S> a geometrical interpretation enables us to link this iterative method with von neumann s algorithm . </S>",
    "<S> moreover , making a connection with the general property of isotonicity of projection onto convex cones , we derive another equivalent algorithm and go further in the analysis . as iterating the algorithm leads to overfitting , several practical stopping criteria are also presented and discussed .    _ index terms _  nonparametric estimation , isotonic regression , additive models , metric projection on convex cones .    _ </S>",
    "<S> 2010 mathematics subject classification _ : 52a20 , 62g08 , 90c33 .    a geometrical approach to iterative isotone regression +    arnaud guyader + universit rennes 2 , inria and irmar + campus de villejean , rennes , france +    nicolas jgou + universit rennes 2 + campus de villejean , rennes , france +    alexander b. nmeth + faculty of mathematics and computer science + babe bolyai university , ro-400084 cluj - napoca , romania +    sndor z. nmeth + school of mathematics , the university of birmingham + birmingham b15 2tt , united kingdom + </S>"
  ]
}