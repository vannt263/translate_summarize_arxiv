{
  "article_text": [
    "manifold learning is a popular data analysis framework that attempts to recover compact low - dimensional embeddings of high - dimensional datasets .",
    "several manifold learning algorithms  including isomap  @xcite , locally linear embedding  @xcite , diffusion maps  @xcite , and laplacian eigenmaps  @xcite  derive coordinate representations that encode the local neighborhood structure of an unlabeled data sample .",
    "these techniques have found considerable success in a wide array of application domains , including computer vision  @xcite , speech processing  @xcite , and natural language processing  @xcite . in  @xcite",
    ", it was shown that these algorithms are all members of a more general graph embedding framework , in which the transformations are derived via a generalized eigendecomposition of the graph laplacian matrix operator for algorithm - specific graph construction methodologies .    in their basic form , these graph",
    "embedding techniques only provide transformations of the training samples used to construct the graph .",
    "thus , even if a large training set is used , computing the output of the estimated map for a novel test sample is not possible . to address this shortcoming , a nonparametric out - of - sample extension technique based on sampling",
    "was developed that leverages the input and target representation pairs for each training sample to approximate what the map would have generated for an arbitrary test point  @xcite .",
    "while generally effective , the extension is a kernel - based method with time complexity that scales linearly with the number of training samples .",
    "this increase in computational cost is especially problematic because manifold methods are most effective when provided the benefit of large training sets for representation learning",
    ". it would be highly beneficial to remove this trade - off between representation quality and extension feasibility with a more efficiently scaleable method for out - of - sample extension .",
    "neural networks have long been known to be a powerful learning framework for classification and regression , capable of distilling large training sets into efficiently evaluated parametric models , and thus are a natural choice for modeling manifold embeddings . in their seminal paper , hornik _",
    "et al . _",
    "@xcite proved that feedforward neural networks can approximate a virtually arbitrary deterministic map between high - dimensional spaces , indicating that they would also be ideally suited for our out - of - sample extension problem .",
    "however , there are two caveats for the use of neural networks as universal approximators : ( i ) there must be sufficient hidden units ( i.e. sufficient model parameters ) , which in turn require additional data samples for training without overfitting ; and ( ii ) the non - convexity of the objective function grows with the number of model parameters , making the search for reliable global solutions increasingly difficult .    with these considerations in mind , we explore the application of recent advances in deep neural network ( dnn ) training methodology to the out - of - sample extension problem .",
    "first , by stabilizing the lanczos eigendecomposition algorithm , we are able to produce exact graph embeddings for training sets with millions of data samples .",
    "this permits an extensive study with deeper architectures than have been previously considered for the task .",
    "second , motivated by success in the supervised classification setting  @xcite , we consider unsupervised dnn pretraining procedures to improve optimization as our larger training samples support commensurate increases in model complexity .    in the work that follows ,",
    "we compare the performance of our parametric dnn approach against a sampling baseline , both in terms of approximation fidelity and test runtime .",
    "we find dnns to match or outperform the approximation fidelity of the method for all training sample sizes .",
    "furthermore , since the dnn approach is parametric , its test - time complexity for fixed network size is constant in the training sample size , producing orders - of - magnitude speedup over sampling for larger training sizes .",
    "the remainder of this paper is organized as follows .",
    "we begin with an overview of prior work in out - of - sample extension for graph embeddings .",
    "we then describe the strategy for stabilizing eigendecompositions for large training sets , followed by a description of the process for training our dnn out - of - sample extension to approximate the embedding for unseen data .",
    "finally , we analyze the reconstruction accuracy and computation speed of both the baseline and the dnn approach .",
    "the most popular methods for extending graph embeddings to unseen data have been based on sampling  @xcite , and thus they will serve as the baseline in our experiments .",
    "this is a nonparametric , kernel - based technique that approximates the embedding of each test sample by computing a weighted interpolation of the embeddings for training samples that were nearby in the original input space .",
    "formally ( see  @xcite for details ) , let @xmath0 be the set of graph embedding training samples , where each @xmath1 .",
    "let @xmath2 be the symmetric , normalized graph laplacian operator defined for the set @xmath3 such that @xmath4 , where @xmath5 for some positive semidefinite kernel function @xmath6 that must be specialized for each specific graph embedding algorithm ; and @xmath7 is the diagonal matrix defined via @xmath8 .",
    "let the spectral decomposition of the normalized laplacian be denoted as @xmath9 , where the diagonal entries of @xmath10 are non - increasing .",
    "the @xmath11-dimensional embedding of @xmath3 is then provided by the first @xmath11 columns of @xmath12 , which we shall denote as @xmath13 .",
    "stated simply , the embedding of @xmath14 is given by the @xmath15-th row of @xmath13 .",
    "to embed an out - of - sample data point @xmath16 via the extension , the @xmath17-th dimension of the extension @xmath18 is given by @xmath19 we see that the complexity of this extension is linear in the size of the training set . in practice , approximate nearest neighbor techniques can be used to speed this up with minimal loss in fidelity ( the implementation we benchmark uses k - d tree for this purpose ) , but the algorithmic complexity still increases with training size .",
    "finally , note that a nearly equivalent formulation based on reproducing kernel hilbert space theory was presented in  @xcite , where the kernelization was introduced into the objective function before the eigendecomposition is performed .",
    "this formulation has the same scalability limitations as the extension .",
    "these computational difficulties motivate our exploration of dnns to model embeddings for out - of - sample extension .",
    "traditional neural networks have also been considered for out - of - sample extension in the past in two limited studies involving small datasets and model architectures  @xcite .",
    "the idea was introduced in  @xcite , but the study failed to include a meaningful quantitative evaluation .",
    "the experiments in  @xcite , which predated the advent of recent deep learning training methodologies , found neural networks to be one of the worst performing methods",
    ". however , with a similar motive of computational efficiency , @xcite explored the use of dnns for approximating expensive sparse coding transformations and produced more compelling results .",
    "a truly scalable out - of - sample extension must simultaneously consume a large amount of training data for detailed modeling and provide a test - time complexity that does not strongly depend on that training set size .",
    "the nonparametric nature of the method leads to a linear dependence on the training set size ( logarithmic if kernel approximations are implemented ) and thus can get bogged down as we feed more data to the graph embedding training .",
    "we begin this section with a simple trick for the eigendecomposition of large graph laplacians , which permits larger training sets and motivates the need for more computationally efficient extension methods .",
    "this is followed by a presentation of the deep neural network architecture we propose to efficiently extend the embedding to arbitrary test points .      in  @xcite",
    ", it is suggested that the stability of the lanczos eigendecomposition algorithm can be greatly increased ( and memory requirements consequently reduced ) by reformulating the eigenproblem to recover the largest eigenvalues .",
    "we can exploit this by observing that if @xmath20 is an eigenvector of @xmath2 with eigenvalue @xmath21 , then @xmath20 is also an eigenvector of @xmath22 with eigenvalue @xmath23 ( which is guaranteed to be less than or equal to 1 ) .",
    "thus , with this small redefinition of the eigenproblem , we can recover the same eigenvectors by considering the largest eigenvalue criterion .",
    "note that when using the arpack implementation , a similar effect can also be accomplished by searching for the smallest _ algebraic _ eigenvalues of @xmath2 directly .",
    "while this trick is by no means a fundamental theoretical innovation on our part , its effects have proven dramatic .",
    "our past efforts to solve for the smallest magnitude eigenvalues of the graph laplacian exceeded our hardware memory limits when our graphs reached the order of 100,000 nodes and 1 million edges . employing this simple trick ,",
    "we have now succeeded in processing graphs with order 100 million nodes and order 10 billion edges on conventional hardware , stably solving for the top 100 eigenvectors in a few days using 32 cores and 0.5 tb of ram .",
    "this problem size even exceeds what was reported using approximate singular value decomposition solvers in the past  @xcite .",
    "for the 1.5 million node graphs we consider in our experiments described below , this method was more than adequate for our ( offline ) embedding training needs .      solving the eigenvalue problem produces a @xmath11-dimensional ( exact ) embedding @xmath24 for each @xmath25 .",
    "rather than viewing new data points as out - of - sample points whose mapping is estimated with interpolation , we instead seek to estimate the mapping ( from @xmath26 to @xmath27 ) itself . to this end",
    ", we now consider feedforward neural architectures with @xmath28 hidden layers , each containing @xmath29 hidden units .",
    "the @xmath30-th hidden layer nonlinearly maps the output of the previous layer @xmath31 to a new hidden representation @xmath32 according to    @xmath33    where each @xmath34 is a parameter matrix , @xmath35 is a bias column vector , and @xmath36 is the activation function ( which we set to @xmath37 in our experiments ) .",
    "the input @xmath38 to the first layer is a point @xmath26 in our input space @xmath39 , while @xmath40 , @xmath41 for @xmath42 , and @xmath43 for @xmath44 .",
    "the output @xmath45 of these @xmath28 hidden layers are finally transformed into a corresponding point @xmath46 according to    @xmath47    where @xmath48 and @xmath49 are the decoding weight matrix and bias column vector , respectively .",
    "the training objective is to solve for parameters @xmath50 that minimize mean squared error between the exact and predicted embedding pairs :    @xmath51    this is generally accomplished with backpropagation and stochastic gradient descent optimization .",
    "it is critical that the training procedure safeguards against overfitting to the training sample , especially as deeper architectures are required in order to approximate the detailed graph embeddings we wish to extend .",
    "our training procedure , which was also employed in  @xcite for a different application , has two steps : ( i ) unsupervised stacked autoencoder pretraining that uses @xmath3 only , and ( ii ) supervised fine - tuning using the @xmath52 pairs as training inputs and targets .",
    "when the input and output targets are the same , our deep network architecture reduces to a stacked autoencoder ( sae ) with @xmath28 encoding layers and one decoding layer .",
    "thus , to initialize model parameters of the dnn to approximately recover the identity mapping , we consider the unsupervised sae pretraining procedure  @xcite . here , we introduce one hidden layer at a time , performing several epochs of stochastic gradient descent to minimize mean squared error between our training samples and themselves at each intermediate network depth .",
    "as we add each new layer , we discard the linear decoding weights from the previous optimization , use the previous hidden representation as input to the new hidden layer , and reoptimize all layer parameters . early stopping is used to prevent exact recovery of the identity map for each layer .      using the above layer - wise pretraining procedure , we now have initialized all parameters in the network .",
    "it only remains to perform several epochs of stochastic gradient descent to reoptimize network parameters to minimize mean squared error between the exact and predicted embedding pairs according to equation  ( [ eq : dnnopt ] ) . after this training is complete , the dnn - based out - of - sample extension @xmath53 for an arbitrary point @xmath54 can be efficiently computed using the standard neural network forward pass defined by equation  ( [ eq : fwdpass ] ) .",
    "this amounts to @xmath55 matrix - vector multiplies and vector additions , plus an evaluation of @xmath36 for each hidden unit . for a fixed network architecture ,",
    "this computation is constant in the number of training samples .",
    "[ cols= \" > , > , > , > , < \" , ]",
    "we choose speech as the application domain for our scalability study for two reasons : ( i ) a single hour of audio recordings typically produce 360,000 high - dimensional data samples known as frames , and so large datasets are readily available ; and ( ii ) manifold learning techniques have been shown to learn representations effective for keyword discovery and search  @xcite .",
    "we use the timit corpus for evaluation , given the past success of manifold embeddings for speech recognition on that data  @xcite .",
    "it consists of over 4 hours of prompted american english speech recordings , and is split into a training set consisting of roughly 1.1 million data samples ( @xmath563 hours ) and a test set of roughly 400,000 data samples ( @xmath561 hour ) .",
    "there is no speaker overlap between the two sets .",
    "our goal is to measure the approximation fidelity of the out - of - sample extension of the test set against a reference embedding that is independent of the extension method .",
    "thus , our strategy is to perform an exact graph embedding of the entire corpus ( train+test ) using the method described in section  [ sec : bigeigs ] ; we call these the _ reference embeddings_. we define each out - of - sample extension using input frames and corresponding reference embeddings from the training set .",
    "this allows us to use reference embeddings for the training set to approximate embeddings for the test set for comparison against the true reference embeddings of the test set .",
    "we measure approximation fidelity in terms of normalized root mean squared error ( nrmse ) between the predicted and reference test embeddings ; here , the normalization constant is taken to be the root mean squared error between the test set reference embeddings and a random permutation of those same samples .",
    "thus , a perfect out - of - sample extension will have nrmse of 0 , while a extension that is a random mapping with the same empirical output distribution will have nrmse of 1 .",
    "in addition to defining the extensions with the entire training set ( 1.1 m samples ) , we also consider the utility of random subsets of sizes 1,000 , 10,000 , and 100,000 .",
    "however , we use the same reference embeddings for all experiments .",
    "our input features are 40-dimensional , homomorphically smoothed , mel - scale , log power spectrograms ( 40 equally spaced mel bands from 0 - 8 khz ) , sampled every 10 ms using a 25 ms hamming window .",
    "we construct the graph laplacian using a symmetrized 10-nearest neighbor adjacency graph with cosine distance as the metric and binary edge weights .",
    "this amounts to a laplacian eigenmaps specialization of the graph embedding framework .",
    "finally , we keep the 40 eigenvectors with the largest eigenvalues to produce a graph embedding with the same dimension as the input space .",
    "while our present focus is on out - of - sample extension fidelity , it is relevant to note that the reference embeddings match the best downstream performance reported in  @xcite , which used an identical embedding strategy .    for the baseline method , we compute equation  ( [ eq : nys ] ) using a nearest neighbor approximation to a radial basis function ( rbf ) kernel .",
    "this approximation is facilitated by preprocessing the training samples into a k - d tree data structure ( as implemented in ` scikit - learn ` ) for efficient retrieval of near - neighbor sets .",
    "note that we tried matching the kernel to that used in the graph construction ( i.e. , using binary weights ) as prescribed by  @xcite , but it performed substantially worse than introducing rbf weights .",
    "we consider kernel squared - bandwidths @xmath57 and number of neighbors @xmath58 .    for our dnn method , we consider @xmath59 hidden layers , and for each depth we evaluate layer sizes of @xmath60 hidden units .",
    "for pretraining , we use the entire training set and optimize each layer for 15 epochs of stochastic gradient descent . following the prescription in  @xcite",
    ", we use a learning rate of @xmath61 and minibatches of 256 samples . for supervised fine - tuning",
    ", we increase the number of epochs for the smaller training sets such that the total number of examples processed is roughly fixed ( 5 epochs for the full train set of 1.1 million samples , 50 epochs for 100,000 samples , etc ) to ensure adequate convergence . also following  @xcite , for fine - tuning we use a learning rate of @xmath62 , but found a smaller minibatch of size 50 improved convergence .",
    "we use the ` pylearn2 ` toolkit  @xcite for all dnn experiments .",
    "c | c | c | c | c | c | c train & & & + size & nrmse & time & nrmse & time & nrmse & time + 1k & 0.36 & 25 & 0.33 & 5.4 & 0.28 & 33 + 10k & 0.29 & 240 & 0.29 & 5.4 & 0.24 & 33 + 100k & 0.25 & 2,200 & 0.29 & 5.4 & 0.23 & 33 + 1.1 m & 0.24 & 12,000 & 0.29 & 5.4 & 0.23 & 33 +      first , we consider the nrmse performance as we vary the amount of training samples used by the out - of - sample extension .",
    "we drew random subsets of the 1.1 million sample training set of sizes 1000 , 10,000 , and 100,000 .",
    "each of these subsets was used for computation of equation  ( [ eq : nys ] ) in the method and for supervised fine - tuning in the case of the dnn method .",
    "table  [ tab : best ] lists for each training subset size the nrmse and test runtime for ( i ) using the optimal set of hyperparameters , ( ii ) a smaller dnn with 2 layers of 60 units each , ( iii ) a larger dnn with 5 layers of 140 units .",
    "we see that the larger dnn matches or outperforms the method for all training set sizes , demonstrating the power of deep learning for accurately approximating complex nonlinear functions . while the small dnn matches for the 2 smaller training sets , it does not have sufficient parameters to keep pace as more training data becomes available .",
    "these results emphasize the importance of each method s sensitivity to the choice of hyparameters that specify the complexity of the out - of - sample extension function .",
    "this is especially true for fully unsupervised representation learning settings , where cross - validation may not be possible . to explore this consideration , for the method , we vary the number of ( approximate ) nearest neighbors that contribute to each test sample , as well as the kernel bandwidth . for the proposed dnn method we vary both the number of layers ( @xmath28 ) and the number of hidden units per layer ( @xmath29 ) .",
    "figure  [ fig : params ] shows heatmaps indicating the nrmse for all hyperparameters considered for the two methods for the four training set sizes . for the method ,",
    "performance is relatively stable across the range of kernel bandwidths , but it is more sensitive to the number of neighbors used in the computation .",
    "moreover , the optimal number of neighbors increases as more training data is available , necessitating some amount of parameter tuning to achieve optimal approximation .",
    "the dnn approach reaches near - optimal performance for all training set sizes provided we include at least 4 layers with at least twice as many units than the input dimension .",
    "critically , there is no performance penalty for overshooting the network size ( other than increased forwardpass runtime , as we discuss below ) .",
    "this suggests the dnn extension would require less tuning than method to achieve optimal approximation in new applications .",
    "+    in typical machine learning scenarios , increasing the number of model parameters for a fixed training set size opens the method up to overfitting and poor generalization .",
    "the trends for the dnn methods in figure  [ fig : params ] defy this conventional wisdom , with no loss in approximation fidelity for any training corpus size as we move to deeper and wider network architectures . indeed , the unsupervised pretraining is responsible for regularizing the parameter estimates , preventing overspecialization to even the smallest training set considered .",
    "this can be seen most clearly in the scatterplots in figure  [ fig : pretrain ] , where each dot represents a single model architecture .",
    "as the total number of parameters increases , the test nrmse of the pretrained models decays roughly monotonically .",
    "the same architectures without pretraining track similarly for smaller model sizes , but , due to overfitting , the test performance degrades as more parameters are made available .",
    "this behavior is especially clear in the case of limited training examples , though we see the beginnings of the same effect for the largest architectures even in the presence of the full training set .      finally , we consider the computational efficiency of applying the two out - of - sample extension methods to a test corpus . table  [ tab : best ] lists the nrmse values and corresponding test times in seconds ( i.e. , the time taken to extend the embedding to the entire 400,000 sample test set ) for the method with optimal hyperparameters and two dnn architectures . as expected , the runtimes for the nonparametric method increase as the training sample gets bigger , since nearest neighbor retrieval remains expensive even when using the k - d tree data structure . meanwhile , the dnn runtimes are virtually constant for a fixed number of parameters .",
    "the small dnn can consume the full training set and produce extensions over 4 times faster than the method with the smallest training set , while simultaneously reducing nrmse by 20% relative .",
    "moreover , the best dnn roughly matches the approximation fidelity of the best nrmse , and the dnn accomplishes it @xmath56350 times faster . for all training sizes tested",
    ", dnn extensions can provide signficant speedup without any sacrifices in fidelity , and , in many cases , improve both speed and fidelity .    for speech processing applications , where interactivity is often critical , even our largest networks can process test samples faster than real - time",
    ". moreover , with the large dnn consisting of 5 layers of 140 hidden units , optimal nrmse is achieved at speeds 120 times faster than real - time using a single processor .",
    "this is comparable to the extraction speed of traditional acoustic front - ends in state - of - the - art implementations  @xcite .",
    "in this work , we used modern deep learning methodologies to perform out - of - sample extensions of graph embeddings . compared with the standard sampling - based out - of - sample extension ,",
    "the dnns approximate the embeddings with higher fidelity and are substantially more computationally efficient .",
    "using unsupervised pretraining for parameter initialization improves dnn generalization , making our dnn approach highly stable across a wide variety of hyperparameter settings .",
    "these results support deep neural networks with unsupervised pretraining as an ideal choice for out - of - sample extensions of learned manifold representations .",
    "the authors would like to thank herman kamper of the university of edinburgh for providing his correspondence autoencoder tools , on which we based our neural network implementation .",
    "32 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , . .",
    ", . , , , .",
    ". . , . , , , , et  al . ,",
    ". . , . , , , , , ,",
    ". . , . , , , , . , in : , . pp . . , ,",
    ". . , . , , . , in : , . pp . . , ,",
    "volume  . .",
    ", , , , , . , in : , pp . .",
    ", , , , , , , , , . .",
    ", , . , in : , pp . .",
    ", , , , , . . ,",
    ", , , . . ,",
    ", , , , , , , , , , et  al . , .",
    ", , . , in : . , , .",
    ", , , . , in : . , , , , . , in : . , , ,",
    ". . , . , ,",
    ". . , . , ,",
    ", , , , , , , , , , et  al . , . , in : . , ,",
    ". . , . , ,",
    ". . , . , , . , in : , .",
    "pp . . , , , ,",
    ", et  al . , . .",
    ", , , . , in : , .",
    "pp . . , , ,",
    ", , . , in : , .",
    "pp . . , , , , , ,"
  ],
  "abstract_text": [
    "<S> several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space . </S>",
    "<S> the resulting eigenvectors encode the embedding coordinates for the training samples only , and so the embedding of novel data samples requires further costly computation . in this paper , we present a method for the out - of - sample extension of graph embeddings using deep neural networks ( dnn ) to parametrically approximate these nonlinear maps . compared with traditional nonparametric out - of - sample extension methods , we demonstrate that the dnns can generalize with equal or better fidelity and require orders of magnitude less computation at test time . </S>",
    "<S> moreover , we find that unsupervised pretraining of the dnns improves optimization for larger network sizes , thus removing sensitivity to model selection . </S>"
  ]
}