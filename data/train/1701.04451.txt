{
  "article_text": [
    "data deduplication is a commonly used technique to reduce storage requirements in data centers and enterprise servers .",
    "it operates by identifying and removing duplicate blocks of data over long ranges . for example , consider a corporate logo used in many slide decks of that corporation .",
    "the enterprise storage server , using deduplication , can store only the first occurrence of the logo and replace subsequent occurrences with pointers to the earlier stored one .    the above example highlights key differences between deduplication and algorithms used to compress single files .",
    "these latter , by now standard , data compression approaches include deflate @xcite ( based on lz77 @xcite and used in the popular zlib and gzip utilities ) and ppm ( prediction by partial match ) @xcite .",
    "they operate by finding small amounts of local redundancy . for example , deflate uses a sliding window and restricts the match length to a maximum of @xcite ( although the typical match length is likely considerably smaller  on the order of a few tens of bytes ) .",
    "similarly , ppm typically uses a context of up to @xcite .",
    "in contrast , data deduplication finds larger amounts of global redundancy .",
    "for example , @xcite reports finding duplicates on the order of a few up to hundred over ranges of several hundreds of up to a few .",
    "thus , the main difference between data deduplication and single - file compression approaches is the scale at which they operate .    to deal with this large scale , deduplication",
    "algorithms use an approach called chunking . in the simplest version ,",
    "the stream of data ( of size up to several ) is split into chunks of fixed size ( say ) .",
    "the algorithm sequentially processes the stream of chunks . for each chunk",
    ", the algorithm computes a hash value , used as key into a hash table .",
    "if the hash table does not already contain an entry with that key , the algorithm enters the chunk into the hash table ( hash collisions can be avoided by proper dimensioning of the length of the hash value ) . the chunk is then deduplicated by replacing it with its hash value .",
    "the hash table and the sequence of chunk hashes are stored on disk . since indexing into the hash table",
    "can be performed in constant time , this chunking approach is computationally efficient and can be performed over large amounts of data .",
    "this fixed - length chunking has the disadvantage that it is susceptible to shifts of the duplicate data blocks .",
    "returning to the corporate logo example , if the positions of the logo in the data stream are not aligned with respect to the chunk boundaries , then duplicates will not be discovered . to address this issue , most deduplication systems instead use variable - length chunking , in which the chunk boundaries are defined by the occurrence of short pre - defined marker sequences .",
    "the chunks now have variable random length . by choosing the length of the marker sequence , the expected length of the chunks can be controlled . the use of marker sequences `` resynchronizes '' the appearance of shifted redundant data blocks , allowing to successfully deduplicate them .",
    "deduplication has received significant amounts of attention in the computer systems literature , as surveyed in section  [ sec : intro_related ] .",
    "it is also widely used in practice ; for example , it is reportedly being used both by dropbox @xcite and by microsoft windows server 2012 @xcite . despite its significance , data deduplication seems not to have been studied from a theoretical point of view .",
    "in particular , an information - theoretic analysis of its performance limits is missing .",
    "this paper provides such an information - theoretic analysis of data deduplication .",
    "the main results of this paper are as follows :    * it introduces a simple source model , which captures the long - range memory and the synchronization issues observed in the data deduplication problem . * it formalizes two concise versions of the data deduplication approach , one with fixed chunk length and one with variable chunk length .",
    "it also proposes a third , novel , multi - chunk deduplication scheme . *",
    "it analyzes the performance of these three schemes .",
    "the fixed - length deduplication scheme is shown to be close to optimal when the source - block lengths are constant and known a - priori . however ,",
    "when the source - block lengths are variable , fixed - length deduplication is shown to be substantially suboptimal .",
    "the reason for this suboptimality is formally shown to be due to the lack of synchronization between source block and deduplication chunk boundaries . * the variable - length deduplication scheme is shown to better handle this lack of synchronization .",
    "careful choice of marker sequence length ( or , equivalently , expected deduplication - chunk length ) is shown to be critical for good performance of variable - length deduplication . *",
    "finally , the proposed multi - chunk deduplication scheme is shown to be much less sensitive to the expected deduplication chunk length . as a consequence",
    ", it can better adapt to the source statistics , and has order - optimal performance under fairly mild conditions .",
    "the use of variable - length chunking for the purposes of detecting similar files in large file systems was proposed in @xcite .",
    "deduplication based on this variable - length chunking idea was proposed in @xcite in the context of a network file system .",
    "the largest gains of data deduplication are achieved when storing different versions of the same data such as in archival storage @xcite and backup systems @xcite .",
    "however , data deduplication has also been successfully applied to primary storage systems @xcite .",
    "a further area of application is virtual machine hosting centers , where data deduplication is used for virtual machine migration @xcite and for virtual machine disk image storage @xcite .",
    "as already mentioned , data deduplication has not yet been investigated from an information - theoretic point of view .",
    "the closest problems in the information theory literature are compression with unknown alphabets @xcite , also known as multi - alphabet source coding @xcite , or the zero - frequency problem @xcite .",
    "in fact , as will become clearer in the following , the large repeated blocks in the source data can be interpreted as being part of an unknown alphabet that has to be learned and described by the encoder .",
    "the related problem of file synchronization has been studied extensively in the information - theoretic literature @xcite .",
    "the synchronization problem is concerned with duplicates between two versions of the same file .",
    "in contrast , deduplication deals with a large number of duplicated files or data blocks , and the correspondence between them is not known a - priori .",
    "the remainder of this paper is organized as follows .",
    "section  [ sec : setting ] provides a formal definition of the problem setting , including the source model and the deduplication schemes .",
    "section  [ sec : results ] presents the main results .",
    "all proofs are deferred to appendices .",
    "we consider a source alphabet @xmath0 , of size @xmath1 , generated randomly as follows .",
    "fix a distribution @xmath2 over @xmath3 with finite mean @xmath4 . generate @xmath5",
    "independently and identically distributed ( i.i.d . )",
    "random variables @xmath6 from @xmath2 .",
    "we next generate a sequence of binary strings @xmath7 for each @xmath8 : starting with @xmath9 , choose @xmath10 uniformly at random from @xmath11 .",
    "thus , each @xmath10 is a binary string of length @xmath12 , called a source symbol in the following .",
    "finally , the source alphabet @xmath13 ( where @xmath14 denotes the set of all finite - length binary sequences ) is defined as the union of all the source symbols , @xmath15 note that @xmath16 is equal to @xmath5 by construction of the @xmath10 . to simplify some of the derivations later on",
    ", we assume that @xmath17 is tightly concentrated around its mean , specifically that @xmath18 . furthermore , to ensure that the source alphabet generation is always well defined , we assume that @xmath19 .    from this",
    "randomly generated source alphabet @xmath0 , we now generate a random source sequence @xmath20 as follows . choose @xmath21 elements @xmath22 uniformly at random from @xmath0 .",
    "thus , each @xmath23 is an element of @xmath14 .",
    "the sequence @xmath23 is referred to as a source block . construct the source sequence @xmath20 as the concatenation @xmath24 of @xmath25 .",
    "we emphasize that @xmath20 is an element of @xmath14 , in other words , the boundaries of the source blocks @xmath25 are _ not _ preserved .",
    "denote by @xmath26 the ( random ) length of @xmath20 .",
    "[ eg : setting ] assume the source alphabet is randomly generated as @xmath27 . from this , @xmath28 elements are drawn uniformly at random , say @xmath29 and @xmath30 .",
    "the resulting source sequence is then @xmath31 .",
    "note that , from @xmath20 alone , it is not clear if @xmath20 was generated as the concatenation of @xmath32 and @xmath33 or as the concatenation of @xmath34 and @xmath35 .",
    "there is a third possible parsing of @xmath20 into the five elements @xmath34 , @xmath36 , @xmath34 , @xmath34 , and @xmath36 from @xmath0 .",
    "however , knowing @xmath28 , this third parsing is not valid .",
    "our goal is to compress the source sequence @xmath20 , knowing only @xmath2 , @xmath5 , and @xmath21 .",
    "in particular , the source alphabet @xmath0 and the parsing into the source blocks @xmath37 are _ not _ known .",
    "more formally , we are looking for a prefix - free source code for the random variable @xmath20 .",
    "we have a complete statistical description of @xmath20 and ( since @xmath4 is finite ) its entropy @xmath38 is well defined . the expected rate @xmath39 of the optimal prefix - free source code for @xmath20",
    "is thus bounded as @xmath40    [ eg : setting2 ] we illustrate the order and relative size of the various quantities in the problem setting using data from a recent large - scale primary ( i.e. , non backup ) data deduplication study  @xcite .",
    "the length @xmath26 of the source sequence ranges from several hundreds of to a few .",
    "the expected value of the length @xmath41 of the source symbols is a bit harder to quantify ( since the notion of source symbol itself is a model abstraction ) , but the experiments in  @xcite suggest that reasonable numbers range from a few to a few .",
    "the number @xmath21 of source blocks is consequently on the order of perhaps @xmath42 to @xmath43 . as in our model ,",
    "the data in  @xcite indicates that duplicates are not localized , but occur over the entire source string . in other words ,",
    "the source has long - range dependence .",
    "the number of distinct source symbols @xmath5 is again hard to quantify , but the results in  @xcite suggest that , depending on the scenario , @xmath5 should be somewhere in the range @xmath44 to @xmath21 .",
    "there are several differences between the source model introduced here and the standard source coding setup .",
    "first , the source considered here is nonergodic in the following sense . as @xmath45",
    ", we obtain a stochastic process",
    "@xmath46 with each element @xmath47 taking values in @xmath48 .",
    "this process is not ergodic . indeed , let @xmath49 so that the source alphabet @xmath0 contains a single symbol .",
    "is made here solely for ease of exposition  the same argument works also under the assumption throughout the remainder of the paper that @xmath50 . ]",
    "then , with nonzero probability , that single source symbol is the all - zero sequence , i.e. , @xmath51 , in which case @xmath52 is the infinite sequence of all zeros . since @xmath53",
    ", this shows that the process is not ergodic .",
    "second , the standard setting is to consider the source statistics , captured by @xmath4 and @xmath5 , as fixed and to let the length of the source , captured by @xmath21 , go to infinity . instead , as indicated by example  [ eg : setting2 ] , we are interested here in the regime when @xmath21 may be of similar order as @xmath5 .",
    "thus , we here allow the source parameters @xmath4 and @xmath5 to grow with @xmath21 .",
    "finally , given the size of the problem and in particular the long range over which the source exhibits memory ( see again example  [ eg : setting2 ] ) , we are interested in compression schemes that scale well . in the next section",
    "we describe three such schemes .",
    "we next provide a formal ( somewhat stylized ) description of the deduplication approach .",
    "there are two types of deduplication schemes that appear frequently in the literature , fixed - length and variable - length , which are presented first .",
    "then , we introduce a novel variant of the deduplication approach , termed multi - chunk deduplication .    for fixed - length deduplication",
    ", we fix a chunk length @xmath54 .",
    "the source sequence @xmath20 is parsed into substrings of length @xmath54 ( except for the final substring that may have length less than @xmath54 ) .",
    "let @xmath55 be this fixed - length parsing of @xmath20 with @xmath56 .",
    "each @xmath57 ( with @xmath58 ) is an element of @xmath59 referred to as a deduplication chunk .",
    "the encoding algorithm starts by describing the length @xmath26 of the source string using a prefix - free code for the positive integers ( such as an elias code  @xcite ) .",
    "the encoding algorithm then traverses through the chunks , starting at @xmath60 , and constructs a growing dictionary of chunks seen up to @xmath61 .",
    "chunk @xmath61 is encoded either as a new dictionary entry or as a pointer into the dictionary at that point ( depending on whether the chunk is new or already in the dictionary ) .",
    "if chunk @xmath61 is new , then it is encoded as the bit @xmath34 followed by the binary string @xmath57 itself .",
    "if chunk @xmath61 is not new , then it is encoded as the bit @xmath62 followed by a pointer into the dictionary .",
    "this fixed - length deduplication scheme is prefix free .",
    "its expected ( with respect to @xmath20 ) number of encoded bits is denoted by @xmath63 .",
    "[ eg : setting_fixed ] continuing with example  [ eg : setting ] , for @xmath64 and with @xmath65 , the fixed - length chunks are @xmath66 , @xmath67 , @xmath68 , @xmath69 .",
    "when the encoding process terminates , the chunk dictionary contains the elements @xmath70 .    without the initial encoding of @xmath26 ,",
    "the source code is still nonsingular",
    "( i.e. , no two different @xmath20 have the same codeword ) , and can hence be decoded . however , because of the variable - length nature of the source , the code may no longer be prefix free .",
    "the initial encoding of @xmath26 is therefore necessary to guarantee that the source code is prefix free .    for variable - length deduplication ,",
    "we fix a marker sequence , which we take here to be the all - zero sequence of length @xmath71 denoted by @xmath72 .",
    "the source sequence @xmath20 is then split into a random number @xmath73 of chunks using this marker .",
    "more formally , the source sequence is parsed as @xmath74 , where each chunk @xmath57 ( except for perhaps the last one ) contains a single appearance of the sequence @xmath72 at the end .",
    "the encoding algorithm again starts by describing the length @xmath26 of the source string using a prefix - free code for the positive integers .",
    "the parsing itself is also performed using a growing dictionary of chunks , similar to the fixed - length scheme .",
    "if chunk @xmath61 is new ( meaning not yet in the dictionary ) , it is encoded as the bit @xmath34 followed by the binary string @xmath57 itself .",
    "since the marker sequence @xmath72 indicates the end of @xmath57 , we do not need to store the length @xmath75 explicitly .",
    "if chunk @xmath61 is not new , then it is encoded , as before , as the bit @xmath62 followed by a pointer into the dictionary .",
    "this variable - length deduplication scheme is also prefix free .",
    "its expected ( with respect to @xmath20 ) number of encoded bits is denoted by @xmath76 .",
    "continuing again with example  [ eg : setting ] , for @xmath64 and with @xmath77 , the variable - length chunks are @xmath78 , @xmath79 .",
    "when the encoding process terminates , the chunk dictionary contains the elements @xmath80 .    for a more realistic example , consider again the setting for the primary data deduplication study  @xcite from example  [ eg : setting2 ] .",
    "the system uses variable - length chunking with expected chunk lengths ranging from to .",
    "the corresponding marker sequence has length ranging from @xmath81 to @xmath82 bits .",
    "@xcite finds that around of chunks appear only once , and that the vast majority of chunks have less than @xmath83 duplicates .",
    "it is worth pointing out that the number of duplicates may be higher in backup or archival scenarios , where deduplication ratios of @xmath84 to @xmath34 or higher can be achieved  @xcite .",
    "we finally describe the novel , multi - chunk deduplication algorithm .",
    "we again split the source sequence into a random number @xmath73 of chunks using the marker @xmath72 , however , this time we ensure that each chunk has length at least @xmath85 .",
    "more formally , the source sequence is parsed as @xmath74 , where each chunk @xmath57 ( except for perhaps the last one ) is the shortest string of length at least @xmath85 ending in @xmath72 .",
    "[ eg : setting_mult ] multi - chunk deduplication with marker length @xmath86 parses the source sequence @xmath87 into the chunks @xmath88 , @xmath89 , @xmath68 .    the encoding algorithm again describes the length @xmath26 of the source string using a prefix - free code for the positive integers , followed by a parsing of @xmath20 using a growing dictionary of chunks .",
    "consider the encoding of chunk @xmath61 .",
    "assume first that it is new , and consider the sequence @xmath90 of chunks .",
    "let @xmath91 be the largest integer such that @xmath92 are all new .",
    "these new chunks are then encoded together as the bit @xmath34 , followed by an encoding of @xmath91 using a prefix - free code for the positive integers , followed by the binary string @xmath93 . since each chunk @xmath94",
    "is terminated by the occurrence of the marker sequence @xmath72 after position @xmath95 , we do not need to store their lengths explicitly .",
    "the encoding process continues with chunk @xmath96 .",
    "assume next that chunk @xmath61 is not new , and consider the sequence @xmath90 of chunks .",
    "let @xmath97 be the smallest index satisfying @xmath98 .",
    "such an index @xmath99 exists since chunk @xmath61 is not new ; in fact @xmath99 corresponds to the first time chunk @xmath57 was seen and hence entered into the dictionary . consider the corresponding dictionary entry and the list of subsequent entries .",
    "let @xmath100 be the largest integer such that the first @xmath100 entries in this list are equal to @xmath101 , @xmath102 , ",
    ", @xmath103 .",
    "then the chunks @xmath61 through @xmath104 are encoded together as the bit @xmath62 , followed by an encoding of @xmath100 using a prefix - free code for the positive integers , followed by a pointer into the dictionary pointing to chunk @xmath105 .",
    "observe that the pointer is to an individual chunk , even if that chunk was part of a larger group of chunks encoded jointly .",
    "the encoding process continues with chunk @xmath106 .",
    "this multi - chunk deduplication scheme is also prefix free .",
    "its expected ( with respect to @xmath20 ) number of encoded bits is denoted by @xmath107 .",
    "the standard performance criterion is the redundancy normalized by the expected length of the source string , e.g. , @xmath108 .",
    "however , since in our setting @xmath39 itself may be @xmath109 , this normalized redundancy may not be a meaningful quantity .",
    "we therefore instead adopt the ratio @xmath110 ( and similar for @xmath76 , @xmath107 ) as our performance metric .",
    "a deduplication scheme with rate @xmath111 is _ order optimal _ if @xmath112 as @xmath45 .",
    "it is _ asymptotically optimal _ if @xmath113 as @xmath45 .    as indicated in section  [ sec : setting_source ] , while the standard approach is to fix the source alphabet parameters , i.e. , @xmath4 and @xmath5 , and to consider the asymptotic behavior as the source length ( as measured by @xmath21 ) increases , we are here instead interested in the behavior as the source alphabet parameters increase together with the source length @xmath21 .",
    "the main result of this paper is an information - theoretic analysis of data deduplication for the source model defined in section  [ sec : setting_source ] .",
    "we start with the fixed - length deduplication scheme as described in section  [ sec : setting_schemes ] .",
    "the first result analyzes the performance of this scheme assuming a constant source - symbol length , i.e. , @xmath114 .",
    "[ thm : fixed ] consider the source model with @xmath21 source blocks drawn with replacement from the @xmath5 source symbols of constant length @xmath115 . the performance of the fixed - length deduplication scheme with chunk length @xmath116 satisfies then @xmath117 for @xmath21 large enough .",
    "the proof of theorem  [ thm : fixed ] is reported in appendix  [ sec : proofs_fixed ] .",
    "the most interesting regime is when the number of source blocks @xmath21 is at least as large as the number of source symbols @xmath5 , in which case the upper bound in theorem  [ thm : fixed ] can be simplified to @xmath118 thus , as long as @xmath119 for some @xmath120 as @xmath45 ( which implies that @xmath121 as @xmath45 by assumption ) , we have that @xmath122 as @xmath21 grows , showing the asymptotic optimality of fixed - length deduplication with known and constant source - symbol lengths    [ eg : running ] motivated by example  [ eg : setting2 ] , consider the source with @xmath123 symbols of fixed length @xmath124 bits and with @xmath125 source blocks .",
    "theorem  [ thm : fixed ] shows then that fixed - length deduplication with chunk length @xmath116 has performance satisfying @xmath126 i.e. , very close to optimal .",
    "unfortunately , the asymptotic optimality of fixed - length deduplication relies crucially on the assumption of known and fixed source - symbol length . while this assumption may be reasonable in some scenarios ( such as for virtual machine disk image deduplication @xcite ) , it is usually not valid .",
    "as soon as this assumption is relaxed , fixed - length deduplication can be substantially suboptimal , as the next example shows .",
    "[ eg : sync ] consider the scenario with @xmath127 source symbols .",
    "set the number of source blocks to @xmath128 . to start with ,",
    "assume the source - symbol length is constant , @xmath129 . by theorem  [ thm : fixed ] ,",
    "fixed - length deduplication with chunk length @xmath116 is then within a constant factor of optimal as @xmath45 .    on the other hand ,",
    "assume next that the symbol - length distribution @xmath2 assigns equal mass to the values @xmath115 and @xmath130 .",
    "appendix  [ sec : proofs_sync ] then shows that fixed - length deduplication with chunk length @xmath116 has rate satisfying @xmath131 as @xmath45 . in other words ,",
    "even with only two source symbols , the fixed - length deduplication scheme can be substantially suboptimal .",
    "the reason for the bad performance of fixed - length deduplication is that the source blocks and the deduplication chunks are not properly synchronized .",
    "initially , the deduplication chunks are aligned with the source blocks . however , whenever a source block of length @xmath130 is observed , the deduplication chunks shift by one bit with respect to the source blocks . over time",
    ", the boundary between deduplication chunks takes on all @xmath115 possible offsets with respect to the source block boundaries . due to",
    "these @xmath115 possible starting points",
    ", the fixed - length deduplication scheme encounters @xmath132 distinct chunks instead of the only @xmath127 distinct source symbols , resulting in the factor @xmath133 overhead compared to the optimal scheme .",
    "this argument is made precise in appendix  [ sec : proofs_sync ] .",
    "from example  [ eg : sync ] , we see that the ( general ) deduplication problem is fundamentally one of synchronizing deduplication chunks with source blocks .",
    "the variable - length deduplication scheme , described in section  [ sec : setting_schemes ] , utilizes marker sequences to achieve this synchronization .",
    "the next theorem bounds its performance .",
    "[ thm : variable ] consider the source model with @xmath21 source blocks drawn with replacement from the @xmath5 source symbols of expected length @xmath4 .",
    "the performance of the variable - length deduplication scheme with optimized marker length @xmath71 satisfies then @xmath134 for @xmath21 large enough .",
    "the proof of theorem  [ thm : variable ] is reported in appendix  [ sec : proofs_variable ] .",
    "we illustrate this result with two examples .",
    "consider again the scenario with @xmath135 source symbols and with @xmath125 source blocks .",
    "this time , the source symbols are not of constant length , but have the same expected length @xmath136 bits as before .",
    "theorem  [ thm : variable ] shows then that variable - length deduplication has performance satisfying @xmath137 i.e. , is within a factor @xmath138 of optimal . by numerically optimizing the value of the marker length @xmath71",
    ", this factor can be further reduced to @xmath139 .",
    "consider again the scenario with @xmath127 source symbols with symbol - length distribution @xmath2 assigning equal mass to the values @xmath115 and @xmath130 , and with @xmath128 source blocks .",
    "recall that the fixed - length deduplication scheme had a rate at least order @xmath21 times larger than the optimal scheme : @xmath140    on the other hand , by tightening the arguments in the proof of theorem  [ thm : variable ] for the case @xmath141 ( see appendix  [ sec : proofs_sync2 ] ) , the rate of the variable - length deduplication scheme satisfies @xmath142 thus , variable - length deduplication is only suboptimal by at most a polylogarithmic as opposed to a linear factor .",
    "the analysis in the proof of theorem  [ thm : variable ] in appendix  [ sec : proofs_variable ] indicates that the optimal choice of the marker length @xmath71 , governing the expected chunk length of the variable - length deduplication scheme , balances two competing requirements . on the one hand , for each already encountered chunk , we need to encode a pointer into the dictionary .",
    "a smaller chunk length increases both the number of chunks that need to be encoded and the size of the pointers . on the other hand , the chunks covering the boundaries of two source blocks",
    "will usually not be contained in the dictionary ( since there are @xmath143 such possible boundaries for @xmath5 source symbols ) , and will have to be encoded directly .",
    "hence , a larger chunk length increases the amount of bits contained in inefficiently encodable boundary chunks . the choice of marker length @xmath144 in the proof of theorem  [ thm : variable ] splits each source block into an average of about @xmath145 chunks of expected length about @xmath145 , which balances these two detrimental effects .    unfortunately , even with this optimal choice of marker length , using deduplication chunks that have lengths of different order than the source blocks can lead to asymptotically significantly suboptimal performance .",
    "this is demonstrated with the next example .",
    "[ eg : duplicate ] consider the scenario with @xmath146 source symbols of constant length @xmath147 . from the preceding discussion , we see that this is the worst - case situation for variable - length deduplication , in which we can expect to see all possible @xmath143 different boundary chunks .",
    "a slightly tightened version of theorem  [ thm : variable ] ( which omits the last term in the denominator using that @xmath17 is constant ) together with a lower bound derived in appendix  [ sec : proofs_duplicate ] , show that then @xmath148 as @xmath45 .",
    "this has two implications : first , that theorem  [ thm : variable ] is tight to within a polylogarithmic factor in @xmath21 for this setting ; and second that variable - length deduplication can still be polynomially suboptimal .",
    "the multi - chunk deduplication scheme proposed in this paper circumvents this trade - off by encoding multiple chunks jointly .",
    "this allows to choose the expected chunk length to be quite small , thereby limiting the effect of the boundary chunks , without the penalty of increased number of dictionary pointers .",
    "the next theorem bounds the performance of this scheme .",
    "[ thm : multi ] consider the source model with @xmath21 source blocks drawn with replacement from the @xmath5 source symbols of expected length @xmath4 .",
    "the performance of the multi - chunk deduplication scheme with optimized marker length @xmath71 satisfies then @xmath149 as @xmath45 .",
    "the proof of theorem  [ thm : multi ] is reported in appendix  [ sec : proofs_multi ] .",
    "the theorem shows that , under the fairly mild conditions @xmath150 and @xmath151 for some constant @xmath152 , multi - chunk deduplication is within a constant factor of optimal as @xmath45 .",
    "further , if @xmath153 , then multi - chunk deduplication is asymptotically optimal as @xmath45 .",
    "consider again the scenario with @xmath135 source symbols and with @xmath154 source blocks with expected length @xmath136 bits .",
    "theorem  [ thm : multi ] ( using the explicit constant in the order notation from appendix  [ sec : proofs_multi ] ) shows then that multi - chunk deduplication has performance satisfying @xmath155 by numerically optimizing the marker length @xmath71 , this factor can be further reduced to @xmath156 .",
    "thus , the proposed multi - chunk deduplication scheme is quite close to optimal in this setting .",
    "consider again the scenario with @xmath146 source symbols of constant length @xmath147 . recall that the variable - length deduplication scheme had a rate at least polynomially suboptimal : @xmath157    on the other hand , a slightly tightened version of theorem  [ thm : multi ] , which omits the last term in the denominator using that @xmath17 is constant , shows that the rate of the multi - chunk deduplication scheme satisfies @xmath158 as @xmath45 .",
    "thus , multi - chunk deduplication is order optimal in this case , as opposed to the polynomial loss factor of variable - length deduplication .",
    "this appendix analyzes the rate of fixed - length deduplication for constant source - symbol length .",
    "the length @xmath26 of the source is in this case the constant @xmath159 .",
    "set the deduplication chunk length @xmath54 to be equal to the fixed length @xmath115 of the source blocks .",
    "the deduplication chunks @xmath160 are then equal to the source blocks @xmath161 .",
    "we start with an upper bound on the rate @xmath63 of the fixed - length deduplication scheme .",
    "the initial encoding of the length @xmath162 using a universal code for the integers takes at most @xmath163 bits ( see , e.g. , ( * ? ? ?",
    "* lemma  13.5.1 ) ) .",
    "consider then the encoding of some chunk @xmath61 .",
    "the flag indicating if the chunk is already in the dictionary takes one bit .",
    "if the chunk is new , then it is added to the dictionary using @xmath54 bits .",
    "if the chunk is already in the dictionary , then a pointer into the dictionary is encoded .",
    "let @xmath164 be the dictionary when processing chunk @xmath61 .",
    "then this encoded pointer takes at most @xmath165 bits .",
    "the expected rate of fixed - length deduplication is thus upper bounded by @xmath166 where @xmath167 denotes the indicator function , and where we have used that @xmath168 using that @xmath169 , @xmath116 , and @xmath170 , this upper bound can be rewritten as @xmath171 where @xmath172 denotes the set of all _ distinct _ source blocks seen up to block @xmath173 .",
    "we continue with a lower bound on the rate @xmath39 of the optimal code .",
    "since the code is prefix free , its rate is lower bounded as @xmath174 ( see , e.g. , ( * ? ? ?",
    "* theorem  5.4.1 ) ) .",
    "as the source blocks are of constant length , we have @xmath175 each term in the sum on the right - hand side satisfies @xmath176 conditioned on @xmath177 and @xmath178 , the source block @xmath23 is uniformly distributed over @xmath179 . hence , @xmath180 using that @xmath181 for @xmath182 and that @xmath183 by assumption for @xmath184 .",
    "this implies that @xmath185 conditioned on @xmath186 and @xmath178 , the source block @xmath23 is uniformly distributed over @xmath187 .",
    "hence , @xmath188 combining  ",
    "yields @xmath189    to obtain a more explicit expression , we further lower bound @xmath39 as @xmath190 where @xmath182 follows from @xmath191 and from the convexity of @xmath192 and jensen s inequality , @xmath184 follows from @xmath193 since the @xmath178 are chosen uniformly with replacement from the set @xmath0 of cardinality @xmath5 , and @xmath194 follows from @xmath195 and from @xmath196 for @xmath197 .    from   and  , we obtain @xmath198 combining this with   yields @xmath199 for @xmath21 large enough , this can be simplified as @xmath200 concluding the proof .",
    "this appendix contains the formal analysis for example  [ eg : sync ] .",
    "we start with an upper bound on @xmath39 .",
    "since @xmath39 is the rate of the optimal prefix - free code , it is upper bounded as @xmath201 ( see , e.g. , ( * ? ? ? * theorem  5.4.1 ) ) .",
    "now , @xmath202 thus , @xmath203     +    we continue with a lower bound on the rate of fixed - length deduplication .",
    "we set the chunk length @xmath54 to be equal to @xmath115 .",
    "since the source blocks have lengths either @xmath115 or @xmath130 , the deduplication chunk boundaries may no longer be aligned with the source block boundaries .",
    "define the offset of the current chunk as the distance from that chunk end to the start of the next source block ( see fig .",
    "[ fig : offset_a ] ) .",
    "assume for the moment that the source alphabet @xmath0 has one source symbol of length @xmath115 and one of length @xmath130 ( this happens with probability @xmath204 ) .",
    "the evolution of this offset is then governed by a markov chain with @xmath130 states as depicted in fig .  [",
    "fig : offset_b ] .",
    "the initial state is @xmath62 and all outgoing edges of a state have uniform probability . observe that in each state we make a transition to the right ( modulo @xmath130 ) with probability at least @xmath204 or stay in the current state with probability at most @xmath204 .",
    "hence , after @xmath205 transitions , we have traversed the chain at least once in expectation .",
    "consider now the two source symbols @xmath206 and @xmath207 ( recall that @xmath127 ) . and assume for the moment that @xmath208 . by the law of large numbers , about @xmath209 of the deduplication chunks",
    "will be a subset of @xmath210 ( the concatenation of @xmath206 with itself ) with high probability for @xmath115 large enough .",
    "consider all possible chunks of length @xmath115 starting with different offsets in @xmath210 .",
    "we next argue that with high probability all these @xmath115 different chunks are unique .    by  (",
    "* example  10.5 ) , there are @xmath211 binary sequences of length @xmath115 for which all circular shifts are distinct ( these are called aperiodic necklaces in combinatorics ) , where @xmath212 is the mbius function . since @xmath213 and @xmath214 , we can lower bound this as @xmath215 this shows that , as @xmath216 , the vast majority of binary sequences have the property that all their circular shifts are distinct .",
    "in particular , with high probability @xmath206 will have this property .",
    "putting these arguments together , we obtain the following . with probability",
    "@xmath204 the two source symbols have distinct lengths .",
    "with probability at least @xmath204 , the shorter of the two source symbols ( the one with length @xmath115 ) will have distinct circular shifts for @xmath115 large enough .",
    "if @xmath128 and @xmath115 large enough , then we will see every possible deduplication chunk offset at least once with probability at least @xmath204 .",
    "moreover , with probability @xmath204 at least @xmath217 of the deduplication chunks will contain circular shifts of the shorter source symbol .",
    "since each of these are distinct , they will all have to be entered into the dictionary , using @xmath115 bits each .",
    "thus , @xmath218    combining   ( with @xmath127 and @xmath219 ) and shows that @xmath220 thus , even with only two source symbols , the fixed - length deduplication scheme can be substantially suboptimal .",
    "we start with an upper bound on the rate @xmath76 of variable - length deduplication .",
    "the initial encoding of the length @xmath26 using a universal code for the integers takes at most @xmath221 bits ( see again ( * ? ? ?",
    "* lemma  13.5.1 ) ) .",
    "consider then the encoding of chunk @xmath61 .",
    "the flag indicating if the chunk is already in the dictionary takes one bit .",
    "if the chunk is new , then it is added to the dictionary using @xmath75 bits .",
    "if the chunk is already in the dictionary , then a pointer into the dictionary is encoded .",
    "let again @xmath164 be the dictionary when encoding chunk @xmath61 .",
    "then this encoded pointer takes at most @xmath165 bits .",
    "let @xmath222 be the rate of variable - length deduplication for a particular source string @xmath20 , so that @xmath223 .",
    "the rate @xmath222 is then upper bounded by @xmath224    now , we have two distinct parsings of the source sequence @xmath20 .",
    "the first is defined by the source blocks , @xmath37 ; the second is defined by the deduplication chunks @xmath225 .",
    "we would like to relate these two parsings .",
    "to this end , let @xmath226 denote the indices of those chunks from @xmath20 starting in @xmath23 .",
    "we say that a chunk @xmath57 is in @xmath23 if @xmath227 .",
    "[ eg : markers ] consider @xmath228 , @xmath229 , @xmath230 so that the source string is @xmath231 .",
    "the parsing of @xmath20 into chunks with marker @xmath232 yields @xmath233 , @xmath234 , @xmath235 , @xmath236 , @xmath237 , @xmath238 .",
    "this situation is depicted in fig .",
    "[ fig : markers ] . in this setting @xmath239 , @xmath240 , and @xmath241 .",
    "consider the chunks in source block @xmath23 . as fig .",
    "[ fig : markers ] shows , some of them depend on the values of the neighboring source blocks @xmath242 and @xmath243 .",
    "we call these the `` boundary '' chunks of @xmath23 and denote their indices by @xmath244 .",
    "other chunks in @xmath23 are the same irrespective of the values of the neighboring source blocks .",
    "we call these the `` interior '' chunks of @xmath23 and denote their indices by @xmath245 . formally , for a source block @xmath23",
    ", we say that @xmath57 with @xmath227 is an interior chunk if it appears in the variable - length chunking of every sequence @xmath246 with @xmath247 .",
    "any chunk @xmath57 that is not an interior chunk is defined to be a boundary chunk . by definition ,",
    "the interior chunks thus have the property that if @xmath248 , then @xmath249 .",
    "note that this last conclusion does generally not hold for @xmath244 and @xmath250 .    in this",
    "setting we have @xmath251 , @xmath252 , @xmath253 and @xmath254 , @xmath255 , @xmath256 .",
    "+   +   +   +    usually , @xmath244 contains only one chunk index , which corresponds to the final chunk starting in @xmath23 but ending in @xmath243 ( see figs .  [",
    "fig : boundary0 ] and  [ fig : boundary1 ] ) .",
    "however , @xmath244 can contain additional chunk indices .",
    "in particular , if the boundary between @xmath242 and @xmath257 forms a marker sequence then the first chunk starting in @xmath23 may also be a boundary chunk ( see figs .",
    "[ fig : boundary1][fig : boundary4 ] ) .",
    "finally , if @xmath23 starts with between @xmath258 and @xmath259 zeros ( where @xmath71 is the marker length ) , then it may contain a third boundary chunk consisting of the marker sequence @xmath72 by itself ( see fig .",
    "[ fig : boundary3 ] ) . observe that when @xmath23 starts with @xmath260 or more zeros , then there is always a @xmath72 chunk , irrespective of the value of @xmath242 , and therefore @xmath72 is not a boundary chunk in this case ( see fig .",
    "[ fig : boundary4 ] ) . in general , we thus have @xmath261",
    ". we will later choose @xmath71 such that @xmath262 as @xmath263 , in which case the vast majority of indices in @xmath226 will correspond to interior chunks .",
    "let us return to the upper bound   for @xmath222 , and consider the first sum corresponding to new chunks .",
    "we can now rewrite this sum as @xmath264 as before , denote by @xmath265 the set of all distinct source blocks seen up to block @xmath173 as defined in   in appendix  [ sec : proofs_fixed ] . note that if @xmath266 for _ any _ @xmath267 , then @xmath177 .",
    "hence @xmath268 where we have used that @xmath269 substituting this into   yields @xmath270    consider then the second sum in   corresponding to old chunks .",
    "we can upper bound this sum as @xmath271    substituting   and   into and simplifying the resulting expression yields @xmath272 taking expectations on both sides results in an upper bound on @xmath76 : @xmath273 we now upper bound each of these expectations in turn .",
    "the first expectation in   is upper bounded as @xmath274 using jensen s inequality .    for the second expectation in  ,",
    "observe that the number of chunks starting in source block @xmath23 is at most @xmath34 plus the number of times the marker @xmath72 appears in @xmath23 alone ( see again fig .  [",
    "fig : markers ] ) .",
    "since the expectation of that latter number is upper bounded by @xmath275 , we obtain @xmath276    the third expectation in   is equal to @xmath277 where we have used the independence of @xmath278 and @xmath279 .",
    "consider next the fourth expectation @xmath280 in  .",
    "consider the boundary chunks arising at the boundary between @xmath23 and @xmath243 ( see again fig .",
    "[ fig : boundary ] ) .",
    "the total number of bits due to those chunks is upper bounded by the sum of two terms : the number of bits from the end of @xmath23 backwards until the end of the first ( counting backwards ) occurrence of @xmath72 ( or @xmath279 if no such match exists in @xmath23 ) .",
    "plus the number of bits from the start of @xmath243 forwards until the end of the first ( counting forwards ) occurrence of @xmath281 ( or @xmath282 if no such match exists in @xmath243 ) .",
    "note that a source symbol @xmath10 is ( by itself ) a @xmath283 process of random length .",
    "the expected value of the end of the first occurrence of @xmath281 in an infinite - length @xmath283 process is @xmath284 by ( * ? ? ?",
    "* theorem  8.3 ) .",
    "the expected value of the end of the first occurrence of @xmath72 in an infinite - length @xmath283 process is @xmath285 by ( * ? ? ?",
    "* theorem  8.2 ) .",
    "hence @xmath286 in this bound , the event that some source blocks may not contain a marker sequence is captured by the event that the match of the marker sequence in the infinite - length @xmath283 process is beyond the length @xmath279 of the corresponding source block .",
    "consider then the last expectation @xmath287 in  .",
    "we have @xmath288 using this , we can upper bound @xmath289 where we have used  .    substituting   into results in @xmath290",
    "we next derive a lower bound on @xmath39 .",
    "as before , we have @xmath291 now , @xmath292 the term @xmath293 can be bounded as @xmath294 using @xmath295 .",
    "the term @xmath296 can be bounded similarly to in appendix  [ sec : proofs_fixed ] as @xmath297 with @xmath298    conditioned on @xmath177 , @xmath178 , and @xmath299 , the source block @xmath23 is uniformly distributed over @xmath300 .",
    "hence , @xmath301 where we have used the independence of @xmath299 and the event @xmath177 . using the assumption that @xmath302 , this last expression can be further lower bounded as @xmath303 so that @xmath304 furthermore , by the same arguments as in   in appendix  [ sec : proofs_fixed ] , @xmath305 where the last line follows from jensen s inequality .    combining  ",
    "yields @xmath306 to obtain a more explicit expression , we can further lower bound @xmath39 as @xmath307 similar to   in appendix  [ sec : proofs_fixed ] .    from   and  , we obtain @xmath308 the two dominant terms in this last expression behave ( to first order ) like @xmath309 and @xmath310 .",
    "hence , the right - hand side of   is approximately minimized by choosing the marker length as @xmath311 this splits each source block into an average of about @xmath145 chunks of expected length about @xmath145 . with this choice of @xmath71 ,",
    "yields @xmath312 where the last inequality holds for @xmath21 large enough . combining this with   yields @xmath313 again for @xmath21 large enough .",
    "this proves the theorem .",
    "the bound   in appendix  [ sec : proofs_variable ] is appropriate when @xmath314 .",
    "when @xmath141 , it can be quite loose , since each pair @xmath315 yields at most three distinct boundary deduplication chunks .",
    "we next derive a tighter bound for the regime @xmath141 .",
    "define the event @xmath316 that at least one source symbol @xmath317 does not contain the substring @xmath281 . then , since each pair @xmath315 yields at most three distinct boundary deduplication chunks , we have on the complement of @xmath316 that @xmath318 where @xmath319 is the string starting from the beginning of @xmath10 up to and including the first occurrence of @xmath281 , and where @xmath320 is the string from the end of @xmath10 backwards until the end of the first ( counting backwards ) occurrence of @xmath72 ( see fig .  [",
    "fig : boundary ] in appendix  [ sec : proofs_variable ] ) .",
    "on @xmath316 , we have @xmath321 combining these last two inequalities yields @xmath322    we have @xmath323 where we have again used ( * ? ? ?",
    "* theorems  8.2  and  8.3 ) .",
    "it remains to analyze @xmath324 . since @xmath325 by assumption , the probability of the event @xmath316 is upper bounded by that of the event that from @xmath5 sequences drawn uniformly at random without replacement from @xmath326 at least one of them contains no occurrence of @xmath281 .",
    "the probability of this last event is upper bounded as @xmath327 hence , @xmath328    substituting   , , and   into in appendix  [ sec : proofs_variable ] yields @xmath329 combined with  , this shows that @xmath330    for the remainder of the argument , we specialize to the setting in example  [ eg : sync ] , namely @xmath127 , @xmath219 , @xmath331 . with this , becomes @xmath332 set the marker length to @xmath333 which results in @xmath334    now , since there are only @xmath127 source symbols of length @xmath115 or @xmath130 , we can with high probability uniquely identify the source blocks @xmath22 from @xmath20 for @xmath115 large enough .",
    "hence , each source block @xmath23 adds asymptotically one bit of information to @xmath20 , i.e. , @xmath335 as @xmath216 .",
    "combining this with   shows that @xmath336 as claimed .",
    "recall that @xmath337 and @xmath147 . following the same steps as those leading to   in appendix  [ sec : proofs_sync ]",
    ", we obtain the upper bound @xmath338 for the rate @xmath39 of the optimal source code .",
    "we continue with a lower bound on the rate @xmath76 of variable - length deduplication . for each chunk @xmath61",
    ", the flag indicating if the chunk is already in the dictionary takes one bit , resulting in a total of @xmath339 bits .",
    "further , each unique boundary chunk needs to be stored in the dictionary .",
    "we next argue that we will see on the order of @xmath340 unique boundary chunks that each have length on the order of @xmath341 .",
    "this will imply that storing the unique boundary chunks takes on the order of @xmath342 bits .     and @xmath343 .",
    "compare to fig .",
    "[ fig : markers ] in  appendix  [ sec : proofs_variable ] . ]",
    "consider the two concatenations of source symbols @xmath344 and @xmath343 , and consider the two resulting boundary deduplication chunks ( see fig .",
    "[ fig : duplicate ] ) .",
    "we can decompose the two boundary chunks as the concatenation @xmath345 and @xmath346 , where @xmath347 and @xmath348 denote the substring of the source symbol contributing to the boundary chunk ( excluding the marker sequence , and truncated to length @xmath349 in case there is no marker sequence before then ) . from fig .",
    "[ fig : duplicate ] we see that if @xmath350 , then one of @xmath351 , @xmath352 is a substring of the other , and one of @xmath353 , @xmath354 is a substring of the other .",
    "consider a source symbol @xmath10 , and assume @xmath355 for now . with probability at least @xmath356 it has @xmath319 of length at least @xmath357 containing a least one symbol @xmath34 in the first @xmath358 bits and it has @xmath320 of length at least @xmath357 and containing a least one symbol @xmath34 in the last @xmath358 bits .",
    "a short calculation shows that this implies that with probability at least @xmath359 , the source alphabet @xmath0 has at least @xmath360 symbols with this property .",
    "moreover , with probability at least @xmath361 the source alphabet @xmath0 has no repeating , nonoverlapping substrings of size @xmath362 .",
    "this argument is reported with more detail in appendix  [ sec : proofs_multi ] .",
    "in particular , if @xmath363 then with probability at least @xmath359 the source alphabet @xmath0 has no repeating , nonoverlapping substrings of size @xmath357 .    combining the two arguments shows that with probability at least @xmath204 there are at least @xmath360 source symbols with both long , duplicate - free heads and tails .",
    "further , each of these heads contains a symbol one within the first @xmath358 bits , and each of these tails contains a symbol one within its first @xmath358 bits .",
    "if this event holds , then @xmath364 of all @xmath143 possible concatenations @xmath315 produce unique boundary chunks of length at least @xmath365 .    finally , since @xmath366 , we will see at least @xmath204 of these possible unique boundary chunks in expectation . therefore , the expected number of bits needed to store just the boundary chunks is at least @xmath367 , assuming   is satisfied .    combining these arguments ,",
    "the rate @xmath76 of variable - length deduplication is lower bounded as @xmath368 the expected number of chunks @xmath339 is lower bounded by @xmath369 , so that @xmath370 this lower bound is minimized by @xmath371 which results in the bound @xmath372 as @xmath45 .    combining   and   yields @xmath373 as @xmath45 .",
    "we start with an upper bound on the rate @xmath107 of multi - chunk deduplication .",
    "the initial encoding of the length @xmath26 using a universal code for the integers takes again at most @xmath221 bits by ( * ? ? ?",
    "* lemma  13.5.1 ) .",
    "consider then the encoding of chunk @xmath61 .",
    "the flag indicating if the chunk is already in the dictionary takes one bit .",
    "if the chunk is new , then @xmath91 is encoded using at most @xmath374 bits , plus the @xmath91 chunks starting at @xmath61 are added to the dictionary using @xmath375 bits . if the chunk is already in the dictionary , then @xmath100 is encoded using at most @xmath376 bits , plus a pointer into the dictionary using at most @xmath165 bits , where @xmath164 is again the dictionary when encoding chunk @xmath61 .",
    "the next chunk to be encoded is either @xmath96 or @xmath106 .    .",
    "the vertical line @xmath377 indicates the boundary between the source blocks @xmath242 and @xmath23 . ]    as before , we denote by @xmath226 those chunks starting in source block @xmath23 .",
    "we again define the notion of boundary chunk indices @xmath244 and interior chunk indices @xmath245 ( see appendix  [ sec : proofs_variable ] ) , but this time with respect to the multi - chunk deduplication scheme , as shown in fig .",
    "[ fig : boundary_mult ] .",
    "let @xmath316 be the event that there is at least one interior chunk of @xmath378 that is either equal to another interior chunk in the source alphabet or to a boundary chunk of @xmath20 .",
    "assume we are on the complement of @xmath316 for now , and consider the first interior chunk @xmath57 in @xmath23 ( i.e. , the smallest @xmath267 ) .",
    "then there are two possibilities , either @xmath177 and all indices @xmath245 correspond to new chunks , or @xmath186 and all indices @xmath245 correspond to chunks already in the dictionary .    in the former case ( @xmath177 ) , the number @xmath91 is larger than or equal to @xmath379 .",
    "the encoding of the chunks with indices in @xmath245 takes thus at most @xmath380 bits . since reducing the number of jointly encoded chunks and encoding them separately increases the aggregate rate , we can upper bound the total rate by assuming that @xmath381 .",
    "the encoding for the first interior chunk @xmath61 in @xmath245 takes in this case at most @xmath382 bits .    in the latter case ( @xmath186 ) , the number @xmath100 is larger than or equal to @xmath379 ( since the corresponding chunks must have been seen in sequence by the uniqueness assumption ) , and all the chunks with indices in @xmath245 are encoded together , taking at most @xmath383 bits .",
    "again , reducing the number of jointly encoded chunks and encoding them separately increases the aggregate rate , and thus we can upper bound the total rate by assuming that @xmath384 .",
    "the encoding for the first interior chunk @xmath61 in @xmath245 takes in this case at most @xmath385 bits .",
    "assume next that we are on @xmath316 .",
    "if an interior chunk @xmath57 is not in the dictionary , it is encoded in the worst case using @xmath386 bits .",
    "if an interior chunk @xmath57 is in the dictionary , then the pointer into the dictionary takes at most @xmath387 bits . in the worst case , each old chunk is encoded separately , leading to an additional @xmath138 bits for the encoding of @xmath100 .",
    "thus , as long as @xmath388 the encoding of each old interior chunk @xmath57 takes at most @xmath389 bits , since each chunk has length at least @xmath85 by construction .",
    "thus , on the complement of @xmath316 and assuming   is satisfied , the encoding of the interior chunks of @xmath20 takes at most @xmath390 bits .",
    "similarly , as long as the condition   is satisfied , the boundary chunks can be encoded using at most @xmath391 bits each , regardless of whether they are in the dictionary .",
    "we can then upper bound the rate @xmath392 for a particular source sequence @xmath20 as @xmath393 where the second inequality follows after some algebra using the bounds @xmath394 , @xmath395 , and @xmath396 .",
    "taking expectations yields @xmath397 it remains to upper bound the last two terms in  .",
    "for the second - to - last term in  , we have the upper bound @xmath398 here , @xmath399 are the bits from the start of @xmath23 forward until the end of the first ( counting forwards ) occurrence of the string @xmath400 such that @xmath401 does not contain @xmath72 . and @xmath402 are the bits from the end of @xmath23 backwards until the end of the first ( counting backwards ) occurrence of the string @xmath72 plus an additional @xmath95 bits .",
    "if no such substring occurs , head and tail denote all the bits in @xmath23 in either case .",
    "the expected value of @xmath403 is upper bounded as @xmath404 as before by ( * ? ? ?",
    "* theorem  8.2 ) ( see again appendix  [ sec : proofs_variable ] ) .",
    "the quantity @xmath405 can be upper bounded by replacing @xmath23 with an infinite - length @xmath283 process .",
    "the head of that bernoulli process is then a concatenation of sub - chunks of the form @xmath406 with @xmath407 for @xmath408 and with @xmath409 . consider the sequence of lengths @xmath410 , @xmath411 ,  .",
    "this sequence forms an stochastic process , and @xmath412 is a stopping time with respect to this process .",
    "thus , we can apply wald s equation together with ( * ? ? ? * theorem  8.2 ) to obtain @xmath413 the random variable @xmath412 is geometrically distributed with probability of success lower bounded by @xmath414 .",
    "thus @xmath415 and @xmath416 the same argument also shows that @xmath417 substituting the last three equations into  , we obtain @xmath418    for the last term @xmath419 in  , we need to upper bound the probability that there is at least one interior chunk in the source alphabet that is either equal to another interior chunk of the source alphabet or to a boundary chunk of the source sequence .",
    "since all chunks have length at least @xmath85 by construction , whenever this last event holds , then @xmath378 contains a nonoverlapping duplicate substring of length @xmath420 ( where the additional factor @xmath204 accounts for the boundary chunks ) .",
    "consider a source symbol , say @xmath206 .",
    "the probability that it contains a nonoverlapping duplicate substring of length @xmath420 is upper bounded by the probability that a @xmath283 process of length @xmath421 contains such a substring .",
    "consider next two source symbols , say @xmath206 and @xmath207 .",
    "condition on their lengths @xmath422 and @xmath423 . if these lengths are distinct , then @xmath206 and @xmath207 are independent @xmath283 processes of given length",
    ". if the lengths are the same , then @xmath206 and @xmath207 are not independent , since they are chosen without replacement .",
    "however , the probability of @xmath206 containing a duplicate substring of length @xmath420 from @xmath207 is upper bounded by drawing them with replacement .",
    "further , in both cases , the probability of the event under consideration is increased if we increase the length of the source symbols . in summary ,",
    "the probability that the source alphabet @xmath0 contains a nonoverlapping duplicate substring of length @xmath420 is upper bounded by the probability that @xmath5 independent @xmath283 processes of length @xmath421 contain such a duplicate .",
    "this probability can in turn be upper bounded by @xmath424 .",
    "therefore , @xmath425    substituting   and   into   yields @xmath426 combined with   in appendix  [ sec : proofs_variable ] , this shows that @xmath427    assume first that @xmath428 , and set @xmath429 note that @xmath430 satisfying . with this choice of @xmath71 ,",
    "becomes after some simplification @xmath431 assume next that @xmath432 , and set @xmath433 note that then @xmath434 again satisfying . with this choice of @xmath71 , becomes after some simplification @xmath435    from   and  , we conclude that , regardless of the relationship of @xmath5 , @xmath21 , and @xmath4 , we have @xmath436 as @xmath45 . together with   in appendix",
    "[ sec : proofs_variable ] , we obtain @xmath437 as @xmath45 .",
    "the author thanks m. a. maddah - ali for helpful initial discussions ."
  ],
  "abstract_text": [
    "<S> deduplication finds and removes long - range data duplicates . </S>",
    "<S> it is commonly used in cloud and enterprise server settings and has been successfully applied to primary , backup , and archival storage . despite its practical importance as a source - coding technique , its analysis from the point of view of information theory is missing . </S>",
    "<S> this paper provides such an information - theoretic analysis of data deduplication . </S>",
    "<S> it introduces a new source model adapted to the deduplication setting . </S>",
    "<S> it formalizes both fixed and variable - length deduplication schemes , and it introduces a novel , multi - chunk deduplication scheme . </S>",
    "<S> it then provides an analysis of these three deduplication variants , emphasizing the importance of boundary synchronization between source blocks and deduplication chunks . </S>",
    "<S> in particular , under fairly mild assumptions , the proposed multi - chunk deduplication scheme is shown to be order optimal . </S>"
  ]
}