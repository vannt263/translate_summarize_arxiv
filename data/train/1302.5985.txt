{
  "article_text": [
    "many problems in human and in computer vision are ill - defined . in problems such as boundary detection",
    ", there is no objective measurement that determines whether there is a _ perceptually meaningful _ boundary in any location in an image . to benchmark the performance of a boundary detection algorithm , human labeled datasets",
    "( e.g. bsds300 @xcite with 200 training images and 100 testing images ) play a critical role .",
    "these datasets characterize the perceptual definition of boundaries in an implicit way by providing exemplar images that have been labeled by a small number of human subjects .",
    "however , labelers do not always agree with each other .",
    "variability is intrinsically related to the ill - defined nature of boundary detection .",
    "yet there is surprisingly little discussion of data variability for boundary detection and its effect on benchmarks .",
    "it is commonly held that the labelers of boundary datasets ( such as bsds300 ) are reliable .",
    "examined separately , each boundary seems to be reasonable with some underlying edge in the image . in @xcite martin",
    "_ et al . _ considers label variability to be due to different labelers drawing in different levels of details .",
    "@xcite believes that even though a labeler may scrutinize some parts of the image in considerable detail , while drawing cursory sketches on other parts , different labelers are consistent in a sense that the dense labels refine the sparse labels without contradicting them . in other words , these different instances of labels all come from the same perceptual hierarchy of an image .    nevertheless , local consistency within a specific region is not strong enough to legitimatize the entire benchmark . to be able to faithfully evaluate an algorithm , the benchmark data has to be free from both type i ( false alarm ) and type ii ( miss ) statistical errors . even though boundaries in a benchmark dataset seem to be reasonable , it is still possible that the labelers may miss some equally important boundaries , leaving us with an imperfect benchmark .",
    "such benchmark that contains type ii errors may incorrectly penalizing an algorithm that detects true boundaries .",
    "we here propose a framework to analyze the quality or _",
    "benchability _ of any benchmark , and demonstrate with a quantitative experiment that the current dataset for benchmarking can be improved .",
    "although different human labels of the same boundary often contain spatial offsets up to several pixels , they rarely contradict each other @xcite ( e.g. with one drawing a horizontal and the other a vertical boundary at the same location ) . based on these observation",
    ", we can merge boundary maps of the same image labeled by different subjects into one master map @xmath0 . at each pixel location @xmath1 ,",
    "the response of labeler @xmath2 is a binary value @xmath3 ( _ i.e. _ , edge or non - edge ) .",
    "@xmath4 $ ] concatenates the response of all labelers .",
    "we use the assignment algorithm and parameters of @xcite to determine whether to merge adjacent lines from different subjects at one location .    to evaluate the correctness of a benchmark",
    ", we used a two - way forced choice paradigm ( shown in fig .",
    "[ fig_forced_choice ] ) . in any one trial ,",
    "a subject is asked to compare the relative _ perceptual strength _ of two local boundary segments .",
    "similar to @xcite , we do not give specific instructions that could potentially bias the result towards one particular type of boundary .",
    "the advantage of this two - alternative experiment is that it cancels out most of the fluctuations of cognitive factors , such as spatial attention bias , subject fatigue , and decision thresholds that are different in each subject .",
    "moreover , compared to the tedious labeling process , this paradigm is much simpler and cheaper to implemented via crowd - sourcing .    given sufficient number of comparisons and subjects , we can determine the relative perceptual strength of any pair of boundary segments .",
    "this framework yields a strict total ordering on the set of boundaries .",
    "we can map the boundary set onto the interval @xmath5 $ ] by assigning each boundary segment a real - value @xmath6 .",
    "this value @xmath6 can be considered as the _ perceptual strength _ of the boundary , because a boundary segment with large @xmath6 , by definition , is stronger ( _ i.e. _ , chosen more frequently by subjects ) than another boundary with smaller @xmath6 .",
    "let @xmath7 be the set of all boundaries in a dataset , @xmath8 be one boundary segment from @xmath7 , and @xmath9 be its perceptual strength .",
    "we can define the _ risk _ of a boundary set @xmath7 in relationship to a boundary set @xmath10 generated by some reference algorithm as :    @xmath11    this paradigm allows us to assess the risk associated with any dataset , such as bsds300 . because of its great popularity , we choose pb boundaries @xcite as the reference algorithm set @xmath10 .",
    "we choose the pb threshold such that the number of boundaries in @xmath10 is the same as in @xmath7 ( @xmath12 ) . to further illustrate the effect",
    ", we further restrict the sampling of human labels @xmath8 within a subset we call _ orphan labels _",
    "@xmath13 , which refers to the boundaries that are labeled by only one labeler ( @xmath14 ) but not by the other @xmath15 labelers . while algorithm algorithm labels from @xmath16 makes the procedure slightly different from the original eq .",
    "[ eq_risk ] ] @xmath17 of the entire boundary set of bsds300 are orphan labels .    .",
    "within @xmath7 , the set of orphan labels @xmath13 is shown in green",
    ". the pb boundary set @xmath10 is the dotted ellipsoid .",
    "the set of edges falsely identified by the algorithm , @xmath16 , is highlit in red . in each trial",
    ", we randomly select one boundary segment from @xmath13 ( green ring ) and another from @xmath16 ( red ellipsoid ) and ask subjects to judge which one is perceptually stronger .",
    "two boundary segments ( high contrast squares with red lines ) are superimposed onto the original image ( shown in the middle figure ) . at the same time , the original is also presented to the subject in a separate window . in total ,",
    "@xmath18 image pairs are compared by all 5 subjects .",
    "the right figure shows the _ risk _ ( that is , how often the false - alarm algorithmic edges are preferred over the human labels ) , of this database for all 5 subjects .",
    "dotted line is chance level ( 0.5).,title=\"fig:\",width=491 ] +    we used 5 subjects to compare 100 pairs of boundary segments comparison ( 500 trials in total ) . for each pair",
    ", we use the mode response of all 5 subjects to determine the ordering .",
    "the mean risk of @xmath13 is @xmath19 .",
    "that is , almost half of the time , a `` false alarm '' algorithmic boundary is perceptually stronger than the orphan label , which would usually be consider `` ground truth '' .",
    "given the large fraction of orphan labels ( almost one third of all boundaries ) , this leaves the validity of using bsds300 to benchmark any one algorithm in doubt .    given threshold @xmath20 , there exist a _ perfect boundary set _",
    "@xmath21 that has zero risk , such that @xmath22 for any @xmath23 , and @xmath24 for any @xmath25 .",
    "this perfect set can be formed by examining boundary strength from all possible boundaries from all images . however , the current _ imperfect _ boundary set @xmath7 annotated by a finite number of unreliable labelers lacks the information of a vast majority of unlabeled pixels .",
    "there is a probability such that a `` qualified '' boundary @xmath8 with @xmath22 exists in the unlabeled pixels .",
    "this probability decreases as @xmath20 increases , because a relatively strong boundary is less likely to be overlooked by all labelers .",
    "in fact , by taking the extremal threshold @xmath26 , we end up with a trivial solution : a risk - free but useless empty boundary set to @xmath27 . ] .    in this paper",
    ", we restrict our analysis within existing boundary labels in bsds300 , and try to infer the perceptual strength for each boundary segment . inferred perceptual strengthes allow the user to choose an appropriate threshold , and form a subset of boundary segments that balances risk and _ utility _ , which we refer to the total available number of data - points in the selected subset . in the next section",
    ", we present a graphical model that estimates the boundary perceptual strength .",
    "during the labeling process , each subject @xmath2 , governed by her / his internal psychophysical parameters @xmath28 , responds to segments of different perceptual strength @xmath9 . for all the boundaries such that @xmath29 , the response @xmath3 yields a mixture of bernoulli distributions , with parameter @xmath30 .",
    "furthermore , we assume @xmath30 yields a sigmoid functional form . the graphical model of the labeling process is shown in fig .",
    "[ fig_gm ] .     and the response profile of the labeler @xmath31 , which is further controlled by a hidden parameter @xmath28 .",
    "the gray circle indicates the observed variable , which is the binary individual response to a boundary segment .",
    "the model outputs estimates of the perceptual strength of each boundary segment as well as the parameters of each labeler.,title=\"fig:\",height=56 ] +    in our model , @xmath9 yields a uniform distribution @xmath32 .",
    "@xmath33 , where @xmath34 is the sigmoid function : @xmath35 .",
    "the conditional probability of @xmath3 is a soft voting of different @xmath36 , such that @xmath37 , where @xmath38 is the gaussian probabilistic density function with zero mean and @xmath39 standard deviation .",
    "we set @xmath40 .",
    "we use the em algorithm to estimate @xmath41 , and @xmath9 .",
    "we start with @xmath42 $ ] as the initial guess @xmath43 . in each iteration , the estimate of @xmath36 is given by @xmath44 .",
    "@xmath45 is updated by @xmath46 . for the estimate of @xmath6",
    ", we have @xmath47 .",
    "the optimization process converges within 20 iterations .",
    "the distribution of the perceptual strength is shown in fig .",
    "[ fig_exp_results ] .",
    "given the inferred perceptual strengthes , we select 4 thresholds @xmath48 , @xmath49 , @xmath50 , and @xmath51 , and formed 4 subsets @xmath52 of boundary segments . for each @xmath52",
    "we use the pb algorithm to generate @xmath53 such that @xmath54 .",
    "finally , a 5-subject experiment is conducted to evaluate the risk of @xmath52 . for each image",
    ", we randomly choose a pair of boundary segments from @xmath53 and @xmath52 , and then take the majority voting of our subjects responses to estimate the relative strength ordering .",
    "a total number of 500 trials are averaged to estimate the risk of each subset .",
    "the result is shown in fig .",
    "[ fig_exp_results ] .    .",
    "in this figure , each color corresponds to one subject .",
    "right 2 : risk estimate based on majority voting of all subjects .",
    "the dotted line in the right figure indicate the mode risk of @xmath55 in fig .",
    "[ fig_forced_choice].,title=\"fig:\",width=529 ] +",
    "there are two main trends in the perceptual strength distributions shown in fig .",
    "[ fig_exp_results ] .",
    "first , the spiky distribution of initial guess has been successfully smoothed out , because each subject has his distinctive labeling characteristics and therefore their response weights differently to the estimated strength .",
    "second , many of the boundary strengthes are automatically suppressed to zero .",
    "in fact , most of these zero - strength boundary segments correspond to the orphan labels , which are the biggest source of the dataset risk . from the right two figures",
    ", we see that the subset risk decreases as the perceptual strength threshold @xmath20 goes up .",
    "this result supports the risk - utility model we mentioned in sec .",
    "[ sec_risk ] .",
    "we have shown that a human - labeled dataset , even if well constructed and tested , can contain serious risks that hinder its ability to evaluate algorithm performance .",
    "we first proposed a psychophysical test to estimate human dataset risk , where by risk we mean mistakenly classifying strong algorithmic boundaries as false alarms .",
    "we discuss an inference model to find the perceptual strength of each boundary segment , and use it to balance the risk utility trade - off .    due to space limitation ,",
    "we are unable to discuss other factors such as the stability of labeler - image assignment and its influence on the perceptual strength estimation ; the information - theoretic limit of the two - way force choice , and result variation by using different algorithms .",
    "these issues will be addressed in the journal submission of this paper @xcite .",
    "the first author would like to thank liwei wang , yin li , xi ( stephen ) chen , and katrina ligett .",
    "the research was supported by the onr via an award made through johns hopkins university and by the mathers foundation ."
  ],
  "abstract_text": [
    "<S> human labeled datasets , along with their corresponding evaluation algorithms , play an important role in boundary detection . </S>",
    "<S> we here present a psychophysical experiment that addresses the reliability of such benchmarks . to find better remedies to evaluate the performance of any boundary detection algorithm </S>",
    "<S> , we propose a computational framework to remove inappropriate human labels and estimate the intrinsic properties of boundaries . </S>"
  ]
}