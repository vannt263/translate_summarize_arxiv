{
  "article_text": [
    "given @xmath0 observations of @xmath1 response variables and @xmath2 predictors , denoted by @xmath3 and @xmath4 for @xmath5 , we consider the multivariate regression model @xmath6 where @xmath7^t$ ] , @xmath8^t$ ] , @xmath9 is an unknown coefficient matrix , and @xmath10^t \\in\\mathbb{r}^{n\\times m}$ ] is a random error matrix . such a high - dimensional multivariate problem , in which both @xmath2 and @xmath1 may be comparable to or even exceed the sample size @xmath0 , has drawn increasing attention in both applied and theoretical statistics .",
    "the conventional linear regression using the least squares ( ls ) ignores the intrinsic multivariate nature of the problem and may easily fail when the model dimension is high relative to the sample size .",
    "dimension reduction holds the key in characterizing the dependence between the response variables and the predictors in a parsimonious way . the famous reduced rank regression ( rrr ) @xcite achieves the purpose through restricting the rank of the coefficient matrix @xmath11 , i.e. , @xmath12 where @xmath13 and @xmath14 denote the trace and rank of the enclosed matrix , respectively , and @xmath15 is a pre - specified positive definite ( p.d . ) weighting matrix @xcite .",
    "the values of @xmath16 of interest are typically much smaller than both @xmath1 and @xmath2 .",
    "nicely , a global minimizer to can be obtained explicitly ; see section [ sec : ipod ] .",
    "a comprehensive account of the developments and extensions of rrr under the classical large-@xmath0 asymptotic regime was provided by @xcite .",
    "finite - sample theories for the penalized form of rrr were developed by @xcite , on both rank selection and nonasymptotic oracle error bounds .",
    "low rank matrix estimation can be achieved more generally , by promoting the singular value sparsity of @xmath11 or @xmath17 , e.g. , the nuclear norm and schatten @xmath2-norms , cf .",
    "@xcite , @xcite , @xcite , @xcite , among others .",
    "rrr is closely connected with principal component analysis , canonical correlation analysis , partial least squares , matrix completion , and many other multivariate tools @xcite .",
    "although rrr can substantially reduce the number of free parameters in multivariate problems , we found that it is extremely sensitive to outliers . in real - world data analysis ,",
    "outliers and leverage points are bound to occur and thus the genuine reduced - rank structure could easily be masked or distorted .",
    "the issue becomes even more serious in high - dimensional or big data applications .",
    "for example , in cancer genetics , multivariate regression is commonly used to explore the associations between genotypical and phenotypical characteristics @xcite , for which imposing the reduced rank regularization could reveal a few latent regulatory pathways linking the two sets of variables .",
    "but the pathway recovery is better not to be dominated or distorted by abnormal samples or subjects . as another example ,",
    "due to extreme market movements , financial time series , even after stationarity transformations , often contain anomalies or demonstrate heavier tails than those of a normal distribution , which may jeopardize the recovery of common market behaviors and the forecast of asset returns .    to see the necessity and power of low - rank robustification ,",
    "it is perhaps best to demonstrate a real - world data example .",
    "let s consider 52 weekly log - returns of stocks for nine of the ten largest american corporations in 2004 , i.e. , @xmath18 , @xmath19 and @xmath20 .",
    "chevron was excluded due to its non - stationarity @xcite .",
    "the nine time series are shown in the top panel of figure [ fig : stock ] .",
    "for the purposes of constructing market factors that drive general stock movements , a vector autoregressive ( var ) model can be used , i.e. , @xmath21 , together with a low - rank assumption on @xmath22 .",
    "however , we observed that several stock returns experienced dramatic short - term changes .",
    "furthermore , the var structure makes any outlier in the time series also a leverage point in the covariates .    using the weekly log - returns in the first two quarters for training and those in the last two quarters for forecast",
    ", we analyzed the data by rrr and a robust reduced rank regression , called @xmath23 , to be introduced later . while both methods resulted in unit - rank models , @xmath23automatically detected three outliers , i.e. , the log - returns of ford at weeks 5 and 17 and the log - return of gm at week 5 .",
    "these outliers correspond to two real major market disturbances attributed to the auto industry . taking these outlying effects into account , @xmath23led to a more reliable model .",
    "the bottom panel of figure [ fig : stock ] shows the two estimated market factors , and table [ table : stock ] displays the factor coefficients indicating how the stock returns are related to the estimated factors .",
    "the two sets of results exhibit very different patterns . in particular , the stock factor estimated by @xmath23has positive influence over all nine companies .",
    "the scaled prediction errors for ls , rrr and @xmath23 , robustly measured by 40% trimmed mean squared error , are @xmath24 , @xmath25 and @xmath26 , respectively .",
    "( the regular non - robust mean squared prediction errors are @xmath27 , @xmath28 and @xmath29 , respectively . ) therefore , the robustification of rank reduction resulted in 20% improvement in prediction !",
    "the example demonstrates the necessity and efficacy of robust low - rank estimation in real world applications .",
    "in this work , we deem _ explicit _ outlier detection an equally important task as robust low rank estimation .",
    "indeed , the reduced - rank component may not even be of direct interest in some applications , as it often represents common background information shared across the response variables , while capturing unusual signal changes or jumps is more helpful and intriguing .",
    "the robustification of low rank estimation is non - trivial .",
    "a straightforward idea is to use a robust loss function @xmath30 instead of the squared error loss in , leading to @xmath31 nevertheless , such an estimator may be difficult to compute .",
    "to the best of our knowledge even when @xmath30 is the convex huber s loss function @xcite , there is no available algorithm for solving , let alone nonconvex losses which are known to be more powerful in dealing with multiple gross outliers with possibly high leverage values .",
    "more importantly , theoretical works are extremely scarce .",
    "in fact , even without the low rank constraint , classical robust theories are either worst - cases studies ( non - random ) or large-@xmath0 asymptotics ( with @xmath2 and @xmath1 fixed ) , which by no means meets modern application needs .    ; the estimates ( at @xmath32 ) are shown on the left side , and the predictions ( for @xmath33 ) are shown on the right.,title=\"fig:\",width=576 ]    we propose a novel * * r**obust * * r**educed * * r**ank * * r**egression , referred to as @xmath23 , for simultaneous robust reduced - rank modeling and outlier identification .",
    "we explicitly include a sparse mean - shift outlier component and formulate a shrinkage multivariate regression in place of , with @xmath2 and/or @xmath1 possibly much larger than @xmath0 .",
    "@xmath23provides a general framework and includes the m - estimation @xcite and principal component pursuit @xcite as particular instances .",
    "the rest of of the paper is organized as follows . in section",
    "[ sec : rquad ] , we show that low rank estimation can be completely destroyed by a single rogue point , and propose the @xmath23framework .",
    "a universal connection between @xmath23and m - estimation is established for models of arbitrary @xmath34 .",
    "section [ sec : theory ] studies finite - sample theoretical properties of the @xmath23estimator .",
    "this extends the classical robust analysis to nonasymptotic study of multivariate data with @xmath35 . a computational algorithm for solving @xmath23is developed in section [ sec : ipod ] .",
    "the algorithm is easy to implement and leads to a coordindatewise minimum point with theoretical guarantee .",
    "section [ sec : sim ] presents simulation studies and section [ sec : app ] shows some real applications .",
    "we conclude in section [ sec : dis ] .",
    "proof details are given in section [ sec : proofs ] .",
    "rrr enforces low rankness for shrinkage estimation in high dimensional multivariate problems . nicely , although problem is highly nonconvex , a global minimizer @xmath36 can be obtained in explicit form .",
    "given any @xmath16 , @xmath37 with @xmath38 , @xmath39 where @xmath40 stands for the moore - penrose pseudoinverse of @xmath41 , @xmath42 denotes the orthogonal projection onto the range of @xmath41 , and @xmath43 is formed by the leading @xmath16 eigenvectors of @xmath44 .",
    "see , e.g. , @xcite for detailed theoretical justifications .",
    "sometimes , we also denote the estimator as @xmath45 to emphasize its dependence on the regularization parameter , or @xmath46 for notational convenience .",
    "anomaly points or outliers are unavoidable in real large - scale data .",
    "the previous financial data set is one example , where the plain rrr was not reliable .",
    "is this just a special case ? to rigorously characterize this phenomenon , we define the finite - sample breakdown point for an arbitrary estimator @xmath46 , in the spirit of @xcite : given finite data @xmath47 and an estimator @xmath48 , its breakdown point is given by @xmath49 where @xmath50\\neq 0\\}|$ ] .",
    "[ th : bp ] given any finite @xmath47 and @xmath51 with @xmath15 p.d . and",
    "@xmath52 , let @xmath53 be an rrr estimator that minimizes @xmath54 .",
    "then its finite - sample breakdown point is exactly @xmath55 .",
    "furthermore , for a general low rank estimator @xmath56 @xmath57 still holds for any finite value of @xmath58 . here , the penalty @xmath59 is associated with a thresholding rule @xmath60 ( cf .",
    "definition [ def : threshold ] ) through , @xmath58 is a regularization parameter , and @xmath61 denote the singular values of @xmath62 .    therefore , for either rrr , or the nuclear norm penalization @xmath63 , or the direct rank penalization @xmath64 , or schatten @xmath2-norms , a single outlier can completely break down the low rank matrix estimation . in conclusion , the popular means of multivariate low - rank regularization , though quite effective for dimension reduction , is extremely sensitive to outliers .",
    "this conclusion seriously limits the usage of rank reduction in big data applications .",
    "+ because the direct robustification through modifying the loss function ( see , e.g. , ) may be tricky in computation and analysis , we will apply a novel ` additive ' robustification motivated by @xcite . first introduce a multivariate mean - shift regression model to * explicitly * represent and handle outliers @xmath65 where @xmath66 gives the matrix of coefficients , @xmath67 describes the outlying effects in response to @xmath68 , and @xmath10^t \\in\\mathbb{r}^{n\\times m}$ ] with @xmath69 i.i.d .",
    "@xmath70 . obviously , this results in an over - parameterized model .",
    "the essence lies in properly regularizing the unknown matrices .",
    "we assume that @xmath11 has low rankness and @xmath71 is a sparse matrix with only a few nonzeros because outliers are inconsistent with the majority of the data .    given a p.d .",
    "weighting matrix @xmath15 , we define the * * r**obust * * r**educed * * r**ank * * r**egression , or @xmath23 , as follows @xmath72 here , @xmath73 is a sparsity - promoting penalty function with @xmath58 controlling the amount of shrinkage , but it can also be a ( group ) @xmath74 constraint such as",
    ". the following form of penalty ( by a little abuse of notation ) can handle element - wise outliers @xmath75 this was used in the stock return analysis .",
    "but assuming outlying samples , or associated rows in @xmath76 , is perhaps more common in robust multivariate statistics , which corresponds to @xmath77 where @xmath78 is the @xmath79th row vector of @xmath71 .",
    "unless otherwise specified , we consider row - wise outlier accommodation and detection .",
    "however , all our algorithms and analyses apply to element - wise outliers after a simple modification .",
    "the choice of the weighting matrix @xmath15 is flexible and can incorporate certain correlation structure into the robust rank reduction .",
    "for example , it can be set based on a ( robust ) covariance estimate , i.e. , @xmath80 when @xmath81 is nonsingular ( or a regularized version @xmath82 with some @xmath83 ) . in light of the decorrelated model @xmath84 when @xmath15 is the inverse error covariance matrix ,",
    "an immediate variant is to replace @xmath71 by @xmath85 in the penalty term in or .",
    "[ unknowncov ] in the literature of reduced rank estimation , it is common to regard the weighting matrix as known @xcite .",
    "although it might be straightforward to consider the joint estimation of the high - dimensional low - rank mean structure and the even higher - dimensional covariance matrix in the presence of outliers , the problem becomes too challenging and is beyond the scope of this paper .",
    "even ignoring the low - rank matrix recovery , the research on robust high - dimensional ( inverse ) covariance estimation is very scarce in the current literature .",
    "( we tried a number of up - to - date large covariance estimation methods but found none to be satisfactory when outliers occur . ) when a reliable estimate of @xmath86 is not available , an empirically good choice is to reduce @xmath15 to a diagonal matrix , or equivalently , an identity matrix after ( robustly ) scaling the response variables .",
    "this is the standard treatment in finance and econometrics forecasting .",
    "therefore , throughout the paper , we always assume @xmath15 is given and is positive definite .",
    "we now show that the proposed ` additive ' framework indeed comes with a robust guarantee , and intriguingly , it generalizes the renowned m - estimation to reduced rank settings .",
    "hereinafter , we write @xmath87^t$ ] , @xmath88^t$ ] .",
    "[ th : rob ] ( i ) suppose @xmath89 is an arbitrarily given _ thresholding rule _",
    "( odd , monotone , unbounded shrinkage rule , cf . definition [ def : threshold ] ) .",
    "let @xmath59 be any penalty associated with @xmath60 by @xmath90 for some nonnegative @xmath91 satisfying @xmath92 , @xmath93 .",
    "consider an @xmath23optimization @xmath94 fixing @xmath11 , @xmath95 gives a globally optimal solution , and for this @xmath96 , the optimization problem for @xmath11 reduces to the robust m - estimation problem @xmath97 with the robust loss function @xmath30 given by @xmath98 and @xmath99 .",
    "\\(ii ) given @xmath100 , consider a constrained @xmath23optimization @xmath101 then given @xmath11 , @xmath102 gives a globally optimization solution , with @xmath103 denoting the quantile thresholding ( cf .",
    "section [ subsec : notation ] ) , and for this @xmath96 ,",
    "the optimization of @xmath46 is equivalent to @xmath104 where @xmath105 are the order statistics of @xmath106 satisfying @xmath107 .",
    "[ remrob1 ] theorem",
    "[ th : rob ] connects the penalty @xmath59 and the robust loss function @xmath30 through the thresholding @xmath60 .",
    "as is well known in robust statistics , modifying the loss function amounts to adding _ multiplicative _ sample weights in least squares . perhaps interestingly , the new additive manner achieves the same robustness .",
    "the connection holds in the case of elementwise outliers ( with @xmath59 and @xmath30 applied in an elementwise manner ) .",
    "in fact , based on the identity established in the proof , the conclusion holds much more generally  for example , an arbitrary penalty or constraint on @xmath11 can be imposed , and it is regardless of the number of response variables and the number of predictors .",
    "this extends the main conclusion in @xcite for single - response models with @xmath108 and under no rank restriction .",
    "[ remrob2 ] theorem [ th : rob ] holds for _ any _ thresholding rules , and nicely , all popularly used penalties ( convex or nonconvex ) are covered by .",
    "for example , the convex group @xmath109 penalty @xmath110 is associated with the soft - thresholding @xmath111 .",
    "however , it suffers from biased estimation and inconsistent selection . in enforcing sparsity , it is of no doubt that the group @xmath74 penalty @xmath112 is ideal , which can be attained by with the hard - thresholding @xmath113 and @xmath114 .",
    "our @xmath60-@xmath59 coupling framework also covers elastic net , @xmath115 ( @xmath116 ) , scad @xcite , mcp @xcite , the capped @xmath109 , and @xmath117 as particular instances ; see @xcite .",
    "[ remrob3 ] the universal link between shrinkage outlier estimation and robust m - estimation provides some insights into the type of regularization and computation .",
    "for example , the convex group @xmath109 penalty used in @xmath23is now equivalent to minimizing huber s loss under rank restriction .",
    "however , it is well known that huber s method is prone to masking and swamping in outlier detection and may easily fail for even moderate leverage points .",
    "in contrast , redescending @xmath118 functions are advocated to handle gross outliers .",
    "they correspond to nonconvex penalties in @xmath23 .",
    "for example , hampel s three - part @xmath118 @xcite gives the famous scad penalty in , skipped mean @xmath118 corresponds to the @xmath74 penalty , and finally the rank constrained least trimmed squares ( lts ) can be rephrased as the @xmath74-constrained @xmath23as in .",
    "hence our approach provides a unified therapy to robustify low rank matrix estimation , and facilitates the computation of reduced rank m - estimators in high dimensions .",
    "we usually favor the group @xmath74 penalty and the constrained form in data analysis and simulation .",
    "the challenge from the nonconvexity is non - trivial .",
    "the problem becomes even more complex with the rank constraint .",
    "the theoretical and computational concerns are addressed in sections [ sec : theory ] and [ sec : ipod ] .",
    "before we dive in to the detailed analysis , it is now in place to discuss the connections and extensions of the proposed @xmath23framework .",
    "the proposed @xmath23framework offers both parsimony and robustness in multivariate estimation , and it connects to several popular unsupervised / supervised learning tools .    in the extreme case that @xmath71 is penalized to be a zero matrix , @xmath23reduces to the plain rrr .",
    "when @xmath15 is set to be the inverse covariance matrix of the multivariate response , we obtain a robust version of canonical correlation analysis @xcite .",
    "although we mainly focus on the rank - constrained form of @xmath23 , there is no difficulty to extend our discussion to @xmath119 where @xmath120 denote the singular values of @xmath11 , and both @xmath121 and @xmath122 are sparsity - inducing penalties .",
    "@xmath23subsumes the popular ` low - rank + sparse + noise ' matrix decomposition problem , which deals with a special case of when @xmath123 is an identity matrix , i.e. , @xmath124 .",
    "the problem is perhaps less challenging than its supervised counterpart , but has wide applications in computer vision and machine learning @xcite .",
    "finally , the @xmath23approach can be extended to handle low - rank vector generalized linear models , by replacing the weighted squared error loss with a negative log - likelihood function .",
    "see , e.g. , @xcite and @xcite for some computation details . in this case ,",
    "how to directly modify the glm loss to gain robustness can be tricky , but the proposed scheme by use of a sparse shift outlier term facilitates the robust design and computation . in all , @xmath23provides a general framework to incorporate robust estimation and dimension reduction in multivariate analysis .",
    "theorem [ th : rob ] gives some helpful intuition of @xmath23 , but it might not be enough from a theoretical point of view . for example , can one theoretically justify the necessity of low - rank robustification , as well as the power of @xmath23 ? is using redescending @xmath118 functions still preferable in rank - deficient settings ?",
    "in contrast to traditional robust analysis , we can not assume infinite sample size with fixed number of predictor / response variables , because @xmath2 and/or @xmath1 can be much larger than @xmath0 in modern statistical applications .",
    "that is , conducting * nonasymptotic * robust analysis would be desirable ( and novel , to the best of our knowledge ) .",
    "we hope the finite - sample results established in this section could contribute to this type of robust analysis .    for simplicity",
    "we assume the model is given by @xmath125 with @xmath126 and consider the @xmath23problem defined by @xmath127 let @xmath128 be a global minimizer of .",
    "we focus on the prediction accuracy measured by @xmath129 , where @xmath130 this predictive learning perspective is always legitimate in evaluating the performance of an estimator .",
    "it avoids signal strength and model uniqueness assumptions .",
    "also , the study of the @xmath131 recovery is fundamental , and such a bound , together with additional regularity assumptions , can be easily adapted to obtain estimation bounds in different norms as well as selection consistency ( e.g. , @xcite , @xcite ) .    for any @xmath132^t$ ] , define @xmath133 to address the problems in arbitrary dimensions ( with possibly large @xmath2 and/or @xmath1 ) , nonasymptotic * oracle inequalities * @xcite will be shown for the @xmath23estimators .    in the first theorem",
    ", we consider a general penalty @xmath134 , where @xmath73 takes @xmath58 as the threshold parameter , and satisfies @xmath135 where @xmath136 .",
    "the latter inequality is natural in view of , because a shrinkage estimator ( with @xmath58 as the threshold ) is always bounded above by the hard thresholding function @xmath137 .",
    "( equivalently , covers all @xmath118-functions bounded below by the skipped mean @xmath138 for any @xmath139 . )",
    "all practically used penalties satisfy , cf .",
    "remark [ remrob2 ] of theorem [ th : rob ] .",
    "[ th_oracle ] let @xmath140 with @xmath141 as a constant and @xmath142 be a global minimizer of .",
    "then for any sufficiently large @xmath141 , the following oracle inequality holds for any @xmath143 satisfying @xmath144 : @xmath145\\lesssim     m ( { { \\boldsymbol{b}}}-{{\\boldsymbol{b}}}^ * , { { \\boldsymbol{c}}}- { { \\boldsymbol{c}}}^ * )   + \\sigma^2 ( q+ m ) r + p({{\\boldsymbol{c } } } ; \\lambda)+\\sigma^{2},\\label{genoracle}\\end{aligned}\\ ] ] where @xmath146 means the inequality holds up to a multiplicative constant .",
    "[ l0oracle ] under the same conditions of theorem [ th_oracle ] , the following oracle inequality holds if @xmath59 is a _ bounded nonconvex _",
    "penalty satisfying @xmath147 , for some constant @xmath148 ( such as the @xmath74 penalty and the scad penalty ) : @xmath149\\lesssim \\\\   &   \\",
    "\\ \\inf_{({{\\boldsymbol{b } } } , { { \\boldsymbol{c } } } ) : r({{\\boldsymbol{b}}})\\leq r}\\ { m ( { { \\boldsymbol{b}}}-{{\\boldsymbol{b}}}^ * , { { \\boldsymbol{c}}}- { { \\boldsymbol{c}}}^ * )   + \\sigma^2 ( q+ m ) r + \\sigma^2 j({{\\boldsymbol{c } } } ) m + \\sigma^2 j({{\\boldsymbol{c } } } ) \\log n \\ }   + \\sigma^2 . \\end{split}\\label{rateoracle}\\end{aligned}\\ ] ]    [ remth1 ] the right hand sides of these oracle inequalities involve a _ bias _",
    "term @xmath150 , @xmath151 , in addition to some complexity terms . simply setting @xmath152 , @xmath153 and @xmath154",
    "so that @xmath155 disappears in , say , , we obtain a prediction error bound of the order @xmath156 ( omitting constant factors ) . here",
    ", @xmath157 is the rank of the true coefficient matrix @xmath22 , and @xmath158 is the number of nonzero rows in @xmath159 , i.e. , the number of outliers . on the other hand , the presence of the bias term ensures the applicability of @xmath23to weakly sparse @xmath159 to handle moderate or mild outliers possibly occurring in practice , and @xmath16 may also deviate from @xmath157 to some extent .",
    "[ remth2 ] the conclusion obtained in theorem [ th_oracle ] is general and applies to a wide family of penalties or @xmath118-functions . for the constrained form or the doubly penalized form of @xmath23 , our proof scheme still applies and gives similar results .",
    "we emphasize that theorem [ th_oracle ] does _ not _ place any requirement on @xmath123 which is in contrast to theorem [ th_oracle - l1 ] .",
    "[ remth3 ] the benefit of applying a _ re - descending _",
    "@xmath118 is clearly revealed . as an example , for huber s @xmath118 which corresponds to the popular convex @xmath109 penalty due to theorem [ th : rob ] , @xmath160 on the right hand side of is unbounded , while hampel s three - part @xmath118 gives a finite rate as shown in .",
    "furthermore , we show that in a minimax sense , the error rate obtained in corollary [ l0oracle ] is essentially optimal . toward this goal , we consider the following signal class @xmath161 where @xmath162 , @xmath163 .",
    "let @xmath164 be a nondecreasing loss function with @xmath165 , @xmath166 . under the rip assumption ,",
    "we show that a minimax lower bound holds for any estimator @xmath167 .",
    "+ assumption @xmath168 .",
    "for any @xmath169 satisfying @xmath170 , @xmath171 holds for some positive constants @xmath172 and @xmath173 .",
    "[ th_minimax ] assume @xmath174 with @xmath175 , @xmath176 , @xmath177 , @xmath178 , @xmath179 , and @xmath123 satisfies @xmath180 .",
    "then there exist positive constants @xmath181 , @xmath182 ( depending on @xmath164 only ) such that @xmath183 \\geq c>0 , \\label{minimaxlowerbound}\\ ] ] where @xmath128 denotes any estimator of @xmath184 and @xmath185    we give some examples of @xmath186 to illustrate the conclusion . using the indicator function @xmath187",
    ", we learn that for any estimator @xmath188 , @xmath189 occurs with positive probability . for @xmath190 , theorem [ th_minimax ]",
    "shows that the risk @xmath191 $ ] is bounded from below by @xmath192 ( up to some multiplicative constant ) .",
    "therefore , essentially achieves the minimax optimal rate ( up to a mild logarithm factor ) , which shows the advantage of redescending @xmath118 s for robust estimation in reduced rank settings .",
    "moreover , our rate analysis is nonasymptotic and applies to any @xmath34 .",
    "[ remth4 ] convex methods are not hopeless . in fact , huber s @xmath118 can achieve the same low error rate , in some less challenging problems .",
    "to show such a sharp bound , some kind of incoherence condition must be assumed for the design matrix .",
    "we introduce a low - rank multivariate restricted eigenvalue assumption as an extension to @xcite and @xcite , and present theorem [ th_oracle - l1 ] on the basis of it .",
    "( in fact , the result can be extended to any sub - additive penalties with the associated @xmath118 sandwiched by @xmath193 and huber s @xmath118 . )",
    "assumption @xmath194 .",
    "we say @xmath195 satisfies @xmath196 for an index set @xmath197 $ ] and positive number @xmath198 , if and only if @xmath199 holds for all @xmath200 satisfying @xmath201 , and @xmath202 , where @xmath203 is the @xmath79th row of @xmath204 and @xmath205 is a constant .",
    "[ th_oracle - l1 ] in the @xmath109 case where @xmath206 , @xmath140 and @xmath141 is a large enough constant ,",
    "@xmath207\\lesssim   m ( { { \\boldsymbol{b}}}-{{\\boldsymbol{b}}}^ * , { { \\boldsymbol{c}}}- { { \\boldsymbol{c}}}^ * )   + \\sigma^2 ( q+ m ) r + \\sigma^2 j({{\\boldsymbol{c } } } ) m + \\sigma^2 j({{\\boldsymbol{c } } } ) \\log n    + \\sigma^2 $ ] holds for any reference signal @xmath208 with @xmath209 , provided that @xmath123 satisfies the restricted eigenvalue assumption @xmath210 for some positive constants @xmath211 and @xmath212 .",
    "[ remth5 ] it could be interesting to compare @xmath23with rrr in thoery , to get an idea of the performance improvement . under the outlier model assumption , we can also prove an oracle inequality for rrr ( cf .",
    "@xcite for a high - probability form result ) @xmath213 & \\lesssim    \\inf_{{{\\boldsymbol{b}}}\\in \\mathbb r^{p\\times m } , r({{\\boldsymbol{b } } } ) \\leq r }     \\|{{\\boldsymbol{x}}}{{\\boldsymbol{b}}}-({{\\boldsymbol{x}}}{{\\boldsymbol{b}}}^ * + { { \\boldsymbol{c}}}^ * ) \\|_f^2   + \\sigma^2 ( q+ m ) r + \\sigma^2 . \\label{rrrbnd}\\end{aligned}\\ ] ] take @xmath152 and @xmath153 .",
    "then the rrr error rate bound , evaluated at the optimal @xmath11 , is of order @xmath214 @xmath215 is of low rank , and so @xmath216 is not null in general .",
    "notable outliers always occur in the orthogonal complement space of the range of @xmath217 ( otherwise the model would be no different than an outlier - free reduced rank model ) .",
    "hence this error bound can be arbitrarily large , which echoes the breakdown point conclusion in theorem [ th : bp ] .    a better way to reduce the magnitude of the bias term is to use a larger rank value in the presence of outliers .",
    "setting @xmath218 in gives @xmath219 where we used @xmath220 . in the large-@xmath2 case , it leads to @xmath221 , much improved over .",
    "but @xmath23guarantees a consistently lower error rate at @xmath222 because @xmath223 .",
    "the performance gain can be dramatic in big data where @xmath123 can be huge and typically multiple outliers are bound to occur .",
    "our mean - shift model characterization , together with theorem [ th : rob ] , greatly facilitates the computation of @xmath23 . as an example , let s consider a penalized form of the @xmath23problem @xmath224 where @xmath15 is a given p.d . matrix .",
    "the penalties of interest are nonconvex in light of the theoretical results in section [ sec : theory ] , where the stringent incoherence assumptions associated with convex penalties can be much relaxed or even removed .",
    "we start from thresholding rules rather than penalty functions to tackle the nonconvexity challenge , considering that different penalty forms may result in the same estimator ( and the same thresholding operator ) .",
    "a simple algorithm for solving is described as follows . here",
    ", @xmath225 is a multivariate version of @xmath60 ( cf .",
    "definition [ def : thresholdmulti ] ) which is arbitrarily given .",
    "the proposed algorithm is computationally efficient and simple to implement .",
    "the two matrices @xmath71 and @xmath11 are alternatingly updated with the other held fixed until reaching convergence .",
    "step 1 performs simple multivariate thresholding operation for updating @xmath71 , and step 2 performs reduced - rank estimation of @xmath11 based on the adjusted data matrix @xmath226 . since in practice the rank @xmath16 of interest is usually quite small , the required eigenvalue decomposition is not computationally expensive .",
    "let @xmath60 be an arbitrary thresholding rule and suppose @xmath15 satisfies @xmath227 with @xmath228 representing the spectral norm .",
    "let @xmath229 be defined in , where @xmath59 is associated with @xmath60 through .",
    "then the @xmath23algorithm has the property that @xmath230 for all @xmath231 .",
    "the proof is a direct application of theorem 1 of @xcite and is omitted .",
    "the algorithm can be mildly modified to deal with , , , or .",
    "for example , when @xmath225 in step 2 is replaced by @xmath60 applied componentwise , we get an algorithm to deal with elementwise outliers . in data",
    "analysis and simulation , we will focus on the following @xmath74-penalized form of @xmath23@xmath232 as well as the constrained form , which correspond to applying hard - thresholding and quantile thresholding operators , respectively @xcite .",
    "the initial points @xmath233 and @xmath234 may affect the final solution . in common with most robust algorithms ,",
    "we apply the multi - start iterative strategy @xcite in general . on the other hand , in many applications we found the rrr initialization @xmath235 given by works pretty well .",
    "to choose an optimal rank @xmath16 and an optimal threshold level @xmath58 , cross - validation appears to be a popular option .",
    "however , it lacks theoretical support in our robust setting , and for large - scale problems , cross validation can be computationally expensive .",
    "adopting the idea of @xcite , we propose a predictive information criterion ( pic ) for model selection .",
    "denote by @xmath236 an @xmath23estimator .",
    "we define @xmath237 where @xmath238 stands for the sum of squared error and @xmath239 is the number of detected outliers .",
    "the term @xmath240 counts the degrees of freedom of the obtained model , and @xmath241 describes the risk inflation , from the proofs of theorem [ th_oracle ] and theorem [ th_minimax ] .",
    "based on computer experiments , we fix the constants at @xmath242 , @xmath243 .",
    "we consider three model setups of different dimensions and/or error structures . in models i and ii , we set @xmath244 , @xmath245 , @xmath246 , and @xmath247",
    ". the design matrix @xmath123 is constructed by generating its @xmath0 rows as i.i.d .",
    "samples from @xmath248 , where @xmath249 is the covariance matrix with diagonal elements being 1 and off - diagonal elements being 0.5 .",
    "this brings in wide - range predictor correlation .",
    "the rows of the noise matrix @xmath250 are generated as i.i.d .",
    "samples from @xmath251 . in model",
    "i , @xmath252 is set to be the @xmath1-dimensional identity matrix , whereas in model ii @xmath252 has the same compound symmetry structure as @xmath249 .",
    "in each simulation , the variance @xmath253 is computed to control the signal to noise ratio ( snr ) , defined as @xmath254 , to be either @xmath255 or @xmath256 .    model iii is a high - dimensional setup with @xmath244 , @xmath257 , @xmath258 , @xmath247 and @xmath259 .",
    "the design matrix @xmath123 is generated as @xmath260 , where @xmath261 , @xmath262 , and all entries of @xmath263 and @xmath264 are i.i.d samples from @xmath265 .",
    "we set @xmath266 and determine @xmath253 as described above to control the snr level .    in each of the three setups ,",
    "the coefficient matrix @xmath22 is generated as @xmath267 , where @xmath268 , @xmath269 and all entries in @xmath270 and @xmath271 are i.i.d .",
    "samples from @xmath272 .",
    "outliers are then added by setting the first @xmath273 rows of @xmath159 as nonzero . here",
    "@xmath274 represents the proportion of outliers .",
    "concretely , the @xmath275th entry in any outlier row of @xmath159 is a scalar @xmath276 times the standard deviation of the @xmath277 row of @xmath215 , where @xmath278 and @xmath279 . to make the problem more challenging ,",
    "we replace each of the first @xmath280 rows of @xmath123 with @xmath281 , where @xmath282 and @xmath283 .",
    "this setting , together with the construction of @xmath159 , yields some outliers with high leverage values .",
    "finally , the response matrix @xmath68 is generated based on the multivariate mean - shift regression model in ( [ model2 ] ) , i.e. , @xmath125 , so that the low rank component @xmath215 is contaminated by both random errors and gross outliers .",
    "each simulated model is thus characterized by the following parameters : the sample size @xmath0 , the number of predictors @xmath2 , the number of response variables @xmath1 , the rank of the coefficient matrix @xmath157 , the proportion of outliers @xmath284 , the outlier magnitude parameter @xmath276 , the number of leverage points @xmath280 , the leverage magnitude parameter @xmath285 , the error correlation structure @xmath252 , and the signal to noise ratio @xmath286 .",
    "the experiment is replicated 100 times in each model setup and parameter setting .",
    "we compare @xmath23with several robust regression approaches and rank reduction methods .",
    "many robust multivariate regression methods exist in large-@xmath0 settings .",
    "we mainly consider the mm - estimator proposed by @xcite as a prototype , using its implementation provided by the r package ` frb ` and the default settings therein .",
    "other robust estimators including the s - estimator @xcite and the gs - estimator @xcite are also examined ; we omit their results as they are very similar to or slightly worse than those from the mm - estimator .",
    "all these classical robust methods are not applicable when the number of variables is comparable to or exceeds the sample size .",
    "therefore , they are only applied in models i and ii .    for reduced - rank methods",
    ", we consider the plain rrr method and the reduced rank ridge regression approach ( rrs ) @xcite , both tuned by 10-fold cross validation . in the absence of outliers , the two methods were shown to be very effective in large-@xmath2/@xmath1 settings .",
    "rrs combines rank reduction and shrinkage estimation , which can potentially further improve the predictive performance of rrr when the predictors exhibit strong correlation .",
    "we also consider a three - step procedure for outlier detection in reduced rank estimation ( rr+o ) .",
    "specifically , the first step conducts the aforementioned plain rrr estimation using all the data . in the second step ,",
    "the value of residual sum of squares is computed for each of the @xmath0 observation rows , and exactly @xmath284 many observations with the largest residual sum of squares are labeled as outliers and discarded . at the third step ,",
    "we refit the rrr model with the rest of the observations .",
    "this method can be regarded as a naive `` oracle '' procedure , as it relies on the knowledge of the true number of outliers in the data , which is not available in real applications .    for the proposed @xmath23approach",
    ", we use the pic criterion in ( [ pic ] ) for tuning .",
    "@xmath23allows the incorporation of the known or estimated covariance structure into estimation through setting the @xmath15 matrix .",
    "we thus consider @xmath15 being either @xmath287 or @xmath288 in model ii , where @xmath81 is a robust estimate of @xmath289 obtained from mm estimation . since it is usually difficult to estimate @xmath86 in high dimensional settings , in model iii we only use @xmath290 . in the sequel",
    "we use the notion @xmath23(@xmath15 ) to indicate the choice of the weighting matrix . in our numerical studies , for each rank @xmath291 , we compute the solutions over a grid of 100 @xmath58 values equally spaced on the log scale , corresponding to a proper interval of the proportion of outliers given by @xmath292 $ ] .",
    "we take @xmath293 and @xmath294 , assuming that the proportion of outliers is under 40% .",
    "all the methods are implemented in r.    to characterize estimation accuracy robustly , we report the 40% trimmed mean of the mean squared error ( mse ) from all runs , where @xmath295 in model ii , we additionally report the 40% trimmed mean of the weighted mse ( wmse ) from all runs , with @xmath296 where @xmath86 is the true error covariance matrix .",
    "we note that the leverage points , if any , are removed from @xmath123 in the calculation of the mse / wmse . for rank reduction ,",
    "we report the average of rank estimates from all runs . to examine the outlier detection performance , we report the average masking rate ( m% ) , i.e. , the fraction of undetected outliers , the average swamping rate ( s% ) , i.e. , the fraction of good points labeled as outliers , and the frequency of correct joint outlier detection ( jd% ) , i.e. , the fraction of simulations with no masking and no swamping",
    ".      tables [ table1][table3 ] summarize the simulation results for models i  iii , respectively . to save space and avoid redundancy ,",
    "we omit the results for @xmath297 , @xmath298 and @xmath299 .    -3pt    in models",
    "i and ii , the mm - estimator achieves much better predictive performance than both rrr and rrs .",
    "this demonstrates that when severe outliers present , it is pivotal to perform robust estimation , without which the naive dimension reduction procedures may easily fail . nicely , @xmath23even in such low - dimensional settings outperforms all other methods . in model",
    "i , the joint outlier detection rates of @xmath23are very high in general . while mm achieves very low masking rates , it always produces some false positives , which translates to efficiency loss in estimation . in model ii ,",
    "when the error terms become correlated , @xmath23still shows impressive performance in both prediction and outlier detection . using the weighting matrix @xmath80",
    "may further improve the performance of @xmath23especially in the case snr=@xmath255 , although in general the gain is not substantial .",
    "the performance of rrr and rrs shows that failing to accommodate outliers could severely jeopardize reduced - rank estimation .",
    "interestingly , both rrr and rrs tend to overestimate the rank especially in the presence of high - leverage outliers .",
    "this complies with the theoretical implication , cf .",
    "remark [ remth5 ] after theorem 5 .",
    "in contrast , @xmath23achieves near - perfect rank determination in all the settings . in our simulation examples ,",
    "the rr+o method behaves well when there is no leverage point , and its performance is slightly worse than that of @xmath23 .",
    "however , as rr+o relies on the residuals to detect outliers , it often fails in the presence of high - leverage outliers .",
    "it is clear that to apply the rr+o approach in practice , determining the number of discarded observations is critical : a number smaller than @xmath284 will inevitably miss some outliers , while a number larger than @xmath284 could to lead to substantial efficiency loss .",
    "one merit of @xmath23is that the task of determining the number of outliers is formulated as a tuning parameter section problem in regularized estimation , for which many well - justified model selection criteria and procedures can be applied .",
    "model iii provides a high - dimensional example .",
    "the behaviors of rrr , rrs and rr+o are similar as before .",
    "not surprisingly , @xmath23still in general outperforms other methods in every category . in the presence of high leverage outliers , the masking rate of @xmath23increases slightly",
    "nonetheless , our experiments show that masking vanishes when snr increases or the leverage magnitude parameter @xmath285 decreases .",
    "in addition to the stock data analysis shown in section [ sec : intro ] , we present two other real data applications in this section .",
    "@xcite studied how pulp fibre characteristics impact paper quality .",
    "there were @xmath300 measurements on four pulp fibre characteristics ( @xmath301 ) and four properties of the produced papers ( @xmath302 ) .",
    "@xcite showed the existence of outliers in this dataset and demonstrated the importance of robust estimation .    although @xmath303 are not large , seeking potential dimension reduction along with robust estimation could still be beneficial .",
    "we use @xmath23with @xmath80 , where @xmath81 is a robust estimate of the error covariance matrix @xmath86 based on mm - estimation .",
    "a rank-2 model is selected by pic , showing that dimension reduction is indeed possible .",
    "there are 4 points captured as outliers , namely , observations 51 , 52 , 56 and 61 .",
    "this automatic identification agrees with the meticulous analysis in @xcite .",
    "we then use a random - splitting procedure to examine the predictive performance of various methods .",
    "the dataset is randomly split into a training set of size @xmath304 and a test set of size @xmath305 .",
    "the test error is calculated as the 40% trimmed mean squared prediction error .",
    "the trimmed mean is used here as each test set may also contain outliers @xcite .",
    "this process is repeated 100 times .",
    "the resulting average errors are @xmath306 , @xmath307 and @xmath308 , for mm , rrr and @xmath23 , respectively .",
    "therefore , @xmath23achieves the best predictive performance , by simultaneously conducting dimension reduction and outlier detection .",
    "isoprenoids are abundant and diverse in plants , and they serve many important biochemical functions and have roles in respiration , photosynthesis and regulation of growth and development in plants . to examine the regulatory control mechanisms in the gene network for isoprenoid in arabidopsis thaliana plant ,",
    "a genetic association study was conducted , and totally @xmath309 genechip microarray experiments were carried out for monitoring the gene expression levels under various experimental conditions @xcite .",
    "@xcite found experimentally strong connections between some downstream pathways and two isoprenoid biosynthesis pathways , namely mva and mep . in this multivariate regression setup ,",
    "the expression levels of @xmath310 genes from mva and mep serve as predictors , and the expression levels of @xmath311 genes from four downstream pathways , namely plastoquinone , caroteniod , phytosterol and chlorophyll , serve as the response variables . because of the relatively small sample size , we apply @xmath23with diagonal weighting to conduct robust and low rank regression analysis .",
    "a rank-5 model was selected by @xmath23with pic tuning .",
    "this dramatically reduced the number of parameters by approximately 80% compared with the least squares method .",
    "moreover , @xmath23captured two outliers , namely , samples 3 and 52 .",
    "figure [ fig : path ] shows the outlier detection paths produced by @xmath23 , i.e. , given any sample @xmath79 , the @xmath131 norm of the @xmath79th row in @xmath312 is shown for a sequence of @xmath58 values .",
    "samples 3 and 52 are distinctive",
    ". these 118 samples were obtained from various experimental conditions ; in particular , sample 3 was the only sample about arabidopsis tissue culture in a baseline experiment , and in sample 52 arabidopsis seedlings were exposed to light and dark conditions in a time - course experiment .",
    "in addition , the detection path shows that sample 27 , where rna was extracted from a root inducible system exposed to hormonal treatments , might deserve some further investigation . in comparison ,",
    "the plain rrr estimate of rank 5 is quite different .",
    "the two outliers led to surprisingly large relative changes in coefficient estimation and prediction , i.e. , @xmath313 , @xmath314 , where @xmath36 and @xmath315 denote the @xmath23and rrr estimators , respectively .    in arabidopsis thaliana data analysis.,title=\"fig:\",width=480 ]",
    ".,title=\"fig:\",width=576 ]    @xmath23gives five underlying factors driving the response genes on the downstream pathways .",
    "let @xmath316 be its svd , where @xmath317 denotes the design matrix with the two detected outliers removed .",
    "then @xmath318 gives a set of orthogonal factors and @xmath319 can be regarded the factor coefficients showing how the responses are related to the extracted factors .",
    "figure [ fig : stock ] shows the factor coefficients of the first three leading factors , which account for 86.0% of variation explained by the five factors . in each panel , the vertical lines indicate the four different pathways , and the two horizonal lines are plotted at heights @xmath320 , indicating the magnitude of the coefficients on factor @xmath321 , @xmath322 . to be specific , for each factor , the genes outside the two horizontal lines can be regarded as load highly or at least above average among all the response genes .",
    "roughly , it is seen that only caroteniod and chlorophyll genes load highly on the first factor , and thus it can be interpreted as representing mainly the joint behavior of the two pathways .",
    "the second factor shows clearly a contrast between caroteniod and phytosterol genes . in factor 3 , most of the genes with high loadings are from phytosterol alone .",
    "plastoquinone consists of only two genes and its behavior couples with that of caroteniod .",
    "these interesting findings demonstrate the distinct behaviors of the downstream pathways and their subgroup structures .",
    "more biological insights could be possibly gained by closely examining the experimental and background conditions .    in many biological studies , due to insufficient sample size",
    ", it has become a common practice to pool the samples under different experimental conditions to perform data analysis .",
    "these problems are usually high dimensional , but most high dimensional tools are vulnerable to extreme observations .",
    "we hope through this analysis , practitioners could realize the lurking danger of pca - like rank reduction techniques in real large data applications .",
    "blindly applying projection based methoeds ( like pca , cca or rrr ) in large datasets could be dangerous . to address the extremely low breakdown point , we proposed a multivariate mean - shift model for explicit outlier detection , where two popular types of regularization of sparsity and low rankness are employed .",
    "we built a universal link between the proposed @xmath23and the m - estimation , which provides a unified therapy to robustify low rank matrix estimation , and facilitates the computation and analysis of reduced rank m - estimators in high dimensions .",
    "we were able to develop nonasymptotic statistical theory in the challenging multivariate setting .",
    "for example , we showed that @xmath23can attain the minimax optimal error rate if the associated @xmath118 corresponds to a bounded nonconvex penalty ; only in some easy problems do convex methods come with the same low error rate guarantee .",
    "we hope the results could to some extend push forward classic robust analysis , which are either worst - case studies or large-@xmath0 asymptotics , into finite - sample studies with possible large @xmath2 and/or @xmath1 .",
    "our real data experiments demonstrated the efficacy of the proposed approaches in joint accommodation and identification of outliers in the process of rank reduction .",
    "let @xmath323 = \\ { 1 , \\cdots , n\\}$ ] . recall that @xmath42 gives the orthogonal projection onto the column space of @xmath41 .",
    "when there is no ambiguity , we also use it to denote the column space of @xmath41 . given @xmath324 , \\mathcal j\\subset [ p]$ ] , @xmath325 $ ] denotes a submatrix of @xmath123 by extracting the rows and columns indexed by @xmath326 and @xmath327 , respectively .",
    "[ def : threshold ] a threshold function is a real valued function @xmath328 defined for @xmath329 and @xmath330 such that ( i ) @xmath331 ; ( ii ) @xmath332 for @xmath333 ; ( iii ) @xmath334 ; ( iv ) @xmath335  for  @xmath336 .    [",
    "def : thresholdmulti ] @xmath225 is defined to be a multivariate function associated with @xmath60 : for any vector @xmath337 , @xmath338 for @xmath339 and @xmath340 otherwise . for any matrix @xmath341^t\\in { \\mathbb r}^{p\\times m}$ ] , @xmath342^t$ ] .",
    "[ def : thresholdmat ] given any threshold function @xmath343 , its matrix version @xmath344 is defined as follows @xmath345 where @xmath346 , @xmath347 , and @xmath348 are obtained from the svd of @xmath11 : @xmath349 .    finally , we describe a * quantile thresholding * @xmath350 which , though not satisfying definition [ def : threshold ] , is convenient to use in analyzing constraint - type problems . it can be seen as a vector variant of the hard - ridge thresholding @xmath351 @xcite . given @xmath352 and @xmath353 ,",
    "@xmath354 is defined for any @xmath355 such that the @xmath182 largest components of @xmath356 ( in absolute value ) are shrunk by a factor of @xmath357 and the remaining components are all set to be zero . in the case of ties ,",
    "a random tie breaking rule is used .",
    "@xmath358 is shorthand notation for @xmath359 .        from the proof of proposition 2.1 in @xcite ,",
    "the following results are obtained : ( i ) any optimal solution @xmath363 must satisfy @xmath364 ( implied by von neumann s trace inequality @xcite ) , and so @xmath365 always solves ; ( ii ) @xmath366 gives a global minimizer of , and @xmath367 for any @xmath363 , where @xmath368 represents the nuclear norm and @xmath369 is a function dependent on the regularization parameter only .",
    "hence it suffices to study the breakdown point of @xmath370 .    because @xmath52 , there must exist @xmath371 $ ] such that the @xmath79th column of @xmath372 is not @xmath340 .",
    "let @xmath373 . where @xmath374 is the unit vector with the @xmath79th entry being @xmath256 .",
    "then @xmath375 as @xmath376 , by the construction of @xmath377 and the positive - definiteness of @xmath15 .",
    "since @xmath378 thresholds the singular values of @xmath379 ( with @xmath58 fixed ) , while the sum of all its singular values can be made arbitrarily large as @xmath380 varies , we must have @xmath381 .",
    "[ uniqsol - gen - grp ] given an arbitrary thresholding rule @xmath60 ( cf .",
    "definition [ def : threshold ] ) , let @xmath59 be any function associated with @xmath60 through for some nonnegative @xmath382 satisfying @xmath383 for all @xmath384 . then , @xmath385 gives a globally optimal solution to the minimization problem @xmath386          without loss of generality , assume @xmath393 . from the definition of @xmath118 and @xmath394 , @xmath395 and @xmath396 .",
    "it suffices to show that @xmath397 in fact , by changing the order of integration , and due to the monotone property of @xmath60 , we get @xmath398 the identity thus follows .",
    "we now have the pieces in place to prove part ( i ) of the theorem . without loss of generality ,",
    "assume @xmath290 .",
    "let @xmath399 , @xmath400 .",
    "by lemma [ uniqsol - gen - grp ] , fixing @xmath11 , @xmath401 with @xmath402 gives an optimal solution to @xmath403 .",
    "for this @xmath401 , @xmath404 by lemma [ thresh - identity ] .",
    "the equivalence thus follows .        throughout the proof ,",
    "we use @xmath181 , @xmath182 , @xmath280 to denote universal constants . they are not necessarily the same at each occurrence .",
    "given any matrix @xmath41 , we use @xmath405 and @xmath406 to denote its column space and row space , respectively .",
    "recall @xmath42 .",
    "denote by @xmath407 the projection onto its orthogonal complement .",
    "recall that @xmath408 , @xmath409 , @xmath410 for convenience , @xmath411 is used to stand for @xmath412 , and @xmath413 and @xmath414 are used similarly . by definition , @xmath128 satisfies the following inequality for any @xmath415 @xmath416 here , @xmath417 , @xmath418 and so @xmath201 .",
    "[ lemma : phostochastic ] for any given @xmath419 , define @xmath420 . then there",
    "exist universal constants @xmath421 such that for any @xmath422 , the following event @xmath423\\ } \\geq a   \\sigma^2 t\\end{aligned}\\ ] ] occurs with probability at most @xmath424 , where @xmath425 and @xmath426 .",
    "[ concengauss ] given @xmath432 , @xmath433 , @xmath178 , define @xmath434 ) \\mbox { for some } \\mathcal j : | \\mathcal j|=j\\}$ ] .",
    "let @xmath435 .",
    "then for any @xmath426 , @xmath436 where @xmath437 are universal constants .",
    "let @xmath438 $ ] . with @xmath439",
    "defined in lemma [ lemma : phostochastic ] , we set @xmath440 then the stochastic term can bounded in the following way @xmath441 + r \\\\ \\leq & \\frac{2}{a } m({{\\boldsymbol{b}}}-{{\\boldsymbol{b}}}^ * , { { \\boldsymbol{c}}}- { { \\boldsymbol{c}}}^ * ) +   \\frac{2}{a } m(\\hat { { \\boldsymbol{b}}}-{{\\boldsymbol{b}}}^ * , { { \\boldsymbol{c}}}- { { \\boldsymbol{c}}}^*)+2 a a_0 \\sigma^2 r(m+q ) + r+ aa_0 p_{2,h}({{\\boldsymbol{\\delta}}}^c ; { \\lambda^o}),\\end{aligned}\\ ] ] where @xmath442 due to lemma [ lemma : phostochastic ] . substituting the bound into , we have @xmath443 it remains to deal with @xmath444 .    _",
    "( i ) _ due to the sub - additivity of the concave function @xmath445 , @xmath446 theorem [ th_oracle ] can be obtained by choosing @xmath447 and @xmath448 with @xmath449 . + _",
    "( ii ) _ when @xmath59 is the @xmath109 penalty , as in theorem [ th_oracle - l1 ] we apply a different treatment . again , let @xmath450 , and by the sub - additivity of @xmath59 , we have @xmath451 where @xmath452 and @xmath453 are abbreviated as @xmath327 , @xmath454 , respectively .        under the restricted eigenvalue assumption @xmath459 with @xmath460 and @xmath212 being positive constants , we have @xmath461 then , choosing the constants @xmath462 , @xmath141 and @xmath463 such that @xmath447 , @xmath464 , and @xmath465 , we obtain the desired oracle inequality . +        since @xmath471 , the occurrence of @xmath472 implies that @xmath473 from lemma [ lemma : phcomp ] and @xmath474 , this further indicates that there exists an optimal solution @xmath475 such that @xmath476 .",
    "hence @xmath477 and it suffices to show @xmath478 .",
    "note that this reduction is by no means trivial because @xmath479 .",
    "each term on the right hand side of can both be handled by lemma [ concengauss ] .",
    "take the first term as an example .",
    ". then @xmath484 by lemma [ concengauss ] , for @xmath280 large enough , @xmath485 \\leq c \\exp(-c t).\\ ] ] similarly , @xmath486 is bounded by @xmath487 with high probability , where @xmath488 and @xmath280 is a large constant .",
    "applying the union bound gives the desired result .            to compute the metric entropy @xmath492 , where @xmath493 is the smallest cardinality of an @xmath494-net that covers @xmath490 under @xmath495",
    ", we characterize each matrix in @xmath490 using its row / column spaces .",
    "in fact , given @xmath496 , its column space must be contained in @xmath497)$ ] for some @xmath327 with @xmath498 , and its row space must be contained in an @xmath16-dimensional subspace in @xmath499 .",
    "hence @xmath500 where @xmath501 and @xmath502 .",
    "@xmath86 is in a @xmath503-dimensional unit ball ( denoted by @xmath504 ) .",
    "the number of candidate @xmath505 is @xmath506 . by a standard volume argument , @xmath507 where @xmath508 is",
    "the euclidean distance in @xmath509 and @xmath510 is a universal constant .",
    "note that @xmath406 is a point on the grassmann manifold ( denoted by @xmath511 ) of all @xmath16-dimensional subspaces of @xmath499 . equipped with the metric @xmath512 given by the operator norm @xmath513 for any @xmath514",
    ", we have @xmath515 where @xmath516 is a universal constant @xcite .",
    "we claim that @xmath517 in fact , given any @xmath518 , with @xmath519 according to , find @xmath520 and @xmath521 such that @xmath522 and @xmath523 , then , for @xmath524 , @xmath525          _ case ( i ) _ @xmath528 .",
    "suppose the svd of @xmath123 is given by @xmath529 with @xmath530 of size @xmath531 .",
    "given an arbitrary estimator @xmath128 , let @xmath532 and @xmath533",
    ". then @xmath534\\\\ \\geq & \\sup _ { ( { { \\boldsymbol{a}}}^ * , { { \\boldsymbol{c}}}^ * ) \\in   \\tilde{\\mathcal s}(r , j ) } { \\,\\mathbb{p}}[\\|{{\\boldsymbol{u}}}{{\\boldsymbol{a}}}^ * - { { \\boldsymbol{u}}}\\hat{{\\boldsymbol{a}}}+ { { \\boldsymbol{c}}}^ * - \\hat{{\\boldsymbol{c}}}\\|_f^2\\geq c { p_o(j , r)}],\\end{aligned}\\ ] ] because for any @xmath535 , @xmath536 satisfies @xmath170 .",
    "the new design matrix @xmath537 has @xmath16 columns and satisfies rip condition @xmath538 .",
    "therefore , without any loss of generality we assume @xmath539 , and so @xmath540 , in the rest of the proof .",
    "consider a signal subclass @xmath541 ,   { { \\boldsymbol{c}}}={\\boldsymbol}{0 } : & \\",
    "b_{jk}=0 \\mbox { or } \\gamma r \\mbox { if }   ( j , k)\\in[q]\\times [ r/2]\\cup [ r/2]\\times [ m ] \\\\ & \\mbox { and }    b_{jk}=0 \\mbox { otherwise } \\}.\\end{aligned}\\ ] ] where @xmath542 and @xmath543 is a small constant to be chosen later .",
    "clearly , @xmath544 , @xmath545 , and @xmath546 , @xmath547 . also , since @xmath548 , @xmath549 for some universal constant @xmath182 .",
    "let @xmath550 be the hamming distance . by the varshamov - gilbert bound ( cf .",
    "lemma 2.9 in @xcite ) , there exists a subset @xmath551 such that @xmath552 for some universal constants @xmath553 .",
    "then @xmath554 .",
    "it follows from the rip assumption that @xmath555 for any @xmath556 , @xmath557 , where @xmath172 is a positive constant .    for gaussian models ,",
    "the kullback - leibler divergence of @xmath558 ( denoted by @xmath559 ) from @xmath560 ( denoted by @xmath561 ) is @xmath562 .",
    "let @xmath563 be @xmath564 . by the rip condition again , for any @xmath565 , we have @xmath566 where we used @xmath567 .",
    "therefore , @xmath568    combining and and choosing a sufficiently small value for @xmath569 , we can apply theorem 2.7 of @xcite to get the desired lower bound .",
    "+ _ case ( ii ) _ @xmath570 .",
    "define a signal subclass @xmath571^t : & \\ { { \\boldsymbol{b}}}= { \\boldsymbol}{0 } , { { \\boldsymbol{c}}}_i = { \\boldsymbol}{0 }   \\mbox { or }   \\gamma r \\cdot [ { \\boldsymbol}{1}^t , { \\boldsymbol}{b}^t]^t \\mbox { with } { \\boldsymbol}{b}\\in \\mathbb \\{0,1\\}^{\\lceil m/2\\rceil } , \\mbox { and } j({{\\boldsymbol{c}}})\\leq j \\}.\\end{aligned}\\ ] ] where @xmath572 and @xmath543 is a small constant . clearly , @xmath573 . by stirling s approximation , @xmath574 for some universal constant @xmath182 .",
    "due to lemma 8.3 in @xcite and the varshamov - gilbert bound , there exists a subset @xmath575 such that @xmath576 for some universal constants @xmath553 .",
    "the afterward treatment follows the same lines as in ( i ) and the details are omitted .",
    "peng , j. , zhu , j. , bergamaschi , a. , han , w. , noh , d .- y . ,",
    "pollack , j.  r. and wang , p. ( 2010",
    ") regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer .",
    "_ , * 4 * , 5377 .",
    "vounou , m. , nichols , t.  e. and montana , g. ( 2010 ) discovering genetic associations with high - dimensional neuroimaging phenotypes : a sparse reduced - rank regression approach",
    ". _ neuroimage _ , * 53 * , 1147  1159 .    wille , a. , zimmermann , p. , vranova , e. , furholz , a. , laule , o. , bleuler , s. , hennig , l. , prelic , a. , von rohr , p. , thiele , l. , zitzler , e. , gruissem , w. and buhlmann , p. ( 2004 ) sparse graphical gaussian modeling of the isoprenoid gene network in arabidopsis thaliana . _ genome biology _ , * 5 * , r92 + .",
    "wright , j. , ganesh , a. , rao , s. , peng , y. and ma , y. ( 2009 ) robust principal component analysis : exact recovery of corrupted low - rank matrices via convex optimization . in _ advances in neural information processing systems 22 _ ( eds .",
    "y.  bengio , d.  schuurmans , j.  lafferty , c.  k.  i. williams and a.  culotta ) , 20802088 .",
    "yuan , m. , ekici , a. , lu , z. and monteiro , r. ( 2007 ) dimension reduction and coefficient estimation in multivariate linear regression .",
    "_ journal of the royal statistical society series b _ , * 69 * , 329346 ."
  ],
  "abstract_text": [
    "<S> in high - dimensional multivariate regression problems , rank reduction is a very effective way for dimension reduction to facilitate both model estimation and interpretation . however , </S>",
    "<S> commonly used reduced rank methods are extremely non - robust against data corruption , as the low - rank dependence structure between response variables and predictors could be easily distorted in the presence of gross outliers . </S>",
    "<S> we propose a robust reduced rank regression approach for joint reduced rank modeling and outlier detection . </S>",
    "<S> the problem is formulated as a regularized multivariate regression with a sparse mean - shift parametrization , and an efficient thresholding - based iterative procedure is developed for optimization . </S>",
    "<S> we show that the proposed approach generalizes and unifies several popular robust estimation methods . </S>",
    "<S> our theoretical investigations focus on nonasymptotic robust analysis , which demonstrates that conducting rank reduction and outlier detection jointly leads to improved prediction accuracy . </S>",
    "<S> the performance of the proposed method is examined empirically by simulation studies and two real data examples .    </S>",
    "<S> keywords : low - rank matrix approximation ; sparsity ; nonasymptotic analysis ; robust estimation . </S>"
  ]
}