{
  "article_text": [
    "in the last two decades there has been intensive research on the statistical physics of neural networks @xcite .",
    "the cooperative behaviour of neurons interacting by synaptic couplings has been investigated using mathematical models which describe the activity of each neuron as well as the strength of the synapses by real numbers .",
    "simple mechanisms change the activity of each neuron receiving signals via the synapses from many other ones , and change the strength of each synapse according to presented examples on which the network is trained .    in the limit of infinitely large networks and for a set of random examples there exist mathematical tools to calculate properties of the system of interacting neurons and synapses exactly . for many models",
    "the dynamics of the network receiving continuously new examples has been described by nonlinear ordinary differential equations for a few order parameters describing the state of the system @xcite . if a network is trained on the total set of examples , the stationary state has been described by a minimum of a cost function . using methods of the statistical mechanics of disordered systems ( spin glasses ) ,",
    "the properties of the network can be described by nonlinear equations of a few order parameters .",
    "it turns out that already very simple models of neural networks have interesting properties with respect to information processing . a network with @xmath0 neurons and @xmath1 synapses",
    "can store a set of order @xmath0 patterns simultaneously .",
    "such a network functions as a content ",
    "addressable , distributed and associative memory .",
    "already a simple feedforward network with only one layer of synaptic weights can learn to classify high dimensional data .",
    "when such a network ( =  student  ) is trained on examples which are generated by a different network ( =  teacher  ) , then the student achieves overlap to the teacher network .",
    "this means that the student has not only learned the training data but it also can classify unknown input data to some extent - it generalizes . using statistical mechanics ,",
    "the generalization error has been calculated exactly as a function of the number of examples for many different scenarios and network architectures @xcite .",
    "an important application of neural networks is the prediction of time series .",
    "there are many situations where a sequence of numbers is measured and one would like to know the following numbers without knowing the rule which produces these numbers @xcite .",
    "there are powerful linear prediction algorithms including assumptions on external noise on the data , but neural networks have proven to be competitive algorithms compared to other known methods .    since 1995 the statistical physics of time series prediction has been studied @xcite .",
    "similar to the static case , the series is generated by a well known rule - usually a different  teachernetwork - and the student network is trained on these data while moving it over the series .",
    "we are interested in the following questions :    1 .",
    "how well can the student network predict the numbers of the series after it has been trained on part of it ?",
    "has the student network achieved some knowledge about the rule ( = network ) which produced the time series ?",
    "it seems to be straightforward to extend the analytic methods and results of the static classification problem to the case of time series prediction .",
    "the only difference seems to be the correlation between input vector and output bit .",
    "however , although many experts in this field looked into this problem , neither the capacity problem nor the prediction problem could be solved analytically up to now , even for the simple perceptron . furthermore , it turned out that already the problem of the generation of a time series by a neural network is not trivial .",
    "a network can produce quasiperiodic or chaotic sequences , depending on the weights and transfer functions .",
    "for some models an analytic solution has been derived , even for multilayer networks @xcite .    in this",
    "talk i intend to give an overview over the statistical physics of neural networks which generate and predict time series .",
    "firstly , i discuss the capacity problem : given a random sequence , what is the maximal length a perceptron can learn perfectly ? secondly , in section 3 a network generating binary or continuous sequences is introduced and analysed .",
    "thirdly , the prediction of quasiperiodic and chaotic sequences is investigated in section 4 . in section 5",
    "it is shown that for any prediction algorithm a sequence can be constructed for which this algorithm completely fails .",
    "section 6 considers the problem of a set of neural networks which learn from each other .",
    "this scenario is applied to a simple economic model , the minority game .",
    "finally , in section 8 it is shown that a simple perceptron can be trained to predict a sequence of bits entered by the reader , even if he / she tries to generate random bits .",
    "a neural network learns from examples . in the case of time",
    "series prediction the examples are defined by moving the network over the sequence , as shown in fig.[fig1 ] .",
    "let us consider the simplest possible neural network , the perceptron .",
    "it consists of an @xmath0dimensional weight vector @xmath2 and a transfer function @xmath3 .",
    "@xmath4 is the input vector , given by the sequence .",
    "we mainly consider two transfer functions , the boolean and the continuous perceptron :    @xmath5    @xmath6 is a parameter giving the slope of the linear part of the transfer function in the continuous case , @xmath7 .",
    "the aim of our network is to learn a given sequence @xmath8 .",
    "this means that the network should find - by some simple algorithms - a weight vector @xmath9 with the property @xmath10 for all time steps @xmath11 .",
    "for the boolean function eq.([eins ] ) this set of equations becomes a set of inequalities @xmath12 for all @xmath11 .",
    "if the bits @xmath13 in eq.([vier ] ) are random , @xmath14 , instead of taken from the time series then the inequalities ( [ vier ] ) have a solution if the number of inequalities is smaller than @xmath15 ( with probability one in the limit @xmath16 ) .",
    "this is the famous result which was found by schlfli in about 1850 and was calculated using replica theory by gardner 140 years later @xcite .    what happens if the bits @xmath13 are not independent but taken from a random time series ?",
    "let us assume that we arrange @xmath17 bits @xmath13 on a ring or , equivalently , look at @xmath18 random bits periodically repeated .",
    "for this case we ask the question : how long is the typical sequence which a boolean perceptron can learn perfectly ?    up to now there is no analytical solution of eq.([vier ] ) for this scenario , although several experts in this field have tried to solve this problem .",
    "however , detailed numerical simulations show that it is harder to learn a random sequence than random patterns : the maximal length of the sequence is @xmath19 @xcite , which should be compared with @xmath20 for random patterns .",
    "obviously tiny correlations between input vectors and output bits make the problem harder to learn for a perceptron .",
    "in the previous section the perceptron learned a short random sequence exactly .",
    "consequently it also can predict it , without errors .",
    "if a neural network is able to _ predict _ a given time series it can also _ generate _ the same series .",
    "generating means , according to fig.[fig1 ] , the network takes the last @xmath0 numbers of the sequence , calculates a new number and moves one step to the right . repeating this procedure generates a sequence @xmath21 given by eq.([drei ] ) .",
    "therefore it is interesting to study the structure of sequences generated by a neural network . here",
    "we discuss the case of _ fixed _",
    "weights @xmath9 , only .",
    "adaptive weights are considered in sections [ secantipred ] to [ sechuman ] .",
    "numerical simulations show that for random weights @xmath9 and random initial states @xmath22 the sequence has a transient initial part and finally runs into a one of several possible cycles .",
    "the structure of these cycles is related to the maxima of the fourier spectrum of the weights @xmath23 .",
    "hence it is important to understand the sequence generated by a single fourier component @xmath24 @xmath25 is an integer frequency and @xmath26 $ ] a phase of the weight vector . for a continuous perceptron we are looking for a solution @xmath27 of an infinite number of equations @xmath28.\\ ] ] for this case an analytic solution could be derived @xcite . for small values of @xmath6",
    "the attractor is zero , the sequence relaxes to @xmath29 .",
    "however , above a critical value of @xmath6 which is independent of the frequency @xmath25 , a nonzero attractor exists ; close to @xmath30 it is given by @xmath31.\\ ] ] the amplitude @xmath32 increases continuously from zero above a critical value @xmath33 therefore , the attractor of the sequence is a _ quasiperiodic _",
    "cycle with a frequency @xmath34 .",
    "the phase @xmath35 of the weights shifts the frequency of the sequence - a result which is not easy to understand without calculating it .    for a multilayer network",
    "the situation is similar : each hidden unit can contribute a quasiperiodic component to the sequence , which has its own critical point .",
    "increasing @xmath6 , more and more components are activated .",
    "this is shown in fig.[fig2 ] for a network with two hidden units : for small values of the parameter @xmath6 the quasiperiodic attractor is one  dimensional , for large @xmath6 both compoments are activated yielding a two  dimensional attractor as shown by the return map @xmath36 .",
    "the attractor dimension is limited by the number of hidden units @xcite",
    ".        if the transfer function is discrete , eq.([eins ] ) , the situation is more complex @xcite in this case we obtain a bit generator whose cycle length is limited by @xmath37 .",
    "however , numerical simulations show that the spectrum of cycle lengths has a much lower bound , namely the value @xmath15 , at least for single component weights with @xmath38 . after a transient part",
    "the bit sequence @xmath13 follows the equation @xmath39.\\ ] ] but the sequence can not follow this equation forever ; namely if a window @xmath40 appeares a second time , the perceptron has to repeat the sequence .",
    "numerical calculations show that eq.([neun ] ) , in addition to this condition , produces cycles shorter than @xmath15 .",
    "it remains a challenge to show this result analytically .    ) .",
    "the perceptron has @xmath41 input components . ]",
    "fig.[fig3 ] shows the cycle length @xmath42 of the bit generator with weights ( [ fuenf ] ) .",
    "this rather complex figure has a simple origin , it just shows the properties of rational numbers .",
    "an integer multiple of the wavelength @xmath43 given by ( [ neun ] ) @xmath44 has to fit into the cycle @xmath45 hence @xmath43 has to be a rational .",
    "the pattern @xmath46 shown in fig.3 turns out to be the numerator as a function of its rational basis .",
    "however , this does not explain why this picture is cut for @xmath47 .",
    "up to now we have discussed quasiperiodic sequences , only .",
    "but time series occurring in applications are in general more complex .",
    "therefore we are interested in the question : can a neural network generate a time series with a more complex power spectrum than a single peak and its higher harmonics ?",
    "it turns out that a multilayer network can not generate a sequence with an arbitrary power spectrum . to generate a sequence with autocorrelations which decay as a power law",
    ", one needs a fully connected asymmetric network , a more complex architecture than a feedforward network @xcite .    however",
    ", a simple perceptron can generate a chaotic sequence .",
    "when the weights have a bias , @xmath48 then there are tiny regions in the @xmath49plane where a chaotic sequence has been observed numerically @xcite .",
    "such a scenario has been called _",
    "fragile chaos_. the fractional dimension of such a chaotic sequence is between one and two , and in the vicinity of chaotic parameters @xmath49 there is always a parameter set with a quasiperiodic sequence .",
    "this situation is different for a nonmonotonic transfer function .",
    "if the function @xmath50 in ( [ zwei ] ) is replaced by @xmath51 there are large compact regions in the parameter space where the sequence is chaotic with a large fractal dimension of the order of @xmath0 .",
    "neural networks with nonmonotonic transfer functions yield high dimensional _ stable chaos _ @xcite . in this case",
    "the attractor dimension can be tuned by the parameter @xmath6 between the values one and @xmath0 .",
    "if a neural network can not generate a given sequence of numbers , it can not predict it with zero error .",
    "but this is not the whole story .",
    "even if the sequence has been generated by an ( unknown ) neural network ( the teacher ) , a different network ( the student ) can try to learn and to predict this sequence . in this context",
    "we are interested in two questions :    1 .",
    "when a student network with the identical architecture as the teacher one is trained on the sequence , how does the overlap between student and teacher develop with the number of training examples (= windows of the sequence ) ? 2 .",
    "after the student network has been trained on a part of the sequence how well can it predict the sequence several steps ahead ?",
    "recently these questions have been investigated numerically for the simple perceptron @xcite .",
    "we have to distinguish several scenarios :    1 .",
    "boolean versus continuous perceptron 2 .   on  line versus",
    "batch learning 3 .",
    "quasiperiodic versus chaotic sequence .    in all cases",
    "we consider only the stationary part of a sequence which was generated by a perceptron .",
    "the student network is trained on the stationary part only , not on the transient .",
    "first we discuss the boolean perceptron of size @xmath0 which has generated a bit cycle with a typical length @xmath52 .",
    "the teacher perceptron has random weights with zero bias , and the cycle is related to one component of the power spectrum of the weights .",
    "the student network is trained using the perceptron learning rule : @xmath53    for this algorithm there exists a mathematical theorem @xcite : if the set of examples can be generated by some perceptron then this algorithm stops , i.e. it finds one out of possibly many solutions . since we consider examples from a bit sequence generated by a perceptron , this algorithm is guaranteed to learn the sequence perfectly .",
    "on  line and batch training are identical , in this case .",
    "the network is trained on the cycle until the training error is zero .",
    "hence the student network can predict the stationary sequence perfectly .",
    "surprisingly , it turns out that the overlap between student and teacher is small , in fact it is zero for infinitely large networks , @xmath54 .",
    "the network learns the projection of the teacher s weight vector onto the sequence , but not the complete vector .",
    "it behaves like a filter selecting one of the components of the power spectrum of the weights .",
    "although it predicts the sequence perfectly , it does not gain much information on the rule which generates this sequence .",
    "this situation seems to be different in the case of a continuous perceptron .",
    "inverting eq.([drei ] ) for a monotonic transfer function @xmath55 gives @xmath0 linear equations for @xmath0 unknowns @xmath56 .",
    "if the stationary part of the sequence is either quasiperiodic or chaotic , all patterns are different and the batch training , using @xmath0 windows , leads to perfect learning .",
    "this holds true for a chaotic time series .",
    "however , for a quasiperiodic one ( eq.([sieben ] ) ) the patterns are almost linearly dependent , yielding an ill  conditioned set of linear equations . without the @xmath57 in eq.([sieben ] ) , one would obtain a two  dimensional space of patterns ; with the nonlinearity one obtains small contributions in the other @xmath58 dimensions of the weight space .",
    "nevertheless , depending on the parameter @xmath59 , even professional computer routines sometimes do not succeed in solving eq.([drei ] ) for quasiperiodic patterns generated by a teacher perceptron .",
    "how does this scenario show up in an on  line training algorithm for a continuous perceptron ?",
    "if a quasiperiodic sequence is learned step by step without iterating previous steps , using gradient descent to update the weights , @xmath60 then one can distinguish two time scales ( time = number of training steps ) :    1 .",
    "a fast one increasing the overlap between teacher and student to a value which is still far away from the value one which corresponds to perfect agreement .",
    "2 .   a slow one increasing the overlap very slowly . numerical simulations for millions times @xmath0 training steps yielded an overlap which was still far away from the value one .    although there is a mathematical theorem on stochastic optimization which seems to guarantee convergence to perfect success @xcite , our on  line algorithm can not gain much information about the teacher network .",
    "it would be interesting to know how these two time scales depend on the size of the system .",
    "in addition we can not exclude that there exist on - line algorithms which can learn our ill - conditioned problem in short times .",
    "this is completely different for a chaotic time series generated by a corresponding teacher network with @xmath61 .",
    "it turns out that the chaotic series appears like a random one : after a number of training steps of the order of @xmath0 the overlap relaxes exponentially fast to perfect agreement between teacher and student .",
    "hence , after training the perceptron with a number of examples of the order of @xmath0 we obtain the two cases : for a quasiperiodic sequence the student has not obtained much information about the teacher , while for a chaotic sequence the student s weight vector comes close to the one of the teacher .",
    "one important question remains : how well can the student predict the time series ?",
    "fig.[figpred ] shows the prediction error as a function of the time interval over which the student makes the predictions .",
    "the student network which has been trained on the quasiperiodic sequence can predict it very well .",
    "the error increases linearly with the size of the interval , even predicting @xmath62 steps ahead yields an error of about 10% of the total possible range . on the other side",
    ", the student trained on the chaotic sequence can not make predictions .",
    "the prediction error increases exponentially with time ; already after a few steps the error corresponds to random guessing , @xmath63 .    in summary",
    "we obtain the surprising result :    1 .",
    "a network trained on a quasiperodic sequence does not obtain much information about the teacher network which generated the sequence .",
    "but the network can predict this sequence over many ( of the order of @xmath0 ) steps ahead .",
    "2 .   a network trained on",
    "a chaotic sequence obtains almost complete knowledge about the teacher network . but this network can not make reasonable predictions on the sequence",
    ".    it would be interesting to find out whether this result also holds for other prediction algorithms , such as multilayer networks .",
    "consider some arbitrary prediction algorithm .",
    "it may contain all the knowledge of mankind , many experts may have developed it .",
    "now there is a bit sequence @xmath64 and the algorithm has been trained on the first @xmath11 bits @xmath65 .",
    "can it predict the next bit @xmath66 ? is the prediction error , averaged over a large @xmath11 interval , less than 50% ?    if the bit sequence is random then every algorithm will give a prediction error of 50% .",
    "but if there are some correlations in the sequence then a clever algorithm should be able to reduce this error .",
    "in fact , for the most powerful algorithm one is tempted to say that for _ any _ sequence it should perform better than 50% error . however , this is not true @xcite . to see this",
    "just generate a sequence @xmath67 using the following algorithm : +    now , if the same algorithm is trained on this sequence , it will always predict the following bit with 100% error .",
    "hence there is no general prediction machine ; to be successful for a class of problems the algorithm needs some preknowledge about it .",
    "the boolean perceptron is a very simple prediction algorithm for a bit sequence , in particular with the on  line training algorithm ( [ dreizehn ] )",
    ". how does the bit sequence look like for which the perceptron completely fails ?",
    "following ( [ dreizehn ] ) we just have to take the negative value @xmath68 and then train the network on this new bit @xmath69 the perceptron is trained on the opposite (= negative ) of its own prediction .",
    "starting from ( say ) random initial states @xmath70 and weights @xmath9 , this procedure generates a sequence of bits @xmath71 and of vectors @xmath72 as well . given this sequence and the same initial state , the perceptron which is trained on it yields a prediction error of 100% .",
    "it turns out that this simple algorithm produces a rather complex bit sequence which comes close to a random one . after a transient time the weight vector @xmath73 performs a kind of random walk on a @xmath0dimensional hypersphere .",
    "the bit sequence runs to a cycle whose average length @xmath74 scales exponentially with @xmath0 , @xmath75 the autocorrelation function of the sequence shows complex properties : it is close to zero up to @xmath0 , oscillates between @xmath0 and @xmath76 and it is similar to random noise for larger distances . its entropy is smaller than the one of a random sequence since the frequency of some patterns is suppressed .",
    "of course , it is not random since the prediction error is 100% instead of 50% for a random bit sequence .",
    "when a second perceptron ( = student ) with different initial state @xmath9 is trained on such an antipredictable sequence generated by eq.([dreizehn ] ) it can perform somewhat better than the teacher : the prediction error goes down to about 78% but it is still larger than 50% for random guessing .",
    "however , the student obtains knowledge about the teacher : the angle between the two weight vectors relaxes to about 45 degrees .",
    "in the previous section we have discussed a neural network which learns from itself . but more interesting may be the scenario where several networks are interacting , learning from each other .",
    "after all , our living world consists of interacting adaptive systems and recent methods of computer science use interacting agents to solve complex problems .",
    "here we consider a simple system of interacting perceptrons as a first example to develop a theory of cooperative behaviour of adaptive agents .",
    "consider @xmath25 boolean perceptrons , each of which has an @xmath0dimensional weight vector @xmath77 .",
    "each perceptron is receiving the same input vector @xmath78 and produces its own output bit @xmath79 now these networks receive information from their neighbours in a ring  like topology : perceptron @xmath80 is trained on the output @xmath81 of perceptron @xmath82 , and @xmath83 is trained on @xmath84 .",
    "training is performed keeping the length of the weight vectors fixed : @xmath85 the learning rate @xmath86 is a parameter controlling the speed of learning .",
    "this problem has been solved analytically in the limit @xmath87 @xcite for random inputs .",
    "the system relaxes to a stationary state , where the angles @xmath88 ( or overlaps ) between different agents take a fixed value . for small learning rate @xmath86",
    "all of these angles are small , i.e. there is good agreement between the agents .",
    "but more surprising : the state of the system is completely symmetric , there is only one common angle @xmath89 between all pairs of networks .",
    "the agents do not recognize the clockwise flow of information .",
    "increasing the lerning rate @xmath86 the common angle @xmath90 increases , too . with",
    "larger learning steps each agent tends to have an opinion opposite to all of its colleagues .",
    "but , due to the symmetry , there is maximal possible angle given by @xmath91 in fact , increasing @xmath86 the system arrives at this maximal angle at some critical value @xmath92 . for larger value of @xmath93",
    "the system undergoes a phase transition : the complete symmetry is broken , but the symmetry of the ring is still conserved : @xmath94 for @xmath25 agents there are @xmath95 values of @xmath96 possible if @xmath25 is odd , and @xmath97 values for even @xmath25 .",
    "this is a simple - but analytically solvable - example of a system of interacting neural networks .",
    "we observe a symmetry breaking transition when increasing the learning rate .",
    "however , this system does not solve any problem . in the following section we will extend this scenario to a case where indeed neural networks interact to solve a special problem , the minority game .",
    "recently a mathematical model of economy receives a lot of attention in the community of statistical physics @xcite . it is a simple model of a closed market :",
    "there are @xmath25 agents who have to make a binary decision @xmath98 at each time step .",
    "all of the agents who belong to the minority gain one point , the majority has to pay one point ( to a cashier which always wins ) .",
    "the global loss is given by @xmath99 if the agents come to an agreement before they make a new decision , it is easy to minimize @xmath100 agents have to choose @xmath101 , then @xmath102 .",
    "however , this is not the rule of the game , the agents are not allowed to cooperate .",
    "each agent knows only the history of the minority decision , @xmath103 , but otherwise he / she has no information .",
    "can the agent find an algorithm to maximize his / her profit ?",
    "if each agent makes a random decision , then @xmath104 is of the order of @xmath105 .",
    "it is not easy to find algorithms which perform better than random @xcite .",
    "here we use a perceptron for each agent to make a decision based on the past @xmath0 steps @xmath106 of the minority decision .",
    "the decision of agent @xmath80 is given by @xmath107 after the bit @xmath13 of the minority has been determined , each perceptron is trained on this new example @xmath108 , @xmath109 this problem could be solved analytically @xcite .",
    "the average global loss for @xmath110 is given by @xmath111 hence , for small enough learning rates the system of interacting neural networks performs better than random decisions .",
    "a pool of adaptive perceptrons can organize itself to yield a successful cooperation .",
    "as a final example of a perceptron predicting a bit sequence we discuss a real application .",
    "assume that the bit sequence @xmath112 is produced by a human being .",
    "now a simple perceptron ( [ eins ] ) with on  line learning ( [ dreizehn ] ) takes the last @xmath0 bits and makes a prediction for the next bit .",
    "then the network is trained on the new true bit , which afterwards appears as part of the input for the following prediction .",
    "eq.([dreizehn ] ) is a simple deterministic equation describing the change of weights according to the new bit and the past @xmath0 bits .",
    "can such a simple equation foresee the reaction of a human being ? on the other side , if a person can calculate or estimate the outcome of eq .",
    "( [ dreizehn ] ) , then he / she can just do the opposite , and the network completely fails to predict .    to answer these questions",
    "we have written a little c program which receives the two bits 0 and 1 from the keyboard @xcite .",
    "the program needs two fields neuron and weight which contain the variable @xmath113 and @xmath56 , respectively . here",
    "are the main steps :    1 .",
    "repeat : + while ( 1 ) \\ { 2 .",
    "calculate the vector product @xmath114 : + for ( h=0 ; i=0 ; i < n ; i++ ) h+=weight[i]*neuron[i ] ; 3 .   read a new bit : + if(getchar()==1 )  input=1 ; else input = -1 ; 4 .",
    "calculate the prediction error : + if(h*input<0 ) \\{error + + ; 5 .",
    "train : + for(i=0 ; i <",
    "n ; i++ ) weight[i]+=input * neuron[i]/(double)n ; } 6 .   shift the input window : + for(i = n-1 ; i>0 ; i ) neuron[i]=neuron[i-1 ] ; neuron[0 ] = input ; }    a graphical version of this program can be accessed over the internet : +    http://theorie.physik.uni-wuerzburg.de/ kinzel    now we ask a person to generate a bit sequence for which the prediction error of the network is high .",
    "we already know from section 2 what happens if the candidate produces a rhythm : if its length is smaller than @xmath115 the perceptron can learn it perfectly , without errors .",
    "hence the candidate should either produce random numbers which give 50% errors or he / she should try to calculate the prediction of the perceptron , in this case an error higher than 50% is possible .",
    "we have tested this program on students of our class .",
    "each student had to send a file with one thousand bits , generated by hand .",
    "it turns out that on average the network predicts with an error of about 35% .",
    "the distribution of errors is broad with a range between 20% and 50% .",
    "apparently , a human being is not a good random number generator .",
    "the simple perceptron ( [ eins ] ) and ( [ dreizehn ] ) succeeds in predicting human behaviour !",
    "some students submitted sequences with 50% error .",
    "it was obvious - and later confessed - that they used random number generators , digits of @xmath116 , the logistic map , etc . instead of their own fingers .",
    "one student submitted a sequence with 100% error .",
    "he was the supervisor of our computer system , knew the program and submitted the sequence described in section 5 .",
    "the theory of time series generation and prediction is a new field of statistical physics .",
    "the properties of perceptrons , simple single  layer neural networks being trained on sequences which were produced by other perceptrons , have been studied .",
    "a random bit sequence is more difficult to learn perfectly than random uncorrelated patterns .",
    "an analytic solution of this capacity problem is still missing .",
    "a multilayer network can be used to generate time series . for the continuous transfer function",
    "an analytic solution of the stationary part of the sequence has been found .",
    "the sequence has a dimension which is bounded by the number of hidden units .",
    "it is not completely clear yet how to extend this solution to the case of a boolean perceptron generating a bit sequence . for nonmonotonic transfer functions the network generates a chaotic sequence with a large fractal dimension .",
    "a perceptron which is trained on a quasiperiodic sequence can predict it very well , but it does not obtain much information on the rule generating the sequence . on the other side , for a chaotic sequence",
    "the overlap between student and teacher is almost perfect , but prediction of the sequence is not possible .    for any prediction algorithm",
    "there is a sequence for which it completely fails . for a simple perceptron such",
    "a sequence is rather complex , with huge cycles and low autocorrelations .",
    "another perceptron which is trained on such a sequence reduces the prediction error from 100% to 78% and obtains overlap to the generating network .",
    "finally it has been demonstrated that human beings are not good random number generators .",
    "even a simple perceptron can predict the bits typed by hand with an error of less than 50% . + * acknowledgement : *                                              w. kinzel , r. metzler , i. kanter , _ dynamics of interacting neural networks _ , j. phys .",
    "a * 33 * l141-l147 ( 2000 ) ; + r. metzler and w. kinzel and i. kanter : _ interacting neural networks _ , phys .",
    "e * 62 * 2 , 2555 ( 2000 )"
  ],
  "abstract_text": [
    "<S> an overview is given about the statistical physics of neural networks generating and analysing time series . </S>",
    "<S> storage capacity , bit and sequence generation , prediction error , antipredictable sequences , interacting perceptrons and the application on the minority game are discussed . finally , as a demonstration a perceptron predicts bit sequences produced by human beings . </S>"
  ]
}