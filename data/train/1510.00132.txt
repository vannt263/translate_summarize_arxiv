{
  "article_text": [
    "the lhcb collaboration is one of the four major experiments at the large hadron collider at cern .",
    "the detector , as well as the monte carlo simulations of physics events , create vast amount of data every year .",
    "this data is kept on disk and tape storage systems .",
    "disks are used for storing data used by physicists for analysis .",
    "they are much faster than tapes , but are way more expensive and hence disk space is limited .",
    "therefore it is highly important to identify which datasets should be kept on disk and which ones should only be kept as archives on tape .",
    "currently , the data volumes on disk and tape are about 10.5 pb and 1.5 pb respectively . the algorithm presented here",
    "is designed to select the datasets which may be used in the future and thus should remain on disk .",
    "input information to the algorithm are the dataset usage history and dataset metadata ( size , type , configuration etc . ) .",
    "the algorithm consists of three separate modules .",
    "the first one is the data popularity estimator .",
    "this module predicts the dataset future popularity by applying a machine learning algorithm to the algorithm s input information .",
    "the data popularity represents the probability for a dataset to be useful in future .",
    "based on data popularity it is possible to identify which datasets can be removed from disk .",
    "the second module is the data intensity predictor .",
    "this module is needed to predict the future usage intensity of each dataset .",
    "time series analysis and regression algorithms are used to make these predictions .",
    "input information for this module is the dataset usage history .",
    "the third module is the data placement optimizer . in this module",
    "the data popularity and the predicted future usage intensities are used to estimate which datasets should be kept on disk and how many replicas they should have . for this purpose a loss function minimization problem is solved .",
    "the loss function represents all requirements for data distribution in the data storage system .",
    "these three modules are described in detail in the following sections . in the results section",
    "we then show a comparison of our algorithm with a simple last recently used ( lru ) algorithm .",
    "a data management algorithm for hybrid hard disk drive ( hdd ) + solid - state drive ( ssd ) data storage system is described in [ 2 ] .",
    "the authors presents a method that shuffles datasets across storage tiers to optimize the data access performance .",
    "the method uses markov chains[1 ] to predict the popularity of dataset accesses .",
    "the dataset placement optimization problem is solved based on the dataset accesses popularity .",
    "a popularity - based prediction and data redistribution tool for the atlas distributed data management is presented in [ 3,4 ] .",
    "the authors use artificial neural networks ( ann)[1 ] to predict possible dataset accesses in the near - term future based on the dataset usage history .",
    "then these predictions are used to redistribute data on the grid , i.e. , adding and removing replicas .",
    "a feature of our study is that dataset usage history in lhcb has a rather low statistics .",
    "the data management algorithm from [ 2 ] needs more statistics for a good performance .",
    "the artificial neural networks from articles [ 3,4 ] are too complicated for our data and as a consequence an overfitting problem[1 ] may occur .",
    "dataset usage history and metadata are used as input information to the algorithm . in this study",
    "we use weekly dataset usage counters collected over the last two years .",
    "dataset usage history represents as time series of 104 points .",
    "each point represents the number of dataset usages during one week ( i.e. the number of files accessed by grid jobs divided by the number of files in the dataset ) .",
    "the dataset metadata contains additional dataset information likes : the origin , the detector configuration , the file type , the data type ( monte carlo simulations or real data ) , the event type , the creation week , the first usage week , the last usage week , the size for one replica , the total size of occupied disk space , the number of replicas on disk and some others .",
    "the algorithm takes as input a file which contains the dataset usage history and the dataset metadata .",
    "this file comes from the file catalogue .",
    "the data popularity estimator module uses a classifier to calculate the data popularity .",
    "the classifier is a supervised machine learning algorithm[1 ] and consists of several steps .",
    "the following subsections describe each step of data popularity estimation .",
    "as the classifier is a supervised machine learning algorithm , each dataset should be labelled as popular or unpopular .",
    "the time series of dataset usage history are very sparse , therefore the last 26 weeks of usage history are used to label the data . if a dataset has not be used during the last 26 weeks we label it as unpopular and assign it a label value `` 1 '' .",
    "otherwise , the dataset is labelled as popular with a label value `` 0 '' .",
    "this label defines the class of the dataset ( 0 for popular , 1 for unpopular ) .",
    "the figures 1 and 2 show each the time series of one dataset of each class .",
    "time series with label `` 1 '' . ]    time series with label `` 1 '' . ]",
    "the dataset metadata are used as input parameters for a classifier .",
    "some new parameters are computed and used in the analysis as well as the existing ones .",
    "these factors describe the shape of the time series of the dataset usage history .",
    "while the last 26 weeks of the time series are used to label the datasets , the first 78 weeks are used to compute these new parameters .",
    "these parameters are _",
    "nb_peaks _ , _ last_zeros _ , _ inter_max _ , _",
    "inter_mean _ , _ inter_std _ , _ inter_rel _ , _ mass_center _ , _ mass_center_sqrt _ , _",
    "mass_moment _ and _",
    "r_moment_.    _ nb_peaks _ is the number of weeks during which a dataset has been used .",
    "_ last_zeros _ is the number of weeks since when the dataset was last used .",
    "_ inter_max _ , _",
    "inter_mean _ , _ inter_std _ are the maximum value , the mean value and the standard deviation of the number of weeks between consecutive weeks of usage .",
    "_ inter_rel _ is the ratio of the _ inter_std _ and _ inter_mean _ values .",
    "_ mass_center _ is the center of gravity of a time series for a dataset , where the `` mass '' is the number of accesses to the dataset for each week .",
    "_ mass_center_sqrt _ , _ mass_moment _ and _ r_moment _ are similar to _ mass_center _ , but `` mass '' and `` coordinate '' have different degrees .",
    "these parameters significantly increase the classifier s quality .      the new parameters , the dataset metadata and their labels are used to train a _ gradient boosting classifier_[1 ] .",
    "all datasets are split into two equal halves , i.e. half the datasets goes to the first halve , the other to the second one .",
    "the classifier is trained on one half of the datasets and then is used to predict probabilities to have label `` 1 '' for the second half of the datasets .",
    "the figure 3 shows the distribution of the probabilities for each class of datasets .",
    "the probability described previously is then transformed into a popularity estimator such that the popularity for datasets which have label `` 1 '' is uniform . the closer the popularity is to 1 the higher is the probability that it will be unused in the future . in this sense",
    "it is rather an unpopularity estimator .",
    "the figure 4 represents the distribution of the popularity for each dataset class .",
    "distributions of the popularity for each dataset class . ]",
    "distributions of the popularity for each dataset class . ]",
    "the data popularity represents the probability that a dataset will be unused in the future . another important feature is predicted dataset usage intensity .",
    "there is a number of time series analysis algorithms that predict future values of time series . since time series in this study",
    "have lack of statistics , parametric models such as polynomial regression , autoregression , arma and arima models , artificial neuron networks ( anns ) and others are not suitable .",
    "this section shows how to use two non - parametric models to predict the dataset usage intensities .",
    "these models are nadaraya - watson kernel smoothing[1 ] and rolling mean values[1 ] .",
    "let points @xmath0 represent a time series and @xmath1 .",
    "then , the _ nadaraya - watson _ equation for kernel smoothing is : @xmath2 where    @xmath3 is the time series value at @xmath4 after kernel smoothing of @xmath5 values ,    @xmath6 is the rbf smoothing kernel ,    @xmath7 is the smoothing window width . for the smoothing window width optimization the _ leave - one - out_[1 ] method was applied : @xmath8 the _ nadaraya - watson _ equation for kernel smoothing with _ loo _",
    "smoothing window width optimization is applied to time series of dataset usage history .",
    "the maximum smoothing window width is 30 weeks .",
    "the figure 5 shows an example of time series after this smoothing is applied .      on the next step",
    "rolling mean values are calculated for additional smoothing of time series of dataset usage history .",
    "let points @xmath9 represent a time series after the kernel smoothing . then , rolling mean values are defined as : @xmath10 where @xmath11 is the width of the moving window .",
    "the window width is chosen such that 90% of all time series with equal _",
    "nb_peaks _ values have _ inter_max _ values less than the window width .",
    "the rolling mean value at moment @xmath12 represents the dataset usage intensity at that moment .",
    "the simplest way to predict future dataset usage intensity is to take dataset usage intensity on last observation as future one .",
    "an example of calculated rolling mean values and predicted dataset usage intensity are shown on the figure 5 .",
    "dependence of optimal number of dataset replicas ( @xmath13 ) from its predicted usage intensity ( @xmath14 ) and @xmath15 . ]",
    "dependence of optimal number of dataset replicas ( @xmath13 ) from its predicted usage intensity ( @xmath14 ) and @xmath15 . ]",
    "this section describes how one can estimate which dataset should be kept on disk and how many replicas they should have using the popularity and the predicted usage intensity for this dataset .",
    "since disk space is more expensive than tapes , we would like to take a minimum of disk space .",
    "but on the other hand it is highly undesirable to remove from disk datasets which will be used in future .",
    "additionally , we would like to create more replicas for the most popular datasets in order to reduce their average access time .",
    "the requirements above are represented by the following loss function : @xmath16    @xmath17 - cost of 1 gb disk storage ,    @xmath18 - cost of 1 gb tape storage ,    @xmath19 - cost of restoring 1 gb data from tape to disk ,    @xmath15 - penalty for low number of replicas ,    @xmath20 - size of one replica of @xmath21 dataset ,    @xmath22 - number of replicas of @xmath21 dataset ,    @xmath23 - predicted usage intensity of @xmath21 dataset ;    @xmath24 is equal to 1 if @xmath21 dataset is on disk , otherwise it is 0 ;    @xmath25 is equal to 1 if @xmath21 dataset was restored from tape to disk .",
    "the first term of the loss function represents the cost of storage of the datasets on disk .",
    "the second term is the cost of storage of the datasets on tape .",
    "the last term is the cost of mistakes , when a dataset was removed from disk but then is used .",
    "the expression in brackets in the first term of the loss function is used to find the optimal number of replicas for datasets on disk based on predicted usage intensities .",
    "the optimal number of replicas for a dataset with predicted usage intensity of @xmath23 and for the value @xmath15 is @xmath26{\\alpha i_{i}},\\ ] ] the figure 6 shows how the optimal number of replicas for a dataset depends on its predicted usage intensity and alpha value . for example , suppose the predicted usage intensity for a dataset is @xmath27 usages per week and @xmath28",
    ". then @xmath29 replicas .",
    "the @xmath24 value in the loss function depends on the data popularity threshold value .",
    "datasets with popularities equal to or higher than this threshold value are removed from disk ( @xmath30 ) .",
    "the @xmath25 value is the product of the @xmath31 and the label of @xmath21 dataset ( 0 or 1 ) .",
    "the loss function optimization consists in finding the data popularity threshold value and dataset optimum number of replicas that provide the minimum value of the loss function .",
    "in this article we compare our algorithm with the last recently used ( lru ) algorithm .",
    "the lru algorithm takes the last observations of the dataset usage history and decides which dataset should be removed from disk . in this",
    "study the first 78 weeks usage history time series are used as the algorithm inputs .",
    "the last 26 weeks are used to measure the quality of the algorithm .",
    "thus if a data set was not used during the last @xmath32 weeks ( from @xmath33 to @xmath34 weeks ) , this dataset is removed from disk . the number of disk replicas are not changed compared to the original number of replicas .",
    "the following function is used to estimate the time of downloading of all datasets by all users ( the generic term downloading is used to represent an access to the dataset from a job ) : @xmath35    where @xmath36    @xmath37 - average time of downloading 1 gb of data from disk ,    @xmath38 - average time of downloading 1 gb of data from tape to disk ,    @xmath39 - constant time needed to restore a dataset from tape to disk ,    @xmath40 - average number of downloading of a dataset per week ,    @xmath20 - size of one replica of @xmath21 dataset ,    @xmath22 - number of replicas of @xmath21 dataset ,    @xmath24 is equal 1 if the @xmath21 dataset is on disk , otherwise it is 0 ,    @xmath25 ( misclassification ) is equal 1 if @xmath21 dataset has to be restored from tape to disk .",
    "the first term of the downloading time equation represents the time of download of all datasets from disk by all users .",
    "the second term represents the time needed to restore from tape datasets that were removed from disk due to an algorithm s bad decision .",
    "the third term represents the time of download of restored datasets by all users.the first 78 weeks of the dataset usage history time series are used as algorithms inputs .",
    "the last 26 weeks are used to measure the quality of the algorithms and to estimate how many times the datasets were downloaded .",
    "datasets which were created and first used earlier than @xmath34 week are used to compare algorithms .",
    "the total number of datasets used for the comparison is 7375 . in this paper",
    "we use rather pessimistic values of the parameters to emphasize that the disk space is highly limited .",
    "the following values of the parameters are used to optimize the loss function : @xmath41 , @xmath42 , @xmath43 .",
    "the values of the parameters for the downloading time function are @xmath44 hour / gb , @xmath45 hours / gb and @xmath46 hours .",
    "@xmath47 represents an idea that the disk space is limited .",
    "@xmath48 means the number of restored datasets should be minimal .",
    "@xmath49 and large @xmath39 value show that a dataset restoring from tape to disk takes a lot of time .",
    "tables 1 and 2 show results for our algorithm with 4 maximum dataset number of replicas and for the lru algorithm .",
    "_ downloading time ratio _ is the ratio of the downloading time after applying the algorithm to the original downloading time .",
    "_ saving space _",
    "column shows how much disk space can be saved using this algorithm .",
    "_ nb of wrong removings _ column represents the number of datasets which are proposed to be removed from disk but are then used again in the future .",
    "both algorithms save about the same amount of disk space , but our algorithm has an extremely low number of mistakes .",
    "the tables show that our algorithm with 4 maximum dataset number of replicas slightly decreases the download time .",
    "table 3 demonstrates that for a maximum number of replicas of 7 our algorithm helps to save up to 40% of disk space and decreases the downloading time by up to 30% .",
    "llll n&downloading time ratio&saving space , % & nb of wrong removings + 1&1.33&63&1973 + 2&1.28&58&1659 + 5&1.4&50&1357 + 10&1.11&44&966 + 15&1.07&38&635 +",
    "20&1.03&33&370 + 25&1.02&30&193 +    llll alpha&downloading time ratio&saving space , % & nb of wrong removings + 0&3.35&71&9 + 0.01&0.99&46&9 + 0.05&0.96&34&9 + 0.1&0.96&30&9 + 0.5&0.96&23&9 + 1&0.96&19&9 + 2&0.96&16&9 +    llll alpha&downloading time ratio&saving space , % & nb of wrong removings + 0&3.35&71&8 + 0.001&1.03&57&8 + 0.005&0.72&40&8 + 0.01&0.68&34&8 + 0.05&0.63&11&8 + 0.1&0.62&1&8",
    "+    a python module implementing our algorithm and its web service can be downloaded from [ 5 ] .",
    "our study is performed by means of a reproducible experiment platform[6 ] - environment for conducting data - driven research in a consistent and reproducible way .",
    "in this paper , we presented a study of developing the algorithm for disk storage management .",
    "the method presented here demonstrates how the algorithms of machine learning , regression and time series analysis can be used in data management of the lhcb data storage system .",
    "the results shows that our algorithm helps to save a significant amount of disk space and reduce the average downloading time .",
    "9      lipeng w , zheng l , qing c , feiyi w , sarp o , bradley s 2014 _ @xmath50 symposium on mass storage systems and technologies ( msst ) : ssd - optimized workload placement with adaptive learning and classification in hpc environments _ ( california : ieee )    beermann t , stewart a , maettig p 2014 _ the international symposium on grids and clouds ( isgc ) 2014 : a popularity - based prediction and data redistribution tool for atlas distributed data management _ ( pos ) p 4"
  ],
  "abstract_text": [
    "<S> this paper presents an algorithm providing recommendations for optimizing the lhcb data storage . </S>",
    "<S> the lhcb data storage system is a hybrid system . </S>",
    "<S> all datasets are kept as archives on magnetic tapes . </S>",
    "<S> the most popular datasets are kept on disks . </S>",
    "<S> the algorithm takes the dataset usage history and metadata ( size , type , configuration etc . ) to generate a recommendation report . </S>",
    "<S> this article presents how we use machine learning algorithms to predict future data popularity . using these predictions it is possible to estimate </S>",
    "<S> which datasets should be removed from disk . </S>",
    "<S> we use regression algorithms and time series analysis to find the optimal number of replicas for datasets that are kept on disk . </S>",
    "<S> based on the data popularity and the number of replicas optimization , the algorithm minimizes a loss function to find the optimal data distribution . </S>",
    "<S> the loss function represents all requirements for data distribution in the data storage system . </S>",
    "<S> we demonstrate how our algorithm helps to save disk space and to reduce waiting times for jobs using this data . </S>"
  ]
}