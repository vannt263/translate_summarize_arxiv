{
  "article_text": [
    "suppose @xmath1 is a random vector in @xmath2 and @xmath3 is a univariate random variable .",
    "let @xmath4 denote a @xmath5 orthogonal matrix with @xmath6 , i.e. @xmath7 , where @xmath8 is a @xmath9 identity matrix .",
    "given @xmath10 , if @xmath3 and @xmath1 are independent , i.e. @xmath11 , then the space spanned by the column vectors @xmath12 , @xmath13 , is called the dimension reduction space .",
    "if all the other dimension reduction spaces include @xmath14 as their subspace , then @xmath13 is the so - called central dimension reduction subspace ( cs ) ; see cook ( 1998 ) .",
    "the column vectors @xmath15 are called the cs directions .",
    "dimension reduction is a fundamental statistical problem both in theory and in practice .",
    "see li ( 1991 , 1992 ) and cook ( 1998 ) for more discussion . if the conditional density function of @xmath3 given @xmath16 exists , then the definition is equivalent to the conditional density function of @xmath17 being the same as that of @xmath18 for all possible values of @xmath1 and @xmath19 , i.e. f__y|x(y|x ) = f__y|b_0^x(y|b_0^x ) . [ definition ]",
    "other alternative definitions for the dimension reduction space can be found in the literature .    in the last decade or so ,",
    "a series of papers ( e.g. hrdle and stocker , 1989 ; li , 1991 ; cook and weisberg , 1991 ; samarov , 1993 ; hristache , juditski , polzehl and spokoiny , 2001 ; yin and cook , 2002 ; xia , tong , li and zhu , 2002 ; cook and li , 2002 ; li , zha and chiaromonte , 2004 ; lue , 2004 ) have considered issues related to the dimension reduction problem , with the aim of estimating the dimension reduction space and relevant functions .",
    "the estimation methods in the literature can be classified into two groups : inverse regression estimation methods ( e.g. sir , li , 1991 and save , cook and weisberg , 1991 ) and direct regression estimation methods ( e.g. ade , hrdle and stoker , 1991 and mave of xia , tong , li and zhu 2002 ) .",
    "the inverse regression estimation methods are computationally easy and are widely used as an initial step in data mining , especially for large data sets . however , these methods have poor performance in finite samples and need strong assumptions on the design of covariates .",
    "the direct regression estimation methods have much better performance for finite samples than the inverse regression estimations .",
    "they need no strong requirements on the design of covariates or on the response variable .",
    "however , the direct regression estimation methods can not find the directions in cs exhaustively , such as those in the conditional variance .",
    "none of the methods mentioned above use the definitions directly in searching for the central space . as a consequence ,",
    "they fail in one way or another to estimate cs efficiently . a straightforward approach in using definition ( [ definition ] ) is to look for @xmath20 in order to minimize the difference between those two conditional density functions .",
    "the conditional density functions can be estimated using nonparametric smoothers . obviously , this approach is not efficient in theory due to the `` curse of dimensionality '' in nonparametric smoothing . in calculations ,",
    "the minimization problem is difficult to implement .",
    "people have observed that the cs in the regression mean function , i.e. the central mean space ( cms ) , can be estimated much more efficiently than the general cs .",
    "see , for example , yin and cook ( 2002 ) , cook and li ( 2002 ) and xia , tong , li and zhu ( 2002 ) .",
    "motivated by this observation , one can construct a regression model such that the cs coincides with the cms space in order to reduce the difficulty of estimation . in this paper",
    ", we first construct a regression model in which the conditional density function @xmath21 is asymptotically equal to the conditional mean function .",
    "then , we apply the methods of searching for the cms to the constructed model . based on the discussion above , this constructive approach is expected to be more efficient than the inverse regression estimation methods for finite samples , and can detect the cs directions exhaustively .    in the estimation of dimension reduction space",
    ", most methods need in one way or another to deal with nonparametric estimation . in terms of nonparametric estimation ,",
    "the inverse regression estimation methods employ a nonparametric regression of @xmath16 on @xmath22 while the direct regression estimation methods employ a nonparametric regression of @xmath3 on @xmath1 .",
    "in contrast to existing methods , the methods in this paper search for cs from both sides by investigating conditional density functions .",
    "a similar idea appeared in yin and cook ( 2005 ) for a general single - index model . to overcome the difficulties of calculation , we propose two algorithms in this paper using a similar idea to xia , tong , li and zhu ( 2002 ) .",
    "the algorithm solves the minimization problem in the method by treating it as two separate quadratic programming problems , which have simple analytic solutions and can be calculated quite efficiently .",
    "the convergence of the algorithm can be proved .",
    "our constructive approach can overcome the disadvantages both in inverse regression estimations , requiring a symmetric design for explanatory variables , and also the disadvantage in direct regression estimation , of not finding the cs directions exhaustively .",
    "simulations suggest that the proposed methods have very good performance for finite samples and are able to estimate the cs directions in very complicated structures . applying the proposed methods to two real data sets , some useful patterns",
    "have been observed , based on the estimations .    to estimate the cs",
    ", we need to estimate the directions @xmath23 as well as the dimension @xmath24 of the space . in this paper , however , we focus on the estimation of the directions by assuming that @xmath25 is known .",
    "as discussed above , the direct regression estimations have good performance for finite samples .",
    "however , it can not detect exhaustively the cs directions in complicated structures .",
    "motivated by these facts , our strategy is to construct a semiparametric regression model such that all the cs directions are captured in the regression mean function . as we can see from ( [ definition ] ) , all the directions can be captured in the conditional density function .",
    "thus , we will construct a regression model such that the conditional density function is asymptotically equal to the regression mean function .",
    "the primary step is thus to construct an estimate for the conditional density function .",
    "here , we use the idea of the `` double - kernel '' local linear smoothing method studied in fan et al ( 1996 ) for the estimation . consider @xmath26 with @xmath27 running through all possible values , where @xmath28 is a symmetric density function",
    ", @xmath29 is a bandwidth and @xmath30 . if @xmath31 as @xmath32 , then from ( [ definition ] ) we have m_b(x , y ) = e(h_b(y - y)|x = x ) = e(h_b(y - y)|b_0^x= b_0^x ) f_y|b_0^x(y|b_0^x ) .",
    "see fan et al ( 1996 ) .",
    "the above equation indicates that all the directions can be captured by the conditional mean function @xmath33 of @xmath26 on @xmath34 with @xmath35 and @xmath27 running through all possible values .",
    "now , consider a regression model nominally for @xmath36 as h_b(y - y ) = m_b(x , y ) + _ _ b(y|x ) , where @xmath37 with @xmath38 .",
    "let @xmath39 .",
    "if ( [ definition ] ) holds , then @xmath40 .",
    "the model can be written as h_b(y - y ) = g_b(b_0^x , y ) + _ _ b(y|x ) .",
    "[ model ] as @xmath31 , we have @xmath41 .",
    "thus , the directions @xmath23 defined in ( [ definition ] ) are all captured in the regression mean function in model ( [ model ] ) if @xmath27 runs through all possible values .",
    "based on model ( [ model ] ) , we propose two methods to estimate the directions .",
    "one of the methods is a combination of the outer product of gradients ( opg ) estimation method ( hrdle , 1991 ; samarov , 1993 ; xia , tong , li and zhu , 2002 ) with the `` double - kernel '' local linear smoothing method ( fan et al , 1996 ) .",
    "the other one is a combination of the minimum average ( conditional ) variance estimation ( mave ) method ( xia , tong , li and zhu , 2002 ) with the `` double - kernel '' local linear smoothing method .",
    "the structure adaptive weights in hristache , juditski and spokoiny ( 2001 ) and hristache , juditski , polzehl and spokoiny ( 2001 ) are used in the estimations .",
    "consider the gradient of the conditional mean function @xmath42 with respect to @xmath35 .",
    "if ( [ definition ] ) holds , then it follows = = b_0 g_b(b_0^x , y ) , [ grad ] where @xmath43 with @xmath44 thus , the directions @xmath23 are contained in the gradients of the regression mean function in model ( [ model ] ) .",
    "one way to estimate @xmath23 is by considering the expectation of the outer product of the gradients e\\ { ( ) ( ) ^ } = b_0 e\\ { g_b(b_0^x , y ) g_b(b_0^x , y ) } b^_0 .",
    "it is easy to see that @xmath45 is in the space spanned by the first @xmath46 eigenvectors of the expectation of the outer products .",
    "suppose that @xmath47 is a random sample from @xmath48 .",
    "to estimate the gradient @xmath49 , we can use the nonparametric kernel smoothing methods . for simplicity",
    ", we adopt the following notation scheme .",
    "let @xmath50 be a univariate symmetric density function and define @xmath51 for any integer @xmath52 and @xmath53 , where @xmath54 is the dimension of @xmath55 and @xmath56 is a bandwidth .",
    "let @xmath57 , where @xmath58 and @xmath59 are defined above .",
    "for any @xmath60 , the principle of the local linear smoother suggests minimizing n^-1 _ i=1^n \\ { h_b , i(y ) - a - b^ ( x_i - x)}^2 k_h ( x_ix ) [ estg ] with respect to @xmath61 and @xmath59 to estimate @xmath62 and @xmath63 respectively , where @xmath64 .",
    "see fan and gijbels ( 1996 ) for more details . for each pair of @xmath65 , we consider the following minimization problem ( a_jk , b_jk ) = _ a_jk , b_jk_i=1^n ^2 w_ij , [ fjkrg9 ] where @xmath66 and @xmath67 .",
    "we consider an average of their outer products = n^-2_k=1^n",
    "_ j=1^n _ jk b_jk b_jk^ , where @xmath68 is a trimming function introduced for technical purpose to handle the notorious boundary points . in this paper , we adopt the following trimming scheme . for any given point @xmath69",
    ", we use all observations to estimate its function value and its gradient as in ( [ estg ] ) .",
    "we then consider the estimates in a compact region of @xmath70 .",
    "moreover , for those points with too few observations around , their estimates might be unreliable .",
    "they should not be used in the estimation of the cs directions and should be trimmed off .",
    "let @xmath71 be any bounded function with bounded second order derivatives on @xmath72 such that @xmath73 if @xmath74 ; @xmath75 if @xmath76 for some small @xmath77 .",
    "we take @xmath78 , where @xmath79 and @xmath80 are estimators of the density functions of @xmath1 and @xmath3 respectively .",
    "the cs directions can be estimated by the first @xmath46 eigenvectors of @xmath81 .    to allow the estimation to be adaptive to the structure of the dependency of @xmath19 on @xmath1",
    ", we may follow the idea of hristache et al ( 2001 ) and replace @xmath82 in ( [ fjkrg9 ] ) by w_ij = k_h ( ^1/2x_ij ) , where @xmath83 is a symmetric matrix with @xmath84 .",
    "repeat the above procedure until convergence .",
    "we call this procedure the method of outer product of gradient based on the conditional density functions ( dopg ) . to implement the estimation procedure , we suggest the following dopg algorithm .    1 .   0 : set @xmath85 and @xmath86 .",
    "2 .   1 : with @xmath87 , calculate the solution to ( [ fjkrg9 ] ) & = & \\{_i=1^n k_h_t(^1/2_(t ) x_ij ) 1 x_ij 1 x_ij ^}^-1 + & & _ i=1^n",
    "k_h_t(^1/2_(t ) x_ij ) 1 x_ij h_b_t , i(y_k ) , where @xmath88 and @xmath89 are bandwidths ( details are given in ( [ band0 ] ) and ( [ band1 ] ) below ) .",
    "3 .   2 : define @xmath90 with @xmath91 where @xmath92 are the eigenvalues of @xmath93 and @xmath94 @xmath95 .",
    "calculate the average of outer products @xmath96 4 .   3 : set @xmath97 .",
    "repeat steps 1 and 2 until convergence .",
    "denote the final value of @xmath98 by @xmath99 .",
    "suppose the eigenvalue decomposition of @xmath100 is @xmath101 , where @xmath102 .",
    "then the estimated directions are the first @xmath24 columns of @xmath103 , denoted by @xmath104 .    in the dopg algorithm , @xmath105 and @xmath106 , @xmath107 , are the estimators of the density functions of @xmath108 and @xmath109 respectively .",
    "a justification is given in the proof of theorem [ mainopg ] in section 6.2 . in calculations ,",
    "the usual stopping criterion can be used .",
    "for example , if the largest singular value of @xmath110 is smaller than @xmath111 then we stop the iteration and take @xmath112 as the final estimator .",
    "the eigenvalues of @xmath113 can be used to determine the dimension of the cs .",
    "however , we will not go into the details on this issue in this paper . in practice",
    ", we may need to standardize @xmath114 by setting @xmath115 and standardize @xmath116 by setting @xmath117 , where @xmath118 and @xmath119 , @xmath120 and @xmath121 .",
    "then the estimated cs directions are the first @xmath24 columns of @xmath122 .",
    "note that if ( [ definition ] ) holds , then the gradients @xmath123 at all @xmath70 are in a common @xmath46-dimensional subspace as shown in equation ( [ grad ] ) . to use this observation ,",
    "we can replace @xmath59 in ( [ estg ] ) , which is an estimate of the gradient , by @xmath124 and have the following local linear approximation n^-1 _ i=1^n \\ { h_b , i(y ) - a - d^b^(x_i - x ) } ^2 k_h(x_ix ) , where @xmath125 is introduced to take the role of @xmath126 in ( [ grad ] ) .",
    "note that the above weighted mean of squares is the local approximation errors of @xmath127 by a hyperplane with the normal vectors in a common space spanned by @xmath128 .",
    "since @xmath128 is common for all @xmath129 and @xmath27 , it should be estimated with aims to minimize the approximation errors for all possible @xmath130 and @xmath131 . as a consequence",
    ", we propose to estimate @xmath132 by minimizing n^-3 _ k=1^n _ j=1^n _ jk _ i = 1^n \\{h_b , i(y_k ) - a_jk - d^_jkx_ij } ^2 w_ij [ mini2 ] with respect to @xmath133 and @xmath134 , where @xmath68 is defined above .",
    "this estimation procedure is similar to the minimum average ( conditional ) variance estimation method ( xia , tong , li and zhu , 2002 ) . because the method is based on the conditional density functions , we call it the minimum average ( conditional ) variance estimation based on the conditional density functions ( dmave ) .    the minimization problem in ( [ mini2 ] )",
    "can be solved by fixing @xmath135 and @xmath136 alternatively . as a consequence ,",
    "we need to solve two quadratic programming problems which have simple analytic solutions . for any matrix @xmath137",
    ", we define operators @xmath138 and @xmath139 respectively as ( b ) = ( _ 1^ , , _ d^)^((b ) ) = b. we propose the following dmave algorithm to implement the estimation .    1 .   3 :",
    "calculate @xmath140 and @xmath141 .",
    "set @xmath142 and go to step 1 .",
    "2 .   4 : repeat steps 13 until convergence .",
    "let @xmath143 be the final value of @xmath144 . then",
    "our estimators of the directions are the columns in @xmath145 , denoted by @xmath146 .",
    "the dmave algorithm needs a consistent initial estimator in step 0 to guarantee its theoretical justification . in the following ,",
    "we use the first iteration estimator of dopg , the first @xmath147 eigenvector of @xmath148 , as the initial value .",
    "actually , any initial estimator that satisfies ( [ rate0 ] ) can be used and theorem [ mainmave ] will hold .",
    "similar to dopg , the standardization procedure can be carried out for dmave in practice . the stopping criterion for dopg",
    "can also be used here .    note",
    "that the estimation in the procedure is related with nonparametric estimations of conditional density functions .",
    "several bandwidth selection methods are available for the estimation .",
    "see , e.g. silverman ( 1986 ) , scott ( 1992 ) and fan et al ( 1996 ) .",
    "our theoretical verification of the convergence for the algorithms requires some constraints on the bandwidths although we believe these constraints can be removed with more complicated technical proofs . to ensure the requirements on bandwidths can be satisfied , after standardizing the variables we use the following bandwidths in our calculations . in the first iteration",
    ", we use slightly larger bandwidths than the optimal ones in terms of mise as h_0 = c_0 n^-1p_0 + 6 , b_0 = c_0 n^-1p_0 + 5 , [ band0 ] where @xmath149 .",
    "then we reduce the bandwidths in each iteration as h_t+1 = \\{r_n h_t , c_0 n^-1q+4 } , b_t + 1 = \\{r_n b_t , c_0n^-1q+3 , c_0 n^-15 } [ band1 ] for @xmath150 , where @xmath151 , @xmath152 as suggested by silverman ( 1986 ) if the epanechnikov kernel is used . here , the bandwidth @xmath59 is selected smaller than @xmath153 based on simulation comparisons .",
    "fan and yao ( 2003 , p.337 ) proposed a method , called the profile least - squares estimation , for the single - index model and its variants by solving a similar minimization problem as in ( [ mini2 ] ) .",
    "the method is also possible to be used here for the estimation of @xmath23 in ( [ model ] ) .",
    "to exclude the trivial cases , we assume that @xmath154 and @xmath155 . let @xmath156 , @xmath157 and @xmath158 be the ( conditional ) density functions of @xmath159 , @xmath160 and @xmath3 respectively .",
    "let  @xmath161 ,  @xmath162 @xmath163 , @xmath164 and @xmath165 . for any matrix @xmath166 ,",
    "let @xmath167 denote its largest singular value , which is same as the eculidean norm if @xmath168 is a vector .",
    "let @xmath169 be such that @xmath170 .",
    "we need the following conditions for ( [ definition ] ) to prove our theoretical results .    1 .",
    "the density function @xmath171 of @xmath172 has bounded second order derivatives on @xmath173 ; @xmath174 for some @xmath175 ; functions @xmath176 and @xmath177 have bounded derivatives with respect to @xmath55 and @xmath128 for @xmath178 in a small neighbor of @xmath23 : @xmath179 for some @xmath180 .",
    "the conditional density functions @xmath181 and @xmath182 @xmath183 have bounded fourth order derivatives with respect to @xmath35 , @xmath184 and @xmath185 for @xmath128 in a small neighbor of @xmath23 ; the conditional density function of @xmath186 and @xmath187 are bounded for all @xmath188 and @xmath189 .",
    "matrix @xmath190 @xmath191 has full rank @xmath147 .",
    "4 .   @xmath192 and @xmath28 are two symmetric univariate density functions with bounded second order derivatives and compact supports .",
    "bandwidths @xmath193 and @xmath194 where @xmath195 , @xmath196 . for @xmath197 , @xmath198 and @xmath199 where @xmath200 , @xmath201 with @xmath202 , and @xmath203 are constants .    in ( c1 ) , the finite moment requirement for @xmath204 can be removed if we adopt the trimming scheme of hrdle et al ( 1993 ) . however , as noticed in delecroix et al ( 2004 ) , this scheme caused some technical problems in the proofs .",
    "based on assumptions ( c2 ) and ( c4 ) , the smoothness of @xmath205 is implied .",
    "lower order of smoothness is sufficient if we are only interested in the estimation consistency .",
    "the second order differentiable requirement in ( c4 ) can ensure the fourier transformations of the kernel functions being absolutely integrable ; see chung ( p.166 , 1968 ) .",
    "the popular kernel functions such as epanechnikov kernel and quadratic kernel are included in ( c4 ) .",
    "the gaussian kernel can be used with some modifications to the proofs .",
    "condition ( c3 ) indicates that the dimension @xmath24 can not be further reduced .",
    "for ease of exposition , we further assume that @xmath206 , @xmath207 and @xmath208 ; otherwise , we take @xmath209 and @xmath210 @xmath211 @xmath212 .",
    "the bandwidths satisfying ( c5 ) can be found easily .",
    "for example , the bandwidths given in ( [ band0 ] ) and ( [ band1 ] ) satisfy the requirements . actually , a wider range of bandwidths can be used ; see the proofs .",
    "let @xmath213 , @xmath214 and @xmath215 . for any square matrix @xmath166 , @xmath216 and @xmath217 denote the inverse ( if it exists ) and the moore - penrose inverse matrices respectively .",
    "[ mainopg ] suppose conditions ( c1)-(c5 ) hold .",
    "then we have | b_dopg b^_dopg - b_0 b_0^| = o(^4 + ^2_q+ _ q ^4 + _ n^2/^2 + n^-1/2 ) in probability as @xmath32 , where @xmath218 and @xmath219",
    ". if @xmath220 can be satisfied , then \\{(b_dopg b^_dopgb_0 ) - ( b_0 ) } n(0 , w_0 ) , where w_0 & = & var [ _ 0(x , y ) m_0 ^ -1 ( f_0(y|b_0^x ) f__y(y ) - e\\{f_0(y|b_0^x ) f__y(y ) | x } ) + & & ( |w__b_0^+(x ) _ _ b_0(x ) ) ] .",
    "the first part of theorem [ mainopg ] indicates that @xmath221 is a consistent estimator of an orthogonal basis , @xmath222 with @xmath223 , in cs and @xmath224 in probability .",
    "see bai et al ( 1991 ) and xia , tong , li and zhu ( 2002 ) for alternative presentations of the asymptotic results . if the bandwidths in ( [ band1 ] ) are used , then the consistency rate is @xmath225 in probability",
    ". faster consistency rate can be obtained by adjusting the bandwidths .",
    "the convergence of the corresponding algorithm is also implied in the proof in section 6 .",
    "if @xmath226 , then the condition for the normality can be satisfied by taking 1 > r_h > 18 , 27 r_h < r_b < 12 - q r_h. if we use higher order polynomial smoothing , it is possible to show that the root-@xmath227 consistency can be achieved for any dimension @xmath24 .",
    "see , e.g. hrdle and stoker ( 1989 ) and samarov ( 1993 ) , where the higher order kernel , a counterpart of the higher order polynomial smoother , was used .",
    "however , using higher order polynomial smoothers increases the difficulty of calculations while the improvement of finite sample performance is not substantial .",
    "[ mainmave ] if conditions ( c1)-(c5 ) holds , then | b_dmave b_dmave^- b_0 b_0^| = o\\{^4 + ^2_q+ _ q ^4 + _ n^2/^2 + n^-1/2 } in probability as @xmath32 .",
    "if @xmath228 can be satisfied , then \\{(b_dmave b_dmave^b_0 ) - ( b_0 ) } n(0 , d_0^+ _ 0 d_0^+ ) , where @xmath229 and _ 0 = var [ _ 0(x , y ) ( f_0(y|b_0^x ) f__y ( y ) - e\\{f_0(y|b_0^x ) f__y ( y)| x } ) _ _ b_0 ( x ) ] .",
    "the proof of theorem [ mainmave ] is given in section 6 .",
    "the convergence of the dmave algorithm is implied in the proof .",
    "similar remarks on dopg are applicable to dmave .",
    "moreover , @xmath230 converges to @xmath231 , where @xmath232 is determined by the initial consistent estimator of the directions .",
    "for example , @xmath233 if @xmath234 is used as the initial estimator .",
    "similarly , the root-@xmath0 consistency holds for @xmath235 .",
    "it is possible that the root-@xmath236 consistency holds for @xmath237 if higher order local polynomial smoothing method is used . in spit of the equivalence in terms of consistency rate for both dopg and dmave",
    ", our simulations suggest that dmave has better performance than dopg in finite samples .",
    "theoretical comparison of efficiencies between the two methods is not clear . in a very special case when @xmath238 and the cs is in the regression mean , xia ( 2006a ) proved that dmave is more efficient than dopg .",
    "we here give some discussions about the requirements on the distributions of @xmath1 and @xmath19 .",
    "if @xmath3 is discrete , we can consider the conditional cumulative distribution functions and have @xmath239 when @xmath240 holds .",
    "similar to ( [ model ] ) , we can consider a regression model i(y < y ) = g(b_0^x , y ) + e(y|x ) , where @xmath241 and @xmath242 .",
    "similar theoretical consistency results are possible to be obtained following the same techniques developed here . if some covariates in @xmath1 are discrete , our algorithms in searching for a consistent initial estimator will fail . however ,",
    "if a consistent initial estimator can be found by for example the methods in horowitz and hrdle ( 1996 ) and hristache , juditski , polzehl and spokoiny ( 2001 ) and that @xmath243 has a continuous density function for all @xmath128 in a neighbor around @xmath45 , then our theoretical results in the above theorems still hold .",
    "we now demonstrate the performance of the proposed estimation methods by simulations . we will compare them with some existing methods including sir ( li , 1991 ) , save ( cook and weisberg , 1991 ) , phd ( li , 1992 ) and rmave ( xia , tong , li and zhu , 2002 ) .",
    "the computer codes used here can be obtained from _ www.jstatsoft.org/ _",
    "_ v07/i01/ _ for sir , save and phd methods ( courtesy of professor s. weisberg ) and _ www.stat.nus .",
    "edu/~ycxia/ _ for rmave , dopg and dmave . in the following calculations",
    ", we use the quadratic kernel @xmath244 and @xmath245 .",
    "the bandwidths in ( [ band0 ] ) and ( [ band1 ] ) are used . for the inverse regression methods",
    ", the number of slices is chosen between 5 to 30 that is most close to @xmath246 .",
    "we define an overall estimation error of estimator @xmath247 by the maximum singular value of @xmath248 ; see li et al ( 2004 ) .",
    "[ likc ] consider model y = ( 2x^_1 +",
    "_ 1)(|2x^_2 + 4 + _ 2|),[model41 ] where @xmath249 is the sign function .",
    "coordinates @xmath250 , unobservable noises @xmath251 and",
    "@xmath252 are independent . for @xmath253 ,",
    "the first 4 elements are all 0.5 and the others are zero . for @xmath254 ,",
    "the first 4 elements are 0.5,-0.5,0.5,-0.5 respectively and all the others are zero .",
    "a similar model was investigated by chen and li ( 1998 ) . in order to show the effect on the estimation performances of the number of covariates , we vary @xmath255 in the simulation . with different sample sizes",
    ", 200 replications are drawn from the model .",
    "the calculation results are listed in table 1 . to get",
    "an intuition about the quantity of estimation errors , figure [ fli ] shows a typical sample of size @xmath256 and its estimate with estimation error @xmath257 .",
    "the structure can be estimated quite well in the sample .    5.5 in    in model ( [ model41 ] ) ,",
    "the cs directions are hidden in a complicated structure and are not easy to be detected directly by the conditional regression mean function .",
    "when sample size is large @xmath258 and @xmath255 is not high ( @xmath259 ) , all the methods have accurate estimates . as @xmath255 increases , rmave performs not so well because the second direction is not captured in the regression mean function ; save and phd also fail to give accurate estimates .",
    "sir performs much better in all the situations than save and phd .",
    "dopg has about the same performance as sir .",
    "dmave is the best in all situations among all the methods .",
    "[ emv ] now , consider the cs in conditional mean as well as the conditional variance as in the following model & & y = 2(x^_1)^d + 2(x^_2 ) , [ mv ] where @xmath260 with @xmath261 and @xmath262 are independent , @xmath263 and @xmath264 @xmath265 . for model ( [ mv ] ) , one cs direction is contained in the regression mean and the other in the conditional variance .",
    "one typical data with size 200 is shown in figure [ fmv ] .",
    "table 2 lists the calculation results of 200 replications .    5.5 in",
    "because rmave can not detect the cs directions hidden in the conditional variance directly , it has very poor overall estimation performance as listed in table 2 .",
    "if @xmath266 , i.e. the regression mean function is monotonic , sir works reasonably well ; if @xmath267 , the regression mean function is symmetric and sir fails to find the direction hidden in the regression mean . as a consequence ,",
    "its performance is very poor .",
    "the performances of save and phd are also far from satisfactory though they are applicable to the model theoretically .",
    "the proposed dopg and dmave perform very well and are better than the existing methods listed in the table .",
    "[ rootn ] in this example , we demonstrate the consistency rates of the estimation methods by checking how the estimation errors change with sample size @xmath268 .",
    "consider model y = + x_3(x_3 + x_4 + 1 ) + 0.1 , [ model43 ] where @xmath269 and @xmath270 are independent .",
    "model ( [ model43 ] ) is a combination of the two examples in li ( 1991 ) . for this model ,",
    "all the theoretical requirements for the methods are fulfilled .",
    "therefore , it is fair to use the model to check their consistency rates .    5.5 in    in the left panel in figure [ figrootn ] , the proposed methods have much smaller estimation errors than the inverse regression estimations . because all the directions are hidden in the regression mean function , it is not surprising that rmave has the best performance .",
    "multiplied by root-@xmath0 , the errors should keep in a constant level if the theoretical root-@xmath0 consistency is applicable to the range of sample size .",
    "the right panel suggests that the estimation errors of sir and save do not start to show a root-@xmath0 decreasing rate for the sample size up to 1000 , while phd , rmave , dopg and dmave demonstrate a clear root-@xmath0 consistency rate .",
    "[ circle ] in our last example , we consider a model with a very complicated structure .",
    "suppose @xmath271 are drawn independently from model @xmath272 , where @xmath273 satisfies @xmath274 , @xmath275 and @xmath276 are defined in example [ likc ] .",
    "the calculation results based on 200 replications are listed in table 3 .",
    "because of the complicated structure as shown in figure [ complex ] , the cs directions are not easy to be estimated and observed directly . however , with moderate sample size , the proposed methods can still estimate the directions accurately .",
    "it is interesting to see that save also works in this example .",
    "based on the simulations , we have the following observations .",
    "( 1 ) the existing methods ( rmave , phd , sir and save ) fail in one way or another to estimate the cs directions efficiently , while dopg and dmave are efficient for all the examples . ( 2 ) dopg and dmave demonstrate very good finite sample performance , even a root-@xmath0 rate of estimation efficiency , while some of the existing methods do not show a clear root-@xmath0 rate in the range of sample sizes investigated . ( 3 ) dopg and dmave are less sensitive to the number of covariates than phd , save and sir .",
    "simulations not reported here also suggest that the asymmetric design of @xmath16 has less effect on dopg and dmave than that on the inverse regression estimations .",
    "( 4 ) if the cs directions are all hidden in the regression mean function , rmave is the best and should be used . otherwise , dopg and dmave are recommended .",
    "[ cars ] this data was used by the american statistical association in its second ( 1983 ) exposition of statistical graphics technology .",
    "the data set is available at http://lib.stat.cmu.edu/datasets/cars.data .",
    "there are 406 observations on 8 variables : miles per gallon ( @xmath277 ) , number of cylinders ( @xmath278 ) , engine displacement ( @xmath279 ) , horsepower ( @xmath280 ) , vehicle weight ( @xmath281 ) , time to accelerate from 0 to 60 mph ( @xmath282 ) , model year ( @xmath283 ) , and origin of a car ( 1 .",
    "american , 2 .",
    "european , 3 .",
    "japanese ) .",
    "now we investigate the relation between response variable @xmath277 and covariates @xmath284 , where @xmath285 are defined above , @xmath286 if a car is from america and 0 otherwise ; @xmath287 if it is from europe and 0 otherwise .",
    "thus , @xmath288 and ( 0,0 ) correspond to american cars , european cars and japanese cars respectively .",
    "for ease of explanation , all covariates are standardized separately . when applying dopg to the data , the first 4 largest eigenvalues are 21.1573 , 1.6077 , 0.2791 and 0.2447 respectively .",
    "thus , we consider cs with dimension 2 . based on dmave , the two directions ( coefficients of @xmath16 ) are estimated as @xmath289 - 0.33 , -0.45 , -0.45 , -0.53 , 0.14 , 0.42 , 0.00 , -0.02@xmath290 and @xmath2910.00 , 0.15 , -0.10 , -0.23 , -0.12 , -0.17 , -0.88 , 0.29@xmath290 respectively .",
    "the plots of @xmath3 against @xmath292 and @xmath293 are shown in figure [ figcars ] .",
    "6.5 in    based on the estimated cs directions and figure [ figcars ] , we have the following insights to the data .",
    "the first direction highlights the common structure for cars of all origins : miles per gallon ( @xmath277 ) decreases with number of cylinders ( @xmath278 ) , engine displacement ( @xmath279 ) , horsepower ( @xmath280 ) and vehicle weight ( @xmath281 ) , and increases with the time to accelerate ( @xmath282 ) and model year ( @xmath283 ) .",
    "the second direction indicates the difference between american cars and european or japanese cars .",
    "[ ozone ] air pollution has serious impact on the health of plants and animals ( including humans ) ; see the report of the world health organization ( who ) ( 2003 ) .",
    "substances not naturally found in the air or at greater concentrations than usual are referred to as `` pollutants '' .",
    "the main pollutants include nitrogen dioxide ( no@xmath294 ) , carbon dioxide ( co ) , sulphur dioxide ( so@xmath294 ) , respirable particulates , ground - level ozone ( o@xmath295 ) and others .",
    "pollutants can be classified as either primary pollutants or secondary pollutants .",
    "primary pollutants are substances directly produced by a process , such as ash from a volcanic eruption or the carbon monoxide gas from a motor vehicle exhaust .",
    "secondary pollutants are products of reactions among primary pollutants and other gases .",
    "they are not directly emitted and thus can not be controlled directly .",
    "the main secondary pollutant is ozone .",
    "next , we investigate the statistical relation between the level of ground - level ozone with the levels of primary pollutants and weather conditions by applying our method to the pollution data observed in hong kong ( 1994 - 1997 , http://www.hku.hk /statistics / paper/ ) and chicago ( 1995 - 2000 , http://www.ihapss.jhsph.edu/data/data.htm ) .",
    "this investigation is of interest in understanding how the secondary pollutant ozone is generated from the primary pollutants and weather conditions .",
    "let @xmath3 , @xmath296 and @xmath297 be the weekly average levels of ozone , nitrogen dioxide ( no@xmath294 ) , sulphur dioxide ( so@xmath294 ) , respirable particulates , temperature and humidity respectively . to include the interaction between primary pollutants and weather conditions into the model directly ,",
    "we further consider their cross - products resulting in 15 covariates all together , denoted by @xmath16 .",
    "for ease of explanation , all covariates are standardized separately .",
    "for all possible working dimensions , only the first two dimensions show clear relations with @xmath19 .",
    "we further calculate the eigenvalues in dopg .",
    "the largest four eigenvalues are @xmath298 respectively for chicago , and @xmath299 for hong kong .",
    "now we consider the dimension reduction with efficient dimension 2 although the estimation of the number of dimension needs further investigation .",
    "the estimates for the first two directions are given in table 4 .",
    "the plots of @xmath3 against the two estimated directions are shown in figure [ figozone ] .",
    "the plots show strong similar patterns in the two separated cities .",
    "if we check the estimated coefficients ( directions ) , no@xmath294 and particulates ( or their interaction ) are the most important pollutants that affect the level of ozone .",
    "temperature and humidity and their interaction are the other important factors .",
    "the interactions of weather conditions with no@xmath294 and particulates also contribute to the variation of ozone levels .",
    "these statistical conclusions give support to the chemical claim that ozone is formed by chemical reactions between reactive organic gases and oxides of nitrogen in the presence of sunlight ; see the report of who ( 2003 ) .",
    "[ 0pt]hong kong     [ 0pt]chicago",
    "the basic idea to prove the theorems is based on the convergence of the algorithms and that the true dimension reduction space is the attractor of the algorithms .",
    "we here give a more detailed outline for the proof of theorem 3.2 .",
    "suppose the estimate of @xmath20 in an iteration of the dmave algorithm is @xmath144 .",
    "it follows from step 2 that ^(t+1 ) & = & ( b_0 ) + \\{_k , j , i=1^n ^(t)_jk k_h_t ( b_(t)^ x_ij ) x^(t)_ijk ( x^(t)_ijk)^ } ^-1 + & & _ k , j , i=1^n ^(t)_jk k_h_t ( b_(t)^ x_ij ) x^(t)_ijk\\{h_b , i(y_k ) - a^(t)_jk - ( b_0)^ x^(t)_ijk } , [ relation0 ] where @xmath300 is defined in the algorithm . by the decomposition in step 3 , we obtain estimate @xmath301 in the next iteration .",
    "if the initial value @xmath302 is a consistent estimator of @xmath23 , by lemmas [ krq ] , [ denominator ] and [ numerator ] below , we will obtain a recurring relation for the iterations as ( b_(t+1 ) ) - ( b_0 ) = _ t \\ { ( b_(t ) ) - ( b_0 ) } + _ n , t , [ relation ] with @xmath303 and @xmath304 almost surely when @xmath197 .",
    "therefore , the dimension reduction space is an attractor in the algorithm .",
    "this recurring relation is then used to prove the convergence of the algorithm and the consistency of the final estimator . to ensure the convergence of the algorithm , we need to consider the consistency with probability 1 .",
    "the details of the proofs are organized as follows . in section 6.2 ,",
    "we first list a series of lemmas , lemmas 6.1 - 6.5 . based on these lemmas the theorems",
    "are then proved .",
    "the proofs of lemmas 6.1 - 6.5 are algebraic albeit complex calculations of lemmas 6.6 and 6.7 .",
    "they can be found in xia ( 2006b ) and are available upon request . lemmas 6.6 and 6.7",
    "are two basic results used in the proof dealing with the uniform consistency .",
    "their proofs are given in section 6.3 .",
    "we first introduce a set of notations .",
    "let @xmath305 , @xmath306 be a compact interior support of @xmath3 , i.e. for any @xmath307 , there exists @xmath308 such that @xmath309 .",
    "similarly , we can define a compact interior support @xmath310 for @xmath1 . for @xmath311 , define @xmath312 . for any index set @xmath313 and random matrix @xmath314 ,",
    "we say @xmath315 , or @xmath316 for simplicity , if @xmath317 indicates that every term in @xmath318 is @xmath319 in probability as @xmath32 .",
    "recall that @xmath320 and @xmath321 .",
    "let @xmath322 ,  @xmath323 and @xmath324 @xmath325 , where @xmath326 , @xmath327 @xmath328 is defined in section 2 and @xmath329 and @xmath330 is defined naturally . by taylor expansion of @xmath331 at @xmath332 , it follows from model ( [ model ] ) that h_b , i(y ) = h_b , i^1,b_0(x ) + h_b , i^2,b_0(x)+ h _ b , i^3,b_0(x)+ _ b , i(y ) + o (    @xmath333 , @xmath334 for any integer @xmath335 , @xmath336 , @xmath337 and @xmath338 .",
    "let @xmath339 and @xmath340 be the density functions of @xmath341 , @xmath16 and @xmath342 respectively .",
    "again , for simplicity , we write @xmath343 for @xmath344 and @xmath345 respectively ; see also the definitions in section 3 .",
    "let @xmath346 be a sequences of positive constants , while @xmath347 may have different values at different places .",
    "[ krp ] [ kernel smoother in the first iteration ] let = \\{_i=1^n k_h(x_ix ) 1 x_ix / h 1 x_ix / h ^}^-1 _",
    "i=1^n k_h(x_ix ) 1 x_ix / h h_b , i(y ) .",
    "[ exp1 ] under assumptions ( c1 ) , ( c2 ) and ( c4 ) , if @xmath348 and @xmath349 , then we have a_xy g_b(b_0^x , y ) + 12 _ = 1^q _ , ^2 g_b(b_0^x , y ) h^2 + (h^3 + _ phb|x__x , y__y ) , + b_xy b_0g_b(b_0^x , y ) + \\{_2p nh^2 f(x)}^-1 _ i=1^n k_h(x_ix ) x_ix _ b , i(y ) + & & + (h^2+_phb|x__x , y__y ) .    [ krqp ]",
    "[ kernel smoother in dopg ] define @xmath350 @xmath351 @xmath352 : @xmath353 , @xmath354 , @xmath355 and @xmath356 @xmath357 .",
    "let s_n^d(x ) = n^-1_i=1^n k_h ( d^1/2 x_ix ) 1 x_ix 1 x_ix ^ and & & = \\{n s_n^ d ( x)}^-1 _ i=1^n k_h ( d^1/2 x_ix ) 1 x_ix h_b , i(y ) . under assumptions ( c1 ) , ( c2 ) and ( c4 ) , if @xmath358 , @xmath359 , @xmath360 and @xmath361 then we have a^d_xy g_b(b_0^x , y ) + 12 _ = 1^q _ , ^2 g_b(b_0^x , y ) h^2 + (h^3 + _ qhb|x__x , y__y , d_q ) ,   + b^d_xy b_0",
    "\\ { g_b ( b_0^x , y)+ (h^2 + _ qh + e_n ) } + ^d_n,0(x , y ) + & & +  ( _ qhb|x__x , y__y , d_q ) , where @xmath362 and ^d_n,0(x , y ) = h^p - q \\ { n f_b ( x ) } ^-1 _ = 1^q _ ^1/2",
    "|w_b^+(x ) _ i=1^n k_h ( d^1/2 x_ix ) \\{_b(x)-x_i } _ b , i(y ) .",
    "[ krq ] [ kernel smoother in dmave ] let _",
    "n^b(x ) = n^-1_i=1^n k_h(b^x_ix ) 1 b^x_ix / h 1 b^x_ix / h ^ and & & = \\{n _ n^b(x)}^-1 _ i=1^n k_h ( b^x_ix ) 1 b^x_ix / h h_b , i(y ) .",
    "under assumptions ( c1 ) , ( c2 ) and ( c4 ) , if @xmath363 , @xmath359 and @xmath360 , then a^b_xy g_b ( b_0^x , y ) + g_b ( b_0^x , y)(b_0-b)^__b ( x ) + 12 _ = 1^q ^2_,g_b ( b_0^x , y ) h^2 + & & + _ 1n^b(x , y ) + (h^4 + _ qh _ qhb + h_b + _ b^2|x__x , y__y , b ) , + d^b_xyh g_b ( b_0^x , y ) h + m_1n^b(x , y ) h^3 + _ 2n^b(x , y ) + & & + (h^4 + _ qh _ qhb + h_b + _ b^2|x__x , y__y , b ) , where @xmath364 @xmath365 @xmath366 are bounded continuous functions ( details can be found in the proofs ) and ^b_n,1(x , y ) = \\ { n f_b ( x ) } ^-1 _ i=1^n k_h(b^x_ix ) _ b , i(y ) , + ^b_n,2(x , y ) = \\ { n h f_b ( x ) } ^-1 _ i=1^n k_h(b^x_ix ) b^x_ix _ b , i(y ) .",
    "[ denominator ] [ denominator of dmave ] let @xmath367 , where @xmath368 let @xmath369 where @xmath370 .",
    "suppose ( c1)(c4 ) hold and @xmath371 @xmath372 , @xmath373 , @xmath359 and @xmath374 .",
    "we have \\ { n^-3 _ k , j , i=1^n",
    "^b_jk k_h(b^ x_ij ) x^b_ijk ( x^b_ijk)^}^-1  ( i_qb ) l_1^b ( i_qb^ ) h^-2 + ( i_qb ) l_2 + & & + l_3 ( i_qb^ ) + 12 d_b^+ + \\{(r_qhb+_qhb)/h| b ) , where @xmath375 and @xmath376 are constant matrices ( details can be found in the proof ) and @xmath377 .",
    "[ numerator ] [ numerator of dmave ] suppose conditions ( c1)(c4 ) hold . if @xmath359 , @xmath378 , @xmath373 and @xmath374 , then n^-3_k , j , i=1^n ^b_jk k_h ( b^ x_ij ) x^b_ijk\\{h_b , i(y_k ) - a^b_jk - ( b_0)^ x^b_ijk}= d_b ( ( b ) - ( b_0 ) ) + + _",
    "n(b_0 ) + \\{h^4 + r_qhb _ qhb+ _ qhb^2 + _ n^2/b^2 + ( _ qhb / h+h)_b|b } , where @xmath379 almost surely and @xmath380 with @xmath381 and @xmath382 where @xmath383 is given in theorem [ mainmave ] .    * proof of theorem [ mainopg ] * by lemma [ krp ] , write b_xy = b_0 c_n(x , y ) + \\{_2p nh_0 ^ 2 f(x ) } ^-1 _ i=1^n k_h__0(x_ix ) x_ix _ b__0,i(y ) + b_0 (h_0 ^ 2+_ph_0 b_0 ) , where @xmath384 is a @xmath385 orthogonal matrix and @xmath386 . by lemma [ basic1 ] , the second term on the right hand side above is @xmath387 .",
    "it follows from step 2 in the dopg algorithm that _ ( 1 ) & = & ( b_0 , b_0 ) c_n ( b_0 , b_0)^ + n^-3 _ i , j , k=1^n ( s_ijk + s_ijk^ ) + & & + \\{(h_0 ^ 2+_ph_0 b_0)_ph__0b__0/h_0 } , [ opga ] where @xmath388 and @xmath389 are defined in the algorithm , @xmath390 @xmath391 and c_n & = & n^-2_j , k=1^n _",
    "jk^(0 ) c_n ( x_j , y_k ) (h_0 ^ 2+_ph_0 b_0 ) c_n(x_j , y_k ) (h_0 ^ 2+_ph_0 b_0)^ + & = & (    cc _",
    "n^(1 ) & (h_0 ^ 2 + _",
    "ph_0b_0 ) + (h_0 ^ 2 + _",
    "ph_0b_0 ) & (h_0 ^ 4 + _ ph_0b_0 ^ 2 )    ) , where @xmath392 . by lemma [ basic1 ]",
    ", we have @xmath393 . by the definition of @xmath394",
    ", we have @xmath395 , where @xmath396 .",
    "let s_ijk & = & ( f(x_j))_b_0 ( f__y(y_k ) ) b_0 g_b__0 ( b_0^x_j , y_k ) + & & \\{_2p h_0 ^ 2 f ( x_j)}^-1 k_h__0(x_ij ) x_ij^_b__0,i(y_k ) . by ( c5 ) and lemma [ basic2 ] , we have @xmath397 thus , n^-3_i , j ,",
    "k=1^n s_ijk = n^-3_i , j , k=1^n s_ijk + \\ { r_ph__0b__0_ph__0b__0 h_0 ^ -1 } = (_n^(1 ) ) , [ eq64 ] where @xmath398 by ( c3 ) and the strong law of large numbers for u - statistics ( cf . hoeffding , 1961 ) , @xmath399 @xmath400 almost surely , which is of full rank asymptotically .",
    "thus its eigenvalues are greater than a positive constant asymptotically . on the other hand ,",
    "the eigenvalues of the lower right principal submatrix in @xmath401 are of order @xmath402 .",
    "let @xmath403 be the eigenvalues of @xmath148 and @xmath404 be the corresponding eigenvectors . by the interlacing theorem ( cf .",
    "ando , 1987 ) , we have @xmath405 and @xmath406 . by ( [ opga ] ) and ( [ eq64 ] ) we have _ ( 1 ) = b_0 _ n^(1 )",
    "b_0^+ (_b^(1 ) ) , [ eq65 ] where @xmath407 .",
    "let @xmath408 .",
    "by lemma 3.1 of bai et al ( 1991 ) , we have b_(1 ) b_(1)^- b_0 b_0^= (_b^(1 ) ) . [ rate0 ]",
    "let @xmath409 .",
    "consider the @xmath410th iteration .",
    "let @xmath411 as defined in lemma [ krqp ] . by the conditions on bandwidths in ( c5 ) , we have @xmath412 and @xmath413 . by lemma [ krqp ] ,",
    "similar to ( [ opga ] ) , we have from the algorithm _ ( t+1 ) = ( b_0 , b_0 ) c^(t)_n ( b_0 , b_0)^+ n^-2 _ j , k=1^n\\ { s^(t)_jk + ( s^(t)_jk)^ } + (_qh_tb_t_qh_tb_t ) , [ opgb ] where @xmath414 and @xmath415 where @xmath416 .",
    "note that @xmath417 , @xmath418 and @xmath419 .",
    "it follows that & & n^-2 _ j , k=1^n\\ { s^(t)_jk + ( s^(t)_jk)^ } + & & = ( b_0 , b_0)(b_0 , b_0)^ + & & = ( b_0 , b_0 ) (    cc 0 & c^(t)_12,n + ( c^(t)_12,n)^ & 0    ) ( b_0 , b_0)^+ ( _ qh_tb_t^(t)_b ) , [ eq68 ] where @xmath420 @xmath421 . similar to @xmath422 , we have @xmath423 where @xmath424 @xmath425 . by ( c5 ) and lemma [ basic2 ] , we have c^(t)_12,n n^-2 _ j , k=1^n _ jk^(t)g_b_t(b_0^x_j , y_k)\\{_n,0^(t)(x_j , y_k)}^b_0 + (r_qh_tb_t _ qh_tb_t + e^(t)_n _ qh_tb_t ) + (_n + _ qh_t b_t^2 + _ n^2 b_t^-2 + r_qh_tb_t _ qh_t b_t+ e^(t)_n _ qh_tb_t ) . [ cc12 ] by the strong law of large numbers for u - statistics",
    ", it follows @xmath426 almost surely , where @xmath427 is defined in ( c3 ) .",
    "let @xmath428 be the eigenvalues of @xmath429 and @xmath301 the first @xmath147 eigenvectors .",
    "by the same arguments as for @xmath430 , it follows from ( [ opgb ] ) , ( [ eq68 ] ) and ( [ cc12 ] ) that @xmath431 and @xmath432 , where @xmath433 . considering @xmath434 , there exists a constant @xmath435 , which does not depend on @xmath436 , such that e_n^(t+1 ) h_t+1 c_1\\ { _ 0,n^(t ) + ^(t)_1,n e_n^(t ) h_t + _ 2,n^(t)_b^(t ) } , [ rate1 ] where @xmath437 , @xmath438 and @xmath439 . by ( [ opgb ] ) and ( [ eq68 ] ) , we write _ ( t+1 ) = b_0 ^(t)_n b_0 + b_0 c^(t)_12,n b_0^+ b_0 ( c^(t)_12,n b_0)^+ \\{_qh_tb_t + _ qh_tb_t_b^(t ) } , [ opgb1 ] where @xmath440 is the first term on the right hand side of the first equation in ( [ cc12 ] ) . by the same arguments as for ( [ rate0 ] )",
    ", we have @xmath441 . that is _",
    "b^(t+1 ) c_2\\ { _ 3,n^(t)+ _ 4,n^(t ) e_n^(t ) h_t + _ 5,n^(t)_b^(t ) } [ bb ] for a constant @xmath442 independent of @xmath436 , where @xmath443 , @xmath444 and @xmath445 .",
    "note that @xmath446 and @xmath447 decreasing with @xmath448 , by ( c5 ) we have @xmath449 .",
    "it follows that @xmath450 , @xmath451 and @xmath452 . recursing ( [ rate1 ] ) and ( [ bb ] )",
    ", it follows that _ b^ ( ) = \\{_3,n^ ( ) + _ 4,n^ ( ) _ 0,n^ ( ) } = \\ { ^4 + _ q ( _ q + ^2 + ^4 ) + _",
    "n^2/^2 + _ n } and @xmath453 .",
    "this is the first part of theorem [ mainopg ] . by ( [ opgb1 ] ) and",
    "the equations above , write _",
    "( ) & = & \\{b_0 + _ n } ^()_n \\{b_0 + _ n } ^+ \\{^4 + _ q ( _ q + ^4)+ _ n^2/^2 } , where @xmath454 . note that @xmath455 and thus @xmath456 .",
    "we have @xmath457 let @xmath458 .",
    "it follows that _ ( ) & = & _ n ^()_n _",
    "n^+ \\{^4 + _ q ( _ q + ^4)+ _ n^2/^2}. let @xmath459 be the first @xmath46 eigenvectors of @xmath460",
    ". by lemma 3.1 of bai et al ( 1991 ) , we have b_dopg b_dopg^- b_0 b_0^= b_0 _ n^+ _",
    "n b_0^+ o\\ { ^4 + _ q ( _ q + ^4)+ _",
    "n^2/^2}.  [ normal1 ] by lemma [ basic2 ] and ( c5 ) , we have _ n & = & n^-2_j , k=1^n ( f_b_0(x_j))(f__y(y_k ) ) _ n,0^()(x_j , y_k ) g_b(b_0^x_j , y_k ) ( _ n^())^-1 + & & + \\ { r_q _ q }   + & = & n^-1 _ i=1^n(f_b_0 ( x_i))(f__y(y_i ) ) |w^+__b_0(x_i ) _ _ b_0(x_i)^_i ( _ n^())^-1 + \\ { r_q _ q } , where @xmath461 .  let @xmath462 @xmath463 @xmath464 . as @xmath31",
    ", we have @xmath465 almost surely , where @xmath427 is defined in ( c3 ) . by calculating the mean and covariance matrix , we have n^-1 _ i=1^n(f_b_0 ( x_i))(f__y(y_i ) ) |w^+__b_0(x_i ) _ _ b_0(x_i ) ( ^_i - ^_i ) = o_p(n^-1/2 ) .",
    "it follows from the two equations above and the conditions in the theorem for the bandwidths _",
    "n = n^-1 _ i=1^n(f_b_0 ( x_i))(f__y(y_i ) ) |w^+__b_0(x_i ) _ _ b_0(x_i)^_i m_0 ^ -1 + o_p(n^-1/2 ) .",
    "[ norma2 ] after vectorizing @xmath466 , the second part of theorem [ mainopg ] follows from ( [ normal1 ] ) , ( [ norma2 ] ) and the central limit theorem . @xmath467    * proof of theorem [ mainmave ] * consider the initial estimator @xmath468 in ( [ rate0 ] ) .",
    "let @xmath469 . for simplicity ,",
    "we assume @xmath470 ; otherwise , we may use basis @xmath231 and consider the expansion in lemmas [ krq ] , [ denominator ] and [ numerator ] at @xmath471 .",
    "let @xmath472 be the consistency rate of the estimator in the @xmath473th iteration .",
    "write @xmath474 by the definition of @xmath475 in lemma [ denominator ] , it follows ( i_qb)^d_b = 0 , i_qb = i_qb_0 + o(_b ) , ( i_qb_0)^_n(b_0 ) = 0 .",
    "[ kfngkedf ] by the definition of the moore - penrose inverse we have @xmath476 where @xmath477 is a @xmath478 orthogonal matrix . by lemmas [ denominator ] , [ numerator ] and ( [ relation0 ] ) , for every @xmath144 in @xmath479 } , if @xmath480 we have ^(t+1 ) & = & ( i_qb_0)\\{(i_q)+o(c^(t)_n ) } + 12 _ ( t ) \\ { ( b_(t ) ) - ( b_0)}+ 12 d_(t)^+ _",
    "n(b_0 ) + & & + \\ { _ t + ( h_t+ _ qh_tb_t / h_t ) _",
    "b^(t ) } , [ fff ] where @xmath481 , @xmath482 , @xmath483 and @xmath484 , where @xmath485 is a projection matrix and @xmath486 is a @xmath478 orthogonal matrix . we have ( * b*^(t+1 ) ) & = & b_0 _",
    "n^(t ) + 12 m ( \\{(b_(t))- ( b_0 ) } ) + 12 m(d_(t)^+ _",
    "n(b_0 ) ) + & & + \\ { _ t + ( h_t+ _ qh_tb_t / h_t ) _",
    "b^(t ) } , where @xmath487 and @xmath488 is defined in section 2.2 . note that @xmath489 where @xmath490 . if @xmath491 almost surely , then by step 3 b_(t+1 ) & = & b_0 + 12 m ( \\{(b_(t))- ( b_0)})+12 m(d_(t)^+ _ n(b_0 ) ) + & & + \\ { _ t + ( h_t+ _ qh_tb_t / h_t ) _",
    "b^(t ) }   + & = & b_0 + 12 m ( \\ { ( b_(t))- ( b_0 ) } ) + \\ { _ n + _",
    "t + ( h_t+ _ qh_tb_t / h_t ) _ b^(t)}. [ fd ] by ( c5 ) and ( [ rate0 ] ) , we have @xmath492 , @xmath493 and @xmath494 almost surely . thus ( [ fd ] ) holds for @xmath495 . by assumption ( c5 ) , it follows that @xmath496 and @xmath497 almost surely .",
    "thus ( [ fd ] ) holds for @xmath498 . recurring the formula",
    ", we have @xmath499 a more detailed deduction was given in xia , tong and li ( 2002 ) .",
    "therefore , the first part of theorem [ mainmave ] follows immediately . by the first equation of ( [ fd ] ) with @xmath500 and lemma [ numerator ]",
    ", we have b _ ( ) - b_0 & = & 12 m(\\ { ( b _ ( ) ) - ( b_0 ) } ) + 12 m(d_()^+ _",
    "n(b_0 ) ) + & & + o_p\\{^4 + ( ^2 + ^4 + _ q ) _ q } .",
    "multiplying both sides by @xmath501 , by ( [ kfngkedf ] ) we have b_0^b _ ( ) - i&= & o_p\\{^4 + ( ^2 + ^4 + _ q ) _ q } .",
    "it follows that b_()b_()^b_0 - b_0 & = & 12 m(\\ { ( b _ ( ) ) - ( b_0 ) } ) + 12 m(d_()^+ _",
    "n(b_0 ) ) + & & + o_p\\{^4 + ( ^2 + ^4 + _ q ) _ q } .",
    "note that @xmath502 . we have ( b _ ( ) b_()^b_0 ) - ( b_0 ) = d_()^+ _ n(b_0 ) + o_p\\{^4 + ( ^2 + ^4 + _ q ) _ q } .",
    "this is the second part of theorem [ mainmave ] . @xmath467",
    "[ basic1 ] suppose @xmath503 are measurable functions of @xmath504 with index @xmath505 , where @xmath52 is an integer , such that ( i ) @xmath506 with @xmath507 for some @xmath508 ; ( ii ) @xmath509 ; and ( iii ) @xmath510 with some @xmath511 and @xmath512 .",
    "suppose @xmath513 is a random sample from @xmath514 . if @xmath515 with @xmath516 and @xmath517 , then for any positive @xmath518 we have @xmath519 almost surely .",
    "* proof of lemma [ basic1 ] * the `` continuity argument '' approach is used here .",
    "see , e.g. mack and silverman ( 1982 ) and hrdle et al ( 1993 ) . note that @xmath520 for some constant @xmath521 .",
    "there are @xmath522 @xmath523 ) balls @xmath524 centered at @xmath525 , @xmath526 , with diameter less than @xmath527 , such that @xmath528 .",
    "it follows that @xmath529 \\big| \\nonumber \\\\ & & \\stackrel{def}{= } \\max _ { 1 \\le k \\le n^{\\alpha_4 } } |r_{n , k,1}| + \\max _ { 1 \\le k \\le n^{\\alpha_4 } } \\sup _ { \\chi \\in b_{n_k } }    definition of @xmath530 , we have @xmath531 by the strong law of large numbers , we have @xmath532 almost surely .",
    "let @xmath533 , @xmath534",
    "@xmath535 and @xmath536 . write @xmath537 + \\frac{1}{n}\\sum_{i=1}^n\\xi_{n_k , i } , \\label{rnk1}\\end{aligned}\\ ] ] where @xmath538 . by the truncation",
    ", it follows that @xmath539 if @xmath540 with @xmath541 , we have @xmath542 again by the truncation , we have @xmath543 for fixed @xmath544 , by the strong law of large numbers , we have @xmath545 almost surely .",
    "the right hand side above is dominated by @xmath546 .",
    "note that @xmath547 increase to @xmath548 with @xmath268 . for large @xmath227 such that @xmath549",
    ", we have @xmath550 almost surely as @xmath551 .",
    "it follows _",
    "1k n^_4 n^-1|_i=1^n m_n^o(_n_k , z_i)| c_n t_n^-r+1 = o\\ { ( a_n n / n)^1/2 } [ df2 ] almost surely . by condition ( ii ) , we have _ 1kn^_4 ( _ i=1^n _ n_k , i ) & & n _ 1kn^_4 e\\ { m_n^i(_n_k , z_1)}^2 + & & n _ 1kn^_4 e \\{m_n(_n_k , z_1)}^2 c_5 n a_n n_1 .",
    "[ var1 ] by the condition on @xmath552 and the definition of @xmath553 , we have @xmath554 let @xmath555 with @xmath556 . by the bernstein s inequality ( cf .",
    "de la pea , 1999 ) , we have from ( [ var1 ] ) and ( [ bound1 ] ) that @xmath557 it follows that @xmath558 by the borel - cantelli lemma ( cf . chow and teicher , 1978 , p.60 ) , we have @xmath559 almost surely . combining ( [ rnk1 ] ) , ( [ df1 ] ) , ( [ df2 ] ) and ( [ lemma413 ] ) , we have @xmath560 almost surely .",
    "lemma [ basic1 ] follows from ( [ ffaacc0 ] ) , ( [ ffaacc1 ] ) and ( [ eqr2 ] ) .",
    "@xmath467    for any function @xmath561 @xmath562 ( or @xmath563 @xmath562 ) , we introduce a projection operator @xmath564 as follows .",
    "e_k g(x_i , y_i , x_j , y_j , x_k , y_k ) = e\\ { g(x_i , y_i , x_j , y_j , x_k , y_k ) |x_i , y_i , x_j , y_j}. [ operator ]    [ basic2 ] let @xmath565 with @xmath566 .",
    "suppose @xmath567 are bounded continuous functions . if conditions ( c2 ) and ( c4 ) hold with @xmath128 replaced by @xmath166 for all @xmath568 , then n^-3 _ i , j , k=1^n k_h ( a^x_ij ) g_1(x_i ) g_2(x_j ) g_0(y_k ) g_b(b_0^x_j , y_k ) _ b , i ( y_k ) + = n^-1_i=1^n e_j e_k \\ { k_h ( a^x_ij ) g_b(b_0^x_j , y_k)_b , i(y_k ) } + (_hb|a ) , where @xmath569 and the first term on the right hand side is @xmath570 .    * proof of lemma [ basic2 ] * for easy of exposition , we consider @xmath571 only .",
    "let @xmath572 be the left hand side of the equation in the lemma .",
    "let @xmath573 @xmath574 and @xmath575 be the fourier transformations , where @xmath576 is the imaginary unit .",
    "it follows from the inverse fourier transformation that @xmath577 @xmath578 .",
    "thus g_b(b_0^x_j , y_k ) = b^-1 _ _ h(t ) g_b(b_0^x_j ) e^-ity_k / b dt , [ cccai ] where @xmath579 .",
    "n(a ) _ _ h(t ) _ i , j , k=1^n\\ { k_h ( a ^x_ij ) g_b(b_0^x_j ) - e_j [ k_h ( a ^x_ij ) + & & g_b(b_0^x_j ) ] } \\{_b , i(y_k ) e^-ity_k / b - e_k[_b , i(y_k ) e^-ity_k / b]}dt + & & + _ _ h(t ) _ i , k=1^n e_j [ k_h ( a ^x_ij ) g_b(b_0^x_j ) ] + & & \\{_b , i(y_k ) e^-ity_k / b - e_k[_b , i(y_k ) e^-ity_k / b ] } dt + & & + _ _ h(t ) _ i , j=1^n e_k[_b , i(y_k ) e^-ity_k / b ]",
    "\\ { k_h ( a ^x_ij ) g_b(b_0^x_j ) + & & - e_j [ k_h ( a ^x_ij ) g_b(b_0^x_j)]}dt + & & + _ _ h(t )",
    "e_j [ k_h ( a ^x_ij ) g_b(b_0^x_j ) ] e_k[_b , i(y_k ) e^-ity_k / b]dt + & = & _ n,1(a ) + _ n,2(a ) + _ n,3(a ) + _ n,4(a ) . [ adf07 ] by the inverse fourier transformation , it follows that @xmath580 @xmath581 and @xmath582 . thus _",
    "n,1(a ) & = & _ = 1 ^ 3 _",
    "i=1^n m_,n(a , s , t , t , x_i , y_i ) _ _ k(s ) _ _ h(t ) _ _ h(t ) ds dt dt , where @xmath583,\\ ] ] @xmath584\\ ] ] and @xmath585    by ( c2 ) , we have that @xmath586 is bounded . for any @xmath587",
    ", it follows that @xmath588 and that @xmath589 where @xmath590 is a finite constant . for any @xmath591 ,",
    "let @xmath592 and @xmath593 , we have from lemma [ basic1 ] _ a , ( t , t,s ) _n n^-1 |_i=1^n m_,n(a , s , t , t , x_i , y_i)| = o ( _ n ) , = 1,2,3 [ vc1 ] almost surely . on the other hand , @xmath594 is bounded .",
    "thus , _ a , ( t , t,s ) n^-1 |_i=1^n m_,n(a , s , t , t , x_i , y_i)| = o(1 ) , = 1,2,3 .",
    "[ vc20 ] by ( c4 ) , the fourier transformation functions @xmath595 and @xmath596 are absolutely integrable ; see chung ( p.166 , 1968 ) .",
    "we can choose @xmath597 such that _",
    "|s|>n^_0 |__k(s ) | ds = o(_n^3 ) , _ |t|>n^_0",
    "|__h(t)|dt < o(_n^3 ) .",
    "[ vc2 ] partition the integration region in @xmath598 into two parts , we have from ( [ vc1])-([vc2 ] ) that _ a | _ n,1(a ) | & & _ ( s , t , t ) _ n _ = 1 ^ 3 _ a | _ i=1^n m_,n(a , s , t , t , x_i , y_i)| + & & |__k(s ) _ _ h(t ) _ _ h(t)| ds dt dt + & & + _ ( s , t , t ) _ n _ = 1 ^ 3 _ a | _",
    "i=1^n m_,n(a , s , t , t , x_i , y_i)| + & & |__k(s ) _ _ h(t ) _ _ h(t)| ds dt dt + & = & ( h^b^2)^-1 o(_n^3)|__k(s ) _ _ h(t ) _ _ h(t)| ds dt dt + & & + ( h^b^2)^-1 o(1 ) _ ( s , t , t ) _",
    "n |__k(s ) _ _ h(t ) _ _ h(t)| ds dt dt + & = & o ( _",
    "n^3 h^- b^-2 )   [ ccbbdd2 ] almost surely .",
    "let @xmath599 $ ] .",
    "it is easy to see that @xmath600 almost surely .",
    "applying the inverse fourier transformation to @xmath601 and using similar arguments leading to ( [ ccbbdd2 ] ) , we have _ a | _ n,2(a ) |= o ( _ n^2 b^-2 ) [ ccbbdd3 ] almost surely . applying the inverse fourier transformation to @xmath602 , similar to ( [ ccbbdd2 ] ) we have _ a | _ n,3(a ) | = o ( _ n^2 h^- b^-1 ) [ ccbbdd4 ] almost surely . by ( [ cccai ] )",
    ", we have _",
    "n,4(a ) = n^-1_i=1^n e_j e_k \\ { k_h ( a^x_ij ) g_b(b_0^x_j , y_k)_b , i(y_k ) } . by lemma [ basic1 ]",
    ", we have _ a _ n,4(a ) = o ( _ n ) [ ccbbdd6 ] almost surely .",
    "finally , lemma [ basic2 ] follows from ( [ ccbbdd2])-([ccbbdd6 ] ) and ( [ adf07 ] ) .",
    "@xmath603    * acknowledgements : * two referees and an associate editor , professor z. d. bai and professor b. brown provided for very valuable comments and suggestions for the paper .",
    "the work was supported by nus frg r-155 - 000 - 048 - 112 .",
    "ando , t. ( 1987 ) totally positive matrices .",
    "_ linear algebra appl . _ * 90 * , 165 - 219 .",
    "li , k. c. ( 1992 ) on principal hessian directions for data visualization and dimension reduction : another application of stein s lemma .",
    "_ journal of the american statistical association _ , * 87 * , 1025 - 1039 ."
  ],
  "abstract_text": [
    "<S> 1.9em    in this paper , we propose two new methods to estimate the dimension - reduction directions of the central subspace ( cs ) by constructing a regression model such that the directions are all captured in the regression mean . </S>",
    "<S> compared with the inverse regression estimation methods ( e.g. li , 1991 , 1992 ; cook and weisberg , 1991 ) , the new methods require no strong assumptions on the design of covariates or the functional relation between regressors and the response variable , and have better performance than the inverse regression estimation methods for finite samples . </S>",
    "<S> compared with the direct regression estimation methods ( e.g. hrdle and stoker , 1989 ; hristache , juditski , polzehl and spokoiny , 2001 ; xia , tong , li and zhu , 2002 ) , which can only estimate the directions of cs in the regression mean , the new methods can detect the directions of cs exhaustively . </S>",
    "<S> consistency of the estimators and the convergence of corresponding algorithms are proved .    _ </S>",
    "<S> key words _ : conditional density function ; convergence of algorithm ; double - kernel smoothing ; efficient dimension reduction ; root-@xmath0 consistency .    </S>",
    "<S> _ short title _ </S>",
    "<S> : constructive dimension reduction    _ ams 200 subject classifications _ : primary 62g08 ; secondary 62g09 , 62h05 .    </S>",
    "<S> [ section ] [ theorem]example [ theorem]lemma [ theorem]note [ theorem]proposition [ theorem]corollary [ theorem]remark    o    1.8em </S>"
  ]
}