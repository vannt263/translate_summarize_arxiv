{
  "article_text": [
    "a standard formulation of supervised learning starts with a parametrized class of mappings , a training set of desired input - output pairs , and a loss function measuring deviation of actual output from desired output .",
    "the goal of learning is to minimize the average loss over the training set .",
    "a popular minimization method is stochastic gradient descent . for each input in sequence ,",
    "the parameters of the mapping are updated in minus the direction of the gradient of the loss with respect to the parameters .",
    "here we are concerned with a class of mappings known as convolutional networks ( convnets ) .",
    "significant effort has been put into parallelizing convnet learning on gpus , as in the popular software packages caffe  @xcite , torch  @xcite and theano@xcite .",
    "convnet learning has also been distributed over multiple machines  @xcite .",
    "however , there has been relatively little work on parallelizing convnet learning for single shared memory cpu machines .",
    "here we introduce a software package called znn , which implements a novel parallel algorithm for convnet learning on multi - core and many - core cpu machines .",
    "znn implements 3d convnets , with 2d as a special case .",
    "znn can employ either direct or fft convolution , and chooses between the two methods by autotuning each layer of the network .",
    "fft convolution was previously applied to 2d convnets running on gpus  @xcite , and is even more advantageous for 3d convnets on cpus .    as far as we know",
    ", znn is the first publicly available software that supports efficient training of sliding window max - pooling convnets , which have been studied by  @xcite",
    ".    there is related work on using xeon phifor supervised deep learning  @xcite . and unsupervised deep learning  @xcite .",
    "we define a convnet using a directed acyclic graph ( dag ) , called the _ computation graph _ ( fig .",
    "[ fig : convnet_dag ] ) .",
    "each node represents a 3d image , and each edge some image filtering operation .",
    "( 2d images are a special case in which one of the dimensions has size one . )",
    "if multiple edges converge on a node , the node sums the outputs of the filtering operations represented by the edges . for convenience",
    ", the discussion below will assume that images and kernels have isotropic dimensions , though this restriction is not necessary for znn .",
    "the image filtering operations are of the four following types .",
    "* convolution * a weighted linear combination of voxels within a sliding window is computed for each location of the window in the image .",
    "the set of weights of the linear combination is called the _",
    "kernel_. if the input image has size @xmath0 and the kernel has size @xmath1 , then the output image has size @xmath2 .",
    "image size decreases because an output voxel only exists when the sliding window is fully contained in the input image .",
    "the convolution is allowed to be sparse , meaning that only every @xmath3th image voxel ( in every dimension ) within the sliding window enters the linear combination .",
    "* max - pooling * divides an image of size @xmath0 into blocks of size @xmath4 , where @xmath5 is divisible by @xmath6 .",
    "the maximum value is computed for each block , yielding an image of size @xmath7 . *",
    "max - filtering * the maximum within a sliding window is computed for each location of the window in the image . for a window of size @xmath1 and an input image of size @xmath0 , the output image has size @xmath8 .",
    "3d max - filtering can be performed by sequential 1d max - filtering of @xmath9 arrays in each of the three directions . for each array",
    "we keep a heap of size @xmath10 containing the values inside the 1d sliding window . each element of the array will be inserted and removed at most once , each operation taking @xmath11 . for each position of the sliding window",
    "the top of the heap will contain the maximum value .",
    ".number of floating point operations ( flops ) required by a layer with @xmath12 nodes that all perform the same nonlinear filtering operation ( max - pooling , max - filtering , or transfer function ) . [ cols=\"<,<,<,<\",options=\"header \" , ]     the 3d convnets contained four fully - connected convolutional ( c ) layers with @xmath13 kernels , each followed by a transfer function layer ( t ) with rectified linear function , and two @xmath14 max - filtering ( m ) layers .",
    "each convolutional layer the sequence of layer types was ctmctmctct .",
    "the output patch size was @xmath15 .",
    "the 2d convnets contained 6 fully - connected convolutional layers with @xmath16 kernels , each followed by rectified linear transfer function layer ( t ) , and two @xmath17 max - filtering layers ( 2nd and 4th ) .",
    "the sequence of layer types was ctmctmctctctct .",
    "the output patch size was @xmath18 .",
    "the znn measurements were performed by first running the gradient learning algorithm for 5 warm - up rounds and then averaging the time required for the next 50 rounds .",
    "the gpu measurements were averaged over 100 rounds .",
    "2d convnets were implemented as a special case of 3d convnets , by setting one of the dimensions to have size one .",
    "the width of the convnets was varied as described below .",
    "fft convolution was employed for 2d , and direct convolution for 3d to illustrate the use of both methods ; reversing this yields similar results .",
    "other network architectures and kernel sizes also yield similar results",
    ".     +    fig .",
    "[ fig:2dspeedups_threads ] shows speedup attained by various cpus as a function of two parameters , number of worker threads and network width .",
    "each graph shows the result of varying the number of workers while network width is held fixed . to achieve near maximal possible speedup znn requires sufficiently wide networks ( @xmath19 for multicore cpus and @xmath20 for the manycore cpu ) and sufficiently many worker threads ( number of hyperthreads for multicore and number of hardware threads for manycore )  .",
    "the value of the maximal speedup is equal to the number of cores or a bit larger ( maximal height of graphs ) .    for a wide network on multicore cpus ,",
    "speedup increases linearly until the number of worker threads equals the number of cores .",
    "after that the increase continues at a slower rate . for wide networks on xeon phi , speedup increases linearly until the number of worker threads equals the number of cores , then more slowly until double that number , and then even slower until the number of hardware threads .",
    "the maximal achieved speedups for networks of different widths are shown in figs .",
    "[ fig:2dspeedups ] and  [ fig:3dspeedups ] .",
    "while the preceding results show that znn can efficiently utilize cpus , it is also important to know how the resulting performance compares to gpu implementations of convnet learning . therefore , we benchmarked znn against caffe @xcite and theano @xcite , two popular gpu implementations .",
    "comparison can be tricky because cpu and gpu implementations by definition can not be run on the same hardware .",
    "we chose to run caffe and theano on a titan x gpu , and znn on an @xmath21 core amazon ec2 instance ( c4.8xlarge ) .",
    "we chose this particular comparison , because the alternatives seemed unfair .",
    "for example , we could have run znn on specialized hardware with more cpu cores than the ec2 instance .",
    "this comparison seemed unfair because the specialized hardware would have been much more costly than titan x and less accessible than amazon ec2 .",
    "also , we could have used gpu instances from amazon ec2 , but these are currently much slower than titan x ( @xmath22 or more on our benchmarks ) and have half the onboard ram .    for caffe , both default and cudnn@xcite implementations were used . for 3d convnets",
    "we only used theano , as the official release of caffe still does not support 3d convnets .",
    "our caffe and theano code is publicly available in the znn repository .",
    "znn used fft convolution for both 2d and 3d , as this was found to be optimal by the auto - tuning capability of znn .",
    "caffe and theano used direct convolution .    , @xmath23 , @xmath24 and @xmath25 respectively .",
    "where caffe data is missing , it means that caffe could not handle networks of the given size.,scaledwidth=50.0% ]    our convnets contained 6 fully - connected convolutional ( c ) layers , each followed by a rectified linear transfer function layer ( t ) , and two max - pooling ( p ) layers , either @xmath26 or @xmath27 .",
    "the sequence of the layer types was ctpctpctctctct .",
    "all networks had width @xmath28 , while the sizes of the kernels and the output patch varied .",
    "all benchmark times were for `` sparse training , '' meaning that the convnet is used to produce predictions for pixels in the output patch that form a lattice with period 4 in every dimension .",
    "the loss of predicted output pixels is due to the two layers of max - pooling .",
    "as noted before , znn can also perform `` dense training , '' meaning that the convnet is used to produce predictions for every pixel in the output patch by applying the convnet to a window that slides across every `` valid '' location in the input patch . requiring caffe or theano to perform dense training",
    "could have been accomplished by computing @xmath29 sparse outputs in 2d and @xmath30 in 3d to assemble a dense output .",
    "this method is very inefficient and would have been no contest with znn .      the comparison of 2d convnets is shown in fig .  [",
    "fig : vsgpu2d ] .",
    "znn is faster than caffe and theano for sufficiently large kernels ( @xmath31 or larger ) .",
    "this makes sense because fft convolution ( znn ) is more efficient than direct convolution ( caffe and theano ) for sufficiently large kernels .",
    "such large kernels are not generally used in practice , so znn may not be competitive with gpu implementations for 2d networks .",
    "on the other hand , znn opens up the possibility of efficiently training networks with large kernels , and these might find some practical application in the future .    the comparison of 3d convnets is shown in fig .  [",
    "fig : vsgpu3d ] .",
    "znn is comparable to theano even for modest kernel sizes of @xmath32 and outperforms theano for kernel sizes of @xmath33 and greater .",
    "such kernel sizes are currently relevant for practical applications  @xcite .",
    "again the benchmark makes sense , because we expect the crossover point for complexity of fft vs. direct convolution to occur for smaller ( linear ) kernel sizes in 3d .    ,",
    "@xmath34 and @xmath35.,scaledwidth=40.0% ]      working memory is another computational resource that is important for training convnets .",
    "given the limited amount of onboard gpu memory , we were unable to use theano to train 3d networks with kernel sizes larger than @xmath36 .",
    "we were also unable to use caffe to train many 2d networks ( see missing bars in fig .  [",
    "fig : vsgpu2d ] ) .",
    "znn enables training of larger networks mostly because a typical cpu system has much more ram than even a top gpu .",
    "titan x , for example , has just 12 gb of onboard ram .",
    "additionally , znn can achieve even higher speed by using extra ram space , as in the case of fft memoization . when using fft - based convolutions , with the memoization disabled , znn is more efficient in its usage of ram than the proposed gpu methods .",
    "the memory overhead of the methods proposed in  @xcite could be very high as it is proportional to the number of kernels in a layer .",
    "in contrast znn s memory overhead is proportional to the number of workers .",
    "znn is implemented in c++ and is publicly available under the gpl2 license ( _ https://github.com/zlateski/znn-release_ ) .",
    "it can use either fftw or intel mkl for ffts and either provided code or intel mkl libraries for direct convolution . using fftw instead of mkl yields same scalability but lower absolute performances due to the differences in single thread performances of the two libraries .",
    "the repository also provides alternative scheduling strategies such as simple fifo or lifo as well as some more complex ones based on work stealing  @xcite .",
    "the alternative scheduling strategies achieve noticeably lower scalability than the one proposed in the paper for most networks .",
    "however , some very specific networks might benefit from alternative scheduling algorithms .",
    "future work can include automatic detection of the best scheduling strategy .",
    "znn achieves high performances by efficiently utilizing the available cpus .",
    "we expect an increase in the number of cores per chip ( or xeon phicard ) in the future , making znn even more practical .",
    "in fact , we have already used znn to achieve state of the art results in boundary detection  @xcite and computation of dendritic arbor densities  @xcite .    having a large amount of ram available to the cpu",
    ", znn can efficiently train very large convnets with large kernels .",
    "znn allows for easy extensions and can efficiently train a convnet with an arbitrary topology , allowing for new research .    unlike the znn s task parallelization model ,",
    "the current gpu implementations employ simd parallelism to perform computation on one whole layer at a time , thus limiting the network structure . mainly , the computation is parallelized such that a single thread computes the value of a single voxel of an output image",
    ". libraries like cudnn provide optimized primitives for fully connected convolutional layers by reducing all the required convolutions in the layer to a matrix multiplication , which is then parallelized on the gpu .",
    "extending the functionality requires the user to provide a parallelized implementation of the new layer type , which typically requires great knowledge of gpu programming , and might take a long time .",
    "contrary to that , znn s task parallelism allows for easy extensions by simply providing serial functions for the forward and backward pass , as well as the gradient computation , if required .",
    "znn s repository contains some sample extensions providing functionality of _ dropout _",
    "@xcite and _ multi - scale _  @xcite networks .",
    "y.  jia , e.  shelhamer , j.  donahue , s.  karayev , j.  long , r.  girshick , s.  guadarrama , and t.  darrell , `` caffe : convolutional architecture for fast feature embedding , '' in _ proceedings of the acm international conference on multimedia _ , pp .",
    "675678 , acm , 2014 .",
    "j.  bergstra , o.  breuleux , f.  bastien , p.  lamblin , r.  pascanu , g.  desjardins , j.  turian , d.  warde - farley , and y.  bengio , `` theano : a cpu and gpu math expression compiler , '' in _ proceedings of the python for scientific computing conference ( scipy ) _ , vol .  4 , p.",
    "3 , austin , tx , 2010 .",
    "j.  dean , g.  corrado , r.  monga , k.  chen , m.  devin , m.  mao , a.  senior , p.  tucker , k.  yang , q.  v. le , _ et  al .",
    "_ , `` large scale distributed deep networks , '' in _ advances in neural information processing systems _ , pp .  12231231 , 2012 .",
    "j.  masci , a.  giusti , d.  ciresan , g.  fricout , and j.  schmidhuber , `` a fast learning algorithm for image segmentation with max - pooling convolutional networks , '' in _ image processing ( icip ) , 2013 20th ieee international conference on _ , pp .  27132717 , ieee , 2013 .",
    "p.  sermanet , d.  eigen , x.  zhang , m.  mathieu , r.  fergus , and y.  lecun , `` overfeat : integrated recognition , localization and detection using convolutional networks , '' _ arxiv preprint arxiv:1312.6229 _ , 2013 .",
    "l.  jin , z.  wang , r.  gu , c.  yuan , and y.  huang , `` training large scale deep neural networks on the intel xeon phi many - core coprocessor , '' in _ proceedings of the 2014 ieee international parallel & distributed processing symposium workshops _ , ipdpsw 14 , ( washington , dc , usa ) , pp .  16221630 , ieee computer society , 2014 .",
    "d.  ciresan , a.  giusti , l.  m. gambardella , and j.  schmidhuber , `` deep neural networks segment neuronal membranes in electron microscopy images , '' in _ advances in neural information processing systems _ , pp .  28432851 , 2012 .",
    "m.  m. michael and m.  l. scott , `` simple , fast , and practical non - blocking and blocking concurrent queue algorithms , '' in _ proceedings of the fifteenth annual acm symposium on principles of distributed computing _",
    ", pp .  267275 , acm , 1996 .",
    "m.  helmstaedter , k.  l. briggman , s.  c. turaga , v.  jain , h.  s. seung , and w.  denk , `` connectomic reconstruction of the inner plexiform layer in the mouse retina , '' _ nature _ , vol .",
    "500 , no .",
    "7461 , pp .  168174 , 2013 .",
    "u.  smbl , a.  zlateski , a.  vishwanathan , r.  h. masland , and h.  s. seung , `` automated computation of arbor densities : a step toward identifying neuronal cell types , '' _ frontiers in neuroanatomy _ , vol .  8 , 2014 .",
    "n.  srivastava , g.  hinton , a.  krizhevsky , i.  sutskever , and r.  salakhutdinov , `` dropout : a simple way to prevent neural networks from overfitting , '' _ the journal of machine learning research _ , vol .  15 , no .  1 , pp .  19291958 , 2014 ."
  ],
  "abstract_text": [
    "<S> convolutional networks ( convnets ) have become a popular approach to computer vision . </S>",
    "<S> it is important to accelerate convnet training , which is computationally costly . </S>",
    "<S> we propose a novel parallel algorithm based on decomposition into a set of tasks , most of which are convolutions or ffts . </S>",
    "<S> applying brent s theorem to the task dependency graph implies that linear speedup with the number of processors is attainable within the pram model of parallel computation , for wide network architectures . to attain such performance on real shared - memory machines , our algorithm computes convolutions converging on the same node of the network with temporal locality to reduce cache misses , and sums the convergent convolution outputs via an almost wait - free concurrent method to reduce time spent in critical sections . </S>",
    "<S> we implement the algorithm with a publicly available software package called znn . </S>",
    "<S> benchmarking with multi - core cpus shows that znn can attain speedup roughly equal to the number of physical cores . </S>",
    "<S> we also show that znn can attain over 90x speedup on a many - core cpu ( xeon phiknights corner ) . </S>",
    "<S> these speedups are achieved for network architectures with widths that are in common use . </S>",
    "<S> the task parallelism of the znn algorithm is suited to cpus , while the simd parallelism of previous algorithms is compatible with gpus . through examples , </S>",
    "<S> we show that znn can be either faster or slower than certain gpu implementations depending on specifics of the network architecture , kernel sizes , and density and size of the output patch . </S>",
    "<S> znn may be less costly to develop and maintain , due to the relative ease of general - purpose cpu programming .    </S>",
    "<S> = 10000 </S>"
  ]
}