{
  "article_text": [
    "the proof of ( [ eq.sin1 ] ) goes as follows .",
    "first @xmath205 we focus on bounding the term @xmath206 , the bound of the other term will follow along the same lines .        in order to prove inequality ( [ eq.sin2 ] ) ,",
    "observe that @xmath208 using previous bounding method , @xmath209 is also bounded by a constant @xmath210 which does not depend on @xmath37 .",
    "this concludes the proof ."
  ],
  "abstract_text": [
    "<S> this note studies a method for the estimation of a finite number of unknown parameters from linear equations , which are perturbed by gaussian noise . in case </S>",
    "<S> the unknown parameters have only few nonzero entries , the proposed estimator performs more efficiently than a traditional approach . </S>",
    "<S> the method consists of three steps : ( 1 ) a classical least squares estimate ( lse ) , ( 2 ) the support is recovered through a linear programming ( lp ) optimization problem which can be computed using a soft - thresholding step , ( 3 ) a de - biasing step using a lse on the estimated support set . </S>",
    "<S> the main contribution of this note is a formal derivation of an associated oracle property of the final estimate . that is , with probability 1 , the estimate equals the lse based on the support of the _ true _ parameters when the number of observations goes to infinity .    </S>",
    "<S> system identification ; parameter estimation ; sparse estimation .    </S>",
    "<S> problem settings    this note considers the estimation of a sparse parameter vector from noisy observations of a linear system . the formal definition and assumptions of the problem are given as follows . </S>",
    "<S> let @xmath0 be a fixed number , denoting the dimension of the underlying parameter vecto , and let @xmath1 denote the number of equations ( observations ) . </S>",
    "<S> the observed signal @xmath2 obeys the following system : @xmath3 where the elements of the vector @xmath4 are considered to be the fixed but unknown parameters of the system . </S>",
    "<S> moreover , it is assumed that @xmath5 is @xmath6-sparse ( i.e. there are @xmath6 nonzero elements in the vector ) . </S>",
    "<S> let @xmath7 denote the support set of @xmath8 ( i.e. @xmath9 ) and @xmath10 be the complement of @xmath11 , i.e. @xmath12 and @xmath13 . </S>",
    "<S> the elements of the vector @xmath14 are assumed to follow the following distribution @xmath15 where @xmath16 .    </S>",
    "<S> applications of such setup appear in many places , to name a few , see the applications discussed in @xcite on the detection of nuclear material , and in @xcite on model selection for aircraft test modeling ( see also the experiment 2 in @xcite on the model selection for the ar model ) . in the experiment section </S>",
    "<S> , we will demonstrate an example which finds application in line spectral estimation , see @xcite .    </S>",
    "<S> the matrix @xmath17 with @xmath18 is the sensing matrix . </S>",
    "<S> such a setting ( @xmath19 is a tall matrix ) makes it different from the setting studied in compressive sensing , where the sensing matrix is fat , i.e. @xmath20 . for an introduction to the compressive sensing theory , see e.g. @xcite .    </S>",
    "<S> denote the singular value decomposition ( svd ) of matrix @xmath21 as @xmath22 in which @xmath23 satisfies @xmath24 , @xmath25 satisfies @xmath26 , and @xmath27 is a diagonal matrix @xmath28 . </S>",
    "<S> the results below make the following assumptions on @xmath19 :    we say that @xmath29 are sufficiently rich if there exists a finite @xmath30 and @xmath31 such that for all @xmath32 the corresponding matrices @xmath21 obey @xmath33 where @xmath34 denotes the @xmath35-th singular value of the matrix @xmath19 , @xmath36 .    </S>",
    "<S> note that the dependence of @xmath19 on @xmath37 is not stated explicitly in order to avoid notational overload .    in @xcite and @xcite </S>",
    "<S> , the authors make the assumption on @xmath19 that the sample covariance matrix @xmath38 converges to a finite , positive - definite matrix : @xmath39 this assumption is also known as _ persistent excitation _ ( pe ) , see e.g. @xcite . </S>",
    "<S> note that our assumption in eq . </S>",
    "<S> ( [ eq.sing ] ) covers a wider range of cases . </S>",
    "<S> for example , eq . </S>",
    "<S> ( [ eq.sing ] ) does not require the singular values of @xmath40 to converge , while only requires that they lie in @xmath41 $ ] when @xmath37 increases .    </S>",
    "<S> classically , properties of the least square estimate ( lse ) under the model given in eq . </S>",
    "<S> ( 1 ) are given by the gauss - markov theorem . </S>",
    "<S> it says that the best linear unbiased estimation ( blue ) of @xmath42 is the lse under certain assumptions on the noise term . for the gauss - markov theorem , please refer to @xcite . </S>",
    "<S> however , the normal lse does not utilize the sparse information of @xmath42 , which raises the question that whether it is possible to improve on the normal lse by exploiting this information . in the literature , several approaches </S>",
    "<S> have been suggested , which can perform as if the true support set of @xmath42 were known . </S>",
    "<S> such property is termed as the oracle property in @xcite . in @xcite , </S>",
    "<S> the scad ( smoothly clipped absolute deviation ) estimator is presented , which turns out to solve a non - convex optimization problem ; later in @xcite , the adalasso ( adaptive least absolute shrinkage and selection operator ) estimator is presented . </S>",
    "<S> the adalasso estimator consists of two steps , which implements a normal lse in the first step , and then solves a reweighed lasso optimization problem , which is convex . </S>",
    "<S> recently , in @xcite , two lasso - based estimators , namely the a - sparseva - aic - re method and the a - sparseva - bic - re method , are suggested . both methods need to do the lse in the first step , then solve a lasso optimization problem , and finally redo the lse estimation .    </S>",
    "<S> this note concerns the case that @xmath8 is a fixed sparse vector . </S>",
    "<S> however , when sparse estimators are applied to estimate non - sparse vectors , erratic phenomena could happen . for details , </S>",
    "<S> please see the discussions in @xcite .    in this note , we will present another approach to estimate the sparse vector @xmath42 , which also possesses the oracle property with a lighter computational cost . </S>",
    "<S> the proposed method consists of three steps , in the first step , a normal lse is conducted , the second step is to solve a lp ( linear programming ) problem , whose solution is given by a soft - thresholding step , finally , redo the lse based on the support set of the estimated vector from the previous lp problem . </S>",
    "<S> details will be given in section 2 .    </S>",
    "<S> in the following , the lower bold case will be used to denote a vector and capital bold characters are used to denote matrices . </S>",
    "<S> the subsequent sections are organized as follows . in section 2 </S>",
    "<S> , we will describe the algorithm in detail and an analytical solution to the lp problem is given . in section 3 , we will analyze the algorithm in detail . in section 4 , </S>",
    "<S> we conduct several examples to illustrate the efficacy of the proposed algorithm and compare the proposed algorithm with other algorithms . </S>",
    "<S> finally , we draw conclusions of the note .    algorithm description the algorithm consists of the following three steps :    * _ lse : _ compute the lse of @xmath42 , denoted as @xmath43 . * _ lp : _ choose @xmath44 and solve the following linear programming problem : @xmath45 where @xmath46 . </S>",
    "<S> detect the support set @xmath47 of @xmath48 . </S>",
    "<S> * _ re - lse : _ compute the lse of @xmath42 based on @xmath47 . </S>",
    "<S> form the matrix @xmath49 , which contains the columns of @xmath19 indexed by @xmath47 and let @xmath50 denote its pseudo - inverse . then </S>",
    "<S> the final estimation @xmath51 is given by @xmath52 , and @xmath53 , in which @xmath54 denotes the complement set of @xmath47 .    </S>",
    "<S> note that the lp problem has an analytical solution . </S>",
    "<S> writing the @xmath55 norm constraint explicitly as @xmath56 we can see that there are no cross terms in both the objective function and the constraint inequalities , so each component can be optimized separately . from this observation , </S>",
    "<S> the solution of the lp problem is given as @xmath57for @xmath58 . </S>",
    "<S> such a solution @xmath48 is also referred to as an application of the soft - thresholding operation to @xmath43 , see e.g. @xcite . </S>",
    "<S> several remarks related to the algorithm are given as follows .    </S>",
    "<S> note that the tuning parameter @xmath59 chosen as @xmath60 is very similar to the one ( which is proportional to @xmath61 ) as given in @xcite based on the akaike s information criterion ( aic ) .    </S>",
    "<S> the _ order _ of @xmath59 chosen as @xmath62 is essential to make the asymptotical oracle property hold . </S>",
    "<S> intuitively speaking , such a choice can make the following two facts hold .    1 .   </S>",
    "<S> whenever @xmath63 , @xmath8 will lie in the feasible region of eq . </S>",
    "<S> ( [ eq.lp ] ) with high probability . </S>",
    "<S> 2 .   </S>",
    "<S> the threshold decreases slower ( in the order of @xmath37 ) than the variance of the pseudo noise term @xmath64 . </S>",
    "<S> with such a choice , it is possible to get a good approximation of the support set of @xmath8 in the second step .    </S>",
    "<S> though the formulation of eq . </S>",
    "<S> ( [ eq.lp ] ) is inspired by the dantzig selector in @xcite , there are some differences between them .    1 .   </S>",
    "<S> as pointed out by one of the reviewer , both the proposed method and the dantzig selector lie in the following class @xmath65 if @xmath66 is chosen as the identity matrix , we obtain the proposed method ; if @xmath66 is chosen as @xmath67 , then we obtain the same formulation as given by the dantzig selector . </S>",
    "<S> 2 .   </S>",
    "<S> as pointed out in @xcite , the solution path of the dantzig selector behaves erratically with respect to the value of the regularization parameter . </S>",
    "<S> however , the solution path of eq . </S>",
    "<S> ( [ eq.lp ] ) with respect to the value of @xmath59 behaves regularly , which is due to the fact that , given @xmath59 , the solution to eq . </S>",
    "<S> ( [ eq.lp ] ) is given by the application of the soft - thresholding operation to the lse estimation . </S>",
    "<S> when @xmath59 increases , the solution will decrease ( or increase ) linearly and when it hits zero , it will remain to be zero . </S>",
    "<S> this in turn implies computational advances when trying to find a @xmath6-sparse solution for given @xmath6 . </S>",
    "<S> a simple illustration of the solution path is given . </S>",
    "<S> assume that @xmath68 and @xmath69^{t}$ ] , then the solution path to eq . </S>",
    "<S> ( [ eq.lp ] ) w.r.t . </S>",
    "<S> @xmath59 is given in fig . </S>",
    "<S> [ fig.path ] ) .    from a computational point of view , the scad method needs to solve a non - convex optimization problem which will suffer from the multiple local minima , see the discussions in @xcite . </S>",
    "<S> hence , the proposed scheme is mainly compared with techniques which can be solved as convex optimization problems . in table </S>",
    "<S> [ comp.table ] , we list the computational steps needed for different methods . in the table </S>",
    "<S> , the term st means the soft - thresholding operation , the term re - lse means redo the lse estimation after detecting the support set of the result obtained from the second step. for a more precise description , see the algorithm description section . from this table , we can see that in the first step , all the methods need to do a lse estimation ; in the second step , except the proposed method ( which is denoted by lp + re - lse ) , the other methods need to solve a lasso optimization problem , which is more computationally involved than a simple soft - thresholding operation as needed by the proposed method ; except the adalasso method , the other methods need to do a re - lse step . from this table </S>",
    "<S> , we can also see that the main computational burden for the proposed method comes from the lse step .    </S>",
    "<S> .computational steps needed for different methods [ cols=\"<,<,<,<\",options=\"header \" , ]     [ comp.table ]    note that the proposed method does not need an `` adaptive step '' ( i.e. to reweigh the cost function ) in order to achieve the oracle property , which is different from the methods presented in @xcite and @xcite .    </S>",
    "<S> analysis of the algorithm in this section , we will discuss the properties of the presented estimator . in the following </S>",
    "<S> , we will denote the smallest singular value of @xmath19 as @xmath70 .    </S>",
    "<S> in the following sections , we assume that the noise variance equals one , i.e. @xmath71 , for the following reasons :    1 .   when the noise variance is given in advance , one can always re - scale the problem accordingly . </S>",
    "<S> 2 .   </S>",
    "<S> even if the noise variance is not known explicitly ( but is known to be finite ) , the support of @xmath42 will be recovered asymptotically . </S>",
    "<S> this is a direct consequence of the fact that finite , constant scalings do not affect the asymptotic statements , i.e. we can use the same @xmath59 for any level of variance without influencing the asymptotic behavior .    the following facts ( lemma 1 - 3 ) will be needed for subsequent analysis . </S>",
    "<S> since their proofs are standard , we state them without proofs here . using the notations as introduced before </S>",
    "<S> , one has that    @xmath72 .    </S>",
    "<S> @xmath73 is a gaussian random vector with distribution @xmath74 .    </S>",
    "<S> given @xmath75 , then @xmath76    in the following , we will first analyze the probability that @xmath8 lies in the constraints set of the lp problem given by eq . </S>",
    "<S> ( [ eq.lp ] ) . </S>",
    "<S> then we give an error estimation of the results given by eq . </S>",
    "<S> ( [ eq.lp ] ) . after this </S>",
    "<S> , we will discuss the capability of recovering the support set of @xmath42 by eq . </S>",
    "<S> ( [ eq.lp ] ) , which will lead to the asymptotic oracle property of the proposed estimator .    for all @xmath77 , </S>",
    "<S> one has that @xmath78 [ lm.prob ]    by lemma 2 , and noticing that @xmath79 is a gaussian random vector with distribution @xmath74 , we have that @xmath80 application of lemma 3 gives the desired result .    for all @xmath77 , if @xmath81 , then @xmath82 .    </S>",
    "<S> define @xmath83 as @xmath84 , so we have @xmath85 . </S>",
    "<S> analyze the @xmath35th element of @xmath86 that @xmath87 the first inequality is by definition , the second inequality comes from the cauchy inequality , the last inequality is due to the assumption of the lemma .    combining the previous two lemmas </S>",
    "<S> gives    @xmath88 .    </S>",
    "<S> the proof goes as follows @xmath89 the first inequality comes from lemma 5 , and the second inequality follows from lemma 4 .    </S>",
    "<S> the above lemma tells us that @xmath8 will lie inside the feasible set of the lp problem as given in eq . </S>",
    "<S> ( @xmath90 ) with high probability </S>",
    "<S> . by a proper choice of @xmath59 , the following result is concluded .    </S>",
    "<S> given @xmath91 , and let @xmath60 , we have that @xmath92    next , we will derive an error bound ( in the @xmath93- norm ) of the estimator given by the lp formulation . </S>",
    "<S> define @xmath94 as the error vector of lp formulation . </S>",
    "<S> we have that the error term @xmath95 is bounded as follows :    for any @xmath96 , if @xmath82 , then we have that @xmath97 .    </S>",
    "<S> we first consider the error vector on @xmath10 which is given by @xmath98 . </S>",
    "<S> since @xmath82 and @xmath99 , we have that @xmath100 . </S>",
    "<S> it follows from the previous discussions that @xmath48 is obtained by application of the soft - shresholding operator with the threshold @xmath59 , applied componentwise to @xmath43 , hence we obtain that @xmath101 . </S>",
    "<S> this implies that @xmath102 .    </S>",
    "<S> next we consider the error vector on the support @xmath11 , denoted as @xmath103 . from the property of the soft - thresholding operation </S>",
    "<S> , it follows that @xmath104 then we have that @xmath105    combining both statements gives that @xmath106 .    plugging in the @xmath59 as chosen in previous section </S>",
    "<S> , we can get the error bound of the lp formulation . </S>",
    "<S> however , the estimate @xmath48 is not the final estimation , instead it will be used to recover the support set of @xmath42 . </S>",
    "<S> the following theorem states this result formally . for notational convenience , </S>",
    "<S> @xmath107 is used to denote the recovered support from the lp formulation , and @xmath108 then denotes the estimate after the second lse step using @xmath37 observations . </S>",
    "<S> finally , the vector @xmath109 denotes the lse as if the support of @xmath8 were known ( i.e. the oracle presents ) using @xmath37 observations .    </S>",
    "<S> we will first get a weak support recovery result and based on this , we further prove that the support as recovered by the lp formulation will converge to the true support @xmath11 with probability 1 when @xmath37 goes to infinity .    given @xmath91 , and assume that the matrix @xmath19 has singular values which satisfies eq . </S>",
    "<S> ( [ eq.sing ] ) , with constants @xmath110 as given there . </S>",
    "<S> let @xmath111 , and @xmath60 , then @xmath112    let the vector @xmath113 denote @xmath114 . since @xmath115 , one has that @xmath116 , in which @xmath117 follows a normal distribution @xmath118 . without loss of generality , assume that @xmath119 are the nonzero elements of @xmath42 and their values are positive . </S>",
    "<S> since @xmath59 decreases when @xmath37 increases , so there exist a number @xmath120 , such that @xmath121 for all @xmath122 . in the following derivations </S>",
    "<S> , we use @xmath123 to denote the element in the @xmath35th row , @xmath124th column of @xmath125 and @xmath126 denotes the @xmath35th element of @xmath117 . </S>",
    "<S> when @xmath127 , we have the following bound of @xmath128 : @xmath129 where @xmath130 . </S>",
    "<S> the second inequality in the chain holds due to the fact that the probability distribution function of @xmath126 is monotonically increasing in the interval @xmath131 $ ] , together with results in lemma 3 .    </S>",
    "<S> then we can see that both terms in ( [ eq.pbconst ] ) will tend to 0 as @xmath132 for any fixed @xmath133 , i.e. @xmath134 .    </S>",
    "<S> notice the fact that @xmath135 and from the previous lemma , we know that the right hand side will tend to 1 as @xmath37 tends to infinity , so it also holds that @xmath136    based on the previous lemma , we have    given @xmath91 , and assume that the matrix @xmath19 has singular values which satisfies eq . </S>",
    "<S> ( [ eq.sing ] ) , with constants @xmath110 as given there . </S>",
    "<S> let @xmath111 , and @xmath60 , then it holds that @xmath137    from the proof in the previous lemma , we have that when @xmath138 @xmath139 since @xmath44 and @xmath140 , one has that @xmath141 and @xmath142 will tend to zero if @xmath132 . </S>",
    "<S> hence there exists a number @xmath143 such that for all @xmath144 one has that @xmath145 and @xmath146 . </S>",
    "<S> hence @xmath147 furthermore , it can be seen that @xmath148 in the following , we will show that @xmath149 . by a change of variable using @xmath150 , we have that @xmath151 with @xmath152 the gamma function . and </S>",
    "<S> hence @xmath153 application of the borel - cantelli lemma [ 4 ] implies that the events in @xmath154 will not happen infinitely often , which concludes the result .    </S>",
    "<S> illustrative experiments    this section supports the findings in the previous section with numerical examples and make the comparisons with the other algorithms which possess the oracle property in the literature .    </S>",
    "<S> experiment 1 this example is taken from @xcite . </S>",
    "<S> the setups are repeated as follows .    * </S>",
    "<S> @xmath42 is set to be @xmath155 ; * rows of matrix @xmath156 are i.i.d . </S>",
    "<S> normal vectors ; * the correlation between the @xmath157-th and the @xmath158-th elements of each row are given as @xmath159 ; * the noise term @xmath160 follows distribution @xmath161 .    </S>",
    "<S> based on these setups , the proposed method and also the methods presented in @xcite ( the a - sparseva - aic - re method and the a - sparseva - bic - re methods ) and @xcite ( the adalasso method ) are applied to recover @xmath42 . in this experiment , @xmath162 for the proposed method </S>",
    "<S> is set to @xmath163 ; @xmath164 for adalasso is chosen as @xmath165 ( this choice satisfies all the assumptions in theorem 2 in @xcite ) , and @xmath166 is set to 1 ; the thresholding value ( for detecting zero components from the solution of the lasso problem ) for the a - sparseva - aic - re and a - sparseva - bic - re are set to be @xmath167 as suggested in @xcite . for the comparison , </S>",
    "<S> we also include the experiment result obtained by using the lasso method , in which we set the tuning parameter as @xmath168 . in fig . </S>",
    "<S> [ fig.exp ] , for every @xmath37 , experiment is repeated 50 times to get the estimated mse . </S>",
    "<S> the following abbreviations are used in fig . </S>",
    "<S> [ fig.exp ] : ( 1 ) the curve with tag lse gives the mse of the estimates by the lse algorithm ; ( 2 ) the curve with tag lp + re - lse gives the mse of the estimates given by the proposed algorithm ; ( 3 ) the curve with tag oracle - lse gives the mse of the estimates by the oracle lse ; ( 4 ) the curves with tags a - sparseva - aic - re and a - sparseva - bic - re give the mse of the estimates by the methods presented in @xcite ; ( 5 ) the curve with tag adalasso gives the mse of the estimates by the adalasso method presented in @xcite ; ( 6 ) the curve with tag lasso gives the mse of the estimates of the lasso method .    note </S>",
    "<S> that , when @xmath37 becomes large , the curves lp + re - lse and oracle - lse exactly match each other .    </S>",
    "<S> fig . </S>",
    "<S> [ fig.support ] demonstrates the efficacy of support recovery of the lp formulation in eq . </S>",
    "<S> ( [ eq.lp ] ) for different choices of @xmath162 . in the plot , portion is defined as the ratio of successful trials over the total number of trials . </S>",
    "<S> we conclude the empirical observations for this experiment in the caption of the figure .    in practice , cross validation technique could be exploited to choose the tuning parameters . in the following </S>",
    "<S> , we will take the adalasso and the proposed method for granted to illustrate the idea and compare the performances for both methods when the parameters are obtained by the cross validation technique . in the adalasso algorithm and the proposed algorithm </S>",
    "<S> , there are two tuning parameters , namely @xmath166 for the adalasso , and @xmath162 for the proposed method . in the following part </S>",
    "<S> , we will apply the 5-fold cross - validation method ( see @xcite ) to choose the tuning parameters and then compare their performances based on the chosen tuning parameters . </S>",
    "<S> the procedure is as follows . at first , the tuning parameter is obtained by 5-fold cross validation , then it is applied to an independently generated test data which has the same dimension as the training data and the evaluation data . for different @xmath37 </S>",
    "<S> , we run 100 i.i.d . </S>",
    "<S> realizations . in each realization , we record the value @xmath169 , where @xmath170 denotes the estimate obtained by the estimator . </S>",
    "<S> @xmath162 are selected from @xmath171 , @xmath166 are selected from @xmath172 , and @xmath37 are chosen from @xmath173 . </S>",
    "<S> the results are reported in fig . </S>",
    "<S> 4 .    </S>",
    "<S> [ fig.comp ]    experiment 2 in this part , we perform an experiment for recovering the sinusoids from noisy measurements . </S>",
    "<S> the data is generated as follows : @xmath174 here both @xmath175 and @xmath176 are unknown , but we know that the frequencies do belong to a ( larger , but of constant size ) set @xmath177 of @xmath178 elements . by sampling the system with period @xmath179 </S>",
    "<S> , we obtain the system @xmath180 where @xmath181^{t}$ ] . </S>",
    "<S> the matrix @xmath17 is defined as follows . </S>",
    "<S> the @xmath35-th row of @xmath19 is given by @xmath182,\\ ] ] for @xmath183 . </S>",
    "<S> the parameter term and noise term are defined as @xmath184^t$ ] , and @xmath185^{t}$ ] .    in this experiment , @xmath186 and @xmath187 , @xmath188 for @xmath189 . </S>",
    "<S> we increase @xmath37 up to 500 and the noise vector @xmath190 satisfies @xmath191 . </S>",
    "<S> we also assume that only the first three entries in @xmath177 occur effectively in the system of eq . </S>",
    "<S> ( [ eq.sin ] ) and the corresponding amplitudes are set to 1 , i.e. @xmath192 and @xmath193 , @xmath194 , @xmath195 . </S>",
    "<S> the sampling period @xmath179 is set to @xmath196 .    </S>",
    "<S> the result using the proposed algorithm to recover @xmath8 is displayed in fig . </S>",
    "<S> [ fig.exp2 ] . </S>",
    "<S> it is again clear that the proposed estimator is as efficient as the oracle estimator if one has enough samples .    </S>",
    "<S> this is indeed predicted by the theory above since the @xmath19 in eq . </S>",
    "<S> ( [ eq.sinsys ] ) obeys the assumption of eq . </S>",
    "<S> ( [ eq.sing ] ) . </S>",
    "<S> this follows from the proposition given as :    there exist constants @xmath197 which do not depend on @xmath37 , such that the following results hold . for any @xmath198 , one has that : @xmath199 and for any @xmath200 that : @xmath201    the proof is given in appendix a. with this proposition , an application of gergorin circle theorem implies that the eigenvalues of @xmath202 will increase with the order of @xmath37 , which in turn implies eq . </S>",
    "<S> ( [ eq.sing ] ) .    </S>",
    "<S> conclusion this note presents an algorithm for solving an over - determined linear system from noisy observations , specializing to the case where the true parameter vector is sparse . </S>",
    "<S> the proposed method does not need one to solve explicitly an optimization problem : it rather requires one to compute twice the lse step , as well to perform a computationally cheap soft - thresholding step . </S>",
    "<S> also , it is shown formally that the proposed method achieves the oracle property . </S>",
    "<S> an open question is to quantify how many samples would be sufficient to guarantee exact recovery of @xmath8 for given sparsity level @xmath6 . in this note , we resort to the asymptotic borel - cantelli lemma ( there exists such a number ) , but it is often of interest to have an explicit characterization of this number . </S>",
    "<S> another open question is how to find a suitable weighting matrix @xmath203 which can further improve the performance of the proposed algorithm .    12    rojas , c. & hjalmarsson , h. ( 2011 ) . </S>",
    "<S> sparse estimation based on a validation criterion . _ the 50th ieee conference on decision and control and european control conference ( cdc - ecc11 ) _ , orlando , usa . </S>",
    "<S> donoho , d. l. ( 2006 ) . compressed sensing . </S>",
    "<S> _ information theory , ieee transactions on , _ </S>",
    "<S> 52(4 ) , 1289 - 1306 . </S>",
    "<S> germany : de gruyter . </S>",
    "<S> plackett , r. l. ( 1950 ) . </S>",
    "<S> some theorems in least squares . </S>",
    "<S> biometrika , 37(1/2 ) , 149 - 157 . </S>",
    "<S> rick , d. ( 2005 ) . </S>",
    "<S> _ probability : theory and examples ( 2nd edition)_. duxbury press . </S>",
    "<S> donoho , d. l. & johnstone , i. m. ( 1995 ) . </S>",
    "<S> adapting to unknown smoothness via wavelet shrinkage . </S>",
    "<S> _ journal of the american statistical association _ , 90(432 ) , 1200 - 1224 . </S>",
    "<S> sderstrm , t. & stoica , p. ( 1989 ) . </S>",
    "<S> _ system identification_. uk : prentice - hall international . </S>",
    "<S> fan , j. & li , r. ( 2001 ) . </S>",
    "<S> variable selection via nonconcave penalized likelihood and its oracle properties . </S>",
    "<S> _ journal of the american statistical association _ </S>",
    "<S> , 96(456 ) , 1348 - 1360 . </S>",
    "<S> cands , e. & tao , t. ( 2007 ) . </S>",
    "<S> the dantzig selector : statistical estimation when @xmath204 is much larger than @xmath178 . _ the annals of statistics _ , 35(6 ) , 2313 - 2351 . </S>",
    "<S> kump , p. , bai , er - w . , </S>",
    "<S> chan , k. s. , eichinger , b. & li k. ( 2012 ) . </S>",
    "<S> variable selection via rival ( removing irrelevant variables amidst lasso iterations ) and its application to nuclear material detection . _ automatica _ , 48(9 ) , 2107 - 2115 . </S>",
    "<S> kukreja , s. l. ( 2009 ) . </S>",
    "<S> application of a least absolute shrinkage and selection operator to aeroelastic flight test data . </S>",
    "<S> _ international journal of control _ , 82(12 ) , 2284 - 2292 . </S>",
    "<S> tibshirani , r. ( 1996 ) . </S>",
    "<S> regression shrinkage and selection via the lasso . </S>",
    "<S> _ journal of the royal statistical society . </S>",
    "<S> series b ( methodological ) _ , 73(3 ) , 267 - 288 . </S>",
    "<S> zou , h. ( 2006 ) . </S>",
    "<S> the adaptive lasso and its oracle properties . </S>",
    "<S> _ journal of the american statistical association _ </S>",
    "<S> , 101(476 ) , 1418 - 1429 . </S>",
    "<S> efron , b. , hastie , t. & tibshirani r. ( 2007 ) . </S>",
    "<S> discussion of the dantzig selector. _ the annals of statistics _ </S>",
    "<S> , 35(6 ) , 2358 - 2364 . </S>",
    "<S> stoica , p. & moses , r. l. ( 1997 ) . </S>",
    "<S> _ introduction to spectral analysis ( vol . </S>",
    "<S> 89)_. new jersey : prentice hall . </S>",
    "<S> kale , b. k. ( 1985 ) . a note on the super efficient estimator . </S>",
    "<S> journal of statistical planning and inference , 12 , 259 - 263 . </S>",
    "<S> leeb , h. & ptscher , b. m. ( 2008 ) . </S>",
    "<S> sparse estimators and the oracle property , or the return of hodges s estimator . </S>",
    "<S> _ journal of econometrics , _ </S>",
    "<S> 142(1 ) , 201 - 211 . </S>",
    "<S> cands , e. j. & wakin , m. b. ( 2008 ) . </S>",
    "<S> an introduction to compressive sampling . </S>",
    "<S> _ signal processing magazine , ieee , _ </S>",
    "<S> 25(2 ) , 21 - 30 . </S>",
    "<S> trevor , j. , hastie , t. , tibshirani , r. j. & friedman , j. h. ( 2005 ) . _ the elements of statistical learning : data mining , inference , and prediction . </S>",
    "<S> _ springer . </S>"
  ]
}