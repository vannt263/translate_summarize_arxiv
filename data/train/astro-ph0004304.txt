{
  "article_text": [
    "clusters of galaxies are the largest objects known to humans ( see figure [ fig : colley_cluster ] ) .",
    "they are the `` mountains '' of the cosmos , and like terrestial mountains they lie in great ranges that define the cosmic `` continents '' and `` oceans '' ( see figure [ fig : ncsa_sheets ] ) .",
    "mapping clusters of galaxies is very much akin to surveying our own world and allows us to understand the creation , evolution and eventual fate of our universe @xcite .",
    "the process by which astronomers detect clusters of galaxies begins with assembling large images of the sky , which are the result of hundreds of nights of observing through a telescope .",
    "these pictures are analyzed to produce a database of galaxies @xmath0 , where @xmath1 .",
    "each record @xmath2 consists of a position on the sky , brightness measurements in one or more bands and possibly hundreds of additional measurements describing the shape and composition of the galaxy .",
    "clusters are local density peaks in the three dimensional distribution of galaxies across the universe . in 3d data , clusters are easy to detect .",
    "unfortunately , the majority of distances to individual galaxies are not known and can only be inferred statistically from empirical models of their brightness .",
    "thus , it is difficult to differentiate small nearby clusters from large far away clusters .",
    "the goal of cluster detection and estimation is to create a catalog , @xmath3 , consisting of thousands of clusters @xmath4 , where @xmath5 .",
    "each cluster in this list , @xmath6 , consists of a position on the sky , a distance estimate , a size estimate and perhaps additional estimated properties of the cluster .",
    "the first catalog of galaxy clusters was compiled by abell @xcite , and has proved extremely useful to astronomers over the past four decades .",
    "abell s catalog was created by visually inspecting hundreds of photographic plates taken from the first palomar observatory sky survey ( poss ) .",
    "modern galaxy databases are too large for such methods to be used today",
    ". subsequent efforts to detect clusters have relied on matched filter techniques taken from statistical signal processing ( e.g. , @xcite , @xcite , @xcite , @xcite and @xcite .",
    "these methods have a strong mathematical foundation , but require extensive prior information and are often computationally prohibitive as they test every possible location in the domain of the cluster space @xmath7 for the presence of a cluster .",
    "the adaptive matched filter @xcite that is described later in this paper is a variation on the matched filter that uses a hierarchical set of filters , as well as software coding and parallel computing techniques that address some of the matched filter s drawbacks .",
    "the amf is adaptive in two ways .",
    "first , the amf uses a two step approach that first applies a coarse filter to find the clusters and then a fine filter to provide more precise estimates of the distance and size of each cluster .",
    "second , the amf uses the location of the data points as a `` naturally '' adaptive grid to ensure sufficient spatial resolution .",
    "a variety of other techniques have also been applied to the cluster finding problem .",
    "the compact nature of clusters make wavelet based signal processing approaches an appealing alternative @xcite and @xcite .",
    "geometric approaches such as voronoi tessellation @xcite have also been used . in this method",
    "each @xmath8 is the seed for the tessellation .",
    "clusters are then found by computing the volume of each tessel and selecting the points with the smallest volume , which presumably have the highest density .",
    "such geometric methods have the advantage that they require very little prior information .    the matched filter , adaptive matched filter , wavelet and voronoi tessellation approaches all use the three to five high affinity dimensions of @xmath9 ( i.e. , angular position and brightness measurements ) .",
    "these dimensions are continuous real variables that lend themselves to euclidean distance metrics . working in these lower dimensions",
    "allows more compute intensive techniques which are necessary to de - project clusters from the observed data domain @xmath10 to the desired underlying domain in which clusters exist @xmath7 , i.e. , angular position , _ distance _ and size .",
    "more recently , there has been interest in exploiting the low affinity dimensions that are also available in galaxy databases @xcite to enhance detection . as the understanding of these methods increases ,",
    "the exploitation of many dimensions should be possible using advanced datamining techniques ( see e.g. @xcite and @xcite and references therein ) .",
    "these methods have enormous potential for detecting new clusters and possibly separating them into distinct groups thus revealing new classes of galaxy clusters .",
    "the rest of this paper presents in greater detail the amf algorithm , its implementation and results . in section two a detailed derivation of the amf",
    "the derivation is meant to be sufficiently general that it can lend itself to other types of databases .",
    "section three presents the implementation of the amf using a layered software architecture and a client / server parallelization model .",
    "again , these methods are not limited to the specific problem presented here and are applicable to a variety areas . in section four",
    "the results of applying the amf on simulated and real data are discussed .",
    "finally , section five gives the summary and conclusions .",
    "matched filter techniques are widely used in statistical signal processing .",
    "the idea is to convolve the data with a model of the desired signal . in many instances this",
    "can be shown to be optimal detection method in the least squares sense . applying matched filter techniques to point set data is less common but has become the standard method for detecting cluster of galaxies .",
    "the advantage of matched filter techniques is that they are mathematically rigorous , provide well defined selection criteria and produce few false detections .    the adaptive matched filter @xcite enhances the matched filter method by creating a pair of filters ( each correct under own its assumptions ) which can be used to trade off computational complexity versus sensitivity .",
    "the filters are derived by computing the likelihood a cluster exists at a particular point @xmath11 given the data @xmath9 .",
    "various likelihood functions can be derived ; the differences are due to the additional assumptions that are made about the distribution of the data .",
    "this section gives the mathematical derivation of the two likelihood functions used in the amf : @xmath12 and @xmath13 .",
    "both derivations are conceptually based on virtually binning the data , but make different assumptions about the distribution of points in the virtual bins .",
    "imagine dividing up the data domain @xmath10 into bins .",
    "we assign to each bin a unique index @xmath14 .",
    "the expected number of data points in bin @xmath14 given that their is a cluster at @xmath15 is denoted @xmath16 .",
    "the number of data points actually found in bin @xmath14 is @xmath17 . in general , the probability of finding @xmath17 points in cell @xmath14 is given by a poisson distribution p_j ( ) = the likelihood of the data given the model is computed from the sum of the logs of the individual probabilities = _ j p_j ( ) .",
    "if the virtual bins are made big enough that there are many galaxies in each bin , then the probability distribution can be approximated by a gaussian p_j ( ) = \\ { - }   .",
    "furthermore , let the model distribution consist of a background field ( that is independent of @xmath15 ) and a cluster component ( that depends on @xmath15 ) n_^j ( ) = n_f^j + n_c^j ( )   .",
    "if the field contribution is approximately uniform and large enough to dominate the noise then p_j ( ) = \\ { - }   . summing the logs of these probabilities results in the following expression for the coarse likelihood _ ( ) & = & _ j p_j + & = & - _ j 2 n_f^j - _ j the first term is independent @xmath15 and can be dropped .",
    "in addition , if the bins can also be made sufficiently small , then the sum over all the bins can be replaced by an integral _ ( ) = -__x d   , where @xmath19 and @xmath20 is a sum of dirac delta functions corresponding to the locations of the points @xmath8 .",
    "expanding the squared term and replacing @xmath21 with @xmath22 yields _ ( ) = -__x d   .",
    "the above expression can be simplified by setting @xmath23 , dropping all expressions that are independent of @xmath15 , and noting that @xmath24 is small compared to the other terms , which leaves _ ( ) = 2 _ i=1^n_x ( _ i ; ) - _ _ x ( ;) n_c ( ; ) d   .",
    "if the virtual bins are chosen to be sufficiently small that no bin contains more than one galaxy , then the calculation of @xmath18 can be significantly simplified because there are only two probabilities that need to be computed .",
    "the probability of the empty bins p_= e^-n_^j and the probability of the filled bins p_= n_^j e^-n_^j .",
    "the sum of the log of the probabilities is then _ &",
    "= & _ p _ + _ p _ + & = & - _ n_^j - _",
    "n_^j + _ n_^j",
    "[ eq : finesum ]    by definition summing over all the empty bins and all the filled bins is the same as summing over all the bins .",
    "thus , the first two terms in equation ( [ eq : finesum ] ) are just the total number of points predicted by the model _ all  bins n_^j & = & _ _ x n _ ( ; ) d + & = & n _ ( ) = n_f + n_c ( )   . @xmath25 and @xmath26 are the total number of field points and cluster points one expects to see inside @xmath10 ; they can be computed by integrating @xmath27 and @xmath28 n_f & = & _ _ x n_f ( ) d   , + n_c ( ) & = & _ _ x n_c ( ; ) d   .    because we retain complete freedom to locate the bins wherever we like , we can center all the filled bins on the points @xmath8 , in which case the third term in equation ( [ eq : finesum ] ) becomes _ n_^j _ i=1^n_x n_(_i ; ) = _ i=1^n_x   , and",
    "the sum is now carried out over all the points instead of all the filled bins . combining these results",
    "we can now write the likelihood in terms that are readily computable from the model and the database _",
    "( ) = - n_f - n_c + _ i   .",
    "subtitutuing @xmath23 and dropping terms that are independent of @xmath15 gives : _",
    "( ) = - n_c + _ i   .",
    "both likelihood functions are applied to the data in a similar manner .",
    "a set of @xmath29 test locations are chosen from the cluster domain @xmath7 .",
    "the likelihood functions are then evaluated at each test location to produce a likelihood map .",
    "the clusters correspond to the peaks in this map that are above a specified threshold . in full generality , producing the likelihood map would require a o(@xmath30 ) function evaluations . for a specific dataset , the model functions @xmath31 and @xmath32 are constructed using prior empirical and theoretical knowledge of the data ( see @xcite for the specific functions ) . from these models",
    "additional symmetries emerge which can be exploited to significantly reduce the computations .",
    "for example , galaxy clusters have a finite angular size so at each test location only the small sub - set of data points which are near the test location need to be considered .",
    "another simplification comes from the fact that clusters have a shape that is roughly independent of the total number of galaxies in the cluster n_c ( ; ) n_c ( ; ) where @xmath33 , and @xmath34 parameterizes the size of the cluster .",
    "this simple modification allows the coarse likelihood function to be re - written as _ ( ) = 2 _ i ( _ i ; ) - ^2 _ _ x ( ;) n_c ( ; ) d   , which can now be solved for @xmath35 by setting @xmath36 _ ( ) = inserting this value back into the previous equation gives _",
    "( ) = _ ( ) _ i ( _ i ; )  .",
    "the result of the above simplification is the elimination of one of the search dimensions , which results in a sizeable computational savings .",
    "the same simplification when applied to the fine likelihood function gives _",
    "= - _ n_c + _ i where @xmath37 is computed by solving n_c = _ i   .",
    "while @xmath38 can be obtained directly from @xmath12 , @xmath37 can only be found by numerically finding the zero point of the above equation .",
    "furthermore , this equation does not lend itself to standard derivative based solvers ( e.g. , newton - raphson ) that produce accurate solutions in only a few iterations . fortunately , the solution can usually be bracketed in the range @xmath39 , thus obtaining a solution with an accuracy @xmath40 takes @xmath41 iterations using a bisection method .",
    "both the coarse and fine likelihood functions are able to exploit the specifics of the model to significantly reduce the number of test locations that need to be evaluated .",
    "the coarse likelihood function requires about 10 times less work to evaluate than the fine likelihood .",
    "unfortunately , the underlying assumptions used in the derivation of the coarse likelihood function are not as accurate as those used to derive the fine likelihood .",
    "thus , while the coarse likelihood is faster , the fine likelihood is more accurate ( see figure  [ fig : coarse_vs_fine ] ) .",
    "the amf addresses this issue by using both likelihood functions in a two stage approach .",
    "first , the coarse likelihood function is applied and then the fine likelihood function is used on the peaks found in the coarse map .    using both filters sequentially",
    "not only produces the best estimate of the cluster locations , it has the added benefit of providing two quasi - independent sets of values for each cluster .",
    "this provides a helpful consistency check because the coarse and fine filter react differently at the detection limit .",
    "the coarse likelihood tends to assign weak detections to small nearby clusters , while the fine likelihood makes these detections large , far away clusters .",
    "thus , if both likelihoods peak at similar size and distance estimates , then the detections are probably real , but if the two likelihoods peak at dramatically different values than the cluster is probably a false detection .",
    "the likelihood functions derived in the previous section represent the core of the amf cluster detection scheme .",
    "both likelihood functions begin with picking a grid of test locations .",
    "the most straightforward method is to use a regularly spaced grid over @xmath7 . recall that each point in @xmath3 consists of an angular position , a distance and a cluster size and that each point in @xmath9 consists of an angular position and a brightness . as shown in the previous section",
    ", the size can be determined without searching , and a regular grid in distance will work reasonably well provided the steps are sufficiently small ( see figure  [ fig : coarse_vs_fine ] ) .",
    "a regular grid in angle has the difficulty of making the grid too big in dense regions and too fine in sparse regions ( i.e. , it is unnecessary to search for clusters where there is no data ) .",
    "a more optimal set of test locations is to use the angular positions of the data @xmath8 , which``naturally '' provides an adpative resolution .      finding peaks in a 3d",
    "regularly gridded map is straightforward .",
    "finding the peaks in the irregularly gridded map is more difficult .",
    "there are several possible approaches .",
    "we present a simple method which is sufficient for selecting individual clusters .",
    "more sophisticated methods will be necessary in order to find small clusters that are close to large clusters .    as a first step we eliminate all low likelihood points @xmath42 , where @xmath43 is the nominal detection limit , which is independent of richness or redshift .",
    "@xmath43 can be estimated from the distribution of the @xmath44 values .",
    "step two consists of finding the largest value of @xmath44 , which is by definition the first and largest cluster @xmath45 .",
    "the third step is to eliminate all test points that are within a certain radius of the cluster .",
    "repeating steps two and three until there are no points left results in a complete cluster list @xmath3 . a different scheme would be to connect the irregularly gridded points in a voronoi tessellation @xcite from which local maxima could be obtained in the same manner as on a grid .",
    "implementation of the amf cluster selection consists of four steps : ( 1 ) reading the database and the model parameter files , ( 2 ) computing @xmath12 over the entire database , ( 3 ) finding clusters by identifying peaks in the @xmath12 map , and ( 4 ) evaluating @xmath13 and obtaining a more precise determination of each cluster s size and distance .    the architecture of this data processing pipeline is shown in figure  [ fig : software_pipeline ] .",
    "the software has been designed so that it can accept both real and simulated data .",
    "one of the challenges of the amf is organizing the software so that it can readily accept new datasets and different parameter files .",
    "critical to adapting to new data is the ability see into the system and observe each step as it takes place . to address these issues",
    "the vast majority of the code has been implemented in an interpreted language ( idl from research systems , inc . ) which provides many mechanisms for reading in files and for monitoring and visualizing output .",
    "the computational driver of the application is the evaluation of @xmath12 .",
    "this function consists of a set of nested for loops which do not lend themselves to the vector notation required to get good performance in an interpreted language .",
    "thus , while the interpreted code is used to set up the calculation , a compiled c routine is called to compute the coarse likelihood function ( see figure  [ fig : one_cpu_app_arch ] ) .",
    "in addition to giving the superior compute performance of a compiled language , this layered software approach also provides a mechanism for exploiting parallel computing .      computing the coarse likelihood map is a highly parallelizable operation .",
    "each test point can be computed independently of the others if all the necessary data is available .",
    "there are a variety of ways to take advantage of this scheme .",
    "the one chosen here is a client / server approach based on the the next generation taskbag ( tnt ) software library @xcite .",
    "tnt is a client - server based applications programming interface ( api ) for distributing and managing multiple tasks on a network - of - workstations ( now ) .",
    "tnt is a c based library which can be used in any compiled program . as such , it is possible to insert the appropriate tnt calls into the compiled layer called by an interpreted language ( see figure  [ fig : mpnow_app_arch ] ) .",
    "the operation of a typical tnt application is shown in figure  [ fig : tnt_app ] .",
    "the server creates a `` taskbag '' of work for clients .",
    "the clients are then executed remotely on a number of processors .",
    "the clients connect with the server and request a task or taskbag ( a group of tasks ) .",
    "when they have completed their tasks they return the results back to the server and ask for more tasks .",
    "the tnt library was developed on linux ( redhat 5.0 ) and tested on freebsd , netbsd , and solaris .",
    "the entire library is written in c using tcp sockets .",
    "a server communicates with the clients using ports , which allows simultaneous servers to be active and listening to different port numbers .",
    "a server can call client functions and a client can call server functions .",
    "this enables the creation of hierarchies of servers .",
    "for example , a `` root '' server can partition a large taskbag into many sub - taskbags and distribute them to a collection of sub - servers .",
    "these sub - servers will then distribute tasks to the clients .",
    "this allows for a more efficient distribution of work across the cluster nodes .    for the amf application",
    ", the interpreted language calls a c routine which then sets up a server with tasks to be executed ( figure  [ fig : tnt_app ] ) . in this case , each task is a sub - set of all the test points .",
    "clients are then started on other computers ( or the same machine if it is multiple processor system ) . at startup",
    "each client receives the database ( or a portion of the database ) from the server . after receiving the data",
    ", each client then asks the server for a task to execute and returns the result .",
    "it is not possible to predict in advance how long a given task is going to take because of the non - linearity of the algorithm and because of heterogeneous capabilities and loads that may exist on the now .",
    "fortunately , tnt is inherently load balancing in the sense that when a client finishes a task it requests additional work .",
    "if there are no tasks remaining then the client exits and frees up the processor .",
    "the processors that run faster will pick up more work and slower processors will pick up less work .",
    "the amf has been extensively tested on simulated data to verify its accuracy and robustness @xcite .",
    "the amf is currently being applied to detect clusters of galaxies from the sloan digital sky survey ( sdss ) @xcite .",
    "the recent parallel implementation of the amf has significantly increased speed of the application .      in real data , neither the distances of the galaxies nor the position and sizes of the clusters are known .",
    "tests on simulated data are the only opportunity to check the detection algorithm in a well understood environment .",
    "the test data consists of 72 simulated clusters with different sizes and distances placed in a simulated field of randomly distributed galaxies .",
    "the data was constructed to be consistent with what is expected from sdss .",
    "the clusters range in size and distance so as to span the full range of expected clusters .",
    "the test data covers an area of 10 square degrees ( 1/1000 of the sdss ) and contains approximately 100,000 points .    to facilitate the subsequent analysis and interpretation of the results ,",
    "the clusters were placed on an 8 by 9 grid .",
    "the cluster centers were separated by 0.4 degrees .",
    "the distribution of all the galaxies in angle is shown in figure  [ fig : simulated_data ] , where each column of clusters are the same size while each row of clusters are at the same distance . from left to right the sizes are @xmath46 10 , 20 , 30 , 40 , 50 , 100 , 200 , and 300 . from bottom",
    "to top the distances are @xmath47 0.1 , 0.15 , 0.2 , 0.25 , 0.3 , 0.35 , 0.4 , 0.45 , and 0.5 .",
    "the simulated data were run through the amf and all clusters above the designated 5-@xmath48 noise limited threshold were detected ( with no false detections ) .",
    "the angular positions of all the detected clusters were well within the expected range . the estimated distance and size of each cluster",
    "is shown in figure  [ fig : simulation_results ] .",
    "as expected , the large and/or nearby clusters are detected and measured more accurately than the small , far away clusters .",
    "these results indicate that the amf can detect and unbiasly estimate the location of clusters .",
    "the sloan digital sky survey is a multi - decade , multi - institution effort to take a million by million pixel composite image of the night sky in five color bands .",
    "analysis of the image data is expected to yield a database of 200,000,000 galaxies @xcite .",
    "test data has been taken since 1998 and been used to test all aspects of the sdss software including the amf ( see figure  [ fig : sloan_results ] ) .",
    "the amf has detected all previously known clusters in the sdss test data it has looked at . in addition , the amf performs at a level equal to or better than the other algorithms with which it has been compared .",
    "detailed results and comparison of various cluster finding algorithms are presented elsewhere @xcite .    adapting the amf to the sdss was a sizeable effort . without the layered software approach",
    "it would have taken considerably longer as a considerable amount of tuning was required to properly set all the model parameters .",
    "a parallel implementation of the amf is crucial for its application to the full sdss database .",
    "currently , the amf requires 1 - 2 hours of cpu time ( 450 mhz pentium ii ) to process a 10 square degree field . on a parallel",
    "now this can easily be sped up by a factor of 100 , which will make processing the entire 10,000 square degree sdss dataset feasible .",
    "the results of running the parallel implementation on a now are shown in table  1 .",
    "these data show that the algorithm experiences good speedup on both heterogeneous and homogeneous nows .",
    "the primary bottlenecks to perfect speedups are the initial sending of data and the granularity of the tasks . the impact of these can be seen in the execution schedules shown in figures  [ fig : timing_a]-[fig : timing_c ] .",
    "the total computation time consists of the time to do the computation plus the slack time due to the granularity of the tasks t_comp +   , where @xmath49 is the number of processors used in the now and @xmath50 is the number of tasks the job was broken into . the total time spent communicating is the time spent gathering the results plus the time to send the initial data to each processor t_comm + n_x n _   .    to achieve good",
    "scalability requires that the computation - to - communication ratio stay high as @xmath49 increases . in both cases the first term scales well while the second term does nt",
    "the second ( granularity ) term in the computation time is due to the fact that some processors will finish first and there will be no additional tasks for them to complete .",
    "this can be alleviated by simply dividing the work up into sufficiently small tasks until this time no longer becomes important .    the second ( startup )",
    "term in the communication time can be dealt with in several ways .",
    "first , the algorithm can be restructured so that each processor gets less data at startup .",
    "second , the communication pattern can be remapped so that the initial data is distributed in multiple steps along a tree . finally ,",
    "since the initial data is the same for each processor , it should be possible to use a multicast to allow the data to be distributed everywhere in a single broadcast . without alleviating this bottleneck the speedup",
    "is limited to approximately 100 on a 100 mbit / s class network .",
    "if the initial transmit bottleneck can be overcome , it should be possible to see speedups in the 5000 range .",
    "we have presented the adaptive matched filter method for the automatic selection of clusters of galaxies from a galaxy database .",
    "the amf is adaptive in two ways .",
    "first , the amf uses a two step approach that first applies a coarse filter to find the clusters and then a fine filter to provide more precise estimates of the distance and size of each cluster .",
    "second , the amf uses the location of the data points as a `` naturally '' adaptive grid to ensure sufficient spatial resolution .",
    "matched filter techniques have a firm mathematical basis in statistical signal processing .",
    "the amf uses a hierarchy of two filters ( each mathematically correct under its assumptions ) . combining these filters",
    "allow the amf to maximize computational performance and accuracy .",
    "the amf also provides two estimates for each cluster which can be compared as an additional check .",
    "this is particular effective for these filters because they react differently when given insufficient data .",
    "the amf relies heavily on models for both the cluster and the background field .",
    "this prior information is quite extensive and makes the amf complex to implement and difficult to adapt to new data sets . to alleviate this coding challenge a hybrid coding approach was used to leverage the ease of use of interpreted languages along with the compute performance of compiled languages . in this way",
    "the complex task of testing model inputs and observing their effect through the data processing pipeline can be done quickly without sacrificing the compute efficiency necessary to complete the application in a timely manner .",
    "a further benefit of the hybrid approach is that it makes available to the compiled code a wide variety of parallel software libraries and tools .",
    "a parallel implementation is critical to the application because matched filter techniques work by testing every possible location in the cluster space for the presence of a cluster .",
    "this is a compute intensive operation , but also provides a high degree of parallelism .",
    "the parallelization scheme used for the amf application is a client / server approach which is a very effective on network - of - workstations .",
    "the tnt client / server software used is lightweight and efficient , and provides a naturally load balancing and fault tolerant framework .",
    "the amf has been extensively tested on simulated data .",
    "these results indicate that it robustly and accurately detects clusters and estimates their positions while having few false positives .",
    "the amf is now being applied to the first results of the sloan digital sky survey @xcite .",
    "these tests have shown that the amf detects all previously known clusters in this data and performs at or above other cluster finding methods .",
    "the amf hybrid application architecture has proven effective in supporting the implementation of new datasets .",
    "the tnt based client / server parallelization scheme has also demonstrated significant speedups which will make it feasible for this application to address to the entire sdss when it becomes available .",
    "jeremy kepner and rita kim would particularly like to thank their ph.d .",
    "advisors profs .",
    "david spergel and michael strauss for supporting this work .",
    "the authors are also grateful to wes colley and mike norman for providing figures  [ fig : colley_cluster ] and [ fig : ncsa_sheets ] . in addition ,",
    "neta bahcall , james gunn , robert lupton and david schlegel provided invaluable assistance in the development of the amf , and aaron marks , maya gokhale , ron minnich , and john degood were most helpful in providing the tnt software library .",
    "we are also greatful to paul monticciolo for his helpful comments .    99 abell , g. 1958 , astrophysical journal supplement , 3 , 211 abell , g. , corwin , h. & olowin , r. 1989 , astrophysical journal supplement , 70 , 1 bahcall , n. 1988 , ara&a , 26 , 631 d. a. bramel , r. c. nichol , a. c. pope to appear in the astrophysical journal colley , w.n . , tyson , j.a . & turner , e.l .",
    ", 1996 , astrophysical journal , 461 , l83 dalton , g. , efstathiou , g. , maddox , s. & sutherland , w. 1994 , monthly notices of the royal astronomical society , 269 , 15 djorgoski , s. et al 1997 , to appear in applications of digital image processing xx , ed .",
    "a.tescher , proc .",
    "3164 b. dasarathy , 1999 , data mining and knowledge discovery : theory , tools and technology , spie volume 3696 li - zhi fang & jesus pando , proceedings of the 5th erice chalonge school on astrofundamental physics , n. snchez and a. zichichi eds .",
    ", world scientfic , 1997 fadda , d. , slezak , e. & bijaoui , a. , 1997 , a&a , 127 , 335 u. fayyad , g. piatetsky - shapiro , p. smyth , and r. uthurasamy , 1996 , advances in knowledge discovery and data mining , mit press r. r. gal , r. r. decarvalho , s. c. odewahn , s. g. djorgovski , v. e. margoniner , 1999 to appear in astronomical journal    kawaski , w. , shimasaku , k. , doi , m. & okamura , s. 1997 astro - ph/9705112 , accepted a&as    j. kepner , x. fan , n. bahcall , j. gunn , r. lupton & g. xu , 1999 astrophysical journal 517 , 78    j. kepner , m. gokhale , r. minnich , a. marks & j. degood , 2000 to appear in cluster computing    kim et al and the sdss collaboration , 2000 bulletin of the american astronomical society , 195 , # 30.03    lumsden , s. , nichol , r. , collins , c. & guzzo , l. 1992 , monthly notices of the royal astronomical society , 258 , 1 postman , m. et al 1996 , astronomical journal , 111 , 615 ramella , m. , nonino , m. , boschlin , w. , & fadda , d. 1998 astro - ph/9810124    alexander s. szalay , peter kunszt , anirudha thakar , jim gray , don slutz adass 99 conference    .",
    "* amf execution times . * execution times in seconds for various numbers of processors and various numbers of tasks .",
    "@xmath49 is the number of processors used .",
    "@xmath49 ( eff ) is the number of processors weighted by their clock speed .",
    "@xmath50 is how many sub - tasks the problem was broken into .",
    "the increased parallel efficiency between rows two and three is due to the use of more tasks which results in less slack time due to the task granularity . the increased parallel efficiency between rows three and four is due to the use of a now with a higher performing interconnect which reduces the time it takes to initially transmit the data . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]        stars .",
    "the flatter reddish patches are spiral galaxies like our own milky way .",
    "the blue arcs around the edge are from a distant background galaxy that has been gravitationally `` lensed '' by the cluster .",
    "this cluster has around one thousand members and lies at a distance of 4.5 billion light years .",
    "[ note : our nearest neighbor the andromeda galaxy is 1.5 million light years away .",
    "] , width=624 ]        ) as a function of distance as computed from the coarse and fine matched filters .",
    "the input cluster has a true distance of 0.35 ( 4.5 billion light years ) and size of @xmath51 .",
    "the coarse likelihood peaks at a distance of 0.31 and estimates the size to be @xmath52 .",
    "the fine likelihood peaks at a distance of 0.35 and a estimates the size to be @xmath53 . in general , the fine likelihood provides better distance and size estimates but at approximately 10 times the computational cost .",
    ", width=624 ]                     10 , 20 , 30 , 40 , 50 , 100 , 200 , and 300 . from bottom",
    "to top the distances are @xmath47 0.1 , 0.15 , 0.2 , 0.25 , 0.3 , 0.35 , 0.4 , 0.45 , and 0.5 .",
    ", width=624 ]                 except that the work has been broken up into 200 tasks which reduces the slack time at the end of the calculation resulting in a speedup of 8.4 out of 10.5 ( 80% ) .",
    ", width=624 ]     but run on sixteen identical dual processor workstations with a higher performance network .",
    "the better network reduces the initial transmit time ( task 0 ) and results in a speedup of 27.3 out of 32 ( 85% ) .",
    ", width=624 ]",
    "* jeremy kepner * received his b.a . in astrophysics from pomona college ( claremont , ca ) .",
    "he obtained his ph.d . focused on computational science from the dept .",
    "of astrophysics at princeton university in 1998 , after which he joined mit lincoln lab .",
    "his research has addressed the development of parallel algorithms and tools and the application of massively parallel computing to a variety of data intensive problems .",
    "e-mail:jvkepner@astro.princeton.edu or kepner@ll.mit.edu    * rita seung jung kim * rita s.j .",
    "kim is completing her ph.d . at princeton university in astrophysical sciences .",
    "she received her b.s . in astronomy at seoul national university ( seoul , korea ) , and spent one year in paris at the institut dastrophysique de paris .",
    "her thesis work concentrates on the properties of cluster galaxies , and has also worked on statistical modeling of the large scale structure of the universe ."
  ],
  "abstract_text": [
    "<S> clusters of galaxies are the most massive objects in the universe and mapping their location is an important astronomical problem . </S>",
    "<S> this paper describes an algorithm ( based on statistical signal processing methods ) , a software architecture ( based on a hybrid layered approach ) and a parallelization scheme ( based on a client / server model ) for finding clusters of galaxies in large astronomical databases . </S>",
    "<S> the adaptive matched filter ( amf ) algorithm presented here identifies clusters by finding the peaks in a cluster likelihood map generated by convolving a galaxy survey with a filter based on a cluster model and a background model . </S>",
    "<S> the method has proved successful in identifying clusters in real and simulated data . </S>",
    "<S> the implementation is flexible and readily executed in parallel on a network of workstations . </S>"
  ]
}