{
  "article_text": [
    "formalized mathematics arises from a desire for rigor .",
    "a formal proof of a theorem is a proof that is complete : every step follows directly from previous steps and known theorems in an algorithmically - verifiable manner .",
    "a number of corpora have been developed for formalized mathematics in various formalisms .",
    "large datasets of formal proofs include metamath  @xcite , the mizar mathematical library  @xcite , flyspeck  @xcite , the archive of formal proofs  @xcite , the coq standard library  @xcite , and the hol light library  @xcite .",
    "these databases cover wide swaths of mathematics .",
    "the metamath ` set.mm ` module , for example , is a collection of theorems and proofs constructing mathematics from zfc .",
    "the module includes a number of important theorems , including zorn s lemma from set theory , the theorem of quadratic reciprocity from number theory , and sylow s theorems from group theory .",
    "the time cost of formalizing proofs is substantial , and so tools to assist in construction of the formal proofs have arisen .",
    "interactive theorem provers automate the technical steps of theorem - proving , leaving the creative steps to the user . over time",
    ", the techniques from interactive theorem provers have been extended to automated theorem provers , complete non - interactive tools for the generation of formal proofs .",
    "rapid advances are being made in automated theorem proving , and recent systems now permit proofs of 40% of the theorems in the mizer mathematical library  @xcite .",
    "these proof systems generally consist of multiple modules , one of which is premise selection : the identification of relevant axioms and theorems .",
    "premise selection has shown promise as a target for machine - learning techniques  @xcite and more recently deep learning  @xcite  the first application of deep learning to automated theorem proving .",
    "while most of the current research has focused on the mizar mathematical library , i demonstrate that the tree structure of metamath proofs is exploitable by modern tree exploration techniques .",
    "holophrasm takes a novel approach to automated theorem proving .",
    "the system uses a variant of uct  @xcite , an algorithm for the tree - based multi - armed bandit problem , to search the space of partial proof trees of a metamath theorem .",
    "recent developments in machine learning have made such searches accessible .",
    "action enumeration is made viable by sequence - to - sequence models  @xcite . in parallel , algorithms developed for go - playing ais describe how neural networks can be used to guide tree exploration  @xcite .",
    "those techniques have been adapted here to create a complete , non - interactive system for proving metamath propositions .",
    "the metamath language is designed for automated theorem verification , utilizing metatheorems and proper substitutions as the standard proof step .",
    "the exact specification of the language is given in section 4 of @xcite , but the relevant details are summarized below      a _ theorem _ in the metamath database is a _ proposition _ if it has a proof or an _ axiom _ if it does not .",
    "the notion of axiom here is general and includes what are traditionally known as axioms , but also includes definitions and the production rules for expressions .",
    "we separate the axioms and propositions in ` set.m ` by their _ type _ , which is  set , \"  class , \"  wff , \" or  @xmath0 \" .",
    "axioms of non-@xmath0\"-type describe the production rules for a context - free grammar with the types as syntactic categories .",
    "an _ expression _ of the given type is a string of the corresponding syntactic category .",
    "these axioms along with the free variables as additional terminal symbols provide a unique parse tree for every expression , and they will be referred to as _",
    "constructor axioms_. henceforth i will conflate the notion of an expression and its parse tree .",
    "propositions of type `` set , '' `` class , '' and `` wff , '' are ignored to maintain uniqueness of the parse trees .",
    "axioms and propositions of `` @xmath0 '' type are assertions that an expression of `` wff''-type is true , that the expression can be proved from the axioms and given hypotheses .",
    "these theorems will be used as nodes in proof trees .",
    "r0.4 [ proof_diagram ]     a theorem @xmath1 of @xmath2-type consists of a number of elements    * @xmath3 , an assertion , which is an expression of `` wff''-type . * @xmath4 , a set of hypotheses , each an expression of `` wff''-type . *",
    "@xmath5 , a set of _ free variables _ that appear in the assertion and hypotheses and a type for each . *",
    "@xmath6 , a set of unordered pairs of _ disjoint variables _ from @xmath5 .",
    "the disjoint variables satisfy @xmath7 for all @xmath8 , and represent pairs of variables which can not share any variables after a proper substitution .",
    "consider a context proposition @xmath9 that is to be proven and an expression @xmath10 of  wff \"- type , either the assertion of @xmath9 or an intermediate step in the proof of @xmath9 .",
    "an application of particular theorem , @xmath1 , to prove @xmath10 consists of a set of substitutions , @xmath11 , into the free variables @xmath5 . in these substitutions ,",
    "earch variable is replaced by an expression of the same type built out of the constructor axioms extended by additional terminal symbols for the free variables of @xmath9 .",
    "the application requires that the assertion of @xmath1 after substitution matches @xmath10 , that is @xmath12 . by performing this process",
    "we reduce the problem of proving @xmath10 to the problem of proving all of the hypotheses @xmath13 .",
    "this process is illustrated in figure  [ proof_diagram ] .",
    "the disjointness property adds a restriction on the allowable substitutions : for every pair @xmath14 , for every free variable @xmath15 , and every free variable @xmath16 it must be the case that @xmath17 .    for a fixed expression , context , and theorem , substitutions that satisfy these properties",
    "are called _",
    "viable_. for a fixed expression and context , a theorem is called viable if it permits a viable set of substitutions .",
    "i divide the variables in @xmath5 into two types . _",
    "constrained _ variables are variables that appear in @xmath3 .",
    "_ unconstrained _ variables are variables that appear in some hypothesis @xmath18 but not @xmath3 .",
    "the substitutions in @xmath11 are called constrained substitutions or unconstrained substitutions if they apply to constrained or unconstrained variables respectively .",
    "constrained substitutions are notable in that , given @xmath10 and @xmath1 , the constrained substitutions are exactly those fixed by the requirement that @xmath19 .",
    "the application of a theorem proves an expression , but also provide a set of hypotheses which must be proven in turn .",
    "this naturally gives a proof a tree structure .",
    "the assertion of the theorem is the root node , and its hypotheses are leaves , because they are assumed to be true without proof . here",
    "i define the notion of a proof tree , but i modify the natural structure slightly to permit compatibility with the notion of a partial proof tree introduced in section  [ ppt ] .    a _ proof tree _ of an expression @xmath10 of  wff \"- type in context @xmath9 is a bipartite tree with two types of nodes , red nodes and blue nodes .",
    "red nodes are labeled by an an expression of  wff \"- type , which is an intermediate step in the proof , and the root node is labeled by @xmath10 . unless its label is a hypothesis in @xmath20 , in which case the node is a leaf , red nodes always have exactly one child , a blue node . blue nodes",
    "are labelled by a pair @xmath21 of a theorem and viable substitutions for that theorem into the parent expression .",
    "blue nodes have one child red node , @xmath22 , for each hypothesis @xmath18 .",
    "if such a tree exists , it is a proof of @xmath10 .",
    "the problem i wish to solve is as follows : given a context theorem @xmath9 , find a proof tree for that theorem s assertion .",
    "the algorithm does so by considering a supertree of potential proofs steps and by using tree exploration techniques to search for the subtree that is a valid proof - tree .",
    "the algorithm will refer to three neural networks , * payoff * , * generative * , and * predictive * which are described in section  [ networks ] .",
    "a _ partial proof tree _ is an extension of the the notion of a proof tree the following changes are made : red nodes are permitted to have no children even if they are not hypotheses .",
    "red nodes are permitted to have multiple multiple child blue nodes , each a potential approach for proving the expression .",
    "a red node is said to be proven if any of its children have been proven or if its label is one of the initial hypotheses .",
    "a blue node is said to the proven if all of its children have been proven .",
    "the subtree of a proven red node is necessarily a supertree of a valid proof tree for its expression . in particular ,",
    "the subtree can be pruned by removing all of the children of red nodes except for one proven blue child .",
    "such a pruned subtree must be valid proof tree .",
    "similarly to uct  @xcite , the algorithm builds a partial proof tree over a series of passes .",
    "each pass traverses the tree downward . at a red node ,",
    "the traversal chooses either to create a new child or to proceed to the highest valuation child blue node . at a blue node",
    ", the traversal proceeds to the worst - performing child .",
    "the pass continues until a new child blue node is created , whereupon its red children are created and valued .",
    "the process repeats until the root node has been proven .    in order to to perform this exploration ,",
    "each node maintains additional state , which is updated whenever the node s children are updated .",
    "red nodes , @xmath10 have an _ initial payoff _ , @xmath23 which is the output of the * payoff * network , evaluated as soon as the node is created .",
    "they additionally have a _ total payoff _ , @xmath24 , which is the the sum of the initial payoff of the node and the total payoffs of its children , a _ visit count _ , @xmath25 , which is 1 plus the sum of the visit counts of its children , and an _ average payoff _ , which is @xmath26 .",
    "blue nodes keep track of their _ least promising child _ , which is the unproven child with the lowest average payoff . the total payoff and",
    "visit count of a blue node are the corresponding values of its least promising child .",
    "blue nodes also have a value @xmath27 , which indicates how likely this substitution is to be applicable and is given by the * relevance * and * generative * networks .      in standard uct ,",
    "when the traversal reaches a leaf , the leaf spawns a new child for each available action at that node , but doing so is impractical in this context . in this variant",
    ", the number of actions available at red nodes can be infinite , since there are infinitely many unconstrained substitutions that could be made into some theorems .",
    "the difficult part of the calculation is determining viable actions rather than calculating payoff . to this end",
    ", we attempt to maintain the number of children of a red node , @xmath10 to @xmath28 , so that more actions are considered after consecutive visits .",
    "when a red node is visited for the first time , the node calculates its initial payoff but does not generate any children . when the node is visited later , it checks if it has sufficiently many children .",
    "if so the algorithm visits an extant chid as described in section  [ redvisit ] . if not , the algorithm creates a new child as described in section  [ expand ] .",
    "when a blue node is visited , it immediately visits its least promising child .      to determine which child",
    "is visited from a red node , each extant child @xmath29 is assigned a priority , @xmath30 for constants @xmath31 and @xmath32 . the highest priority child is then visited .",
    "the @xmath33 arises in the standard ucb algorithm as the upper confidence bound and the correction @xmath34 encourages exploring propositions with a high probability first  @xcite . during the experiment , @xmath32 and @xmath31",
    "were assigned the values 1.0 and 0.5 respectively .",
    "the children of a red node are constructed by assigning to them a theorem and substitutions .",
    "each pair @xmath35 of theorem @xmath1 and substitution @xmath11 is assigned a value @xmath36 . here",
    "@xmath37 is the probability that @xmath1 is the next theorem to apply , as given by the * relevance * network .",
    "@xmath38 is correspondingly the probability of the best theorem . for a fixed @xmath1 ,",
    "@xmath39 is the probability of those substitutions as given by the * generative * network .",
    "@xmath40 is correspondingly the probability of the best substitution .",
    "children of the red node are added from this expansion queue in decreasing order of value .",
    "evaluation of the the * relevance * and * generative * network are performed in a just - in - time manner , evaluating * relevance * during the second visit to a node and evaluating * generative * only when a previously unconsidered theorem is due to be added as a child .    when a blue node is added to the child in this way",
    ", the algorithm immediately visits each of the blue node s children once to estimate their payoffs .      in practice",
    "a few additional changes can be made to make the algorithm more efficient .",
    "* circularity * : any attempt to create a red node with the same expression as one its ancestors fails : the parent blue node is removed from the expansion queue of its parent and the next pair @xmath41 is added instead .    *",
    "node death * : while the theoretical number of actions from a given red node is infinite , in practice the number of actions is limited by the beam search width of the * generative * network . this may lead to instances where a red node has no children and its expansion queue is empty .",
    "such a red node is called _",
    "dead_. a blue node is said to be dead if any of its children are dead .",
    "dead blue nodes are removed from the graph and their ancestors are subsequently checked for death .",
    "* multiprocessing * : the algorithm can be run efficiently in parallel .",
    "when doing so , the different threads traverse the proof tree asyncronously .",
    "following @xcite , the priority function for a blue node @xmath29 from a red node is modified by replacing the @xmath42 term with @xmath43 , where @xmath44 is the number of threads currently exploring a descendent of @xmath29 and @xmath45 is a constant , chosen here to be 3 .    * generative length limits * : when evaluating the * generative * network , outputs with more than 75 tokens in total across all unconstrained substitutions are discarded during the beam search .",
    "the beam search returns no substitutions if all items in the beam reached the size limit .",
    "if so , a dummy child is added to the red node with a payoff of 0 to discourage further exploration of this node    * last step * : when a red node is added when proving the context @xmath9 , the viable theorems are determined . for the viable theorems , @xmath1 , if there are viable substitutions @xmath11 such that @xmath46 then that theorem and those substitutions are immediately added as a blue node .",
    "three distinct neural networks were used in the algorithm . the * payoff * network estimates the payoff of red nodes . the * relevance * network predicts which theorems will be useful at a given step .",
    "the * generative * network generated unconstrained substitutions .",
    "a token is created for each constructor axiom .",
    "a number of dummy variables are created and assigned tokens , the minimum number such that for each theorem the numbers of  set \" ,  wff \" ,  class \" free variables are at most the number of dummy variables of the corresponding type .",
    "five special tokens are added , ` eoh ' for the end of a hypothesis , ` eos ' for the end of a section , ` start ' for the start of sequence generation , ` uv ' for an unconstrained variable , and ` target ' for a target unconstrained variable .",
    "the inputs are modified for each iteration by randomly replacing the free variables that appear with distinct dummy variables of the corresponding type .",
    "each expression is tokenized by reading the tokens of the constructor axioms in its parse tree in a depth - first pre - order .",
    "hypotheses are separated by the ` eoh ' token .",
    "if multiple different components are inputted , such as an assertion and set of hypotheses , they are separated by the ` eos ' token .",
    "the other three special tokens are used only by the * generative * network and are are described later .",
    "the networks all share a number of similar features .",
    "in general , the embedding vectors for tokens were inputted into 2 layers of bidirectional grus with internal biases for the gru units and hidden layer dimensions of size 128 .",
    "gru weights were permitted to vary between different sections of input and output , but the token embedding vectors were shared .",
    "the embedding vectors were augmented with four additional dimensions describing the graph structure of the input , the depth of the node , the degree of the node , the degree of its parent , and its position into the degree of its parent .",
    "all fully - connected layers used leaky relus with @xmath47 and had dimension 128 unless otherwise specified .",
    "weights were regularized by their @xmath48-norm with a regularization factor of @xmath49 .",
    "the * payoff * network takes as input an expression , @xmath10 , and a set of hypotheses @xmath20 , which are fed into the grus . the network attempts to predict whether the expression is provable from the hypotheses .",
    "the outputs of both sides of the bidirectional network are concatenated and fed through two fully connected layers with leaky relu units and a fully connected layer with a sigmoid to obtain the classification probability , @xmath50 .",
    "the network is trained on known proof steps as positive examples and on incorrect proof steps generated by the * relevance * and * generative * networks as negative examples . during training",
    ", the cross - entropy loss is minimized .",
    "the * relevance * network takes as input an expression , @xmath10 , and a set of hypotheses @xmath20 , and attempts to classify the next proposition that will used in the proof of @xmath10 .",
    "the * relevance * network is designed as two parallel networks .",
    "the first parallel branch takes @xmath10 and @xmath20 as inputs and returns a 128-dimensional expression - vector @xmath51 .",
    "the second parallel branch is evaluated separately for all theorem @xmath1 of  @xmath0\"-type , inputs @xmath3 and @xmath4 , and returns a 128-dimensional theorem - vector @xmath52 . the probabilities are computed as the softmax over theorems @xmath1 , @xmath53 , where @xmath54 for some weight matrix @xmath55 .",
    "this structure permits generalizability to new theorems while simultaneously allowing for the theorem vectors to be precomputed and cached .",
    "the network is trained using a negative - sampling loss with four negative samples ; at each iteration , only five theorems are considered : the correct theorem @xmath56 and four incorrect theorems @xmath57 chosen uniformly at random from the viable theorems for @xmath10 .",
    "the training loss is computed as @xmath58 and is minimized .",
    "given an expression , @xmath10 , a set of hypotheses , @xmath20 , and a theorem , @xmath1 , to be applied , the * generative * network uses a sequence - to - sequence model @xcite with an intermediate fully - connected layer to create expressions for the unconstrained substitutions .    to execute the network , an unconstrained variable in @xmath5",
    "is chosen uniformly at random to be the target .",
    "a set of substitutions @xmath11 is generated as follows .",
    "for @xmath59 a constrained variable , @xmath60 is the expression needed for @xmath12 . @xmath11",
    "additionally maps the target unconstrained variable to the ` target ' special token and the other unconstrained variables to the ` uv ' special token .",
    "the sequence - to - sequence model is used to generate an expression for @xmath11 applied to the target variable .",
    "@xmath11 is updated to include this as a substitution , a new target unconstrained variable is chosen , and the process repeats until all variables have substitutions .",
    "the network takes as inputs @xmath13 and @xmath20 .",
    "a fully - connected layer is applied to the outputs of each direction and the result is used as the initial state of the grus for the sequence - to - sequence output .",
    "an attentional model is added , following the _ general _ model of @xcite .",
    "the output sequence is initialized with the ` start ' token .",
    "the outputs of the last gru layer are fed through a fully - connected layer with relu nonlinearity and then a fully - connected layer with softmax nonlinearly to obtain token probabilities . during training , the total cross - entropy loss of the output tokens is minimized .    during execution ,",
    "multiple outputs are given , following the beam search technique of @xcite .",
    "the tokens which can be included are restricted by a number of filters .",
    "only constructor axioms defined before the current context may be used",
    ". no  wff \" or `` class '' variables may be added unless they appear elsewhere in the context . at most one new such set variable",
    "is considered during selection for a given token .",
    "furthermore , no token may be added if doing so would violate the disjoint variable conditions .",
    "the theorems of the metamath ` set.mm ` module are used as the data set , discarding axioms and keeping only propositions of  @xmath0\"-type . of these propositions ,",
    "21786 were selected as a training set , 2711 as a validation set and 2720 as a test set .",
    "the proofs are expanded into a full proof tree , and each step of ",
    "@xmath0\"-type was recorded .",
    "the * relevance * network was trained and evaluated on all of the proof steps for the propositions .",
    "the expansion of the propositions into proof steps provides 1.2 m training proof steps , 120k validation proof steps and 158k testing proof steps .",
    "the * generative * network was trained on the proof steps where a proposition was applied that had at least one unconstrained variable .",
    "this constraint leaves 426k training proof steps , 38k validation proof steps , and 56k testing proof steps .",
    "data for the * payoff * network was generated by including all of the proof steps as positive examples excluding duplicates .",
    "additionally , negative examples were generated by using the trained * relevance * and * generative * networks to predict the best two proposition / substitution pairs using the valuation described in section  [ expand ] .",
    "the hypotheses generated by applying these propositions were included as negative examples after removing all the the hypotheses that were equivalent to positive examples .",
    "there were 587k positive and 960k negative training examples , 69k positive and 113k negative validation examples , and 74k positive and 120k negative test examples .",
    "network weights were initialized with xavier initialization @xcite .",
    "training for each network was done using stochastic gradient descent with a batch size of 100 , with an adam optimizer @xcite .",
    "learning rates started at @xmath49 , were decayed by a factor of 2 each time the validation loss failed to decrease , and training was ended after the validation loss failed to decrease for three consecutive epochs .",
    "the neural networks are trained separately .",
    "the * relevance * network is tested , selecting from all viable propositions . on the test data * relevance * obtains a 55.3% top-1 accuracy , a 72.8% top-5 accuracy and an 87.4% top-20 accuracy .",
    "the * generative * network has a perplexity of 2.08 on the test set , when selecting from 1083 tokens .",
    "i also measure the probability that a beam search creates the correct substitutions for all unconstrained variables as one of the results . on the test set , * generative *",
    "achieves an accuracy of 39.1% with a beam width of 1 , 51.3% with a beam width of 5 , and 57.5% with a beam width of 20 .",
    "the * payoff * network achieves a classification accuracy of 77.6% on the test set . for comparison ,",
    "a baseline prediction of negative achieves a 62.1% accuracy .",
    "the system as a whole was tested on each test theorem by expanding the proof trees for 10000 passes or until 5 minutes had passed .",
    "multiple attempts were made for each proposition with the beam search width set to 1 , 5 , or 20 . under these parameters",
    ", the system finds proofs for 388 of 2720 , or 14.3% of the test propositions .",
    "the system works particularly well on the initial part of the database , finding proofs for 45.1% of the 457 test propositions in the first 5000 theorems .    in the cases that a valid proof is generated",
    ", the system works quickly .",
    "the discovered proofs were created with a median of 17 passes .",
    "in this paper i have proposed a nonconventional approach to automated theorem proving for higher - order logic , and tested performance on the metamath ` set.mm ` module . while the system does not achieve state - of - the - art performance , it is the first effective complete automated theorem prover to not exploit hand - crafted features .",
    "holophrasm takes a unconventional approach to automated theorem proving , attempting to emulate the processes and intuition of human proof exploration .",
    "a number of new techniques and novel adaptions of current technologies have been introduced :    * tree - based bandit algorithms for proof exploration * tree - reduction during exploration passes to permit actions to have multiple subtrees * deep networks for estimating statement provability * the theorem - vector encoding for rapid theorem selection * sequence - to - sequence models for enumeration from an infinite set of actions    while the results of holophrasm are not directly comparable to current results on the mizar dataset , the developments show promise as generalizable techniques .",
    "they highlight the feasibility of deep learning as an approach to automated theorem proving ."
  ],
  "abstract_text": [
    "<S> i propose a system for automated theorem proving in higher order logic using deep learning and eschewing hand - constructed features . </S>",
    "<S> holophrasm exploits the formalism of the metamath language and explores partial proof trees using a neural - network - augmented bandit algorithm and a sequence - to - sequence model for action enumeration . </S>",
    "<S> the system proves 14% of its test theorems from metamath s ` set.mm ` module . </S>"
  ]
}