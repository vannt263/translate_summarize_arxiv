{
  "article_text": [
    "artificial intelligence has two grand challenges : build models that can make multiple computational steps in answering a question and models that can describe long term dependence in sequential data .",
    "most machine learning models lack in the ability to easily read and write to a memory ( large ) component and infer using a small part of this large memory .",
    "for example , tasks to answer questions from a set of facts in a story , to watch a movie and answer questions about it or to conduct dialogues can not be solved by these models . in principle",
    "they can be solved by a language modeler such as a recurrent neural network ( rnn ) ( @xcite ; @xcite ) , but their memory is too small and not compartmentalized enough to remember the required facts accurately . even in simple memorization task like copying a just seen sequence rnns",
    "are known to have problems @xcite .",
    "rnns are turing complete @xcite and therefore have the capacity to simulate arbitrary procedures but in practice they are not able to .    in this survey ,",
    "i discuss and compare some of the proposed solutions to these problems . in neural turing machine ( ntm )",
    "@xcite , these problems are attempted in analogy to turing s enrichment of finite - state machine by an infinite memory tape .",
    "ntm work like a working memory by solving tasks that require the application of approximate rules to rapidly - created variables @xcite and by using an attentional process to read from and write to memory selectively . in memory networks",
    "@xcite , the idea is to combine successful machine learning strategies with memory component that can be read and written to .",
    "end - to - end memory networks ( memn2n ) @xcite extends on memory networks by removing problem in backpropagation and requirement of strong supervision at each layer .",
    "it is a continuous model that only require input - output pairs in comparison to memory network which require supporting facts in memory ( only during training ) as well .",
    "the organization of this survey paper begins with a brief background about basic neural architectures that use some kind of memory for reasoning and inference ( section [ sec : background ] ) .",
    "it follows with explanation of some of the prominent approaches in using large memory ( section [ sec : approaches ] ) .",
    "some of the additional techniques like memory focus and their continuous representation are discussed along with the approaches . after approaches , the experiments done using them , their results and conclusions are discussed ( section [ sec : experiments ] ) .",
    "finally , conclusion on comparison is drawn from experiments about these approaches ( section [ sec : conclusion ] ) .",
    "recurrent neural networks are neural networks with loop at a hidden node i.e. output of the hidden node is put back into the hidden node alongwith the input at next timestamp .",
    "thus , the output of hidden node acts like a dynamic state whose evolution depends on both the input and current state ( output of hidden node at previous timestamp ) . by unfolding rnn through time one can perceive that the context ( dynamic state ) from an earlier timestamp could affect the behaviour of the network at later timestamps .",
    "+ rnn give way to vanishing and exploding gradient problem .",
    "as the gradient moves across timestamps in backpropagation it s multiplied with @xmath0 ( weight of loop link ) depending on it s value it vanishes ( @xmath1 ) or explodes ( @xmath2 ) .",
    "thus , rnn can unfold into a limited number of timetamps , as increasing after wo nt have any effect because of this problem .          to solve the problem of vanishing and exploding gradient , another architecture called long short - term memory ( lstm ) @xcite was developed .",
    "it solves the problem by embedding perfect integrators@xcite for memory storage in the network .",
    "this is implemented with a complex structure of gates as shown in fig .",
    "[ fig : lstmcell ] . for understanding purpose ,",
    "take a simple perfect integrator @xmath3 , where i(t ) is input and x(t ) is memory storage . as weight on x(t ) here is identity , gradient does not vanish or explode . if we now attach a mechanism to choose when integrator takes the input i.e. a programmable gate depending on context : @xmath4 , we can now selectively store information for indefinite length of time .",
    "gates in similar sense are used in lstm to make this possible .",
    "architecture of neural turing machine ( ntm)@xcite contains mainly : a neural network controller and a memory bank .",
    "the controller interacts with outside environment using input vector and output vector . unlike other neural networks",
    ", ntm can also interact with a memory matrix using selective read and write operations ( called heads ) .",
    "every component of this architecture is differentiable , so gradient descent can be applied to train the network .    in ntm",
    "an attention mechanism uses degree of blurriness  which defines the degree to which the head reads or writes at each memory location . in other words ,",
    "the head can read or write completely at one location or distributed on many locations .",
    "the components of ntm are defined as follows .",
    "@xmath5 is the contents of @xmath6 memory matrix at time t , where n is the number of memory locations and m the size of memory vector .",
    "the model defines @xmath7 , a normalized vector over n locations .",
    "normalization of weight vector implies :    @xmath8    the length m read vector @xmath9 is defined by following equation : @xmath10      write operation has two parts : an erase followed by an add operation . for erase operation , the model defines an erase vector @xmath11 whose m elements lie in [ 0,1 ] . the old memory @xmath12 is erased using the following equation : @xmath13,\\ ] ] where * 1 * is a row - vector of all 1-s , and the multiplication against the memory locations acts point - wise .",
    "memory is erased only when both weighting and erase element at that location are 0 .",
    "+ for add operation a length m add vector @xmath14 is defined .",
    "it is performed after erase as follows : @xmath15 since both multiplication in erase and addition in add operations are commutative , the order in which multiple heads write is irrelevant .",
    "the final memory content @xmath5 is obtained when all heads have done their write operation .",
    "weightings @xmath7 defined in _ reading _ and _ writing _ operations",
    "are produced using the addressing mechanism .",
    "two types of addressing mechanism that complement each other are used :    * content - based addressing : focusses attention on memory locations related to values emitted by controller @xcite .",
    "* location - based addressing : for mathematical functions like @xmath16 , location of the variable is more important than content of the variable . to convey this",
    "information location - based addressing is used .    * focusing by content * the model uses a length m key vector @xmath17 produced from head ( read or write ) , a positive key strength @xmath18 , which can amplify or attenuate the precision of the focus , and a similarity measure @xmath19 $ ] ( e.g. cosine similarity ) between @xmath17 and memory vector @xmath20 .",
    "these are combined according to following equation to give normalized ( using softmax ) content based weighting : @xmath21 \\right)}{\\sigma_j \\exp \\left ( \\beta_t k[k_t , m_t(j ) ] \\right)}\\ ] ]    * focusing by location * the location - based addressing mechanism facilitates both simple iteration across the memory locations and random access jumps .",
    "first , a scalar interpolation gate @xmath22 ( @xmath23 $ ] ) is used to have weighted focus on the content weighting @xmath24 and/or the weighting from previous timestep @xmath25 : @xmath26    second , shift weighting @xmath27 is defined as a normalized distribution over the allowed integer shifts ( 0 to n-1 memory locations ) .",
    "then , rotation is applied to @xmath28 by @xmath27 by following convolution : @xmath29    the combined addressing system can operate in three complementary modes :    * weighting chosen only by content system * wighting chosen by content system and then shifted by location system .",
    "this allows head to find a contiguous block of data and then , access a particular element within that block .",
    "* weighting from previous timestep is rotated by location system .",
    "this allows weighting to iterate through a sequence of addresses by advancing same distance at each timestep .",
    "two choices for the neural network to be used as controller network are discussed :    * recurrent controller such as lstm : internal memory in this network can be considered as ram and hidden activations as registers if controller is taken as a cpu .",
    "this allows controller to mix information ( by unfolding rnn ) across multiple time steps of operations .",
    "* feedforward controller : it can mimic recurrent network by reading and writing from the same location in memory at each step .",
    "additionally , these read and write operations on memory matrix are easier to interpret than internal state read and write operations in rnn    however , the number of concurrent read and write heads in feedforward controller imposes limitations on type of computation by ntm : with one single read head only unary operations can be performed on memory at each timestep , with two - binary operations , and follows . in rnn , it s taken care of by storing read vectors internally , from previous time steps .",
    "a memory network@xcite consists of a memory m and four components : + * i : * input feature map - converts input to internal features + * g : * generalization - updates old memories ( state ) according to the new input + * o : * output feature map - produces output in feature representation space based on the new input and the current memory state + * r : * response - converts output into desired format +   + flow of the model :    1 .",
    "convert input x to internal input representation i(x ) 2 .",
    "update memories m using g 3 .",
    "compute output features o using o 4 .",
    "decode output features o to give the final response      when neural networks are used as components of a memory network ( defined above ) , it is called memory neural network ( memnn ) . +",
    "* basic model * four components of memnn are defined as follows : + * i : * set of sentences x(question or statement of a fact ) transformed as embedding vectors i(x ) + * g : * new memories are just stored ( no updates ) @xmath30 .",
    "let their number be n memories .",
    "+ * o : * output features are produced by finding k ( taken as 2 ) supporting memories given x. first memory @xmath31 ( k = 1 ) is retrieved using the following equation : @xmath32 where @xmath33 is a scoring function on match between i(x ) and @xmath34 . for k = 2 , second memory @xmath35",
    "is found given the first found in previous iteration : @xmath36,m_i)\\ ] ] where @xmath34 is scored with respect to both the original input and @xmath31 , square brackets denote a list . + * r : * it produces a textual response r. limiting textual response to a single word ( out of all words ) , response is produced by ranking them : @xmath37,w)\\ ] ] where w is the set of all words in the dictionary , and @xmath38 is a function that scores the match .",
    "* training : * it is done in fully supervised setting where desired inputs and responses , and the supporting sentences are labeled as such in the training data ( but not in the test data , where only inputs are given ) .",
    "thus , both @xmath31 and @xmath35 are known at training time .",
    "training is performed with margin ranking loss and stochastic gradient descent ( sgd ) .",
    "this model@xcite takes discrete input representations @xmath39 to store them in memory , a query q , and outputs an answer a. each of the @xmath40 , q and * a * contains symbols from a dictionary with vocabulary v. the model converts x ( upto a fixed buffer size ) and q to a continuous representation .",
    "this representation is processed via multiple hops to output * a*. as all these representations are continuous we can use backpropagation for training .      in single layer case , the model implements a single memory hop operation .",
    "structure and flow of single layer model is as follows : + * input memory representation : * using embedding matrices a , b ( of size d @xmath41",
    "v ) we convert input x and query q respectively to same continuous space of dimension d. transformed input is memory vectors \\{@xmath34 } and transformed query is u. in the embedding space we compute the similarity between u and @xmath34 by taking the inner product followed by a softmax : @xmath42 @xmath43 as defined above is probability vector over the inputs . + * output memory representation : * using emedding matrix c , each input @xmath40 is transformed to output vector @xmath44 .",
    "the output response vector o is computed by the following equation : @xmath45 * generating the final prediction : * the predicted label is computed using the final weight matrix w ( of size v @xmath41 d ) by following equation : @xmath46 all three embedding matrices a , b and c , and w are jointly learned during training ( stochastic gradient descent ) by minimizing a standard cross entropy loss between @xmath47 and the true label a.",
    "experiments were done on a set of simple algorithms tasks like copying and sorting data sequences .",
    "the goal of the experiments was to observe problem solving and learning of compact internal programs by the ntm architecture .",
    "such solution programs could generalize well beyond training data .",
    "for example network trained to copy sequences of length 20 was tested on sequences of length 100 .",
    "+ three architectures are compared in experiments :    * ntm with a feedforward controller * ntm with a lstm controller * standard lstm network    tasks were episodic and thus , the dynamic state ( previous hidden state ) was reset ( to learned bias vector ) at the start of each input sequence .",
    "all the tasks were supervised learning problems ; all network had logistic sigmoid output layers and were trained with cross - entropy objective function .",
    "sequence prediction errors are reported in bits - per - sequence .",
    "the task was to copy a sequence of random binary vectors followed by a delimiter ( to fix length ) .",
    "thus , input was a sequence with delimiter and output was the same sequence without delimiter .",
    "it was done to compare effect of longer time delays on ntm with lstm . + as can be seen from fig .",
    "[ fig : ntmcopy ] ntm learned much faster than lstm alone , and converged to a lower cost .",
    "ntm continues to copy as the length increases while lstm rapidly degrades beyond length 20 .",
    "these disparities suggest a qualitative rather than a quantitative difference in problem solving by the two architectures .              sorting capacity of ntm",
    "was tested in this task .",
    "input was a collection of random binary vectors with priority from the range [ -1,1 ] .",
    "hypothesis for ntm was that it uses the priorities to determine the relative location of each write . to test the hypothesis a linear function of the priority",
    "was fitted on the write locations .",
    "[ fig : ntmsort ] shows the results , locations returned by the linear function closely match write locations of ntm and reads from the memory locations are in increasing order , thereby sequences were traversed in sorted manner .",
    "the learning curves in fig . [ fig : ntmsortcrop ] show that ntm outperforms lstm .",
    ".results on the large - scale qa task of @xcite(adapted from @xcite ) .",
    "[ tab : fader - qa ] [ cols=\"<,^,^\",options=\"header \" , ]     * results : * the results across all 20 tasks are given in table [ tab : memn2n1 ] for the 1k training set , along with mean performance for 10k training set .",
    "following observations are made :    * the best memn2n models are reasonably close ( mean error ) to the supervised models . *",
    "all variants of memn2n comfortably beat the weakly supervised baseline methods . *",
    "joint training on all tasks help * more computational hops give improved performance .",
    "ntms enrich the capabilities of recurrent networks most profoundly by using attention mechanism , memory write and a large addressable memory .",
    "however , the results of ntms are only shown on simple tasks of copying and sorting as discussed in section [ sec : ntm_exp ] . results of memnn and memn2n are compared in table [ tab : memn2n1 ] . these suggest , for strong supervision ( when supporting facts are known during training ) memnn work the best with least error percentage .",
    "but , in case of weak supervision memn2n are better .",
    "it has been consistently observed in all the experiments ( tables [ tab : simqa ] , [ tab : memn2n1 ] , fig .",
    "[ fig : ntmcopy ] , [ fig : ntmsortcrop ] ) that these new architectures are better in performance than rnn , lstm for tasks that require large memory lookup for inference .",
    "memn2n have further been applied in many situations like dialogs in a restaurant setting , qa based on a story , goal oriented dialogs etc .",
    "these research suggest the prominence of neural networks in reasoning tasks ."
  ],
  "abstract_text": [
    "<S> reason and inference require process as well as memory skills by humans . </S>",
    "<S> neural networks are able to process tasks like image recognition ( better than humans ) but in memory aspects are still limited ( by attention mechanism , size ) . recurrent neural network ( rnn ) and it s modified version lstm are able to solve small memory contexts , but as context becomes larger than a threshold , it is difficult to use them . </S>",
    "<S> the solution is to use large external memory . </S>",
    "<S> still , it poses many challenges like , how to train neural networks for discrete memory representation , how to describe long term dependencies in sequential data etc . </S>",
    "<S> most prominent neural architectures for such tasks are memory networks : inference components combined with long term memory and neural turing machines : neural networks using external memory resources . </S>",
    "<S> also , additional techniques like attention mechanism , end to end gradient descent on discrete memory representation are needed to support these solutions . </S>",
    "<S> preliminary results of above neural architectures on simple algorithms ( sorting , copying ) and question answering ( based on story , dialogs ) application are comparable with the state of the art . in this paper , </S>",
    "<S> i explain these architectures ( in general ) , the additional techniques used and the results of their application .    networks , turing machine , rnn , lstm , gradient descent , sorting , discrete memory , external memory , long term memory    -5 mm </S>"
  ]
}