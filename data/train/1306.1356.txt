{
  "article_text": [
    "compressive sensing @xcite is a recent field that has seen enormous research activity in the past years .",
    "it predicts that certain signals ( vectors ) can be recovered from what was previously believed to be incomplete information using efficient reconstruction methods .",
    "applications of this principle range from magnetic resonance imaging over radar and remote sensing to astronomical signal processing and more .",
    "the key assumption of the theory is that the signal to be recovered is sparse or can at least be well - approximated by a sparse one .",
    "most research activity so far has been dedicated to the synthesis sparsity model where one assumes that the signal can be written as a linear combination of only a small number of elements from a basis , or more generally an overcomplete frame . in certain situations",
    ", however , it turns out to be more efficient to work with an analysis - based sparsity model . here",
    ", one rather assumes that the application of a linear map yields a vector with a large number of zero entries . while the synthesis and the analysis model are equivalent in special cases , they are very distinct in an overcomplete case . by now , comparably few investigations have been dedicated to the analysis sparsity model and its rigorous understanding is still in its infancy .",
    "the analysis based sparsity model and corresponding reconstruction methods were introduced systematically in recent work of nam et al .",
    "nevertheless we note that it appeared also in earlier works , see e.g.  @xcite .",
    "in particular , the popular method of total variation minimization @xcite in image processing is closely related to analysis based sparsity with respect to a difference operator .",
    "an estimate of the number of gaussian measurements for successful recovery via total variation minimization has been recently obtained in @xcite .    in this paper",
    "we consider the analysis based sparsity model for the important case that the analysis transform is given by inner products with respect to a possibly redundant frame . as reconstruction method",
    "we study a corresponding analysis @xmath0-minimization approach .",
    "furthermore , we assume that the linear measurements are obtained via an application of a gaussian random matrix .",
    "the main results of this paper provide precise estimates of the number of measurements required for the reconstruction of a signal whose analysis representation has a given number of zero elements .",
    "moreover , stability estimates are given .",
    "an alternative bound on the number of measurements can be found in @xcite .",
    "we consider the task of reconstructing a signal @xmath1 from incomplete and possibly corrupted measurements given by @xmath2 where @xmath3 with @xmath4 is the measurement matrix and @xmath5 corresponds to noise .",
    "since this system is underdetermined it is impossible to recover @xmath6 from @xmath7 without additional information , even when @xmath8 .",
    "as already mentioned , the underlying assumption in compressive sensing is sparsity .",
    "the _ synthesis sparsity prior _ assumes that @xmath6 can be represented as a linear combination of a small number of elements of a dictionary @xmath9 , i.e. , @xmath10 where the number of non - zero elements of @xmath11 , denoted by @xmath12 , is considerably less than @xmath13 .",
    "often @xmath14 is chosen as a unitary matrix , which refers to sparsity of @xmath6 in an orthonormal basis .",
    "unfortunately , the approach to recover @xmath15 , or @xmath6 respectively , from @xmath16 ( assuming the noiseless case for simplicity ) via @xmath17-minimization , i.e. , @xmath18 is np - hard in general .",
    "a by - now well - studied tractable alternative is the @xmath0-minimization approach of finding the minimizer @xmath19 of @xmath20 the restored signal is then given by @xmath21 .",
    "this optimization problem is referred to as basis pursuit @xcite . in the noisy case",
    ", one passes to @xmath22 where @xmath23 corresponds to an estimate of the noise level .",
    "the _ analysis sparsity prior _ assumes that @xmath6 is sparse in some transform domain , that is , given a matrix @xmath24  the so - called _ analysis operator _ ",
    "the vector @xmath25 is sparse .",
    "for instance , such operators can be generated by the discrete fourier transform , the finite difference operator ( related to total variation ) , wavelet @xcite , curvelet @xcite or gabor transforms @xcite .",
    "analogously to , a possible strategy for the reconstruction of analysis - sparse vectors ( or cosparse vectors , see below ) is to solve the analysis @xmath0-minimization problem @xmath26 or , in the noisy case , @xmath27 both optimization problems can be solved efficiently using convex optimization techniques , see e.g.  @xcite .",
    "if @xmath28 is an invertible matrix , then these analysis @xmath0-minimization problems are equivalent to and . however , in general the analysis @xmath0-minimization problems can not be reduced to the standard @xmath0-minimization problems .",
    "we note , that one may also pursue greedy or other iterative approaches for recovery , see e.g.  @xcite for an overview in the standard synthesis sparsity case and see e.g.  @xcite for the analysis sparsity case .",
    "however , we will concentrate on the above optimization approaches here .    in the remainder of this paper , we assume that the analysis operator is given by a frame . put formally , let @xmath29 , @xmath30 , @xmath31 , be a frame , i.e. , there exist positive constants @xmath32 , @xmath33 such that for all @xmath1 @xmath34 its elements are collected as rows of the matrix @xmath24 .",
    "the analysis representation of a signal @xmath6 is given by the vector @xmath35 .",
    "( we note that in the literature it is often common to collect the elements of a frame rather as columns of a matrix .",
    "however , for our purposes it is more convenient to collect them as rows . )",
    "the frame is called tight if the frame bounds coincide , i.e. , @xmath36 .",
    "cosparsity is now defined as follows .",
    "[ defcosparsity ] given an analysis operator @xmath24 , the cosparsity of @xmath1 is defined as @xmath37 the index set of the zero entries of @xmath38 is called the cosupport of @xmath6 . if @xmath6 is @xmath39-cosparse , then @xmath38 is @xmath40-sparse with @xmath41 .",
    "the motivation to work with the cosupport rather than the support in the context of analysis sparsity is that in contrast to synthesis sparsity , it is the location of the _ zero_-elements which define a corresponding subspace .",
    "in fact , if @xmath42 is the cosupport of @xmath6 , then it follows from definition  [ defcosparsity ] that @xmath43 hence , the set of @xmath39-cosparse signals can be written as @xmath44 : \\ # \\lambda = l } w_\\lambda,\\ ] ] where @xmath45 denotes the orthogonal complement of the linear span of @xmath46 .",
    "in contrast to standard sparsity , there are often certain restrictions on the values that the cosparsity can take .",
    "in fact , in the generic case that the frame elements @xmath47 are in general position in @xmath48 , then every set of @xmath49 rows of @xmath50 are linearly independent .",
    "then the maximal number of zeros that can be achieved for a nontrivial vector @xmath6 in the analysis representation @xmath38 is less than @xmath49 , since otherwise @xmath51 .",
    "thus , for the cosparsity @xmath39 of any non - zero vector @xmath6 it holds @xmath52 in this case . nevertheless , if there are linear dependencies among the frame elements @xmath47 , then larger values of @xmath39 are possible .",
    "this applies to certain redundant frames as well as to the difference operator ( related to total variation ) .",
    "our main results concern the minimal number @xmath53 of measurements that are necessary to recover an @xmath39-cosparse vector @xmath6 from @xmath54 with @xmath55 .",
    "as it is hard to come up with theoretical guarantees for deterministic matrices @xmath56 , we pass to random matrices . as common in compressive sensing , we work with gaussian random matrices , that is , with matrices having independent standard normal distributed entries .",
    "gaussian random matrices have already proven to provide accurate theoretical guarantees in the context of standard synthesis sparsity , see e.g.  @xcite . moreover , empirical tests indicate that also other types of random matrices behave very similar to gaussian random matrices in terms of recovery performance @xcite , although a theoretical justification may be much harder than for gaussian matrices .",
    "we both provide so - called nonuniform and uniform recovery guarantees . the nonuniform result states that a given fixed cosparse vector @xmath6 is recovered via analysis @xmath0-minimization from @xmath54 with high probability using a random choice of a gaussian measurement matrix @xmath56 under a suitable condition on the number of measurements .",
    "in contrast , the uniform result states that a single random draw of a gaussian matrix @xmath56 is able to recover _ all _ cosparse signals @xmath6 simultaneously with high probability . clearly , the uniform statement is stronger than the nonuniform one , however , as we will see , the uniform statement requires more measurements .",
    "we start with the nonuniform guarantee for recovery of cosparse signals with respect to frames using gaussian measurement matrices .",
    "[ thmainresultforframe ] let @xmath57 be a frame with frame bounds @xmath58 and let @xmath6 be @xmath39-cosparse , that is , @xmath38 is @xmath40-sparse with @xmath59 . for a gaussian random matrix @xmath3 and @xmath60 , if @xmath61 then with probability at least @xmath62 , the vector @xmath6 is the unique minimizer of @xmath63 subject to @xmath64 .",
    "roughly speaking , that is , ignoring terms of lower order , a fixed @xmath39-cosparse vector is recovered with high probability from @xmath65 gaussian measurements where @xmath59 .",
    "note that the number of measurements increases with increasing frame ratio @xmath66 , and the optimal behavior occurs for tight frames .",
    "for @xmath67 , this bound slightly strengthens the main result for sparse recovery in @xcite .",
    "we will also show stability of the reconstruction with respect to noise on the measurements , see theorem  [ thnoisymeasurements ] below .",
    "the proof of the above result ( given in section  [ sec : nonuniform ] ) relies on a characterization of the minimizer via tangent cones ( theorems  [ threcoveryviatangentcones ] and [ threcoveryviatangentconeswithnoise ] ) which is similar to corresponding conditions stated in @xcite .",
    "moreover , our proof uses an extension of the gordon s escape through a mesh theorem ( theorem  [ thmodifiedgordonsescapethroughthemesh ] ) .",
    "we now pass to the uniform recovery result which additionally takes into account that in practice the signals are often only approximately cosparse .",
    "the quantity @xmath68 describes the @xmath0-best approximation error to @xmath38 by @xmath40-sparse vectors .",
    "[ thuniformrecoverywithframe ] let @xmath3 be a gaussian random matrix , @xmath69 and @xmath60 .",
    "if @xmath70 then with probability at least @xmath62 for every vector @xmath1 a minimizer @xmath71 of @xmath63 subject to @xmath64 approximates @xmath6 with @xmath72-error @xmath73    roughly speaking , with high probability every @xmath39-cosparse vector can be recovered via analysis @xmath0-minimization using a single random draw of a gaussian matrix if @xmath74 moreover , the recovery is stable under passing to approximately cosparse vectors when adding slightly more measurements .",
    "the proof of this theorem relies on an extension of the null space property , which is well known in the synthesis sparsity case @xcite and was adapted to the analysis sparsity setting in @xcite .",
    "in fact , for the standard case @xmath75 , we improve a result of rudelson and vershynin @xcite ( also relying on the null space property ) with respect to the constants in and add stability in @xmath72 . we further show that recovery is robust under perturbations of the measurements in theorem [ throbustuniformrecoverywithframe ] .",
    "we note , that in the standard exact sparse case with no noise , the constant @xmath76 in ( [ eqapprnumberofmeasurementsuniformrecovery ] ) can be replaced by @xmath77 , see the contribution by donoho and tanner in @xcite .",
    "their methods , however , are completely different to ours , and it is not clear , whether they can be extended to analysis sparsity .      let us discuss briefly related theoretical studies on recovery of analysis sparse vectors and compare them with our main results",
    ". an earlier version of theorem  [ thuniformrecoverywithframe ] was shown by cands and needell in @xcite . however , they were only able to treat the case that the analysis operator is given by a tight frame , that is , when @xmath78 .",
    "moreover , their analysis is based on a version of the restricted isometry property and does not provide explicit constants in the corresponding bound on the required number of measurements . to be fair ,",
    "we note , however , that their analysis applies to general subgaussian random matrices .",
    "the results of @xcite were extended to the case of non - tight frames and weibull matrices in the work of foucart in @xcite .",
    "the analysis in @xcite incorporates the robust null space property , the verification of which for the weibull matrices relies on a variant of the classical restricted isometry property . in our work",
    "we prove that gaussian random matrices satisfy the robust null space property by referring to a modification of the gordon s escape through a mesh theorem .    a recent contribution by needell and ward @xcite provides theoretical recovery guarantees for the special case of total variation minimization , which corresponds to analysis @xmath0-minimization with a certain difference operator .",
    "unfortunately , we can not cover this situation with our main results because the difference operator is not a frame .",
    "nevertheless , it would be interesting to pursue theoretical recovery guarantees for total variation minimization and gaussian random matrices using the approach of this paper .",
    "nam et al.s work @xcite provides a systematic introduction of the analysis sparsity model and treats also greedy recovery methods , see also @xcite .",
    "further contributions are contained in @xcite .",
    "our nonuniform recovery guarantees rely on a geometric characterization of the successful recovery .",
    "we obtain quantitative estimates by bounding a certain gaussian width which can be thought as an intrinsic complexity measure .",
    "the authors of @xcite exploit the geometry of optimality conditions to study phase transition phenomena in random linear inverse problems and random demixing problems .",
    "they express their results in terms of the statistical dimension which is essentially equivalent to the gaussian width , see section 10 . 3 of @xcite for further details .    also , we note that the optimization problems ( [ eqproblemp1 ] ) and ( [ eqproblemp1noise ] ) often appear in image processing @xcite .",
    "we use the notation @xmath79 to refer to a submatrix of @xmath50 with the rows indexed by @xmath42 ; @xmath80 stands for the vector whose entries indexed by @xmath81 coincide with the entries of @xmath38 and the rest are filled by zeros .",
    "as we have already mentioned , the @xmath17-norm @xmath82 of a vector corresponds to the number of non - zero elements in it .",
    "the unit ball in @xmath83 with respect to the @xmath84-norm is denoted by @xmath85 .",
    "the operator norm of a matrix @xmath86 is defined by @xmath87 and the frobenius norm is given by @xmath88 it is well - known that frobenius norm dominates the operator norm , @xmath89 . finally , @xmath90 $ ] is the set of all natural numbers not exceeding @xmath91 , i.e. , @xmath90=\\{1,2,\\ldots , p\\}$ ] .",
    "in this section we prove theorem  [ thmainresultforframe ] and extend it to robust recovery in theorem  [ thnoisymeasurements ] . the proof strategy is similar as in @xcite .",
    "we rely on conditions on the measurement matrix @xmath56 involving tangent cones , which should be of independent interest . in order to check these conditions for a gaussian random matrix we rely on an extension of the gordon s escape through a mesh theorem .",
    "( in contrast to @xcite , the standard version of the gordon s result is not sufficient for our purposes . )      our conditions for successful recovery of cosparse signals are formulated via tangent cones . for fixed @xmath1",
    "we define the convex cone @xmath92 where the notation `` cone '' stands for the conic hull of the indicated set .",
    "the following result is analogous to proposition  2.1 in @xcite .",
    "[ threcoveryviatangentcones ] let @xmath3 .",
    "a vector @xmath1 is the unique minimizer of @xmath63 subject to @xmath93 if and only if @xmath94 .",
    "first assume that @xmath94 .",
    "let @xmath95 be a vector that satisfies @xmath93 and @xmath96 .",
    "this means that @xmath97 and @xmath98 . according to our assumption",
    "we conclude that @xmath99 , so that @xmath6 is the unique minimizer .",
    "the other direction is proved by contradiction .",
    "let @xmath100 be the unique minimizer of ( [ eqproblemp1 ] ) .",
    "take any @xmath101 .",
    "then @xmath102 can be written as @xmath103 since @xmath104 , it holds @xmath105 and we can define @xmath106 .",
    "suppose @xmath107 .",
    "then @xmath108 so that @xmath109 .",
    "together with the estimate @xmath110 and uniqueness of the minimizer , this implies @xmath111 .",
    "hence @xmath112 , which leads to a contradiction .",
    "thus , @xmath113 .",
    "when the measurements are noisy , we use the following condition for successful recovery @xcite .",
    "[ threcoveryviatangentconeswithnoise ] let @xmath1 , @xmath3 and @xmath114 with @xmath115 .",
    "if @xmath116 for some @xmath117 , then a minimizer @xmath118 of ( [ eqproblemp1noise ] ) satisfies @xmath119    since @xmath71 is a minimizer of ( [ eqproblemp1noise ] ) , we have @xmath120 and @xmath121 .",
    "our assumption ( [ eqinfovercone ] ) implies @xmath122 on the other hand , we can give an upper bound for @xmath123 by @xmath124 combining ( [ eqmeasurementofdifferenceincone ] ) and ( [ eqmeasurementofdifferenceinitialcondition ] ) we get the desired estimate .      to prove the non - uniform recovery result for gaussian random measurements ( theorem [ thmainresultforframe ] ) we rely on the condition stated in theorem [ threcoveryviatangentcones ] , which requires that the null space of the measurement matrix @xmath56 misses the set @xmath125 .",
    "the next ingredient of the proof is a variation of the gordon s escape through a mesh theorem , which was first used in the context of compressed sensing in @xcite . to state",
    "this theorem , we introduce some notation and formulate auxiliary lemmas .",
    "let @xmath126 be a standard gaussian random vector , that is , a vector of independent mean zero , variance one normal distributed random variables .",
    "then for @xmath127 we have @xmath128 see @xcite . for a set @xmath129 we define its gaussian width by @xmath130 where @xmath131 is a standard gaussian random vector .",
    "[ lmgordon ] let @xmath132 and @xmath133 , @xmath134 , @xmath135 be two mean - zero gaussian random variables . if @xmath136 then @xmath137    [ gordon : inf ] gordon s lemma extends to the case of gaussian processes indexed by possibly infinite sets where the expected maxima or minima are replaced by corresponding lattice suprema or infima , see for instance ( * ? ?",
    "* remark 8.28 ) or @xcite .",
    "we further exploit the concentration of measure phenomenon , which asserts that lipschitz functions concentrate well around their expectation @xcite .",
    "[ lmconcentrationofmeasure ] let @xmath138 be an @xmath139-lipschitz function : @xmath140 let @xmath141 be a vector of independent standard normal random variables . then , for all @xmath142 , @xmath143-f(\\mathbf{g})\\geq t\\right)}\\leq e^{-\\frac{t^2}{2l^2}}.\\ ] ]    next , we state our modification of the gordon s escape through a mesh theorem , see @xcite for the original version .",
    "below , @xmath144 corresponds to the set of elements produced by applying @xmath50 to the elements of @xmath145 .",
    "[ thmodifiedgordonsescapethroughthemesh ] let @xmath24 be a frame with constants @xmath32 , @xmath33 .",
    "let @xmath3 be a gaussian random matrix and @xmath145 be a subset of the unit sphere @xmath146 .",
    "then , for @xmath142 , it holds @xmath147    recall that @xmath148 for @xmath149 and @xmath150 we compare the two gaussian processes @xmath151 where @xmath152 and @xmath153 are independent standard gaussian random vectors .",
    "let @xmath154 and @xmath155 .",
    "since @xmath156 are independent with @xmath157 , @xmath158 , we have @xmath159 independence and the isotropicity of the gaussian vectors @xmath160 and @xmath161 together with the fact that @xmath50 is a frame with lower frame bound @xmath32 imply @xmath162 when @xmath163 , we have @xmath164 combining ( [ eqestimateforxprocess ] ) and ( [ eqestimateforyprocess ] ) , we obtain @xmath165 and since @xmath166 and similarly for @xmath167 , it follows that @xmath168 moreover , we have @xmath169 due to gordon s lemma ( lemma [ lmgordon ] ) and remark [ gordon : inf ] , @xmath170 let @xmath171 .",
    "for any @xmath172 @xmath173 \\ [ \\leq\\underset{\\mathbf{x}\\in t}\\inf{\\vert \\mathbf{b}\\mathbf{x } \\vert}_2+{\\vert \\mathbf{a}-\\mathbf{b } \\vert}_{2\\to2 } \\leq \\underset{\\mathbf{x}\\in t}\\inf{\\vert \\mathbf{b}\\mathbf{x } \\vert}_2+{\\vert \\mathbf{a}-\\mathbf{b } \\vert}_{f}.\\ ] ] the second inequality follows from the fact that @xmath174 . by interchanging @xmath86 and @xmath175",
    "we conclude that @xmath176 this means that @xmath177 is @xmath178-lipschitz with respect to the frobenius norm ( which corresponds to the @xmath72-norm when interpreting a matrix as a vector ) and due to concentration of measure ( lemma  [ lmconcentrationofmeasure ] ) @xmath179 applying the estimate ( [ eqexpectationoflipschitzfunctionestimate ] ) to the previous inequality gives @xmath180 which concludes the proof .",
    "the previous result suggests to estimate the gaussian width of @xmath144 with @xmath181 . since @xmath50 is a frame with upper frame constant @xmath182 , we have @xmath183 where @xmath184 the supremum over a larger set can only increase , hence @xmath185 we next recall an upper bound for the gaussian width @xmath186 from @xcite involving the polar cone @xmath187 defined by @xmath188    [ prgaussianwidthsbypolarcone ] let @xmath152 be a standard gaussian random vector .",
    "then @xmath189    the proof relies on tools from convex analysis , see @xcite , ( * ? ? ?",
    "let @xmath40 be the sparsity of the vector @xmath190 . then @xmath191    by proposition [ prgaussianwidthsbypolarcone ] and hlder s inequality @xmath192 let @xmath81 denote the support of @xmath38 . then one can verify that @xmath193 see ( * ? ? ? * lemma 9.23 ) for a proof .",
    "to proceed , we fix @xmath194 , minimize @xmath195 over all possible entries @xmath196 , take the expectation of the obtained expression and finally optimize over @xmath194 . according to ( [ eqpolarconeasunionovert ] ) , we have @xmath197 where @xmath198 is the soft - thresholding operator given by @xmath199 taking expectation we arrive at @xmath200}+\\operatorname{\\mathbb{e}}{\\left[\\sum_{i\\in s^c}s_t(g_i)^2\\right]}\\notag\\\\ & = s(1+t^2)+(p - s)\\operatorname{\\mathbb{e}}s_t(g)^2\\label{eqexpectationestimatewitht } , \\end{aligned}\\ ] ] where @xmath201 is a univariate standard gaussian random variable .",
    "to calculate the expectation of @xmath202 , we apply the direct integration @xmath203}\\notag\\\\ & = \\frac{2}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty } x^2e^{-\\frac{(x+t)^2}{2}}\\,dx = \\frac{2e^{-\\frac{t^2}{2}}}{\\sqrt{2\\pi}}\\int\\limits_{0}^{\\infty } x^2e^{-\\frac{x^2}{2}}e^{-xt}\\,dx\\notag\\\\ & \\leq e^{-\\frac{t^2}{2}}\\sqrt{\\frac{2}{\\pi}}\\int\\limits_{0}^{\\infty } x^2e^{-\\frac{x^2}{2}}\\,dx = e^{-\\frac{t^2}{2}}.\\label{eqexpectationofsoftthreshold}\\end{aligned}\\ ] ] substituting the estimate ( [ eqexpectationofsoftthreshold ] ) into ( [ eqexpectationestimatewitht ] ) gives @xmath204 setting @xmath205 finally leads to @xmath206 this concludes the proof .    by combining inequalities ( [ eqgwidthbykoneandsphere ] ) and ( [ eqgaussianwidthofcone ] )",
    "we obtain @xmath207    set @xmath208 . the fact that @xmath209 along with condition ( [ eqnumberofmeasurementsforframe ] ) yields @xmath210 theorem  [ thmodifiedgordonsescapethroughthemesh ] implies @xmath211 which guarantees that @xmath212 with probability at least @xmath62 . as the final step we apply theorem [ threcoveryviatangentcones ] .",
    "we now extend theorem  [ thmainresultforframe ] to robust recovery .",
    "[ thnoisymeasurements ] let @xmath57 be a frame with frame bounds @xmath213 and let @xmath6 be @xmath39-cosparse and @xmath214 . for a random draw @xmath3 of a gaussian random matrix , let noisy measurements @xmath215 be given with @xmath115 .",
    "if for @xmath60 and some @xmath216 @xmath217 then with probability at least @xmath62 , any minimizer @xmath71 of ( [ eqproblemp1noise ] ) satisfies @xmath119    we use the recovery condition stated in theorem [ threcoveryviatangentconeswithnoise ] . set @xmath208 . our previous considerations and the choice of @xmath53 in ( [ eqnumberofmeasurementsforframenoise ] ) guarantee that @xmath218 the monotonicity of probability and theorem [ thmodifiedgordonsescapethroughthemesh ] yield @xmath219",
    "this section is dedicated to the proof of the uniform recovery result in theorem  [ thuniformrecoverywithframe ] .",
    "it relies on the @xmath50-null space property , which extends the null space property known from the standard synthesis sparsity case , see e.g.  @xcite .",
    "we analyze this property directly for gaussian random matrices with similar techniques as used in the previous section .",
    "let us start with the @xmath50-null space property which is a sufficient condition for the exact reconstruction of every cosparse vector .",
    "[ defnsp ] a matrix @xmath3 is said to satisfy the @xmath50-null space property of order @xmath40 with constant @xmath69 , if for any set @xmath220 $ ] with @xmath221 it holds @xmath222    if @xmath50 is the identity map @xmath223 , then ( [ eqnsp ] ) is the standard null space property .",
    "we start with a result on exact recovery of cosparse vectors .",
    "[ threcoverywithomegansp ] if @xmath3 satisfies the @xmath50-null space property of order @xmath40 with @xmath69 , then every @xmath39-cosparse vector @xmath1 with @xmath224 is the unique solution of ( [ eqproblemp1 ] ) with @xmath225 .",
    "this theorem follows immediately from the next result , which also implies a certain stability estimate in @xmath0 .",
    "[ thconecostraint ] let @xmath1 be an arbitrary vector and @xmath71 be a solution of ( [ eqproblemp1 ] ) with @xmath226 , where @xmath3 satisfies the @xmath50-null space property of order @xmath40 with constant @xmath227 . then @xmath228    since @xmath71 is the solution of ( [ eqproblemp1 ] ) , we must have @xmath229 .",
    "take any @xmath220 $ ] with @xmath221 .",
    "then @xmath230 by the triangle inequality , the vector @xmath231 satisfies @xmath232 which implies @xmath233 hereby , we have applied the @xmath50-null space property . rearranging and choosing a set @xmath42 of size @xmath234 which minimizes @xmath235 yields @xmath236 furthermore , another application of the @xmath50-null space property gives @xmath237 this completes the proof .    in order to provide a suitable stability estimate in @xmath72 we require a slightly stronger version of the @xmath28-null space property .",
    "[ defl2stablensp ] a matrix @xmath3 is said to satisfy the @xmath72-stable @xmath50-null space property of order @xmath40 with constant @xmath69 , if , for any set @xmath220 $ ] with @xmath221 , it holds @xmath238    [ rem : l1l2 ] the hlder s inequality implies @xmath239 for any set @xmath240 $ ] with @xmath241 .",
    "this means that if @xmath242 satisfies the @xmath72-stable @xmath28-null space property of order @xmath40 with constant @xmath69 , then it satisfies the @xmath28-null space property of the same order and with the same constant .",
    "[ threcoverywithl2nsp ] let @xmath3 satisfy the @xmath72-stable @xmath50-null space property of order @xmath40 with constant @xmath69 .",
    "then for any @xmath1 the solution @xmath71 of ( [ eqproblemp1 ] ) with @xmath226 approximates the vector @xmath6 with @xmath72-error @xmath243    inequality ( [ eql2stablerecovery ] ) means that @xmath39-cosparse vectors are exactly recovered by ( [ eqproblemp1 ] ) and vectors @xmath1 , such that @xmath38 is close to an @xmath40-sparse vector in @xmath0 , can be well approximated in @xmath72 by a solution of ( [ eqproblemp1 ] ) .",
    "the proof goes along the same lines as in the standard case in @xcite .",
    "the novelty here is that we exploit the sparsity not of the signal itself , but of its analysis representation .",
    "so first we extend the @xmath0-error estimate above to an @xmath72-error estimate for @xmath38 and use the fact that @xmath28 is a frame to bound the @xmath72-error @xmath244 .",
    "the statement of theorem [ threcoverywithl2nsp ] was generalized to the setting of a perturbed frame and imprecise knowledge of the measurement matrix in ( * ? ? ? * theorem 3.1 ) .",
    "we define the vector @xmath245 and denote by @xmath246 $ ] an index set of @xmath40 largest absolute entries of @xmath247 . since @xmath248 and @xmath3 satisfies the @xmath72-stable @xmath50-null space property , it follows @xmath249 we partition the indices of @xmath250 into subsets @xmath251 , @xmath252 , @xmath253 of size @xmath40 in order of decreasing magnitude of @xmath254 .",
    "then for each @xmath255 , @xmath256 , @xmath257 along with the triangle inequality this gives @xmath258 inequalities ( [ eqestimatebysnsp ] ) and ( [ eqestimatebydescendingindexes ] ) together with remark  [ rem : l1l2 ] and theorem  [ thconecostraint ] yield @xmath259 finally , we use that @xmath50 is a frame with lower frame bound @xmath32 to conclude that @xmath260 this completes the proof .    when the measurements are given with some error , the author in @xcite introduced the following extension of the @xmath50-null space property in order to guarantee robustness of the recovery .",
    "[ defl2robuststablensp ] a matrix @xmath3 is said to satisfy the robust @xmath72-stable @xmath50-null space property of order @xmath40 with constant @xmath69 and @xmath216 , if for any set @xmath220 $ ] with @xmath221 it holds @xmath261    if @xmath107 , the term @xmath262 vanishes , and we see that the robust @xmath72-stable @xmath50-null space property implies the @xmath72-stable @xmath50-null space property . the robust @xmath72-stable null space property guarantees the stability and robustness of the @xmath0-minimization ( [ eqproblemp1noise ] ) .",
    "[ threcoverywithrobustl2nsp ] let @xmath3 satisfy the robust @xmath72-stable @xmath50-null space property of order @xmath40 with constants @xmath69 and @xmath216 .",
    "then for any @xmath1 the solution @xmath71 of ( [ eqproblemp1noise ] ) with @xmath215 , @xmath263 , approximates the vector @xmath6 with @xmath72-error @xmath264    theorem 5 in @xcite with @xmath265 provides the bound for @xmath266 .",
    "taking into account that @xmath28 is a frame with lower frame constant @xmath32 , we obtain estimate ( [ eqrobustl2stablerecovery ] ) .",
    "we now show theorem  [ thuniformrecoverywithframe ] by establishing the @xmath72-stable @xmath50-null space property of order @xmath40 for a gaussian measurement matrix @xmath56 by following a similar strategy as in section  [ sec : nonuniform ] . to this end",
    "we introduce the set @xmath267,\\ ; \\#\\lambda = p - s\\right\\}.\\ ] ] in fact , if @xmath268 then for all @xmath269 and any @xmath220 $ ] with @xmath270 we have @xmath271 which means that @xmath272 satisfies the @xmath72-stable @xmath50-null space property of order @xmath40 . to show ( [ eqminnormpositivity ] ) we apply theorem [ thmodifiedgordonsescapethroughthemesh ] , which requires to study the gaussian width of the set @xmath273 .",
    "since @xmath50 is a frame with upper frame bound @xmath182 , we have @xmath274 with @xmath275,\\ ; \\ # s = s\\right\\}.\\ ] ] then @xmath276    [ lminclusioninuniversalset ] let @xmath277 be the set defined by @xmath278    a.   [ itunitball ] then @xmath277 is the unit ball with respect to the norm @xmath279}^{1/2},\\ ] ] where @xmath280 , @xmath281 and @xmath282 is the non - increasing rearrangement of @xmath6 .",
    "b.   [ itinclusion ] it holds @xmath283    a similar result was stated as lemma 4.5 in @xcite . for the sake of completeness we present the proof .",
    "[ itunitball ] suppose @xmath284 .",
    "it can be represented as @xmath285 with @xmath286 , @xmath287 and @xmath288 , @xmath289",
    ". then @xmath290 . by the triangle inequality",
    "@xmath291 this proves that @xmath277 is a subset of the unit ball with respect to the @xmath292-norm .    on the other hand ,",
    "let @xmath293 .",
    "we partition the index set @xmath90 $ ] into subsets @xmath251 , @xmath252 ,  of size @xmath40 in order of decreasing magnitude of entries @xmath294 .",
    "set @xmath295 .",
    "then @xmath100 can be written as @xmath296 and , for @xmath297 , @xmath298 .",
    "thus @xmath299 .",
    "[ itinclusion ] take an arbitrary @xmath300 . to show ( [ eqinclusioninuniversalset ] ) we estimate @xmath301 .",
    "according to the definition of @xmath292 in lemma [ lminclusioninuniversalset ] [ itunitball ] , @xmath302}^{\\frac{1}{2}}\\notag\\\\ & = { \\left[\\sum_{i=1}^s{\\left(x_i^*\\right)}^2\\right]}^{\\frac{1}{2}}+{\\left[\\sum_{i = s+1}^{2s}{\\left(x_i^*\\right)}^2\\right]}^{\\frac{1}{2}}+\\sum_{l\\geq 3}^l{\\left[\\sum_{i\\in i_l}{\\left(x_i^*\\right)}^2\\right]}^{\\frac{1}{2}}\\label{eq : estimatefordnorminblocks}.\\end{aligned}\\ ] ] to bound the last term in the inequality above , we first note that for each @xmath303 , @xmath304 , @xmath305}^{1/2}\\leq\\frac{1}{\\sqrt s}\\sum_{j\\in i_{l-1}}x^*_j.\\ ] ] summing up over @xmath304 yields @xmath306}^{\\frac{1}{2}}\\leq\\frac{1}{\\sqrt s}\\sum_{l\\geq 2}\\sum_{j\\in i_l}x^*_j.\\ ] ] since @xmath300 , it holds @xmath307 and there is @xmath308 $ ] , @xmath309 , such that @xmath310 . then @xmath311}^{\\frac{1}{2}}\\ ] ] and @xmath306}^{\\frac{1}{2}}\\leq\\rho^{-1}{\\left[\\sum_{i=1}^s(x_i^*)^2\\right]}^{\\frac{1}{2}}.\\ ] ] applying the last estimate to ( [ eq : estimatefordnorminblocks ] ) and taking into account that @xmath312 we derive that @xmath313}^{\\frac{1}{2}}+{\\left[\\sum_{i = s+1}^{2s}{\\left(x_i^*\\right)}^2\\right]}^{\\frac{1}{2}}\\\\ & \\leq1+\\rho^{-1}){\\left[\\sum_{i=1}^s(x_i^*)^2\\right]}^{\\frac{1}{2}}+{\\left[1-\\sum_{i=1}^{s}{\\left(x_i^*\\right)}^2\\right]}^{\\frac{1}{2}}. \\end{aligned}\\ ] ] set @xmath314}^{\\frac{1}{2}}$ ] .",
    "the maximum of the function @xmath315 is attained at the point @xmath316 and is equal to @xmath317 .",
    "thus for any @xmath318 it holds @xmath319 which proves ( [ eqinclusioninuniversalset ] ) .",
    "lemma [ lminclusioninuniversalset ] [ itinclusion ] implies @xmath320    [ lmestimategaussianwidthofd ] the gaussian width of the set @xmath277 defined by ( [ eqdefinitionofd ] ) satisfies @xmath321    the supremum of the linear functional @xmath322 over @xmath277 is achieved at an extreme point , i.e. , at an @xmath323 with @xmath324 .",
    "hence , by hlder s inequality @xmath325 , \\#s = s}\\max{\\vert \\mathbf{g}_s \\vert}_2.\\ ] ] an estimate on the maximum squared @xmath72-norm of a sequence of standard gaussian random vectors ( see e.g.  ( * ? ? ?",
    "* lemma 3.2 ) or ( * ? ? ?",
    "* proposition 8.2 ) ) gives @xmath326 , \\#s = s}\\max{\\vert \\mathbf{g}_s \\vert}_2 ^ 2 } \\leq\\sqrt{2\\ln\\binom{p}{s}}+\\sqrt s\\leq\\sqrt{2s\\ln\\frac{ep}{s}}+\\sqrt s.\\ ] ] the last inequality follows from the fact that @xmath327 , see e.g.  ( * ? ? ?",
    "* lemma c.5 ) .",
    "expressions ( [ eqinclusionduetoframe ] ) , ( [ eqgaussianwidthofconeandd ] ) and lemma [ lmestimategaussianwidthofd ] show that @xmath328}}\\ell(d)\\notag\\\\ & \\leq\\sqrt{b{\\left[1+(1+\\rho^{-1})^2\\right]}}{\\left(\\sqrt{2s\\ln\\frac{ep}{s}}+\\sqrt s\\right)}\\label{eq : estimategwimageofthesetuniformrecovery}.\\end{aligned}\\ ] ] set @xmath208 .",
    "the fact that @xmath209 along with condition ( [ eqnumberofmeasurementsforframeuniformrecovery ] ) yields @xmath329 the monotonicity of probability and theorem [ thmodifiedgordonsescapethroughthemesh ] imply @xmath330 which guarantees that with probability at least @xmath62 @xmath331 for all @xmath332 and any @xmath220 $ ] with @xmath270 , see ( [ eqminnormpositivity ] ) .",
    "this means that @xmath56 satisfies the @xmath72-stable @xmath50-null space property of order @xmath40 .",
    "finally , we apply theorem [ threcoverywithl2nsp ] .",
    "finally , we extend to robustness of the recovery with respect to perturbations of the measurements .",
    "[ throbustuniformrecoverywithframe ] let @xmath3 be a gaussian random matrix , @xmath69 , @xmath60 and @xmath333 .",
    "if @xmath334 then with probability at least @xmath62 for every vector @xmath1 and perturbed measurements @xmath335 with @xmath336 a minimizer @xmath71 of ( [ eqproblemp1noise ] ) approximates @xmath6 with @xmath72-error @xmath337    condition ( [ eqnumberofmeasurementsforframerobustuniformrecovery ] ) together with @xmath209 imply @xmath338 which is equivalent to @xmath339 taking into account ( [ eq : estimategwimageofthesetuniformrecovery ] ) we may conclude @xmath340 then according to theorem [ thmodifiedgordonsescapethroughthemesh ] @xmath341 this means that for any @xmath342 such that @xmath343 and any set @xmath220 $ ] with @xmath221 it holds with probability at least @xmath62 @xmath344 for the remaining vectors @xmath342 , we have @xmath345 , which together with the fact that @xmath28 is a frame with upper frame bound @xmath182 leads to @xmath346 thus , for any @xmath342 , @xmath347 finally , we apply theorem [ threcoverywithrobustl2nsp ] .",
    "in this section we present the results of numerical experiments on synthetic data performed in matlab using the ` cvx ` package .",
    "for the first set of experiments we constructed tight frames @xmath50 as an orthonormal basis of the range of the matrix the rows of which were drawn randomly and independently from @xmath348 . in order to obtain also non - tight frames we simply varied the norms of the rows of @xmath50 . as dimensions for the analysis operator , we have chosen @xmath349 and @xmath350 .",
    "the maximal number of zeros that can be achieved in the analysis representation @xmath38 is less than @xmath49 , since otherwise @xmath51 .",
    "therefore , the sparsity level of @xmath38 was always greater than @xmath351 . for each trial we fixed a cosparsity @xmath39 ( resulting in the sparsity @xmath352 ) and selected at random @xmath39 rows of an analysis operator @xmath50 that constitute the cosupport @xmath42 of the signal . to produce a signal @xmath6 we constructed a basis @xmath175 of @xmath353 , drew a coefficient vector @xmath354 from a normalized standard gaussian distribution and set @xmath355 .",
    "we ran the algorithm and counted the number of times the signal was recovered correctly out of @xmath356 trials . a reconstruction error of less than @xmath357 was considered as a successful recovery .",
    "the curves in figure  [ figdifframesmeasurementsvssparsity ] depict the relation between the number of measurements and the sparsity level such that the recovery was successful at least @xmath358 of the time .",
    "each point on the line corresponds to the maximal sparsity level that could be achieved for the given number of measurements .     of @xmath359 and for the blue one @xmath360 .",
    "]    the experiments clearly show that analysis @xmath0-minimization works very well for recovering cosparse signals from gaussian measurements .",
    "( note that only a comparison of the experiments with the nonuniform recovery guarantees make sense . )",
    "the frame bound ratio @xmath66 indeed influences the performance of the recovery algorithm ( [ eqproblemp1 ] )  although the degradation with increasing value of @xmath66 is less dramatic than indicated by our theorems .",
    "the reason for this may be that the theorems give estimates for the worst case , while the experiments can only reflect the typical behavior .",
    "m.  kabanava and h.  rauhut acknowledge support by the hausdorff center for mathematics , university of bonn , and by the european research council through the grant stg 258926 .",
    "donoho d.  l. , tanner j. : bserved universality of phase transitions in high - dimensional geometry , with implications for modern data analysis and signal processing .",
    "ser . a math .",
    "367(1906 ) , 42734293 ( 2009 )            gordon y. : on milman s inequality and random subspaces which escape through a mesh in @xmath362 .",
    "in geometric aspects of functional analysis ( 1986/87 ) , volume 1317 of lecture notes in math .",
    "springer , berlin ( 1988 )"
  ],
  "abstract_text": [
    "<S> this paper provides novel results for the recovery of signals from undersampled measurements based on analysis @xmath0-minimization , when the analysis operator is given by a frame . </S>",
    "<S> we both provide so - called uniform and nonuniform recovery guarantees for cosparse ( analysis - sparse ) signals using gaussian random measurement matrices . </S>",
    "<S> the nonuniform result relies on a recovery condition via tangent cones and the uniform recovery guarantee is based on an analysis version of the null space property . </S>",
    "<S> examining these conditions for gaussian random matrices leads to precise bounds on the number of measurements required for successful recovery . in the special case of standard sparsity , our result improves a bound due to rudelson and vershynin concerning the exact reconstruction of sparse signals from gaussian measurements with respect to the constant and extends it to stability under passing to approximately sparse signals and to robustness under noise on the measurements .    </S>",
    "<S> compressive sensing , @xmath0-minimization , analysis regularization , frames , gaussian random matrices . </S>"
  ]
}