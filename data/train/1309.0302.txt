{
  "article_text": [
    "data is usually generated by mixing several components of different structures .",
    "these structures are often compressible , and are able to provide semantic interpretations of the data content . in addition , they can reveal the difference and similarity among data samples , and thus produce robust features playing vital roles in supervised or unsupervised learning tasks .",
    "two types of structures have drawn lots of research attentions in recent years : 1 ) in compressed sensing @xcite , a sparse signal can be exactly recovered from its linear measurements at a rate significant below the nyquist rate , in sparse coding @xcite , an over - complete dictionary leads to sparse representations for dense signals of the same type ; 2 ) in matrix completion @xcite , a low - rank matrix can be precisely rebuilt from a small portion of its entries by restricting the rows ( samples ) to lie in a subspace . in dimension reduction @xcite ,",
    "low - rank structure @xcite has been broadly leveraged for exploring the geometry of point cloud .",
    "although sparse and low - rank structures have been studied separately by a great number of researchers for years , the linear combination of them or their extensions is rarely explored until recently @xcite .",
    "intuitively , fitting data with either sparse or low - rank structure is mature technique but is inevitably restricted by the limited data types they can model , while recent study shows that the linear mixture of them is more expressive in modeling complex data from different applications .",
    "a motivating example is robust pca @xcite ( rpca ) , which decomposes the data matrix @xmath8 as @xmath9 .",
    "the low - rank part @xmath0 summarizes a subspace that is shared by all the samples and thus reveals the global smoothness , while the sparse part @xmath1 captures the individual differences or abrupt changes among samples . a direct application of robust pca is separating the sparse moving objects from the low - rank background in video sequence .",
    "another interesting example is morphological component analysis ( mca ) @xcite , which decompose the data into two parts that have sparse representations on two incoherent over - complete dictionaries , i.e. , the first part has a very non - sparse representation on the dictionary of the second part , and vise versa .",
    "this requirement suggests that the two parts are separable on their sparse representations .",
    "note that both rpca and mca can only work on data whose two building parts are incoherent , i.e. , the content of one part can not be moved to the other part without changing either of their structures ( low - rank , sparse , dictionary , etc . ) .",
    "this incoherence condition could be viewed as a general extension of the statistical independence supporting independent component analysis ( ica ) @xcite blindly separating non - gaussian source signals .",
    "it leads to the identifiability of the structures in theory , and is demonstrated to be fulfilled on a wide class of real data .",
    "however , new challenges arises when many recent studies tend to focus on big data with complex structures .",
    "firstly , existing algorithms are computationally prohibitive to processing these data .",
    "for instance , the update of low - rank part in rpca and in its extensions invoke a full singular value decomposition ( svd ) per iterate , while mca requires challenging @xmath10 or @xmath11 minimization per sample / feature and previously achieved incoherent dictionaries / transform operators encouraging sparse representations . thus they suffer from a dramatic growth in time complexity when either feature dimensions or data samples increase . in previous methods , the structured information such as low - rank and sparse properties",
    "are always achieved at the price of time - consuming optimization , but are rarely leveraged for the purpose of improving the scalability .",
    "recent progresses in randomized approximation and rank - revealing algorithms shed some light on the speedup of the robust pca typed algorithms : the subspace of the low - rank part can be estimated from random sampling of its columns / rows or projections of its columns / rows on a random ensemble with bounded precision @xcite . however , straightforward invoking this technique in rpca problem needs to apply it to the updated residual matrix per iterate and thus may lead to costly computation . besides , determining the rank of the low - rank part is not a trivial problem in practice .    secondly , the simple low - rank , sparse and sparse representation assumptions can not fully capture the sophisticated relation , individuality and sparsity of data samples with complex structures . while low - rank structure summarizes a global linear relationship between data points , the nonlinear relationship , local geometry and correlated functions are more common in big data and more expressive for a much wider class of structures .",
    "moreover , the sparse matrix is simply explained by random noises on random positions in the past , but current studies reveal that it may have rich structured information that could be the central interests of various applications . for instance , the sparse motions captured by rpca on video sequence data includes immense unexplored information favored by object tracking and behavior analysis .",
    "furthermore , although the sparse representation is more general than sparse features , its quality largely relies on whether the given dictionary or transform operator fits the nature of data well .",
    "but this is difficult to evaluate when the data is of large volume and in general type .",
    "thirdly , two building parts are not sufficient to cover all the mixtures of incoherent structures in big data .",
    "one the one hand , dense noise is an extra component that has to be separated from the low - rank and sparse parts in many cases where the exact decomposition @xmath12 does not hold .",
    "this noisy assumption has been considered in stable pcp @xcite , drmf @xcite and other theoretical studies @xcite , and its robustness and adaptiveness to a broad class of data has also been verified .",
    "but efficient algorithm for the noisy model lacks . on the other hand , further decomposing the low - rank or sparse part to multiple distinguishable sub - components is potential to tell locally spatial or temporal relations within each identifiable structure and differences between them , which usually play pivot roles in supervised and unsupervised learning tasks .",
    "although it appeals to be a natural extension to the two - part model in rpca , how to formulate a proper decomposition model for learning problems and develop a practical algorithm are challenging .",
    "we start this paper by studying a novel low - rank and sparse matrix decomposition model `` go decomposition ( godec ) '' @xcite @xmath2 , which takes an extra dense noisy part @xmath3 into account and casts the decomposition into alternating optimization of low - rank @xmath0 and sparse @xmath1 . in order to overcome the computational burden caused by the large volume of data ,",
    "we propose two acceleration strategies in designing the decomposition algorithms : the first is `` bilateral random projection ( brp ) '' @xcite based fast low - rank approximation that results in a randomized update of the low - rank part or its nonlinear variant , this technique is based on recently developed random matrix theories that show a few random projections of a matrix is able to reveal its associated principle subspace @xcite ; the other is a frank - wolfe typed optimization scheme called `` greedy bilateral ( greb ) '' paradigm @xcite that updates the left and right factors of the low - rank matrix variable in a mutually adaptive and greedy incremental manner .",
    "we show the two strategies generates considerably scalable algorithms for low - rank and sparse matrix decomposition .",
    "moreover , both strategies have provable performance guarantee given by rigorous theoretical analysis ( appendix i and ii ) .    in order to deal with the complicated structures that can not be captured by the sum mixture of low - rank and sparse matrices , we proposes three variants of godec more expressive and general for learning from big data .",
    "the first variant `` shifted subspace tracking ( sst ) '' @xcite is developed for motion segmentation @xcite from raw pixels of video sequence .",
    "sst further analyzes the unexplored rich structure of the sparse part @xmath1 of godec , which could be seem as a sum mixture of several motions with distinct appearance and trajectories .",
    "sst unifies detection , tracking and segmenting multiple motions from complex scenes in a simple matrix factorization model .",
    "the second variant `` multi - label subspace ensemble ( mse ) '' @xcite extends the low - rank part @xmath0 of godec to the sum of multiple low - rank matrices defined by distinguishable but correlated subspaces .",
    "mse provides a novel insight into the multi - label learning ( ml ) problem @xcite .",
    "it addresses this problem by jointly learning inverse mappings that map each label to the feature space as a subspace , and formulating the prediction as finding the group sparse representation @xcite of a given sample on the ensemble of subspaces .",
    "there are only @xmath13 subspaces needed to be learned , and the label correlations are fully used via considering correlation among subspaces .",
    "the third variant `` linear functional godec ( lingodec ) '' learns scoring functions of users from their ratings matrix @xmath8 and features of scored items @xmath7 .",
    "it extends the low - rank part @xmath0 of godec to @xmath5 , where @xmath6 represents the linear functions and is constrained to be low - rank , while the rows of @xmath7 contain the features of items in the training set .",
    "in addition , the sparse part @xmath1 is able to detect the advertising effects or anomaly of users ratings on specific items .",
    "lingodec formulates the collaborative filtering problem as supervised learning , and thus avoids time - consuming completion of the whole matrix when only a new item s scores ( a new row ) are needed to be predicted .",
    "the rest of this paper is organized as following : section 2 introduces godec ; section 3 proposes the two acceleration strategies for processing large - scale data ; section 4 proposes the three variants of godec and their practical algorithms ; section 5 shows the experimental results of all the proposed algorithms on different application problems and justifies both the effectiveness and efficiency of them .",
    "the rows of all data matrices mentioned in this paper represents the samples and the columns denote the features .",
    "in rpca @xcite , pcp recovers @xmath0 and @xmath1 from @xmath8 by minimizing sum of the trace norm of @xmath0 and the @xmath11 norm of @xmath1 . it can be proved that the solution to this convex relaxation is the exact recovery if @xmath12 indeed exists and @xmath0 and @xmath1 are sufficiently incoherent @xcite .",
    "that is , @xmath0 obeys the incoherence property and thus is not sparse , while @xmath1 has nonzero entries uniformly selected at random and thus is not low - rank .",
    "popular optimization algorithms such as augmented lagrangian multiplier , accelerated proximal gradient method and accelerated projected gradient method @xcite have been applied .",
    "but full svd as a costly subroutine is required to be repeatedly invoked in any of them .    despite the strong theoretical guarantee of robust pca",
    ", the exact decomposition @xmath12 does not always hold for real data matrix @xmath8 due to extra noise and complicated structure of @xmath1 that does not following bernoulli - gaussian distribution .",
    "thus a more adaptive model @xmath2 is preferred , where @xmath9 approximates @xmath8 and @xmath3 is the dense noise .",
    "we then study the approximated `` low - rank+sparse '' decomposition of a matrix @xmath8 , i.e. , @xmath14 in this section , we develop `` go decomposition '' ( godec ) to estimate the low - rank part @xmath0 and the sparse part @xmath1 from @xmath8 by solving the following optimization problem , which aims at minimizing the decomposition error : @xmath15      we propose the nave godec algorithm at first and will study how to achieve an highly accelerated version in the next section .",
    "the optimization problem of godec ( [ e : ls_app ] ) can be solved by alternatively solving the following two subproblems until convergence : @xmath16 although both subproblems ( [ e : raw_ls ] ) have nonconvex constraints , their global solutions @xmath17 and @xmath18 exist .",
    "let the svd of a matrix @xmath8 be @xmath19 and @xmath20 or @xmath21 stands for the @xmath22 largest singular value of @xmath8 ; @xmath23 is the projection of a matrix to an entry set @xmath24 .    in particular , the two subproblems in ( [ e : raw_ls ] ) can be solved by updating @xmath17 via singular value hard thresholding of @xmath25 and updating @xmath18 via entry - wise hard thresholding of @xmath26 , respectively , i.e. , @xmath27 the main computation in the nave godec algorithm ( [ e : raw_lss ] ) is the svd of @xmath25 in the updating @xmath17 sequence .",
    "svd requires @xmath28 flops , so it is impractical when @xmath8 is of large size , and more efficient algorithm is needed to be developed later .",
    "godec alternatively assigns the @xmath29-rank approximation of @xmath4 to @xmath0 and assigns the sparse approximation with cardinality @xmath13 of @xmath30 to @xmath1 .",
    "the updating of @xmath0 is obtained via singular value hard thresholding of @xmath4 , while the updating of @xmath1 is obtained via entry - wise hard thresholding @xcite of @xmath30 .",
    "the term `` go '' is owing to the similarities between @xmath0/@xmath1 in the godec iteration rounds and the two players in the game of go .    except the additional noisy part @xmath3 and faster speed ,",
    "the direct constraints to the rank of @xmath0 and the cardinality @xmath1 also makes godec different from rpca minimizing their convex polytopes .",
    "this makes the rank and cardinality controllable , which is preferred in practice . because prior information of these two parameters can be applied and lots of computations might be saved .",
    "in addition , godec introduces an efficient matrix completion algorithm @xcite , in which the cardinality constraint is replaced by a fixed support set . convergence and robustness analysis of godec",
    "is given in appendix i based on theory of alternating projection on two manifolds @xcite .",
    "we firstly introduce the bilateral random projections ( brp ) based low - rank approximation and its power scheme modification .",
    "brp reduces the time consuming svd in nave godec to a closed - form approximation merely requiring small matrix multiplications .",
    "however , we need to invoke more expensive power scheme of brp when the matrix spectrum does not have dramatic decreasing .",
    "moreover , the rank needs to be estimated for saving unnecessary computations .",
    "thus we propose greedy bilateral sketch ( grebske ) , which augments the matrix factors column / rows - wisely by selecting the best rank - one directions for approximation .",
    "it can adaptively determines the rank by stopping the augmenting when error is sufficiently small , and has accuracy closer to svd .",
    "given @xmath29 bilateral random projections ( brp ) of an @xmath31 dense matrix @xmath8 ( w.l.o.g , @xmath32 ) , i.e. , @xmath33 and @xmath34 , wherein @xmath35 and @xmath36 are random matrices , @xmath37 is a fast rank-@xmath29 approximation of @xmath8 .",
    "the computation of @xmath0 includes an inverse of an @xmath38 matrix and three matrix multiplications .",
    "thus , for a dense @xmath8 , @xmath39 floating - point operations ( flops ) are required to obtain brp , @xmath40 flops are required to obtain @xmath0 .",
    "the computational cost is much less than svd based approximation .    in order to improve the approximation precision of @xmath0 in ( [ e : lr_app ] )",
    "when @xmath41 and @xmath42 are standard gaussian matrices , we use the obtained right random projection @xmath43 to build a better left projection matrix @xmath42 , and use @xmath44 to build a better @xmath41 . in particular , after @xmath33 , we update @xmath45 and calculate the left random projection @xmath34 , then we update @xmath46 and calculate the right random projection @xmath33",
    ". a better low - rank approximation @xmath0 will be obtained if the new @xmath43 and @xmath44 are applied to ( [ e : lr_app ] ) .",
    "this improvement requires additional flops of @xmath47 in brp calculation .",
    "when singular values of @xmath8 decay slowly , ( [ e : lr_app ] ) may perform poorly .",
    "we design a modification for this situation based on the power scheme @xcite . in the power scheme modification",
    ", we instead calculate the brp of a matrix @xmath48 , whose singular values decay faster than @xmath8 .",
    "in particular , @xmath49 . both @xmath8 and @xmath50",
    "share the same singular vectors .",
    "the brp of @xmath50 is : @xmath51 according to ( [ e : lr_app ] ) , the brp based @xmath29 rank approximation of @xmath50 is : @xmath52 in order to obtain the approximation of @xmath8 with rank @xmath29 , we calculate the qr decomposition of @xmath43 and @xmath44 , i.e. , @xmath53 the low - rank approximation of @xmath8 is then given by : @xmath54^{\\frac{1}{2q+1}}q_2^t.\\ ] ] the power scheme modification ( [ e : mlr_app ] ) requires an inverse of an @xmath38 matrix , an svd of an @xmath38 matrix and five matrix multiplications .",
    "therefore , for dense @xmath8 , @xmath55 flops are required to obtain brp , @xmath56 flops are required to obtain the qr decompositions , @xmath57 flops are required to obtain @xmath0 . the power scheme modification reduces the error of ( [ e : lr_app ] ) by increasing @xmath58 .",
    "when the random matrices @xmath41 and @xmath42 are built from @xmath43 and @xmath44 , @xmath47 additional flops are required in the brp calculation .",
    "thorough error bound analysis of brp and its power scheme is given in appendix ii .",
    "since brp based low - rank approximation is near optimal and efficient , we replace svd with brp in nave godec in order to significantly reduce the time cost .",
    "we summarize godec using brp based low - rank approximation ( [ e : lr_app ] ) and power scheme modification ( [ e : mlr_app ] ) in algorithm 1 . when @xmath59 , for dense @xmath8 , ( [ e : lr_app ] ) is applied .",
    "thus the qr decomposition of @xmath43 and @xmath44 in algorithm 1 are not performed , and @xmath17 is updated as @xmath60 . in this case ,",
    "algorithm [ alg : godec ] requires @xmath61 flops per iteration .",
    "when integer @xmath62 , ( [ e : mlr_app ] ) is applied and algorithm 1 requires @xmath63 flops per iteration .",
    "[ alg : godec ] * initialize * @xmath64 , @xmath65 , @xmath66      the major computation in nave godec is the update of the low - rank part @xmath0 , which requires at least a truncated svd . although the proposed randomized strategy provides a faster and svd - free algorithm for godec , how to determine the rank of @xmath0 and the cardinality of @xmath1 is still an unsolved problem in real applications .",
    "in fact , these two parameters are not easy to determine and could lead to unstable solutions when estimated incorrectly .",
    "noisy robust pca methods such as stable pcp @xcite , godec @xcite and drmf @xcite usually suffer from this problem .",
    "another shortcoming of the randomized strategy is that the time complexity is dominated by matrix multiplications , which could be computationally slow on high - dimensional data .",
    "in this part , we describe and analyze a general scheme called `` greedy bilateral ( greb ) '' paradigm for solving optimizing low - rank matrix in mainstream problems . in greb",
    ", the low - rank variable @xmath0 is modeled in a bilateral factorization form @xmath67 , where @xmath68 is a tall matrix and @xmath69 is a fat matrix .",
    "it starts from @xmath68 and @xmath69 respectively containing a very few ( e.g. , one ) columns and rows , and optimizes them alternately .",
    "their updates are based on observation that the object value is determined by the product @xmath67 rather than individual @xmath68 or @xmath69 .",
    "thus we can choose a different pair @xmath70 producing the same @xmath67 but computed faster than the one derived by alternating least squares like in irls - m @xcite and als @xcite .",
    "in greb , the updates of @xmath68 and @xmath69 can be viewed as mutually adaptive update of the left and right sketches of the low - rank matrix .",
    "such updates are repeated until the object convergence , then a few more columns ( or rows ) are concatenated to the obtained @xmath68 ( or @xmath69 ) , and the alternating updates are restarted on a higher rank . here , the added columns ( or rows ) are selected in a greedy manner .",
    "specifically , they are composed of the rank-@xmath71 column ( or row ) directions on which the object decreases fastest .",
    "greb incrementally increases the rank until when @xmath67 is adequately consistent with the observations .",
    "[ alg : greb ] * initialize * @xmath72 ( and @xmath1 )    greb s greedy strategy avoids the failures brought by possible biased rank estimation .",
    "moreover , greedy selecting optimization directions from @xmath71 to @xmath29 is faster than updating @xmath29 directions in all iterates like in lmafit @xcite and @xcite .",
    "in addition , the lower rank solution before each rank increment is invoked as the `` warm start '' of the next higher rank optimization and thus speed up convergence .",
    "furthermore , its mutually adaptive updates of @xmath68 and @xmath69 yields a simple yet efficient svd - free implementation . under greb paradigm ,",
    "the overall time complexity of matrix completion is @xmath73 ( @xmath24-sampling set , @xmath31-matrix size , @xmath29-rank ) , while the overall complexities of low - rank approximation and noisy robust pca are @xmath74 .",
    "an improvement on sample complexity can also be justified .",
    "an theoretical analysis of greb solution convergence based on the result of geco @xcite is given in appendix iii .    in the following , we present greb by using it to derive a practical algorithm `` greedy bilateral smoothing ( grebsmo ) '' for godec .",
    "it can also be directly applied to low - rank approximation and matrix completion [ ] .",
    "we summarize general greb paradigm in algorithm [ alg : greb ] , and then present the detailed grebsmo algorithm .",
    "in particular , we formulate godec by replacing @xmath0 with its bilateral factorization @xmath75 and regularizing the @xmath11 norm of @xmath1 s entries : @xmath76 note the @xmath11 regularization is a minor modification to the cardinality constraint in ( [ e : ls_app ] ) .",
    "it induces soft - thresholding in updating @xmath1 , which is faster than sorting caused by cardinality constraint in godec and drmf .    alternately optimizing @xmath68 , @xmath69 and @xmath1 in ( [ equ : grebsmo ] )",
    "immediately yields the following updating rules : @xmath77 where @xmath78 is an element - wise soft thresholding operator with threshold @xmath79 such that @xmath80\\times [ n]\\right\\}.\\ ] ] the same trick of replacing the @xmath70 pair with a faster computed one is applied and produce @xmath81 the above procedure can be performed in @xmath82 flops for @xmath83 and @xmath84 .    in grebsmo , ( [ equ : grebsmoa ] )",
    "is iterated as a subroutine of greb s greedy incremental paradigm . in particular , the updates in ( [ equ : grebsmoa ] ) are iterated for @xmath85 times or until the object converging , then @xmath86 rows are added into @xmath69 as the new directions for decreasing the object value . in order to achieve the fastest decreasing directions , we greedily select the added @xmath86 rows as the top @xmath86 right singular vectors of the partial derivative @xmath87 we also allow to approximate row space of the singular vectors via random projections @xcite .",
    "the selected @xmath86 rows maximize the magnitude of the above partial derivative and thus lead to the most rapid decreasing of the object value , a.k.a .",
    ", the decomposition error .",
    "grebsmo repeatedly increases the rank until a sufficiently small decomposition error is achieved .",
    "so the rank of the low - rank component is adaptively estimated in grebsmo and does not relies on initial estimation .",
    "although the two strategies successfully generate efficient low - rank and sparse decomposition capable to tackle large volume problem of big data , the complicated structures widely existing in big data can not be always expressed by the sum of low - rank and sparse matrices and thus may still lead to the failure of rpca typed models .",
    "therefore , we address this problem by developing several godec s variants that unravel different combination of incoherent structures beyond low - rank and sparse matrices , where the two strategies can be still used to achieve scalable algorithms .",
    "sst decomposes @xmath1 of godec into the sum of several matrices , each of whose rows are generated by imposing a smooth geometric transformation sequence to the rows of a low - rank matrix .",
    "these rows store moving object in the same motion after aligning them across different frames , while the geometric transformation sequence defines the shared trajectories and deformations of those moving objects across frames . in the following ,",
    "we develop an efficient randomized algorithm extracting the motions in sequel , where the low - rank matrix for each motion is updated by brp , and the geometric transformation sequence is updated in a piece - wise linear approximation manner .",
    "we consider the problem of motion segmentation from the raw video data . given a data matrix @xmath88 that stores a video sequence of @xmath89 frames , each of which has @xmath90 pixels and reshaped as a row vector in @xmath8 , the goal of sst framework is to separate the motions of different object flows , recover both their low - rank patterns and geometric transformation sequences .",
    "this task is decomposed as two steps , background modeling that separates all the moving objects from the static background , and flow tracking that recovers the information of each motion . in this problem",
    ", @xmath91 stands for the @xmath22 entry of a vector or the @xmath22 row of a matrix , while @xmath92 signifies the entry at the @xmath22 row and the @xmath93 column of a matrix",
    ".    the first step can be accomplished by either godec or grebsmo .",
    "after obtaining the sparse outliers @xmath1 storing multiple motions , sst treats the sparse matrix @xmath1 as the new data matrix @xmath8 , and decomposes it as @xmath94 , wherein @xmath95 denotes the @xmath22 motion , @xmath1 stands for the sparse outliers and @xmath3 stands for the gaussian noise .",
    "the motion segmentation in sst is based on an observation to the implicit structures of the sparse matrix @xmath95 . if the trajectory of the object flow @xmath95 is known and each frame ( row ) in @xmath95 is shifted to the position of a reference frame , due to the limited number of poses for the same object flow in different frames , it is reasonable to assume that the rows of the shifted @xmath95 exist in a subspace . in other words ,",
    "@xmath95 after inverse geometric transformation is low - rank .",
    "hence the sparse motion matrix @xmath95 has the following structured representation @xmath96=l(i)\\circ\\tau(i).\\ ] ] the invertible transformation @xmath97 denotes the 2-d geometric transformation ( to the reference frame ) associated with the @xmath22 motion in the @xmath93 frame , which is represented by @xmath98 . to be specific , the @xmath93 row in @xmath95 is @xmath98 after certain permutation of its entries .",
    "the permutation results from applying the nonlinear transformation @xmath99 to each nonzero pixel in @xmath98 such that , @xmath100 where @xmath99 could be one of the five geometric transformations @xcite , i.e. , translation , euclidean , similarity , affine and homography , which are able to be represented by @xmath101 , @xmath102 , @xmath103 , @xmath104 and @xmath105 free parameters , respectively .",
    "for example , affine transformation is defined as @xmath106      = \\left [         \\begin{array}{cc }           \\rho\\cos\\theta & \\rho\\sin\\theta \\\\",
    "-\\rho\\sin\\theta & \\rho\\cos\\theta \\\\         \\end{array }       \\right ]       \\left [         \\begin{array}{c }           x \\\\",
    "y \\\\",
    "\\end{array }       \\right]+       \\left [         \\begin{array}{c }           t_x \\\\           t_y \\\\",
    "\\end{array }       \\right],\\ ] ] wherein @xmath107 is the rotation angle , @xmath108 and @xmath109 are the two translations and @xmath110 is the scaling ratio .",
    "it is worth to point out that @xmath99 can be any other transformation beyond the geometric group .",
    "so sst can be applied to sparse structure in other applications if parametric form of @xmath99 is known .",
    "we define the nonlinear operator @xmath111 as @xmath112 therefore , the flow tracking in sst aims at decomposing the sparse matrix @xmath8 ( @xmath1 obtained in the background modeling ) as @xmath113 in sst , we iteratively invoke @xmath13 times of the following matrix decomposition to greedily construct the decomposition in ( [ e : sstmodel ] ) : @xmath114 in each time of the matrix decomposition above , the data matrix @xmath8 is @xmath1 obtained by former decomposition . in order to save the computation and facilitate the parameter tuning",
    ", we cast the decomposition ( [ e : submodel ] ) into an optimization similar to ( [ e : ls_app ] ) , @xmath115    flow tracking in sst solves a sequence of optimization problem of type ( [ e : sstopt ] ) .",
    "thus we firstly apply alternating minimization to ( [ e : sstopt ] ) .",
    "this results in iterative update of the solutions to the following three subproblems , @xmath116      the first subproblem aims at solving the following series of nonlinear equations of @xmath118 , @xmath119 albeit directly solving the above equation is difficult due to its strong nonlinearity , we can approximate the geometric transformation @xmath120 by using piece - wise linear transformations , where each piece corresponds to a small change of @xmath118 defined by @xmath121",
    ". thus the solution of ( [ e : tau ] ) can be approximated by accumulating a series of @xmath121 .",
    "this can be viewed as an inner loop included in the update of @xmath117 .",
    "thus we have linear approximation @xmath122 where @xmath123 is the jacobian of @xmath120 with respect to the transformation parameters in @xmath118 .",
    "therefore , by substituting ( [ e : tauapp ] ) into ( [ e : tau ] ) , @xmath121 in each linear piece can be solved as @xmath124 the update of @xmath118 starts from some initial @xmath118 , and iteratively solves the overdetermined linear equation ( [ e : tauequ ] ) with update @xmath125 until the difference between the left hand side and the right hand side of ( [ e : tau ] ) is sufficiently small .",
    "it is critical to emphasize that a well selected initial value of @xmath118 can significantly save computational time .",
    "based on the between - frame affinity , we initialize @xmath118 by the transformation of its adjacent frame that is closer to the template frame @xmath126 , @xmath127 another important support set constraint , @xmath128 , needs to be considered in calculating @xmath120 during the update of @xmath117 .",
    "this constraint ensures that the object flows or segmented motions obtained by sst always belong to the sparse part achieved from the background modeling , and thus rules out the noise in background . hence , suppose the complement set of @xmath129 to be @xmath130 , each calculation of @xmath120 follows a screening such that , @xmath131      the second subproblem has the following global solution that can be updated by brp based low - rank approximation ( [ e : lr_app ] ) and its power scheme modification , @xmath132 wherein @xmath133 denotes the inverse transformation towards @xmath117 .",
    "the svds can be accelerated by brp based low - rank approximation ( [ e : ls_app ] ) .",
    "another acceleration trick is based on the fact that most columns of @xmath134 are nearly all - zeros .",
    "this is because the object flow or motion after transformation occupies a very small area of the whole frame .",
    "therefore , the update of @xmath135 can be reduced to low - rank approximation of a submatrix of @xmath134 that only includes dense columns .",
    "since the number of dense columns is far less than @xmath136 , the update of @xmath135 can become much faster .",
    "the third subproblem has a global solution that can be obtained via soft - thresholding @xmath137 similar to the update of @xmath1 in grebsmo , @xmath138    [ a : sst ]    a support set constraint @xmath139 should be considered in the update of @xmath1 as well .",
    "hence the above update follows a postprocessing , @xmath140    note the transformation computation @xmath111 in the update can be accelerated by leveraging the sparsity of the motions .",
    "specifically , the sparsity allows sst to only compute the transformed positions of the nonzero pixels .",
    "we summarize the sst algorithm in algorithm [ a : sst ] .",
    "mse provides a novel insight into the multi - label learning ( ml ) problem , which aims at predicting multiple labels of a data sample .",
    "most previous ml methods @xcite focus on training effective classifiers that establishes a mapping from feature space to label space , and take the label correlation into account in the training process . because it has been longly believed",
    "that label correlation is useful for improving prediction performance .",
    "however , in these methods , both the label space and the model complexity will grow rapidly when increasing the number of labels and simultaneously modeling their joint correlations .",
    "this usually makes the available training samples insufficient for learning a joint prediction model .",
    "mse eliminates this problem by jointly learning inverse mappings that map each label to the feature space as a subspace , and formulating the prediction as finding the group sparse representation @xcite of a given sample on the ensemble of subspaces . in the training stage ,",
    "the training data matrix @xmath8 is decomposed as the sum of several low - rank matrices and a sparse residual via a randomized optimization .",
    "each low - rank part defines a subspace mapped by a label , and its rows are nonzero only when the corresponding samples are annotated by the label . the sparse part captures the rest contents in the features that can not be explained by the labels .",
    "the training stage of mse approximately decomposes the training data matrix @xmath88 into @xmath141 .",
    "for the matrix @xmath142 , the rows corresponding to the samples with label @xmath143 are nonzero , while the other rows are all - zero vectors .",
    "the nonzero rows denote the components explained by label @xmath143 in the feature space .",
    "we use @xmath144 to denote the index set of samples with label @xmath143 in the matrix @xmath8 and @xmath142 , and then the matrix composed of the nonzero rows in @xmath142 is represented by @xmath145 . in the decomposition , the rank of @xmath145 is upper bounded , which indicates that all the components explained by label @xmath143 nearly lies in a linear subspace .",
    "the matrix @xmath1 is the residual of the samples that can not be explained by the given labels . in the decomposition ,",
    "the cardinality of @xmath1 is upper bounded , which makes @xmath1 sparse .",
    "if the label matrix of @xmath8 is @xmath146 , the rank of @xmath145 is upper bounded by @xmath147 and the cardinality of @xmath1 is upper bounded by @xmath85 , the decomposition can be written as solving the following constrained minimization problem : @xmath148 therefore , each training sample in @xmath8 is decomposed as the sum of several components , which respectively correspond to multiple labels that the sample belongs to .",
    "mse separates these components from the original sample by building the mapping from the labels to the feature space . for label @xmath143",
    ", we obtain its mapping in the feature space as the row space of @xmath145 .",
    "although the rank constraint to @xmath145 and cardinality constraint to @xmath1 are not convex , the optimization in ( [ e : ms ] ) can be solved by alternating minimization that decomposes it as the following @xmath149 subproblems , each of which has the global solution : @xmath150    the solutions of @xmath145 and @xmath1 in the above subproblems can be obtained via hard thresholding of singular values and the matrix entries , respectively .",
    "note that both svd and matrix entry - wise hard thresholding have global solutions .",
    "in particular , @xmath145 is built from the first @xmath147 largest singular values and the corresponding singular vectors of @xmath151 , while @xmath1 is built from the @xmath85 entries with the largest absolute value in @xmath152 , i.e. , @xmath153=u\\lambda v^t ; \\\\",
    "s=\\mathcal { p}_{\\phi}\\left(x-\\sum\\limits_{j=1}^kl^j\\right ) , \\phi:\\left|\\left(x-\\sum\\limits_{j=1}^kl^j\\right)_{{r , s}\\in{\\phi}}\\right|\\neq0 \\\\ { \\rm~and~ } \\geq \\left|\\left(x-\\sum\\limits_{j=1}^kl^j\\right)_{{r , s}\\in{\\overline{\\phi}}}\\right| , |\\phi|\\leq k.    \\end{array } \\right.\\ ] ] the projection @xmath154 represents that the matrix @xmath1 has the same entries as @xmath155 on the index set @xmath156 , while the other entries are all zeros .",
    "the decomposition is then obtained by iteratively solving these @xmath149 subproblems in ( [ e : mssub ] ) according to ( [ e : mssolution ] ) . in this problem , we initialize @xmath145 and @xmath1 as @xmath157 in each subproblem , only one variable is optimized with the other variables fixed .",
    "similar to godec , brp based acceleration strategy can be applied to the above model and produces the practical training algorithm in algorithm [ a : msetraining ] .    in the training ,",
    "the label correlations is naturally preserved in the subspace ensemble , because all the subspaces are jointly learned . since only @xmath13 subspaces",
    "are learned in the training stage , mse explores label correlations without increasing the model complexity .",
    "[ a : msetraining ] initialize @xmath142 and @xmath1 according to ( [ e : msinitial ] ) , @xmath66 qr decomposition @xmath158 for @xmath159 , @xmath160      in the prediction stage of mse , we use group _ lasso _ @xcite@xcite to estimate the group sparse representation @xmath161 of a test sample @xmath162 on the subspace ensemble @xmath163 $ ] , wherein the @xmath13 groups are defined as index sets of the coefficients corresponding to @xmath164 . since group _ lasso _ selects nonzero coefficients group - wisely , nonzero coefficients in the group sparse representation will concentrate on the groups corresponding to the labels that the sample belongs to .    according to the above analysis",
    ", we solve the following group _ lasso _ problem in the prediction stage of mse @xmath165 where the index set @xmath166 includes all the integers between @xmath167 and @xmath168 ( including these two ) .    to obtain the final prediction of the label vector @xmath169 for a test sample @xmath170",
    ", we use a simple thresholding of the magnitude sum of coefficients in each group to test which groups that the sparse coefficients in @xmath171 concentrate on @xmath172 although @xmath173 can also be obtained via selecting the groups with nonzero coefficients when @xmath79 in ( [ e : mspredict ] ) is chosen properly , we set the threshold @xmath174 as a small positive value to guarantee the robustness to @xmath79 .",
    "although low - rank matrix completion provides an effective and simple mathematical model predicting a user s rating to an item from her / his ratings to other items and the ratings of other users by exploring the user relationships , a primary problem of this model is that adding a new item or a new user to the model requires an new optimization of the whole low - rank rating matrix , which is not practical due to its expensive time cost .",
    "moreover , although the attributes of users are always missing in real recommendation systems , features of the items have been proved to be helpful side information that is much easier to obtain .",
    "but previous matrix completion methods and godec can not leverage this information in their models .",
    "furthermore , robust rating prediction should allow advertising effects in known ratings .",
    "in this part , we propose a variant of godec called `` linear functional godec ( lingodec ) '' .",
    "it formulates the collaborative filtering problem as supervised learning , and avoids time - consuming completion of the whole matrix when only a new item s scores ( a new row ) are needed to be predicted .",
    "in particular , lingodec decomposes rating matrix @xmath8 whose rows index the users , columns index the items , and entries denote the scores of items given by different users .",
    "given the features of some items , which are usually available , and the ratings of these items scored by all users , lingodec learns a scoring function for each user so that efficient prediction of ratings can be made item - wisely .",
    "it studies the case when the scoring functions of different users are linear and related to each other . in the mode",
    ", it replaces the low - rank part @xmath0 of godec with @xmath5 , where @xmath6 represents the linear related functions and the rows of @xmath7 are items represented by features .",
    "the sparse part @xmath1 is able to capture the advertising effects or anomaly of users ratings on specific items , which can not be represented by the low - rank scoring functions . in the algorithm of lingodec ,",
    "the update of low - rank @xmath6 is accomplished by invoking an elegant closed - form solution for least square rank minimization @xcite , which could be accelerated by brp .",
    "lingodec aims at solving the following optimization , @xmath175    we constrain @xmath6 to be low - rank so that the functions of different users share the same small set of basis functions . in addition , we apply @xmath11 regularization to the entries of @xmath1 so that the advertising effects in training ratings can be captured and ruled out from the learning of @xmath6 . by applying alternating minimization to ( [ equ : lingodec ] )",
    ", we have @xmath176 the update of @xmath177 in above procedures equals to solve a least squares rank minimization , which has been discovered owning closed - form solution that can be obtained by truncated svd [ ] when @xmath8 is singular ( the most common case in our problem ) . by applying bilateral random projection based acceleration to the truncated svd , we immediately achieve the final fast algorithm for lingodec .",
    "lingodec has a similar model as rank - regularized multi - task learning , but the major difference is that the sparse matrix in lingodec is a component of the data matrix rather than the linear functions @xmath6 .",
    "this section evaluates both the effectiveness and the efficiency of all the algorithms proposed in this paper , and compares them with state - of - the - art rivals .",
    "we will show experimental results of godec and grebsmo on both surveillance video sequences for background modeling and synthetic data",
    ". then we will apply sst , mse and lingodec to the problems of motion segmentation , multi - label learning and collaborative filtering .",
    "we run all the experiments in matlab on a server with dual quad - core 3.33 ghz intel xeon processors and 32 gb ram .",
    "the relative error @xmath178 is used to evaluate the effectiveness , wherein @xmath8 is the original matrix and @xmath179 is an estimate / approximation .",
    ".relative error and time cost of rpca and godec in low - rank+sparse decomposition tasks .",
    "the results separated by `` @xmath180 '' are rpca and godec , respectively .",
    "[ cols=\"^,^,^,^,^,^,^ \" , ]     [ table : rpcatime ]      for real data , three robust pca algorithms , i.e. , inexact augmented lagrangian multiplier method for pcp , godec and grebsmo are applied to separate the low - rank background and sparse moving objects in 3 video sequences from the same dataset used in godec experiment above .",
    "we show the robust pca decomposition results of one frame for each video sequence obtained by grebsmo in the left plot of figure [ fig : grebsmoexp ] .",
    "the time costs for all the three methods are listed in table [ table : rpcatime ] .",
    "it shows grebsmo considerably speed up the decomposition and performs @xmath181-@xmath182 times faster than most existing algorithms .",
    "we evaluate sst by using it to track object flows in four surveillance video sequences from the same dataset . in these experiments , the type of geometric transformation @xmath117 is simply selected as translation .",
    "the detection , tracking and segmentation results as well as associated time costs are shown in figure [ fig : hallshop ] .    [ fig : hallshop ]    the results show sst can successfully recover both the low - rank patterns and the associated geometric transformations for motions of multiple object flows from the sparse component achieved by godec .",
    "the detection , tracking and segmentation are seamlessly unified in a matrix factorization framework and achieved with high accuracy .",
    "moreover , it also verifies that sst performs significantly robust on complicated motions in complex scenes .",
    "this is attributed to their distinguishing shifted low - rank patterns , because different object flows can hardly share a subspace after the same geometric transformation .",
    "since sst show stable and appealing performance in motion detection , tracking and segmentation for either crowd or individual , it provides a more semantic and intelligent analysis to the video content than existing methods .",
    "we evaluate mse on 13 benchmark datasets from different domains and of different scales , including corel5k ( image ) , scene ( image ) , mediamill ( video ) , enron ( text ) , genbase ( genomics ) , medical ( text ) , emotions ( music ) , slashdot ( text ) and @xmath183 sub datasets selected in yahoo dataset ( web data ) .",
    "these datasets were obtained from mulan s website and meka s website .",
    "they were collected from different practical problems .",
    "we compare mse with br @xcite , ml - knn @xcite and mddm @xcite on four evaluation metrics for evaluating the effectiveness , as well as the cpu seconds for evaluating the efficiency . in multi - label prediction ,",
    "four metrics , which are precision , recall , f1 score and accuracy , are used to measure the prediction performance .",
    "the detailed definitions of these metrics are given in section 7.1.1 of @xcite .",
    "a fair evaluation of prediction performance should include integrative consideration of all the four metrics , whose importances can be roughly given by @xmath184 .",
    "we show the prediction performance and time cost in cpu seconds of br , ml - knn , mddm and mse in table [ table : exp ] and table [ table : yahoo ] . in br",
    ", we use the matlab interface of libsvm 3.0 to train the classic linear svm classifiers for each label .",
    "the parameter @xmath185 with the best performance on the training set was used . in ml - knn ,",
    "the number of neighbors was @xmath181 for all the datasets .    in mddm ,",
    "the regularization parameter for uncorrelated subspace dimensionality reduction was selected as @xmath186 and the dimension of the subspace was set as @xmath187 of the dimension of the original data . in mse , we selected @xmath147 as an integer in @xmath188 $ ] , @xmath189 $ ] , @xmath190 $ ] and @xmath191 $ ] .",
    "we roughly selected @xmath103 groups of parameters in the ranges for each dataset and chose the one with the best performance on the training data .",
    "lasso _ in mse is solved by slep @xcite in our experiments .",
    "the experimental results show that mse is competitive on both speed and prediction performance , because it explores label correlations and structure without increasing the problem size .",
    "in addition , the bilateral random projections further accelerate the computation .",
    "in particular , its training time increases much more slowly than other methods , so it is more efficient when applied to large scale datasets such as mediamill , arts and education .",
    "mddm is faster than mse on a few datasets because mddm invokes ml - knn on the data after dimension reduction , while mse is directly applicable to the original high dimensional data .    2     & br & @xmath192 & @xmath193 & @xmath194 & @xmath195 & @xmath196 + & ml - knn & @xmath197 & @xmath198 & @xmath193 & @xmath104 & @xmath199 + & mddm & @xmath200 & @xmath104 & @xmath201 & @xmath183 & @xmath202 + & mse & @xmath203 & @xmath204 & @xmath205 & @xmath206 & @xmath207 + & br & @xmath208 & @xmath209 & @xmath206 & @xmath194 & @xmath210 + & ml - knn & @xmath211 & @xmath104 & @xmath205 & @xmath183 & @xmath212 + & mddm & @xmath213 & @xmath183 & @xmath194 & @xmath183 & @xmath214 + & mse & @xmath215 & @xmath203 & @xmath216 & @xmath217 & @xmath218 + & br & @xmath219 & @xmath220 & @xmath220 & @xmath221 & @xmath222 + & ml - knn & @xmath223 & @xmath105 & @xmath220 & @xmath224 & @xmath225 + & mddm & @xmath226 & @xmath198 & @xmath227 & @xmath104 & @xmath228 + & mse & @xmath215 & @xmath229 & @xmath230 & @xmath181 & @xmath231 + & br & @xmath232 & @xmath233 & @xmath233 & @xmath233 & @xmath234 + & ml - knn & @xmath213 & @xmath103 & @xmath235 & @xmath103 & @xmath236 + & mddm & @xmath226 & @xmath103 & @xmath233 & @xmath103 & @xmath237 + & mse & @xmath205 & @xmath238 & @xmath217 & @xmath194 & @xmath239 + & br & @xmath240 & @xmath241 & @xmath192 & @xmath242 & @xmath243 + & ml - knn & @xmath200 & @xmath105 & @xmath223 & @xmath224 & @xmath244 + & mddm & @xmath226 & @xmath198 & @xmath208 & @xmath198 & @xmath245 + & mse & @xmath219 & @xmath246 & @xmath247 & @xmath247 & @xmath248 +    [ table : yahoo ]     & br & @xmath208 & @xmath203 & @xmath249 & @xmath250 & @xmath251 + & ml - knn & @xmath215 & @xmath104 & @xmath252 & @xmath183 & @xmath253 + & mddm & @xmath230 & @xmath183 & @xmath254 & @xmath103 & @xmath255 + & mse & @xmath211 & @xmath247 & @xmath254 & @xmath256 & @xmath257 + & br & @xmath258 & @xmath206 & @xmath203 & @xmath195 & @xmath259 + & ml - knn & @xmath258 & @xmath198 & @xmath260 & @xmath183 & @xmath261 + & mddm & @xmath262 & @xmath224 & @xmath229 & @xmath198 & @xmath217 + & mse & @xmath263 & @xmath262 & @xmath204 & @xmath206 & @xmath264 + & br & @xmath101 & @xmath194 & @xmath183 & @xmath101 & @xmath265 + & ml - knn & @xmath266 & @xmath198 & @xmath267 & @xmath104 & @xmath268 + & mddm & @xmath241 & @xmath102 & @xmath181 & @xmath101 & @xmath269 + & mse & @xmath230 & @xmath270 & @xmath271 & @xmath194 & @xmath272 + & br & @xmath273 & @xmath221 & @xmath274 & @xmath275 & @xmath276 + & ml - knn & @xmath242 & @xmath275 & @xmath205 & @xmath224 & @xmath277 + & mddm & @xmath238 & @xmath71 & @xmath103 & @xmath71 & @xmath278 + & mse & @xmath279 & @xmath280 & @xmath256 & @xmath209 & @xmath281 + & br & @xmath282 & @xmath283 & @xmath226 & @xmath284 & @xmath285 + & ml - knn & @xmath247 & @xmath197 & @xmath208 & @xmath252 & @xmath286 + & mddm & @xmath266 & @xmath287 & @xmath208 & @xmath254 & @xmath288 + & mse & @xmath280 & @xmath289 & @xmath223 & @xmath200 & @xmath290 + & br & @xmath282 & @xmath254 & @xmath258 & @xmath291 & @xmath292 + & ml - knn & @xmath200 & @xmath206 & @xmath215 & @xmath221 & @xmath293 + & mddm & @xmath252 & @xmath206 & @xmath215 & @xmath221 & @xmath293 + & mse & @xmath204 & @xmath182 & @xmath294 & @xmath256 & @xmath295 + & br & @xmath183 & @xmath238 & @xmath105 & @xmath183 & @xmath296 + & ml - knn & @xmath182 & @xmath262 & @xmath297 & @xmath262 & @xmath298 + & mddm & @xmath299 & @xmath258 & @xmath297 & @xmath258 & @xmath300 + & mse & @xmath301 & @xmath302 & @xmath303 & @xmath223 & @xmath304 + & br & @xmath101 & @xmath235 & @xmath103 & @xmath101 & @xmath305 + & ml - knn & @xmath197 & @xmath71 & @xmath102 & @xmath306 & @xmath307 + & mddm & @xmath197 & @xmath71 & @xmath198 & @xmath71 & @xmath308 + & mse & @xmath105 & @xmath273 & @xmath224 & @xmath183 & @xmath309 +    [ table : exp ]    in the comparison of performance via the four metrics ,",
    "the f1 score and accuracy of mse outperform those of other methods on most datasets . moreover ,",
    "mse has smaller gaps between precision and recall on different tasks than other methods , and this implies it is robust to the imbalance between positive and negative samples .",
    "note in multi - label prediction , only large values of all four metrics are sufficient to indicate the success of the prediction , while the combination of some large valued metrics and some small valued ones are always caused by the imbalance of the samples .",
    "therefore , mse provides better prediction performance than other methods on most datasets .",
    "[ fig : phase750 ]      since most public available dataset for recommendation system rarely fulfill our demands for the training data in lingodec , we justify lingodec on synthetic data .",
    "specifically , the rating matrix @xmath8 is generated by @xmath310 .",
    "the weight matrix of linear functions @xmath6 is generated as the product of two gaussian matrices .",
    "entries in both the item feature matrix @xmath7 and noise matrix @xmath3 are generated by i.i.d .",
    "gaussian distribution .",
    "the sparse part has a bernoulli model generated support set on which @xmath311 values are randomly assigned .",
    "we show the phase diagram and the corresponding time cost in figure [ fig : phase750 ] .",
    "it could be seem that lingodec has a slightly larger region ( the white region ) for successful recovery than both grebsmo and robust pca @xcite .",
    "this is because side - information , i.e. , the features of items , is utilized in lingodec . moreover ,",
    "the time cost of lingodec is still small due to the closed - form update of @xmath6 and brp based acceleration .",
    "therefore , lingodec is capable to achieve the scoring functions of users , which can not be learned by previous matrix completion based methods , and is effective to rule out the advertising effects in user ratings .",
    "its fast speed makes it very efficient when applied to practical systems .",
    "we theoretically analyze the convergence of godec .",
    "the objective value ( decomposition error ) @xmath312 monotonically decreases and converges to a local minimum . since the updating of @xmath0 and @xmath1 in godec is equivalent to alternatively projecting @xmath0 or @xmath1 onto two smooth manifolds , we use the framework proposed in @xcite to prove the asymptotical property and linear convergence of @xmath0 and @xmath1 .",
    "the asymptotic and convergence speeds are mainly determined by the angle between the two manifolds .",
    "we discuss how @xmath0 , @xmath1 and @xmath3 influence the speeds via influencing the cosine of the angle .",
    "the analyses show the convergence of godec is robust to the noise @xmath3 .    in particular , we first prove that the objective value @xmath312 ( decomposition error ) converges to a local minimum .",
    "then we demonstrate the asymptotic properties of godec and prove that the solutions @xmath0 and @xmath1 respectively converge to local optimums with linear rate less than @xmath71 .",
    "the influence of @xmath0 , @xmath1 and @xmath3 to the asymptotic / convergence speeds is analyzed .",
    "the speeds are slowed down by augmenting the magnitude of noise part @xmath313 .",
    "however , the convergence still holds unless @xmath314 or @xmath315 .",
    "we have the following theorem about the convergence of the objective value @xmath312 in ( [ e : ls_app ] ) .",
    "[ t : ls_convergence ] ( * convergence of objective value * ) .",
    "the alternative optimization ( [ e : raw_ls ] ) produces a sequence of @xmath312 that converges to a local minimum .",
    "let the objective value @xmath312 after solving the two subproblems in ( [ e : raw_ls ] ) be @xmath316 and @xmath317 , respectively , in the @xmath318 iteration .",
    "on the one hand , we have @xmath319 the global optimality of @xmath18 yields @xmath320 .",
    "on the other hand , @xmath321 the global optimality of @xmath322 yields @xmath323 .",
    "therefore , the objective values ( decomposition errors ) @xmath312 keep decreasing throughout godec ( [ e : raw_ls ] ) : @xmath324 since the objective of ( [ e : ls_app ] ) is monotonically decreasing and the constraints are satisfied all the time , ( [ e : raw_ls ] ) produces a sequence of objective values that converge to a local minimum .",
    "this completes the proof .",
    "the asymptotic property and the linear convergence of @xmath0 and @xmath1 in godec are demonstrated based on the framework proposed in @xcite .",
    "we firstly consider @xmath0 . from a different prospective ,",
    "godec algorithm shown in ( [ e : raw_lss ] ) is equivalent to iteratively projecting @xmath0 onto one manifold @xmath325 and then onto another manifold @xmath326 .",
    "this kind of optimization method is the so called `` alternating projections on manifolds '' . to see this , in ( [ e : raw_lss ] ) , by substituting @xmath327 into the next updating of @xmath322 , we have : @xmath328 both @xmath325 and @xmath326 are two @xmath329-manifolds around a point @xmath330 : @xmath331 according to the above definitions , any point @xmath332 satisfies : @xmath333 thus any point @xmath332 is a local solution of @xmath0 in ( [ e : ls_app ] ) .",
    "we define the angle between two manifolds @xmath325 and @xmath326 at point @xmath0 as the angle between the corresponding tangent spaces @xmath334 and @xmath335 .",
    "the angle is between @xmath336 and @xmath337 with cosine : @xmath338 in addition , if @xmath339 is the unit sphere in @xmath340 , the angle between two subspaces @xmath341 and @xmath342 in @xmath340 is defined as the angle between @xmath336 and @xmath337 with cosine : @xmath343    we give the following proposition about the angle between two subspaces @xmath341 and @xmath342 :    [ p : setcal ] following the above definition of the angle between two subspaces @xmath341 and @xmath342 , we have @xmath344    the angle between @xmath325 and @xmath326 is used in the asymptotical property and the linear convergence rate of `` alternating projections on manifolds '' algorithms .",
    "[ t : asymptpro ] ( * asymptotic property * @xcite ) .",
    "let @xmath325 and @xmath326 be two transverse @xmath345-manifolds around a point @xmath330",
    ". then @xmath346 a refinement of the above argument is @xmath347 for @xmath348 and @xmath349 .",
    "[ t : linearcov ] ( * linear convergence of variables * @xcite ) . in @xmath340 ,",
    "let @xmath325 and @xmath326 be two transverse manifolds around a point @xmath330 .",
    "if the initial point @xmath350 is close to @xmath351 , then the method of alternating projections @xmath352 is well - defined , and the distance @xmath353 from the iterate @xmath17 to the intersection @xmath354 decreases q - linearly to zero . more precisely , given any constant @xmath355 strictly larger than the cosine of the angle of the intersection between the manifolds , @xmath356 , if @xmath357 is close to @xmath351 , then the iterates satisfy @xmath358 furthermore , @xmath17 converges linearly to some point @xmath359 , i.e. , for some constant @xmath360 , @xmath361    since godec algorithm can be written as the form of alternating projections on two manifolds @xmath325 and @xmath326 given in ( [ e : manifoldmn ] ) and they satisfy the assumptions of theorem [ t : asymptpro ] and theorem [ t : linearcov ] , @xmath0 in godec converges to a local optimum with linear rate .",
    "similarly , we can prove the linear convergence of @xmath1 .    since cosine @xmath362 in theorem [ t : asymptpro ] and theorem [ t : linearcov ] determines the asymptotic and convergence speeds of the algorithm .",
    "we discuss how @xmath0 , @xmath1 and @xmath3 influence the asymptotic and convergence speeds via analyzing the relationship between @xmath0 , @xmath1 , @xmath3 and @xmath362 .",
    "[ t : acspeed ] ( * asymptotic and convergence speed * ) . in godec",
    ", the asymptotical improvement and the linear convergence of @xmath0 and @xmath1 stated in theorem [ t : asymptpro ] and theorem [ t : linearcov ] will be slowed by augmenting @xmath363 however , the asymptotical improvement and the linear convergence will not be harmed and is robust to the noise @xmath3 unless when @xmath364 and @xmath365 , which lead the two terms increasing to @xmath71 .",
    "godec approximately decomposes a matrix @xmath2 into the low - rank part @xmath0 and the sparse part @xmath1 . according to the above analysis",
    ", godec is equivalent to alternating projections of @xmath0 on @xmath325 and @xmath326 , which are given in ( [ e : manifoldmn ] ) . according to theorem [ t : asymptpro ] and theorem [ t : linearcov ] ,",
    "smaller @xmath362 produces faster asymptotic and convergence speeds , while @xmath366 is possible to make @xmath0 and @xmath1 stopping converging .",
    "below we discuss how @xmath0 , @xmath1 and @xmath3 influence @xmath362 and further influence the asymptotic and convergence speeds of gedec .    according to ( [ e : tangentangle ] ) , we have @xmath367 substituting the equation given in proposition [ p : setcal ] into the right - hand side of the above equation yields @xmath368 the normal spaces of manifolds @xmath325 and @xmath326 on point @xmath351 is respectively given by @xmath369 where @xmath370 represents the eigenvalue decomposition of @xmath351 , @xmath371 $ ] and @xmath372 $ ] .",
    "assume @xmath373 , wherein @xmath374 is the noise corresponding to @xmath351 , we have @xmath375=\\overline l+\\delta.\\end{aligned}\\ ] ] thus the normal space of manifold @xmath326 is @xmath376 since the tangent space is the complement space of the normal space , by using the normal space of @xmath325 in ( [ e : normalmn ] ) and the normal space of @xmath326 given in ( [ e : normaln ] ) , we can verify @xmath377 by substituting the above results into ( [ e : cosmn ] ) , we obtain @xmath378 hence we have @xmath379 the last equivalence is due to @xmath380 in ( [ e : normalmn ] ) .",
    "thus @xmath381 where the diagonal entries of @xmath382 and @xmath383 are composed by eigenvalues of @xmath384 and @xmath173 , respectively .",
    "the last inequality is obtained by considering the case when @xmath170 and @xmath173 have identical left and right singular vectors . because @xmath385 infers @xmath386 , we have @xmath387 since @xmath355 in theorem [ t : linearcov ] can be selected as any constant that is strictly larger than @xmath388 , we can choose @xmath389 . in theorem [ t : asymptpro ]",
    ", the cosine @xmath390 is directly used .",
    "therefore , the asymptotic and convergence speeds of @xmath0 will be slowed by augmenting @xmath391 , and vice versa .",
    "however , the asymptotical improvement and the linear convergence will not be jeopardized unless @xmath392 . for general @xmath393 that is not normalized onto the sphere @xmath339",
    ", @xmath391 should be replaced by @xmath394 .    for the variable @xmath1 , we can obtain an analogous result via an analysis in a similar style as above . for general @xmath393 without normalization",
    ", the asymptotic / convergence speed of @xmath1 will be slowed by augmenting @xmath395 , and vice versa , wherein @xmath396 the asymptotical improvement and the linear convergence will not be jeopardized unless @xmath397 .",
    "this completes the proof .",
    "theorem [ t : acspeed ] reveals the influence of the low - rank part @xmath0 , the sparse part @xmath1 and the noise part @xmath3 to the asymptotic / convergence speeds of @xmath0 and @xmath1 in godec .",
    "both @xmath398 and @xmath399 are the element - wise hard thresholding error of @xmath400 and the singular value hard thresholding error of @xmath401 , respectively .",
    "large errors will slow the asymptotic and convergence speeds of godec . since @xmath402 and @xmath403 , the noise part @xmath3 in @xmath398 and @xmath399 can be interpreted as the perturbations to @xmath1 and @xmath0 and deviates the two errors from @xmath336 .",
    "thus noise @xmath3 with large magnitude will decelerate the asymptotical improvement and the linear convergence , but it will not ruin the convergence unless @xmath364 or @xmath365 . therefore , godec is robust to the additive noise in @xmath8 and is able to find the approximated @xmath9 decomposition when noise @xmath3 is not overwhelming .",
    "we analyze the error bounds of the brp based low - rank approximation ( [ e : lr_app ] ) and its power scheme modification ( [ e : mlr_app ] ) .      in low - rank approximation , the left random projection matrix @xmath42 is built from the left random projection @xmath33 , and then the right random projection matrix @xmath41 is built from the left random projection @xmath34 . thus @xmath415 and @xmath416 .",
    "hence the approximation error given in ( [ e : unblock ] ) has the following form : @xmath417\\right\\|.\\ ] ]    the following theorem [ t : deterministicbound ] gives the bound for the spectral norm of the deterministic error @xmath418 .",
    "[ t : deterministicbound ] * ( deterministic error bound ) * given an @xmath419 real matrix @xmath8 with singular value decomposition @xmath420 , and chosen a target rank @xmath421 and an @xmath422 ( @xmath423 ) standard gaussian matrix @xmath41 , the brp based low - rank approximation ( [ e : lr_app ] ) approximates @xmath8 with the error upper bounded by @xmath424    see section [ s : proof ] for the proof of theorem [ t : deterministicbound ] .",
    "if the singular values of @xmath8 decay fast , the first term in the deterministic error bound will be very small .",
    "the last term is the rank-@xmath29 svd approximation error .",
    "therefore , the brp based low - rank approximation ( [ e : lr_app ] ) is nearly optimal .",
    "[ t : deterministicboundpower ] * ( deterministic error bound , power scheme ) * frame the hypotheses of theorem [ t : deterministicbound ] , the power scheme modification ( [ e : mlr_app ] ) approximates @xmath8 with the error upper bounded by @xmath425    see section [ s : proof ] for the proof of theorem [ t : deterministicboundpower ] .    if the singular values of @xmath8 decay slowly , the error produced by the power scheme modification ( [ e : mlr_app ] ) is less than the brp based low - rank approximation ( [ e : lr_app ] ) and decreasing with the increasing of @xmath58 .    the average error bound of brp based low - rank approximation is obtained by analyzing the statistical properties of the random matrices that appear in the deterministic error bound in theorem [ t : deterministicbound ] .",
    "[ t : averageerrorbound ] * ( average error bound ) * frame the hypotheses of theorem [ t : deterministicbound ] , @xmath426    see section [ s : proof ] for the proof of theorem [ t : averageerrorbound ]",
    ".    the average error bound will approach to the svd approximation error @xmath427 if @xmath428 and @xmath429 .",
    "the average error bound for the power scheme modification is then obtained from the result of theorem [ t : averageerrorbound ] .",
    "[ t : averageerrorboundpower ] ( * average error bound , power scheme * ) frame the hypotheses of theorem [ t : deterministicbound ] , the power scheme modification ( [ e : mlr_app ] ) approximates @xmath8 with the expected error upper bounded by @xmath430^{1/(2q+1)}.\\end{aligned}\\ ] ]    see section [ s : proof ] for the proof of theorem [ t : averageerrorboundpower ] .    compared the average error bounds of the brp based low - rank approximation with its power scheme modification , the latter produces less error than the former , and the error can be further decreased by increasing @xmath58 .",
    "the deviation bound for the spectral norm of the approximation error can be obtained by analyzing the deviation bound of @xmath431 in the deterministic error bound and by applying the concentration inequality for lipschitz functions of a gaussian matrix .",
    "[ t : deviationerrorbound ] ( * deviation bound * ) frame the hypotheses of theorem [ t : deterministicbound ] .",
    "assume that @xmath432 . for all @xmath433 ,",
    "it holds that @xmath434 except with probability @xmath435 .",
    "see section [ s : proof ] for the proof of theorem [ t : deviationerrorbound ] .",
    "the following lemma and propositions from @xcite will be used in the proof .",
    "[ l : conjugate ] suppose that @xmath436 .",
    "for every @xmath437 , the matrix @xmath438 .",
    "in particular , @xmath439    [ p : range ] suppose @xmath440 .",
    "then , for each matrix @xmath437 , it holds that @xmath441 and that @xmath442 .",
    "[ p : inversepurtubation ] suppose that @xmath436 .",
    "then @xmath443    [ p : blocknorm ] we have @xmath444 for each partitioned positive semidefinite matrix @xmath445.\\ ] ]    the proof of theorem [ t : deterministicbound ] is given below .",
    "since an orthogonal projector projects a given matrix to the range ( column space ) of a matrix @xmath341 is defined as @xmath446 , the deterministic error ( [ e : newunblock ] ) can be written as @xmath447    by applying proposition [ p : range ] to the error ( [ e : errorproj ] ) , because @xmath448 , we have @xmath449 where @xmath450 ( v_1^ta_1)^\\dagger\\lambda_1^{-2}= \\left [    \\begin{array}{c }      i \\\\      h \\\\",
    "\\end{array } \\right ] .\\ ] ] thus @xmath451 can be written as @xmath452\\ ] ]    for the top - left block in ( [ e : isubpnblock ] ) , proposition [ p : inversepurtubation ] leads to @xmath453 .",
    "for the bottom - right block in ( [ e : isubpnblock ] ) , lemma [ l : conjugate ] leads to @xmath454 .",
    "therefore , @xmath455\\ ] ]    by applying lemma [ l : conjugate ] , we have @xmath456\\end{aligned}\\ ] ]    according to proposition [ p : blocknorm ] , the spectral norm of @xmath457 is bounded by @xmath458    by substituting ( [ e : rawbound ] ) into ( [ e : projnm ] ) , we obtain the deterministic error bound .",
    "this completes the proof .      the following proposition from @xcite",
    "will be used in the proof .",
    "[ p : powernorm ] let @xmath459 be an orthogonal projector , and let @xmath437 be a matrix . for each nonnegative @xmath58 , @xmath460    the proof of theorem [ t : deterministicboundpower ] is given below .",
    "the power scheme modification ( [ e : mlr_app ] ) applies the brp based low - rank approximation ( [ e : lr_app ] ) to @xmath461 rather than @xmath8 . in this case , the approximation error is @xmath462 according to theorem [ t : deterministicbound ] , the error is upper bounded by @xmath463 the deterministic error bound for the power scheme modification is obtained by applying proposition [ p : powernorm ] to ( [ e : unblockpower ] ) .",
    "this completes the proof .",
    "the following propositions from @xcite will be used in the proof .",
    "[ p : sgt ] fix matrices @xmath1 , @xmath464 , and draw a standard gaussian matrix @xmath3 . then it holds that @xmath465    [ p : pesudoinvgaussian ] draw an @xmath466 standard gaussian matrix @xmath3 with @xmath467",
    ". then it holds that @xmath468    the proof of theorem [ t : averageerrorbound ] is given below .",
    "the distribution of a standard gaussian matrix is rotational invariant . since 1 ) @xmath41 is a standard gaussian matrix and 2 ) @xmath69 is an orthogonal matrix , @xmath469 is a standard gaussian matrix , and its disjoint submatrices @xmath470 and @xmath471 are standard gaussian matrices as well .",
    "theorem [ t : deterministicbound ] and the hlder s inequality imply that @xmath472 we condition on @xmath470 and apply proposition [ p : sgt ] to bound the expectation w.r.t .",
    "@xmath471 , i.e. , @xmath473 the frobenius norm of @xmath474 can be calculated as @xmath475\\\\ \\notag&={\\rm trace}\\left[\\left(\\left(\\lambda_1v_1^ta_1\\right)\\left(\\lambda_1v_1^ta_1\\right)^t\\right)^{-1}\\right].\\end{aligned}\\ ] ] since 1 ) @xmath470 is a standard gaussian matrix and 2 ) @xmath405 is a diagonal matrix , each column of @xmath476 follows @xmath29-variate gaussian distribution @xmath477 .",
    "thus the random matrix @xmath478 follows the inverted wishart distribution @xmath479 .",
    "according to the expectation of inverted wishart distribution @xcite , we have @xmath480\\\\ \\notag & = { \\rm trace}~\\mathbb e\\left[\\left(\\left(\\lambda_1v_1^ta_1\\right)\\left(\\lambda_1v_1^ta_1\\right)^t\\right)^{-1}\\right]\\\\ & = \\frac{1}{p-1}\\sum\\limits_{i=1}^r\\lambda_i^{-2}.\\end{aligned}\\ ] ] we apply proposition [ p : pesudoinvgaussian ] to the standard gaussian matrix @xmath470 and obtain @xmath481 therefore , ( [ e : exaa ] ) can be further derived as @xmath482 by substituting ( [ e : exaafinal ] ) into ( [ e : exsubl ] ) , we obtain the average error bound @xmath483 this completes the proof .",
    "the proof of theorem [ t : averageerrorboundpower ] is given below .    by using hlder s inequality and theorem [ t : deterministicboundpower ] ,",
    "we have @xmath484 we apply theorem [ t : averageerrorbound ] to @xmath50 and @xmath485 and obtain the bound of @xmath486 , noting that @xmath487 . @xmath488 by substituting ( [ e : tildexsubl ] ) into ( [ e : powerxsubl ] ) , we obtain the average error bound of the power scheme modification shown in theorem [ t : averageerrorboundpower ] .",
    "this completes the proof .",
    "the following propositions from @xcite will be used in the proof .",
    "[ p : lipschitzconcentration ] suppose that @xmath489 is a lipschitz function on matrices : @xmath490 draw a standard gaussian matrix @xmath3",
    ". then @xmath491    [ p : gaussiannormdeviation ] let @xmath3 be a @xmath466 standard gaussian matrix where @xmath432 . for all @xmath492 , @xmath493    the proof of theorem [ t : deviationerrorbound ] is given below .",
    "according to the deterministic error bound in theorem [ t : deterministicbound ] , we study the deviation of @xmath494 .",
    "consider the lipschitz function @xmath495 , its lipschitz constant @xmath0 can be estimated by using the triangle inequality : @xmath496 hence the lipschitz constant satisfies @xmath497 .",
    "we condition on @xmath470 and then proposition [ p : sgt ] implies that @xmath498\\leq&\\left\\|\\lambda_2 ^ 2\\right\\|\\left\\|\\left(v_1^ta_1\\right)^\\dagger\\right\\|_f\\left\\|\\lambda_1^{-1}\\right\\|_f+\\\\ \\notag&\\left\\|\\lambda_2 ^ 2\\right\\|_f\\left\\|\\left(v_1^ta_1\\right)^\\dagger\\right\\|\\left\\|\\lambda_1^{-1}\\right\\|.\\end{aligned}\\ ] ] we define an event @xmath464 as @xmath499 according to proposition [ p : gaussiannormdeviation ] , the event @xmath464 happens except with probability @xmath500 applying proposition [ p : lipschitzconcentration ] to the function @xmath501 , given the event @xmath464 , we have @xmath502 according to the definition of the event @xmath464 and the probability of @xmath503 , we obtain @xmath504 therefore , @xmath505 since theorem [ t : deterministicbound ] implies @xmath506 , we obtain the deviation bound in theorem [ t : deviationerrorbound ] .",
    "this completes the proof .",
    "it is not direct to analyze the theoretical guarantee of greb due to its combination of alternating minimization and greedy forward selection .",
    "hence , we consider analyzing its convergence behavior by leveraging the results from geco @xcite analysis .",
    "this is reasonable because they share the same objective function yet different optimization variables . in particular",
    ", the risk function in geco is @xmath507 , where @xmath508 .",
    "it can be seen that the variable @xmath437 in geco is able to be written as @xmath509 without any loss of generality .",
    "therefore , for the same selection of @xmath510 , we can compare the objective value of geco and greb at arbitrary step of their algorithm .",
    "this results in the following theorem .",
    "assume @xmath510 is a @xmath171-smooth function according to geco @xcite and @xmath511 , and @xmath512 is the objective function of greb . given a rank constraint @xmath29 to @xmath437 and a tolerance parameter @xmath513 .",
    "let @xmath514 is the solution of greb .",
    "then for all matrices @xmath509 with @xmath515 we have @xmath516 .    according to lemma 3 in geco @xcite ,",
    "let @xmath517 , where @xmath518 is the value of @xmath79 at the beginning of iteration @xmath143 and @xmath519 fulfills @xmath520 , we have @xmath521 at the end of iteration @xmath143 , the objective value of greb equals @xmath522 , while geco optimizes @xmath79 over the support of @xmath523 ( i.e. , optimizes @xmath1 when fixing @xmath68 and @xmath69 ) .",
    "we use the same notation @xmath524 to denote the variable in iteration @xmath143 .",
    "this yields @xmath525 at the beginning of iteration @xmath526 , both geco and greb computes the direction @xmath527 along which the object declines fastest",
    ". however , geco adds both @xmath528 and @xmath529 to the ranges of @xmath68 and @xmath69 , while greb only adds @xmath529 to @xmath69 and then optimizes @xmath68 when fixing @xmath69 . because the range of @xmath68 in greb is optimized rather than previously fixed , we have @xmath530)\\leq\\\\ & \\min\\limits_\\eta f(\\lambda^{(i)}+\\eta e^{u , v } ) .",
    "\\end{array}\\ ] ] plug ( [ equ : ith ] ) and ( [ equ : i1th ] ) into ( [ equ : gecoiequ ] ) , we gain a similar result : @xmath531)\\geq \\frac{\\epsilon_i^2(1-\\tau)^2}{2\\beta\\|a\\|_{tr}^2}.\\ ] ] following the analysis after lemma 3 in geco @xcite , we can immediately obtain the results of the theorem .",
    "the theorem states that greb solution is at least close to optimum as geco .",
    "note when sparse @xmath1 is alternatively optimized with @xmath67 in greb scheme , such as grebcom , the theorem can still holds .",
    "this is because after optimizing @xmath1 in each iteration of grebcom , we have @xmath532 , which enforces the objective function @xmath533 degenerates to that of geco , which is @xmath534 ."
  ],
  "abstract_text": [
    "<S> learning big data by matrix decomposition always suffers from expensive computation , mixing of complicated structures and noise . in this paper , we study more adaptive models and efficient algorithms that decompose a data matrix as the sum of semantic components with incoherent structures . </S>",
    "<S> we firstly introduce `` go decomposition ( godec ) '' , an alternating projection method estimating the low - rank part @xmath0 and the sparse part @xmath1 from data matrix @xmath2 corrupted by noise @xmath3 . </S>",
    "<S> two acceleration strategies are proposed to obtain scalable unmixing algorithm on big data : 1 ) bilateral random projection ( brp ) is developed to speed up the update of @xmath0 in godec by a closed - form built from left and right random projections of @xmath4 in lower dimensions ; 2 ) greedy bilateral ( greb ) paradigm updates the left and right factors of @xmath0 in a mutually adaptive and greedy incremental manner , and achieve significant improvement in both time and sample complexities . </S>",
    "<S> then we proposes three nontrivial variants of godec that generalizes godec to more general data type and whose fast algorithms can be derived from the two strategies : 1 ) for motion segmentation , we further decompose the sparse @xmath1 ( moving objects ) as the sum of multiple row - sparse matrices , each of which is a low - rank matrix after specific geometric transformation sequence and defines a motion shared by multiple objects ; 2 ) for multi - label learning , we further decompose the low - rank @xmath0 into subcomponents with separable subspaces , each corresponds to the mapping a single label in feature space . </S>",
    "<S> then the prediction can be effectively conducted by group lasso on the subspace ensemble ; 3 ) for estimating scoring functions of each user in recommendation system , we further decompose the low - rank @xmath0 as @xmath5 , where the rows of @xmath6 is the linear scoring functions and the rows of @xmath7 are the items represented by available features . </S>",
    "<S> empirical studies show the efficiency , robustness and effectiveness of the proposed methods in real applications .    </S>",
    "<S> low - rank and sparse matrix decomposition , bilateral random projection , greedy bilateral paradigm , multi - label learning , background modeling , motion segmentation , recommendation systems </S>"
  ]
}