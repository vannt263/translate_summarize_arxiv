{
  "article_text": [
    "random forest ( rf ) is a popular , powerful ensemble machine learning method proposed by @xcite .",
    "although the canonical version of this algorithm is known to be very versatile and perform well in numerous applications , many variants of this method have been proposed , for many different purposes : to extend the rf capabilities , to generalise over specific , non - standard data , to increase accuracy in a certain conditions , to improve the attribute importance measure produced by this method , to speed - up training or prediction , just to name a few .",
    "this work aims to provide a conceptual framework of generalised random forest ( grf ) methods , useful both in classification of existing rf variants and defining research opportunities in this field .",
    "in the presented model of the generalised random forest , we assume a following three - layer nested ensemble structure , as shown on figure  [ fig : gengrf ] .",
    "the data is ingested into the model via numerous _ pivot models _ , which have to fit the form of the data and",
    "are expected to be very simple and easy to train , although are not necessary have to be neither accurate nor robust ; their intended role is to become an interface between the input and internal logic of the grf model . in other words",
    ", we assume that they handle the feature construction step of the modelling process .",
    "pivot models are grouped are converted into a meaningful ensemble models with the _ sharpening ensembles _ ; this modules of this class orchestrate the generation of pivots by using the information from the decision attribute .",
    "the sharpening ensemble is expected to produce accurate models regardless of their robustness ; however , they must not require external optimisation of any kind and should be computationally efficient .",
    "the outermost layer of the grf is the _ conditioning ensemble _ which builds and groups sharpening ensembles .",
    "its role is to finally build a robust model by joining a lot of accurate models for which one does not know which are overfitted and which are not .",
    "it is trivial to show that this can be achieved even with a very small fraction of meaningful models , provided that sharpening ensembles will not be correlated ; to this end the conditioning ensemble usually involves some permutation strategy .    moreover , conditioning ensemble can be expanded to provide a generalisation of some of the additional features of random forest , like internal error approximation , object - object dissimilarity measure or feature importance .",
    "it is also a good place to implement parallel computing capabilities .",
    "structure of a generalised random forest .",
    "a )  pivots , b )  sharpening ensemble , c )  conditioning ensemble .",
    "x and y denote , respectively , predictor and decision part of the information system . ]",
    "pivot models are the only part of the grf structure that has a direct contact with predictors of an information system , thus it is a place for modifying the interface with the data structure .",
    "the form of an output of a pivot classifier and the algorithm which is used to train it is naturally enforced by a form of the sharpening ensemble ; most often pivot forms an embranchment in some kind of a decision tree , thus shall return a direction in which certain object should descent within the tree . as the trees are most often binary , this gives us two options , which we will later in the paper call @xmath0 ( for right ) and @xmath1 ( for left ) .    in a standard random forest @xcite , it is assumed that an information system is composed of either categorical or continuous , can be treated as continuous without any loss of generality . ] predictors .",
    "this way , we have only two possible pivot classifiers , respectively for categorical and continuous feature , in a following forms : @xmath2 where @xmath3 is a set of categories of @xmath4 , and @xmath5 in either case , @xmath4 is a feature on which the pivot is executed , and @xmath6 is the _ threshold _ value . both are selected when pivot is generated within the sharpening ensemble ; most often they are optimised to maximise the decision homogeneity within the subsets of objects sent left and right , usually measured by information gain or gini index .",
    "the other popular idea , started by @xcite with their extra - trees method , is to generate pivots at random , obviously with constraint that a resulting criterion should not sent all objects in one direction .",
    "such a method removes a problem of finding an appropriate homogeneity measure and performing the optimisation , which in return increases the computational efficiency and , in a number of problems , the robustness of a final model as it leads to a greater divergence among sharpening ensemble models .",
    "naturally , this way one also removes an impact of a possible problems with the homogeneity measure , which may surface for instance in case of unbalanced sets .",
    "still , there are problems in the probability of finding even a slightly meaningful pivot is very small , and they usually case random pivot - based algorithm to perform poorly .    and in - between solution is to use some kind of heuristic instead of a full optimisation ; a simple example of this technique is to generate some number of random pivots and select the best one .",
    "other approaches include generic soft optimisation methods like genetic algorithms , randomly reducing the search space ( often the set of used predictors , ad done by the standard random forest ) or disturbing the homogeneity measure .",
    "@xcite proposed to employ pca in pivot generation .",
    "the other way of generalising the pivot models is to modify their structure to accommodate information systems going beyond simple sets of categorical and continuous predictors .",
    "@xcite perform image classification with grf with pivots calculating two vector image descriptors , summing their valus with random weights and comparing with a random threshold .",
    "@xcite proposed to employ some kernel function used for svm and build pivot by partial application of this kernel to a randomly selected object and compare the result with a threshold optimised in terms of information gain .",
    "this approach was later used for sound and temporal gene expression patterns @xcite .",
    "examples of sharpening ensemble structures .",
    "embranchment pivot based : a )  decision tree , b )  fern , c )  null ensemble ; generic : d )  boosting , e )  decision trunk . ]",
    "the selection of a sharpening module is more complex , since it gives a lot of space for different solutions and approaches .",
    "some of a popular options , illustrated in figure  [ fig : varse ] , are :    * * decision tree * is a sharpening module used by the random forest method ; it uses simple embranchment pivots stacked in an iterative manner , i.e. after applying a pivot a sub - tree is build separately for each of the branches , until the sets of objects in leaves will become homogeneous or some pre - set maximum tree depth will be achieved .",
    "training of pivots within a decision tree may be performed both randomly or via optimisation ; the leaves of a tree will likely end up homogeneous either way . to this end , the output of a tree sharpening module is a direct prediction of class . * * decision fern * @xcite is basically a form of a full decision tree in which each pivot module at a given depth is identical . to this end ,",
    "evaluation of a given pivot is not dependent on the others and can happen in any order , which makes a fern more computationally effective than a decision tree .",
    "+ on the other hand , this makes optimisation of a fern is highly non - obvious , thus individual pivots are usually generated randomly . this way",
    "the leaves of a fern are often non - homogeneous , and thus the prediction of a fern is often encoded as a vector of class probabilities , which naturally requires appropriate adaptation of the voting scheme present in the conditioning ensemble . * * decision trunk * , proposed by @xcite , is composed of a flat series of pivot models similar to a fern , though requires the decision to be binary ( say @xmath7 or @xmath8 ) , and employs pivot modules ( _ segments _ ) which classify into three groups , @xmath7 , @xmath8 meaning that it is certain that an object belongs to a respective class , and @xmath9 meaning that the decision is relayed to a next pivot classifier within the trunk .",
    "obviously , different than in case of decision ferns , the order of trunk segments is significant because the consideration at level @xmath10 comes in a context that the object was claimed undecided by segments @xmath11 .",
    "trunks practically can not be implemented without optimisation of pivot models and thus they provide sharp predictions similar to decision trees .",
    "+ one should note that ternary pivot models can be realised by combining a pair of regular entrancement pivots , only modified to optimise homogeneity in one branch , respectively for class @xmath7 and @xmath8 ; @xmath9 is then given to objects directed to second branch in both pivots and for those for which prediction of both pivots are in conflict .",
    "* although * boosting * @xcite is almost always considered as a stand - alone ensemble , wrapping it in another layer may lead to a better resilience to noise and mislabelled objects . moreover",
    ", boosting has a good support for regression problems , making it a promising alternative to regression trees in context of some of their known problems in this set - up .",
    "* there is also a degenerate option which we will call here * null ensemble * , i.e. just using a single pivot classifier .",
    "this solution can be effective in case when pivot classifiers are very good on their own and applying a more complex sharpening will only result in an increased computational load .",
    "a litmus test for such a situation is when a more complex sharpening ensemble of a dynamic complexity , such as a decision tree , is creating shallow models composed of a small number of pivots .",
    "moreover , sometimes the procedure of building the sharpening ensemble is modified to support the outer conditioning ensemble in de - correlation of individual sharpening ensembles .",
    "for instance , in the canonical random forest each pivot is build on a randomly generated subset of attributes ; this approach is very generic and can be easily ported over other sharpeners , yet obviously a number of alternative methods can be applied as well .",
    "as mentioned earlier , the role of the conditioning ensemble is to justify a robust prediction from a set of sharpening ensembles of an unknown reliability . to this end",
    ", this module has to either somehow assess the member models or ensure independence of members so that the noise generated by the overfitted ones will average - out during voting .",
    "the first approach is yet rarely used , mostly because having a reliable enough method of assessing robustness would in practice mean that employing the whole grf structure is redundant .",
    "the second approach is mostly , as in the canonical random forest algorithm , realised through _ bootstrap aggregation _ ( _ bagging _ ) , also proposed by @xcite , or some variation of this method .",
    "precisely , the procedure generates a number of object sub - samples for each of the sharpening ensemble ; this way they are presented with differently stressed incarnations of the training data and thus exploring a wider range of aspects and becoming less correlated .",
    "similar approach can be applied to features or implemented as weighting instead of strict selection .",
    "the other substantial option is to simply use a very variable sharpener that will generate diverse models on its own .",
    "by re - imagining random forest as a three - level nested ensemble , we propose a generic , modular framework for extending and modifying this method .",
    "anna bosch , andrew zisserman , and xavier munoz .",
    "image classification using random forests and ferns . in _",
    "2007 ieee 11th international conference on computer vision _ , pages 18 .",
    "ieee , 2007 .",
    "isbn 978 - 1 - 4244 - 1630 - 1 .",
    "url http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4409066 .",
    "jiguo cao and guangzhe fan .",
    "signal classification using random forest with kernels .",
    "_ 2010 sixth advanced international conference on telecommunications _ , pages 191195 , 2010 .",
    "doi : 10.1109/aict.2010.81 .",
    "url http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5489853 .",
    "guangzhe fan .",
    "kernel - induced classification trees and random forests , 2009 .",
    "url http://math.uwaterloo.ca/statistics-and-actuarial-science/sites/ca.statistics-and-actuarial-science/files/uploads/files/2009-06.pdf .",
    "guangzhe fan , jiguo cao , and jiheng wang .",
    "functional data classification for temporal gene expression data with kernel - induced random forests . in _",
    "2010 ieee symposium on computational intelligence in bioinformatics and computational biology _ , volume  1 , pages 15 .",
    "ieee , may 2010 .",
    "isbn 978 - 1 - 4244 - 6766 - 2 .",
    "url http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5510482 .",
    "pierre geurts , damien ernst , and louis wehenkel .",
    "extremely randomized trees . _ machine learning _",
    ", 630 ( 1):0 342 , march 2006 .",
    "issn 0885 - 6125 .",
    "doi : 10.1007/s10994 - 006 - 6226 - 1 .",
    "url http://www.springerlink.com/index/10.1007/s10994-006-6226-1 .",
    "miron  bartosz kursa . : an implementation of the random ferns method for general - purpose machine learning .",
    "_ journal of statistical software _ , 610 ( 10):0 113 , 2014 .",
    "url http://www.jstatsoft.org/v61/i10/paper .",
    "juan  j rodrguez , ludmila  i kuncheva , and carlos  j alonso .",
    "rotation forest : a new classifier ensemble method .",
    "_ ieee transactions on pattern analysis and machine intelligence _ , 280 ( 10):0 161930 , october 2006",
    "issn 0162 - 8828 .",
    "doi : 10.1109/tpami.2006.211 .",
    "url http://www.ncbi.nlm.nih.gov/pubmed/16986543 .",
    "benjamin ulfenborg , karin klinga - levan , and bjrn olsson .",
    "classification of tumor samples from expression data using decision trunks .",
    "_ cancer informatics _ , 12:0 5366 , 2013 .",
    "issn 1176 - 9351 .",
    "doi : 10.4137/cin.s10356 ."
  ],
  "abstract_text": [
    "<S> assuming a view of the random forest as a special case of a nested ensemble of interchangeable modules , we construct a generalisation space allowing one to easily develop novel methods based on this algorithm . </S>",
    "<S> we discuss the role and required properties of modules at each level , especially in context of some already proposed rf generalisations . </S>"
  ]
}