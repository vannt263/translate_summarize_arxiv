{
  "article_text": [
    "a divergence @xmath1 is a smooth distortion measure that quantifies the dissimilarity between any two data points @xmath2 and @xmath3 ( with @xmath4 iff .",
    "@xmath5 ) . a statistical divergence is a divergence between probability ( or positive ) measures .",
    "there is a rich literature of divergences  @xcite that have been proposed and used depending on their axiomatic characterizations or their empirical performance .",
    "one motivation to design new divergence families , like the proposed total jensen divergences in this work , is to elicit some statistical robustness property that allows to bypass the use of costly @xmath6-estimators  @xcite .",
    "we first recall the main geometric constructions of divergences from the graph plot of a convex function .",
    "that is , we concisely review the jensen divergences , the bregman divergences and the total bregman divergences induced from the graph plot of a convex generator .",
    "we dub those divergences : _ geometrically designed divergences _ ( not to be confused with the geometric divergence  @xcite in information geometry ) .      for a strictly convex and differentiable function @xmath7 , called the _ generator _ , we define a family of parameterized distortion measures by :    @xmath8    where @xmath9 and @xmath10 .",
    "the skew jensen divergence is depicted graphically by a jensen convexity gap , as illustrated in figure  [ fig : gd ] and in figure  [ fig : sj ] by the plot of the generator function .",
    "the skew jensen divergences are _ asymmetric _ ( when @xmath11 ) and does _ not _ satisfy the triangular inequality of metrics . for @xmath12 ,",
    "we get a symmetric divergence @xmath13 , also called burbea - rao divergence  @xcite .",
    "it follows from the strict convexity of the generator that @xmath14 with equality if and only if @xmath5 ( a property termed the _ identity of indiscernibles _ ) . the skew jensen divergences _ may not _ be convex divergences . for all @xmath15 .",
    "see  @xcite for further details . ]",
    "note that the generator may be defined up to a constant @xmath16 , and that @xmath17 for @xmath18 . by rescaling those divergences by a fixed factor @xmath19",
    ", we obtain a continuous @xmath20-parameter family of divergences , called the _",
    "@xmath21-skew jensen divergences _ , defined over the _",
    "full _ real line @xmath22 as follows  @xcite :    @xmath23    where @xmath24 denotes underscript in all the @xmath25 and @xmath26 divergence notations . ]",
    "the bregman divergence  @xcite :    @xmath27    with @xmath28 denoting the inner product ( _ e.g. _ scalar product for vectors ) .",
    "figure  [ fig : gd ] shows graphically the bregman divergence as the ordinal distance between @xmath29 and the first - order taylor expansion of @xmath7 at @xmath3 evaluated at @xmath2 .",
    "indeed , the limit cases of jensen divergences @xmath30 when @xmath31 or @xmath32 tend to a bregman divergence  @xcite : @xmath33    the skew jensen divergences are related to _ statistical divergences _ between probability distributions :    the statistical skew bhattacharrya divergence  @xcite : @xmath34 between parameterized distributions @xmath35 and @xmath36 belonging to the _ same _ exponential family amounts to compute equivalently a skew jensen divergence on the corresponding natural parameters for the log - normalized function @xmath7 : @xmath37 .",
    "@xmath38 is the counting measure for discrete distributions and the lebesgue measure for continous distributions .",
    "see  @xcite for details .",
    "when considering a family of divergences parameterized by a smooth convex generator @xmath7 , observe that the convexity of the function @xmath7 is preserved by an affine transformation . indeed",
    ", let @xmath39 , then @xmath40 since @xmath41 .     of a convex generator @xmath7 : the symmetric jensen ( j ) divergence @xmath42 is interpreted as the vertical gap between the point @xmath43 of @xmath44 and the interpolated point @xmath45 . the asymmetric bregman divergence ( b )",
    "is interpreted as the ordinal difference between @xmath29 and the linear approximation of @xmath7 at @xmath3 ( first - order taylor expansion ) evaluated at @xmath2 .",
    "the total bregman ( tb ) divergence projects orthogonally @xmath46 onto the tangent hyperplane of @xmath44 at @xmath3 .",
    "[ fig : gd],scaledwidth=70.0% ]      let us consider an application in medical imaging to motivate the need for a particular kind of invariance when defining divergences : in diffusion tensor magnetic resonance imaging ( dt - mri ) , 3d raw data are captured at voxel positions as 3d ellipsoids denoting the water propagation characteristics  @xcite . to perform common signal processing tasks like denoising , interpolation or segmentation tasks",
    ", one needs to define a proper _ dissimilarity measure _ between any two such ellipsoids .",
    "those ellipsoids are mathematically handled as _",
    "symmetric positive definite _ ( spd ) matrices  @xcite that can also be interpreted as 3d gaussian probability distributions .- divergences , including the prominent kullback - leibler divergence , remain unchanged after applying an invertible transformation on the parameter space of the distributions , see appendix  [ app : invariance ] for such an illustrating example . ] in order not to be biased by the chosen coordinate system for defining those ellipsoids , we ask for a divergence that is invariant to rotations of the coordinate system . for a divergence parameterized by a generator function @xmath7 derived from the graph of that generator , the invariance under rotations means that the geometric quantity defining the divergence should not change if the original coordinate system is rotated .",
    "this is clearly not the case for the skew jensen divergences that rely on the vertical axis to measure the ordinal distance , as illustrated in figure  [ fig : sj ] . to cope with this drawback , the family of _ total bregman divergences _",
    "( tb ) have been introduced and shown to be statistically robust  @xcite .",
    "note that although the traditional kullback - leibler divergence ( or its symmetrizations like the jensen - shannon divergence or the jeffreys divergence  @xcite ) between two multivariate gaussians could have been used to provide the desired invariance ( see appendix  [ app : invariance ] ) , the processing tasks are not robust to outliers and perform less well in practice  @xcite .",
    "the _ total bregman divergence _ amounts to compute a scaled bregman divergence : namely a bregman divergence multiplied by a _",
    "conformal factor _ : @xmath47 . by analogy",
    ", we shall call a conformal divergence @xmath48 , a divergence @xmath1 that is multiplied by a conformal factor : @xmath49 . ]",
    "@xmath50 :    @xmath51    figure  [ fig : gd ] illustrates the total bregman divergence as the orthogonal projection of point @xmath46 onto the tangent line of @xmath44 at @xmath3 .    for example , choosing the generator @xmath52 with @xmath53 , we get the _ total square euclidean distance _ :    @xmath54    that is , @xmath55 and @xmath56 .    total bregman divergences  @xcite have proven successful in many applications : diffusion tensor imaging  @xcite ( dti ) , shape retrieval  @xcite , boosting  @xcite , multiple object tracking  @xcite , tensor - based graph matching  @xcite , just to name a few .",
    "the total bregman divergences can be defined over the space of symmetric positive definite ( spd ) matrices met in dt - mri  @xcite .",
    "one key feature of the total bregman divergence defined over such matrices is its invariance under the special linear group  @xcite @xmath57 that consists of @xmath58 matrices of unit determinant :    @xmath59    see also appendix  [ app : invariance ] .",
    "the paper is organized as follows : section  [ sec : tj ] introduces the novel class of total jensen divergences ( tj ) and present some of its properties and relationships with the total bregman divergences .",
    "in particular , we show that although the square root of the jensen - shannon is a metric , it is not true anymore for the square root of the total jensen - shannon divergence .",
    "section  [ sec : centroid ] defines the centroid with respect to total jensen divergences , and proposes a doubly iterative algorithmic scheme to approximate those centroids .",
    "the notion of robustness of centroids is studied via the framework of the influence function of an outlier .",
    "section  [ sec : cluster ] extends the @xmath0-means++ seeding to total jensen divergences , yielding a fast probabilistically guaranteed clustering algorithm that do not require to compute explicitly those centroids .",
    "finally , section  [ sec : concl ] summarizes the contributions and concludes this paper .",
    "recall that the skew jensen divergence @xmath60 is defined as the `` vertical '' distance between the interpolated point @xmath61 lying on the line segment @xmath62 $ ] and the point @xmath63 lying on the graph of the generator .",
    "this measure is therefore _",
    "dependent _ on the coordinate system chosen for representing the sample space @xmath64 since the notion of `` verticality '' depends on the coordinate system . to overcome this limitation",
    ", we define the _ total _ jensen divergence by choosing the _ unique orthogonal projection _ of @xmath63 onto the line @xmath65)$ ] .",
    "figure  [ fig : sj ] depicts graphically the total jensen divergence , and shows its invariance under a rotation .",
    "figure  [ fig : projoutside ] illustrates the fact that the orthogonal projection of point @xmath63 onto the line @xmath66 may fall outside the segment @xmath62 $ ] .",
    "divergence is defined as the unique orthogonal projection of point @xmath67 onto the line passing through @xmath46 and @xmath68 .",
    "it is invariant by construction to a rotation of the coordinate system .",
    "( observe that the bold line segments have same length but the dotted line segments have different lengths . ) except for the paraboloid @xmath52 , the divergence is not invariant to translations .",
    "[ fig : sj ] ]    we report below a geometric proof ( an alternative purely analytic proof is also given in the appendix  [ app : analytic ] ) .",
    "let us plot the epigraph of function @xmath7 restricted to the vertical plane passing through distinct points @xmath2 and @xmath3 .",
    "let @xmath69 and @xmath70 ( for @xmath71 ) .",
    "consider the two right - angle triangles @xmath72 and @xmath73 depicted in figure  [ fig : gproof ] .",
    "since jensen divergence @xmath26 and @xmath74 are vertical line segments intersecting the line passing through point @xmath46 and point @xmath68 , we deduce that the angles @xmath75 are the same .",
    "thus it follows that angles @xmath76 are also identical .",
    "now , the cosine of angle @xmath76 measures the ratio of the adjacent side over the hypotenuse of right - angle triangle @xmath77 .",
    "therefore it follows that :    @xmath78    where @xmath79 denotes the @xmath80-norm . in right - triangle @xmath81 , we furthermore deduce that :    @xmath82    scaling by factor @xmath19 , we end up with the following lemma :    the total jensen divergence @xmath83 is invariant to rotations of the coordinate system .",
    "the divergence is mathematically expressed as a scaled skew jensen divergence @xmath84 , where @xmath85 is symmetric and independent of the skew factor .",
    "observe that the scaling factor @xmath86 is _ independent _ of @xmath21 , symmetric , and is always less or equal to @xmath20 .",
    "furthermore , observe that the scaling factor depending on both @xmath2 and @xmath3 and is _ not separable _ :",
    "that is , @xmath87 can not be expressed as a product of two terms , one depending only on @xmath2 and the other depending only on @xmath3 .",
    "that is , @xmath88 .",
    "we have @xmath89 . because the conformal factor is independent of @xmath21 , we have the following asymmetric ratio equality :    @xmath90    by rewriting ,    @xmath91    we intepret the non - separable _ conformal factor _ as a function of the square of the _ chord slope _ @xmath92 .",
    "table  [ tab : tj ] reports the conformal factors for the total bregman and total jensen divergences .     onto the line @xmath66 at position @xmath93 may fall outside the line segment @xmath62 $ ] .",
    "left : the case where @xmath94 .",
    "right : the case where @xmath95 .",
    "( using the convention @xmath96 . )",
    "[ fig : projoutside],scaledwidth=70.0% ]    @xmath97    @xmath98 & x\\log x+(1-x)\\log ( 1-x ) &   \\sqrt{\\frac{1}{1+\\log^2 \\frac{q}{1-q } } } &   \\sqrt{\\frac{1}{1+\\left(\\frac{p\\log p+(1-p)\\log p -q\\log q-(1-q)\\log(1-q)}{p - q}\\right)^2 } } \\\\ \\text{squared mahalanobis } & \\mathbb{r}^d & \\frac{1}{2 } x^\\top q x & \\sqrt{\\frac{1}{1+\\|q q\\|_2 ^ 2 } } &     \\sqrt{\\frac{1}{1+\\left(\\frac{p q^\\top p - q q^\\top q}{\\|p - q\\|_2}\\right)^2}},\\quad   q\\succ 0\\\\ \\hline \\end{array}\\ ] ]    [ rq : differenttj ] we could have defined the total jensen divergence in a different way by fixing the point on the line segment @xmath99 $ ] and seeking the point @xmath100 on the function plot that ensures that the line segments @xmath62 $ ] and @xmath101 $ ] are orthogonal .",
    "however , this yields a mathematical condition that may not always have a closed - form solution for solving for @xmath102 .",
    "this explains why the other way around ( fixing the point on the function plot and seeking the point on the line @xmath62 $ ] that yields orthogonality ) is a better choice since we have a simple closed - form expression .",
    "see appendix  [ app : secondtotaldiv ] .    from a scalar divergence",
    "@xmath103 induced by a generator @xmath104 , we can always build a _ separable divergence _ in @xmath105 dimensions by adding coordinate - wise the axis scalar divergences : @xmath106 this divergence corresponds to the induced divergence obtained by considering the multivariate separable generator : @xmath107    the jensen - shannon divergence  @xcite is a separable jensen divergence for the shannon information generator @xmath108 :    @xmath109    although the jensen - shannon divergence is symmetric it is not a metric since it fails the triangular inequality .",
    "however , its square root @xmath110 is a metric  @xcite .",
    "one may ask whether the conformal factor @xmath111 destroys or preserves the metric property :    the square root of the total jensen - shannon divergence is not a metric .",
    "it suffices to report a counterexample as follows : consider the three points of the @xmath20-probability simplex @xmath112 , @xmath113 and @xmath114 .",
    "we have @xmath115 , @xmath116 and @xmath117 .",
    "the triangular inequality fails because @xmath118 .",
    "the triangular inequality deficiency is @xmath119 .",
    "although the underlying rationale for deriving the total jensen divergences followed the same principle of the total bregman divergences ( _ i.e. _ , replacing the `` vertical '' projection by an orthogonal projection ) , the total jensen divergences _ do not coincide _ with the total bregman divergences in limit cases : indeed , in the limit cases @xmath120 , we have :    @xmath121    since @xmath122 .",
    "thus when @xmath123 , the total jensen divergence _ does not tend _ in limit cases to the total bregman divergence .",
    "however , by using a taylor expansion with exact lagrange remainder , we write :    @xmath124    with @xmath125 $ ] ( assuming wlog .",
    "@xmath126 ) .",
    "that is , @xmath127 .",
    "thus we have the squared slope index : @xmath128    therefore when @xmath129 , we have @xmath130 , and the total jensen divergence tends to the total bregman divergence for any value of @xmath21 . indeed , in that case , the bregman / jensen conformal factors match : @xmath131 for @xmath125 $ ] .",
    "note that for univariate generators , we find explicitly the value of @xmath132 : @xmath133 where @xmath134 is the legendre convex conjugate , see  @xcite . we recognize the expression of a stolarsky mean  @xcite of @xmath2 and @xmath3 for the strictly monotonous function @xmath135 .",
    "therefore when @xmath136 , we have @xmath137 and the total jensen divergence converges to the total bregman divergence .    the total skew jensen divergence @xmath138 can be equivalently rewritten as @xmath139 for @xmath140 $ ] . in particular , when @xmath129 , the total jensen divergences tend to the total bregman divergences for any @xmath21 . for @xmath120 ,",
    "the total jensen divergences is not equivalent to the total bregman divergence when @xmath141 .",
    "let the chord slope @xmath142 for @xmath143 $ ] ( by the mean value theorem ) .",
    "it follows that @xmath144 $ ] .",
    "note that since @xmath7 is strictly convex , @xmath135 is strictly increasing and we can approximate arbitrarily finely using a simple 1d dichotomic search the value of @xmath132 : @xmath145 with @xmath146 $ ] and @xmath147 . in 1d , it follows from the strict increasing monotonicity of @xmath148 that @xmath149 $ ] ( for @xmath150 ) .",
    "consider the chord slope @xmath151 with one fixed extremity , say @xmath2 .",
    "when @xmath152 , using lhospital rule , provided that @xmath153 is bounded , we have @xmath154 .",
    "therefore , in that case , @xmath155 .",
    "thus when @xmath156 , we have @xmath157 . in particular , when @xmath158 or @xmath159 , the total jensen divergence tends to the total bregman divergence .",
    "note that when @xmath152 , we have @xmath160 .",
    "we finally close this section by noticing that total jensen divergences may be interpreted as a kind of jensen divergence with the generator function scaled by the conformal factor :    [ lemma : tjequalj ] the total jensen divergence @xmath161 is equivalent to a jensen divergence for the convex generator @xmath162 : @xmath163 .     and @xmath77 .",
    "we deduce that angles @xmath75 and @xmath76 are respectively identical from which the formula on @xmath164 follows immediately .",
    "[ fig : gproof],scaledwidth=70.0% ]",
    "thanks to the invariance to rotations , total bregman divergences proved highly useful in applications ( see  @xcite , etc . ) due to the statistical robustness of their centroids .",
    "the conformal factors play the role of regularizers .",
    "robustness of the centroid , defined as a notion of centrality robust to `` noisy '' perturbations , is studied using the framework of the _ influence function _  @xcite .",
    "similarly , the total skew jensen ( right - sided ) _ centroid _ @xmath165 is defined for a finite weighted point set as the minimizer of the following loss function :    @xmath166    where @xmath167 are the normalized point weights ( with @xmath168 ) .",
    "the left - sided centroids @xmath169 are obtained by minimizing the equivalent right - sided centroids for @xmath170 : @xmath171 ( recall that the conformal factor does not depend on @xmath21 ) .",
    "therefore , we consider the right - sided centroids in the remainder .    to minimize eq .",
    "[ eq : centroid ] , we proceed iteratively in two stages :    * first , we consider @xmath172 ( initialized with the barycenter @xmath173 ) given .",
    "this allows us to consider the following simpler minimization problem : + @xmath174 + let @xmath175 be the updated renormalized weights at stage @xmath176 . * second , we minimize : + @xmath177 + this is a convex - concave minimization procedure  @xcite ( cccp ) that can be solved iteratively until it reaches convergence  @xcite at @xmath178 .",
    "that is , we iterate the following formula  @xcite a given number of times @xmath0 : + @xmath179 + we set the new centroid @xmath180 , and we loop back to the first stage until the loss function improvement @xmath181 goes below a prescribed threshold .",
    "( an implementation in java is available upon request . )",
    "although the cccp algorithm is guaranteed to converge _ monotonically _ to a local optimum , the two steps weight update / cccp does not provide anymore of the monotonous converge as we have attested in practice .",
    "it is an open and challenging problem to prove that the total jensen centroids are unique whatever the chosen multivariate generator , see  @xcite .    in section  [ sec : cluster ] , we shall show that a careful initialization by picking centers from the dataset is enough to guarantee a probabilistic bound on the @xmath0-means clustering with respect to total jensen divergences _ without _ computing the centroids .",
    "the centroid defined with respect to the total bregman divergence has been shown to be robust to outliers whatever the chosen generator  @xcite .",
    "we first analyze the robustness for the symmetric jensen divergence ( for @xmath12 ) .",
    "we investigate the _ influence function _",
    "@xcite @xmath182 on the centroid when adding an outlier point @xmath183 with prescribed weight @xmath184 . without loss of generality ,",
    "it is enough to consider two points : one _ outlier _ with @xmath185 mass and one _ inlier _ with the remaining mass .",
    "let us add an outlier point @xmath183 with weight @xmath185 onto an inliner point @xmath2 .",
    "let @xmath186 and @xmath187 denote the centroids before adding @xmath183 and after adding @xmath183 .",
    "@xmath188 denotes the influence function .",
    "the jensen centroid minimizes ( we can ignore dividing by the renormalizing total weight inlier+outlier : @xmath189 ) :    @xmath190    the derivative of this energy is :    @xmath191    the derivative of the jensen divergence is given by ( not necessarily a convex distance ) : @xmath192 where @xmath193 is the univariate convex generator and @xmath194 its derivative .",
    "for the optimal value of the centroid @xmath195 , we have @xmath196 , yielding :    @xmath197    using taylor expansions on @xmath198 ( where @xmath188 is the influence function ) on the derivative @xmath194 , we get :    @xmath199 and    @xmath200 ( ignoring the term in @xmath201 for small constant @xmath202 in the taylor expansion term of @xmath203 . )    thus we get the following mathematical equality :    @xmath204    finally , we get the expression of the influence function :    @xmath205    for small prescribed @xmath184 .",
    "the jensen centroid is robust for a strictly convex and smooth generator @xmath193 if @xmath206 is bounded on the domain @xmath207 for any prescribed @xmath2 .",
    "note that this theorem extends to separable divergences . to illustrate this theorem ,",
    "let us consider two generators that shall yield a non - robust and a robust jensen centroid , respectively :    * jensen - shannon : @xmath208 , @xmath209 , @xmath210 , @xmath211 .",
    "+ we check that @xmath212 is unbounded when @xmath213 .",
    "the influence function @xmath214 is unbounded when @xmath215 , and therefore the centroid is not robust to outliers .",
    "* jensen - burg : @xmath208 , @xmath216 , @xmath217 , @xmath218 + we check that @xmath219 is always bounded for @xmath220 .",
    "+ @xmath221 + when @xmath215 , we have @xmath222 . the influence function is bounded and the centroid is robust .",
    "jensen centroids are not always robust ( e.g. , jensen - shannon centroids ) .",
    "consider the total jensen - shannon centroid .",
    "table  [ tab : tj ] reports the total bregman / total jensen conformal factors .",
    "namely , @xmath223 and @xmath224 .",
    "notice that the total bregman centroids have been proven to be robust ( the left - sided @xmath176-center in  @xcite ) whatever the chosen generator .    for the total jensen - shannon centroid ( with @xmath12 ) ,",
    "the conformal factor of the outlier point tends to :    @xmath225    it is an ongoing investigation to prove ( 1 ) the uniqueness of jensen centroids , and ( 2 ) the robustness of total jensen centroids whathever the chosen generator ( see the conclusion ) .",
    "the most famous clustering algorithm is @xmath0-means  @xcite that consists in first initializing @xmath0 distinct seeds ( set to be the initial cluster centers ) and then iteratively assign the points to their closest center , and update the cluster centers by taking the centroids of the clusters . a breakthrough was achieved by proving the a randomized seed selection , @xmath0-means++  @xcite , guarantees probabilistically a constant approximation factor to the optimal loss .",
    "the @xmath0-means++ initialization may be interpreted as a discrete @xmath0-means where the @xmath0 cluster centers are choosen among the input .",
    "this yields @xmath226 combinatorial seed sets .",
    "note that @xmath0-means is np - hard when @xmath227 and the dimension is not fixed , but not discrete @xmath0-means  @xcite .",
    "thus we do not need to compute centroids to cluster with respect to total jensen divergences .",
    "skew jensen centroids can be approximated arbitrarily finely using the concave - convex procedure , as reported in  @xcite .    on a compact domain @xmath228",
    ", we have @xmath229 , with @xmath230 and @xmath231    we are given a set @xmath232 of points that we wish to cluster in @xmath0 clusters , following a hard clustering assignment .",
    "we let @xmath233 for any @xmath234 .",
    "the optimal total hard clustering jensen potential is @xmath235 , where @xmath236 . finally , the contribution of some @xmath234 to the optimal total jensen potential having centers @xmath237 is @xmath238 .",
    "total jensen seeding picks randomly without replacement an element @xmath239 in @xmath240 with probability proportional to @xmath241 , where @xmath237 is the current set of centers .",
    "when @xmath242 , the distribution is uniform .",
    "we let total jensen seeding refer to this biased randomized way to pick centers .",
    "we state the theorem that applies for any kind of divergence :    [ th1 ] suppose there exist some @xmath243 and @xmath244 such that , @xmath245 : @xmath246 then the average potential of total jensen seeding with @xmath0 clusters satisfies @xmath247 \\leq 2 u^2(1+v ) ( 2+\\log k){\\mathrm{tj}_{\\mathrm{opt},\\alpha}}$ ] , where @xmath248 is the minimal total jensen potential achieved by a clustering in @xmath0 clusters .    the proof is reported in appendix  [ app : triangularineq ] .",
    "to find values for @xmath243 and @xmath244 , we make two assumptions , denoted h , on @xmath7 , that are supposed to hold in the convex closure of @xmath232 ( implicit in the assumptions ) :    * first , the maximal condition number of the hessian of @xmath7 , that is , the ratio between the maximal and minimal eigenvalue ( @xmath249 ) of the hessian of @xmath7 , is upperbounded by @xmath250 . *",
    "second , we assume the lipschitz condition on @xmath7 that @xmath251 , for some @xmath252",
    ".    then we have the following lemma :    [ lem0 ] assume @xmath253 .",
    "then , under assumption h , for any @xmath254 , there exists @xmath184 such that : @xmath255    notice that because of eq .",
    "[ ppp ] , the right - hand side of eq .",
    "[ triang1 ] tends to zero when @xmath21 tends to @xmath256 or @xmath20 .",
    "a consequence of lemma [ lem0 ] is the following triangular inequality :    the total skew jensen divergence satisfies the following triangular inequality : @xmath257    the proof is reported in appendix  [ app : triangularineq ] .",
    "thus , we may set @xmath258 in theorem  [ th1 ] .    symmetric inequality condition ( [ si ] ) holds for @xmath259 , for some @xmath260 .    the proof is reported in appendix  [ app : triangularineq ] .",
    "we described a novel family of divergences , called total skew jensen divergences , that are invariant by rotations .",
    "those divergences scale the ordinary jensen divergences by a conformal factor independent of the skew parameter , and extend naturally the underlying principle of the former total bregman divergences  @xcite .",
    "however , total bregman divergences differ from total jensen divergences in limit cases because of the non - separable property of the conformal factor @xmath87 of total jensen divergences .",
    "moreover , although the square - root of the jensen - shannon divergence ( a jensen divergence ) is a metric  @xcite , this does not hold anymore for the total jensen - shannon divergence .",
    "although the total jensen centroids do not admit a closed - form solution in general , we proved that the convenient @xmath0-means++ initialization probabilistically guarantees a constant approximation factor to the optimal clustering .",
    "we also reported a simple two - stage iterative algorithm to approximate those total jensen centroids .",
    "we studied the robustness of those total jensen and jensen centroids and proved that skew jensen centroids robustness depend on the considered generator .    in information geometry ,",
    "conformal divergences have been recently investigated  @xcite : the single - sided conformal transformations ( similar to total bregman divergences ) are shown to flatten the @xmath21-geometry of the space of discrete distributions into a dually flat structure .",
    "conformal factors have also been introduced in machine learning  @xcite to improve the classification performance of support vector machines ( svms ) . in that latter case ,",
    "the conformal transfortion considered is double - sided separable : @xmath261 .",
    "it is interesting to consider the underlying geometry of total jensen divergences that exhibit non - separable double - sided conformal factors .",
    "a java program that implements the total jensen centroids are available upon request .    to conclude this work , we leave two open problems to settle :    are ( total ) skew jensen centroids defined as minimizers of weighted distortion averages unique ? recall that jensen divergences may not be convex  @xcite .",
    "the difficult case is then when @xmath7 is multivariate non - separable  @xcite .",
    "are total skew jensen centroids always robust ?",
    "that is , is the influence function of an outlier always bounded ?    10 [ 1]#1 url@samestyle [ 2]#2 [ 2]l@#1=l@#1#2    m.  basseville , `` divergence measures for statistical data processing - an annotated bibliography , '' _ signal processing _ , vol .",
    "93 , no .  4 , pp . 621633 , 2013 .",
    "b.  vemuri , m.  liu , shun - ichi amari , and f.  nielsen , `` total bregman divergence and its applications to dti analysis , '' 2011 .",
    "h.  matsuzoe , `` construction of geometric divergence on @xmath3-exponential family , '' in _ mathematics of distances and applications _ , 2012 .",
    "f.  nielsen and s.  boltz , `` the burbea - rao and bhattacharyya centroids , '' _ ieee transactions on information theory _ , vol .",
    "57 , no .  8 , pp . 54555466 , august 2011 .",
    "f.  nielsen and r.  nock , `` jensen - bregman voronoi diagrams and centroidal tessellations , '' in _ proceedings of the 2010 international symposium on voronoi diagrams in science and engineering ( isvd)_.1em plus 0.5em minus 0.4emwashington , dc , usa : ieee computer society , 2010 , pp . 5665 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1109/isvd.2010.17    j.  zhang , `` divergence function , duality , and convex analysis , '' _ neural computation _ , vol .",
    "16 , no .  1 ,",
    "pp . 159195 , 2004 .",
    "a.  banerjee , s.  merugu , i.  s. dhillon , and j.  ghosh , `` clustering with bregman divergences , '' _ journal of machine learning research _ , vol .",
    "17051749 , 2005 .",
    "s.  amari and h.  nagaoka , _ methods of information geometry _ , a.  m. society , ed.1em plus 0.5em minus 0.4emoxford university press , 2000 .    f.  nielsen and r.  nock , `` hyperbolic voronoi diagrams made easy , '' in _ international conference on computational science and its applications ( iccsa ) _ , vol .",
    "1.1em plus 0.5em minus 0.4emlos alamitos , ca , usa : ieee computer society , march 2010 , pp . 7480 .",
    "m.  liu , `` total bregman divergence , a robust divergence measure , and its applications , '' ph.d .",
    "dissertation , university of florida , 2011 .",
    "m.  liu , b.  c. vemuri , s.  amari , and f.  nielsen , `` shape retrieval using hierarchical total bregman soft clustering , '' _ transactions on pattern analysis and machine intelligence _ , vol .",
    "34 , no .  12 , pp . 24072419 , 2012 .",
    "m.  liu and b.  c. vemuri , `` robust and efficient regularized boosting using total bregman divergence , '' in _ ieee computer vision and pattern recognition ( cvpr ) _ , 2011 , pp .",
    "28972902 .",
    "a.  r.  m. y  tern , m.  gouiffs , and l.  lacassagne , `` total bregman divergence for multiple object tracking , '' in _ ieee international conference on image processing ( icip ) _ , 2013 .",
    "f.  escolano , m.  liu , and e.  hancock , `` tensor - based total bregman divergences between graphs , '' in _ ieee international conference on computer vision workshops ( iccv workshops ) _ , 2011 , pp .",
    "14401447 .",
    "j.  lin and s.  k.  m. wong , `` a new directed divergence measure and its characterization , '' _ int .",
    "j.  general systems _ , vol .",
    "17 , pp . 7381 , 1990 .",
    "[ online ] .",
    "available : http://www.tandfonline.com/doi/abs/10.1080/03081079008935097#.ui1c68ysaso    j.  lin , `` divergence measures based on the shannon entropy , '' _ ieee transactions on information theory _",
    "37 , no .  1 ,",
    "145  151 , 1991 .",
    "b.  fuglede and f.  topsoe , `` jensen - shannon divergence and hilbert space embedding , '' in _ ieee international symposium on information theory _ , 2004 , pp .",
    "k.  b. stolarsky , `` generalizations of the logarithmic mean , '' _ mathematics magazine _ , vol .  48 , no .  2 , pp .",
    "8792 , 1975 .",
    "f.  r. hampel , p.  j. rousseeuw , e.  ronchetti , and w.  a. stahel , _ robust statistics : the approach based on influence functions_.1em plus 0.5em minus 0.4emwiley series in probability and mathematical statistics , 1986 .",
    "a.  l. yuille and a.  rangarajan , `` the concave - convex procedure , '' _ neural computation _ , vol",
    ".  15 , no .  4 , pp . 915936 , 2003 .",
    "s.  p. lloyd , `` least squares quantization in pcm , '' bell laboratories , tech . rep . , 1957 .",
    "d.  arthur and s.  vassilvitskii , `` @xmath0-means@xmath262 : the advantages of careful seeding , '' in _ proceedings of the eighteenth annual acm - siam symposium on discrete algorithms ( soda)_.1em plus 0.5em minus 0.4em society for industrial and applied mathematics , 2007 , pp .",
    "10271035 .",
    "m.  mahajan , p.  nimbhorkar , and k.  varadarajan , `` the planar @xmath0-means problem is np - hard , '' _ theoretical computer science _ ,",
    "442 , no .  0 , pp . 13  21 , 2012 .",
    "a.  ohara , h.  matsuzoe , and shun - ichi amari , `` a dually flat structure on the space of escort distributions , '' _ journal of physics : conference series _ , vol .",
    "201 , no .  1 ,",
    ", 2010 .",
    "s.  wu and shun - ichi amari , `` conformal transformation of kernel functions a data dependent way to improve support vector machine classifiers , '' _ neural processing letters _",
    "15 , no .  1 , pp . 5967 , 2002 .",
    "r.  wilson and a.  calway , `` multiresolution gaussian mixture models for visual motion estimation , '' in _ icip ( 2 ) _ , 2001 , pp .",
    "921924 .",
    "d.  arthur and s.  vassilvitskii , `` @xmath0-means++ : the advantages of careful seeding , '' 2007 , pp .",
    "1027  1035 .",
    "r.  nock , p.  luosto , and j.  kivinen , `` mixed bregman clustering with approximation guarantees , '' 2008 , pp .",
    "consider figure  [ fig : sj ] . to define the total skew jensen divergence ,",
    "we write the _ orthogonality constraint _ as follows :    @xmath263    note that @xmath264 .",
    "fixing one of the two unknowns or @xmath102 , and solve for the other quantity .",
    "fixing @xmath21 always yield a closed - form solution .",
    "fixing @xmath102 may not necessarily yields a closed - form solution .",
    "therefore , we consider @xmath21 prescribed in the following . ] , we solve for the other quantity , and define the total jensen divergence by the euclidean distance between the two points :    @xmath265    for a prescribed @xmath21 , let us solve for @xmath266 :    @xmath267    note that for @xmath268 , @xmath102 may be falling outside the unit range @xmath269 $ ] .",
    "it follows that : @xmath270    we apply pythagoras theorem from the right triangle in figure  [ fig : sj ] , and get :    @xmath271    that is , the length @xmath272 is @xmath273 times the length of the segment linking @xmath274 to @xmath275 .",
    "thus we have : @xmath276 finally , get the ( unscaled ) total jensen divergence : @xmath277",
    "as mentioned in remark  [ rq : differenttj ] , we could have used the orthogonal projection principle by fixing the point @xmath93 on the line segment @xmath62 $ ] and seeking for a point @xmath278 on the function graph that yields orthogonality . however",
    ", this second approach does not always give rise to a closed - form solution for the total jensen divergences .",
    "indeed , fix @xmath102 and consider the point @xmath279 that meets the orthogonality constraint .",
    "let us solve for @xmath280 $ ] , defining the point @xmath278 on the graph function plot .",
    "then we have to solve for the unique @xmath280 $ ] such that :    @xmath281    in general , we can not explicitly solve this equation although we can always approximate finely the solution . indeed , this equation amounts to solve : @xmath282 with coefficients @xmath283 , @xmath284 , and @xmath285 . in general",
    ", it does not admit a closed - form solution because of the @xmath286 term .",
    "for the squared euclidean potential @xmath52 , or the separable burg entropy @xmath287 , we obtain a closed - form solution .",
    "however , for the most interesting case , @xmath288 the shannon information ( negative entropy ) , we can not get a closed - form solution for @xmath21 .    the length of the segment joining the points @xmath278 and @xmath93",
    "is :    @xmath289    scaling by @xmath290 , we get the second kind of total jensen divergence :    @xmath291    the second type total jensen divergences do not tend to total bregman divergences when @xmath292 .",
    "information geometry  @xcite is primarily concerned with the differential - geometric modeling of the space of statistical distributions . in particular , statistical manifolds induced by a parametric family of distributions",
    "have been historically of prime importance . in that case , the statistical divergence @xmath293 defined on such a probability manifold @xmath294 between two random variables @xmath295 and @xmath296 ( identified using their corresponding parametric probability density functions @xmath297 and @xmath298 ) should be _ invariant _ by ( 1 ) a _ one - to - one mapping _",
    "@xmath299 of the parameter space , and by ( 2 ) sufficient statistics  @xcite ( also called a markov morphism ) .",
    "the first condition means that @xmath300 .",
    "as we illustrate next , a transformation on the observation space @xmath64 ( support of the distributions ) may sometimes induce an _ equivalent transformation _ on the parameter space @xmath301 . in that case , this yields as a byproduct a third invariance by the transformation on @xmath64 .",
    "for example , consider a @xmath105-variate normal random variable @xmath302 and apply a rigid transformation @xmath303 to @xmath304 so that we get @xmath305 .",
    "then @xmath183 follows a normal distribution .",
    "in fact , this normal@xmath306normal conversion is well - known to hold for any _ invertible affine transformation _",
    "@xmath307 with @xmath308 on @xmath207 , see  @xcite : @xmath309 .",
    "consider the kullback - leibler ( kl ) divergence ( belonging to the @xmath193-divergences ) between @xmath310 and @xmath311 , and let us show that this amounts to calculate equivalently the kl divergence between @xmath312 and @xmath313 :    @xmath314    the kl divergence @xmath315 between two @xmath105-variate normal distributions @xmath310 and @xmath311 is given by : @xmath316 with @xmath317 and @xmath318 denoting the determinant of the matrix @xmath319 .",
    "consider @xmath320 and let us show that the formula is equivalent to @xmath321 .",
    "we have the term @xmath322 , and from the trace cyclic property , we rewrite this expression as @xmath323 since @xmath324 . furthermore , @xmath325 .",
    "it follows that : @xmath326 and @xmath327    thus we checked that the kl of gaussians is indeed preserved by a rigid transformation in the observation space @xmath64 : @xmath328 .",
    "the key to proving the theorem is the following lemma , which parallels lemmata from @xcite ( lemma 3.2 ) , @xcite ( lemma 3 ) .",
    "let @xmath329 be an arbitrary cluster of @xmath330 and @xmath237 an arbitrary clustering . if we add a random element @xmath239 from @xmath329 to @xmath237 , then @xmath331 & \\leq & 2 u^2(1+v ) { \\mathrm{tj}_{\\mathrm{opt},\\alpha}}(a)\\:\\:,\\label{eqp1}\\end{aligned}\\ ] ] where @xmath332 denotes the seeding distribution used to sample @xmath239 .",
    "first , @xmath333 = e_{x\\sim \\pi_a } [ { \\mathrm{tj}_\\alpha}(a , x)]$ ] because we require @xmath239 to be in @xmath329 .",
    "we then have : @xmath334 and so averaging over all @xmath335 yields : @xmath336 the participation of @xmath337 to the potential is : @xmath338 & = & \\sum_{{y } \\in { \\mathcal{a } } } { \\left\\{\\frac{{\\mathrm{tj}_\\alpha}(y : c_y)}{\\sum_{{x } \\in { \\mathcal{a } } } { { \\mathrm{tj}_\\alpha}(x : c_x ) } } \\sum_{{x } \\in { \\mathcal{a } } } { \\min\\left\\{{\\mathrm{tj}_\\alpha}(x : c_x ) , { \\mathrm{tj}_\\alpha}(x : y)\\right\\ } } \\right\\}}\\:\\ : . \\label{firstp}\\end{aligned}\\ ] ] we get : @xmath338 & \\leq & \\frac{u}{|{\\mathcal{a}}|}\\sum_{{y } \\in { \\mathcal{a } } } { \\left\\{\\frac{\\sum_{x\\in { \\mathcal{a}}}{{\\mathrm{tj}_\\alpha}(y : x)}}{\\sum_{{x } \\in          { \\mathcal{a } } } { { \\mathrm{tj}_\\alpha}(x : c_x ) } } \\sum_{{x } \\in { \\mathcal{a } } }      { \\min\\left\\{{\\mathrm{tj}_\\alpha}(x : c_x ) , { \\mathrm{tj}_\\alpha}(x : y)\\right\\ } } \\right\\}}\\nonumber\\\\    & & +   \\frac{u}{|{\\mathcal{a}}|}\\sum_{{y } \\in { \\mathcal{a } } } { \\left\\{\\frac{\\sum_{x\\in { \\mathcal{a}}}{{\\mathrm{tj}_\\alpha}(x : c_x)}}{\\sum_{{x } \\in          { \\mathcal{a } } } { { \\mathrm{tj}_\\alpha}(x : c_x ) } } \\sum_{{x } \\in { \\mathcal{a } } }      { \\min\\left\\{{\\mathrm{tj}_\\alpha}(x : c_x ) , { \\mathrm{tj}_\\alpha}(x : y)\\right\\ } } \\right\\ } } \\label{eqq2}\\\\   & \\leq & \\frac{2u}{|{\\mathcal{a}}| } \\sum_{{x } \\in     { \\mathcal{a}}}{\\sum_{{y } \\in { \\mathcal{a}}}{{\\mathrm{tj}_\\alpha}(x : y ) } } \\label{eqq3}\\\\   & \\leq & 2u^2 \\left ( \\sum_{{x } \\in     { \\mathcal{a}}}{{\\mathrm{tj}_\\alpha}(x : c_x ) } + \\frac{1}{|{\\mathcal{a}}|}\\sum_{{x } \\in     { \\mathcal{a}}}{\\sum_{{y } \\in { \\mathcal{a}}}{{\\mathrm{tj}_\\alpha}(c_x : y)}}\\right ) \\label{eqq4}\\\\   & & = 2u^2 \\left ( { \\mathrm{tj}_{\\mathrm{opt},\\alpha}}({\\mathcal{a } } ) + \\frac{1}{|{\\mathcal{a}}|}\\sum_{{x } \\in     { \\mathcal{a}}}{\\sum_{{y } \\in",
    "{ \\mathcal{a}}}{{\\mathrm{tj}_\\alpha}(c_x : y)}}\\right ) \\label{eqq5}\\\\ & \\leq & 2u^2 \\left ( { \\mathrm{tj}_{\\mathrm{opt},\\alpha}}({\\mathcal{a } } ) + \\frac{v}{|{\\mathcal{a}}|}\\sum_{{x } \\in     { \\mathcal{a}}}{\\sum_{{y } \\in       { \\mathcal{a}}}{{\\mathrm{tj}_\\alpha}(y : c_x)}}\\right)\\label{pppp1}\\\\   & & = 2u^2(1+v ) { \\mathrm{tj}_{\\mathrm{opt},\\alpha}}({\\mathcal{a}})\\:\\:,\\label{eqq6}\\end{aligned}\\ ] ] as claimed . we have used ( [ ti ] ) in ( [ eqq2 ] ) , the `` min '' replacement procedure of @xcite ( lemma 3.2 ) , @xcite ( lemma 3 ) in ( [ eqq3 ] ) , a second time ( [ ti ] ) in ( [ eqq4 ] ) , ( [ si ] ) in ( [ pppp1 ] ) , and the definition of @xmath339 in ( [ eqq5 ] ) and ( [ eqq6 ] ) .        * first , the maximal condition number of the hessian of @xmath7 , that is , the ratio between the maximal and minimal eigenvalue ( @xmath249 ) of the hessian of @xmath7 , is upperbounded by @xmath250 .",
    "* second , we assume the lipschitz condition on @xmath7 that @xmath340 , for some @xmath252 .      because the hessian @xmath342 of @xmath7 is symmetric positive definite , it can be diagonalized as @xmath343 for some unitary matrix @xmath344 and positive diagonal matrix @xmath345 .",
    "thus for any @xmath239 , @xmath346 .",
    "applying taylor expansions to @xmath7 and its gradient @xmath135 yields that for any @xmath347 in @xmath232 there exists @xmath348 and @xmath349 in the simplex @xmath350 such that :      eq .",
    "( [ eqj ] ) holds because a taylor expansion of @xmath352 yields that @xmath353 , there exists @xmath354 such that @xmath355 hence , using ( [ eqj2 ] ) , we obtain : @xmath356 multiplying by @xmath357 and using the fact that @xmath358 , @xmath359 , yields the statement of the lemma .",
    "we make use again of ( [ ppp ] ) and have , for some @xmath361 and any @xmath362 , both @xmath363 and @xmath364 , which implies , for some : @xmath365 @xmath366 using the fact that @xmath367 , @xmath368 , yields the statement of the lemma ."
  ],
  "abstract_text": [
    "<S> we present a novel class of divergences induced by a smooth convex function called _ total jensen divergences_. those total jensen divergences are invariant by construction to rotations , a feature yielding regularization of ordinary jensen divergences by a conformal factor . </S>",
    "<S> we analyze the relationships between this novel class of total jensen divergences and the recently introduced total bregman divergences . </S>",
    "<S> we then proceed by defining the _ total jensen centroids _ as average distortion minimizers , and study their robustness performance to outliers . finally , we prove that the @xmath0-means++ initialization that bypasses explicit centroid computations is good enough in practice to guarantee probabilistically a constant approximation factor to the optimal @xmath0-means clustering .    total bregman divergence , skew jensen divergence , jensen - shannon divergence , burbea - rao divergence , centroid robustness , stolarsky mean , clustering , @xmath0-means++ . </S>"
  ]
}