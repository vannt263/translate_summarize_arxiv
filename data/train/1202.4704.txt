{
  "article_text": [
    "parameter sensitivity analysis is one of the most important tools available for modelling biochemical networks .",
    "such analysis is particularly crucial in systems biology , where models may have hundreds of parameters whose values are uncertain .",
    "sensitivity analysis allows one to rank parameters in order of their influence on network behaviour , and hence to target experimental measurements towards biologically relevant parameters and to identify possible drug targets . for deterministic models",
    ", the adjunct ode method provides an efficient way to compute the local sensitivity of a model to small changes in parameters . for stochastic models , however , parameter sensitivity analysis can be computationally intensive , requiring repeated simulations for perturbed values of the parameters . here ,",
    "we demonstrate a method , based on trajectory reweighting , for computing local parameter sensitivity coefficients in stochastic kinetic monte - carlo simulations without the need for repeated simulations .    sensitivity analysis of biochemical network models may take a number of forms .",
    "one may wish to determine how a model s behaviour changes as a parameter is varied systematically within some range ( a parameter sweep ) , its dependence on the initial conditions of a simulation , or its sensitivity to changes in the structure of the model itself ( alternate mode - of - action hypotheses ) . in this paper , we focus on the computation of local parameter sensitivity coefficients .",
    "these coefficients describe how a particular output @xmath0 of the model varies when the @xmath1-th parameter of the model , @xmath2 , is varied by an infinitesimal amount , @xmath3 : @xmath4 where @xmath5 is the output of the model computed in a system with @xmath2 changed to @xmath6 . for deterministic models , where the dynamics of the variables @xmath7 can be described by a set of deterministic ordinary differential equations ( odes ) @xmath8 , differentiation of the odes with respect to @xmath2 shows that the sensitivity coefficients @xmath9 obey an _",
    "set of odes , @xmath10 these adjunct odes can be integrated alongside the original odes to compute the sensitivity coefficients `` on the fly '' in a deterministic simulation of a biochemical network .",
    "stochastic models of biochemical networks are ( generally ) continuous - time markov processes @xcite which are solved numerically by kinetic monte - carlo simulation , using standard methods such as the gillespie @xcite or gibson - bruck @xcite algorithms .",
    "replicate simulations will produce different trajectories ; we wish to compute how the _ average _ value @xmath11 of some function @xmath12 of the model changes with the parameter @xmath2 : @xmath13 where the averages are taken across replicate simulation runs . if one is interested in steady - state ( _ i.e. _  time - independent ) parameter sensitivities , the averages in eq .",
    "may instead be time averages taken over a single simulation run .",
    "nave  evaluation of parameter sensitivities via eq .",
    "is very inefficient , since one is likely to be looking for a small difference between two fluctuating quantities .",
    "there are several existing approaches that get around this problem : spectral methods @xcite , a method based on the girsanov measure transform @xcite , and methods which re - use the random number streams @xcite . in this paper , we develop a method based on trajectory reweighting , which is simple to implement in existing kinetic monte carlo codes and provides a way to compute steady - state parameter sensitivity coefficients `` on - the - fly '' in stochastic simulations of biochemical networks .",
    "the method provides an accessible alternative to the girsanov measure transform pioneered by plyasunov and arkin @xcite .",
    "indeed several of our equations in section [ sec : tw ] are equivalent to those ref .  .",
    "however , we go beyond previous work by showing in practical terms how the method can be implemented in standard stochastic simulation algorithms , extending the method to the computation of parameter sensitivities in the steady state , and showing how time - step preaveraging can be used to improve the efficiency of the calculations .",
    "the basic idea behind trajectory reweighting is as follows . in a kinetic monte - carlo simulation , for a given set of parameters",
    "any given trajectory has a statistical weight which measures the probability that it will be generated by the algorithm @xcite ; this weight can be expressed as an analytical function of the states of the system along the trajectory and of the parameter set .",
    "this analytical function also allows us to compute the statistical weight for this _ same _ trajectory , in a system with a _ different _ set of parameters : _",
    "i.e. _   its weight in the ensemble of trajectories with perturbed parameters .",
    "this allows us in principle to compute the average @xmath14 in eq .   for the perturbed parameter set ,",
    "using only a set of trajectories generated with the unperturbed parameter set . for most applications this is inefficient , because the weight of a trajectory in the perturbed ensemble is typically very low , resulting in poor sampling .",
    "however , it turns out that trajectory reweighting does provide an effective way to compute local parameter sensitivity coefficients .",
    "more specifically , let us consider a typical implementation of the gillespie algorithm @xcite ( similar arguments apply to more recent algorithms , such as gibson - bruck @xcite ) . here",
    ", the state of the system is characterised by a set of discrete quantities @xmath15 , typically representing the number of molecules of chemical species @xmath16 .",
    "transitions between states are governed by propensity functions @xmath17 where @xmath18 labels the possible reaction channels and the quantities @xmath2 are parameters in the problem , typically reaction rates ( @xmath2 represents the @xmath1-th such parameter ) .",
    "a kinetic monte - carlo trajectory is generated by stepping through the space of states @xmath15 in the following way .",
    "we first compute the propensity functions @xmath19 for all the possible transitions out of the current state .",
    "we then choose a time step ( _ i.e. _  waiting time ) @xmath20 from an exponential distribution @xmath21 , where the state - dependent mean timestep ( the mean waiting time before exiting the current state ) is @xmath22 .",
    "we choose a reaction channel with probability @xmath23 .",
    "we advance time by @xmath20 and update the values of @xmath15 according to the chosen reaction channel .",
    "we are now in a new state , and the above steps are repeated .",
    "now let us consider the statistical weight of a given trajectory generated by this algorithm @xcite . in each step",
    ", the probability of choosing the value of @xmath20 that we actually chose is proportional to @xmath24 , and the probability of choosing the reaction channel that we actually chose is equal to @xmath25 .",
    "we can therefore associate a weight @xmath26 with the whole trajectory , which is proportional to the probability of generating the sequence of steps which we actually generated : @xmath27 & = \\textstyle{\\prod_{\\mathrm{steps}}}\\ , a_\\mu \\,e^{- ( \\sum a_\\mu ) \\delta t}\\ , .",
    "\\end{array } \\label{eq : pdef}\\ ] ] the second line follows by eliminating @xmath28 ( note that because eq .",
    "is not normalized , @xmath26 is a _",
    "weight _ rather than a true probability ) .    in a typical kinetic monte - carlo simulation ,",
    "we generate multiple independent trajectories of length @xmath29 , for a given parameter set .",
    "the probability of generating any given trajectory in this sample will be proportional to its weight @xmath26 , defined in eq .  .",
    "we then compute the average @xmath30 of some function @xmath12 of the state of the system by summing over the values of @xmath0 , at time @xmath29 , for these trajectories .",
    "having generated this set of trajectories , let us now suppose we wish to re - use them to compute the average @xmath31 which we would have obtained had we repeated our simulations for some _ other _ parameter set .",
    "it turns out that we can compute this average by summing over the same set of trajectories , multiplied by the ratio of their statistical weights for the perturbed and unperturbed parameter sets . to see this",
    ", we first recall that an average , _ e.g. _",
    "@xmath30 , can be written as a sum over all _ possible _ trajectories @xmath32 of length @xmath29 , multiplied by their statistical weights @xmath33 : @xmath34 . writing the perturbed average @xmath31 in this way , we obtain @xmath35 where @xmath26 and @xmath36 are the trajectory weights ( calculated using eq .  ) for the original and perturbed models respectively . in another context ,",
    "eq.([eq : pppp ] ) has been used to reweight trajectory statistics in order to sample rare events in biochemical networks @xcite ; it also forms the basis of umbrella sampling methods for particle - based monte carlo simulations @xcite .",
    "whilst in principle eq.([eq : pppp ] ) provides a completely general way to transform between trajectory ensembles with different parameter sets , in practice it is useless for any significant deviation of the parameter set from the original values , for two reasons .",
    "first , the statistical errors in the computation of @xmath14 grow catastrophically with the size of the perturbation , because the original trajectories become increasingly unrepresentative of the perturbed model .",
    "second , the computational cost of determining the trajectory weights for the perturbed and unperturbed parameter sets via eq .",
    "is only marginally less than the cost of computing @xmath14 directly by generating a new set of trajectories for the perturbed parameter set .",
    "it turns out , however , that eq .",
    "is useful for the computation of parameter sensitivity coefficients , where the deviation between the original and perturbed parameter sets is infinitesimal .",
    "let us suppose that the perturbed problem corresponds to a small change in a single parameter , such as @xmath37 ; the corresponding sensitivity coefficient is defined by eq .  .",
    "as we show in supplementary material section [ sec : rat ] , differentiating eq .   with respect to @xmath38 leads to the following expression for the sensitivity coefficient : @xmath39 where @xmath40 supplementary material section [ sec : rat ] also shows how to generalize this approach to higher - order derivatives . combining eq .   with eq .",
    "shows that the `` weight function '' @xmath41 can be expressed as a sum over all steps in the trajectory : @xmath42 where @xmath43    eqs .  ",
    "are the key results of this paper , since they point to a practical way to compute parameter sensitivity coefficients in kinetic monte - carlo simulations . to evaluate the ( time - dependent ) parameter sensitivity @xmath44 , one tracks a weight function @xmath45 , which evolves according to eqs .   and",
    "one also tracks the function @xmath12 of interest .",
    "the covariance between @xmath41 and @xmath0 , at the time @xmath29 of interest , computed over multiple simulations , then gives the sensitivity of @xmath30 to the parameter in question ( as in eq .  ) .",
    "tracking @xmath41 should be a straightforward addition to standard kinetic monte - carlo schemes .",
    "moreover we note that @xmath0 could be any function of the variables of the system  for example , if one were interested in the parameter sensitivity of the noise in particle number @xmath15 , one could choose @xmath46 .",
    "more complex functions of the particle numbers , involving multiple chemical species , could also be used ( see examples below ) .",
    "this prescription for computing parameter sensitivities presents , however , some difficulties in terms of statistical sampling .",
    "the two terms in eq .",
    "are statistically independent quantities with the same expectation value , @xmath47 .",
    "hence they cancel on average but the variances add .",
    "thus we expect that @xmath41 is a stochastic process with a zero mean , @xmath48 , and a variance that should grow approximately linearly with time  as shown for a simple example case in supplementary material section [ sec : linear]in effect @xmath41 behaves as a random walk ( _ i.e. _  a wiener process ) . in terms of controlling the sampling error",
    ", this means that the number of trajectories over which the covariance is evaluated should increase in proportion to the trajectory length , since the standard error in the mean is expected to go as the square root of the variance divided by the number of trajectories @xcite . in section [ sec : ss ] , we discuss a way to avoid this problem , when computing steady - state parameter sensitivities .      without loss of generality",
    "we can presume that the parameter @xmath2 will appear in only one of the propensity functions , which we call @xmath49 @xcite . with this presumption , eq .",
    "becomes @xmath50 eq .   makes a direct link with the girsanov measure transform method introduced by plyasunov and arkin , being essentially the same as eq .",
    "( 31b ) in ref .  .",
    "a further simplification occurs if @xmath2 is the rate coefficient of the @xmath1-th reaction , so that @xmath49 is linearly proportional to @xmath2 .",
    "one then has @xmath51 and eq .",
    "becomes @xmath52 \\label{eq : key2c}\\ ] ] where @xmath53 counts the number of times that the @xmath1-th reaction is visited .",
    "this is essentially the same as eq .",
    "( 9b ) of plyasunov and arkin s work , ref .  .",
    ".   suggests a very simple way to implement parameter sensitivity computations in existing kinetic monte - carlo codes .",
    "one simply modifies the chemical reaction scheme such that each reaction whose rate constant is of interest generates a `` ghost '' particle in addition to its other reaction products ( this is similar to the clock trick in ref .  ) .",
    "there should be a different flavour of ghost particle for each reaction of interest , and ghost particles should not participate in any other reactions .",
    "@xmath54 is then simply given by the number of ghost particles associated with the @xmath1-th reaction which are present at time @xmath29 . in section [ sec : egs ] , we use this approach to compute sensitivity coefficients using the _ unmodified _ copasi  @xcite simulation package . in supplementary material section [ sec : linear ]",
    "we also exploit this trick to obtain some exact results for linear propensity functions .",
    "so far , we have discussed the computation of time - dependent parameter sensitivity coefficients @xmath55 , by evaluating the covariance of the weight function @xmath45 with the function @xmath12 over multiple simulation runs .",
    "often , however , one is interested in the parameter sensitivity of the _ steady - state _ properties of the system @xmath56 ; this is a time - independent quantity .",
    "we now discuss the computation of steady - state parameter sensitivities using trajectory reweighting .",
    "we show that in this case , first , the problem of poor sampling of @xmath45 for long times can be circumvented , second , one can obtain sensitivity coefficients from a single simulation run , and third , one can improve efficiency by a procedure which we call time - step pre - averaging .      to compute steady - state parameter sensitivities",
    ", one might imagine that we could simply apply the method discussed in section [ sec : tw ] , taking the limit of long times , when the system should have relaxed to its steady state . however , this does not work , because the variance between trajectories of the weight function @xmath41 increases in time , making it impossible to obtain good statistics at long times . to circumvent this problem",
    ", we note that the right hand side of eq .   is unaltered if @xmath41 is offset by a constant .",
    "thus we may write the parameter sensitivity in the form of a two - point time - correlation function : @xmath57 { } \\hspace{8em}- { \\langle f(n_i , t)\\rangle}\\,{\\langle \\delta w_{k_\\alpha}(t , t_0)\\rangle } \\end{array } \\label{eq : cdef}\\ ] ] where @xmath58 and @xmath59 is some arbitrary reference time such that @xmath60 .",
    "this relation has the advantage that we may choose @xmath61 sufficiently small to make the variance of @xmath62 manageable .",
    "importantly , in steady - state conditions , we expect that the correlation function depends only on the time difference and not separately on the two times , so that @xmath63 , with @xmath64 and @xmath65 as @xmath66 .",
    "thus to calculate the sensitivity coefficient under steady state conditions , all we need to do is compute the steady - state correlation function defined in eq .",
    ", choosing a suitable `` reference '' time @xmath59 when the system is already in the steady - state , then take the asymptotic ( large @xmath61 ) value of this correlation function .",
    "we expect the correlation function to approach its asymptotic value on a timescale governed by the ( likely short ) relaxation time spectrum in the steady state , so that for most problems large values of @xmath61 should not be required .",
    "noting that in this method , as in section [ sec : tw ] , the averages in eq .   are computed over multiple independent simulation runs , we term this approach the _ ensemble - averaged correlation function method_.    from a practical point of view",
    ", this method involves the following set of steps or ` recipe ' :    1 .",
    "choose two time points @xmath67 and @xmath68 such that the system has already reached its steady state at time @xmath67 and @xmath69 where @xmath61 is greater than the typical relaxation time of the quantity @xmath0 of interest ( typically this is the same as the longest relaxation time in the system as a whole ) .",
    "2 .   compute @xmath41 at times @xmath67 and @xmath68 and @xmath0 at time @xmath68 .",
    "3 .   calculate the difference @xmath70 .",
    "compute also the product @xmath71 .",
    "repeat steps 1 - 3 for many independent simulation runs and compute the averages @xmath72 , @xmath73 and @xmath74 over the replicate simulations .",
    "calculate the correlation function @xmath75 .",
    "as long as @xmath61 is large enough this provides a measurement of @xmath76 .",
    "it turns out , however , that for steady - state parameter sensitivities , we do not need to average over multiple simulation runs  we can instead compute time - averages over a single simulation run .",
    "this amounts to replacing the steady - state ensemble averaged sensitivity @xmath77 by the _ time averaged _ version @xmath78 , where @xmath79 ( recalling that in kinetic monte carlo , the timestep @xmath20 varies between steps ) . in principle , @xmath80 could be obtained by computing the time - averaged version of eq .  :",
    "@xmath81 and taking the limit of large @xmath82 .",
    ".   requires one to keep track of @xmath41 a precise time @xmath61 in the past ; since the time step is not constant in kinetic monte carlo , this is rather inconvenient to implement .",
    "fortunately , however , tracking the weight function at a precise time in the past turns out to be unnecessary .",
    "as @xmath61 becomes large , the stochastic differences between individual time steps cancel out and it becomes equivalent simply to compute the average @xmath83 where @xmath84 and to use the fact that @xmath85 as @xmath86 .",
    "one can quite easily keep track of @xmath87 , for instance by maintaining a circular history array storing @xmath41 over the last @xmath88 steps .",
    "this approach , which we denote the _ time - averaged correlation function method _",
    ", has the important advantage that one can obtain the steady state parameter sensitivity from a single simulation run .",
    "the recipe for using the time - averaged correlation function method is then :    1 .",
    "choose a time interval @xmath61 which is greater than the typical relaxation time of the quantity @xmath0 of interest .",
    "estimate the typical number of steps @xmath88 taken in time @xmath61 : @xmath89 .",
    "2 .   for a simulation of the system in the steady state , record @xmath0 and @xmath41 every @xmath88 steps ( we denote each of these recordings a ` timeslice ' ) .",
    "3 .   for each timeslice @xmath16 , compute the difference between @xmath90 and its value in the previous timeslice : @xmath91 .",
    "compute also @xmath92 .",
    "4 .   compute the averages over all timeslices of @xmath0 , @xmath93 and @xmath94 .",
    "5 .   calculate the correlation function @xmath95 .",
    "as long as @xmath88 is large enough this provides a measurement of @xmath96 .",
    "the time - averaged correlation function method is a convenient way to compute parameter sensitivities in a standard kinetic monte carlo scheme , in which both a new timestep and a new reaction channel are chosen stochastically at every step .",
    "however , choosing a new time step at every iteration is computationally expensive since it requires a random number , and is not strictly necessary for the computation of steady - state parameter sensitivity coefficients .",
    "improved efficiency can be achieved by choosing only the new reaction channel stochastically at each iteration , and replacing @xmath20 by the mean timestep @xmath97 corresponding to the current state ( note that this is state dependent since it depends on the propensity functions ) .",
    "this amounts to _ pre - averaging _ over the distribution of possible time steps for a given state of the system .",
    "it can be proved formally that if we run our simulations for a sufficiently long time , eq .   is equivalent to @xmath98 intuitively , this relation arises because a sufficiently long trajectory , under steady state conditions , will visit each state an arbitrarily large number of times and thus thoroughly sample the distribution of waiting times in each state .",
    "one can not , however , compute the parameter sensitivity @xmath99 simply by evaluating the time averages in eq .   or using the new definition , eq .  .",
    "this is because @xmath100 itself depends on the parameter @xmath2 . instead ,",
    "a slightly more complicated expression for @xmath101 is required ; this is given in supplementary material section [ sec : preav ] .",
    "thus , time - step pre - averaging provides a more efficient way to compute the steady - state parameter sensitivity ( since the time does not need to be updated in the monte carlo algorithm ) , at the cost of a slight increase in mathematical complexity .",
    "we now apply the methods described above to three case studies : a model for constitutive gene expression for which we can compare our results to analytical theory , a simple model for a signaling pathway with stochastic focussing , and a model for a bistable genetic switch .",
    "the second and third examples are chosen because they exhibit the kind of non - trivial behaviour found in real biochemical networks , yet the state space is sufficiently compact that the parameter sensitivities can be checked using finite - state projection ( fsp ) methods @xcite . our implementation of the fsp methods is described more fully in supplementary material section [ sec : fsp ] .     for the sensitivity of the average protein number @xmath102 ( top ) and the average mrna number @xmath103 to the model parameters , for the constitutive gene expression model in section [ sec : santmac ] .",
    "points with error bars are simulations using the ensemble - average correlation function method ; error bars are estimated by block - averaging ( 100 blocks of @xmath104 trajectories ; total @xmath105 time steps ) .",
    "open circles are simulations using the time - average correlation function method with time step pre - averaging ( plotted as a function of @xmath106 ) ; results are averages over 10 trajectories each of length @xmath107 steps ( in this case the error bars are smaller than symbols ) .",
    "solid lines are theoretical predictions from eq .  .",
    "parameters are @xmath108 , @xmath109 , @xmath110 , @xmath111 , corresponding to the _ cro _ gene in a recent model of phage lambda @xcite . for these parameters @xmath112 and @xmath113.[fig : corrn ] ]      we first consider a simple stochastic model for the expression of a constitutive ( unregulated ) gene , represented by the following chemical reactions : @xmath114 here , m represents messenger rna ( synthesis rate @xmath115 , degradation rate @xmath116 ) and n represents protein ( synthesis rate @xmath117 , degradation rate @xmath18 ) .",
    "this model has linear propensities ( as defined in supplementary material section [ sec : linear ] ) , which implies that the mean copy numbers @xmath118 and @xmath119 of mrna and protein respectively obey the chemical rate equations @xmath120 from which follow the steady state mean copy numbers : @xmath121 for this problem , steady - state sensitivity coefficients can be computed analytically by taking derivatives of eqs .   with respect to the parameters of interest .",
    "moreover , as shown in supplementary material section [ sec : linear ] , explicit expressions can also be found for the components of the correlation functions defined by eqs .   and :",
    "@xmath122 { \\langle m \\delta w_{\\ln\\rho}\\rangle}_{{\\mathrm{ss}}}={\\langle m \\delta w_{\\ln\\mu}\\rangle}_{{\\mathrm{ss}}}=0,\\\\[6pt ] { \\langle",
    "n \\delta w_{\\ln k}\\rangle}_{{\\mathrm{ss}}}=-{\\langle n \\delta w_{\\ln\\lambda}\\rangle}_{{\\mathrm{ss}}}\\\\ { } \\hspace{1em}={\\langle n\\rangle}_{{\\mathrm{ss}}}[{\\lambda(1-e^{-\\mu\\delta t})-\\mu(1-e^{-\\lambda\\delta t})}]/ ( { \\lambda-\\mu}),\\\\[6pt ] { \\langle n \\delta w_{\\ln\\rho}\\rangle}_{{\\mathrm{ss}}}=-{\\langle n \\delta w_{\\ln\\mu}\\rangle}_{{\\mathrm{ss}}}={\\langle n\\rangle}_{{\\mathrm{ss}}}(1-e^{-\\mu\\delta t } ) , \\end{array } \\label{eq : santmaccf}\\ ] ] where for notational convenience we consider the sensitivity with respect to the logarithm of the parameter value ( _ e.g. _  @xmath123 ) .    figure",
    "[ fig : corrn ] shows the time correlation functions of eqs .   and , computed over multiple stochastic simulation runs using the ensemble - averaged correlation function method , together with the analytical results of eq .",
    "( solid lines ) .",
    "the agreement between the analytic theory and simulation results is excellent .",
    "the time correlation functions converge to the expected steady state sensitivity coefficients ( horizontal lines in fig .",
    "[ fig : corrn ] ) . for the protein correlation functions ( @xmath124 )",
    ", this occurs on a timescale governed by the relaxation rate of protein number fluctuations @xmath125min , while the mrna correlation functions ( @xmath126 ) reach their asymptotic values on a timescale governed by the mrna decay rate @xmath127min .",
    "figure [ fig : corrn ] ( open circles ) also shows the same correlation functions , computed instead from a single stochastic simulation run , using the time - averaged correlation function method , with time - step pre - averaging .",
    "although this method gives correlation functions ( eqs .   and ) in terms of the number of steps @xmath88 in the history array , rather than the time difference @xmath61 , these can be converted to time correlation functions by multiplying @xmath88 by the expected _ global _ mean time step @xmath128 ( the average over states of the state - dependent mean time step @xmath100 ) .    comparing the results of the ensemble - averaged and time - averaged correlation function methods in fig .",
    "[ fig : corrn ] we see that the two methods give essentially the same results , but the time - averaged method produces greater accuracy ( smaller error bars ) , for the same total number of simulation steps .",
    "moreover , because we have used time - step pre - averaging with the time - averaged correlation function method , each simulation step is computed approximately twice as fast as in the original kinetic monte carlo algorithm , since one does not need to generate random numbers for the time steps @xcite .",
    "we now turn to a more sophisticated case study , based on the stochastic focusing model of paulsson _ et al .",
    "_  @xcite . in this biochemical network ,",
    "a input signal molecule s downregulates the production of an output signal ( or response ) molecule r. stochastic fluctuations play a crucial role , making the output much more sensitive to changes in the input than would be predicted by a deterministic ( mean - field ) model .    our reaction scheme , given in eq .",
    ", contains just two chemical species , s and r. the production and degradation of s ( the input signal ) are straightforward poisson processes with rates @xmath129 and @xmath130 respectively . the production of r ( the output signal )",
    "is negatively regulated by s , and its degradation rate is set to unity to fix the time scale .",
    "thus we have @xmath131 where we use a michaelis - menten - like form to represent the negative regulation : @xmath132 , with @xmath133 being the copy number of the input signal molecule . taking a mean - field approach",
    ", we might suppose that the average copy numbers @xmath134 and @xmath135 should obey the chemical rate equations @xmath136 and that therefore the steady state copy numbers should be given by @xmath137 in reality , while eq .   gives the correct result for the mean input signal @xmath138 , it is manifestly _ incorrect _ for the mean output signal @xmath139 .",
    "for example for @xmath140 we find from kinetic monte carlo simulations @xmath141 , as predicted by eq .  , but @xmath142 , whereas eq .",
    "predicts @xmath143 .",
    "this failure of the mean - field prediction arises because of the non - linearity of the michaelis - menten - like form of the production propensity for r.    , where @xmath129 is varied to control @xmath138 , with other parameters as in eq .  .",
    "open circles are gillespie simulations using the time - average correlation function method with time step pre - averaging ; error bars are estimated by averaging over 10 trajectories of length @xmath144 steps .",
    "the history array length was @xmath145 .",
    "the filled circle ( blue ) is from a gibson - bruck simulation in copasi  using the ensemble - average correlation function method ; error bars are from block averaging ( 10 blocks of @xmath104 samples ) .",
    "the thick solid line ( red ) is the numerical result from the finite state projection ( fsp ) algorithm .",
    "the dashed line is the mean - field theory ( mft ) prediction.[fig : stochf ] ]    our aim is to compute the _ differential gain _ ,",
    "@xmath146 which describes the local steepness of the signal - response relation ( @xmath147 ) .",
    "the gain measures the sensitivity of the system s output @xmath139 to its input @xmath138 ; this can be computed by measuring the sensitivities of @xmath139 and @xmath138 to the production and degradation rates of the signal molecule .",
    "let us suppose that the signal @xmath138 is varied by changing its production rate @xmath129 infinitesimally at fixed degradation rate @xmath130 ( we could have chosen instead to vary @xmath130 or , in principle , both @xmath129 and @xmath130 ) .",
    "the gain is then @xmath148 where the second equality follows from the fact that @xmath149 , since @xmath150 .",
    "we use the methods described in section [ sec : ss ] to compute the steady - state sensitivity @xmath151 , and hence the gain @xmath152 .",
    "figure [ fig : stochf ] shows the absolute differential gain @xmath153 computed using the time - averaged correlation function method , with time - step pre - averaging , as a function of the signal strength @xmath138 , as @xmath129 is varied ( note that in this region the actual gain is negative so @xmath154 ) .",
    "the results are in excellent agreement with the finite state projection method ( fsp , see supplementary material section [ sec : fsp ] ) .",
    "[ fig : stochf ] ( dashed line ) also shows the mean - field theory prediction derived from the second of eqs .",
    ", namely @xmath155 .",
    "stochastic focusing , as predicted by paulsson _",
    "et al _ @xcite , is clearly evident : the gain is much greater in magnitude for the stochastic model than the mean - field theory predicts , implying that fluctuations greatly increase the sensitivity of the output signal to the input signal @xcite .",
    "in this example , the parameter of interest ( @xmath129 ) is the rate constant of a single reaction ( production of s ) . as discussed in section [ sec : tw ] , this implies that the parameter sensitivity can be computed simply by counting the number of times this reaction is visited , which can be achieved by modifying the reaction scheme to @xmath156 then computing the weight function @xmath157\\ , .",
    "\\label{eq : qtrick}\\ ] ] ( which is the analogue of eq .  ) , and using this to obtain the relevant time - correlation functions .",
    "this requires no changes to the simulation algorithm , making it easy to use with existing software packages . as a demonstration",
    ", we computed the differential gain for the parameters in eq .",
    ", using the open source simulation package copasi  @xcite . to achieve this , we used the gibson - bruck algorithm ( as implemented in copasi ) to generate samples of @xmath158 and @xmath159 at equi - spaced time points with a spacing @xmath160 time units ( chosen to be longer than the expected relaxation time of the output signal , set by the decay constant for r ) . by taking the difference between successive time points",
    "we compute @xmath161 and hence the correlation function defined in eq .  . the result , shown in blue in fig .",
    "[ fig : stochf ] , is in good agreement with our other calculations .     and",
    "@xmath162 in the gardner _ et al .",
    "_  genetic switch model ( section [ sec : gardsw ] ) .",
    "parameters are @xmath163 , @xmath164 , @xmath165 , and @xmath166.[fig : gardsw_cps ] ]     in the gardner _",
    "_  switch , and the sensitivity to @xmath167 .",
    "points ( with small error bars in the case of the sensitivity ) are from gillespie simulations using the time - average correlation function method with time step pre - averaging ; error bars are estimated by averaging over 10 trajectories of @xmath107 steps .",
    "the history array length was @xmath168 .",
    "the solid lines ( red ) are the results of fsp applied to this problem .",
    "model parameters are as in fig .",
    "[ fig : gardsw_cps].[fig : gardsw_op ] ]      as a final example , we consider a model for a bistable genetic switch , of the type constructed experimentally by gardner _ et al . _",
    "@xcite , in which two proteins u and v mutually repress each other s production . we suppose that transcription factor binding to the operator is cooperative , and can be described by a hill function ; the rate of production of protein u is then given by @xmath169 while the rate of production of v is given by @xmath170 ( here @xmath171 and @xmath172 describe the maximal production rates while @xmath167 and @xmath116 are the hill exponents , describing the degree of cooperativity ) .",
    "the units of time are fixed by setting the degradation rates of u and v to unity . for a suitable choice of parameter values , stochastic simulations of this model show switching between a u - rich state and a v - rich state , as illustrated in fig .",
    "[ fig : gardsw_cps ] ; the steady - state probability distribution @xmath173 for the quantity @xmath174 is bimodal , as shown in fig .",
    "[ fig : gardsw_op ] .",
    "we use this example to illustrate the computation of parameter sensitivities for more complicated scenarios where the system property of interest is not simply a mean copy number and the parameter of interest is not a simple rate constant .",
    "in particular we compute the sensitivity of the steady - state probability distribution @xmath173 to the hill exponent @xmath167 .",
    "our model consists of the following reaction scheme : @xmath175 in which proteins u and v are created and destroyed with propensities given by : @xmath176 let us first suppose we wish to compute @xmath177 using the time - averaged correlation function method , without time step pre - averaging .",
    "we use the propensity functions in eqs .   to run a standard kinetic monte carlo ( gillespie ) simulation , choosing at each step a next reaction and a time step @xmath20 . at each simulation step",
    ", we also compute the quantity @xmath178 and update the weight function @xmath179 according to eq .  , _",
    "i.e. _  if reaction 1 is chosen as the next reaction , we increment @xmath179 by @xmath180 , otherwise , we increment @xmath179 by @xmath181 ( note that it is correctly @xmath182 that features in this , irrespective of the chosen next reaction ) .",
    "we keep track not only of the current value of @xmath179 , but also of its value a fixed number @xmath88 steps ago . at the same time , we keep track of the function of interest ( denoted @xmath0 in sections [ sec : tw ] and [ sec : ss ] ) . because we are computing the parameter sensitivity of the _ distribution _",
    "@xmath173 , we have a function @xmath183 , and a time - correlation function @xmath184 , for _ each _ value of @xmath185 . at each simulation step",
    ", we check the current value of @xmath186 .",
    "the function @xmath183 is unity if @xmath187 and zero otherwise ( _ i.e. _   @xmath188 ) . for each value of @xmath185",
    ", we then compute the time correlation function @xmath184 as prescribed in eq .  .",
    "as long as @xmath189 ( where @xmath190 is the global average time step ) is longer than the typical relaxation time of the system , @xmath184 should give a good estimate for @xmath177 .",
    "if we are instead using time step pre - averaging , we employ a slight modification of the above procedure . at each simulation step , we choose a next reaction , but we do not choose a time step @xmath191 . in our update rules for @xmath179 , we replace @xmath20 by the state - dependent mean timestep @xmath100 where @xmath192 . as well as keeping track of @xmath179 and @xmath183 we also need to compute at each step @xmath193 this quantity is then used to compute @xmath184 and hence @xmath177 using the modified algorithm given in supplementary material section [ sec : preav ] .",
    "an important technical point here concerns the relaxation time of the system , or the number of steps @xmath88 over which we need to remember the system s history in order that the correlation function @xmath194 gives a good estimate of the steady - state parameter sensitivity . for the previous examples studied , this timescale was given by the slowest decay rate ( typically that of the protein molecules ) .",
    "the genetic switch , however , shows dynamical switching behaviour on a timescale that is much longer than the protein decay rate ( see for example fig .",
    "[ fig : gardsw_cps ] ) .",
    "we therefore need to choose a value of @xmath88 such that @xmath189 is longer than the typical switching time .",
    "kinetic monte - carlo simulations ( like those in fig .",
    "[ fig : gardsw_cps ] ) show that for our model , the typical time between switching events is approximately 160 time units , while the global average time step @xmath195 .",
    "the typical number of steps per switching event is therefore @xmath196 .",
    "our chosen value of @xmath88 should be at least this large . in practice",
    "we find that the correlation functions are fully converged ( to within a reasonable accuracy ) by @xmath197 steps ( @xmath198 switching events ) , but not quite converged by @xmath199 steps ( @xmath200 switching events ) .",
    "these lengthy convergence times mean that much longer simulations are needed to obtain good statistical estimates for the parameter sensitivity in this model than in the previous examples .",
    "figure [ fig : gardsw_op ] shows the steady state probability distribution @xmath173 together with its sensitivity @xmath177 , computed using the time - averaged correlation function method with time step pre - averaging , for the same parameters as in fig .  [ fig : gardsw_cps ] .",
    "this method gives results in excellent agreement with fsp .",
    "@xmath173 has the bimodal shape typical of a stochastic genetic switch , with a large peak at @xmath201 and a much broader peak around @xmath202 , with a minimum around @xmath203 .",
    "the sensitivity coefficient @xmath204 measures how the behaviour of the switch depends on the cooperativity @xmath167 of binding of the transcription factor v. we see that increasing @xmath167 leads to an increased peak at @xmath205 , and a decreased peak at @xmath206 , in other words the switch spends more time in the v - rich state .",
    "also the minimum around @xmath203 decreases , suggesting that the switching frequency decreases as @xmath167 increases .",
    "this is confirmed by further study using the ensemble - averaged correlation function method of the sensitivity coefficient of the switching frequency to changes in @xmath167 ; the details of this will be presented elsewhere .",
    "in this paper , we have shown how trajectory reweighting can be used to compute parameter sensitivity coefficients in stochastic simulations without the need for repeated simulations with perturbed values of the parameters .",
    "the methods presented here are simple to implement in standard kinetic monte carlo ( gillespie ) simulation algorithms and in some cases can be used without any changes to the simulation code , making them compatible with packages such as copasi  @xcite . for computation of time - dependent sensitivity coefficients ,",
    "the method involves tracking a weight function ( which depends on the derivative of the propensities with respect to the parameter of interest ) and computing its covariance with the system property of interest , at the time of interest , across multiple simulations . for computing time - independent steady - state parameter sensitivities ,",
    "we show that the sensitivity coefficient can be obtained as the long - time limit of a time correlation function , which can be computed either across multiple simulations ( ensemble - averaged correlation function method ) , or as a time average in a single simulation run ( time - averaged correlation function method ) .",
    "we further show that time step pre - averaging removes the need to choose a new time step at each simulation step , significantly improving computational efficiency . in either the time - dependent or the time - independent case ,",
    "it is a trivial matter to compute multiple sensitivity coefficients ( e.g. with respect to different parameters ) at the same time  one simply tracks each of the corresponding weight functions simultaneously .    in deterministic models ,",
    "parameter sensitivity coefficients can be computed by simultaneous integration of a set of _ adjunct _ odes , alongside the set of odes describing the model ( see eq .  ) .",
    "we consider the trajectory reweighting approach described here to be the exact stochastic analogue of the adjunct ode method ; the integration of the adjunct odes alongside the original odes is directly analogous to the procedure of generating a trajectory weight alongside the normal trajectory in a kinetic monte - carlo scheme . indeed",
    ", one can derive an _",
    "adjunct chemical master equation _ by taking the derivative of the chemical master equation with respect to the parameter of interest ; it turns out that the trajectory reweighting scheme is essentially a stochastic solution method for the adjunct master equation @xcite .    in section [ sec : stochf ] , we demonstrated the use of trajectory reweighting to compute parameter sensitivities , and hence the differential gain , for a model of a stochastic signaling network .",
    "we believe that this approach has widespread potential application to signaling pathways , because it can be implemented for any existing model without any modifications to the underlying kinetic monte - carlo simulation code . as long as a stochastic input signal is generated by a process @xmath207 , one can use the ghost particle trick to compute the sensitivity of any quantity of interest to the input signal ( controlled by varying the rate @xmath129 of the signal production reaction ) by modifying the production step to @xmath208 , computing the weight function from @xmath209 ( see eq .  ) and computing the appropriate correlation function for its covariance with the system property of interest . as proof - of - principle we calculated the differential gain for the model in section [ sec : stochf ] ( see fig .",
    "[ fig : stochf ] ) , using copasi  @xcite to generate the simulation data , and a standard spreadsheet package to compute the correlation function .    in section [ sec : gardsw ]",
    ", we used the methodology to compute the sensitivity of the probability distribution function for a bistable genetic switch , to the degree of cooperativity ( hill exponent ) of binding of one of its transcription factors .",
    "this example demonstrates that trajectory reweighting is not a panacea for all problems .",
    "the bistable genetic switch has a long relaxation time , which requires the correlation function of the weight to the computed over long times , with a corresponding need for large sample sizes to obtain good statistical sampling . while trajectory reweighting works for this example , preliminary attempts to compute the parameter dependence of the switching rate show that finite differencing may be more efficient .",
    "in fact plyasunov and arkin @xcite already discuss in which cases it may be more efficient to use finite - differencing . because of their long relaxation times , genetic switches are notoriously difficult to study in stochastic simulations @xcite . a plethora of sophisticated schemes have been developed to address this problem @xcite , some of which could perhaps be extended to incorporate trajectory reweighting .",
    "the present study considers how to compute parameter sensitivity coefficients_i.e .",
    "_  first derivatives of system properties with respect to the parameters .",
    "the same approach can , however , easily be used to compute higher derivatives , such as the hessian matrix , as discussed in supplementary material section [ sec : rat ] .",
    "this raises the possibility of combining the present methods with gradient - based search algorithms , to make a sophisticated _",
    "parameter estimation _",
    "algorithm for stochastic modeling .",
    "this would offer a novel approach to a major class of problems in systems biology .    to summarise",
    ", we believe the trajectory reweighting schemes presented here are an important and useful addition to the stochastic simulation toolbox .",
    "further research should address in detail their performance with respect to existing methods @xcite and their application to challenging models such as those with long relaxation times , as well as their potential for use in more sophisticated parameter search algorithms .",
    "the authors thank mustafa khammash for detailed advice about the fsp method and assistance with its implementation .",
    "rja was supported by a royal society university research fellowship .",
    "the collaboration leading to this work was facilitated by the stomp research network under bbsrc grant bb / f00379x/1 and by the e - science institute under theme 14 : `` modelling and microbiology '' .",
    "23 natexlab#1#1bibnamefont # 1#1bibfnamefont # 1#1citenamefont # 1#1url # 1`#1`urlprefix[2]#2 [ 2][]#2    , _ _ ( , , ) .    , *",
    "* , ( ) .    , * * , ( ) .",
    ", , , * * , ( ) .    , * * , ( ) .    , , , * * , ( ) .    , * * , ( ) .    , * * , ( ) .    , , , * * , ( ) .    , , _ _ ( , , )    , , , , * * , ( ) .    , _ et al . _ , * * , ( ) .    ,",
    "* * , ( ) .    ,",
    "* * , ( ) .",
    ", , , * * , ( ) .    for this model",
    "we find that it does not make any detectable difference whether @xmath152 is computed at fixed @xmath130 ( as here ) or fixed @xmath129 .",
    "this is almost certainly not generally true , and in the present case is likely connected to the fact that the noise in s is uncorrelated with the noise in p , in the sense of , , , * * , ( ) .",
    ", , , * * , ( ) .    , , , * * , ( ) .",
    ", , , * * , ( ) .    , , , * * , ( ) .    , * * , ( ) .    , , , * * , ( ) .    ,",
    "* * , ( ) .",
    "here , we present a convenient way to compute the derivatives of average quantities with respect to the parameters of the model , that are required to arrive at eqs .   and in the main text .",
    "we also show that this method generalizes easily to higher derivatives .    noting that in the perturbed system the parameter @xmath2 has been changed to @xmath210 , we use eq .   in the main text to write the average of the function @xmath0 in the perturbed system as @xmath211 where @xmath212 the function @xmath213 has the property that @xmath214 where @xmath215 .",
    "we then have @xmath216 taking the limit @xmath217 ( for an infinitesimal perturbation ) , and noting thereby that @xmath218 and @xmath219 , yields eqs .   and in the main text .",
    "taking this procedure further allows the computation of higher derivatives ; one can show for instance that the hessian is @xmath220 { } \\hspace{10em } - { \\langle f\\rangle}{\\langle   w_{k_\\alpha}w_{k_\\beta}\\rangle } - { \\langle f\\rangle}{\\langle   w_{k_\\alpha k_\\beta}\\rangle } + 2{\\langle f\\rangle}{\\langle   w_{k_\\alpha}\\rangle}{\\langle w_{k_\\beta}\\rangle}\\ , .",
    "\\end{array } \\label{eq : fab}\\ ] ] where @xmath221 eq .   is potentially useful for gradient search algorithms .",
    "this expression is likely to simplify in many cases  for instance we expect that @xmath222 often vanishes for @xmath223 .",
    "one might also use the fact that @xmath224 , but it may improve the statistical sampling to retain these terms ( see discussion in main text ) .",
    "in this section we describe some exact results that can be obtained for models with linear propensity functions , in particular for the correlation functions defined in eqs .   and in the main text .",
    "the analysis draws heavily on established literature results ( which we summarize below ) .",
    "more details and links to earlier literature can be found in the appendix to supplementary ref .  .    to fix notation ,",
    "let us suppose that the @xmath1-th propensity function depends linearly on the copy numbers @xmath225 , namely @xmath226 where @xmath227 and @xmath228 are constants which we assume to be proportional to the rate consant @xmath2 .",
    "our aim is to compute the sensitivity coefficients @xmath229 .",
    "it is well known that for linear propensity functions the moment equations close successively .",
    "thus , the mean copy numbers @xmath230 obey @xmath231 where @xmath232 is the stoichiometry matrix ( describing the change in the copy number of the @xmath16-th species due to the firing of the @xmath1-th reaction ) , @xmath233 and @xmath234 .",
    "note that @xmath235 is usually asymmetric .",
    "for the second moments , the variance - covariance matrix @xmath236 , where @xmath237 , obeys @xmath238 where @xmath239 note that @xmath240 is symmetric . finally the time - ordered two - point correlation functions @xmath241 with @xmath242 obey a regression theorem @xmath243 this concludes our survey of the established literature results .     in the main text.[fig :",
    "dwsq ] ]    we now employ the ghost particle trick of section [ sec : tw ] in the main text , and suppose that reaction @xmath1 now creates a noninteracting species @xmath53 , in addition to its usual products .",
    "following eq .  , the mean copy number @xmath244 obeys @xmath245 for the second moments , eq .",
    "becomes @xmath246 where @xmath247 .",
    "the second term in this can be rewritten as @xmath248 the stoichiometry matrix entry for @xmath53 consists of a ` @xmath249 ' for the @xmath1-th reaction , and zero elsewhere , so that eq .",
    "becomes @xmath250 .",
    "finally we have @xmath251 by similar argumentation we also obtain @xmath252 where @xmath253 .",
    "armed with these results , let us now turn to the problem of computing the weight function @xmath41 .",
    "we write the continuous - time analogue of eq .   in the main text :",
    "@xmath254 ( note that by substituting eq .   into eq .",
    "we recover our previous observation that @xmath48 ) . again writing @xmath255",
    ", it follows from eq .",
    "that @xmath256 thus , now noting explicitly the time - dependence of the various terms , we obtain @xmath257 exploiting the linearity of the propensity functions , the regression theorem implies @xmath258 hence @xmath259 eliminating the time integrals between this and eq .  , we get @xmath260 eliminating @xmath261 between this and eq .   gives finally @xmath262 this is an ode which give the evolution of @xmath263 in terms of known quantities .",
    "it can be compared with the adjunct ode that is obtained by differentiating eq .   with respect to @xmath264 .",
    "the two odes are identical and share the same initial conditions . for this specific case , this is a direct proof of the general result in the main text , namely that @xmath265 .",
    "whilst this is interesting , it is not quite what we are after , which is a theory for the correlation functions defined in the main text . to find this we first note a generalisation of the regression theorem is @xmath266 subtracting this from eq .",
    "generates a set of coupled odes for the correlation functions @xmath267 , which should be solved with the initial conditions @xmath268 at @xmath269 . in steady state",
    "these odes are @xmath270 the initial conditions are @xmath271 .",
    "this is the key result of this section , as in principle it allows for explicit calculation of the correlation functions . comparing to the adjunct ode obtained by differentiating eq .   with respect to @xmath264 , we see that for this case we also have a direct proof of the general result claimed in the main text , that @xmath272 as @xmath66 . for the constitutive gene expression model in section",
    "[ sec : santmac ] in the main text , we solved eq .   to obtain the results given in eq .   in the main text .",
    "to complete the general discussion here , let us derive an expression for the variance of @xmath273 . from the definition in eq .",
    "we have @xmath274 thus @xmath275 the last two terms in this cancel , on account of the regression theorem .",
    "further cancellations occur when eq .   for @xmath276",
    "is inserted , giving finally @xmath277 . since @xmath278 , and @xmath279 and @xmath273",
    "are uncorrelated , it follows that @xmath280 .",
    "integrating @xmath277 and inserting in this last expression gives @xmath281 .",
    "as a particular case , in steady state , @xmath282 .",
    "thus we do indeed see that in steady state @xmath273 has a zero mean and a variance that grows linearly in time , justifying our claim that it behaves essentially like a random walk . some results confirming this analysis",
    "are shown in fig .",
    "[ fig : dwsq ] .",
    "in the time step pre - averaging approach , we do not select time steps as part of our kinetic monte carlo algorithm , but instead use the state - dependent average time step @xmath97 in our expression for the time average of quantity @xmath0 , as in eq .   in the main text .",
    "for the purposes of this section , we define a new notation : @xmath283 in which the sum is over the values of the system function @xmath0 multiplied by the ( state - dependent ) mean time step , computed at each step along a kinetic monte - carlo trajectory of length @xmath284 .",
    "note that @xmath285 can be computed using an algorithm that does not keep track of time but only of the choice of reaction channel .",
    "we can then rewrite eq .   in the main text as @xmath286 when using time step pre - averaging , the correlation function eq .",
    "in the main text must be modified because the relative probability of generating a given sequence of states ( eq .   in the main text )",
    "takes a different form when the algorithm does not keep track of time , and because the average time step @xmath100 in eq .",
    "itself usually depends on the parameter in question .    in a kinetic monte carlo scheme in which the next reaction is selected as normal , but time is not tracked ,",
    "the probability of generating a given trajectory is proportional to @xmath287 ( i.e. the part of eq .   in the main text concerning the time step distribution",
    "is discarded ) . in analogy to eq .",
    "in the main text , one can write the average @xmath288 for the perturbed problem in terms of averages over unperturbed trajectories : @xmath289 taking derivatives of eq .   with respect to the parameter @xmath2 as described in section [ sec : rat ] above , it follows that @xmath290 where the fact that @xmath100 depends on @xmath2 leads to an extra term ( the first term ) in comparison to eq .   in the main text .",
    "computing @xmath291 using eq .  , it turns out that @xmath292 has the same form as before , but with @xmath20 replaced by @xmath100 , @xmath293 since the weight function in eq",
    ".   behaves like a random walk , steady - state parameter sensitivities should be computed using the correlation function trick ( as in section [ sec : ss ] in the main text ) . from eq .",
    "we have : @xmath294 where @xmath295 with @xmath296 given by eq .   in the main text , using the present eq .   to generate @xmath41",
    "finally , the parameter sensitivity coefficient itself is given by differentiating eq .",
    ", @xmath297 the quantity @xmath298 in the second term is given by setting @xmath299 in eqs .   and . when using time step pre - averaging in combination with the time - averaged correlation function method , one computes the parameter sensitivity coefficient using eq .   rather than simply taking the limit of the correlation function as @xmath300",
    "we note that while eq .",
    "looks formidable , its actual computation is fairly straightforward . to obtain both @xmath301 and @xmath302 ,",
    "one computes trajectory averages of the set of quantities defined by @xmath303 , together with @xmath304 and @xmath305 .",
    "these averages are calculated by summing the respective quantities along the trajectory and dividing by the number of steps .",
    "the master equation describes the evolution of the probability @xmath306 that a system is in the state @xmath15 at time @xmath29 . for the sake of compactness",
    "we will adopt the notation @xmath307 and @xmath308 for the states @xmath15 and @xmath309 respectively .",
    "the master equation is @xcite @xmath310 \\label{eq : master}\\ ] ] where @xmath311 is the transition rate from @xmath307 to @xmath308 , given by @xmath312 the finite state projection ( fsp ) algorithm is a numerical solution scheme for the master equation based on the idea of truncating the state space . for full details of the original fsp algorithm",
    "we refer to the work of munksy and khammash @xcite . here",
    "we outline the basic principles of the scheme and the small changes needed to adapt it to the computation of steady - state sensitivity coefficients .",
    "the starting point is to note that eq .",
    "is a _ linear _",
    "ode for @xmath26 , and may be written in the matrix form @xmath313 where @xmath314 is an infinite - dimensional sparse matrix . to make this into a tractable numerical proposition , the fsp algorithm truncates the state space to a finite size @xmath315 .",
    "the truncation is chosen so as to contain almost all of the probability @xmath316 under the conditions of interest .",
    "for the problems encountered here , a ( hyper-)rectangular truncation scheme works , @xmath317 , for which @xmath318 where @xmath319 .",
    "the question then is how to handle the states _ not _ included in the truncation scheme . in the original fsp algorithm",
    "the extra states are lumped together into a single meta - state .",
    "all the transitions leaving the truncated state space are connected to this new meta - state , and all the transitions entering the truncated state space are discarded . with this approximation @xmath314",
    "becomes a @xmath320 sparse matrix , and one can use standard numerical methods to exponentiate the matrix and advance the probability distribution , _ i.e.",
    "_  @xmath321 .",
    "the advantage of introducing the meta - state is that munsky and khammash can prove some sophisticated truncation theorems which provide a certificate of accuracy for the scheme .    for the present problem we are interested in the steady state probability distribution @xmath322 .",
    "however the meta - state is an absorbing state , which frustrates the direct computation of @xmath322 . to avoid this , we discard _ all _ transitions which leave or enter the truncated state space whilst , obviously , retaining all the transitions contained entirely within the truncated state space",
    "the meta - state is then no longer needed and @xmath314 becomes a @xmath323 sparse matrix .",
    "the steady state distribution is found by solving @xmath324 , in other words @xmath322 is the right - eigenvector of @xmath314 belonging to eigenvalue zero . that such an eigenvector exists is a textbook argument @xcite : conservation of probability , @xmath325 , implies @xmath326 and hence @xmath327 where @xmath328 is a row - vector with entries all equal to unity . since",
    "therefore @xmath328 is a _",
    "left_-eigenvector of @xmath314 with eigenvalue zero , it follows under mild and non - restrictive conditions @xcite that there is a corresponding _ right_-eigenvector of @xmath314 with the same eigenvalue .",
    "this is the desired steady state probability distribution .",
    "well - established numerical methods exist to obtain the eigenvectors of a sparse matrix . for the present problems we have used the functionality provided in mathworks matlab . for an open - source solution , we have also had good success with the octave  interface to arpack  which implements an implicitly restarted arnoldi method @xcite . from a practical point of view , we find we are limited to truncated state spaces of size @xmath329 for matlab , and somewhat smaller for the octave  interface to arpack .",
    "this effectively limits consideration to problems involving at most two state variables @xmath15 ( @xmath330 ) and motivates the choice of examples in the main text .",
    "once @xmath322 is found we can calculate @xmath331 .",
    "sensitivity coefficients like @xmath332 are then found by solving the fsp problem at @xmath2 and @xmath333 and using eq .   in the main text , with @xmath334 typically being a few percent of @xmath2 .",
    "note that although the master equation describes a stochastic process , it is itself a _ deterministic _ ode .",
    "hence this method of computating sensitivity coefficients by finite differencing is appropriate . in the absence of truncation theorems",
    ", convergence is verified empirically ."
  ],
  "abstract_text": [
    "<S> parameter sensitivity analysis is a powerful tool in the building and analysis of biochemical network models . for stochastic simulations , </S>",
    "<S> parameter sensitivity analysis can be computationally expensive , requiring multiple simulations for perturbed values of the parameters . here , we use trajectory reweighting to derive a method for computing sensitivity coefficients in stochastic simulations without explicitly perturbing the parameter values , avoiding the need for repeated simulations . </S>",
    "<S> the method allows the simultaneous computation of multiple sensitivity coefficients . </S>",
    "<S> our approach recovers results originally obtained by application of the girsanov measure transform in the general theory of stochastic processes [ a. plyasunov and a.  p. arkin , j. comp . </S>",
    "<S> phys . * 221 * , 724 ( 2007 ) ] . </S>",
    "<S> we build on these results to show how the method can be used to compute steady - state sensitivity coefficients from a single simulation run , and we present various efficiency improvements . for models of biochemical signaling networks </S>",
    "<S> the method has a particularly simple implementation . </S>",
    "<S> we demonstrate its application to a signaling network showing stochastic focussing and to a bistable genetic switch , and present exact results for models with linear propensity functions . </S>"
  ]
}