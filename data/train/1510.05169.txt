{
  "article_text": [
    "saddle - point problems arise in constrained optimization via the lagrangian formulation and , more generally , are equivalent to variational inequality problems .",
    "these formulations find applications in cooperative control of multi - agent systems , in machine learning and game theory , and in equilibrium problems in networked systems , motivating the study of distributed strategies that are guaranteed to converge , scale well with the number of agents , and are robust against a variety of failures and uncertainties .",
    "our objective in this paper is to design and analyze distributed algorithms to solve general convex - concave saddle - point problems .",
    "this work builds on three related areas : iterative methods for saddle - point problems  @xcite , dual decompositions for constrained optimization  ( * ? ? ?",
    "5 ) , @xcite , and consensus - based distributed optimization algorithms ; see , e.g. ,  @xcite and references therein .",
    "historically , these fields have been driven by the need of solving constrained optimization problems and by an effort of parallelizing the computations  @xcite , leading to consensus approaches that allow different processors with local memories to update the same components of a vector by averaging their estimates .",
    "saddle - point or min - max problems arise in optimization contexts such as worst - case design , exact penalty functions , duality theory , and zero - sum games , see e.g.  @xcite , and are equivalent to the variational inequality framework  @xcite , which includes as particular cases constrained optimization and many other equilibrium models relevant to networked systems , including traffic  @xcite and supply chain  @xcite . in a centralized scenario , the work  @xcite studies iterative subgradient methods to find saddle points of a lagrangian function and establishes convergence to an arbitrarily small neighborhood depending on the gradient stepsize . along these lines",
    ",  @xcite presents an analysis for general convex - concave functions and studies the evaluation error of the running time - averages , showing convergence to an arbitrarily small neighborhood assuming boundedness of the estimates . in  @xcite",
    ", the boundedness of the estimates in the case of lagrangians is achieved using a truncated projection onto a closed set that preserves the optimal dual set , which  @xcite shows to be bounded when the strong slater condition holds .",
    "this bound on the lagrange multipliers depends on global information and hence must be known beforehand .",
    "dual decomposition methods for constrained optimization are the melting pot where saddle - point approaches come together with methods for parallelizing computations , like the alternating direction method of multipliers  @xcite .",
    "these methods rely on a particular approach to split a sum of convex objectives by introducing agreement constraints on copies of the primal variable , leading to distributed strategies such as distributed primal - dual subgradient methods  @xcite where the vector of lagrange multipliers associated with the laplacian s nullspace is updated by the agents using local communication .",
    "ultimately , these methods allow to distribute global constraints that are sums of convex functions via _ agreement on the multipliers _  @xcite . regarding distributed constrained optimization , we highlight two categories of constraints that determine the technical analysis and the applications : the first type concerns a",
    "_ global _ decision vector in which agents need to agree , see , e.g. ,  @xcite , where all the agents know the constraint , or see , e.g. ,  @xcite , where the constraint is given by the intersection of abstract closed convex sets . the second type _",
    "couples _ _ local _ decision vectors across the network , and is addressed by  @xcite with linear equality constraints , by  @xcite with linear inequalities , by  @xcite with inequalities given by the sum of convex functions on local decision vectors , where each one is only known to the corresponding agent , and by  @xcite with semidefinite constraints .",
    "the work  @xcite considers a distinction , that we also adopt here , between _ constraint graph _",
    "( where edges arise from participation in a constraint ) and _ communication graph _ , generalizing other paradigms where each agent needs to communicate with all other agents involved in a particular constraint  @xcite .",
    "when applied to distributed optimization , our work considers both kinds of constraints , and along with  @xcite , has the crucial feature that agents participating in the same constraint are able to coordinate their decisions without direct communication .",
    "this approach has been successfully applied to control of camera networks  @xcite and decomposable semidefinite programs  @xcite .",
    "this is possible using a strategy that allows an agreement condition to play an independent role on a subset of _ both _ primal and dual variables .",
    "our novel contribution tackles these constraints from a more general perspective , namely , we provide a multi - agent distributed approach for the _ general saddle - point problems _ under an additional agreement condition on a subset of the variables of both the convex and concave parts .",
    "we do this by combining the saddle - point subgradient methods in  ( * ? ? ?",
    "3 ) and the kind of linear proportional feedback on the disagreement typical of consensus - based approaches , see e.g. ,  @xcite , in distributed convex optimization .",
    "the resulting family of algorithms solve more general saddle - point problems than existing algorithms in the literature in a decentralized way , and also particularize to a novel class of primal - dual consensus - based subgradient methods when _ the convex - concave _ function is the _",
    "lagrangian _ of the minimization of a sum of convex functions under a constraint of the same form . in this particular case ,",
    "the recent work  @xcite uses primal - dual perturbed methods which enhance subgradient algorithms by evaluating the latter at precomputed arguments called _",
    "perturbation points_. these auxiliary computations require additional subgradient methods or proximal methods that add to the computation and the communication complexity .",
    "similarly , the work  @xcite considers primal - dual methods , where each agent performs a minimization of the local component of the lagrangian with respect to its primal variable ( instead of computing a subgradient step ) .",
    "notably , this work makes explicit the treatment of semidefinite constraints .",
    "the work  @xcite applies the _ cutting - plane consensus _ algorithm to the dual optimization problem under linear constraints .",
    "the decentralization feature is the same but the computational complexity of the local problems grows with the number of agents .",
    "the generality of our approach stems from the fact that our saddle - point strategy is applicable beyond the case of lagrangians in constrained optimization .",
    "in fact , we have recently considered in  @xcite distributed optimization problems with nuclear norm regularization via a min - max formulation of the nuclear norm where the convex - concave functions involved have , unlike lagrangians , a quadratic concave part .",
    "we consider general saddle - point problems with explicit agreement constraints on a subset of the arguments of both the convex and concave parts .",
    "these problems appear in dual decompositions of constrained optimization problems , and in other saddle - point problems where the convex - concave functions , unlike lagrangians , are not necessarily linear in the arguments of the concave part .",
    "this is a substantial improvement over prior work that only focuses on dual decompositions of constrained optimization .",
    "when considering constrained optimization problems , the agreement constraints are introduced as an artifact to distribute both primal and dual variables independently .",
    "for instance , separable constraints can be decomposed using agreement on dual variables , while a subset of the primal variables can still be subject to agreement or eliminated through fenchel conjugation ; local constraints can be handled through projections ; and part of the objective can be expressed as a maximization problem in extra variables . driven by these important classes of problems , our main contribution is the design and analysis of distributed coordination algorithms to solve general convex - concave saddle - point problems with agreement constraints , and to do so with subgradient methods , which have less computationally complexity .",
    "the coordination algorithms that we study can be described as projected saddle - point subgradient methods with laplacian averaging , which naturally lend themselves to distributed implementation . for these algorithms",
    "we characterize the asymptotic convergence properties in terms of the network topology and the problem data , and provide the convergence rate .",
    "the technical analysis entails computing bounds on the saddle - point evaluation error in terms of the disagreement , the size of the subgradients , the size of the states of the dynamics , and the subgradient stepsizes . finally , under assumptions on the boundedness of the estimates and the subgradients , we further bound the cumulative disagreement under joint connectivity of the communication graphs , regardless of the interleaved projections , and make a choice of decreasing stepsizes that guarantees convergence of the evaluation error as  @xmath0 , where @xmath1 is the iteration step .",
    "we particularize our results to the case of distributed constrained optimization with objectives and constraints that are a sum of convex functions coupling local decision vectors across a network . for this class of problems ,",
    "we also present a distributed strategy that lets the agents compute a bound on the optimal dual set .",
    "this bound enables agents to project the estimates of the multipliers onto a compact set ( thus guaranteeing the boundedness of the states and subgradients of the resulting primal - dual projected subgradient dynamics ) in a way that preserves the optimal dual set .",
    "various simulations illustrate our results .",
    "here we introduce basic notation and notions from graph theory and optimization used throughout the paper .",
    "we denote by  @xmath2 the @xmath3-dimensional euclidean space , by @xmath4 the identity matrix in  @xmath2 , and by  @xmath5 the vector of all ones .",
    "given two vectors , @xmath6 , @xmath7 , we denote by @xmath8 the entry - wise set of inequalities @xmath9 , for each @xmath10 . given a vector @xmath7 ,",
    "we denote its euclidean norm , or two - norm , by @xmath11 and the one - norm by @xmath12 . given a convex set @xmath13 , a function @xmath14 is convex if @xmath15 for all @xmath16 $ ] and @xmath17 .",
    "a vector @xmath18 is a subgradient of @xmath19 at @xmath20 if @xmath21 , for all @xmath22 .",
    "we denote by @xmath23 the set of all such subgradients .",
    "the function @xmath19 is concave if @xmath24 is convex .",
    "a vector @xmath18 is a subgradient of a concave function @xmath19 at @xmath20 if @xmath25 .",
    "given a _ closed _",
    "convex set  @xmath13 , the orthogonal projection @xmath26 onto  @xmath27 is @xmath28 this value exists and is unique .",
    "( note that _ compactness _ could be assumed without loss of generality taking the intersection of @xmath27 with balls centered at @xmath29 . )",
    "we use the following basic property of the orthogonal projection : for every @xmath20 and @xmath30 , @xmath31 for a symmetric matrix @xmath32 , we denote by @xmath33 and @xmath34 its minimum and maximum eigenvalues , and for any matrix @xmath35 , we denote by @xmath36 its maximum singular value .",
    "we use @xmath37 to denote the kronecker product of matrices .",
    "we review basic notions from graph theory following  @xcite .",
    "a ( weighted ) digraph @xmath38 is a triplet where @xmath39 is the vertex set , @xmath40 is the edge set , and @xmath41 is the weighted adjacency matrix with the property that @xmath42 if and only if @xmath43 .",
    "the complete graph is the digraph with edge set @xmath44 .",
    "given @xmath45 and @xmath46 , their union is the digraph @xmath47 .",
    "a path is an ordered sequence of vertices such that any pair of vertices appearing consecutively is an edge .",
    "a digraph is strongly connected if there is a path between any pair of distinct vertices .",
    "a sequence of digraphs @xmath48 is @xmath49-nondegenerate , for @xmath50 , if the weights are uniformly bounded away from zero by @xmath49 whenever positive , i.e. , for each @xmath51 , @xmath52 whenever @xmath53 .",
    "a sequence @xmath54 is @xmath55-jointly connected , for @xmath56 , if for each @xmath57 , the digraph @xmath58 is strongly connected .",
    "the laplacian matrix @xmath59 of a digraph @xmath60 is @xmath61 .",
    "note that @xmath62 . the weighted out - degree and in - degree of @xmath63 are , respectively , @xmath64 and @xmath65 .",
    "a digraph is weight - balanced if @xmath66 for all @xmath67 , that is , @xmath68 . for convenience , we let @xmath69 denote the laplacian of the complete graph with edge weights  @xmath70 .",
    "note that @xmath71 is idempotent , i.e. , @xmath72 .",
    "for the sake of the reader , table  [ tab : matrices - definitions - saddle - methods ] collects some shorthand notation .",
    "@xmath73 & @xmath74 & @xmath75 + @xmath76 & @xmath77 & @xmath78 +      for any function  @xmath79 , the _ max - min inequality _",
    "* sec 5.4.1 ) states that @xmath80 when equality holds , we say that  @xmath81 satisfies the _ strong max - min property _",
    "( also called the _ saddle - point _ property ) .",
    "a point @xmath82 is called a _ saddle point _",
    "if @xmath83 ( * ? ? ?",
    "2.6 ) discusses sufficient conditions to guarantee the existence of saddle points .",
    "note that the existence of saddle points implies the strong max - min property .",
    "given functions @xmath84 ,",
    "@xmath85 and @xmath86 , the _ lagrangian _ for the problem @xmath87 is defined as @xmath88 for @xmath89 . in this case ,",
    "inequality   is called _ weak - duality _ , and if equality holds , then we say that  _ strong - duality _ ( or lagrangian duality ) holds .",
    "if a point @xmath90 is a saddle point for the lagrangian , then @xmath91 solves the constrained minimization problem   and @xmath92 solves the _ dual problem _ , which is maximizing the _ dual function _ @xmath93 over @xmath94 .",
    "this implication is part of the _ saddle point theorem_. ( the reverse implication establishes the existence of a saddle - point and thus strong duality adding a _ constraint qualification _ condition . ) under the saddle - point condition , the _ optimal dual vectors _ @xmath92 coincide with the _ lagrange multipliers _",
    "5.1.4 ) . in the case of affine linear constraints",
    ", the dual function can be written using the _ fenchel conjugate _ of  @xmath19 , defined in @xmath2 as @xmath95",
    "this section describes the problem of interest .",
    "consider closed convex sets @xmath96 , @xmath97 , @xmath98 , @xmath99 and a function @xmath100 which is jointly convex on the first two arguments and jointly concave on the last two arguments .",
    "we seek to solve the constrained saddle - point problem : @xmath101 where @xmath102 and @xmath103 .",
    "the motivation for distributed algorithms and the consideration of explicit agreement constraints in   comes from decentralized or parallel computation approaches in network optimization and machine learning . in such scenarios , global decision variables , which need to be determined from the aggregation of local data ,",
    "can be duplicated into distinct ones so that each agent has its own local version to operate with .",
    "agreement constraints are then imposed across the network to ensure the equivalence to the original optimization problem .",
    "we explain this procedure next , specifically through the dual decomposition of optimization problems where objectives and constraints are a sum of convex functions .",
    "we illustrate here how optimization problems with constraints given by a sum of convex functions can be reformulated in the form   to make them amenable to distributed algorithmic solutions .",
    "our focus are constraints coupling the local decision vectors of agents that can not communicate directly .",
    "consider a group of agents @xmath104 , and let @xmath105 and the components of @xmath106 be convex functions associated to agent @xmath107 .",
    "these functions depend on both a local decision vector @xmath108 , with @xmath109 convex , and on a global decision vector  @xmath110 , with  @xmath97 convex .",
    "the optimization problem reads as @xmath111 this problem can be reformulated as a constrained saddle - point problem as follows .",
    "we first construct the corresponding lagrangian function   and introduce copies @xmath112 of the lagrange multiplier @xmath113 associated to the global constraint in  , then associate each @xmath114 to @xmath115 , and impose the agreement constraint @xmath116 for all @xmath117 , @xmath118 .",
    "similarly , we also introduce copies @xmath119 of the global decision vector @xmath120 subject to agreement , @xmath121 for all @xmath122 .",
    "the existence of a saddle point implies that strong duality is attained and there exists a solution of the optimization  .",
    "formally ,    [ eq : separable - constraints - all ] @xmath123    this formulation has its roots in the classical dual decompositions surveyed in  ( * ? ? ?",
    "2 ) , see also  ( * ? ? ?",
    "1.2.3 ) and  ( * ? ?",
    "5.4 ) for the particular case of resource allocation .",
    "while  @xcite suggest to broadcast a centralized update of the multiplier , and the method in  @xcite has an implicit projection onto the probability simplex , the formulation   has the multiplier associated to the global constraint estimated in a decentralized way .",
    "the recent works  @xcite implicitly rest on the above formulation of _ agreement on the multipliers _",
    "section  [ sec : application - constrained - optimization ] particularizes our general saddle - point strategy to these distributed scenarios .",
    "[ re : fenchel ]    to illustrate the generality of the min - max problem  , we show here how _ only _ the particular case of _ linear _ constraints can be reduced to a maximization problem under agreement .",
    "consider the particular case of @xmath124 , subject to a linear constraint @xmath125 with @xmath126 and @xmath127 .",
    "the above formulation suggests a distributed strategy that _ eliminates _ the primal variables using fenchel conjugates  . taking  @xmath128 such that @xmath129 , this problem can be transformed , if a saddle - point exists ( so that strong duality is attained ) , into    [ eq : fenchel - formulation - distributed ] @xmath130    where @xmath131 is either @xmath132 or @xmath133 depending on whether we have equality or inequality ( @xmath134 ) constraints in  . by  (",
    "11.3 ) , the optimal primal values can be recovered locally as @xmath135 without extra communication .",
    "thus , our strategy generalizes the class of convex optimization problems with linear constraints studied in  @xcite , which distinguishes between the _ constraint graph _",
    "( where edges arise from participation in a constraint ) and the _ network graph _ , and defines _ distributed _ with respect to the latter .",
    "we propose a projected subgradient method to solve constrained saddle - point problems of the form  .",
    "the agreement constraints are addressed via laplacian averaging , allowing the design of distributed algorithms _ when _ the convex - concave functions are separable as in sections  [ sec - linear - semidefinite - constraints - intro ] .",
    "the generality of this dynamics is inherited by the general structure of the convex - concave min - max problem  .",
    "we have chosen this structure both for convenience of analysis , from the perspective of the saddle - point evaluation error , and , more importantly , because it allows to model problems beyond constrained optimization ; see , e.g. ,  @xcite regarding the variational inequality framework , which is equivalent to the saddle - point framework .",
    "formally , the dynamics is    [ eq : minmax - dyn - wnxnzn ] @xmath136    where @xmath137 or @xmath138 , depending on the context , with @xmath139 the laplacian matrix of @xmath140 ; @xmath141 is the consensus stepsize , @xmath142 are the learning rates ; @xmath143 and @xmath144 represents the orthogonal projection onto the closed convex set  @xmath145 as defined in  .",
    "this family of algorithms particularize to a novel class of primal - dual consensus - based subgradient methods _ when _ the convex - concave function takes the lagrangian form discussed in section  [ sec - linear - semidefinite - constraints - intro ] .",
    "in general , the dynamics   goes beyond any specific multi - agent model .",
    "however , when interpreted in this context , the laplacian component corresponds to the model for the interaction among the agents . in the upcoming analysis",
    ", we make network considerations that affect the evolution of  @xmath146 and  @xmath147 , which measure the disagreement among the corresponding components of @xmath148 and @xmath149 via the laplacian of the time - dependent adjacency matrices .",
    "these quantities are amenable for distributed computation , i.e. , the computation of the @xmath117th block requires the blocks @xmath150 and @xmath151 of the network variables corresponding to indexes @xmath118 with @xmath152 . on the other hand , whether the subgradients in   can be computed with _",
    "information _ depends _ on the structure of the function @xmath153 in   in the context of a given networked problem .",
    "since this issue is anecdotal for our analysis , for the sake of generality we consider a general convex - concave function  @xmath153 .",
    "here we present our technical analysis on the convergence properties of the dynamics  .",
    "our starting point is the assumption that a solution to   exists , namely , a saddle point  @xmath154 of  @xmath153 on @xmath145 under the agreement condition on @xmath155 and @xmath156 .",
    "that is , with @xmath157 and @xmath158 for some @xmath159 .",
    "( we can not actually conclude the feasibility property of the original problem _ from _ the evolution of the estimates . )",
    "we then study the evolution of the _ running time - averages _ ( sometimes called _ ergodic sums _ ; see , e.g. ,  @xcite ) @xmath160    we summarize next our overall strategy to provide the reader with a _",
    "roadmap _ of the forthcoming analysis . in section  [ sec : saddle - point evaluation error ] , we bound the saddle - point evaluation error @xmath161 in terms of the following quantities : the initial conditions , the size of the states of the dynamics , the size of the subgradients , and the cumulative disagreement of the running time - averages .",
    "then , in section  [ sec : cum - disagreement ] we bound the cumulative disagreement in terms of the size of the subgradients and the learning rates .",
    "finally , in section  [ sec : proof - main - result - saddle - partial - agreement ] we establish the saddle - point evaluation convergence result using the assumption that the estimates generated by the dynamics  , as well as the subgradient sets , are uniformly bounded .",
    "( this assumption can be met in applications by designing projections that preserve the saddle points , particularly in the case of distributed constrained optimization that we discuss later . ) in our analysis , we conveniently choose the learning rates  @xmath162 using the doubling trick scheme  ( * ? ? ?",
    "2.3.1 ) to find lower and upper bounds on   proportional to  @xmath163 .",
    "dividing by  @xmath1 finally allows us to conclude that the saddle - point evaluation error of the running time - averages is bounded by  @xmath164 .      here",
    ", we establish the saddle - point evaluation error of the running time - averages in terms of the disagreement .",
    "our first result , whose proof is presented in the appendix , establishes a pair of inequalities regarding the evaluation error of the states of the dynamics with respect to a generic point in the variables of the convex and concave parts , respectively .",
    "[ le : basic - bound - convex - concave - function - differences ] let the sequence @xmath165 be generated by the coordination algorithm   over a sequence of arbitrary weight - balanced digraphs  @xmath54 such that @xmath166 , and with @xmath167 then , for any sequence of learning rates @xmath168 and any @xmath169 , the following holds : @xmath170 also , for any @xmath171 , the analogous holds , @xmath172    building on lemma  [ le : basic - bound - convex - concave - function - differences ] , we next obtain bounds for the sum over time of the evaluation errors with respect to a generic point and the running time - averages .",
    "[ le : cummulative - concave - function - differences ] under the same assumptions of lemma  [ le : basic - bound - convex - concave - function - differences ] , for any @xmath173 , the difference @xmath174 is upper - bounded by @xmath175 , while the difference @xmath176 is lower - bounded by @xmath177 , where @xmath178 and @xmath179 .    by adding   over @xmath180",
    ", we obtain @xmath181 this is bounded from above by  @xmath182 because @xmath183 , which follows from the triangular inequality , young s inequality , the sub - multiplicativity of the norm , and the identity @xmath184 .",
    "finally , by the concavity of  @xmath153 in the last two arguments , @xmath185 so the upper bound in the statement follows .",
    "similarly , we obtain the lower bound by adding   over @xmath180 and using that @xmath153 is jointly convex in the first two arguments , @xmath186 which completes the proof .",
    "the combination of the pair of inequalities in lemma  [ le : cummulative - concave - function - differences ] allows us to derive the saddle - point evaluation error of the running time - averages in the next result .",
    "[ prop : approx - saddle - points ] under the same hypotheses of lemma  [ le : basic - bound - convex - concave - function - differences ] , for any saddle point  @xmath154 of  @xmath153 on @xmath187 with @xmath188 and @xmath189 for some @xmath190 , the following holds : @xmath191    we show the result in two steps , by evaluating the bounds from lemma  [ le : cummulative - concave - function - differences ] in two sets of points and combining them .",
    "first , choosing @xmath192 in the bounds of lemma  [ le : cummulative - concave - function - differences ] ; invoking the saddle - point relations @xmath193 where @xmath194 , for each @xmath195 , by convexity ; and combining the resulting inequalities , we obtain @xmath196 choosing @xmath197 in the bounds of lemma  [ le : cummulative - concave - function - differences ] , multiplying each by @xmath198 and combining them , we get @xmath199 the result now follows by summing   and  .",
    "given the dependence of the saddle - point evaluation error obtained in proposition  [ prop : approx - saddle - points ] on the cumulative disagreement of the estimates  @xmath148 and  @xmath149 , here we bound their disagreement over time .",
    "we treat the subgradient terms as perturbations in the dynamics   and study the input - to - state stability properties of the latter .",
    "this approach is well suited for scenarios where the size of the subgradients can be uniformly bounded .",
    "since the coupling in   with  @xmath200 and @xmath201 , as well as among the estimates @xmath148 and  @xmath149 themselves , takes place only through the subgradients , we focus on the following pair of decoupled dynamics ,    [ eq : disagreement - minmax - dyn - wnxnzn ] @xmath202    where @xmath203 , @xmath204 are arbitrary sequences of disturbances , and  @xmath205 is the orthogonal projection onto  @xmath206 as defined in  .",
    "the next result characterizes the input - to - state stability properties of   with respect to the agreement space .",
    "the analysis builds on the proof strategy in our previous work  ( * ? ? ?",
    "v.4 ) . the main trick here is to bound the projection residuals in terms of the disturbance .",
    "the proof is presented in the appendix .    [ prop : cum - disagreement - projected - subg - saddle ] let @xmath207 be a sequence of @xmath55-jointly connected , @xmath49-nondegenerate , weight - balanced digraphs . for @xmath208 , let @xmath209 where @xmath210 then , for any choice of consensus stepsize such that @xmath211 ,    \\end{aligned}\\ ] ] the dynamics   over  @xmath212 is input - to - state stable with respect to the nullspace of the matrix @xmath213 .",
    "specifically , for any @xmath51 and any @xmath214 , @xmath215 where @xmath216 and the cumulative disagreement satisfies @xmath217 analogous bounds hold interchanging  @xmath148 with  @xmath149 .      here we characterize the convergence properties of the dynamics   using the developments above . in informal terms , our main result states that , under a mild connectivity assumption on the communication digraphs , a suitable choice of decreasing stepsizes , and assuming that the agents estimates and the subgradient sets are uniformly bounded , the saddle - point evaluation error under   decreases proportionally to  @xmath218 .",
    "we select the learning rates according to the following scheme .    [",
    "ass : doubling - trick ] the agents define a sequence of epochs numbered by @xmath219 , and then use the constant value @xmath220 in each epoch  @xmath221 , which has @xmath222 time steps @xmath223 .",
    "namely , @xmath224 and so on . in general , @xmath225    note that the agents can compute the values in assumption  [ ass : doubling - trick ] without communicating with each other .",
    "figure  [ fig : doubling - trick ] provides an illustration of this learning rate selection and compares it against constant and other sequences of stepsizes .",
    "note that , unlike other choices commonly used in optimization  @xcite , the doubling trick gives rise to a sequence of stepsizes that is not square summable .     against a constant stepsize , the sequence @xmath226 , and the square - summable harmonic sequence @xmath227 .",
    "]    [ th : convergence - general - saddle - point - w - d - mu - z ] let @xmath165 be generated by   over a sequence @xmath54 of @xmath55-jointly connected , @xmath49-nondegenerate , weight - balanced digraphs satisfying @xmath228 with  @xmath229 selected as in  .",
    "assume @xmath230 for all @xmath51 whenever the sequence of learning rates @xmath142 is uniformly bounded .",
    "similarly , assume @xmath231 for all @xmath51 .",
    "let the learning rates be chosen according to the _ doubling trick _ in assumption  [ ass : doubling - trick ] .",
    "then , for any saddle point  @xmath154 of  @xmath153 on @xmath187 with @xmath188 and @xmath189 for some @xmath190 , which is assumed to exist , the following holds for the running time - averages : @xmath232 where @xmath233 with @xmath234 and @xmath235 is analogously defined .",
    "we divide the proof in two steps . in step  ( i )",
    ", we use the general bound of proposition  [ prop : approx - saddle - points ] making a choice of constant learning rates over a fixed time horizon  @xmath236 . in step",
    "( ii ) , we use multiple times this bound together with the doubling trick to produce the implementation procedure in the statement . in  ( i ) , to further bound  , we choose @xmath237 for all @xmath238 in both  @xmath239 and @xmath240 . by doing this",
    ", we make zero the first two lines in  , and then we upper - bound the remaining terms using the bounds on the estimates and the subgradients .",
    "the resulting inequality also holds replacing @xmath241 by @xmath242 , @xmath243 regarding the bound for @xmath239 , we just note that @xmath244 , whereas for @xmath240 , we note that , by the triangular inequality , we have @xmath245 that is , we get @xmath246 we now further bound @xmath247 in   noting that @xmath248 , to obtain @xmath249 substituting this bound in  , taking @xmath250 and noting that @xmath251 , we get @xmath252 where @xmath253 this bound is of the type @xmath254 , where @xmath255 depends on the initial conditions .",
    "this leads to step  ( ii ) .",
    "according to the doubling trick , for @xmath256 , the dynamics is executed in each epoch of @xmath257 time steps @xmath258 , where at the beginning of each epoch the initial conditions are the final values in the previous epoch .",
    "the bound for @xmath259 in each epoch is @xmath260 , where @xmath261 is the multiplicative constant in   that depends on the initial conditions in the corresponding epoch . using the assumption that the estimates are bounded , i.e.",
    ", @xmath262 , we deduce that the bound in each epoch is @xmath263 . by the doubling trick ,",
    "@xmath264 we conclude that @xmath265 similarly , @xmath266 the desired pair of inequalities follows substituting these bounds in   and dividing by  @xmath267 .    in the statement of theorem",
    "[ th : convergence - general - saddle - point - w - d - mu - z ] , the constant @xmath268 appearing in   encodes the dependence on the network properties .",
    "the running time - averages can be updated sequentially as @xmath269 without extra memory .",
    "note also that we assume feasibility of the problem because this property does not follow from the behavior of the algorithm .",
    "[ re : boundedness - estimates ] the statement of theorem  [ th : convergence - general - saddle - point - w - d - mu - z ] requires the subgradients and the estimates produced by the dynamics to be bounded . in the literature of distributed ( sub- ) gradient methods ,",
    "it is fairly common to assume the boundedness of the subgradient sets relying on their continuous dependence on the arguments , which in turn are assumed to belong to a compact domain .",
    "our assumption on the boundedness of the estimates , however , concerns a saddle - point subgradient dynamics for general convex - concave functions , and its consequences vary depending on the application .",
    "we come back to this point and discuss the treatment of dual variables for distributed constrained optimization in section  [ sec : distributed - bound - optimal - dual - set ] .",
    "in this section we particularize our convergence result in theorem  [ th : convergence - general - saddle - point - w - d - mu - z ] to the case of convex - concave functions arising from the lagrangian of the constrained optimization   discussed in section  [ sec - linear - semidefinite - constraints - intro ] .",
    "the lagrangian formulation with explicit agreement constraints   matches the general saddle - point problem   for the convex - concave function  @xmath270 defined by @xmath271 here the arguments of the convex part are , on the one hand , the local primal variables across the network , @xmath272 ( not subject to agreement ) , and , on the other hand , the copies across the network of the global decision vector , @xmath273 ( subject to agreement ) .",
    "the arguments of the concave part are the network estimates of the lagrange multiplier , @xmath274 ( subject to agreement ) .",
    "note that this convex - concave function is the associated lagrangian for   _ only _ under the agreement on the global decision vector and on the lagrange multiplier associated to the global constraint , i.e. , @xmath275 in this case , the _ saddle - point dynamics with laplacian averaging",
    "_   takes the following form : the updates of each agent @xmath107 are as follows ,    [ eq : minmax - dyn - constrained - agent - updates ] @xmath276    where the vectors @xmath277 and @xmath278 are subgradients of @xmath279 with respect to the first and second arguments , respectively , at the point @xmath280 , i.e. , @xmath281 and the matrices @xmath282 and @xmath283 contain in the @xmath284th row an element of the subgradient sets @xmath285 and  @xmath286 , respectively .",
    "( note that these matrices correspond , in the differentiable case , to the jacobian block - matrices of the vector function @xmath106 . )",
    "we refer to this strategy as the _ consensus - based saddle - point ( sub- ) gradient ( c - sp - sg ) algorithm _ and present it in pseudo - code format in algorithm  [ alg : saddle - point - constrained - optimization ] .",
    "note that the orthogonal projection of the estimates of the multipliers in   is unique .",
    "the radius  @xmath287 employed in its definition is a design parameter that is either set _ a priori _ or determined by the agents .",
    "we discuss this point in detail below in section  [ sec : distributed - bound - optimal - dual - set ] .",
    "the characterization of the saddle - point evaluation error under   is a direct consequence of theorem  [ th : convergence - general - saddle - point - w - d - mu - z ] .",
    "[ cor : app - constrained - opt ] for each @xmath107 , let the sequence @xmath288 be generated by the coordination algorithm  , over a sequence of graphs @xmath54 satisfying the same hypotheses as theorem  [ th : convergence - general - saddle - point - w - d - mu - z ] .",
    "assume that the sets @xmath289 and  @xmath290 are compact ( besides being convex ) , and the radius @xmath287 is such that  @xmath291 contains the optimal dual set of the constrained optimization  .",
    "assume also that the subgradient sets are bounded , in @xmath292 , as follows , @xmath293 for all @xmath294 .",
    "let  @xmath295 be any saddle point of the lagrangian  @xmath81 defined in   on the set @xmath296 .",
    "( the existence of such saddle - point implies that strong duality is attained . ) then , under assumption  [ ass : doubling - trick ] for the learning rates , the saddle - point evaluation error   holds for the running time - averages : @xmath297 for  @xmath298 and @xmath299 as in  , with @xmath300 where @xmath301 refers to the diameter of the sets .",
    "the proof of this result follows by noting that the hypotheses of theorem  [ th : convergence - general - saddle - point - w - d - mu - z ] are automatically satisfied .",
    "the only point to observe is that all the saddle points of the lagrangian @xmath81 defined in   on the set @xmath302 , are also contained in  @xmath303 .",
    "note also that we assume feasibility of the problem because this property does not follow from the behavior of the algorithm .",
    "[ re : complexity - particular - dynamics ]    we discuss here the complexities associated with the execution of the c - sp - sg algorithm :    * * time complexity * : according to corollary  [ cor : app - constrained - opt ] , the saddle - point evaluation error is smaller than @xmath304 if @xmath305 .",
    "this provides a lower bound @xmath306 on the number of required iterations . *",
    "* memory complexity * : each agent  @xmath117 maintains the current updates @xmath307 , and the corresponding current running time - averages @xmath308 with the same dimensions . *",
    "* computation complexity * : each agent  @xmath117 makes a choice / evaluation of subgradients , at each iteration , from the subdifferentials  @xmath309 , @xmath310 , @xmath311 , @xmath312 , the latter for @xmath294 .",
    "each agent also projects its estimates on the set  @xmath313 .",
    "the complexity of this computation depends on the sets @xmath290 and @xmath289 . * * communication complexity : * each agent @xmath117 shares with its neighbors at each iteration a vector in  @xmath314 . with",
    "the information received , the agent updates the global decision variable @xmath315 in   and the lagrange multiplier @xmath316 in  .",
    "( note that the variable @xmath315 needs to be maintained and communicated only if the optimization problem   has a global decision variable . )",
    "the motivation for the design choice of _ truncating _ the projection of the dual variables onto a bounded set in   is the following .",
    "the subgradients of  @xmath153 with respect to the primal variables are _ linear _ in the dual variables .",
    "to guarantee the boundedness of the subgradients of  @xmath153 and of the dual variables , required by the application of theorem  [ th : convergence - general - saddle - point - w - d - mu - z ] , one can introduce a projection step onto a compact set that preserves the optimal dual set , a technique that has been used in  @xcite .",
    "these works select the bound for the projection _ a priori _ , whereas  @xcite proposes a distributed algorithm to compute a bound preserving the optimal dual set , _ for _ the case of a global inequality constraint _ known to all the agents_. here , we deal with a complementary case , where the constraint is a sum of functions , each known to the corresponding agents , that couple the local decision vectors across the network . for this case , we next describe how the agents can compute , in a distributed way , a radius @xmath317 such that the ball @xmath291 contains the optimal dual set for the constrained optimization  .",
    "a radius with such property is not unique , and estimates with varying degree of conservativeness are possible .    in our model , each agent @xmath117 has only access to the set  @xmath290 and the functions  @xmath279 and @xmath115 . in turn",
    ", we make the important assumption that there are no variables subject to agreement , i.e. , @xmath318 and @xmath319 for all @xmath107 , and we leave for future work the generalization to the case where agreement variables are present .",
    "consider then the following problem , @xmath320 where each @xmath290 is compact as in corollary  [ cor : app - constrained - opt ] .",
    "we first propose a bound on the optimal dual set and then describe a distributed strategy that allows the agents to compute it .",
    "let @xmath321 be a vector satisfying the _ strong slater condition _",
    "7.2.3 ) , called _ slater vector _ , and define @xmath322 which is positive by construction .",
    "according to  ( * ? ? ?",
    "* lemma 1 ) ( which we amend imposing that the slater vector belongs to the abstract constraint set  @xmath323 ) , we get that the optimal dual set  @xmath324 associated to the constraint @xmath325 is bounded as follows , @xmath326 for any @xmath327 , where @xmath328 is the dual function associated to the optimization  , @xmath329 note that the right hand side in   is nonnegative by weak duality , and that @xmath330 does not coincide with @xmath331 for any @xmath327 because each set  @xmath290 is compact . with this notation , @xmath332 using this bound in  , we conclude that @xmath333 , with @xmath334 now we briefly describe the distributed strategy that the agents can use to bound the set  @xmath335 . the algorithm can be divided in three stages :    1 .",
    "each agent finds the corresponding component  @xmath336 of a slater vector .",
    "for instance , if @xmath290 is _ compact _ ( as is the case in corollary  [ cor : app - constrained - opt ] ) , agent  @xmath117 can compute @xmath337 the resulting vector @xmath338 is a slater vector , i.e. , it belongs to the set @xmath339 which is nonempty by the strong slater condition .    1 .   similarly , the agents compute the corresponding component @xmath340 defined in  .",
    "the common value  @xmath327 does not depend on the problem data and can be  @xmath341 or any other value agreed upon by the agents beforehand .    1 .   the agents find a lower bound for  @xmath342 in   in two stages : first they use a distributed consensus algorithm and at the same time they estimate the fraction of agents that have a positive estimate .",
    "second , when each agent is convinced that every other agent has a positive approximation , given by a precise termination condition that is satisfied in finite time , they broadcast their estimates to their neighbors to agree on the minimum value across the network .    formally , each agent sets @xmath343 and @xmath344 , and executes the following iterations    @xmath345    until an iteration @xmath346 such that @xmath347 ; see lemma  [ le : termination - condition ] below for the justification of this termination condition . then , agent @xmath117 re - initializes @xmath348 and iterates @xmath349 ( where agent @xmath117 does not need to know if a neighbor has re - initialized ) .",
    "the agents reach agreement about @xmath350 in a number of iterations no greater than @xmath351 counted after @xmath352 ( which can be computed if each agent broadcasts once @xmath353 ) .",
    "therefore , the agents obtain the same lower bounds @xmath354 where the first lower bound is coordinate - wise .    1 .",
    "the agents exactly agree on @xmath355 and @xmath356 using the finite - time algorithm analogous to  .",
    "in summary , the agents obtain the same upper bound @xmath357 which , according to  , bounds the optimal dual set for the constrained optimization  , @xmath358 to conclude , we justify the termination condition of step  ( ii ) .",
    "[ le : termination - condition ] if each agent knows the size of the network @xmath359 , then under the same assumptions on the communication graphs and the parameter @xmath229 as in  theorem  [ th : convergence - general - saddle - point - w - d - mu - z ] , the termination time @xmath346 is finite .",
    "note that @xmath360 is not guaranteed to be negative but , by construction of each @xmath361 in step ( i ) , it holds that the convergence point for   is @xmath362 this , together with the fact that laplacian averaging preserves the convex hull of the initial conditions , it follows ( inductively ) that @xmath363 decreases monotonically to @xmath198 . thanks to the exponential convergence of   to the point  ,",
    "it follows that there exists a finite time @xmath364 such that @xmath347 .",
    "this termination time is determined by the constant @xmath55 of joint connectivity and the constant @xmath49 of nondegeneracy of the adjacency matrices .",
    "the complexity of the entire procedure corresponds to    * each agent computing the minimum of two convex functions ; * executing laplacian average consensus until the agents estimates fall within a centered interval around the average of the initial conditions ; and * running two agreement protocols on the minimum of quantities computed by the agents .",
    "here we simulate the performance of the consensus - based saddle - point ( sub- ) gradient algorithm ( cf .",
    "algorithm  [ alg : saddle - point - constrained - optimization ] ) in a network of @xmath365 agents whose communication topology is given by a fixed connected small world graph  @xcite with maximum degree  @xmath366 . under this coordination strategy ,",
    "the  @xmath367 agents solve collaboratively the following instance of problem   with nonlinear convex constraints : @xmath368}&\\ ,      \\sum_{i=1}^{50 } c_i w_i \\notag",
    "\\\\    \\rm{s.t.}&\\ , \\sum_{i=1}^{50 } -d_i \\log(1 + w_i)\\le -b .\\end{aligned}\\ ] ] problems with constraints of this form arise , for instance , in wireless networks to ensure quality - of - service . for each @xmath369 ,",
    "the constants @xmath370 , @xmath371 are taken randomly from a uniform distribution in  @xmath372 $ ] , and @xmath373 .",
    "we compute the solution to this problem , to use it as a benchmark , with the optimization toolbox using the solver _ fmincon _ with an _ interior point _ algorithm .",
    "since the graph is connected , it follows that @xmath374 in the definition of joint connectivity . also , the constant of nondegeneracy is  @xmath375 and @xmath376 . with these values ,",
    "we derive from   the theoretically feasible consensus stepsize @xmath377 . for the projection step in   of the c - sp - sg algorithm ,",
    "the bound on the optimal dual set  , using the slater vector @xmath378 and @xmath379 , is @xmath380 for comparison , we have also simulated the consensus - based dual decomposition ( coba - dd ) algorithm proposed in  @xcite using ( and adapting to this problem ) the code made available online by the authors .",
    "( the bound for the optimal dual set used in the projection of the estimates of the multipliers is the same as above . )",
    "we should note that the analysis in  @xcite only considers constant learning rates , which necessarily results in steady - state error in the algorithm convergence .",
    "we have simulated the c - sp - sg and the coba - dd algorithms in two scenarios : under the doubling trick scheme of assumption  [ ass : doubling - trick ] ( solid blue and magenta dash - dot lines , respectively ) , and under constant learning rates equal to  @xmath381 ( darker grey ) and  @xmath382 ( lighter grey ) . fig .",
    "[ fig : log_constraint ] shows the saddle - point evaluation error for both algorithms .",
    "the saddle - point evaluation error of our algorithm is well within the theoretical bound established in corollary  [ cor : app - constrained - opt ] , which for this optimization problem is approx .",
    "( this theoretical bound is overly conservative for connected digraphs because the ultimate bound for the disagreement  @xmath268 in  , here @xmath384 , is tailored for sequences of digraphs that are @xmath55-jointly connected instead of relying on the second smallest eigenvalue of the laplacian of connected graphs . )",
    "[ fig : log_constraint_cost_and_constraint_evolution ] compares the network cost - error and the constraint satisfaction .",
    "we can observe that the c - sp - sg and the coba - dd  @xcite algorithms have some characteristics in common :    * they both benefit from using the doubling trick scheme . *",
    "they approximate the solution , in all metrics of fig .",
    "[ fig : log_constraint ] and fig .",
    "[ fig : log_constraint_cost_and_constraint_evolution ] at a similar rate .",
    "although the factor in logarithmic scale of the c - sp - sg algorithm is larger , we note that this algorithm does not require the agents to solve a local optimization problem at each iteration for the updates of the primal variables , while both algorithms share the same communication complexity . *",
    "the empirical convergence rate for the saddle - point evaluation error under the doubling trick scheme is of order  @xmath0 ( logarithmic slope  @xmath385 ) , while the empirical convergence rate for the cost error under constant learning rates is of order  @xmath386 ( logarithmic slope  @xmath198 ) .",
    "this is consistent with the theoretical results here and in  @xcite ( wherein the theoretical bound concerns the practical convergence of the cost error using constant learning rates ) .",
    "( 0,0)(0,0 )    ( 0,0)(0,0 )",
    "we have studied projected subgradient methods for saddle - point problems under explicit agreement constraints .",
    "we have shown that separable constrained optimization problems can be written in this form , where agreement plays a role in making distributed both the objective function ( via agreement on a subset of the primal variables ) and the constraints ( via agreement on the dual variables ) .",
    "this approach enables the use of existing consensus - based ideas to tackle the algorithmic solution to these problems in a distributed fashion .",
    "future extensions will include , first , a refined analysis of convergence for constrained optimization in terms of the cost evaluation error instead of the saddle - point evaluation error .",
    "second , more general distributed algorithms for computing bounds on lagrange vectors and matrices , which are required in the design of truncated projections preserving the optimal dual sets .",
    "( an alternative route would explore the characterization of the intrinsic boundedness properties of the proposed distributed dynamics . )",
    "third , the selection of other learning rates that improve the convergence rate of our proposed algorithms . finally , we envision applications to semidefinite programming , where chordal sparsity allows to tackle problems that have the dimension of the matrices grow with the network size , and also the treatment of low - rank conditions .",
    "particular applications will include efficient optimization in wireless networks , control of camera networks , and estimation and control in smart grids .",
    "the authors thank the anonymous reviewers for their useful feedback that helped us improve the presentation of the paper .",
    "this research was partially supported by nsf award cmmi-1300272",
    ".    10    d.  mateos - nez and j.  corts , `` distributed subgradient methods for saddle - point problems , '' in _ ieee conf .  on decision and control",
    "_ , ( osaka , japan ) , pp .  54625467 , 2015 .",
    "k.  arrow , l.  hurwitz , and h.  uzawa , _ studies in linear and non - linear programming_. stanford , california : stanford university press , 1958 .",
    "a.  nedi and a.  ozdaglar , `` subgradient methods for saddle - point problems , '' _ journal of optimization theory & applications _ ,",
    "vol .  142 , no .  1 , pp .  205228 , 2009 .",
    "n.  parikh and s.  boyd , `` proximal algorithms , '' vol .  1 , no .  3 , pp .",
    "123231 , 2013 .",
    "s.  boyd , n.  parikh , e.  chu , b.  peleato , and j.  eckstein , `` distributed optimization and statistical learning via the alternating direction method of multipliers , '' _ foundations and trends in machine learning _ , vol .  3 , no .  1 ,",
    "1122 , 2011 .",
    "b.  johansson , t.  keviczky , m.  johansson , and k.  h. johansson , `` subgradient methods and consensus algorithms for solving convex optimization problems , '' in _ ieee conf .  on decision and control",
    "_ , ( cancun , mexico ) , pp .  41854190 , 2008 .",
    "a.  nedic and a.  ozdaglar , `` distributed subgradient methods for multi - agent optimization , '' _ ieee transactions on automatic control _ , vol .",
    "54 , no .  1 ,",
    "pp .  4861 , 2009 .",
    "j.  wang and n.  elia , `` a control perspective for centralized and distributed convex optimization , '' in _ ieee conf .  on decision and control",
    "_ , ( orlando , florida ) , pp .  38003805 , 2011 .",
    "m.  zhu and s.  martnez , `` on distributed convex optimization under inequality and equality constraints , '' _ ieee transactions on automatic control _ , vol .",
    "57 , no .  1 ,",
    "pp .  151164 , 2012 .",
    "r.  carli , g.  notarstefano , l.  schenato , and d.  varagnolo , `` distributed quadratic programming under asynchronous and lossy communications via newton - raphson consensus , '' in _",
    "european control conference _ , ( lind , austria ) , pp .  25142520 , 2015 .",
    "b.  gharesifard and j.  corts , `` distributed continuous - time convex optimization on weight - balanced digraphs , '' _ ieee transactions on automatic control _ ,",
    "59 , no .  3 , pp .",
    "781786 , 2014 .",
    "j.  n. tsitsiklis , _ problems in decentralized decision making and computation_. phd thesis , massachusetts institute of technology , nov .",
    "available at http://web.mit.edu/jnt/www/papers/phd-84-jnt.pdf .",
    "j.  n. tsitsiklis , d.  p. bertsekas , and m.  athans , `` distributed asynchronous deterministic and stochastic gradient optimization algorithms , '' _ ieee transactions on automatic control _",
    "31 , no .  9 , pp .  803812 , 1986 .",
    "d.  p. bertsekas and j.  n. tsitsiklis , _ parallel and distributed computation : numerical methods_. athena scientific , 1997 .",
    "d.  p. bertsekas , a.  nedi , and a.  e. ozdaglar , _ convex analysis and optimization_. belmont , ma : athena scientific , 1st  ed . , 2003 .",
    "g.  scutari , d.  p. palomar , f.  facchinei , and j.  s. pang , `` convex optimization , game theory , and variational inequality theory , '' _ ieee signal processing magazine _ , vol .",
    "27 , no .  3 , pp .",
    "3549 , 2010 .",
    "s.  dafermos , `` traffic equilibrium and variational inequalities , '' _ transportation science _ , vol .",
    "14 , no .  1 ,",
    "pp .  4254 , 1980 .",
    "a.  nagurney , m.  yu , a.  h. masoumi , and l.  s. nagurney , _ networks against time : supply chain analytics for perishable products_. springerbriefs in optimization , springer , 2010 .",
    "a.  nedi and a.  ozdaglar , `` approximate primal solutions and rate analysis for dual subgradient methods , '' _ siam j. optimization _ ,",
    "19 , no .  4 , pp .",
    "17571780 , 2010 .",
    "hiriart - urruty and c.  lemarchal , _ convex analysis and minimization algorithms i_. grundlehren text editions , new york : springer , 1993 .",
    "m.  brger , g.  notarstefano , and f.  allgwer , `` a polyhedral approximation framework for convex and robust distributed optimization , '' _ ieee transactions on automatic control _ , vol .",
    "59 , no .  2 , pp .",
    "384395 , 2014 .",
    "chang , a.  nedi , and a.  scaglione , `` distributed constrained optimization by consensus - based primal - dual perturbation method , '' _ ieee transactions on automatic control _",
    "59 , no .  6 , pp .",
    "15241538 , 2014 .",
    "a.  simonetto and h.  jamali - rad , `` primal recovery from consensus - based dual decomposition for distributed convex optimization , '' _ journal of optimization theory and applications _ ,",
    "168 , no .  1 ,",
    "pp .  172197 , 2016 .",
    "d.  yuan , s.  xu , and h.  zhao , `` distributed primal - dual subgradient method for multiagent optimization via consensus algorithms , '' _ ieee trans .",
    "systems , man , and cybernetics- part b _",
    "41 , no .",
    "6 , pp .  17151724 , 2011 .",
    "d.  yuan , d.  w.  c. ho , and s.  xu , `` regularized primal - dual subgradient method for distributed constrained optimization , '' _ ieee transactions on cybernetics _ , 2015 . to appear .",
    "a.  nedic , a.  ozdaglar , and p.  a. parrilo , `` constrained consensus and optimization in multi - agent networks , '' _ ieee transactions on automatic control _ , vol .",
    "55 , no .  4 , pp .",
    "922938 , 2010 .",
    "i.  necoara , i.  dumitrache , and j.  a.  k. suykens , `` fast primal - dual projected linear iterations for distributed consensus in constrained convex optimization , '' in _ ieee conf .  on decision and control _ , ( atlanta , ga ) , pp .",
    "13661371 , dec .",
    "d.  mosk - aoyama , t.  roughgarden , and d.  shah , `` fully distributed algorithms for convex optimization problems , '' _ siam j. optimization _ , vol .",
    "20 , no .",
    "6 , pp .  32603279 , 2010 .",
    "g.  notarstefano and f.  bullo , `` distributed abstract optimization via constraints consensus : theory and applications , '' _ ieee transactions on automatic control _ ,",
    "56 , no .",
    "10 , pp .  22472261 , 2011 .",
    "d.  richert and j.  corts , `` robust distributed linear programming , '' _ ieee transactions on automatic control _ , vol .",
    "60 , no .",
    "10 , pp .  25672582 , 2015 .",
    "a.  a. morye , c.  ding , a.  k. roy - chowdhury , and j.  a. farrell , `` distributed constrained optimization for bayesian opportunistic visual sensing , '' _ ieee transactions on control systems technology _ , vol .",
    "22 , no .  6 , pp .",
    "23022318 , 2014 .",
    "a.  kalbat and j.  lavaei , `` a fast distributed algorithm for decomposable semidefinite programs , '' 2015 .",
    "available electronically at http://www.ee.columbia.edu/~lavaei/admm_sdp_2015.pdf .",
    "d.  mateos - nez and j.  corts , `` distributed optimization for multi - task learning via nuclear - norm approximation , '' in _ ifac workshop on distributed estimation and control in networked systems _ , vol .",
    "48 , ( philadelphia , pa ) , pp .  6469 , 2015 .",
    "f.  bullo , j.  corts , and s.  martnez , _ distributed control of robotic networks_. applied mathematics series , princeton university press , 2009 .",
    "electronically available at http://coordinationbook.info .",
    "s.  boyd and l.  vandenberghe , _",
    "convex optimization_. cambridge university press , 2009 .",
    "d.  p. bertsekas , _ nonlinear programming_. belmont , ma : athena scientific , 2nd  ed . , 1999 .",
    "a.  nedi and a.  ozdaglar ,",
    "`` cooperative distributed multi - agent optimization , '' in _",
    "convex optimization in signal processing and communications _",
    "( y.  eldar and d.  palomar , eds . ) , cambridge university press , 2010 .",
    "r.  t. rockafellar and r.  j.  b. wets , _ variational analysis _ , vol .",
    "317 of _ comprehensive studies in mathematics_. new york : springer , 1998 .",
    "s.  shalev - shwartz , _ online learning and online convex optimization _ ,",
    "12 of _ foundations and trends in machine learning_. now publishers inc , 2012 .",
    "d.  mateos - nez and j.  corts , `` distributed online convex optimization over jointly connected digraphs , '' _ ieee transactions on network science and engineering _ , vol .  1 , no .  1 , pp .  2337 , 2014 .",
    "d.  j. watts and s.  h. strogatz , `` collective dynamics of ` small - world ' networks , '' _ nature _ , vol .",
    "393 , pp .  440442 , 1998 .",
    "r.  a. horn and c.  r. johnson , _ matrix analysis_. cambridge university press , 1985 .",
    "[ mycounter]definition [ mycounter]lemma [ mycounter]theorem",
    "here we present the proofs of the results lemma  [ le : basic - bound - convex - concave - function - differences ] and proposition  [ prop : cum - disagreement - projected - subg - saddle ] stated in section  [ sec : convergence - analysis ] .    in this proof",
    "we extend the saddle - point analysis for the ( centralized ) subgradient methods in  ( * ? ? ?",
    "* lemma 3.1 ) by incorporating the treatment on the disagreement from our previous work in  ( * ? ? ?",
    "* lemma v.2 ) .",
    "we first define @xmath387 since @xmath388 is a stochastic matrix ( because @xmath229  satisfies  ) , then its product by any vector is a convex combination of the entries of the vector .",
    "hence , the fact that @xmath389 implies that @xmath390 . using this together with the definition of orthogonal projection  ,",
    "we get @xmath391 similarly , since @xmath392 , we also have @xmath393 left - multiplying the dynamics of @xmath200 and @xmath148 from   and   ( in terms of the residual  ) by the block - diagonal matrix @xmath394 , and using @xmath395 , we obtain @xmath396 subtracting @xmath397 on each side , taking the norm , and noting that @xmath398 and @xmath399 , we get @xmath400 we can bound the term @xmath401 by subtracting and adding @xmath402 inside the bracket and using convexity , @xmath403 where we have used @xmath404 and the fact that @xmath405 and @xmath406 . using this bound and , we get @xmath407 we now bound each of the terms in the last three lines of  . first , using the cauchy - schwarz inequality , we get @xmath408 for the terms in the second to last line , using the triangular inequality , the submultiplicativity of the norm , the fact that @xmath409 , and the bound  , we have @xmath410 and , similarly , @xmath411 finally , regarding the term @xmath412 , we use the definition of @xmath413 and also add and subtract @xmath414 inside the bracket . with the fact that @xmath415 ( because @xmath289 is convex ) , we leverage the property   of the orthogonal projection to derive the first inequality . for the next two inequalities we use the cauchy - schwarz inequality , and then the bound in   for the residual , and also the definition of  @xmath414 , the fact that @xmath416 , and the triangular inequality .",
    "formally , @xmath417 where in the last inequality we have also used a bound for the term  @xmath418 invoking  @xmath419 that we explain next . from the courant - fischer min - max theorem  @xcite applied to the matrices @xmath420 and @xmath421 ( which are symmetric with the same nullspace ) , we deduce that for any @xmath422 , @xmath423 where @xmath424 refers to the second smallest eigenvalue , which for the matrix @xmath72 is @xmath425 .",
    "( note that all its eigenvalues are @xmath425 , except the smallest that is @xmath341 . ) with the analogous inequality for kronecker products with the identity @xmath426 , the bound needed to conclude   is then @xmath427 similarly to  , now without the disagreement terms , @xmath428 substituting the bounds  ,   and  , and their counterparts for @xmath200 , in  , we obtain @xmath429 and   follows .",
    "the bound   can be derived similarly , requiring concavity of  @xmath153 in  @xmath430 .",
    "since both dynamics in   are structurally similar , we study the first one , @xmath431 where @xmath413 is as in   and satisfies ( similarly to ) that @xmath432 the dynamics   coincides with that of  ( * ? ? ?",
    "( 29 ) ) where , in the notation of the reference , one sets @xmath433 .",
    "therefore , we obtain a bound analogous to  ( * ? ? ?",
    "( 34 ) ) , @xmath434 where @xmath435 . to derive   we use three facts : first @xmath436 ; second , @xmath437 for any @xmath438 and in particular for @xmath439 ; and third , @xmath440 the constant  @xmath268 in the statement is obtained recalling that @xmath441 to obtain  , we sum   over the time horizon  @xmath236 and bound the double sum as follows : using @xmath439 for brevity , we have @xmath442 finally , we use again the bound @xmath443 ."
  ],
  "abstract_text": [
    "<S> we present distributed subgradient methods for min - max problems with agreement constraints on a subset of the arguments of both the convex and concave parts . </S>",
    "<S> applications include constrained minimization problems where each constraint is a sum of convex functions in the local variables of the agents . in the latter case </S>",
    "<S> , the proposed algorithm reduces to primal - dual updates using local subgradients and laplacian averaging on local copies of the multipliers associated to the global constraints . for the case of general convex - concave saddle - point problems , </S>",
    "<S> our analysis establishes the convergence of the running time - averages of the local estimates to a saddle point under periodic connectivity of the communication digraphs . </S>",
    "<S> specifically , choosing the gradient step - sizes in a suitable way , we show that the evaluation error is proportional to  @xmath0 , where  @xmath1 is the iteration step . </S>",
    "<S> we illustrate our results in simulation for an optimization scenario with nonlinear constraints coupling the decisions of agents that can not communicate directly . </S>"
  ]
}