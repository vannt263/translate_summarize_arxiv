{
  "article_text": [
    "video - based individual , interactive , and group activity recognition has attracted more and more interests in the computer vision community .",
    "using fixed cameras for collecting videos suffers from the problem of only covering very limited areas .",
    "this problem will get even worse when recognizing activities in a social event , such as a concert , ceremony or party , where multiple people are present and move from time to time .",
    "recently , wearable cameras , such as google glass or go pro , provide a new solution , where all or part of the involved persons wear a camera over head to record what they see over time  @xcite .    by combining the temporally synchronized videos from different wearers",
    ", we can recognize the activity occurred in a large area , because camera wearers can walk or move the head to follow the people or event of interest  @xcite .",
    "an important problem arising from this setting is to identify the co - interest person ( cip ) that attracts the attentions from multiple wearers since this person usually plays a central role in ongoing event of interest .",
    "the cip and his / her activities are of particular importance for surveillance , anomaly detection and social network construction . for examples , in a public scenario such as an airport , cip can be a person with abnormal behavior or activity who usually draws attention from multiple camera - wearing security guards and the quick detection of such cips can promote the public security . in a kindergarten , cip may be a kid with strange behavior that continuously draws joint attentions from camera - wearing teachers or other kids . in this case",
    ", the cip detection can facilitate the early findings of various child development issues . in a group discussion",
    ", people usually focus on the person who leads or gives the speech at any time and the identification of such cips over time can help summarize and edit all the videos from the attendee s cameras for more effective information management and retrieval . in this paper , we develop a new approach to detect cips from multiple videos taken by wearable cameras .        in many social events , attendees may wear clothes with similar color and texture , such as wearing specific uniforms in work and suits in a formal dinner . in these cases , it is very difficult to identify cips by performing appearance matching across multiple videos , as shown in fig .",
    "[ fig : fig1](a ) . in this paper",
    ", we identify the cip based on his / her motion patterns : it is unlikely that two persons in the view keep showing exactly same motion over time .",
    "however , it is a very challenging problem to identify the person with the same motion pattern from different videos even if these videos are temporally synchronized , because the motion pattern of a person is defined in 3d and can only be partially reflected in each 2d video . in practice",
    ", the 3d motion of a same person may be projected to completely different 2d motions in different videos , as illustrated in fig.1(a ) .",
    "in addition , in this research , the inference of the 2d motion pattern of a person is further complicated by the use of the wearable cameras : camera motion and person motion are mixed in generating each video .    in this paper , we address this challenging problem by combining the temporally synchronized frames from different videos using a conditional random field ( crf ) model .",
    "we first perform human detection to obtain a set of candidates of the cip .",
    "then we build a crf by taking each frame as a node and the candidates on that frame as its states . in this crf",
    ", we define an inter - video energy that reflects the motion - pattern difference of the candidates drawn from different videos , as illustrated in fig .",
    "[ fig : fig1](a ) .",
    "in particular , we use histogram of optical flow ( hof ) , hankelets  @xcite and motion pattern histograms ( mph )  @xcite to describe the human motion .",
    "we also include an intra - video energy term in the crf to measure the location and size consistency of candidates across frames of a same video , as illustrated in fig .",
    "[ fig : fig1](b ) .",
    "the minimization of the proposed crf energy will generate a cip on each frame of each video that shows both inter - video and intra - video properties . to handle the case where a frame contains no cip , e.g. ,",
    "the cip can not see himself in his egocentric video , as shown by video 3 in fig.1(a ) , we also introduce an idle state in each frame .",
    "related to this paper is a series of prior research on video co - segmentation , where common objects are segmented from multiple videos .",
    "video co - segmentation can be treated as an extension of the long - studied image co - segmentation  @xcite , where the input is a set of images instead of videos .",
    "however , different from the proposed cip detection , the multiple videos used for video co - segmentation are usually not temporarily synchronized : they may record the same object at different time . as a result",
    ", the co - segmented person may not show motion consistency across different videos . in practice ,",
    "almost all the existing co - segmentation algorithms are based on object - appearance matching .",
    "for example , @xcite and @xcite model the co - segmentation as a foreground / background separation problem based on the appearance information .",
    "wang et al .",
    "@xcite develop an appearance based weakly supervised co - segmentation algorithm which also needs the labels for a few frames . in  @xcite , the common objects are localized in different videos by using the appearance and local features .",
    "some of prior video co - segmentation methods use the motion information to help track and/or segment the objects in each video but not corresponding objects across videos as in the proposed cip detection . chiu and fritz @xcite",
    "propose a multi - class co - segmentation algorithm based on a non - parametric bayesian model which uses the motion information for object segmentation . in @xcite ,",
    "a number of tracklets are detected inside each video and the appearance and shape information along the tracklets are then extracted to identify the common target in multiple videos . in @xcite , co -",
    "segmentation is formulated as a co - selection graph where motions are estimated to measure the spatial temporal consistency . in @xcite",
    ", motion trajectories are detected to match the action across video pairs .",
    "however , the action matching is only in the high - level of the action type .",
    "there is no frame - by - frame motion consistency between these videos since they are not temporally synchronized .",
    "in addition , when multiple people are present in the view of each video , most works on video co - segmentation identify all of them as a common object  person . in the proposed cip detection , we need to distinguish them and identify one person with presence in all or most of the videos .      also related to our work is the research on gaze concurrences of multiple video takers .",
    "robertson and reid @xcite estimate face orientation by learning 2d face features from different views . in @xcite ,",
    "the points of interest are estimated in a crowded scene . however , these methods rely on video data captured from a third person . as a result ,",
    "the area covered by these videos are quite limited and the accuracy of head pose estimation degrades when distance to the camera increases @xcite .",
    "park et al .",
    "present an algorithm to locate gaze concurrences directly from videos taken by head - mounted cameras .",
    "however , this algorithm requires a prior scanning of the area of interest ( for example , room or an auditorium ) to reconstruct the reference structure .",
    "this may not be available in practice .",
    "to detect cip over time , we record a set of @xmath0 temporally synchronized long - streaming videos that are taken by @xmath0 wearable cameras over time @xmath1 $ ] .",
    "the cip in these videos may change over time . to simplify the problem , we first apply a sliding window technique to divide the time @xmath2 $ ] into overlapped short time windows with length @xmath3 . over each short time window , we assume that the cip does not change in these @xmath0 videos and we propose an algorithm to detect such a person in each window .",
    "the proposed algorithm also provides an energy for the cip detection in each window .",
    "this energy value negatively reflected the confidence of cip detection .",
    "finally , we merge the cip detection results over all the windows based on their energies to achieve a cip detection at each frame over @xmath2 $ ] , as illustrated in fig .",
    "[ fig : model ] .        to merge cip detection results from all the windows",
    ", we always select the one with lowest detection energy at each frame . specifically , by using sliding window technique , the constructed windows are partially overlapped and each frame , say @xmath4 , is covered by multiple windows , say @xmath5 . in each window",
    "@xmath6 , the cip detection algorithm ( to be introduced in section  [ sec : crfconstruction ] ) generates a cip detection @xmath7 and an associated energy @xmath8 .",
    "we find the one with the lowest energy as @xmath9 and set @xmath10 as the final cip detection in this frame @xmath4 .",
    "an example is shown in fig .",
    "[ fig : slidingwin ] . in this figure",
    ", @xmath11 denotes the partially overlapped windows , and @xmath12 and @xmath13 denote the cip detected in each window @xmath11 and its energy , respectively . if @xmath14 and @xmath15 , as shown in fig .",
    "[ fig : slidingwin ] , then the red dashed line actually indicates a time when cip is changed from @xmath16 to @xmath17 .        in the following , we focus on developing the proposed cip detection algorithm in each window @xmath18 .",
    "over a short - time window @xmath18 , the @xmath0 input videos are actually cropped into @xmath0 synchronized short video clips @xmath19 with @xmath20 where @xmath21 is the @xmath4-th frame in the @xmath22-th video clip .    as shown in fig .",
    "[ fig : model ] , we first perform the human detection on each frame and take each detection as a cip candidate . a conditional random field ( crf )  @xcite is then constructed by treating each frame as a node and each candidate on this frame as a state of this node .",
    "using this crf model , our goal is to seek a candidate @xmath23 on each frame @xmath21 as the detected cip .",
    "specifically , the cip detection @xmath24 has a posterior probability @xmath25 where @xmath26 is a energy of matching @xmath23 and @xmath27 as the same person and taking it as the cip . in the remainder of the paper",
    ", we simplify the notation of this pairwise energy as @xmath28 and the energy function @xmath29 as @xmath30 when there is no ambiguity .",
    "this way , the cip detection in the short time window is reduced to a problem of finding an optimal @xmath31 that minimizes the energy @xmath29 .",
    "the major problem to be solved here is the definition of the pairwise energy @xmath28 , which should reflect the correspondence of the cip between a pair of frames drawn from @xmath32 . in this paper , we consider two cases : 1 ) the two frames are from the same video clip ( intra - video ) , and 2 ) the two frames are from different video clips ( inter - video ) . for case 1 ) , the cip in a same video clip shows two typical properties : ( i ) its relative location in the frame does not change much over time , because the camera wearer usually moves his head / eyes to follow the cip even if the cip is moving ; ( ii ) the size of the cip does not change much between neighboring frames . for case 2 ) , we only consider the synchronized frame pairs from different video clips . in this case , the detected cip should show consistent 3d motions .    in our crf model ,",
    "we define two different energies @xmath33 and @xmath34 for the intra - video and inter - video frame pairs , respectively , as illustrated in fig .",
    "[ fig : graph ] and rewrite the energy function @xmath30 in eq .",
    "( [ eq : crf ] ) as @xmath35 different from many previous works  @xcite , no unary energy term is defined in this paper since we do not consider the candidate s appearance information .",
    "the construction of @xmath33 and @xmath34 will be elaborated in the following section .          * _ intra - video energy .",
    "* ideally , a cip that draws a camera - wearer s attention usually stays in the view center of the wearer .",
    "however , the view center of the wearer may not be perfectly aligned with the center of the camera he / she wears .",
    "therefore , we do not consider center bias in defining the intra - video energy in this work .",
    "instead , the relative location of the cip usually does not change much in a short video clip and we can penalize the location change between frames for cip detection .",
    "in addition , in a short video clip , the size of cip should not change substantially .",
    "considering these two properties , we define the intra - vidoe energy as @xmath36 where @xmath37 , @xmath38 ( @xmath39 , @xmath40 ) denote the center ( size ) of the candidate in frame @xmath4 and @xmath41 in video @xmath22 , respectively .",
    "@xmath42 is the indicator function that equals to 1 if @xmath43 and 0 otherwise .",
    "the inclusion of this indicator function ensures that the penalty to the cip size change is only defined for adjacent frames .    *",
    "_ inter - video energy .",
    "_ * as mentioned above , the inter - video energy is based on motion patterns of the cip . in this paper , we extract the motion patterns using two types of features : frame - based and trajectory based .",
    "the frame - based features are defined to measure the momentary motion of the cip using the information from a pair of neighboring frames .",
    "specifically , we calculate the optical flow using neighboring frames  @xcite . to remove the influence of camera motion",
    ", we further calculate the relative optical flow for each candidate by subtracting the average optical flow in its surrounding region .",
    "an example is shown in the top row of fig .",
    "[ fig : of](b ) , where the red box indicates a candidate and the region between the red box and its surrounding blue box is taken for computing the average optical flow for subtraction .    in this paper , we assume all the videos are taken from a similar altitude . this way , at a specific time the a 3d vertical motion of the cip should be projected to similar directions ( up or down ) in all the cameras but",
    "a 3d horizontal motion may be projected to opposite directions in different cameras .",
    "for example , in fig .",
    "[ fig : of](a ) , the same hand motion is from right to left when viewed from front , but from left to right when viewed from back .",
    "therefore , in this paper we propose to ignore the horizontal motion direction information in constructing the frame - based features .",
    "many previous works use a histogram of optical flow ( hof ) quantized at 8 directions : east(e ) , west(w ) , north(n ) , south(s ) , north - east(ne ) , north - west(nw ) , south - east(se ) and south - west(sw ) as motion features . by ignoring the horizontal motion directions , in this paper , we reduce these 8 directions into 5 by merging three histogram - bin pairs , i.e. , merging nw into ne , w into e , and sw into se , which are vertically symmetric , as shown in fig .  [",
    "fig : of](b ) .    to construct the frame - based features for each cip candidate on each frame ,",
    "we divide its bounding box along the vertical direction in a pyramid style , as shown fig .",
    "[ fig : of](b ) .",
    "the bounding box is first uniformly divide into two smaller boxes , each of which is then further divided into two equal - size boxes . in our experiment , we perform 3 rounds of pyramid division and in total achieve @xmath44 boxes in 4 scales for each candidate . by computing and concatenating the 5-bin hof ( as mentioned above ) for the original bounding box and the subdivided boxes",
    ", we construct an hof based feature @xmath45 with a dimension of @xmath46 . within each box ( including the original bounding box and its subdivided boxes ) , we further compute the average magnitudes of the optical flow along @xmath47 and @xmath48 directions , and the corresponding standard deviations of these magnitudes along @xmath47 and @xmath48 directions , respectively to construct a magnitude based feature @xmath49 with a dimension of @xmath50 . in this paper , the frame - based feature is defined as the union of the hof - based and the magnitude - based features .    in practice",
    ", the change of the camera angle usually results in the change of the optical - flow magnitudes in @xmath49 .",
    "therefore , when comparing frame - based features between two candidates , we use l1 distance for the hof - based features and the correlation metric for the magnitude features : @xmath51    in addition to the frame - based features , we also extract trajectory - based features based on short tracklets to capture the motion over a longer time . in this paper , we use hankelets features and movement pattern histograms ( mph ) features for this purpose since both of them show good view - invariance property and",
    "have been successfully used for cross - view action recognition  @xcite .",
    "* tracklet . * starting from each candidate , we generate a tracklet with the typical length of 15 frames . in this paper",
    ", we use a simple greedy tracking strategy @xcite : given a candidate in a frame , the candidate in the next frame with the highest spatial overlap is taken and this process is then repeated frame by frame to form the tracklet",
    ".    * dense trajectory .",
    "* improved dense trajectories have been used to efficiently represent videos with camera motions  @xcite . in this paper , we extract such improved trajectory features ( typically 15 frames ) . if the majority part of a trajectory , e.g. , on more than 8 out of 15 frames , is not coincident with a tracklet , we treat it to be a trajectory in the background . in this paper",
    ", we remove background trajectories and only keep the trajectories in the foreground .",
    "* hankelet . * following  @xcite , we construct one hankelet ( a 16@xmath528 hankel matrix ) for each trajectory . the hankelets feature for a candidate",
    "is the combination of the hanklets for all the trajectories in this candidate s bounding box .",
    "* the mph features for a candidate s trajectories consist of 5 histograms , corresponding to the 5 motion directions as used in the frame - based features ( see fig .",
    "[ fig : of](b ) ) . for each direction , the histogram takes each frame as a bin and the histogram value corresponds to the total trajectory magnitude along this motion direction in this frame .",
    "the difference between two hankelets @xmath53 and @xmath54 is defined as @xmath55 @xcite . as mentioned above",
    ", each candidate corresponds to a set of hanklets , one for each trajectory . in this paper",
    ", we define the hankelet based difference between two candidates as the average one over all hankelet pairs across these two candidates . by using l1 distance for the mph features ,",
    "we define the trajectory - based energy term as @xmath56 where @xmath57 denotes the number of all different hankelet pairs across two candidates and @xmath58 indicates the @xmath59-th histogram ( in total 5 directions ) in the mph features .",
    "finally , we define the inter - video energy as @xmath60 .",
    "one problem of the crf model defined above is its assumption that there is always a cip in each frame .",
    "this may not be true in practice .",
    "for example , the cip s egocentric video usually can not capture himself .",
    "similar issues may occur when the cip is occluded in some of the frames . to handle this issue",
    ", we add an idle state for each node ( frame ) .",
    "let @xmath61 denote the state set which includes the idle states @xmath62 .",
    "the energy function is redefined as @xmath63 where @xmath64 and @xmath65 denote the intra - video and inter - video energies that involve idle states , respectively . in this paper , we simply define them using the average intra - video energy and inter - video energy over the candidate pairs : @xmath66 where @xmath67 and @xmath68 denote the number of all different candidate pairs used in calculating the average intra - video and inter - video energies , respectively . as illustrated in fig .",
    "[ fig : ego ] , the average energy is located between the minimal energy for a pair of cips and the energies between a pair of candidates with at least one non - cip .",
    "this will facilitate the selection of idle state in a frame when the cip is missing in this frame .",
    "( [ eq : idlefunction ] ) is also known as the discrete energy minimization  @xcite . in this paper",
    ", we use the trw - s algorithm  @xcite to solve for an approximately optimal solution .",
    "we collect three sets of temporally synchronized videos taken by multiple wearable cameras .",
    "these three sets of videos , denoted as v1 , v2 and v3 respectively , are taken in different scenes , including both indoor and outdoor settings . for each video set , there are 6 persons who are both performers and camera wearers and therefore generate 6 videos .",
    "each person wears a gopro camera over the head .",
    "we arrange the video recording in a way that the 6 performers alternately play as the cip in the video recording by performing different actions .",
    "all 6 persons wear white shirts and bluish jeans thus sharing very similar appearances .",
    "we manually label the cip by a bounding box in each frame by using the video annotation tool provided in  @xcite . in total",
    ", we collected 24,000 frames ( 16 minutes ) , 25,000 frames ( 16 minutes 40 seconds ) and 20,000 frames ( 13 minutes 20 seconds ) for these three video sets v1 , v2 , and v3 respectively .",
    "we first show an example to illustrate the effectiveness of the proposed motion features for identifying the same person from different videos that are temporally synchronized . as shown in fig .",
    "[ fig : simplecase](a ) , blue bounding boxes indicate the detected cip candidates and red points indicate the improved dense trajectories for each candidate .",
    "the mph features , the color histograms in lab color channels , and the hof features are visualized below the corresponding frames . in fig .",
    "[ fig : simplecase](b ) , confusion matrices between different candidates are given when using different features ",
    "each element in the confusion matrices indicates the energy in matching one candidate from frame f1 and a candidate from frame f2 .",
    "note that c1 and d1 are the same person , and c2 and d2 are also the same person .",
    "bold font in these matrices indicates the matching energy ( i.e. , feature difference ) of the same person across these two frames and clearly the smaller , the better .",
    "we can see that when using the four motion features , these bold - font elements are usually the smallest elements in the respective confusion matrices . however , when using the color features , the bold - font elements are not the smallest in their respective confusion matrix .",
    "this shows that the motion features can be more effective than the color features in person identification when the involved people share a very similar appearance .",
    "we then evaluate the proposed algorithm on the collected three video sets . for each detected cip , denoted by @xmath69 , if there is a ground truth box @xmath70 with an overlap @xmath71 larger than 0.5 , we count this detected cip @xmath69 to be a true positive . in this way we can calculate the @xmath72 , @xmath73 , and the @xmath74-score@xmath75 .",
    "table  [ tab : performance ] shows the quantitative performance of the proposed algorithm and a state - of - the - art video co - segmentation method  @xcite , as well as the variants of the proposed algorithm using different features . for the comparison method  @xcite , instead of using the object proposal result",
    ", we directly feed the bounding boxes of the detected candidates to its pipeline .",
    "`` frame based '' and `` trajectory based '' are the variants of the proposed methods using only the frame - based features and the trajectory - based features , respectively .",
    " color based \" is another variant of the proposed method using only the color features of lab histograms instead of any motion features .",
    "we can see that the comparison method  @xcite shows a similar performance as  color based \" and both of them do not perform as good as the proposed algorithm . to demonstrate the usefulness of the location - change penalty term in eq .",
    "( [ eq : intravideo ] ) , we also report the results of the proposed algorithm without this location - change penalty term , indicated by  w / o location penalty \" in table  [ tab : performance ] .",
    ".the performance of the proposed algorithm and its variants , and a comparison video co - segmentation method  @xcite . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     ' '' ''    [ tab : humandection ]            figures  [ fig : s05 ] and  [ fig : s07 ] show the cip detection results on sample frames from v1 and v3 , respectively .",
    "blue , red and green boxes indicate the detected candidates , the detected cip and the ground truth , respectively .",
    "frames with a solid red square on the top - left corner indicate that no cip is detected by our algorithm , e.g. , they are drawn from the cip s egocentric video or the cip is occluded in these frames .",
    "frames with a solid blue square on the top - left corner indicate that no candidate is detected in these frames . as shown in fig .",
    "[ fig : s05 ] , the proposed algorithm can detect cip even if the cip shows similar appearance to other people in the same scene .",
    "from the top four rows of video 3 , the bottom row of video 1 , and the second row of video 2 in fig .",
    "[ fig : s05 ] , we can see that the proposed algorithm can handle cip missing cases , e.g. , on the frames drawn from the cip s egocentric video , by introducing the idle states .",
    "the second row of video 4 in fig .",
    "[ fig : s05 ] shows a failure case , which is caused by the partial occlusion of the cip .",
    "the top two rows of video 3 in fig .",
    "[ fig : s07 ] show another failure case where the cip is not detected because it is not among the detected candidates .    the most time consuming steps in the proposed algorithm are the extraction of the raw features , such as the dense trajectories and optical flow .",
    "the candidate detection is also time consuming .",
    "the major components of the algorithm , including the motion - feature generation , the crf construction and the crf optimization , take an average time of 20 seconds ( dependent on the number of candidates detected in a video clip ) on a laptop with intel i7 - 2620 m cpu and 4 gb ram , where each crf is constructed for a 100-frame window over 6 synchronized videos .",
    "therefore , in total 600 frames are modeled by a crf in our experiments .",
    "in this paper , we developed a new algorithm to detect co - interest persons ( cips ) from multiple , temporally synchronized videos that are taken by multiple wearable cameras from different view angles .",
    "in particular , the proposed algorithm extracts and matches the motion patterns across these videos for cip detection and can handle the case where the cip shares a very similar appearance to other nearby non - cip persons . the proposed algorithm is based on a crf model which integrates both intra - video and inter - video properties . in the experiments , we collected three video sets , each of which contains six 13 + minute gopro videos that are temporally synchronized for performance evaluation .",
    "the results show that the proposed alglorithm outperforms a state - of - the - art video co - segmentation method and other color - based methods ."
  ],
  "abstract_text": [
    "<S> wearable cameras , such as google glass and go pro , enable video data collection over larger areas and from different views . in this paper , we tackle a new problem of locating the co - interest person ( cip ) , i.e. , the one who draws attention from most camera wearers , from temporally synchronized videos taken by multiple wearable cameras . </S>",
    "<S> our basic idea is to exploit the motion patterns of people and use them to correlate the persons across different videos , instead of performing appearance - based matching as in traditional video co - segmentation / localization . this way </S>",
    "<S> , we can identify cip even if a group of people with similar appearance are present in the view . more specifically </S>",
    "<S> , we detect a set of persons on each frame as the candidates of the cip and then build a conditional random field ( crf ) model to select the one with consistent motion patterns in different videos and high spacial - temporal consistency in each video . </S>",
    "<S> we collect three sets of wearable - camera videos for testing the proposed algorithm . </S>",
    "<S> all the involved people have similar appearances in the collected videos and the experiments demonstrate the effectiveness of the proposed algorithm . </S>"
  ]
}