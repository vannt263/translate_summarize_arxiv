{
  "article_text": [
    "the lyon  fedder  mobarry ( lfm ) magnetohydrodynamical model , coupled with the mix model for the ionosphere , creating the coupled lfm  mix , is a state - of - the - art physical model for geomagnetic storms occurring in near - earth space [ @xcite ] . the lfm ",
    "mix is used to explore and understand the physics of space weather , and is a crucial part of an ongoing effort to build a space weather forecasting system . the lfm  mix contains three input parameters embedded in physical equations for turning the lfm state parameters into energy and flux [ @xcite ] .",
    "exact values of these input parameters are unknown , and our goal is to quantify the uncertainty surrounding these parameters when model output is compared to an observed storm , posing substantial statistical challenges including large spatiotemporal systems of observations and model output , as well as the need to incorporate multiple versions of the lfm  mix .",
    "geomagnetic storms play an increasingly important role in society .",
    "a recent national academy of sciences report outlined past occurrences of geomagnetic storm disruptions , and discussed the importance of preparedness in the future when the sun returns to its solar peak in 2013 , which leads to larger and more frequent geomagnetic storms [ @xcite ] .",
    "intense geomagnetic storms adversely affect satellites and can have significant associated costs ; in 1994 a canadian telecommunication satellite experienced an outage due to a strong storm , and recovery of the satellite cost between $ 50 million and $ 70 million .",
    "large storms can interact with electric grids ; a superstorm in march 1989 shut off electricity to the province of qu ' ebec , canada for nine hours . global position systems ( gps ) and communication systems",
    "are affected by large storms ; the federal aviation administration s wide area augmentation system ( waas ) is a gps location system for aircraft , whose vertical navigation system was shut down for approximately 30 hours in 2003 due to a series of powerful storms . as society has become increasingly reliant on electricity and satellite communication , the potential devastating effects of geomagnetic storms are magnified .",
    "geomagnetic storms are caused by the interaction of the plasma and magnetic field of the sun interacting with earth s magnetic field .",
    "coronal mass ejections ( cmes ) from the sun release massive twisted magnetic field configurations that can deposit substantial energy in the region of near - earth space known as the magnetosphere .",
    "the energy is stored for a while , and then is released in an explosive fashion , sending particles down magnetic field lines into the ionosphere causing the aurora borealis or northern lights .      in the computer experiments literature ,",
    "the tuning of physical model parameters to observations is called an inverse problem , and is sometimes referred to as a calibration problem [ @xcite ] .",
    "two features of our setup make the traditional approach to design and analysis of computer experiments infeasible .",
    "first , observational data and computer model output are highly multivariate ; modeling model output and observations as realizations from a gaussian process [ e.g. , as popularized by @xcite , see also @xcite and @xcite ] is impractical due to the dimensionality of the covariance matrix .",
    "the second issue is that the lfm ",
    "mix is available at multiple fidelities .",
    "in particular , solving the physical equations making up the lfm at a lower resolution yields model output that is jointly faster to calculate but does not match up as well with observations , a version we call low fidelity .",
    "alternatively , at a higher resolution the lfm yields output whose spatial features are more consistent with observational data , but which takes substantially longer to run ( approximately an eightfold increase in computation time ) , a version we call high fidelity .",
    "we aim to exploit a statistical link between the model fidelities , thereby allowing us to explore the input parameter space using the cheaper low fidelity version , while performing fewer runs of the high fidelity version .",
    "the problem of high - dimensional observations and model output has recently become acknowledged in the computer experiments literature . @xcite",
    "recommend decomposing model output and model bias terms as weighted sums of orthogonal basis functions .",
    "the weights on the basis functions are then modeled as gaussian processes .",
    "indeed , the notion of an orthogonal decomposition has been further used by various authors to reduce the high dimensionality of vector - valued model output [ @xcite ] . @xcite",
    "introduce a fast approach to calibration for large complex computer models . in the geophysical sciences ,",
    "model output is often spatiotemporal in nature , which typically gives rise to large data sets .",
    "@xcite develop a calibration approach for multivariate spatial data , modeling the model output as a gaussian process across space and input setting , exploiting a separable covariance structure .",
    "our model and data also evolve across time , and the presence of multiple fidelities of model output challenge the approach of @xcite .    accounting for multiple versions of model output is a second problem that has recently arisen in the computer experiments literature . @xcite introduce an autoregressive markov property for multiple fidelities of model output , modeling the innovation as a gaussian process . while their idea is extended to a continuum of model fidelities , a crucial and restrictive assumption is that the model output is scalar .",
    "@xcite develop an approach to combining two levels of fidelity that is extended to a bayesian hierarchical setting by @xcite .",
    "the idea is to decompose the high fidelity output as a regression on the low fidelity version , and model the intercept and slope as gaussian processes . @xcite and @xcite recommend co - kriging for multiple fidelities of output , but do not consider the issue of large data sets .",
    "we exploit similar ideas to these authors in our construction , although we must take care to reduce the dimensionality of the data , as both versions of the lfm ",
    "mix are highly multivariate .",
    "it is worth mentioning that there is some literature on emulators for multivariate computer models , but our current interest is not in emulation , but rather parameter identification [ @xcite ] .    herein we develop methodology for quantifying the uncertainty about tuning parameters for high - dimensional spatiotemporal observations and the physical model with two levels of fidelity .",
    "we exploit an empirical orthogonal function ( eof ) decomposition of the low fidelity spatial field , and an eof decomposition of a discrepancy function linking the low and high fidelity versions of the computer model .",
    "our work generalizes that of @xcite to account for large spatiotemporal data sets .",
    "the techniques introduced below also generalize the approach of @xcite to account for two levels of model fidelity .",
    "the methodology is illustrated on the lfm  mix and the lorenz ",
    "96 system of equations governing a simplified atmosphere [ @xcite ( @xcite ) ] , where we know true values of the input parameters .",
    "for both models , after initial parameter estimation , we propose a sequential design based on expected improvement [ ei , @xcite ] .",
    "our development of expected improvement generalizes the approach of @xcite to sequential design for spatiotemporal data .",
    "the physical model we examine is a coupled magnetospheric  ionospheric model for geomagnetic storms in near - earth space .",
    "the magnetohydrodynamical solver is the lyon  fedder ",
    "mobarry ( lfm ) model which consists of five physical equations defining the spatial and temporal evolution of the interaction between the solar wind and earth s magnetosphere .",
    "these five magnetohydrodynamic equations must be solved numerically by discretizing the equations to a spatiotemporal grid , using the partial donor method [ @xcite ] .",
    "there is a coarsest grid on which the equations are solved that still yields physically meaningful model output at a reduced computational cost . discretizing the equations on a finer grid by doubling the number of spatiotemporal points ( in the polar and azimuthal angle directions , as well as at a finer temporal scale ) results in higher fidelity model output , but substantially increases the computational time required to complete model runs",
    "intuitively , doubling the grid density in three directions results in a @xmath0-fold increase in computation time ; in practice , the higher resolution version is an approximately @xmath1 to @xmath2-fold increase in computation time as compared to the lower resolution . as boundary conditions ,",
    "the lfm requires solar wind , initial strength of the magnetic field , and the level of ultraviolet light from the sun . for any single geomagnetic storm ,",
    "these boundary conditions are fixed and are not considered input parameters .",
    "the lfm solver is coupled to an ionospheric model , the mix , forming the fully coupled lfm  mix .",
    "the mix model requires information about the energy and number flux of the electrons precipitating into the ionosphere along magnetic field lines .",
    "three physical equations define energy and number flux inputs .",
    "the equations relate initial energy @xmath3 , sound speed @xmath4 , number flux @xmath5 , the density of innermost cells of the magnetospheric grid @xmath6 , the field aligned electrical potential energy difference @xmath7 , and upward field aligned current @xmath8 as @xmath9 see @xcite for further discussion .",
    "an important quantity called total energy is defined as @xmath10 . here , @xmath11 , and @xmath12 are tuning factors that are included to account for physical processes outside the scope of the lfm .",
    "the exact values of these parameters are unknown , and we seek to quantify the uncertainty about these parameters when model output is compared to observations .",
    "the parameter @xmath13 accounts for the effects of calculating electron temperature from the single fluid temperature , @xmath14 is included to adjust for possible plasma anisotropy and controls a loss filling cone , while @xmath12 allows scaling of the parallel potential drop based on the sign of the current and accounts for the possibility of being outside the regime of the scaling .",
    "notice the total energy is a nonlinear function of @xmath13 and  @xmath12 , while flux is a function of @xmath14 ; later when we develop the statistical model , we take advantage of these functional relationships .    regardless of the resolution of the lfm input , the mix coupler output is always on the same spatiotemporal resolution .",
    "hence , unlike uncoupled models , the low and high resolution lfm ",
    "mix output is co - located , and we will refer to the low resolution output as low fidelity , and the high resolution output as high fidelity .",
    "this allows us to directly compute the scalar difference between the two fidelities without regridding .",
    "model output from the lfm ",
    "mix is a bivariate spatiotemporal field , for the variables of energy ( in kev ) and flux ( in @xmath15 ) . developing a bivariate spatiotemporal model is beyond the scope of the current manuscript , and we focus on uncertainty estimation using only the energy model output .",
    "the observational data set we examine is a bivariate spatiotemporal field observed during a january 10 , 1997 , geomagnetic storm from 2  pm to 4  pm utc , with 18 equally spaced time points .",
    "the storm was observed by the ultraviolet imager on the polar satellite , deriving the two variables of energy ( in kev ) and energy times flux ( in @xmath16 ) simultaneously .",
    "the observations were recorded on a grid of 170 locations , leading to a data set of 6120 correlated observations .",
    "the lfm  mix model output is on a grid of 1656 locations such that the observational grid is a subset of the model output .",
    "we require initial runs of the low and high fidelity model to inform a statistical relationship between the two . as our initial experimental design",
    ", we run the lfm  mix at a sampling of points in the three - dimensional space defined by @xmath17 , \\beta\\in[0,2.5]$ ] , and @xmath18 $ ] , which is the hyperrectangle defining physically feasible values of @xmath19 .      using the hyperrectangle @xmath20 \\times[0,2.5 ]",
    "\\times[0,0.1]$ ] of values for @xmath21 , we ran the low fidelity version at 20 sets of input settings based on a space - filling design [ @xcite ] .",
    "call this model output @xmath22 at location @xmath23 , time @xmath24 , and input setting @xmath25",
    ". we also ran the high fidelity version at a nested , space - filled subset of 5 of the original 20 .",
    "similar to the low fidelity , call the model output @xmath26 , for @xmath27 . setting up the initial design in such a way that the low and high fidelity versions are nested , that is , run at co - located input parameter settings , yields direct observations of the discrepancy @xmath28 , and assists in developing the statistical relationship between the two . if the design were not nested , we would require estimated discrepancies @xmath29 or @xmath30 to explore the statistical relationship , thereby introducing additional uncertainty .",
    "the choice of 20 and 5 runs for the low and high fidelity model , respectively , is due to the expensive computational cost of running the lfm  mix . for our study geomagnetic",
    "storm , the low fidelity model runs in 16 hours , while the high fidelity model requires approximately 84 hours per run on a linux cluster with 8 processors . in total , the initial design took approximately 740 hours to run .",
    "note the benefit of exploiting the lower fidelity , but faster running version ",
    "had we run the high fidelity model on the initial design of 20 input settings , the computational time would be approximately 1680 hours .",
    "hence , the inclusion of the cheaper low fidelity model allows us to reduce the initial computational load by about @xmath31 .      following an approach popularized by @xcite",
    ", we suppose there is an unknown setting , @xmath32 , for which the high fidelity model is an adequate representation of reality .",
    "in particular , for observations of energy ( in kev ) , @xmath33 , at grid point @xmath34 and time @xmath24 , we have @xmath35 where @xmath36 is measurement error , which we assume to be normally distributed with mean zero and variance @xmath37 .",
    "our approach slightly differs from @xcite in that we do not entertain a model discrepancy term .",
    "our setup is a large - scale inverse problem , where model discrepancy is not part of the traditional setup [ @xcite ]",
    ". we also point out that we have only one geomagnetic storm , and any model bias term would be confounded with the error process @xmath36 , without severe simplifying assumptions .    to fully exploit the information from the low fidelity model",
    ", we require a link between the coarse model @xmath38 and the higher fidelity model @xmath39 , which yields output fields that are more consistent with observational data . specifically , we link the low and high fidelity models with an additive discrepancy function @xmath40 , where @xmath41 @xcite considered including a multiplicative discrepancy function as well , yielding a decomposition of the form @xmath42 . for the lfm  mix ,",
    "both fidelities produce output fields that are of approximately the same magnitude , so we consider only an additive discrepancy function , although the greater flexibility of a full multiplicative and additive bias may be useful in other settings . by defining a statistical relationship between the low and high fidelity versions of the lfm ",
    "mix , we have inherently also developed an emulator for the high fidelity model , based on runs from the cheaper low fidelity version , but reassert that our main interest is in the parameters @xmath43 .",
    "the model and observations are highly multivariate space  time fields , where , with only one storm and 20@xmath445 initial computer model runs , we have 748,260 correlated points ( 1656 grid locations for the 25 lfm  mix output runs at 18 time points plus 170 observation locations over 18 time points ) . the traditional approach used by",
    "@xcite is challenging to implement for large space ",
    "time data sets , as this would require inverting a covariance matrix of dimension 748,260@xmath45748,260 . indeed ,",
    "in their implementation , the covariance matrix would have to be inverted at each step of an mcmc procedure .",
    "hence , with spirit similar to @xcite , we use a principal component decomposition approach to reduce dimensionality .",
    "in particular , we decompose the low resolution model output and discrepancy function as weighted sums of orthogonal spatial basis functions . in the geophysical sciences ,",
    "these spatial functions are known as empirical orthogonal functions [ eofs ; @xcite ] . in particular , define the spatial vectors @xmath46 , where @xmath47 is the total number of grid points of model output , @xmath48 is the number of time points , @xmath49 and @xmath50 .",
    "define the @xmath51 dimensional matrix @xmath52\\ ] ] so that each column is a spatial vector at a given time point and input setting .",
    "the eofs are the columns of @xmath53 , where we use the singular value decomposition @xmath54 , and the eof coefficients are contained in @xmath55 .",
    "in particular , there are @xmath56 eofs , each of which is length @xmath57 .",
    "we perform a similar decomposition for the discrepancy process @xmath58 , where there are @xmath59 eofs , each of which is length @xmath57 .",
    "our motivation for decomposing the model output as basis functions over space , rather than space ",
    "time , is driven by exploratory analysis . in particular , the first main spatial mode of variation of the low fidelity model output ( i.e. , the first eof ) exhibits a magnitude with a structured form that is similar to the physical equation ( [ eqabr ] ) and whose magnitude modulates up and down as the cme passes over the earth .",
    "this aligns with expert understanding of geomagnetic storms , as the effect of the cme passing over the earth is a period of increasing energy and flux , followed by a decay to pre - storm conditions .",
    "we statistically model the low fidelity model output as a truncated sum of weighted eofs , @xmath60 and similarly the discrepancy function as @xmath61 where the @xmath62 basis functions are the eofs contained in the @xmath53 matrices above , and the @xmath63 and @xmath64 coefficients are the loadings contained in the @xmath55 matrices .",
    "we choose sum limits of @xmath65 and @xmath66 to capture @xmath67 of variability of low fidelity model output , and @xmath68 of variability of the discrepancy process , respectively .",
    "to capture @xmath67 of variability for the discrepancy process , for example , we would require the first 26 eofs , which would detract from a parsimonious formulation ; @xcite also suggest that a gaussian process representation of high order basis function coefficients tends to perform poorly in terms of prediction . here ,",
    "@xmath69 and @xmath70 are independent mean zero normally distributed white noise error terms with variances @xmath71 and @xmath72 , respectively . the statistical model is completed by assuming the coefficient processes @xmath73 and @xmath74 are gaussian processes .    based on the physical equations that define the total energy and number flux of precipitating electrons for the mix model",
    ", we impose a nontrivial mean function on the first low fidelity loading , @xmath75 . utilizing the functional form of the total energy equation , @xmath76",
    ", we specify a nonlinear mean function @xmath77 the harmonics in the mean function are due to the nature of geomagnetic storms ; as the cme passes over the earth , the average background energy field increases in magnitude followed by a decay to the average background .",
    "the harmonics capture the physical temporal evolution of the geomagnetic storm over the period of our observations .",
    "we give the @xmath78 loading process a constant mean parameter , allowing the variability of the discrepancy process across input setting to be captured by second order structures . for all @xmath79 , @xmath80 .",
    "all that remains to be specified are the covariance functions on the eof loading processes .",
    "we use a separable matrn correlation structure [ @xcite ] .",
    "the matrn correlation is defined as @xmath81 , @xmath82 is the smoothness parameter and @xmath83 is the range parameter .",
    "the model correlation is @xmath84 where we fix the matrn smoothness at 2 .",
    "a process with matrn correlation with a smoothness of 2 has realizations that are almost twice differentiable ; in particular , this imposed assumption aligns with the evolution of the geomagnetic storm across time , as a smoothly varying process .",
    "second , numerical model output typically smoothly varies with input setting , and researchers in the computer experiments literature often use a gaussian correlation function @xmath85 , which coincides with the matrn class with infinite smoothness .",
    "however , it is well known that these gaussian correlation functions lead to numerically poorly behaved covariance matrices , and , in fact , researchers often add an artificial ridge to the covariance matrix for stability .",
    "the smoothness of a spatial process is difficult to estimate , and using a fixed smoothness of 2 on the coefficient processes implies model output varies smoothly between input settings .",
    "the model is completed by specifying the covariance functions of the eof loadings as @xmath86 the same separable covariance model is assumed for the @xmath87 coefficients , but with distinct parameters .",
    "notice that although we use a separable structure for the coefficient processes at each level of eof , the final statistical model is not separable , but rather has a covariance function that is a weighted sum of separable covariances ; this class of covariances is a type of well established product - sum covariances [ @xcite ] .",
    "@lccccc@ & & & & & + @xmath88 & 16.0 & 180 & 2804 & @xmath89 & 16.6 +   + & & & & & + @xmath90 & 11.2 & 0.22 & 0.19 & 0.1 & 0.051 + @xmath91 & 88.8 & 3.10 & 0.08 & 0.1 & 0.248 + @xmath92 & 80.1 & 2.58 & 0.24 & @xmath93 & 0.200 + @xmath94 & 24.5 & 1.05 & 0.58 & 0.01 & 0.067 + @xmath95 & 18.7 & @xmath93 & 0.03 & 3.21 & 0.046 + @xmath96 & 16.9 & 0.17 & @xmath97 & 5.98 & 0.035 + @xmath98 & 15.3 & 0.18 & 1.52 & 0.02 & 0.028 +      the main parameters of interest are the input parameters @xmath99 , and all other statistical parameters , such as mean function coefficients and covariance function ranges and variances , are of secondary interest .",
    "@xcite argue that the uncertainty in these secondary parameters is typically substantially less than the uncertainty in the input parameters , so that fixing the statistical parameters is justifiable in practice . in this light , we take an empirical bayes approach to uncertainty quantification , where the mean function parameters of the eof loading processes are estimated by ordinary least squares ( ols ) , and the remaining covariance function parameters are estimated by maximum likelihood ( ml ) , conditional on the mean estimates . the observational error is taken to be @xmath100 of the empirical standard deviation of energy observations , aligning with our collaborators expert knowledge of the typical observational error for this type of data set .",
    "table  [ tabests ] displays the ols estimates of the mean function parameters and ml estimates of the separable matrn covariance function parameters .",
    "recall the results of @xcite in that the inclusion of higher order principal component terms typically does not assist in prediction . as anticipated with a basis decomposition ,",
    "the low order coefficients have more variability than the high order coefficients ( noting that much of the variability of @xmath101 is accounted for in the nonstationary mean function ) .",
    "the input parameters in table  [ tabests ] have been standardized to the unit interval to ease comparisons between input parameter , and we see that the greatest correlation for the low fidelity decomposition is across the @xmath13 index , with @xmath14 and @xmath12 on the same order of correlation decay .",
    "the discrepancy function , on the other hand , tends to be more highly controlled by the @xmath12 index , with @xmath13 and @xmath14 sharing approximately the same decay rate of correlation on average .",
    "this indicates that , while there is some information regarding @xmath14 contained in the energy model output , there is substantially more for @xmath13 and @xmath12 , which is expected , recalling the physical equation ( [ eqabr ] ) .",
    "fixing the mean and covariance estimates , we impose independent uniform priors on @xmath11 , and @xmath12 , with uniformity over the bounding boxes described at the head of this section .",
    "define the following vectors : @xmath102 where @xmath103 is the number of locations of observations ; note we implicitly order the observations and model output ( and corresponding eofs ) such that the first @xmath104 entries are the shared locations between the observations and model output , and the last @xmath105 to @xmath57 entries of @xmath106 and @xmath107 are the model output locations with no corresponding observations . then combine these vectors into @xmath108",
    "finally , combine the high and low fidelity vectors across input settings , @xmath109 then @xmath110 is viewed as a realization from the stochastic process defined by ( [ eqobs ] ) , ( [ eqreslink ] ) , ( [ eqleof ] ) and ( [ eqdeltaeof ] ) .",
    "conditional on the realization @xmath111 , the posterior distribution of @xmath112 is sampled using a metropolis  hastings algorithm by block updating the vector @xmath112 at each step .",
    "in particular , we use independent normal proposal densities centered at the current mcmc sample , with standard deviation one - tenth of the standard deviation of the initial design points ( over @xmath112 ) .",
    "computation of the density of @xmath111 is difficult due to the large dimension ; for our initial design and observations @xmath111 is of length 748,260 .",
    "utilizing result 1 from @xcite alleviates this problem .",
    "in particular , @xcite suppose @xmath113 and @xmath114 are independent .",
    "let @xmath115 , and define @xmath116",
    ". then the likelihood function of @xmath111 can be written @xmath117 \\\\[-8pt ] \\nonumber & & { } \\times \\exp \\bigl ( -\\tfrac{1}{2 } { \\mathbf{z}}'\\bigl(\\sigma_{\\xi}^{-1 } - \\sigma_{\\xi}^{-1 } { \\mathbf{u}}\\bigl({\\mathbf{u } } ' \\sigma_{\\xi}^{-1 } { \\mathbf{u}}\\bigr)^{-1 } { \\mathbf{u } } ' \\sigma_{\\xi}^{-1}\\bigr){\\mathbf{z}}\\bigr ) l(\\hat{\\bolds{\\beta}}).\\end{aligned}\\ ] ]    in our case , @xmath53 is a block diagonal matrix of eofs , with @xmath118 blocks .",
    "the very first block corresponds to the observations and is itself a block diagonal matrix with @xmath119 identical blocks , each of which contains the truncated eofs corresponding to the observation locations : @xmath120 so that the first block of @xmath53 has dimension @xmath121 , in our case @xmath122 .",
    "the next 5 blocks of @xmath53 correspond to the high resolution model output , and again contain @xmath119 blocks of eof matrices , each of which is @xmath123 hence , each of these 5 blocks of @xmath53 is of dimension @xmath124 , in our case @xmath125 .",
    "the final 20 blocks of @xmath53 correspond to the low fidelity model output , each of which is a block diagonal matrix consisting of @xmath119 blocks of the following eof matrices : @xmath126 thus , each of the last 20 blocks of @xmath53 is of dimension @xmath127 , in our case @xmath128 .",
    "the entries of @xmath129 are eof weights @xmath130 and @xmath131 . as with the matrix @xmath53",
    ", it is convenient to divide @xmath129 into @xmath118 segments .",
    "the first segment consists of the observation eof coefficients @xmath132 where @xmath133 the following 5 segments correspond to the high fidelity runs , each of which consists of @xmath134 for @xmath27 .",
    "the final 20 segments correspond to the low fidelity runs and consist of @xmath135 for @xmath50 .",
    "note that result 1 of @xcite requires @xmath129 be centered at zero ; to this end , we apply result 1 to @xmath136 .    similar to @xmath53 and @xmath129 ,",
    "we break up @xmath137 segments of @xmath138 .",
    "the first @xmath139 have variances @xmath140 ; the following @xmath141 have variances @xmath142 and the remaining @xmath143 entries have variances @xmath144 .",
    "this completes our model s formulation of the likelihood decomposition of result 1 of @xcite .    exploiting the eof decomposition of the model output",
    "dramatically reduces dimensionality of the problem .",
    "for example , a typical gaussian process approach to our setup would require inverting a matrix of dimension @xmath145 , whereas , for example , inverting @xmath146 is feasible , as it is a matrix of dimension @xmath147 .",
    "initially , we begin by running five independent chains of posterior samples simultaneously , from random starting values . the posterior samples based on the initial design are shown in figure  [ figinitial ] as small black dots .",
    "notice the distribution is multimodal , and there is an apparent nonlinear inverse relationship between @xmath13 and @xmath12 .",
    "in fact , the curve along which the posterior samples fall for @xmath148 define a posterior distribution of total energy . recall equation ( [ eqmeanfunction ] ) , where we exploited the functional form of total energy , of a form @xmath149 .",
    "these results suggest that the quantity of total energy is well defined based on our observations and initial design , and a combination of pairs of input parameters @xmath150 that approximately yield this total energy are appropriate for our data set .",
    "notice that @xmath14 is not especially well identified based on our observations .",
    "this is expected , as we currently are modeling only energy , and @xmath14 is a controlling parameter for flux , although the information in the energy variable regarding @xmath14 is not negligible .",
    "let us illustrate the benefit of using the low fidelity model in conjunction with the high fidelity model .",
    "if there were no extra information added by including the low fidelity model output , we would expect the posterior samples based exclusively on the high fidelity version to be the same as including both model fidelities .",
    "the small grey dots of figure  [ figinitial ] are posterior samples for the input parameters based on only the five high fidelity runs , here ignoring the 20 low fidelity runs . in particular",
    ", the statistical model remains the same , except where we write @xmath151 where @xmath152 , and @xmath88 has the same functional form as ( [ eqmeanfunction ] ) .",
    "comparing the two sets of posterior samples in figure  [ figinitial ] shows the gain in augmenting the high fidelity runs with the low fidelity information  the location of the curve in panel  ( b ) for the pair @xmath150 is adjusted downward when also using the low fidelity runs and a posterior mode is ruled out .",
    "specifically , the posterior mode about @xmath153 is no longer present .",
    "hence , our posterior uncertainty regarding the parameters @xmath13 and @xmath12 has decreased due to the inclusion of the low fidelity output .",
    "the posterior samples for @xmath14 are slightly adjusted when the low fidelity information is included , although not necessarily the same amount as for @xmath13 and @xmath12 , again , due to the fact that @xmath14 is linked to flux .",
    "there are two potential explanations for the multimodal nonlinear behavior of the posterior distribution shown in figure  [ figinitial](b ) .",
    "the first is that the observations have no information regarding the specific pair of @xmath148 that is optimal or , alternatively , the curve is an artefact of the sparse initial design . in particular , with only 5 runs of the high fidelity model , it is unlikely that the discrepancy function @xmath40 has been well estimated , and given more runs of the lfm ",
    "mix , the posterior distribution may shrink to one of the modes of figure  [ figinitial ] . to this end",
    ", we develop a sequential design based on expected improvement .",
    "we seek to perform an additional run of the lfm ",
    "mix based on current information , and expected improvement ( ei ) is one approach to sequential design that incorporates accuracy and uncertainty .",
    "expected improvement was originally developed for black - box function optimization [ @xcite ] , but we adjust the idea for our purposes of parameter identification . to begin , we define the improvement function for a given location and time as minimizing the squared residual between the high fidelity model output and observations : @xmath154 where @xmath155 is the observed minimized squared residual over the initial runs of the lfm  mix .",
    "the ei is defined as a sum of expected improvement functions over all locations and times , @xmath156 and is a function only of input parameter @xmath112 .    to write the closed form of ei at an arbitrary setting @xmath112",
    ", we require the conditional distribution of the high fidelity model , given the current runs .",
    "in particular , we have @xmath157 where @xmath158 and @xmath159 are simply a conditional mean and variance of the multivariate normal defined by equations ( [ eqreslink ] ) , ( [ eqleof ] ) , and ( [ eqdeltaeof ] ) .",
    "let @xmath160 , we simplify notation by setting @xmath161 and @xmath162 and @xmath163 are the standard normal density and cumulative distribution functions , respectively",
    ". then the expected improvement at location @xmath34 and time @xmath24 has closed form @xmath164 \\\\[-8pt ] \\nonumber & & { } + \\hat{\\sigma } \\bigl ( ( \\sqrt{f_\\mathrm{min } } + \\hat{h } - y)\\phi ( q_+ ) + ( \\sqrt{f_\\mathrm{min } } + y - \\hat{h})\\phi(q_- ) \\bigr).\\end{aligned}\\ ] ] see the for a derivation . notice that ei is indeed a weighting between uncertainty @xmath165 and accuracy @xmath166 .",
    "for example , if , at a new setting @xmath112 , our predictive variance for the high fidelity model output was small , then the latter term of ( [ eqeiclosed ] ) will be negligible , and the ei will be controlled by the accuracy in the first term as a function of @xmath167 .     with input pairs",
    "at which the low fidelity model was run ( unfilled circles ) and input pairs at which both low and high fidelity models were run ( filled circles ) . ]",
    "figure  [ figei ] shows the ei surface as a function of @xmath14 and @xmath12 for the best value of @xmath13 ( @xmath168 ) . as previously ,",
    "the open circles are locations at which we ran the low fidelity model , and the closed circles are the locations at which we ran both fidelities .",
    "there are a number of interesting features illustrated by this surface .",
    "the ei surface is multimodal , with the most pronounced mode at @xmath169 , falling directly between two modes of the initial posterior samples . in this area ,",
    "the uncertainty is substantial enough that an optimum may be in the area .",
    "note there are no high fidelity model runs in the immediate area ; that the ei maximum also falls directly between two posterior sample modes indicates that ei is indeed a weighting between uncertainty and accuracy .",
    "ei is sensitive to the initial design , and at most of the locations where the low or high fidelity model was run , there are relatively low values of ei , as we have already reduced our uncertainty in those areas",
    ". however , the ei surface also follows the general trend of the initial posterior samples , indicating our initial samples fell in areas of high model accuracy .",
    "we ran the high and low fidelity version of the lfm  mix at the greatest mode indicated by the ei surface , specifically at @xmath170 , and conditional on this additional run , sampled from the posterior distribution of the input parameters .",
    "if no extra information were added due to the sequential design run , we would see the same posterior samples as in figure  [ figinitial ] .",
    "the second round of posterior samples , conditional on the initial design plus the single additional run suggested by ei , are shown in figure  [ figsecond ] .",
    "the substantial change between figures  [ figinitial ] and [ figsecond ] can be seen in the third panel ( c ) , the pairwise posterior samples for @xmath14 and @xmath12 . in particular ,",
    "the upper leftmost mode that was present in figure  [ figinitial](c ) has been ruled out now , as there are no posterior samples in this area .",
    "our posterior uncertainty has decreased due to the single additional run suggested by ei .",
    "our information regarding @xmath12 has also increased due to the added ei run , as the initial middle mode about @xmath171 has now split into two smaller modes .    in previous experiments with the lfm ",
    "mix , continuing sequential design based on ei improves the posterior distribution of @xmath150 slowly and primarily explores the three - dimensional @xmath19 space over @xmath14 .",
    "this reiterates the substantial uncertainty in @xmath14 based on the energy variable alone , and , unfortunately , due to the high budgetary demand of running the lfm  mix , at 100 hours for each run of the high and low fidelity model on 8 processors , it is not within our current budget to continue the sequential design .",
    "future work is aimed at including observations for flux , which we anticipate greatly improving identification of @xmath14 .",
    "in the previous section we outlined a statistical model for combining high and low fidelity model output for large spatiotemporal data sets with an application of quantifying the uncertainty in input parameters for the lfm  mix computer model .",
    "the initial posterior distributions illustrated a strong nonlinear relationship between the parameters @xmath13 and @xmath12 , and based on a sequential design framework , we saw the posterior distributions shrink in variability , ruling out an area of the parameter space present in the initial multimodal posterior distribution . in this section",
    "we illustrate a similar statistical model using a physical model with known truth .",
    "the goal in this section is to compare our ability to identify model parameters using the eof approximation model with differing initial design sizes , and to assess the ability of sequential design under expected improvement in improving the posterior estimates of unknown parameters .",
    "the lorenz  96 system ( hereafter l96 ) of equations was developed by edward lorenz to be a simplified one - dimensional atmospheric model that exhibits chaos [ @xcite ] .",
    "the physical model is for 40 variables ( known as state variables in the atmospheric sciences ) . for variable @xmath33 , location @xmath172 and time  @xmath24",
    ", we have @xmath173 \\\\[-8pt ] \\nonumber & & { } - y ( { \\mathbf{s}},t ) + f({\\mathbf{s}}),\\end{aligned}\\ ] ] where @xmath174 is a location dependent forcing term , and @xmath33 is available at any integer value of @xmath34 by setting @xmath175 . for the forcing term , @xcite used @xmath176 , but for our purposes we wish to mimic the behavior of the lfm  mix using this reduced atmospheric model .",
    "analogous to the lfm  mix case , we have two forcing functions , corresponding to a low and a high fidelity simulator .",
    "in particular , we , respectively , define the low and high fidelity forcing functions as @xmath177 notice the functional form here , @xmath178 , is akin to the total energy equation of the lfm  mix , which was of the form @xmath179 .",
    "fixing true values of @xmath180 and @xmath181 at @xmath182 and @xmath183 , respectively , the first panel of figure  [ figlorenz ] shows the corresponding forcing functions for the low and high fidelity versions .",
    "notice the low fidelity version appears to smear out the peak defined by the high fidelity forcing function .",
    "this is akin to the relationship between the differing fidelities of the lfm ",
    "mix , where the low fidelity model tends to produce output that is a ( spatially ) less peaked version of the more peaked high fidelity model output .     and @xmath184 .",
    "]    the observations are generated from the high fidelity version of the l96 , based on 40 independent initial unit uniform random variables . solving the equations every 6 hours",
    ", we run the l96 for 300 years , and use 30-year averaged output , garnering approximate climate of the l96 .",
    "the motivation for time - averaging is that each single realization from the l96 is highly erratic , as seen in figure  [ figlorenz ] , whereas taking time - averages over long periods tends to reproduce the forcing function , also displayed in figure  [ figlorenz ] . to these 10 time realizations ,",
    "we add independent normal errors for each variable at all time points , whose mean is zero and whose standard deviation is five percent of the empirical standard deviation of the model output , again to line up with the expert understanding of measurement error for the lfm ",
    "mix example .",
    "we suppose it is known that @xmath185 $ ] and @xmath186 $ ] .",
    "to explore different design approaches , we run two initial designs .",
    "the first design assumes greater resources than are available for the lfm  mix .",
    "in this situation , we run the low fidelity model at 40 pairs of input settings based on a space - filling design , and the high fidelity model at a space - filled subset at 20 points of the original 40 .",
    "this setup is designed is to illustrate our ability to tune model parameters in the situation with more resources than are currently available .",
    "the second design utilizes a space - filled subset of 20 runs of the low fidelity computer model , with an additional 5 runs of the high fidelity version , aligning directly with our setup for the lfm  mix scenario .    to align with the lfm ",
    "mix modeling approach , we suppose the observations are adequately represented by the high fidelity version of l96 , up to white noise . in particular , using similar notation as in the previous section where @xmath187 , we write @xmath188 where @xmath36 is a white noise process , which we assume to be normally distributed with mean zero and variance @xmath37 . as with the lfm ",
    "mix , we link the low and high fidelity models with an additive discrepancy function @xmath40 , where @xmath189    whereas the lfm  mix is highly multivariate , our l96 example does not require the same dimension reduction techniques employed earlier .",
    "although not required , we use similar modeling techniques to those employed for the lfm ",
    "mix above in order to explore our ability to identify physical parameters in a setting where approximations are required .",
    "hence , we write @xmath190 and @xmath191 putting @xmath192 and @xmath193 ( capturing more than @xmath67 of the variability ) , the residual processes @xmath194 and @xmath195 are modeled as normally distributed white noise terms with variances @xmath144 and @xmath196 , respectively . as in the lfm  mix case , we model @xmath197 , and @xmath198 as gaussian processes .",
    "each is endowed with a mean function of the form @xmath199 , a functional form that was decided upon after elementary data analysis ; notice we find similar behavior to the @xmath178 form of the forcing functions ( [ eqfl ] ) and ( [ eqfh ] ) . unlike the lfm ",
    "mix , we suppose the @xmath63 and @xmath64 processes are independent across time ; indeed , with the l96 , we consider long term averages , and viewing the realizations as independent across time is justifiable , whereas in the lfm ",
    "mix case , our realizations arise from a continuous process over a relatively short time interval .",
    "the functional form of the covariance for the @xmath63 and @xmath64 coefficient processes is @xmath200 , where @xmath201 , and @xmath202 where naturally each @xmath203 , and @xmath78 has distinct covariance and regression parameters .    for physical parameter estimation , we sample the posterior distribution of @xmath112 conditional on @xmath111 , which is made up of the following components .",
    "define the vectors @xmath204 , @xmath205 , and @xmath206 , where the number of low and high fidelity samples are @xmath207 and @xmath208 , respectively .",
    "combine these vectors into the single time point vector @xmath209 , then @xmath210 .",
    "posterior distributions are shown in figure  [ figlorenzcalibrate ] , with the truth indicated by the intersection of solid lines .",
    "we consider three cases for posterior sampling ",
    "the first is based on a dense design of @xmath211 and @xmath212 , shown in panel ( 1 ) .",
    "the posterior distribution covers the truth , but is spread over a swath of plausible values , falling along a curve of the form @xmath213 , exhibiting similar behavior as the lfm  mix ; note the substantially larger initial design size , however .",
    "the posterior mode is at approximately @xmath214 , indicating accurate point estimation , but still displaying substantial uncertainty .    , indicated by the intersection of two solid lines .",
    "each panel contains posterior densities with contours overlying posterior samples for ( 1 ) : large initial design , ( 2 ) : sparse initial design similar to the lfm  mix , ( 3 ) : sparse initial design with seven additional runs chosen sequentially by expected improvement .",
    "input settings at which the low fidelity model was run are displayed as circles both filled and unfilled , and settings where the high fidelity model was also run are shown as filled circles . ]",
    "the middle panel of figure  [ figlorenzcalibrate ] replicates the situation of the lfm ",
    "mix more closely in that we use only @xmath215 and @xmath216 points in the initial design .",
    "the posterior distribution covers the true value of @xmath217 , and again we see a swath of density following a curve similar to @xmath213 . here , however , the posterior mode is at @xmath218 , so while the truth is indeed captured within the posterior samples , there appears to be some bias . following this sparse initial sample , we run both low and high fidelity versions of the l96 at seven additional input settings chosen sequentially based on the expected improvement criterion .",
    "the final panel of figure  [ figlorenzcalibrate ] displays the posterior distributions based on these @xmath219 and @xmath220 samples .",
    "indeed , the posterior variability has decreased as compared to that based on the initial design , but also notice that the posterior has substantially less variability than the dense initial sample of panel ( 1 ) .",
    "these results suggest we can perform fewer runs initially , and rely on a sequential design such as expected improvement to home in on the true values .",
    "the posterior mode after sequential design is approximately @xmath221 , indicating accurate posterior estimation .",
    "an interesting note is that the final posterior distribution displays three distinct modes ( although the mode about the truth is of higher posterior density ) . given that the sequential design runs cover the posterior modes , we do not anticipate the posterior distribution improving greatly , but reiterate that the posterior distribution contains and is indeed centered about the truth .",
    "we have introduced an approach to quantify the uncertainty about input parameters for large spatiotemporal data sets with high and low fidelity model outputs .",
    "we suppose the high fidelity model is an adequate representation of reality at some unknown set of input parameters up to white noise .",
    "the high and low fidelity models are linked through an additive discrepancy function .",
    "this link allows us to run the higher cost high fidelity model at fewer sets of input parameters , and explore the input setting space with the cheaper low fidelity model . in our first example we examined the lfm  mix model for geomagnetic storms occurring in earth s near space environment , which is partially parameterized by three unknown input parameters controlling energy and flux .",
    "based on an initial experimental design , using observations of energy , we discovered a nonlinear relationship between a subset of the input settings , which was a level curve for the total energy quantity .",
    "one input setting was not well identified , but considering that particular variable contributes mainly to flux , it is unsurprising that it is not well identified using only energy observations .    to improve posterior estimation , we developed an expected improvement criterion for sequential design .",
    "the improvement function seeks to minimize squared distance between the high fidelity model and observations .",
    "we derived the closed form for ei over arbitrarily many locations and times , which simultaneously weights uncertainty and accuracy .",
    "based on the ei criterion , we performed an additional run of the lfm  mix and found that the posterior distributions for the input parameters indeed shrunk in width .",
    "this suggests that the nonlinear behavior of the initial posterior distribution is potentially an artefact of our sparse initial design . comparing these results to the contrived lorenz  96 system with known truth",
    ", we would anticipate some improvement manifesting as smaller posterior variability if we were to continue sequential design based on ei , with the posterior mode eventually settling around the true unknown parameter value .    in a previous set of experiments",
    ", we explored sequential design based on ei , and found that the criterion primarily becomes overwhelmed by the uncertainty surrounding the input parameter involving flux .",
    "due to the high budgetary demand of running the lfm  mix , it is not within our current capacity to continue the sequential design .",
    "our current research is aimed at including observations for flux , which we anticipate greatly improving the posterior distributions of all three input parameters .",
    "we reduced dimensionality of the large data set by projecting spatial fields onto empirical orthogonal functions ; the motivation was driven by exploratory analysis where the first main mode of spatial variation exhibited a magnitude with functional form similar to physical equations governing energy and flux for the lfm  mix .",
    "in other contexts for other space  time computer models , a different approach may be required .",
    "for instance , if the model output is a highly nonlinear response of input parameters , a principal component approach is likely to be unsuccessful in statistically modeling physical model output . in such cases",
    "the practitioner may need to perform statistical tests for space ",
    "time separability , such as those developed by @xcite or @xcite .",
    "the clearest route of future research is to develop a bivariate model for energy and flux , which will allow us to simultaneously identify the three parameters controlling these two distinct variables .",
    "one potential solution to this added complication is to use a similar eof decomposition for flux , and use a multivariate gaussian process representation for the eof coefficient processes for both energy and flux , thereby accounting for correlation between the two distinct variables .",
    "the statistical model did not account for systematic model bias .",
    "our approach is consistent with the mathematical formulation of solving large scale inverse problems using computer models and observed data [ see , e.g. , the cosmic microwave background application in @xcite ] . with",
    "only one observed geomagnetic storm , model bias is confounded with the residual process ; with multiple storms we could potentially include a full bias term across space and time .",
    "however , it is believed by space physicists that the infinite resolution version of the lfm  mix is unbiased , and",
    "our high fidelity version is an approximation to this infinite resolution .",
    "the discrepancy function we introduced connected the low and high fidelity versions of the model , which is notably different than the original suggestion of @xcite of including an additive model discrepancy term .",
    "in our situation , we have only one realization of the spatiotemporal process and , hence , model bias is unidentifiable without some simplifying assumptions ( such as constancy across time or space ) .",
    "@xcite also examine the lfm  mix , taking a predictive process approach to dimension reduction [ @xcite ] , and assume a rotational bias across time .",
    "that is , the authors assume there is an unknown spatial rotation at each time point that defines model bias for the high fidelity version .",
    "their posterior distributions differ from those found herein , generally centering on approximately @xmath222 .",
    "this is not contradictory to our results in that the assumptions regarding model bias are different  indeed , optimal parameter values under _ rotated _ model output are expected to be different than those under no such rotations . with additional geomagnetic storms , our goal is to determine the need for such rotations and potentially fully general space ",
    "time model biases , but it is currently unclear which of these competing assumptions is necessary .",
    "the low and high fidelity versions of the lfm  mix are generated by differing resolutions of the lfm model . while in the current work we used only two resolutions",
    ", there is potential for a higher resolution available that is extremely computationally intense , and must be run on a supercomputer on at least 32 processors .",
    "potentially , one way to include this `` highest '' fidelity is to maintain our model s formulation , and write the high fidelity model as a sum of the highest fidelity and a secondary discrepancy function .",
    "it is likely that the discrepancy connecting the lower fidelities will be correlated with the discrepancy connecting the higher fidelities and , hence , we anticipate requiring a multivariate gaussian process model for the discrepancy processes .    [ app ]",
    "in this appendix we derive the closed form for the expected improvement at a single location @xmath34 and time @xmath24 , equation ( [ eqeiclosed ] ) . for notational simplicity ,",
    "write @xmath223 , @xmath224 , and @xmath225",
    ". then we have @xmath226 utilizing the change of variables @xmath227 .",
    "the three integrals of @xmath228 , and @xmath229 can be written @xmath230 using integration by parts and the fact that the antiderivative of @xmath231 is @xmath232 . combining terms yields ( [ eqeiclosed ] ) .",
    "we gratefully acknowledge doug nychka for numerous discussions and providing the lorenz  96 code .",
    "the national center for atmospheric research is managed by the university corporation for atmospheric research under the sponsorship of nsf ."
  ],
  "abstract_text": [
    "<S> geomagnetic storms play a critical role in space weather physics with the potential for far reaching economic impacts including power grid outages , air traffic rerouting , satellite damage and gps disruption . </S>",
    "<S> the lfm  </S>",
    "<S> mix is a state - of - the - art coupled magnetospheric  </S>",
    "<S> ionospheric model capable of simulating geomagnetic storms . imbedded in this model </S>",
    "<S> are physical equations for turning the magnetohydrodynamic state parameters into energy and flux of electrons entering the ionosphere , involving a set of input parameters . </S>",
    "<S> the exact values of these input parameters in the model are unknown , and we seek to quantify the uncertainty about these parameters when model output is compared to observations . </S>",
    "<S> the model is available at different fidelities : a lower fidelity which is faster to run , and a higher fidelity but more computationally intense version . </S>",
    "<S> model output and observational data are large spatiotemporal systems ; the traditional design and analysis of computer experiments is unable to cope with such large data sets that involve multiple fidelities of model output . </S>",
    "<S> we develop an approach to this inverse problem for large spatiotemporal data sets that incorporates two different versions of the physical model . after an initial design </S>",
    "<S> , we propose a sequential design based on expected improvement . </S>",
    "<S> for the lfm  </S>",
    "<S> mix , the additional run suggested by expected improvement diminishes posterior uncertainty by ruling out a posterior mode and shrinking the width of the posterior distribution . </S>",
    "<S> we also illustrate our approach using the lorenz  96 system of equations for a simplified atmosphere , using known input parameters . for the lorenz  96 system , after performing sequential runs based on expected improvement , the posterior mode converges to the true value and the posterior variability is reduced .    ,    ,    ,    , </S>"
  ]
}