{
  "article_text": [
    "general matrix - matrix multiplication ( gemm ) is one of the most crucial operations in computational science and modeling .",
    "the operation multiplies a matrix @xmath0 of size @xmath1 with a matrix @xmath2 of size @xmath3 and gives a resulting matrix @xmath4 of size @xmath5 . in many linear solvers and graph problems such as algebraic multigrid method ( amg )  @xcite , breadth",
    "first search  @xcite , finding shortest path  @xcite , colored intersection  @xcite and sub - graphs  @xcite , it is required to exploit sparsity of the two input matrices and the resulting matrix because their dense forms normally need huge storage space and computation cost for the zero entries . therefore general sparse matrix - matrix multiplication ( spgemm ) becomes a common building block in these applications .",
    "compared to cpus , modern graphics processing units ( gpus ) promise much higher peak floating - point performance and memory bandwidth .",
    "thus a lot of research has concentrated on gpu accelerated sparse matrix - dense vector multiplication  @xcite and sparse matrix - dense matrix multiplication  @xcite and achieved relatively attractive performance . however , despite the prior achievements on these gpu sparse blas routines , massive parallelism in gpus is still significantly underused for the spgemm algorithm , because it has to handle three more challenging problems : ( 1 ) the number of nonzero entries in the resulting matrix is unknown in advance , ( 2 ) very expensive parallel insert operations at random positions in the resulting matrix dominate the execution time , and ( 3 ) load balancing must account for sparse data in both input matrices with diverse sparsity structures .",
    "previous gpu spgemm methods  @xcite have proposed a few solutions for the above problems and demonstrated relatively good time and space complexity .",
    "however , the experimental results showed that they either only work best for fairly regular sparse matrices  @xcite , or bring extra high memory overhead for matrices with some specific sparsity structures  @xcite .",
    "moreover , in the usual sense , none of these methods can constantly outperform well optimized spgemm approach  @xcite for multicore cpus .",
    "our work described in this paper particularly focuses on improving gpu spgemm performance for matrices with arbitrary irregular sparsity structures by proposing more efficient methods to solve the above three problems on gpus and emerging cpu - gpu heterogeneous processors .    in this paper",
    ", we make the following contributions :    * * _ a framework for fast spgemm_*. we design a 4-stage framework for implementing spgemm on manycore platforms including homogeneous gpus and heterogeneous processors composed of cpu cores , gpu cores and shared virtual memory .",
    "this framework effectively organizes memory allocation , load balancing and gpu kernel launches . * * _ a hybrid method for the resulting matrix pre - allocation_*. we present a hybrid method that initially allocates memory of upper bound size for short rows and progressively allocates memory for long rows .",
    "the experimental results show that our method saves a large amount of global memory space and efficiently utilizes the very limited on - chip scratchpad memory .",
    "* * _ parallel insert operations through fast merging_*. we propose an efficient parallel insert method for long rows of the resulting matrix by using the fastest merge algorithm available on gpus .",
    "we make an experimental evaluation and choose gpu merge path algorithm from five candidate gpu merge approaches . * * _ heuristic - based load balancing_*. we develop a load balancing oriented heuristic method that assigns rows of the resulting matrix to multiple bins with different subsequent computational methods .",
    "our approach guarantees load balancing in all calculation steps .",
    "our framework and corresponding algorithms delivers excellent performance in two experimental scenarios : ( 1 ) calculating triple matrix galerkin products ( i.e. , @xmath6 ) in amg for 2d and 3d poisson problems , and ( 2 ) computing matrix squaring ( i.e. , @xmath7 ) on a benchmark suite composed of 23 sparse matrices with diverse sparsity structures .    in the context of galerkin products",
    ", our method constantly outperforms the state - of - the - art gpu spgemm methods in two vendor supplied libraries cusparse and cusp .",
    "average speedups of 1.9x ( up to 2.6x ) and 1.7x ( up to 2.7x ) are achieved when compared to cusparse and cusp , respectively .    in the context of matrix squaring ,",
    "more comparison methods are included .",
    "_ first _ , on two nvidia gpus ( i.e. , a geforce gtx titan black and a geforce gtx 980 ) , compared with cusparse and cusp , our approach delivers on average 3.1x ( up to 9.5x ) and 4.6x ( up to 9.9x ) speedups , respectively .",
    "_ second _ , compared to a recently developed cuda - specific spgemm method rmerge  @xcite , our method offers on average 2.5x ( up to 4.9x ) speedup on the nvidia geforce gtx 980 gpu .",
    "_ third _ , compared to the spgemm method in the latest intel math kernel library ( mkl ) on a six - core xeon e5 - 2630 cpu and quad - channel system memory , our method gives on average 2.4x ( up to 5.2x ) and 2.1x ( up to 4.2x ) speedups on the nvidia geforce gtx 980 gpu and an amd radeon r9 290x gpu , respectively .",
    "furthermore , our approach can utilize re - allocatable memory controlled by cpu - gpu heterogeneous processors . on an amd a10 - 7850k heterogeneous processor ,",
    "compared to merely using its gpu cores , our framework delivers on average 1.2x ( up to 1.8x ) speedup while utilizing re - allocatable shared virtual memory in the system .",
    "for the sake of generality , the spgemm algorithm description starts from discussion of the gemm and gradually takes sparsity of the matrices @xmath0 , @xmath2 and @xmath4 into consideration . for the matrix @xmath0 , we write @xmath8 to denote the entry in the @xmath9th row and the @xmath10th column of @xmath0 and @xmath11 to denote the vector consisting of the @xmath9th row of @xmath0 .",
    "similarly , the notation @xmath12 denotes the @xmath10th column of @xmath0 . in the gemm",
    ", the @xmath9th row of the resulting matrix @xmath4 can be defined by @xmath13 where the operation @xmath14 is the dot product of the two vectors .",
    "we first give consideration to the sparsity of the matrix @xmath0 . without loss of generality , we assume that the @xmath9th row of @xmath0 only consists of two nonzero entries in the @xmath15th and the @xmath16th column , respectively .",
    "thus @xmath11 becomes @xmath17 .",
    "since all other entries are zeros , we do not record them explicitly and ignore their influence on the dot products in the calculation of the @xmath9th row of @xmath4 .",
    "then we obtain @xmath18    we can see in this case , only entries in the @xmath15th and the @xmath16th row of @xmath2 have contribution to the @xmath9th row of @xmath4 . then row vector form instead of column vector form",
    "is used for the matrix @xmath2 .",
    "so we obtain @xmath19    since the matrix @xmath2 is sparse as well , again without loss of generality , we assume that the @xmath15th row of @xmath2 has only two nonzero entries in the @xmath20th and the @xmath21th column , and the @xmath16th row of @xmath2 also has only two nonzero entries in the @xmath22th and the @xmath21th column",
    ". so the two rows are given by @xmath23 and @xmath24 .",
    "then @xmath25    because the matrix @xmath4 is also sparse and the @xmath9th row of @xmath4 only has three nonzero entries in the @xmath20th , the @xmath22th and the @xmath21th column , the row can be given by @xmath26 where @xmath27 , @xmath28 and @xmath29 .",
    "in general there are more nonzero entries per rows of the matrices @xmath0 , @xmath2 and @xmath4 .",
    "but from the above derivation we can see that the spgemm can be represented by operations on row vectors of the matrices .",
    "therefore , in this work we store all sparse matrices in compressed sparse row ( csr ) format .",
    "the csr format of a matrix consists of three separate arrays : ( 1 ) row pointer array of size @xmath30 , where @xmath31 is the number of rows of the matrix , ( 2 ) column index array of size @xmath32 , where @xmath32 is the number of nonzero entries of the matrix , and ( 3 ) value array of size @xmath32 .",
    "hence the overall space complexity of the csr format is @xmath33 .",
    "actually compressed sparse column ( csc ) format is also widely used for sparse matrices stored in column - major order  @xcite .",
    "the spgemm in the csc format is almost the same as in the csr format except rows are changed to columns and vice versa .",
    "the above csr - based spgemm algorithm can be performed by pseudocode in algorithm  [ spgemm.jpdc.alg.spgemm ] .",
    "an early description of this algorithm was given by gustavson  @xcite .    `",
    "set ` @xmath34 ` to ` @xmath35 ` load ` @xmath36 @xmath37 ` insert ` @xmath38 ` to ` @xmath34 @xmath39 @xmath40      a classic cpu spgemm algorithm , also known as matlab algorithm , was proposed by gilbert et al .  @xcite .",
    "this approach uses a dense vector - based sparse accumulator ( or spa ) and takes @xmath41 time to complete the spgemm , where @xmath42 is defined as the number of necessary arithmetic operations on the nonzero entries , @xmath43 is defined as the number of nonzero entries in the matrix @xmath2 , and @xmath31 is the number of rows / columns of the input square matrices .",
    "matam et al .",
    "@xcite developed a similar matlab algorithm implementation for gpus .",
    "sulatycke and ghose  @xcite proposed a cache hits - oriented algorithm runs in relatively longer time @xmath44 . a fast serial spgemm algorithm with time complexity @xmath45",
    "was developed by yuster and zwick  @xcite .",
    "bulu and gilbert  @xcite presented an spgemm algorithm with time complexity independent to the size of the input matrices under assumptions that the algorithm is used as a sub - routine of 2d distributed memory spgemm and the input matrices are hypersparse ( @xmath46 ) .",
    "recent gpu - based spgemm algorithms showed better time complexity .",
    "the spgemm algorithm in the cusparse library  @xcite utilized gpu hash table for the insert operations ( lines 711 in algorithm  [ spgemm.jpdc.alg.spgemm ] ) .",
    "so time complexity of this approach is @xmath47 on average and @xmath48 in the worst case , where @xmath49 is defined as the average number of nonzero entries in the rows of the matrix @xmath4 .",
    "because the algorithm allocates one hash table of fixed size for each row of @xmath4 , the space complexity is @xmath50 .",
    "the cusp library  @xcite developed an spgemm method called expansion , sorting and compression ( esc ) that expands all candidate nonzero entries generated by the necessary arithmetic operations ( line 6 in algorithm  [ spgemm.jpdc.alg.spgemm ] ) into an intermediate sparse matrix @xmath51 , sorts the matrix by rows and columns and compresses it into the resulting matrix @xmath4 by eliminating entries in duplicate positions . by using gpu radix sort algorithm ( with linear time complexity while size of the index data type of the matrices is fixed ) and prefix - sum scan algorithm ( with linear time complexity ) as building blocks , time complexity of the esc algorithm is @xmath52 .",
    "since @xmath53 equals half of @xmath42 , the esc algorithm takes the optimal @xmath47 time .",
    "dalton et al .",
    "@xcite improved the esc algorithm by executing sorting and compression on the rows of @xmath51 , but not on the entire matrix .",
    "therefore fast on - chip memory has a chance to be utilized more efficiently .",
    "the improved method sorts the very short rows ( of size no more than 32 ) by using sorting network algorithm ( with time complexity @xmath54 ) instead of the radix sort algorithm which is mainly efficient for long lists .",
    "so the newer method is more efficient in practice , even though its time complexity is not lower than the original esc algorithm .",
    "because both of the esc algorithms allocate an intermediate matrix @xmath51 , they have the same space complexity @xmath55 ) .",
    "rmerge algorithm , recently proposed by gremse et al .",
    "@xcite , iteratively merges rows in the matrix @xmath2 into the resulting matrix @xmath4 .",
    "because this approach underutilizes thread interaction and generates one intermediate sparse matrix for each iteration step , it works best for input matrices with evenly distributed short rows . for irregular input matrices , load imbalance and large memory allocation",
    "make this method inefficient .      because cuda and opencl are both widely used in gpu programming and they actually deliver comparable performance  @xcite , our spgemm algorithm support both of them .",
    "we use cuda implementation on nvidia gpus and opencl implementation on amd gpu in our spgemm evaluation .    for simplicity",
    ", we define the following unified terminologies : ( 1 ) _ thread _ denotes _ thread _ in cuda and _ work item _ in opencl , ( 2 ) _ thread bunch _ denotes _ warp _ in nvidia gpu and _ wavefront _ in amd gpu , ( 3 ) _ thread group _ denotes _ thread block _ or _ cooperative thread array ( cta ) _ in cuda and _ work group _ in opencl , ( 4 ) _ core _ denotes _ streaming multiprocessor ( smx ) _ or _",
    "maxwell streaming multiprocessor ( smm ) _ in nvidia gpu and _ compute unit _ in amd gpu , and ( 5 ) _ scratchpad memory _ denotes _ shared memory _ in cuda and _ local memory _ in opencl .",
    "compared to spgemm , other sparse matrix multiplication operations ( e.g. , multiplication of sparse matrix and dense matrix  @xcite and its special case sparse matrix - vector multiplication  @xcite ) pre - allocate a dense resulting matrix or vector .",
    "thus the size of the result of the multiplication is trivially predictable , and the corresponding entries are stored to predictable memory addresses .",
    "however , because the number of nonzero entries in the resulting sparse matrix @xmath4 is unknown in advance , precise memory allocation of the spgemm is impossible before real computation .",
    "moreover , physical address of each new entry is unknown either ( consider line 7 in algorithm  [ spgemm.jpdc.alg.spgemm ] , the position @xmath15 is only a column index that can not trivially map to a physical address on memory space ) .    to solve this problem ,",
    "the previous spgemm algorithms proposed four different solutions : ( 1 ) precise method , ( 2 ) probabilistic method , ( 3 ) upper bound method , and ( 4 ) progressive method .",
    "the first method , _ precise method _ , pre - computes a simplified spgemm in the same computational pattern .",
    "we can imagine that multiplication of sparse boolean matrices is more efficient than multiplication of sparse floating - point matrices .",
    "rmerge algorithm and the spgemm methods in cusparse and mkl are representatives of this approach .",
    "even though the pre - computation generates precise size of @xmath56 , this method is relatively expensive since the spgemm operation in the same pattern is executed twice .    the second method , _ probabilistic method _ , estimates an imprecise @xmath56 .",
    "this group of approaches  @xcite are based on random sampling and probability analysis on the input matrices .",
    "since they do not guarantee a safe lower bound for the resulting matrix @xmath4 and extra memory has to be allocated while the estimation fails , they were mostly used for estimating the shortest execution time of multiplication of multiple sparse matrices .    the third method , _ upper bound method _",
    ", computes an upper bound of the number of nonzero entries in the resulting matrix @xmath4 and allocates corresponding memory space .",
    "numerically , the upper bound size equals @xmath53 , or half of @xmath42 , the number of necessary arithmetic operations .",
    "the esc algorithms use this method for memory pre - allocation .",
    "even though this approach saves cost of the pre - computation in the precise method , it brings another problem that the intermediate matrix @xmath51 may be too large to fit in the device global memory .",
    "since the spgemm algorithm does not take into consideration cancellation that eliminates zero entries generated by arithmetic operations , the resulting matrix is normally larger than the input matrices .",
    "table  [ spgemm.jpdc.tab.benchmarksuite ] shows that @xmath53 is much larger than @xmath56 while squaring some matrices .",
    "for example , the sparse matrix _ wind tunnel",
    "_ generates 626.1 million nonzero entries ( or 7.5 gb memory space for 32-bit index and 64-bit value ) for the intermediate matrix @xmath51 while the real product @xmath4 ( i.e. , @xmath7 ) only contains 32.8 million nonzero entries .",
    "although the upper bound method can partition the intermediate matrix @xmath51 into multiple sub - matrices , higher global memory pressure may reduce overall performance .",
    "the last method , _ progressive method _ , first allocates memory of a proper size , starts sparse matrix computation and re - allocates the buffer if larger space is required .",
    "some cpu sparse matrix libraries use this method .",
    "for instance , sparse matrix computation in the matlab  @xcite increases the buffer by a ratio of 50% if the current memory space is exhausted .",
    "since the upper bound method sacrifices space efficiency for the sake of improved performance and the progressive method is good at saving space , we use a hybrid method composed of the both approaches .",
    "however , compared to the relatively convenient upper bound method , it is hard to directly implement a progressive method for discrete gpus .",
    "the reason is that although modern gpu devices have the ability of allocating device global memory while kernels are running , they still can not re - allocate device memory on the fly .",
    "we will describe our hybrid method designed for discrete gpus in the next section .    on the other hand , emerging heterogeneous processors , composed of multiple cpu cores and gpu cores in one chip , supply both flexibility and efficiency .",
    "amd accelerated processing units ( apus )  @xcite , intel multi - cpu and gpu system - on - a - chips ( soc ) devices  @xcite , nvidia echelon heterogeneous gpu architecture  @xcite , and many mobile processors ( e.g. , nvidia tegra  @xcite and qualcomm snapdragon  @xcite ) are representatives of the heterogeneous processor .",
    "heterogeneous system architecture ( hsa )  @xcite and opencl 2.0  @xcite deliver programming tools for some heterogeneous processors . in this architecture , integrated gpu cores can directly use system memory allocated by the cpu part .",
    "then data transfer through connection interfaces such as pcie link can be avoided to obtain higher performance  @xcite .",
    "this gives our spgemm algorithm a chance to let integrated gpus use re - allocatable system memory for a better overall performance .",
    "later on , we will show the corresponding performance gain by using an amd apu .      as shown in algorithm  [ spgemm.jpdc.alg.spgemm ] , for each trivial arithmetic computation ( line 6 ) ,",
    "one much more expensive insert operation ( lines 711 ) is required . to the best of our knowledge",
    ", none of the previous gpu spgemm methods takes into account that the input sequence ( line 4 ) is ordered because of the csr format .",
    "one of our algorithm design objectives is to efficiently utilize this property .",
    "based on experiments by kim et al .",
    "@xcite , as the simd units are getting wider and wider , merge sort methods will outperform hash table methods on the join - merge problem , which is a similar problem in the spgemm .",
    "then our problem converts to finding a fast gpu method for merging sorted sequences . later on we will describe our strategy in detail .      because distribution patterns of nonzero entries in both input sparse matrices can be very diverse ( consider plots of the matrices in table  [ spgemm.jpdc.tab.benchmarksuite ] )",
    ", input space - based data decomposition  @xcite normally does not bring efficient load balancing .",
    "one exception is that computing spgemm for huge sparse matrices on large scale distributed memory systems , 2d and 3d decomposition on input space methods demonstrated good load balancing and scalability by utilizing efficient communication strategies  @xcite .",
    "however , in this paper we mainly consider load balancing for fine - grained parallelism in gpu and cpu - gpu shared memory architectures .    therefore we use the other group of load balancing methods based on output space decomposition .",
    "dalton et al .",
    "@xcite presented a method that sorts rows of the intermediate matrix @xmath51 , divides it into 3 sub - matrices that include the rows in different size ranges , and uses differentiated esc methods for the sub - matrices .",
    "we have a similar consideration , but our implementation is completely different . we do not strictly sort rows of the intermediate matrix @xmath51 but just assign rows to a fixed number of bins through a much faster linear time traverse on cpu .",
    "moreover , we decompose the output space in a more detailed way that guarantees much more efficient load balancing",
    ". we will demonstrate that our method is always load balanced in all stages for maximizing resource utilization of gpus .",
    "our spgemm framework includes four stages : ( 1 ) calculating upper bound , ( 2 ) binning , ( 3 ) computing the resulting matrix , and ( 4 ) arranging data .",
    "figure  [ spgemm.jpdc.fig.framework ] plots this framework .        * the first stage * ,",
    "calculating upper bound , generates the upper bound number of nonzero entries in each row of the resulting matrix @xmath4 .",
    "we create an array @xmath57 of size @xmath58 , where @xmath58 is the number of rows of @xmath4 , for the upper bound sizes of the rows .",
    "we use one gpu thread for computing each entry of the array @xmath57 .",
    "algorithm  [ spgemm.jpdc.alg.stage1 ] describes this procedure .",
    "@xmath59 @xmath60    * the second stage * , binning , deals with load balancing and memory pre - allocation .",
    "we first allocate 38 bins and put them into five bin groups .",
    "the bins contain the indices of the entries in the array @xmath57 and present as one array of size @xmath58 with 38 segments .",
    "then all rows are assigned to corresponding bins according to the number of nonzero entries .",
    "finally , based on the sizes of the bins , we allocate a temporary matrix for nonzero entries in the resulting matrix @xmath4 .",
    "the first bin group includes one bin that contains the indices of the rows of size 0 .",
    "the second bin group also only has one bin that contains the indices of the rows of size 1 .",
    "because the rows in the first two bins only require trivial operations , they are excluded from subsequent more complex computation on gpus .",
    "thus a better load balancing can be expected .",
    "the third bin group is composed of 31 bins that contain the indices of the rows of sizes 232 , respectively . since the sizes of these rows are no more than the size of a single thread bunch ( 32 in current nvidia gpus or 64 in current amd gpus ) and these rows require non - trivial computation , using one thread bunch or one thread group for each row can not bring efficient instruction throughput on gpus .",
    "therefore , we use one thread for each row .",
    "further , because each bin only contains the rows of the same upper bound size , the bins can be executed separately on gpus with different kernel programs for efficient load balancing .",
    "in other words , 31 gpu kernel programs will be executed for the 31 bins , if not empty .",
    "the fourth bin group consists of 4 bins that contain the indices of the rows located in size ranges 3364 , 65128 , 129256 and 257512 , respectively .",
    "the rows of these sizes are grouped because of three reasons : ( 1 ) each of them is large enough to be efficiently executed by a thread group , ( 2 ) each of them is small enough for scratchpad memory ( 48 kb per core in current nvidia kepler gpus , 96 kb per core in current nvidia maxwell gpus and 64 kb per core in current amd graphics core next , or gcn , gpus ) , and ( 3 ) the final sizes of these rows in the resulting matrix @xmath4 are predictable in a reasonable small range ( no less than the lower bound of size 1 and no more than the corresponding upper bound sizes ) . even though the rows in each bin do not have exactly the same upper bound size",
    ", a good load balancing still can be expected because each row is executed by using one thread group and inter - thread group load balancing is naturally guaranteed by the gpu low - level scheduling sub - systems .",
    "the fifth bin group includes the last bin that contains the indices of the rest of the rows of size larger than 512 .",
    "these rows have two common features : ( 1 ) their sizes can be too large ( recall @xmath61 in table  [ spgemm.jpdc.tab.benchmarksuite ] ) to fit in the scratchpad memory , and ( 2 ) predicting the final sizes of these rows to a small range ( scratchpad memory level ) is not possible in advance .",
    "therefore , we execute them in a unified progressive method described later . again because we use one thread group for each row , load balancing is naturally guaranteed .",
    "since we do not use precise method for memory pre - allocation , a temporary memory space for the resulting matrix @xmath4 is required .",
    "we design a hybrid method that allocates a csr format sparse matrix @xmath62 of the same size of the resulting matrix @xmath4 as temporary matrix .",
    "we set @xmath63 to @xmath64 while the row index @xmath9 is located in the bin groups 14 because compared with modern gpu global memory capacity , the total space requirement of these rows is relatively small .",
    "for the rows in the bin group 5 , we set @xmath63 to a fixed size 256 since normally this is an efficient working size for the scratchpad memory .",
    "therefore , we can see that if all of the indices of the rows are in the bin groups 14 , our hybrid method converts to the upper bound method , on the other extreme end , our method converts to the progressive method .",
    "but generally , we obtain benefits from the both individual methods .",
    "the stage 2 is executed on cpu since it only requires a few simple linear time traverses , which are more efficient for the cpu cache sub - systems .",
    "the pseudocode is shown in algorithm  [ spgemm.jpdc.alg.stage2 ] .",
    "` insert ` @xmath9 ` to ` @xmath65 @xmath66 ` insert ` @xmath9 ` to ` @xmath67 @xmath68 ` insert ` @xmath9 ` to ` @xmath69 @xmath70 ` insert ` @xmath9 ` to ` @xmath71 @xmath70 ` insert ` @xmath9 ` to ` @xmath72 @xmath70 ` insert ` @xmath9 ` to ` @xmath73 @xmath70 ` insert ` @xmath9 ` to ` @xmath74 @xmath70 ` insert ` @xmath9 ` to ` @xmath75 @xmath76",
    "@xmath77    * the third stage * , computing the resulting matrix , generates @xmath78 and the final nonzero entries stored in the temporary matrix @xmath62 .    for the rows in the bin groups 12 , we simply update the numbers of corresponding nonzero entries .",
    "for the rows in the bin groups 35 , we use three totally different methods : ( 1 ) heap method , ( 2 ) bitonic esc method , and ( 3 ) merge method , respectively . note that each bin has a counter ( at the host side ) that records the number of rows included . so the host can easily decide if a gpu kernel will be issued for a certain bin . in other words ,",
    "our approach only issue kernels for non - empty bins .",
    "the heap method first creates an empty implicit index - value pair heap ( or priority queue ) of the upper bound size for each row in the bin group 3 .",
    "the heaps are located in the scratchpad memory and collect all candidate nonzero entries for corresponding rows .",
    "then each heap executes a heapsort - like operation to generate an ordered sequence located in the tail part of the heap .",
    "the difference between this operation and the classic heapsort operation is that the entries in the resulting sequence are duplicate - free while the initial heap includes duplicate entries . in each delete - max step in our variant heapsort ,",
    "the root node and the first entry of the resulting sequence are fused if they share the same index ; otherwise the root node is inserted to the head part of the sequence .",
    "our method is also distinguished from a heap - based sparse accumulator given by gilbert et al .",
    "@xcite by the mechanism of eliminating duplicate entries .",
    "figure  [ spgemm.jpdc.fig.2heap ] gives two steps of an example of our heap method .",
    "finally , the sorted sequence without duplicate indices is generated in the scratchpad memory and saved to the matrix @xmath62 in the global memory .",
    "in addition , the numbers of nonzero entries in the rows of the resulting matrix @xmath4 are updated to the sizes of the corresponding resulting sequences .    for the rows in each bin of the bin group 4 ,",
    "a typical esc algorithm is used .",
    "the method first collects all candidate nonzero entries to an array in the scratchpad memory , then sorts the array by using basic bitonic sort and compresses duplicate indices in the sequence by using prefix - sum scan .",
    "figure  [ spgemm.jpdc.fig.bitonic ] shows an example of this procedure .",
    "finally , a sorted sequence without duplicate indices is generated in the scratchpad memory and saved to the matrix @xmath62 , and the numbers of nonzero entries in the rows are updated .",
    "for the rows in the bin group 5 , our method inserts each input nonzero entry to the corresponding row of the resulting matrix @xmath4 ( lines 711 in algorithm  [ spgemm.jpdc.alg.spgemm ] ) in parallel .",
    "we can see that the input sequence ( the candidate nonzero entries ) and the resulting sequence ( the selected nonzero entries in the current row of @xmath4 ) should always be kept ordered and duplicate - free because of the csr format .",
    "therefore , we can convert the parallel insert operations to parallel merge operations that merge ordered sequences and the final resulting sequence is ordered and duplicate - free .    each parallel merge operation can be split into multiple sub - steps : ( 1 ) a binary search operation on the resulting sequence for fusing entries with the same indices and tagging them , ( 2 ) a prefix - sum scan operation on the input sequence for getting continuous positions in the incremental part of the resulting sequence , ( 3 ) copying non - duplicate entries from the input sequence to the resulting sequence , and ( 4 ) merging the two sequences in one continuous memory space .",
    "figure  [ spgemm.jpdc.fig.merge ] shows an example of this procedure .",
    "after all input sequences are merged into one resulting sequence , it is saved to the matrix @xmath62 .",
    "then the numbers of nonzero entries in the rows are updated .",
    "as we allocate a limited scratchpad memory space for the resulting sequence , a potential overflow may happen . in this case",
    ", we first compare total size of the two sequences ( note that the input sequence is in the thread registers , but not in the scratchpad memory yet ) with the allocated size of the resulting sequence in the scratchpad memory .",
    "if a merge operation is not allowed , our method records current computation position as a checkpoint and dumps the resulting sequence from the scratchpad memory to the global memory . then the host allocates more global memory ( we use 2x each time ) and re - launches kernel with a 2x large scratchpad memory setting .",
    "the relaunched kernels obtain checkpoint information , and load existing results to the scratchpad memory and continue the computation . the global memory dumping and reloading bring an extra overhead , but actually it does not affect the total execution time too much because of three reasons : ( 1 ) the global memory access is almost completely coalesced , ( 2 ) the latency could be hidden by subsequent computation , and ( 3 ) this overhead is only a small factor of large computation ( short rows normally do not face this problem ) . for very long rows that exceed the scratchpad memory capacity ,",
    "our method still allocates a space in the scratchpad memory as a level-1 merge sequence , executes the same merge operations on it and merges the level-1 sequence in the scratchpad memory and the resulting sequence in the global memory only once before the kernel is ready to return .",
    "it is worth noting that the parameters of the binning depends on specifications ( e.g. , thread bunch size and scratchpad memory capacity ) of gpu architectures . in this paper",
    ", we use the abovementioned fixed - size parameters for assigning the rows into the bins since the current nvidia gpus and amd gpus have comparable hardware specifications .",
    "however , the strategies in stages 2 and 3 can be easily extended for future gpus with changed architecture designs .    * the fourth stage * , arranging data , first sums the numbers of nonzero entries in all rows of the resulting matrix @xmath4 and allocates its final memory space .",
    "then our method copies existing nonzero entries from the temporary matrix @xmath62 to the resulting matrix @xmath4 . for the rows in the bin group 1 , the copy operation is not required . for the rows in the bin group 2 ,",
    "we use one thread for each row . for the rest of the rows in the bin groups 35 , we use one thread group for each row .",
    "after all copy operations , the spgemm computation is done .",
    "because both the binary search and the prefix - sum scan take fast logarithmic time for each entry in the input sequence , these operations have relatively good efficiency and performance stability on modern gpus . therefore , a fast merge algorithm is very crucial for the performance of the merge method in our spgemm framework .",
    "recently some new merge algorithms  @xcite have been proposed for gpus . but",
    "which one is the fastest in practice is still an open question . because the main objective of the research  @xcite is efficiently merging large data in the global memory , they still use basic methods , such as bitonic sort and ranking - based merge , as building blocks for small data in the scratchpad memory .",
    "peters et al .",
    "@xcite proposed a locality - oriented advanced bitonic sort method that can reduce synchronization overhead by merging data in fast private memory instead of relatively slow shared memory .",
    "therefore we evaluate 5 gpu merge algorithms : ( 1 ) ranking merge  @xcite , ( 2 ) merge path  @xcite , ( 3 ) basic oddeven merge  @xcite , ( 4 ) basic bitonic merge  @xcite , and ( 5 ) advanced bitonic merge  @xcite . the implementation of the algorithm ( 2 ) is extracted from the modern gpu library  @xcite .",
    "the implementations of the algorithm ( 3 ) and ( 4 ) are extracted from the nvidia cuda sdk .",
    "we implement the algorithm ( 1 ) and ( 5 ) .",
    "additionally , another reason why we conduct the evaluation is that none of the above literature presented performance of merging short sequences of size less than @xmath79 , which is the most important length ( consider the @xmath49 in table  [ spgemm.jpdc.tab.benchmarksuite ] ) for our spgemm .",
    "our evaluation results of merging 32-bit keys , 32-bit key-32-bit value pairs and 32-bit key-64-bit value pairs are shown in figure  [ spgemm.jpdc.fig.testmergemethods ] .",
    "the experimental platforms are described in table  [ spgemm.jpdc.tab.testbeds ] .",
    "each of the five algorithms merges two short ordered sequences of size @xmath16 into one ordered output sequence of size @xmath80 .",
    "the sorting network methods in our evaluation only execute the last stage , since both inputs are sorted . to saturate throughput of gpus , the whole problem size",
    "is set to size @xmath81 .",
    "for example , @xmath82 thread groups are launched while each of them merges two sub - sequences of size @xmath83 .",
    "we execute each problem set through multiple thread groups of different sizes and record the best performance for the evaluation .    in figure  [ spgemm.jpdc.fig.testmergemethods ]",
    ", we can see that the gpu merge path algorithm almost always outperforms other methods while sub - sequence size is no less than @xmath84 .",
    "since our merge method starts from size 256 , the merge path method is chosen for our spgemm implementation .",
    "the extra advantages of the merge path method are that it can evenly assign work load to threads and can easily deal with the input sequences of arbitrary sizes .",
    "detailed description and complexity analysis of the gpu merge path algorithm can be found in  @xcite .",
    "other algorithms are not chosen because of various reasons .",
    "we can see that the ranking merge is faster than the merge path method in figure  [ spgemm.jpdc.fig.testmergemethods](f ) .",
    "but it is not chosen in our implementation , since this algorithm is an out - of - place method that requires more scratchpad memory and thus can not scale to longer sequences . because the basic bitonic merge and the basic oddeven merge in general do not show better performance and can not simply deal with data of arbitrary sizes , none of them is chosen .",
    "the advanced bitonic sort method is always the slowest because it loads data from the scratchpad memory to thread private memory ( register file or an off - chip memory space ) for data locality .",
    "however , due to the small or negatively large latency gap between the scratchpad memory and the thread private memory , the load operations actually reduce the overall performance . thus this method",
    "should only be used for migrating global memory access to scratchpad memory access .",
    "we can also see that the amd radeon r9 290x gpu is almost always much faster than the two nvidia gpus in all tests .",
    "the reason is that the capacity of the scratchpad memory ( 2816 kb , 64 kb / core @xmath85 44 cores , in the amd gpu , 1536 kb , 96 kb / core @xmath85 16 cores , in the nvidia maxwell - based gtx 980 gpu and 720 kb , 48 kb / core @xmath85 15 cores , in the nvidia kepler - based gtx titan black gpu ) heavily influence the performance of merging small sequences .",
    "for the same reason , the gtx 980 gpu delivers better overall performance than the gtx titan black gpu . on the other hand , even though the amd gpu has 64 kb scratchpad memory per core , each instance of the kernel program can only use up to 32 kb .",
    "thus the amd gpu can not scale to longer sub - sequences ( e.g. , @xmath79 with 32-bit key-32-bit value pairs ) that can be executed by using the nvidia gpus .",
    "we use four platforms ( one cpu and three gpus ) shown in table  [ spgemm.jpdc.tab.testbeds ] for evaluating the spgemm algorithms .",
    "the host side of all gpus is a quad - core 3.7ghz cpu in an amd a10 - 7850k apu with 8 gb ddr3 - 1600 dual - channel system memory and 64-bit ubuntu linux 14.04 .    [",
    "cols=\"<,<,<,<,<\",options=\"header \" , ]      calculating galerkin products plays an important role in amg .",
    "we use smoothed aggregation preconditioner with jacobi smoother ( described in  @xcite and implemented in the cusp library  @xcite ) as a test scenario for evaluating spgemm algorithms . in each level of an amg hierarchy in this context ,",
    "we multiply three sparse matrices @xmath86 , @xmath0 and @xmath87 , where rectangular matrix @xmath86 is a restriction operator , square matrix @xmath0 is initially the system matrix , and rectangular matrix @xmath87 is a prolongation operator .    figures",
    "[ spgemm.jpdc.fig.amg-sp ] and  [ spgemm.jpdc.fig.amg-dp ] show execution time of galerkin products @xmath6 in constructing an amg hierarchy ( typically including 3 - 5 levels ) for a smoothed aggregation preconditioner in single precision and double precision , respectively .",
    "the input system matrix @xmath0 is from 2d 5-point , 2d 9-point , 3d 7-point or 3d 27-point poisson problem , respectively .",
    "the two 2d problems have dimensions @xmath88 and generate system matrices of size @xmath89 .",
    "the two 3d problems have dimensions @xmath90 and generate system matrices of size @xmath91 .",
    "the spgemm approaches in three libraries , cusp v0.4.0 , cusparse v6.5 and bhsparse , are tested on nvidia geforce gtx titan black and geforce gtx 980 gpus . to obtain the best spgemm performance",
    ", cusp uses the coordinate ( coo ) format for its input matrices .",
    "the other two libraries use the csr format . because the operation multiplies three sparse matrices @xmath86",
    ", @xmath0 and @xmath87 , the order of multiplication may influence overall performance . here",
    "we test the two possible orders @xmath92 and @xmath93 . in our experiments ,",
    "matrix data transfer time between the host and the device is not included since the spgemm is normally one of the building blocks for more complex problem completely running on gpus .    in figures  [ spgemm.jpdc.fig.amg-sp ] and  [ spgemm.jpdc.fig.amg-dp ]",
    ", we can see that our method is constantly faster than spgemm algorithms in the other two libraries .",
    "when using system matrix from 3d 27-point poisson problem , bhsparse delivers up to 2.6x and up to 2.7x speedups over cusparse and cusp , respectively . on average , speedups of 1.9x and 1.7x",
    "are achieved when compared with the above two libraries , respectively .    as for the order of multiplication",
    ", we can see that our method in general gives better performance while doing @xmath93 , compared to running @xmath92 . in contrast , the order of multiplication does not bring obvious performance difference for cusp .",
    "when cusparse is used , @xmath92 delivers better throughput for the two 2d problems , but degrades throughput for the two 3d problems .",
    "we also evaluate multiplication of sparse square matrix and itself ( i.e. , @xmath94 ) to avoid introducing another sparse matrix as a multiplier with different sparsity structure .",
    "we choose 23 sparse matrices as our benchmark suite .",
    "16 of them were widely used for performance evaluations in previous sparse matrix computation research  @xcite .",
    "the other 7 new matrices are chosen since they bring more diverse irregular sparsity structures that challenge the spgemm algorithm design .",
    "the variety of sparsity structures are from many application fields , such as finite element methods , macroeconomic model , protein data , circuit simulation , web connectivity and combinational problem .",
    "all of the 23 matrices are downloadable from the university of florida sparse matrix collection  @xcite .",
    "note that symmetry in the sparse matrices is not used in our spgemm algorithm , although some matrices in the benchmark suite are symmetric .",
    "also note that we use the standard csr format that does not consider symmetric storage pattern .",
    "besides the input matrix @xmath0 , the work complexities of the different spgemm algorithms also depend on the intermediate matrix @xmath51 and the resulting matrix @xmath4 .",
    "so we list characteristics of the three matrices in table  [ spgemm.jpdc.tab.benchmarksuite ] .",
    "the set of characteristics includes matrix dimension ( @xmath31 ) , the number of nonzero entries ( @xmath32 ) and the average number of nonzero entries in rows ( @xmath95 ) .",
    "the upper 9 matrices in the table have relatively regular nonzero entry distribution mostly on the diagonal .",
    "the other 14 matrices include various irregular sparsity structures .",
    "> m0.17 > m0.04 > m0.07 > m0.17 >",
    "m0.17 > m0.17 * name * & * plot * & &    &    &     + fem / cantilever &        & 63 k &    4 m , 64    &    269.5 m , 4315    &    17.4 m , 279     + economics &        & 207 k &    1.3 m , 6    &    7.6 m , 37    &    6.7 m , 32     + epidemiology &        & 526 k &    2.1 m , 4    &    8.4 m , 16    &    5.2 m , 10     + filter3d &        & 106 k &    2.7 m , 25    &    86 m , 808    &    20.2 m , 189     + wind tunnel &        & 218 k &    11.6 m , 53    &    626.1 m , 2873    &    32.8 m , 150     + fem / ship &        & 141 k &    7.8 m , 55    &    450.6 m , 3199    &    24.1 m , 171     + fem / harbor &        & 47 k &    2.4 m , 51    &    156.5 m , 3341    &    7.9 m , 169     + protein &        & 36 k &    4.3 m , 119    &    555.3 m , 15249    &    19.6 m , 538     + fem / spheres &        & 83 k &    6 m , 72    &    463.8 m , 5566    &    26.5 m , 318     + 2cubes_sphere &        & 102 k &    1.6 m , 16    &    27.5 m , 270    &    9 m , 88     + fem / accelerator &        & 121 k &    2.6 m , 22    &    79.9 m , 659    &    18.7 m , 154     + cage12 &        & 130 k &    2 m , 16    &    34.6 m , 266    &    15.2 m , 117     + hood &        & 221 k &    10.8 m , 49    &    562 m , 2548    &    34.2 m , 155     + m133-b3 &        & 200 k &    0.8 m , 4    &    3.2 m , 16    &    3.2 m , 16     + majorbasis &        & 160 k &    1.8 m , 11    &    19.2 m , 120    &    8.2 m , 52     + mario002 &        & 390 k &    2.1 m , 5    &    12.8 m , 33    &    6.4 m , 17     + mono_500hz &        & 169 k &    5 m , 30    &    204 m , 1204    &    41.4 m , 244     + offshore &        & 260 k &    4.2 m , 16    &    71.3 m , 275    &    23.4 m , 90     + patents_main &        & 241 k &    0.6 m , 2    &    2.6 m , 11    &    2.3 m , 9     + poisson3da &        & 14 k &    0.4 m , 26    &    11.8 m , 871    &    3 m , 219     + qcd &        & 49 k &    1.9 m , 39    &    74.8 m , 1521    &    10.9 m , 222     + circuit &        & 171 k &    1 m , 6    &    8.7 m , 51    &    5.2 m , 31     + webbase &        & 1 m &    3.1 m , 3    &    69.5 m , 70    &    51.1 m , 51     +      the single precision and double precision absolute performance of the spgemm algorithms that compute @xmath94 are shown in figures  [ spgemm.jpdc.fig.spsgemm ] and  [ spgemm.jpdc.fig.spdgemm ] , respectively .",
    "four gpu methods from cusp v0.4.0 , cusparse v6.5 , rmerge  @xcite and bhsparse are evaluated on three gpus : nvidia geforce gtx titan black , nvidia geforce gtx 980 and amd radeon r9 290x . one cpu method in intel mkl v11.0 is evaluated on intel xeon e5 - 2630 cpu .",
    "the performance of another recent esc - based gpu spgemm work  @xcite is not included in the comparison because its source code is not available to us yet .",
    "the intel mkl spgemm program is multithreaded and utilizes all six cores in the intel xeon cpu . for gpu algorithms ,",
    "again , the host - device data transfer time is not included .    -6pt",
    "-6pt -6pt -6pt -6pt    -6pt -6pt -6pt -6pt",
    "-6pt    we first compare the performance of the four different gpu spgemm algorithms on the nvidia gpus .",
    "we can see that bhsparse always outperforms cusp , cusparse and rmerge on most sparse matrices in the benchmark suite .",
    "compared to the two vendor supplied libraries , our method obtains better spsgemm and spdgemm performance on 21 and 21 matrices out of the whole 23 matrices over cusp , and on 19 and 21 matrices over cusparse , respectively .",
    "compared to rmerge , another cuda - specific method , bhsparse achieves better spsgemm and spdgemm performance on 19 and 10 matrices on the gtx titan black gpu , and on 19 and 20 matrices on the gtx 980 gpu .    from the perspective of speedup , our method delivers on average 4.6x ( up to 9.6x ) and 3.1x ( up to 8.8x ) speedup on spsgemm performance over cusp and cusparse , and on average 4.6x ( up to 9.9x ) and 3.1x ( up to 9.5x ) speedup on spdgemm performance over them , respectively .",
    "compared to rmerge , our method offers on average 1.4x ( up to 2.5x ) speedup and 2.8x ( up to 4.9x ) speedup for spsgemm and on average 1.0x ( up to 1.5x ) and 2.1x ( up to 3.4x ) speedup for spdgemm on the gtx titan black gpu and gtx 980 gpu , respectively .",
    "we can see that the cusparse method outperforms our approach when and only when the input matrices are fairly regular ( belong to the first 9 matrices in table  [ spgemm.jpdc.tab.benchmarksuite ] ) . for all irregular matrices and some regular ones ,",
    "our bhsparse is always more efficient . on the other hand ,",
    "the absolute performance of the cusp method is very stable since its execution time almost only depends on the number of necessary arithmetic operations .",
    "therefore this approach is insensitive to sparsity structures .",
    "actually this insensitivity may bring better performance on matrices with some specific sparsity structures .",
    "however in most cases , the cusp method suffers with higher global memory pressure .",
    "the rmerge method offers significant speedups over the other methods on three matrices ( i.e. , _ epidemiology _ , _ m133-b3 _ and _ mario002 _ ) , which are characterized by short rows .",
    "however , for the other matrices , rmerge supplies relatively lower performance due to imbalanced workload and high - overhead global memory operations between iterative steps .",
    "further , we can see that since rmerge mainly relies on computational power of the simd units , its performance decreases from gtx titan black ( 2880 cuda cores running at 889 mhz ) to gtx 980 ( 2048 cuda cores running at 1126 mhz ) .",
    "in contrast , our method also depends on capacity of scratchpad memory .",
    "thus we can see that bhsparse obtains better performance while using gtx 980 ( 1536 kb scratchpad ) over gtx titan black ( 720 kb scratchpad ) .",
    "compared to intel mkl on the intel cpu , our cuda - based implementation on the nvidia gpus obtains better spsgemm and spdgemm performance on all 23 matrices , and delivers on average 2.5x ( up to 5.2x ) and 2.2x ( up to 4.9x ) spsgemm and spdgemm speedup , respectively .",
    "our opencl - based implementation on the amd gpu in the machine 2 obtains better spsgemm and spdgemm performance on 23 and 18 matrices , and delivers on average 2.3x ( up to 4.2x ) and 1.9x ( up to 3.8x ) spsgemm and spdgemm speedup , respectively .",
    "the relative performance ( harmonic mean ) of the spgemm algorithms that compute @xmath94 is shown in figure  [ spgemm.jpdc.fig.relativeperf ] .",
    "we can see that our method in general delivers the best performance on the used testbeds while running the 23 matrices as a benchmark suite .",
    "if we set the intel mkl spgemm performance in this scenario as a baseline , our approach is the only gpu spgemm that constantly outperforms well optimized cpu method .",
    "figure  [ spgemm.jpdc.fig.space ] shows the comparison of the three memory pre - allocation methods , while benchmarking @xmath96 .",
    "we can see that , for small matrices ( e.g. , _",
    "2cubes_sphere _ ) , our hybrid method shows exactly the same space requirements as the upper bound method does .",
    "however , for large matrices , allocated memory sizes through our hybrid method are much closer to the memory sizes allocated by the precise method . taking the matrix _ protein _ as an example , our hybrid method requires 2.7x memory space over the precise method , while the upper bound method needs 20.6x space requirement .",
    "one exception is the matrix _ webbase _ , our hybrid method actually allocates more memory space than the upper bound method .",
    "the reasons are that the reduced rate of the intermediate matrix @xmath51 to the resulting matrix @xmath4 is very low ( see table  [ spgemm.jpdc.tab.benchmarksuite ] ) and our 2x progression mechanism just allocates memory across the upper bound size .",
    "but overall , our hybrid method saves space allocation of the upper bound method and execution time of the precise method without introducing any significant extra space requirements .      for some matrices with relatively long rows in the bin group 5 ,",
    "our method dumps scratchpad data to global memory , allocates a larger memory block , copies the old data to the newly allocated portion , reloads values and continues processing .",
    "we have to do the allocation / copy operation pair and pay the overhead since current gpus are not able to re - allocate memory ( i.e. , change the size of the memory block pointed to a certain pointer ) .",
    "however , the emerging heterogeneous processors with shared virtual memory ( or unified memory ) address space deliver a possibility that lets integrated gpus use system memory , which is re - allocatable from the cpu side .",
    "we evaluated two memory allocation strategies ( i.e. , a typical allocation / copy approach and an improved re - allocation approach ) of our opencl - based spgemm algorithm on the gpu part ( 8 gcn core , 512 radeon cores running at 1028 mhz , 1052.7 gflop / s sp peak , 65.8 gflop / s dp peak ) in the amd a10 - 7850k apu .",
    "figure  [ spgemm.jpdc.fig.apuperf ] shows the results .",
    "we can see that re - allocatable memory brings on average 1.2x ( up to 1.6x ) speedup and on average 1.2x ( up to 1.8x ) speedup for spsgemm and spdgemm , respectively .",
    "therefore , our gpu spgemm method may deliver further performance improvement on future gpus with re - allocatable memory , or on emerging heterogeneous processors composed of cpu cores and gpu cores .",
    "moreover , both cpu cores and gpu cores can be utilized for stage 3 in our framework .",
    "we leave this heterogenous workload partitioning ( similar to the methods described in  @xcite ) to future work .",
    "in this paper we demonstrated an efficient spgemm framework and corresponding algorithms on gpus and emerging cpu - gpu heterogeneous processors for solving the three challenging problems in the spgemm .",
    "in the two experimental scenarios using matrices with diverse sparsity structures as input , our spgemm algorithm delivered excellent absolute and relative performance as well as space efficiency over the previous gpu spgemm methods . moreover , on average , our approach obtained around twice the performance of the start - of - the - art cpu spgemm method .",
    "further , we showed that our method obtained higher performance on emerging heterogeneous processors with re - allocatable memory .",
    "the authors would like to thank jianbin fang at the delft university of technology for supplying access to the machine with the intel xeon cpu .",
    "the authors further thank felix gremse at the rwth aachen university for sharing source code of the rmerge algorithm and a preprint copy of the corresponding paper  @xcite .",
    "the authors also thank the anonymous reviewers of jpdc and ipdps 14 for their insightful feedback on this version and a shorter version  @xcite of this paper .",
    "w.  liu , b.  vinter , an efficient gpu general sparse matrix - matrix multiplication for irregular data , in : proceedings of the 2014 ieee 28th international parallel and distributed processing symposium , ipdps 14 , 2014 , pp .",
    "370381 .",
    "j.  gilbert , s.  reinhardt , v.  shah , high - performance graph algorithms from parallel sparse matrices , in : b.  kgstrm , e.  elmroth , j.  dongarra , j.  waniewski ( eds . ) , applied parallel computing .",
    "state of the art in scientific computing , vol .",
    "4699 of lecture notes in computer science , springer berlin heidelberg , 2007 , pp .",
    "260269 .",
    "h.  kaplan , m.  sharir , e.  verbin , colored intersection searching via sparse rectangular matrix multiplication , in : proceedings of the twenty - second annual symposium on computational geometry , scg 06 , 2006 , pp .",
    "5260 .      n.  bell , m.  garland , implementing sparse matrix - vector multiplication on throughput - oriented processors , in : proceedings of the conference on high performance computing networking , storage and analysis , sc 09 , 2009 , pp",
    ". 18:118:11 .",
    "w.  liu , b.  vinter , csr5 : an efficient storage format for cross - platform sparse matrix - vector multiplication , in : proceedings of the 29th acm on international conference on supercomputing , ics 15 , 2015 , pp .",
    "339350 .",
    "f.  vazquez , g.  ortega , j.  fernandez , i.  garcia , e.  garzon , fast sparse matrix matrix product based on ellr - t and gpu computing , in : parallel and distributed processing with applications ( ispa ) , 2012 ieee 10th international symposium on , 2012 , pp .",
    "669674 .",
    "p.  sulatycke , k.  ghose , caching - efficient multithreaded fast multiplication of sparse matrices , in : parallel processing symposium , 1998 .",
    "ipps / spdp 1998 .",
    "proceedings of the first merged international parallel processing symposium and symposium on parallel and distributed processing 1998 , 1998 , pp . 117123 .",
    "e.  saule , k.  kaya , .  v. atalyrek , performance evaluation of sparse matrix multiplication kernels on intel xeon phi , in : proc of the 10th intl conf . on parallel processing and applied mathematics ( ppam ) , 2013 .",
    "s.  williams , l.  oliker , r.  vuduc , j.  shalf , k.  yelick , j.  demmel , optimization of sparse matrix - vector multiplication on emerging multicore platforms , in : supercomputing , 2007 .",
    "proceedings of the 2007 acm / ieee conference on , 2007 , pp . 112 .",
    "a.  bulu , s.  williams , l.  oliker , j.  demmel , reduced - bandwidth multithreaded algorithms for sparse matrix - vector multiplication , in : parallel distributed processing symposium ( ipdps ) , 2011 ieee international , 2011 , pp . 721733 .",
    "e.  cohen , on optimizing multiplications of sparse matrices , in : w.  cunningham , s.  mccormick , m.  queyranne ( eds . ) , integer programming and combinatorial optimization , vol . 1084 of lecture notes in computer science , springer berlin heidelberg , 1996 , pp . 219233 .",
    "r.  pagh , m.  stckel , the input / output complexity of sparse matrix multiplication , in : a.  schulz , d.  wagner ( eds . ) , algorithms - esa 2014 , lecture notes in computer science , springer berlin heidelberg , 2014 , pp .",
    "750761 .",
    "s.  damaraju , v.  george , s.  jahagirdar , t.  khondker , r.  milstrey , s.  sarkar , s.  siers , i.  stolero , a.  subbiah , a 22 nm ia multi - cpu and gpu system - on - chip , in : solid - state circuits conference digest of technical papers ( isscc ) , 2012 ieee international , 2012 , pp . 5657 .              c.  gregg , k.  hazelwood , where is the data ?",
    "why you can not debate cpu vs. gpu performance without the answer , in : performance analysis of systems and software ( ispass ) , 2011 ieee international symposium on , 2011 , pp .",
    "134144 .    c.  kim , t.  kaldewey , v.  w. lee",
    ", e.  sedlar , a.  d. nguyen , n.  satish , j.  chhugani , a.  di  blas , p.  dubey , sort vs. hash revisited : fast join implementation on modern multi - core cpus , proc .",
    "vldb endow .",
    "2  ( 2 ) ( 2009 ) 13781389 .",
    "g.  ballard , a.  bulu , j.  demmel , l.  grigori , b.  lipshitz , o.  schwartz , s.  toledo , communication optimal parallel multiplication of sparse random matrices , in : proceedings of the 25th acm symposium on parallelism in algorithms and architectures , spaa 13 , 2013 , pp .",
    "222231 .",
    "p.  kipfer , r.  westermann , improved gpu sorting , in : m.  pharr ( ed . ) , gpu gems 2 : programming techniques for high - performance graphics and general - purpose computation , addison - wesley , 2005 , ch .",
    "46 , pp . 733746 .",
    "h.  peters , o.  schulz - hildebrandt , n.  luttenberger , a novel sorting algorithm for many - core architectures based on adaptive bitonic sort , in : parallel distributed processing symposium ( ipdps ) , 2012 ieee 26th international , 2012 , pp .",
    "227237 .",
    "h.  inoue , t.  moriyama , h.  komatsu , t.  nakatani , aa - sort : a new parallel sorting algorithm for multi - core simd processors , in : parallel architecture and compilation techniques , 2007 .",
    "pact 2007 .",
    "16th international conference on , 2007 , pp .",
    "189198 .",
    "m.  r.  b. kristensen , s.  a.  f. lund , t.  blum , k.  skovhede , b.  vinter , bohrium : a virtual machine approach to portable parallelism , in : proceedings of the 2014 ieee international parallel & distributed processing symposium workshops , 2014 , pp .",
    "312321 .",
    "a.  bulu , j.  r. gilbert , challenges and advances in parallel sparse matrix - matrix multiplication , in : proceedings of the 2008 37th international conference on parallel processing , icpp 08 , 2008 , pp . 503510 .",
    "j.  shen , a.  l. varbanescu , p.  zou , y.  lu , h.  sips , improving performance by matching imbalanced workloads with heterogeneous platforms , in : proceedings of the 28th acm international conference on supercomputing , ics 14 , 2014 , pp ."
  ],
  "abstract_text": [
    "<S> general sparse matrix - matrix multiplication ( spgemm ) is a fundamental building block for numerous applications such as algebraic multigrid method ( amg ) , breadth first search and shortest path problem . </S>",
    "<S> compared to other sparse blas routines , an efficient parallel spgemm implementation has to handle extra irregularity from three aspects : ( 1 ) the number of nonzero entries in the resulting sparse matrix is unknown in advance , ( 2 ) very expensive parallel insert operations at random positions in the resulting sparse matrix dominate the execution time , and ( 3 ) load balancing must account for sparse data in both input matrices . in this work </S>",
    "<S> we propose a framework for spgemm on gpus and emerging cpu - gpu heterogeneous processors . </S>",
    "<S> this framework particularly focuses on the above three problems . </S>",
    "<S> memory pre - allocation for the resulting matrix is organized by a hybrid method that saves a large amount of global memory space and efficiently utilizes the very limited on - chip scratchpad memory . </S>",
    "<S> parallel insert operations of the nonzero entries are implemented through the gpu merge path algorithm that is experimentally found to be the fastest gpu merge approach . </S>",
    "<S> load balancing builds on the number of necessary arithmetic operations on the nonzero entries and is guaranteed in all stages .    compared with the state - of - the - art cpu and gpu spgemm methods </S>",
    "<S> , our approach delivers excellent absolute performance and relative speedups on various benchmarks multiplying matrices with diverse sparsity structures . </S>",
    "<S> furthermore , on heterogeneous processors , our spgemm approach achieves higher throughput by using re - allocatable shared virtual memory .    </S>",
    "<S> sparse matrix , sparse matrix - matrix multiplication , linear algebra , gpu , heterogeneous processor , merging , parallel algorithm </S>"
  ]
}