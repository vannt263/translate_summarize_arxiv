{
  "article_text": [
    "large data sets of points in high - dimension often lie close to a smooth low - dimensional manifold .",
    "a fundamental problem in processing such data sets is the construction of an efficient parameterization that allows for the data to be well represented in fewer dimensions .",
    "such a parameterization may be realized by exploiting the inherent manifold structure of the data .",
    "however , discovering the geometry of an underlying manifold from only noisy samples remains an open topic of research .",
    "the case of data sampled from a linear subspace is well studied ( see @xcite , for example ) .",
    "the optimal parameterization is given by principal component analysis ( pca ) , as the singular value decomposition ( svd ) produces the best low - rank approximation for such data .",
    "however , most interesting manifold - valued data organize on or near a nonlinear manifold .",
    "pca , by projecting data points onto the linear subspace of best fit , is not optimal in this case as curvature may only be accommodated by choosing a subspace of dimension higher than that of the manifold .",
    "algorithms designed to process nonlinear data sets typically proceed in one of two directions .",
    "one approach is to consider the data globally and produce a nonlinear embedding .",
    "alternatively , the data may be considered in a piecewise - linear fashion and linear methods such as pca may be applied locally .",
    "the latter is the subject of this work .",
    "local linear parameterization of manifold - valued data requires the estimation of the local tangent space ( `` tangent plane '' ) from a neighborhood of points .",
    "however , sample points are often corrupted by high - dimensional noise and any local neighborhood deviates from the linear assumptions of pca due to the curvature of the manifold .",
    "therefore , the subspace recovered by local pca is a perturbed version of the true tangent space .",
    "the goal of the present work is to characterize the stability and accuracy of local tangent space estimation using eigenspace perturbation theory .",
    "1ex 1ex    the proper neighborhood for local tangent space recovery must be a function of intrinsic ( manifold ) dimension , curvature , and noise level ; these properties often vary as different regions of the manifold are explored .",
    "however , local pca approaches proposed in the data analysis and manifold learning literature often define locality via an _ a priori _ fixed number of neighbors or as the output of an algorithm ( e.g. , @xcite ) .",
    "other methods @xcite adaptively estimate local neighborhood size but are not tuned to the perturbation of the recovered subspace .",
    "our approach studies this perturbation as the size of the neighborhood varies to guide the definition of locality .",
    "on the one hand , a neighborhood must be small enough so that it is approximately linear and avoids curvature . on the other hand ,",
    "a neighborhood must be be large enough to overcome the effects of noise . a simple yet instructive example of these competing criteria is shown in figure [ fig : example ] .",
    "the tangent plane at every point of a noisy 2-dimensional data set embedded in @xmath0 is computed via local pca .",
    "each point is color coded according to the angle formed with the true tangent plane .",
    "three different neighborhood definitions are used : a small , fixed radius ( figure [ fig : example]a ) ; a large , fixed radius ( figure [ fig : example]b ) ; and radii defined adaptively according the analysis presented in this work ( figure [ fig : example]c ) . as small neighborhoods may be within the noise level and large neighborhoods exhibit curvature , the figure shows that neither allows for accurate tangent plane recovery .",
    "in fact , because the curvature varies across the data , only the adaptively defined neighborhoods avoid random orientation due to noise ( as seen in figure [ fig : example]a ) and misalignment due to curvature ( as seen in figure [ fig : example]b ) .",
    "figure [ fig : example]c shows accurate and stable recovery at almost every data point , with misalignment only in the small region of very high curvature that will be troublesome for any method .",
    "the present work quantifies this observed behavior in the high - dimensional setting .",
    "we present a non - asymptotic , eigenspace perturbation analysis to bound , with high probability , the angle between the recovered linear subspace and the true tangent space as the size of the local neighborhood varies .",
    "the analysis accurately tracks the subspace recovery error as a function of neighborhood size , noise , and curvature .",
    "thus , we are able to adaptively select the neighborhood that minimizes this bound , yielding the best estimate to the local tangent space from a large but finite number of noisy manifold samples .",
    "further , the behavior of this bound demonstrates the non - trivial existence of such an optimal scale .",
    "we are also able to accurately and efficiently estimate the curvature and noise level of the local neighborhood .",
    "finally , we introduce a geometric uncertainty principle quantifying the limits of noise - curvature perturbation for tangent space recovery .",
    "our analysis is related to the very recent work of tyagi , _ et al . _",
    "@xcite , in which neighborhood size and sampling density conditions are given to ensure a small angle between the pca subspace and the true tangent space of a noise - free manifold .",
    "results are extended to arbitrary smooth embeddings of the manifold model , which we do not consider .",
    "in contrast , we envision the scenario in which no control is given over the sampling and explore the case of data sampled according to a fixed density and corrupted by high - dimensional noise .",
    "crucial to our results is a careful analysis of the interplay between the perturbation due to noise and the perturbation due to curvature .",
    "nonetheless , our results can be shown to recover those of @xcite in the noise - free setting .",
    "our approach is also similar to the analysis presented by nadler in @xcite , who studies the finite - sample properties of the pca spectrum . through matrix perturbation theory",
    ", @xcite examines the angle between the leading finite - sample - pca eigenvector and that of the leading population - pca eigenvector . as a linear model",
    "is assumed , perturbation results from noise only . despite this difference ,",
    "the two analyses utilize similar techniques to bound the effects of perturbation on the pca subspace and our results recover those of @xcite in the curvature - free setting .",
    "other recent related works include that of singer and wu @xcite , who use local pca to build a tangent plane basis and give an analysis for the neighborhood size to be used in the absence of noise . using the hybrid linear model , zhang , _ et al . _",
    "@xcite assume data are samples from a collection of `` flats '' ( affine subspaces ) and choose an optimal neighborhood size from which to recover each flat by studying the least squares approximation error in the form of jones @xmath1-number ( see @xcite and also @xcite in which this idea is used for curve denoising ) .",
    "an analysis of noise and curvature for normal estimation of smooth curves and surfaces in @xmath2 and @xmath3 is presented by mitra , _ et al .",
    "_ @xcite with application to computer graphics .",
    "finally , chen , _ et al . _",
    "@xcite present a multiscale pca framework with which to analyze the intrinsic dimensionality of a data set .",
    "we consider the problem of recovering the best approximation to a local tangent space of a nonlinear @xmath4-dimensional riemannian manifold @xmath5 from noisy samples presented in dimension @xmath6 .",
    "working about a reference point @xmath7 , an approximation to the tangent space of @xmath5 at @xmath7 is given by the span of the top @xmath4 eigenvectors of the centered data covariance matrix ( where `` top '' refers to the @xmath4 eigenvectors or singular vectors associated with the @xmath4 largest eigenvalues or singular values ) .",
    "the question becomes : how many neighbors of @xmath7 should be used ( or in how large of a radius about @xmath7 should we work ) to recover the best approximation ?",
    "we will often use the term `` scale '' to refer to this neighborhood size or radius .    to answer this question",
    ", we consider the perturbation of the eigenvectors spanning the estimated tangent space in the context of the `` noise - curvature trade - off . '' to balance the effects of noise and curvature ( as observed in the example of the previous subsection , figure [ fig : example ] ) , we seek a scale large enough to be above the noise level but still small enough to avoid curvature .",
    "this scale reveals a linear structure that is sufficiently decoupled from both the noise and the curvature to be well approximated by a tangent plane . at this scale ,",
    "the recovered eigenvectors span a subspace corresponding very closely to the true tangent space of the manifold at @xmath7 .",
    "we note that the concept of noise - curvature trade - off has been a subject of interest for decades in dynamical systems theory @xcite .    the main result of this work is a bound on the angle between the computed and true tangent spaces .",
    "define @xmath8 to be the orthogonal projector onto the true tangent space and let @xmath9 be the orthogonal projector constructed from the @xmath4-dimensional eigenspace of the neighborhood covariance matrix .",
    "then the distance @xmath10 corresponds to the sum of the squared sines of the principal angles between the computed and true tangent spaces and we use eigenspace perturbation theory to bound this norm .",
    "momentarily neglecting probability - dependent constants to ease the presentation , the first - order approximation of this bound has the following form : + * informal main result .",
    "* @xmath11}{\\frac{r^2}{d+2 } - \\frac{k^2r^4}{2(d+2)^2 } - \\sigma^2\\left(\\sqrt{d } + \\sqrt{d - d}\\right)},\\ ] ] where @xmath12 is the radius ( measured in the tangent plane ) of the neighborhood containing @xmath13 points , @xmath14 is the noise level , and @xmath15 is the ( rescaled ) norm of the vector of mean curvatures . +",
    "the quantities @xmath13 , @xmath12 , @xmath14 , and @xmath15 as well as the sampling assumptions are more formally defined in section [ sec : prelim ] and the formal result is presented in section [ sec : mainresult ] .",
    "the denominator of this bound , denoted here by @xmath16 , @xmath17 quantifies the separation between the spectrum of the linear subspace ( @xmath18 ) and the perturbation due to curvature ( @xmath19 ) and noise ( @xmath20 ) .",
    "clearly , we must have @xmath21 to approximate the appropriate linear subspace , a requirement made formal by theorem [ thm : mainresult1 ] in section [ sec : mainresult ] .",
    "in general , when @xmath16 is zero ( or negative ) , the bound becomes infinite ( or negative ) and is not useful for subspace recovery .",
    "however , the geometric information encoded by offers more insight .",
    "for example , we observe that a small @xmath16 indicates that the estimated subspace contains a direction orthogonal to the true tangent space ( due to the curvature or noise ) .",
    "we therefore consider @xmath16 to be the condition number for subspace recovery and use it to develop our geometric interpretation for the bound .",
    "the noise - curvature trade - off is readily apparent from .",
    "the linear and curvature contributions are small for small values of @xmath12 .",
    "thus for a small neighborhood ( @xmath12 small ) , the denominator is either negative or ill conditioned for most values of @xmath14 and the bound becomes large .",
    "this matches our intuition as we have not yet encountered much curvature but the linear structure has also not been explored .",
    "therefore , the noise dominates the early behavior of this bound and an approximating subspace may not be recovered from noise .",
    "as the neighborhood radius @xmath12 increases , the conditioning of the denominator improves , and the bound is controlled by the @xmath22 behavior of the numerator .",
    "this again corresponds with our intuition : the addition of more points serves to overcome the effects of noise as the linear structure is explored .",
    "thus , when @xmath23 is well conditioned , the bound on the angle may become smaller with the inclusion of more points .",
    "eventually @xmath12 becomes large enough such that the curvature contribution approaches the size of the linear contribution and @xmath23 becomes large .",
    "the @xmath22 term is overtaken by the ill conditioning of the denominator and the bound is again forced to become large .",
    "the noise - curvature trade - off , seen analytically here in and , will be demonstrated numerically in section  [ sec : numerical ] .    enforcing a well conditioned recovery bound yields a geometric uncertainty principle quantifying the amount of curvature and noise we may tolerate . to recover an approximating subspace",
    ", we must have : + * geometric uncertainty principle .",
    "* @xmath24 by preventing the curvature and noise level from simultaneously becoming large , this requirement ensures that the linear structure of the data is recoverable . with high probability ,",
    "the noise component normal to the tangent plane concentrates on a sphere with mean curvature @xmath25 .",
    "as will be shown , this uncertainty principle expresses the intuitive notion that the curvature of the manifold must be less than the curvature of this noise - ball .",
    "otherwise , the combined effects of noise and curvature perturbation prevent an accurate estimate of the local tangent space .",
    "the remainder of the paper is organized as follows .",
    "section [ sec : prelim ] provides the notation , geometric model , and necessary mathematical formulations used throughout this work .",
    "eigenspace perturbation theory is reviewed in this section .",
    "the main results are stated formally in section [ sec : mainresult ] .",
    "we demonstrate the accuracy of our results and test the sensitivity to errors in parameter estimation in section [ sec : numerical ] .",
    "methods for recovering neighborhood curvature and noise level are also demonstrated in this section .",
    "we conclude in section [ sec : discussion ] with a discussion of the relationship to previously established results and algorithmic considerations .",
    "technical results and proofs are presented in the appendices .",
    "a @xmath4-dimensional riemannian manifold of codimension  1 may be described locally by the surface @xmath26 , where @xmath27 is a coordinate in the tangent plane . after translating the origin",
    ", a rotation of the coordinate system can align the coordinate axes with the principal directions associated with the principal curvatures at the given reference point @xmath7 .",
    "aligning the coordinate axes with the plane tangent to @xmath5 at @xmath7 gives a local quadratic approximation to the manifold . using this choice of coordinates ,",
    "the manifold may be described locally @xcite by the taylor series of @xmath28 at the origin @xmath7 : @xmath29 where @xmath30 are the principal curvatures of @xmath5 at @xmath7 . in this coordinate system",
    ", @xmath7 has the form @xmath31^t,\\ ] ] and points in a local neighborhood of @xmath7 have similar coordinates .",
    "generalizing to a @xmath4-dimensional manifold of arbitrary codimension in @xmath32 , there exist @xmath33 functions @xmath34 for @xmath35 , with @xmath36 representing the principal curvatures in codimension @xmath37 at @xmath7 .",
    "then , given the coordinate system aligned with the principal directions , a point in a neighborhood of @xmath7 has coordinates @xmath38 $ ] .",
    "we truncate the taylor expansion and use the quadratic approximation @xmath39 @xmath40 , to describe the manifold locally .",
    "consider now discrete samples from @xmath5 obtained by uniformly sampling the first @xmath4 coordinates ( @xmath41 in the tangent space inside @xmath42 , the @xmath4-dimensional ball of radius @xmath12 centered at @xmath7 , with the remaining @xmath33 coordinates given by . because we are sampling from a noise - free linear subspace ,",
    "the number of points @xmath13 captured inside @xmath42 is a function of the sampling density @xmath43 : @xmath44 where @xmath45 is the volume of the @xmath4-dimensional unit ball .",
    "of course it is unrealistic for the data to be observed in the described coordinate system .",
    "as noted , we may use a rotation to align the coordinate axes with the principal directions associated with the principal curvatures .",
    "doing so allows us to write as well as . because we will ultimately quantify the norm of each matrix using the unitarily - invariant frobenius norm",
    ", this rotation will not affect our analysis .",
    "we therefore proceed by assuming that the coordinate axes align with the principal directions .",
    "equation represents an exact quadratic embedding of @xmath5 . while it may be interesting to consider more general embeddings , as is done for the noise - free case in @xcite , a taylor expansion followed by rotation and translation will result in an embedding of the form . noting that the numerical results of @xcite indicate no loss in accuracy when truncating higher - order terms , proceeding with an analysis of remains sufficiently general .",
    "the true tangent space we wish to recover is given by the pca of @xmath56 . because we do not have direct access to @xmath56",
    ", we work with @xmath60 as a proxy , and instead recover a subspace spanned by the corresponding eigenvectors of @xmath61 .",
    "we will study how close this recovered invariant subspace of @xmath61 is to the corresponding invariant subspace of @xmath62 as a function of scale . throughout this work",
    ", scale refers to the number of points @xmath13 in the local neighborhood within which we perform pca .",
    "given a fixed density of points , scale may be equivalently quantified as the radius @xmath12 about the reference point @xmath7 defining the local neighborhood .",
    "given the decomposition of the data , we have @xmath63 we introduce some notation to account for the centering required by pca .",
    "define the sample mean of @xmath13 realizations of random vector @xmath64 as @xmath65 where @xmath66 denotes the @xmath37th realization . letting @xmath67",
    "represent the column vector of @xmath13 ones , define @xmath68 to be the matrix with @xmath13 copies of @xmath69 as its columns .",
    "finally , let @xmath70 denote the centered version of @xmath71 : @xmath72 then we have @xmath73    the problem may be posed as a perturbation analysis of invariant subspaces . rewrite as @xmath74 where @xmath75 is the perturbation that prevents us from working directly with @xmath76 .",
    "the dominant eigenspace of @xmath77 is therefore a perturbed version of the dominant eigenspace of @xmath76 . seeking to minimize the effect of this perturbation",
    ", we look for the scale @xmath78 ( equivalently @xmath79 ) at which the dominant eigenspace of @xmath77 is closest to that of @xmath76 . before proceeding , we review material on the perturbation of eigenspaces relevant to our analysis .",
    "the reader familiar with this topic is invited to skip directly to theorem [ thm : stewart ] .",
    "the distance between two subspaces of @xmath32 can be defined as the spectral norm of the difference between their respective orthogonal projectors @xcite .",
    "as we will always be considering two equidimensional subspaces , this distance is equal to the sine of the largest principal angle between the subspaces . to control all such principal angles , we state our results using the frobenius norm of this difference .",
    "our goal is therefore to control the behavior of @xmath80 , where @xmath8 and @xmath9 are the orthogonal projectors onto the subspaces computed from @xmath56 and @xmath60 , respectively .",
    "the norm @xmath80 may be bounded by the classic @xmath81 theorem of davis and kahan @xcite .",
    "we will use a version of this theorem presented by stewart ( theorem v.2.7 of @xcite ) , modified for our specific purpose .",
    "first , we establish some notation , following closely that found in @xcite .",
    "consider the eigendecompositions @xmath82~\\begin{bmatrix } \\lambda_1 & \\\\ & \\lambda_2 \\end{bmatrix } ~[u_1~u_2]^t , \\label{eq : eigdecompl } \\\\",
    "\\frac{1}{n}\\widetilde{x}\\widetilde{x}^t & = \\widehat{u } \\widehat{\\lambda } \\widehat{u}^t = [ \\widehat{u}_1~\\widehat{u}_2]~\\begin{bmatrix } \\widehat{\\lambda}_1 & \\\\ & \\widehat{\\lambda}_2 \\end{bmatrix}~[\\widehat{u}_1~\\widehat{u}_2]^t ,   \\label{eq : eigdecompa}\\end{aligned}\\ ] ] such that the columns of @xmath83 are the eigenvectors of @xmath84 and the columns of @xmath85 are the eigenvectors of @xmath86 .",
    "the eigenvalues of @xmath84 are arranged in descending order as the entries of diagonal matrix @xmath87 .",
    "the eigenvalues are also partitioned such that diagonal matrices @xmath88 and @xmath89 contain the @xmath4 largest entries of @xmath87 and the @xmath33 smallest entires of @xmath87 , respectively .",
    "the columns of @xmath90 are those eigenvectors associated with the @xmath4 eigenvalues in @xmath88 , the columns of @xmath91 are those eigenvectors associated with the @xmath33 eigenvalues in @xmath89 , and the eigendecomposition of @xmath86 is similarly partitioned .",
    "the subspace we recover is spanned by the columns of @xmath92 and we wish to have this subspace as close as possible to the tangent space spanned by the columns of @xmath90 .",
    "the orthogonal projectors onto the tangent and computed subspaces , @xmath8 and @xmath9 respectively , are given by @xmath93 define @xmath94 to be the @xmath4th largest eigenvalue of @xmath84 , or the last entry on the diagonal of @xmath88 .",
    "this eigenvalue corresponds to variance in a tangent space direction .",
    "we are now in position to state the theorem .",
    "note that we have made use of the fact that the columns of @xmath83 are the eigenvectors of @xmath76 , that @xmath95 are hermitian ( diagonal ) matrices , and that the frobenius norm is used to measure distances .",
    "the reader is referred to @xcite for the theorem in its original form .",
    "[ thm : stewart ] let @xmath96 and consider    * ( condition 1 ) @xmath97 * ( condition 2 ) @xmath98 .",
    "then , provided that conditions 1 and 2 hold , @xmath99    2ex    it is instructive to consider the perturbation @xmath100 as an operator with range in @xmath101 .",
    "ideally , the perturbation would have very little effect on the tangent space ; @xmath100 would map points from the column space of @xmath90 to the column space of @xmath90 . as this will not be the case in general",
    ", we expect @xmath102 will have a component that is normal to the tangent space .",
    "the numerator @xmath103 of measures this normal component , thereby quantifying the effect of the perturbation on the tangent space .",
    "then @xmath104 measures the component that remains in the tangent space after the action of @xmath100 .",
    "as this component does not contain curvature , @xmath104 corresponds to the spectrum of the noise projected in the tangent space .",
    "similarly , @xmath105 measures the spectrum of the curvature and noise perturbation normal to the tangent space .",
    "thus , when @xmath100 leaves the column space of @xmath90 mostly unperturbed ( i.e. , @xmath103 is small ) and the spectrum of the tangent space is well separated from that of the noise and curvature , the estimated subspace will form only a small angle with the true tangent space . in the next section",
    ", we use the machinery of this classic result to bound the angle caused by the perturbation @xmath100 and develop an interpretation of the conditions of theorem [ thm : stewart ] suited to the noise - curvature trade - off .",
    "given the framework for analysis developed above , the terms appearing in the statement of theorem [ thm : stewart ] ( @xmath106 , @xmath107 , @xmath108 , @xmath109 , and @xmath94 ) must be controlled .",
    "we notice that @xmath100 is a symmetric matrix , so that @xmath110 . using the triangle inequality and the geometric constraints @xmath111 the norms",
    "may be controlled by bounding the contribution of each term in the perturbation @xmath100 : @xmath112 importantly , we seek control over each ( right - hand side ) term in the finite - sample regime , as we assume a possibly large but finite number of sample points @xmath13 .",
    "therefore , bounds are derived through a careful analysis employing concentration results and techniques from non - asymptotic random matrix theory .",
    "the technical analysis is presented in the appendix and proceeds by analyzing three distinct cases : the covariance of bounded random matrices , unbounded random matrices , and the interaction of bounded and unbounded random matrices .",
    "the eigenvalue @xmath94 is bounded again using random matrix theory . in all cases , care is taken to ensure that bounds hold with high probability that is independent of the ambient dimension @xmath49 .",
    "other , possibly tighter , avenues of analysis may be possible for some of the bounds presented in the appendix .",
    "however , the presented analysis avoids large union bounds and dependence on the ambient dimension to state results holding with high probability .",
    "alternative analyses are possible , often sacrificing probability to exhibit sharper concentration .",
    "we proceed with a theoretical analysis holding with the highest probability while maintaining accurate results .",
    "we are now in position to apply theorem [ thm : stewart ] and state our main result .",
    "first , we make the following definitions involving the principal curvatures : @xmath113 @xmath114 the constant @xmath115 is the mean curvature ( rescaled by a factor of @xmath4 ) in codimension @xmath37 , for @xmath116 .",
    "the curvature of the local model is quantified by @xmath15 and is a natural result of our use of the frobenius norm .",
    "note that @xmath117 .",
    "we also define the constants @xmath118 to be used when strictly positive curvature terms are required .",
    "the main result is formulated in the appendix and makes the following benign assumptions on the number of sample points @xmath13 and the probability constants @xmath119 and @xmath120 : @xmath121 we note that these assumptions are easily satisfied for any reasonable sampling density .",
    "further , the assumptions are not crucial to the result but allow for a more compact presentation .",
    "[ thm : mainresult1 ] define @xmath96 and let the following conditions hold ( in addition to the benign assumptions stated above ) :    * ( condition 1 ) @xmath97 , * ( condition 2 ) @xmath122 .",
    "then , with probability greater than @xmath123 over the joint random selection of the sample points and random realization of the noise , @xmath124 }     { \\frac{r^2}{d+2 } - \\frac{\\mathcal{k}r^4}{2(d+2)^2(d+4 ) } - \\sigma^2\\left(\\sqrt{d } + \\sqrt{d-   d}\\right ) - \\frac{1}{\\sqrt{n}}\\zeta_{\\text{denom}}(\\xi,\\xi_{\\lambda})},\\ ] ] where the following definitions have been made to ease the presentation :    * geometric constants @xmath125 ^ 2\\right]^{\\frac{1}{2 } } , & \\text{(curvature ) } \\\\",
    "\\nu(\\xi ) & = \\frac{1}{2}\\frac{(d+3)}{(d+2)}p_1(\\xi ) , & \\text{(linear -- curvature ) } \\\\",
    "\\eta_1 & = \\sigma , & \\text{(noise ) } \\\\      \\eta_2(\\xi_{\\lambda } ) & = \\frac{r}{\\sqrt{d+2}}p_2(\\xi_{\\lambda } ) , & \\text{(linear -- noise)}\\\\       \\eta_3(\\xi ) & = \\frac{\\mathcal{k}^{1/2}r^2}{(d+2)\\sqrt{2(d+4)}}p_5(\\xi ) , & \\text{(curvature -- noise ) } \\\\      \\eta(\\xi,\\xi_{\\lambda } ) & = p_3(\\xi,\\sqrt{d(d - d ) } ) \\bigg[\\eta_1 + \\eta_2(\\xi_{\\lambda } ) + \\eta_3(\\xi)\\bigg ] ,      \\end{aligned}\\ ] ] * finite sample correction terms ( numerator ) @xmath126 * finite sample correction terms ( denominator ) @xmath127 , & \\text{(linear)}\\\\          \\zeta_4(\\xi ) & = \\frac{(k^{(+)})^2r^4}{4}\\left(p_1(\\xi ) + \\frac{1}{\\sqrt{n}}p_1 ^ 2(\\xi)\\right ) , & \\text{(curvature)}\\\\          \\zeta_5(\\xi,\\xi_{\\lambda } ) & = 2 r \\sigma \\frac{d}{\\sqrt{d+2 } } p_2(\\xi_{\\lambda } ) p_3(\\xi , d ) , & \\text{(linear -- noise)}\\\\          \\zeta_6(\\xi ) & = 2 \\mathcal{k}^{\\frac{1}{2 } } r^2 \\sigma \\frac{(d - d)}{(d+2)\\sqrt{2(d+4)}}p_3(\\xi , d - d)p_5(\\xi ) , & \\text{(curvature -- noise ) } \\\\",
    "\\zeta_7(\\xi ) & = \\frac{5}{2}\\sigma^2\\left[\\sqrt{d}p_4(\\xi,\\sqrt{d } ) + \\sqrt{d - d}p_4(\\xi,\\sqrt{d - d } ) \\right ] , & \\text{(noise)}\\\\          \\zeta_{\\text{denom}}(\\xi,\\xi_{\\lambda } ) & = \\zeta_3(\\xi ) + \\zeta_4(\\xi ) + \\zeta_5(\\xi,\\xi_{\\lambda } ) + \\zeta_6(\\xi ) + \\zeta_7(\\xi ) ,      \\end{aligned}\\ ] ] + and * probability - dependent terms ( i.e. , terms depending on the probability constants ) + @xmath128 @xmath129 @xmath130    condition 2 is simplified from its original statement in theorem [ thm : stewart ] by noticing that @xmath100 is a symmetric matrix so that @xmath110 .",
    "then , applying the norm bounds computed in the appendix to theorem [ thm : stewart ] and choosing the probability constants @xmath131 yields the result .",
    "2ex    the bound will be demonstrated in section [ sec : numerical ] to accurately track the angle between the true and computed tangent spaces at all scales .",
    "as the bound is either monotonically decreasing ( for the curvature - free case ) , monotonically increasing ( for the noise - free case ) , or decreasing at small scales and increasing at large scales ( for the general case ) , we expect that it has a unique minimizer .",
    "the optimal scale , @xmath78 , for tangent space recovery may therefore be selected as the @xmath13 for which is minimized ( an equivalent notion of the optimal scale may be given in terms of the neighborhood radius @xmath12 ) .",
    "note that the constants @xmath119 and @xmath120 need to be selected to ensure that this bound holds with high probability .",
    "for example , setting @xmath132 and @xmath133 yields probabilities of 0.81 , 0.80 , and 0.76 when @xmath134 and @xmath135 , respectively .",
    "we also note that the probability given by is more pessimistic than we expect in practice .    as introduced in section [ sec : overview ] , we may interpret @xmath136 as the condition number for tangent space recovery . noting that the denominator in is a lower bound on @xmath137",
    ", we analyze the condition number via the bounds for @xmath94 , @xmath138 , and @xmath139 . using these bounds in the main result",
    ", we see that when @xmath136 is small , we recover a tight approximation to the true tangent space .",
    "likewise , when @xmath136 becomes large , the angle between the computed and true subspaces becomes large .",
    "the notion of an angle loses meaning as @xmath136 tends to infinity , and we are unable to recover an approximating subspace .",
    "condition 1 , requiring that the denominator be bounded away from zero , has an important geometric interpretation .",
    "as noted above , the conditioning of the subspace recovery problem improves as @xmath137 becomes large .",
    "condition 1 imposes that the spectrum corresponding to the linear subspace ( @xmath94 ) be well separated from the spectra of the noise and curvature perturbations encoded by @xmath140 . in this way",
    ", condition 1 quantifies our requirement that there exists a scale such that the linear subspace is sufficiently decoupled from the effects of curvature and noise .",
    "when the spectra are not well separated , the angle between the subspaces becomes ill defined . in this case , the approximating subspace contains an eigenvector corresponding to a direction orthogonal to the true tangent space .",
    "condition 2 is a technical requirement of theorem [ thm : stewart ] .",
    "provided that condition 1 is satisfied , we observe that a sufficient sampling density will ensure that condition 2 is met .",
    "further , we numerically observe that the main result accurately tracks the subspace recovery error even in the case when condition 2 is violated . in such a case",
    ", the bound may not remain as tight as desired but its behavior at all scales remains consistent with the subspace recovery error tracked in our experiments .    before numerically demonstrating our main result",
    ", we quantify the separation needed between the linear structure and the noise and curvature with a geometric uncertainty principle .",
    "condition 1 indeed imposes a geometric requirement for tangent space recovery .",
    "solving for the range of scales for which condition 1 is satisfied and requiring the solution be real yields the geometric uncertainty principle stated in section [ sec : overview ] .",
    "we note that this result is derived using @xmath16 , defined in equation , as the full expression for @xmath137 does not allow for an algebraic solution .",
    "the geometric uncertainty principle expresses a natural requirement for the subspace recovery problem , ensuring that the perturbation to the tangent space is not too large .",
    "recall that , with high probability , the noise orthogonal to the tangent plane concentrates on a sphere with mean curvature @xmath25 .",
    "we therefore expect to require that the curvature of the manifold be less than the curvature of this noise - ball .",
    "recalling the definitions of @xmath115 and @xmath15 from equation , @xmath141 is the mean curvature in codimension @xmath37 .",
    "the quadratic mean of the @xmath33 mean curvatures is given by @xmath142 and we denote this normalized version of curvature as @xmath143 .",
    "then requires that @xmath144 noting that @xmath145 , the uncertainty principle indeed requires that the mean curvature of the manifold be less than that of the perturbing noise - ball .    as we expected to enforce @xmath146 ,",
    "the uncertainty principle is in fact more restrictive than our intuition .",
    "however , as only finite - sample corrections have been neglected in @xmath16 , the @xmath147 restriction of is of the correct order .",
    "interestingly , this more restrictive requirement for tangent space recovery is only accessible through the careful perturbation analysis presented above and an estimate obtained by a more naive analysis would be too lax .",
    "in this section we present an experimental study of the tangent space perturbation results given above . in particular , we demonstrate that the bound presented in the main result ( theorem [ thm : mainresult1 ] ) accurately tracks the subspace recovery error at all scales . as this analytic result requires no decompositions of the data matrix , our analysis provides an efficient means for obtaining the optimal scale for tangent space recovery .",
    "we first present a practical use of the main result , demonstrating its accuracy when the intrinsic dimensionality , curvature , and noise level are known .",
    "we then experimentally test the stability of the bound when these parameters are only imprecisely available , as is the case when they must be estimated from the data .",
    "finally , we demonstrate the accurate estimation of the noise level and local curvature .",
    "we generate a data set sampled from a 3-dimensional manifold embedded in @xmath148 according to the local model by uniformly sampling @xmath149 points inside a ball of radius @xmath150 in the tangent plane .",
    "curvature and the standard deviation @xmath14 of the added gaussian noise will be specified in each experiment .",
    "we compare our bound with the true subspace recovery error .",
    "the tangent plane at reference point @xmath7 is computed at each scale @xmath13 via pca of the @xmath13 nearest neighbors of @xmath7 .",
    "the true subspace recovery error @xmath80 is then computed at each scale . note that computing the true error requires @xmath13 svds .",
    "a `` true bound '' is computed by applying theorem [ thm : stewart ] after measuring each perturbation norm directly from the data .",
    "while no svds are required , this true bound utilizes information that is not practically available and represents the best possible bound that we can hope to achieve .",
    "we will compare the mean of the true error and mean of the true bound over 10 trials ( with error bars indicating one standard deviation ) to the bound given by our main result in theorem [ thm : mainresult1 ] , holding with probability greater than 0.8 .",
    "for the experiments in this section , the bound is computed with full knowledge of the necessary parameters .",
    "in fact , as knowledge of @xmath4 provides an exact expression for @xmath94 , ( i.e. , no additional geometric information is encoded in @xmath94 that is not already encoded in @xmath4 ) we compute @xmath94 exactly .",
    "further , as the principle curvatures are known , we compute a tighter bound for @xmath151 using @xmath152 in place of @xmath153 .",
    "doing so only affects the height of the curve ; its trend as a function of scale is unchanged . in practice ,",
    "the important information is captured by tracking the trend of the true error regardless of whether it provides an upper bound to any random fluctuation of the data . in fact , the numerical results indicate that an accurate tracking of error is possible even when condition 2 of theorem [ thm : mainresult1 ] is violated .",
    ".principal curvatures of the manifold for figures [ fig : results]b and [ fig : results]c .",
    "[ cols=\"^,^,^,^\",options=\"header \" , ]     while it is unrealistic for data to be observed in the desired coordinate system aligned with the principal directions , tracking the trajectory of the center in each dimension yields the rotation necessary to transform to this coordinate system .",
    "further , tracking the trajectory may yield a clean estimate of the reference point of the local model in the presence of noise .",
    "while the noise leaves this trajectory unstable at small scales , it is very stable at scales above the noise level .",
    "using the stability of the trajectory at large scales may allow us to extrapolate back and accurately recover the trajectory at small scales , yielding an estimate of the `` denoised '' reference point .",
    "local pca of manifold - valued data has received attention in several recent works ( for example , those referenced in section [ sec : intro ] ) . in particular , the analyses of @xcite and @xcite , after suitable translation of notation and assumptions , demonstrate growth rates for the pca spectrum that match those computed in the present work .",
    "the focus of our analysis is the perturbation of the eigenspace recovered from the local data covariance matrix .",
    "we therefore confirm our results with those most similar from the literature .",
    "two closely related results are those of @xcite , in which matrix perturbation theory is used to study the pca spectrum , and @xcite , where neighborhood size and sampling conditions are given to ensure an accurate tangent space estimate from noise - free manifold valued data .    in @xcite , a finite - sample pca analysis assuming a linear model is presented .",
    "keeping @xmath13 and @xmath49 fixed , the noise level @xmath14 is considered to be a small parameter . much like the analysis of the present paper , the results are derived in the non - asymptotic setting .",
    "however , the bound on the angle between the finite - sample and population eigenvectors is summarized in @xcite for the asymptotic regime where @xmath13 and @xmath49 become large .",
    "the result , restated here in our notation , takes the form : @xmath154 we note that the main results of @xcite are stated for @xmath155 and that our analysis expects the opposite in general , although it is not explicitly required .",
    "nonetheless , by setting curvature terms to zero , our results recover the reported leading behavior following the same asymptotic regime as @xcite , where terms @xmath156 are neglected and @xmath14 is treated as a small parameter . after setting all curvature terms to zero",
    ", we assume condition 1 holds such that the denominator @xmath137 is sufficiently well conditioned and we may drop all terms other than @xmath94 . then our main result has the form : @xmath157 = \\frac{\\sigma}{\\sqrt{\\lambda_d}}\\frac{\\sqrt{d(d - d)}}{\\sqrt{n } } + \\mathcal{o}(\\sigma^2).\\ ] ] setting @xmath158 to match the analysis in @xcite recovers its curvature - free result .",
    "next , @xcite presents an analysis of local pca differing from ours in two crucial ways .",
    "first , the analysis of @xcite does not include high - dimensional noise perturbation and the data points are assumed to be sampled directly from the manifold .",
    "second , the sampling density is not fixed , whereas the neighborhood size determines the number of sample points in our analysis .",
    "in fact , a goal of the analysis in @xcite is to determine a sampling density that will yield an accurate tangent space estimate .",
    "allowing for a variable sampling density has the effect of decoupling the condition number @xmath136 from the norm @xmath159 measuring the amount of `` lift '' in directions normal to the tangent space due to the perturbation . the analysis of @xcite proceeds by first determining the optimal neighborhood radius @xmath79 in the asymptotic limit of infinite sampling , @xmath160 .",
    "this approach yields the requirement that the spectra associated with the tangent space and curvature be sufficiently separated .",
    "translating to our notation , setting noise terms to zero , and assuming the asymptotic regime of @xcite such that we may neglect finite - sample correction terms , we recover condition 1 of our main result theorem [ thm : mainresult1 ] : @xmath161 thus , theorem 1 of @xcite requires that @xmath12 be chosen such that the subspace recovery problem is well conditioned in the same sense that we require by condition 1 . substituting the expectations for each term in yields @xmath162 implying the choice @xmath163 ( for a constant @xmath164 ) , in agreement with the analysis of @xcite .",
    "once the proper neighborhood size has been selected , the decoupling assumed in @xcite allows a choice of sampling density large enough to ensure a small angle .",
    "again translating to our result , once @xmath12 is selected so that the denominator @xmath137 is well conditioned , the density may be chosen such that the @xmath22 decay of the numerator @xmath159 allows for a small recovery angle .",
    "thus , we see that in the limit of infinite sampling and absence of noise , our results are consistent with those of @xcite in the fixed density setting .",
    "as explored in the experimental section , practical methods must be developed to recover parameters such as dimension , curvature , and noise .",
    "such parameters are necessary for any analysis or algorithm and should be recovered directly from the data rather than estimated by _ a priori _ fixed values . in section [ sec : numerical ] , we demonstrate methods for recovery of noise level and curvature .",
    "the accuracy and stability of such schemes remain to be tested .",
    "there exist other statistical methods for estimating the noise level present in a data set that should be useful in this context ( see , for example , @xcite ) . in @xcite , the smallest multiscale singular values are used as an estimate for the noise level and a scale - dependent estimate of noise variance is suggested in @xcite for curve - denoising .",
    "methods for estimating curvature ( e.g. , @xcite ) have been developed for application to computer vision and extensions to the high - dimensional setting should be explored .",
    "further , if one is willing to perform many svds of large matrices , our method presented in section [ sec : numerical ] combined with the analysis of @xcite might yield the individual principal curvatures .",
    "the experimental results presented above suggest the particular importance of accurately estimating the intrinsic dimension @xmath4 , for which there exist several algorithms .",
    "fukunaga introduced a local pca - based approach for estimating @xmath4 in @xcite .",
    "the recent work in @xcite presents a multiscale approach that estimates @xmath4 in a pointwise fashion . performing an svd at each scale ,",
    "@xmath4 is determined by examining growth rate of the multiscale singular values .",
    "it would be interesting to investigate if this approach remains robust if only a coarse exploration of the scales is performed , as it may be possible to reduce the computational cost through an svd - update scheme .",
    "another scale - based approach is presented in @xcite and the problem was studied from a dynamical systems perspective in @xcite .      for a tractable analysis",
    ", assumptions about sampling must be made . in this work",
    "we have assumed uniform sampling in the tangent plane .",
    "this is merely one choice and we have conducted initial experiments uniformly sampling the manifold rather than the tangent plane .",
    "results suggest that for a given radius , sampling the manifold yields a smaller curvature perturbation than that from sampling the tangent plane .",
    "while more rigorous analysis and experimentation is needed , it is clear that consideration must be given to the sampling assumptions for any practical algorithm .",
    "the tangent plane recovered by our approach may not provide the best approximation over the entire neighborhood from which it was derived . depending on a user - defined error tolerance",
    ", a smaller or larger sized neighborhood may be parameterized by the local chart .",
    "if high accuracy is required , one might only parameterize a neighborhood of size @xmath165 to ensure the accuracy requirement is met . similarly ,",
    "if an application requires only modest accuracy , one may be able to parameterize a larger neighborhood than that given by @xmath78 .",
    "finally , we may wish to use tangent planes recovered from different neighborhoods to construct a covering of a data set .",
    "there exist methods for aligning local charts into a global coordinate system ( for example @xcite , to name a few ) .",
    "care should be taken to define neighborhoods such that a data set may be optimally covered .",
    "this work was supported by the national science foundation [ dms-0941476 to f.g.m . and d.n.k .",
    ", dge-0801680 to d.n.k . ] ; and the department of energy [ de - sc0004096 to f.g.m . ]",
    ".    99    brand , m. ( 2003 ) charting a manifold . in _ adv .",
    "neural inf . process .",
    "15 _ , pp . 961968 . mit press .",
    "broomhead , d. & king , g. ( 1986 ) extracting qualitative dynamics from experimental data .",
    "d _ , * 20*(2 - 3 ) , 217236 .",
    "chen , g. , little , a. , maggioni , m. & rosasco , l. ( 2011 ) some recent advances in multiscale geometric analysis of point clouds . in _ wavelets and",
    "multiscale analysis : theory and applications _",
    ", ed . by j.",
    "cohen , & a.  zayed , pp .",
    "springer .",
    "davis , c. & kahan , w. ( 1970 ) the rotation of eigenvectors by a perturbation iii .",
    "_ siam j. numer .",
    "_ , * 7 * , 146 .",
    "donoho , d. & johnstone , i. ( 1995 ) adapting to unknown smoothness via wavelet shrinkage .",
    "_ j. amer .",
    "_ , * 90 * , 12001224 .",
    "edelman , a. ( 1988 ) eigenvalues and condition numbers of random matrices .",
    "_ siam j. matrix anal .",
    "_ , * 9*(4 ) , 543560 .",
    "feiszli , m. & jones , p. ( 2011 ) curve denoising by multiscale singularity detection and geometric shrinkage .",
    "_ , * 31 * , 392409",
    ".    froehling , h. , crutchfield , j. , farmer , d. , packard , n. & shaw , r. ( 1981 ) on determining the dimension of chaotic flows .",
    "d _ , * 3 * , 605617 .",
    "fukunaga , k. & olsen , d. ( 1971 ) an algorithm for finding intrinsic dimensionality of data .",
    "_ ieee trans .",
    "_ , * c-20*(2 ) , 176183 .",
    "giaquinta , m. & modica , g. ( 2009 ) _ mathematical analysis : an introduction to functions of several variables_. springer .",
    "golub , g. & loan , c.  v. ( 1996 ) _ matrix computations_. jhu press .    johnstone , i. ( 2001 ) on the distribution of the largest eigenvalue in principal component analysis . _",
    "_ , * 29 * , 295327 .",
    "jones , p. ( 1990 ) rectifiable sets and the traveling salesman problem . _",
    "_ , * 102 * , 115 .",
    "jung , s. & marron , j. ( 2009 ) consistency in high dimension , low sample size context .",
    "_ , * 27 * , 41044130 .",
    "kambhatla , n. & leen , t. ( 1997 ) dimension reduction by local principal component analysis .",
    "_ neural comput .",
    "_ , * 9 * , 14931516 .",
    "krsek , p. , lukcs , g. & martin , r.  r. ( 1998 ) algorithms for computing curvatures from range data . in _ the mathematics of surfaces",
    "viii , information geometers _ , pp .",
    "laurant , b. & massart , p. ( 2000 ) adaptive estimation of a quadratic functional by model selection .",
    "_ , * 28*(5 ) , 13021338 .",
    "lin , t. & zha , h. ( 2008 ) riemannian manifold learning .",
    "_ ieee trans . pattern anal .",
    "_ , * 30 * , 796809 .",
    "mitra , n. , nguyen , a. & guibas , l. ( 2004 ) estimating surface normals in noisy point cloud data .",
    "_ internat .",
    "j. comput .",
    "_ , * 14*(45 ) , 261276 .",
    "muirhead , r. ( 1982 ) _ aspects of multivariate statistical theory_. wiley .",
    "nadler , b. ( 2008 ) finite sample approximation results for principal component analysis : a matrix perturbation approach .",
    "_ , * 36 * , 27922817 .",
    "ohtake , y. , belyaev , a. & seidel , h .-",
    "( 2006 ) a composite approach to meshing scattered data . _ graph . models _ , * 68 * , 255267 .",
    "roweis , s. & saul , l. ( 2000 ) nonlinear dimensionality reduction by locally linear embedding .",
    "_ science _ , * 290 * , 23232326 .",
    "roweis , s. , saul , l. & hinton , g. ( 2002 ) global coordination of locally linear models . in _ adv .",
    "neural inf . process .",
    "14 _ , pp . 889896 . mit press .",
    "shawe - taylor , j. & cristianini , n. ( 2003 ) estimating the moments of a random vector with applications . in _ proc . of gretsi 2003 conference _ ,",
    ". 4752 .",
    "singer , a. & wu , h .-",
    "( 2012 ) vector diffusion maps and the connection laplacian .",
    "pure appl .",
    "_ , * 64 * , 10671144 .",
    "stewart , g. & sun , j. ( 1990 ) _ matrix perturbation theory_. academic press .",
    "tropp , j. ( 2011 ) user - friendly tail bounds for sums of random matrices .",
    "_ , * 12*(4 ) , 389434 .",
    "tyagi , h. , vural , e. & frossard , p. ( 2012 ) tangent space estimation for smooth embeddings of iemannian manifolds .",
    "_ arxiv preprint , available at http://arxiv.org / abs/1208.1065_.    vershynin , r. ( 2012 ) introduction to the non - asymptotic analysis of random matrices . in _ compressed sensing ,",
    "theory and applications _",
    ", ed . by y.",
    "eldar , & g.  kutyniok , pp .",
    "cambridge .",
    "wang , x. & marron , j. ( 2008 ) a scale - based approach to finding effective dimensionality in manifold learning . _ electron .",
    "_ , * 2 * , 127148 .    williams , d. & shah , m. ( 1992 ) a fast algorithm for active contours and curvature estimation .",
    "image und .",
    "_ , * 55*(1 ) , 1426 .",
    "yang , l. ( 2008 ) alignment of overlapping locally scaled patches for multidimensional scaling and dimensionality reduction .",
    "_ ieee trans . pattern anal .",
    "_ , * 30 * , 438450 .",
    "zhang , t. , szlam , a. , wang , y. & lerman , g. ( 2010 ) randomized hybrid linear modeling by local best - fit flats . in _",
    "cvpr _ , pp .",
    "19271934 .",
    "zhang , z. & zha , h. ( 2004 ) principal manifolds and nonlinear dimensionality reduction via tangent space alignment .",
    "_ siam j. sci .",
    "_ , * 26 * , 313338 .",
    "technical calculations are presented in this appendix . in particular , the norm of each random matrix contributing to the perturbation term @xmath100 , defined in equation , is bounded with high probability . the analysis is divided between three cases : ( 1 ) norms of products of bounded random matrices ; ( 2 ) norms of products of unbounded random matrices ; and ( 3 ) norms of products of bounded and unbounded random matrices .",
    "each case requires careful attention to derive a tight result that avoids large union bounds and ensures high probability that is independent of the ambient dimension @xmath49 .",
    "the analysis proceeds by bounding the eigenvalues of the matrices @xmath166 , @xmath167 , and @xmath168 using results from random matrix theory and properties of the spectral norm . a detailed analysis of each of the three cases follows .    before we start the proofs , one last comment is in order .",
    "the reader will notice that we sometimes introduce benign assumptions about the number of samples @xmath13 or the dimensions @xmath4 or @xmath49 in order to provide bounds that are simpler to interpret .",
    "these assumptions are not needed to derive any of the results ; they are merely introduced to help us simplify a complicated expression , and introduce upper bounds that hold under these fairly benign assumptions .",
    "this should help the reader interpret the size of the different terms .",
    "we often vectorize matrices by concatenating the columns of a matrix .",
    "if @xmath169 $ ] , then we define @xmath170 we denote the largest and smallest eigenvalue of a matrix @xmath71 by @xmath171 respectively . in the main body of the paper , we use the standard notation @xmath172 to denote the sample mean of @xmath13 columns from the matrix @xmath60 . in this appendix , we introduce a second notation to denote the same concept , @xmath173 } = \\overline{x } = \\frac{1}{n } \\sum_{n=1}^n x^{(n)}.\\ ] ] finally , we denote by @xmath174 $ ] the expectation of the random matrix @xmath60 and by @xmath175 the probability of event @xmath176 .",
    "we seek a bound on the maximum and minimum ( nonzero ) eigenvalue of the matrix @xmath177 as only the nonzero eigenvalues are of interest , we proceed by considering only the nonzero upper - left @xmath178 block of the matrix in , or equivalently , by ignoring the trailing zeros of each realization @xmath179 .",
    "thus , momentarily abusing notation , we consider the matrix in to be of dimension @xmath178 .",
    "the analysis utilizes the following theorem found in @xcite .",
    "[ thm : tropp ] consider a finite sequence @xmath180 of independent , random , self adjoint matrices that satisfy @xmath181 compute the minimum and maximum eigenvalues of the sum of expectations , @xmath182 then @xmath183 \\leq      d\\left[\\frac{e^{-\\delta}}{(1-\\delta)^{(1-\\delta)}}\\right]^{\\mu_{\\min}/\\lambda_{\\infty } } ,      \\quad \\text{for } \\delta \\in [ 0,1 ] , \\text { and } \\\\       & \\mathbb{p}\\left[\\lambda_{\\max}\\left(\\sum_{k=1}^n x_k\\right ) \\geq        ( 1+\\delta)\\mu_{\\max } \\right ] \\leq      d\\left[\\frac{e^{\\delta}}{(1+\\delta)^{(1+\\delta)}}\\right]^{\\mu_{\\max}/\\lambda_{\\infty } } ,      \\quad \\text{for } \\delta \\geq 0",
    ".    \\end{aligned}\\ ] ]    we apply this result to @xmath184 clearly @xmath185 is a symmetric positive - definite matrix and we have @xmath186 .",
    "next , @xmath187 and we set @xmath188 .",
    "simple computations yield @xmath189\\right ) =    \\frac{r^2}{d+2}\\left[1-\\frac{1}{n}\\right]^2 ,",
    "\\quad \\text{and}\\quad     \\lambda_{\\min}\\left(\\sum_{k=1}^n\\operatorname{\\mathbb{e}}[x_k]\\right ) =    \\frac{r^2}{d+2}\\left[1-\\frac{1}{n}\\right]^2,\\ ] ] and we set @xmath190 ^ 2.\\ ] ] by theorem [ thm : tropp ] and using standard manipulations , we have the following result bound for the smallest eigenvalue , @xmath94 in our notation , @xmath191 ^ 2\\left[1 -      \\xi_{\\lambda_d }      \\frac{1}{\\sqrt{n}}\\frac{\\sqrt{8(d+2)}}{(1-\\frac{1}{n } ) } \\right]\\ ] ] with probability greater than @xmath192 .",
    "similarly , the following result holds for the largest eigenvalue , @xmath193 in our notation : @xmath194\\ ] ] with probability greater than @xmath195 , as soon as @xmath196 .",
    "we define the last upper bound as @xmath197 ,    \\label{eig - linear - bound}\\ ] ] and we can use this bound to control the size of all the eigenvalues of the matrix @xmath198 ,",
    "@xmath199    \\ge 1-de^{-\\xi^2}.    \\label{eig - linear - prob}\\ ] ] now that we have computed the necessary bounds for all nonzero linear eigenvalues , we return to our standard notation for the remainder of the analysis : each @xmath179 is of length @xmath49 with @xmath200 for @xmath201 and @xmath202 $ ] is a @xmath203 matrix .      to bound the largest eigenvalue , @xmath204 , of @xmath205 we note that the spectral norm is bounded by the frobenius norm and we use the bound on the frobenius norm derived in section [ sec : pure - curvature ] .",
    "we can use this bound to control the size of all the eigenvalues of the matrix @xmath206 , @xmath207    \\ge 1 - 2e^{-\\xi^2}.    \\label{eig - curvature - prob}\\ ] ] where @xmath208 ^ 2 }    + \\frac{(k^{(+)})^2 r^4}{4 \\sqrt{n}}\\left[(2+\\xi\\sqrt{2 } ) +      \\frac{(2+\\xi\\sqrt{2})^2}{\\sqrt{n } } \\right ] .",
    "\\label{eq : eigcc^t - bound}\\ ] ] the proof of the bound on the frobenius norm is delayed until section [ sec : pure - curvature ] .",
    "+    a different ( possibly tighter ) bound may be derived using theorem [ thm : tropp ] .",
    "however , such a bound would hold with a probability that becomes small when the ambient dimension @xmath49 is large .",
    "we therefore proceed with the bound above , noting that we sacrifice no additional probability by using it here since it is required for the analysis in section [ sec : pure - curvature ] .",
    "we may control the eigenvalues of @xmath209 using standard results from random matrix theory . in particular ,",
    "let @xmath210 and @xmath211 denote the smallest and largest singular value of matrix @xmath58 , respectively .",
    "the following result ( corollary 5.35 of @xcite ) gives a tight control on the size of @xmath210 and @xmath211 when @xmath58 has gaussian entries .",
    "[ thm : gaussiansvs ] let @xmath212 be a @xmath203 matrix whose entries are independent standard normal random variables . then for every @xmath213 , with probability at least @xmath214 one has @xmath215    define @xmath216 and note that the entries of @xmath217 are independent standard normal random variables . let us partition the gaussian vector @xmath46 into the first @xmath4 coordinates , @xmath218 , and last @xmath219 coordinates , @xmath220 , @xmath221 and observe that the matrix @xmath222 only depends on the realizations of @xmath218 .",
    "similarly , the matrix @xmath223 only depends on the realizations of @xmath220 . by theorem [ thm : gaussiansvs ]",
    ", we have @xmath224 with probability at least @xmath225 over the random realization of @xmath218 , as soon as @xmath226 , a condition easily satisfied for any reasonable sampling density .",
    "similarly , @xmath227 with probability at least @xmath228 over the random realization of @xmath220 , as soon as @xmath229 .",
    "begin by recalling the notation used for the curvature constants , @xmath231 the constant @xmath115 quantifies the curvature in codimension @xmath37 , for @xmath232 @xmath233 .",
    "the overall compounded curvature of the local model is quantified by @xmath15 and is a natural result of our use of the frobenius norm .",
    "we note that @xmath117 .",
    "we also recall the positive constants @xmath234    our strategy for bounding the matrix norm @xmath235 begins with the observation that @xmath205 is a sample mean of @xmath13 covariance matrices of the vectors @xmath236 , @xmath237 .",
    "that is , @xmath238})(c-{\\widehat{\\operatorname{\\mathbb{e}}}[c]})^t]}.\\ ] ] we therefore expect that @xmath205 converges toward the centered covariance matrix of @xmath52 .",
    "we will use the following result of shawe - taylor and cristianini @xcite to bound , with high probability , the norm of the difference between this sample mean and its expectation .",
    "[ thm : shawe ] _ ( shawe - taylor & cristianini , @xcite)_.given @xmath13 realizations of a random matrix @xmath239 distributed with probability distribution @xmath240 , we have @xmath241 - { \\widehat{\\operatorname{\\mathbb{e}}}[y]}\\right\\|_f \\leq        \\frac{r}{\\sqrt{n}}\\left(2+\\xi\\sqrt{2}\\right ) \\right\\ } \\ge 1-e^{-\\xi^2}.      \\label{eq : shawe - taylor}\\ ] ] the constant @xmath242 , where @xmath243 is the support of distribution @xmath240 .",
    "we note that the original formulation of the result involves only random vectors , but since the frobenius norm of a matrix is merely the euclidean norm of its vectorized version , we formulate the theorem in terms of matrices .",
    "we also note that the choice of @xmath244 in need not be unique .",
    "our analysis will proceed by using upper bounds for @xmath245 which may not be suprema .",
    "let @xmath246 using theorem [ thm : shawe ] and modifying slightly the proof of corollary 6 in @xcite , which uses standard inequalities , we arrive at @xmath247)(c-\\operatorname{\\mathbb{e}}[c])^tu_2]\\right\\|_f -    \\left\\|{\\widehat{\\operatorname{\\mathbb{e}}}[u_2^t(c-{\\widehat{\\operatorname{\\mathbb{e}}}[c]})(c-{\\widehat{\\operatorname{\\mathbb{e}}}[c]})^tu_2]}\\right\\|_f \\bigg| &    \\nonumber \\\\",
    "\\leq \\left\\|\\operatorname{\\mathbb{e}}[u_2^tcc^tu_2 ] - { \\widehat{\\operatorname{\\mathbb{e}}}[u_2^tcc^tu_2]}\\right\\|_f    \\negthickspace + \\left\\|\\operatorname{\\mathbb{e}}[u_2^tc ] - { \\widehat{\\operatorname{\\mathbb{e}}}[u_2^tc]}\\right\\|_f^2 \\leq    \\frac{r_{c}^2}{\\sqrt{n}}\\left(2+\\xi_{c}\\sqrt{2}\\right ) & +    \\frac{r_{c}^2}{n}\\left(2+\\xi_{c}\\sqrt{2}\\right)^2\\end{aligned}\\ ] ] with probability greater than @xmath248 over the random selection of the sample points . to complete the bound",
    "we must compute @xmath249 and @xmath250)(c-\\operatorname{\\mathbb{e}}[c])^tu_2]\\right\\|_f$ ] .",
    "a simple norm calculation shows @xmath251 and we set @xmath252 .",
    "next , the expectation takes the form @xmath253)(c-\\operatorname{\\mathbb{e}}[c])^tu_2]\\big\\|_f ~=~    \\big\\|\\operatorname{\\mathbb{e}}[u_2^tcc^tu_2 ] - \\operatorname{\\mathbb{e}}[u_2^tc]\\operatorname{\\mathbb{e}}[c^tu_2 ] \\big\\|_f.\\ ] ] noting that @xmath254 for @xmath255 , we have @xmath256 = \\frac{\\left[3k_{nn}^{ij } + k_{mn}^{ij}\\right]r^4}{4(d+2)(d+4)},\\quad\\text{and}\\quad    \\operatorname{\\mathbb{e}}[c_i]\\operatorname{\\mathbb{e}}[c_j ] = \\frac{\\left[k_{nn}^{ij } +        k_{mn}^{ij}\\right]r^4}{4(d+2)^2}.\\ ] ] computing the norm , @xmath257)(c-\\operatorname{\\mathbb{e}}[c])^tu_2]\\big\\|_f =    \\frac{r^4}{2(d+2)^2(d+4 ) } \\sqrt{\\sum_{i , j = d+1}^d\\left[(d+1)k_{nn}^{ij}-k_{mn}^{ij}\\right]^2}.\\end{aligned}\\ ] ] finally , putting it all together , we conclude that @xmath258 ^ 2 }    \\nonumber \\\\     & + \\frac{1}{\\sqrt{n}}\\frac{(k^{(+)})^2      r^4}{4}\\left[\\left(2+\\xi_{c}\\sqrt{2}\\right ) +      \\frac{1}{\\sqrt{n}}\\left(2+\\xi_{c}\\sqrt{2}\\right)^2    \\right ] \\end{aligned}\\ ] ] with probability greater than @xmath248 over the random selection of the sample points .      our approach for bounding the matrix norm @xmath260 mirrors that of section [ sec : pure - curvature ] .",
    "here , we use that @xmath261 = 0 $ ] for @xmath262 and proceed as follows .",
    "we have @xmath263 reasoning as in the previous section , we have @xmath247)(\\ell-\\operatorname{\\mathbb{e}}[\\ell])^tu_1]\\right\\|_f -    \\left\\|{\\widehat{\\operatorname{\\mathbb{e}}}[u_2^t(c-{\\widehat{\\operatorname{\\mathbb{e}}}[c]})(\\ell-{\\widehat{\\operatorname{\\mathbb{e}}}[\\ell]})^tu_1]}\\right\\|_f \\bigg| &    \\nonumber \\\\     \\leq \\left\\|\\operatorname{\\mathbb{e}}[u_2^tc\\ell^tu_1 ] - { \\widehat{\\operatorname{\\mathbb{e}}}[u_2^tc\\ell^tu_1]}\\right\\|_f +    \\left\\|{\\widehat{\\operatorname{\\mathbb{e}}}[\\ell^tu_1 ] } - \\operatorname{\\mathbb{e}}[\\ell^tu_1]\\right\\|_f    \\bigg(\\left\\|{\\widehat{\\operatorname{\\mathbb{e}}}[u_2^tc ] } - \\operatorname{\\mathbb{e}}[u_2^tc]\\right\\|_f    + &    \\left\\|\\operatorname{\\mathbb{e}}[u_2^tc]\\right\\|_f\\bigg ) \\nonumber \\\\    \\leq    \\frac{r_{c}r_{\\ell}}{\\sqrt{n}}\\left(2+\\xi_{c\\ell}\\sqrt{2}\\right ) +",
    "\\frac{r_{\\ell}}{\\sqrt{n}}\\left(2+\\xi_{\\ell}\\sqrt{2}\\right )    \\bigg[\\frac{r_{c}}{\\sqrt{n}}\\left(2+\\xi_{c}\\sqrt{2}\\right ) +    \\left\\|\\operatorname{\\mathbb{e}}[u_2^tc]\\right\\|_f \\bigg ] & \\end{aligned}\\ ] ] with probability greater than @xmath264 over the random selection of the sample points .",
    "finally , we set @xmath265 and conclude @xmath266\\end{aligned}\\ ] ] with probability greater than @xmath267 over the random selection of the sample points .",
    "we seek bounds for the matrix norms of the form @xmath269 because @xmath58 is composed of @xmath13 columns of independent realizations of a @xmath49-dimensional gaussian vector , the matrix @xmath212 defined by @xmath270 is wishart @xmath271 , where @xmath272 . as a result",
    ", we can quickly compute bounds on the terms ( [ noise - noise ] ) since they can be expressed as the norm of blocks of @xmath212 .",
    "indeed , let us partition @xmath212 as follows @xmath273 where @xmath274 is @xmath178 , @xmath275 is @xmath276 .",
    "we now observe that @xmath277 is not equal to @xmath278 , but both matrices have the same frobenius norm .",
    "precisely , the two matrices differ only by a left and a right rotation , as explained in the next few lines .",
    "since only the first @xmath4 entries of each column in @xmath90 are nonzero , we can define two matrices @xmath279 and @xmath280 that extract the first @xmath4 entries and apply the rotation associated with @xmath90 , respectively , as follows @xmath281 we define similar matrices @xmath282 and @xmath283 such that @xmath284 . we conclude that @xmath285 in summary , we can control the size of the norms ( [ noise - noise ] ) by controlling the norm of the sub - matrices of a wishart matrix .",
    "we first estimate the size of @xmath286 and @xmath287 .",
    "this is a straightforward affair , since we can apply theorem [ thm : gaussiansvs ] with @xmath288 and @xmath289 , respectively , to get the spectral norm of @xmath274 and @xmath275 .",
    "we then apply a standard inequality between the spectral and the frobenius norm of a matrix @xmath71 , @xmath290 this bound is usually quite loose and equality is achieved only for the case where all singular values of matrix @xmath212 are equal .",
    "it turns out that this special case holds in expectation for the matrices in the analysis to follow , and thus provides a tight estimate of the frobenius norm . using , and we have the following bound @xmath291\\ ] ] with probability greater than @xmath225 over the random realization of the noise . by",
    ", we also have @xmath292\\ ] ] with probability greater than @xmath228 over the random realization of the noise .",
    "it remains to bound @xmath293 .",
    "here we proceed by conditioning on the realization of the last @xmath219 coordinates of the noise vectors in the matrix @xmath58 ; in other words , we freeze @xmath294 . rather than working with gaussian matrices , we prefer to vectorize the matrix @xmath295 and define @xmath296 note that here we unroll the matrix @xmath295 row by row to build @xmath297 . because the frobenius norm of @xmath295 is the euclidean norm of @xmath297 , we need to find a bound on @xmath298 .",
    "conditioning on the realization of @xmath294 , we know ( theorem 3.2.10 of @xcite ) that the distribution of @xmath297 is a multivariate gaussian variable @xmath299 , where @xmath300 is the zero vector of dimension @xmath301 and @xmath302 is the @xmath303 block diagonal matrix containing @xmath4 copies of @xmath304 @xmath305 let @xmath306 be a generalized inverse of @xmath302 ( such that @xmath307 ) , then ( see e.g. theorem 1.4.4 of @xcite ) @xmath308 now , because of theorem [ thm : gaussiansvs ] , @xmath275 has full rank , @xmath33 , with probability @xmath309 , and therefore @xmath302 has full rank , @xmath301 , with the same probability . in the following ,",
    "we derive an upper bound on the size of @xmath298 when @xmath275 has full rank . a similar  but tighter ",
    "bound can be derived when @xmath302 is rank deficient ; we only need to replace @xmath33 by the rank of @xmath275 in the bound that follows .",
    "because the bound derived when @xmath275 is full rank will hold when @xmath275 is rank deficient ( an event which happens with very small probability , anyway ) , we only worry about this case in the following . in this case , @xmath310 and @xmath311 finally , using a corollary of laurant and massart ( immediately following lemma 1 of @xcite ) , we get that , @xmath312 with probability greater than @xmath313 . in the following , we assume that @xmath314 , which happens as soon as @xmath4 or @xmath49 have a moderate size . under this mild assumption",
    "we have @xmath315 in order to compare @xmath316 to @xmath317 , we compute the eigendecomposition of @xmath302 , @xmath318 where @xmath319 is a unitary matrix and @xmath320 contains the eigenvalues of @xmath304 , repeated @xmath4 times .",
    "letting @xmath321 be the largest eigenvalue of @xmath322 , we get the following upper bound , @xmath323 we conclude that , conditioned on a realization of the last @xmath219 entries of @xmath58 , we have @xmath324      | e_2\\right\\ } \\ge 1-e^{-\\xi_{e_3}^2}.    \\label{conditioned - wishart}\\ ] ] to derive a bound on @xmath298 that holds with high probability , we consider the event @xmath325    \\right\\}.\\ ] ] as we will see in the following , the event @xmath326 happens with high probability .",
    "this event depends on the random realization of the top @xmath4 coordinates , @xmath218 , of the gaussian vector @xmath46 ( see ) .",
    "let us define a second likely event , which depends only on @xmath220 ( the last @xmath219 coordinates of @xmath46 ) , @xmath327 theorem [ thm : gaussiansvs ] tells us that the event @xmath328 is very likely , and @xmath329 .",
    "we now show that the probability of @xmath330 is also very small , @xmath331 in order to bound the first term , we condition on @xmath220 , @xmath332 now the two conditions , @xmath333 \\\\      \\frac{1}{\\sqrt{n } }       \\left(1 + \\frac{\\sqrt{d - d } + \\varepsilon}{\\sqrt{n } } \\right )      \\ge    \\sqrt{\\lambda_{\\max}\\left(\\frac{1}{n}a_{22}\\right ) }    \\end{cases}\\ ] ] imply that @xmath334 , \\ ] ] and thus @xmath335       \\lvert e_2 \\right).\\ ] ] because of ( [ conditioned - wishart ] ) the probability on the right - hand side is less than @xmath336 , which does not depend on @xmath220 .",
    "we conclude that @xmath337 finally , since @xmath338 we have @xmath339\\ ] ] with probability greater than @xmath340 over the realization of the noise .",
    "our goal is to bound the matrix norm @xmath342 , with high probability , for @xmath343 .",
    "we detail the analysis for the case where @xmath344 and note that the analysis for @xmath345 is identical up to the difference in dimension . using the decomposition of the matrix @xmath346 defined in the previous section , we have @xmath347 before proceeding with a detailed analysis of this term , let us a derive a bound , which will proved to be very precise , using a back of the envelope analysis .",
    "the entry @xmath348 in the matrix @xmath349 is given by @xmath350 and it measures the average correlation between coordinate @xmath351 of the ( centered ) noise term and coordinate @xmath352 of the linear tangent term .",
    "clearly , this empirical correlation has zero mean , and an upper bound on its variance is given by @xmath353 where the top eigenvalue @xmath193 measures the largest variance of the random variable @xmath51 , measured along the first column of @xmath90 . since the matrix @xmath354 is @xmath178 , we expect @xmath355 we now proceed with the rigorous analysis .",
    "the singular value decomposition of @xmath356 is given by @xmath357 where @xmath358 is the @xmath178 matrix of the singular values , and @xmath359 is a matrix composed of @xmath4 orthonormal column vectors of size @xmath13 . injecting the svd of @xmath360 we have @xmath361",
    "define @xmath362 each row of @xmath363 is formed by the projections of the corresponding row of @xmath364 onto the @xmath4-dimensional subspace of @xmath365 formed by the columns of @xmath359 . as such , the projected row is a @xmath4-dimensional gaussian vector , the norm of which scales like @xmath366 with high probability .",
    "the only technical difficulty involves the fact that the columns of @xmath359 change with the different realizations of @xmath56 .",
    "we need to check that this random rotation of the vectors in @xmath359 does not affect the size of the norm of @xmath363 .",
    "proceeding in two steps , we first freeze a realization of @xmath56 , and compute a bound on @xmath367 that does not depend of @xmath56 .",
    "we then remove the conditioning on @xmath56 , and compute the probability that @xmath368 be very close to @xmath4 .    instead of working with @xmath363",
    ", we define the @xmath369-dimensional vector @xmath370 consider the @xmath371-dimensional gaussian vector @xmath372 in the next few lines , we construct an orthogonal projector @xmath373 such that @xmath374 . as a result",
    ", we will have that @xmath375 , and using standard results on the concentration of the gaussian measure , we will get an estimate of @xmath376 . + first , consider the following @xmath377 matrix @xmath378 formed by stacking @xmath4 copies of @xmath379 in a block diagonal fashion with no overlap ( note that @xmath379 is not a square matrix ) .",
    "we observe that because no overlap exists between the blocks , the rows of @xmath380 are orthonormal and @xmath380 is an orthogonal projector from @xmath381 to @xmath382 .    now , we consider the @xmath383 permutation matrix @xmath384 constructed as follows .",
    "we first construct the @xmath385 matrix @xmath386 by interleaving blocks of zeros of size @xmath387 between the columns vectors of the @xmath178 identity matrix , @xmath388 now consider the matrix @xmath389 obtained by performing a circular shift of the columns @xmath386 to the right by one index , @xmath390 we can iterate this process @xmath391 times and construct @xmath13 such matrices , @xmath392 .",
    "finally , we stack these @xmath13 matrices to construct the @xmath393 permutation matrix @xmath394 by construction , @xmath384 only contains a single nonzero entry , equal to one , in every row and every column , and therefore is a permutation matrix .",
    "finally , the matrix @xmath384 allows to move the action of @xmath359 from the right of @xmath58 to the left , and we have @xmath395 putting everything together , we conclude that the matrix defined by @xmath396 is an orthogonal projector , and therefore @xmath375 . using again the previous bound ( [ norm - chi ] ) on the norm of a gaussian vector",
    ", we have @xmath397 to conclude the proof , we remove the conditioning on @xmath56 , and using ( [ z1-conditioned ] ) we have @xmath398 since @xmath399 , we have @xmath400 finally , combining ( [ eig - linear - bound ] ) , ( [ eig - linear - prob ] ) , ( [ u1q1 ] ) , ( [ le - ev ] ) , and ( [ norm - z1 ] ) we conclude that @xmath401 which implies @xmath402      \\left(d + \\frac{6}{5}\\varepsilon\\right)\\right ) \\nonumber \\\\    & \\ge ( 1 - e^{-\\varepsilon^2/2 } ) ( 1 - d e^{-\\xi^2}).\\end{aligned}\\ ] ] a similar bound holds for @xmath403 .",
    "indeed , we define @xmath404 again , we can construct an orthogonal projector @xmath405 and a permutation @xmath406 with sizes @xmath407 and @xmath408 , respectively , so that @xmath409 by combining ( [ orthoproj1 ] ) and ( [ orthoproj2 ] ) , we can control the concatenated vector @xmath410 by estimating the norm of @xmath411 . we conclude that @xmath412    \\begin{bmatrix }      \\\\ d & + & \\frac{6}{5 } \\xi_{e\\ell}\\\\ \\\\ \\sqrt{d(d - d ) } & + &      \\frac{6}{5 } \\xi_{e\\ell } \\\\ \\\\    \\end{bmatrix }    \\label{le - all}\\ ] ] with probability greater than @xmath413 over the joint random selection of the sample points and random realization of the noise .      the analysis to bound the matrix norm @xmath415 for @xmath343 proceeds in an identical manner to that for the bound on @xmath416 .",
    "we therefore give only a brief outline here . mimicking the reasoning that leads to ( [ le - lambda1 ] )",
    ", we get @xmath417 where @xmath418 is the bound on all the eigenvalues of @xmath419 defined in ( [ eq : eigcc^t - bound ] ) .",
    "this leads to a bound similar to ( [ le - all ] ) for the tangential and curvature components of the noise , @xmath420 with probability greater than @xmath421 over the joint random selection of the sample points and random realization of the noise ."
  ],
  "abstract_text": [
    "<S> constructing an efficient parameterization of a large , noisy data set of points lying close to a smooth manifold in high dimension remains a fundamental problem . </S>",
    "<S> one approach consists in recovering a local parameterization using the local tangent plane . </S>",
    "<S> principal component analysis ( pca ) is often the tool of choice , as it returns an optimal basis in the case of noise - free samples from a linear subspace . to process noisy data samples from a nonlinear manifold </S>",
    "<S> , pca must be applied locally , at a scale small enough such that the manifold is approximately linear , but at a scale large enough such that structure may be discerned from noise . using eigenspace perturbation theory and non - asymptotic random matrix theory , we study the stability of the subspace estimated by pca as a function of scale , and bound ( with high probability ) the angle it forms with the true tangent space . by </S>",
    "<S> adaptively selecting the scale that minimizes this bound , our analysis reveals an appropriate scale for local tangent plane recovery . </S>",
    "<S> we also introduce a geometric uncertainty principle quantifying the limits of noise - curvature perturbation for stable recovery . </S>",
    "<S> manifold - valued data , tangent space , principal component analysis , subspace perturbation , local linear models , curvature , noise . </S>",
    "<S> + 2000 math subject classification : 62h25 , 15a42 , 60b20 </S>"
  ]
}