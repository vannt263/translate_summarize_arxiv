{
  "article_text": [
    "modeling has achieved significant recognition in a variety of research areas such as signal processing , machine learning , computer vision , and pattern recognition .",
    "sparse models refer to the formulation of a signal of interest ( or an approximation of it ) as the linear combination of a small number of elements ( known as atoms ) drawn from a so - called sparsity dictionary ( or dictionary for short ) .",
    "the _ sparse recovery _",
    "problem refers to the identification of the relevant dictionary atoms for a particular signal of interest .",
    "sparse modeling and recovery has a rich history in signal processing , and has received significant attention recently due to the emergence of compressed sensing  @xcite , a framework for compressed signal acquisition that leverages sparse modeling .",
    "we can further restrict the coefficients of the atoms to be non - negative .",
    "in general , non - negativity is advantageous as it makes the model parameters more interpretable .",
    "for instance , lee and seung present non - negative matrix factorization  @xcite , which can learn a part - based representation of faces or documents . just adding non - negativity constraints on a linear model to decompose spectral data",
    "gives the model coefficients the meaning of fractional abundances  @xcite .",
    "non - negative constraints have been applied to independent component analysis in face recognition tasks@xcite .",
    "many approaches have combined non - negativity and sparse modeling . by adding non - negative constraints , several researchers  @xcite refined the performance of applying sparse modeling on a face recognition task obtained by wright et al .",
    "non - negative least squares ( nnls ) has been traditionally used , sometimes accompanied with abundance sum - to - one constraints , to extract the spectral components from hyperspectral pixels ( e.g. ,  @xcite ) , a process called _ spectral unmixing_. recently , nnls has been combined with sparsity with improvements in the unmixing performance  @xcite .",
    "other examples of combining non - negativity and sparse modeling can be found in astronomical imaging  @xcite , proteomics  @xcite , and economics  @xcite .",
    "it has been noted that sparse solutions can be obtained by nnls with subsequent thresholding  @xcite .",
    "several contributions on theoretical analysis of non - negative sparse modeling approaches exist in the literature .",
    "many of them  @xcite are devoted to _ modeling in the absence of noise _ , focusing on questions such as the performance of convex optimization - based approaches for sparse recovery and the uniqueness of the sparse solution .",
    "other works studied the theoretical performance of non - negative sparse modeling in the presence of noise  @xcite ; however , those analyses focus on the specific case of either gaussian or sub gaussian noise .",
    "in contrast to the statistical noise modeling used in previous analyses  @xcite , this paper considers the performance of non - negative sparse modeling under a more general scenario , where the observed signals have an unknown arbitrary distortion . although our analysis can be applied to the additive random noise to obtain existing results , it is also immediately applicable to many other types of distortion , e.g. , distortion present due to nonlinear mixing of the individual components . in the case of spectral unmixing ,",
    "nonlinear mixing may come from nonlinear mixing of atoms  @xcite or spectral mismatches between the spectra of the minerals in the library and those involved in the observation  @xcite .    under this general scenario",
    ", we will investigate the conditions for successful reconstruction  or regression  of a signal using non - negative lasso by expanding previous analyses on the general lasso .",
    "some of these studies consider dictionaries drawn according to a random distribution  @xcite .",
    "others assume an arbitrary dictionary and pose conditions for successful regression that require a combinatorial amount of computation : examples include the spark , restricted isometry property , the restricted eigenvalue property , and the irrepresentable condition  @xcite .",
    "it is possible to relax the computational complexity of the verification process , using tools such as _ coherence _  @xcite ; however , all the aforementioned frameworks consider all possible combinations of atoms simultaneously , and therefore are found to often give very conservative assessments of regression performance .",
    "this paper follows the line of tropp s work  @xcite .",
    "in contrast to the references above , tropp has performed an analysis based on the so - called _ exact recovery condition _ ( erc ) , which provides conditions on successful reconstruction for all combinations of a fixed subset of atoms .",
    "the erc can be easily computed and is compatible with well - known optimization - based and iterative greedy algorithms for sparse signal recovery and regression , and so it appears suitable for the analysis of non - negative versions .",
    "furthermore , because the erc is focused on sparse signals featuring a specific support ( i.e. , a set of indices for the signal s nonzero entries ) , its guarantees are less pessimistic than those provided by alternative approaches , which consider success for all sparse signals simultaneously , regardless of their support . nonetheless , one could conceivably argue that restricting the set of signals of interest to a fixed support with non - negative entries may provide guarantees that are even closer to the actual performance of non - negative sparse recovery .",
    "while the proposed conditions require a specific set of atoms as an input , they are motivated by applications , such as hyperspectral unmixing , in which it is more useful to determine whether a specific set of atoms can be identified via sparsity - based methods , rather than provide guarantees on the recovery for all subsets , since many combinations of atoms might never materialize .      throughout this paper , we assume a linear model with non - negative coefficients ,    cl[cvx : linmod ] = + & ( ) [ eq : lmm ]    where @xmath0 is an observation vector , @xmath1 is a dictionary matrix where the components @xmath2 @xmath3 , called atoms , are stored in its columns , @xmath4 is a non - negative coefficient vector , @xmath5 is an error vector , and @xmath6 ( and its variants ) denote element - wise inequality .",
    "based on this model , we consider the problem of inferring atoms contributing the observation .",
    "in particular , we focus on the non - negative lasso ( nlasso , also known as the non - negative garrote in the statistics literature  @xcite ) :    l[cvx : nlasso ]    ll & -_2 ^ 2 + _ 1 + & ,    where @xmath7 is a positive constant that controls the degree of sparsity .",
    "the weight @xmath7 could be adaptively tuned for each element , e.g. , as done in adaptive lasso  @xcite .",
    "nlasso has been used as a regression method in hyperspectral unmixing  @xcite , as a variable selection method in economics  @xcite , and as a sparse recovery algorithm for face recognition  @xcite and hyperspectral classification  @xcite .",
    "many methods have been proposed for solving ( [ cvx : nlasso ] ) such as non - negativity constrained least angle regression and selection  @xcite , full regularization path  @xcite , alternating direction algorithms  @xcite , iterative reweighted shrinkage  @xcite , split bregman  @xcite , interior point  @xcite , and multiplicative updates  @xcite .",
    "we will derive model recovery conditions ( mrcs ) for nlasso ( [ cvx : nlasso ] ) .",
    "the mrcs allow us to predict if the correct atoms are identifiable via nlasso given a signal model for a specific set of atoms with noise or nonlinearity .",
    "the mrcs are reminiscent of  ( * ? ? ?",
    "* theorem 6 ) due the fact that both results are based on on the karush - kuhn - tucker ( kkt ) conditions for convex optimization solutions .",
    "our contribution are    1 .",
    "the development of mrcs in geometrically interpretable forms that are directly adopted to performance analysis of nlasso on any observation , 2 .",
    "the development of an approximately perfect mrc which not only guarantees correct signal recovery but also provides a `` practical converse '' that guarantees the failure of recovery in a practical sense .",
    "our mrcs indicate whether a certain distortion to a linear observation is tolerable by nlasso while succeeding in identifying the components of the dictionary being observed ; for the specific case of nonlinear mixing , our result predicts accurately whether nlasso succeeds in component identification under nonlinear distortion , depending on its specific direction and magnitude .",
    "the approximately perfect mrc practically meets both necessity and sufficiency .",
    "although the approximately perfect mrc is imperfect in a rigorous mathematical sense , it is quite powerful and in our experiments provides perfect prediction of the performance of nlasso .",
    "we also present some simplified variants of the approximately perfect mrc , which can be considered as customizations of tropp s conditions  @xcite and are rigorously proved mathematically .",
    "our criterion for perfect identification is that the atoms present in the observation are exactly identified by the algorithm ( i.e. , no missed atoms and no false alarms ) , without consideration for accuracy of the coefficient estimate values involved .",
    "we also showcase how our theorem can be used in real applications .",
    "more specifically , our theorem can predict whether nlasso will succeed in selecting the correct materials from a hyperspectral unmixing dictionary in the presence of deviations from the noiseless linear model , including measurement nonlinearities , bias , mismatch , and noise .",
    "our experiments show that the approximately perfect mrc practically gives perfect assessment of the performance of nlasso .",
    "we specify the mathematical notation used in this paper .",
    "the support of @xmath4 is the set of the indices associated with the non - zero elements of @xmath8 , denoted by @xmath9 .",
    "@xmath10 is the range of the matrix @xmath11 .",
    "@xmath12 , @xmath13 , and @xmath14 denote the transpose , inverse , and pseudoinverse of the matrix @xmath15 , respectively .",
    "@xmath16 is an @xmath17 matrix operator norm and gives the maximum @xmath18-norm of the row vectors of @xmath15 .",
    "we denote a subset of the column indices of @xmath19 by @xmath20 , and the subdictionary that is composed of atoms associated with indices in @xmath21 by @xmath22 .",
    "note that all of the theorems are discussed on a subset @xmath21 of the column indices as tropp  @xcite did . for any coefficient vector @xmath4 defined in  ,",
    "we denote the vector composed of the elements of @xmath8 indexed by @xmath21 by @xmath23 .",
    "we also denote the whole column index set @xmath24 , and the complement of @xmath21 by @xmath25 where @xmath26 is the difference of two sets .",
    "the rest of this paper is organized as follows .",
    "section  [ sec : previouswork ] introduces a sufficient mrc shown in our previous work  @xcite .",
    "section  [ sec : nlasso ] demonstrates several mrcs for nlasso of varying tightness .",
    "section  [ sec : application ] illustrates an application of our mrcs to a hyperspectral unmixing task and section  [ sec : conclusion ] concludes this paper .",
    "we start our discussion with our previous work  @xcite , which introduced a sufficient mrc to guarantee correct model recovery using nlasso .",
    "the previous work stands on the work by tropp  @xcite and depends on the exact recovery coefficient ( erc ) , defined by @xmath27 note that it is implicitly assumed that the columns of @xmath22 are linearly independent so that the pseudoinverse exists . broadly speaking",
    ", the coefficient evaluates how far the atoms outside of @xmath21 are from the convex hull determined by the atoms in @xmath21 and their antipodes .",
    "intuitively , a larger @xmath28 is preferred because it reduces correlation between @xmath29 and atoms outside the set .",
    "the following theorem provides performance guarantees for the lasso that are specific to a particular support @xmath21 .",
    "tropp considers the general lasso    l    ll & -_2 ^ 2 + _ 1 +    [ cvx : pl - lasso ]    and derived a theorem to guarantee the performance of this problem :    ( * ? ? ?",
    "* theorem 8) let @xmath21 index a linearly independent collection of columns of @xmath30 for which @xmath31 .",
    "suppose that @xmath32 is an input signal whose @xmath33 best approximation @xmath34 over @xmath35 satisfies the correlation condition @xmath36 let @xmath37 be the solution of the lasso   with parameter @xmath7 .",
    "we may conclude the following .",
    "* @xmath38 , is contained in @xmath21 ; * the distance between @xmath37 and the optimal coefficient vector @xmath39 ( appropriately zero - padded ) satisfies @xmath40 * and @xmath38 contains the indices @xmath41 for which @xmath42    [ thm : tropp_mrc_lasso ]    using theorem  [ thm : tropp_mrc_lasso ] , we can derive the following theorem to guarantee the performance of nlasso :    ( * ? ? ?",
    "* theorem 2)[thm : smrc_nlasso_erc - based ] assume a signal model @xmath43 , where the abundance vector @xmath44 , @xmath45 indexes a linearly independent collection of columns of @xmath46 , and @xmath47 represents the effect of noise or nonlinear distortion during acquisition .",
    "let @xmath48 be the solution of nlasso with parameter @xmath7 .",
    "if the noise @xmath47 obeys    c ^_^ _ ( ) [ eq : smrc_nlasso_erc - based : nscc ]    where @xmath49 is the projector onto the orthogonal complement of @xmath50 , and    c _ ^ ( ^__)^-1 _ , -_^ , [ eq : smrc_nlasso_erc - based : mcc ]    then we have that @xmath51 .",
    "we begin by considering the solution @xmath37 to the lasso with parameter @xmath7 for the input @xmath32 . by applying theorem  [ thm : tropp_mrc_lasso ] and seeing that    r , c , l ^(- _ ) _ & = & ^(- _ ) _ + & = & ^(^+-_(^+ ) ) _ + & = & ^(__^+-_(__^+ ) ) _ + & = & ^(__^+-__^-__^ ) _ + & = & ^(-__^)_= ^(-__^ ) _ + & = & ^_^ _ , [ eq : proofnoise ]    r , c , l & & ^(- _ ) _ + & = & ^(- _ ) _ + & = & ^(^+-_(^+ ) ) _ + & = & ^(__^+-_(__^+ ) ) _ + & = & ^(__^+-__^-__^ ) _ + & = & ^(-__^)_= ^(-__^ ) _ + & = & ^_^ _ , [ eq : proofnoise ]    we have that and imply .",
    "thus , we obtain the following results :    * the support of @xmath37 , @xmath38 , is contained in @xmath21 , and * the distance between @xmath37 and the optimal coefficient vector + r , c , l _ & = & _ ^ = _",
    "^(^+ ) = _ ^(__^+ ) + & = & _ ^+_^ + ( appropriately zero - padded ) satisfies @xmath52    the result implies that for each @xmath53 we have    r , c , l |^(n)-(^(n)+(n))| & & ( ^__)^-1 _ , , + -(^__)^-1 _ , & & ^(n)-^(n)-(n ) , + ^(n)+(n)-(^__)^-1 _ , & & ^(n ) ,    c |^(n)-(^(n)+(n))| ( ^__)^-1 _ , , + -(^__)^-1 _ , ^(n)-^(n)-(n ) , + ^(n)+(n)-(^__)^-1_,^(n ) ,    where we denote @xmath54 .",
    "thus , from the condition , we have that @xmath55 for all @xmath53 , which implies that @xmath56 .",
    "furthermore , since @xmath57 , then we have that @xmath58 and so it follows that @xmath59 , i.e. , the solution of the lasso is non - negative .",
    "this implies that the solution of nlasso for the same input and parameter value obeys @xmath60 ( i.e. , the solution of nlasso matches the solution of the unconstrained lasso ) , and so @xmath61 .",
    "the sufficient condition is composed of two inequalities ; the first one   explains how much deviation from linearity is allowed , and the second one   shows the minimum value of the coefficient to be detected .",
    "this condition is a demanding sufficient mrc , as shown in  @xcite .",
    "more specifically , there are still many observations on which nlasso is successful , but for which the condition is not met .",
    "we note that nlasso is equivalent to    l[cvx : nlasso2 ]    ll & -_2 ^ 2 + ^_n + & ,    where @xmath62 is the @xmath63 length vector with all elements being one .",
    "this minimization problem becomes nnls when @xmath64 .",
    "first , we provide mrcs for which the subset of atoms @xmath21 contains the support of minimizers of nlasso",
    ". in particular , we will give a condition for which a solution to the restricted nlasso    l    ll & -___2 ^ 2 + ^_j _ + & _ ,    [ cvx : nlasso_lambda ]    also becomes a solution to the original nlasso  .",
    "the condition is given by the following theorem .",
    "[ thm : apmrc_nlasso_base ] let @xmath21 be a subset of column indices of the dictionary matrix @xmath46 such that @xmath65 .",
    "if the inequalities    cl ( -__)^_j < & j ^[eq : apmrc_nlasso_base ]    hold for all solutions @xmath66 of the restricted nlasso   over the column subset @xmath21 , then all solutions to the general nlasso   have their supports contained in @xmath21 .    a proof of this theorem is found in the appendix .",
    "this theorem states a condition for which the restricted nlasso   yields a global solution of the original problem  .",
    "although this condition requires knowledge of the solutions of the restricted nlasso , the theorem considers quite general cases :    1 .",
    "the subdictionary @xmath22 can have linearly dependent columns , 2 .",
    "the restricted nlasso over columns in @xmath21 can have multiple minimizers , 3 .",
    "the columns of @xmath46 are not restricted to be normalized .",
    "thus , the theorem serves as a fundamental result to derive other practical mrcs in subsequent sections .",
    "when atoms associated with indices in @xmath21 are linearly independent to each other , we can further assume the uniqueness of the solution because the restricted problem has the unique solution .",
    "the condition   is a sufficient but not necessary condition for the event @xmath67 .",
    "however , is quite close to a necessary condition , as shown in the following theorem .",
    "[ thm : apmrc_nlasso_base_eq ] under the assumption of theorem  [ thm : apmrc_nlasso_base ] , if the support @xmath68 of each solution @xmath48 to the general nlasso   is contained in @xmath21 , then the following condition holds for all solutions @xmath66 of the restricted nlasso   over the column subset @xmath21 :    cl ( -__)^_j & j ^.[eq : apmrc_nlasso_base_eq ]    the proof of this theorem is found in the appendix .",
    "this theorem indicates that the condition   is a necessary condition for @xmath67 .",
    "hence , a necessary and sufficient condition for the event @xmath67 lies somewhere between and .",
    "more specifically , equalities need to be added to only for some indices @xmath69 to obtain a necessary and sufficient condition .",
    "nonetheless , it is worth noting that the cases in which holds with equality will be rare in practice .",
    "therefore , the conditions and are practically identical , implying that is a practically valid necessary and sufficient condition for the event @xmath67 .",
    "just like nlasso becomes nnls when @xmath64 , a restricted nnls problem is given by the minimization problem   when @xmath64 .",
    "as a special case of theorem  [ thm : apmrc_nlasso_base ] , we can define an optimal condition for nnls specific to an index set @xmath21 :    [ thm : apmrc_nnls_base ] let @xmath21 be a subset of column indices of the dictionary matrix @xmath46 such that @xmath65 . if the inequalities    cl ( -__)^_j < 0 & j ^[eq : apmrc_nnls_base ]",
    "hold for the solution @xmath66 of the restricted nnls problem over the column subset @xmath21 , then all solutions to the general nnls have their supports contained in @xmath21 .",
    "we note in passing that the set of inequalities is identical to one of the stopping criteria introduced for non - negative orthogonal matching pursuit  @xcite , an alternative algorithm for non - negative sparse signal recovery .      .",
    "]    this section provides mrcs for which the subset of atoms @xmath21 exactly matches the support of the minimizers of nlasso .",
    "we again assume that the atoms associated with indices in @xmath21 are linearly independent .",
    "first , we define a metric that we call _ positive subset coherence _ ( psc ) :    c ( ; j ) : = 1-_j^_^ _ j.    the @xmath70 measures how positively aligned the @xmath71 atom @xmath2 in the library is to the convex cone determined by the columns of @xmath22 . the index @xmath69 is usually selected from outside @xmath21 . figure  [ fig : psc ] illustrates a geometric interpretation of @xmath70 focusing on when the sign of @xmath70 changes .",
    "the @xmath70 becomes positive when the orthogonal projection of @xmath2 onto the subspace spanned by the column vectors of @xmath22 falls on the same side of the hyperplane passing through the column vectors of @xmath22 as the origin ; negative when the origin and the column @xmath2 are on opposite sides of the aforementioned hyperplane ; and zero when @xmath2 is contained in this hyperplane .",
    "this @xmath70 plays an important role in the next approximately perfect mrc .",
    "[ thm : apmrc_nlasso ] let @xmath21 be a subset of the column indices of the dictionary matrix @xmath46 such that @xmath72 and the atoms associated with indices in @xmath21 are linearly independent .",
    "let @xmath48 be a solution to nlasso .",
    "the support of @xmath48 , @xmath68 , is equal to @xmath21 if the following two conditions hold :    l , l 1 ) & + & _ ^ ( ^__)^-1_j[eq : apmrc_nlasso : mcc ] + 2 ) & + & ^^__j < ( ; j ) j ^.[eq : apmrc_nlasso : nscc ]    -l , l 1 ) & + & _ ^ ( ^__)^-1_j[eq : apmrc_nlasso : mcc ] + 2 ) & + & ^^__j < ( ; j ) j ^.[eq : apmrc_nlasso : nscc ]    furthermore , the minimizer @xmath48 is equal to the appropriate zero - padding of the solution to the restricted nlasso    c _ = _ ^ - ( ^__)^-1_j .",
    "the proof of this theorem is found in  section  [ sec : proof : pmrc_nlasso_specific ] .",
    "the mcc measures whether the entries of the least squares solution @xmath73 are sufficiently large .",
    "the nscc specifies the degree of nonlinear distortion that can be tolerated with respect to each @xmath70 and dictionary atom @xmath2 .",
    "the left hand side of   is the inner product between @xmath2 and the orthogonal projection of @xmath32 onto the orthogonal complement of @xmath74 .",
    "the latter projection can be interpreted as nonlinear noise because it is considered as the deviation of the observation @xmath32 from the span of @xmath22 .",
    "figure  [ fig : nscc ] shows a geometric interpretation of the nscc . as explained , the right hand side of   quantifies the alignment of @xmath2 with the convex cone obtained from @xmath22 . because @xmath7 is usually positive , a larger @xmath70 relaxes the upper bound of @xmath75 .",
    "thus , it is preferable for @xmath2 to be less aligned to the columns of @xmath22 .",
    "the inequality   needs to strictly hold for mathematical rigor because of the definition of the support ( the set of indices that have non - zero entries ) .",
    "if we instead allowed for equality in  , we would not be able to guarantee that @xmath21 exactly matches the support of @xmath48 since some of the entries in @xmath76 might be zero - valued , cf .",
    "( [ eq : sol_nlasso_specific ] ) , although the solution @xmath48 would be still expressed in the same way , i.e. , as the appropriate zero - padding of the solution @xmath76 to the restricted nlasso .",
    "nonetheless , equality can be added in practice since the event for which the equations hold with equality is quite rare , as described in the discussion of section  [ sec : fund_nlasso ] .",
    "this discussion is also true for the inequality  ; equality could be added to the inequality in practical settings .    ]",
    "interestingly , conditions and in theorem  [ thm : smrc_nlasso_erc - based ] are similar to the nscc and mcc , but the former are not specific to particular indices @xmath69 .",
    "this structure of the condition is shared with the simplified sufficient conditions derived in section  [ sec : smrcs ] .",
    "theorem  [ thm : apmrc_nlasso ] can be specialized to specific noise models . in the case of random noise ( e.g. , gaussian or subgaussian ) ,",
    "it is possible to obtain the likelihood of the nscc being met in terms of the noise variance ; such a result matches  ( * ? ? ?",
    "* theorem 6 ) . under a linear noise model ( e.g.",
    ", the distortion corresponds to a linear combination of the atoms in lies in @xmath22 ) , the left hand side of always becomes equal to zero , and it suffices to require a non - negative @xmath70 for each of the atoms indexed in @xmath77 . in all other cases ,",
    "the nscc allows us to distinguish between tolerable and intolerable distortions .",
    "if the nonlinear distortion ( i.e. , the portion of the distortion in the space orthogonal to @xmath74 ) forms obtuse angles with the atoms indexed by @xmath77 , the nscc will be satisfied regardless of the magnitude of the nonlinearity .",
    "conversely , for atoms for which these angles are acute , the value of the @xmath70 for the specific atom will dictate the tolerable magnitude of the projection of the nonlinear distortion onto the atom .",
    "we note that meeting the mcc and nscc will also depend on the value of the trade - off parameter @xmath7 . intuitively , the chance for false alarm increases as @xmath7 decreases ( promoting denser solutions ) and missed detection is more likely to occur as @xmath7 increases ( promoting sparser solutions ) .",
    "the mcc provides an upper bound on @xmath7 needed to avoid missed detections , while the nscc provides a lower bound needed to prevent false alarms .",
    "the bounds will depend on the observation @xmath32 and support @xmath21 , which indicates that the performance of nlasso can be improved by adaptively optimizing @xmath7 .",
    "it is also easy to see that one can formulate configurations @xmath78 for which no value of @xmath7 meets both the mcc and nscc .",
    "theorem  [ thm : apmrc_nlasso ] can also be specialized to nnls ( e.g. , @xmath79 ) , providing the following corollary .",
    "[ thm : apmrc_nnls ] let @xmath21 be a subset of column indices of the dictionary matrix @xmath46 such that @xmath72 and the atoms associated with indices in @xmath21 are linearly independent .",
    "let @xmath48 be the solution of nnls .",
    "the support of @xmath48 , @xmath68 , is equal to @xmath21 if @xmath80 and @xmath81 .    in this case",
    ", the mcc requires for the restricted least squares solution to be non - negative , while the nscc requires all angles between the atoms indexed in @xmath77 and the nonlinear distortion in the orthogonal space of @xmath74 to be obtuse .",
    "recall that the atoms associated with the index set @xmath21 are linearly independent to each other and the two conditions   and hold .",
    "first let us consider the restricted nlasso defined by  .",
    "the problem   has the unique minimizer because the objective function is strictly convex and the domain is a convex region .",
    "the lagrangian of   is given by    c l ( _ , ) = -___2 ^ 2 + _",
    "j^_- ^ _ ,    where @xmath82 is a lagrange multiplier with @xmath83 .",
    "according to  ( * ? ? ? * theorem 28.3 , p.  281 ) , @xmath84 and @xmath85 are optimal if and only if they satisfy the kkt conditions    -sl 1 ) & _ , ( n)_(n ) = 0 n=1,  ,j + 2 ) & = l ( _ , ) / _ |__=_.    those conditions are true for @xmath86 and @xmath87 . taking the uniqueness of the solution into consideration",
    ", we can conclude that the unique minimizer of   is given by    c _ = _ ^ - ( ^__)^-1_j .[eq : sol_nlasso_specific ]    by manipulating the inequality   and substituting  , we have    r , r , c , l & ( - _ ^_)^_j & < & ( 1-^_j_^_j ) + & ( - _ ^_+_j)^_j & < & + & \\ { - _ ( ^_-_j)}^_j & < & + & ( - _ _ ) ^_j & < & ( )    for all @xmath88 . directly applying theorem  [ thm : apmrc_nlasso_base ] , we can assert that @xmath21 contains the support of @xmath48 .",
    "furthermore , since all the elements of the minimizer   are greater than zero , @xmath21 is the support of @xmath48 .",
    "this completes the proof .      although these conditions are quite simple , they are still more elaborate than those provided by tropp  @xcite for general lasso",
    ". we will further simplify and introduce two sufficient conditions in this section . to start with ,",
    "we define the positive exact recovery coefficient ( @xmath89 ) by    c ( ) : = _ j^ ( ; j ) .",
    "we call this @xmath89 because this is considered as the positive counterpart of the @xmath28 , and @xmath89 is interpreted as the minimum of the right hand side in the nscc  . note that @xmath90 is equivalent to non - negative irrepresentable constant of  @xcite",
    ". we can also take the maximum on the left hand side of the inequality   after concatenating all @xmath2 , and then a modified sufficient condition for the multiple nsccs is written as    c _ j ^ _ j^^ _ < ( ) [ eq : nscc_perc - max ]",
    ".    we refer to this condition   as perc - max .",
    "using this , we can derive the next corollary .",
    "[ cor : smrc_nlasso_perc - max ] under the assumptions of theorem  [ thm : apmrc_nlasso ] , the support of @xmath48 , @xmath68 , is equal to @xmath21 if the two conditions , mcc   and perc - max  , hold .",
    "furthermore , the minimizer @xmath48 is also given as in theorem  [ thm : apmrc_nlasso ] .",
    "we can still introduce another nscc condition that is more strict than corollary  [ cor : smrc_nlasso_perc - max ] but more relaxed than theorem  [ thm : smrc_nlasso_erc - based ] by taking the absolute maximum value on the left side of  ,    c   ^^ _ _ 0 + ( ) _ 2 ^ 2 + _",
    "j x_j(-_j^(- _ _ ) ) > 0 , [ eq : proof : pmrc_nlasso_lambda : simppert ]    , l ( ) _ 2 ^ 2 + ^_n ( ) - ( - _ _ ) ^ ( ) > 0 + ( ) _ 2 ^ 2 + _",
    "j x_j(-_j^(- _ _ ) ) > 0 , [ eq : proof : pmrc_nlasso_lambda : simppert ]    where @xmath24 is the whole column index set as defined in section  [ sec : mathnotation ] .",
    "next , we explore a property of @xmath84 .",
    "the lagrangian of the restricted nlasso above is given by    c l _ ( _ , _ ) = -___2 ^ 2 + ^_j_- _ ^ _ ,    where @xmath91 is a vector of lagrangian multipliers . from the kkt condition in theorem 28.3  @xcite , @xmath92 and @xmath93",
    "become a minimizer and a kuhn - tucker vector , respectively , if and only if the following three conditions hold :    sl 1 ) & _ , _ [ eq : kktcnd1_pl - nlass_lambda ] + 2 ) & _ ( n)_(n ) = 0 n [ eq : kktcnd2_nlasso_lambda ] + 3 ) & = l _ ( _ , _ ) / _ |__=_[eq : kktcnd3_nlasso_lambda ] .",
    "-sl 1 ) & _ , _ [ eq : kktcnd1_pl - nlass_lambda ] + 2 ) & _ ( n)_(n ) = 0",
    "n [ eq : kktcnd2_nlasso_lambda ] + 3 ) & = l _ ( _ , _ ) / _ |__=_[eq : kktcnd3_nlasso_lambda ] .",
    "the third kkt condition   is equivalently expressed as :    rl & ^_(__- ) + _ j - _ = + & -^_j ( - _ _ ) = _ ( j ) j    for @xmath94 , we have @xmath95 , leading to @xmath96 because of the second kkt condition  . for @xmath97 , we have @xmath98 , leading to @xmath99 .",
    "thus we have    c    -^_j ( - _ _ ) = 0 j + -^_j ( - _ _ ) 0 j .",
    "[ eq : proof : pmrc_nlasso_lambda : kkt3derivativesimple ]    considering the conditions above for @xmath84 , the inequality   is equivalently transformed into    , l ( ) _ 2 ^ 2 + _",
    "j x_j(-_j^(- _ _ ) ) + _ j ^x_j(-_j^(- _ _ ) ) > 0 , [ eq : proof : apmrc_nlasso_base : simppert2 ]    , l ( ) _ 2 ^ 2 + _ j x_j(-_j^(- _ _ ) )   + + _ j ^x_j(-_j^(- _ _ ) ) > 0 , [ eq : proof : apmrc_nlasso_base : simppert2 ]    where the second term is always non - negative because of the non - negativity of the two factors ( and ) .",
    "summarizing this discussion , @xmath100 if and only if the inequality   holds for all @xmath101 in the defined region .",
    "we now prove the statement   for theorem  [ thm : apmrc_nlasso_base ] . given",
    "the condition   is true , then the summation of the third term on the left side in   becomes always non - negative .",
    "furthermore , because we are considering @xmath101 with a non - zero @xmath71 element for any @xmath88 , the third term is always strictly positive",
    ". therefore , holds for every @xmath101 in the defined region  ( [ eq : proof : pmrc_nlasso_lambda : pert2 ] ) . because that condition is equivalent to @xmath102 , the statement   is proven .",
    "next , we prove the statement   for theorem  [ thm : apmrc_nlasso_base_eq ] .",
    "we prove this by the principle of contradiction .",
    "assume the inequality   is true for every @xmath101 in the defined region and every solution @xmath84 .",
    "suppose there exists a solution @xmath84 and an index @xmath103 such that    c ( -__)^_j > ,    which is the opposite of ( [ eq : apmrc_nlasso_base_eq ] ) .",
    "the inequality   is true for @xmath104 such that only the @xmath105 element is greater than zero and the others are zero .",
    "let such a @xmath104 be    c  = [ 0,  0,x_j,0 ,  0]^ ( x_j>0 ) .",
    "the inequality   then becomes    c _",
    "j_2 ^ 2(x_j)^2 - ( ( -__)^_j-)x_j > 0 .",
    "[ eq : aux ]    -c _",
    "j_2 ^ 2(x_j)^2 - ( ( -__)^_j-)x_j > 0 .",
    "[ eq : aux ]    the left hand side is a quadratic function with regard to a scalar variable @xmath106 . by defining the quadratic equation s coefficients as    r , c , l b_j & = & _ j_2 ^ 2 > 0 + c_j & = & ( -__)^_j- > 0",
    ",    the quadratic inequality ( [ eq : aux ] ) becomes    c x_j(b_jx_j^ - c_j ) > 0 .",
    "because both the coefficients @xmath107 and @xmath108 are positive , the left hand side becomes negative for sufficiently small @xmath106 such that @xmath109 .",
    "since @xmath106 can take any positive value , we can say that there exists a @xmath106 that breaks the inequality  .",
    "this contradicts to our starting assumption .",
    "thus , by the principle of contradiction , the statement   is proven .",
    "we thank rose kontak for providing us with the oil painting , and dr .",
    "bioucas - dias and dr .",
    "figueiredo for making matlab code for sunsal available online .",
    "y.  itoh , m.  f. duarte , and m.  parente , `` performance guarantees for sparse regression - based unmixing , '' in _ ieee grss workshop hyperspectral image signal process .",
    "evolution remote sens .",
    "( whispers ) _ , tokyo , japan , june 2015 .",
    "d.  heinz and c .- i .",
    "chang , `` fully constrained least squares linear spectral mixture analysis method for material quantification in hyperspectral imagery , '' _ ieee trans .",
    "remote sens .",
    "_ , vol .  39 , no .  3 , pp . 529545 , mar",
    ". 2001 .",
    "r.  he , w .- s .",
    "zheng , b .-",
    "hu , and x .- w .",
    "kong , `` two - stage nonnegative sparse representation for large - scale face recognition , '' _ ieee trans .",
    "neural networks learn .",
    "_ , vol .  24 , no .  1 , pp . 3546 , jan .",
    "2013 .",
    "a.  szlam , z.  guo , and s.  osher , `` a split bregman method for non - negative sparsity penalized least squares with applications to hyperspectral demixing , '' in _ proc .",
    "conf . image process .",
    "_ , hong kong , sep .",
    "2010 , pp . 19171920 .",
    "a.  bruckstein , m.  elad , and m.  zibulevsky , `` on the uniqueness of nonnegative sparse solutions to underdetermined systems of equations , '' _ ieee trans .",
    "inf . theory _",
    "54 , no .  11 , pp .",
    "48134820 , nov .",
    "2008 .",
    "n.  dobigeon , j .- y .",
    "tourneret , c.  richard , j.  bermudez , s.  mclaughlin , and a.  hero , `` nonlinear unmixing of hyperspectral images : models and algorithms , '' _ ieee signal process . mag .",
    "_ , vol .",
    "31 , no .  1 ,",
    "8294 , jan .",
    "2014 .",
    "x.  fu , w .- k .",
    "ma , j.  bioucas - dias , and t .- h .",
    "chan , `` semiblind hyperspectral unmixing in the presence of spectral library mismatches , '' _ ieee trans . geosci .",
    "remote sens . _ , vol .",
    "54 , no .  9 , pp . 51715184 , 2016 .",
    "m.  wainwright , `` sharp thresholds for high - dimensional and noisy sparsity recovery using @xmath18-constrained quadratic programming ( lasso ) , '' _ ieee trans .",
    "inf . theory _ ,",
    "55 , no .  5 , pp .",
    "21832202 , may 2009 .                        j.  bioucas - dias and m.  figueiredo , `` alternating direction algorithms for constrained sparse regression : application to hyperspectral unmixing , '' in _ ieee grss workshop hyperspectral image signal process .",
    "evolution remote sens .",
    "( whispers ) _ , june 2010 , pp . 14 .",
    "j.  kim , n.  ramakrishnan , m.  marwah , a.  shah , and h.  park , `` regularization paths for sparse nonnegative least squares problems with applications to life cycle assessment tree discovery , '' in _",
    "ieee 13th int .",
    "2013 , pp . 360369 .",
    "kim , k.  koh , m.  lustig , s.  boyd , and d.  gorinevsky , `` an interior - point method for large - scale @xmath18-regularized least squares , '' _ ieee j. sel .",
    "top . signal process .",
    "_ , vol .  1 , no .  4 , pp",
    "606617 , dec . 2007 .          j.  bioucas - dias , a.  plaza , n.  dobigeon , m.  parente , q.  du , p.  gader , and j.  chanussot , `` hyperspectral unmixing overview : geometrical , statistical , and sparse regression - based approaches , '' _",
    "ieee j. sel .",
    "top . appl .",
    "earth obs .",
    "remote sens .",
    "_ , vol .  5 , no .  2 ,",
    "pp . 354379 , apr ."
  ],
  "abstract_text": [
    "<S> sparse modeling has been widely and successfully used in many applications such as computer vision , machine learning , and pattern recognition . accompanied with those applications , </S>",
    "<S> significant research has studied the theoretical limits and algorithm design for convex relaxations in sparse modeling . however , theoretical analyses on non - negative versions of sparse modeling are limited in the literature either to a noiseless setting or a scenario with a specific statistical noise model such as gaussian noise . </S>",
    "<S> this paper studies the performance of non - negative sparse modeling in a more general scenario where the observed signals have an unknown arbitrary distortion , especially focusing on non - negativity constrained and l1-penalized least squares , and gives an exact bound for which this problem can recover the correct signal elements . </S>",
    "<S> we pose two conditions to guarantee the correct signal recovery : minimum coefficient condition ( mcc ) and nonlinearity vs.  subset coherence condition ( nscc ) . </S>",
    "<S> the former defines the minimum weight for each of the correct atoms present in the signal and the latter defines the tolerable deviation from the linear model relative to the positive subset coherence ( psc ) , a novel type of `` coherence '' metric . </S>",
    "<S> we provide rigorous performance guarantees based on these conditions and experimentally verify their precise predictive power in a hyperspectral data unmixing application .    sparse modeling , sparse regression , sparse recovery , non - negative constraint , lasso , recovery conditions , hyperspectral unmixing . </S>"
  ]
}