{
  "article_text": [
    "among different aspects of software defect prediction process , one of the key elements is proper selection of metrics for training and verification dataset preparation .",
    "most popular data is source code metrics  @xcite , but also different types of metrics are considered effective in term of defect prediction , such as design metrics  @xcite , change metrics  @xcite , mining metrics  @xcite or process metrics  @xcite .",
    "separate group of design metrics are metrics based on code smells , also known as bad smells or code bad smells .",
    "the term was formulated by kent beck in 2006  @xcite .",
    "the concept was popularized by martin fowler in his book _ refactoring . improving the structure of existing code",
    "_  @xcite .",
    "kent beck was a co - author of the chapter on code smells .",
    "kent beck on his website explains the idea of code smells :    note that a code smell is a hint that something might be wrong , not a certainty .",
    "a perfectly good idiom may be considered a code smell because it s often misused , or because there s a simpler alternative that works in most cases .",
    "calling something a code smell is not an attack ; it s simply a sign that a closer look is warranted .",
    "@xcite    due to nature of code smells described above , there is ongoing discussion if code smells could be used effectively in quality assurance in code development  @xcite .",
    "major motivation for this research was to investigate , if code smells can improve software defect prediction .    in industrial software development ,",
    "only holschuh et al . investigated code smells metrics effectiveness in defect prediction process for java programming language  @xcite .",
    "no code smells metrics for defect prediction in .net oriented industrial software projects are known to authors .",
    "thus , we decided use long - term defect prediction research project run in volvo group  @xcite as an occasion for conducting an experiment with introduction of bad smells based metrics to prediction process and observe the results , if they improved prediction effectiveness or not :    * rq : * how code bad smells based metrics impact defect prediction in industrial software development project ?      project , on which the study was conducted , is a software development of critical industry system used in volvo group vehicle factories called prosit+ .",
    "it is created based on client - server architecture .",
    "the main functionality of prosit+ system is : programming , testing , calibration and electrical assembly verification of electronic control units ( ecus ) in volvo s vehicle production process .",
    "prosit+ system consists of few coexisting applications . the most important one , desktop application ",
    "`` prosit operator '' , communicates in real time with a mobile application , located on palmtop computer used by vehicle factory workers to transfer all production related information to a local server .",
    "the server is responsible for storage and distribution of configuration- , system- and product - related data .",
    "such communication can generate extremely heavy data transfer loads in large factories , when more than 100 mobile applications are used . other application include : `` prosit designer '' , `` prosit factory manager '' and web application `` prosit viewer '' .",
    "all of them are also connected to the same server .",
    "development of each prosit+ version lasts one year .",
    "after this period software is released to the end - user . as this period of time",
    "is connected to factory production cycle it can not be fastened or postponed .    all applications within prosit+ system were developed using microsoft .net technology and microsoft visual studio as the integrated development environment . for version control purposes ,",
    "microsoft team foundation server was used . before release of version 11 of the prosit+ system ,",
    "ibm clearquest was used for software defect management . until the development of version 11",
    ", team foundation server was used for defect tracking .",
    "project lacks of bottlenecks described by hryszko and madeyski@xcite , which could hinder or prevent from applying defect prediction process .",
    "however , we observed relatively high number of naming issues in the project .",
    "main reason of that situation we consider high maturity of the software system  over the time , naming conventions have changed .",
    "we consider naming issues as negligible problem and we will exclude them from the further investigation .",
    "defect prediction was already an ongoing process in investigated project .",
    "it used sourcemonitor software as metric source and as prediction tool  knime - based depress extensible framework proposed by madeyski and majchrzak  @xcite .",
    "this tool , based on knime  @xcite , provides with a wide range of data - mining techniques , including defects prediction , in various it projects , independently of technology and programming language used .",
    "we will also use knime / depress for purpose of our research .    to investigate the possible impact of code - smell metrics on defect prediction , we developed the following plan to follow :    1 .",
    "generate metrics from sourcemonitor ; 2 .   generate code smells metrics from codeanalysis ; 3 .",
    "parse results from codeanalysis and merge them with metrics from sourcemonitor .",
    "link check - ins to defects ; 5 .",
    "link classes from check - ins to defects ( the assumption is that if a class was changed while fixing a defect , that class was partially or fully responsible for that defect ) ; 6 .   merge list of classes with merged metrics from codeanalysis and sourcemonitor ; 7 .   use different software defect prediction approaches combinations to select optimal prediction set - up for evaluation purposes ; 8 .   divide prosit+ code into 20 sub - modules and",
    "run prediction model training and evaluation using data from each module separately ; 9 .",
    "collect and interpret the results .",
    "defect prediction process in prosit+ is based on metrics that are gathered using sourcemonitor tool  @xcite .",
    "that tool performs static computer code analysis on complete files and extracts 24 different kinds of metrics .",
    "example metrics extracted are :    * lines of code , * methods per class , * percentage of comments , * maximum block depth , * average block depth .      in our experiment",
    ", we decided to use microsoft codeanalysis tool to gather code smells metrics .",
    "primary deciding factor was cost : codeanalysis tool is delivered as a part of microsoft visual studio software development suite for .net based projects . thus , there was no additional costs of introduction of this tool into the investigated software development project .",
    "codeanalysis for managed code analyzes managed assemblies and reports information about the assemblies , such as violations of the programming and design rules set forth in the microsoft .net framework design guidelines  @xcite .    according to documentation ,",
    "there are approximately two hundred rules in codeanalysis  @xcite , trigerring 11 kinds of warnings ( table  [ table : warnings ] ) .",
    "tool can be run from command line and results are then stored in an .xml file , that can be later parsed and analyzed further .",
    ".bad smell warnings in codeanalysis [ cols=\"<,<\",options=\"header \" , ]        & recall & 0.9608 & 0.0278 + & f - measure & 0.9433 & 0.0188 + & recall & 0.666 & 0.2961 + & f - measure & 0.6447 & 0.1157 + & recall & 0.9637 & 0.0303 + & f - measure & 0.9494 & 0.0228 + & recall & 0.9824 & 0.0146 + & f - measure & 0.9704 & 0.012 + & recall & 0.8424 & 0.0542 + & f - measure & 0.8286 & 0.0559 + & recall & 0.9859 & 0.0206 + & f - measure & 0.9792 & 0.0136 +      _ conclusion validity .",
    "_ in our research , we tested 20 datasets collected from different software modules .",
    "more research using larger data set , collected from different sources is needed to confirm our findings .",
    "_ internal validity .",
    "_ we have used aggregation of codeanalysis metrics for each file , by adding metrics collected for each class . such solution was introduced to solve metrics breakdown difference problem and make combination of two metric sources possible , however it could impact the final result of our research .    _ external validity . _",
    "our research is based only on metrics gathered from one software development project . despite the fact ,",
    "that we were able to collect 34 different metric kinds for 20 different program modules , we were still constrained by single environment : development team and its programming habits , programming language , tools used , etc . because of this fact , more research is needed to verify our findings in other software development environments ( contexts ) .",
    "when selecting optimal defect prediction set - up for further verification if code smell - based metrics can improve prediction results , we observed that best result was achieved for dataset with bad smell metrics included ( f - measure = 0.9713 ) .",
    "however , for the same setup , but without code smells metrics , f - measure value was only by 0.0059 lower ( table  [ table : results ] ) what makes the difference between sourcemonitor and codeanalysis results negligible .",
    "final results collected from 20 different software sub - modules confirmed that statement : average accuracy value for prediction based on dataset constructed basing on both sources was only by 0.0091 better than result for sourcemonitor - only based metrics ( average f - measure value difference = 0.0088 ) , while standard deviation value was 0.0136 .",
    "worth noticing is drop of codeanalysis - only based prediction results , when feature selection ( fs ) process was removed from the experimental setup .",
    "results of our experiment of using code smells metrics in software defect prediction , show irrelevant  in our opinion  impact on effectiveness of the process , when basic dataset ( sourcemonitor - based ) was extended by codeanalysis metrics . because even if prediction effectiveness measures are slightly higher , the stay within the limits of error .",
    "but when only use of codeanalysis - based metrics were used for prediction ( without basic set of sourcemonitor - based metrics ) , such process resulted with high accuracy ( 0.8249 ) and f - measure ( 0.8286 ) results .",
    "thus , answering the research question : _ how code bad smells based metrics impact defect prediction in industrial software development project ? _ we want to state , that in industrial environment , such as prosit+ software development project , impact of code bad smells based metrics is negligibly small , and usage of codeanalysis - based metrics should not be considered useful , due to fact that additional effort needed for introducing code smell - based metrics to software defect prediction process is not compensated by relatively high increase of prediction effectiveness .    however , we observed surprisingly high effectiveness of prediction , when dataset based on codeanalysis only was used .",
    "authors believe , that code bad smells can be effectively used for defect prediction process especially there , where other metrics are not available , or computing power is insufficient to handle large sets of different metrics ( for example 24 kinds of metrics for sourcemonitor ) , while codeanalysis metrics set , used in our research , contained only 11 different kinds of metrics . due these promising results , aspects of using code",
    "bad smells only based metrics in defect prediction processes should be investigated further .",
    "hall , t. , beecham , s. , bowes , d. , gray , d. , counsell , s. : a systematic literature review on fault prediction performance in software engineering .",
    "ieee transactions on software engineering 38(6 ) , 12761304 ( 2012 )    holschuh , t. , pauser , m. , herzig , k. , zimmermann , t. , premraj , r. , zeller , a. : predicting defects in sap java code : an experience report . in : icse - companion 2009 .",
    "31st international conference on software engineering .",
    "172181 ( 2009 )    hryszko , j. , madeyski , l. : bottlenecks in software defect prediction implementation in industrial projects .",
    "foundations and computing and decision sciences 40(1 ) , 1733 ( 2015 ) , http://dx.doi.org/10.1515/fcds-2015-0002    hryszko , j. , madeyski , l. : assessment of the software defect prediction cost effectiveness in an industrial project . in : software engineering : challenges and solutions , advances in intelligent systems and computing , vol .",
    "springer ( 2017 )                    madeyski , l. , jureczko , m. : which process metrics can significantly improve defect prediction models ?",
    "an empirical study .",
    "software quality journal 23(3 ) , 393422 ( 2015 ) , http://dx.doi.org/10.1007/s11219-014-9241-7        moser , r. , pedrycz , w. , succi , g. : a comparative analysis of the efficiency of change metrics and static code attributes for defect prediction . in : software engineering , 2008 .",
    "acm / ieee 30th international conference on .",
    "181190 ( 2008 )        succi , g. , pedrycz , w. , stefanovic , m. , miller , j. : practical assessment of the models for identification of defect - prone classes in object - oriented commercial systems using design metrics .",
    "journal of systems and software 65(1 ) , 112 ( 2003 )"
  ],
  "abstract_text": [
    "<S> _ background . </S>",
    "<S> _ defect prediction in software can be highly beneficial for development projects , when prediction is highly effective and defect - prone areas are predicted correctly . </S>",
    "<S> one of the key elements to gain effective software defect prediction is proper selection of metrics used for dataset preparation .    </S>",
    "<S> _ objective . </S>",
    "<S> _ the purpose of this research is to verify , whether code smells metrics , collected using microsoft codeanalysis tool , added to basic metric set , can improve defect prediction in industrial software development project .    _ results . </S>",
    "<S> _ we verified , if dataset extension by the code smells sourced metrics , change the effectiveness of the defect prediction by comparing prediction results for datasets with and without code smells - oriented metrics . in a result , we observed only small improvement of effectiveness of defect prediction when dataset extended with bad smells metrics was used : average accuracy value increased by 0.0091 and stayed within the margin of error . however , when only use of code smells based metrics were used for prediction ( without basic set of metrics ) , such process resulted with surprisingly high accuracy ( 0.8249 ) and f - measure ( 0.8286 ) results . </S>",
    "<S> we also elaborated data anomalies and problems we observed when two different metric sources were used to prepare one , consistent set of data .    </S>",
    "<S> _ conclusion . </S>",
    "<S> _ extending the dataset by the code smells sourced metric does not significantly improve the prediction effectiveness . </S>",
    "<S> achieved result did not compensate effort needed to collect additional metrics . </S>",
    "<S> however , we observed that defect prediction based on the code smells only is still highly effective and can be used especially where other metrics hardly be used . </S>"
  ]
}