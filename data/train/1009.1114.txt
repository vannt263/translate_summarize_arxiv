{
  "article_text": [
    "in the early eighties , the perception that the dynamics of the celebrated hopfield model of associative memory @xcite was solving an optimization problem , namely , that of finding which stored pattern is closest to the input configuration , led to the proposal of a powerful general - purpose optimization heuristic , the so - called hopfield - tank neural network @xcite .",
    "a similar situation happened in the late nineties , when kennedy @xcite pointed out that axelrod s model of culture dissemination @xcite could work as a collective problem - solving system provided that one associates the cultures of the agents ( represented by strings of integer numbers ) with the trial solutions of a given optimization problem .",
    "that proof - of - concept paper demonstrated then that social interaction is a natural computation method .",
    "in contrast with hopfield - tank neural network , the optimization heuristic based on social interaction , which henceforth we refer to as the adaptive culture heuristic ( ach ) , has not enjoyed great popularity among the physics and computer science community , perhaps because of the appearance at the same time of a related algorithm , called particle swarm optimization , which has by now become an established optimization paradigm @xcite .",
    "particle swarm optimization , however , suits best to search in space of real - valued variables , whereas ach is proper to explore configuration spaces of discrete - valued variables , which is the case of most combinatorial optimization problems that have attracted the attention of the statistical physics community @xcite . here",
    "we attempt to change this situation by showing that the performance of the ach seems to scale very favorably ( it improves exponentially fast ) with the number of agents in the system .    following axelrod s model @xcite ,",
    "the ach requires a population of @xmath5 agents placed at the sites of a square lattice of size @xmath6 with periodic boundary conditions .",
    "the agents can interact with their four nearest neighbors only .",
    "each agent is characterized by a binary string of length @xmath0 , which represents the agent s solution to the optimization problem in the ach interpretation . in axelrod s model",
    "this string , which is not necessarily binary , represents the culture of the agent .",
    "the interaction between any two neighboring agents occurs whenever the agents have different strings , regardless of their associated cost , and it is such that the string of the agent with the higher cost solution is slightly modified to become more similar to that of the more efficient partner .",
    "we recall that in axelrod s model the interaction between two neighboring agents takes place with probability proportional to the number of entries their cultural strings have in common and so agents with completely different cultures do not interact . in the case",
    "the agents are allowed to interact , the interaction results in the increase of the similarity between the cultures of the two agents , as in the ach update rule .",
    "the fact that some agents are prohibited to interact is the key ingredient for the existence of stable globally polarized states ( i.e. , culturally heterogeneous absorbing configurations ) which is the major outcome of axelrod s model @xcite . in the ach ,",
    "however , we seek homogeneous absorbing configurations associated to low cost solutions of the target optimization problem and so the homogenizing interactions are always allowed regardless of the similarity between the strings of the neighboring agents @xcite .    in order to obtain statistically reliable results on the scaling of the performance of ach with the size @xmath0 of the optimization problem and the number @xmath1 of agents in the lattice ,",
    "we focus on a specific optimization problem which involves the manipulation of binary variables only , namely , the categorization of binary input patterns by the boolean binary perceptron .",
    "this is a np - complete problem @xcite for which there is no efficient specific heuristic optimization method available @xcite and whose random version has received a considerable attention from the statistical mechanics community ( see , e.g. , @xcite ) because its phase diagram exhibits a frozen phase similar to that of the random energy model @xcite .",
    "the main result of this paper is that , given a fixed probability of success , the overall computational cost of ach to find a minimum - cost solution for the learning problem in a boolean binary perceptron scales with the sixth power of the size of the input string .",
    "of course , this finding has no implication on the celebrated @xmath7 conjecture of computer science since the @xmath8 scaling holds for typical realizations of the input - output mapping , rather than for all realizations as would be required to disprove that assertion .",
    "in addition , ach is _ not _ a deterministic algorithm which disqualifies the heuristic as a candidate to disprove the @xmath7 conjecture .",
    "the rest of this paper is organized as follows .",
    "first we introduce the target optimization problem  categorization of binary patterns by the boolean binary perceptron  on which we will measure the performance of the adaptive culture heuristic ( sect .",
    "[ sec : bbp ] ) .",
    "this heuristic is then described in great detail in sect .",
    "[ sec : ach ] and the results of its performance on the training task , measured by the probability that the heuristic finds a minimum cost solution , are presented in sect .   [",
    "sec : res ] . in this section",
    "we present also the performance of the ach in the case the agents are placed at the nodes of random symmetric graphs and argue that the square lattice connectivity @xmath9 yields the best performance . finally , in sect .",
    "[ sec : conc ] we present our concluding remarks .",
    "the boolean binary perceptron is a single - layer neural network whose weights are constrained to take on binary values only .",
    "more explicitly , the network consists of an input layer with @xmath0 binary neurons @xmath10 with each input neuron connected to the output unit @xmath11 through the weights @xmath12 .",
    "the state of the output unit is given by the equation @xmath13 where @xmath14 for @xmath15 and @xmath16 otherwise .",
    "we will restrict @xmath0 to take on odd integer values only , so we can guarantee that the argument of the sign function will never vanish .",
    "the learning task is to find a set of weights @xmath17 that emulates the input - output mapping @xmath18 for @xmath19 .",
    "if the weights were allowed to assume real values then this learning task could easily be accomplished by the perceptron learning algorithm or by the widrow - hoff rule @xcite . however , when the binary constraint is taken into account the learning task becomes an np - complete problem since it is equivalent to integer programming @xcite .",
    "assuming that @xmath7 , this means that no deterministic algorithm can find @xmath20 ( if it exists ) for any realization of the input - output mapping in a time that grows polynomially with the parameter @xmath0 .",
    "here we focus on random versions of the input - output mapping where the input entries @xmath21 are statistically independent random variables chosen as @xmath22 with equal probability . as for the output @xmath23 we consider two schemes . in the first scheme ,",
    "we choose @xmath24 at random with equal probability  so - called random output mapping . in this case , it is not possible to guarantee that there is a set of binary weights that emulates the input - output mapping perfectly .",
    "in fact , statistical mechanics studies based on the landmarking paper by gardner @xcite , show that in the limit @xmath25 there are optimal sets of weights provided that the ratio @xmath26 is less than @xmath27 @xcite .",
    "so , in this limit , we say that the input - output mapping is linearly boolean separable for @xmath28 .    however , it is convenient to consider input - output mappings which are linearly boolean separable for any choice of the parameters @xmath0 and @xmath29 .",
    "this observation motivates the second scheme to set the values of the outputs @xmath23 , which are given by @xmath30 for @xmath31 . here",
    "@xmath32 are statistically independent random variables that take on the values @xmath22 with equal probability .",
    "clearly , such input - output mapping is linearly boolean separable by construction , since the set of binary weights @xmath32 emulates it perfectly .",
    "the solution weight space of this problem was studied numerically @xcite and analytically @xcite , resulting in the conclusion that in the limit @xmath25 the only solution to the mapping is the teacher perceptron @xmath32 for @xmath33 .    from the perspective of interpreting the neural network training as an optimization problem",
    "we define the following cost function @xmath34 where @xmath35 if @xmath36 and @xmath37 otherwise .",
    "hence the cost @xmath38 yields the number of misclassified inputs and so its minimum ( optimum ) value is zero in the case of a linearly boolean separable mapping .    in this paper we will concentrate mostly on the linearly boolean separable mappings defined by eq .",
    "( [ lbsp ] ) because in this case the optimal solution is known _ a priori _ so we can evaluate the performance of the ach for relatively large problems ( @xmath39 ) , whereas in the random output mapping we are restricted to the range @xmath40 , since we need to carry out an exhaustive search over the @xmath41 possible weight configurations in order to find the minimum cost solution .",
    "however , our findings indicate that , regarding the scaling with respect to the relevant parameters of the problem , the performance of the heuristic is essentially the same regardless of whether the mapping is linearly boolean separable or not .",
    "the set of weights of a boolean binary perceptron is completely specified by a binary string of length @xmath0 . in the adaptive culture heuristic",
    ", each such string is interpreted as the culture of an agent and its cost , given by eq .",
    "( [ cost ] ) , measures the unworthiness of the culture .",
    "the idea behind the ach is that the agents should prefer to adopt more valuable cultures , i.e. , those cultures associated with low cost values @xcite . in this context , it is more convenient to refer to the strings that characterize the agents as solutions rather than cultures .",
    "as already pointed out , the agents are fixed at the sites of a square lattice of size @xmath6 with periodic boundary conditions and can interact with their four nearest neighbors only . at each time",
    "we pick an agent at random ( this is the target agent ) as well as one of its four neighbors",
    ". these two agents will interact provided that the cost ( [ cost ] ) of the solution associated to the target agent is greater or equal to the cost of the solution associated to the randomly selected neighbor .",
    "an interaction consists of selecting at random and then flipping one of the entries which distinguish the target agent from its neighbor .",
    "note that only the string of the target agent is updated , i.e. , the agent with the higher cost solution is changed to become more similar to its neighbor .",
    "this change may actually increase the cost of the solution of the target agent , due to the highly nonlinear dependence of the cost ( [ cost ] ) on the individual entries of the binary string .",
    "this procedure is repeated until the dynamics freezes in a homogeneous absorbing configuration .",
    "we can guarantee that the frozen configurations are homogeneous because we allow interactions , and so changes in the target agent , even when the two interacting agents have the same cost value .    because of the need to re - calculate the cost function after each interaction , the implementation of the ach to search for near optimal weights of the boolean binary perceptron is a very computationally demanding problem and so an extensive statistical analysis of the performance of this heuristic requires a highly optimized code .",
    "in particular , to simulate efficiently the ach for large lattices we use a procedure based on the concept of active agents ( see @xcite ) .",
    "an active agent is an agent whose solution differs from the solution of at least one of its four neighbors .",
    "clearly , only active agents can change their strings and so it is more efficient to select the target agent randomly from the list of active agents rather than from the entire lattice . in the case that the solution string of the target agent is modified by the updating rule , we need to re - examine the active / inactive status of the target agent as well as of all its neighbors so as to update the list of active agents .",
    "the dynamics is frozen when the list of active agents is empty .",
    "note that the cost of the solution string plays no role in the definition of active agents .",
    "all our results are obtained for @xmath42 so that for the linearly boolean separable case the teacher set of weights @xmath43 is the only global minimum ( zero - cost ) solution of the cost function ( [ cost ] ) , provided that @xmath0 is sufficiently large . however , what is crucial for our purposes is the knowledge that for any value of @xmath0 there is at least one solution for which the cost is zero , so that we can focus on the number of runs of the ach which results in this minimal cost , regardless of whether the actual solution found by the heuristic is the teacher solution or another degenerate zero - cost solution . in particular , for each realization of the input - output mapping we run the ach for @xmath44 random initial settings of the agents solutions and calculate the fraction of runs for which the heuristic reaches a minimum cost solution .",
    "this fraction is then averaged over a variable number , ranging from @xmath45 to @xmath46 , of realizations of the input - output mapping .    as pointed out before , most of our results are for the linearly boolean separable case since in this case we know by construction the cost of the optimum solution and so we can study the performance of the heuristic for large values of @xmath0 . at the end of this section we present some results for the random boolean mapping in the region @xmath47 since",
    "then we first need to perform an exhaustive search in the solution space to find the minimum cost .",
    "the main quantity we focus here is the mean fraction of runs for which the heuristic reached the minimum - cost solution , which can be interpreted as the probability @xmath48 that a run of the ach finds the optimum cost .",
    "this quantity is shown in fig .",
    "[ fig:1 ] for the linearly boolean separable case as function of the size @xmath0 of the problem and of the number @xmath1 of agents in the system .     for lattices with ( left to right ) @xmath49 and @xmath50 agents .",
    "the error bars are smaller than the sizes of the symbols and the lines are guides to eye .",
    "[ fig:1 ] , scaledwidth=52.0% ]    figure [ fig:1 ] reveals a most surprising aspect about the performance of the ach , namely , that for small @xmath1 , say @xmath51 , a fourfold increment on the number of agents in the system , increases the probability of finding an optimal solution by several orders of magnitude .",
    "actually , this observation holds true even for large @xmath1 , provided that @xmath0 is large enough . to quantify this observation , in fig .",
    "[ fig:2 ] we show how @xmath48 approaches @xmath52 as the number of agents @xmath1 increases for two values of the input size @xmath0 .",
    "this analysis shows that for @xmath53 , the probability @xmath54 that the heuristic fails to find the optimum cost vanishes like @xmath55 where the ( fitting ) parameter @xmath56 is inversely proportional to @xmath0 .     that a run of the ach does not find a zero - cost solution for linearly boolean separable mappings as function of @xmath57 for @xmath58 and @xmath59 .",
    "the dashed straight lines are the fittings @xmath60 .",
    "[ fig:2 ] , scaledwidth=52.0% ]     plotted in terms of the rescaled variable @xmath61 .",
    "the data for @xmath62 lie in approximately the same curve given by the scaling function @xmath63 .",
    "[ fig:3 ] , scaledwidth=52.0% ]     for @xmath64 and @xmath65 .",
    "the error bars are smaller than the sizes of the symbols .",
    "the solid straight line yields the probability that the optimal solution is chosen in a random selection , @xmath66 , whereas the dashed straight lines are the fittings @xmath67 .",
    "[ fig:4 ] , scaledwidth=52.0% ]    these findings prompt us to redraw fig .",
    "[ fig:1 ] in terms of the rescaled variable @xmath68 , which is done in fig .",
    "[ fig:3 ] .",
    "the collapse of the data for @xmath62 into a single curve implies that @xmath69 .",
    "we note that the failure of the scaling function @xmath70 to describe the data for @xmath71 was already expected from the results of fig .",
    "[ fig:2 ] .",
    "in fact , those results show that in the limit @xmath72 we have @xmath73 with @xmath74 .",
    "the study of the scaling function @xmath75 in the other extreme limit , @xmath76 , requires very large input sizes ( @xmath77 ) for relatively large lattices ( @xmath62 ) which is computationally unfeasible because of the need to use a huge number of samples to get a reliable statistics since @xmath78 in this limit . nevertheless , in fig .",
    "[ fig:4 ] we present such analysis in the case of small lattices @xmath51 and @xmath79 , for which we know the scaling behavior is not valid . as expected , the results show that @xmath48 vanishes exponentially with increasing @xmath0 , i.e. , @xmath80 . here",
    "the fitting parameter is given by @xmath81 , indicating that for small @xmath1 the gain on performance obtained by increasing the number of agents is much larger than the gain in the scaling regime where @xmath82 .",
    "in addition , fig .",
    "[ fig:4 ] is useful to highlight the enormous gain on performance resulting from the increase of the number of agents involved in the optimization procedure .",
    "a most appealing feature of the ach is that the dynamics always freezes in a homogeneous absorbing configuration and so the algorithm halts .",
    "we must note , however , that the ach is a stochastic heuristic since the same initial configuration of the lattice can lead to different absorbing configurations depending on the sequence of site updates .",
    "the fact that the dynamics eventually freezes allows us to define a relaxation time for the ach , which is a quite unexpected bonus for a stochastic heuristic . accordingly , in fig .",
    "[ fig:5 ] we show the scaled average relaxation time @xmath83 as function of the input size @xmath0 . the unsurprising fact that @xmath84 scales linearly with the number of agents @xmath1 is manifested by the coincidence of results for different lattice sizes .",
    "the instructive result here is that @xmath84 grows with the square of the input size only .",
    "this result will be useful for the evaluation of the overall computational demand of the ach ( see sect .",
    "[ sec : conc ] ) .     for @xmath85 and @xmath86 .",
    "the error bars are smaller than the sizes of the symbols .",
    "the dashed curve is the fitting @xmath87 .",
    "[ fig:5 ] , scaledwidth=52.0% ]    the effect of the use of linearly boolean separable input - output mappings on the measured performance of ach can be appreciated in figure [ fig:6 ] where we show a comparison between the performance of that heuristic for the random and the linearly boolean separable mapping . as mentioned before , in the case of the random mapping the minimum cost is not necessarily zero and the global minimum is obtained through an exhaustive search in the configuration space ( hence the restriction to @xmath47 ) .",
    "although the random mapping seems to be a harder problem to the ach , there is no qualitative difference between the dependence of our performance measure @xmath48 on the parameters @xmath1 and @xmath0 for the two mappings , and so our scaling results are likely to remain true for the random mapping as well .     and @xmath88 . the error bars are smaller than the sizes of the symbols and the lines are guides to the eye .",
    "[ fig:6 ] , scaledwidth=52.0% ]    to conclude our analysis , a word is in order about the impact of the connectivity between the agents on the performance of the ach .",
    "it is well - known that the expansion of the influence range of the agents , modeled by increasing the connectivity of the lattice @xcite or by placing the agents in more complex networks @xcite ( e.g. , small - world and scale - free networks ) , results in the cultural homogenization of the population in axelrod s model .",
    "hence , it is not unreasonable to expect that by increasing the connectivity of the lattice ( or network ) the relaxation time would decrease and so the computational cost of the heuristic would be reduced .",
    "alas , that is not so .",
    "in fact , the results of fig .",
    "[ fig:7 ] , which shows the scaled relaxation time @xmath83 as function of the connectivity @xmath89 of a random symmetric network composed of @xmath79 agents , indicate that @xmath83 reaches a minimum around @xmath9 . as expected , we find that the probability @xmath48 of reaching the optimal solution is not affected by the choice of the connectivity @xmath89 , and so the connectivity @xmath9 yields the best performance , in the sense of the least computational cost , of the ach for not too small @xmath0 .",
    "in addition , the finding that the results of the random symmetric network with @xmath9 are indistinguishable from the results obtained for the regular square lattice ( data not shown ) suggests that the topology of the network does not influence the performance of the ach .     of random symmetric networks of @xmath79 agents for @xmath90 and @xmath91 .",
    "each symbol represents the average over @xmath92 distinct random symmetric networks of fixed connectivity .",
    "the error bars are smaller than the sizes of the symbols and the lines are guides to the eye .",
    "[ fig:7 ] , scaledwidth=52.0% ]",
    "understanding and quantifying how cooperation can improve the performance of groups of individuals to solve problems is an issue of great interest to many areas - ranging from computer science to business administration @xcite .",
    "our findings about the performance of the adaptive culture heuristic ( ach ) indicate that the number of agents participating of the collective solution of an optimization problem may influence the outcome of the process in a highly non - linear way ( see , e.g. , fig .",
    "[ fig:1 ] ) .",
    "our results were derived for a particular np - complete optimization problem , namely , the classification of linearly boolean separable input patterns by a boolean binary perceptron , whose optimal ( zero - cost ) solution is known by construction and which involves the manipulation of binary variables only .",
    "these two features allowed the study of the performance of the ach for very large input sizes @xmath0  which essentially measures the ` size ' of the optimization problem  and for a large number @xmath1 of agents involved in the collective problem solving task .",
    "we focused on a single performance measure @xmath48 , which yields the probability that a run of the ach finds an optimal solution , and found that it is a function of the reduced variable @xmath93 for @xmath94 ( see figs .",
    "[ fig:2 ] and [ fig:3 ] ) .",
    "this is a most remarkable and useful result which informs how the number of agents must scale with the problem size for a given fixed performance of the ach , namely , @xmath3 . recalling that the scaled relaxation time @xmath83 scales with @xmath95 ( see fig .",
    "[ fig:5 ] ) we find that the overall computational cost to find an optimal solution with a fixed probability scales with @xmath4 . as mentioned in sect .",
    "[ sec : intro ] , this finding has no bearing on the @xmath7 conjecture of computer science .",
    "in addition , a surprising result , which is summarized in fig .",
    "[ fig:7 ] , indicates that the implementation of the ach on a square lattice or on a random symmetric network of connectivity @xmath9 , yields the best performance when compared with the implementation on a random network of different connectivity",
    ".    it would be most interesting to find out whether the @xmath4 scaling law derived for the problem of learning linearly separable patterns by a boolean binary perceptron holds for other optimization problems as well . in that case",
    ", one would have revealed a genuine property of the ach which , given the minimal nature of the underlying social interaction mechanism , might serve as a bound to the performance of heuristics based on collective computation .",
    "eberhart and y. shi , _ particle swarm optimization : developments , applications and resources _ in proceedings of the 2001 congress on evolutionary computation .",
    "seoul , south korea , pp .",
    "8186 ( 2001 ) ."
  ],
  "abstract_text": [
    "<S> we investigate the performance of a variant of axelrod s model for dissemination of culture  the adaptive culture heuristic ( ach )  on solving an np - complete optimization problem , namely , the classification of binary input patterns of size @xmath0 by a boolean binary perceptron . in this heuristic , </S>",
    "<S> @xmath1 agents , characterized by binary strings of length @xmath0 which represent possible solutions to the optimization problem , are fixed at the sites of a square lattice and interact with their nearest neighbors only . </S>",
    "<S> the interactions are such that the agents strings ( or cultures ) become more similar to the low - cost strings of their neighbors resulting in the dissemination of these strings across the lattice . </S>",
    "<S> eventually the dynamics freezes into a homogeneous absorbing configuration in which all agents exhibit identical solutions to the optimization problem . </S>",
    "<S> we find through extensive simulations that the probability of finding the optimal solution is a function of the reduced variable @xmath2 so that the number of agents must increase with the fourth power of the problem size , @xmath3 , to guarantee a fixed probability of success . in this case </S>",
    "<S> , we find that the relaxation time to reach an absorbing configuration scales with @xmath4 which can be interpreted as the overall computational cost of the ach to find an optimal set of weights for a boolean binary perceptron , given a fixed probability of success . </S>"
  ]
}