{
  "article_text": [
    "the hidden markov model ( hmm ) is widely used in many disciplines , including applied mathematic , statistics , economics and finance ; see @xcite for an overview . in this article , we are interested in hmms given by diffusions which are partially observed , discretely in time . in particular , we assume that in order to fit the model to the data , one _ must _ resort to a discretization of the diffusion , for instance , using euler - maruyama .",
    "in addition , we assume that associated to the model is a static ( non - time - varying ) finite dimensional parameter , which one is interested to infer given a fixed length data record .",
    "in simple terms , the discretization , of level @xmath0 say , where as @xmath1 one obtains the exact diffusion , induces a posterior say @xmath2 on the static parameter @xmath3 and hidden states at the observation times , say @xmath4 .",
    "we seek to approximate @xmath5 $ ] for appropriately defined real - valued functions .",
    "ultimately , one might seek to remove the dependence upon @xmath0 and get the exact expectation with no discretization bias .",
    "we remark that the model will be formally introduced in the next section .",
    "this framework is relevant to a broad range of applications in science and engineering ; see @xcite    the task of computing the expectation for any fixed @xmath6 is a non - trivial task , which often requires quite advanced monte carlo methods . as has been remarked in many articles in the literature , ofen the joint correlation between @xmath3 and @xmath4 means even standard mcmc methods may produce very inaccurate of inefficient approximations of the expectation of interest , despite their theoretical validity .",
    "an important algorithm that has , to an extent , helped to alleviate these difficulties is the particle mcmc ( pmcmc ) methods of @xcite and their subsequent developments ( e.g.  @xcite ) .",
    "intrinsically , this method uses a sequential monte carlo ( smc ) ( e.g.  @xcite ) method to help move the samples around the state - space , for instance , inside a metropolis - hastings acceptance / rejection scheme , although gibbs versions also exist .",
    "pmcmc delivers a markov chain which provides consistent estimates of expectations of the form @xmath5 $ ] , for any fixed @xmath0 smc methods are well - known as being efficient techniques for filtering , when the state - variable at time @xmath7 , @xmath8 , is of moderate to low dimension and all the static parameters are fixed .    in the context of this article , there is an additional degree of freedom , which can be utilized to further enhance the pmcmc method .",
    "this is associated to the discretization level @xmath0 .",
    "we consider using the multilevel monte carlo ( mlmc ) framework @xcite .",
    "this allows one to leverage in an optimal way the nested problems arising in this context , hence minimizing the necessary cost to obtain a given level of mean square error .",
    "set @xmath9 as the posterior on @xmath10 with no discretization bias and @xmath11 as the time - discretized posterior on @xmath10 with time discretization @xmath12 , one has for an intergrable , real - valued function @xmath13 and @xmath14 ( the levels ) @xmath15 = \\sum_{l=0}^l\\{\\mathbb{e}_{\\pi_{h_l}}[\\varphi(\\theta , x_{0:n } ) ] - \\mathbb{e}_{\\pi_{h_{l-1}}}[\\varphi(\\theta , x_{0:n})]\\}\\ ] ] where @xmath16 is the expectation operator and @xmath17:=0 $ ] .",
    "the idea of mlmc is then to approximate each summand by independently simulating @xmath18 samples from a dependent coupling of @xmath19 . in such scenarios",
    ", one can show that the overall mean square error ( mse ) associated to the approximation of @xmath20 $ ] is : @xmath21 where @xmath22-\\mathbb{e}_{\\pi } [ \\varphi(\\theta , x_{0:n})]| \\ , \\ ] ] and @xmath23 are a collection of constants .",
    "it is remarked that it is the coupled samples which induce @xmath24 to be a function of @xmath12 which is often critical as we explain below .",
    "assuming the cost of @xmath25 per level , per sample , the cost of the algorithm is then @xmath26 .",
    "fixing @xmath27 and given an appropriate parameterization of @xmath12 ( e.g.  @xmath28 ) , one then chooses @xmath29 to ensure that @xmath30 and then given @xmath31 characterised as a function of @xmath12 optimizes @xmath32 to minimize the cost so that the term @xmath33 ; @xcite gives the solution to this constrained optimization problem . in many scenarios of practical interest the associated mlmc algorithm can achieve a mse of @xmath34 at a cost which is less than i.i.d .  sampling from @xmath35 ; note that this has not yet been established in the problem under study here .",
    "the main issue is that sampling independently from the couples @xmath19 is not possible in our context .    in this paper",
    "we show how to implement a new approximation of the multilevel collapsing sum identity .",
    "our approach comprises constructing an approximate coupling of the posterior density of the joint on the parameter and hidden space at two different discretization levels and then correcting by an importance sampling method , whose variance of the weights are independent of the length of the observed data set .",
    "the utility of such a method is that it comprises using known and efficient simulation methodologies , instead of coupling algorithms as explored in @xcite . in particular",
    ", our approach facilitates a mathematical analysis which allows us to establish that our approach can be better than sampling ( e.g.  by pmcmc ) from the posterior associated to the most precise discretization .",
    "the algorithm presented here is distinct from either of the previously introduced multilevel mcmc ( mlmcmc ) algorithms @xcite , and may be generalized .",
    "this article is structured as follows . in section [ sec : model ]",
    "the model is described . in section [ sec : app_theory ] we describe our approach and give a mathematical result associated to the mse of the method . in section",
    "[ sec : numerics ] we give practical simulations to establish the theory .",
    "the appendix contains some of the proofs for the result of section [ sec : app_theory ] .",
    "we consider the following partially - observed diffusion process : @xmath36 with @xmath37 , @xmath38 , @xmath39 has initial probability density @xmath40 and @xmath41}$ ] a brownian motion of appropriate dimension .",
    "@xmath42 is a static parameter of interest .",
    "the following assumptions will be made on the diffusion process .",
    "@xmath43 , @xmath44 satisfy    * * global lipschitz property * : there is a @xmath45 such that @xmath46 for all @xmath47 and all @xmath48 ; * * bounded moments * : @xmath49 for all @xmath50    [ ass : diff ]    notice that ( i ) and ( ii ) together imply that @xmath51 for all @xmath52 .",
    "it will be assumed that the data are regularly spaced ( i.e.  in discrete time ) observations @xmath53 , @xmath54 .",
    "it is assumed that conditional on @xmath55 , for discretization @xmath56 , @xmath57 is independent of all other random variables with density @xmath58 . for simplicity of notation",
    "let @xmath59 ( which can always be done by rescaling time ) , so @xmath60 .",
    "it is noted that we assume that one does not have access to a non - negative and unbiased estimate of the transition density of the diffusion and we are forced to work with a discretized process .",
    "the above formulation can then summarized as follows , on discretizing the diffusion process with discretization level @xmath0 .",
    "we have a pair of discrete - time stochastic processes , @xmath61 and @xmath62 , where @xmath63 ( with associated @xmath64algebra @xmath65 ) is an unobserved process and @xmath66 ( with associated @xmath64algebra @xmath67 ) is observed .",
    "let @xmath42 be a parameter .",
    "the hidden process @xmath68 is a  markov chain with initial density @xmath69 at time @xmath70 and transition density @xmath71 , i.e.  for each @xmath48 @xmath72 where @xmath73 denotes probability , @xmath74 and @xmath75 is a dominating @xmath76-finite measure .",
    "in addition , the observations @xmath77  conditioned upon @xmath78 are statistically independent and have marginal density @xmath79 , i.e.@xmath80 with @xmath81 and @xmath82 the dominating @xmath76-finite measure .",
    "the hmm is given by equations ( [ eq : evol])-([eq : obs ] ) and is often referred to in the literature as a state - space model . in our context",
    "@xmath48 is a parameter of interest with prior @xmath83 .",
    "given the joint density on @xmath84 @xmath85 for @xmath86 , where @xmath87 are the bounded and real - valued measurable functions on @xmath88 and @xmath89 are the lipschitz , measurable functions on @xmath88 , and for @xmath90 we would like to compute @xmath91 = \\sum_{l=0}^l\\big\\{\\mathbb{e}_{\\pi_{h_l}}[\\varphi(\\theta , x_{0:n } ) ] - \\mathbb{e}_{\\pi_{h_{l-1}}}[\\varphi(\\theta , x_{0:n } ) ] \\big\\}\\ ] ] where @xmath92 = 0 $ ] .",
    "we will use the mlmc approach . consider only a single pair @xmath93 - \\mathbb{e}_{\\pi_{h'}}[\\varphi(\\theta , x_{0:n})]$ ] , @xmath94 .",
    "it is well known that if one can sample from a dependent coupling of @xmath95 , such as the maximal coupling , then monte carlo estimation of such a difference can be performed at a lower cost than i.i.d  sampling from the independent coupling of @xmath95 @xcite .",
    "the main issue is that such couplings are typically not available up - to a non - negative and unbiased estimator .",
    "we consider the scenario where one samples from a sensible , approximate , coupling and corrects via importance sampling .",
    "we are to approximate the identity .",
    "our procedure , when considering the summands from @xmath96 will be to run @xmath29 independent pairs of the idea to be described below . the case @xmath97 is simply using ( e.g. ) pmcmc to approximate @xmath98 $ ] ; we refer the reader to @xcite for details on pmcmc - a simple decsription is below .",
    "we only consider a pair @xmath93 - \\mathbb{e}_{\\pi_{h'}}[\\varphi(\\theta , x_{0:n})]$ ] , @xmath94",
    ". the methodology and analysis in this context of one pair will suffice to justify our approach as we will explain below .",
    "let @xmath99 and @xmath100 be any coupling ( other than the independent one ) of @xmath101 .",
    "for instance , in the context of an euler discretization a description can be found in @xcite ( see also appendix [ app : couple_euler ] ) .",
    "let @xmath102 ( note that alternative choices of @xmath103 are possible ) .",
    "we propose to sample from the probability density on @xmath104 ( write the associated @xmath64algebra as @xmath105 ) @xmath106 then for @xmath86 : @xmath107 - \\mathbb{e}_{\\pi_{h'}}[\\varphi(\\theta , x_{0:n } ) ] = \\ ] ] @xmath108}{\\mathbb{e}_{\\pi_{h , h'}}[h_{1,\\theta}(\\theta , z_{0:n } ) ] } -   \\frac{\\mathbb{e}_{\\pi_{h , h'}}[\\varphi(\\theta , x_{0:n}')h_{2,\\theta}(\\theta , z_{0:n})]}{\\mathbb{e}_{\\pi_{h , h'}}[h_{2,\\theta}(\\theta , z_{0:n } ) ] } \\label{eq : master_eq}\\ ] ] where @xmath109 we note that our choice of @xmath110 ensures that @xmath111 and @xmath112 are uniformly upper - bounded by 1 and hence that the variance w.r.t .",
    "any probability is independent of @xmath52 .",
    "let @xmath113 be a measurable space such that @xmath114 .",
    "let @xmath115 $ ] be any ergodic markov kernel of invariant measure @xmath116 such that one can consistently estimate expectations w.r.t .",
    "for instance , if for every @xmath118 @xmath119 our construction allows a particle mcmc approach to be adopted , which is not quite as the displayed equation , but nonetheless allows one to infer @xmath117 .",
    "we focus on one particle mcmc method for completeness , but , we reiterate that one can use the analysis here for more advanced versions of the algorithm , or indeed , any mcmc of the form above .",
    "we will now describe the particle marginal metropolis - hastings ( pmmh ) algorithm .",
    "let @xmath120 and @xmath3 be fixed , and introduce random variables @xmath121 , which will denote the indices of the selected particles upon resampling at the given steps .",
    "one can run a particle filter @xcite to approximate @xmath122 by sampling from the following joint , on the space @xmath123 @xmath124 where @xmath125 . note that better algorithms can be constructed ,",
    "but we just present the most simple approach .",
    "we remark that @xmath126 is an unbiased estimator of @xmath127 ; see @xcite .",
    "the pmmh algorithm works as follows .",
    "the superscripts for @xmath128 are the iteration ( time ) counter of the mcmc .",
    "1 .   initialize : sample @xmath129 from the prior and then sample @xmath130 from @xmath131 as in , and store @xmath132 as in .",
    "select a path @xmath133 , constructed by drawing @xmath134 with probability proportional to @xmath135 , and setting @xmath136 ; set @xmath137 as the index of the selected path .",
    "set @xmath138 .",
    "iterate : sample @xmath139 according to a proposal with conditional density @xmath140 then from @xmath141 as in .",
    "select a path @xmath133 with probability proportional to @xmath142 and constructed as described above ; set @xmath143 as the index of the selected path .",
    "set @xmath144 , @xmath145 with probability : @xmath146 otherwise @xmath147 , @xmath148 .",
    "set @xmath149 and return to the start of 2 .",
    "we denote by @xmath150 the pmmh kernel and denote by @xmath113 the measurable space for which it is defined upon .",
    "the invariant measure is denoted @xmath116 . for the analysis",
    ", we assume the mcmc algorithm is started in stationarity .",
    "then one estimates by @xmath151 this estimate is consistent in the limit as @xmath152 grows ; see @xcite . to simplify the notation we replace @xmath153 in the superscripts by @xmath154 from here on .",
    "as described for mlmc in the introduction , we will approximate the expectation using the telescopic sum identity given in",
    ". we will establish error estimates for @xmath155 where @xmath156 is a consistent estimator of @xmath157 - \\mathbb{e}_{\\pi_{h_{l-1}}}[\\varphi(\\theta , x_{0:n})]$ ] .",
    "therefore is a consistent estimator of @xmath158 $ ] and the the mse can be bounded , up to a constant , by the sum of the squared error of and bias@xmath159 , as given by , which is @xmath160 for example using euler maruyama .    using @xmath16 to denote the expectation w.r.t .",
    "the law associated to our algorithm , assuming the markov chain is started in stationarity , our objective is therefore to investigate @xmath161 = \\sum_{l=0}^l\\mathbb{e}[\\bar{e}_l^{n_l}(\\varphi)^2 ]   \\label{eq : var_cmcmc}\\ ] ] so as to optimally allocate @xmath32 as described in the introduction .",
    "thus we must investigate terms such as @xmath162 $ ] for a given @xmath163 .",
    "below @xmath164 are the collection of probability measures on @xmath113 .",
    "[ hyp : a ] for every @xmath165 there exist @xmath166 such that for every @xmath167 , @xmath48 , @xmath168 for every @xmath165 , @xmath169 is globally lipschitz on @xmath170 .",
    "[ hyp : b ] for any @xmath171 , @xmath172 there exists a @xmath173 such that for any @xmath174 there exists a @xmath175 @xmath176    [ hyp : c ] suppose that for any @xmath177 there exist a @xmath178 and @xmath179 such that for each @xmath180 , @xmath181 , @xmath182 : @xmath183 @xmath150 is @xmath116-reversible , that is , @xmath184 for any @xmath185 .",
    "we note that ( a[hyp : a ] ) can be verified for some state - space models ( especially if @xmath186 and @xmath187 are compact ) and ( a[hyp : c ] ) can be verified for a pmcmc kernel , if @xmath188 are compact - indeed , the constants would all be independent of @xmath52 under appropriate settings of the algorithm .",
    "[ thm : main ] assume ( a[hyp : a]-[hyp : c ] ) .",
    "then for any @xmath177 , there exists a @xmath173 such that for any @xmath189 there exists a @xmath175 such that @xmath190 @xmath191}{\\mathbb{e}_{\\pi_{h , h'}}[h_{1,\\theta}(\\theta , z_{0:n } ) ] } -   \\frac{\\mathbb{e}_{\\pi_{h , h'}}[\\varphi(\\theta , x_{0:n}')h_{2,\\theta}(\\theta , z_{0:n})]}{\\mathbb{e}_{\\pi_{h , h'}}[h_{2,\\theta}(\\theta , z_{0:n } ) ] } \\bigg ) \\bigg)^2\\bigg ] \\leq \\frac{c(h')^{\\beta}}{n}.\\ ] ]    the result follows by using lemma c.3 .  of @xcite , the @xmath192inequality , the boundedness of certain quantities and proposition [ prop :",
    "main].the proof is omitted as it is similar to the calculations in @xcite .",
    "returning to section [ sec : mlmc ] , we assume that @xmath193 and introduce the further assumption    [ ass : cost ] the cost to simulate @xmath194 in is controlled by @xmath195 , and the bias is controlled by @xmath196 for @xmath197 .    following assumption ( a[hyp : b ] ) , @xmath198 satisfies the above , but it may be larger , e.g. for euler - maruyama in which @xmath199 .    given @xmath200 , in order to ensure the mse is @xmath34 , the term must be @xmath34 . following from assumption ( a[hyp : b ] )",
    ", it suffices to let @xmath201 so that @xmath202 .",
    "following from theorem [ thm : main ] , @xmath203   \\leq c\\sum_{l=0}^l \\frac{h_l^\\beta}{n_l},\\ ] ] and note that the constant @xmath204 may depend upon the time parameter @xmath52 , which has been suppressed from the notation ; we return to this point below .",
    "suppose we minimize cost @xmath205 subject to @xmath206 as a function of @xmath32 .",
    "this is exactly considered in @xcite for @xmath207 and later in @xcite for @xmath208 , and yields that @xmath209 where @xmath210 ( see also @xcite ) .",
    "this gives a cost of @xmath211 per time step .",
    "hence the following corollary is immediate .",
    "[ cor : cost ] given ( a[hyp : a]-[hyp : c ] ) and assumption [ ass : cost ] , for any @xmath177 and any @xmath189 , @xmath212 can be chosen such that the estimator @xmath213 with @xmath194 given in , satisfies @xmath214 \\leq c \\epsilon^2 \\ , \\ ] ] for some @xmath45 , for a total cost controlled by @xmath215    in contrast , for the same scenario , the computational cost of pmcmc is @xmath216 per time step , which is asymptotically greater than the method developed here .",
    "it is remarked that all of our constants depend upon the time parameter ( number of data points ) and this element has been ignored .",
    "this is due to the technical complexity of the approach .",
    "we expect that the constants can be made time - uniform , and hence we conjecture that the results hold true uniformly in time",
    ". then @xmath18 can be chosen as above , and for euler maruyama ( @xmath217 @xcite ) the cost for a given @xmath52 will be @xmath218 , with similar results for @xmath219 , according to .",
    "this results because one needs to take @xmath220 for the particle filter in pmmh @xcite and the cost to obtain a single sample particle filter trajectory is @xmath221 .",
    "a verification of this is left for future work .",
    "first , we consider the following ornstein - uhlenbeck process , @xmath222 where @xmath223 denotes the normal distribution with mean @xmath224 and variance @xmath225 .",
    "further , the parameters @xmath226 are unknown and are given the following priors , @xmath227 where @xmath228 denotes the gamma distribution with shape @xmath229 and scale @xmath230 .",
    "the remaining parameters are defined as constants , @xmath231 , @xmath232 , @xmath233 , and @xmath234 . a data set with 100 observations is simulated with @xmath235 and @xmath236 .      consider the following langevin sde , @xmath237 where @xmath238 denote the probability density function of a student s @xmath239-distribution with @xmath3 degrees of freedom .",
    "the parameters of interest are @xmath226 , and these are given prior , @xmath240 the constants are @xmath241 and @xmath242 .",
    "a data set with 1,000 observations is simulated with @xmath243 , @xmath244 , and @xmath245 .",
    "the simulations proceed as the following .",
    "let @xmath246 be the accuracy parameter . at each level @xmath163 , we set the number of particles in the pmcmc kernel be @xmath247 fixed , and set the number of pmcmc samples for estimation according to the multilevel analysis .",
    "let @xmath248 denote the number of samples at level @xmath163 within a simulation that targets @xmath29-level error , @xmath249 .",
    "the value of @xmath250 is determined empirically with variance estimated from 100 samplers . for comparison",
    ", a single - level pmcmc sampler is also considered for each @xmath29 .",
    "its number of samples @xmath251 is determined empirically by running 100 simulations simultaneously . and",
    "these chains are run until the estimated error of the 100 estimates matches that of the multilevel sampler . in all situations , a fixed burn - in period of 10,000 iterations is used .",
    "this is reasonable given the fast decorrelation of the chains , as illustrated by the estimated autocorrelation of the single level pmcmc sampler for @xmath252 in figure [ fig : acf ] .",
    "the autocorrelation functions look similar for all @xmath163 for the multilevel sampler .",
    "we consider the choice of @xmath247 .",
    "the main results of the cost vs.  error are shown in figure  [ fig : cost ] .",
    "the estimated cost rates are listed in table  [ tab : rates ] .",
    "it is shown in the appendix that for euler discretization the method satisfies the assumptions ( a[hyp : a]-[hyp : c ] ) with @xmath253 in ( a[hyp : b ] ) , since the diffusion term @xmath254 is constant in @xmath255 @xcite .",
    "furthermore , assumption [ ass : cost ] holds with @xmath256 .",
    "therefore , the theoretical results of theorem [ thm : main ] and corollary [ cor : cost ] predict the rate @xmath257 .",
    "standard pmmh will incur a cost of @xmath258 .",
    "the numerical results confirm this .",
    ".estimated rates of convergence of mse with respect to cost for various parameters , fitted to the curves in figure [ fig : cost ] . [ cols=\"^,^,^,^\",options=\"header \" , ]      aj & yz were supported by an acrf tier 2 grant : r-155 - 000 - 161 - 112 .",
    "aj is affiliated with the risk management institute , the center for quantitative finance and the or & analytics cluster at nus .",
    "kk & aj acknowledge crest , jst for additionally supporting the research .",
    "kjhl was supported by ornl ldrd strategic hire grant 32112580 .",
    "a markov kernel @xmath150 can be viewed as a linear operator @xmath259 for @xmath260 on a hilbert space @xmath261 with an inner product @xmath262 and norm @xmath263 .",
    "let @xmath264 be the operator norm .    by dblin condition ( a[hyp : c ] )",
    ", we have the total variation distance bound @xmath265 for some @xmath266 . since @xmath150 is an metropolis - hastings kernel , it has @xmath116-reversibility . therefore , the total variation bound implies @xmath267-spectral gap @xmath268 by theorem 2.1 of @xcite .",
    "for @xmath269 a finite measure on a measurable space @xmath270 and @xmath271 @xmath272    defining @xmath273 as the relevant variables of @xmath274 from the mcmc kernel , and defining @xmath275 we are interested in estimates of the form : @xmath276    [ prop : main ] assume ( a[hyp : a]-[hyp : c ] ) .",
    "suppose that @xmath277 is a markov chain with the markov kernel @xmath150 , and @xmath278 .",
    "then for any @xmath177 , there exists a @xmath173 such that for any @xmath189 there exists a @xmath175 such that @xmath279 \\leq \\frac{c(h')^{\\beta}}{n},\\ ] ] where @xmath280 is the relevant variables of @xmath281 .",
    "denote the map @xmath282 by @xmath283",
    ". then @xmath279 = \\mathbb{e}\\big[\\big(\\frac{1}{n}\\sum_{i=1}^nf(w^i)\\big)^2\\big]\\ ] ] for @xmath284 . by simple algebra",
    ", @xmath285= & \\frac{1}{n^2}\\sum_{i , j=1}^n\\langle f , k^{|i - j| } f\\rangle\\\\ = & \\frac{1}{n}\\|f\\|_2 ^ 2 + \\frac{2}{n^2}\\sum_{n=1}^{n-1}(n - n)\\langle f , k^nf\\rangle \\\\ \\le & \\frac{1}{n}\\|f\\|_2 ^ 2 + \\frac{2}{n^2}\\sum_{n=1}^{n-1}(n - n)\\|k\\|_2^n\\|f\\|_2 ^ 2 \\\\ \\le & \\frac{1}{n}\\|f\\|_2 ^ 2 + \\frac{2}{n}\\sum_{n=1}^\\infty\\|k\\|_2^n\\|f\\|_2 ^ 2 = \\frac{1}{n}\\frac{3-\\|k\\|_2}{1-\\|k\\|_2}\\|f\\|_2 ^ 2\\le \\frac{3\\|f\\|_2 ^ 2}{n\\xi}. \\end{aligned}\\ ] ] on the other hand , by lemma [ lem : tres1 ] , @xmath286\\le c(h')^\\beta . \\end{aligned}\\ ] ] thus , the claim follows .",
    "[ lem : tres1 ] assume ( a[hyp : a]-[hyp : b ] ) .",
    "then for any @xmath177 , @xmath172 there exists a @xmath173 such that for any @xmath189 there exists a @xmath175 @xmath287 \\right)^{3-q } \\leq c ( h')^{\\beta}.\\ ] ]    we prove the result for @xmath288 , the case @xmath289 being almost the same .",
    "the result is proved by induction on @xmath52 . set @xmath290 , then @xmath291   = \\ ] ] @xmath292 @xmath293 as @xmath294",
    "is uniformly ( in @xmath295 ) bounded below , the denominator on the r.h.s .",
    "is uniformly lower bounded by a constant that is independent of @xmath182 .",
    "the numerator on the r.h.s .",
    "is @xmath296 @xmath297 application of ( a[hyp : b ] ) hence yields @xmath298 \\leq c ( h')^{\\beta/2}.\\ ] ] assuming the result for @xmath299 , @xmath300 , by the above argument we only have to consider @xmath301 @xmath302 the r.h.s .",
    "can be upper - bounded by @xmath303 @xmath304 the first term can be treated by the induction hypothesis and the second term via ( a[hyp : b ] ) which completes the proof .",
    "consider @xmath305 , the current position of the discretized diffusions .",
    "now we have @xmath182 the discretization levels , with @xmath306 and for simplicity set @xmath307 .",
    "associated to the discretization level @xmath0 ( resp .",
    "@xmath308 ) , one must sample @xmath309 ( resp .",
    "@xmath310 ) points to obtain the sampled position of the diffusion at the next observation time .",
    "set @xmath311 then one can sample the fine discretization , for @xmath312 as @xmath313 where @xmath314 ( @xmath315 is the @xmath316 identity matrix ) .",
    "for the course discretization , using the same simulated @xmath317 we set for @xmath318 @xmath319.\\ ] ]        by assumption , @xmath323 is the density of @xmath324 given @xmath325 .",
    "then , under assumption [ ass : diff ] ( i , ii ) , the condition ( a[hyp : b ] ) is satisfied with @xmath326 for any @xmath327 , since this is the @xmath328 bound of the euler - maruyama scheme ( in fact for constant diffusion coefficient @xmath254 it coincides with the milstein method and @xmath253 ) @xcite .",
    "next , we want to check the condition ( a[hyp : c ] ) .",
    "the proposal density @xmath283 on @xmath329 of pmmh is @xmath330 where @xmath331 , @xmath332 , and @xmath333 is defined in .",
    "the transition kernel @xmath150 is @xmath334 where the acceptance probability @xmath335 is @xmath336 and the rejection probability @xmath337 is @xmath338 by ( a[hyp : a ] ) together with assumption [ ass : compact ] , @xmath339 , and @xmath340 for a constant @xmath341 with a probability density @xmath342 .",
    "thus , we have @xmath343 in particular , the condition ( a3 ) is satisfied .",
    ", c. , scheichl , r. & teckentrup , a. l.  ( 2015 ) . a hierarchical multilevel markov chain monte carlo algorithm with applications to uncertainty quantification in subsurface flow .",
    "_ siam / asa journal on uncertainty quantification _ , 3(1 ) , pp.1075 - 1108 .",
    "( 1997 ) geometric ergodicity and hybrid markov chains . _ electron .",
    "_ , * 2*:2 , 1325 . , d. , thiery , a. & jasra , a.  ( 2016 ) . on coupling particle filter trajectories",
    ". _ arxiv preprint arxiv:1606.01016_."
  ],
  "abstract_text": [
    "<S> in this article we consider static bayesian parameter estimation for partially observed diffusions that are discretely observed . </S>",
    "<S> we work under the assumption that one must resort to discretizing the underlying diffusion process , for instance using the euler maruyama method . given this assumption , </S>",
    "<S> we show how one can use markov chain monte carlo ( mcmc ) and particularly particle mcmc [ andrieu , c. , doucet , a. & holenstein , r. ( 2010 ) . </S>",
    "<S> particle markov chain monte carlo methods ( with discussion ) . </S>",
    "<S> _ j. r. statist . </S>",
    "<S> soc . </S>",
    "<S> ser . </S>",
    "<S> b _ , 72 , 269342 ] to implement a new approximation of the multilevel ( ml ) monte carlo ( mc ) collapsing sum identity . </S>",
    "<S> our approach comprises constructing an approximate coupling of the posterior density of the joint distribution over parameter and hidden variables at two different discretization levels and then correcting by an importance sampling method . </S>",
    "<S> the variance of the weights are independent of the length of the observed data set . </S>",
    "<S> the utility of such a method is that , for a prescribed level of mean square error , the cost of this mlmc method is provably less than i.i.d .  </S>",
    "<S> sampling from the posterior associated to the most precise discretization . </S>",
    "<S> however the method here comprises using only known and efficient simulation methodologies . </S>",
    "<S> the theoretical results are illustrated by inference of the parameters of two prototypical processes given noisy partial observations of the process : the first is an ornstein uhlenbeck process and the second is a more general langevin equation . + * key words * : multilevel monte carlo , markov chain monte carlo , diffusion processes </S>"
  ]
}