{
  "article_text": [
    "deep learning ( dl ) is a branch of machine learning aiming at understanding high - level representations of data using a deeper structure of multiple processing layers  @xcite .",
    "dl has been successfully applied in pattern recognition and classification tasks such as image recognition and language processing .",
    "recently , the application of dl to physics research is rapidly growing , such as in particle physics  @xcite , nuclear physics  @xcite , and condensed matter physics  .",
    "dl is shown to be very powerful in extracting pertinent features especially for complex non - linear systems with high - order correlations that conventional techniques are unable to tackle .",
    "this suggests that it could be utilized to unveil hidden information from the highly implicit data of heavy - ion experiments .",
    "one primary goal of ultra - relativistic heavy - ion collisions is to study the qcd transition from confining states at lower temperature and density to asymptotically free states at high temperature or density .",
    "the qcd transition is conjectured to be a crossover at small density ( and moderately high temperature ) , and first order at moderate density ( and lower temperature ) , with a critical point separating the two , see fig .",
    "[ fig : phase ] for a schematic qcd phase diagram and @xcite for some reviews .",
    "though it is believed that strongly coupled qcd matter can be formed in heavy - ion collisions currently carried out at the relativistic heavy ion collider ( rhic , brookhaven national laboratory , usa ) , large hadron collider ( lhc , european organization for nuclear research , switzerland ) , and at the forthcoming facility for anti - proton and ion research ( fair , gsi helmholtz centre for heavy ion research , germany ) , a direct access to the bulk properties of the matter such as the equation of state ( eos ) and transport coefficients is impossible due to the highly dynamical nature of the collisions .",
    "what experiments can directly measure are the final - state particle spectra @xmath0 at different rapidities , where @xmath1 is the transverse momentum and @xmath2 is the azimuthal angle of the final charged hadrons . thus far no noticeable and unique correspondence between @xmath0 and the bulk properties during the evolution has been established using conventional observables .",
    "this complication induces big uncertainties in testing non - perturbative qcd in the bulk through heavy - ion experiments , and has thus posed a pressing challenge in high energy physics .",
    "conjectured qcd phase diagram and equations of state for the crossover and the first order phase transition .",
    ", scaledwidth=60.0% ]    relativistic hydrodynamics has been very successful in simulating heavy - ion collisions and connecting experiments with theory  @xcite .",
    "the aim of the present exploratory study is a first step in directly connecting qcd bulk properties and raw data of heavy - ion collisions using state - of - the - art deep - learning techniques .",
    "we find unique encoders of bulk properties ( here we focus on the eos ) inside @xmath0 in terms of high - level representations using deep - learning techniques , which are not captured by conventional observables .",
    "this is achieved by constructing a convolutional neural network ( cnn ) and training it with labeled @xmath3 of charged pions generated by the event - by - event hydrodynamic package clvisc  @xcite with two different eoss as input : crossover  @xcite and first order  @xcite .",
    "the cnn is then trained with supervision in identifying different eoss .",
    "the performance is surprisingly robust against other simulation parameters such as the initial state conditions , equilibrium time @xmath4 , transport coefficients and freeze out temperature .",
    "different from the bayesian method which determines multiple properties at the same time from global fitting  @xcite , supervised learning with deep cnn finds the hydrodynamic response which is much more tolerant to uncertainties at the initial stage . @xmath3 as generated by independent simulations ( clvisc with different setup parameters and another hydrodynamic package iebe - vishnu  @xcite which implements different numerical solver for partial differential equations ) are used for testing  a @xmath5 testing accuracy is obtained .",
    "it has been recently pointed out that model - dependent features may generate large uncertainties in the network performance and there has been no efficient way to reduce it  @xcite .",
    "the present results imply that model - dependent features can be strongly suppressed in the method as developed below .",
    "the decisive ingredients for the success of hydrodynamic modeling of relativistic heavy - ion collisions are the bulk - matter eos and the viscosity . in the study of the qcd phase transition in heavy - ion collisions ,",
    "one of the holy - grail question is : how to reliably extract eos and the nature of the qcd phase transition from the experimental data ?",
    "the so - called convolutional neural network ( cnn )  @xcite is a powerful technique in tasks such as image and video recognition , natural language processing .",
    "supervised training of the cnn with labeled @xmath3 generated by clvisc is tested with @xmath3 generated by iebe - vishnu .",
    "the training and testing @xmath3 can be regarded as numerical experimental data .",
    "hence , analyzing real experimental data is possible with straightforward generalizations of the current prototype setup .",
    "bins and 48 azimuthal angle @xmath2 bins.,scaledwidth=80.0% ]    our cnn architecture is shown in fig .",
    "[ fig : cnn_eos ] .",
    "the input @xmath0 consists of 15 @xmath1-bins and 48 @xmath2-bins .",
    "we use two convolutional layers each followed by batch normalization  @xcite , dropout  @xcite with a rate 0.2 and prelu activation  @xcite .",
    "we refer the readers to appendix for a brief introduction to these technical terms . in the first convolutional layer ,",
    "there are 16 filters of size @xmath6 scanning through the input @xmath0 and creating 16 features of size @xmath7 .",
    "these features are further convoluted in the second convolutional layer that has @xmath8 filters of size @xmath9 .",
    "the weight matrix of both convolutional layers are initialized with normal distribution and constrained with l2 regularization  @xcite . in a convolutional layer",
    ", each neuron only locally connects to a small chunk of neurons in the previous layer by a convolution operation  this is a key reason for the success of the cnn architecture .",
    "dropout , batch normalization , prelu and l2 regularization work together to prevent overfitting that may generate model - dependent features from the training dataset and thus hinder the generalizability of the method .",
    "the resulting 32 features of size @xmath10 from the second convolutional layer are flattened and connected to a 128-neuron fully connected layer with batch normalization , dropout with rate 0.5 and sigmoid activation  @xcite .",
    "the output layer is another fully connected layer with softmax activation  @xcite and 2 neurons to indicate the type of the eos .",
    "for multi - class classification , one may use more neurons in the output layer .",
    "the evolution of strongly coupled qcd matter can be well described by second - order dissipative hydrodynamics governed by @xmath11 , with @xmath12 the energy - momentum tensor containing viscous corrections governed by the israel - stewart equations  @xcite . in order to close the hydrodynamic equations",
    ", one must supply the eos of the medium as another crucial input .",
    "the nature of the qcd phase transition in the eos strongly affects the hydrodynamic evolution  @xcite , since different transitions are associated with different pressure gradients which consequently induce different expansion rates , see the small chart in fig .  1 .",
    "final @xmath0 are obtained from the cooper - frye formula for particle @xmath13 at mid - rapidity @xmath14 here @xmath15 is the particle number density , @xmath16 is the rapidity , @xmath17 is the degeneracy , @xmath18 is the freeze - out hypersurface element , @xmath19 is the thermal distribution . in the following ,",
    "we employ the lattice - eos parametrization  @xcite ( dubbed as eosl ) for the crossover transition and maxwell construction  @xcite ( dubbed as eosq ) for the first - order phase transition .",
    "the training dataset of @xmath0 ( labelled with eosl or eosq ) is generated by event - by - event hydrodynamic package clvisc  @xcite with fluctuating ampt initial conditions  @xcite .",
    "the simulation generated about 22000 @xmath0 for different types of collisions .",
    "then the size of the training dataset is doubled by label - preserving left - right flipping along the @xmath2 direction .",
    "we randomly select @xmath20 of all the @xmath0 for validation and use the rest for training . in tab .",
    "[ tab : training ] we list the details of the training dataset .    .training dataset : numbers of @xmath0 generated by the clvisc hydrodynamic package with the ampt initial conditions in the centrality range @xmath21 .",
    "@xmath22 is ratio of shear viscosity to entropy density .",
    "@xmath23fm for the au - au collisions and @xmath24fm for the pb - pb collisions .",
    "the freeze - out temperature is set to be 137mev . [ cols=\"^,^,^,^,^,^ \" , ]     after training and validating the network , it is tested on the testing set of @xmath0 events . as shown in tab .",
    "[ tab : results ] , high accuracies  on average @xmath5  are achieved for both groups of the testing datasets , which indicates that our method is highly independent of initial conditions .",
    "the network is robust against shear viscosity and @xmath4 due to the inclusion of events with different @xmath22 and @xmath4 in the training . in the testing stage",
    "the neural network identifies the type of the qcd transition solely from the spectra of each single event .",
    "furthermore , in the training only one freeze - out temperature is used , while the network is tolerant to a wide range of freeze - out temperatures during the testing . for simplicity",
    ", the exploratory study has not included pions from resonance decays .",
    "[ fig : cnn_conv2_weight ] shows the 32 features ( activation maps of size @xmath10 with one spectra as input ) of the second convolutional layer learned by the network .",
    "deeper colors correspond to larger correlations for particles in the local phase space .",
    "these features are distributed representations operated jointly on the raw spectra to extract the eos information .",
    "they are further forwarded in groups to two fully connected layers to complete the classification task .",
    "the present method yields a novel perspective on identifying the nature of the qcd transition in heavy - ion collisions . with the help of deep cnns",
    ", we firmly demonstrate that discriminative and traceable projections , `` encoders '' , from the qcd transition onto the final - state @xmath0 do exist , in the complex , highly dynamical heavy - ion collisions , although these encoders may not be intuitive .",
    "the deep cnn provides a powerful and efficient `` decoder '' from which the eos information can be extracted directly from the @xmath0 .",
    "it is in this sense that the high - level representations which helps decoding the eos information in the present method , act as an `` eos - meter '' for dense matter created in relativistic heavy - ion collisions .",
    "this can provide the key to success for the experimental determination of qcd eos and search of the critical end point .",
    "an intriguing application of our framework is to extract the qgp transport coefficients from heavy - ion collisions .",
    "the present method can be improved by including hadronic transport and detector efficiency corrections in the future .",
    "l.g.p . and h.p .",
    "acknowledge funding of a helmholtz young investigator group vh - ng-822 from the helmholtz association and the gsi helmholtzzentrum fr schwerionenforschung ( gsi ) .",
    "k.z . and n.s .",
    "acknowledge the support from gsi .",
    "acknowledges the support through the judah m. eisenberg laureatus chair at goethe university .",
    "x.n.w was supported in part by nsfc under the grant no .",
    "11521064 , by most of china under grant no .",
    "2014dfg02050 , by the major state basic research development program ( msbrd ) in china under the grant no .",
    "2015cb856902and by u.s .",
    "doe under contract no .",
    "de - ac02 - 05ch11231 .",
    "this work was supported in part by the helmholtz international center for the facility for antiproton and ion research ( hic for fair ) within the framework of the landes - offensive zur entwicklung wissenschaftlich - oekonomischer exzellenz ( loewe ) program launched by the state of hesse .",
    "the computations were done in the green - cube gpu cluster lcsc at gsi , the loewe - csc at goethe university , and the gpu cluster at central china normal university .",
    "_ feedforward neural network _ learns one target function @xmath25 that maps the input vector @xmath26 to output vector @xmath27 with parameter @xmath28 . elements of @xmath26 and @xmath27 form the neurons in the input and output layers respectively . in - between",
    "there can be multiple hidden layers with the numbers of neurons as hyper - parameters .",
    "the connections between two layers form a trainable weight matrix @xmath29 . each layer",
    "( except the input layer ) learns representations of its previous layer through firstly a linear operation @xmath30 and then use it as the argument of an activation function @xmath31 .",
    "the linear operation can perform various operations , such as scaling , rotating , boosting , increasing or decreasing dimensions , on the vector @xmath26 , with the bias @xmath32 a trainable parameter .",
    "@xmath31 activates the neurons of the present layer with their values and computes the correlations between the neurons of the previous layer . by stacking with multiple hidden layers ,",
    "the deep neural network may learn high - level representations that can be classified or interpreted easily .",
    "the activation functions used in our study are shown in fig .",
    "[ fig : activation ] .      _ loss function _",
    "@xmath35 is the difference between the true value @xmath27 ( from the input of supervised learning ) and the predicted value @xmath36 by the neural network in a forward pass .",
    "@xmath35 is minimized during the training by adjusting network parameter @xmath28 .",
    "the simplest loss function is the mean square error @xmath37 . in this paper",
    "we use the cross entropy loss function from information theory , @xmath38 \\label{eq : cross_entropy}\\ ] ] with l1 or l2 regularizations , the loss function receives another term used to constrain the values of @xmath28 from going wildly , @xmath39 where @xmath40 is the regularization strength .",
    "larger @xmath40 leads to smaller @xmath28 , especially for high orders in the target function , which increases the generalizability of the neural network .    _",
    "back propagation _ indicates the gradients of the loss function in parameter space propagate in the backward direction of a neural network in order to update @xmath28 .",
    "for example , in the stochastic gradient decent ( sgd ) method , @xmath28 is updated with fixed learning rate @xmath41 @xmath42 in practice we train the network in batches , where @xmath28 is updated once for all the samples in one batch , @xmath43 where @xmath44 is the batchsize , @xmath45 is the parameter obtained by training with the @xmath13th sample in a batch . in our study , we use the adamax method @xcite , which computes adaptive learning rates for different parameters based on estimating the first and second moments of the gradients .",
    "we initially set the learning rate as @xmath46 and keep the other parameters the same as in @xcite .",
    "_ batch normalization _ solves the so - called internal covariate shift problem , which is a common issue in dl that hinders the learning efficiency  @xcite .",
    "using the batch mean @xmath47 and batch variance @xmath48 , the input vector @xmath26 is normalized as @xmath49 that has mean 0 and variance 1 , with @xmath41 a small number preventing divergence .",
    "the @xmath50 is further scaled and shifted by @xmath51 before going to the next layer , where @xmath52 and @xmath53 are trainable parameters .",
    "note that during the testing , population mean and variance of the training dataset are used .",
    "_ dropout _ is a regularization technique that reduces overfitting by randomly discarding a fraction of neurons ( features ) and all their associated connections to prevent co - adaption of neurons for each training sample @xcite .",
    "j.  schmidhuber , neural netw .",
    "* 61 * , 85 ( 2015 ) .",
    "y.  lecun , y.  bengio , and g.  hinton , nature * 521 * , 436 ( 2015 ) .",
    "p.  baldi , p.  sadowski , and d.  whiteson , nature commun .",
    "* 5 * , 4308 ( 2014 ) .",
    "p.  baldi , p.  sadowski , and d.  whiteson , phys .",
    "lett .   * 114 * , 111801 ( 2015 ) .",
    "j.  searcy , l.  huang , m.  a.  pleier , and j.  zhu , phys .",
    "d * 93 * , 094033 ( 2016 ) .",
    "j.  barnard , e.  n.  dawe , m.  j.  dolan , and n.  rajcic , arxiv:1609.00607 [ hep - ph ] .",
    "i.  moult , l.  necib , and j.  thaler , arxiv:1609.07483 [ hep - ph ] .",
    "r.  utama , w.  c.  chen , and j.  piekarewicz , arxiv:1608.03020 [ nucl - th ] .",
    "p.  mehta and d.  j.  schwab , arxiv:1410.3831 [ stat.ml ] .",
    "h.  stcker and w.  greiner , phys .",
    "* 137 * , 277 ( 1986 ) .",
    "m.  a.  stephanov , pos lat * 2006 * ( 2006 ) 024 .",
    "k.  fukushima and t.  hatsuda , rept .",
    "phys .   * 74 * , 014001 ( 2011 ) .",
    "u.  w.  heinz , landolt - bornstein * 23 * , 240 ( 2010 ) .",
    "romatschke , int .",
    "j.  mod .",
    "e * 19 * , 1 ( 2010 ) .",
    "d.  a.  teaney , arxiv:0905.2433 [ nucl - th ] . c.  gale , s.  jeon , and b.  schenke , int .",
    "j.  mod .",
    "a * 28 * , 1340011 ( 2013 ) .",
    "m.  strickland , acta phys .",
    "b * 45 * , 2355 ( 2014 ) .",
    "l.  g.  pang , q.  wang , and x.  n.  wang , phys .",
    "c * 86 * , 024911 ( 2012 ) .",
    "l.  g.  pang , y.  hatta , x.  n.  wang , and b.  w.  xiao , phys .",
    "d * 91 * , 074027 ( 2015 ) .",
    "p.  huovinen and p.  petreczky , nucl .",
    "a * 837 * , 26 ( 2010 ) .",
    "j.  sollfrank , p.  huovinen , m.  kataja , p.  v.  ruuskanen , m.  prakash , and r.  venugopalan , phys .",
    "c * 55 * , 392 ( 1997 ) .",
    "s.  pratt , e.  sangaline , p.  sorensen , and h.  wang , phys .",
    "lett .   * 114 * , 202301 ( 2015 ) j.  e.  bernhard , j.  s.  moreland , s.  a.  bass , j.  liu , and u.  heinz , phys .",
    "c * 94 * , 024907 ( 2016 ) c.  shen , z.  qiu , h.  song , j.  bernhard , s.  bass , and u.  heinz , comput .",
    "commun .   * 199 * , 61 ( 2016 ) .",
    "a.  krizhevsky , i.  sutskever , and g.  e.  hinton , advances in neural information processing systems 25 ( nips 2012 ) .",
    "softmax activation is a normalized exponential function that contract a q - dimensional vector @xmath55 of arvitrary real values to a q - dimensional vector @xmath56 of real values belong to range @xmath57 and add up to 1 : @xmath58 .",
    "f.  chollet , https://github.com/fchollet/keras .",
    "m.  abadi , _ et al .",
    "_ , arxiv:1603.04467 [ cs.dc ] , http://tensorflow.org/. g.  e.  hinton , n.  srivastava , a.  krizhevsky , i.  sutskever , and r.  r.  salakhutdinov , arxiv:1207.0580 [ cs.ne ] ."
  ],
  "abstract_text": [
    "<S> supervised learning with a deep convolutional neural network is used to identify the qcd equation of state ( eos ) employed in relativistic hydrodynamic simulations of heavy - ion collisions . </S>",
    "<S> the final - state particle spectra @xmath0 provide directly accessible information from experiments . </S>",
    "<S> high - level correlations of @xmath0 learned by the neural network act as an `` eos - meter '' , effective in detecting the nature of the qcd transition . </S>",
    "<S> the eos - meter is model independent and insensitive to other simulation input , especially the initial conditions . </S>",
    "<S> thus it provides a formidable direct - connection of heavy - ion collision observable with the bulk properties of qcd . </S>"
  ]
}