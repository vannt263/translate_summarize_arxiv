{
  "article_text": [
    "bipartite graph matching is one of the fundamental problems in graph theory and combinatorial optimization .",
    "the problem asks for a maximum set of vertex disjoint edges in a given bipartite graph .",
    "it has many applications in a variety of fields such as image processing  @xcite , chemical structure analysis  @xcite , and bioinformatics  @xcite  ( see also another two discussed by burkard  et  al .",
    "* section 3.8 ) ) .",
    "our motivating application lies in solving sparse linear systems of equations , as algorithms for computing a maximum cardinality bipartite matching are run routinely in the related solvers . in this",
    "setting , bipartite matching algorithms are used to see if the associated coefficient matrix is reducible ; if so , substantial savings in computational requirements can be achieved  ( * ? ? ?",
    "* chapter 6 ) .    achieving good parallel performance on graph algorithms is challenging , because they are memory bounded , and there are poor localities of the memory accesses .",
    "moreover , because of the irregularity of the computations , it is difficult to exploit concurrency .",
    "algorithms for the matching problem are no exception",
    ". there have been recent studies that aim to improve the performance of matching algorithms on multicore and manycore architectures .",
    "for example , vasconcelos and rosenhahn @xcite propose a gpu implementation of an algorithm for the maximum weighted matching problem on bipartite graphs .",
    "fagginger auer and bisseling @xcite study an implementation of a greedy graph matching on gpu .",
    "atalyrek  et  al .",
    "@xcite propose different greedy graph matching algorithms for multicore architectures .",
    "azad  et  al .",
    "@xcite introduce several multicore implementations of maximum cardinality matching algorithms on bipartite graphs .",
    "we propose gpu implementations of two maximum cardinality matching algorithms .",
    "we analyze their performance and employ further improvements .",
    "we thoroughly evaluate their performance with a rigorous set of experiments on many bipartite graphs from different applications .",
    "the experimental results conclude that one of the proposed gpu - based implementation is faster than its existing multicore counterparts .",
    "the rest of this paper is organized as follows .",
    "the background material , some related work , and a summary of contributions are presented in section  [ sec : back ] .",
    "section  [ sec : met ] describes the proposed gpu algorithms .",
    "the comparison of the proposed gpu - based implementations with the existing sequential and multicore implementations is given in section  [ sec : exp ] .",
    "section  [ sec : con ] concludes the paper .",
    "a bipartite graph @xmath0 consists of a set of vertices @xmath1 where @xmath2 , and a set of edges @xmath3 such that for each edge , one of the endpoints is in @xmath4 and other is in @xmath5 . since our motivation lies in the sparse matrix domain , we will refer to the vertices in the two classes as row and column vertices .",
    "a matching @xmath6 in a graph @xmath7 is a subset of edges @xmath3 where a vertex in @xmath1 is in at most one edge in @xmath6 . given a matching @xmath6 , a vertex @xmath8 is said to be matched by @xmath6 if @xmath8 is in an edge of @xmath6 , otherwise @xmath8 is called unmatched .",
    "the cardinality of a matching @xmath6 , denoted by @xmath9 , is the number of edges in @xmath6 .",
    "a matching @xmath6 is called maximum , if no other matching @xmath10 with @xmath11 exists . for a matching @xmath6 , a path @xmath12 in @xmath7",
    "is called an @xmath6-alternating if its edges are alternately in @xmath6 and not in @xmath6 .",
    "an @xmath6-alternating path @xmath12 is called @xmath13-augmenting if the start and end vertices of @xmath12 are both unmatched .",
    "there are three main classes of algorithms for finding the maximum cardinality matchings in bipartite graphs . the first class of algorithms is based on augmenting paths  ( see a detailed summary by duff  et  al .",
    "push - relabel - based algorithms form a second class  @xcite .",
    "a third class , pseudoflow algorithms , is based on a more recent work  @xcite .",
    "there are @xmath14 algorithms in the first two classes  ( e.g. , hopcroft - karp algorithm  @xcite and a variant of the push relabel algorithm  @xcite ) , where @xmath15 is the number of vertices and @xmath16 is the number of edges in the given bipartite graph .",
    "this is asymptotically best bound for practical algorithms .",
    "most of the other known algorithms in the first two classes and in the third class have the running time complexity of @xmath17 .",
    "these three classes of algorithms are described and compared in a recent study  @xcite .",
    "it has been demonstrated experimentally that the champions of the first two families are comparable in performance and better than that of the third family .",
    "since we investigate gpu acceleration of augmenting - path - based algorithms , a brief description of them is given below ( the reader is invited to two recent papers  @xcite and the original resources cited in those papers for other algorithms ) .",
    "algorithms based on augmenting paths follow the following common pattern .",
    "given an initial matching @xmath6 ( possible empty ) , they search for an @xmath6-augmenting path @xmath12 .",
    "if none exists , then the matching @xmath6 is maximum by a theorem of berge  @xcite .",
    "otherwise , the augmenting path @xmath12 is used to increase the cardinality of @xmath6 by setting @xmath18 where @xmath19 is the edge set of the path @xmath12 , and @xmath20 is the symmetric difference .",
    "this inverts the membership in @xmath6 for all edges of @xmath12 .",
    "since both the first and the last edge of @xmath12 were unmatched in @xmath6 , we have @xmath21 .",
    "the augmenting - path - based algorithms differ in the way these augmenting paths are found and the associated augmentations are realized .",
    "they mainly use either breadth - first - search ( bfs ) , or depth - first - search ( dfs ) , or combination of these two techniques to locate and perform the augmenting paths .",
    "multicore counterparts of a number of augmenting - path based algorithms are proposed in a recent work  @xcite .",
    "the parallelization of these algorithms is achieved by using atomic operations at bfs and/or dfs steps of the algorithm .",
    "although using atomic operations might not harm the performance on a multicore machine , they should be avoided in a gpu implementation because of very large number of concurrent thread executions .    as a",
    "reasonably efficient dfs is not feasible with gpus , we accelerate two bfs - based algorithms , called hk  @xcite and hkdw  @xcite .",
    "hk has the best known worst - case running time complexity of @xmath22 for a bipartite graph with @xmath15 vertices and @xmath16 edges .",
    "hkdw is a variant of hk and incorporates techniques to improve the practical running time while having the same worst - case time complexity",
    ". both of these algorithms use bfs to locate the shortest augmenting paths from unmatched columns , and then use dfs - based searches restricted to a certain part of the input graph to augment along a maximal set of disjoint augmenting paths .",
    "hkdw performs another set of dfs - based searches to augment using the remaining unmatched rows .",
    "as is clear , the dfs - based searches will be a big obstacle to achieve efficiency . in order to overcome this hurdle",
    ", we propose a scheme which alternates the edges of a number of augmenting paths with a parallel scheme that resembles to a breadth expansion in bfs .",
    "the proposed scheme offers a high degree of parallelism but does not guarantee a maximal set of augmentations , potentially increasing the worst case time complexity to @xmath23 on a sequential machine .",
    "in other words , we trade theoretical worst case time complexity with a higher degree of parallelism to achieve better practical running time with a gpu .",
    "we propose two algorithms for the gpu implementation of maximum cardinality matching .",
    "these algorithms use bfs to find augmenting paths , speculatively perform some of them , and fix any inconsistencies that can be resulting from speculative augmentations .",
    "the overall structure of the first gpu - based algorithm is given in algorithm  [ alg : apbfsee ] , apsb .",
    "it largely follows the common structure of most of the existing sequential algorithms , and corresponds to hk .",
    "it performs a combined bfs starting from all unmatched columns to find unmatched rows , thus locating augmenting paths .",
    "some of those augmentations are then realized using a function called ( will be described later ) .",
    "the parallelism is exploited inside the initbfsarray , bfs , , and fixmatchingfunctions .",
    "algorithm  [ alg : apbfsee ] is given the adjacency list of the bipartite graph with its number of rows and columns .",
    "any prior matching is given in @xmath24 and @xmath25 arrays as follows : @xmath26 = c$ ] and @xmath27 = r$ ] , if the row @xmath28 is matched to the column @xmath29 ; @xmath26 = -1 $ ] , if @xmath28 is unmatched ; @xmath27 = -1 $ ] , if @xmath29 is unmatched .",
    "@xmath30 ; +    the outer loop of algorithm  [ alg : apbfsee ] iterates until no more augmenting paths are found , thereby guaranteeing a maximum matching .",
    "the inner loop is responsible from completing the breadth - first - search of the augmenting paths .",
    "a single iteration of this loop corresponds to a level of bfs .",
    "the inner loop iterates until all shortest augmenting paths are found .",
    "then , the edges in these shortest augmenting paths are alternated inside function . unlike the sequential hk algorithm ,",
    "apsbdoes not find a maximal set of augmenting paths .    by removing the lines 9 and 10 of algorithm  [ alg : apbfsee ] ,",
    "another matching algorithm is obtained .",
    "this method will continue with the bfss until all possible unmatched rows are found ; it can be therefore considered as the gpu implementation of the hkdw algorithm .",
    "this variant is called apfb .    @xmath31 ; +    we propose two implementations of the bfs kernel .",
    "algorithm  [ alg : kernelbfs ] is the first one .",
    "the bfs kernel is responsible from a single level bfs expansion .",
    "that is , it takes the set of vertices at a bfs level and adds the union of the unvisited neighbors of those vertices as the next level of vertices .",
    "initially , the input @xmath32 filled with @xmath33 = { { \\sc l_0}\\xspace } { } - 1 $ ] if @xmath27 > -1 $ ] and @xmath33 = { { \\sc l_0}\\xspace}{}$ ] if @xmath27 = -1 $ ] by a simple initbfsarray kernel ( @xmath34 denotes bfs start level ) .",
    "the gpu threads partition the columns vertices in a single dimension .",
    "each thread with i d @xmath35 is assigned a number of columns which is obtained via the following function :    once the number of columns are obtained , the threads traverse their first assigned column vertex .",
    "the indices of the columns assigned to a thread differ by @xmath36 to allow coalesced global memory accesses .",
    "threads traverse the neighboring row vertices of the current column , if its bfs level is equal to the current @xmath37 .",
    "if a thread encounters a matched row during the traversal , its matching column is retrieved .",
    "if the column is not traversed yet , its @xmath37 is marked on @xmath32 .",
    "on the other hand , when a thread encounters an unmatched row , an augmenting path is found . in this case , the match of the neighbor row is set to @xmath38 , and this information is used by later .",
    "@xmath39 ; +    [ !",
    "htb ]   and @xmath40 are matched ; others are not .",
    "two augmenting paths starting from @xmath41 are possible.,title=\"fig:\",scaledwidth=40.0% ]    algorithm  [ alg : kernelswap_edges ] gives the description of the function .",
    "this kernel alternates the matching edges with unmatching edges of the augmenting paths found ; some of those paths end up being augmenting ones and some are only partially alternated . here",
    ", each thread is assigned a number of rows . since @xmath24 of an unmatched row ( that is also an endpoint of an augmenting path )",
    "has been set to @xmath38 in the bfs kernel , only the threads whose row vertex s match is @xmath38 start .",
    "since there might be several augmenting paths for an unmatched column , race conditions while writing on @xmath25 and @xmath24 arrays are possible .",
    "such a race condition might cause infinite loops ( inner while loop ) or inconsistencies , if care is not taken .",
    "we prevent these by checking the predecessor of a matched row ( line-8 ) .",
    "for example , in fig .",
    "[ fig : swapedges ] , two different augmenting paths that end with @xmath42 and @xmath43 are found for @xmath41 . if the thread of @xmath42 starts before the thread of @xmath43 in , the match of @xmath40 will be updated to @xmath42 ( line-10 ) .",
    "then , @xmath43 s thread will read @xmath44 of @xmath40 as @xmath42 ( line-7 ) .",
    "this would cause an infinite loop without the check at line-8 .",
    "inconsistencies may occur when the threads of @xmath42 and @xmath43 are in the same warp . in this case , the if - check will not hold for both threads , and their row vertices will be written on @xmath25 ( line-10 ) . since only one thread will be successful at writing , this will cause an inconsistency .",
    "such inconsistencies are fixed by fixmatching kernel which implements : @xmath45 \\gets -1$}$ ] for any @xmath28 satisfying @xmath46 \\neq r$ ] .",
    "algorithm  [ alg : kernelbfs2 ] gives the description of a slightly different bfs kernel function .",
    "this function takes @xmath47 array as an extra argument .",
    "initially , the root array is filled with @xmath48 = 0 $ ] if @xmath27 > -1 $ ] , and @xmath48 = c$ ] if @xmath27 = -1 $ ] .",
    "this array holds the root ( as the index of the column vertex ) of an augmenting path , and this information is transferred down during bfs .",
    "whenever an augmenting path is found , the entry in @xmath32 for the root of the augmenting path is set to @xmath49 .",
    "this information is used at the beginning of bfs kernel .",
    "no more bfs traversals is done , if an augmenting path is found for the root of the traversed column vertex .",
    "therefore , while the method increases the global memory accesses by introducing an extra array , it provides an early exit mechanism for bfs .",
    "@xmath31 ; +    we further improve gpubfs - wr by making use of the arrays @xmath47 and @xmath32 .",
    "bfs kernels might find several rows to match with the same unmatched column , and set @xmath50 $ ] to @xmath38 for each .",
    "these cause to start from several rows that can be matched with the same unmatched column .",
    "therefore , it may perform unnecessary alternations , until these augmenting paths intersect . conflicts may occur at these intersection points ( which are then resolved with fixmatchingfunction ) . by choosing @xmath34 as 2",
    ", we can limit the range of the values that @xmath32 takes to positive numbers . therefore , by setting the @xmath32 to @xmath51 at line 19 of algorithm  [ alg : kernelbfs2 ] , we can provide more information to the function . with this , can determine the beginning and the end of an augmenting path , and it can alternate only among the correct augmenting paths .",
    "apsb - gpubfs - wr  ( and function used together ) includes these improvements .",
    "however , they are not included in apfb - gpubfs - wr since they do not improve its performance .",
    "the running time of the proposed implementations are compared against the sequential hk and pfp implementations  @xcite , and against the multicore parallel implementations p - pfp , p - dbfs , and p - hk  @xcite .",
    "the cpu implementations are tested on a computer with 2.27ghz dual quad - core intel xeon cpus with 2-way hyper - threading and 48 gb main memory .",
    "the algorithms are implemented in c++ and openmp .",
    "the gpu implementations are tested on nvidia tesla c2050 with usable 2.6 gb of global memory .",
    "c2050 is equipped with 14 multiprocessors each containing 32 cuda cores , totaling 448 cuda cores .",
    "the implementations are compiled with gcc-4.4.4 , cuda-4.2.9 and -o2 optimization flag . for the multicore algorithms ,",
    "8 threads are used . a standard heuristic ( called the cheap matching ,",
    "see  @xcite ) is used to initialize all tested algorithms .",
    "we compare the running time of the matching algorithms after this common initialization .",
    "two different main algorithms apfband apsbcan use two different bfs kernel functions ( gpubfs and gpubfs - wr ) .",
    "moreover , each of these algorithms can have two versions ( i ) ct : uses a constant number of threads with fixed number of grid and block size ( @xmath52 ) and assigns multiple vertices to each thread ; ( ii ) mt : tries to assign one vertex to each thread .",
    "the number of threads used in the second version is chosen as @xmath53 where @xmath54 is the number of columns , and @xmath55 is the maximum number of threads of the architecture .",
    "therefore , we have eight gpu - based algorithms .",
    "the algorithms are run on bipartite graphs corresponding to 70 different matrices from variety of classes at ufl matrix collection  @xcite .",
    "we also permuted the matrices randomly by rows and columns and included them as a second set ( labeled rcp ) .",
    "these permutations usually render the problems harder for the augmenting - path - based algorithms  @xcite . for both sets",
    ", we report the performance for a smaller subset which contains those matrices in which at least one of the sequential algorithms took more than one second .",
    "we call these sets o_s1 ( 28 matrices ) and rcp_s1 ( 50 matrices ) .",
    "we also have another two subsets called o_hardest20 and rcp_hardest20 that contain the set of 20 matrices on which the sequential algorithms required the longest running time .",
    "[ tab : gputime ]    [ ! htb ]    first , we compare the performance of the proposed gpu algorithms .",
    "table  [ tab : gputime ] shows the geometric mean of the running time on different sets . as we see from the table , using constant number of threads ( ct )",
    "always increases the performance of an algorithm , since it increases the granularity of the work performed by each thread .",
    "gpubfs - wr is always faster than gpubfs .",
    "this is because of the unnecessary bfs traversals in the gpubfs algorithm .",
    "gpubfs can not determine whether an augmenting path has already been found for an unmatched column , therefore it will continue to explore .",
    "this unnecessary bfs traversals not only increase the bfs time , but also reduce the likelihood of finding an augmenting path for other unmatched columns .",
    "moreover , the scheme turns out to be more suitable for apfbthan apsb , in which case it can augment along more paths ( there is a larger set of possibilities ) .",
    "for example , figs .",
    "[ fig : bfs_ham ] and  [ fig : bfs_del ] show the number of bfs iterations and the number of bfs levels in each iteration for , respectively , hamrle3 and delanuay_n23 graphs .",
    "as clearly seen from both of the figures , apfbvariants converges in smaller number of iterations than apsbvariants ; and for most of the graphs , the total number of bfs kernel calls are less for apfb(as in fig .  [",
    "fig : bfs_ham ] ) .",
    "however , for a small subset of the graphs , although the augmenting path exploration of apsbconverges in larger number of iterations , the numbers of the bfs levels in each iterations are much less than apfb(as in fig .",
    "[ fig : bfs_del ] ) . unlike the general case , apsboutperforms apfbin such cases . since apfbusing gpubfs - wr and ct almost always obtains the best performance",
    ", we only compare the performance of this algorithm with other implementations in the following .    [ !",
    "htb ]    figures  [ fig : subos ] and  [ fig : subrcs ] give the log - scaled speedup profiles of the best gpu and multicore algorithms on the original and permuted graphs .",
    "the speedups are calculated with respect to the fastest of the sequential algorithms pfp and hk ( on the original graphs hk was faster ; on the permuted ones pfp was faster ) .",
    "a point @xmath56 in the plots corresponds to the probability of obtaining at least @xmath57 speedup is @xmath58 .",
    "as the plots show , the gpu algorithm has the best overall speedup .",
    "it is faster than the sequential hk algorithm for @xmath59 of the original graphs , while it is faster than pfp on @xmath60 of the permuted graphs .",
    "p - dbfsobtains the best performance among the multicore algorithms .",
    "however , its performance degrades on permuted graphs .",
    "although p - pfpis more robust than p - dbfsto permutations , its overall performance is inferior to that of p - dbfs .",
    "p - hkis outperformed by the other algorithms in both sets .",
    "[ ! htb ]    figures  [ fig : subop ] and  [ fig : subrcp ] show the performance profiles of the gpu and multicore algorithms .",
    "a point @xmath56 in this plot means that with @xmath58 probability , the algorithm obtains a performance that is at most @xmath61 times worse than the best running time .",
    "the plots clearly show the separation among the gpu algorithm and the multicore ones , especially for original graphs and for @xmath62 for the permuted ones , thus marking gpu as the fastest in most cases . in particular , gpu algorithm obtains the best performance in @xmath63 of the original graphs , while this ratio increases to @xmath64 for the permuted ones .    [ !",
    "htb ]     figure  [ fig : overallspeedup ] gives the overall speedups .",
    "the proposed gpu algorithm obtains average speedup values of at least @xmath65 and @xmath66 on , respectively , original and permuted graphs . the speedups increase for the hardest instances , where the gpu algorithm achieves @xmath67 and @xmath68 speedup , respectively , on original and permuted graphs .",
    "table  [ tab : actualrun ] gives the actual running time for o_hardest20 sets for the best gpu and multicore algorithms , together with the sequential algorithms ( the running time of all mentioned algorithms on the complete set of graphs can be found at http://bmi.osu.edu/hpc/software/matchmaker2/maxcardmatch.html ) .",
    "as seen from this table , except six instances among the original graphs and another two among the permuted graphs , the gpu algorithm is faster than the best sequential algorithm .",
    "it is also faster than the multicore ones in all , except five original graphs .",
    "[ tab : actualrun ]",
    "we proposed a parallel bfs based gpu implementation of maximum cardinality matching algorithm for bipartite graphs .",
    "we presented experiments on various datasets , and compared the performance of the proposed gpu implementation against sequential and multicore algorithms .",
    "the experiments showed that the proposed gpu implementations are faster than the existing parallel multicore implementations .",
    "the speedups achieved with respect to well - known sequential implementations varied from @xmath69 to @xmath70 , averaging @xmath68 on a set of 20 hardest problems with respect to the fastest sequential algorithm .",
    "a gpu is a restricted memory device .",
    "although , an out - of - core or distributed - memory type algorithm is amenable when the graph does not fit into the device , a direct implementation of these algorithms will surely not be efficient .",
    "we plan to investigate the techniques to obtain good matching performance for extreme - scale bipartite graphs on gpus ."
  ],
  "abstract_text": [
    "<S> we design , implement , and evaluate gpu - based algorithms for the maximum cardinality matching problem in bipartite graphs . </S>",
    "<S> such algorithms have a variety of applications in computer science , scientific computing , bioinformatics , and other areas . </S>",
    "<S> to the best of our knowledge , ours is the first study which focuses on gpu implementation of the maximum cardinality matching algorithms . </S>",
    "<S> we compare the proposed algorithms with serial and multicore implementations from the literature on a large set of real - life problems where in majority of the cases one of our gpu - accelerated algorithms is demonstrated to be faster than both the sequential and multicore implementations . </S>"
  ]
}