{
  "article_text": [
    "the storage capacity is one of the most important characteristics of a neural network .",
    "it is the maximal number of random input - output patterns per input entry that a network is able to correctly classify with probability one .",
    "this quantity is independent of the algorithm used to learn the weights of the network ; it only depends on its architecture",
    ". we will refer to it as the _ architecture storage capacity _ @xmath0 in the following , to distinguish it from the algorithm - dependent one , hereafter called _ algorithm storage capacity _ , @xmath1 .",
    "the simplest neural network , the perceptron , has the inputs directly connected to the output .",
    "geometrical arguments  @xcite and a statistical mechanics calculation  @xcite determined that @xmath2 .",
    "several perceptron learning algorithms , like the adatron  @xcite , are known to achieve such storage capacity .",
    "the capacity can be increased using networks with more complicated architectures .",
    "the next one on increasing complexity is the extensively studied  @xcite monolayer perceptron ( mlp ) , which has @xmath3 `` hidden ''",
    "perceptrons connected to the output unit . as a mlp can store any function of its inputs , provided that the number of hidden units is adequate , it is not worth to consider more complex architectures for the storage problem .    given an input pattern , the hidden units states define a @xmath3-dimensional vector , the pattern s _ internal representation _ ( ir ) .",
    "the network s output is a function of the ir . in the following ,",
    "we consider binary neurons , of states @xmath4 , and focus on the parity machine , whose output is the product of the @xmath3 components of the ir .",
    "the main problem when training mlps is that the irs of the input patterns are unknown .",
    "it has been proposed  @xcite to build the hidden layer using a constructive procedure , called tilinglike learning algorithm ( tla ) , in which the hidden perceptrons are included one after the other and trained to correct the learning errors of the preceding unit .",
    "as each unit can at least correct one error , convergence is ensured  @xcite .",
    "it is straightforward to show that the tla generates a parity machine  @xcite , but the number of included hidden units depends crucially on the performance of the perceptron learning algorithm used to train them .",
    "geometric arguments  @xcite and a statistical mechanics replica calculation  @xcite , showed that the architecture storage capacity of the parity machine in the limit of a large number of hidden units @xmath3 is @xmath5 .",
    "however , it was not clear whether this storage capacity could be actually achieved with a learning algorithm . in this paper , we show that the tilinglike learning algorithm ( tla ) can reach a storage capacity close to the architecture storage capacity , provided that the hidden perceptrons are trained with an appropriate learning algorithm . in section [ sec.tla ]",
    "we describe more precisely the setting and the tla .",
    "the analytical expression of the algorithm storage capacity in the limit of large @xmath3 is determined in section [ sec.alpha ] , where we show that the learning algorithm used to train the hidden units must satisfy stringent condition for the tla to converge with a finite number of hidden units .",
    "the results presented in section [ sec.algo ] show that these conditions rule out some perceptron learning algorithms , like the adatron .",
    "the conclusions are presented in section [ sec.concl ] .",
    "let us assume a training set @xmath6 of @xmath7 input - output patterns .",
    "the inputs @xmath8 are random gaussian @xmath9-dimensional vectors with zero mean and unit variance in each direction .",
    "the corresponding outputs @xmath10 are the learning targets .",
    "their values @xmath11 are randomly selected with probability : @xmath12 with @xmath13 .",
    "the rle of the bias @xmath14 introduced in ( [ proba ] ) will become clear in the following .",
    "the probability of the targets @xmath15 is unbiased .",
    "the tla constructs the parity machine by including successive perceptrons in the hidden layer .",
    "each unit is connected to the input @xmath16 through weights @xmath17 where @xmath18 is a threshold .",
    "the inputs are classified through @xmath19 .",
    "thus , a perceptron separates linearly the input space with a hyperplane orthogonal to @xmath20 ( we assume @xmath21 ) at a distance @xmath18 to the origin .",
    "the weights and the threshold are learned through the minimization of a cost function : @xmath22 where the potential @xmath23 is the contribution of each pattern to the cost , and @xmath24 is the distance of the pattern to the hyperplane .    within the tla heuristics , the first perceptron @xmath25 is trained to learn targets @xmath26 .",
    "after learning , its weights are @xmath27 ; its training error is @xmath28 where @xmath29 is the heaviside function and @xmath30 the perceptron s output to pattern @xmath31 .",
    "if @xmath32 , the training set is correctly classified ; the tla stops with only one simple perceptron .",
    "otherwise , a new perceptron is introduced .",
    "the successive perceptrons @xmath33 are trained to learn training sets @xmath34 with targets @xmath35 , that is , @xmath36 if the pattern @xmath37 is correctly classified by the previous perceptron and @xmath38 otherwise . if the perceptron learning algorithm is correctly chosen it can be shown that the successive training errors @xmath39 are strictly decreasing  @xcite .",
    "thus , the tla procedure necessarily converges to a mlp with @xmath3 units , where the @xmath40 perceptron is the first one to meet the condition @xmath41 .",
    "then , the product @xmath42 gives the correct output to the patterns of the training set @xmath43 .",
    "the algorithm storage capacity of the tla , @xmath44 , is simply the inverse function of @xmath45 , the average number of perceptrons typically included by the tla when the training set has a size @xmath46 . in order to determine @xmath45 , consider the @xmath47 hidden unit : the probability of its targets @xmath48 depends on the training error @xmath49 of the previous perceptron .",
    "although there exist some correlations between the outputs @xmath50 , due to the correlations in the weights of the successive perceptrons , in the limit of a large training sets ( @xmath51 ) they may be neglected  @xcite .",
    "thus , we may assume that the targets @xmath50 are independently drawn with probability ( [ proba ] ) , with a bias @xmath49 .",
    "then , the successive training errors satisfy a simple recursive relation  @xcite : @xmath52 where @xmath53 is the training error of a simple perceptron trained with a training set of size @xmath54 and biased targets @xmath55 drawn with a probability @xmath56 given by ( [ proba ] ) .",
    "the number @xmath3 of perceptrons necessary to correctly classify the initial training set satisfies  @xcite : @xmath57 where @xmath58 stands for @xmath59 and the symbol @xmath60 for the composition of functions . in the limit of a large @xmath54 ,",
    "the training error @xmath59 is close to @xmath14 and the number of simple perceptrons @xmath45 is large .",
    "it is thus possible to use the continuum limit : @xmath61 with @xmath62 .",
    "after integration of this differential equation we obtain  @xcite the typical number of hidden units introduced by the tla : @xmath63 @xmath45 depends on the specific cost function ( [ cost ] ) used to train the perceptrons through @xmath64 , which is the training error of a simple perceptron learning a training set with biased targets .",
    "in this section we determine the perceptron s training error @xmath64 for biased training sets , in the thermodynamic limit ( @xmath65 with @xmath46 fixed ) . for different possible choices of the potential @xmath66 in ( [ cost ] ) ,",
    "we deduce the number of hidden units , @xmath67 and the algorithm storage capacity of the tla .",
    "only the main results are presented here , the interested reader can find the details in  @xcite .",
    "the adatron potential is @xmath68 , where @xmath69 is a positive parameter called stability .",
    "all the patterns with negative @xmath70 and those with @xmath71 but closer than @xmath69 to the hyperplane ( which are correctly classified ) contribute to the cost .",
    "the training error @xmath64 is obtained through a replica calculation assuming replica symmetry ( rs ) , which can be shown to hold for all @xmath54 .",
    "it turns out that for fixed @xmath69 , in the limit of large @xmath54 , @xmath72 . as a consequence ,",
    "the constraint that the successive training errors are strictly decreasing , necessary for the convergence of the tla , is not satisfied .",
    "this problem can be circumvented at the price of considering @xmath69 as a free parameter , and minimizing the training error with respect to it . in that case ,",
    "@xmath73 and the algorithm storage capacity is @xmath74 in the limit of large @xmath54 .",
    "the potential of the gardner - derrida ( gd ) cost function  @xcite is @xmath75 .",
    "the hypothesis of rs is incorrect for this potential , and the obtained value of @xmath76 is a lower bound to the true training error .",
    "consequently , the replica calculation allows only to determine an upper bound to @xmath44 . if @xmath77 , the cost is nothing else but the number of misclassified patterns .",
    "it gives the lowest bound to @xmath76 . in the limit of large training set size @xmath54 , we obtain : @xmath78 where @xmath79 satisfies @xmath80/[2 \\ , ( \\cosh a - a \\sinh a - 1)]$ ] .",
    "this leads to @xmath81 and @xmath82 , larger than @xmath83 , probably due to the failure of the rs hypothesis .    in order to obtain a lower bound to @xmath83 we used the kuhn - tucker cavity method  @xcite , which gives an upper bound to @xmath76 . as a result of both calculations ,",
    "we can bound @xmath84 : @xmath85 on view of this result , we expect that the algorithm storage capacity of the tla behaves like @xmath86 with @xmath87 .",
    "a calculation with one step of replica symmetry breaking would give an estimate of the exponent @xmath88 .",
    "since the rs solution gives a better approximation of the training error than the kuhn - tucker cavity method , we expect the exponent @xmath88 to be close to @xmath89 , leading to an algorithm storage capacity close to the architecture s capacity .",
    "this result shows that the tla may build a nearly optimal network .    in order to improve the robustness against noise in the data , it is usually useful to impose some finite stability @xmath69 to the patterns .",
    "the corresponding gd potential is @xmath90 . as with @xmath77 , here",
    "also the rs solution is unstable .",
    "the bounds on @xmath91 deduced from the results for @xmath76 obtained with the rs hypothesis and the kuhn - tucker cavity method  @xcite , give : @xmath92 strikingly , imposing a finite stability @xmath69 has an important effect on the algorithm storage capacity , which in this case behaves as @xmath86 with @xmath93 .",
    "the prefactors of the bounds of @xmath94 are @xmath69-dependent and they both diverge for @xmath95 . the exponent @xmath88 is independent of @xmath69 for finite @xmath69 but differs from the one corresponding to @xmath96 .",
    "we determined analytically the storage capacity of the tilinglike learning algorithm for the parity machine , a constructive procedure generating a monolayer perceptron of binary hidden units .",
    "a training set of input - output examples is used to determine the number of hidden units , which are introduced one after the other .",
    "these are simple perceptrons that have to learn their weights using increasingly biased target distributions .",
    "we have shown that the storage capacity of the tla depends crucially on the learning errors of the successively introduced perceptrons .",
    "the properties of the algorithm used to train the latter have thus dramatic consequences on the size of the hidden layer generated by the tla , and may even hinder the convergence to a finite size network .",
    "this arises , in particular , if the perceptrons are trained with the adatron algorithm unless the stability is adapted to the successive targets biases .",
    "the smallest network is obtained using the gardner - derrida cost function with vanishing stability , which corresponds to minimizing the training error .",
    "based on the results obtained within the replica symmetry hypothesis , and those using the kuhn - tucker cavity method , we expect a supra - linear storage capacity @xmath97 with @xmath98 , very close to the theoretical capacity corresponding to the architecture considered ."
  ],
  "abstract_text": [
    "<S> the storage capacity of an incremental learning algorithm for the parity machine , the tilinglike learning algorithm , is analytically determined in the limit of a large number of hidden perceptrons . </S>",
    "<S> different learning rules for the simple perceptron are investigated . </S>",
    "<S> the usual gardner - derrida one leads to a storage capacity close to the upper bound , which is independent of the learning algorithm considered . </S>"
  ]
}