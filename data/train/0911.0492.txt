{
  "article_text": [
    "we would like to find a solution to the sparsest recovery problem with noise @xmath2 here , @xmath3 specifies the noise level , @xmath4 is an @xmath5-by-@xmath6 matrix with @xmath7 , and @xmath8 is the number of nonzero entries of @xmath9 .",
    "this problem comes up in fields such as image processing @xcite , seismics @xcite , astronomy @xcite , and model selection in regression @xcite .",
    "since is known to be ill - posed and np - hard @xcite , various convex , @xmath10-relaxed formulations are often used .",
    "relaxing the zero - norm in gives the basis pursuit denoising ( bpdn ) problem @xmath11 the special case of @xmath12 is the basis pursuit problem @xcite .",
    "two other commonly used @xmath10-relaxations are the lasso problem @xcite @xmath13 and the penalized least - squares problem @xmath14 proposed by chen , donoho , and saunders @xcite .",
    "a large amount of work has been done to show that these formulations give an effective approximation of the solution to ; see @xcite . in particular , under certain conditions on the sparsity of the solution @xmath9 , @xmath9 can be recovered exactly provided that @xmath4 satisfies the _ restricted isometry property _ ( rip ) .",
    "there are a wide variety of algorithms which solve the bp@xmath15 , qp@xmath16 , and ls@xmath17 problems .",
    "refer to section  [ sec : solvers ] for descriptions of some of the current algorithms .",
    "our work has been motivated by the accuracy and speed of the recent solvers nesta and spgl1 . in @xcite",
    ", nesterov presents an algorithm to minimize a smooth convex function over a convex set with an optimal convergence rate . an extension to the nonsmooth case",
    "is presented in @xcite .",
    "nesta solves the bp@xmath15 problem using the nonsmooth version of nesterov s work .    for appropriate parameter choices of @xmath18 and @xmath19 , the solutions of bp@xmath15 , qp@xmath16 , and ls@xmath17 coincide @xcite .",
    "although the exact dependence is usually hard to compute @xcite , there are solution methods which exploit these relationships .",
    "the matlab solver spgl1 is based on the pareto root - finding method @xcite which solves bp@xmath15 by approximately solving a sequence of ls@xmath17 problems . in spgl1",
    ", the ls@xmath17 problems are solved using a spectral projected - gradient ( spg ) method .",
    "while we are ultimately interested in solving the bpdn problem in , our main result is an algorithm for solving the lasso problem .",
    "our algorithm , nesta - lasso ( cf .",
    "algorithm  [ alg : nesta - lasso ] ) , effectively uses nesterov s work to solve the lasso problem .",
    "additionally , we propose a modification of nesterov s work which results in the local linear convergence of nesta - lasso under reasonable assumptions .",
    "finally , we show that replacing the spg method in the pareto root - finding procedure , used in spgl1 , with our nesta - lasso method leads to an effective method for solving bp@xmath15 .",
    "we call this modification parnes and compare its efficacy with the state - of - the - art solvers presented in section  [ sec : solvers ] .      in section  [ sec : nesta - lasso ] , we present and describe the background of nesta - lasso .",
    "we show in section  [ sec : opt ] that , under some reasonable assumptions , nesta - lasso can exhibit linear convergence . in section  [ sec : parnes ] , we describe the pareto root - finding procedure behind the bpdn solver spgl1 and show how nesta - lasso can be used to solve a subproblem . section 5 describes some of the available algorithms for solving bpdn and the equivalent qp@xmath16 problem .",
    "lastly , in section 6 , we show in a series of numerical experiments that using nesta - lasso in spgl1 to solve bpdn is comparable with current competitive solvers .",
    "we present the main parts of our method to solve the lasso problem . our algorithm , nesta - lasso ( cf .",
    "algorithm  [ alg : nesta - lasso ] ) , is an application of the accelerated proximal gradient algorithm of nesterov @xcite outlined in section  [ sec : nesterov ] .",
    "additionally , we have a prox - center update improving convergence which we describe in section  [ sec : opt ] . in each iteration",
    ", we use the fast @xmath20-projector of duchi , shalev - shwartz , singer , and chandra @xcite given in section  [ sec:1proj ] .",
    "let @xmath21 be a convex closed set .",
    "let @xmath22 be smooth , convex and lipschitz differentiable with @xmath23 as the lipschitz constant of its gradient , i.e.@xmath24 nesterov s accelerated proximal gradient algorithm iteratively defines a sequence @xmath25 as a judiciously chosen convex combination of two other sequences @xmath26 and @xmath27 , which are in turn solutions to two quadratic optimization problems on @xmath28 .",
    "the sequence @xmath27 involves a strongly convex _ prox - function _ , @xmath29 , which satisfies @xmath30 for simplicity ,",
    "we have chosen the right hand side of with @xmath31 as our prox - function throughout this paper .",
    "the @xmath32 in the prox - function is called the _ prox - center_. with this prox - function , we have :    @xmath33+\\frac{l}{2}\\lvert z - c\\rvert_{2}^{2},\\\\ x_{k }   &   = \\frac{2}{k+3}z_{k}+\\frac{k+1}{k+3}y_{k}.\\end{aligned}\\ ] ]    nesterov showed that if @xmath34 is the optimal solution to @xmath35 then the iterates defined above satisfy@xmath36 an implication is that the algorithm requires @xmath37 iterations to bring @xmath38 to within @xmath39 of the optimal value .    initialize @xmath40 ; compute @xmath41 and @xmath42 ; @xmath43 ; @xmath44+\\frac{l}{2}\\lvert z - c\\rvert_{2}^{2}$ ] ; @xmath45 ;    in @xcite , nesterov extends his work to minimize nonsmooth convex functions @xmath46 .",
    "nesterov shows that one can obtain the minimum by applying his algorithm for smooth minimization to a smooth approximation @xmath47 of @xmath46 .",
    "since @xmath48 is shown to have lipschitz constant @xmath49 , if @xmath50 is chosen to be proportional to @xmath51 , it takes @xmath52 iterations to bring @xmath53 within @xmath51 of the optimal value .",
    "the recent algorithm nesta solves bp@xmath54 using nesterov s algorithm for nonsmooth minimization .",
    "our algorithm , nesta - lasso , solves ls@xmath55 using nesterov s smooth minimization algorithm .",
    "we are motivated by the accuracy and speed of nesta , and the fact that smooth version of nesterov s algorithm has a faster convergence rate than the nonsmooth version .",
    "we apply nesterov s accelerated proximal gradient method in algorithm  [ alg : nesta ] to the lasso problem ls@xmath17 , and in each iteration , we use the fast @xmath20-projector @xmath56 described in the next section .",
    "the pseudocode for this is given in algorithm  [ alg : nesta - lasso ] .",
    "we make one slight improvement to algorithm  [ alg : nesta ] , namely , we update our prox - centers from time to time .",
    "in fact , we will see in section  [ sec : opt ] that the prox - centers may be updated in an optimal fashion and that this leads to linear convergence under a suitable application of rip ( see corollary  [ cor : opt ] for details ) .    in our case , @xmath57 , @xmath58 , and @xmath28 is the one - norm ball @xmath59 .",
    "the initial point @xmath40 is used as the prox - center @xmath32 . to compute the iterate @xmath26",
    ", we have @xmath60 where @xmath61 returns the projection of the vector @xmath62 onto the one - norm ball of radius @xmath19 . by similar reasoning ,",
    "computing @xmath27 can be shown to be equivalent to computing @xmath63    @xmath64 , @xmath65 , @xmath66 , @xmath67 , @xmath68 ; @xmath69 @xmath70 ; @xmath71 ; @xmath72 ; @xmath45 ; @xmath73 ; @xmath74 ; @xmath75 ; @xmath76 ; @xmath77 ;      the one - projector , @xmath78 , is used twice in each iteration of algorithm  [ alg : nesta - lasso ] .",
    "we briefly describe the algorithm of duchi , shalev - schwartz , singer , and chandra @xcite for fast projection to an @xmath20-ball in high - dimension .",
    "a similar algorithm is presented in @xcite .",
    "the algorithm costs @xmath79 in the worst case , but it has been shown to cost much less experimentally @xcite .",
    "note that the two calls to one - projector in each iteration can be reduced to one call with results in tseng s paper @xcite .",
    "however , we make no change to our algorithm due to the low cost of the one - projector .",
    "consider the projection of an @xmath6-vector @xmath32 onto the one - norm ball @xmath80 .",
    "this is given by the minimization problem @xmath81 without loss of generality , the symmetry of the one - norm ball allows us to assume that @xmath32 has all nonnegative entries . assuming the coefficients of the vector @xmath32 are ordered from largest to smallest , the solution @xmath82 is given by @xmath83 where @xmath84 is the largest index such that @xmath85 .",
    "please refer to @xcite for more details on the implementation .",
    "in nesta - lasso , we update the prox - center , @xmath32 , in a provably optimal way which yields a local linear convergence rate .",
    "we analyze the case where the solution @xmath34 is @xmath86-sparse . the case where @xmath34 is approximately sparse",
    "has not been analyzed ; we plan to do this for future work .",
    "nesterov has shown that when his accelerated proximal gradient algorithm is applied to minimize the objective function @xmath46 , the @xmath84th iterate @xmath26 and the minimizer @xmath34 satisfy@xmath87 where @xmath23 is the lipschitz constant for @xmath88 and @xmath32 is the prox - center @xcite .    in our case , @xmath89 where the underdetermined matrix @xmath4 is understood to represent",
    "a dictionary of atoms , usually sampled via some measurements .",
    "we will assume that @xmath4 satisfies the _ restricted isometry property _",
    "@xmath90 of order @xmath91 as described in @xcite .",
    "namely , there exists a constant @xmath92 such that@xmath93 whenever @xmath94 .",
    "since the rip helps ensure that the solution to is closely approximated by the solution to @xcite , and we are ultimately interested in solving , this is a reasonable assumption .",
    "additionally , we assume the iterates @xmath26 are @xmath86-sparse when sufficiently close to @xmath34 .",
    "this is reasonable since each iteration of nesta - lasso involves projecting the current iteration onto a @xmath95-norm ball . due to the geometry of the projection",
    ", there is a high likelihood that our assumption will hold . under the rip and the assumption that @xmath96 is @xmath86-sparse , it follows by theorem  3.2 in @xcite that the lasso problem has a unique solution .",
    "since the one - norm ball is compact , this implies that @xmath26 converges to @xmath96 .",
    "let @xmath97 .",
    "we have@xmath98 to see the first inequality , let @xmath99 for @xmath100 $ ] . due to the convexity of the one - norm ball , @xmath101 is feasible . since @xmath34 is the minimum , for any @xmath100 $ ] , @xmath102 thus , @xmath103 .",
    "the second inequality follows from .",
    "then from , we have@xmath104 putting everything together gives@xmath105 the above relation and suggest that we can speed up algorithm  [ alg : nesta ] by updating the prox - center , @xmath32 , with the current iterate @xmath26 every @xmath84 steps . with our assumptions ,",
    "we prove in the following theorem that there is an optimal number of such steps .",
    "allow the iterates to be represented by @xmath106 where @xmath107 is the number of times the prox - center has been changed ( the outer iteration ) and @xmath84 is number of iterations after the last prox - center change ( the inner iteration ) .",
    "[ thm : opt]suppose @xmath4 satisfies the _ restricted isometry property _ of order @xmath91 , the solution @xmath34 is @xmath86-sparse , and the iterates @xmath106 are eventually @xmath86-sparse .",
    "then for each @xmath107 , @xmath108 where @xmath109 and @xmath110 is the base of the natural logarithm .",
    "moreover , the total number of iterations , @xmath111 , to get @xmath112 is minimized with this choice of @xmath113 .",
    "first observe that implies @xmath114 when@xmath115 this relation allows us to choose @xmath84 to minimize the product @xmath116 .",
    "since@xmath117 taking derivative of the expression on the right shows that @xmath116 is minimized when@xmath118 where @xmath110 is the base of the natural logarithm .",
    "thus , @xmath119    the assumption that the iterates @xmath106 will eventually be @xmath86-sparse is reasonable precisely because we expect the lasso relaxation to be a good proxy for . in other words ,",
    "we expect our solutions to to be sparse .",
    "of course this argument is merely meant to be a heuristic ; a more rigorous justification along the lines of @xcite may be possible and is in our future plans .",
    "let @xmath113 be as in .",
    "for each prox - center change , @xmath107 , perform @xmath113 inner iterations , and let @xmath120 be the output to the @xmath107th outer iteration .",
    "an immediate consequence of theorem  [ thm : opt ] is that the relative decrease after @xmath113 steps of the inner iteration in algorithm  [ alg : nesta - lasso ] is @xmath121 , i.e.@xmath122 and in general , this is the best possible .",
    "[ cor : opt ] if @xmath4 satisfies the _ restricted isometry property _ of order @xmath91 , the solution @xmath34 is @xmath86-sparse , and the iterates @xmath123 are eventually @xmath86-sparse , then algorithm  [ alg : nesta - lasso ] is linearly convergent .    in our experiments , there are some cases where updating the prox - center will eventually cause the duality gap to jump to a higher value than the previous iteration .",
    "this can cause the algorithm to run for more iterations than necessary .",
    "a check is added to prevent the prox - center from being updated if it no longer helps .    in tables",
    "[ table3 ] and  [ table4 ] , we give some results showing that updating the prox - center is effective when using nesta - lasso to solve the lasso problem .",
    "llll number of rows of @xmath4 & number of columns of @xmath4 & @xmath19 & @xmath124 + 100 & 256 & 6.28 & 69 + 200 & 512 & 12.6 & 77 + 400 & 1024 & 25.1 & 157 +    llll number of rows of @xmath4 & number of columns of @xmath4 & @xmath19 & @xmath124 + 100 & 256 & 6.28 & 37 + 200 & 512 & 12.6 & 47 + 400 & 1024 & 25.1 & 45 +",
    "in applications where the noise level of the problem is approximately known , it is preferable to solve bp@xmath15 .",
    "the pareto root - finding method used by van  den  berg and friedlander @xcite interprets bp@xmath15 as finding the root of a single - variable nonlinear equation whose graph is called the pareto curve .",
    "their implementation of this approach is called spgl1 . in spgl1 , an inexact version of newton s method",
    "is used to find the root , and at each iteration , an approximate solution to the lasso problem , ls@xmath17 , is found using an spg approach .",
    "refer to @xcite for more information on the inexact newton method . in section  [ sec : num ] , we show experimentally that using nesta - lasso in place of the spg approach can lead to improved results .",
    "we call this version of the pareto root - finding method , parnes . the pseudocode of parnes is given in algorithm  [ alg : parnes ] .",
    "suppose @xmath4 and @xmath125 are given , with @xmath126 .",
    "the points on the pareto curve are given by @xmath127 where @xmath128 , @xmath129 , and @xmath130 solves ls@xmath17 .",
    "the pareto curve gives the optimal trade - off between the 2-norm of the residual and 1-norm of the solution to ls@xmath17 .",
    "it can also be shown that the pareto curve also characterizes the optimal trade - off between the 2-norm of the residual and 1-norm of the solution to bp@xmath15 .",
    "refer to @xcite for a more detailed explanation of these properties of the pareto curve .",
    "let @xmath131 be the optimal objective value of bp@xmath132 .",
    "the pareto curve is restricted to the interval @xmath133 $ ] with @xmath134 and @xmath135 .",
    "the following theorem , proved in @xcite , shows that the pareto curve is convex , strictly decreasing over the interval @xmath133 $ ] , and continuously differentiable for @xmath136 .",
    "[ paretothm ] the function @xmath137 is    1 .",
    "convex and nonincreasing .",
    "2 .   continuously differentiable for @xmath136 with @xmath138 where @xmath139 is the optimal dual variable to ls@xmath17 and @xmath140 with @xmath141 .",
    "3 .   strictly decreasing and @xmath142 for @xmath133 $ ] .",
    "since the pareto curve characterizes the optimal trade - off for both bp@xmath15 and ls@xmath17 , solving bp@xmath15 for a fixed @xmath3 can be interpreted as finding a root of the non - linear equation @xmath143 .",
    "the iterations consist of finding the solution to ls@xmath17 for a sequence of parameters @xmath144 where @xmath145 is the optimal objective value of bp@xmath15 .    applying newton s method to @xmath137 gives @xmath146 since @xmath137 is convex , strictly decreasing and continuously differentiable , @xmath147 superlinearly for all initial values @xmath148 ( see proposition  1.4.1 in @xcite ) . by theorem  [ paretothm ]",
    ", @xmath149 is the optimal value to ls@xmath150 and @xmath151 is the dual solution to ls@xmath150 . since evaluating @xmath149 involves solving a potentially large optimization problem ,",
    "an inexact newton method is carried out with approximations of @xmath149 and @xmath152 .",
    "let @xmath153 and @xmath154 be the approximations of the @xmath155 and @xmath156 defined in theorem  [ paretothm ] .",
    "the duality gap at each iteration is given by @xmath157 the authors of @xcite have proved the following convergence result .",
    "suppose @xmath4 has full rank , @xmath158 , and the inexact newton method generates a sequence @xmath159 . if @xmath160 and @xmath161 is close enough to @xmath145 , we have @xmath162 where @xmath163 and @xmath164 is a positive constant .      approximating @xmath149 and @xmath151 require approximately minimizing ls@xmath17 .",
    "the solver spgl1 uses a spectral projected - gradient ( spg ) algorithm .",
    "the method follows the algorithm by birgin , martnez , and raydan @xcite and is shown to be globally convergent .",
    "the costs include evaluating @xmath165 , @xmath166 , and a projection onto the one - norm ball @xmath167 . in parnes",
    ", we replace this spg algorithm with our algorithm , nesta - lasso ( cf.algorithm  [ alg : nesta - lasso ] ) .",
    "@xmath168 , @xmath169 , @xmath170 ; @xmath171 ; @xmath172 nesta - lasso@xmath173 ; @xmath174 ; @xmath175 ; @xmath176 ; @xmath177 ;",
    "in the nesta paper , @xcite , extensive experiments are carried out , comparing the effectiveness of state - of - the - art sparse reconstruction algorithms .",
    "the code used to run these experiments is available at http://www.acm.caltech.edu/~nesta .",
    "we have modified this nesta experiment infrastructure to include parnes in its tests and repeated some of the tests in @xcite with the same experimental standards and parameters .",
    "refer to the @xcite for a detailed description of the experiments .",
    "the algorithms tested and their experimental details are described below . note that the algorithms either solve bp@xmath15 or qp@xmath16 .",
    "nesta is used to solve bp@xmath15 .",
    "its code is available at http://www.acm.caltech.edu/~nesta .",
    "the parameters for nesta are set to be @xmath178 where @xmath40 is the initial guess and @xmath50 is the smoothing parameter for the one - norm function in bp@xmath15 .",
    "continuation techniques are used to speed up nesta in @xcite .",
    "such techniques are useful when it is observed that a problem involving some parameter @xmath179 is faster for large @xmath179 , @xcite .",
    "thus , the idea of continuation is to solve a sequence of problems for decreasing values of @xmath179 . in the case of nesta",
    ", it is observed that convergence is faster for larger values of @xmath50 . when continuation is used in the experiments ,",
    "there are four continuation steps with @xmath180 and @xmath181 for @xmath182 .",
    "gpsr is used to solve the penalized least - squares problem qp@xmath16 .",
    "the code is available at http://www.lx.it.pt/~mtf/gpsr .",
    "the problem is first recast as a bound - constrained quadratic program ( bcqp ) by using a standard change of variables on @xmath9 . here , @xmath183 , and the variables",
    "are now given by @xmath184 $ ] where the entries are positive .",
    "the new problem is then solved using a gradient projection ( gp ) algorithm .",
    "the parameters are set to the default values in the following experiments .    a version of gpsr with continuation is also tested .",
    "the number of continuation steps is 40 , the variable tolerancea is set to @xmath185 , and the variable minitera is set to 1 .",
    "all other parameters are set to their default values .",
    "sparsa is used to minimize functions of the form @xmath186 where @xmath46 is smooth and @xmath32 is non - smooth and non - convex .",
    "the qp@xmath16 problem is a special case of functions of this form .",
    "the code for sparsa is available at http://www.lx.it.pt/~mtf/sparsa .    in a sense",
    ", sparsa is an iterative shrinkage / thresholding algorithm . utilizing continuation and a brazilai - borwein heuristic @xcite to find step sizes ,",
    "the speed of the algorithm can be increased .",
    "the number of continuation steps is set to 40 and the variable minitera is set to 1 .",
    "all remaining variables are set to their default values .",
    "spgl1 is available at http://www.cs.ubc.ca/labs/scl/spgl1 .",
    "the parameters for our numerical experiments are set to their default values .",
    "due to the vast number of available and upcoming algorithms for sparse reconstruction , the authors of spgl1 and others have created sparco @xcite . in sparco , they provide a much needed testing framework for benchmarking algorithms .",
    "it consists of a large collection of imaging , compressed sensing , and geophysics problems .",
    "moreover , it includes a library of standard operators which can be used to create new test problems .",
    "sparco is implemented in matlab and was originally created to test spgl1 .",
    "the toolbox is available at http://www.cs.ubc.ca/labs/scl/sparco .",
    "fista solves qp@xmath16 .",
    "it can be thought of as a simplified version of the nesterov algorithm in section  [ sec : nesterov ] since it involves two sequences of iterates instead of three . in section  4.2 of @xcite ,",
    "fista is shown to give very accurate solutions provided enough iterations are taken . due to its ease of use and accuracy ,",
    "fista is used to compute reference solutions in @xcite and in this paper .",
    "the code for fista can be found in the nesta experiments code at http://www.acm.caltech.edu/~nesta .",
    "fpc solves the general problem @xmath187 where @xmath188 is differentiable and convex .",
    "the special case with @xmath189 is the qp@xmath16 problem .",
    "the algorithm is available at http://www.caam.rice.edu/~optimization/l1/fpc .",
    "fpc is equivalent to iterative soft - thresholding .",
    "the approach is based on the observation that the solution solves a fixed - point equation @xmath190 where the operator @xmath191 is a composition of a gradient descent - like operator and a shrinkage operator .",
    "it can be shown that the algorithm has @xmath192-linear convergence and also , finite - convergence for some components of the solution .",
    "since the parameter @xmath179 affects the speed of convergence , continuation techniques are used to slowly decrease @xmath179 for faster convergence .",
    "a more recent version of fpc , fpc - bb , uses brazilai - borwein steps to speed up convergence .",
    "both versions of fpc are tested with their default parameters .",
    "fpc - as is an extension of fpc into a two - stage algorithm which solves qp@xmath16 .",
    "the code can be found at http://www.caam.rice.edu/~optimization/l1/fpc .",
    "it has been shown in @xcite that applying the shrinkage operator a finite number of times yields the support and signs of the optimal solution .",
    "thus , the first stage of fpc - as involves applying the shrinkage operator until an active set is determined . in the second stage , the objective function",
    "is restricted to the active set and @xmath193 is replaced by @xmath194 where @xmath32 is the vector of signs of the active set .",
    "the constraint @xmath195 is also added .",
    "since the objective function is now smooth , many available methods can now be used to solve the problem . in the following tests , the solvers l - bfgs and conjugate gradients ,",
    "cg ( referred to as fpc - as ( cg ) ) , are used .",
    "continuation methods are used to decrease @xmath179 to increase speed . for experiments involving approximately sparse signals ,",
    "the parameter controlling the estimated number of nonzeros is set to @xmath6 , and the maximum number of subspace iterations is set to 10 .",
    "the other parameters are set to their default values .",
    "all other experiments were tested with the default parameters .",
    "the bregman iterative algorithm consists of solving a sequence of qp@xmath16 problems for a fixed @xmath179 and updated observation vectors @xmath125 .",
    "each qp@xmath16 is solved using the brazilai - borwein version of fpc .",
    "typically , very few ( around four ) outer iterations are needed .",
    "code for the bregman algorithm can be found at http://www.caam.rice.edu/~optimization/l1/2006/10/bregman-iter + ative-algorithms-for.html .",
    "all parameters are set to their default values .",
    "there is a new state - of - the - art method solving bp@xmath15 called @xmath196 which has been shown to be competitive with spgl1 and nesta .",
    "the experimental results in our paper do not include salsa , but we hope to make comparisons in future work .",
    "the code for @xmath196 can be found at http://cascais.lx.it.pt/~mafonso/salsa.html .",
    "as mentioned above , some of the algorithms we test solve qp@xmath16 and some solve bp@xmath15 . comparing the algorithms",
    "thus requires a way of finding a @xmath197 pair which for which the solutions coincide .",
    "the tests in @xcite use a two - step procedure . from the noise level",
    ", @xmath3 is chosen , and then spgl1 is used to solve @xmath198 .",
    "the spgl1 dual solution gives an estimate of the corresponding @xmath179 and then fista is used to compute a second @xmath3 corresponding to this @xmath179 with high accuracy .    in the nesta experiments",
    ", fista is used to determine the accuracy of the computed solutions while nesta is used to compute a solution that is used in the stopping criteria .",
    "please refer to @xcite for a complete description of the experimental details .",
    "section  4.2 of @xcite , shows that fista gives very accurate solutions provided enough iterations are taken . for each test",
    ", fista is run twice .",
    "fista is first run , with no limit on the number of iterations , until the relative change in the function value is less than @xmath199 .",
    "this solution is used to determine the accuracy of the computed solutions .",
    "fista is ran a second time with one of the following stopping criterion ; the results of this run are recorded in the tables .    in @xcite , nesta ( with continuation ) ,",
    "is used to compute a solution @xmath200 . in the tests , the following stopping criteria",
    "are used where @xmath201 is the @xmath84th - iteration in the algorithm being tested .",
    "@xmath202    or @xmath203 in other words , the algorithms are run until they achieve a solution at least as accurate as nesta .",
    "the rationale for having two stopping criteria is to reduce any potential bias arising from the fact that some algorithms solve qp@xmath16 , for which is the most natural , while others solve bp@xmath15 , for which is the most natural .",
    "it is evident from the tables below that there is not a significant difference whether or is used .",
    "the algorithms are recorded to not have converged ( dnc ) if the number of calls to @xmath4 or @xmath204 exceeds 20,000 .    in tables",
    "[ table51 ] and [ table52 ] , we repeat the experiments done in tables  5.1 and 5.2 of @xcite .",
    "these experiments involve recovering an unknown signal that is exactly @xmath86-sparse with @xmath205 , @xmath206 , and @xmath207 .",
    "the experiments are performed with increasing values of the dynamic range @xmath208 where @xmath209 db . for each run ,",
    "the measurement operator is a randomly subsampled discrete cosine transform , and the noise level is set to @xmath210 .",
    "the dynamic range , @xmath208 , is a measure of the ratio between the largest and smallest magnitudes of the non - zero coefficients of the unknown signal .",
    "problems with a high dynamic range occur often in applications . in these cases ,",
    "high accuracy becomes important since one must be able to detect and recover lower - power signals with small amplitudes which may be obscured by high - power signals with large amplitudes .",
    "the last two tables , tables  [ table53 ] and [ table53 - 1 ] , replicate tables 5.3 and 5.4 of @xcite .",
    "there are five runs of each experiment .",
    "each run involves an approximately sparse signals obtained from a permutation of the haar wavelet coefficients of a @xmath211 image .",
    "the measurement vector @xmath125 consists of @xmath212 random discrete cosine measurements , and the noise level is set to @xmath210",
    ". for more specific details , refer to @xcite . in applications ,",
    "the signal to be recovered is often approximately sparse rather than exactly sparse .",
    "again , high accuracy is important when solving these problems .",
    "it can be seen that nesta + ct , sparsa , spgl1 , parnes , and both versions of fpc - as perform well in the case of exactly sparse signals for all values of the dynamic range .",
    "however , in the case of approximately sparse signals , all versions of fpc and sparsa no longer converge in 20,000 function calls .",
    "parnes still performs well , converging in under 2000 iterations for all runs .",
    "the accuracy of the various algorithms is compared in table  [ table41 ] .",
    "lllllll methods & @xmath124 & @xmath213 & @xmath214 & @xmath215 & @xmath216 & @xmath217 + parnes & @xmath218 & @xmath219 & @xmath220 & @xmath221 & @xmath222 & @xmath223 + nesta & @xmath224 & @xmath225 & @xmath226 & @xmath227 & @xmath228 & @xmath229 + nesta + ct & @xmath230 & @xmath231 & @xmath226 & @xmath232 & @xmath233 & @xmath234 + gpsr & dnc & dnc & dnc & dnc & dnc & dnc + gpsr + ct & @xmath235 & @xmath236 & @xmath237 & @xmath238 & @xmath239 & @xmath240 + sparsa & @xmath241 & @xmath242 & @xmath243 & @xmath244 & @xmath245 & @xmath246 + spgl1 & @xmath247 & @xmath248 & @xmath249 & @xmath250 & @xmath251 & @xmath252 + fista & @xmath253 & @xmath254 & @xmath255 & @xmath256 & @xmath257 & @xmath258 + fpc - as & @xmath259 & @xmath260 & @xmath261 & @xmath262 & @xmath263 & @xmath264 + fpc - as ( cg ) & @xmath265 & @xmath266 & @xmath267 & @xmath268 & @xmath269 & @xmath270 + fpc & @xmath271 & @xmath254 & @xmath272 & @xmath273 & @xmath274 & @xmath275 + fpc - bb & @xmath276 & @xmath277 & @xmath278 & @xmath279 & @xmath280 & @xmath281 + bregman - bb & @xmath282 & @xmath283 & @xmath284 & @xmath285 & @xmath286 & @xmath287 +    llllll method & 20 db & 40 db & 60 db & 80 db & 100 db + parnes & 122 & 172 & 214 & 470 & 632 + nesta & 383 & 809 & 1639 & 4341 & 15227 + nesta + ct & 483 & 513 & 583 & 685 & 787 + gpsr & 64 & 622 & 5030 & dnc & dnc + gpsr + ct & 271 & 219 & 357 & 1219 & 11737 + sparsa & 323 & 387 & 465 & 541 & 693 + spgl1 & 58 & 102 & 191 & 374 & 504 + fista & 69 & 267 & 1020 & 3465 & 12462 + fpc - as & 209 & 231 & 299 & 371 & 287 + fpc - as ( cg ) & 253 & 289 & 375 & 481 & 361 + fpc & 474 & 386 & 478 & 1068 & 9614 + fpc - bb & 164 & 168 & 206 & 278 & 1082 + bregman - bb & 211 & 223 & 309 & 455 & 1408 +    llllll method & 20 db & 40 db & 60 db & 80 db & 100 db + parnes & 74 & 116 & 166 & 364 & 562 + nesta & 383 & 809 & 1639 & 4341 & 15227 + nesta + ct & 483 & 513 & 583 & 685 & 787 + gpsr & 62 & 618 & 5026 & dnc & dnc + gpsr + ct & 271 & 219 & 369 & 1237 & 11775 + sparsa & 323 & 387 & 463 & 541 & 689 + spgl1 & 43 & 99 & 185 & 365 & 488 + fista & 72 & 261 & 1002 & 3477 & 12462 + fpc - as & 115 & 167 & 159 & 371 & 281 + fpc - as ( cg ) & 142 & 210 & 198 & 481 & 355 + fpc & 472 & 386 & 466 & 1144 & 9734 + fpc - bb & 164 & 164 & 202 & 276 & 1092 + bregman - bb & 211 & 223 & 309 & 455 & 1408 +    llllll method & run 1 & run 2 & run 3 & run 4 & run 5 + parnes & 838 & 810 & 1038 & 1098 & 654 + nesta & 8817 & 10867 & 9887 & 9093 & 11211 + nesta + ct & 3807 & 3045 & 3047 & 3225 & 2735 + gpsr & dnc & dnc & dnc & dnc & dnc + gpsr + ct & dnc & dnc & dnc & dnc & dnc + sparsa & 2143 & 2353 & 1977 & 1613 & dnc + spgl1 & 916 & 892 & 1115 & 1437 & 938 + fista & 3375 & 2940 & 2748 & 2538 & 3855 + fpc - as & dnc & dnc & dnc & dnc & dnc + fpc - as ( cg ) & dnc & dnc & dnc & dnc & dnc + fpc & dnc & dnc & dnc & dnc & dnc + fpc - bb & 5614 & 7906 & 5986 & 4652 & 6906 + bregman - bb & 3288 & 1281 & 1507 & 2892 & 3104 +    llllll method & run 1 & run 2 & run 3 & run 4 & run 5 + parnes & 1420 & 1772 & 1246 & 1008 & 978 + nesta & 11573 & 10457 & 10705 & 8807 & 13795 + nesta + ct & 7543 & 13655 & 11515 & 3123 & 2777 + gpsr & dnc & dnc & dnc & dnc & dnc + gpsr + ct & dnc & dnc & dnc & dnc & dnc + sparsa & 12509 & dnc & dnc & 3117 & dnc + spgl1 & 1652 & 1955 & 2151 & 1311 & 2365 + fista & 10845 & 12165 & 10050 & 7647 & 11997 + fpc - as & dnc & dnc & dnc & dnc & dnc + fpc - as ( cg ) & dnc & dnc & dnc & dnc & dnc + fpc & dnc & dnc & dnc & dnc & dnc + fpc - bb & dnc & dnc & dnc & dnc & dnc + bregman - bb & 3900 & 3684 & 2045 & 3292 &",
    "3486 +      as tseng observed , accelerated proximal gradient algorithms will converge so long as the condition given as equation ( 45 ) in @xcite is satisfied . in our case",
    "this translates into @xmath288 upon setting @xmath289 and @xmath290 in ( 45 ) in @xcite .",
    "in other words , the value of @xmath23 need not necessarily be fixed at the lipschitz constant of @xmath88 but may be decreased and decreasing @xmath23 has the same effect as increasing the stepsize .",
    "tseng suggested to decrease @xmath23 adaptively by a constant factor until ( 45 ) is violated then backtrack and repeat the iteration ( cf .",
    "note 6 in @xcite ) . for simplicity , and",
    "very likely at the expense of speed , we do not change our @xmath23 adaptively in parnes and nesta - lasso .",
    "instead , we choose a small fixed @xmath23 by trying a few different values so that is satisfied for all @xmath84 , and likewise for the tolerance @xmath291 in algorithm  [ alg : nesta - lasso ] .",
    "however , even with this crude way of selecting @xmath23 and @xmath291 , the results obtained are still rather encouraging .",
    "as seen in the numerical results , spgl1 and nesta are among some of the top performing solvers available for basis pursuit denoising problems .",
    "we have therefore made use of nesterov s accelerated proximal gradient method in our algorithm nesta - lasso and shown that updating the prox - center leads to improved results . through our experiments , we have shown that using nesta - lasso in the pareto root - finding method leads to results comparable to the results given by currently available state - of - the - art methods .",
    "we would like to give special thanks to emmanuel cands for helpful discussions and ideas .",
    "the numerical experiments in this paper rely on the shell scripts and matlab codes of jrme bobin . we have also benefited from michael friedlander and ewout van den berg s matlab codes for spgl1 .",
    "we are grateful to them for generously making their codes available on the web .",
    "figueiredo , m. , nowak , r. , wright , s. : gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems .",
    "ieee j.  selected top",
    ".  signal process .",
    "* 1*(4 ) , 586 - 597 ( 2007 )              hennenfent , g. , herrmann , f. j. : sparseness - constrained data continuation with frames : applications to missing traces and aliased signals in 2/3-d . seg tech .  program expanded abstracts * 24*(1 ) , 2162 - 2165 ( 2005 )                          van den berg , e. , friedlander , m. p. , hennenfent , g. , herrmann , f. j. , saab , r. , yilmaz , . : algorithm 890 : sparco : a testing framework for sparse reconstruction .",
    "acm trans .  math .",
    "software * 35*(4 ) art .",
    "29 , pp .  16 ( 2009 )",
    "afonso , m. , bioucas - dias , j. , figueiredo , m. : fast frame - based image deconvolution using variable splitting and constrained optimization .",
    "ieee / sp 15th workshop on statistical signal processing , 2009 .",
    "ssp 09 , 109 - 112 ( 2009 )"
  ],
  "abstract_text": [
    "<S> in this article we propose an algorithm , nesta - lasso , for the lasso problem ( i.e. , an underdetermined linear least - squares problem with a one - norm constraint on the solution ) that exhibits linear convergence under the _ restricted isometry property _ ( rip ) and some other reasonable assumptions . inspired by the state - of - the - art sparse recovery method , nesta , we rely on an accelerated proximal gradient method proposed by nesterov in 1983 that takes @xmath0 iterations to come within @xmath1 of the optimal value . </S>",
    "<S> we introduce a modification to nesterov s method that regularly updates the prox - center in a provably optimal manner , resulting in the linear convergence of nesta - lasso under reasonable assumptions .    </S>",
    "<S> our work is motivated by recent advances in solving the basis pursuit denoising ( bpdn ) problem ( i.e. , approximating the minimum one - norm solution to an underdetermined least squares problem ) . </S>",
    "<S> thus , one of our goals is to show that nesta - lasso can be used to solve the bpdn problem . </S>",
    "<S> we use nesta - lasso to solve a subproblem within the pareto root - finding method used by the state - of - the - art bpdn solver spgl1 . </S>",
    "<S> the resulting algorithm is called parnes , and we show , experimentally , that it is comparable to currently available solvers .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}