{
  "article_text": [
    "the shannon separation theorem @xcite , states that source coding and channel coding can be performed separately and sequentially , while maintaining optimality .",
    "however , this is true only in the case of asymptotically long blocks of data .",
    "thus , considerable interest has developed in various schemes of joint source - channel coding , where the inherent redundancy of the source is utilized for error correction , possibly with the aid of some side information ( see , for instance , @xcite ) . combining the two processes",
    "may be motivated by reducing the total complexity of the procedure , and by some gain in the overall performance . moreover ,",
    "some uncompressed files ( e.g. bitmap , text ) are expected to be resilient to single bit errors , which may corrupt entire blocks in the case of the separation scheme .",
    "shannon s lower bound for the channel capacity of a binary symmetric channel ( bsc ) with flip probability @xmath2 , bit error rate @xmath3 and source entropy @xmath4 per bit is given by @xcite : @xmath5    where @xmath6 , is the entropy of @xmath7 , and the capacity , @xmath8 , is the maximal ratio between the source length @xmath9 and the transmitted length @xmath10 .",
    "in this paper we propose an extension of the low - density - parity - check codes ( ldpc ) @xcite decodeing algorithm , primarily designed for i.i.d .",
    "sequences , to the case of uncompressed data .",
    "our approach is to regard the source sequence , @xmath11 , as driven from some memoryless stationary markov process with a finite alphabet @xmath12 , and transition matrix @xmath0 of dimensions @xmath13 , that describes the probability of transition from symbol @xmath14 to symbol @xmath15 : @xmath16 .",
    "the markov entropy ( per symbol ) of such a process is given by : @xmath17},\\ ] ]    where @xmath18 is the stationary solution of the markov process .",
    "the entropy _ per bit _ , @xmath19 , ( @xmath20 being the number of bits in the binary representation of a symbol ) can be utilized as @xmath4 in eq .",
    "( [ bsc_capacity ] ) .",
    "neighboring symbols in a markov sequence are _",
    "correlated_. hence , information about symbols @xmath21 and @xmath22 , immediately implies some knowledge about @xmath23 , too .",
    "the main contribution of this work , is a method of incorporating this additional knowledge into the belief propagation decoding scheme .",
    "our joint source - channel scheme is based on mackay and neel s algorithm ( a thorough introduction may be found in @xcite ) , a variant of the earlier gallager code @xcite .",
    "although originally proposed for the binary field , extending the mn algorithm to higher finite fields is straight - forward as demonstrated in @xcite .",
    "the original motivation for moving to higher fields was reducing the number of edges ( and short loops ) in the code s graph . for our purpose , this enables us to treat markov sequences with a richer alphabet , consisting of @xmath24 symbols ( @xmath14 being an integer ) .",
    "the algorithm consists of two sparse matrices known both to the sender and the receiver : @xmath25 , and @xmath26 , where @xmath9 is the source block length , @xmath10 is the transmitted block length , and the code rate being @xmath27 .",
    "all non - zero elements in @xmath28 and @xmath29 are from @xmath30 , and @xmath29 must be invertible .",
    "encoding of a source vector @xmath31 into a codeword @xmath32 is performed ( all operations are done over @xmath1 ) by : @xmath33 @xmath32 is converted to binary representation and transmitted over the channel . during transmission , noise @xmath34 is added to @xmath32 , therefore the received vector is @xmath35 . upon receipt ,",
    "the decoder reconverts @xmath36 back to the original field , and computes the syndrome vector @xmath37 .",
    "the receiver then faces the following decoding problem : @xmath38\\cdot x,\\ ] ]        where square brackets denote appending of matrices , and @xmath7 is a concatenation of @xmath31 and @xmath34 .",
    "the decoding problem can be visualized as a bipartite graph ( fig [ 3layer ] ) , the elements of @xmath7 ( circles ) and @xmath39 ( squares ) are termed `` variable '' and `` check '' nodes , respectivly .",
    "the edges of the graph correspond to the nonzero elements in @xmath40 $ ] . for the mn algorithm",
    ", one should further distinguish between `` source variables '' - the @xmath31 elements in @xmath7 ( filled circles ) , and `` noise variables '' - the @xmath34 elements in @xmath7 ( empty circles ) .",
    "the decoding problem may be solved using the belief propagation ( bp ) ( or sum - product ) algorithm @xcite .",
    "bp is an iterative algorithm with two alternating steps , horizontal pass ( check@xmath41 variable messages ) and vertical pass ( variable @xmath41 check messages ) . during the vertical pass ,",
    "some prior knowledge is assigned to each decoded symbol , according to the assumed statistics ( for the i.i.d .",
    "case this would simply be : @xmath42 for all the source symbols ) .",
    "the key point here is that one can re - estimate and re - assign these priors after every iteration _ individually for each decoded symbol _ @xcite .",
    "the outcome of each iteration is an a - posteriori probability @xmath43 , for each symbol ( both source and noise ) .",
    "the mn decoder is linear in the size of the source block , @xmath9 , with complexity @xmath44 ( per iteration ) , where @xmath45 is the average number of checks per symbol @xcite .    a proper construction of the matrices @xmath28 and @xmath29 is crucial in order to ensure nearly capacity - achieving performance .",
    "in this work we follow the kanter and saad ( ks ) constructions @xcite , which are very sparse , simple to construct , and preform very close to the bound .",
    "the @xmath29 matrix has a systematic construction : diagonal and sub diagonal , which simplifies computation tasks @xcite .",
    "the ks construction for @xmath46 , @xmath47 , is schematically displayed in fig .",
    "[ ks_construction ] , black regions denote nonzero elements . extending the construction to higher @xmath1",
    "is done by randomly replacing the nonzero elements with elements of the corresponding field .",
    "although constructed originally for i.i.d .",
    "sources , we successfully used ks matrices for uncompressed sources , however , we mention the possibility of improving the performance by devising better codes .    , @xmath29 , @xmath48 for rate @xmath49 , black regions denote nonzero elements .",
    ", width=288 ]    the mn algorithm is also applicable for the case of an additive white gaussian noise ( awgn ) channel @xcite .",
    "the binary transmitted vector ( assumed for simplicity to be @xmath50 ) is corrupted by noise with zero mean and variance @xmath51 , hence , the received vector , @xmath52 , is real valued .",
    "the _ binary _ received vector , @xmath36 , is determined by hard - decision , namely , @xmath53 if @xmath54 .",
    "the probability of the transmitted bit @xmath55 is given by : @xmath56 and the probability of an error in the @xmath57 hard - decision bit is given by : @xmath58 eq .",
    "( [ gaussian_n_prob ] ) is used for calculating a prior for each noise variable .",
    "apart of these modifications , the mn algorithm for an awgn channel is identical to the bsc case .",
    "the channel capacity for awgn is given by @xcite :    @xmath59    for binary source messages , ( rather then real source messages ) , however , there exist a tighter bound , @xcite : @xmath60 where x is the transmitted bit , @xmath61 and @xmath62 is the received ( corrupted ) bit , with @xmath63.\\ ] ]",
    "in every iteration of the mn algorithm , a better estimate of each variable node is attained ( on average ) . in this section",
    "we shall describe our method of incorporating the statistical knowledge about the source , and these local estimates .",
    "consider three successive symbols @xmath64 in a sequence generated by a markov process with transition matrix @xmath0 and alphabet @xmath1 .",
    "the probability of a triplet @xmath65 is given by @xcite :    @xmath66    where use has been made of the bayes rule : @xmath67 , and the fact that the process is memoryless .",
    "now , given the a - posteriori probabilities for the first and last symbols in the triplet : @xmath68 and @xmath69 , one can calculate a prior for the probability that @xmath70 :    @xmath71    where @xmath72 is a normalization constant such that : @xmath73 . we term eq . ( [ prior(b ) ] ) the dynamical block prior ( dbp ) .",
    "the extension of the mn algorithm to the joint source - channel case consists of the following steps :    1 .",
    "a binary sequence of @xmath74 bits is converted to @xmath9 @xmath1 symbols . 2 .",
    "the encoder measures @xmath0 and @xmath75 for all the @xmath76 symbols over the source , and transmits reliably this side information to the decoder .",
    "the source is encoded according to ( [ mnencode ] ) , then reconverted to binary representation and transmitted over the bsc .",
    "4 .   the decoder maps the received signal back to gf(@xmath76 ) , and performs the regular decoding ( [ mndecode ] ) , but after every iteration of the bp , the prior for each source symbol is recalculated according to ( [ prior(b ) ] ) .",
    "the complexity of calculating the @xmath76 priors for a single symbol according to the posteriors of its neighbors is reduced from @xmath77 in the naive calculation , to @xmath78 by eq .",
    "( [ prior(b ) ] ) .",
    "the decoder s complexity remains linear , with total complexity of @xmath79 per iteration .",
    "the above - mentioned procedure may be thought of as adding a layer to the bipartite random graph represented by the matrix @xmath40 $ ] : the dbp s , eq .",
    "( [ prior(b ) ] ) , are messages passed only among _ source _ variable nodes , which are spatially related . in fig .",
    "[ 3layer ] , the diamonds represent this new ( directional ) layer , which connects neighboring source nodes .",
    "we note that the possibility of extending this scheme to gallager codes is an open question , since the source is not explicitly represented in the graph .",
    "we report here results for a bsc with rate @xmath46 , and for an awgn with rate @xmath80 , using the corresponding ks constructions for @xmath28 and @xmath29 devised in @xcite .",
    "other rates , constructions and block length were also checked .",
    "random vectors of length @xmath81 bits ( @xmath82 for @xmath83 ) were generated by the markov process , then mapped to a vector in @xmath1 with length @xmath84 $ ] , and were encoded and decoded as described in the previous section .",
    "for each reported result , at least 1000 sample vectors were generated and transmitted .",
    "the threshold for infinite source length , @xmath85 , is estimated from the scaling argument of the convergence time , which was previously observed for @xmath86 @xcite .",
    "the convergence time , measured in iterations of the mn algorithm , is assumed to diverge as the level of noise approaches the threshold from below .",
    "more precisely , we found that the scaling for the divergence of @xmath87 is _ independent of @xmath76 _ and is consistent with : @xmath88 for a bsc , and an awgn channel , respectivly .",
    "this extrapolation is independent of @xmath9 @xcite ( for @xmath89 ) , so by monitoring @xmath87 , for moderate @xmath9 , the threshold can be found by a linear fit .",
    "( see the inset of fig .",
    "[ pb_vs_f ] ) note that the estimation of @xmath87 is a simple computational task in comparison with the estimation of low bit error probabilities for large @xmath9 , especially close to the threshold .",
    "we also note that the analysis is based on @xmath87 instead of the _ average _ number of iterations , since we wish to prevent the dramatic effect of a small fraction of samples with slow convergence or no convergence .",
    "some selected results are presented in table [ res_table ] .",
    "the columns correspond to : the field size @xmath76 ; the source length in symbols , @xmath9 ; the entropy ( per bit ) of the source @xmath90 @xcite ; and the corresponding maximal noise @xmath91 ( eq .",
    "( [ bsc_capacity ] ) ) ; the critical noise , @xmath92 , up to which the bit error rate @xmath93 ; and the threshold , @xmath94 , eq .",
    "( [ scaling ] ) .",
    ".simulation results for bsc with rate @xmath49 . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      binary sequences of length @xmath95 were generated using the following transition matrix : @xmath96 having markov entropy @xmath97 .",
    "for rate @xmath98 , this entropy corresponds to maximal noise , eq .",
    "( [ binary_capacity ] ) , @xmath99 , ( @xmath100 db ) .",
    "the sequences were transmitted over an awgn channel , using three different fields : @xmath101 .",
    "[ gaussian_res ] presents the scaling behavior , eq.([scaling ] ) , for these fields ( triangles , squares , and circles , respectively ) .",
    "the symbols mark working points with @xmath102 , and were used for estimating the corresponding thresholds : @xmath103 .",
    "it is evident that as @xmath76 increases , both @xmath104 and @xmath105 improve .",
    "the results of the previous sections indicate that the performance of the presented joint coding is not too far from shannon s lower bound and , most probably , using an optimized code , the channel capacity can be nearly saturated .",
    "however , for a finite block length , the main drawback of our algorithm is the overhead of the header ( i.e. the transmitted side information ) which must be encoded and transmitted reliably .",
    "one has to remember that the size of the header , ( @xmath0 ) , scales with @xmath78 where the precision of each element is of the order @xmath106 .",
    "this overhead is especially intolerable in the limit where : @xmath107 .",
    "note that this is indeed the situation even for very large messages , @xmath108 , and a symbol size of @xmath109 bits ( a `` char '' , @xmath110 ) .",
    "this point may be tackled by observing that for a process with low entropy , characterized by enhanced repetitions and correlations , @xmath0 is dominated by a small number of elements , while the rest of the elements are negligible .",
    "we therefore repeated our simulations , using only the @xmath76 largest elements in @xmath0 as side information .",
    "the decoder would then set all other elements in each row of @xmath0 equally , to obey the normalization condition @xmath111 . in fig .",
    "[ pb_vs_f ] the empty squares / triangles represent working points for the algorithm with @xmath112 . in both cases ,",
    "the critical noise level @xmath92 is only slightly decreased , but the size of the side information becomes considerably smaller .",
    "in this section we describe how the markovian decoder can be implemented without any transmission of side information .",
    "the key points are the special properties of the ks construction ( fig .",
    "[ ks_construction ] ) : the first @xmath9 rows of @xmath28 are characterized by one non - zero element per row and column , where the first @xmath9 rows of @xmath29 are characterized by @xmath113 non - zero elements . furthermore , due to the systematic form of @xmath29 , each row can not be written as a linear combination of the other rows . hence , the first @xmath9 bits of the syndrome vector @xmath39 , are equal ( up to a simple permutation ) to the source , with an effective flip rate , @xmath114 . for @xmath47 for instance , @xmath115 ( @xmath14 marks the position of the nonzero element in the @xmath116 row of @xmath28 ) , and @xmath117 .",
    "the first @xmath9 symbols of @xmath39 are therefore a result of a _ hidden _ markov model ( hmm ) .",
    "the underlaying transition matrix , @xmath0 , generating the source sequence , can be estimated by means of the em algorithm @xcite , which is a standard tool for solving such _",
    "parametric estimation _ problems , with linear complexity . having @xmath0 ( approximately ) revealed ,",
    "the dbp s can be calculated as described in eq .",
    "( [ prior(b ) ] ) .    for the general construction of the mn algorithm one",
    "adds / subtracts rows of the concatenated matrix @xmath40 $ ] and the corresponding symbols in @xmath39 , such that a situation is finally reached as follows : the first @xmath9 rows of @xmath28 are the identity matrix , regardless of the construction of the first @xmath9 rows of @xmath29 . from the knowledge of the noise level @xmath2 and the structure of @xmath57 row of @xmath29 one",
    "can now calculate the effective noise , @xmath118 , of the @xmath57 received source symbol . since all @xmath119 are functions of a unique noise level @xmath2 , one can estimate the parameters of the markovian process using some variants of the em algorithm .",
    "note , that in the general case the first @xmath9 rows of @xmath29 contain loops , hence @xmath120 , are correlated .",
    "however , these correlations are assumed to be small as the typical loop size is of @xmath121@xcite .",
    "the only remaining major drawback of the presented decoder is that the complexity ( per iteration ) , scales as @xmath122 , this may considerably slow down the decoder even for moderate alphabet size .",
    "note however , that for large @xmath76 , such that @xmath123 , and low entropy sequences , the transition matrix , @xmath0 , is expected to be very sparse , and dominated by elements of @xmath124 .",
    "taking advantage of the sparseness of @xmath0 , the complexity of the decoder can be further reduced .",
    "the one - dimensional markovian decoder can be easily extended to coding of a two - dimensional array of symbols or even to an array of symbols in higher dimensions @xcite .",
    "the naive complexity of the dbp calculation scales as @xmath125 , where @xmath126 is the number of blocks in the array , and @xmath127 denotes the dimension . using markovian and bayesian assumptions ,",
    "the complexity can be reduced to @xmath128 .",
    "mackay , m.c .",
    "davey , `` gallager codes for short block length and high rate applications '' , codes , systems and graphical models , i m a volumes in mathematics and its applications , springer - verlag ( 2000 ) ."
  ],
  "abstract_text": [
    "<S> belief propagation ( bp ) decoding of ldpc codes is extended to the case of joint source - channel coding . </S>",
    "<S> the uncompressed source is treated as a markov process , characterized by a transition matrix , @xmath0 , which is utilized as side information for the joint scheme . </S>",
    "<S> the method is based on the ability to calculate a dynamical block prior ( dbp ) , for each decoded symbol separately , and re - estimate this prior after every iteration of the bp decoder . </S>",
    "<S> we demonstrate the implementation of this method using mackay and neel s ldpc algorithm over @xmath1 , and present simulation results indicating that the proposed scheme is comparable with separation scheme , even when advanced compression algorithms ( such as ac , ppm ) are used . </S>",
    "<S> the extension to 2d ( and higher ) arrays of symbols is straight - forward . </S>",
    "<S> the possibility of using the proposed scheme without side information is briefly sketched . </S>"
  ]
}