{
  "article_text": [
    "one of the central goals of neuroscience is to understand how the structure of neural circuits underlies the processing of information in the brain , and in recent years a considerable effort has been focused on measuring neural connectivity empirically [ @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] .",
    "`` functional '' approaches to this neural connectivity problem rely on statistical analysis of neural activity observed with experimental techniques such as multielectrode extracellular recording [ @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] or calcium imaging [ @xcite ; @xcite ; @xcite ; @xcite ] .",
    "although functional approaches [ @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] have not yet been demonstrated to directly yield the true physical structure of a neural circuit , the estimates obtained via this type of analysis play an increasingly important role in attempts to understand information processing in neural circuits [ @xcite ; @xcite ; @xcite ; @xcite ] .",
    "perhaps the biggest challenge for inferring neural connectivity from functional data  and indeed in network analysis more generally  is the presence of hidden nodes which are not observed directly [ @xcite ( @xcite , @xcite ) ; @xcite ; @xcite ; @xcite ; @xcite ] . despite swift progress in simultaneously recording activity in massive populations of neurons ,",
    "it is still beyond the reach of current technology to monitor a complete set of neurons that provide the presynaptic inputs even for a single neuron [ though see , e.g. ,  @xcite for some recent progress in this direction ] .",
    "since estimation of functional connectivity relies on the analysis of the inputs to target neurons in relation to their observed spiking activity , the inability to monitor all inputs can result in persistent errors in the connectivity estimation due to model misspecification .",
    "developing a principled and robust approach for incorporating such unobserved neurons , whose spike trains are unknown and constitute hidden or latent variables , is an area of active research in connectivity analysis [ @xcite ; @xcite ; @xcite ; @xcite ] .    incorporating these latent variables into connectivity estimation",
    "is a challenging task .",
    "the standard approach for estimating the connectivity requires us to compute the likelihood of the observed neural activity given an estimate of the underlying connectivity . computing this likelihood",
    ", however , requires us to integrate out the probability distribution over the activity of all hidden neurons .",
    "this latent activity variable will typically have very high dimensionality , making any direct integration methods infeasible .",
    "thus , it is natural to turn to markov chain monte carlo ( mcmc ) approaches here [ @xcite ] .",
    "the best design of such an mcmc sampler is not at all obvious in this setting , since it may be necessary to develop sophisticated proposal densities to capture the dependence of the hidden spike trains on the observed spiking data in order to guarantee a reasonable proposal acceptance rate [ @xcite ] .",
    "for example , the simplest gibbs sampling approaches may not perform well given the strong dependencies between adjacent spiking timebins that are inherent to neural activity .",
    "in this paper we develop metropolis  hastings ( mh ) algorithms for efficiently sampling from the probability distribution over the activity of hidden neurons .",
    "we derive a proposal density that is asymptotically correct in the limit of weak interneuronal couplings , and demonstrate the utility of this proposal density on simulated networks of neurons with neurophysiologically feasible parameters [ @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] .",
    "we also consider a hybrid mh strategy based on fast hidden markov model ( hmm ) sampling approaches that can be exploited to extend the range of applicability of the basic algorithm to more strongly coupled networks of neurons . in each case",
    ", the resulting mcmc chain mixes quickly and each step of the chain may be computed quickly .    of special interest",
    "is the problem of sampling from the probability distribution over unknown spike trains when a neuron is indirectly observed using calcium imaging ( instead of electrical recordings ) [ @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] .",
    "this problem can be naturally related to the sampling problem described above .",
    "we develop an approach to incorporate calcium fluorescence imaging observations directly into our proposal density .",
    "this allows us to efficiently sample from the probability distribution over the activity of multiple hidden neurons given calcium fluorescence observations , improving on the methods introduced in @xcite.=-1",
    "we model the activity of individual neurons with a  discrete - time generalized linear model ( glm ) [ @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ; @xcite ] : @xmath0 , \\nonumber\\\\[-8pt]\\\\[-8pt ] j_i(t ) & = & b_i(t ) + \\sum _ { j=1}^n\\sum _ { t'<t } w_{ij}(t - t')n_j(t'),\\nonumber\\end{aligned}\\ ] ] where the spike indicator function for neuron @xmath1 , @xmath2 , is defined on time bins @xmath3 of size @xmath4 .",
    "connectivity between neurons is described via the `` connectivity matrix , '' @xmath5 ; the self - terms @xmath6 describe refractory effects ( there is a minimal `` refractory '' interspike interval of a millisecond or two in most neurons , e.g. ) , and @xmath7 represents the statistical effect of a spike in neuron @xmath8 at time @xmath9 upon the spiking rate of neuron @xmath1 at time @xmath3 .",
    "@xmath10 is the number of neurons in the neural population , including hidden and observed neurons , and @xmath11 denotes a baseline driving term ( which might depend on some observed covariates such as a sensory stimulus ) .",
    "we assume that the maximal firing frequency @xmath12 is much larger than the typical spiking rate , @xmath13 $ ] , in which case the bernoulli spiking model in ( [ eqn : genmodel ] ) is closely related to models in which @xmath14 is drawn from a poisson distribution at every time step ( see the references above for a number of examples ) .    in the presence of fluorescence observations from a calcium - sensitive indicator dye ( the calcium imaging setting ) ,",
    "the model ( [ eqn : genmodel ] ) is supplemented by two variables per neuron @xmath1 : the intracellular calcium concentration @xmath15 ( which is not observed directly ) and the fluorescence observations @xmath16 .",
    "we model these terms according to a hidden markov model governed by a  simple driven autoregressive process [ @xcite ; @xcite ] , @xmath17\\\\[-8pt ] f_i(t ) & \\sim & \\mathcal{n}[s(c_i(t)),v(c_i(t))].\\nonumber\\end{aligned}\\ ] ] thus , under nonspiking conditions , @xmath15 is set to the baseline level of @xmath18 .",
    "whenever the neuron fires a spike , @xmath19 , the calcium variable @xmath15 jumps by a fixed amount @xmath20 , and subsequently decays with time constant  @xmath21 ; @xmath21 is on the order of hundreds of milliseconds in the cases of most interest here .",
    "the fluorescence signal @xmath16 corresponds to the count of photons collected at the detector per neuron per imaging frame .",
    "this photon count may be modeled with approximately gaussian statistics ( poisson photon count models are also tractable in this context [ @xcite ] , though we will not pursue this detail here ) , with the mean given by a saturating hill - type function @xmath22 @xcite and the variance  @xmath23 scaling with the mean .",
    "see @xcite for full details and further discussion .",
    "note that observations of calcium - indicator fluorescence are typically performed at a low `` frame - rate , '' @xmath24 , measured in frames per second .",
    "thus , we will restrict our attention to the case @xmath25 , that is , @xmath16 observations are available only every few timesteps @xmath4 . in addition , it will be useful to define an effective snr , following @xcite , as @xmath26 } { e[(f_i(t)-f_i(t-\\delta))^2/2   |   n_i(t)=0]^{1/2}},\\ ] ] that is , the size of a spike - driven fluorescence jump divided by a rough measure of the standard deviation of the baseline fluorescence .",
    "in @xcite we introduced an expectation - maximization approach for estimating the model parameters given calcium fluorescence data ; @xcite discuss a related approach for fitting the model given spiking data recorded from extracellular electrodes .",
    "the maximization step in this context is often quite tractable , involving a separable convex optimization problem [ @xcite ; @xcite ; @xcite ; @xcite ] .",
    "the expectation step is much more difficult ; analytical approaches are often infeasible .",
    "monte carlo solutions to this problem require us to obtain samples from the probability distribution over the activity of all hidden neurons .",
    "this problem is the focus of the present paper . in the context of the expectation maximization framework , we assume therefore that an estimate of the model parameters @xmath27 is available , and the problem is to obtain a  sample from @xmath28 ( in the case that the spike trains of a  subset of neurons is observed directly ) , or @xmath29 ( in the case that spike trains are observed indirectly via calcium fluorescence measurements ) .",
    "here and throughout we use bold notation for vectors , that is , @xmath30 and @xmath31\\}$ ] ; @xmath27 denotes the set of all parameters in the model , including @xmath32 , and @xmath33 . in cases where no confusion is possible below",
    "we will suppress the dependence on @xmath27 .      in @xcite , we noted that in order to sample from the desired joint distribution over the activity of all hidden neurons , @xmath34 , it is sufficient to be able to efficiently sample sequentially from the conditional distribution over one hidden neuron given all of the other hidden neurons , @xmath35 : if this is possible , then a sample from the full joint distribution , @xmath34 , can be obtained using a block - wise gibbs algorithm .",
    "this blockwise gibbs approach makes sense in this context because the connectivity weights @xmath36 , are relatively weak in many neural contexts ; for example , synaptic strengths between cortical neurons are typically fairly small ( see the discussion in section [ seciif ] below ) .",
    "however , dependencies between the spike indicator variables @xmath37 within a single neuron may be quite large if the time gap @xmath38 is small , since the self - terms @xmath39 are typically strong over small timescales ; for example , as mentioned above , after every spike a neuron will enter a refractory state during which it is unable to spike again for some time period .",
    "see @xcite for further discussion .",
    "thus , we will focus below exclusively on the single - neuron conditional sampling problem , @xmath40 . in the next couple sections we will discuss methods for sampling from @xmath41 ; in section [ sec : sampl - ca ] we will discuss adaptations of these methods for incorporating calcium fluorescence observations in @xmath42 .",
    "-order markov model for neural dynamics in terms of the instantaneous spikes , @xmath14 , and a simpler markov model in terms of the grouped states @xmath43 . ]      in some special cases we can solve the single - neuron conditional sampling problem quite explicitly . specifically , if the support of each of the coupling terms @xmath5 is contained in the interval @xmath44 $ ] for some sufficiently small number of time steps @xmath45 , then we can employ standard hidden markov model methods to sample from the desired conditional distribution exactly .",
    "the key is to note that in this case equation ( [ eqn : genmodel ] ) defines a @xmath45th - order markov process ; thus , it is convenient to apply the standard substitution @xmath43 . here",
    "@xmath46 denotes the `` state '' of neuron @xmath1 , which is completely characterized by the binary @xmath45-string [ e.g. , @xmath47 describing its spiking over the @xmath45 most recent time bins .",
    "clearly , the size of the state - space is @xmath48 .",
    "see figure  [ fig : gmodel ] for an illustration .    to exploit this markov structure , note",
    "that we may write the conditional distributions @xmath49 $ ] and @xmath50 $ ] explicitly using model ( [ eqn : genmodel ] ) , and , in fact , it is easy to see that the system @xmath51 , \\nonumber\\\\[-8pt]\\\\[-8pt ] \\mathbf{s}_{\\setminus i}(t+\\delta ) & \\sim & p[\\mathbf{s}_{\\setminus i}(t+\\delta ) | \\mathbf{s}_{\\setminus i}(t ) , s_i(t)]\\nonumber\\end{aligned}\\ ] ] forms an autoregressive hidden markov model [ @xcite ] , with @xmath52 playing the role of the observation at time @xmath3 ( figure  [ fig : gmodel ] ) . thus , if @xmath45 is not too large , standard finite hidden markov model ( hmm ) techniques can be applied to obtain the desired spike train samples .",
    "specifically , we can use the standard filter forward - sample backward algorithm [ @xcite ; @xcite ] to obtain samples from @xmath53 , from which we can easily transform back to obtain the desired sample @xmath54 .",
    "this requires @xmath55 computation time and @xmath56 memory , since the transition matrix @xmath49 $ ] is sparse [ due to the redundant definition of the state variable @xmath46 ] , so the matrix  vector multiplication required in the transition step scales linearly with the number of states , instead of quadratically , as would be seen in the general case .",
    "note , for clarity , that in the above discussion we are assuming that the coupling terms @xmath5 are known ( or have been estimated in a separate step ) , and therefore @xmath45 is also known , directly from the maximal support of the couplings @xmath5 ; thus , we do not apply any model selection or estimation techniques for choosing @xmath45 here .",
    "clearly , the hmm strategy described above will become ineffective if @xmath45 becomes larger than about @xmath58 or so , due to the exponential growth of the markov state - space .",
    "therefore , it is natural to seek more general alternate methods .",
    "recall that the interneuronal coupling terms @xmath59 , are small .",
    "thus , it seems promising to examine the conditional spike train probability in the limit of weak coupling ( @xmath60 ) , in the hope that a good sampling method might suggest itself .    the log - likelihood of the hidden spike train @xmath54 can be written as @xmath61\\\\[-8pt ] & = & \\sum_i\\sum_t n_i(t ) \\log f[j_i(t ) ] - f[j_i(t ) ] \\delta - n_i(t ) ! + \\mbox{const.};\\nonumber\\end{aligned}\\ ] ] for notational simplicity , we have used a poisson model for @xmath14 given the past spike trains , but it turns out that we can use any exponential family with linear sufficient statistics in the following computations ; see the for details .",
    "now , if we make the abbreviation @xmath62 we can easily expand to first order in @xmath63 : @xmath64 - f[j_i(t ) ] \\delta - n_i(t ) ! \\nonumber \\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad { } + \\sum_{j \\neq i } \\sum_t n_j(t ) \\log f [ b_j(t ) + j^-_j(t ) ] - f [ b_j(t ) + j^-_j(t ) ] \\delta + \\mbox{const . } \\nonumber \\\\",
    "\\hspace*{-4pt}&&\\qquad = \\sum_t n_i(t ) \\log f[j_i(t ) ] - f[j_i(t ) ] \\delta - n_i(t ) ! \\nonumber \\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad { } + \\sum_{j \\neq i } \\sum_t n_j(t ) \\log f [ b_j(t ) ] - f [ b_j(t ) ] \\delta\\nonumber \\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad \\hphantom{{}+ \\sum_{j \\neq i } \\sum_t } { } +   \\biggl ( n_j(t ) \\frac { f'[b_j(t ) ] } { f[b_j(t ) ] } - f'[b_j(t ) ] \\delta   \\biggr ) j^-_j(t ) + o(w ) + \\mbox{const . }",
    "\\nonumber \\\\ \\hspace*{-4pt } & & \\qquad = \\sum_t n_i(t ) \\log f[j_i(t ) ] - f[j_i(t ) ] \\delta - n_i(t ) !   \\nonumber\\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad { } + \\sum_{j \\neq i } \\sum_t   \\biggl ( n_j(t ) \\frac { f'[b_j(t ) ] } { f[b_j(t ) ] } - f'[b_j(t ) ] \\delta   \\biggr ) \\sum_{s",
    "< t } \\sum_k w_{jk}(t - s ) n_k(s)\\nonumber \\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad { } + o(w ) + \\mbox{const . }",
    "\\nonumber \\\\ \\hspace*{-4pt}&&\\qquad = \\sum_t n_i(t ) \\log f[j_i(t ) ] - f[j_i(t ) ] \\delta - n_i(t ) !   \\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad { } + \\sum_{j \\neq i } \\sum_t \\sum_{s > t }   \\biggl ( n_j(s ) \\frac { f'[b_j(s ) ] } { f[b_j(s ) ] } - f'[b_j(s ) ] \\delta   \\biggr ) w_{ji}(s - t ) n_i(t ) \\nonumber \\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad { } + o(w ) + \\mbox{const . }",
    "\\nonumber \\\\ \\hspace*{-4pt}&&\\qquad = \\sum_t n_i(t ) \\biggl\\ { \\log f [ j_i(t ) ] + \\sum_{j \\neq i } \\sum_{s > t } \\frac { f'[b_j(s ) ] } { f[b_j(s ) ] } w_{ji}(s - t )   [ n_j(s ) - f[b_j(s ) ] \\delta   ]   \\biggr\\ } \\nonumber \\\\",
    "\\hspace*{-4pt}&&\\qquad \\quad\\hphantom{\\sum_t } { } - n_i(t ) !",
    "+ o(w ) + \\mbox{const.},\\nonumber\\end{aligned}\\ ] ] where we have used the definition of @xmath65 in the third equality , rearranged sums ( and changed variables ) over @xmath3 and @xmath66 in the fourth equality , regathered terms in the final equality , and retained only terms involving a factor of  @xmath14 throughout .    thus ,",
    "if we note the resemblance between equation  ( [ eqn : corfull ] ) and the poisson log - probability , we see that one attractive approach is to sample from a spike train proposal @xmath14 with log - rate equal to the term within brackets , @xmath67 + \\sum _ { j \\neq i } \\sum_{s > t } \\frac { f'[b_j(s ) ] } { f[b_j(s ) ] } w_{ji}(s - t )   \\bigl[n_j(s ) - f[b_j(s ) ] \\delta   \\bigr];\\ ] ] since we are conditioning on @xmath68 ( i.e. , these terms are assumed fixed ) , and there are no @xmath69 terms affecting the proposed rate of @xmath14 for @xmath70 , it is straightforward to sample @xmath14 recursively forward according to this rate , and then use metropolis  hastings to compute the required acceptance probability and obtain a sample from the desired distribution .    we note that this approach is conceptually quite similar to that of @xcite , who proposed sampling recursively from a process of the form @xmath71\\delta ] , \\nonumber\\\\[-8pt]\\\\[-8pt ] j^{\\mathit{pl}}_i(t ) & = & b^{\\mathit{pl}}_i + \\sum_{j}\\sum_{s",
    "< t } w^{\\mathit{pl}}_{ij}(t - s)n_j(s ) + \\sum_{j \\neq i } \\sum _ { s > t } w^{\\mathit{pl}}_{ij}(t - s)n_i(s).\\nonumber\\end{aligned}\\ ] ] @xcite discussed a somewhat computationally - intensive procedure in which the parameters @xmath72 and @xmath73 are reoptimized iteratively via a maximum pseudolikelihood procedure .",
    "if we reverse the logic we used to obtain equation ( [ eqn : corfull ] ) , it is clear that our approach simply uses an `` effective input '' @xmath74\\\\[-8pt ] & & { } + \\sum_{j\\neq i } \\sum_{s > t } \\frac{f[b_i(t ) ] } { f'[b_i(t ) ] } \\frac{f'[b_j(s ) ] } { f[b_j(s ) ] } w_{ji}(s - t )   \\bigl[n_j(s)- f[b_j(s ) ]    \\delta   \\bigr]\\nonumber\\end{aligned}\\ ] ] in place of @xmath75 above .",
    "further , in the special case of an exponential nonlinearity , @xmath76 , all pre - factors in equation ( [ eqn : weakcur ] ) cancel out , leading to an expression that resembles equation  ( [ eqn : plproposal ] ) quite closely : @xmath77\\\\[-8pt ] & & { } + \\sum _ { j\\neq i } \\sum _ { s > t } w_{ji}(s - t )   \\bigl[n_j(s)- f[b_j(s)]\\delta \\bigr].\\nonumber\\end{aligned}\\ ] ] thus , in the weak - coupling limit @xmath78 , and ignoring self - interactionterms  @xmath39 , we see that the optimal `` back - inputs '' @xmath79 , from @xcite may be analytically identified as the time- and index - reversed original forward couplings , @xmath80 .",
    "so far we have developed two methods for sampling from @xmath81 : the hmm method ( section [ seciib ] ) is exact but becomes inefficient when @xmath45 is large , while the weak - coupling method ( section [ seciic ] ) becomes inefficient when the coupling terms  @xmath82 become large .",
    "what is needed is a hybrid approach that combines the strengths of these two methods .",
    "the key is that the cross - coupling terms  @xmath63 are typically fairly weak ( as emphasized above ) , and the self - couplingterms  @xmath83 are large only for a small number of time delays @xmath3 .    to take advantage of this special structure , we begin by constructing a  truncated hmm that retains all coupling terms @xmath5 for @xmath3 up to some maximal delay @xmath84 , where @xmath84 is chosen to keep the size of the state - space acceptably small but large enough so that the coupling terms are captured to an acceptable degree .",
    "then we include the discarded coupling terms ( i.e. , terms at lags longer than @xmath84 ) via the first - order approximation .",
    "more concretely , denote the conditional probability of @xmath85 under the truncated hmm as @xmath86 [ recall the correspondence between the @xmath45-string state variable @xmath46 and the binary spiking variable @xmath14 ] .",
    "note that this forms an inhomogeneous markov chain in the grouped state variables @xmath46 , and an inhomogeneous ( @xmath88)-markov chain in @xmath14 , as discussed in section [ seciib ] .",
    "now we want to incorporate the discarded coupling terms up to the first order : we simply form the product @xmath89\\\\[-8pt ]   \\qquad & & \\qquad { } \\times\\exp   \\biggl ( n_{i}(t)\\sum _ { j\\neq i } \\sum_{t'>t_{\\max } } w_{ji}(t'-t)\\frac{f'(b_j(t'))}{f(b_j(t ' ) ) }   [ n_j(t ' ) - f(b_j(t'))\\delta ]   \\biggr).\\nonumber\\end{aligned}\\ ] ] we use this simple product form here in an analogy to the hmm case , in which we condition on observations by simply forming the ( normalized ) product of the prior distribution on state variables and the likelihood of the observations given the state variables ; in this case , the first term plays the role of the prior over the state variables @xmath46 and the second term corresponds to the pseudo - observations represented by the weak coupling terms incorporating the observed firing of the other neurons @xmath90 .",
    "although , to be clear , this joint probability does not correspond rigorously to any small - parameter expansion of @xmath91 , we might expect that this hybrid may outperform alternative strategies such as using only the truncated hmm , given by equation  ( [ eqn : trunchmm ] ) , or only the weak - coupling correction , given by equation  ( [ hmmmh6 ] ) , as we will see in the section below ( see especially figure  [ fig : combinedhyhmmmh ] ) . since this product form for the joint probability is structurally equivalent to the conditional probability of an hmm in the state variables @xmath46more precisely , the graphical model [ @xcite ] corresponding to the above expression is a chain in terms of @xmath46we can employ the forward  backward procedure to sample from this proposal , and then compute the metropolis ",
    "hastings acceptance probability to obtain samples from the desired conditional @xmath92 .",
    "note that the mh acceptance probability will decrease as a function of the dimensionality @xmath93 of the desired spike train and of the size of the discarded weights @xmath94 , since for large @xmath82 the proposal density discussed above will approximate the target density less accurately . in general , it is necessary to employ a blockwise approach : that is , we update the spike train @xmath85 in blocks whose length is chosen to be small enough that the mh acceptance probability is sufficiently high , but large enough so that the total correlations between blocks are small and the overall chain mixes quickly .",
    "we will examine these trade - offs in more quantitative detail in the section below .      in this section",
    "we turn our attention to the problem of sampling from the fluorescent - conditional distribution @xmath95 . in principle",
    ", we could apply the same basic approach as before , exploiting the hmm structure of equations ( [ eqn : genmodel])([eqn : genmodelf ] ) in the variables @xmath96 ; however , the state - space for the calcium variable @xmath15 is continuous ( instead of discrete ) , requiring us to adapt our methods somewhat .",
    "we will briefly mention two alternative methods before introducing the novel approach that is the focus of this section .",
    "first , in @xcite we introduced a sampler based on a technique from @xcite .",
    "this method requires drawing a rather large auxiliary sample from the continuous @xmath15 state space and then employing a modified forward ",
    "backward approach to obtain spike train samples .",
    "the techniques we will discuss below do not require such an auxiliary sample , and are therefore significantly more computationally efficient ; we will not compare these methods further here .",
    "second , standard pointwise gibbs sampling is fairly straightforward in this setting : we write @xmath97 , and then note that both @xmath98 and @xmath99 can be computed easily as a function of @xmath14 . [ to compute the latter quantity , note from equation  ( [ eqn : genmodelf ] ) that @xmath14 only affects the values of @xmath100 appreciably for @xmath101 , for a  suitably large number of time constants @xmath102 . ]",
    "we will discuss the performance of the gibbs approach further below .",
    "now we turn to the main theme of this section : to adapt the mh - based blockwise approach discussed above , it is essential to develop a proposal density that efficiently incorporates the observed fluorescence data , in order to ensure reasonable acceptance rates .",
    "we use a filter - backward - and - sample - forward approach .",
    "the basic idea is to compute , via a backward recursion , the conditional future observation density @xmath103 given the current state @xmath104 , for all @xmath105 .",
    "then , we may easily sample from a proposal of the form @xmath106\\\\[-8pt ] c_i(t ) & = & c_i(t-\\delta ) - \\delta/\\tau^c_i \\bigl(c_i(t-\\delta)-c_i^b\\bigr ) + a_i n_i(t),\\nonumber\\end{aligned}\\ ] ] and by appending the samples @xmath46 , @xmath107 , we obtain a sample from the desired density @xmath108 . here",
    "the spiking term @xmath109 may include weak - coupling terms or truncations to keep the state - space tractably bounded , as discussed in the previous section .",
    "[ again , recall the correspondence between the @xmath45-string state variable @xmath46 and the binary spiking variable @xmath14 . ]    to calculate @xmath110 , we can use the standard backward hmm recursion @xcite , @xmath111 with @xmath112 we have already discussed the transition probability @xmath113 .",
    "the observation density @xmath110 is a continuous functionof  @xmath15 .",
    "we could solve this backward recursion directly by breaking the  @xmath15 axis into a large number of discrete intervals and employing standard numerical integration methods to compute the required integrals at each time step .",
    "however , this approach is computationally expensive in the high snr regime [ i.e. , where the ratio of the spike - driven calcium bump @xmath20 is large relative to the fluorescence noise scale @xmath114 , where a fine discretization becomes necessary .",
    "@xcite introduced a more efficient approximate recursion for this density that we adapt here .",
    "the first step is to approximate @xmath110 with a mixture of gaussians ; this approximation is exact in the limit of linear and gaussian fluourescence observations [ i.e. , in the case that the @xmath115 is linear and @xmath116 is constant in equation  ( [ eqn : genmodelf ] ) ] , and works reasonably in practice [ see @xcite for further details ] .",
    "it is straightforward to see in this setting that we require @xmath117 mixture components to represent @xmath110 exactly ; each term in the mixture corresponds to a distinct sequences of spikes @xmath118 and initial conditions @xmath46 . for large @xmath93 ,",
    "such a mixture of course can not be computed explicitly . however , as @xcite pointed out , we may further approximate the intractable @xmath117 mixture with a smaller @xmath119 mixture , parametrized by the total number of spikes @xmath120 instead of the full spike sequence @xmath118 ( since mixture components with the same total number of spikes overlap substantially , due to the long timescale @xmath121 of the calcium decay ) ; thus , we may avoid any catastrophic exponential growth in the complexity of the representation .    in order to calculate and update this approximate mixture ,",
    "we proceed as follows . at each timestep @xmath3",
    "we represent @xmath110 as a mixture of @xmath122 gaussian components with weights @xmath123 , means @xmath124 , and variances @xmath125 , for @xmath48 distinct states @xmath66 and @xmath126 . in order to update this mixture",
    "backward one timestep , from time @xmath3 to time , we first integrate over @xmath127 in equation ( [ eqn : genrec1 ] ) .",
    "since @xmath15 evolves deterministically given @xmath14 , this reduces to updating the means and the variances in each gaussian component , @xmath128 /    [ 1-\\delta / \\tau^c_i ] , \\nonumber \\\\",
    "v_{s , k}&\\rightarrow & v_{s , k}/(1-\\delta/\\tau^c_i)^2 , \\\\",
    "p_{s , k}&\\rightarrow & p_{s , k}/(1-\\delta/\\tau^c_i ) .",
    "\\nonumber\\end{aligned}\\ ] ] second , we perform the multiplication with the observation density @xmath129 in equation ( [ eqn : genrec ] ) . in the case that @xmath16 is linear and gaussian in @xmath15 , each such product is again gaussian , and the means and",
    "the variances are updated as follows [ @xmath130 and @xmath131 are the mean and the variance for @xmath132 , resp . ] : @xmath133.\\nonumber\\end{aligned}\\ ] ] more generally [ i.e. , in the case of a nonlinear @xmath115 or nonconstant @xmath116 in equation  ( [ eqn : genmodelf ] ) ] , standard gaussian approximations may be used to arrive at a similar update rule ; again , see @xcite for details .",
    "third , we perform the summation over @xmath134 in equation  ( [ eqn : genrec1 ] ) , and reorganize the obtained mixture to reduce the number of components from @xmath135 to @xmath136 for each @xmath46 . equation ( [ eqn : genrec1 ] ) doubles each mixture component into two new gaussians , corresponding to the two terms in the sum for @xmath137 or @xmath138 . for brevity",
    ", we denote these terms as @xmath139 and @xmath140 , where @xmath141 stands for the state @xmath134 describing a spike at time @xmath142 , and @xmath143 denotes the absence of a spike at time @xmath142 .",
    "we group all new components in pairs such that each pair corresponds to a given number of spikes on the interval @xmath144 .",
    "each such pair of gaussian components is then merged into a  single gaussian with equivalent weight , mean , and variance : @xmath145 and the @xmath146 term corresponds to the variance of the means , @xmath147\\\\[-8pt ] & & { } + w^- \\bigl(m_{s^-,k}(t ) - m_{s , k}(t-\\delta)\\bigr)^2\\bigr)/(w^+ + w^-).\\nonumber\\end{aligned}\\ ] ] see figure  [ fig : fluorrecursion ] below for an illustration .      to test the performance of different sampling algorithms",
    ", we simulated a population of @xmath148800 neurons , following the approach described in @xcite .",
    "briefly , we simulated a spontaneously active randomly connected neural network , with each neuron described by model equation ( [ eqn : genmodel ] ) , and connectivity and functional parameters of individual neurons chosen randomly from distributions based on the experimental data available for cortical networks in the literature [ @xcite ; @xcite ; @xcite ; @xcite ] .",
    "networks consisted of 80% excitatory and 20% inhibitory neurons [ @xcite ; @xcite ] .",
    "neurons were connected to each other in a sparse , spatially homogeneous manner : the probability that any two neurons @xmath1 and @xmath8 were connected ( i.e. , that either @xmath63 or @xmath149 was nonzero ) was @xmath150 [ @xcite ; @xcite ] .",
    "the scale of the connectivity weights @xmath63 was matched to results from the cortical literature , as cited above , and the overall average firing rate of the networks was set to be about 5 hz . the connectivity waveforms @xmath5 were modeled as exponential functions with time constant fixed for all neurons at @xmath58 msec ; for the self - coupling terms @xmath83 , neurons strongly inhibited themselves over short time scales ( an absolute refractory effect of 2 ms ) and weakly inhibited themselves with an exponentially - decaying weight over a timescale of 10 ms .",
    "finally , we used an exponential nonlinearity , @xmath151 , for simplicity .",
    "again , see @xcite for full details and further discussion.=-1",
    "the efficiency of any metropolis ",
    "hastings sampler is determined by the ease with which we can sample from the proposal density ( and compute the acceptance probability ) versus the degree to which the proposal approximates the true target @xmath152 . in this section",
    "we will numerically compare the efficiency of the proposal densities we have discussed above . in particular , we will examine the following proposal densities , listed in rough order of complexity :    * time - homogeneous poisson process . * point process with log - conditional intensity function given by the delayed input @xmath153 , equation ( [ eqn : lagcur ] ) . * point process with log - rate determined by full effective input in weak - coupling approximation @xmath154 , equation ( [ eqn : weakcur ] ) . * hybrid truncated hmm proposal including weak - coupling terms [ equation  ( [ hmmmh6 ] ) ] .    as a benchmark",
    ", we also compare these samplers against a simple pointwise gibbs sampler , in which we draw from @xmath155 sequentially over  @xmath3 .",
    "we begin by inspecting a simpler toy model of neural spiking . in this model",
    "both refractory and interneuronal coupling effects are assumed to be short , that is , on the time scale of @xmath15620 msec .",
    "this simplification allows us to characterize the neural state fully by @xmath157 past time bins ( @xmath158 ms in these simulations ) , with the state variable @xmath43 . in this case ,",
    "equation  ( [ eqn : genmodel ] ) describes a hidden markov model with a  state space that is small enough to sample from directly , using the forward ",
    "backward procedure detailed in section [ seciib ] .",
    "thus , in this case we can obtain the probability distribution @xmath152 explicitly , and compare different mh algorithms against a ground truth .    in figure",
    "[ fig : rates ] we inspect the true instantaneous posterior spiking rate @xmath159 for the hidden neuron in this toy model , calculated exactly with the forward  backward procedure and estimated using different effective rates for a few of the mh proposals discussed above .",
    "we see that the true rate @xmath160 varies widely around its mean value , implying that the simplest proposal density ( the time - homogeneous poisson process ) will result in rather low acceptance rates , as indeed we will see below .",
    "similarly , a naive approximation for @xmath160 using only the delayed input @xmath161 fails to capture much of the structure of @xmath160 . incorporating both the past and the future spiking activity of the observed neurons via the weak - coupling input @xmath154 leads to a much more accurate approximation of the true rate @xmath160 .     of one neuron conditioned on the spiking activity of all other neurons in a population of @xmath148 neurons ;",
    "the rate is computed exactly via forward ",
    "backward procedure described in section [ seciib ] .",
    "a population of spontaneously spiking neurons with neurophysiologically feasible parameters ( section [ seciif ] ) was simulated for a total of @xmath162 sec at a time resolution of @xmath163  msec .",
    "approximate spiking rates obtained using just the delayed input , @xmath164 [ equation  ( [ eqn : lagcur ] ) ] , or full weak - coupling input , @xmath165 [ equation  ( [ eqn : weakcursimple ] ) ] , are shown in red and green , respectively .",
    "( in particular , the delayed input  @xmath153 and the full effective input @xmath154 are computed as described in section [ seciic ] , and then we approximate the rates using @xmath166 $ ] or @xmath167 $ ] . ) while the proposal based on the delayed inputs @xmath164 reproduces the true spiking rate somewhat poorly , the weak - coupling  @xmath165 approximation is significantly more accurate . ]",
    ", we see that the homogeneous ( `` uniform '' ) poisson mh algorithm mixes slowly ; the sampler based on the full weak - coupling input @xmath154 mixes significantly more quickly than the sampler based on the delayed inputs @xmath153 .",
    "the standard gibbs sampler performed about as well as the weak - coupling mh sampler .",
    "simulation details are as in figure [ fig : rates ] , except a  @xmath58  sec spike train was simulated here to collect a  sufficient amount of data to distinguish the different algorithms . ]",
    "similar results are obtained when we apply the mh algorithm using these proposal densities ( figures [ fig : aucorrelations ] and [ fig : marginals ] ) .",
    "we use mh with @xmath168 samples for each proposal density , with a burn - in period of 1,000 samples , to obtain both the autocorrelation functions ( figure  [ fig : aucorrelations ] ) and estimates for the instantaneous spiking rate @xmath160 ( figure  [ fig : marginals ] ) .",
    "we observe that the homogeneous poisson proposal density leads to a very long autocorrelation scale , implying inefficient mixing , that is , long runs are necessary to obtain accurate estimates for quantities of interest such as @xmath160 . indeed , we see in figure  [ fig : marginals ] that 5,000 samples are insufficient to accurately reconstruct the desired rate @xmath160 . similarly , for the proposal density based on the delayed inputs @xmath164 , the autocorrelation scale is shorter , but still in the range of 1020 samples . the weak - coupling proposal , which incorporates information from both past and future observed spiking activity , mixes quite well , with an autocorrelation scale on the order of a single sample , and leads to an accurate reconstruction of the true rate in figure  [ fig : marginals ] .",
    "interestingly , the simplest gibbs sampling algorithm also performs well , with an autocorrelation length similar to that of the best mh sampler shown here .     for one hidden neuron , estimated from 5,000 samples using the gibbs sampler and the three mh samplers compared in the preceding two figures .",
    "the results are similar : the homogeneous poisson proposal performs badly [ in the sense that the @xmath160 computed based on these @xmath169 samples approximates the true @xmath160 poorly ] , while the weak - coupling and gibbs samplers outperform the sampler based on the delayed inputs  @xmath153 ( in terms of variance around the true rate , computed via the full forward  backward hmm method , shown in blue ) .",
    "mh acceptance rates , @xmath170 , varied from @xmath171 for  mh using the homogeneous poisson proposal to @xmath172 for mh using the proposal with full effective input  @xmath154 .",
    "simulation details are as in figure [ fig : rates ] .",
    "a shorter time interval of the simulated spike train is shown for greater clarity here . ]",
    "we also study the performance of the weak - coupling proposal as a function of the strength of the interneuronal interactions in the network , and as a  function of the number of neurons @xmath10 in the population and the length  @xmath93 of the desired spike train ( figure  [ fig : combinedmh ] ) .",
    "the acceptance rate falls at a rate approximately inverse to the coupling strength , which seems sensible , since this proposal is based on the approximation that the coupling strength is weak .",
    "we observe similar behavior with respect to the size of the neural population .",
    "in particular , the acceptance rate drops below @xmath173 when @xmath1741,000 . on the other hand , increasing the spike train length @xmath93 affects performance to only a moderate degree : even when the length of the sampled spike train is increased from @xmath58 sec to @xmath175 sec , the acceptance rate remains above @xmath176 .    , neural population size , @xmath10 , and spike length , @xmath93 .",
    "we simulated sparsely connected networks of inhibitory and excitatory neurons as described in section [ seciif ] , with refractory effects up to @xmath58 msec long and interneural interactions up to @xmath177 msec long . here",
    "a coupling strength value of @xmath178 corresponds to the neurobiologically motivated set of parameters in section [ seciif ] ( also in figures  [ fig : rates][fig : marginals ] ) ; other values of @xmath179 corresponded to scaling @xmath180 in equation  ( [ eqn : genmodel ] ) .",
    "@xmath58 sec of neural activity was simulated at a time resolution of @xmath181 msec , with the neural population spiking at @xmath15645  hz .",
    "64 trials of 500 samples were simulated , with a new random neural network generated in each trial , from which we estimated @xmath182 .",
    "average acceptance rate @xmath170 and standard deviation are shown for such trials , as well as examples of several individual trials ( `` x '' ) .",
    "mh algorithm performance degraded significantly as @xmath179 or @xmath10 increased . on the other hand , for larger values of @xmath93 performance degraded much less substantially , and even for the largest @xmath93 we examined ( @xmath175 sec ) the acceptance rate remained above @xmath183 . ]",
    "one would expect that the performance of the mh algorithm in the case of more strongly - coupled neural networks can be improved by including strong short - term interaction and refractory effects explicitly into the proposal density .",
    "we discussed such an approach in section [ seciid ] .",
    "for weakly coupled networks , we expect the performance of this hybrid algorithm to be similar to that of our original proposal density ; however , for more strongly coupled neural networks , we expect a better performance .",
    "this expectation is borne out in the simulations shown in figure [ fig : combinedhyhmmmh ] : the hybrid sampler ( solid black line ) performs significantly better than the original mh algorithm ( dashed black line ) and uniformly better than alternative hybrid strategies , for example , where only the short - scale truncated hmm is used , or where all interneural interactions are accounted for via the weak - coupling approximation , while the self - terms @xmath39 are accounted for via a truncated hmm ( gray lines ) .",
    "similar results are obtained when we vary the size of the neural population  @xmath10 ( figure [ fig : combinedhyhmmmh ] ) .    ) ] as a function of the overall coupling strength @xmath179 and neural population size @xmath10 . the hybrid algorithm ( solid black line )",
    "performs substantially better than the original mh algorithm described in section [ seciic ] ( dashed black line ) , and uniformly better than alternative hybrid strategies , for example , where only a short - scale truncated hmm is used or where all interneural interactions are accounted for via weak - coupling approximation ( gray lines ) .",
    "simulations are as in figure  [ fig : combinedmh ] . ]    in the simulation settings described above ( timestep @xmath163 msec , with coupling currents @xmath63 lasting up to @xmath177 ms ) the mh algorithm ( coded in matlab without any particular algorithmic optimization ) took @xmath184300400 sec to produce 1,000 samples of 1,000 time - ticks each on a pc laptop ( intel core duo 2 ghz ) , dominated by the time necessary to produce 1,000 spike train proposals using the forward recursion .",
    "the hybrid algorithm had a  similar computational complexity ( @xmath185 ms in the truncated hmm ) .",
    "the gibbs algorithm had a somewhat higher computational cost , due largely to the fact that updates of the currents @xmath186 for up to @xmath177 ms needed to be performed during each step , to decide whether to flip the state of each spike variable  @xmath14 .",
    "this resulted in running times for the gibbs sampler which were 520 times slower than for the mh sampler .",
    "the gibbs updates are even slower in the calcium imaging setting ( discussed at more length in the next section ) , due to the fact that each update requires us to update @xmath15 over @xmath187 timesteps ( recall section [ sec : sampl - ca ] ) for each proposed flip of @xmath14 .",
    "next we examine the performance of the method we developed in section [ sec : sampl - ca ] for sampling from @xmath108 .",
    "we find that this sampler performs quite well in moderate and high snr settings .",
    "more precisely , for values of esnr greater than @xmath1565 [ recall the definition of esnr in equation  ( [ eq : esnr ] ) ] , the conditional distribution @xmath188 is localized near the true spike train quite effectively , leading to a high mh acceptance rate ( figures [ fig : fluortrace][fig : fluorsamples ] ) .",
    "indeed , we find [ as in @xcite ] that it is possible to achieve a sort of `` super - resolution '' in the sense that the mh algorithm can successfully return @xmath188 on the time - scale of @xmath163 msec even when fluorescence observations are obtained at a much lower frame - rate ( here @xmath189 hz ) .",
    "and relatively high @xmath190 .",
    "@xmath163  msec , and calcium imaging frame - rate @xmath189 hz .",
    "@xmath58 sec spike trains were simulated , with the neural population spiking at @xmath15645 hz .",
    "actual spikes of the target neuron are indicated with stars . ]",
    "calculated as a @xmath93-mixture of gaussians , as a function of time ( x - axis ) and @xmath15 ( y - axis ) ; colorbar indicates the probability density at time @xmath3 . for reference ,",
    "true calcium concentration is shown in red , and `` observed '' calcium concentration at the imaging frames [ i.e. , @xmath191 is shown with green `` x. '' simulation details are as in figure [ fig : fluortrace ] , first second ( 50 frames ) is shown for clarity .",
    "time ticks correspond to @xmath163 msec .",
    "actual spike times of the target neuron are shown with red lines . ]    , estimated using mh algorithm . while the mh algorithm performs well for @xmath192 and above ( @xmath193 ) , for low @xmath194 acceptance rate",
    "is only @xmath195 .",
    "simulation details are as in figure [ fig : fluortrace ] , first second ( 50 frames ) is shown for clarity .",
    "time ticks correspond to @xmath163 msec .",
    "actual spikes of the target neuron are shown with asterisks .",
    "note that in the high - snr case the sampler successfully recovers the smoothly - varying @xmath196 on a finer time - scale ( @xmath197 msec ) than the original fluorescence imaging data provided ( @xmath198  msec ) .",
    "however , in the low - snr case the recovered firing rate is overly spiky and variable due to the slow mixing speed of the m - h chain ; recall that similar behavior is visible in figure  [ fig : marginals ] . ]",
    "conversely , for low values of @xmath199 , for example , @xmath200 , the backward density @xmath201 becomes noninformative [ i.e. , relatively flat as a function of @xmath14 and @xmath15 ] , and the acceptance rate of our mh algorithm reverts to the rate obtained under the conditions of no calcium imaging data .",
    "however , in the low - to - moderate snr regime ( e.g. , @xmath203 ) , the performance of the mh sampler can drop substantially .",
    "this is primarily due to deviations in the shape of @xmath132 from gaussian at low snr .",
    "recall that we made several approximations in computing the backward density : first , this density is truly a mixture of @xmath204 components , whereas we approximate it with a mixture of only @xmath205 components .",
    "second , we assume that each mixture component is gaussian . although the fluorescence  @xmath16 is described by normal statistics given the calcium variable @xmath15 , the relationship between the fluorescence mean and calcium transient can be nonlinear , and the variance may depend on @xmath15 [ recall equation  ( [ eqn : genmodelf ] ) ] .",
    "this makes the conditional distribution of @xmath15 non - gaussian in general , particularly at low - to - moderate levels of snr ( where the likelihood term is informative but not sufficiently sharp to justify a simple laplace approximation ) .",
    "we tested the impact of each of these approximations individually by constructing a set of toy models where these different approximations were made exact ; we found that the non - gaussianity of @xmath206 , due to nonlinear dependence of @xmath16 on @xmath15 , was the primary factor responsible for the drop in performance .",
    "thus , we expect that in cases where the saturating function @xmath115 is close to linear , the sampler should perform well across the full snr range ( figure  [ fig : perffluor ] ) ; in highly nonlinear settings , more sophisticated approximations [ based on numerical integration techniques such as expectation propagation @xcite ] may be necessary , as discussed in the section below .",
    "the mh sampler here took about twice as long as in the fully - observed spike train case discussed in the previous section , largely due to the increased complexity of the backward recursion described in section [ sec : sampl - ca ] .    ) .",
    "calculations for two calcium signal models , with a toy linear ( gray dashed ) and realistic hill ( solid black ) transfer function , @xmath207 , are shown .",
    "[ see vogelstein et  al .",
    "( @xcite ) ; mishchenko , vogelstein and paninski ( @xcite ) for further details on the precise form of the nonlinear function @xmath207 ; in this case , three or four spikes were sufficient to drive the calcium variable into a regime where the fluorescence signal was significantly saturated . ]",
    "performance of the mh algorithm is good both for high and low esnr , but can suffer for intermediate @xmath2083 .",
    "this performance drop is primarily due to deviations of the conditional distribution @xmath206 from the gaussian shape , as exemplified by the much better performance in the model where @xmath115 is linear and @xmath206 is thus gaussian .",
    "simulation details are as in figure  [ fig : fluortrace ] . for each esnr",
    "50 simulations for different neural populations were performed , and the average and standard error of these simulations are shown . ]     ( black ) and true spike times ( red ) were recorded from a  single neuron via simultaneous fluorescence imaging and intracellular patch - clamp electrophysiological recording ; see vogelstein et  al .",
    "( @xcite ) for further experimental details .",
    "blue trace indicates posterior @xmath209 computed by the hybrid sampler .",
    "bottom : @xmath210 computed by the hybrid sampler .",
    "y - axis units are arbitrary in this case and have been suppressed .",
    "note that the sampler infers spikes and jumps in @xmath211 at the correct times . ]    given the good performance of gibbs sampling noted in the calcium - free setting discussed above , we also examined the gibbs approach here .",
    "however , we found that the gibbs algorithm was not able to procure the samples successfully given calcium imaging observations . in many cases we found that the gibbs sampler converged rapidly to a particular spike",
    "train close to the truth , but would then become `` stuck . ''",
    "if initialized again , the sampler would often converge to a different spike train , close to the truth , only to become stuck again .",
    "this behavior is due to the fact that the conditional distributions @xmath212 can be quite sharply concentrated , leading to poor mixing between spike train configurations with high posterior probability ; of course , this is a common problem with the gibbs sampler ( and indeed , this well - understood poor mixing behavior of the standard gibbs chain is what led us to develop the more involved methods presented here in the first place ) . roughly speaking",
    ", the extra constraints imposed by the fluorescence observations in @xmath212 relative to @xmath213 make the former distribution more `` frustrated , '' in physics language , making it harder for the gibbs sampler to reach nearby states with high posterior probability and leading to the relatively slower gibbs mixing rate in the calcium - imaging setting .",
    "finally , we applied the hybrid sampler to a sample of real calcium fluorescence imaging data ( figure  [ fig : real ] ) , in which @xmath214 and the true spike times  @xmath215 were recorded simultaneously .",
    "thus , we have access to ground truth for @xmath215 here , though of course only the observed fluorescence @xmath216 is used to infer @xmath217 .",
    "the model parameters were estimated using the em method discussed in @xcite ; again , only the observed @xmath216 was used to infer the model parameters , not the true spike times @xmath215 .",
    "about @xmath177 spikes worth ( @xmath169 fluorescence frames ) of data was sufficient to adequately constrain the parameters .",
    "then we applied the hybrid sampler using these parameters to the subset of data shown in figure  [ fig : real ] .",
    "the sampler does a good job of recovering the spike rate @xmath215 from the observed fluorescence data @xmath216 , and seems to do a reasonable job of recovering the corresponding jumps in @xmath210 that occur at spike times .",
    "note that we do not have access to the true intracellular calcium concentration @xmath211 , and therefore no ground truth comparisons are possible for this variable .",
    "in this work we developed several metropolis  hastings approaches for sampling from the conditional distribution of neuronal spike trains , given either the activity of other neurons in the network or calcium - sensitive imaging observations .",
    "the most effective approach was the hybrid method described in section [ seciid ] , which takes advantage of the fact that strong short - term temporal dependencies within a single spike train may be handled via forward  backward hidden markov model methods , while weaker long - term dependencies between neurons may be handled with the weak - coupling expansion developed in section  [ seciic ] . in each case , to sample efficiently from the spike train at time @xmath3 , it is important to incorporate not only past but also future information ( i.e. , spiking observations from times both before and after @xmath3 ) ; @xcite made a similar point . in the appendix we show that these methods may be extended rather easily to other exponential families ( not just the bernoulli and poisson cases of most interest in the neuroscience setting ) ; further applications to weakly - coupled markov chains in nonneural settings seem worth exploring .",
    "two major avenues are open for future work .",
    "first , as noted in figure  [ fig : fluorrecursion ] , the proposed sampler suffers somewhat in the case of strongly nonlinear fluorescence observations , largely because in this case our mixture - of - gaussians approximation of the backward density @xmath218 $ ] can break down .",
    "more sophisticated methods for approximating this density are available , and should be explored more thoroughly .",
    "second , as discussed in @xcite , applications of these methods to real data are ongoing , via monte carlo - expectation - maximization methods similar to those discussed in @xcite ; @xcite , with the fast sampler introduced here replacing the slower monte carlo approaches discussed in @xcite .",
    "calcium - fluorescence imaging methods have exploded in popularity over the last several years , and we hope the methods presented here will prove useful in quantifying the cross - correlations and effective connectivity in neural populations observed via fluorescence imaging and multielectrode recording methods .",
    "as mentioned in section [ seciic ] , it is straightforward to develop a first - order proposal density in the weak - coupling limit more generally , in the case that the variables of interest are drawn from an exponential family distribution with linear sufficient statistics .",
    "we begin by writing down our exponential family model , using slightly more compact notation than in section  [ seciic ] : @xmath219 with the coupling introduced via @xmath220 as before , we may easily expand the joint log - density to the first order : @xmath221 \\sum_{s>0 } w_s^{ji } n_{i , t - s}\\nonumber\\\\[-8pt]\\\\[-8pt ]    & & { } + \\mbox{const}(n_{it } ) + o(w).\\nonumber\\end{aligned}\\ ] ] now if we introduce the assumption that the sufficient statistic is linear , that is , @xmath222 , then after rearranging the double sum over @xmath66 and @xmath3 we obtain @xmath223     \\biggr ] n_{it}\\\\     & & { } + h(n_{it } ) + \\mbox{const}(n_{it } ) + o(w),\\end{aligned}\\ ] ] where again we have suppressed terms [ such as @xmath224 which do not involve  @xmath225 ; thus , to first order , the conditional distribution of @xmath226 given  @xmath227 , , remains within the same exponential family , but with a parameter shift @xmath228.\\ ] ] in the canonical parameterization , @xmath229 , standard exponential family theory [ @xcite ] shows that @xmath230 , and the parameter shift simplifies to @xmath231.\\ ] ] see @xcite for a discussion of some related results .    as a concrete example , consider the gaussian case : @xmath232 with @xmath233 as above . in this case",
    "we may define @xmath234 and @xmath235 ; thus , we find that the parameter shift in this case is @xmath236 .",
    "\\label{eq : gauss - shift}\\ ] ] in this linear - gaussian case , we may compute the exact conditional distribution of @xmath237 via the usual gaussian conditioning formula ; the necessary covariance and inverse covariance matrices may be obtained via standard ar model computations .",
    "it is straightforward to check that this exact formula agrees with equation  ( [ eq : gauss - shift ] ) up to @xmath238 terms in the small-@xmath82 limit .",
    "thanks to t.  machado , m.  nikitchenko , j.  pillow and j.  vogelstein for many helpful conversations , and again to t.  sippy and r.  yuste for the data example shown in figure  [ fig : real ] .",
    "we also gratefully acknowledge the use of the hotfoot shared cluster computer at columbia university ."
  ],
  "abstract_text": [
    "<S> monte carlo approaches have recently been proposed to quantify connectivity in neuronal networks . </S>",
    "<S> the key problem is to sample from the conditional distribution of a single neuronal spike train , given the activity of the other neurons in the network . </S>",
    "<S> dependencies between neurons are usually relatively weak ; however , temporal dependencies within the spike train of a single neuron are typically strong . in this paper </S>",
    "<S> we develop several specialized metropolis  </S>",
    "<S> hastings samplers which take advantage of this dependency structure . </S>",
    "<S> these samplers are based on two ideas : ( 1 ) an adaptation of fast forward  backward algorithms from the theory of hidden markov models to take advantage of the local dependencies inherent in spike trains , and ( 2 )  a  first - order expansion of the conditional likelihood which allows for efficient exact sampling in the limit of weak coupling between neurons . </S>",
    "<S> we also demonstrate that these samplers can effectively incorporate side information , in particular , noisy fluorescence observations in the context of calcium - sensitive imaging experiments . </S>",
    "<S> we quantify the efficiency of these samplers in a variety of simulated experiments in which the network parameters are closely matched to data measured in real cortical networks , and also demonstrate the sampler applied to real calcium imaging data .    and    . </S>"
  ]
}