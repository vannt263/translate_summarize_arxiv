{
  "article_text": [
    "physicists have long been fascinated by the notion of information . since the time of maxwell",
    "s demon @xcite , a number of papers have appeared exploring the energy cost of information as well as the connection to computation e.g. @xcite .",
    "a few papers have dealt with the role of the observer in thermodynamic systems and the information acquired by the observer e.g. @xcite .",
    "however none have considered the physics of information acquisition in a truly biological context .    in this paper ,",
    "i show that a physics - based measure of information is not only relevant for the study of biological systems , but it allows for the derivation of equations characterizing the sensory transduction process .",
    "these equations can be used to interpret and to compare with neurophysiological data .",
    "this work follows from an original approach that has appeared outside of physics and will be described next .",
    "a series of papers have explored the mathematical theory of sensation based on an entropy approach @xcite . from this theory ,",
    "over 150 years of sensory science can be unified by a entropy measure of uncertainty and a few auxiliary assumptions .",
    "this work was later extended to neurophysiology @xcite . despite the use of entropy",
    ", the exact connection of this approach to physics has not been thoroughly explored .",
    "central to the theory is the association of the entropy of a sensory distribution @xmath0 as measured by @xmath1 to a physiological variable @xmath2 measuring sensory magnitude . in the case of neurophysiology",
    ", @xmath2 is the firing rate or spike response of a neuron .",
    "the relationship between @xmath2 and @xmath3 is given by @xmath4 where @xmath5 is a constant with units of spikes per second .",
    "that is , uncertainty is equated to sensory magnitude .",
    "the entropy approach has been shown to unify a wide range of disparate sensory phenomena including discrimination @xcite and reaction time @xcite .",
    "the similarity of this approach to statistical thermodynamics is striking and invariably raises the question of whether there is an analogous `` second law '' governing sensory processes .    in physics ,",
    "there has been considerable interest in characterizing and understanding the thermodynamic arrow of time .",
    "parallel to this , scientists have also identified other arrows of time , including a psychological arrow which applies to mental processes ( e.g. @xcite ) .",
    "this paper is concerned with the psychological / perceptual arrow of time but one that is extended to the neurophysiological level .",
    "our discussion begins first however with the thermodynamic arrow of time .",
    "shalizi explores the connections between statistical inference , information and thermodynamic entropy to highlight difficulties with the bayesian approach to statistical mechanics @xcite .",
    "his argument is succinct and germane to this paper and will be repeated here .",
    "shalizi considers an ideal observer carrying out statistical inference on a dynamical system with random variable @xmath6 and associated density @xmath7 .",
    "he makes three assumptions : ( 1 ) time - reversible dynamics ; ( 2 ) bayesian updating of probabilities ; ( 3 ) equality of information and thermodynamic entropy , @xmath8 ( @xmath9 is boltzmann s constant ) .",
    "consider a system with initial distribution @xmath10 . at the next time",
    "instance ( @xmath11 ) , the system has evolved to @xmath12 where @xmath13 is the time - evolution operator ( e.g. frobenius - perron operator ) . first , by liouville s theorem , reversible dynamics are entropy preserving .",
    "hence @xmath14 where the notation @xmath15 denotes the entropy of the distribution @xmath7 .",
    "a measurement @xmath16 is performed at @xmath11 yielding @xmath17 . by bayes rule",
    "we get @xmath18 where @xmath19 is the likelihood and @xmath20 the posterior distribution given @xmath17 .",
    "thus the entropy of the _ posterior distribution _ is given by @xmath21 where the inequality follows from a fundamental theorem of information theory ( e.g. @xcite ) .",
    "that is , the average entropy decreases monotonically with each measurement .",
    "if @xmath3 is decreasing and @xmath8 , this demonstrates a reverse arrow of time in contradiction to the second law .",
    "since the derivation is made under very general conditions , shalizi considers the possibility that one ( or more ) of the assumptions are incorrect .",
    "assumption one ( of reversible dynamics ) is the least controversial and is the basis of much of modern statistical mechanics .",
    "assumption two ( bayesian updating ) provides a logical and systematic way of updating the measurement process .",
    "the most likely issue is with the identification of information with thermodynamic entropy .",
    "indeed , i argue that while information as used above leads to contradictions in the thermodynamic domain , it provides instead an appropriate description of another domain  that of _ sensory information processing_. by making a subtle change from @xmath8 to @xmath22 , shazili s derivation along with three additional assumptions permits the derivation of a set of equations that governs the acquisition of information at the sensory level .",
    "thermodynamic entropy is replaced by a new concept which i term _ sensorial or sensory entropy_. from this , a mathematical demonstration of a sensory or perceptual arrow of time follows naturally .",
    "this paper is restricted to the problem of _ intensity coding_. however , the methodology is general so that it can be applied to any other type of biological information acquisition as well .",
    "intensity coding is the process by which neurons encode information about the sensory stimulus strength . increasing magnitudes of stimuli typically induce higher rates of response ( in terms of action potentials per unit time ) .",
    "also , the response of a neuron to a steady signal drops monotonically over time , a process known as adaptation .",
    "intensity and neural coding have long been topics of active research interest .",
    "a number of studies have probed the general principles underlying sensory and neural response to both simple and complex stimuli e.g. @xcite . by contrast",
    "the theory presented in this paper deals only with simple stimuli and works in a more restricted domain .",
    "the aim of the approach is not necessarily to make comprehensive neural predictions , but rather to explore the generic process of sensation .",
    "the approach also seeks to identify a deep connection between sensory information and statistical physics .",
    "the original version of the theory first appeared over 30 years ago @xcite and was later extended to the neurophysiological level ( e.g. @xcite ) . in both cases",
    "the theory was introduced without a comprehensive tie - in to physics , with assumptions that were less than satisfying .",
    "one goal of this paper is to re - develop the theory in the language of physics and to incorporate a number of recent discoveries in statistics and complexity theory .",
    "indeed many of the steps crucial to moving this approach forward appeared only after the original works were published .",
    "consider an ideal observer ( sensory receptor ) sampling the fluctuating sensory signal magnitude ( microstate ) to estimate the mean intensity ( macrostate ) .",
    "a series of measurements ( samples ) are recorded from the signal and stored locally .",
    "the entropy is calculated through the uncertainty in the mean .",
    "the response is then derived from the entropy , and equated to the firing rate or spike rate response of the associated neuron ( i.e. the primary afferent neuron ) . as increasing numbers of measurements",
    "are taken , the uncertainty or entropy decreases corresponding to a drop in neuronal spike frequency .",
    "this , in essence , is the process described by the following derivation .",
    "since the problem considered here is different from shalizi s original problem , it is prudent to define what constitutes the _ system_. sensory transduction is the process whereby sensory stimuli are converted to a neural response . as such there are at least two components to the system : one that defines the sensory signal prior to its contact with the receptive field ( _ the physical system _ ) and the other which consists of the receptor , the associated neuron as well as any accessory structures ( _ the sensory system _ ) .",
    "equilibrium in one domain does not imply equilibrium in the other .",
    "for example , a taste solution placed in the mouth can be in thermodynamic equilibrium such that its thermal and chemical properties do not change with time .",
    "however , the gustatory system requires time to adapt to this solution and therefore the neural response to the stimulus may not in fact be in equilibrium .",
    "we begin with a modification and adaptation of shalizi s assumptions to the sensory problem .",
    "let @xmath6 be the random variable representing the stimulus strength ( i.e. microstate ) of the sensory signal and @xmath7 its associated density .",
    "we make the stronger assumption that the physical system is in equilibrium so that the distribution over microstates is time - invariant .",
    "finally , @xmath8 is replaced with @xmath22 .    while the same @xmath3 function is used in both cases , it is pertinent to remember that thermodynamic @xmath3 is calculated over a distribution in phase space whereas sensorial @xmath3 is calculated over a distribution of stimulus magnitudes .    to summarize :    * the physical system is in equilibrium . *",
    "the observer ( receptor ) updates probabilities in accordance with bayes rule .",
    "* spike response of the neuron @xmath2 equals the entropy .",
    "that is , @xmath23    from here , we require only three more assumptions .    *",
    "the observer ( receptor ) operates with finite resolution .",
    "resolution error is normally distributed with zero mean and constant variance @xmath24 independent of the signal @xmath6 .",
    "* variance of @xmath6 is related to its mean through a power law ( i.e. _ the fluctuation scaling law _",
    "@xcite ) . that is @xmath25 with @xmath26 constant . *",
    "the asymptotic , equilibrium spike response @xmath2 exhibits constant _ index of dispersion_. that is , @xmath27 this ratio is independent of signal mean .",
    "the significance of these assumptions will be discussed later .",
    "this is all that is needed to derive a full equation of five parameters that is capable of describing the response of a sensory neuron to simple time - varying inputs below physiological saturation levels .",
    "the equation can also be compared to experimental data .",
    "_ it is important to realize that no detailed assumptions about the underlying physiological mechanism are required in the derivation ! _ the handful of assumptions allows one to derive the asymptotic , near equilibrium behaviour of a sensory neuron which will be shown next .    while subjectivity and experience may affect the choice of the prior distribution @xmath28 we are interested in the asymptotic properties of the _ posterior distribution _ as the number of measurements grows large .",
    "the posterior distribution after @xmath29 successive measurements @xmath30 is given by @xmath31 where @xmath32 is the likelihood and @xmath33 the normalization constant .",
    "when the number of samples is large ( @xmath34 ) , the posterior distribution approaches a normal distribution asymptotically @xcite .",
    "moreover , if the estimate of the mean of @xmath6 is efficient , @xmath35 where @xmath36 is a normal distribution with variance @xmath37 . is @xmath38 where @xmath39 is the maximum likelihood estimate of @xmath6 and @xmath40 the fisher information .",
    "if the estimator is efficient , the variance of the estimate achieves the cramr - rao lower bound with @xmath41 . ] those objecting to the bayesian approach can re - derive this exact result using the central limit theorem .",
    "since the processing of measurements occurs with finite resolution ( * a4 * ) , entropy is calculated from the mutual information of both the posterior and the error distributions @xcite .",
    "taking the entropy of their convolution and subtracting the equivocation gives @xmath42 this is just the shannon - hartley theorem for an additive gaussian channel with signal - to - noise ratio @xmath43 @xcite .    from the fluctuation scaling law ( * a5 * ) , we introduce @xmath44 where @xmath45 is the signal mean and @xmath46 is a constant of proportionality .",
    "noting the constancy of @xmath24 , we introduce a new constant @xmath47 to obtain @xmath48    the input mean @xmath49 consists of both external and internal sources .",
    "the external source is the sensory signal itself and any other environmental signals .",
    "internal sources may include other signals generated internally which elicit a sensory or neural response including thermal noise , self - generated signals ( e.g. otoacoustic emissions in the ear ) , etc .",
    "we model the signal mean as a sum of the two components @xmath50 where @xmath51 is the total magnitude of external sources and @xmath52 the sum of internal sources .    the sample size increases with the number of measurements :",
    "@xmath29 is a function of time and @xmath53 refers to the _ sampling rate_. through memory and storage considerations , it is reasonable to assume that sampling does not occur _",
    "ad infinitum_. sampling is thus a function of the difference between @xmath29 and @xmath54 , the equilibrium or steady - state value of @xmath29 .",
    "that is , @xmath55 where @xmath56 is an unknown function with the condition @xmath57 ( sampling stops when @xmath58 ) .",
    "near equilibrium , we take a taylor expansion around @xmath59 to obtain @xmath60 since @xmath61 and @xmath62 , @xmath63 is a positive time constant .",
    "solutions of @xmath29 are used to calculate @xmath2 from eq .",
    "( [ interm ] ) with a choice of @xmath54 .",
    "there is one final step required to complete the derivation .",
    "given a finite sample size @xmath29 , it is more proper to think of @xmath3 in eq .",
    "( [ hfunction ] ) as a function of the _ sample variance _ @xmath64 . as such",
    ", a distribution for @xmath2 can be calculated from the sample variance distribution . in the limit of large @xmath29 , and at equilibrium ( @xmath58 ) , it is straightforward to show that the index of dispersion of @xmath2 is given by @xmath65 please see ( see appendix [ sec : a1 ] ) .",
    "this ratio is constant with respect to the signal mean @xmath50 ( see * a6 * ) , and thus @xmath66 that is , the equilibrium sample size must grow as a function of the signal magnitude for the dispersion index to remain constant . ) makes little sense since the units on both sides do not match .",
    "however it is easy to see that if we were to introduce a multiplicative constant to correct for the imbalance in units , this constant will be incorporated into @xmath47 . ]    summarizing , we have @xmath67 these equations have been shown to give a good description of the neural response to most simple time - varying sensory input ( up to physiological saturation levels ) for many sensory modalities and animal species ( e.g. @xcite ) .",
    "analytical solutions of these equations to a sample problem are provided in appendix [ sec : a2 ] . the predicted neural response to many common experimental conditions can be found in appendix [ sec : a3 ] .    in a sense",
    "it can be argued that eqs .",
    "( [ gut1])-([gut4 ] ) represent _ universal equations _ of five parameters governing complex biological behaviour .",
    "why universal ?",
    "one reason is that the assumptions used in the derivation are generic , without reference to specific biological mechanisms .",
    "these are assumptions that would pertain to just about any type of sensory measurement system .",
    "even a cursory glance at the plethora of available experimental data shows that there is in fact a common level of description across the different sensory modalities ( e.g. @xcite ) .",
    "for example , despite vast differences in anatomy and signal energy ( e.g. the visual system transduces photonic energy , whereas hearing is based on mechanoreception of vibration ) , _ adaptation _ and _ growth in response _ are both observed universally in all sensory modalities .",
    "adaptation is the phenomenon whereby the response to an increase in signal level induces a rapid rise in spike response followed by a slow decay back to equilibrium .",
    "the growth in response typically refers to the compressive growth of spike frequency to increasing signal magnitudes .",
    "compression is an important property of sensory neurons since sensory signals can range over several orders of magnitude ( e.g. for sound pressure the ratio is approximately @xmath68:1 ) whereas the dynamic range of a peripheral neuron is at most 1000:1 .",
    "the significance of compression be discussed later .",
    "both phenomena have clear psychological analogue : adaptation is observed in the habituation to environmental signals like noise or smells ; growth in response is simply a reflection that larger , more intense signals result in higher levels of sensation .",
    "such behaviour is observed universally in all sensory modalities .    during adaptation ,",
    "a sensory neuron is initially in equilibrium in a ` quiet ' environment . a sudden introduction of a steady signal ( @xmath69 ) results in an increase in uncertainty ( * a5 * ) .",
    "consequently , the target value of the sample size @xmath54 increases according to eq .",
    "( [ meq ] ) .",
    "samples are drawn and the uncertainty is reduced via bayes updating . from eq .",
    "( [ hfunction ] ) we observe that entropy @xmath3 decreases ( @xmath70 ) demonstrating that , _ as a general principle , the introduction of uncertainty to the sensory system results in a monotonic reduction of average entropy in accordance with an arrow of time .",
    "_ since perception in many cases mirrors the activity at the periphery , we can extend the arrow from a neurophysiological to a perceptual level to obtain a _ perceptual arrow of time_. that stimuli held fixed fade from the sensorium is a well - known phenomenon .",
    "the process of fading is attributed here to a gradual reduction of uncertainty through bayesian sampling and updating .",
    "implicit in the derivation above is the role of _ memory_. no matter how sampling is performed , uncertainty can only decrease if there is some form of storage of past measurements or samples .",
    "since the equilibrium sample size @xmath54 is itself finite , uncertainty can not be reduced to zero .",
    "for the neural response , this means that adaptation can only occur up to a minimum value in spike frequency corresponding to the equilibrium value of @xmath2 .",
    "incomplete adaptation is observed universally ( e.g. @xcite ) .",
    "the interpretation provided here is that a non - zero equilibrium response encodes residual uncertainty about the signal magnitude due to finite memory .",
    "put together , the measurement process and the memory form the `` engine '' which drives the perceptual arrow of time .",
    "the sensory transduction process obeys a form of le chatelier s principle whereby the system shifts to cancel any change introduced into the system .",
    "appendix [ sec : a2 ] illustrates one such example where a sensory signal is added and later removed .",
    "the accompanying neurophysiological response ( for both experiment and theory ) shows that any perturbations are minimized by the system .",
    "the perceptual arrow of time provides bounds on the time to return to sensory equilibrium , much like how the second law provides a time for return to equilibrium for le chatelier - type problems @xcite .    at the heart of the approach",
    "lies a number of critical assumptions .",
    "do they in fact reflect the operation of real sensory systems ?",
    "for example , it is commonly believed that the sensory coding of intensity involves the coupling of input signal strength to the response of the primary afferent neuron .",
    "that is , the firing rate is a function of signal mean",
    ". however assumption * a3 * ( firing rate equals uncertainty ) runs counter to this claim .",
    "recall that the differential entropy of a normal distribution is a function only of the variance and not the mean @xcite . through @xmath22 , the dependence of firing rate on the mean",
    "how can such a result be justified ?    first it should be noted that the entropy approach to sensation is _ not _ the only approach to postulate a link between information or variance and sensory processing .",
    "a bayesian theory of surprise was put forth recently as a means of determining human visual attention and gaze behaviour , where surprise is calculated from the relative entropy @xcite ; a theory of differential coupling was proposed whereby the internal ( neural ) excitation is coupled to the variance of the signal @xcite . in both cases , arguments were made in support of the idea that the sensory system processes _ change _ rather than the mean itself .",
    "there are several well - known experiments that illustrate the connection between variance and sensation .",
    "the phenomenon of brightness enhancement ( aka the brcke - bartley effect ) shows that the apparent brightness of a flickering light can change depending on the frequency of flicker .",
    "time - average luminance remains constant , however flickering contributes to temporal variations in the signal resulting in changes in apparent brightness .",
    "other experiments involving the stabilization of an image on the retina show that prolonged exposure to a fixed image leads to the fading of the visual percept . in each of these cases , we see that the sensory response is coupled to changes in the signal rather than to the mean level of stimulation .    however , neither of the experiments probe the exact relationship between _ firing rate _ and variance . instead , consider the following proposal for a new experimental test . light exhibits very different statistical behaviour depending on whether it is in the classical or quantum limit .",
    "photon bunching is the phenomenon whereby the statistics of the photon count deviates from a poisson distribution where variance equals mean ( e.g. @xcite ) . if a photoreceptor is stimulated with such a signal , the resulting neural response can be recorded to test the dependency of firing rate on signal variance with mean kept constant .",
    "and yet it is clear that the neural response is linked to the mean signal magnitude .",
    "where does this connection arise ? some recent work has shown that many complex systems exhibit a power - law relationship between mean and variance ( * a5 * ) @xcite .",
    "the fluctuation scaling law was discovered first in ecology through animal population studies @xcite and is known also as taylor s law . a compelling explanation for the origin of taylor s law",
    "was recently proposed @xcite . the family of probability distributions known as the tweedie distributions has a mean - variance power relationship .",
    "a convergence theorem has been established showing that any exponential dispersion model exhibiting an asymptotic mean - variance power relationship must have at its basis a tweedie model @xcite .",
    "the theorem therefore suggests a reason for the ubiquity of the power law in complex systems .",
    "the tweedie exponential dispersion models can be categorized according to the value of @xmath26 .",
    "tweedie models can be found for all real values of @xmath26 _ except _ for @xmath71 .",
    "this has important consequences for the growth of the neural function to be demonstrated next .",
    "recall that compression is the neural phenomenon whereby a wide input range is reduced to a more compact output range .",
    "this typically would involve a power function relationship with exponent less than one .",
    "compression can be observed in the equations governing sensory entropy . for the asymptotic , equilibrium neural response",
    ", one can easily derive from the various equations @xmath72 this function exhibits compression if and only if @xmath73 . since @xmath26 is positive , and no such tweedie model exists for @xmath71 , this implies that the only possible range of exponents are for @xmath74 .",
    "such tweedie models are known as _",
    "compound poisson - gamma models_. a compound poisson - gamma model can be generated via a sum of gamma - distributed random variables , with the number of summed terms itself poisson distributed @xcite .",
    "a compound poisson - gamma model appears well - suited to describe the mechanism of interaction between the signal and the receptive field of the sensory organ . in the olfactory system , for example , odourant molecules bind with receptor sites on the cilia in the epithelial layer",
    ". the number of receptor sites on each cilium may well be poisson distributed .",
    "each receptor site is a cluster and cluster sizes are often modelled by gamma distributions .",
    "similar arguments have been made in justification for the use of a compound poisson - gamma model in a wide range of complex systems from population dynamics and genomics to rainfall modelling ( e.g. @xcite ) .",
    "the last assumption ( * a6 * ) concerns the constancy of the index of dispersion .",
    "the distribution of the response @xmath2 in the asymptotic , equilibrium limit is a normal distribution with mean and variance derived in appendix [ sec : a1 ] .",
    "the normal distribution of @xmath2 can be proved using laplace s method or the saddle - point approximation . despite a continuous distribution being attributed to @xmath2 ,",
    "the neural response is fundamentally a discrete variable ( counts per unit interval of time ) .",
    "in fact , the firing rate is derived experimentally from the neural count @xmath75 ( number of spikes ) summed over the time interval @xmath76 . at equilibrium ,",
    "@xmath77 and the index of dispersion over a finite time window is also referred to as the _ fano factor _",
    "( e.g. @xcite ) .",
    "* a6 * is therefore an assumption about the fano factor of the sensory response .",
    "a constant fano factor has been reported widely across different sensory modalities and different neuron types ( e.g. @xcite ) ; its empirical veracity is generally accepted . the dispersion index or fano factor acts like a signal - to - noise ratio and its constancy",
    "is thought to allow for the decoding of intensity information from the spike train ( e.g. @xcite ) .",
    "the existence of a perceptual arrow of time has been demonstrated through the identification of a new type of entropy called sensory entropy .",
    "sensory entropy is a direct extension of traditional , physics - based entropy or information . as such",
    ", the study of sensory processing can be viewed as a continuation of the methods of statistical physics .",
    "the equating of entropy to neural response can be seen in direct parallel to @xmath8 as proposed by boltzmann and gibbs in early statistical mechanics .",
    "this work was supported by a discovery grant from the natural sciences and engineering research council of canada ( nserc ) .",
    "the author is grateful for the helpful discussion of the manuscript with professors kenneth norwich , harel shouval and bent jrgenson .",
    "35 natexlab#1#1bibnamefont # 1#1bibfnamefont # 1#1citenamefont # 1#1url # 1`#1`urlprefix[2]#2 [ 2][]#2    , _",
    "_ ( , ) , ed .    , * * , ( ) .    , * * , ( ) .    , * * , ( ) .    , * * , ( ) .    , * * , ( ) .    ,",
    "_ _ ( , ) .    , * * , ( ) .    ,",
    "thesis , ( ) .    , * * , ( ) .",
    ", , , * * , ( ) .    , * * , ( ) .    , ( ) .    ,",
    "_ _ ( , ) .    , * * , ( ) .    , * * , ( ) .    , * * , ( ) .    , * * , ( ) .    , , , * * , ( ) .    ,",
    "_ _ ( , ) .    , _",
    "_ , vol .",
    "( , ) .    , , , * * , ( ) .    , * * , ( ) .    ,",
    "_ _ ( , ) .    , * * , ( ) .    , * * , ( ) .    , * * , ( ) .    ,",
    "_ _ ( , ) .    , * * , ( ) .    , * * , ( ) .    , in _ _ , edited by ( , ) , pp .",
    ", , , * * , ( ) .    , * * , ( ) .    , in _ _ , edited by , , ( , ) , pp . .",
    "in the limit of large @xmath29 , we observe from eq .",
    "( [ hfunction ] ) that @xmath78 where @xmath79 has been replaced by the sample variance @xmath64 . secondly for large @xmath29",
    ", we have by cochrane s theorem @xmath80 that is , irrespective of the original distribution for @xmath6 , the sample variance has an asymptotic chi - squared distribution with @xmath29 degrees of freedom . from this",
    ", the mean and variance of @xmath2 can be calculated using the expectation and variance operators acting on eq .",
    "( [ approxh ] ) together with @xmath22 .",
    "thus , in the limit of large @xmath29 , @xmath81 where the result for @xmath82 follows from the fact that the variance of a chi - squared variable equals @xmath83 . the dispersion ratio is then @xmath84 at equilibrium , @xmath58 and eq .",
    "( [ onebeforemeq ] ) is obtained .",
    "the equations governing sensory entropy can be solved for different inputs or experimental configurations , much like how schrdinger s equation can be solved for different potentials and systems .",
    "the steps are as follows : ( 1 ) parameterize signal magnitude as a function of time @xmath85 ; ( 2 ) calculate from this the equilibrium sample size @xmath54 ; ( 3 ) solve the differential equation for @xmath86 ; ( 4 ) obtain the response @xmath2 from both @xmath87 and @xmath86 .",
    "the value of @xmath29 is continuous across boundaries .",
    "here we illustrate the example of adaptation and recovery , where a constant signal is turned on and later turned off .    given input signal @xmath88 we assume that the neuron is fully equilibrated ( i.e. fully adapted ) prior to @xmath89 .",
    "we divide the solution into three distinctive regions : region i ( @xmath90 ) , ii ( @xmath91 ) and iii ( @xmath92 ) .",
    "the sample size is given by @xmath93 where @xmath94 and @xmath95 .",
    "continuity ensures that @xmath96 and @xmath97 .",
    "substitution of @xmath29 and @xmath51 into eqs .",
    "( [ gut1])-([gut4 ] ) gives the response of the neuron @xmath2 for all three regions .",
    "the equations show a response typical of what is observed during a neural adaptation experiment ( initial rapid rise followed by slow decay of spike frequency ) . at the cessation of input , a recovery",
    "is observed as the neuron returns to equilibrium ( @xmath98 ) .",
    "note that even when there is no external input , spontaneous activity is present ( @xmath99 ) .",
    "figure [ fig : fig1 ] shows several commonly observed experimental conditions .",
    "each condition represents a parameterization of the signal magnitude @xmath51 as a function of time .",
    "the response @xmath2 can be calculated from eqs .",
    "( [ gut1])-([gut4 ] ) using an analytical approach illustrated in appendix [ sec : a2 ] or with a numerical solution of the differential equation . in either case",
    ", a response typical of those shown in figure [ fig : fig1 ] can be obtained over a wide range of parameter values provided that the choice of parameters conform to an appropriate range of values to be discussed next .",
    "a total of five parameters are required to generate a response .",
    "the first is the scaling parameter @xmath5 which is positive .",
    "it has been suggested that @xmath5 satisfies the equation @xmath100 where @xmath101 and @xmath102 are estimated from the peak and steady - state values of the adaptation curve @xcite .",
    "equation ( [ magic ] ) has been interpreted as the upper bound in information transmission ( channel capacity ) of a sensory neuron although it is currently no more than a rule of thumb . for @xmath103 , there are no restrictions except that it is positive .",
    "the contribution of internal sources @xmath52 is also positive although it is generally much smaller in magnitude than the external source , i.e. @xmath104 . the exponent @xmath26 was discussed earlier and lies in the range @xmath105 .",
    "finally , the parameter @xmath106 determines the time constant of adaptation and is greater than zero .",
    "so long as these restrictions are obeyed , solutions similar to those shown in figure [ fig : fig1 ] can be obtained .",
    "such responses are commonly observed in neurophysiological experiments .",
    "next we consider a comparison with real data .",
    "the challenge is to find an experiment setup that allows for the determination of five unknown parameters .",
    "for example , a typical adaptation curve yields no more than three parameters  a single data set will not suffice .",
    "however , a suitable combination of two or more experiments involving different conditions can allow for a more robust determination of parameter values .",
    "the simultaneous fitting of multiple datasets also provides for a much more stringent test of the theory .",
    "an example is shown in figure [ fig : fig2 ] showing data obtained from an adaptation experiment ( constant @xmath51 ; duration @xmath107 is varied ) and intensity - rate experiment ( constant @xmath107 ; @xmath51 is varied ) @xcite .",
    "data was recorded from the auditory fiber of an anesthetized mongolian gerbil . in the adaptation experiment ,",
    "the number of spikes counted in a 960 ms interval was converted to a firing rate and observed as a function of time .",
    "an averaged firing rate was obtained over 91 trials .",
    "figure [ fig : fig2]a ( jagged line ) shows the response to a 39 db spl tone presented at the characteristic frequency of the fiber ( 2.44 khz ) . in the intensity - rate experiment ,",
    "the maximal firing rate during a one millisecond interval is recorded as a function of different sound intensities .",
    "figure [ fig : fig2]b shows the intensity - rate response curve ( open circles ) . after 40 db , the response saturates .    ) -([gut4 ] ) using a common set of parameters for both figures .",
    "( a ) firing rate measured as a function of sound duration for a 39 db tone ( jagged line ) .",
    "( b ) peak firing rate measured as a function of sound intensity in decibels ( open circles).,scaledwidth=40.0% ]    the theoretical expression used to fit the data was obtained from the solution provided in appendix [ sec : a2 ] . from eq .",
    "( [ middle ] ) , the response is @xmath108 \\label{adapt}\\ ] ] since both experiments were conducted on the same auditory fiber , a common set of five parameters was used ( @xmath109 , @xmath110 , @xmath111 , @xmath112 and @xmath113 hz ) .",
    "stimulus magnitude @xmath51 was calculated from rms pressure relative to 20 @xmath49pa .",
    "an additional fitting parameter was required for figure [ fig : fig2]b since the intensity - rate experiment does not conform to the condition of constant stimulus duration @xmath107 .",
    "a value of @xmath114ms was introduced to capture the approximate `` average '' recording duration .",
    "thus a total of six parameters was used to fit the results of two separate experiments yielding approximately three parameters per curve .    using the peak and asymptotic values for the firing rate obtained in figure [ fig : fig2]a",
    ", we see that the choice of @xmath115 conforms well with eq .",
    "( [ magic ] ) .",
    "moreover , the exponent @xmath112 lies in the expected range of @xmath105 .",
    "the value of @xmath26 also implies that the equilibrium response eq .",
    "( [ feq ] ) grows with exponent @xmath116 , a value suggestive of the power function exponent found for psychophysical scaling laws in loudness vs. sound pressure .    despite the compatibility of theory with data",
    ", it should be noted that the current approach also has a number of limitations . since the theory was derived in the limit of the large sample , near equilibrium limit ( @xmath117 , where @xmath118 ) , the theory is expected to perform poorly at points where there are rapid changes in the stimulus .",
    "the theory also fails to capture several important characteristics of real sensory neurons .",
    "for example , the theory does not predict a saturation in response due to the refractory period of the neuron , nor does it always capture the complete response to a complex signal ."
  ],
  "abstract_text": [
    "<S> a perceptual arrow of time is demonstrated through the derivation of equations governing the acquisition of sensory information at the neural level . </S>",
    "<S> only a small number of mathematical assumptions are required , with no knowledge of the detailed underlying neural mechanism . </S>",
    "<S> this work constitutes the first attempt at formalizing a biological basis for the physics of information acquisition , continuing from a series of earlier works detailing an entropy approach to sensory processing . </S>"
  ]
}