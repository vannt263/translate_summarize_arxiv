{
  "article_text": [
    "the closest pair problem ( cpp ) has a rich history and has been extensively studied . on an input set of @xmath2 points ,",
    "the problem is to identify the closest pair of points .",
    "a straight forward algorithm for cpp takes quadratic ( in @xmath2 ) time .",
    "most of the algorithms proposed in the literature are concerned with the euclidean space . in the rest of this paper ,",
    "unless otherwise mentioned , we imply the euclidean space . in his seminal paper ,",
    "rabin proposed a randomized algorithm with an expected run time of @xmath3 @xcite ( where the expectation is in the space of all possible outcomes of coin flips made in the algorithm ) .",
    "algorithm used the floor function as a basic operation . in 1979 , fortune and hopcroft presented a deterministic algorithm with a run time of @xmath4 assuming that the floor operation takes @xmath5 time @xcite . both of these algorithms assume a constant - dimensional space ( and the run times have an exponential dependency on the dimension ) .",
    "the algorithm of preparata and shamos for points in 2d is deterministic and runs in @xmath6 time @xcite .",
    "yao has proven a lower bound of @xmath7 on the algebraic decision tree model ( for any dimension ) @xcite .",
    "this lower bound holds under the assumption that the floor function is not allowed .",
    "the algorithm of khuller and matias is also randomized and has an expected linear run time utilizing the floor operation @xcite .",
    "there are two steps in the algorithm . in the first step ,",
    "the distance between the closest pair of points is estimated within a factor of 3 . in the second step ,",
    "the neighborhood of each point @xmath1 is explored to identify those points that are within a distance of @xmath8 from @xmath1 , where @xmath8 is the estimate that step 1 comes up with .",
    "using this neighborhood information , the correct pair is identified .",
    "one of the major issues with the above algorithms is the fact that their run times are exponentially dependent on the dimension .",
    "for example , the expected run time of @xcite s algorithm is @xmath9 on @xmath2 points from a @xmath10-dimensional space .",
    "so , even for a moderate value of @xmath10 , the algorithm may be very slow in practice .",
    "there are numerous applications for which the dimension is very large . in the following sections , we consider two such application domains .",
    "time series motif mining ( tsmm ) is a crucial problem that can be thought of as cpp in a large dimensional space . in one version of the tsmm problem , we are given a sequence @xmath11 of real numbers and an integer @xmath12 .",
    "the goal is to identify two subsequences of @xmath11 of length @xmath12 each that are the most similar to each other ( from among all pairs of subsequences of length @xmath12 each ) .",
    "these most similar subsequences are referred to as _ time series motifs_. let @xmath13 be a collection of all the @xmath12-mers of @xmath11 .",
    "( an @xmath12-mer is nothing but a contiguous subsequence of @xmath11 of length @xmath12 ) .",
    "clearly , the @xmath12-mers in @xmath13 can be thought of as points in @xmath14 . as a result ,",
    "the tsmm problem is the same as cpp in @xmath14 .",
    "any of the above mentioned algorithms can thus be used to solve the tsmm problem .",
    "a typical value for @xmath12 of practical interest is several hundreds ( or more ) . for these values of @xmath12 , the above algorithms ( @xcite,@xcite,@xcite,@xcite ) will take an unacceptable amount of time ( because of the exponential dependence on the dimension ) . designing an efficient practical and exact algorithm for the tsmm problem",
    "remains an ongoing challenge .",
    "mueen , et al .",
    "have presented an elegant exact algorithm called mk for tsmm @xcite .",
    "mk improves the performance of the brute - force algorithm with a novel application of the triangular inequality .",
    "mk is currently the best - performing algorithm in practice for tsmm .",
    "a number of probabilistic as well as approximate algorithms are also known for solving this problem ( see e.g. , @xcite ) .",
    "for instance , the algorithm of @xcite exploits algorithms proposed for finding @xmath15-motifs from biological data .",
    "the idea here is to partition the time series data into frames of certain width .",
    "followed by this , the mean value in each frame is computed .",
    "this mean is quantized into four intervals and as a result , the original time series data is converted into a string of characters from an alphabet of size 4 .",
    "finally , any @xmath15-motif finding algorithm is applied on the transformed string to identify the time series motifs .",
    "an application of great interest in bioinformatics is genome wide association study ( gwas ) .",
    "a lot of effort has been spent to identify mappings between phenotypical traits and genomic data . due to the advent of next generation high throughput sequencing technologies , nowadays it is possible to study the genomic structure of individuals in detail .",
    "single nucleotide polymorphisms ( snps ) are positions in the genome where nucleotides vary among individuals @xcite .",
    "snps in the human genome are considered to be responsible for different phenotypical traits . in gwas",
    ", two different problems have been focused on . in single locus association study",
    ", researchers try to find out the association between phenotypical traits and individual snps . in two locus association study ,",
    "the goal is to figure out the association between pairs of snps and phenotypical traits .",
    "a major task in this study is that of identifying the most correlated pair of snps .",
    "two locus associations are also known as gene - gene interactions .",
    "such interactions are believed to be major factors responsible for many complex phenotypical traits @xcite .",
    "given that the number of snps found in humans is @xmath16 to @xmath17 , a brute force way of scanning through every possible pair of snps to identify the most correlated pair is not feasible in practice .",
    "a number of algorithms for the two locus problem can be found in the literature .",
    "for instance , genetic algorithms are used in @xcite and @xcite .",
    "the algorithms proposed in @xcite and @xcite take @xmath18 time , where @xmath2 is the number of snps and @xmath19 is the number of subjects .",
    "an algorithm with an expected run time of @xmath20 , where @xmath21 is a constant , has been presented in @xcite .",
    "this algorithm exploits an algorithm known for the light bulb problem @xcite and locality sensitive hashing ( lsh ) @xcite .",
    "in this paper we present efficient algorithms for cpp .",
    "our algorithms improve the results reported in several papers including @xcite , @xcite , and @xcite .",
    "our main contributions can be summarized as follows :    1 .",
    "we improve the cpp algorithm of @xcite by introducing a novel idea .",
    "specifically , @xcite s algorithm is based on two basic ideas .",
    "we contribute a third idea that results in an improvement of the run time for exact tsmm by a factor of around 1.5 .",
    "we also show how to extend our algorithm to solve the fixed radius nearest neighbors problem ( frnnp ) .",
    "2 .   we present an algorithm for cpp when the domain of interest has strings of characters ( from a finite alphabet ) and the metric is hamming distance .",
    "it turns out that mk does not perform well for the case of hamming distance , and to be fair , we note that the authors of mk do not claim it might .",
    "a comparison of our algorithm with mk reveals that our algorithm outperforms mk ( by a factor of around 200 ) .",
    "our algorithm can also be used in approximate tsmm .",
    "specifically , instead of using @xmath15-motif search algorithms , our algorithm can be used in @xcite . in this case , the run time of the algorithm in @xcite will improve significantly , since exact algorithms for solving the @xmath15-motif search problem take time that is exponential in @xmath12 and @xmath10 .",
    "our algorithm is a modified version of the light bulb algorithm of @xcite .",
    "3 .   the light bulb algorithm of @xcite finds the most correlated pair of bulbs .",
    "the light bulb problem can be thought of as cpp in the space of * binary * strings with hamming distance as the metric .",
    "we extend this algorithm when the strings are from an arbitrary ( finite ) alphabet .",
    "more importantly , we present an algorithm for finding the * least * correlated pair of strings ( from an arbitrary alphabet ) .",
    "the algorithm of @xcite also solves this problem utilizing locality sensitive hashing ( lsh ) @xcite .",
    "our algorithm does not use lsh .",
    "instead , it uses a novel deterministic mapping function that we have come up with .",
    "4 .   using the above algorithm for finding the least correlated pair",
    ", we present a novel algorithm for solving the two locus gwas problem .",
    "an experimental comparison reveals that our algorithm is four times faster than the algorithm of @xcite .",
    "we note here that the authors of @xcite use pearson s correlation coefficient to measure the similarity between a pair of snps , whereas we use the complement of the hamming distance as the measure of similarity .",
    "the rest of this paper is organized as follows . in section  [ sec2 ]",
    "we provide some notations . in section  [ sec3 ]",
    "we present our improved algorithm for cpp ( called mpr ) and compare it with mk . in this section",
    "we also provide an analysis of mk and experimentally compare the performances of mk and mpr .",
    "section  [ sec4 ] deals with the case of character strings and hamming distance .",
    "specifically , we show how to modify the light bulb algorithm of @xcite to get an algorithm for finding the most correlated pair of strings from an arbitrary finite alphabet .",
    "we compare this algorithm with mk experimentally . in section  [ sec5 ]",
    "we present an algorithm for finding the least correlated pair of strings .",
    "this algorithm is based on a novel mapping function that we have come up with .",
    "section  [ sec6 ] is devoted to the problem of two locus association in gwas .",
    "in particular , we present a novel algorithm for this problem and compare our algorithm with that of @xcite .",
    "some concluding remarks are given in section  [ sec7 ] .",
    "let @xmath22 be a sequence of real numbers ( or characters from a finite alphabet ) .",
    "an @xmath12-mer of @xmath23 is nothing but a subsequence of @xmath23 of @xmath12 contiguous elements of @xmath23 .",
    "the @xmath12-mers of @xmath23 are @xmath24 , for @xmath25 .    if the elements of @xmath23 are real numbers , then the euclidean distance between @xmath26 and @xmath27 , denoted as @xmath28 , is @xmath29 .",
    "if the elements of @xmath23 are characters from an alphabet @xmath30 , then the hamming distance between @xmath26 and @xmath27 , denoted as @xmath28 , is @xmath31 where @xmath32 if @xmath33 and @xmath34 if @xmath35 ( for any @xmath36 ) .",
    "a sequence of characters can be thought of as a string of characters , since we can obtain a string from the sequence by concatenating the characters .",
    "thus we ll use the terms ` a sequence of characters ' and ` a string of characters ' interchangeably .",
    "let @xmath37 and @xmath38 be two sequences of characters .",
    "also , let the hamming distance between @xmath39 and @xmath40 be @xmath10 .",
    "then , by the _ number of matches _ between @xmath39 and @xmath40 we mean @xmath41 . also , the _ correlation _ between @xmath39 and @xmath40 is defined to be @xmath42 .",
    "the input for this problem are a sequence @xmath22 and an integer @xmath12 .",
    "the goal is to find two @xmath12-mers of @xmath23 that are the closest to each other ( from among all the pairs of @xmath12-mers of @xmath23 ) .",
    "a general version of this problem is one where the input consists of @xmath2 points from @xmath14 and we want to identify the two closest points .",
    "a straight forward algorithm will compute the distance between every pair of @xmath12-mers and output the pair with the least distance . since we can compute the distance between two @xmath12-mers in @xmath43 time , this simple algorithm for tsmm will run in a total of @xmath44 time .",
    "the mk algorithm of @xcite speeds up the brute force method by pruning off a large number of pairs that can not possibly be the closest .",
    "there are two main ideas used in mk .",
    "the first idea in the algorithm is to speedup the computation of distances .",
    "let @xmath45 and @xmath46 be any two @xmath12-mers . to compute the distance between @xmath47 and @xmath48 , the algorithm keeps adding @xmath49 for @xmath50 .",
    "when the sum exceeds @xmath51 , this pair is immediately dropped ( without completing the rest of the distance computation ) .",
    "this technique is known as _",
    "early abandoning_.",
    "the second idea in mk uses the triangular inequality in a novel way .",
    "let @xmath47 and @xmath48 be any two @xmath12-mers . at any stage in the algorithm , we have an upper bound @xmath52 on the distance between the closest pair of @xmath12-mers .",
    "if @xmath53 can be inferred to be greater than @xmath52 , then we can drop the pair @xmath54 from future consideration ( since this pair can not be the closest ) .",
    "ideally , we would like to calculate @xmath53 exactly for every pair of @xmath12-mers @xmath47 and @xmath48 . but this will take too much time .",
    "mk circumvents this problem by * estimating * the distance between @xmath47 and @xmath48 via the triangular inequality .",
    "in particular , a random reference @xmath12-mer @xmath55 is chosen and the distance between each @xmath12-mer and @xmath55 is computed .",
    "the @xmath12-mers are kept in an ascending order of their distances to @xmath55 . from thereon ,",
    "@xmath56 is used as a lower bound on @xmath53 .",
    "if this lower bound is @xmath57 , then @xmath54 is dropped from future consideration .",
    "the above algorithm is generalized to employ multiple reference @xmath12-mers .",
    "the use of multiple references speeds up the algorithm .      in this section",
    "we provide an ( informal ) analysis of the mk algorithm to explain why the algorithm has a very good performance .",
    "specifically , if we choose multiple random reference points , the algorithm achieves a much better run time than having a single reference point . we explain why this is the case .    for ease of understanding",
    "consider the 2d euclidean space .",
    "the analysis can be extended to points in @xmath14 .",
    "for any two @xmath12-mers @xmath47 and @xmath48 , the closer @xmath56 is to @xmath53 , the better will be our estimate and hence the better will be our chance of dropping @xmath54 ( if @xmath54 is not the closest pair ) .",
    "it turns out that the quality of the lower bound @xmath56 is decided by two factors : 1 ) the angle @xmath58 and 2 ) @xmath59 .",
    "we illustrate this with an example .",
    "let @xmath60 and @xmath61 .",
    "consider a reference point @xmath62 ( figure  [ figure1](a ) ) .",
    "note that @xmath63 is at a distance of @xmath64 from @xmath47 . in this case @xmath65 .",
    "also , @xmath66 .",
    "let @xmath67 be the point we get by keeping the distance between the reference point and @xmath47 the same , but changing this angle to @xmath68 ( figure  [ figure1](b ) ) . in this case",
    ", @xmath69 improves to @xmath70 . as another example , if the reference point @xmath55 lies on the perpendicular bisector of @xmath47 and @xmath48 , then @xmath71        [ fig:1 ]    for any two input points @xmath47 and @xmath48 , if we pick multiple reference points randomly , then we would expect that at least one of these reference points @xmath55 will be such that the angle @xmath58 will be such that @xmath72 will be ` large ' .",
    "in contrast , if we have only one reference point , for some pairs of points the corresponding angles may be ` good ' , but for a good percentage of the pairs , the angles may not be ` good ' .",
    "( a reference point @xmath55 is ` good ' if @xmath72 is close to @xmath53 ) .",
    "the effect of @xmath59 on @xmath72 can be seen with the same examples .",
    "consider the example of figure  [ figure1](a ) .",
    "assume that we keep the angle the same but increase @xmath73 to @xmath74 and get the reference point @xmath75 ( figure  [ figure1](c ) ) .",
    "in this case , @xmath76 improves to @xmath77 .",
    "also , in the example of figure  [ figure1](b ) , say we keep the angle the same but increase @xmath78 to @xmath74 and get the reference point @xmath79 . in this case",
    ", @xmath80 improves to @xmath81 . of course",
    ", if the reference point @xmath55 lies on the perpendicular between @xmath47 and @xmath48 , then however large @xmath59 could be , @xmath72 will continue to be zero !",
    "however , the probability of this happening is low . for a given angle @xmath82",
    ", we can compute the limit of @xmath72 as @xmath59 tends to @xmath83 .",
    "for instance when the angle is @xmath84 ( figure  [ figure1](a ) ) , this limit is @xmath85 .",
    "our proposed new algorithm indeed exploits the relationship between @xmath59 and @xmath72 .",
    "in particular , we pick a collection @xmath13 of random reference points and project each of these points out by multiplying each coordinate value of each point by a factor of @xmath86 .",
    "for example , @xmath86 could be @xmath74 .",
    "the rest of the algorithm is the same as mk .",
    "a pseudocode for our algorithm , called _ motif discovery with projected reference points ( mpr ) _ , is given below .",
    "* algorithm * mpr + _ input : _",
    "@xmath22 and an integer @xmath12 , where each @xmath87 is a real number ( for @xmath88 ) .",
    "input are also @xmath89 and @xmath86 , where @xmath89 is the number of references and @xmath86 is the projection factor .",
    "+ _ output : _ the two closest @xmath12-mers of @xmath23 .",
    "+ * 1 ) * pick @xmath89 random @xmath12-mers of @xmath23 as references ; project these references by multiplying each element in each @xmath12-mer by @xmath86 . let these projected references be @xmath90 .",
    "+ * 2 ) * compute the distance between every @xmath12-mer of @xmath23 and every projected reference @xmath12-mer .",
    "+ * 3 ) * sort the @xmath12-mers of @xmath23 with respect to their distances to @xmath63 .",
    "let the sorted @xmath12-mers be @xmath91 .",
    "+ * 4 ) * let @xmath92 ; let @xmath93 ; + * 5 ) for * @xmath94 * to * @xmath95 * do * + @xmath96 * to * @xmath95 * do * + @xmath97 ; + @xmath98 * to * @xmath89 * do * + @xmath99 * then * + @xmath100 ; * exit * ; + @xmath101 * then exit else * + compute @xmath102 ; + @xmath103 * then * + @xmath104 ; @xmath105 ; + * 6 ) output * @xmath106 .",
    "* observation : * please note that even though the above algorithm has been presented for solving the tsmm problem , it is straight forward to extend it to solve cpp in @xmath14 .      in this section",
    "we show why our idea of projecting reference points improves the performance of the algorithm .",
    "let the input points be from @xmath107 for some integer @xmath10 .",
    "consider any two input points @xmath39 and @xmath40 .",
    "let @xmath0 be any reference point .",
    "note that any three points are coplanar .",
    "consider any hyperplane @xmath108 containing @xmath109 and @xmath0 .",
    "if we multiply every coordinate of @xmath0 by the same number , then the resultant point will also lie in @xmath108 .",
    "this is because the equation defining @xmath108 will be of the form @xmath110 .",
    "thus , in order to see how @xmath111 changes with a scaling of @xmath0 , it suffices to consider the case that these three points are in 2d .    without loss of generality",
    "let @xmath39 be @xmath112 and @xmath40 be @xmath113 , for some real number @xmath114 .",
    "there are two cases to consider for the position of @xmath0 relative to @xmath39 and @xmath40 : 1 ) @xmath0 is to the right of the perpendicular bisector of @xmath39 and @xmath40 ; 2 ) @xmath0 is to the left of the perpendicular bisector between @xmath39 and @xmath40 .",
    "these two cases are illustrated in figures  [ fig2](a ) and [ fig2](b ) , respectively .",
    "note that when @xmath0 lies on the perpendicular bisector of @xmath39 and @xmath40 , @xmath111 will be zero .        in case 1 ,",
    "let @xmath115 be @xmath116 , @xmath117 being a scaling factor .",
    "@xmath118 , @xmath119 , and @xmath120 . as a result , @xmath121 @xmath122 . using the fact that @xmath123 ( when @xmath124 ) , @xmath125 .",
    "thus , @xmath126 .",
    "clearly , when @xmath114 and @xmath82 are the same , the value of @xmath111 increases when @xmath117 increases .    in case 2 , let @xmath115 be @xmath116 , for a scaling factor of @xmath117 .",
    "clearly , @xmath118 , @xmath127 .",
    "thus , @xmath128 .",
    "also , @xmath129 . using the approximation mentioned in case 1",
    ", we see that @xmath130 .",
    "therefore , @xmath131 . in this case , when @xmath114 and @xmath82 are the same , the value of @xmath132 increases when @xmath117 decreases .",
    "but for a given reference point , and two input points @xmath39 and @xmath40 , we do not know which of the two cases will hold .",
    "but we can expect that half of the randomly chosen reference points will fall under case 1 and the other half will be expected to fall under case 2 . if we only employ a scaling factor @xmath117 that is greater than one , then , for an expected half of the reference points we expect to see an improvement in the estimate of a lower bound for @xmath133 . this explains why our algorithm performs better than mk .",
    "the above analysis can also be used to better understand the mk algorithm .      in this problem",
    "we are given @xmath2 points @xmath134 in @xmath14 and a radius @xmath0 ( which is a real number ) and the problem is to identify the @xmath0-neighborhood of each input point .",
    "if @xmath1 is an input point , its @xmath0-neighborhood is defined to be the set of all input points that are within a distance of @xmath0 from @xmath1 .",
    "frnnp has numerous applications .",
    "one of the applications of vital importance is that of molecular simulations .",
    "we can modify mpr to solve this problem as well .",
    "the modified version is given below .",
    "let @xmath135 denote the @xmath0-neighborhood of @xmath136 .    * algorithm * mpr - frnns + _ input : _ @xmath22 and @xmath0 , where each @xmath87 is a point in @xmath14 ( for @xmath88 ) and",
    "@xmath0 is a real number .",
    "input are also @xmath89 and @xmath86 , where @xmath89 is the number of references and @xmath86 is the projection factor .",
    "+ _ output : _",
    "@xmath135 , for @xmath88 . +",
    "* 1 ) * pick @xmath89 random points of @xmath23 as references ; project these + references by multiplying each coordinate of each reference + point by @xmath86 .",
    "let these projected references be @xmath90 .",
    "+ * 2 ) * compute the distance between every point of @xmath23 and + every projected reference point .",
    "+ * 3 ) * sort the points of @xmath23 with respect to their distances + to @xmath63 .",
    "let the sorted points be @xmath137 . + * 4 ) for * @xmath94 * to * @xmath2 * do * @xmath138 ; + * 5 ) for * @xmath94 * to * @xmath2 * do * + @xmath96 * to * @xmath2 * do * + @xmath97 ; + @xmath98 * to * @xmath89 * do * + @xmath139 * then * + @xmath100 ; * exit * ; + @xmath101 * then exit else * + compute @xmath102 ; + @xmath140 * then * add @xmath141 to @xmath135 and + add @xmath142 to @xmath143 ; + * 6 ) output * @xmath144 for @xmath88 .",
    "a typical algorithm in the literature for cpp has two phases . in the first phase pairs of points that can not possibly be the closest",
    "are eliminated . in the second phase distance",
    "is computed between every pair of points that survive the first phase .",
    "the time spent in the first phase is typically very small and hence is negligible ( compared to the time spent in the second phase ) .",
    "also , the time needed to process the pairs in the second phase is linear in the number of surviving pairs . as a result , it suffices to report the number of pairs ( to be processed in the second phase ) as a measure of performance ( see e.g. , @xcite ) . in this paper",
    "also we use this measure of performance throughout .",
    "we have experimentally compared the performance of mk and mpr on different data sets .",
    "the machine we have used has an intel(r ) core(tm ) i7 - 2640 m 2.8 ghz cpu with 8 gb ram running windows 7 ( 64 bit ) .",
    "the same machine has been used for all the experiments reported in this paper .    as mentioned in @xcite ,",
    "random walk data set is the most difficult case for time series mining algorithms since the probability of the existence of very close motif pairs is very low .",
    "we have also used the same data for our comparison .",
    "in particular , we have used 10 different random walk data sets of sizes ranging from 10k to 100k .",
    "we have also varied the motif length to see how the performances change .",
    "our algorithm performs better than mk for higher motif lengths .",
    "both the algorithms have been run 10 times and the averages computed .",
    "we do not perform any comparison with the brute force method as that has already been done in @xcite .    in table",
    "[ table1 ] we show the number of pairs processed ( in the second phase ) in mk and mpr .",
    "the size ( i.e. , the length ) of the time series data varies from 10k to 100k , the motif length being 1024 .",
    "the following parameter values have been used : @xmath145 and @xmath146 . from this table",
    "we see that mk processes around 1.5 times the number of pairs processed by mpr .",
    "figure  [ figure2 ] presents a graphical plot of the runtime requirements of mk and mpr algorithms .",
    "this figure shows that the run time of mk is around 1.5 times the run time of mpr .",
    "this improvement is quite significant for the following reason : there are two ideas used in mk , namely , early abandoning and the use of random reference points . as the authors point out in @xcite , the difference between using early abandoning alone and both the ideas is small , especially on random walk data sets .",
    "however , mk performs much better than using early abandoning alone on real datasets .",
    "table  [ table2 ] and figure  [ figure3 ] show how the number of pairs reduces with an increase in the number @xmath89 of the reference points for mk and mpr algorithms . from these",
    ", we note that as @xmath89 increases , the difference between mk and mpr widens .",
    "the bar graph in figure  [ figure3 ] pictorially represents this comparison .",
    "the blue and red bars represent the number of pairs processed by mk and mpr , respectively .",
    "we have run both mk and mpr algorithms on some of the real data sets from http://www.cs.ucr.edu/~mueen / mk/. table  [ table3 ] shows the performance of mk and mpr algorithms . on dataset 1 , mpr is around 2 times faster than mk and on dataset 2 , mpr is around 1.5 times faster than mk . in this table , ` - ' indicates that the algorithm did not stop within 40 minutes .",
    "in this section we consider the cpp when the space is one of character strings and the metric is hamming distance .",
    "an algorithm for this version of cpp has numerous applications including approximate tsmm ( see e.g. , @xcite ) . when the alphabet is @xmath147 , the light bulb algorithm of @xcite can be used to solve this problem . in this section",
    "we show how to modify the light bulb algorithm for the case of generic alphabets . before presenting details on the modification",
    ", we provide a brief summary of the light bulb problem .",
    "the light bulb problem is that of identifying the most correlated pair of bulbs from out of @xmath2 given bulbs @xmath148 .",
    "this problem is solved by observing the state of each bulb in @xmath149 discrete time steps ( for some relevant value of @xmath149 ) .",
    "the states of bulb @xmath142 in these @xmath149 time steps can be represented as a vector @xmath150 ( for @xmath88 ) .",
    "we can think of @xmath151 as a sample from a probability distribution that governs the state of bulb @xmath142 .",
    "it can be shown that if @xmath149 is sufficiently large ( e.g. , @xmath152 ) , then the pair of bulbs that is the most correlated in the samples is also the most correlated pair with high probability .",
    "thus the light bulb problem can be stated as follows : we are given @xmath2 boolean vectors @xmath153 .",
    "the problem is to find the pair of vectors that are the most similar ( i.e. , the hamming distance between them is the smallest ) . from hereon , we will use this formulation of the problem .",
    "note that , given two vectors , we can find the hamming distance between them in @xmath154 time .",
    "a straight forward algorithm to identify the most correlated pair of bulbs takes @xmath155 time .",
    "this algorithm computes the hamming distance between every pair of bulbs .",
    "the algorithm of @xcite takes subquadratic time .",
    "in particular , the expected run time of this algorithm is @xmath156 assuming that @xmath157 . here , @xmath158 is the correlation between the most correlated pair of bulbs and @xmath159 is the correlation between the second most correlated pair of bulbs .",
    "note that if the correlation between two bulbs @xmath142 and @xmath141 is @xmath160 then the expected hamming distance between @xmath151 and @xmath161 is @xmath162 .",
    "equivalently , the similarity ( i.e. , the number of matches ) between @xmath151 and @xmath161 is @xmath163 .",
    "consider a matrix @xmath164 of size @xmath165 , such that the @xmath142th row of @xmath164 is @xmath151 , for @xmath88 .",
    "the algorithm of @xcite iteratively collects pairs of bulbs that are candidates to be the most correlated .",
    "once it collects enough pairs , it computes the distance between each pair in this collection and outputs the closest .",
    "there are @xmath166 iterations in the algorithm and in each iteration , some candidate pairs are generated and added to the collection @xmath13 . in any iteration , the algorithm picks @xmath167 columns of @xmath164 at random ( for some constant @xmath114 ) .",
    "the rows are sorted based on the characters in the randomly chosen columns . as a result of this sorting",
    ", the bulbs get partitioned into buckets such that all the bulbs with equal values ( in the @xmath167 random columns ) fall into the same bucket .",
    "a pair of bulbs @xmath168 will get added to @xmath13 in any iteration if they fall into the same bucket in this iteration .",
    "the authors of @xcite show that after @xmath166 iterations , @xmath13 will have the most correlated pair of bulbs with high probability ( i.e. , with a probability of @xmath169 ) .",
    "the above algorithm has been proposed for the case of binary strings .",
    "we can modify this algorithm to handle the case of an arbitrary ( finite ) alphabet and get the following theorem .",
    "[ theorem0 ] let @xmath164 be a matrix of size @xmath165 .",
    "each entry in this matrix is an element from some set @xmath30 of cardinality @xmath170 .",
    "we can find the most correlated pair of columns of @xmath164 in an expected @xmath171 time where @xmath158 is the correlation between the most correlated pair of columns , @xmath159 is the correlation between the second most correlated pair of columns , and @xmath172 is the word length of the machine .",
    "this expected run time will be @xmath173 if we use a general sorting algorithm .",
    "( here correlation is based on hamming distance .",
    "for example , @xmath158 is the largest fraction of rows in which any two columns agree ) .",
    "* proof : * the only difference in the algorithm is that instead of sorting binary strings we will have to sort strings from an arbitrary alphabet . without loss of generality ,",
    "let @xmath174 be the alphabet under concern .",
    "in the original algorithm , one has to sort @xmath2 @xmath175-bit integers in every iteration . for a generic alphabet",
    ", we have to sort @xmath2 @xmath176-bit integers .",
    "if one uses any comparison based sorting algorithm , this sorting takes @xmath6 time .",
    "if we use an integer sorting algorithm , this sorting can be done in @xmath177 time where @xmath172 is the word length of the machine .",
    "this is the time spent in each iteration of the algorithm .",
    "therefore , the total expected run time is @xmath171 .",
    "@xmath178    let this modified version of the light bulb algorithm be called mlba .      the algorithm of @xcite for approximate tsmm partitions the input time series data @xmath23 based on a window of size @xmath172 ( for an appropriate value of @xmath172 ) , computes the mean of every window , and discretizes the mean into four possible values . as a result",
    ", the time series data is transformed into a string @xmath179 of characters form the alphabet @xmath180 .",
    "it then uses any @xmath15-motif finding algorithm to find the motifs in @xmath179",
    ". however , all the exact algorithms for finding @xmath15-motifs take time that is exponential on @xmath12 and @xmath10 .",
    "note that the last step of finding @xmath15 motifs can be replaced with a problem of finding time series motifs in @xmath179 which is nothing but cpp in the domain of strings of characters , the motif length being @xmath12 .",
    "one could employ mk to solve cpp in the domain of character strings .",
    "the only difference is that we have to replace euclidean distance with hamming distance .",
    "we have implemented this algorithm .",
    "it turns out that mk does not perform well for the case of hamming distance . to be fair",
    ", the authors of mk have not tested mk for this case .",
    "we have compared mk with mlba and the results are shown in table  [ table4 ] . as this table reveals , mlba is around 200 times faster than mk .",
    "it is also clear that if we employ mlba in place of @xmath181-motif finding algorithms , the performance of the approximate tsmm algorithm given in @xcite will improve significantly .",
    "figure  [ figure4 ] shows a runtime comparison of mk and mlba for the case of character strings from a finite alphabet .",
    "the light bulb algorithm of @xcite identifies the closest pair of strings , from out of @xmath2 given binary strings . an interesting question is if we can use the same algorithm to identify the * furthest * pair of strings .",
    "this problem has relevance in many problems including the two locus problem in gwas .",
    "the authors of @xcite present an elegant adaptation of the light bulb algorithm to solve this problem when the strings are binary .",
    "they also show how to solve this problem for arbitrary alphabets using locality sensitive hashing ( lsh ) @xcite .",
    "they map the input strings into binary strings using lsh .",
    "lsh closely preserves similarities with a high probability . in this section",
    "we show how to avoid lsh . in particular , we present novel deterministic mappings of the input strings to binary strings such that similarities are preserved deterministically .",
    "our experimental comparison shows that our algorithm has a significantly better run time than that of @xcite .",
    "let @xmath182 stand for the number of matches between two strings ( of equal length ) @xmath47 and @xmath48 .",
    "for instance , if @xmath183 and @xmath184 , then @xmath185 ( since they match in positions 2 and 4 ) .",
    "let @xmath186 and @xmath187 be two sequences of strings ( each string having the same length ) .",
    "we define @xmath188 to be @xmath189 .",
    "consider the sequences @xmath190 , for @xmath88 , where each @xmath191 is @xmath192 or @xmath193 ( for @xmath194 ) .",
    "note that each @xmath195 is a sequence of strings where each string is of length 1 .",
    "let @xmath196 .",
    "each @xmath195 can be thought of as a string from the alphabet @xmath197 . in the application of gwas",
    ", we can let @xmath195 correspond to the snp @xmath142 , for @xmath88 .",
    "specifically , @xmath191 is the value of the @xmath142th snp in subject @xmath141 , for @xmath194 .",
    "if we are interested in finding the two most correlated snps , then we can use mlba to identify this pair ( as shown in section  [ sec4 ] ) . on the other hand ,",
    "if our goal is to identify the least correlated pair , then , it is not clear how to do this using mlba . to solve the two locus gwas problem",
    ", we have to identify not only the most correlated pair of bulbs but also the least correlated pair .",
    "the authors of @xcite present an elegant solution for this problem when each @xmath195 has only zeros and ones . in this case , each @xmath195 can be thought of as a light bulb .",
    "the idea is to construct a matrix @xmath198 of size @xmath199 where each column of @xmath198 corresponds to either a bulb or its ` complement ' , specifically , the first @xmath2 columns correspond to the bulbs and the next @xmath2 columns correspond to the complements of the bulbs . in other words , @xmath200=a_j^i$ ] , for @xmath201 and @xmath200=\\bar{a}_j^i$ ] for @xmath202 . here , if @xmath47 is any bit , then , @xmath203 denotes its complement .",
    "let @xmath204 and @xmath205 . the algorithm of @xcite for finding the least correlated pair works as follows .",
    "consider all the pairs of columns @xmath168 such that @xmath206 and @xmath207 . from out of these pairs ,",
    "identify the pair @xmath208 of columns with the maximum number of matches . if @xmath209 and @xmath210 , then @xmath106 is the least correlated pair of bulbs .",
    "finding such a pair @xmath208 can be done using the light bulb algorithm of @xcite .",
    "the correctness of this algorithm follows from the fact that if the two bulbs @xmath142 and @xmath141 have the least number of matches , then , column @xmath142 and the complement of column @xmath141 will have the most number of matches .",
    "it is not clear how to extend the above idea when the sequences have three ( or more ) possible elements .",
    "the authors of @xcite reduce such general cases to the case of zeros and ones using locality sensitive hashing ( lsh ) .",
    "the measure of correlation used by @xcite is different from what we use in this paper .",
    "we define the correlation between two strings @xmath195 and @xmath211 as @xmath212 .",
    "in contrast , @xcite use pearson s correlation coefficient .    in this section",
    "we present an elegant algorithm for the problem of identifying the least correlated pair of strings without employing lsh .",
    "the idea of @xcite is to map input strings into boolean vectors .",
    "if @xmath142 and @xmath141 are any two strings , then the sequences @xmath195 and @xmath211 are mapped to boolean vectors @xmath213 and @xmath214 by lsh such that the distance between @xmath195 and @xmath211 will be nearly the same as the distance between @xmath213 and @xmath215 with some probability .",
    "the larger the length of @xmath213 is , the better will be the accuracy of lsh in preserving distances .",
    "our algorithm also maps each @xmath195 into a boolean vector @xmath213 deterministically such that @xmath216 , for @xmath88 .",
    "consider an alphabet @xmath30 with three strings where @xmath217 . clearly , @xmath218 if @xmath219 and @xmath220 if @xmath221 for any @xmath222 . also , @xmath223 if @xmath219 and @xmath224 if @xmath221 . here",
    "@xmath225 stands for the string obtained from @xmath48 by complementing each bit .",
    "for example , if @xmath226 then @xmath227 .    consider the sequences @xmath190 , for @xmath88 , where each @xmath191 is @xmath192 or @xmath193 ( for @xmath194 ) .",
    "note that each @xmath195 is a sequence of strings where each string is of length 1 .",
    "let @xmath196 .",
    "assume now that we encode each @xmath191 as follows ( for @xmath88 and @xmath194 ) : @xmath228 : and @xmath229 .",
    "let the encoded version of @xmath195 be denoted as @xmath213 for @xmath88 .",
    "note that @xmath230 , for any @xmath88 .",
    "it is easy to see that @xmath231 , for any @xmath142 and @xmath141 ( @xmath232 ) .",
    "for any @xmath233 , let @xmath234 , for @xmath88 . clearly , @xmath235 for any @xmath232 .",
    "clearly , the following statement is true : if , from out of all the pairs of strings , @xmath106 has the largest correlation , i.e. , @xmath236 is the largest , then from out of all the boolean vectors generated , @xmath213 and @xmath215 will have the largest correlation .",
    "also , if @xmath236 is the smallest , then , @xmath213 and @xmath237 will have the largest correlation ( from out of the pairs @xmath238 ) .",
    "we can now form a matrix @xmath198 of size @xmath239 where the first @xmath2 columns correspond to ( transformed ) strings and the next @xmath2 columns correspond to complements of ( transformed ) strings .",
    "let @xmath204 and @xmath205 .",
    "consider all the pairs of columns @xmath168 such that @xmath206 and @xmath207 . from out of these pairs ,",
    "identify the pair @xmath208 of columns with the maximum number of matches . if @xmath209 and @xmath210 , then @xmath106 is the least correlated pair of strings .",
    "finding such a pair @xmath208 can be done using the light bulb algorithm of @xcite .",
    "[ theorem1 ] given @xmath2 strings , we can find the closest pair of strings in an expected time of @xmath156 , where @xmath158 and @xmath159 are the largest and the second largest correlation values , respectively . also , we can find the least correlated pair of strings in an expected time of @xmath240 , where @xmath241 and @xmath242 are the smallest and the next smallest correlation values , respectively .",
    "* proof : * when we transform input strings to binary sequences , the ordering of pairs is preserved in terms of correlations as we have shown before .",
    "let @xmath158 be the correlation of the largest correlated pair and @xmath159 be the correlation of the second largest correlated pair .",
    "how do these values change in the transformed domain ?",
    "if @xmath243 and @xmath244 are the transformed values of these correlations , respectively , it can be seen that @xmath245 and @xmath246 .",
    "if @xmath241 and @xmath242 are the correlations of the smallest and the second smallest correlated pairs , respectively , and if @xmath247 and @xmath248 are the transformed values of these , respectively , then we can see that : @xmath249 and @xmath250 . to find the largest correlated pair",
    ", we can use mlba ( theorem  [ theorem0 ] ) .",
    "we use the mapping only to find the least correlated pair .",
    "@xmath178      we have thus far considered the case where the alphabet is @xmath197 .",
    "we can extend the mapping to a general alphabet and get the following theorem .    given @xmath2 strings ,",
    "we can find the largest correlated pair of strings in an expected time of @xmath156 , where @xmath158 and @xmath159 are the largest and the second largest correlation values , respectively .",
    "also , we can find the least correlated pair of strings in an expected time of @xmath251 , where @xmath241 and @xmath242 are the smallest and the next smallest correlation values , respectively , and there are @xmath170 characters in the alphabet .    * proof : * consider sequences from the alphabet @xmath252 . in this case",
    "we map each element of this alphabet to a binary string of length @xmath170 where there is only one 1 . specifically , we use the following mapping : @xmath253 etc . as",
    "before , we do nt need any mapping if our goal is to find the largest correlated pair .",
    "the mapping is used only to find the least correlated pair .",
    "@xmath178    we can improve the above theorem by employing a random mapping as follows : we will use a binary string of length @xmath170 to encode each symbol in the alphabet .",
    "the encoding for each symbol is obtained by ( uniformly ) randomly choosing each bit in the string ( of length @xmath170 ) .",
    "let @xmath47 and @xmath48 be any two symbols in the alphabet ( with @xmath221 ) and let @xmath254 and @xmath255 be their encodings , respectively . then",
    ", clearly , the expected value of @xmath256 is @xmath257 . also , the expected value of @xmath258 is @xmath257 .",
    "if @xmath114 is the correlation between a pair of strings and if @xmath259 is the transformed value , then , it follows that the expected value of @xmath259 is @xmath260 .",
    "an application of the chernoff bounds will readily imply that the value of @xmath259 will indeed be very close to this expected value with a probability of @xmath261 .",
    "therefore , we get :    given @xmath2 strings , we can find the least correlated pair of strings in an expected time of @xmath262 , where @xmath241 and @xmath242 are the smallest and the next smallest correlation values , respectively , and there are @xmath170 characters in the alphabet . @xmath178",
    "the two locus association problem is defined as follows .",
    "input is a matrix @xmath164 of size @xmath263 where @xmath264 is the number of patients ( subjects ) each with @xmath2 snps . here",
    "@xmath265 is the number of cases and @xmath266 is the number of controls .",
    "there are three possible values for each snp , namely , @xmath192 or @xmath193 .",
    "the cases are of phenotype @xmath267 and the controls are of phenotype @xmath268 .",
    "rows @xmath267 through @xmath265 of @xmath164 correspond to cases .",
    "let this submatrix be called @xmath39 .",
    "rows @xmath269 through @xmath270 of @xmath164 correspond to controls and let this submatrix be called @xmath40 .",
    "each column of @xmath164 corresponds to an snp .",
    "the two locus association problem is to identify the pair of snps whose statistical correlation with phenotype is maximally different between cases and controls . as mentioned in @xcite ,",
    "the goal is to identify the pair :    @xmath271    if @xmath272 is any matrix , then , @xmath273 stands for the correlation between the columns @xmath142 and @xmath141 of @xmath272 .",
    "the algorithm of @xcite exploits the light bulb algorithm of @xcite and locality sensitive hashing ( lsh ) @xcite .",
    "they use lsh to transform matrices @xmath39 and @xmath40 to @xmath274 and @xmath275 , respectively .",
    "in particular , each column @xmath276 of @xmath39 is converted to a column @xmath277 of zeros and ones .",
    "the size of @xmath276 is @xmath278 and the size of @xmath277 is chosen to be @xmath279 .",
    "the matrix @xmath40 is also transformed into @xmath275 in a similar manner using lsh .",
    "followed by this , the pair of interest is identified .    to be precise , using @xmath274 and @xmath275 , the matrix @xmath198 is formed where @xmath280\\ ] ]    where @xmath281 is obtained from @xmath275 by complementing every element of @xmath275 .",
    "note that @xmath198 is of size @xmath282 .",
    "let @xmath283 and @xmath284 .",
    "consider all the pairs of columns @xmath106 such that @xmath285 and @xmath286 . from out of these pairs ,",
    "identify the pair @xmath287 of columns with the maximum number of matches . if @xmath288 and @xmath289 , then @xmath168 is the pair of interest .",
    "this pair can be found using the light bulb algorithm of @xcite .",
    "we can use our mapping ideas to get the following theorem .",
    "[ theorem6.1 ] we can find the pair @xmath106 of snps that maximizes @xmath290 in an expected time of @xmath291 , where @xmath158 and @xmath159 are the smallest and the next smallest values of @xmath290 , respectively , over all possible pairs @xmath106 of snps .    * proof : * our algorithm also uses the same method except that instead of using lsh to map @xmath39 and @xmath40 to @xmath274 and @xmath275 , respectively , we employ the deterministic mapping we have proposed in section  [ mapping ] .",
    "let @xmath142 and @xmath141 be two snps ( i.e. , two columns in @xmath39 and @xmath40 ) .",
    "let @xmath292 and @xmath293 . now consider columns @xmath142 and @xmath294 of @xmath198 .",
    "what can we say about the correlation of these columns ? from the discussion in section  [ mapanalysis ] , we realize that this correlation is @xmath295 .",
    "this also proves the correctness of our algorithm .",
    "let @xmath158 be the maximum value of @xmath290 over all possible pairs @xmath106 and let @xmath159 be the second largest value . then , the run time follows from theorem  [ theorem0 ] .",
    "@xmath178    in a similar manner we can also find the pair that maximizes @xmath296 and hence identify the pair that maximizes @xmath297 .",
    "the notion of similarity ( between two snps ) used in @xcite is pearson s correlation coefficient . in this paper",
    "the similarity we use is based on the hamming distance . specifically , the complement of the hamming distance is the measure of similarity we employ .",
    "the authors of @xcite have tested their algorithms on different data sets ( including random data ) . since we do not have access to either these data sets or their algorithms , the only comparison we can do was on the random data .",
    "as explained in @xcite , we have also generated snps from binomial distributions .",
    "in particular , for each subject , the value of each snp is chosen uniformly randomly to be either 0 or 1 with equal probability .",
    "this dataset is called noise data in @xcite ( c.f . table  [ table6 ] in @xcite ) . like in @xcite",
    ", we have also generated data of sizes @xmath74k , @xmath298k , and @xmath299k . for each size",
    "we have generated two different data sets and computed the average number of pairs to be processed .",
    "we compare these numbers with the ones reported in @xcite .",
    "as can be seen from table  [ table5 ] , our algorithm is around 4 times faster than the one in @xcite .",
    "note that this a significant improvement since the typical processing times for the two locus problem are quite high .",
    "for example , the authors of @xcite report that on some of the data sets ( with no more than @xmath17 snps ) , the brute force algorithm for the two locus problem took several days on 1000 cpus ! thus any improvement in the run time could make a noticeable difference .",
    "how does one ensure that the output of an algorithm for the two locus problem is correct ?",
    "for small data sizes , one could run the exhaustive brute force algorithm to identify the correct pair and use it to verify correctness .",
    "in fact when the number of snps is either @xmath74k or @xmath298k , we first found the correct answer and then used it to measure the run time of our algorithm as follows .",
    "we ll run our algorithm one iteration at a time until the correct pair(s ) is ( are ) picked up by our algorithm . at this point",
    "we will stop and report the total number of pairs collected .",
    "the numbers shown in table  [ table5 ] have been obtained in this manner .",
    "we could not use this method for @xmath300k , since the brute force algorithm was taking too much time .",
    "when @xmath2 is very large , we _ inject _ pairs with known correlations .",
    "as an example , consider the problem of finding the largest correlated pair of columns in a @xmath301 matrix @xmath39 .",
    "say we generate each column by picking each element to be either 0 or 1 with equal probability .",
    "for any two columns , clearly , the expected correlation is @xmath302 .",
    "we can perform a probabilistic analysis to get a high probability bound on the largest correlation between any two columns .",
    "one could also get this estimate empirically .",
    "for example , for @xmath303 , we generated several random data sets and computed the largest correlation in each and calculated an average .",
    "the average maximum correlation was @xmath304 .",
    "let @xmath1 be this value . to inject a pair with a correlation of @xmath305 where @xmath305 is @xmath306 we generate a column @xmath307 with all ones and another column @xmath308 with @xmath309 ones and @xmath310 zeros .",
    "we then replace ( any ) two columns of @xmath39 with @xmath307 and @xmath308 .",
    "clearly , the correlation between @xmath307 and @xmath308 is @xmath305 .",
    "the expected correlation between @xmath307 and any other column of @xmath39 ( other than @xmath308 ) is @xmath302 .",
    "similarly , the expected correlation between @xmath308 any other column of @xmath39 ( other than @xmath307 ) is @xmath302 .",
    "thus the pair @xmath168 is likely to be the winner with high probability .",
    "we stop our algorithm when this pair is picked by our algorithm .",
    "we have picked a value for @xmath305 that is only slightly larger than @xmath1 so as to get an accurate estimate on the run time .",
    "we have used a similar technique to inject pairs for the two locus problem as well . in table",
    "[ table6 ] we show the results for our algorithm . in these cases we have generated the snps randomly from a binomial distribution as before .",
    "we have employed 200 cases and 200 controls .",
    "clearly , the expected value of @xmath311 is zero for this data .",
    "if we map this data using our deterministic mapping , then the expected correlation between any two snps will be @xmath302 . here",
    "again we empirically found that the largest correlation was around 58.3% ( when @xmath2 was @xmath74k ) .",
    "therefore , we have injected a pair whose correlation was @xmath312% .",
    "this pair is likely to be the winner .",
    "our algorithm was run until this pair was picked . at this time",
    ", the algorithm was stopped .",
    "we also checked if the algorithm picked any pair whose correlation was better than that of the injected one and found none .",
    "in all the datasets we tried , we were always able to find a pair ( other than the injected one ) whose correlation was very close to 60% and hence the numbers shown in table  [ table6 ] are very close to the case with no injections .",
    "* practical considerations : * another important question is for how long we should run the program before we can be sure that the correct pair has been obtained ( with a high confidence ) .",
    "please note that we do not know the values of @xmath158 and @xmath159 ( c.f .",
    "theorem  [ theorem6.1 ] ) .",
    "theorem  [ theorem6.1 ] suggests that the run time of our algorithm is @xmath313 for some relevant @xmath314 .",
    "we can empirically estimate @xmath314 .",
    "the idea is to measure the run time of the algorithm for various values of @xmath2 ( as explained above ) , the maximum value of @xmath2 being as much as possible ( for the given computing platform and time constraints ) .",
    "then we could use any numerical procedure to estimate @xmath314 .",
    "in this paper we have presented novel algorithms for the closest pair problem ( cpp ) .",
    "cpp is a ubiquitous problem that has numerous applications in varied domains .",
    "we have offered algorithms for the cases of euclidean as well as hamming distances .",
    "we have applied our algorithms for two well studied and important problems , namely , time series motif mining and two locus genome wide association study .",
    "our algorithms significantly improve the best - known algorithms for these problems .",
    "specifically , we improve the results presented in many prior papers including @xcite , @xcite , and @xcite .",
    "this research has been supported in part by the nih grant nih - r01-lm-010101 .",
    "p. achlioptas , b. scholkopf and k. borgwardt , two - locus association mapping in subquadratic runtime , acm sigkdd international conference on knowledge discovery and data mining ( kdd ) 2011 .",
    "aston , d.a .",
    "ralph , d.p .",
    "lalo , s. manjeshwar , b.a .",
    "gramling , d.c .",
    "defreese , a.d .",
    "west , d.e .",
    "branam , l.f .",
    "thompson , m.a .",
    "craft , d.s .",
    "mitchell , c.d .",
    "shimasaki , j.j .",
    "mulvihill , and e.r .",
    "jupe , oligogenic combinations associated with breast cancer risk in women under 53 years of age , _ human genetics _ , 116(3):208 - 221 , feb .",
    "p. beaudoin , m. van de panne , p. poulin and s. coros , motion - motif graphs , symposium on computer animation 2008 .",
    "m. s. charikar , similarity estimation techniques from rounding algorithms , proc .",
    "acm symposium on theory of computing ( stoc ) , 2002 .",
    "b. chiu , e. keogh , and s. lonardi , probabilistic discovery of time series motifs , proc . of the 9th international conference on knowledge discovery and data mining ( kdd ) , pp .",
    "493 - 498 , 2003 .",
    "cho , d.l .",
    "nicolae , l.h .",
    "gold , c.t .",
    "fields , m.c .",
    "labuda , p.m. rohal , m.r .",
    "pickles , l. qin , y. fu , j s. mann , b.s .",
    "kirschner , e.w .",
    "jabs , j. weber , s.b .",
    "hanauer , t.m .",
    "bayless , and s.r .",
    "identification of novel susceptibility loci for inflammatory bowel disease on chromosomes 1p , 3q , and 4q : evidence for epistasis between 1p and ibd1 proceedings of the national academy of sciences of the united states of america , 95(13):7502 - 7507 , june 1998 . t. h. consortium",
    ". a second generation human haplotype map of over 3.1 million snps .",
    "nature , 449(7164):851 - 61 , oct . 2007 .",
    "h. j. cordell . detecting gene - gene interactions that underlie human diseases .",
    "nat rev genet , 10(6):392 - 404 , june 2009 .",
    "cox , m. frigge , d.l .",
    "nicolae , p. concannon , c.l .",
    "hanis , g.i .",
    "bell , and a. kong , loci on chromosomes 2 ( niddm1 ) and 15 interact to increase susceptibility to diabetes in mexican americans , nature genetics , 21(2):213 - 215 , feb . 1999 .",
    "s. fortune and j. s. hopcroft , _ a note on rabin s nearest - neighbor algorithm _ , information processing letters , 8(1 ) , pp .",
    "20 - 23 , 1979 .",
    "t. guyet , c. garbay and m. dojat , knowledge construction from time series data using a collaborative exploration system , journal of biomedical informatics 40(6 ) : 672 - 687 ( 2007 ) .",
    "s. khuller and y. matias , _ a simple randomized sieve algorithm for the closest - pair problem _ , information and computation , vol 188 ( 1 ) , pp . 3437 , 1995 . j. meng , j.yuan , m. hans and y. wu , mining motifs from human motion , proc . of eurographics , 2008 .",
    "d. minnen , c.l .",
    "isbell , i. essa , and t. starner , discovering multivariate motifs using subsequence density estimation and greedy mixture learning , proc .",
    "22nd conference on artificial intelligence ( aaai?07 ) , 2007 .",
    "a. mueen , e. keogh , q. zhu , s. cash , b. westover , _",
    "`` exact discovery of time series motifs '' _ , siam international conference on data mining ( sdm ) 2009 .",
    "s. k. musani , d. shriner , n. liu , r. feng , c. s. coffey , n. yi , h. k. tiwari , and d. b. allison .",
    "detection of gene x gene interactions in genome - wide association studies of human population data .",
    "human heredity , 63(2):67 - 84 , 2007 .",
    "r. nakamichi , y. ukai , and h. kishino , detection of closely linked multiple quantitative trait loci using a genetic algorithm , genetics , 158(1):463 - 475 , may 2001 .",
    "r. paturi , s. rajasekaran , and j. reif .",
    "the light bulb problem . in proc .",
    "workshop on comput .",
    "learning theory , pp .",
    "261 - 268 , san mateo , ca , 1989 .",
    "morgan kaufmann .",
    "f. preparata and m. shamos , _ computational geometry _ , springer verlag , 1986 .",
    "m. rabin , _",
    "probabilistic algorithms _ , algorithms and complexity , recent results and new directions , academic press , pp 21 - 39 , 1976 .",
    "s. rombo and g. terracina , discovering representative models in large time series databases , proc .",
    "6th international conference on flexible query answering systems , pp .",
    "84 - 97 , 2004 .",
    "y. tanaka , k. iwamoto , and k. uehara , discovery of time - series motif from multi - dimensional data based on mdl principle , machine learning , 58(2 - 3):269 - 300 , 2005 .",
    "s. tata , declarative querying for biological sequences , ph.d thesis , the university of michigan , 2007 .",
    "( advisor jignesh m. patel ) .",
    "y. wang , x. liu , k. robbins , and r. rekaya , antepiseeker : detecting epistatic interactions for case - control studies using a two - stage ant colony optimization algorithm , bmc bioinformatics , 3:117 - 117 , 2010 . j. xu , c.d .",
    "langefeld , s.l .",
    "zheng , e.m .",
    "gillanders , b. chang , s.d .",
    "isaacs , a.h .",
    "williams , k e. wiley , l. dimitrov , d.a .",
    "meyers , p.c .",
    "walsh , j.m .",
    "trent , and w.b .",
    "isaacs , interaction effect of pten and cdkn1b chromosomal regions on prostate cancer linkage , human genetics , 115(3):255 - 262 , aug .",
    "lower bounds for algebric computation trees with integer inputs _ , siam j. comput . , 20:4 , pp .",
    "655 - 668 , 1991 .",
    "x. zhang , s. huang , f. zou , and w. wang . team : efficient two - locus epistasis tests in human genome - wide association study .",
    "bioinformatics ( oxford , england ) , 26(12):i217 - 227 , june 2010 .",
    "x. zhang , f. zou , and w. wang , fastanova : an efficient algorithm for genome - wide association study , in proc .",
    "14th acm sigkdd international conference on knowledge discovery and data mining ( kdd ) , pages 821 - 829 , las vegas , nevada , usa , 2008 ."
  ],
  "abstract_text": [
    "<S> the closest pair problem ( cpp ) is one of the well studied and fundamental problems in computing . given a set of points in a metric space , the problem is to identify the pair of closest points . </S>",
    "<S> another closely related problem is the fixed radius nearest neighbors problem ( frnnp ) . </S>",
    "<S> given a set of points and a radius @xmath0 , the problem is , for every input point @xmath1 , to identify all the other input points that are within a distance of @xmath0 from @xmath1 . </S>",
    "<S> a naive deterministic algorithm can solve these problems in quadratic time . </S>",
    "<S> cpp as well as frnnp play a vital role in computational biology , computational finance , share market analysis , weather prediction , entomology , electro cardiograph , n - body simulations , molecular simulations , etc . as a result , any improvements made in solving cpp and frnnp will have immediate implications for the solution of numerous problems in these domains . </S>",
    "<S> we live in an era of big data and processing these data take large amounts of time . </S>",
    "<S> speeding up data processing algorithms is thus much more essential now than ever before . in this paper </S>",
    "<S> we present algorithms for cpp and frnnp that improve ( in theory and/or practice ) the best - known algorithms reported in the literature for cpp and frnnp . </S>",
    "<S> these algorithms also improve the best - known algorithms for related applications including time series motif mining and the two locus problem in genome wide association studies ( gwas ) . </S>"
  ]
}