{
  "article_text": [
    "currently we do not understand how anesthesia leads to loss of consciousness ( loc ) .",
    "one popular idea is that we loose consciousness when brain areas loose their ability to communicate with each other  as anesthetics might interrupt transmission on nerve fibers coupling them .",
    "this idea has been tested by measuring the amount of information transferred between brain areas , and taking this to reflect the coupling itself . yet ,",
    "information that is nt available in the source area ca nt be transferred to a target .",
    "hence , the decreases in information transfer could be related to less information being available in the source , rather than to a decoupling .",
    "we tested this possibility measuring the information available in source brain areas and found that it decreased under anesthesia .",
    "in addition , a stronger decrease in source information lead to a stronger decrease of the information transfered .",
    "thus , the input to the connection between brain areas determined the communicated information , not the strength of the coupling ( which would result in a stronger decrease in the target ) .",
    "we suggest that interrupted information processing within brain areas has an important contribution to loc , and should be focused on more in attempts to understand loss of consciousness under anesthesia .",
    "to this day it is an open question in anesthesia research how general anesthesia leads to loss of consciousness ( loc ) .",
    "several recent theories agree in proposing that anesthesia - induced loc may be caused by the disruption of long range inter - areal information transfer in cortex @xcite  a hypothesis supported by a series of recent studies @xcite . in all of these studies ,",
    "information transfer is quantified using transfer entropy @xcite , an information theoretic measure , which has become a quasi - standard for the estimation of information transfer in anesthesia research , or by transfer entropy s linear implementation as a granger causality . in many of these studies",
    "reduced information transfer has been interpreted as a sign of inter - areal long range connectivity being disrupted by anesthesia .    yet , information transfer between a source of information and a target depends on information ( entropy ) being available at the source in the first place .",
    "considering constraints of this kind we can easily conceive of cases where a decrease in information transfer under anesthesia is observed despite unchanged long range coupling , e.g. when the available information at the source decreases due to an anesthesia - related change in _ local _ information processing .",
    "ultimately , this dissociation between information transfer and causal coupling just reflects that information transfer is _",
    "one _ possible consequence of physical coupling , but not identical to it @xcite .",
    "therefore , we consider it necessary to evaluate the hypothesis that the reduced inter - areal information transfer observed under anesthesia possibly originates from disrupted information processing in local circuits rather than from disrupted long range connectivity .",
    "this alternative hypothesis receives additional support for the case of isoflurane , which potentiates agonist actions at @xmath0-receptors and inhibits nicotinic acetylcholine ( nachr ) receptors .",
    "conversely , evidence on direct inhibitory effects of isoflurane on ampa and nmda synapses , which are the dominant mediators of long - range cortico - cortical interactions , is sparse at best ( see table 2 in @xcite ) . under the alternative hypothesis of changed local information processing , a decrease in transfer entropy under anesthesia",
    "must be accompanied by :    1 .   a reduction in locally available information per brain area , i.e. in the sources of information transfer , 2 .   and the fact that the strongest decrease in locally available information is found at the source of the link with the strongest decrease in information transfer , rather than at its target ( i.e. the end point ) ,    here , we perform tests of these predictions by estimating local information processing in and information transfer between local field potentials ( lfps ) , simultaneously recorded from primary visual cortex ( v1 ) and prefrontal cortex ( pfc ) of two anesthetized ferrets .",
    "we quantify local information processing by estimating the signal entropy ( measuring available information ) and by estimating active information storage ( measuring stored information ) ; and we quantified information transfer between recording sites by estimating transfer entropy .    to better understand potential changes in local information processing we also quantified the active information storage , a measure of the information available at a recording site that can be predicted from past signals at that site , i.e. the information stored from past to present .    because the estimation of such information theoretic quantities from finite data is difficult in general ,",
    "we employ two complementary strategies : ( i ) probability density estimation based on nearest - neighbor searches in continuous data , and ( ii ) bayesian estimation based on discretized data .",
    "we also test whether the previously reported decrease of transfer entropy under anesthesia can indeed be replicated when avoiding some recently identified pitfalls in estimation of information transfer related to the use of symbolic time series , suboptimal embedding of the time series , and the use of net transfer entropy without identification of the individual information transfer delays ( for problems related to these approaches see @xcite ) .",
    "our results provide first evidence for the alternative hypothesis of altered local information processing causing reduced information transfer , as the above predictions were indeed met .",
    "we suggest to consider the alternative hypothesis as a serious candidate mechanism for loc , and to use causal interventions to gather further experimental evidence .",
    "preliminary results for this study were published in abstract form in @xcite .",
    "overall , under anesthesia both locally available information and information transfer were decreased , while information storage in local activity increased .    as the estimation of information theoretic measures from finite length neural recordings poses a considerable challenge we present detailed , converging results from two complementary strategies to deal with this challenge  nearest - neighbor based estimators , and a bayesian approach to entropy estimation suggested by nemenman , shafe , and bialek ( nsb - estimator ) @xcite .",
    "this latter approach required a discretization of the continuous - valued lfp data , but yields principled control of bias , while the first approach allows the estimation of information - theoretic measures directly from continuous data , and thus conserves the information originally present in those data .",
    "statistical testing was performed using a nonparametric permutation anova ( panova ) , and a linear mixed model ( lmm ) .",
    "the lmm approach was used in addition to the main panova for the purpose of comparison to older studies using parametric statistics .    [",
    "[ results - based - on - next - neighbor - based - estimation - from - continuous - data . ] ] results based on next neighbor - based estimation from continuous data .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    under anesthesia , we found an overall reduction in the locally available information ( @xmath1 ) , and in the information transfer ( @xmath2 ) .",
    "we found an increase in the locally predictable ( stored ) information ( @xmath3 ) ( table [ tab : panova ] and fig .",
    "[ fig : panova_main_effects_all ] ) .    ) , active information storage ( @xmath3 ) , and entropy ( @xmath1 ) . *",
    "left columns show interactions ` anesthesia x direction ` and ` anesthesia x recording site ` for both animals ; right columns show main effects ` anesthesia ` .",
    "grey lines in interaction plots indicate @xmath2 from prefrontal cortex ( pfc ) to primary visual areas ( v1 ) , or @xmath1 and @xmath3 in pfc ; black lines indicate @xmath2 from v1 to pfc , or @xmath1 and @xmath3 in v1 .",
    "error bars indicate the standard error of the mean ( sem ) ; stars indicate significant interactions or main effects ( @xmath4 ; @xmath5 ; @xmath6 ) .",
    "axis units for all information theoretic measures based on continuous variables are z - normalized values across conditions . ]",
    ".results of permutation analysis of variance for information theoretic measures ( @xmath7-values ) . [ cols=\"<,<,^,^\",options=\"header \" , ]     note that we also applied an alternative bayesian estimation scheme based on pitman - yor - process priors @xcite .",
    "however , for this estimation procedure , we observed that the data were insufficient to allow for a robust estimation of the tailing behavior of the distribution as indicated by large variances and unreasonably high estimates across different sample sizes .",
    "as noted in the introduction , the estimation of information theoretic measures from finite data is challenging .",
    "for the measures that describe distributed computation in complex systems , such as transfer entropy , estimation is further complicated because the available data are typically only scalar observations of a process with multidimensional dynamics .",
    "this necessitates the approximate reconstructions of the process states via a form of embedding @xcite , where a certain number of past values of the scalar observations spaced by an embedding delay are taken into account ( e.g. for a pendulum swinging through its zero position one additional past position values will clarify whether it s going left or right ) . an important part of proper transfer entropy estimation is , thus , optimization of this number of past values ( embedding dimension ) , and of the embedding delay .",
    "these two embedding parameters then approximately define past states , whose correct identification is crucial for the estimation of transfer entropy @xcite , but also for the estimation of active information storage .",
    "without it information storage may be underestimated and , erroneous values of the information transfer will be obtained ; even a detection of information transfer in the wrong direction is likely ( see @xcite and methods section ) .",
    "existing studies using transfer entropy often omitted the optimization of embedding parameters , and instead used ad - hoc choices , which may have had a detrimental effect on the estimation of transfer entropy  hence the need for a confirmation of previous results in this study .    in the present study",
    ", we therefore used a formal criterion proposed by ragwitz @xcite , to find an optimal embedding defined by the embedding dimension @xmath8 ( the number of values collected ) and delay @xmath9 ( the temporal spacing between them ) , to find embeddings for the @xmath2 and @xmath3 past states .",
    "we used the implementation of this criterion provided by the trentool toolbox .",
    "since the bias of the estimators used depends on @xmath8 , we used the maximum @xmath8 over all conditions and directions of interaction as the common embedding dimension for estimation from each trial to make the estimated values statistically comparable .",
    "the resulting dimension used was 15 samples , which is considerably higher than the value commonly used in the literature @xcite , when choosing the embedding dimension ad - hoc or using other criteria as for example the sample size @xcite .",
    "the embedding delay @xmath9 was optimized individually for single trials in each condition and animal blue as it has no influence on the estimator bias .",
    "several previous studies on information transfer under anesthesia reported the sign of the so called net transfer entropy as a measure of the predominant direction of information transfer between two sources .",
    "@xmath10 is essentially just the normalized difference between the transfer entropies measured in the two direction connecting a pair of recording sites ( see methods ) .",
    "when calculating @xmath10 , it is particularly important to individually account for physical delays @xmath11 in the two potential directions of information transfer , because otherwise the sign of @xmath10 may become meaningless ( see examples in @xcite ) . as this delay is unknown a priori it has to be found before to the actual estimation of information transfer .",
    "we have recently shown that this can be done by using a variable delay parameter @xmath12 in a delay sensitive transfer entropy estimator @xmath2 ( see methods ) ; here , the @xmath12 that maximizes the transfer entropy over a range of assumed values for @xmath12 reflects the physical delay @xcite .",
    "the necessity to individually optimize @xmath12 for each interaction is not a mere theoretical concern but was clearly visible in the present study : fig .",
    "[ fig : delay_reconstruction]c shows representative results from ferret 1 in the _ awake _ condition , where apparent @xmath2 values strongly varied as a function of the delay @xmath12 . as a consequence , @xmath10 values also varied if a _ common _",
    "delay @xmath12 was chosen for both directions .",
    "in other words , the sign of the @xmath10 varied as a function of individual choices for @xmath13 and @xmath14 for each direction of information transfer and hence became uninterpretable ( fig .",
    "[ fig : delay_reconstruction]d ) .    ) depends on the choice of the delay parameter @xmath12 , if @xmath12 is much smaller or bigger than the true delay @xmath11 , information arrives too late or too early in the target time series and information transfer is not correctly measured ; ( b , modified from @xcite ) ) @xmath2 values estimated from two simulated , bidirectionally coupled lorenz systems ( see @xcite for details ) as a function of @xmath12 for both directions of analysis , @xmath15 ( black line ) and @xmath16 ( gray line ) : @xmath2 values vary with the choice of @xmath12 and so does the absolute difference between values ; using individually optimized transfer delays for both directions of analysis yields the meaningful difference @xmath17 ( dashed lines ) , where @xmath18 ; ( c ) example transfer entropy analysis for one recorded trial : @xmath2 values vary greatly as a function of @xmath12 , optimal choices of @xmath12 are marked by dashed lines ; ( d ) sign ( gray : negative , white : positive ) of @xmath10 for different values of @xmath13 ( x - axis ) and @xmath14 ( y - axis ) , calculated from @xmath2 values shown in panel c : the sign varies with individual choices of @xmath12 ; the black frame marks the combination of individually optimal choices for both parameters that yields the correct result . ]    to obtain estimates of transfer entropy that were not biased by a non - optimal choice for the information transfer delay @xmath12 , we individually optimized @xmath12 for each direction of information transfer in each condition and animal to estimate the true delay of information transfer following the mathematical proof in @xcite , and using the implementation in trentool @xcite .",
    "we scanned values for @xmath12 ranging from 0 to 20  ms .",
    "averages for optimized delays ranged from 4 - 7  ms across animals and anesthesia levels ( fig .",
    "[ fig : opt_u ] ) .",
    "our results clearly indicate the necessity to individually optimize information transfer delays and that often employed ad - hoc choices for @xmath12 may result in spurious results in information transfer .",
    "for both directions of interaction and three levels of anesthesia , by animal .",
    "* bars denote averages over recordings per condition ; error bars indicate the standard error of the mean ( sem ) . there was a significant main effect of ` anesthesia ` for ferret 1 ( @xmath19 ) . ]",
    "we analyzed long - range information transfer between areas v1 and pfc in two ferrets under different levels of anesthesia .",
    "we found that transfer entropy was indeed reduced under anesthesia and that this reduction was more pronounced in top - down directions .",
    "these results validate earlier findings made using different estimation procedures @xcite . as far as information transfer alone",
    "was concerned our results are compatible with an interpretation of reduced long - range information transfer due to reduced inter - areal coupling . yet , this interpretation provides no direct explanation for the findings of reduced locally available information as explained below .",
    "in contrast , the alternative hypothesis that the reduced long range information transfer is a secondary effect of changes in local information processing provides a concise explanation for our findings both with respect to locally available information , and information transfer .      to test our alternative hypothesis",
    "we evaluated two of its predictions about changes in locally available information , as measured by signal entropy , under anesthesia .",
    "first , entropy should be reduced ; second , the strongest decrease in information transfer should originate from the source node with the strongest decrease in entropy , rather than end in this node .",
    "indeed , we found that signal entropy decreased ; the most pronounced decrease in signal entropy was found in pfc . in accordance with our prediction , we found that pfc ",
    "the node with the larger decrease in entropy  was also at the source , not the target , of the most pronounced decrease in transfer entropy .",
    "this is strong evidence against the theoretical possibility that in a recording site , entropy decreased due to a reduced influx of information  because in this latter scenario the strongest reduction in entropy should have been found at the target ( end point ) of the most pronounced decrease in information transfer .",
    "hence , our hypothesis that long - range cortico - cortical information transfer is reduced because of changes in local processing must be taken as a serious alternative to the currently prevailing theories of anesthetic action based on disruptions of long range interactions .",
    "we suggest that a renewed focus on local information processing in anesthesia research will be pivotal to advance our understanding of how consciousness is lost .",
    "our predictions that reductions in entropy should potentially be reflected in reduced information transfer derives from the simple principle that information that is not available at the source can not be transferred .",
    "we may thus in principle reduce @xmath2 to arbitrarily low values by reducing the entropy of one of the involved processes , without changing the physical coupling between the two systems , just by changing their internal information processing .",
    "this trivial but important fact has been neglected in previous studies when interpreting changes in information transfer as changes in coupling strength .",
    "even when this bound is not attained , e.g. because only a certain fraction of the local information is transferred even in the awake state , it seems highly plausible that reductions in the locally available information affect the amount of information transfered .",
    "a possible indication of how exactly local information processing is changed is given by the observation of increased active information storage in pfc and v1 ( also see the next paragraph ) .",
    "this means that more old information is kept stored in a source s activity under anesthesia , rather than being dynamically generated .",
    "such stored source information will not contribute to a measurable transfer entropy under most circumstances because it is already known at the target .",
    "in general , @xmath3 increases if a signal becomes more predictable when knowing it s past , but is unpredictable otherwise .",
    "this also means that the absolute @xmath3 is upper - bounded by the system s entropy @xmath1 ( see methods , eq .",
    "[ eq : ais_entropy ] ) .",
    "thus , a decrease in @xmath1 can in principle lead to a decrease in @xmath3 , i.e. , fewer possible system states may lead to a decrease in absolute @xmath3 .",
    "however , in the present study , we observed an _ increase _ in @xmath3 while @xmath1 decreased  this indicates an increase in predictability that compensates for the decrease in locally available information .",
    "in other words , the system visited fewer states in total but the next state visited became more predictable from the system s past .",
    "thus , a reduction in entropy and increase in predictability points at highly regular neural activity for higher anesthesia concentrations .",
    "such a behavior in activity is in line with existing electrophysiological findings : under anesthesia signals have been reported to become more uniform , exhibiting repetitive patterns , interrupted by bursting activity ( see @xcite for a review ) .",
    "for example , purdon and colleagues observed a reduction of the median frequency and an overall shift towards high - power , low - frequency activity during loc @xcite .",
    "in particular , slow - wave oscillatory power was more pronounced during anesthesia - induced loc . in their study , loc was also accompanied by a significant increase in power and a narrowing of oscillatory bands in the alpha frequency range .",
    "unfortunately a quantitative comparison between different information theoretic estimates that would directly relate @xmath1 , @xmath3 and @xmath2 is not possible using the continuous estimators applied in this study .",
    "their estimates are not comparable because these estimators come with a substantial bias , which depends on the number of points used for estimation as well as on the dimensionality of involved variables ( in this application , the dimensionality is determined mainly by the dimension of the past state vectors , and by how many different state variables enter the computation of a measure ) .",
    "this lack of comparability makes it impossible to normalize estimates ; for example , transfer entropy is often normalized by the conditional entropy of the present target state to compare the fraction of transferred information to the fraction of stored information .",
    "we forgo this possibility of comparison for a greater sensitivity and specificity in the detection of changes in the individual information theoretic measures here .",
    "new estimators , e.g. bayesian estimators like the ones tested here , promise more comparable estimates by tightly controlling the biases . yet",
    ", these estimators were not as reliable as expected in our study , displaying a relatively high variance .",
    "we here tested the possibility that changes local information processing lead to the frequently observed reduction in information transfer between cortical areas under anesthesia , instead of altered long - range coupling .",
    "results on entropies and active information storage suggest that this is a definite possibility from a mathematical point of view .",
    "this is supported by the neurophysiology related to the mode of action of isoflurane , because a dominant influence of altered long - range coupling on @xmath2 would mandate that synaptic terminals of the axons mediating long range connectivity should be targets of isoflurane .",
    "such long range connectivity is thought to be dominated by glutamatergic ampa receptors for inter - areal bottom - up connections , and glutamatergic nmda receptors for inter - areal top - down connections , building upon findings in @xcite and @xcite ( but see @xcite for some evidence of gabaergic long range connectivity ) . yet ,",
    "evidence for isoflurane effects on ampa and nmda receptors is sparse to date ( table 2 in @xcite ) .",
    "in contrast , the receptors most strongly influenced by isoflurane seem to be @xmath0 and nicotinic acetylcholine ( nachr ) receptors .",
    "more specifically , isoflurane potentiates agonist interactions at the former , while inhibiting the latter .",
    "thus , if one adopts the current state of knowledge on the synapses involved in long - range inter - areal connectivity , evidence speaks against a dominant effect of modulation of effective long - range connections by isoflurane .",
    "this , in turn , points at local information processing as a more likely reason for changed transfer entropy under isoflurane anesthesia .",
    "this interpretation is perfectly in line with our finding that decreases in source entropy seem to determine the transfer entropy decreases , instead of decreases in transfer entropy determining the target entropies .    nevertheless , targeted local interventions by electrical or optogenetic activation of projection neurons , combined with the set of information theoretic analyses used here , will most likely be necessary to reach final conclusions on the causal role of local entropy changes in reductions of transfered information .      investigating long - range information transfer under anesthesia",
    "is motivated by the question how changed information transfer may cause loc . to that effect , our findings ",
    "a dominant decrease in top - down information transfer under anesthesia , and a decrease in locally available information possibly driving it may be interpreted in the framework of predictive coding theory @xcite .",
    "predictive coding proposes that the brain learns about the world by constructing and maintaining an internal model of the world , such that it is able to _ predict _ future sensory input at lower levels of the cortical hierarchy . whether predictions match actual future input",
    "is then used to further refine the internal model .",
    "it is thus assumed that top - down information transfer serves the propagation of predictions @xcite .",
    "theories of conscious perception within this predictive coding framework propose that conscious perception is `` determined '' by the internal prediction ( or hypothesis ) that matches the actual input best @xcite",
    ". it may be conversely assumed that the absence of predictions leads to an absence of conscious perception .    in the framework of predictive coding theory",
    "two possible mechanisms for loc can be inferred from our data : ( 1 ) the disruption of information transfer , predominantly in top - down direction , may indicate a failure to propagate predictions to hierarchically lower areas ; ( 2 ) the decrease in locally available information and in entropy rates in pfc may indicate a failure to integrate information in an area central to the generation of a coherent model of the world and the generation of the corresponding predictions .",
    "these hypotheses are in line with findings reviewed in @xcite and @xcite , which discuss activity in frontal areas and top - down modulatory activity as important to conscious perception .",
    "future research should investigate top - down information transfer more closely ; for example , recent work suggests that neural activity in separate frequency bands may be responsible for the propagation of predictions and prediction errors respectively @xcite .",
    "future experiments may target information transfer within a specific band to test if the disruption of top - down information transfer happens in the frequency band responsible for the propagation of predictions .",
    "last , it should be kept in mind that different anesthetics may lead to loss of consciousness by vastly different mechanisms .",
    "ketamine , for example , seems to increase , rather than decrease , overall information transfer  at least in sub - anesthetic doses @xcite .",
    "the main analysis of this study was based on neighbor - distance based information theoretic estimators and a permutation anova . in addition , used alternative approaches , first , a bayesian estimator for information theoretic measures , and second , statistical testing of non - aggregated data using lmms . both approaches returned results qualitatively very similar to those of the main analysis .",
    "specifically , replacing neighbor - distance based estimators with bayesian variants , we replicated the main finding of our study ",
    "a reduction in information transfer and locally available information , and an increase in information storage under anesthesia .",
    "however , using the bayesian estimators we did not find a predominant reduction of top - down compared to bottom up information transfer .",
    "in contrast , using lmms for statistical testing instead of a panova additionally revealed a more pronounced reduction in top - down information transfer also in ferret 2 ( in this animal , this effect was not significant when performing permutation tests on aggregated data ) .",
    "the bayesian estimators performed slightly worse than the next neighbor - based estimators in terms of their higher variance in estimates across recordings .",
    "thus , even though bayesian estimators are currently the best available estimators for discrete data , they may be a non - optimal choice for continuous data . a potential reason for this",
    "is the destruction of information on neighborhood relationships through data binning .",
    "this is , however , necessary to make the current bayesian estimators applicable to continuous data .    in sum",
    ", we obtained similar results through three different approaches .",
    "this makes us confident that locally available information and information transfer indeed decrease under anesthesia , while the amount of predictable information increases .",
    "when interpreting the results obtained in this study it should be kept in mind that lfp signals are not in themselves the immediate carriers of information relevant to individual neurons .",
    "this is because from a neuron s perspective information arrives predominantly in the form postsynaptic potentials generated by incoming spikes and chemical transmission in the synaptic cleft ( but see @xcite for a potential influence of lfps on neural dynamics and computation ) .",
    "thus , lfp signals merely reflect a coarse grained view of the underlying neural information processing . as a consequence ,",
    "our results only hold in as far as at least some relevant information about the underlying information processing survives this coarse graining in the recording process , and little formal mathematical work has been carried out to estimate bounds on the amount of information available after coarse graining ( but see @xcite ) .    yet , the enormous success that brain reading approaches had when based on local field potentials or on even more coarse grained magnetoencephalography ( meg ) recordings indicates that relevant information on neural information processing is indeed available at the level of these signals .",
    "however , successful attempts at decoding neural representations of stimuli or other features of the experimental setting should not lead us to misinterpret the information captured by information theoretic measures of neural processing as necessarily _ being about something we can understand and link to the outside world_. quite to the contrary , the larger part of information captured by these measures may be related to intrinsic properties of the unfolding neural computation .      using two different methods for transfer entropy estimation , and two different statistical approaches , we found that locally available information and information transfer are reduced under anesthesia .",
    "the larger decrease in the locally available information was found at the source of the larger decrease of information transfer , not at its end point , or target .",
    "therefore , previously reported reductions in information transfer under anesthesia may be caused by changes in local information processing rather than a disruption of long range connectivity .",
    "we suggest to put this hypothesis more into the focus of future research effort to understand the loss of consciousness under anesthesia .",
    "this suggestion receives further support from the fact that the synaptic targets of the anesthetic isoflurane , as used in this study , are most likely located in local circuits .",
    "we conducted simultaneous electrophysiological recordings of the local field potential ( lfp ) in primary visual cortex ( v1 ) and prefrontal cortex ( pfc ) of two female ferrets ( 17 to 20 weeks of age at study onset ) when awake and under different levels of anesthesia ( fig .",
    "[ fig : recording_sites ] ) .",
    "the choice of the animal model is discussed further in @xcite .",
    "recordings were made in a dark environment during multiple , individual sessions of max . 2  h length , during which the animals heads were fixed . for recordings , we used single metal electrodes acutely inserted in putative layer iv , measured 0.30.6  mm from the surface of cortex ( tungsten micro - electrode , 250-@xmath20 m shank diameter , 500-k@xmath21 impedance , fhc , bowdoin , me ) .",
    "a silver chloride wire placed between the skull and soft tissue was used as the reference . to verify that electrode placement was indeed in v1 , we mapped receptive fields by eliciting visually evoked potentials in a separate series of experiments .",
    "we confirmed electrode placement in pfc by lesioning through the recording electrode after completion of data collection and post - mortem histology ( as described in @xcite ) .",
    "details on surgical procedures can be found in @xcite .",
    "unfiltered signals were amplified with gain 1,000 ( model 1800 , a - m systems , carlsborg , wa ) , digitized at 20  khz ( power 1401 , cambridge electronic design , cambridge , uk ) , and digitally stored using spike2 software ( cambridge electronic design ) .",
    "for analysis , data were low pass filtered ( 300  hz cutoff ) and down - sampled to 1000  hz .    center     all procedures were approved by the university of north carolina - chapel hill institutional animal care and use committee ( unc - ch iacuc ) and exceed guidelines set forth by the national institutes of health and u.s .",
    "department of agriculture .",
    "lfps were recorded during wakefulness ( condition _ awake _ , number of recording sessions : 8 and 5 for ferret 1 and 2 , respectively ) and with different concentrations of anesthetic : 0.5  % isoflurane with xylazine ( condition _ iso 0.5  % _ , number of sessions : 5 and 6 ) , as well as 1.0  % isoflurane with xylazine ( condition _ iso 1.0  % _ , number of sessions : 10 and 11 ) . in the course of pilot experiments ,",
    "both concentrations _ iso 0.5  % _ and _ iso 1.0  % _ lead to a loss of the righting reflex ; however , a systematic assessment of this metric during recordings was technically not feasible . additionally , animals were administered 4.25 ml / h 5  % dextrose lactated ringer and 0.015 ml / h xylazine via iv .",
    "lfp recordings from each session were cut into trials of 4.81  s length .",
    "this resulted in 196 to 513 trials per recording ( mean : 428.6 ) for ferret 1 , and 211 to 526 trials ( mean : 472.8 ) for ferret 2 .",
    "trials with movement artifacts were manually rejected ( determined by extreme values in the lfp raw traces ) . in the _ awake _ condition ,",
    "infrared videography was used to verify that animals were awake during the whole recording ; additionally , _ awake _ trials with a relative delta power ( 0.5 to 4.0  hz ) of more than 30  % of the total power from 0.5 to 50  hz were rejected to ensure that only trials during which the animal was truly awake entered further analysis .      to measure information transfer between recording sites v1 and pfc , we estimated the transfer entropy @xcite in both directions of possible interactions , @xmath22 and @xmath23 . to investigate local information processing within each recording site",
    ", we estimated active information storage ( @xmath3 ) @xcite as a measure of predictable information , and we estimated differential entropy ( @xmath1 ) @xcite as a measure of information available locally .",
    "we will now explain the applied measures and estimators in more detail , before we describe how these estimators were applied to data from electrophysiological recordings in the next section . to mathematically formalize the estimation procedure from these data",
    ", we assume that neural time series recorded from two systems @xmath24 and @xmath25 ( e.g. cortical sites ) can be treated as collections of realizations @xmath26 and @xmath27 of random variables @xmath28 , @xmath29 of two random processes @xmath30 and @xmath31 .",
    "the index @xmath32 here indicates samples in time , measured in units of the dwell time ( inverse sampling rate ) of the recording .",
    "[ [ transfer - entropy ] ] transfer entropy + + + + + + + + + + + + + + + +    transfer entropy @xcite is defined as the mutual information between the future of a process @xmath33 and the past of a second process @xmath34 , conditional on the past of @xmath33 .",
    "transfer entropy thus quantifies the information we obtain about the future of @xmath33 from the past of @xmath34 , taking into account information from the past of @xmath33 .",
    "taking this past of @xmath33 into account here removes information redundantly available in the past of both @xmath34 and @xmath33 , and reveals information provided synergistically by them @xcite . in this study",
    ", we used an improved estimator of transfer entropy presented in @xcite , which accounts for arbitrary information transfer delays :    @xmath35    where @xmath36 is the conditional mutual information ( or the differential conditional mutual information for continuous valued variables ) between @xmath29 and @xmath37 , conditional on @xmath38 ; @xmath29 is the future value of random process @xmath33 , and @xmath37 , @xmath38 are the past states of @xmath34 and @xmath33 , respectively .",
    "past states are collections of past random variables    @xmath39    that form a delay embedding of length @xmath40 @xcite , and that render the future of the random process conditionally independent of all variables of the random process that are further back in time than the variables forming the state .",
    "parameters @xmath9 and @xmath8 denote the embedding delay and embedding dimension and can be found through optimization of a local predictor as proposed in @xcite ( see also next section ) .",
    "past states constructed in this manner are then maximally informative about the present variable of the target process , @xmath29 , which is an important prerequisite for the correct estimation of transfer entropy ( see also @xcite ) .",
    "typically , embedding parameters @xmath8 and @xmath9 , are not known when estimating @xmath2 from experimental data .",
    "therefore these parameters have to be optimized prior to estimating @xmath2 or @xmath3 .",
    "we will describe possible optimization procedures below .    in our estimator",
    "[ eq : te ] ) , the variable @xmath12 describes the assumed information transfer delay between the processes @xmath34 and @xmath33 , which accounts for a physical delay @xmath41 @xcite .",
    "the estimator thus accommodates arbitrary physical delays between processes .",
    "the true delay @xmath42 must be recovered by scanning various assumed delays and keeping the delay that maximizes @xmath2 @xcite :    @xmath43    [ [ active - information - storage ] ] active information storage + + + + + + + + + + + + + + + + + + + + + + + + + +    @xmath3 @xcite is defined as the ( differential ) mutual information between the future of a signal and its immediate past state    @xmath44    where @xmath33 again is a random process with present value @xmath29 and past state @xmath38 ( see eq .",
    "[ eq : state ] ) .",
    "@xmath3 thus quantifies the amount of predictable information in a process or the information that is currently in use for the next state update @xcite .",
    "@xmath3 is low in processes that produce little information or are highly unpredictable , e.g. , fully stochastic processes , whereas @xmath3 is highest for processes that visit many equi - probable states in a predictable sequence , i.e. , without branching . in other words",
    ", @xmath3 is high for processes with `` rich dynamics '' that are predictable from the processes past @xcite .",
    "a reference implementation of @xmath3 can be found in the java information dynamics toolkit ( jidt ) @xcite . as for @xmath2 estimation , an optimal delay embedding @xmath38 may be found through optimization of the local predictor proposed in @xcite .",
    "note , that @xmath3 is upper bounded by the entropy as :    @xmath45    [ [ differential - entropy ] ] differential entropy + + + + + + + + + + + + + + + + + + + +    the differential entropy @xmath1 ( see for example @xcite ) expands the classical concept of shannon s entropy for discrete variables to continuous variables :    @xmath46    where @xmath47 is the probability density function of @xmath29 over the support @xmath48 .",
    "entropy quantifies the average information contained in a signal .",
    "based on the differential entropy the corresponding measures for mutual and conditional mutual information and , thereby , active information storage and transfer entropy can be defined .",
    "the transfer entropy from eq .",
    "[ eq : te ] can be rewritten as :    @xmath49    by dropping the negative term on the right hand side we obtain an upper bound ( as already noticed by  @xcite ) , and by realizing that a conditional entropy is always smaller than the corresponding unconditional we arrive at    @xmath50    this indicates that the overall entropy of the source states is an upper bound .",
    "several interesting other bounds on information transfer exist as detailed in @xcite , yet these are considerably harder to interpret and were not the focus of the current presentation .      in this section",
    "we will describe how the information theoretic measures presented in the last section may be estimated from neural data . in doing so",
    ", we will also describe the methodological pitfalls mentioned in the introduction in more detail and we will describe how these were handled here . if not stated otherwise , we used implementations of all presented methods in the open source toolboxes trentool @xcite and jidt @xcite , called through custom matlab^^ scripts ( matlab 8.0 , the mathworks^^ inc . , natick , ma , 2012 ) .",
    "[ [ estimating - information - theoretic - measures - from - continuous - data ] ] estimating information theoretic measures from continuous data + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    estimation of information theoretic measures from continuous data is often handled by simply discretizing the data .",
    "this is done either by binning or the use of symbolic time series  mapping the continuous data onto a finite alphabet .",
    "specifically , the use of symbolic time series for transfer entropy estimation was first introduced by @xcite and maps the continuous values in past state vectors with length @xmath8 ( eq . [ eq : state ] ) onto a set of rank vectors .",
    "hence , the continuous - valued time series is mapped onto an alphabet of finite size @xmath51 .",
    "after binning or transformation to rank vectors transfer entropy and the other information theoretic measures can then be estimated using plug - in estimators for discrete data , which simply evaluate the relative frequency of occurrences of symbols in the alphabet . discretizing the data",
    "therefore greatly simplifies the estimation of transfer entropy from neural data , and may even be necessary for very small data sets . yet",
    ", binning ignores the neighborhood relations in the continuous data and the use of symbolic times series destroys important information on the absolute values in the data .",
    "an example where transfer entropy estimation fails due to the use of symbolic time series is reported in @xcite and discussed in @xcite : in this example , information transfer between two coupled logistic maps was not detected by symbolic transfer entropy @xcite ; only when estimating @xmath2 directly using an estimator for continuous data , the information transfer was identified correctly @xcite . to circumvent the problems with binned or symbolic time series",
    ", we here used a nearest - neighbor based @xmath2-estimator for continuous data , the kraskov - stgbauer - grassberger ( ksg ) estimator for mutual information described in @xcite . at present",
    ", this estimator has the most favorable bias properties compared to similar estimators for continuous data .",
    "the ksg - estimator leads to the following expression for the estimation of @xmath2 as introduced in eq .",
    "[ eq : te ] @xcite :    @xmath52    where @xmath53 denotes the digamma function , @xmath54 is the number of neighbors in the highest - dimensional space spanned by variables @xmath55 , @xmath56 , @xmath57 , and is used to determine search radii for the lower dimensional subspaces ; @xmath58 are the number of neighbors within these search radii for each point in the lower dimensional search spaces spanned by the variable indicated in the subscript .",
    "angle brackets indicate the average over realizations @xmath59",
    "( e.g. observations made over an ensemble of copies of the systems or observations made over time in case of stationarity , which we assumed here ) . for a detailed derivation of @xmath2-estimation using the ksg - estimator see @xcite .    since the ksg - estimator estimates mutual information",
    ", it can be used for the estimation of both @xmath2 ( eq . [ eq : te_est ] ) and @xmath3 :    @xmath60    where again @xmath53 denotes the digamma function , @xmath54 is the number of neighbors in the highest - dimensional space , @xmath61 is the number of realizations , and @xmath58 denotes the number of neighbors for each point in the respective search space .    a conceptual predecessor of the ksg - estimator for mutual information is the kozachenko - leonenko ( kl ) estimator for differential entropies @xcite .",
    "the kl - estimator also allows for the estimation of @xmath1 from continuous data and reads    @xmath62    where @xmath63 is twice the distance from data point @xmath64 to its @xmath54-th nearest neighbor in the search space spanned by all points .",
    "[ [ bayesian - estimators - for - discretized - data ] ] bayesian estimators for discretized data + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for bayesian estimation we converted the continuous lfp time series to discrete data by applying voltage bins as follows : the voltages @xmath65 standard deviations around the mean of the lfp were subdivided into @xmath61 equally spaced bins .",
    "we added two additional bins containing all the values that were either smaller or larger than the 3 sd region , amounting to a total number of bins @xmath66 .",
    "we then calculated @xmath1 , @xmath3 and @xmath2 for the discrete data . for @xmath3 and @xmath2 estimation states",
    "were defined using the same dimension @xmath8 and @xmath9 as for the ksg - estimator , optimized using the ragwitz criterion .",
    "we decomposed @xmath2 into four entropies ( eq . [ eq : te_est ] ) , and @xmath3 into three entropies , which we then estimated individually @xcite . to reduce the bias introduced by the limited number of observed states , we used the nsb - estimator by nemenman , shafee , and bialek @xcite , which is based on the construction of an almost uniform prior over the expected entropy using a mixture of symmetric dirichlet priors @xmath67 .",
    "the estimator has been shown to be unbiased for a broad class of distributions that are typical in @xmath67 @xcite .",
    "we further applied the recently proposed estimator by archer et al .",
    "@xcite that uses a prior over distributions with infinite support based on pitman - yor - processes .",
    "in contrast to the nsb prior , this prior also accounts for heavy - tailed distributions that one encounters frequently in neuronal systems and does not require knowledge of the support of the distribution .    when estimating entropies for the embedding dimensions used here , the number of possible states or `` words '' is between @xmath68 and @xmath69 .",
    "this is much larger than the typical number of observed states per recording of around @xmath70 . as a consequence ,",
    "a precise estimation of entropies is only possible if the distribution is sparse , i.e. most words have vanishing probability . in this case , however , the estimates should be independent of the choice of support @xmath71 as long as @xmath71 is sufficiently large and does not omit states of finite probability .",
    "we chose @xmath72 for the results shown in this paper , which allowed a robust computation of the nsb estimator instead of the maximum support @xmath71 that results from simple combinatorics .",
    "[ [ finding - optimal - embedding - parameters ] ] finding optimal embedding parameters + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the second methodological problem raised in the introduction was the choice of embedding parameters for transfer entropy estimation .",
    "one important parameter here is the choice of the total signal history when constructing past states for source and target signal ( see eq .",
    "[ eq : te ] and [ eq : state ] ) . failure to properly account for signal histories may lead to a variety of errors , such as underestimating transfer entropy , failure to detect transfer entropy altogether , or the detection of spurious transfer entropy .",
    "transfer entropy is underestimated or missed if the past state of the _ source _ time series does not cover all the relevant history , i.e. , the source is under - embedded in contrast , spurious transfer entropy may be detected if the past state of the _ target _ time series is under embedded such spurious detection of transfer entropy is a false positive and therefore the most serious error .",
    "one scenario where spurious transfer entropy results from under - embedding is shown in fig .",
    "[ fig : underembedding ] .     and @xmath33 .",
    "( b ) mutual information between the present value in @xmath34 , @xmath26 , and a value in the far past of @xmath34 , @xmath73 , conditional on all intermediate values @xmath74 ( shaded box ) , the mutual information is non - zero , i.e. , @xmath73 holds some information about @xmath26 .",
    "( c ) both directions of interaction are analyzed ; ( d ) information in @xmath73 ( white sample point ) is transferred to @xmath33 ( solid arrow ) , because of the actual coupling @xmath75 .",
    "the information in @xmath73 about @xmath26 is thus transferred to the past of @xmath33 , @xmath76 , which thus becomes predictive of @xmath26 as well .",
    "assume now , we analyzed information transfer from @xmath33 to @xmath34 , @xmath77 , without a proper embedding of @xmath34 , @xmath74 : because of the actually transferred information from @xmath73 to @xmath76 , the mutual information @xmath78 is non - zero .",
    "if we now under - embed @xmath34 , such that the information in @xmath73 is not contained in @xmath79 and is not conditioned out , @xmath80 will be non - zero as well . in this case , under - embedding of the target @xmath34 will lead to the detection of spurious information transfer in the non - coupled direction @xmath81 .",
    "( e ) information transfer is falsely detected for both directions of interaction , the link from @xmath33 to @xmath34 is spurious ( dashed arrow ) . ]    the choice of an optimal embedding is also relevant for the estimation of @xmath3 , where under - embedding leads to underestimation of the true @xmath3 .",
    "note that on the other hand , we can not increase the embedding length to arbitrarily high values because this leads to computationally intractable problem sizes and requires exponentially more data for estimation .",
    "optimal embedding parameters @xmath8 and @xmath9 may be found through the optimization of a local predictor proposed by ragwitz @xcite .",
    "ragwitz criterion tests different combinations of a range of values for @xmath8 and @xmath9 .",
    "the current combination is used to embed each point in a time series , then , the future state for each point is predicted from the future states of its neighbors .",
    "the parameter combination that leads to the best prediction on average is used as the optimal embedding .",
    "further details on ragwitz criterion can be found in the documentations of the trentool @xcite and jidt @xcite toolbox .",
    "other approaches for embedding parameter optimization have been proposed , see for example non - uniform embedding using mutual information to determine all relevant past samples as proposed by @xcite .",
    "[ [ reconstruction - of - information - transfer - delays ] ] reconstruction of information transfer delays + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the third methodological problem raised in the introduction was failure to account for a physical delay @xmath11 between neural sites when estimating transfer entropy . in our estimator @xmath2 ( eq . [ eq : te_est ] )",
    "we account for @xmath11 by introducing the parameter @xmath12 .",
    "the delay @xmath12 needs to be optimized to correctly estimate @xmath2 .",
    "if @xmath12 is not optimal , i.e. , @xmath12 is not sufficiently close to @xmath11 ( fig .",
    "[ fig : delay_reconstruction ] and @xcite ) , information transfer may be underestimated or not measured at all .",
    "this is because choosing the parameter @xmath12 too large ( @xmath82 ) means that the information present in the evaluated samples of the source is also present in the history of the target already , and conditioned away .",
    "in contrast , choosing the parameter @xmath12 too small means that the information of the evaluated samples of the source will only arrive in the future of the current target sample , and is useless for providing information about it ( fig .",
    "[ fig : delay_reconstruction]a ) .",
    "it can be proven for bivariate systems that @xmath2 becomes maximal when the true delay @xmath11 is chosen for @xmath12 @xcite . therefore , the true delay @xmath11and thus an optimal choice for @xmath12can be found by using the value for @xmath12 that maximizes @xmath2 @xcite .",
    "this optimal @xmath12 can be found by scanning a range of assumed values . in the present study",
    ", we scanned values ranging from 0 to 20 ms .",
    "( note that assumed values should be physiologically plausible to keep the computations practically feasible . )",
    "accounting for the information transfer @xmath11 by finding optimal parameters @xmath12 for @xmath2 estimation has important consequences when calculating indices from estimated @xmath2 , such as @xmath10 :    @xmath83    or variations of this measure .",
    "the @xmath10 is popular in anesthesia research @xcite and indicates the predominant direction of information transfer between two bidirectionally coupled processes @xmath34 and @xmath33 ( @xmath84 if @xmath85 and @xmath86 if @xmath87 ) .",
    "however , if values for @xmath88 and @xmath89 are not optimized individually , @xmath10 may take on arbitrary signs : in fig .",
    "[ fig : delay_reconstruction]b , we show a toy example of two coupled lorenz systems @xcite , where the absolute difference between raw @xmath2 values changes as a function of a common @xmath12 for both directions and where the difference even changes signs for values @xmath90 . to obtain a meaningful value from @xmath10",
    "we thus need to find the individually optimal choices of @xmath12 for both directions of transfer  in the example , these optima are found at @xmath91 and @xmath92 , leading to the `` true '' difference .      to test for statistically relevant effects of anesthesia levels and direction of interaction or recording site on estimated measures , we performed two - factorial permutation analyses of variance ( panova ) for each animal and estimated measure @xcite . for @xmath2 estimates factors were ` anesthesia ` and ` direction ` , and for @xmath3 and @xmath1 estimates factors were ` anesthesia ` and ` recording site ` .",
    "the number of permutations was set to 10,000 .",
    "we used the median to aggregate the estimated values for each recording session over individual trials , because the distribution of measures over trials was skewed and we considered the median a more exact representation of the distributions central tendency .",
    "the aggregation of data was necessary , because of the nested experimental design , where data trials were recorded in sessions , nested in animals ( see for example @xcite for a discussion of nested designs ) .",
    "this aggregation resulted in relatively few observations per anova cell and also in unequal number of observations between cells , two potential problems for parametric statistical analysis , because parametric assumptions are no longer testable .",
    "therefore , we used the non - parametric permutation approach , which does not make any assumptions on data structure .",
    "as described above , lfp recordings were conducted in trials over multiple recording sessions .",
    "this introduces a so - called `` nested design '' @xcite , i.e. , a hierarchical structure in the data , where data trials are nested within recordings , which are nested in animals .",
    "such structures lead to systematic errors or dependence within the data .",
    "this violates the assumption of uncorrelated errors made by the most common tests derived from the general linear model and leads to an inflation of the type i error @xcite .",
    "a measure of the degree of dependence is the intraclass correlation coefficient ( icc ) , which was 0.35 for ferret 1 and 0.19 for ferret 2 , indicating a significant dependency within the data ( see @xcite for a discussion of this measure ) .",
    "thus , we performed an additional statistical test on @xmath2 values obtained from individual trials , where we again tested for a significant effect of the two factors ` anesthesia ` and ` direction ` as well as their interaction , but we additionally modeled the nested structure in our data by a random factor ` recordings ` .",
    "such a model is called a linear mixed effects model @xcite , and may yield higher statistical power than aggregating data within one level of the nested design ( as was done for the permutation anova ) .",
    "we used the following model for both animals separately :    @xmath93    where @xmath94 is the @xmath2 value from the @xmath64-th trial in recording @xmath95 , modeled as a function of anesthesia level and direction of interaction , where @xmath96 describes the model intercept , @xmath97 are the regression coefficients and describe fixed effects , @xmath98 is the random deviation of recording @xmath95 from the intercept @xmath96 , and @xmath99 describes random noise . @xmath100 ,",
    "@xmath101 , and @xmath102 are predictor variables , encoding factors ` direction ` as    @xmath103    and factor ` anesthesia ` as    @xmath104    @xmath105    note that we used dummy coding for factor ` direction ` so that the the estimated effect @xmath106 can be interpreted like the simple or main effect in a standard anova framework .",
    "we used contrast coding for factor ` anesthesia ` which allows to interpret estimated effects @xmath107 and @xmath108 as deviations from a reference group ( in this case the condition _ awake _ ) .",
    "we further assume that noise was i.i.d . and",
    "@xmath109 and @xmath110 .",
    "we used the r language @xcite and the function ` lmer ` from the ` lme4`-package @xcite for model fitting .",
    "we assessed statistical significance of individual factors by means of model comparison using the maximum likelihood ratio between models @xcite . to allow for this model comparison , we used maximum likelihood estimation , instead of restricted maximum likelihood estimation , of random and fixed effects . to test for main effects , we compared models including individual factors ` anesthesia ` ( @xmath111 ) and ` direction ` ( @xmath112 ) to a null model ( @xmath113 ) including only the random effect ; to furthermore test for an interaction effect , we compared the model including an interaction term ( @xmath114 ) to a model where both factors only entered additively ( @xmath115 ) .",
    "the models were fitted to 15,973 @xmath2 values from ferret 1 and 18,202 @xmath2 values from ferret 2 .",
    "10                                  wollstadt p , sellers kk , hutt a , frhlich f , wibral m. anesthesia - related changes in information transfer may be caused by reduction in local information generation . in : engineering in medicine and biology society ( embc ) , 2015 37th annual international conference of the ieee .",
    "ieee ; 2015 .",
    "p. 40454048 .",
    "rivolta d , heidegger t , scheller b , sauer a , schaum m , birkner k , et  al .",
    "ketamine dysregulates the amplitude and connectivity of high - frequency oscillations in cortical ",
    "subcortical networks in humans : evidence from resting - state magnetoencephalography - recordings . schizophrenia bulletin . 2015;41(5):11051114 ."
  ],
  "abstract_text": [
    "<S> the disruption of coupling between brain areas has been suggested as the mechanism underlying loss of consciousness in anesthesia . </S>",
    "<S> this hypothesis has been tested previously by measuring the information transfer between brain areas , and by taking reduced information transfer as a proxy for decoupling . yet , information transfer is influenced by the amount of information available in the information source  such that transfer decreases even for unchanged coupling when less source information is available . </S>",
    "<S> therefore , we reconsidered past interpretations of reduced information transfer as a sign of decoupling , and asked whether impaired local information processing leads to a loss of information transfer . an important prediction of this alternative hypothesis is that changes in locally available information ( signal entropy ) should be at least as pronounced as changes in information transfer . </S>",
    "<S> we tested this prediction by recording local field potentials in two ferrets  awake , and anesthetized with isoflurane at two concentrations ( 0.5  % , 1.0  % ) .    </S>",
    "<S> we found strong decreases in the source entropy under isoflurane in visual area v1 and the prefrontal cortex ( pfc )  as predicted by the alternative hypothesis . </S>",
    "<S> the decrease in source entropy was more pronounced in pfc compared to v1 . in addition , information transfer between v1 and pfc was reduced bidirectionally , but with a more pronounced decrease from pfc to v1 . </S>",
    "<S> this links the stronger decrease in information transfer to the stronger decrease in source entropy  suggesting that the reduced source entropy reduces information transfer . </S>",
    "<S> thus , changes in information transfer under isoflurane seem to be a consequence of changes in local processing more than of decoupling between brain areas . </S>",
    "<S> our results fit the observation that the synaptic targets of isoflurane are located in local cortical circuits rather than on the synapses formed by interareal axonal projections . </S>"
  ]
}