{
  "article_text": [
    "in this section , we shall extend the latter theory on the case when the probability vector has @xmath32 components , i.e. @xmath33 . if @xmath32 is an even number than we can introduce the map of indices similar to , namely it holds @xmath34 hence the probabilities are given in the form of the matrix @xmath35 , @xmath36 , @xmath37 with components @xmath38 if @xmath32 is an odd number than we assign a zero vector @xmath39 to the @xmath32-component vector @xmath4",
    ". then we get @xmath40-component vector @xmath41 .",
    "thus the insertable map of the indices is @xmath42 and probabilities are given in the form of the matrix @xmath35 , @xmath36 , @xmath43 with components @xmath44 similarly to we can write marginal probability distributions determined by the joint probability distribution . for an odd @xmath32 these are the following @xmath45 for it is possible to obtain inequalities similar to . to this end",
    ", we define shannon entropies by @xmath46 then the shannon information that is similar to is based on the entropies @xmath47 obviously the inequality @xmath48 for all the angles @xmath9 is valid .    the spin projections @xmath49 and @xmath6 can take @xmath50 values for the spin @xmath51 . if @xmath51 is a fractional number ( @xmath32 is even ) , than projections can be @xmath52",
    ". then the components of the probability @xmath4 are given by .",
    "on the other hand , if @xmath51 is an integer number ( @xmath32 is odd ) , than spin projections can be @xmath53 . then the components of the probability vector are given by .",
    "one can take the polynomials @xmath54 as the probabilities for a fixed @xmath6 and for all @xmath49 .",
    "one of the possible choices for an odd @xmath32 is the following @xmath55 in this notations inequality for @xmath56 and @xmath57 is given by @xmath58 or for @xmath59 and @xmath60 it looks like @xmath61 our aim is to represent and by jacobi polynomials . to this end , we introduce the new notation in @xmath62 where @xmath63 hence inequalities and can be rewritten in the new terms as @xmath64 @xmath65 let us consider a special case when @xmath66 , where @xmath67 is an integer number and @xmath68 .",
    "then can be rewritten as @xmath69 where @xmath70 are the legendre polynomials @xcite .",
    "hence for this special case is determined by @xmath71    ]    ]     + and is rewritten by @xmath72 here we use the notation @xmath73 for legendre polynomials .",
    "information is shown in figures [ fig:10 ] and [ fig:11 ] for entropies and various parameters @xmath74 , @xmath75 and spins @xmath51 .",
    "to conclude we point out the main results of the work . considering the matrix elements of the unitary irreducible representations of the group @xmath76 and applying known subadditivity condition for joint probability distributions constructed from these matrix elements we obtained new inequalities for the jacobi and legendre polynomials .",
    "the inequalities correspond to entropic inequalities for shannon entropies of bipartite classical systems .",
    "the results are shown in detail on the example of spin @xmath77 , where the shannon information of the bipartite system is expressed in terms of the polynomials . the general approach to get analogous information and entropic inequalities for the arbitrary spins @xmath51",
    "is formulated .",
    "l. a. m. acknowledges the financial support provided within the russian foundation for basic research , grant 13 - 08 - 00744 a."
  ],
  "abstract_text": [
    "<S> using known entropic and information inequalities new inequalities for some classical polynomials are obtained . </S>",
    "<S> examples of jacobi and legendre polynomials are considered .    </S>",
    "<S> [ sh ]    * *    [ cols=\"^ \" , ]     where it was used the notation @xmath0 .    </S>",
    "<S> using we can write the following relation for @xmath1 @xmath2 therefore , it is also straightforward to verify the following equations for the matrix elements @xmath3 hence probabilities associated with them are also equal .    the probability vector @xmath4 can be chosen as in . </S>",
    "<S> for example , for the fixed @xmath5 and considering all elements with different @xmath6 , i.e. the elements of the first column in the table [ tab:1 ] we obtain @xmath7 the sum of latter probabilities is equal to @xmath8 for any angle @xmath9 .    </S>",
    "<S> for example , for the angle @xmath10 the probabilities are the following @xmath11 and their sum is equal to @xmath12 .    </S>",
    "<S> it is possible to fix @xmath6 and consider all matrix elements of the @xmath6th row of the table [ tab:1 ] . </S>",
    "<S> note that it is possible to construct vector @xmath4 by a variety of other combinations of elements in the table [ tab:1 ] . </S>",
    "<S> this conclusion is a direct consequence of the equality . </S>",
    "<S> it is only necessary to remember that the sum of the elements of such a vector must always be equal to one .    </S>",
    "<S> this gives us the opportunity to form a large number of inequalities based on inequality for the shannon information .    </S>",
    "<S> let us introduce the following notation @xmath13 . substituting in and </S>",
    "<S> we can write the following inequality @xmath14 entropies @xmath15 , @xmath16 , @xmath17 and the shannon information @xmath18 for probabilities are shown in figures [ fig:1 ] and [ fig:2 ] . </S>",
    "<S> obviously the information reaches its maximum value at the point @xmath19 .     for probabilities ]     for probabilities ]    let us consider how the permutation of the probabilities affects on the entropies @xmath20 and @xmath21 . </S>",
    "<S> the new vector @xmath22 can be determined by @xmath23 entropies @xmath15 , @xmath16 , @xmath17 and information @xmath18 for the probability vector @xmath24 are shown on figures [ fig:3 ] and [ fig:4 ] .     for probabilities ]     for probabilities ]    evidently the latter permutations impact only the entropy @xmath20 . </S>",
    "<S> the information turns to zero at the point @xmath19 . substituting in and we can write the following inequality @xmath25 let us select the probability vector @xmath4 using another combination of @xmath1 . for the fixed @xmath26 </S>",
    "<S> we take the elements of the second column in table [ tab:1 ] . </S>",
    "<S> then the probability vector has the following components @xmath27 substituting in we can write the following inequality @xmath28 entropies @xmath15 , @xmath16 , @xmath17 and information @xmath18 for probability vector are shown on figures [ fig:5 ] and [ fig:6 ] . evidently , they differ from the entropies and information constructed by the polynomials based on the first column of table [ tab:1 ] .     for probabilities ]     for probabilities ]    it is also interesting to see how the permutation of the components of probability vector @xmath4 will change the information . to this end </S>",
    "<S> we do the same procedure as in .    </S>",
    "<S> the new vector @xmath29 can be formed by @xmath30 thus , substituting in and new entropies and information can be obtained . </S>",
    "<S> the latter are shown in figures [ fig:7 ] and [ fig:8 ] . </S>",
    "<S> in contrast to figure [ fig:6 ] , the information for permuted vector @xmath31 equals to zero at point @xmath19 .     for probabilities ]     for probabilities ]    finally we summarize all results of figures [ fig:2 ] , [ fig:4 ] , [ fig:6 ] , [ fig:8 ] in figure [ fig:9 ] . </S>",
    "<S> obviously , for systems and the information is non zero for @xmath19 and for permutated vectors the information is zero .        from it is easy to verify that the latter two examples cover all possible probabilities . </S>",
    "<S> other combinations chosen from table [ tab:1 ] determine their permutations . </S>",
    "<S> however , jacobi polynomials corresponding to them are of course not the same . </S>",
    "<S> it allows us to obtain many different inequalities of the form . </S>"
  ]
}