{
  "article_text": [
    "micro aerial vehicles ( mavs ) , such as quadcopters equipped with a camera ( figure [ fig : bebop ] ) , are widely used in many applications , such as rescue , exploration , and entertainment . in recent years",
    ", outdoor autonomous navigation has been successfully accomplished through the use of global positioning system ( gps ) @xcite . however , gps shows limited precision in indoor environments , which brings many challenges for indoor autonomous flight .",
    "several solutions have been proposed for indoor autonomous navigation .",
    "one solution is a simultaneous localization and mapping ( slam ) . using laser range finders , rgb - d sensors , or a single camera , a 3-d map of unknown indoor environments and its position in the map",
    "can be inferred for autonomous flight ( @xcite-@xcite ) .",
    "another solution is based on stereo vision . by computing disparity between stereo images , depth",
    "can be estimated ( @xcite,@xcite ) .",
    "slam , however , is not practical for mavs because building a 3d model is computationally heavy .",
    "additionally , the constructed 3d structure often does not perform well in environments with devoid of trackable features ( e.g. , walls ) .",
    "depth estimated by stereo vision shows low performance in texture - less regions and can suffer from specular reflections . the additional fact",
    "that most of publicly available quadcopters have only one built - in camera makes the solutions not practical .    in this paper , we present a practical system enabling a quadcopter to navigate autonomously indoors and find a specific target , i.e. , a book bag . our system does not require any range finding sensors , but only a single camera .",
    "our approach is to train a classifier that mimics an expert pilot s choice of action .",
    "a deep learning model , convolutional neural network ( convnet ) , is used to train the classifier with our custom dataset .",
    "our classifier consistently receives a purely visual input from a quadcopter and returns a flight command that best mimics the expert s action . through our real - time test experiments ,",
    "we show that our system correctly finds a target while concurrently performing autonomous navigation with a success rate of 70 - 80% .",
    "one advantage of our approach is that it prevents mavs from colliding into a wall .",
    "this advantage could supplement slam : when it fails to localize its position due to the devoid of trackable environments , our approach can be used instead , until it is certain about its position .",
    "our system also does not construct a 3-d map , so it has relatively less computation .",
    "additionally , our approach does not require high resolution cameras , which makes our system attractive for many widely distributed mavs .    in the discussion section , we use visualization techniques ( @xcite,@xcite ) to visualize features and representations learned by our classifier . through the visualization",
    ", we examine what features our classifier learned from training and which feature affected the classification performance .",
    "this paper makes three principal contributions .",
    "first , we introduce a practical system based on deep learning for autonomous indoor flight .",
    "the system is practical because it requires only a single camera and is computationally less expensive .",
    "second , we provide our custom dataset .",
    "the dataset is composed of 7 indoor locations , and each location has its own unique appearance .",
    "the diversity of our dataset would be useful for other indoor research .",
    "third , we visualize the trained deep model by visualization techniques .",
    "the visualization adds knowledge for understanding the model .",
    "the remainder of this paper describes these contributions in detail .",
    "section 2 introduces related works about autonomous navigation and flight .",
    "section 3 explains our hardware platform , dataset , and training details .",
    "section 4 demonstrates our test experiments .",
    "section 5 includes discussion about our deep learning model using visualization techniques . finally , section 6 offers concluding remarks .",
    "there has been impressive research on autonomous navigation and flight of mavs . in this section ,",
    "we examine related works .    * range sensor * : one possible solution is to use range sensors , such as laser range finders , infrared sensors , or rgb - depth sensors .",
    "bry et al .",
    "@xcite presented a state estimation method using an on - board laser range finder and inertial measurement unit and showed aggressive flight in gps - denied environments .",
    "roberts et al .",
    "@xcite used one ultrasonic sensor and four infrared sensors and showed fully autonomous flight , i.e. , collision avoidance .",
    "the range sensor , however , is not practical to most of publicly available quadcopters as the on - board device is often too heavy for mavs and consume lots of power .",
    "our work is based on only a monocular camera , which consumes a low power and is built in to most of quadcopters .",
    "* slam * : using range sensors or visual sensors , a 3-d map of unknown indoor environments can be inferred , while simultaneously estimating its position in the map ( @xcite,@xcite,@xcite ) .",
    "bachrach et al .",
    "@xcite used a laser rangefinder sensor for a high - level slam implementation and exploring unknown indoor environments .",
    "celik et al @xcite presented autonomous indoor navigation based on slam using monocular vision .",
    "however , slam is computationally expensive due to the 3-d reconstruction .",
    "this causes unacceptable delay between perception and action .",
    "also , slam shows low accuracy when it is applied to indoor environments , like walls , which contain insufficient feature points that can be tracked frame to frame",
    ". our system does not perform path - planning .",
    "thus our approach is closely related to minimizing the delay by reacting fast to its currently faced situation .",
    "our system also shows robust performance on detecting and avoiding walls .",
    "* stereo vision * : accurate depth estimation and relative position estimation are possible using stereo cameras ( @xcite,@xcite ) .",
    "however , stereo vision algorithms suffer when they are used in texture - less regions , as it is hard to match features in one image to the corresponding features in the other image . the additional fact",
    "that most of publicly available quadcopters have only one built - in camera makes the solution not practical to a public .",
    "our system shows robust performance in texture - less environments .",
    "* other approach * : other approach uses vanishing points .",
    "bills et al .",
    "@xcite used a monocular camera and found vanishing points .",
    "the points were used to fly in corridor environments . for staircase environments , they found center of the staircase . a front - facing short - range sensor , however , was additionally used to avoid collisions in corners and unknown environments .",
    "our approach does not require the additional range sensor and can successfully perform collision avoidance with a monocular camera .",
    "another approaches that are most closely related to our approach are approaches that learn control policies from input data .",
    "the alvinn project @xcite showed how the 3-layer artificial neural networks imitated a human driver s response on road and performed autonomous vehicle driving .",
    "ross et al .",
    "@xcite applied a novel imitation learning strategy , the dagger algorithm , and learned a controller policy that imitated human pilot s choice of action from demonstrations of the desired behavior .",
    "the system demonstrated a fast autonomous flight in natural forest environments .",
    "we extend these learning approaches and employ an advanced classifier , convnets , which learns to autonomously fly and finds a target based on purely visual input .",
    "the goal of our work is to learn a controller strategy that mimics an expert pilot s choice of action . given real - time images taken from mavs , our trained classifier returns flight commands that best mimics the expert s actions until a target is found , as outlined in algorithm 1 .",
    "the classifier is trained through an expert pilot demonstrating a desired flight given real - time images .",
    "our training strategy allows the classifier to learn the most effective controller strategy with minimizing possible mistakes .",
    "a deep learning model of convnet is used as a classifier .",
    "we train the model by supervised learning .",
    "the architecture of our network has 5 convolutional layers and 3 fully connected layers ( figure [ fig : network ] ) .",
    "the model s parameters are learned through fine - tuning from the pre - trained caffenet model @xcite with our custom dataset .",
    "initialize and take - off a drone",
    "in this section , we will first explain our hardware platform",
    ". then we will demonstrate our custom dataset used for training convnet and details about training in the following section .",
    "our primary quadcopter is the parrot bebop drone ( figure [ fig : bebop ] ) .",
    "this quadcopter is currently available to the general public .",
    "the bebop drone contains a single forward - facing camera , an ultrasound sensor for measuring ground altitude , and an on - board computer .",
    "commands and images are exchanged via a wifi connection between our host machine and the bebop drone .",
    "the wifi connection has a signal range of 250 meters ( 0.155 miles ) . from the bebop s stream connection",
    ", we receive an image of resolution of 640x368 pixels .",
    "we run our classifier on the host machine  a 2015 alienware 15 ( nvidia geforce gtx 970 m , intel core i5 , 16 gb memory ) , running ubuntu 14.04 .",
    "there exist many publicly available indoor images datasets ( @xcite,@xcite ) .",
    "most of them , however , are not applicable to our approach because none of them do not provide ground - truth in flight commands .",
    "in addition , manually labeling flight commands by inferring could lead to a small mistake in a training dataset , which could potentially cause compounding errors . thus creating our own dataset",
    "is necessary to achieve our goal .",
    "our dataset is composed of images collected from seven different indoor locations .",
    "the locations are either a corridor or corner .",
    "the selected environments have their own unique appearance , such as different building structure , brightness , and objects ( i.e. desks ) .",
    "the locations are described in figure [ fig : trainingfloorplan ] .",
    "we notice that a constant height ( i.e. 1 meter ) is sufficient for flying in corridors or corners . except for stairways",
    ", altering altitude is generally not important , and obstacles can be easily avoided using other directions ( i.e. turning right or left ) .",
    "furthermore , considering a constant height effectively reduces control complexity .",
    "we therefore control the quadcopter at a constant height of 1 meter using only six flight commands : move forward , move right , move left , spin right , spin left , and stop ( figure [ fig : flightcommand ] ) .",
    "the stop flight command is for when the quadcopter finds a target .",
    "we set a target as a book bag ( figure [ fig : bag ] ) .                for each training location , an expert pilot controls the quadcopter using the six flight commands and demonstrates sample flights multiple times .",
    "we collect images taken from mavs and corresponding flight commands for each demonstration . during the demonstrations , we cover as many as possible failure cases .",
    "for instance , when the quadcopter is close to colliding into walls , it should avoid the collision by turning to the opposite direction of its current propagation direction .",
    "the images streamed from the bebop drone have a noise . to train a classifier robust to noise",
    ", we generate additional images and augment our dataset size by adding gaussian white noise of mean 0 and variance 0.01 to our dataset .",
    "the final dataset , therefore , has two times larger size than the previous dataset without noise .",
    "total images after data collection is summarized in table [ table : totaldataset ] .",
    "please note that the dataset includes a large number of move forward flight command as moving forward is the most frequently used command , and it is a natural behavior to navigate corridors or corners .",
    "the dataset can be downloaded from : http://www.dongkikim.com/research/nav/index.html .",
    ".number of training images taken from each location ( loc ) . depending on the location type ,",
    "either a corridor or corner , number of images for each flight command differs . for corridor environments , move forward images",
    "are collected more than the other flight commands . for corner environments ,",
    "flight commands related to turning ( move right & left , spin right & left ) are collected more than the others . [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "visualization techniques ( @xcite,@xcite,@xcite,@xcite ) have been proposed to understand deep learning models better .",
    "they provides us qualitative inner representations learned by deep networks , and they also allows us to diagnose potential problems with deep models . in this section ,",
    "we use several visualization techniques ( @xcite,@xcite ) to understand more about our trained network .",
    "the objective of class model visualization @xcite is to generate a synthetic input image , which causes a high score for a specific class .",
    "the resulting synthetic image represents what a trained model is looking for a specific class .",
    "we generate the synthetic image by computing a gradient using back - propagation and performing regularizations , such as l2 decay and gaussian blur , as described in @xcite .",
    "we initialize the optimization with the random image",
    ". then we use regularization of l2 decay and apply gaussian blur for every four optimization steps .",
    "the resulting synthetic images for each class ( flight command ) is shown in figure [ fig : classvis ] .",
    "the visualization result suggests that our classifier has learned correct features for each flight command .",
    "the stop flight command visualization , for instance , looks for features of the book bag .",
    "the distinction between different class visualization verifies that our network has learned unique features between different flight commands . however , less clear visualizations for spin right and spin left class suggest that our network could learn better about these classes . increasing number of images for spin",
    "right and spin left flight command could be one possible solution .",
    "given an image , an image - specific class saliency map is generated by using the class score derivative and rearranging the derivate @xcite .",
    "the saliency map indicates which part of an image affected a class score the most .",
    "we select 50 images for each flight command that score the most and generate the saliency maps .",
    "some results for each class is shown in figure [ fig : imagevis ] . the results highlight edges in common , which suggests edges are an important feature that affect classification performance .",
    "we have presented a deep learning based system that enables a quadcopter to navigate autonomously indoors and find a specific target . through our real - time experiments ,",
    "we show that our approach performs well in diverse indoor locations .",
    "our approach is practical as it requires only a single camera and computationally efficient as it does not reconstruct a 3-d map . in this paper , we have investigated our trained network deeper by using visualization techniques . in future work , we want to expand our dataset and experiments to more diverse indoor environments , such as stairways .",
    "this work was supported by engineering learning initiatives at cornell university .",
    "we thank robert a. cowie for research grant .",
    "we thank hang chu , yuka kihara , amandianeze nwana , and kuan - chuan peng at advanced multimedia processing laboratory for useful discussions and help .",
    "m. achtelik , a. bachrach , r. he , s. prentice , and n. roy . stereo vision and laser odometry for autonomous helicopters in gps - denied indoor environments . in proc .",
    "spie unmanned systems technology xi , 2009 .",
    "m. alexander , c. olah , and michael tyka .",
    "( 2015 , june 17 ) .",
    "inceptionism : going deeper into neural networks [ online ] .",
    "available : http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html"
  ],
  "abstract_text": [
    "<S> autonomous indoor navigation of micro aerial vehicles ( mavs ) possesses many challenges . </S>",
    "<S> one main reason is because gps has limited precision in indoor environments . </S>",
    "<S> the additional fact that mavs are not able to carry heavy weight or power consuming sensors , such as range finders , makes indoor autonomous navigation a challenging task . in this paper </S>",
    "<S> , we propose a practical system in which a quadcopter autonomously navigates indoors and finds a specific target , i.e. a book bag , by using a single camera . </S>",
    "<S> a deep learning model , convolutional neural network ( convnet ) , is used to learn a controller strategy that mimics an expert pilot s choice of action . </S>",
    "<S> we show our system s performance through real - time experiments in diverse indoor locations . to understand more about our trained network , we use several visualization techniques . </S>"
  ]
}