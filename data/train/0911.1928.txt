{
  "article_text": [
    "there are a number of statistical models that contain some sort of graphical structure .",
    "examples include image analysis , disease risk mapping and discrete spatial variation .",
    "we focus on those for which penalized regression is appropriate , and can be thought of in terms of the ` signal + noise ' framework .",
    "we consider the regression of a continuous response variable on one or more explanatory covariates . often there is some sort of graphical structure in and between the observations , or some obvious neighbouring scheme that gives rise to a graph .",
    "we think of the locations of the observations as the vertices of the graph . the edges may be suggested by the neighbouring scheme or by the covariate values .",
    "we will see some examples in this section .",
    "a model for data on the graph @xmath2 , which has vertices in the set @xmath3 and edges in the set @xmath4 , is @xmath5 the noise terms , @xmath6 , are usually assumed to be independent realizations of a random variable with zero mean and unit variance . under this model regression on",
    "a graph involves estimating the underlying signal values @xmath7 , for all vertices @xmath8 in the set @xmath3 .",
    "we use the edges to measure the complexity of the estimate .",
    "figure  [ graph ] shows an example of regression on a graph : a small , noisy image with 64 pixels .",
    "the responses are the grey levels of the pixels , so each pixel is a vertex of the graph . a natural choice of edges connects each pixel with its neighbours , resulting in the graph superimposed on the left - hand image in figure  [ graph ] .",
    "regression on this graph involves estimating the underlying signal image , which is displayed in the right - hand image .",
    "example of a graphical structure present in a regression situation .",
    "the noisy image ( left ) shows a suitable graph for regression , based on the 4-neighbourhood . on the noiseless version",
    "( right ) only the edges in the active set are shown.,title=\"fig : \" ] example of a graphical structure present in a regression situation .",
    "the noisy image ( left ) shows a suitable graph for regression , based on the 4-neighbourhood . on the noiseless version",
    "( right ) only the edges in the active set are shown.,title=\"fig : \" ]    in this article we discuss penalized regression on the graph @xmath2 .",
    "penalized regression fits an estimate that is close to the data , but penalises rough or complicated estimates . with an observation at every vertex",
    ", we can measure the distance between observed and estimated values by the sum of the distances at each vertex .",
    "the complexity of the estimate can be measured by the differences between the estimated values at adjacent observations .",
    "this measurement is therefore the sum of absolute differences at each edge .",
    "we discuss the penalized regression estimate that minimises @xmath9 for given weights @xmath10 , for @xmath11 , and smoothing parameters @xmath12 , for @xmath13 .",
    "this is the sum of a term that penalises distance from the data plus a term that penalises roughness .",
    "the first term is the distance from the data , measured at every vertex in the @xmath0 norm .",
    "the second term is the weighted sum of roughness at every edge , measured in the @xmath1 norm .",
    "our model allows for a different weight or smoothing parameter at each vertex and each edge .",
    "although it is usual , in graph theory , to denote the edges by unordered pairs , we will treat @xmath4 as a set of ordered pairs for convenience of notation .",
    "this does not mean that @xmath2 is a directed graph , since the ordering can be completely arbitrary .",
    "we do , however , consider there to be at most one edge that joins any pair of vertices .",
    "this is because it makes no sense to split the penalty between two vertices over more than one edge .",
    "as a first motivating example , we consider the problem of nonparametric regression between two continuous variables .",
    "suppose we have response observations @xmath14 taken at strictly ordered design points .",
    "there is a natural neighbouring structure : the first observation is adjacent to the second , the second is next to the third , and so on .",
    "hence a natural graphical structure for this example is given by @xmath15 , where @xmath16    the minimization of @xmath17 provides an estimate of @xmath7 at every observation .",
    "if we let @xmath18 for all @xmath19 and use the convenient shorthand @xmath20 , then @xmath17 becomes @xmath21 and the roughness penalty is the weighted total variation of the estimate .",
    "total variation can be extended to higher dimensions to tackle , for example , image analysis .",
    "an image can be thought of as an @xmath22 grid of pixels , with observations at each pixel .",
    "then the set of vertices of the graph is the set of pixels @xmath23    there are a number of neighbouring structures in use in image analysis .",
    "the simplest is the 4-neighbourhood ( winkler 2003 , p.  57 ) in which a pixel has neighbours immediately above , below , to the left and to the right .",
    "this neighbouring scheme suggests the set of edges @xmath24 figure  [ graph ] shows a picture of this graph .    using the graph @xmath25",
    ", we can find a denoised image by minimising @xmath17 . now",
    "the roughness penalty is a measure of the total variation in the horizontal direction plus the total variation in the vertical direction .",
    "mammen and van de geer ( 1997 ) first discussed the estimator obtained by minimising ( [ eq : tvdenoising ] ) where @xmath26 is a global smoothing parameter .",
    "some authors have allowed the smoothing parameters to differ .",
    "for example davies and kovac ( 2001 ) alter them during their local squeezing procedure .",
    "there are fast algorithms that find the solution to this specific minimization problem , in particular the taut string algorithm of davies and kovac ( 2001 ) , which has @xmath27 computational complexity .",
    "the estimator that minimises ( [ eq : tvdenoising ] ) , in which error is measured in the @xmath0 norm and roughness in the @xmath1 norm , is a nonparametric version of the least absolute shrinkage ( lasso ) estimator ( tibshirani 1996 ) .",
    "therefore the estimator that minimises @xmath17 can be seen as a generalization of the nonparametric lasso to any graph .",
    "there are other methods of penalized regression , with different roughness measures , that have been applied to observations on a graph .",
    "belkin et al .",
    "( 2004 ) describe an algorithm for tikhonov regularization .",
    "their algorithm measures roughness at every edge in the @xmath0 norm .",
    "koenker and mizera ( 2004 ) employ a penalty term for triograms .",
    "given irregularly - spaced observations , they create a graph by computing a delaunay triangulation of the observations .",
    "their penalty term is also a weighted sum over all edges of the triangulation .",
    "however they measure roughness as the squared ( @xmath0 ) differences between gradients .",
    "jansen et al .",
    "( 2009 ) have discussed wavelet lifting as a method for regression on a graph . like koenker and mizera ,",
    "the authors use a delaunay triangulation .",
    "our algorithm is based on ideas similar to active set methods , which features in a number of algorithms , including that of goldfarb and idnani ( 1983 ) .",
    "in theorem  [ theorem : one ] below we give a sufficient condition for @xmath28 to minimize @xmath17 and in subsection [ section : algorithm ] we present a fast algorithm for finding such a minimizer .",
    "the minimum exists because @xmath17 , as a sum of convex functions , is convex itself .",
    "therefore any local minimum of @xmath17 will be a global minimum , and the set of all global minima will be a convex set . in the important case where all the weights @xmath29 are strictly positive a unique global minimum exists , because @xmath17 is strictly convex .",
    "the solution to the minimization problem is characterized by _ regions of constant value _ , that is , sets of neighbouring vertices that share the same value of @xmath28 .",
    "we define such regions by use of a special _ active set _ of edges , indexed by @xmath30 .",
    "this consists of edges @xmath13 for which @xmath31 , such that the graph @xmath32 is acyclic .",
    "note that , unlike the definition of active set used in many optimization algorithms , there can still be edges @xmath33 such that @xmath31 .",
    "we will denote by @xmath34 the entire region of constant value that contains the vertex @xmath35 .",
    "more formally let @xmath36 we will also denote by @xmath37 that subset of the active set that holds the region @xmath34 together , so @xmath38 figure  [ graph ] shows an example of an active set in the graph @xmath25 .",
    "note how the edges in the active set join together vertices that share the same value , thus holding together regions of constant value .",
    "since @xmath32 is acyclic , the graph @xmath39 is a connected , acyclic graph .",
    "this feature is crucial as it allows the region @xmath34 to be split into two subregions by removing just one edge @xmath40 from @xmath37 .",
    "we will denote these two subregions by @xmath41 and @xmath42 , where @xmath43 we associate with the region or subregion @xmath44 ( where @xmath45 or @xmath46 ) the quantities @xmath47 +    [ theorem : one ] suppose there exists a fit @xmath28 and set of edges @xmath30 such that @xmath31 for all @xmath48 and @xmath49 is acyclic .",
    "also suppose there are values @xmath50 such that @xmath51 then @xmath28 minimises @xmath17 .    a proof is given in the appendix .",
    "these conditions can be shown to be similar to the taut string of davies and kovac ( 2001 ) . when the graph is @xmath15 the condition ( [ eq : splitcondition ] ) describes a tube and ( [ eq : entirecondition ] ) describes a string threaded through the tube and pulled taut ( mammen and van de geer 1997 ) .",
    "the algorithm that we describe can be considered to search for the graph @xmath49 and vector @xmath52 described in theorem  [ theorem : one ] . at any point during the algorithm the current value of @xmath52 defines a working objective function @xmath53 when we have satisfied the constraints ( [ eq : stoppingrule ] ) then @xmath54 .",
    "the current value of @xmath28 always minimises @xmath55 so when ( [ eq : stoppingrule ] ) holds it also minimises @xmath17 . for @xmath28 to minimize @xmath55 a slightly modified version of theorem  [ theorem : one ] tells us that we must have @xmath56 ( [ eq : nonzero ] ) and ( [ eq : entirecondition ] ) must hold , and @xmath57    we start with @xmath58 . in this initial case @xmath59 , so we start with @xmath60 as this is the minimizer .",
    "our algorithm gradually increases the penalty on each edge : at each iteration @xmath61 moves from @xmath62 to @xmath63 for one particular edge @xmath64 . once",
    "( [ eq : stoppingrule ] ) is satisfied for an edge , then it remains satisfied .",
    "the algorithm stops when ( [ eq : stoppingrule ] ) is satisfied at all edges",
    ". this event will occur in a finite time , as stated by theorem  [ theorem : two ] below .",
    "[ theorem : two ] the algorithm described here will terminate in a finite time , and finds a minimizer of @xmath17 , for any graph , data , weights and smoothing parameters .",
    "the proof is contained in the appendix .",
    "+ we now give precise details about each iteration of our algorithm . at each iteration",
    "we start with @xmath28 that minimises @xmath65 and move to @xmath66 that minimises @xmath67 .",
    "we start each iteration with an edge @xmath68 chosen such that @xmath69 and @xmath70 .",
    "we want to move in the direction that satisfies @xmath71 .",
    "the condition ( [ eq : entirecondition ] ) tells us we need @xmath72 such that @xmath73 for @xmath74 and @xmath75 .",
    "it is clear that as @xmath52 changes , @xmath28 must change to compensate .",
    "as @xmath61 changes , the penalty on the edge @xmath68 increases , so we must reduce @xmath76 in order to move to the minimum of @xmath77 .    this change must take place within the constraints of the active set .",
    "therefore we must alter @xmath78 and @xmath79 uniformly on the whole of the regions @xmath34 and @xmath80 .",
    "this means @xmath81 for @xmath82 and @xmath83 for @xmath84 . in order to preserve ( [ eq : entirecondition ] )",
    "we must have @xmath85 for @xmath86 .",
    "so the regions @xmath34 and @xmath80 will move closer together in value .    as the regions move closer together there may need to be changes to the active set . to make sure that these changes happen we will increase the penalty on @xmath68 in small steps",
    ". specifically we will change @xmath28 and @xmath52 only by enough to trigger the first change in the active set .    in this subsection",
    "we will discuss the possible changes to the active set as @xmath34 and @xmath80 move closer together .",
    "there are four possible events that could happen : no change , merging of @xmath34 and @xmath80 , amalgamation with a neighbouring region , and splitting a region .    for each of these events",
    "we give , below , the associated values of @xmath87 , @xmath88 and @xmath89 .",
    "we also describe appropriate adjustments to the active set . in order to trigger the first change in the active set",
    ", the algorithm chooses the event for which @xmath90 and @xmath91 are both smallest .",
    "the appendix contains proofs of these values .",
    "once the no change or merging steps are complete , we can set @xmath92 and the iteration is over .",
    "we choose another edge @xmath68 for which @xmath69 and @xmath93 and iterate again .",
    "if there is no such edge then the algorithm stops , since @xmath54 .",
    "once amalgamation or splitting has taken place , we proceed to further reduce @xmath94 , now altering @xmath28 uniformly on a changed region .      there may be no disruption necessary to the active set before @xmath95 is satisfied .",
    "this means that we have @xmath96 and @xmath97 , and ( [ eq : modifiedsplit ] ) still holds for all @xmath98 .",
    "this event can only occur if @xmath99 and @xmath100 .",
    "the associated changes in @xmath78 and @xmath79 are @xmath101      before we reach the target value of @xmath102 , the regions @xmath34 and @xmath80 might meet each other in value .",
    "this would mean that @xmath103 and @xmath76 can be decreased no further .",
    "the changes in @xmath78 and @xmath79 are @xmath104 if @xmath105 then we can choose @xmath106 and @xmath107 .",
    "since we now have @xmath108 we merge the two regions @xmath34 and @xmath80 by adding @xmath68 to the active set .",
    "if there are other edges that join @xmath34 and @xmath80 , then they will not be added to @xmath30 , even though they share the same value of @xmath28 .",
    "this will ensure that the graph @xmath32 remains acyclic .",
    "before we reach the minimizer of @xmath77 , the value of @xmath28 in the region @xmath34 may meet the value in a neighbouring region that is not @xmath80 .",
    "more formally there may be a vertex @xmath82 and @xmath109 for which @xmath110 or @xmath111 , and @xmath112 or @xmath113 .",
    "this event is only possible if @xmath100 , or if @xmath114 , or if @xmath115 and @xmath116 .",
    "the changes to @xmath28 associated with this event are @xmath117    we now have @xmath118 and if we proceed to alter @xmath28 we may break the constraint ( [ eq : weakcondition ] ) at the edge @xmath119 or @xmath120 .",
    "therefore , if @xmath121 or @xmath122 , we add this edge to the active set .",
    "this will amalgamate the region @xmath123 into @xmath34 .",
    "if there are other edges that join @xmath34 and @xmath123 then they will not be added to @xmath30 .",
    "this ensures that the graph @xmath32 remains acyclic .",
    "of course a similar amalgamation might occur with a neighbour of @xmath80 .      before arriving at the minimizer of @xmath77 we must test whether an edge @xmath98 should be removed from the active set .",
    "this will split the region @xmath34 or @xmath80 into two subregions .",
    "if the split takes place it may be necessary to swap the sign of @xmath124 , in order to preserve the constraint ( [ eq : weakcondition ] ) at @xmath40 .",
    "this will not affect @xmath55 .",
    "we use condition ( [ eq : modifiedsplit ] ) to tell us when an edge should be removed , once we have accounted for the possible sign change .    this event can only occur if @xmath99 and @xmath100 .",
    "the values of @xmath28 and @xmath52 at which @xmath125 should be removed are given by @xmath126 with @xmath127 for @xmath128 and @xmath129 for @xmath130 .",
    "the corresponding values for @xmath131 are obtained by swapping @xmath35 and @xmath132 .",
    "we now discuss the computational complexity of our algorithm in the setting of image analysis , in which the graph is @xmath25 . for the sake of simplicity we consider a square image , letting @xmath133 be an @xmath134 grid of vertices .",
    "we are interested in the computational complexity in terms of the number of observations , or vertices , @xmath135 .",
    "so @xmath136 and the set @xmath137 contains @xmath138 edges .",
    "suppose we were to use a generic active set method to minimize @xmath55 subject to ( [ eq : stoppingrule ] ) .",
    "this would be very computationally expensive , mainly because we may need to try all possible combinations of @xmath52 in @xmath139 , which leads to exponential complexity .",
    "our algorithm does not need to try all combinations of @xmath52 .",
    "in fact once @xmath92 is satisfied it will remain satisfied until our algorithm stops .",
    "therefore we only have to consider each edge once when satisfying ( [ eq : stoppingrule ] ) .",
    "so we need only perform @xmath27 iterations instead of @xmath140 .",
    "in addition , our algorithm does not need to check all possible active sets every time we add an edge . in the process of satisfying ( [ eq : stoppingrule ] ) for one edge we may need to change the active set many times , through repeated splitting or amalgamation .",
    "since @xmath94 decreases monotonically , once an edge has been removed from @xmath37 or @xmath141 it can not be included again during this iteration .",
    "therefore , during one iteration , every edge may be added once , and removed once , from the active set .",
    "so our algorithm considers at most @xmath142 active sets per iteration .",
    "finally , for each of these active sets we will need to make some calculations .",
    "it is possible to calculate @xmath143 , @xmath144 and @xmath145 , @xmath146 for all @xmath98 without visiting a vertex in @xmath147 more than twice .",
    "the algorithm must check for possible neighbouring regions to amalgamate with .",
    "it must also check condition ( [ eq : modifiedsplit ] ) at every edge in @xmath37 and @xmath141 .",
    "since @xmath39 and @xmath148 are connected , acyclic graphs , there will only be @xmath149 and @xmath150 edges to check",
    ". therefore the complexity of the calculation is @xmath151 .",
    "this is at most @xmath27 , compared with @xmath152 for methods based on matrix inversion , such as that of goldfarb and idnani ( 1983 ) .",
    "we can reduce the computational complexity even further by working with small sub - images that gradually increase in size .",
    "we control the order in which the edge constraints ( [ eq : stoppingrule ] ) are satisfied in order to keep @xmath153 and @xmath154 as small as possible . here",
    "we describe an implementation of our algorithm in which the maximum size of a region grows dyadically . for the sake of simplicity",
    "we will consider @xmath155 to be an integer power of 2 .",
    "it is easy to adapt this method for other values of @xmath155 , and for non - square images .",
    "the edge constraints are satisfied in stages , there being @xmath156 stages in total . at stage",
    "@xmath157 we consider those edges in the set @xmath158 followed by those in the set @xmath159    the effect is that as the edges are considered the graph of satisfied edges grows dyadically . at the first stage the graph looks like pairs of vertices , followed by squares of @xmath160 vertices . at the second stage",
    "the graph looks like connected rectangles of @xmath161 vertices , followed by squares of @xmath162 vertices .",
    "the process continues until all edges are satisfied and the whole square of @xmath134 vertices are connected .",
    "the advantage of this implementation is our algorithm will never allow an edge @xmath68 in the active set if @xmath163",
    ". therefore @xmath34 and @xmath80 can never be larger than the rectangle connected by satisfied edges that contains @xmath35 and @xmath132 . at stage @xmath157",
    "this rectangle will contain at most @xmath164 vertices .",
    "furthermore in the process of satisfying @xmath92 , the active set will only change on edges inside this connected rectangle .",
    "so there are at most @xmath165 active sets to consider .",
    "it is possible to find the total computational complexity of this implementation . at every stage",
    "we must satisfy constraints on @xmath166 edges . for each of these edges we may have to check @xmath167 active sets and for each active set perform @xmath167 calculations .",
    "therefore the overall complexity is @xmath168",
    "the data shown in figure  [ baseline ] are an excerpt from the spectroscopic analysis of a gallstone .",
    "looking at the data , it seems reasonable to think of the points as having been generated by a function that is a flat baseline with occasional spikes .",
    "furthermore we have information about the correct location and number of spikes ( davies and kovac 2001 ) .",
    "the left - hand plot in figure  [ baseline ] shows an estimate obtained by minimising ( [ eq : tvdenoising ] ) .",
    "the smoothing parameters @xmath169 were chosen by local squeezing , which aims to arrive at the smoothest function that satisfies the multiresolution condition the smoothing parameters are only reduced in intervals where the multiresolution condition is not satisfied . the estimates also show a mean correction : after running our algorithm we reset @xmath7 to the mean of the observations in @xmath170 , for all @xmath8 .",
    "see davies and kovac ( 2001 ) for more details .",
    "the estimate in figure  [ baseline ] identifies all the spikes .",
    "however the left - hand estimate has not identified the constant baseline well .",
    "outside of the spikes , at the flat parts of the estimate , the fitted function takes many different values .",
    "scatterplots of data from spectroscopy .",
    "the solid lines show the function fitted by means of a total variation penalty ( left ) and the improved estimate of the baseline ( right).,title=\"fig : \" ] scatterplots of data from spectroscopy .",
    "the solid lines show the function fitted by means of a total variation penalty ( left ) and the improved estimate of the baseline ( right).,title=\"fig : \" ]    we propose a different graph that enables the algorithm to find a better estimate of the constant baseline .",
    "we introduce a new vertex , indexed @xmath171 .",
    "this corresponds to a dummy observation with value @xmath172 .",
    "we set the weight @xmath173 , so that the value of @xmath174 can not influence the fitted function @xmath175 .",
    "this new vertex is connected to the rest of the graph with @xmath135 new edges .",
    "one new edge connects each existing observation to the dummy observation .",
    "the idea is that the baseline regions ( those observations or vertices that are not at a spike ) will be joined together via the dummy vertex .",
    "all of the baseline regions can be joined into one region .",
    "the result is a constant baseline everywhere that there is not a bump .",
    "the estimate of the baseline value will also improve , since the region contains more observations .",
    "it is assumed that there are more observations in the baseline region than at a spike , so the dummy vertex will join the baseline region and not another region .",
    "it remains to fix the values @xmath176 . with no prior knowledge about the location of the spikes , we set @xmath177 . by using equal smoothing parameters we will not encourage any particular vertex to join the baseline region .",
    "the other smoothing parameters , @xmath169 are still chosen by local squeezing .",
    "we suggest setting @xmath178 so that no vertex will be influenced by the baseline more than its neighbours .",
    "it is easy to see , in the right - hand plot of figure  [ baseline ] , the improvement that this graph causes at the baseline .",
    "figure  [ image ] shows , on the left , a noisy image that was used as an example by polzehl and spokoiny ( 2000 ) .",
    "this example demonstrates the use of our algorithm in the case where the graph is @xmath179 , which is suggested by the 4-neighbourhood .",
    "this particular image exhibits areas of solid colour , with sharp discontinuities between them .",
    "we would expect to see this in many images .",
    "our algorithm works well on this kind of image , because the areas of solid colour can be represented by regions of constant value .",
    "there are many proposed methods for choosing the smoothing parameters .",
    "as , at this point , we are only interested in demonstrating our algorithm , we have employed a simple method suggested by rudin et al .",
    "it uses a global smoothing parameter , @xmath26 , and is based around an estimate of the global variance , @xmath180 .",
    "of course our algorithm allows different smoothing parameters at every edge , so we can make use of more complicated methods if we wish .    in order to find the simplest image for which the residuals behave as expected , we increase @xmath26 until @xmath181 . according to chambolle ( 2004 )",
    "this value of @xmath26 will always exist .",
    "of course we require an estimate of @xmath180 that is independent of the residuals .",
    "we can use , for example , one similar to that proposed by davies and kovac ( 2001 ) : @xmath182    the output of our algorithm , the image estimated by use of the graph @xmath25 , is shown in the right - hand image of figure  [ image ] .",
    "noisy ( left ) and denoised ( right ) versions of the image of polzehl and spokoiny ( 2000).,title=\"fig : \" ] noisy ( left ) and denoised ( right ) versions of the image of polzehl and spokoiny ( 2000).,title=\"fig : \" ]      we generated 1000 covariates uniformly on @xmath183 \\times [ 0,1]$ ] . at each of these points we calculated a value from the function @xmath184",
    "this function describes a surface with a broad bump at @xmath185 and two sharper , inverted bumps at @xmath186 and @xmath187 . to each of these values we added gaussian noise with zero mean and standard deviation 0.05 to make 1000 noisy response observations .",
    "the noisy surface is shown in figure  [ tri ] .",
    "example of irregularly - spaced data .",
    "the noisy simulated data is shown ( top left ) together with a kernel estimate ( bottom left ) .",
    "note the presence of many additional bumps in the kernel estimate .",
    "the delaunay triangulation ( top right ) shows the location of vertices , and the edges of the graph that we obtain . the final plot ( bottom right )",
    "shows the estimate obtained by minimising @xmath17 over the vertices of this graph .",
    ", title=\"fig:\",width=240 ] example of irregularly - spaced data .",
    "the noisy simulated data is shown ( top left ) together with a kernel estimate ( bottom left ) .",
    "note the presence of many additional bumps in the kernel estimate .",
    "the delaunay triangulation ( top right ) shows the location of vertices , and the edges of the graph that we obtain .",
    "the final plot ( bottom right ) shows the estimate obtained by minimising @xmath17 over the vertices of this graph .",
    ", title=\"fig : \" ] + example of irregularly - spaced data .",
    "the noisy simulated data is shown ( top left ) together with a kernel estimate ( bottom left ) .",
    "note the presence of many additional bumps in the kernel estimate .",
    "the delaunay triangulation ( top right ) shows the location of vertices , and the edges of the graph that we obtain . the final plot ( bottom right )",
    "shows the estimate obtained by minimising @xmath17 over the vertices of this graph .",
    ", title=\"fig:\",width=240 ] example of irregularly - spaced data .",
    "the noisy simulated data is shown ( top left ) together with a kernel estimate ( bottom left ) .",
    "note the presence of many additional bumps in the kernel estimate .",
    "the delaunay triangulation ( top right ) shows the location of vertices , and the edges of the graph that we obtain . the final plot ( bottom right )",
    "shows the estimate obtained by minimising @xmath17 over the vertices of this graph .",
    ", title=\"fig:\",width=240 ]    in order to calculate an estimate for @xmath28 the delaunay triangulation was used to connect the irregularly spaced covariates by a graph , see figure  [ tri ] .    for the sake of comparison ,",
    "figure  [ tri ] also shows a kernel estimate applied to the data .",
    "we chose the global bandwidth that minimises the true squared error between the kernel estimate and the function given by ( [ eq : dfunc ] ) .",
    "so this can be thought of as the ` best ' global - bandwidth kernel estimate .",
    "although it identifies the three bumps , it also exhibits many additional bumps in locations where the signal function is practically flat .",
    "the bottom right plot in figure  [ tri ] shows the output of our algorithm , the result of minimising @xmath17 on the graph given by the delaunay triangulation .",
    "we chose a global smoothing parameter by the same method as the image analysis example .",
    "this estimate identifies the three signal bumps but does not suffer from the introduction of extra bumps .",
    "there is a large region of constant value where the signal function is flat , so the estimate is also flat in these locations .",
    "we will show that ( [ eq : entirecondition ] ) , ( [ eq : weakcondition ] ) and ( [ eq : modifiedsplit ] ) are sufficient for @xmath28 to minimize @xmath55 .",
    "theorem  [ theorem : one ] easily follows when ( [ eq : stoppingrule ] ) also holds .",
    "the problem of minimising @xmath55 can be posed as a constrained optimization problem with objective function @xmath188 minimized subject to @xmath189 and @xmath190 for all @xmath13 .    the karush ",
    "kuhn  tucker conditions ( see for example bazaraa , sherali and shetty 1993 , chap .  4 ) give a sufficient condition for @xmath28 and @xmath191 to be a solution .",
    "we require the existence of lagrange multipliers @xmath192 and @xmath193 such that @xmath194 if @xmath195 and @xmath196 if @xmath197 , where @xmath198 and @xmath199 when ( [ eq : weakcondition ] ) holds @xmath200 and hence @xmath194 if @xmath201 .",
    "otherwise the non - negativity requirements on @xmath202 and @xmath203 imply @xmath204 .",
    "now suppose there exists an active set @xmath30 such that @xmath49 is acyclic , and ( [ eq : entirecondition ] ) and ( [ eq : modifiedsplit ] ) hold .",
    "the system of equations in ( [ eq : lagrange ] ) is equivalent to the system of equations obtained by summing ( [ eq : lagrange ] ) over all regions and subregions defined by @xmath30 .",
    "this system is : for every @xmath205 or @xmath206 , @xmath207 when ( [ eq : entirecondition ] ) and ( [ eq : modifiedsplit ] ) hold appropriate lagrange multipliers exist for the above system of equations to be sufficient for @xmath28 to minimize @xmath55 .",
    "namely @xmath208 if @xmath209 and @xmath210 otherwise .      in this subsection",
    "we prove the different values of @xmath87 , @xmath88 and @xmath89 associated with the events described in subsection [ section : algorithm ] .",
    "the condition ( [ eq : entirecondition ] ) tells us that @xmath211 and @xmath212 , and also @xmath213 and @xmath214 .",
    "combining these equations we see that we must have @xmath215      if there are no necessary changes to the active set , then @xmath61 will reach the target value of @xmath216 .",
    "therefore @xmath217 .",
    "the values of @xmath87 and @xmath88 follow from ( [ eq:1k ] ) and ( [ eq:1l ] ) respectively , as does the requirement that @xmath99 and @xmath100 .      the two regions @xmath34 and @xmath80 will merge when @xmath218 .",
    "provided that @xmath99 and @xmath100 , combining the above equation with ( [ eq:1k ] ) and ( [ eq:1l ] ) gives ( [ eq : merge ] ) .",
    "if @xmath105 then we can set @xmath108 equal to any value that we choose , such as the mean and median value @xmath219 .      given a suitable vertex @xmath220 , the two regions @xmath34 and @xmath123 will amalgamate when @xmath221 .",
    "when @xmath100 the values in ( [ eq : amalg ] ) follow immediately from ( [ eq:1k ] ) and ( [ eq:1l ] ) .",
    "when @xmath115 equating ( [ eq:1k ] ) and ( [ eq:1l ] ) shows that either @xmath222 or @xmath223 . in either case",
    "it makes little sense to alter @xmath79 , so we let @xmath224 .",
    "suppose we split @xmath34 by removing @xmath40 from @xmath30 .",
    "the value of @xmath78 at which this happens satisfies ( [ eq : modifiedsplit ] ) in equality . without loss of generality",
    "suppose @xmath130 .",
    "we will need to swap the sign of @xmath124 if @xmath225 . once this is taken into account @xmath146",
    "becomes @xmath226 and ( [ eq : modifiedsplit ] ) becomes @xmath227 the value for @xmath87 follows when the upper limit is satisfied in equality .",
    "if @xmath222 then @xmath145 = 0 so ( [ eq : modifiedsplit ] ) will never change when @xmath78 changes . if @xmath115 then @xmath223 from equating ( [ eq:1k ] ) and ( [ eq:1l ] ) .",
    "clearly for @xmath78 to change and a split to occur we must have @xmath99 and @xmath100 .",
    "the value for @xmath88 follows from equating ( [ eq:1k ] ) and ( [ eq:1l ] ) .",
    "we will show that the objective function at the end of each iteration , @xmath228 , is never less than the objective function at the start of the iteration , @xmath55 . since @xmath28 minimises @xmath55 and @xmath73 except for @xmath229",
    ", we have @xmath230 equality can only occur when @xmath231 or @xmath232 .",
    "so the only time that @xmath55 does not increase is during merging or amalgamation .",
    "therefore an edge can not be removed from the active set without an increase in @xmath55 .",
    "this means that the algorithm never visits the same value of @xmath52 and @xmath30 twice , and will always arrive at the situation described in ( [ eq : stoppingrule ] ) and terminate .",
    "bazaraa , m. s. , sherali , h. d. , and shetty , c. m. ( 1993 ) , _ nonlinear programming _ , new york : john wiley & sons .",
    "belkin , m. , matveeva , i. , and niyogi , p.  ( 2004 ) , `` regularization and semi - supervised learning on large graphs , '' in _ learning theory _ , eds . j.  shawe - taylor and y.  singer , berlin : springer - verlag , pp .",
    "624638 .",
    "jansen , m. , nason , g. p. , and silverman , b. w. ( 2009 ) , `` multiscale methods for data on graphs and irregular multidimensional situations , '' _ journal of the royal statistical society , series b _ , * 71 * , 97125 ."
  ],
  "abstract_text": [
    "<S> the ` signal plus noise ' model for nonparametric regression can be extended to the case of observations taken at the vertices of a graph . </S>",
    "<S> this model includes many familiar regression problems . </S>",
    "<S> this article discusses the use of the edges of a graph to measure roughness in penalized regression . </S>",
    "<S> distance between estimate and observation is measured at every vertex in the @xmath0 norm , and roughness is penalized on every edge in the @xmath1 norm . </S>",
    "<S> thus the ideas of total - variation penalization can be extended to a graph . </S>",
    "<S> the resulting minimization problem presents special computational challenges , so we describe a new , fast algorithm and demonstrate its use with examples .    </S>",
    "<S> further examples include a graphical approach that gives an improved estimate of the baseline in spectroscopic analysis , and a simulation applicable to discrete spatial variation . in our example , penalized regression outperforms kernel smoothing in terms of identifying local extreme values . in all examples we use fully automatic procedures for setting the smoothing parameters . </S>"
  ]
}