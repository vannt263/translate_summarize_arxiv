{
  "article_text": [
    "the megacam data archive at the cadc is one of the largest astronomical imaging archives in the world . while smaller in sky coverage than the sloan digital sky survey ( sdss ) , the higher resolution of megacam means that the data volume is many times greater .",
    "the biggest barrier to using archival megacam images is the effort required to process them .",
    "megacam is a 36 ccd mosaic camera on cfht @xcite .",
    "each megacam image is about 0.7 gb ( in 16-bit integer format ) , making image retrieval over the web tedious . because of the distortion of the megaprime focal plane",
    ", the images must be resampled .",
    "this involves substantial computational demands both in terms of cpu and disk space .",
    "while the barriers to using archival megacam data are not insurmountable , they make using these data considerably less attractive .",
    "megapipe aims to increase usage of megacam data by removing these barriers by processing all public megacam data , combining the multiple individual input images into a single , well - calibrated , distortion - free output image .",
    "this paper briefly describes the megapipe image processing pipeline .",
    "megapipe is discussed in greater detail in @xcite .",
    "the first step of the megapipe pipeline is to ensure that each input image can be calibrated .",
    "images with short exposure times ( @xmath050 seconds ) , images of nebulae or images taken under conditions of poor transparency can not be used . in order for photometric calibration to take place , an image",
    "must either be taken on a photometric night or contain photometric standards , either from the sdss @xcite or sources from previously processed megapipe images .",
    "each image is inspected visually .",
    "images with obviously asymmetric psfs ( due to loss of tracking ) , unusually bad seeing , bad focus or other major defects are discarded .",
    "images taken at similar pointings on the sky are then grouped together . in the case of a particular project like the cfhtls ,",
    "this grouping is obvious ; related images are known _",
    "a priori_. when processing the rest of the megacam archival images , megapipe scans through the database of megacam centers and groups neighboring images using a friends - of - friends algorithm .",
    "two images are deemed `` friends '' if their centers lie within 0.1 .",
    "it is generally not worth stacking less than four images , so only groups with at least four images in one filter are queued for processing .",
    "the first step in the astrometric calibration pipeline is to run the well - known sextractor @xcite source detection software on each image .",
    "the sextractor catalog is cleaned of cosmic rays and extended sources to leave only real objects with well - defined centres : stars and ( to some degree ) compact galaxies .",
    "for the first band to be reduced , these observed source catalogs are matched with an external astrometric catalog to provide an initial astrometric solution .",
    "this external catalog is either the usno a2 or the sdss data release 7 ( dr7 ) .",
    "for the subsequent bands , the image catalogs are first matched to the usno to provide a rough wcs and then matched to a catalog generated using the first image so as to precisely register the different bands .",
    "the higher order terms of the astrometric solution are determined on the scale of the entire mosaic ; the distortion map of the entire focal plane is measured .",
    "this distortion is well described by a polynomial with second- and fourth - order terms in radius measured from the centre of the mosaic . determining the distortion in this way means that only two parameters need to be determined ( the coefficients of @xmath1 and @xmath2 ) with typically ( 20 - 50 stars per chip ) @xmath3 ( 36 chips ) @xmath4 1000 observations .",
    "if the analysis is done chip - by - chip , a third - order solution requires ( 10 parameters per chip ) @xmath3 ( 36 chips)= 360 parameters .",
    "this is less satisfactory .",
    "> from the global distortion , the distortion local to each ccd is determined .",
    "the local distortion is translated into a linear part ( described by the cd matrix ) and a higher order part ( described by the pv keywords ) .",
    "the higher order part is third order as well , but the coefficients depend directly and uniquely on the two - parameter global radial distortion .",
    "the sdss dr7 serves as the basis of the photometric calibration .",
    "all images lying in the sdss can be directly calibrated without referring to other standard stars .",
    "the catalog for each megacam image is matched to the corresponding catalog from the sdss .",
    "the difference between the instrumental megacam magnitudes and the sdss magnitudes ( converted to the megacam system ) gives the zero - point for that exposure or that ccd .",
    "the zero - point difference is determined by median , not mean .",
    "there are about 10,000 sdss sources per square degree , but when one cuts by stellarity and magnitude this number drops to around 1,000 .",
    "it is best to only use the stars ( the above colour terms are more appropriate to stars than galaxies ) and to only use the objects with @xmath5 ( the brighter objects are usually saturated in the megacam image and including the fainter objects only increases the noise in the median ) .",
    "this process can be used for data from any night , photometric or not .    for objects outside the sdss ,",
    "the elixir @xcite photometric keywords are used , with modifications .",
    "the elixir zero - points were compared to those determined from the sdss using the procedure above for a large number of images .",
    "there are systematic offsets between the two sets of zero - points , particularly for the u - band .",
    "these offsets show variations with epoch , which are caused by modifications to the elixir pipeline . for megapipe",
    ", the offsets are applied from the elixir zero - points to bring them in line with the sdss zero - points .",
    "the calibrated images are coadded using the program swarp @xcite .",
    "the resulting stacks are simple fits files ( not multi - extension fits files ) measuring about 20000 pixels by 20000 pixels ( about 1 degree by 1 degree ) , depending on the input dither pattern , and are about 1.7 gb in size .",
    "they have a sky level of 0 counts .",
    "they are scaled to have a photometric zero - point of 30.000 in ab magnitudes . a weight map ( inverse variance ) of the same size is also produced .",
    "sextractor is run on each stack using the weight map .",
    "the resulting catalogs only pertain to a single band ; no multi - band catalogs have been generated , except for special cases , like the cfhtls . while this fairly simple approach works well in many cases , it is probably not optimal in some situations .",
    "depending on the application , some users may wish to run their own catalog generation software on the stacks .",
    "the positions and magnitudes of the objects in the catalogs are used to check the astrometric and photometric uncertainties .",
    "catalogs from different bands of the same group are matched up .",
    "the residuals indicate the internal astrometric uncertainty is typically 0.025 .",
    "similarly , the astrometric residuals between independently produced overlapping catalogs indicate the the repeatability is 0.06 . examining the residuals between the catalogs and the astrometric reference catalogs ( either the usno or the sdss ) give an external astrometric accuracy of 0.2 , after allowing for the uncertainties in the astrometric reference catalogs themselves .",
    "the photometric accuracy is assessed by comparing catalogs from different pointings and measuring the systematic offset .",
    "the average offset was found to be 0.015 magnitudes .",
    "this only measures the repeatability ; the ultimate accuracy is limited by the photometric accuracy of the sdss and the uncertainty in converting from sdss to megacam magnitudes or the accuracy of the elixir calibration .",
    "analyses of both methods yield accuracies of 0.02 - 0.03 magnitudes .",
    "the limiting magnitude of the images is assessed in three ways : locating the peak of the number counts , locating the @xmath6 detection limit by finding the faintest object whose magnitude error is 0.198 mag or less , and finally by adding fake objects to a subsection of the image .",
    "this last method also assesses the surface brightness limit as well as the magnitude limit .",
    "the cfht data access policy is such that large chunks of data become public on a semi - annual basis .",
    "therefore , every six months , the grouping algorithm described in section [ sec : group ] is run to select new groups of images for stacking .",
    "( pi programs are processed on a case - by - case basis . )",
    "megapipe is run on the cadc processing grid , using sun grid engine as a scheduler .",
    "input images are retrieved from the megacam archive , processed and the output megapipe images are inserted back into the archive .",
    "manual intervention occurs at the beginning ( visually inspecting the input images ) and the end ( examining various diagnostics of the astrometric and photometric calibrations and spot checking of the images themselves ) .",
    "the processing itself is completely autonomous .",
    "the cadc processing cluster can run megapipe on six months worth of megacam images ( typically 300 groups ) in three days .",
    "the megapipe images and catalogs are distributed via the cadc website .",
    "the archive is searchable via a conventional query page as well as a graphical interface ( based on the google maps api ) described elsewhere in these proceedings .",
    "in addition an image cutout service is provided .",
    "based on observations obtained with megaprime / megacam , a joint project of cfht and cea / dapnia , at the canada - france - hawaii telescope ( cfht ) which is operated by the national research council ( nrc ) of canada , the institute national des sciences de lunivers of the centre national de la recherche scientifique of france , and the university of hawaii ."
  ],
  "abstract_text": [
    "<S> this paper describes the megapipe image processing pipeline at the canadian astronomical data centre ( cadc ) . </S>",
    "<S> the pipeline takes multiple images from the megacam mosaic camera on cfht and combines them into a single output image . </S>",
    "<S> megapipe takes as input detrended megacam images and does a careful astrometric and photometric calibration on them . </S>",
    "<S> the calibrated images are then resampled and combined into image stacks . </S>",
    "<S> megapipe is run on pi data by request , data from large surveys ( the cfht legacy survey and the next generation virgo survey ) and all non - proprietary megacam data in the cfht archive . </S>",
    "<S> the stacked images and catalogs derived from these images are available through the cadc website . </S>",
    "<S> currently , 1500 square degrees have been processed . </S>"
  ]
}