{
  "article_text": [
    "astronomical wide field ( hereafter wf ) imaging encompasses the use of images larger than @xmath0 pxls ( lipovestky 1993 ) and is the only tool to tackle problems based on rare objects or on statistically significant samples of optically selected objects . therefore ,",
    "wf imaging has been and still is of paramount relevance to almost all field of astrophysics : from the structure and dynamics of our galaxy , to the environmental effects on galaxy formation and evolution , to the large scale structure of the universe . in the past ,",
    "wf was the almost exclusive domain of schmidt telescopes equipped with large photographic plates and was the main source of targets for photometric and spectroscopic follow - up s at telescopes in the 4 meter class .",
    "nowadays , the exploitation of the new generation 8 meter class telescopes , which are designed to observe targets which are often too faint to be even detected on photographic material ( the poss - ii  detection limit in b  is @xmath1 @xmath2 , requires digitised surveys realized with large format ccd detectors mounted on dedicated telescopes .",
    "much effort has therefore been devoted worldwide to construct such facilities : the megacam project at the cfh , the eso  wide field imager at the 2.2 meter telescope , the sloan -  dss  and the eso - oac  vst ( mancini et al .",
    "1999 ) being only a few among the ongoing or planned experiments .",
    "one aspect which is never too often stressed is the humongous problem posed by the handling , processing and archiving of the data produced by these instruments : the vst alone , for instance , is expected to produce a flow of almost 30 gbyte of data per night or more than 10 tbyte per year of operation .",
    "the scientific exploitation of such a huge amount of data calls for new data reduction tools which must be reliable , must require a small amount of interactions with the operators and need to be as much independent on a priori choices as possible .    in processing a wf image ,",
    "the final goal is usually the construction of a catalogue containing as many as possible astrometric , geometric , morphological and photometric parameters for each individual object present on the image .",
    "the first step in any catalogue construction is therefore the detection of the objects , a step which , as soon as the quality of the images increases ( both in depth and in resolution ) , becomes much less obvious than what it may seem at first glance . the traditional definition of  object \" as a set of connected pixels having brightness higher than a given threshold , has in fact several well known pitfalls .",
    "for instance , low surface brightness galaxies very often escape recognition since i ) their central brightness is often comparable or fainter than the detection threshold , and ii ) their shape is clumpy , which implies that even though there may be several nearby pixels above the threshold , they can often be not connected and thus escape the assumed definition .    a similar problem is also encountered in the catalogues extracted from the _ hubble deep field _",
    "( hdf ) where a pletora of small  clumpy `` objects is detected but it is not clear whether each clump represents an individual object or rather is a fragment of a larger one .",
    "ferguson ( 1998 ) stresses some even stronger limitations of the traditional approach to object detection : i ) a comparison of catalogues obtained by different groups from the same raw material and using the same software shows that , near the detection limits the results are strongly dependent on the assumed definition of ' ' object `` ; ii ) object detection performed by the different groups is worse than what even an untrained astronomer can attain by visually inspecting an ] the theory ----------    in the ai domain there are dozens of different nn s used and optimised to perform the most various tasks . in the astronomical literature , instead , only two types of nn s are used : the ann , called in the ai literature multi - layer perceptron ( mlp ) with back - propagation learning algorithm , and the kohonen s self - organizing maps ( or their supervised generalization ) .",
    "we followed a rather complex approach which can be summarised as follows : principal component analysis ( pca ) nn s were used to reduce the dimensionality of the input space .",
    "supervised nn s need a large amount of labelled data to obtain a good classification while unsupervised nn s overcome this need , but do not provide good performances when classes are not well separated .",
    "hybrid and unsupervised hierarchical nn s are therefore very often introduced to simplify the expensive post - processing step of labelling the output neurons in classes ( such as objects / background ) , in the object detection process . in the following subsections",
    "we illustrate the properties of several types of nn s which were used in one or another of the various tasks .",
    "all the discussed models were implemented , trained and tested and the results of the best performing ones are illustrated in detail in the next sections .",
    "a pattern can be represented as a point in a @xmath3-dimensional parameter space . to simplify the computations",
    ", it is often needed a more compact description , where each pattern is described by @xmath4 , with @xmath5 , parameters .",
    "each @xmath3-dimensional vector can be written as a linear combination of @xmath3 orthonormal vectors or as a smaller number of orthonormal vectors plus a residual error .",
    "pca is used to select the orthonormal basis which minimizes the residual error .",
    "let @xmath6 be the @xmath3-dimensional zero mean input data vectors and @xmath7 be the covariance matrix of the input vectors @xmath6 .",
    "the @xmath8-th principal component of @xmath6 is defined as @xmath9 , where @xmath10 is the normalized eigenvector of @xmath11 corresponding to the @xmath8-th largest eigenvalue @xmath12 .",
    "the subspace spanned by the principal eigenvectors @xmath13 is called the pca subspace ( with dimensionality @xmath4 ; oja 1982 ; oja et al .",
    "1996 ) . in order to perform pca , in some cases and expecially in the non linear one , it is convenient to use nn s which can be implemented in various ways ( baldi & hornik 1989 ; jutten & herault 1991 ; oja 1982 ; oja , ogawa & wangviwattana 1991 ; plumbley 1993 ; sanger 1989 ) . the pca nn used by us was a feedforward neural network with only one layer which is able to extract the principal components of the stream of input vectors .",
    "2 summarises the structure of the pca nn s .",
    "as it can be seen , there is one input layer , and one forward layer of neurons which is totally connected to the inputs . during the learning phase",
    "there are feedback links among neurons , the topology of which classifies the network structure as either hierarchical or symmetric depending on the feedback connections of the output layer neurons .",
    "typically , hebbian type learning rules are used , based on the one unit learning algorithm originally proposed in ( oja 1982 ) .",
    "the adaptation step of the learning algorithm - in this case the network is composed by only one output neuron - is then written as :    @xmath14   \\label{eq2.1}\\ ] ]    where @xmath15 , @xmath16 and @xmath17 are , respectively , the value of the @xmath18-th input , of the @xmath18-th weight and of the network output at time @xmath19 , while @xmath20 is the learning rate .",
    "@xmath21 is the hebbian increment and eq.1 satisfies the condition :    @xmath22    many different versions and extensions of this basic algorithm have been proposed in recent years ( karhunen & joutsensalo 1994 = kj94 ; karhunen & joutsensalo 1995 = kj95 ; oja et al .",
    "1996 ; sanger 1989 ) .",
    "the extension from one to more output neurons and to the hierarchical case gives the well known generalized hebbian algorithm ( gha ) ( sanger 1989 ; kj95 ) :    @xmath23   \\label{eq2.3}\\ ] ]    while the extension to the symmetric case gives the oja s subspace network ( oja 1982 ) :    @xmath24   \\label{eq2.4}\\ ] ]    in both cases the weight vectors must be orthonormalized and the algorithm stops when :    @xmath25    where @xmath26 is an arbitrarily choosen small value .",
    "after the learning phase , the network becomes purely feedforward .",
    "kj94 and kj95 proved that pca neural algorithms can be derived from optimization problems , such as variance maximization and representation error minimization .",
    "they generalized these problems to nonlinear problems , deriving nonlinear algorithms ( and the relative networks ) having the same structure of the linear ones : either hierarchical or symmetric .",
    "these learning algorithms can be further classified in robust pca algorithms and nonlinear pca algorithms . kj95",
    "defined robust pca as those in which the objective function grows slower than a quadratic one .",
    "the non linear learning function appears at selected places only .",
    "in nonlinear pca algorithms all the outputs of the neurons are nonlinear function of the responses .",
    "more precisely , in the robust generalization of variance maximization , the objective function @xmath27 is assumed to be a valid cost function such as @xmath28 or @xmath29 .",
    "this leads to the adaptation step of the learning algorithm :    @xmath30    where :    @xmath31    @xmath32    in the hierarchical case @xmath33 . in the symmetric case @xmath34 ,",
    "the error vector @xmath35 becomes the same @xmath36 for all the neurons , and eq .",
    "[ eq2.5 ] can be compactly written as :    @xmath37    where @xmath38 is the instantaneous vector of neuron responses at time @xmath19 .",
    "the learning function @xmath39 , derivative of @xmath40 , is applied separately to each component of the argument vector .",
    "the robust generalisation of the representation error problem ( kj95 ) with @xmath41 , leads to the stochastic gradient algorithm :    @xmath42    this algorithm can be again considered in both the hierarchical and symmetric cases . in the symmetric case @xmath34 , the error vector is the same @xmath36 for all the weights @xmath43 . in the hierarchical case @xmath33 , eq .",
    "[ eq2.7 ] gives the robust counterparts of principal eigenvectors @xmath10 .    in eq .",
    "[ eq2.7 ] the first update term    @xmath44    is proportional to the same vector @xmath45 for all weights @xmath16 .",
    "furthermore , we can assume that the error vector @xmath46 is relatively small after the initial convergence .",
    "hence , we can neglect the first term in eq .",
    "[ eq2.7 ] and this leads to :    @xmath47    let us consider now the nonlinear extensions of pca algorithms which can be obtained in a heuristic way by requiring all neuron outputs to be always nonlinear in eq .",
    "[ eq2.5 ] , then :    @xmath48    where :    @xmath49    in previous experiments ( tagliaferri et al . 1999 , tagliaferri et al . 1998 ) we found that the hierarchical robust nn of eq .",
    "[ eq2.5 ] with learning function @xmath50 achieves the best performance with respect to all the other mentioned pca nn s and linear pca .",
    "unsupervised nn s partition the input space into clusters and assign to each neuron a weight vector which univocally individuates the template characteristic of one cluster in the input feature space .",
    "after the learning phase , all the input patterns are classified .",
    "kohonen ( 1982 , 1988 ) self organizing maps ( som ) are composed by one neuron layer structured in a rectangular grid of @xmath51 neurons .",
    "when a pattern @xmath6 is presented to the nn , each neuron @xmath8 receives the input and computes the distance @xmath52 between its weight vector @xmath53 and @xmath6 .",
    "the neuron which has the minimum @xmath52 is the winner .",
    "the adaptation step consists in modifying the weights of the neurons in the following way :    @xmath54    where @xmath55 is the learning rate ( @xmath56 ) decreasing in time , @xmath57 is the distance in the grid between the @xmath18 and the @xmath58 neurons and @xmath59 is a unimodal function with variance @xmath60 decreasing with @xmath6 .",
    "the neural - gas nn is composed by a linear layer of neurons and a modified learning algorithm ( martinetz berkovitch & shulten 1993 ) .",
    "it classifies the neurons in an ordered list @xmath61 accordingly to their distance from the input pattern .",
    "the weight adaptation depends on the position @xmath62 of the @xmath18-th neuron in the list :    @xmath63    and works better than the preceding one : in fact , it is quicker and reaches a lower average distortion value be the pattern probability distribution over the set @xmath64 and let @xmath65 be the weight vector of the neuron which classifies the pattern @xmath6 .",
    "the average distortion is defined as @xmath66 .",
    "the growing cell structure ( gcs ) ( fritzke 1994 ) is a nn which is capable to change its structure depending on the data set .",
    "aim of the net is to map the input pattern space into a two - dimensional discrete structure @xmath67 in such a way that similar patterns are represented by topological neighboring elements .",
    "the structure @xmath67 is a two - dimensional simplex where the vertices are the neurons and the edges attain the topological information .",
    "every modification of the net always maintains the simplex properties .",
    "the learning algorithm starts with a simple three node simplex and tries to obtain an optimal network by a controlled growing process : _ i d est _ , for each pattern @xmath6 of the training set , the winner @xmath58 and the neighbors weights are adapted as follows :    @xmath68    @xmath69 connected to @xmath58 ; where @xmath70 and @xmath71 are constants which determine the adaptation strength for the winner and for the neighbors , respectively .",
    "the insertion of a new node is made after a fixed number @xmath72 of adaptation steps .",
    "the new neuron is inserted between the most frequent winner neuron and the more distant of its topological neighbors .",
    "the algorithm stops when the network reaches a pre - defined number of elements .",
    "the on - line k - means clustering algorithm ( lloyd 1982 ) is a simpler algorithm which applies the gradient descent ( = gd ) directly to the average distortion function as follows :    @xmath73    the main limitation of this technique is that the error function presents many local minima which stop the learning before reaching the optimal configuration .",
    "finally , the maximum entropy nn ( rose , gurewitz & fox , 1990 ) applies the gd to the error function to obtain the adaptation step :    @xmath74    where @xmath75 is the inverse temperature and takes value increasing in time and @xmath76 is the distance between the @xmath18-th and the winner neurons .",
    "hybrid nn s are composed by a clustering algorithm which makes use of the information derived by one unsupervised single layer nn . after the learning phase of the nn , the clustering algorithm splits the output neurons in a number of subsets which is equal to the number of the desired output classes . since the aim is to put similar input patterns in the same class and dissimilar input patterns in different classes , a good strategy consists in applying a clustering algorithm directly to the weight vectors of the unsupervised nn .    a non - neural agglomeration clustering algorithm that divides the pattern set ( in this case the weights of the neurons ) @xmath77 in @xmath78 clusters ( with @xmath79 )",
    "can be briefly summarized as follows :    1 .",
    "it initially divides @xmath80 in @xmath51 clusters @xmath81 such that @xmath82 ; 2 .",
    "then it computes the distance matrix @xmath83 with elements @xmath84 ; 3 .",
    "then it finds the smallest element @xmath85 and unifies the clusters @xmath86 and @xmath87 in a new one @xmath88 ; 4 .",
    "if the number of clusters is greater than @xmath78 then it goes to step 2 else , it finally stops .",
    "many algorithms quoted in literature ( everitt 1977 ) differ only in the way in which the distance function is computed .",
    "for example :    @xmath89 ( nearest neighbor algorithm ) ;    @xmath90 ( centroid method ) ;    @xmath91 ( average between groups ) .",
    "the output of the clustering algorithm will be a labelling of the patterns ( in this case neurons ) in @xmath78 different classes .",
    "unsupervised hierarchical nn s add one or more unsupervised single layers nn to any unsupervised nn , instead of a clustering algorithm as it happens in hybrid nn s .    in this way , the second layer nn learns from the weights of the first layer nn and clusters the neurons on the basis of a similarity measure or a distance .",
    "the iteration of this process to a few layers gives the unsupervised hierarchical nn s .",
    "the number of neurons at each layer decreases from the first to the output layer and , as a consequence , the nn takes the pyramidal aspect shown in fig .",
    "the nn takes as input a pattern @xmath6 and then the first layer finds the winner neuron .",
    "the second layer takes the first layer winner weight vector as input and finds the second layer winner neuron and so on up to the top layer .",
    "the activation value of the output layer neurons is 1 for the winner unit and 0 for all the others . in short : the learning steps of a @xmath92 layer hierarchical nn with training set @xmath93 are the following :    * the first layer is trained on the patterns of @xmath93 with one of the learning algorithms for unsupervised nn s ; * the second layer is trained on the elements of the set @xmath94 which is composed by the weight vectors of the first layer winner units ; * the process is iterated to the @xmath95 layer nn ( @xmath96 ) on the training set which is composed by the weight vectors of the winner neurons of the @xmath97 layer when presenting @xmath93 to the first layer nn , @xmath94 to the second layer and so on .    by varying the learning algorithms we obtain different nn s with different properties and abilities . for instance , by using only som s we have a multi - layer som ( ml - som ) ( koh j. , suk & bhandarkar 1995 ) where every layer is a two - dimensional grid .",
    "we can easily obtain ( tagliaferri , capuano & gargiulo 1999 ) _ ml - neuralgas _ , _ ml - maximum entropy _ or _ ml - k means _ organized on a hierarchy of linear layers .",
    "the ml - gcs has a more complex architecture and has at least 3 units for layer .    by varying the learning algorithms in the different layers",
    ", we can take advantage from the properties of each model ( for instance , since we can not have a ml - gcs with 2 output units we can use another nn in the output layer ) .",
    "a hierarchical nn with a number of output layer neurons equal to the number of the output classes simplifies the expensive post - processing step of labelling the output neurons in classes , without reducing the generalization capacity of the nn .",
    "a _ multi - layer perceptron _ ( mlp ) is a layered nn composed by :    * one input layer of neurons which transmit the input patterns to the first hidden layer ; * one or more hidden layers with units computing a nonlinear function of their inputs ; * and one output layer with elements calculating a linear or a nonlinear function of their inputs .",
    "aim of the network is to minimize an error function which generally is the sum of squares of the difference between the desired output ( target ) and the output of the nn .",
    "the learning algorithm is called back - propagation since the error is back - propagated in the previous layers of the nn in order to change the weights . in formulae , let @xmath98 be an @xmath99dimensional input vector with corresponding target output @xmath100 .",
    "the error function is defined as follows :    @xmath101    where @xmath102 is the output of the @xmath103 output neuron .",
    "the learning algorithm updates the weights by using the gradient descent ( gd ) of the error function with respect to the weights .",
    "if we define the input and the output of the neuron @xmath18 respectively as :    @xmath104 and @xmath105    where @xmath106 is the connection weight from the neuron @xmath8 to the neuron @xmath18 , and @xmath107 is linear or sigmoidal for the output nodes and sigmoidal for the hidden nodes .",
    "it is well known in literature ( bishop 1995 ) that these facts lead to the following adaptation steps :    @xmath108    and    @xmath109    for the output and hidden units , respectively .",
    "the value of the learning rate @xmath110 is small and causes a slow convergence of the algorithm .",
    "a simple technique often used to improve it is to sum a momentum term to eq .",
    "[ eq2.15 ] which becomes :    @xmath111    this technique generally leads to a significant improvement in the performances of gd algorithms but it introduces a new parameter @xmath20 which has to be empirically chosen and tuned .    bishop ( 1995 ) and press et al .",
    "( 1993 ) summarize several methods to overcome the problems related to the local minima and to the slow time convergence of the above algorithm . in a preliminary step of our experiments , we tried all the algorithms discussed in chapter 7 of bishop ( 1995 ) finding that a hybrid algorithm based on the scaled conjugate gradient for the first steps and on the newton method for the next ones , gives the best results with respect to both computing time and relative number of errors . in this paper",
    "we used it in the mlp s experiments .",
    "in this work we use a 2000x2000 arcsec@xmath112 area centered on the north galactic pole extracted from the slightly compressed poss - ii f plate n. 443 , available via network at the canadian astronomy data center ( http://cadcwww.dao.nrc.ca ) .",
    "poss - ii data were linearised using the sensitometric spots recorded on the plate .",
    "the seeing fwhm  of our data was 3 arcsec .",
    "the same area has been widely studied by others and , in particular , by infante & pritchet ( 1992 , = ip92 ) and infante , pritchet & hertling ( 1995 ) who used deep observations obtained at the 3.6 m cfht telescope in the @xmath113 photographic band under good seeing conditions ( fwhm @xmath114 arcsec ) to derive a catalogue of objects complete down to @xmath115 , i d est , much deeper than the completeness limit of our plate . their catalogue is therefore based on data of much better quality and accuracy than ours , and it was for the availability of such good template that we decided to use this region for our experiments .",
    "we also studied a second region in the coma cluster ( which happens to be in the same n. 443 plate ) but since none of the catalogues available in literature is much better than our data , we were forced to neglect it in most of the following discussion .",
    "the characteristics of the selected region , a relatively empty one , slightly penalise our nn detection algorithms which can easily recognise objects of quite different sizes . on the contrary of what happens to other algorithms",
    "next works well even on areas where both very large and very small objects are present such as , for instance , the centers of nearby clusters of galaxies as our preliminary test on a portion of the coma clusters clearly shows ( tagliaferri et al .",
    "1998 ) .",
    "the detection and classification of the objects are a multi - step task :    \\1 ) first of all , following a widely used ai approach , we mathematically transform the detection task into a classification one by compressing the redundant information contained in nearby pixels by means of a non  linear pca nn s .",
    "principal vectors of the pca are computed by the nn on a portion of the whole image .",
    "the values of the pixels in the transformed @xmath4 dimensional eigen - space obtained via the principal vectors of the pca nn are then used as inputs to unsupervised nn s to classify pixels in few classes .",
    "we wish to stress that , in this step , we are still classifying pixels , and not objects .",
    "the adopted nn is unsupervised , i.e. we never feed into the detection algorithm any a priori definition of what an object is , and we leave it free to find its own object definition .",
    "it turns out that image pixels are split in few classes , one coincident with what astronomers call background and some others for the objects ( in the astronomical sense ) .",
    "afterwords , the class containing the background pixels is kept separated from the other classes which are instead merged together . therefore , as final output , the pixels in the image are divided in  object \" or  background \" .",
    "\\2 ) since objects are seldom isolated in the sky , we need a method to recognise overlapping objects and deblend them .",
    "we adopt a generalisation of the method used by focas ( jarvis & tyson 1981 ) .",
    "\\3 ) due to the noise , object edges are quite irregular .",
    "we therefore apply a contour regularisation to the edges of the objects in order to improve the following star / galaxy classification step .",
    "\\4 ) we define and measure the features used , or suitable , for the star / galaxy classification , then we choose the best performing features for the classification step , through the sequential backward elimination strategy ( bishop 1995 ) .",
    "\\5 ) we then use a subset of the ip92 catalog to learn , validate and test the classification performed by next on our images .",
    "the training set was used to train the nn , while the validation was used for model selection , i.e. to select the most performing parameters using an independent data set . as template classifier , we used sex , whose classifier is also based on nns .    the detection and classification performances of our algorithm",
    "were then compared with those of traditional algorithms , such as sex .",
    "we wish to stress that in both the detection and classification phases , we were not interested in knowing how well next can reproduce sex or the astronomer s eye performances , but rather to see whether the sex and next catalogs are or are not similar to the  true \" , represented in our case by the ip92 catalog .    finally , we would like to stress that in statistical pattern recognition , one of the main problems in evaluating the system performances is the optimisation of all the compared systems in order not to give any unfair advantage to one of the systems with respect to the others ( just because it is better optimised than the others ) .",
    "for instance , since the background subtraction is crucial to the detection , all algorithms , including sex , were run on the same background subtracted image .      from the astronomical point of view",
    ", segmentation allows to disentangle objects from noisy background . from a mathematical point of view , instead , the segmentation of an image @xmath113 consists in splitting it into disconnected homogeneous ( accordingly to a uniformity predicate @xmath116 ) regions @xmath117 , in such a way that their union is not homogeneous :    @xmath118    where @xmath119 @xmath120 and @xmath121 when @xmath122 is adjacent to @xmath123 . the two regions are adjacent when they share a boundary , i.e. when they are neighbours .",
    "a segmentation problem can be easily transformed into a classification one if classes are defined on pixels and @xmath116 is written in such a way that @xmath119 if and only if all the pixels of @xmath122 belong to the same class .",
    "for instance , the segmentation of an astronomical image in background and objects leads to assign each pixel to one of the two classes . among the various methods discussed in the literature ,",
    "unsupervised nn s usually provide better performance than any other nn type on noisy data ( pal & pal 1993 ) and have the great advantage of not requiring a definition ( or exhaustive examples ) of object .",
    "the first step of the segmentation process consists in creating a numerical mask where different values discriminate the background from the object ( fig .",
    "5 ) .    in well sampled images ,",
    "the attribution of a pixel to either the background or to the object classes depends on both the pixel value and on the properties of its neighbours : for instance , a `` bright '' isolated pixel in a `` dark '' environment is usually just noise .",
    "therefore , in order to classify a pixel , we need to take into account the properties of all the pixels in a @xmath124 window centered on it .",
    "this approach can be easily extended to the case of multiband images .",
    "@xmath125 , however , is a too high dimensionality to be effectively handled ( in terms of learning and computing time ) by any classification algorithm .",
    "therefore , in order to lower the dimensionality , we first use a pca to identify the @xmath4 ( with @xmath126 ) most significant features . in detail :    \\i ) we first run the @xmath127 window on a sub - image containing representative parts of the image .",
    "we used both a @xmath128 and a @xmath129 windows .",
    "\\ii ) then we train the pca nn s on these patterns .",
    "the result is a projection matrix @xmath80 with dimensionality @xmath130 , which allows us to reduce the input feature number from @xmath131 to @xmath4 .",
    "we considered only the first three components since , accordingly to the pca , they contain almost @xmath132  of the information while the remaining @xmath133  is distributed over all the others .",
    "\\iii ) the @xmath4-dimensional projected vector @xmath134 is the input of a second nn which classifies the pixels in the various classes .",
    "\\iv ) finally , we merge all classes except the background one in order to reduce the classification problem to the usual  object / background  dichotomy .    much attention has also to be paid to the choice of the type of pca .",
    "after several experiments , we found that - for our specific task which is characterised by a large dynamical range in the luminosities of the objects ( or , which is the same , in the pixel values ) - pca s can be split into two gross groups : pca s with linear input - output mapping ( hereafter linear pca nn s ) and pca s with non linear input - output mapping ( non - linear pca nn s ) ( see section [ section3.1 ] ) .",
    "linear pca nn s turned out to misclassify faint objects as background .",
    "non - linear pca nn s based on a sigmoidal function allowed , instead , the detection of faint sources .",
    "this can be better understood from fig .",
    "6 and 7 which give the distributions of the training points in the simpler case of two dimensional inputs for the two types of pca nn s .",
    "linear pca nn s produce distributions with a very dense core ( background and faint objects ) and only a few points ( luminous objects ) spread over a wide area .",
    "such a behaviour results from the presence of very luminous objects in the training set which compress the faint ones to the bottom of the scale .",
    "this problem can be circumvented by avoiding very luminous objects in the training set , but this would make the whole procedure too much dependent on the choice of the training set .",
    "non - linear pca nn s , instead , produce better sampled distributions and a better contrast between background and faint objects .",
    "the sigmoidal function compresses the dynamical range squeezing the very luminous objects into a narrow region ( see fig .",
    "7 ) .    among all ,",
    "the best performing nn ( tagliaferri et al .",
    "1998 ) turned out to be the hierarchical robust pca nn with learning function @xmath135 given in eq .",
    "[ eq2.5 ] .",
    "this nn was also the faster among the other non - linear pca  nn s .",
    "the principal components matrices are detailed in the tables 1 - 3 and 4 - 6 for the @xmath136 and @xmath137 cases , respectively . in tables 13 ,",
    "numbers are rounded to the closest integere since they differ from an integer only at the 7-th decimal figure .",
    "not surprisingly , the first component turns out to be the mean in the @xmath138 case .",
    "the other two matrices can be seen as anti - symmetric filters with respect to the centre .",
    "the convolution of these filters ( see fig .",
    "8) with the input image gives images where the objects are the regions of high contrast .",
    "similar results are obtained for the @xmath137 case .    at this stage",
    "we have the principal vectors and , for each pixel , we can compute the values of the projection of each pixel in the eigenvector space .",
    "the second step of the segmentation process consists in using unsupervised nn s to classify the pixels into few classes , having as input the reduced input patterns which have been just computed .",
    "supervised nn would require a training set specifying , for each pixel , whether that pixel belongs to an object or to the background .",
    "we no longer consider such a possibility , due to the arbitrariness of such a choice at low fluxes , the lack of elegance of the method and the problems which are encountered in the labelling phase .",
    "unsupervised nn s are therefore necessary .",
    "we considered several types of nn s .",
    "as already mentioned several times , our final goal is to classify the image pixels in just two classes : objects and background , which should correspond to two output neurons .",
    "this simple model , however , seldom suffice to reproduce real data in the bidimensional case ( but similar results are obtained also for the 3-d or multi - d cases ) , since any unsupervised algorithm fails to produce spatially well separated clusters and more classes are needed .",
    "a trial and error procedure shows that a good choice of classes is @xmath139 : fewer classes produce poor classifications while more classes produce noisy ones . in all cases , only one class ( containing the lowest luminosity pixels )",
    "represents the background , while the other classes represent different regions in the objects images .",
    "we compared hierarchical , hybrid and unsupervised nn s with @xmath139 output neurons . from theoretical considerations and from preliminary work ( tagliaferri et .",
    "al 1998 ) we decided to consider only the best performing nn s , i d est neural gas , ml - neural gas , ml - som , and gcs+ml - neural gas . for a more quantitative and detailed discussion",
    "see section [ section3.5 ] , where the performances of these nn s are evaluated .    after this stage",
    "all pixels are classified in one of six classes .",
    "we merge together all classes , with the exception of the background one and reduce the classification to the usual astronomical dichotomy : object or background .",
    "finally , we create the masks , each one identifying one structure composed by one or more objects .",
    "this task is accomplished by a simple algorithm , which , while scanning the image row by row , when it finds one or more adjacent pixels belonging to the object class expands the structure including all equally labelled pixels adjacent to them .",
    "once objects have been identified we measure a first set of parameters .",
    "namely : the photometric barycenter of the objects computed as :    @xmath140    where @xmath141 is the set of pixels assigned to the object in the mask , @xmath142 is the intensity of the pixel @xmath143 , and    @xmath144 is the flux of the object integrated over the considered area . the semimajor axis of the object contour defined as : @xmath145    with position angle defined as:@xmath146    where @xmath147 is the most distant pixel from the barycenter belonging to the object .",
    "the semiminor axis of the faintest isophote is given by : @xmath148 \\right| \\cdot r(x , y)\\ ] ]    these parameters are needed in order to disentangle overlapping objects .",
    "our method recognises multiple objects by the presence of multiple peaks in the light distribution .",
    "search for double peaks is performed along directions at position angles @xmath149 with @xmath150 . at difference with focas , ( jarvis & tyson 1981 )",
    ", we sample several position angles because not always objects are aligned along the major axis of their light distribution , as focas implicitly assumes . in our experiments the maximum @xmath151 was set to @xmath152 .",
    "when a double peak is found , the object is split into two components by cutting it perpendicularly to the line joining the two peaks .",
    "spurious peaks can also be produced by noise fluctuations , a case which is very common in photographic plates near saturated objects .",
    "a good way to minimise such noise effects is , just for deblending purposes , to reduce the dynamical range of the pixels values , by rounding the intensity ( or pixel values ) in @xmath153 equi - espaced levels .",
    "multiple ( i.e. @xmath154 or more components ) objects pose a more complex problem . in the case shown in fig .",
    "9 , the segmentation mask includes three partially overlapping sources .",
    "the search for double peaks produces a first split of the mask into two components which separate the third and faintest component into two fragments .",
    "subsequent iterations would usually produce a set of four independent components therefore introducing a spurious detection . in order to solve the problem posed by multiple \" non spurious  objects erroneously split",
    ", a recomposition loop needs to be run .",
    "most celestial objects - does not matter whether resolved or unresolved - present a light distribution rapidly decreasing outwards from the centre .",
    "if an object has been erroneously split into several components , then the adjacent pixels on the corresponding sides of the two masks will have very different values .",
    "the implemented algorithm checks each component ( starting from the one with the highest average luminosity and proceeding to the fainter ones ) against the others .",
    "let us now consider two parts of an erroneously split object .",
    "when the edge pixels have luminosity higher than the average luminosity of the faintest component , the two parts are recomposed .",
    "this procedure also takes care of all spurious components produced by the haloes of bright objects ( an artifact which is a major shortcoming of many packages available in the astronomical community ) .      the last operation before measuring the objects parameters consists in the regularization of the contours since  due to noise , overlapping images , image defects , etc .",
    " segmentation produces patterns that are not similar to the original celestial objects that they must represent . for the contour regularisation ,",
    "we threshold the image at several sigma over the background and we then expand the ellipse describing the objects in order to include the whole area measured in the object detection .      after the above described steps , it becomes possible to measure and compare the performances of the various nn models .",
    "we implemented and compared : neural gas ( ng3 ) , ml - neural gas ( mlng3 or mlng5 ) , ml - som ( k5 ) , gcs+ml - neural gas ( ngcs5 ) . the last digit in the nn name indicating the dimensions of the running window .",
    "attention was paid in choosing the training set , which needed to be at the same time small but significant . by trial and error",
    ", we found that for pca nn s and unsupervised nn s it was enough to choose @xmath155 sub - images , each one @xmath156 pixels wide and not containing very large objects .",
    "as all the experienced users know , the choice of the sex parameters ( minimum area , threshold in units of the background noise , and deblending parameter ) is not critical and the default values were choosen ( 4 pixel area , @xmath157 ) .",
    "table 7 shows the number of objects detected by the five nn s and sex .",
    "it has to be stressed that @xmath158 objects out of the 4819 available in the ip92 reference catalogue are beyond the detection limit of our plate material .",
    "sex detects a larger number of objects but many of them ( see table 7 ) are spurious .",
    "nn s detect a slightly smaller number of objects but most of them are real .",
    "in particular : mng5 looses , with respect to sex , only 79 real objects but detects 400 spurious objects less ; mng3 is a little less performing in detecting true objects but is even cleaner of spurious detections .",
    "the upper panel of fig .",
    "10 shows the number of \" true  objects ( i.e. objects in the ip92 catalogue ) .",
    "most of them are fainter than @xmath159 mag , i d est they are fainter than the plate limit .",
    "the lower panel shows instead the number of objects detected by the various nn s relative to sex .",
    "the curves largely coincide and , in particular , mlng5 and sex do not statistically differ in any magnitude bin while mlng3 slightly differs only in the faintest bin ( @xmath159 ) .",
    "the class of `` missed '' objects ( i d est objects which are listed in the reference catalogue but are not in the nn s or sex catalogues ) needs a detailed discussion .",
    "we focus first on brighter objects .",
    "they can be divided in :     few `` true '' objects with a nearby companion which are blended in our image but are resolved in ip92 .",
    " parts of isolated single large objects incorrectly split by ip92 . a few cases .",
    " a few detections aligned in the e - w direction on the two sides of the images of a bright star .",
    "they are likely false objects ( diffraction spikes detected as individual objects in the ip92 catalog ) .",
    " objects in ip92 which correspond to empty regions in our images : they can be missing because variable , fast moving , or with an overestimated luminosity in the reference catalog ; they can also be missed because spurious in the template catalog .",
    "therefore , a fair fraction of the `` missed '' objects is truly non existent and the performances of our detection tools are lower bounded at @xmath160 mag .",
    "we wish to stress here that even though there is nothing like a perfect catalogue , the ip92 template is among the best ones ever produced to our knowledge .",
    "the upper panel of fig .",
    "11 is the same as in fig .",
    "the lower panel shows instead the fraction of \" false  objects , i d est of the objects detected by the algorithms but not present in the reference catalogue .",
    "ip92 were interested to faint objects and masked out the bright ones , therefore their catalogue may exclude a few `` true '' objects ( in particular at @xmath161 ) .",
    "we believe that all objects brighter than @xmath162 mag are really `` true '' since they are detected both by sex and nn s with high significance . for objects brighter than @xmath162 mag ,",
    "the nn s and sex have similar performances .",
    "they differ only at fainter magnitudes .",
    "the catalogue with the largest contamination by `` false '' objects is sex , followed by mlng5 , mlng3 and the other nn s beeing much less contaminated .",
    "mlng5 is quite efficient in detecting `` true '' objects and has a @xmath163 cleaner detection rate in the highly populous bin @xmath164 mag .",
    "mlng3 is less efficient than mlng5 in detecting `` true '' objects but it is even cleaner than mlng5 of false detections .",
    "let us now consider whether or not the detection efficiency depends on the degree of concentration of the light ( stars have steeper central gradients than galaxies ) . in ip92 objects",
    "are classified in two major classes , star & galaxies , and a few minor ones ( merged , noise , spike , defects , etc . ) that we neglect .",
    "the efficiency of the detection is shown in fig .",
    "12 for three representative detection algorithms : mlng5 , k5 , and sex . at @xmath165 mag , the detection efficiency is large , close to 1 and independent on the central concentration of the light",
    ". please note that there are no objects in the image having @xmath166 mag and that in the first bin there are only 4 galaxies . at fainter magnitudes ( @xmath167 mag )",
    "detection efficiencies differ as a function of both the algorithm and of the light concentration .",
    "in fact , sex , mlng5 , and to less extent k5 , turn out to be more efficient in detecting galaxies rather than stars ( in other words : `` missed '' objects are preferentially stars ) . for sex , a possible explanation is that a minimal area above the background is required in order for the object to be detected and at @xmath168 mag and noise fluctuations can affect the isophotal area of unresolved objects bringing it below the assumed threshold ( 4 pixels ) .",
    "this bias is minimum for the k5 nn . however , this is more likely due to the fact that k5 misses more galaxies than the other algorithms , rather than to the fact that it detects more stars .    in conclusion :",
    "mlng3 and mlng5 turn out to have high performances in detecting objects : they produce catalogs which are cleaner of false detections at the price of a slightly larger uncompleteness than the sex catalogues below the plate completness magnitude .",
    "we also want to stress that since the less performing nn s produce catalogs which are much cleaner of false detections , the selected objects are in large part  true  , and not just noise fluctuations .",
    "these nn s can therefore be very suitable to select candidates for possible follow  up detailed studies at magnitudes where many of the objects detected by sex would be spurious .",
    "deeper catalogs having a large number of spurious source , such as those produced by sex or other packages are instead preferable if , for instance , they can be cleaned by subsequent processing ( for instance by matching the detected objects with other catalogs ) .    a posteriori , one could argue that performances similar to those of each of the nn s could be achieved by running sex with appropriate settings .",
    "however , it would be unfair ( and methodologically wrong ) to make a fine tuning of any of the detection algorithms using an a - posteriori knowledge .",
    "it would also make cumbersome the authomatic processing of the images which is the final goal of our procedure .      in this section",
    "we discuss the feature extraction and selection of the features which are useful for the star / galaxy classification .",
    "features are chosen from the literature ( jarvis & tyson 1981 ; miller & coe 1996 ; odewahn et al .",
    "1992 ( = o92 ) , godwin & peach 1977 ) , and then selected by a sequential forward selection process ( bishop 1995 ) , in order to extract the most performing ones for classification purposes .",
    "the first five features are those defined in the previous section and describing the ellipses circumscribing the objects : the photometric barycenter coordinates ( @xmath169 ) , the semimajor axis ( @xmath170 ) , the semiminor axis ( @xmath171 ) . and the position angle ( @xmath172 ) . the sixth one is the object area , @xmath141 , i.e. the number of pixels forming the object .",
    "the next twelve features have been inspired to the pioneeristic work by o92 : the object diameter ( @xmath173 ) , the ellipticity ( @xmath174 ) , the average surface brightness ( @xmath175 ) , the central intensity ( @xmath176 ) , the filling factor ( @xmath177 ) , the area logarithm ( @xmath178 ) , the harmonic radius ( @xmath179 ) .",
    "the latter beeing defined as : @xmath180    and five gradients @xmath181 , @xmath182 , @xmath183 , @xmath184 and @xmath185 defined as : @xmath186    where @xmath187 is the average surface brightness within an ellipse , with position angle @xmath172 , semimajor axis @xmath188 , @xmath189 . and ellipticity @xmath190 .",
    "two more features are added following miller & coe ( 1996 ) : the ratios @xmath191 and @xmath192 .",
    "finally , five focas features ( jarvis & tyson 1981 ) have been included : the second ( @xmath193 ) and the fourth ( @xmath194 ) total moments defined as : @xmath195    where @xmath196 are the object central momenta computed as : @xmath197    the average ellipticity : @xmath198    the central intensity averaged in a @xmath199 @xmath200 and , finally , the  kron  radius defined as : @xmath201    for each object we therefore measure @xmath202 features , where the first @xmath139 are reported only to easy the graphical representation of the objects and have a low discriminating power .",
    "the complete set of the extracted features is given in table 8 .",
    "our list of features includes therefore most of those usually used in the astronomical literature for the star / galaxy classification .",
    "are all these features truly needed ? and , if this is not and a smaller subset contains all the needed information , what are the most useful ones ?",
    "we tried to answer these questions by evaluating the classification performance of each set of features through the a - priori knowledge of the true classification of each object , as it is listed in a much deeper and higher quality reference catalog .",
    "most of the defined features are not independent .",
    "the presence of redundant features decreases the classification performances since any algorithm would try to minimise the error with respect to features which are not particularly relevant for the task .",
    "furthermore , by introducing useless features the computational speed would be lowered .",
    "the feature selection phase was realised through the sequential backward elimination strategy ( bishop 1995 ) , which works as follows : let us suppose to have @xmath4 features in one set and to run the classification phase with this set .",
    "then , we build @xmath4 different sets with @xmath203 features in each one and then we run the classification phase for each set , keeping the set which attains the best classification .",
    "this procedure allows us to eliminate the less significant feature .",
    "then , we repeat @xmath203 times the procedure eliminating one feature at each step . in order to further reduce the computation time we do not use the validation set and the classification error is evaluated directly on the test set .",
    "it has to be stressed that this procedure is common in the statistical pattern recognition literature where , very often , for this task are also introduced simplified models .",
    "this however could be avoided in our case due to the speed and good performances of our nn s    unsupervised nn s were not successful in this task , because the input data feature space is not separated into two not overlapping classes ( or , in simpler terms , the images and therefore the parameters of stars and galaxies fainter than the completeness limit of the image are quite similar ) , and they reach a performance much lower than supervised nn s .    supervised learning nn s give far better results .",
    "we used a mlp with one hidden layer of @xmath204 neurons and only one output , assuming value @xmath205 for star and value @xmath206 for galaxy .",
    "after the training , we calculate the nn output as @xmath206 if it is greater than @xmath207 and @xmath205 otherwise for each pattern of the test set .",
    "the experiments produce a series of catalogues , one for each set of features .",
    "13 shows the classification performances as a function of the adopted features .",
    "after the first step , the classification error remains almost constant up to @xmath208 , i d est up to the point where features which are important for the classification are removed .    a high performance can be reached using just 6 features . with a lower number of features the classification",
    "worsen , whereas a larger number of features is unjustified , because it does not increase the performances of the system .",
    "the best performing set of features consists of features 11 , 12 , 14 , 19 , 21 , 25 of table 8 .",
    "they are two radii , two gradients , the second total moment and a ratio which involves measures of intensity and area .",
    "let us discuss now how the star / galaxy classification takes place .",
    "the first step is accomplished by  teaching \" the mlp nn using the selected best features . in this case",
    "we divided the data set into three independent data sets : training , validation and test sets .",
    "the learning optimization is performed using the training set while the early stopping technique ( bishop 1995 ) is used on the validation set to stop the learning to avoid overfitting .",
    "finally , we run the mlp nn on the test set .    as comparison classifier",
    ", we adopt sex , which is based on a mlp nn .",
    "as features useful for the classification , sex uses eight isophotal areas and the peak intensity plus a parameter , the fwhm of stars .",
    "since the sex nn training was already realised by bertin & arnouts ( 1996 ) on @xmath209 simulated images of stars and galaxies , we limit ourselves to tune sex in order to obtain the best performances on the validation set . both sex and our system use nn s for the classification , but they follow two different , alternative approaches : sex uses a very large training set of simulated stars and galaxies , our system uses noisy , real data . furthermore , while the features of sex are fixed by the authors , and the nn s output is a number @xmath6 , @xmath210 ; our system selects the best performing ones and its output is an integer : 0 or 1 ( i d est star or galaxy ) .",
    "therefore , we use the validation set for choosing the threshold which maximises the number of correct classifications by sex ( see fig .",
    "the experimental results are shown in fig .",
    "15 where the errors are plotted as a function of the magnitude . at all magnitudes next misclassify less objects than sex . out of 460 objects",
    ", sex makes 41 incorrect classifications , while next just 28 .    in order to check the",
    "that our feature selection is optimal , we also compared our classification with those obtained using our mlp nn s with others feature sets , selected as shown in table 9 .",
    "the total number of misclassified objects in the test set of 460 elements were : o - f , 43 errors ; o - l , 30 errors ; o - s , 35 errors ; gp1 , 48 errors ; gp2 , 49 errors .",
    "16 shows the classification performances of the considered feature sets as a function of the magnitude of the objects .",
    "results for stars are presented as solid line , while for galaxies we used dotted lines .",
    "the perfomances of next are presented in the top - left panel : galaxies are correctly classified as long as they are detected , whereas the correctness of the classification of stars drops to 0 at @xmath211 .",
    "fainter stars are pratically absent in the ip92 catalog , thus explaining why the stars point stop at brighter magnitudes than galaxies .",
    "o92 selected a 9 features set ( o - f ) for the star / galaxy classification .",
    "their set ( central  left panel ) is slightly less performing for bright ( @xmath212 ) galaxies and for faint stars ( @xmath213 ) than the set of features selected by us ( upper  left panel ) .",
    "they select also a smaller ( four ) set of features ( o - f ) quite useful to classify large objects .",
    "the classification performances of this set , when applied to our images , turn out to be better than the larger feature dataset : in fact , bright galaxies are not misclassified ( see the bottom left panel ) .",
    "even with respect to our dataset o - f performs well : their set is sligthly better in classifying bright galaxies , at the price of a achieving lower performances on faint stars .",
    "the further set of features by o92 ( o - s ) was aimed to the accurate detection of faint sources and performs similarly to their full set : it misclassifies bright galaxies and faint stars .",
    "the performances of the traditional classifiers , @xmath214 ( gp1 ) and @xmath215 ( gp2 ) , are presented in the central and low right panels . with just two features , all the faint objects are classified as galaxies , and due to the absence of stars in our reference catalog , the classification performances are @xmath216 . however , this is not a real classification . at bright magnitudes ,",
    "the classification of the traditional classified dataset are as large as , or sligthly lower , than the next dataset .",
    "in this paper we discuss a novel approach to the problem of detection and classification of objects on wf images . in section 2",
    "we shortly review the theory of some type of nn s which are not familiar to the astronomical community .",
    "based on these considerations , we implemented a neural network based procedure ( next ) capable to perform the following tasks : i ) to detect objects against a noisy background ; ii ) to deblend partially overlapping objects ; iii ) to separate stars from galaxies .",
    "this is achieved by a combination of three different nn s each performing a specific task .",
    "first we run a non linear pca nn to reduce the dimensionality of the input space via a mapping from pixels intensities to a subspace individuated through principal component analysis . for the second step we implemented a hierarchical unsupervised nn to segmentate the image and , finally after a deblending and reconstruction loop we implemented a supervised mlp to separate stars from galaxies .    in order to identify the best performing nn s we implemented and tested in homogeneous conditions several different models .",
    "next offers several methodological and practical advantages with respect to other packages : i ) it requires only the simplest a priori definition of what an `` object '' is ; ii ) it uses unsupervised algorithms for all those tasks where both theory and extensive testing show that there is no loss in accuracy with respect to supervised methods",
    ". supervised methods are in fact used only to perform star / galaxy separation since , at magnitudes fainter than the completeness limit , stars are usually almost indistinguishable from galaxies and the parameters characterizing the two classes do not lay in disconnected subspaces .",
    "iii ) instead of using an arbitrarily defined and often specifically tailored set of features for the classification task next , after measuring a large set of geometric and photometric parameters , uses a sequential backward elimination strategy ( bishop 1995 ) to select only the most significant ones .",
    "the optimal selection of the features was checked against the performances of other classificators ( see sect .",
    "3.8 ) .    in order to evaluate the performances of next , we tested it against the best performing package known to the authors ( i d est sex ) using a dposs field centered on the north galactic pole .",
    "we want also to stress here that - in order to have an objective test and at difference of what is currently done in literature - next was checked not against the performances of an arbitrarily choosen observer but rather against a much deeper catalogue of objects obtained from better quality material .",
    "the comparison of next performances against those of sex show that in the detection phase , next is at least as effective as sex in detecting  true \" objects but much cleaner of spurious detections . for what classification",
    "is concerned , next nn performs better than the sex nn : 28 errors for next against 41 for sex on a total of 460 objects , most of the errors referring to objects fainter than the plate detection limit .",
    "other attempts , besides those described in the previous sections , to use nn for similar tasks have been discussed in the literature .",
    "balzell & peng ( 1998 ) , used the same north galactic pole field ( but extracted from poss - i plates ) used in this work .",
    "they tested their star / galaxy classification nn on objects which are both too few ( 60 galaxies and 27 stars ) and too bright ( a random check of their objects shows that most of the galaxies extend well over than 20 pixels ) to be of real interest .",
    "it needs also to be stressed that , due to their preprocessing strategy , their nn s are forced to perform cluster analysis on a huge multidimensional imput space with scarsely populated samples .",
    "naim ( 1997 ) follows instead a strategy which is similar to ours and makes use of a fairly large dataset extracted from poss - i material .",
    "he , however , trained the networks to achieve the same performances of an experienced human observer while , as already mentioned , next is checked against a catalogue of  true \" objects . even though his target is the classification of objects fainter and larger than those we are dealing with , he tested the algorithm in a much more crowded and difficult region of the sky near the galactic plane .",
    "o92 makes use of a traditional mlp and succeeded in demonstrating that ai methods can reproduce the star / galaxy classification obtained with traditional diagnostic diagrams by trained astronomers .",
    "their aim , however , was less ambitious than that of  performing the correct star / galaxy classification \" which is instead the final goal of next .",
    "this paper is a first step toward the application of artificial intelligence methods to astronomy .",
    "foreseen improvements of our approach are the use of ica ( independent component analysis ) nn s instead of pca nn s and the adoption of bayesian learning techniques to improve the classification performences of mlp s .",
    "these developments and the application of next to other wide field astronomical data sets obtained at large format ccd detectors will be discussed in forthcoming papers .",
    "* acknowledgements * the authors wish to thank chris pritchet for providing them with a digital version of the ip92 catalogue .",
    "we also acknoledge the canadian astronomy data center for providing us with poss - ii material .",
    "this work was partly sponsored by the special grant murst cofin 1998 , n.9802914427 ."
  ],
  "abstract_text": [
    "<S> astronomical wide field imaging performed with new large format ccd detectors poses data reduction problems of unprecedented scale which are difficult to deal with traditional interactive tools . </S>",
    "<S> we present here next ( neural extractor ) : a new neural network ( nn ) based package capable to detect objects and to perform both deblending and star / galaxy classification in an automatic way . </S>",
    "<S> traditionally , in astronomical images , objects are first discriminated from the noisy background by searching for sets of connected pixels having brightnesses above a given threshold and then they are classified as stars or as galaxies through diagnostic diagrams having variables choosen accordingly to the astronomer s taste and experience . in the extraction step , assuming that images are well sampled , next requires only the simplest a priori definition of  what an object is \" ( i d est , it keeps all structures composed by more than one pixels ) and performs the detection via an unsupervised nn approaching detection as a clustering problem which has been thoroughly studied in the artificial intelligence literature . </S>",
    "<S> the first part of the next procedure consists in an optimal compression of the redundant information contained in the pixels via a mapping from pixels intensities to a subspace individuated through principal component analysis . at magnitudes fainter than the completeness limit , </S>",
    "<S> stars are usually almost indistinguishable from galaxies , and therefore the parameters characterizing the two classes do not lay in disconnected subspaces , thus preventing the use of unsupervised methods . </S>",
    "<S> we therefore adopted a supervised nn ( i.e. a nn which first learns the rules to classify objects from examples and then applies them to the whole dataset ) . in practice , each object is classified depending on its membership to the regions mapping the input feature space in the training set . in order to obtain an objective and reliable classification , instead of using an arbitrarily defined set of features </S>",
    "<S> , we use a nn to select the most significant features among the large number of measured ones , and then we use their selected features to perform the classification task . in order to optimise the performances of the system we implemented and tested several different models of nn . </S>",
    "<S> the comparison of the next performances with those of the best detection and classification package known to the authors ( sextractor ) shows that next is at least as effective as the best traditional packages .    </S>",
    "<S> psfig.tex    = = = = = = = =    # 1 # 1 # 1 # 1 @mathgroup@group @mathgroup@normal@groupeurmn @mathgroup@bold@groupeurbn @mathgroup@group @mathgroup@normal@groupmsamn @mathgroup@bold@groupmsamn = `` 019 = ' ' 016 = `` 040 = ' ' 336 = \" 33e = = = = = = = =    # 1 # 1 # 1 # 1 = = = = = = = =    [ firstpage ]    astronomical instrumentation , methods and techniques  methods : data analysis ; astronomical instrumentation , methods and techniques  techniques : image processing ; astronomical data bases  catalogues . </S>"
  ]
}