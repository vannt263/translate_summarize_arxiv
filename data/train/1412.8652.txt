{
  "article_text": [
    "from the 20th century to the 21st , various disciplines have tried to infer something about scarcely observed events : zoologists about species , cryptologists about cyphers , linguists about vocabularies , and data scientists about almost everything .",
    "these problems are all about ` small data ' within possibly much bigger data .",
    "can we make such inference ?",
    "_ problem setting_. to move into a concrete setting , let @xmath6 be i.i.d .",
    "observations from a fixed but unknown distribution @xmath7 over a discrete set of symbols @xmath8 .",
    "we consider each @xmath9 in @xmath10 as a discrete _ symbol _ devoid of numerical significance .",
    "the terminology of ` infinite urn scheme ' comes from the analogy to @xmath11 independent throws of balls over an infinity of urns , @xmath12 being the probability of a ball falling into urn @xmath9 , at any @xmath13th throw .",
    "we alternatively adhere to the symbols or the urns perspective , based on which carries the intuition best .",
    "species , cyphers , and vocabularies all being discrete , are well modeled as such .",
    "the sample size @xmath11 may be fixed in advance ; we call this the _ binomial setting_. it may be randomly set by the duration of an experiment ; this gives rise to the _ poisson setting_. more precisely , in the latter case we write it as @xmath1 , a poisson random variable independent of @xmath14 and with expectation @xmath15 .",
    "we index all poisson - setting quantities by @xmath15 and write them with functional notations , instead of subscripts used for the fixed-@xmath11 scheme .    for each @xmath16 ,",
    "let @xmath17 be the number of times symbol @xmath9 occurs in a sample of size @xmath11 , and @xmath18 the poisson version . in questions of underrepresented data , the central objects are sets of symbols that are repeated a small number @xmath2 of times .",
    "the central quantities are the _ occupancy counts _ @xmath19 [ respectively , @xmath20 for the poisson setting ] , defined as the number of symbols that appear exactly @xmath2 times in a sample of size @xmath11 : @xmath21    the collection @xmath22 [ resp .",
    "@xmath23 has been given many names , such as the `` profile '' ( in information theory ( orlitsky _ et al . _",
    "@xcite ) ) or the `` fingerprint '' ( in theoretical computer science ( batu _ et al . _",
    "@xcite , valiant and valiant @xcite ) ) of the probability distribution @xmath24 . here",
    "we refer to them by _ occupancy counts _ individually , and _",
    "occupancy process _ all together .",
    "the occupancy counts then combine to yield the cumulated occupancy counts @xmath25 [ respectively @xmath26 and the total number of distinct symbols in the sample , or the total number of occupied urns , often called the _ coverage _ and denoted by @xmath27 [ respectively @xmath28 : @xmath29 and @xmath30    in addition to the occupancy numbers and the number of distinct symbols , we also address the _ rare _ ( or small - count ) _ probabilities _ @xmath31 [ respectively , @xmath32 , defined as the probability mass corresponding to all symbols that appear exactly @xmath2 times : @xmath33 in particular , we focus on @xmath34 , which is called the missing mass , and which corresponds to the probability of all the unseen symbols",
    ".    explicit formulas for the moments of the occupancy counts and masses can be derived in the binomial and poisson settings .",
    "the occupancy counts expectations are given by @xmath35 formulas for higher moments can also be computed explicitly but their expression , especially in the binomial setting where a lot of dependencies are involved , often has an impractical form .",
    "this classical occupancy setting naturally models a host of different application areas , including computational linguistics , ecology , and biology .",
    "urns may represent species , and we are interested in the number of distinct species observed in a sample ( the ecological diversity ) or in the probability of the unobserved species . in linguistics , urns may represent words . in both of these applications , the independence assumption of the random variables @xmath36 may seem unrealistic .",
    "for instance in a sentence , the probability of appearance of a word strongly depends on the previous words , both for grammatical and semantic reasons .",
    "likewise , the nucleotides in a dna sequence do not form an i.i.d . sample . in @xmath11-gram models",
    ", independence is only conditional and the observations are assumed to satisfy a markovian hypothesis : the probability of occurrence of a word depends on the @xmath37 previous words .",
    "but the i.i.d . case , although very simple , yields results that are interesting in themselves , and upon which a more sophisticated framework may be built .    many practical questions may now be formulated in this setting .",
    "if we double the duration of a first experiment , how many yet unobserved specimens would we find ( how does @xmath38 compare to @xmath19 [ resp .",
    "@xmath39 to @xmath20 ] ) ( fisher _ et al . _",
    "if certain cypher keys have been observed , what is the probability for the next to be different ( how does one estimate @xmath41 ) ?",
    "for instance , good  @xcite and turing observed that @xmath42 for all @xmath43 , and proposed to estimate the missing mass using the jackknife estimator @xmath44 ( the proportion of symbols seen just once ) .",
    "_ contributions_. to study the good  turing estimator or other quantities that depend significantly on the small - count portion of the observations , we need to understand the occupancy counts well . our contribution here is to give sharp concentration inequalities with explicit and reasonable constants , for @xmath27 , @xmath19 , and @xmath41 [ resp .",
    "@xmath45 , @xmath20 , @xmath46 .",
    "we give distribution - free results , and then exhibit a vast domain where these results are tight , namely the domain of distributions with a heavier tail than the geometric . in this domain , the non - asymptotic exponential concentration properties that we establish are sharp in the sense that the exponents are order - optimal , precisely capturing the scale of the variance .",
    "for this reason , we dedicate a portion of the paper to establishing bounds on various variances .",
    "_ organization_. the paper is organized as follows . in section  [ sec : notation ] , we present our terminology and give a concise summary of the results . in section  [ sec : concentration ]",
    "we present our variance bounds and concentration results for the occupancy counts and the missing mass in great generality . in section  [ sec : regular - variation ] , we specialize these results to regularly varying distributions , the aforementioned domain of distributions where concentration can be characterized tightly .",
    "we then elaborate on some applications in section  [ sec : applications ] , and conclude with a discussion of the results and possible extensions in section  [ sec : discussion ] .",
    "we group all proofs in the end , in section  [ sec : proofs ] .",
    "_ terminology_. our concentration results mostly take the form of bounds on the log - laplace transform .",
    "our terminology follows closely ( boucheron _ et al . _",
    "we say that the random variable @xmath47 is _ sub - gaussian _ on the right tail ( resp . on the left tail ) with variance factor @xmath48 if , for all @xmath49 ( resp .",
    "@xmath50 ) , @xmath51 we say that a random variable @xmath47 is _ sub - poisson _ with variance factor @xmath48 if , for all @xmath52 , @xmath53 with @xmath54 .",
    "we say that a random variable @xmath47 is _ sub - gamma on the right tail _ with variance factor @xmath48 and scale parameter @xmath55 if @xmath56",
    "the random variable @xmath47 is _ sub - gamma on the left tail _ with variance factor @xmath48 and scale parameter  @xmath55 , if @xmath57 is sub - gamma on the right tail with variance factor @xmath48 and scale parameter  @xmath55 . if @xmath47 is sub - poisson with variance factor @xmath48 , then it is sub - gaussian on the left tail with variance factor @xmath48 , and sub - gamma on the right tail with variance factor @xmath48 and scale parameter @xmath58 .",
    "these log - laplace upper bounds then imply exponential tail bounds .",
    "for instance , inequality ( [ eq:6 ] ) results in a bernstein - type inequality for the right tail , that is , for @xmath59 our inequalities have the form @xmath60+\\sqrt{2vs } + cs\\bigr\\ } \\leq\\mathe^{-s},\\ ] ] while inequality ( [ eq:4 ] ) for all @xmath50 entails @xmath61-\\sqrt{2vs } \\bigr\\ } \\leq\\mathe^{-s}.\\ ] ] we present such results first without making distributional assumptions , beyond the structure of those quantities themselves .",
    "these concentrations then specialize in various settings , such as that of regular variation .    _",
    "main results_. we proceed by giving a coarse description of our main results . in the poisson setting , for each @xmath62 , @xmath63 are independent , hence @xmath64 is a sum of independent bernoulli random variables , and it is not too surprising that it satisfies sub - poisson , also known as bennett , inequalities . for @xmath65",
    ", we have : @xmath66 \\phi ( \\lambda).\\ ] ] the proofs are elementary and are based on the careful application of efron  stein  steele inequalities and the entropy method ( boucheron _ et al . _",
    "@xcite ) .    as for the binomial",
    "setting , the summands are not independent but we can use negative association arguments ( dubhashi and ranjan @xcite ) ( see section  [ sec : proofs ] ) to obtain bennett inequalities for the cumulated occupancy counts @xmath25 .",
    "these hold either with the jackknife variance proxy given by the efron ",
    "stein inequality , @xmath67 or with the variance proxy stemming from the negative correlation of the summands , @xmath68 .",
    "letting @xmath69 , we have , for all @xmath52 : @xmath70 this in turn implies a concentration inequality for @xmath19 . letting @xmath71 we have , for all @xmath72 , @xmath73    we obtain distribution - free bounds on the log - laplace transform of @xmath41 , which result in sub - gaussian concentration on the left tail , sub - gamma concentration on the right tail with scale proxy @xmath74 .",
    "more precisely , letting @xmath75 and @xmath76 , we show that , for all @xmath50 , @xmath77 and , for all @xmath49 , @xmath78    indeed , these results are distribution - free .",
    "but though the variance factor @xmath79 is a sharp bound for the variance of the missing mass , @xmath80 may be much larger .",
    "this leads us to look for distribution - specific conditions ensuring that @xmath80 captures the right order for the variance , such as by using a tail asymptotic stability condition as in extreme value theory .",
    "karlin @xcite pioneered such a condition by assuming that the function @xmath81\\to\\mathbb{n}$ ] , defined by @xmath82 satisfies a regular variation assumption , namely @xmath83 near @xmath84 , with @xmath85 $ ] ( see also gnedin _ et al . _",
    "@xcite , ohannessian and dahleh @xcite ) . here",
    "@xmath86 is a slowly varying function at @xmath87 , i.e. for all @xmath3 , @xmath88 as @xmath89 .",
    "this condition allows us to compare the asymptotics of the various occupancy scores .",
    "in particular , in this framework @xmath90 and @xmath91 have the same order of growth , and , divided by @xmath92 they both are of the same order as the variance of the missing mass . hence , regular variation provides a framework in which our concentration inequalities are order - optimal .    to handle the case @xmath93",
    ", we move from _ karamata _ to _ de haan _ theory , and take @xmath94 to have an extended regular variation property , with the additional assumption that @xmath95 tends to @xmath84 .",
    "this domain corresponds to light - tailed distributions which are still heavier than the geometric . in this case",
    ", we manage to show the sub - gamma concentration of the missing mass only for @xmath11 large enough , that is , that there exists @xmath96 such that for all @xmath97 , for @xmath98 , we have @xmath99 , with @xmath100 .    back to our examples of applications , considerable insight may be gained from these concentration results .",
    "for instance , heavy tails lead to multiplicative concentration for @xmath41 and the consistency of the good  turing estimator : @xmath101 . generally , new estimators can be derived and shown to be consistent in a unified framework , once one is able to estimate @xmath102 consistently .",
    "for instance , when @xmath103 is regularly varying with index @xmath102 , @xmath104 is a consistent estimator of @xmath102 .",
    "then , to estimate the number of new species in a sample twice the size of the original , we immediately get that @xmath105 is a consistent estimator of @xmath106 .",
    "this methodology is very similar to extreme value theory ( beirlant _ et al . _",
    "@xcite ) : harnessing limiting expressions and tail parameter estimation . these results strengthen and extend the contribution of ohannessian and dahleh @xcite , which is restricted to power - laws and implicit constants in the inequalities . beyond consistency results ,",
    "we also obtain confidence intervals for the good  turing estimator in the poisson setting , using empirical quantities .",
    "_ historical notes and related work_. there exists a vast literature on the occupancy scheme , as formulated here and in many other variations .",
    "the most studied problems are the asymptotic behavior of @xmath27 and @xmath19 .",
    "this is done often in a finite context , or a scaling model where probabilities remain mostly uniform .",
    "of particular relevance to this paper , we mention the work of karlin  @xcite , who built on earlier work by bahadur @xcite , credited as one of the first to study the infinite occupancy scheme .",
    "karlin s main results were to establish central limit theorems in an infinite setting , under a condition of regular variation .",
    "he also derived strong laws of large numbers .",
    "@xcite present a general review of these earlier results , as well as more contemporary work on this problem .",
    "the focus continues to be central limit theorems , or generally asymptotic results .",
    "for example , the work of hwang and janson @xcite ( effectively ) provides a local limit theorem for @xmath27 provided that the variance tends to infinity .",
    "somewhat less asymptotic results have also been proposed , in the form of normal approximations , such as in the work of bogachev _ et al .",
    "_ @xcite and barbour and gnedin @xcite .",
    "besides occupancy counts analysis , a distinct literature investigates the number of species and missing mass problems .",
    "these originated in the works of fisher _ et al . _",
    "@xcite , good @xcite , and good and toulmin @xcite , and generated a long line of research to this day ( bunge and fitzpatrick @xcite ) .",
    "here , instead of characterizing the asymptotic behavior of these quantities , one is interested in estimating @xmath107 for a @xmath108 , that is the number of discoveries when the sample size is multiplied by @xmath109 , or estimating @xmath41 : estimators are proposed , and then their statistical properties are studied .",
    "one recently studied property by mcallester and schapire @xcite and mcallester and ortiz @xcite , is that of concentration , which sets itself apart from the clt - type results in that it is non - asymptotic in nature .",
    "based on this , ohannessian and dahleh @xcite showed that in the regular variation setting of karlin , one could show multiplicative concentration , and establish strong consistency results .",
    "conversely , characterizing various aspects of concentration allows one to systematically design new estimators . for example , this was illustrated in ohannessian and dahleh @xcite for the estimation of rare probabilities , to both justify and extend good s ( good @xcite ) work that remains relevant in some of the aforementioned applications , especially computational linguistics .",
    "such concentration results for rare probabilities have been also used in the general probability estimation problem , such as by acharya _",
    "in order to understand the fluctuations of occupancy counts @xmath27 , @xmath45 , @xmath19 , @xmath20 , we start by reviewing and stating variance bounds .",
    "we start with the poisson setting where occupancy counts are sums of independent bernoulli random variables with possibly different success probabilities , and thus variance computations are straightforward .",
    "exact expressions may be derived ( see , for example , gnedin _ et al . _",
    "@xcite , equation ( 4 ) ) , but ingenuity may be used to derive more tractable and tight bounds .",
    "we start by stating a well - known connection between the variance of the number of occupied urns and the expected number of singletons ( gnedin _ et al . _",
    "@xcite , karlin @xcite ) . in the binomial",
    "setting , similar bounds can be derived using the efron ",
    "steele inequalities , outlined in section  [ sec : efron - stein ] ( see boucheron _ et al . _ @xcite , section  3.1 ) .",
    "[ prop : varkt ] in the poissonized setting , we have @xmath110 in the binomial setting , we have @xmath111 \\leq\\exp k_{n,1}.\\ ] ]    the upper bounds in these two propositions parallel each other , but in the binomial setting , we can not hope to establish lower bounds like @xmath112 for some constant @xmath113 in full generality . to see this , consider the following example which shows that the variance of @xmath45 and of @xmath27 may differ significantly , and that the variance of @xmath27 may be much smaller than the expected value of @xmath114 .",
    "[ ex : birthday1 ] in the so - called _ birthday paradox _",
    "scenario , @xmath11 balls are thrown independently into @xmath92 urns with uniform probabilities @xmath115 . in the poisson setting for time @xmath116 ,",
    "since we have @xmath117 , using an upper and lower taylor expansion we can obtain the bounds : @xmath118 since @xmath119 , it follows that : @xmath120 meanwhile , we have @xmath121=\\sum_j np_j \\mathe^{-np_j}= n\\mathe ^{-1/n}$ ] , which can be bounded similarly : @xmath122\\leq n-1+\\frac{1}{2n}.\\ ] ] we can thus see that the poisson birthday paradox satisfies the spirit of proposition  [ prop : varkt ] , if not its letter ( because of being outside of our fixed-@xmath123 setting ) .",
    "namely , the poisson quantities @xmath124 and @xmath95 are of the same order of magnitude , roughly @xmath11 .    on the other hand , in the binomial",
    "setting , since @xmath125 , @xmath126=\\exp \\biggl[k_{n,1}\\frac{k_n}{n^2 } \\biggr ] \\leq \\frac{1}{n } \\exp k_{n,1 } \\leq 1 , \\ ] ] where we have used the same variance bound as in the proof of proposition  [ prop : varkt ] ( section  [ sec : proof - vari - bounds - occ - counts ] ) .",
    "while this implies that the upper bound @xmath127 is satisfied , it also shows that a lower bound of the kind of @xmath112 is not possible , since similarly to @xmath95 , we have @xmath128 .",
    "another straightforward bound on @xmath129 comes from the fact that the bernoulli variables @xmath130 are negatively correlated .",
    "thus , ignoring the covariance terms , we get @xmath131 let us denote this bound by @xmath132 , as it is a variance proxy obtained by considering that the summands in @xmath27 are independent .",
    "one can observe that the expression of @xmath133 is very similar to the variance in the poissonized setting , @xmath134 .",
    "it is insightful to compare the true variance , the poissonized proxy , and the negative correlation proxy , to quantify the price one pays by resorting to the latter two as an approximation for the first .",
    "we revisit this in more detail in section  [ sec : cost - poisson ] .",
    "we now investigate the fluctuations of the individual occupancy counts @xmath19 and @xmath20 , and that of the cumulative occupancy counts @xmath135 and @xmath136 .",
    "[ prop : varknrinf ] in the poisson setting , for @xmath62 , @xmath137 , @xmath138 in the binomial setting , for @xmath139 , @xmath140    for each setting , the first bound follows from efron  stein ",
    "steele inequalities , the second from negative correlation .",
    "these techniques are presented briefly in sections  [ sec : efron - stein ] and [ sec : negative - association ] , respectively .    except for @xmath141",
    ", there is no clear - cut answer as to which of these two bounds is the tightest . in the regular variation scenario with index @xmath1420,1]$ ] as explored in gnedin _",
    "et al . _",
    "@xcite , the two bounds are asymptotically of the same order , indeed , @xmath143 see section  [ sec : regular - variation ] for more on this .",
    "bounds on @xmath144 can be easily derived as @xmath64 is a sum of independent bernoulli random variables , however , noticing that @xmath145 and that @xmath25 and @xmath146 are positively correlated , the following bound is immediate .    [",
    "prop : varknr ] in the poisson setting , for @xmath62 , @xmath137 , @xmath147 in the binomial setting , for @xmath139 , @xmath148      concentration inequalities refine variance bounds .",
    "these bounds on the logarithmic moment generating functions are indeed bennett ( sub - poisson ) inequalities with the variance upper bounds stated in the preceding section . for @xmath25 ,",
    "the next proposition gives a bernstein inequality where the variance factor is , up to a constant factor , the efron ",
    "stein upper bound on the variance .",
    "[ prop : conc - ineq - knrinf ] let @xmath62 , and let @xmath149 . then , for all @xmath150 , @xmath151 with @xmath54 .",
    "it is worth noting that the variance bound @xmath68 in this concentration inequality can also be obtained using a variant of stein s method known as size - biased coupling ( bartroff _ et al . _",
    "@xcite , chen _ et al . _",
    "@xcite ) .    a critical element of the proof of proposition  [ prop : conc - ineq - knrinf ] is to use the fact that each @xmath152 is a sum of negatively associated random variables ( section  [ sec : negative - association ] ) .",
    "this is not the case for @xmath19 , and thus negative association can not be invoked directly . to deal with this",
    ", we simply use the observation of ohannessian and dahleh @xcite that since @xmath153 , the concentration of @xmath19 follows from that of those two terms .",
    "we can show the following .",
    "[ prop : conc - ineq - knr ] let @xmath154 then , for @xmath72 , @xmath73        recall that @xmath155 , and we can readily show that the summands are negatively associated weighted bernoulli random variables ( section  [ sec : negative - association ] ) .",
    "this results in a handy upper bound for the variance of the missing mass .",
    "[ prop : vari - missingmass ] in the poisson setting , @xmath156 while in the binomial setting , @xmath157    note that whereas the expected value of the missing mass is connected to the number of singletons , its variance may be upper bounded using the number of _ doubletons _ ( in the poisson setting ) .",
    "this connection was already pointed out in good @xcite and esty @xcite .      moving on to the concentration properties of the missing mass",
    ", we first note that as a sum of weighted sub - poisson random variables ( following boucheron _",
    "@xcite ) , the missing mass is itself a sub - gamma random variable on both tails",
    ". it should not come as a surprise that the left tail of @xmath41 is sub - gaussian with the variance factor derived from negative association .",
    "this had already been pointed out by mcallester and schapire @xcite and mcallester and ortiz @xcite .",
    "[ prop : bennett : left : mn0 ] in the poisson setting , the missing mass @xmath158 is sub - gaussian on the left tail with variance factor the effective variance @xmath159 .    in the binomial",
    "setting , the missing mass @xmath41 is sub - gaussian on the left tail with variance factor @xmath160 or @xmath161 .    for @xmath50 , @xmath162 \\leq \\frac{v \\lambda^2}{2 } \\leq \\frac{2 v_n^- \\lambda^2}{2}.\\ ] ]",
    "the following concentration inequalities for the right tail of the missing mass mostly rely on the following proposition , which bounds the log - laplace transform of the missing mass in terms of the sequence of expected occupancy counts @xmath163 , for @xmath164 .",
    "[ prop : log - laplace - missing - mass ] for all @xmath98,@xmath162 \\leq \\sum _ { r=2}^\\infty \\biggl(\\frac{\\lambda}{n } \\biggr)^r \\exp k_r(n).\\ ] ]    this suggests that if we have a uniform control on the expected occupancy scores @xmath165 , then the missing mass has a sub - gamma right tail , with some more or less accurate variance proxy , and scale factor @xmath74 .",
    "the next theorem shows that the missing mass is sub - gamma on the right tail with variance proxy @xmath166 and scale proxy @xmath74 . despite its simplicity and its generality , this bound exhibits an intuitively correct scale factor : if there exist symbols with probability of order @xmath74 , they offer the major contribution to the fluctuations of the missing mass .",
    "[ thmm : struct - ineq - right ] in the binomial as well as in the poisson setting , the missing mass is sub - gamma on the right tail with variance factor @xmath167 and scale factor @xmath74 . for @xmath49 , @xmath162 \\leq \\frac{v_n^+\\lambda^2}{2(1-\\lambda / n ) } .\\ ] ] if the sequence @xmath168 is non - increasing , the missing mass is sub - gamma on the right tail with variance factor @xmath169 and scale factor @xmath74 , @xmath162 \\leq \\frac{v_n^-\\lambda^2}{2 ( 1-\\lambda / n)}.\\ ] ]    mcallester and ortiz @xcite and berend and kontorovich @xcite point out that for each bernoulli random variable @xmath170 , for all @xmath150 @xmath171 where @xmath172 ( or @xmath173 if @xmath174 ) is the optimal logarithmic sobolev constant for bernoulli random variables with success probability @xmath123 ( this sharp and nontrivial result has been proven independently by a number of people : kearns and saul @xcite , berend and kontorovich @xcite , raginsky and sason @xcite , berend and kontorovich @xcite ; the constant also appears early on in the exponent of one of hoeffding s inequalities ( hoeffding @xcite , theorem  1 , equation ( 2.2 ) ) . from this observation , thanks to the negative association of the @xmath175 , it follows that the missing mass is sub - gaussian with variance factor @xmath176 an upper bound on @xmath177 does not mean that @xmath177 is necessarily larger than @xmath178 . nevertheless , it is possible to derive a simple lower bound on @xmath177 that proves to be of order @xmath179 .",
    "assume that the sequence @xmath0 is such that @xmath180 for all @xmath181 .",
    "then @xmath182 and the statement follows from the observation that @xmath183 .",
    "the variance factor @xmath177 from ( [ eq:7 ] ) is usually larger than @xmath184 . in the scenarios discussed in section  [ sec : regular - variation ]",
    ", @xmath185 even tends to @xmath186 as @xmath11 tends to infinity .",
    "_ motivation_. are the variance bounds in the results of section  [ sec : concentration ] tight ? in some pathological situations , this may not be the case .",
    "consider the following example ( which revisits example  [ ex : birthday1 ] ) .",
    "[ ex : birthday2 ] we may challenge the tail bounds offered by proposition  [ prop : bennett : left : mn0 ] and theorem  [ thmm : struct - ineq - right ] in the simplest setting where we have @xmath187 symbols all of which have equal probabilities @xmath188 .",
    "then the missing mass is @xmath189 , its variance is @xmath190 . in the birthday paradox setting ( @xmath191 ) , @xmath192 , and the variance bound",
    "@xmath193 is not tight .",
    "indeed , one can verify that @xmath194 so that @xmath195 .",
    "however , in what is called the _ central domain _ in kolchin _",
    "@xcite , that is when @xmath196 while @xmath197 , the tail bounds become relevant .",
    "the variance of @xmath27 is equivalent to @xmath198 while its expectation is equivalent to @xmath199 .",
    "note that in this setting all @xmath163 and @xmath200 are of the same order of magnitude as @xmath201 , indeed @xmath202 .",
    "these examples are illustrative although they do not fall in the fixed @xmath123 regime we are considering in this paper .",
    "we use them because they have tractable expressions , and they provide informative diagnostics . to parallel the phenomenon of mismatched variance proxies in our setting",
    ", one can simply look at the geometric distribution for a concrete example .",
    "if @xmath203 defines a geometric distribution @xmath204 , then @xmath90 remains bounded , while @xmath91 scales like @xmath205 as @xmath11 tends to infinity .",
    "in particular , we may conjecture that theorem  [ thmm : struct - ineq - right ] is likely to be sharp when the first terms of the sequence @xmath168 grow at the same rate as @xmath206 , or at least as @xmath91 , which is not the case in the birthday paradox setting of example  [ ex : birthday2 ] .",
    "we see in what follows that the regular variation framework introduced by karlin @xcite leads to such asymptotic equivalents .",
    "the most useful aspect of these equivalent growth rates is a simple characterization of the variance of various quantities , particularly relative to their expectation .",
    "we focus on the right tail of the missing mass , which exhibits the highest sensitivity to this asymptotic behavior , by trying to specialize theorem  [ thmm : struct - ineq - right ] under regular variation .    _",
    "definition_. regularly varying frequencies can be seen as generalizations of power - law frequencies .",
    "one possible definition is as follows : for @xmath207 , the sequence @xmath208 is said to be regularly varying with index @xmath209 if , for all @xmath210 , @xmath211 it is easy to see that pure power laws do indeed satisfy this definition .",
    "however , in order to extend the regular variation hypothesis to @xmath93 and @xmath212 , we need a more flexible definition , which requires some new notation .",
    "this definition relies on the _ counting function _",
    "@xmath213 , defined for all @xmath214 by : @xmath215 the overhead arrow is not a vector notation , and rather codifies that we are counting points with probability `` to the right '' of @xmath3 .",
    "more precisely , letting @xmath216 be the _ counting measure _ defined by @xmath217 then , for all @xmath214 , @xmath218 $ ] .",
    "henceforth , following karlin @xcite , we say that the probability mass function @xmath219 is regularly varying with index @xmath220 $ ] , if @xmath221 is @xmath102-regularly varying in the neighbourhood of @xmath87 , which reads as @xmath222 where @xmath86 is a slowly varying function , that is , for all @xmath214 , @xmath223 .",
    "we use the notation @xmath224 to indicate that @xmath225 is @xmath102-regularly varying .",
    "we now note that when @xmath207 , the regular variation assumption on @xmath0 is indeed equivalent to the regular variation assumption on the counting function @xmath94 ( see gnedin et al .",
    "@xcite , proposition  23 ) : if @xmath0 is regularly varying with index @xmath209 as @xmath9 tends to infinity , then @xmath221 is @xmath102-regularly varying , that is @xmath226 for @xmath227 ( see also bingham _ et al . _",
    "the second definition however lends itself more easily to generalization to @xmath93 and @xmath212 .    in what follows",
    ", we treat these three cases separately : the nominal regular variation case with @xmath207 strictly , the _ fast variation _ case with @xmath228 , and the _ slow variation _ case with @xmath93 .    in the latter case , that is if frequencies @xmath12 are regularly varying with index @xmath186 , we find that the mere regular variation hypothesis is not sufficient to obtain asymptotic formulas .",
    "for this reason , we introduce further control in the form of an _ extended _ regular variation hypothesis ( given by definition  [ dfn : slow - variation-1 ] of section  [ sec : slow - variation ] ) .",
    "[ rem : necessary ] before we proceed , as further motivation , we note that the regular variation hypothesis is very close to being a necessary condition for exponential concentration .",
    "for example , considering proposition  [ prop : log - laplace - missing - mass ] , we see that if the sampling distribution is such that the ratio @xmath229 remains bounded , then we are able to capture the right variance factor .",
    "now , defining the shorthand @xmath230 and @xmath231 following the notation of gnedin _ et al . _",
    "@xcite , we have @xmath232 hence , @xmath233 , and if instead of boundedness , we further require that this ratio converges to some finite limit , then , by the converse part of karamata s theorem ( see de haan and ferreira @xcite , theorem b.1.5 ) , we find that @xmath234 ( and then @xmath235 ) is regularly varying , which in turn implies that @xmath236 is regularly varying .",
    "we elaborate on this further in our discussions , in section  [ sec : extensions ] .",
    "we first consider the case @xmath238 .",
    "the next theorem states that when the sampling distribution is regularly varying with index @xmath239 , the variance factors in the bernstein inequalities of proposition  [ prop : bennett : left : mn0 ] and theorem  [ thmm : struct - ineq - right ] are of the same order as the variance of the missing mass .",
    "[ prop : bernstein : right : regvar ] assume that the counting function @xmath94 satisfies the regular variation condition with index @xmath207 , then the missing mass @xmath41 ( or @xmath240 ) is sub - gaussian on the left tail with variance factor @xmath161 and sub - gamma on the right tail with variance factor @xmath241 .",
    "the variance factors satisfy @xmath242 and thus @xmath243    the second ratio deteriorates when @xmath102 approaches @xmath186 , implying that the variance factor for the right tail gets worse for lighter tails .",
    "we do not detail the proof of theorem  [ prop : bernstein : right : regvar ] , except to note that it follows from proposition  [ prop : bennett : left : mn0 ] , theorem  [ thmm : struct - ineq - right ] , and the following asymptotics ( see also gnedin _ et al . _",
    "@xcite , ohannessian and dahleh @xcite ) .",
    "[ th : portmanteau ] if the counting function @xmath94 is regularly varying with index @xmath207 , for all @xmath62 ,    * @xmath244 , * @xmath245 , * @xmath246    and the same hold for the corresponding poissonized quantities .",
    "note that all expected occupancy counts are of the same order , and the asymptotics for @xmath91 follows directly from the difference between @xmath247 and @xmath95 .",
    "we refer to the regular variation regime with @xmath228 as _ _ fast variation . _",
    "_ from the perspective of concentration , this represents a relatively `` easy '' scenario . in a nutshell , this is because the variance of various quantities grows much slower than their expectation .",
    "the result of this section is to simply state that theorem  [ prop : bernstein : right : regvar ] continues to hold as is for @xmath228 .",
    "the justification for this , however , is different . in particular",
    ", the asymptotics of theorem  [ th : portmanteau ] do not apply : the number of distinct symbols @xmath27 and the singletons @xmath114 continue to have comparable growth order , but now their growth dominates that of @xmath19 for all @xmath164 .",
    "intuitively , under fast variation almost all symbols appear only once in the observation , with only a vanishing fraction of symbols appearing more than once .",
    "we formalize this in the following theorem .",
    "[ thmm : portmanteau - rapid - var ] assume @xmath249 with @xmath250 ( note that @xmath86 tends to @xmath186 at @xmath87 ) .",
    "define @xmath251 by @xmath252 .",
    "then @xmath253 and @xmath254 and the following asymptotics hold :    * @xmath255 , * @xmath256 , * @xmath257 , @xmath164 ,    and the same hold for the corresponding poissonized quantities .",
    "as the expected missing mass scales like @xmath258 while its variance scales like @xmath259 , theorem  [ thmm : portmanteau - rapid - var ] quantifies our claim that this is an `` easy '' concentration . to establish theorem",
    "[ prop : bernstein : right : regvar ] , it remains to show that @xmath260 is also of the same order as @xmath90 , with the correct limiting ratio for @xmath228 .",
    "for this , we give the following proposition , which is in fact sufficient to prove theorem  [ prop : bernstein : right : regvar ] for both @xmath261 and @xmath228 .    [ prop : asymp - cumulated - occup ] assume that the counting function @xmath94 satisfies the regular variation condition with index @xmath85 $ ] , then for all @xmath164 , @xmath262    thus , when @xmath228 , @xmath163 and @xmath263 for @xmath164 all grow like @xmath264 , which is dominated by the @xmath265 growth of @xmath247 and @xmath95 , as @xmath266 .",
    "specializing for @xmath267 , we do find that our proxies still capture the right order of the variance of the missing mass , and that we have the desired limit of theorem  [ prop : bernstein : right : regvar ] , @xmath268 .",
    "when @xmath261 , another good variance proxy would have been @xmath269 . for @xmath228 , however , singletons should be removed to get the correct order .",
    "we also note that when @xmath228 , the missing mass is even more stable . if we let @xmath270 denote either @xmath271 or @xmath184 , then we have the following comparison between the expectation and the fluctuations of the missing mass , with the appropriate constants : @xmath272      the setting where the counting function @xmath94 satisfies the regular variation condition with index @xmath186 represents a challenge .",
    "we refer to this regime simply as _",
    "slow variation_. recall that this means that @xmath274 converges to @xmath212 as @xmath11 goes to infinity , yet to deal with this case we need to control the speed of this convergence , exemplified by the notion of extended regular variation that was introduced by de haan ( see bingham _",
    "@xcite , de haan and ferreira @xcite ) .",
    "as we illustrate in the end of this section , one may face rather irregular behavior without such a hypothesis .",
    "[ dfn : slow - variation-1 ] a measurable function @xmath275 has the extended slow variation property , if there exists a nonnegative measurable function @xmath276 such that for all @xmath214 @xmath277 the function @xmath278 is called an auxiliary function .",
    "when a function @xmath86 has the extended slow variation property with auxiliary function @xmath279 , we denote it by @xmath280 .",
    "note that the auxiliary function is always slowly varying and grows slower than the original function , namely it satisfies @xmath281 .",
    "furthermore , any two possible auxiliary functions are asymptotically equivalent , that is if @xmath282 and @xmath283 are both auxiliary functions for @xmath86 , then @xmath284 .",
    "the notion of extended slow variation and the auxiliary function give us the aforementioned control needed to treat the @xmath93 case on the same footing as the @xmath261 case .",
    "in particular , in what follows in this section we assume that @xmath285 , with the additional requirement that the auxiliary function @xmath279 tends to @xmath84 .",
    "this domain corresponds to light - tailed distributions just above the geometric distribution ( the upper - exponential part of gumbel s domain ) . for the geometric distribution with frequencies @xmath286 ,",
    "@xmath287 the counting function satisfies @xmath288 , but the auxiliary function @xmath289 does not tend to infinity .",
    "frequencies of the form @xmath290 on the other hand do fit this framework .",
    "[ thmm : portmanteau - slow - var ] assume that @xmath291 is in @xmath292 , where @xmath279 is slowly varying and tends to infinity .",
    "the following asymptotics hold for each @xmath293 :    * @xmath294 , * @xmath295 , * @xmath296 , * @xmath297 .",
    "the same equivalents hold for the corresponding poissonized quantities .    in this case , the expectations @xmath298 are of the same order but are much smaller than @xmath201 , and the variables @xmath27 and @xmath299 are all almost surely equivalent to @xmath300 .",
    "it is also remarkable that all the expected masses @xmath301 are equivalent .",
    "the variance of the missing mass is of order @xmath302 , whereas the proxy @xmath166 is of much faster order @xmath303 , and is thus inadequate . by exploiting more carefully the regular variation hypothesis",
    ", we obtain uniform control over @xmath304 for large enough @xmath11 , leading to a variance proxy of the correct order .",
    "[ thmm : bernstein : right : slowvar ] assume that @xmath86 defined by @xmath305 is in @xmath292 where the slowly varying function @xmath279 tends to infinity , and let @xmath306 .",
    "we have :    @xmath307 , thus @xmath308 .",
    "there exists @xmath309 that depends on @xmath94 such that for all @xmath310 , for all @xmath98 , @xmath311 \\leq\\frac{v_n \\lambda^2}{2 ( 1-\\lambda / n)}.\\ ] ]    the same results hold for @xmath158 .    by standard chernoff bounding , theorem  [ thmm : bernstein : right : slowvar ]",
    "implies that there exists @xmath312 such that for all @xmath97 , @xmath72 , @xmath313      we conclude this section by motivating why it is crucial to have a heavy - enough tail in order to obtain meaningful concentration .",
    "for example , even under regular variation when @xmath93 , but @xmath213 is not in a de haan class @xmath292 with @xmath314 , the behavior of the occupancy counts and their moments may be quite irregular . in this section ,",
    "we collect some observations on those light - tailed distributions .",
    "we start with the geometric distribution which represents in many respects a borderline case .",
    "the geometric case is an example of slow variation : @xmath315 . indeed , with @xmath204 , @xmath316",
    ", we have @xmath317 and thus @xmath318 , with @xmath86 slowly varying .    in this case ,",
    "@xmath319 .",
    "[ geom ] when the sampling distribution is geometric with parameter @xmath320 , letting @xmath321 , @xmath322    in the case of geometric frequencies , the missing mass can fluctuate widely with respect to its expectation , and one can not expect to obtain sub - gamma concentration with both the correct variance proxy and scale factor @xmath74 .",
    "indeed , intuitively , the symbol which primarily contributes to the missing mass fluctuations , is the quantile of order @xmath323 . with @xmath324 , and @xmath325 the generalized inverse of @xmath326 , @xmath327 omitting the slowly varying functions , when @xmath328 , @xmath261 , @xmath329 is of order @xmath330 and @xmath331 is of order @xmath332 .",
    "the closer to @xmath212 is @xmath102 , the smaller the probability of @xmath329 .",
    "when @xmath102 goes to @xmath186 , this probability becomes @xmath74 . with geometric frequencies ,",
    "@xmath329 is @xmath333 and @xmath331 is @xmath334 .",
    "hence , around the quantile of order @xmath335 , there are symbols which may contribute significantly to the missing mass fluctuations .",
    "another interesting case consists of distributions which are very light - tailed , in the sense that @xmath336 when @xmath337 .",
    "an example of these is the poisson distribution @xmath338 , for which @xmath339 .",
    "the next proposition shows that for such concentrated distributions , the missing mass essentially concentrates on two points .",
    "[ prop : potpourri ] in the infinite urns scheme with probability mass function @xmath340 , if @xmath341 for all @xmath187 and @xmath342 , then there exists a sequence of integers @xmath343 such that @xmath344 where @xmath345 .",
    "when working in the regular variation setting , the most basic estimation task is to estimate the regular variation index @xmath102 .",
    "we already mentioned in section  [ sec : notation ] the fact that , when @xmath346 , the ratio @xmath347 provides a consistent estimate of @xmath102 .",
    "this is actually only one among a family of estimators of @xmath102 that one may construct .",
    "the next result shows this , and is a direct consequence of proposition  [ prop : asymp - cumulated - occup ] .",
    "[ prop : alpha : estimate ] if @xmath348 $ ] , then for all @xmath62 @xmath349 is a strongly consistent estimator of @xmath102 .    thus ,",
    "writing @xmath350 , at time @xmath11 , we can have up to @xmath351 non - trivial estimators of @xmath102 .",
    "one would expect these estimators to offer various bias - variance trade - offs , and one could ostensibly select an `` optimal '' @xmath2 via model selection .",
    "the good  turing estimation problem ( good @xcite ) is that of estimating @xmath31 from the observation @xmath352 . for large scores @xmath2 ,",
    "designing estimators for @xmath31 is straightforward : we assume that the empirical distribution mimics the sampling distribution , and that the empirical probabilities @xmath353 are likely to be good estimators .",
    "the question is more delicate for rare events .",
    "in particular , for @xmath354 , it may be a bad idea to assume that there is no missing mass @xmath355 , that is to assign a zero probability to the set of symbols that do not appear in the sample .",
    "various `` smoothing '' techniques have thus developed , in order to adjust the maximum likelihood estimator and obtain more accurate probabilities .",
    "in particular , good  turing estimators attempt to estimate @xmath356 from @xmath357 for all @xmath2 .",
    "they are defined as @xmath358    the rationale for this choice comes from the following observations .",
    "@xmath359}{n}= \\exp m_{n-1,0 } = \\exp m_{n,0 } + \\frac { \\exp m_{n,1}}{n}\\ ] ] and @xmath360 in the poisson setting , there is no bias : @xmath361 .    here , we primarily focus on the estimation of the missing masses @xmath41 and @xmath158 , though most of the methodology extends also to @xmath362 , with the appropriate concentration results . from ( [ eq:1 ] ) and ( [ eq:2 ] ) , good  turing estimators",
    "look like slightly biased estimators of the relevant masses . in particular , the bias @xmath363 is always positive but smaller than @xmath74 .",
    "it is however far from obvious to determine scenarios where these estimators are consistent and where meaningful confidence regions can be constructed .",
    "when trying to estimate the missing mass @xmath41 or @xmath364 , consistency needs to be redefined since the estimand is not a fixed parameter of interest but a random quantity whose expectation further depends on @xmath11 .",
    "additive consistency , that is bounds on @xmath365 is not a satisfactory notion , because , as @xmath41 tends to @xmath186 , the trivial constant estimator @xmath186 would be universally asymptotically consistent .",
    "relative consistency , that is control on @xmath366 looks like a much more reasonable notion .",
    "it is however much harder to establish .    in order to establish relative consistency of a missing mass estimator",
    ", we have to check that @xmath367 $ ] is not too large with respect to @xmath364 , and that both @xmath368 and @xmath41 are concentrated around their mean values .",
    "as shown in ohannessian and dahleh @xcite , the good  turing estimator of the missing mass is not universally consistent in this sense .",
    "this occurs principally in very light tails , such as those described in section  [ sec : too - slow ] .",
    "when the sampling distribution is geometric with small enough @xmath320 , there exists @xmath369 , and a subsequence @xmath370 such that for @xmath13 large enough , @xmath371 with probability no less than @xmath372 .    on the other hand ,",
    "the concentration result of corollary  [ prop : bernstein : right : regvar ] gives a law of large numbers for @xmath41 ( by a direct application of the borel ",
    "cantelli lemma ) , which in turn implies the strong multiplicative consistency of the good  turing estimate .",
    "[ cor : lgnmissingmass ] we have the following two regimes of consistency for the good  turing estimator of the missing mass .    if the counting function @xmath94 is such that @xmath373 remains bounded and @xmath374 ( in particular , when @xmath213 is regularly varying with index @xmath375 $ ] or @xmath93 and @xmath376 with @xmath377 ) , @xmath378 and the good  turing estimator of @xmath41 defined by @xmath379 , is multiplicatively consistent in probability : @xmath380    if furthermore @xmath381 remains bounded and if , for all @xmath113 , @xmath382 ( in particular , when @xmath213 is regularly varying with index @xmath375 $ ] ) , then these two convergences occur almost surely .",
    "one needs to make assumptions on the sampling distribution to guarantee the consistency of the good  turing estimator .",
    "in fact , there is no hope to find a universally consistent estimator of the missing mass without any such restrictions , as shown recently by mossel and ohannessian @xcite .",
    "consistency is a desirable property , but the concentration inequalities provide us with more power , in particular in terms of giving confidence intervals that are asymptotically tight . for brevity , we focus here on the poisson setting to derive concentration inequalities which in turn yield confidence intervals . a similar , but somewhat more tedious , methodology yields confidence intervals in the binomial setting as well .      in the poisson setting ,",
    "the analysis of the good  turing estimator is illuminating .",
    "as noted earlier , the first pleasant observation is that the good  turing estimator is an unbiased estimator of the missing mass .",
    "second , the variance of @xmath384 is simply related to occupancy counts : @xmath385 third , simple yet often tight concentration inequalities can be obtained for @xmath384 .",
    "[ prop : conc - ineq - gtt-1 ] the random variable @xmath384 is sub - gamma on the right tail with variance factor @xmath386 and scale factor @xmath387 , and sub - gamma on the left tail with variance factor @xmath388 and scale factor @xmath387 .    for all @xmath49 ,    @xmath389    @xmath390",
    "we are now in a position to build confidence intervals for the missing mass .",
    "[ prop : conf - missing ] with probability larger than @xmath391 , the following hold @xmath392    to see that these confidence bounds are asymptotically tight , consider the following central limit theorem .",
    "a similar results can be paralleled in the binomial setting .",
    "[ prop : tcl - ratio - gt - mn0 ] if the counting function @xmath94 is regularly varying with index @xmath85 $ ] , the following central limit theorem holds for the ratio @xmath393 : @xmath394    note that when @xmath228 , this convergence occurs faster : the speed is of order @xmath395 instead of @xmath396 .",
    "fisher s number of species problem ( fisher _ et al . _",
    "@xcite ) consists of estimating @xmath397 for @xmath398 , the number of distinct new species one would observe if the data collection runs for an additional fraction @xmath399 of time .",
    "this was posed primarily within the poisson model in the original paper ( fisher _ et al . _",
    "@xcite ) and later by efron and thisted @xcite , but the same question may also be asked in the binomial model .",
    "the following estimates come from straightforward computations on the asymptotics given in theorems [ th : portmanteau ] , [ thmm : portmanteau - rapid - var ] and [ thmm : portmanteau - slow - var ] .",
    "if the counting function @xmath94 is regularly varying with index @xmath85 $ ] , letting @xmath400 be any of the estimates @xmath401 of @xmath102 from proposition  [ prop : alpha : estimate ] , then any of the following quantities @xmath402 is a strongly consistent estimate of @xmath403 , the number of newly discovered species when the sample size is multiplied by @xmath399 .",
    "if the counting function @xmath94 is in @xmath292 , with @xmath404 , then , for each @xmath62 , @xmath405 is an estimate of @xmath403 , consistent in probability .",
    "to conclude the paper , we review our results in a larger context , and propose some connections , extensions , and open problems .",
    "resorting to poissonization or negative correlation may have a price .",
    "it may lead to variance overestimates .",
    "( @xcite , lemma  1 ) , asserts that for some constant @xmath55 @xmath406 this bound conveys a mixed message . as @xmath258 tends to @xmath186",
    ", it asserts that @xmath407 tends to @xmath186 .",
    "but there exist scenarios where @xmath408 tends to infinity .",
    "it is shown in gnedin _",
    "_ @xcite that @xmath409 tends to @xmath186 , so that , as soon as @xmath410 tends to infinity ( which might not always be the case ) , the two variances @xmath129 and @xmath411 are asymptotically equivalent",
    ".    it would be interesting to find necessary and sufficient conditions under which there is equivalence .",
    "though these are nt generally known , it is instructive to compare @xmath412 , @xmath129 and @xmath133 the variance upper bound obtained from negative correlation by bounding their differences . for instance , one can show that for any sampling distribution we have : @xmath413 and @xmath414 these bounds are insightful but , without any further assumptions on the sampling distribution , they are not sufficient to prove asymptotic equivalence .",
    "the regular variation hypothesis is an elegant framework , allowing one to derive , thanks to karamata and tauberian theorems , simple and intelligible equivalents for various moments . as we have seen in remark  [ rem : necessary ] , regular variation comes very close to being a necessary condition for exponential concentration .",
    "it may however seem too stringent . without getting too specific ,",
    "let us mention that other less demanding hypotheses also yield the asymptotic relative orders that work in favor of the concentration of the missing mass .",
    "for instance , referring back to remark  [ rem : necessary ] , one could instead ask for : @xmath415 recalling that @xmath416 , and applying corollary  2.6.2 . of bingham _",
    "@xcite , one obtains that @xmath235 is in the class @xmath417 of @xmath418-regularly varying functions and @xmath419 is in the class @xmath420 of extended regularly varying functions , that is , for all @xmath421 @xmath422 and @xmath423 for some constants @xmath55 and @xmath424 . observe that this result , which is the equivalent of karamata s theorem , differs from the regular variation setting , in the sense that the control on the derivative @xmath235 is looser than the one on @xmath234 , whereas , in the karamata theorem , both the function and its derivative inherit the regular variation property .",
    "we can in turn show that @xmath425 is in the class @xmath417 and , by theorem  2.10.2 of bingham _ et al . _",
    "@xcite , this is equivalent to @xmath426 , as @xmath427 is the laplace ",
    "stieltjes transform of @xmath428 .      as noted by gnedin _",
    "@xcite , the asymptotics for the moments of the occupancy counts in the regular variation setting is still valid when the frequencies @xmath0 are random , in which case the measure @xmath216 is defined by @xmath429= \\int_0 ^ 1 f(x)\\nu ( \\mathrm { d}x ) , \\ ] ] for all functions @xmath430 .",
    "we can also define the measure @xmath431 by @xmath432= \\int_0 ^ 1 f(x)\\nu _",
    "1(\\mathrm{d}x ) , \\ ] ] for all functions @xmath430 .",
    "this measure corresponds to the distribution of the frequency of the first discovered symbol .",
    "for instance , when @xmath0 are poisson ",
    "dirichlet(@xmath433,@xmath186 ) with @xmath261 , the measure @xmath434 is the size - biased distribution of @xmath435 , that is beta @xmath436 ( see pitman and yor @xcite ) .",
    "thus , we have : @xmath437&=&\\frac{1}{\\mathcal{b}(1-\\alpha,\\alpha ) } \\int_0^x t^{-\\alpha } ( 1-t)^{\\alpha-1}\\,\\mathrm{d}t \\\\ & \\mathop{\\sim}\\limits_{x\\to0 } & \\frac{x^{1-\\alpha}}{(1-\\alpha)\\mathcal { b}(1-\\alpha,\\alpha)}\\end{aligned}\\ ] ] and , by gnedin _",
    "@xcite , proposition  13 , this is equivalent to @xmath438    thus , denoting by @xmath439 the random number of frequencies @xmath12 which are larger than @xmath3 , the expectation @xmath440 is regularly varying .",
    "one can also show that the mass - partition mechanism of the distribution @xmath441 almost surely generates @xmath439 to be regularly varying . to see this , refer to pitman and yor @xcite , proposition  10 or to bertoin @xcite , proposition  2.6 , which assert that the limit @xmath442 exists almost surely .",
    "this is equivalent to @xmath443    the @xmath435 distribution can be generated through a poisson process with intensity measure @xmath444)=cx^{-\\alpha}$ ] . without entering into further details , let us mention that similar almost sure results hold even when the intensity measure @xmath216 is not a strict power , but satisfies the property @xmath445\\bigr)\\mathop{\\sim}\\limits_{x\\to0 } x^{-\\alpha}\\ell(x ) , \\ ] ] with @xmath86 slowly varying , gnedin @xcite , section  6 . working with a regular variation hypothesis",
    "thus gives us more flexibility than assuming specific bayesian priors .",
    "our variance bounds mostly follow from the efron  stein  steele inequality ( efron and stein  @xcite ) , which states that when a random variable is expressed as a function of many independent random variables , its variance can be controlled by the sum of the local fluctuations .",
    "if @xmath455 are independent copies of @xmath456 , then letting @xmath457 , @xmath458 \\leq\\sum_{i=1}^n \\exp\\bigl[(z - z_i)^2 \\bigr ] , \\ ] ] where the random variables @xmath459 are arbitrary @xmath460-measurable and square - integrable random variables .",
    "the random variables @xmath27 , @xmath19 , and @xmath31 are sums or weighted sums of bernoulli random variables .",
    "these summands depend on the scores @xmath461 and therefore are not independent . transforming the fixed-@xmath11 binomial setting into a continuous time poisson setting is one way to circumvent this problem .",
    "this is the poissonization method . in this setting ,",
    "the score variables @xmath462 are independent poisson variables with respective means @xmath463 .",
    "results valid for the poisson setting can then be transferred to the fixed-@xmath11 setting , up to approximation costs .",
    "for instance , gnedin _ et al .",
    "_ @xcite ( lemma  1 ) provide bounds on the discrepancy between expectations and variances in the two settings .",
    "( see also our discussion in section  [ sec : cost - poisson ] . )",
    "another approach to deal with the dependence is to invoke the notion of negative association , which provides a systematic comparison between moments of certain monotonic functions of the occupancy scores . in our present setting",
    ", this will primarily be useful for bounding the logarithmic moment generating function , which is an expectation of products , by products of expectations , thus recovering the structure of independence .",
    "this has already been used to derive exponential concentration for occupancy counts ( see dubhashi and ranjan @xcite , shao @xcite , mcallester and schapire  @xcite , ohannessian and dahleh @xcite ) .",
    "it is also useful for bounding variances .",
    "we use this notion throughout the proofs , and therefore present it here formally .",
    "real - valued random variables @xmath464 are said to be negatively associated if , for any two disjoint subsets @xmath465 and @xmath466 of @xmath467 , and any two real - valued functions @xmath468 and @xmath469 that are both either coordinate - wise non - increasing or coordinate - wise non - decreasing , we have : @xmath470 \\leq\\exp \\bigl[f(z_a ) \\bigr]\\cdot \\exp \\bigl[g(z_b ) \\bigr].\\ ] ]        as monotonic functions of negatively associated variables are also negatively associated , the variables @xmath130 ( respectively , @xmath472 ) are negatively associated as increasing ( respectively , decreasing ) functions of @xmath473 .",
    "this is of pivotal importance for our proofs of concentration results for @xmath27 and @xmath41 . for @xmath62 ,",
    "the variables @xmath474 appearing in @xmath19 are not negatively associated . however , following ohannessian and dahleh @xcite , one way to deal with this problem is to observe that @xmath475 recalling that @xmath476 is the number of urns that contain at least @xmath2 balls and that the bernoulli variables appearing in @xmath25 are negatively associated .                    moving on to the binomial setting ,",
    "let @xmath492 denote the number of occupied urns when the @xmath13th ball is replaced by an independent copy . then @xmath493 , \\ ] ] where @xmath494 denotes the positive part .",
    "now , @xmath495 is positive if and only if ball @xmath13 is moved from a singleton into in a nonempty urn .",
    "thus @xmath496 $ ] .",
    "proof of proposition  [ prop : varknrinf ] the bound @xmath67 follows from the efron ",
    "stein inequality : denoting by @xmath497 the number of cells with occupancy score larger than @xmath2 when ball @xmath13 is removed , then @xmath498 and thus , we get @xmath499 .        proof of proposition  [ prop : conc - ineq - knrinf ] let @xmath502 denote the occupancy score of cell @xmath503 , then @xmath504 as noted in section  [ sec : negative - association ] , @xmath152 is a sum of negatively associated bernoulli random variables .",
    "moreover , the efron ",
    "stein inequality implies that for each @xmath505 , @xmath506    thus we have @xmath507 where the first inequality comes from negative association , the second inequality is bennett s inequality for bernoulli random variables , and the last inequality comes from the efron ",
    "stein inequality .",
    "the other bound comes from the fact that @xmath508 .",
    "proof of proposition  [ prop : conc - ineq - knr ] as @xmath145 , @xmath509 by proposition  [ prop : conc - ineq - knrinf ] , bernstein inequalities hold for both @xmath25 and @xmath510 , with variance proxies @xmath511 ( or @xmath68 ) and @xmath512 ( or @xmath513 ) respectively .",
    "hence , @xmath514 the same reasoning works for the alternative variance proxies and for the left tails .",
    "proof of proposition  [ prop : bennett : left : mn0 ] for all @xmath150 , @xmath517 & = & \\log\\exp \\bigl [ \\mathe^{\\lambda\\sum_{j=1}^\\infty p_j ( y_j-\\exp y_j ) } \\bigr ] \\\\ & \\leq & \\sum_{j=1}^\\infty\\log\\exp \\bigl [ \\mathe^{\\lambda p_j ( y_j - \\exp[y_j ] ) } \\bigr ] \\\\ & \\leq & \\sum_{j=1}^\\infty(1-p_j)^n \\bigl(1-(1-p_j)^n\\bigr ) \\phi(\\lambda p_j ) , \\end{aligned}\\ ] ] where the first inequality comes from negative association , and the second is bennett s inequality for bernoulli random variables .",
    "thus , for @xmath522 , @xmath517 & \\leq & \\sum _ { j=1}^\\infty p_j^2 ( 1-p_j)^n\\bigl(1-(1-p_j)^n \\bigr ) \\frac{\\lambda^2}{2 } \\\\ & = & \\sum_{j=1}^\\infty p_j^2 \\var[y_j ] \\frac{\\lambda^2}{2}.\\end{aligned}\\ ] ] recall that @xmath523 \\leq2 \\exp k_2(n ) /n^2 $ ] ( proposition  [ prop : vari - missingmass ] ) .",
    "proof of proposition  [ prop : log - laplace - missing - mass ] from the beginning of the proof of proposition  [ prop : bennett : left : mn0 ] , that is , thanks to negative association and to the fact that each bernoulli random variable satisfies a bennett inequality , @xmath162 \\leq \\sum _ { j=1}^\\infty\\mathe^{-np_j } \\phi ( \\lambda p_j).\\ ] ] now , using the power series expansion of @xmath524 , @xmath525 we recognize that for each @xmath2 , @xmath526 , so that @xmath517&\\leq & \\sum _ { r=2}^\\infty \\biggl(\\frac{\\lambda}{n } \\biggr)^r\\exp k_r(n).\\end{aligned}\\ ] ]    proof of theorem  [ thmm : struct - ineq - right ] using proposition  [ prop : log - laplace - missing - mass ] and noticing that for each @xmath164 , @xmath527 , we immediately obtain that @xmath517&\\leq & \\exp k_{\\overline{2}}(n ) \\sum_{r=2}^\\infty \\biggl(\\frac{\\lambda } { n } \\biggr)^r \\\\ & = & \\lambda^2 \\frac { \\exp k_{\\overline{2}}(n)/n^2}{1-\\lambda / n } , \\end{aligned}\\ ] ] which concludes the proof .",
    "proof of proposition  [ prop : asymp - cumulated - occup ] by monotonicity of @xmath25 , we have the following strong law for any sampling distribution @xmath528 ( see gnedin _",
    "@xcite , the discussion after proposition  2 ) .",
    "recall that @xmath529 and that , if @xmath530 , then @xmath531 = \\frac{\\gamma ( k+1,\\lambda)}{k!}$ ] , where @xmath532 is the incomplete gamma function . hence @xmath533 \\\\ & = & \\sum_{j= 1}^\\infty\\frac{1}{(r-1)!}\\int _ 0^{np_j } \\mathe^{-t}t^{r-1 } \\,\\mathrm{d}t \\\\ & = & \\frac{1}{(r-1 ) ! } \\int_0 ^ 1 \\int _ 0^{nx } \\mathe^{-t}t^{r-1 } \\mathrm { dt}\\cdot\\nu(\\mathrm{d}x ) \\\\ & = & \\frac{1}{(r-1 ) ! } \\int_0 ^ 1 n \\mathe^{-nx}(nx)^{r-1}\\vec\\nu(x)\\ , \\mathrm { d}x \\\\ & = & \\frac{1}{(r-1)!}\\int_0^{+\\infty } \\mathe^{-z}z^{r-1}\\vec\\nu ( z / n)\\,\\mathrm{d}z \\\\ & \\mathop{\\sim}\\limits_{+\\infty } & \\frac{\\vec\\nu(1/n)}{(r-1)!}\\gamma(r-\\alpha).\\end{aligned}\\ ] ]          we explore more carefully the structure of @xmath163 and show that these quantities are uniformly ( in @xmath2 ) bounded by a function of order @xmath539 for large enough @xmath11 , that is , that there exists @xmath540 and @xmath541 such that for all @xmath542 , for all @xmath62 , @xmath543 .    before going into the proof",
    ", we observe that for @xmath544 , the result is true . indeed , from the identity @xmath545",
    ", we deduce that @xmath546 , so that for @xmath544 , @xmath547",
    ". thus , we assume that @xmath548 .        for the contribution of the symbols with probability larger than @xmath74 , integration by part and change of variable yield :",
    "@xmath553_{1/n}^1 + \\int_{1/n}^1 \\mathe^{-nx } \\frac{n^r}{r!}\\bigl(rx^{r-1}-nx^r \\bigr)\\vec\\nu ( x)\\,\\mathrm { d}x \\\\ & = & \\frac{\\vec\\nu(1/n)\\mathrm{e}^{-1}}{r ! } + \\int_1^\\infty \\mathe^{-z } \\biggl(\\frac { z^{r-1}}{(r-1)!}-\\frac{z^r}{r ! } \\biggr)\\vec \\nu(z / n)\\,\\mathrm{d}z.\\end{aligned}\\ ] ]      notice that when @xmath556 $ ] , the integrand is negative , so we simply ignore this part of the integral and restrict ourselves to @xmath557 which we try to bound by a constant term for @xmath11 greater than some integer that does not depend on @xmath2 .",
    "the main ingredient of our proof is the next version of the potter ",
    "drees inequality ( see theorem  [ thmm : potter : rv ] in section  [ sec : potter ] and de haan and ferreira @xcite , point @xmath558 of corollary b.2.15 ) : for @xmath482 , for arbitrary @xmath559 , there exists @xmath484 such that for all @xmath485 , and for all @xmath560 with @xmath561 , @xmath562      as @xmath548 , taking , if necessary , @xmath11 large enough so that @xmath569 , we have @xmath570 and @xmath571 with @xmath572 where @xmath573 is the incomplete gamma function . using the fact that @xmath574 , we have @xmath575    by stirling s inequality , for all @xmath2 , @xmath576 thus , taking @xmath577 , the right - hand term is uniformly bounded by @xmath212 . and @xmath578 is also bounded by 1 . thus @xmath579 and @xmath580 by stirling s inequality , this bound is smaller than @xmath581 , which tends to @xmath186 as @xmath582 . thus , there exists @xmath583 such that for all @xmath584 , and all @xmath570 , @xmath585 .",
    "proof of proposition  [ prop : potpourri ] under the condition of the proposition  [ prop : potpourri ] , from gr \" ubel and hitczenko @xcite , with probability tending to @xmath212 , the sample is gap - free , hence the missing mass is @xmath594 .",
    "proof of corollary  [ cor : lgnmissingmass ] let us assume that @xmath597 . using the fact that @xmath598 , we notice that as soon as @xmath599 , @xmath600 . now by chebyshev s inequality , @xmath601 & \\leq&\\frac{\\var(m_{n,0})}{\\varepsilon^2 ( \\exp m_{n,0})^2 } \\leq \\frac{2\\exp k_2(n)}{\\varepsilon^2n^2(\\exp m_{n,0})^2 } \\\\ & \\sim & \\frac{2(\\exp k_{n,2 } + o(1))}{\\varepsilon^2 ( \\exp k_{n,1})^2 } , \\end{aligned}\\ ] ] where we used that @xmath602 ( see lemma  1 , gnedin _ et al . _ @xcite ) . on the other hand , @xmath603 \\leq \\frac{\\var(k_{n,1})}{\\varepsilon^2 ( \\exp k_{n,1})^2 } \\leq\\frac{\\exp k_{n,1}+2\\exp k_{n,2}}{\\varepsilon^2(\\exp k_{n,1})^2 } , \\ ] ] showing that if , furthermore , @xmath604 remains bounded , the ratios @xmath605 , @xmath606 and thus @xmath607 converge to @xmath212 in probability . to get almost sure convergence , we use theorem  [ thmm : struct - ineq - right ] to get that when @xmath597 , @xmath601 & \\leq & 2\\exp \\biggl(-\\frac{\\varepsilon^2(\\exp m_{n,0})^2}{2(2\\exp k_{\\overline{2}}(n)/n^2 + \\exp m_{n,0}/n ) } \\biggr ) \\\\ & = & 2\\exp \\biggl(-\\frac{\\varepsilon^2(\\exp k_{n,1}+o(\\exp k_{n,1}))^2}{2(2\\exp k_{n,\\overline{2 } } + \\exp k_{n,1 } + o(\\exp k_{n,1 } ) ) } \\biggr).\\end{aligned}\\ ] ] if @xmath608 remains bounded , this becomes smaller than @xmath609 . hence ,",
    "if @xmath610 is summable for all @xmath113 , we can apply the borel ",
    "cantelli lemma and obtain the almost sure convergence of @xmath605 to @xmath212 .",
    "moreover , by proposition  [ prop : conc - ineq - knr ] , @xmath611 & \\leq & 4\\exp \\biggl(-\\frac{\\varepsilon^2(\\exp k_{n,1})^2}{2(4\\max(\\exp k_{n,1 } , 2\\exp k_{n,2})+2/3 ) } \\biggr ) , \\end{aligned}\\ ] ] which shows that under these assumptions @xmath612 also tends to @xmath212 almost surely .",
    "proof of proposition  [ prop : conf - missing ] with probability greater than @xmath618 , by proposition  [ prop : conc - ineq - gtt-1 ] , @xmath619 and @xmath620 we may now invoke concentration inequalities for @xmath621 and @xmath45 . indeed , with probability greater than @xmath622 , @xmath623 which entails @xmath624 .",
    "the proof for the binomial setting is very similar , the only difficulty being that @xmath638 and @xmath364 are no longer equal .",
    "however , the bias becomes negligible with respect to the fluctuations , that is , for @xmath270 either @xmath639 or @xmath640 @xmath641        acharya , j. , jafarpour , a. , orlitsky , a. , and suresh , a. t. ( 2013 ) .",
    "optimal probability estimation with applications to prediction and classification . in _ colt 2013 .",
    "j. of mach . learn .",
    "research  proc",
    ". track _ * 30 * 764796 ."
  ],
  "abstract_text": [
    "<S> an infinite urn scheme is defined by a probability mass function @xmath0 over positive integers . </S>",
    "<S> a random allocation consists of a sample of @xmath1 independent drawings according to this probability distribution where @xmath1 may be deterministic or poisson - distributed . </S>",
    "<S> this paper is concerned with occupancy counts , that is with the number of symbols with @xmath2 or at least @xmath2 occurrences in the sample , and with the missing mass that is the total probability of all symbols that do not occur in the sample . without any further assumption on the sampling distribution , </S>",
    "<S> these random quantities are shown to satisfy bernstein - type concentration inequalities . </S>",
    "<S> the variance factors in these concentration inequalities are shown to be tight if the sampling distribution satisfies a regular variation property . </S>",
    "<S> this regular variation property reads as follows . </S>",
    "<S> let the number of symbols with probability larger than @xmath3 be @xmath4 for @xmath5 $ ] and the variance of the number of distinct symbols in a sample tends to infinity as the sample size tends to infinity . among other applications , these concentration inequalities allow us to derive tight confidence intervals for the good  turing estimator of the missing mass .    </S>",
    "<S> ./style / arxiv - general.cfg    ,     + </S>"
  ]
}