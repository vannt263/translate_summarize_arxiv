{
  "article_text": [
    "privacy concerns are becoming more and more acute , especially in the digitized world where new supercomputers with an increasing processing capacities appear almost every day .",
    "these new machines together with impressive new technologies make the process of data collection , data storing and data analysis as easy as ever .",
    "this `` ease of use , '' may be manipulated by untrustful elements , whose aim is to deliberately cause harm by , for example , identifying and exposing sensitive data .",
    "it is the goal of privacy preserving methods to prevent or at least lessen the chances of such harmful actions from happening . in this paper",
    "we present a novel way to achieve the goal when a certain statistical analysis is required .    preserving the privacy of individual databases when carrying out statistical calculations",
    "has a long history in statistics and had been the focus of much attention in machine learning , e.g. , see  @xcite . once data are merged across sources , however , privacy becomes far more complex and a number of privacy issues arise for the linked individual files that go well beyond those that are considered with regard to the data within individual sources .",
    "when the goal is the production of the results of some statistical calculation , such as a regression analysis , c.f .",
    "karr et al .",
    "@xcite , we can often exploit results from the cryptography literature , borrowing tools such as secure multi - party computation , e.g. , see  @xcite .",
    "secure multi - party protocols are concerned with distributed computation where each participating party , holding a private input , learns nothing but the result ( see section [ sec : smpc ] ) .",
    "this paper has two main themes . in both themes",
    "we conceptualize the existence of a single combined database containing all of the information for the individuals in the separate databases and for the union of the variables .",
    "we propose an approach that gives full statistical calculation on this combined database without actually combining information sources , see  @xcite .",
    "we focus mainly on logistic regression , but our methods and tools are essentially adaptable to other statistical models , as indicated in section [ sec : extension ] .",
    "our approach provides only the final result of the calculation compared with other methods that share intermediate values and thus present an opportunity for compromise of values in the combined database , c.f .",
    "we remark that our problem differs from the one studied by chaudhuri and monteleoni  @xcite using differential privacy , since they are concerned with information leakage by the output of the computation , whereas we are concerned with leakage from the computation itself ! the first theme is the development of a novel approach to perform the calculations required for fitting logistic regression models when the data are distributed among several parties . in our settings",
    "the parties are unwilling or are simply forbidden ( by law regulations ) to share their respective data .",
    "they acknowledge the fact that pooling their private data into a conceptual global database , and running the logistic regression on the pooled data , rather than on their own data , can only lead to a better statistical analysis .",
    "we develop a secure protocol to compute the maximum likelihood estimates of the logistic parameters . throughout the paper",
    "we make repeated use of what is known as random secret sharing , which enables us to keep intermediate parameter values secret .",
    "the first theme aims at performing the required calculations by using operations which are restricted to a linear algebra type .",
    "note that the fitting process requires computing the logistic function which is highly non - linear . in principle",
    ", we may perform any computation securely , by making use of yao s general protocol @xcite .",
    "nonetheless , this is in essence a theoretical construction which will often be inefficient for large computations @xcite .",
    "instead , we craft a specially designed approximation to the logistic function , which can be securely evaluated using the machinery of random shares and yao s millionaire protocol .",
    "we establish the theoretical validity of the secure protocol for computing the logistic parameters , and show its performances in practice . in high dimensional problems with large number of cases",
    "our protocol may require faster computing resources .",
    "this is mainly because our approximation requires computing the predicate `` greater - than , '' which may take many encryptions . indeed , evaluating this predicate by a reduction to yao s protocol takes roughly @xmath0 encryptions where @xmath1 is the number of bits used to represent the numbers ( this becomes dauntingly large due to the secret sharing scheme ) .",
    "this leads us to the second theme , which tries to amend the protocol is a way that speeds up the computation of the logistic function .",
    "the main idea here is to avoid special circuit sub - protocols , such as yao s protocol . to that end , we show that we can perform the fitting process using only sums and products .",
    "the advantage to this is that these computations are very well studied primitives in secure multiparty computation and thus we can instantiate our method in a different secure multiparty computation scheme ( e.g. , @xcite ) , depending on the security demands of the data holders .",
    "we propose to approximate the vector of logistic function values , by repeatedly taylor expanding around the current value and stepping along the gradient .",
    "operations other than sums and products , are not needed here . in principle",
    ", the approach represents the logistic function as the solution to a ordinary differential equation , then applies euler s method to approximate the solution . as with the first theme ,",
    "we show that we can make the approximation arbitrarily accurate , at the expense of computational efficiency , and we present an illustrative empirical result .",
    "we close the introduction with a brief description of logistic regression , mainly for the purpose of setting notation .",
    "logistic regression is used for predicting binary outcomes or class membership given a set of explanatory variables or predictors .",
    "we can use the fitted model to predict class membership for a newly obtained record consisting of only the values of the predictors .",
    "the basic framework of logistic regression treats binary responses @xmath2 as realizations of @xmath3 independent bernoulli random variables , @xmath4 , whose mean depends on a set of predictors @xmath5 , as follows : @xmath6 where @xmath7 , is the sigmoid ( or the logistic ) function , and @xmath8 is a @xmath9-dimensional parameter vector .",
    "this makes the log odds , @xmath10 , linear in the predictors .",
    "a standard method for computing the maximum likelihood estimates of @xmath8 is newton - raphson s method , since closed form expressions do not exist .",
    "the fitting process requires the user to supply the log - likelihood function associated with logistic regression , along with its first two derivatives . suppressing dependence on the data and vector of parameters , we let @xmath11 be the log - likelihood , i.e. , @xmath12 .",
    "we also put on record the first two derivatives :    @xmath13    the gradient and the hessian are assembled together to produce an estimate of the logistic parameters through the iterative process : @xmath14 our protocol will be structured in rounds , where each round corresponds to an iteration of newton s method ( [ eq : nr ] ) followed by a convergence check .",
    "each round involves a loop through all the cases @xmath15 to compute the contribution to the gradient and hessian .",
    "we keep intermediate values of @xmath16 unshared between the parties .",
    "this is made possible by representing @xmath16 as random shares ( see section [ sec : blocks ] ) .",
    "the remainder of the paper is organized as follows .",
    "section [ sec : smpc ] presents the multi - party setup . in section [ sec : blocks ] we provide several sub - protocols which we will need .",
    "sections [ sec : protocol1 ] and [ sec : protocol2 ] describe our protocol and an approach for speeding up the calculation involved , respectively .",
    "section [ sec : security ] describes implementation details .",
    "section [ sec : experiment ] illustrates aspects of the computation on an extract of data from the current population survey divided between two parties .",
    "section [ sec : extension ] discusses possible extensions .",
    "we defer all technical details to appendices [ sec : validity_1 ] and [ sec : validity_2 ] .",
    "we let @xmath17 denote the @xmath18 design matrix , and @xmath19 the @xmath3-dimensional response vector .",
    "we assume the presence of @xmath20 parties who are interested in computing logistic regression on the total of their data .",
    "we suppose that the union of the parties data corresponds to the @xmath17 and @xmath19 of the logistic regression .",
    "in particular , we suppose that party @xmath21 holds onto the pair @xmath22 with @xmath23 and @xmath24 , where @xmath25 is the @xmath26 party design matrix , and @xmath27 is her ( binary ) response vector .    in this work we consider a setting where each party has an `` additive share '' of the dataset .",
    "that is , @xmath28 and @xmath29 where @xmath17 and @xmath19 correspond to the design matrix and response vector of the combined data on which the logistic regression is performed .",
    "this subsumes all the partitioning schemes for the database ( e.g. , vertical and horizontal partitioning which are the cases considered in @xcite ) as in these cases for each element , one party holds the value and the remaining parties hold zero .",
    "furthermore this setup is applicable in a case where parties may have overlapping data , and the logistic regression is to be learned by using a linear function of the overlapping data ( e.g. , a weighted average ) as a kind of measurement error model .",
    "we suppose that the union of the individual data sets gives the complete data . in cases where some data are missing",
    ", we can apply a privacy preserving imputation method such as in @xcite as a preprocess , and then run our protocol .",
    "we note that our method is general in the sense that it is applicable to every partitioning scheme , but it is clearly possible to treat specific cases such as vertically partitioned data with more efficient specialized protocols .",
    "ideally we would like our method to provide only the output of the calculation to the parties involved , and reveal nothing more .",
    "this is a lofty goal without the aid of trusted third parties , however it is relaxed in a useful way in the cryptographic literature .",
    "first it is assumed that the parties are not able to quickly solve computationally hard problems ( such as breaking rsa encryption ) .",
    "then , a protocol is secure so long as intermediate values in the computation either contain almost no information ( in the sense that the protocol would have to be re - run astronomically many times on the same input data in order to detect any information in the messages ) , or will only reveal information as the result of an intractable computation .",
    "we now briefly review the security model we intend to use .",
    "we consider the `` functionality '' ( see @xcite ) which maps the data of each party into the logistic regression parameter vector @xmath8 :    @xmath30    the right hand side represents @xmath31 copies of the parameter , so that each party receives the same output .",
    "note that each design matrix is of the same dimensions .",
    "a protocol for computing the functionality is just a sequence of steps , consisting of parties performing local computations , and sending intermediate messages to each other . in this work",
    "we build up a protocol for computing ( [ lr - fun ] ) which is secure in the presence of `` semi - honest '' parties .",
    "that is , parties who obey the protocol ( and do not try to e.g. , inject malformed data ) but keep a transcript of all the messages they receive .",
    "intuitively , a protocol is secure in this setting whenever the intermediate messages give no information about the secret inputs of other parties .",
    "formally , the `` view '' of the @xmath26 party during the protocol is :    @xmath32    where @xmath33 is a record of all the random draws made by party @xmath21 , and @xmath34 is the @xmath35 message received by that party ( we have dropped dependence of @xmath36 on @xmath21 for readability ) .",
    "the protocol is secure so long as there exists a polynomial time algorithm which , when given only the input and output of party @xmath21 , may output a random transcript of message which is _ computationally indistinguishable _ from @xmath37 .",
    "see @xcite for a definition and discussion of computational indistinguishability .",
    "in essence , if the distribution of the sequence of messages depends only on the private input and output of party @xmath21 then we can simulate messages by drawing from this distribution ( so long as the random number generator returns samples which are computationally indistinguishable from draws from the distribution ) .",
    "the existence of a simulator shows that intermediate messages do not depend on the private input of other parties , and so the protocol is secure in the sense that parties gain no more information about each other s private inputs than that revealed by the output of the protocol .",
    "note that this type of security is `` orthogonal '' to that studied in @xcite , which seeks to prevent leakage of secret information in the parameter vector .",
    "an example of a protocol which does not achieve this definition of security is one where all parties send their data to party 1 , who computes the parameter locally on the combined data and then sends it back to all other parties . in this case",
    "the messages received by party 1 consist of the data of other parties , in general it is impossible to simulate these messages given only the input and output belonging to party 1 .    in the next section we present a protocol for performing newton s method on the logistic regression objective in a way which is secure in the presence of semi - honest parties .",
    "our protocol makes use of a specially designed approximation for the logistic function .",
    "section [ sec : protocol2 ] then describes a different approximation necessitating the operations of only sums and products , and thus speeding - up the computations .",
    "although we propose to use the cryptographic model for security , others exist and deserve a place in the theory of privacy preserving data analysis .",
    "the main alternatives we see are `` weak '' security , and _ perturbation _ of the data .",
    "the former comprises a body of literature summarized in @xcite .",
    "the idea is that by giving weaker privacy guarantees , we can implement much more efficient protocols .",
    "whether it is acceptable to have this weaker privacy guarantee is a question which one must consider on a case - by - case basis .",
    "although we describe our protocol in terms of the cryptographic model , by replacing the primitive operations ( in section  [ sec : blocks ] ) with their weakly - secure counterparts , we convert our protocol into a weakly secure ( but also computationally more efficient ) one .",
    "the second alternative is data perturbation or sanitization .",
    "the idea would be for each party to somehow perturb his data until he is happy to release it to the other parties ( e.g. , through the addition of random noise ) .",
    "thereupon the parties would each have a noisy copy of all the data , and could locally compute whatever statistical method they wanted on the union of the data .",
    "the difficulty with this approach is that to protect privacy may require the addition of noise of such amplitude as to render the data itself useless .",
    "in this section we lay out some primitives and sub - protocols which we will commbine to make a full logistic regression protocol . while , details of the implementation of these primitives are in the references cited , we also include some in the appendix .      in our construction",
    "we make extensive use of additive secret sharing .",
    "the idea is to divide a quantity of interest @xmath38 into @xmath31 random numbers @xmath39 ( one for each party ) so that @xmath40 .",
    "if the @xmath39 are distributed uniformly in the field ( e.g. , the entirety of @xmath41 ) then any subset of the @xmath39 will reveal nothing about @xmath38 .",
    "in fact the sum over any subset is a random variable , the distribution of which does not depend on the secret value .",
    "we use this construction to keep all intermediate quantities secret during the evaluation of newton s method ( i.e. , the gradient , hessian and intermediate parameter vectors ) .",
    "as long as we can construct sub - protocols which compute random shares of a quantity , from random shares of inputs , then we can compose these sub - protocols together to finally obtain random shares of the logistic regression estimate . with these in hand the parties",
    "can then exchange shares and reveal the vector itself .",
    "although the joint distribution of the @xmath39 concentrates on the linear subspace corresponding to the secret value , marginally the shares are uniformly distributed and do not depend on any parameters .",
    "hence we can easily simulate messages based on these shares since the marginal distributions are known , and we achieve security as defined in section  [ sec : smpc ] .",
    "next we show how to compute additive shares of all the intermediate quantities using the abstract definition of additive shares .",
    "although this approach is intuitively appealing , computers would quickly run into problems representing samples drawn uniformly from @xmath41 .",
    "therefore , in the appendix we show how to approximate arbitrarily well the same computations in modular arithmetic on @xmath42 for some large @xmath43 .      to implement newton s method we must essentially perform linear algebraic operations on random shares , for example by computing shares of the newton step from shares of the gradient and inverse hessian . in this section",
    "we describe how to obtain random shares of sums and products of quantities that are themselves represented as random shares . using these constructions ,",
    "we compute inner and outer products of vectors of random shares , and hence also matrix multiplies .",
    "computing shares of the sum of two secret quantities @xmath44 and @xmath45 is direct , as it involves only the local computation @xmath46 for each party @xmath47 . that is , party @xmath21 simply adds his shares @xmath39 and @xmath48 together to get a random share of the quantity @xmath49 .",
    "obtaining random shares of the product of two secret quantities is more involved :    @xmath50    the elements of the first sum on the right hand side can be computed locally by each party .",
    "the second ( double ) sum , however , requires products between random shares held by different parties . to obtain these terms",
    "while maintaining the security of the protocol , we turn to _ oblivious function evaluation_. that is , we pose the problem of computing the product as evaluating a function so that one party only knows the function and the other party only knows his input and the value of the function applied to that input .",
    "the function set up by party @xmath21 and evaluated by party @xmath51 on his input , @xmath52 , is : @xmath53    where @xmath54 is a quantity generated uniformly at random by party @xmath21 .",
    "evaluation is done in a manner so that party @xmath21 learns nothing of the output ( and thus only learns about @xmath54 which he generated ) and party @xmath51 learns only the output . since party @xmath51 does not know the value of the random variable @xmath54 he has learned potentially nothing about the true value of the product .",
    "taking @xmath55 and @xmath56 we have that @xmath57 , and thus they form random shares of the product @xmath58 .",
    "once parties compute random shares for all the terms @xmath58 , they can locally compute random shares of @xmath59 as : @xmath60 summing up these quantities , and utilizing the definition of the linear function set up in ( [ prod - share - fn ] ) , we easily obtain : @xmath61 which shows that the @xmath62 s in ( [ eq : prod_shares ] ) are indeed ( additive ) shares of the product .",
    "this protocol generates random shares of the product even if the original shares were nt themselves random , e.g. , if they were due to the partitioning of the data . a method which implements these encrypted multiplications using fixed - point arithmetic",
    "is given in @xcite .",
    "we also note that dividing one secret value into another securely is much more difficult than dealing with products and requires more elaborate ( and computationally demanding ) protocols .",
    "below we show how matrix inversion can be performed without any divisions .",
    "we suppose we are able to evaluate the following predicate in a secure way : @xmath63 where @xmath64 are secret values held by separate parties .",
    "this is known as yao s `` millionaires problem , '' since he described it in the context of determining which millionaire has the most money , without disclosing actual bank balances .",
    "an example of a protocol which computes this predicate is given by @xcite .",
    "we can also trivially extend it so that each party receives a random share of the output bit ( i.e. , each party receives a random bit , the `` xor '' of which yields the correct output bit ) . using this technique we can also check whether a secret value ( i.e. , a sum of random shares ) is greater or less than some constant :    @xmath65    where @xmath66 are the random shares of @xmath38 held by two parties .",
    "we use a matrix inversion routine built up entirely of matrix multiplications and subtractions , thus allowing us to use the constructions of the preceding sections to implement it securely .",
    "we obtain the reciprocal of a number @xmath38 without necessitating any actual division by an application of newton s method to the function @xmath67 .",
    "iterations follow @xmath68 , which requires multiplication and subtraction only .",
    "it turns out that we can apply the same scheme to matrix inversion , e.g. , see  @xcite and references therein .",
    "a numerically stable , coupled iteration for computing @xmath69 , takes the form : @xmath70 where @xmath71 , and @xmath72 is to be chosen by the user .",
    "a possible choice , leading to a quadratic convergence of @xmath73 @xmath74 , is @xmath75 . in our actual implementation",
    "we used instead the trace ( which dominates the largest eigenvalue , as the matrix in question is positive definite ) , since we can compute shares of the trace from shares of the matrix locally by each party .",
    "to compute @xmath76 we use the same iteration , with scalars instead of matrices . for this iteration",
    "we initialize with an arbitrarily small @xmath77 ( as convergence depends on the magnitude of the initial value being lower than that of the inverse we compute ) .",
    "we use the constructions of section  [ sec_la ] to iterate through ( [ matrix - inv ] ) until convergence . as @xmath78",
    ", we check for convergence by considering the absolute difference between the trace of @xmath79 and the data dimension @xmath9 , and we can evaluate the function @xmath80 on random shares of the trace of @xmath79 using the same form as ( [ yaos - gt - eqn ] ) .",
    "we recall the usual newton - raphson iteration expression ( [ eq : nr ] ) . to perform the iteration",
    "we first compute random shares of the update direction : @xmath81 , via the formulation of matrix - vector products of random shares .",
    "we can then add these random shares to the current parameters @xmath16 to obtain random shares of @xmath82 . to check convergence recall ( from e.g. ,",
    "@xcite ) we should end if : @xmath83 we can compute ( [ convergence ] ) securely using the same form as ( [ yaos - gt - eqn ] ) .",
    "the result is sharable among all the parties , and the protocol ends whenever the result is 0 , i.e. , when @xmath84 is not greater than @xmath85 .    by using the constructions of the previous section",
    ", we have the tools required to invert shares of the hessian , and thus to compute the newton step .",
    "all that we need to do is construct a secure protocol to evaluate the logistic ( sigmoid ) function . in principle , a specialized sub - protocol could be built up using the construction of yao @xcite .",
    "the method would be to construct circuit that evaluates the sigmoid function in the same manner that the arithmetic logic unit in a cpu would . then we could give this circuit the secure treatment and make it into a protocol following @xcite . the disadvantage with this approach",
    "is that the circuit evaluation protocols are prohibitively expensive and thus they are not useful in practice except for trivial circuits , see e.g. , @xcite .",
    "instead we use a specially crafted approximation to the logistic function in terms of indicator functions .",
    "we describe this next .",
    "the logistic function itself is the cdf of the logistic distribution .",
    "we propose to approximate this function with an `` empirical cdf . ''",
    "this is a function of a set of @xmath86 samples @xmath87 , taken independently from a logistic distribution :    @xmath88    based on the glivenko - cantelli theorem , and later work by dvoretzky , kiefer and wolfowitz , the rate at which the empirical cdf converges to the true cdf ( i.e. , the logistic function which is of interest ) is known . using these results , we obtain bounds on the maximum difference between the logistic function and our approximation , which hold with high probability .",
    "see the remark below in section  [ sec : ecdf ] about the accuracy of the approximation .",
    "we now turn attention to obtaining random shares of the logistic function evaluated at random shares of @xmath89 .",
    "we obtain random shares of @xmath89 by using the inner product construction for multiplying together random shares .",
    "if we denote shares of this inner product by @xmath90 for party @xmath21 , we write : @xmath91 thus the problem reduces to getting random shares of the sum of indicators .",
    "note that we can re - write each indicator function as : @xmath92    if party 2 generates the logistic random variables then we have a trivial reduction to ( [ yaos - gt - eqn ] ) . in order to restrict the view of either party to a random share",
    ", we restrict the output to random bits @xmath93 , and @xmath94 , such that @xmath95 where @xmath96 is the exclusive or .",
    "the right - hand side of equation ( [ eq : sigma - approx2 ] ) requires ( random shares of ) the fraction of outputs with @xmath97 .",
    "this can be established by noticing that @xmath98 where we denote @xmath99 for @xmath100 .",
    "jagannathan and wright  @xcite use this method to convert xor shares into additive shares for a different privacy - preserving task .    in order for the output to behave this way",
    ", we can either use yao s protocol directly , or take a ( more efficient ) gt protocol and modify it to give a ( xor ) random share . in this work we use the protocol of @xcite .",
    "having computed random shares of the logistic function , we can use the constructions of section  [ sec_la ] to compute random shares of the gradient and hessian , and hence build a full logistic regression protocol .      a comment about the accuracy of approximation ( [ sigma - appox ] ) , and the resulting logistic parameter estimator is in order .",
    "the tail behavior of the sup - norm of the error is given , for every @xmath77 , by : @xmath101 known as the dvoretzky - kiefer - wolfowitz ( dkw ) inequality .",
    "one possible way to choose the number of logistic variables @xmath86 in practice , is by ensuring that the above probability is no more than a prescribed level of accuracy , say @xmath102 . solving for @xmath86 we obtain the ( very conservative ) bound @xmath103 .",
    "a less conservative bound might entail the maximum absolute error restricted to some interval ( containing the origin ) .",
    "we relate error in the approximation of the sigmoid ( and hence the gradient ) to the error in the convergent parameter by the following inequality :    @xmath104 is the optimizer of the exact log likelihood , and @xmath8 is the optimizer of our approximation , @xmath105 is the smallest eigenvalue of the fisher information matrix @xmath106 ( on some interval , see appendix  [ sec : validity_1 ] ) , and @xmath107 is the radius of a ball which containing all the data vectors ( i.e. , @xmath108 ) .",
    "the proof of this inequality follows lemma 1 of @xcite in which the two convex functions are the exact log likelihood objective , and the difference between the exact and approximate objectives .",
    "see appendix  [ sec : validity_1 ] for detailed theoretical derivation .",
    "since we can use expression ( [ dkw_ineq ] ) to bound the numerator of expression ( [ parm_err_bound ] ) , the parameter output by our protocol , and that output by the exact ( non - private ) algorithm can be brought arbitrarily close ( except on a set of negligible probability ) by increasing the parameter @xmath86 .",
    "later , we perform an experiment to show how well the method performs with reasonably small @xmath86 . note that for newton s method to converge in this approximation , we must use the same sample of @xmath86 logistic random variables each time we approximate the sigmoid . otherwise assessing convergence",
    "would be difficult as the objective function would be constantly shifting .",
    "we propose that the parties draw @xmath86 logistic variables ahead of time , and use these for all the computations .",
    "notice that the newton raphson method requires inverting a matrix ( the hessian of the log likelihood ) at each iteration . in our setting , using our iterative inversion method this becomes very expensive .",
    "therefore we propose to use a well - studied approximation @xcite , which replaces the iteration by :    @xmath109    first note that under this technique the algorithm only ever needs a single matrix inversion , since @xmath110 is constant throughout all the iterations .",
    "second , this algorithm still eventually converges to the correct parameter value ( modulo the other approximations we make in our protocol ) .",
    "the reason is that the inverse hessian is always greater than @xmath111 , in the sense that the difference is positive semi - definite , see e.g , minka @xcite for more details .",
    "what s more , this technique ensures that progress towards the optimum is monotonic , and so assessing convergence may be simpler .",
    "first we count how many times we must run each of our primitives for each iteration of newton s method .",
    "the approximation of section  [ sec : rs_of_sigma ] requires @xmath112 instances of the gt protocol per round , as @xmath86 instances are required per case .",
    "computing the gradient and the hessian requires @xmath113 multiplications .",
    "inverting the hessian takes @xmath114 multiplies and one gt per iteration of ( [ matrix - inv ] ) .",
    "since this inner iteration is quadratically convergent , it takes @xmath115 iterations to converge , and thus takes @xmath116 multiplies and @xmath115 instances of gt . in total",
    "then , each outer iteration takes @xmath117 multiplies , and @xmath118 invocations of the gt protocol .",
    "each multiplication requires a number of encryptions and decryptions ; this scales quadratically with the number of parties @xmath31 since they must exchange with one another .",
    "thus the computational workload increases as the data are split into more pieces .",
    "note that although repeated use of the cryptosystem is quite expensive , performance on normal hardware is relatively rapid .",
    "a machine dedicated to the computation and running multiple threads can do thousands of encryptions per second .",
    "each instance of gt using the protocol of @xcite requires @xmath119 encryptions and decryptions ( and operations on encrypted values etc . ) .",
    "therefore in total our approximation of section  [ sec : rs_of_sigma ] requires @xmath120 encryptions per iteration .",
    "this may be too computationally demanding for large @xmath86 .",
    "one way to reduce this cost is to run the scheme using a coarse approximation to the sigmoid ( i.e. , a small @xmath86 ) to convergence , then increase @xmath86 , resample the logistic variables and then continue newton s method from the previous convergent parameter .",
    "although the latter iterations will still be computationally burdensome , there will be fewer of them .",
    "another way is to use a different approximation to the sigmoid function .",
    "this is outlined next in section [ sec : protocol2 ] .",
    "note that the total amount of communication by all parties is also proportional to the number of multiples and gt invocations . for an invocation of",
    "either , a party must transmit @xmath121 bits to another party , and then receive a message of the same length .",
    "there are a total of @xmath122 messages which must be sent for each iteration .",
    "if the number of parties or cases , or the granularity of the approximation is large , running the protocol over a high speed local area network would make the communication overhead manageable .",
    "as we mentioned above , the computation complexity of evaluating approximation ( [ sigma - appox ] ) to the logistic function scales linearly with @xmath86 , since on each of newton s iteration we invoke yao s protocol to compute the gt predicate , and we do it for every case @xmath123",
    ". this may be prohibitively expensive even for a moderate @xmath86 . a possible way to reduce this computational burden",
    "was briefly described in section [ sec : complexity_1 ] . here ,",
    "we provide full details of a more structured approach , which is reminiscent of euler s method .",
    "the approach is built ( again ) on computing newton s iteration ( [ eq : nr ] ) .",
    "it would be more natural in this section to treat the logistic function in a _",
    "vectorized _ fashion , i.e. , @xmath124 , for an @xmath3-dimensional vector @xmath125 .",
    "therefore , we use different , albeit equivalent , representations for the gradient and hessian : @xmath126 here @xmath17 is the design matrix whose rows are @xmath127 , the units or feature vectors ( see ( [ grad - hess ] ) ) . the symbol `` @xmath128 '' denotes the element - wise product , i.e. , @xmath129 .",
    "we modify the iteration so that we neither explicitly compute the logistic function @xmath130 which is involved in both the gradient and hessian , nor use the approximation in expression ( [ sigma - appox ] ) .",
    "note that throughout the procedure we may treat each unit @xmath15 as having an associated logistic function value @xmath131 .",
    "we propose to track a vector of approximate function values @xmath132 which will be updated after each iteration .",
    "then , these approximate values will be used to compute the next iteration of @xmath16 .",
    "note that the derivative of the logistic function is given by : @xmath133 therefore , knowing the value @xmath134 , we can determine the derivative of the logistic function around @xmath38 by a single multiplication .",
    "linearizing around some value @xmath135 gives : @xmath136 where the second derivative is evaluated at some value @xmath137 in the interval between @xmath38 and @xmath135 .",
    "denote by @xmath138 as in section [ sec : protocol1 ] , then may make use of the approximation : @xmath139 where @xmath140 is applied element - wise to @xmath141 . over the course of the entire algorithm , the approximation @xmath141 is updated repeatedly , in a manner very similar to using euler s method to numerically integrate the differential equation ( [ logistic_deriv ] ) .",
    "it is well known that the error of this method decreases with the size of the `` step '' taken at each iteration . in the above ,",
    "the steps are of size @xmath142 , which will in general be different on each iteration , and will also be different for each unit . in order to control the error we amend this approximation by breaking down the step into @xmath51 smaller steps each of size @xmath143 , and performing @xmath51 such updates .",
    "as we shall see , we may base our choice of @xmath51 on some aspect of the design matrix , @xmath17 , in order to reach a desired level of error in the approximation .",
    "we write this approximation as :    @xmath144    where the @xmath145 are the intermediate values corresponding to the inner iterations , and we define @xmath146 as the function which gives the average value of @xmath140 evaluated on these values .    we summarize our method in the following coupled iteration : @xmath147 where @xmath148 is the @xmath9-dimensional vector of zeros and @xmath149 is the @xmath3-dimensional vector of ones .",
    "the proposed iteration differs from the protocol of section [ sec : protocol1 ] ( and from the usual newton - raphson method ) .",
    "the main difference is that we have replaced the logistic function approximation ( [ sigma - appox ] ) with our taylor approximation .",
    "note that we are using again the bound on the hessian ( see section [ sec : hess_bound ] ) , which would make computation easier .",
    "we use this technique in our method for this reason , and also since it interacts well with our taylor approximation by ensuring that convergence towards the optimum is in a sense monotonic , as shown in section [ sec : convergence ] . in keeping with our goal of using only sums and products ,",
    "we recall that it is possible to invert a matrix with just these operations ( see section [ sec : matrix_inversion ] )",
    ".    we now present a bound on the distance from our approximated regression coefficients @xmath16 , to the true optimizer of the log - likelihood which we denote by @xmath150 , as in ( [ parm_err_bound ] ) . since our iterations are guaranteed to converge ( see section [ sec : convergence ] )",
    ", we can choose to run the iterations until @xmath151 is smaller than some threshold @xmath1 ( i.e. , by choosing @xmath152 accordingly ) : @xmath153    therefore we can bound the norm of the gradient of the logistic log - likelihood taken at our final parameter estimate : @xmath154 where @xmath107 is the radius of a ball containing all the data vectors , exactly as in ( [ parm_err_bound ] ) , @xmath72 is some constant , and @xmath155 is a quantity upper bounding the maximal euler s step size .",
    "we can use this to construct our main result about the quality of our approximation .",
    "suppose we choose @xmath156 , then from the above we have : @xmath157 is the smallest eigenvalue of the fisher information matrix @xmath158 in the line segment between @xmath8 and @xmath150 .",
    "note that @xmath159 and the factors of @xmath3 cancel .    therefore we can make the accuracy of our approximation arbitrarily good by decreasing @xmath155 although , as we shall see",
    "there is a tradeoff involved .",
    "a smaller @xmath155 usually means a higher @xmath51 , resulting in increased computational demands .",
    "we refer the reader to appendix  [ sec : validity_2 ] for complete technical details .",
    "thus far we have that the error of the approximation decreases as @xmath155 is decreased ; however , this last variable is not controlled directly ( as @xmath86 was in protocol 1 ) but rather is a function of @xmath51 , the number of steps taken for each outer iteration of the algorithm .    in principle , to get at a prescribed step size @xmath155 , we can choose @xmath51 by noting that :    @xmath160    this leads to the overly conservative choice of @xmath161 .",
    "an alternative choice is to run the protocol with a small value of @xmath51 , e.g. , 10 , and then to re - run with different values to assess the sensitivity of the computation . in section  [ sec : experiment ] we show that this technique performs wekk even with small @xmath51",
    ".      we can measure the overall complexity of our method in terms of the number of products that are needed , since these are the most time - consuming operations we use .",
    "first note that to construct the matrix @xmath110 takes @xmath162 products , and inversion of this matrix takes @xmath163 using ( [ matrix - inv ] ) , where the constant is related to the condition of the matrix . then on each iteration , to compute @xmath164 takes @xmath165 products .",
    "our approximation to the logistic function takes @xmath166 products , for a total of @xmath167 products per iteration .",
    "we compare this with the cost of a protocol which computes the logistic function via a specially designed sub protocol based on circuit evaluation , cf .",
    "if the latter may be evalutated using @xmath168 encryptions , then the complexity would be @xmath169 operations per iteration . as mentioned before this number would typically be much larger than @xmath51 ( for example on the order of the number of bits used to represent the numbers ) .",
    "therefore on each iteration we can save a multiple of @xmath3 operations , which may be especially important when @xmath3 is large .",
    "since our protocol runs until convergence , the number of rounds is variable and depends on the data itself .",
    "furthermore a matrix inversion was performed by an iterative scheme which itself took some variable number of iterations to converge .",
    "therefore we amend the protocol so that the output for each party is a triple consisting of the convergent parameter value , and the number of iterations it took to converge , and the number of iterations taken for the matrix inversion .",
    "this way the messages received from testing convergence are easily simulated ( i.e. a zero on every round up until the number specified in the output , then a one on that iteration ) and this clearly reveals no more information since the parties know `` where they are '' in the protocol at all times and could count these numbers of iterations . having dealt with this technicality we will consider simulating the other intermediate messages in our simulator , and consider these convergence tests already taken care of .    in both of our protocols , the messages which are transmitted are always part of some sub - protocol , namely multiplication or evaluation of the `` greater than '' predicate . the only exception to this is the final messages which are sent immediately before the output is reconstructed .",
    "as those messages are themselves random shares they may be simulated easily ( although they must be simulated in such a way that they sum to the correct output values , but this is trivial ) .",
    "the messages which are passed during the sub - protocols may be simulated based on their respective input and outputs so long as the sub - protocols are cryptographically secure . since we take care to ensure that the intermediate values are random shares , the simulators for the sub - protocols `` compose '' to form a simulator for the main protocol ( see @xcite ) .",
    "we provide two illustrative experiments to demonstrate our approach .",
    "the first aims at showing the performance of our protocol from section [ sec : protocol1 ] .",
    "specifically , we examine the effect of approximation ( [ sigma - appox ] ) on the resulting parameter values , when small , and large number of logistic variables @xmath86 are being used .",
    "the second example takes a look at the altered protocol from section [ sec : protocol2 ] , which uses the coupled iteration ( [ eq : coupled ] ) instead of approximation ( [ sigma - appox ] ) , and reports its performances for different values of @xmath51 , the number of euler s `` steps '' .    for both experiments we use",
    "an extract from the current population survey ( cps ) data ( see http://www.bls.gov/cps/ ) , which includes data on a sample of slightly more than 50,000 u.s .",
    "we focus on predicting whether household income is greater than 50,000 dollars .",
    "we converted @xmath170-category features into @xmath171 binary features , and divided age into 4 bins corresponding to 20 year intervals .",
    "note that although we expressed our approach in terms of continuous covariates , it handles binary flags just as well , where said covariates take on e.g. , @xmath172 and @xmath173 .",
    "our protocol from section [ sec : protocol1 ] deviates from the exact computation in two ways , first we use an approximation to the gradient , and second we perform all the calculations in fixed - point arithmetic .",
    "both of these approximations can be made arbitrarily tight but at the expense of computational efficiency . to demonstrate that our protocol can be implemented in an efficient manner and produce reasonably accurate results we implemented it in a simulator and compared the results to exact logistic regression on the cps data .",
    "for each of @xmath174 and @xmath175 we ran our first protocol 100 times .",
    "the table below shows the means and standard deviations of the resulting parameter values .",
    "evidently , as @xmath86 gets bigger , the accuracy of the parameter values improves .",
    "figures [ fig_like1 ] and [ fig_like1a ] show how the likelihood of the estimate maintained by the protocol increases with the number of iterations .",
    "we computed the error bars by removing the 5 samples that deviated from the mean by the greatest amount , and plotting the minimum and maximum from the remaining ones .",
    "this then corresponds to an approximate 95% confidence interval , and would become an exact interval if we were to perform more and more simulations . for the purposes of comparison , we also plotted the likelihood achieved by the exact non - private newton raphson algorithm , and a non - private algorithm which we referred to as `` hessian lower bound . ''",
    "both give upper bounds for what we hope to achieve , the latter is an algorithm where we just use the approximation of ( [ eq_hessian_lb ] ) , and exact ( i.e. , non - private ) logistic sigmoid values . we see that as @xmath86 increases , the first protocol more closely approximates the hessian lower bound technique , which converges more slowly than the exact newton raphson method .    for the second experiment , we ran our coupled iteration on the cps data with @xmath176 .",
    "although each iteration of our algorithm may be cheap , all is for nought if we require many more iterations for convergence . to determine whether this happens we compared our method to the hessian lower bound method of ( [ eq_hessian_lb ] ) , since this represents our algorithm without the approximation . in figure",
    "[ fig_like2 ] , we plot the likelihoods of the second protocol against the iteration number .",
    "since there is no randomness in the second approximation , there are no error bars . even for small values of @xmath51 ,",
    "much smaller than those suggested by ( [ eq_choose_k ] ) , the approximation to the hessian lower bound technique is quite good , and increasing @xmath51 further ( e.g. , to 50 ) results in curves which are exactly the same as that of the hessian lower bound method . in table",
    "[ tab : protocol2 ] we show the resulting parameter estimates for both methods .",
    ".estimates produced by the exact method ( newton raphson ) , and the two protocols , for different parameter settings of the protocols . [ cols=\"<,>,>,>,>,>,>,>,>\",options=\"header \" , ]     , and that of the `` hessian lower bound '' algorithm , which is the same as protocol 1 except with exact sigmoid evaluations .",
    "we also compare to the full newton raphson method , which inverts the hessian on each iteration.,width=672 ]    , and that of the `` hessian lower bound '' algorithm , which is the same as protocol 1 except with exact sigmoid evaluations .",
    "we also compare to the full newton raphson method , which inverts the hessian on each iteration.,width=672 ]    , and that of the `` hessian lower bound '' algorithm , which is the same as protocol 1 except with exact sigmoid evaluations .",
    "we also compare to the full newton raphson method , which inverts the hessian on each iteration.,width=672 ]",
    "we can use the construction of section [ sec : protocol1 ] to build secure protocols for similar statistical calculations , e.g. , the constructions for computing shares of outer products and matrix inverses naturally yield a secure algorithm for performing linear regression , for details see  @xcite . furthermore using the `` ridge regression '' penalty on the weights ( i.e. , computing a map estimate under a gaussian prior ) can naturally be added to the protocol for both linear and logistic regression .",
    "it is also possible to implement the coordinate ascent computation of the lasso ( or sparse logistic regression ) using these constructions ( i.e. , using the gt protocol to perform soft thresholding ) .",
    "our protocol generalizes to the class of generalized linear models ( glms ) including logistic regression with other link functions .",
    "glms consist of a random component @xmath177 from an exponential family , a systematic component with a linear predictor @xmath178 , and a link function @xmath179 , where @xmath180 .",
    "if @xmath181 makes the linear predictor @xmath182 , where @xmath183 is the natural parameter of the exponential family , @xmath181 is canonical .    for poisson log - linear models with the canonical link , @xmath184 , we approximate the exponential function similarly . for gamma models with the canonical link , @xmath185 , and for inverse - gaussian models with the canonical link , @xmath186 , we can use the number inverting without division scheme .",
    "our approach can also be extended to treat binary regression with non - canonical links , such as the _ probit _ link function , or more generally , inverse cdf link functions .",
    "the general form of the gradient is : @xmath187 let @xmath188 denote a given cdf ( @xmath189 leads to the probit link function , while , of course , @xmath190 leads to the logit link function ) .",
    "then , @xmath191 , and thus @xmath192 , where @xmath193 is the density .",
    "therefore , we should find approximations for @xmath193 as well as for @xmath188 ( approximation for @xmath188 will follow the same idea as for @xmath194 , i.e. , using the empirical cdf ) .",
    "we have demonstrated that a fully secure approach to logistic regression based on the cryptographic notion of security may be made practical for use on moderately large datasets shared between several parties .",
    "although it is slower than methods with weaker security guarantees , it offers more rigorous guarantees with respect to the privacy of the input data .",
    "we emphasize that our protocol ( like any cryptographic protocol ) prevents leakage of information which may arise from the computation itself .",
    "it does not address any leakage which results from the output .",
    "the problem of secure regression is far from solved however , we have yet to deal with the problem of record linkage , and have implicity assumed that the parties know how their respective datasets are aligned . furthermore record linkage due to a statistical model may be incorrect and may result in errorful estimates of model parameters .",
    "here we show how a bound on the error in the approximation ( [ sigma - appox ] ) to the logistic function leads to a bound on the quality of the convergent parameter vector output by the protocol .",
    "specifically , we establish the validity of ( [ parm_err_bound ] ) .",
    "let @xmath107 denote a constant such that @xmath195 , for @xmath196 .",
    "recall the expressions for the gradient @xmath197 and hessian @xmath198 given in ( [ grad - hess ] ) .",
    "define the approximated gradient , by substituting @xmath194 for @xmath199 : @xmath200 rewriting @xmath201 , and applying the triangle inequality we obtain a bound on the norm of the gradient of the logistic objective : @xmath202 next we convert a bound in the norm of the gradient into a bound on the distance to the optimum .",
    "[ lem_meanval ] let @xmath203 be the optimizer of the logistic regression objective , and let @xmath204 denote the smallest eigenvalue of the negative hessian in the line segment between @xmath8 and @xmath203 . then : latexmath:[\\[\\label{distance_bound }      we use the mean - value theorem ( for vector - valued functions ) to write the difference between gradient vectors at @xmath8 and @xmath203 : @xmath206 now , for every ( symmetric ) matrix @xmath43 , and a non - zero vector @xmath207 , the rayleigh quotient satisfies @xmath208 , where @xmath209 is the minimal eigenvalue of @xmath43 .",
    "if @xmath210 , for a positive definite ( symmetric ) matrix @xmath211 , this reduces ( after taking the square root on both sides ) to @xmath212 . applying this to ( [ eq : mean - value ] ) , and using weyl s inequality , we have : @xmath213 this completes the proof .",
    "[ lem_gradmin ] using the same notation we have : @xmath214 where @xmath215 is a ( non - empty ) set of logistic parameters defined in the proof .",
    "consider a continuous , monotonically non - decreasing function @xmath216 which satisfies @xmath217 .",
    "such a function clearly exists , for example the smooth nondecreasing curve which goes through all points @xmath218 where @xmath219 ( where @xmath220 is @xmath26 smallest logistic variable used in @xmath194 ) .",
    "since @xmath216 is nondecreasing , it is the derivative of some convex function : @xmath221 consider the approximation to the logistic gradient which uses @xmath140 instead of @xmath194 : @xmath222 this is the derivative of a concave function : @xmath223 which is indeed concave since it is a linear function minus a convex function .",
    "hence @xmath224 has a unique maximum somewhere .",
    "consider the functions @xmath216 so that the maximum is in the interior of the space @xmath225 ( i.e. , is not at infinity ) .",
    "hence for each such @xmath140 we have a point @xmath226 where the gradient is zero , i.e. , @xmath227 .",
    "denote the set of such @xmath228 by @xmath215 , and note that @xmath215 is not empty . an argument similar to the one that led to ( [ exactnorm_bound ] ) shows that : @xmath229 therefore : @xmath230 which completes the proof .",
    "we now put this all together and state the main result about our approximation @xmath194 .",
    "if our approximation @xmath231 is used as an approximation to the gradient of the logistic log likelihood , and numerical optimization is performed until @xmath232 , then : @xmath233 where @xmath203 is the optimizer of the exact logistic regression objective , @xmath8 is the result of our numerical optimization , @xmath107 is the radius of a ball containing all the @xmath15 , and @xmath234 is the smallest eigenvalue of the fisher information matrix @xmath158 in the line segment between @xmath8 and @xmath203 .    notice that @xmath232 is guaranteed in light of lemma [ lem_gradmin ] .",
    "the proof follows by substituting ( [ exactnorm_bound ] ) into ( [ distance_bound ] ) , and by noticing that @xmath159 and the factors of @xmath3 cancel .",
    "here we establish the convergence of the coupled iteration ( [ eq : coupled ] ) , and the error in our taylor approximation of the logistic function .",
    "we show that the update described in ( [ eq : coupled ] ) converges monotonically towards some final value @xmath8 .",
    "we relate the size of the step taken at one iteration to the size of the step in the previous iteration .",
    "we aim to show that first , these steps are always in the same directions for each unit , and secondly , the steps are monotonically decreasing and eventually the iterations converge .",
    "if we define the idempotent matrix @xmath237 , then we write : @xmath238   \\nonumber\\\\                & = & 4mm(y-\\hat{\\sigma}_{t-1})-16m\\ , \\text{diag}\\ , ( \\tilde{g}_k(\\hat{\\sigma}_{t-1 } ) )   m(y-\\hat{\\sigma}_{t-1 } )   \\nonumber\\\\                & = & 4m\\,\\text{diag}\\,(1 - 4\\tilde{g}_k(\\hat{\\sigma}_{t-1}))m(y-\\hat{\\sigma}_{t-1 } )   \\nonumber\\\\                & = & m\\,\\text{diag}\\,(1 - 4\\tilde{g}_k(\\hat{\\sigma}_{t-1}))x\\delta_t   \\ ; , \\label{stepsize_rel}\\end{aligned}\\ ] ] where we made use of the idempotency of @xmath170 .",
    "next considering the element - wise product as the diagonal of the outer product of these two matrices ,      since we clearly have that @xmath240 no matter what value @xmath241 takes ( due to the definition of @xmath242 ) , we have that this matrix is the product of positive semi - definite matrices , and therefore is itself positive semi - definite",
    ". therefore the diagonal elements are all non - negative , and we have proved the claim .",
    "this result allows us to analyze our approximation to the logistic function as though we were using the forwards euler method to integrate the differential equation ( [ logistic_deriv ] ) , since all the steps for any particular unit will be in the same direction .",
    "suppose that the step is positive for all units and @xmath245 , then : @xmath246 so we also have that @xmath247 .",
    "likewise for units which are involved in a negative step , if they are greater than 0 , then they remain so into the next iteration by an argument which is symmetric to the one above .",
    "therefore we have that our logistic values never leave the interval @xmath248 .      since @xmath170 has eigenvalues which are each either 0 or 1 .",
    "this shows that the magnitude of the steps for the individual units is shrinking towards zero .",
    "therefore we conclude that eventually , our approximations of the logistic values stop updating .",
    "if we assume that @xmath17 has @xmath9 linearly independent columns , then this also implies that @xmath164 is going towards zero , and therefore our algorithm eventually converges .",
    "we now analyze the error in the approximation of the logistic function values .",
    "we then use this together with the convexity of the problem to yield a bound on the error in the convergent parameters ( see ( [ eq : error_2 ] ) ) . to aid the notation , in this section we consider the problem of estimating the logistic values for just a single case , and specifically one for which the steps are all positive .",
    "due to the symmetry of the logistic function about 0 , we will then have the same type of bounds on the error when the approximation updates in the negative direction .",
    "we first show a loose upper bound on the supremum of the error which would be encountered if the approximation was run for an infinite number of steps of size at most @xmath155 , and then use this to bound the error after finitely many such steps .",
    "as we have shown by the above monotonicity argument , our approximation to the logistic function is essentially analogous to using euler s method to integrate the derivative of the logistic function .",
    "since we consider approximating a single value , we change the names of our variables to avoid confusion with the previous vector valued approximation .",
    "if we denote by @xmath251 the approximated value after @xmath152 steps of various sizes , @xmath252",
    ". thus @xmath253 where @xmath254 .",
    "we compare this approximation to the exact values and consider the error : @xmath255    making use of the step ( [ sigma_lin ] ) , we evaluate the error in the next iteration : @xmath256 + \\zeta_t \\\\            & = \\xi_t + \\tau_t(\\hat{s}_t - s_t)g^\\prime(\\cdot)|_{s^{\\star}_t } + \\zeta_t \\\\            & = \\xi_t(1+\\tau_tg^\\prime(\\cdot)\\big|_{s^{\\star}_t } ) + \\zeta_t \\\\            & = \\xi_t(1+\\tau_t -2\\tau_ts^{\\star}_t ) + \\zeta_t\\end{aligned}\\ ] ] where we have defined @xmath257    and @xmath258 is some value in the interval about which the second derivative is taken . likewise @xmath259 is bounded between @xmath260 and @xmath251 . as we have seen from ( [ approx_sigma_bound ] ) , as long as @xmath261 then @xmath262 for all @xmath152 .",
    "since we only consider positive steps @xmath263 then we have that @xmath264 , and hence the same bound applies to @xmath265 .",
    "therefore we have that : @xmath266 therefore we see that : @xmath267    examining the form of @xmath268 , we find it to be a function which is everywhere negative . examining the third derivative , we find that the second derivative has exactly one stationary point in @xmath269 which is located at :                    fienberg , s. e. , fulp , w. j. , and slavkovic , a. b. and wrobel , t. a. ( 2006 )",
    ". `` secure '' log - linear and logistic regression analysis of distributed databases .",
    "_ privacy in statistical databases : cenex - sdc project international conference , psd 2006 _ , 277290 .",
    "fienberg , s. e. , slavkovic , a. b. , and nardi , y. ( 2009 ) .",
    "valid statistical analysis for logistic regression with multiple sources . in p. kantor and m.",
    "lesk , eds .",
    "workshop on interdisciplinary studies in information privacy and security  isips 2008 _ , usa lncs volume , springer - verlag , new york .",
    "karr a.f . , and lin , x. , and reiter , j.p . and sanil , a .p .",
    "( 2006 ) secure analysis of distributed databases . in d. olwell and a. g. wilson and g. wilson , eds .",
    ", _ statistical methods in counterterrorism : game theory , modeling , syndromic surveillance , and biometric authentication _ , springer - verlag , new york , 237261 ."
  ],
  "abstract_text": [
    "<S> preserving the privacy of individual databases when carrying out statistical calculations has a long history in statistics and had been the focus of much recent attention in machine learning in this paper , we present a protocol for computing logistic regression when the data are held by separate parties without actually combining information sources by exploiting results from the literature on multi - party secure computation . </S>",
    "<S> we provide only the final result of the calculation compared with other methods that share intermediate values and thus present an opportunity for compromise of values in the combined database . </S>",
    "<S> our paper has two themes : ( 1 ) the development of a secure protocol for computing the logistic parameters , and a demonstration of its performances in practice , and ( 2 ) and amended protocol that speeds up the computation of the logistic function . </S>",
    "<S> we illustrate the nature of the calculations and their accuracy using an extract of data from the current population survey divided between two parties . </S>",
    "<S> + * keywords : * distributed analysis ; logistic regression ; privacy - preserving computation ; secure multiparty computation . </S>"
  ]
}