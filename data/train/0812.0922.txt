{
  "article_text": [
    "multi - variate discrimination techniques are used in high energy physics to distinguish signal from background events based on a set of measured characteristic observables .",
    "the information contained in the individual observables is combined into a single `` discriminant '' variable , on which then a cut is applied to separate signal from background . for an introduction to multi - variate discrimination techniques see e.g. ref .",
    "@xcite .",
    "besides other approaches , non - parametric probability density estimaton ( pde ) methods are used .",
    "non - parametric pde methods calculate a discriminant for each event to be classified based on the density of signal and background events in the vicinity of its coordinate in the multi - dimensional phase space . in the following we consider only methods that sample the event densities with probe volumes of fixed size .",
    "these methods have been used e.g. in searches for new physics at the tevatron  @xcite and at hera  @xcite and for particle identification at the lhc  @xcite .    a pde method based on range searching ( ` pde - rs ` )  @xcite has been used successfully for classification problems in higher - dimensional observable spaces and with arbitrary correlations between the observables .",
    "large samples of monte - carlo ( mc ) simulated signal and background training events are stored in binary - search trees .",
    "an efficient range - searching algorithm is used to sample the signal and background densities in small multi - dimensional boxes around the phase - space points to be classified .",
    "it handles the involved statistical uncertainties in a transparent way and has the size of the sampling box as the only free parameter .",
    "an apparent limitation of ` pde - rs ` , on the other hand , is the fact that large signal and background training samples are required to densely populate the multi - dimensional phase space .",
    "this is of particular importance in applications with many dimensions .",
    "furthermore , these samples have to be accessible in the main memory of the computer used for the classification and the classification time scales with the number of training events like @xmath8 .",
    "though adaptive resizing and kernel - convolution mechanisms for the sampling box have been implemented for ` pde - rs ` in the toolkit for multi - variate data analysis with root ( tmva )  @xcite , the geometry of the sampling box is always identical in all dimension and it is therefore not optimally adapted to cases where the density distributions vary for the different dimensions involved .    in this paper , we propose an improvement of the original ` pde - rs ` method  @xcite that reduces the sensitivity to statistical fluctuations of the training samples and results in a very fast and memory - efficient classification phase , independent of the size of the training samples .",
    "a self - adapting binning method is used to divide the multi - dimensional phase space in a finite number of hyper - rectangles ( boxes ) .",
    "only the binned density information is preserved in binary trees after the training phase , allowing for a very fast and memory - efficient classification of events .",
    "the implementation of the binning algorithm ( ` pde - foam ` ) is based on the mc event - generation package ` foam `  @xcite .",
    "pde methods are based on the assumption that the probability for an event @xmath9 ( characterised by a set of @xmath10 observables @xmath11 ) to belong to the signal class is given as a uniformly continuous function @xmath12 . according to bayes theorem ,",
    "@xmath12 is derived from the probability density functions for signal and background , @xmath13 and @xmath14 , and from the a - priori probabilities @xmath15 and @xmath16 for an event to be of class signal or background , respectively , as @xmath17 the signal probability @xmath12 is a monotonously rising function of the _ discriminant _ @xmath18 : @xmath19 in the relevant range @xmath20 , such that a cut on @xmath21(x ) is always equivalent to a cut on @xmath22(x ) .",
    "@xmath22(x ) and hence @xmath21(x ) are the optimal discriminants according to the neyman - pearson lemma  @xcite .",
    "estimates @xmath23 and @xmath24 of the signal and background probability density functions are obtained by sampling the @xmath10-dimensional phase space with events of known type .",
    "such events can be either obtained from mc simulations or by defining data control samples in an appropriate way .",
    "the estimated discriminant : @xmath25 approximates the true discriminant @xmath18 for sufficiently densely populated sampling space .    for any given combination of observables , @xmath9 , the discriminant @xmath26 assigns a single value , which allows to discriminate background from signal events . in the framework of a physics analysis , a cut on a particular value of @xmath27",
    "is applied , depending on the required purity and efficiency of the event selection .",
    "the signal and background probability density functions have to be approximated with sufficient accuracy .",
    "this poses a challenging problem in particular for high - dimensional cases .",
    "a solution based on range searching ( ` pde - rs `  @xcite ) counts the number of mc generated signal and background events in the vicinity of each event to be classified .",
    "the discriminant @xmath28 is defined from the number of signal events @xmath29 and the number of background events @xmath30 in a small volume @xmath31 around the point @xmath9 : @xmath32 the normalisation constant @xmath33 has to be chosen such that the total number of simulated signal events , @xmath34 , is equal to @xmath33 times the total number of background events , @xmath35 : @xmath36    the statistical uncertainty on the value of the discriminant @xmath21 is obtained from a propagation of the uncertainty on the number of events contained in the counting volume : @xmath37 where @xmath38 and @xmath39 are the statistical uncertainties of the number of signal and background events respectively .",
    "@xmath26 provides a good estimate of @xmath18 for sufficiently small probe volumes @xmath31 and large numbers of mc simulated sample events .",
    "figure [ fig : discr ] shows the distribution of the discriminant @xmath40 for signal and background testing events of an arbitrary example .",
    "the discriminant takes values between 0 and 1 .",
    "most signal events are found at large values of @xmath27 , while the distribution for background events peaks at small values of @xmath27 .",
    "a given cut value @xmath41 results in an efficiency for signal and background testing samples , @xmath42 and @xmath43 .",
    "figure  [ fig : roc ] shows the relation between signal efficiency @xmath44 and background rejection @xmath45 when scanning all values of @xmath41 between 0 and 1 .",
    "the area under this so called _ receiver operating characteristic _ (",
    "roc ) is a measure of the average estimator performance .",
    "a value of 0.5 is obtained for a random classification of signal and background .",
    "a value of 1.0 is obtained for a perfect discrimination between signal and background .",
    "for the example shown in fig .",
    "[ fig : discr_roc ] , the obtained roc value is 0.88 .",
    "the optimal performance measure for a particular practical application has to be chosen according to the desired balance between purity and efficiency . in high energy physics ,",
    "mva methods are often used in searches for rare events , where a small signal is overwhelmed by background . in such cases ,",
    "the signal efficiency for a given large value of the background rejection is a more relevant performance measure than the overall area of the roc curve .",
    "in addition to the roc area , one therefore often quotes @xmath46 , the signal efficiency at a background rejection of 99% .",
    "for the example shown in fig .",
    "[ fig : discr_roc ] , the resulting value is @xmath47 .",
    "in the following we propose an alternative method to calculate the discriminant @xmath18 based on a binned sampling of the phase space .",
    "simple binning methods often suffer from excessive memory consumption and lack of statistical accuracy , since the number of bins increases as @xmath48 , where @xmath49 is the number of bins per dimension .",
    "the bin size has to be small enough to follow fine changes in the event distributions in phase - space regions where signal and background overlap .",
    "this often leads to a large number of scarcely populated bins . in many practical applications",
    "the phase space is effectively only populated in a sub - space of lower dimensionality , since the _ intrinsic dimensionality _ of the actual problem is often reduced due to correlations among the observables .    to overcome this problem , a self - adaptive binning method , called `` ` pde - foam ` '' ,",
    "is used to project the information contained in the signal and background samples into a grid of @xmath10-dimensional cells with non - equidistant cell boundaries , called the `` foam of cells '' .",
    "the method is based on an algorithm originally developed for the multi - dimensional general purpose mc event generator ` foam `  @xcite . for a given @xmath10-dimensional",
    "analytically known distribution , the ` foam ` algorithm creates a hyper - rectangular `` foam of cells '' , which is more dense around the peaks of the distribution and less dense in areas where the distribution is only slowly varying .",
    "the foam is iteratively produced using a binary - split algorithm for the cells acting on samplings of the input distribution within the cell boundaries .",
    "the number of cells is a predefined free parameter and a priori only limited by the amount of available computer memory .",
    "the optimal number of cells depends on the statistical accuracy of the training samples .",
    "[ foam - build - up - algorithm ] in the context of pde , ` foam ` has been adapted such that the splitting of cells is based on an input distribution that is sampled from mc training events using the ` pde - rs ` method .",
    "the steering parameters introduced in the following are summarised in table  [ tab : parameters ] and their usage and optimisation is discussed in section  [ sec : pdefoam - parameters ] .    .main",
    "` pde - foam ` parameters and their default values .",
    "the parameters and their optimisation are discussed in sections [ subsec : foam - algorithm ] and [ sec : pdefoam - parameters ] . [ cols=\"<,<,<\",options=\"header \" , ]     the build - up of the foam starts with the creation of the base cell , which corresponds to a @xmath10-dimensional hyper - rectangle containing all mc training events .",
    "the coordinate system of the foam is normalised such that the base cell extends from 0 to 1 in each dimension .",
    "the coordinates of the events in the corresponding training samples are linearly transformed into the coordinate system of the foam .",
    "tails of the input distributions are removed from the base cell by an adjustable parameter ` tailcut ` .",
    "an upper and a lower bound are determined for each dimension such that on both sides of the corresponding one - dimensional distribution a fraction of ` tailcut ` of all events are excluded .",
    "starting from this base cell , a binary splitting algorithm iteratively splits cells of the foam along hyperplanes until a predefined maximum number of cells , ` nactivecells ` , is reached .",
    "the implementation is identical to the one of the original ` foam ` code  @xcite .",
    "it minimises the relative variance of the density @xmath50 across each cell is either defined as the sampled density of events of a given type or as the sampled density of the discriminant , as will be discussed in the following section . ] .",
    "for each cell a predefined number ` nsampl ` of random points uniformly distributed over the cell volume are generated .",
    "for each of these points a small box of fixed size ` volfrac ` centred around this point is considered to estimate the local event density of the corresponding training sample as the number of training events contained in this box divided by its volume .",
    "events from neighbouring cells are also counted in cases where the sampling box extends beyond the cell boundaries .",
    "the obtained densities for all sampled points in the cell are projected on the @xmath10 axes of the cell and the projected values are filled in histograms with a predefined number of bins , ` nbin ` .    the cell to be split next and the corresponding division edge ( bin ) for the split",
    "are selected as the ones that have the largest relative variance .",
    "after the split , the two new daughter cells become ` active ' cells and the old mother cell remains in the binary tree , marked as being ` inactive ' .",
    "a detailed description of the splitting algorithm and the ` foam ` data structure can be found elsewhere  @xcite .",
    "the geometry of the final foam reflects the distribution of the training sample : phase - space regions where the density is approximately constant are combined in large cells , while in regions with large gradients in density many small cells are created .",
    "figure  [ fig:2d_gauss ] shows a two - dimensional gaussian - ring distribution and fig .",
    "[ fig : foam_density_2000c ] shows a graphical representation of the resulting foam with 2000 active cells .",
    "each cell contains the number of events from the input distribution belonging to the volume of the cell .",
    "the foam consists of only a few large and sparsely populated cells in the centre and corner regions of the two - dimensional plane , where the gradient of the gaussian radial component of the distribution is small .",
    "close to the centre of the ring , however , where the radial component of the distribution has a steep gradient , the foam consists of many small and densely populated cells .",
    "this example is particularly challenging for the foam algorithm , as the rectangular geometry of the foam cells does not match the angular symmetry of the example .",
    "the foam structure is formally equivalent to a decision tree  @xcite .",
    "the cut values of the decision tree correspond to the cell - splitting boundaries stored in the binary tree representing the foam .",
    "optimisation of the decision tree ( e.g. boosting ) is replaced in case of ` pde - foam ` by the sampling and minimisation algorithm described above .      in order to use the ` foam ` for mva , two different concepts have been implemented :    1 .   _",
    "separate signal and background foams _",
    "+ during the _ training phase _ two separate foams are created : one for signal and one for background events .",
    "the splitting of cells is based on the corresponding event densities .",
    "the number of signal ( background ) events contained in each cell of the final signal ( background ) foam is stored with the corresponding cell . during the _ classification phase _ the value of the discriminant for a given event @xmath9 is calculated based on the number of events contained in the corresponding cells : @xmath51 where @xmath52 and @xmath53 are the number of events contained in cell @xmath54 of the signal foam and cell @xmath55 of the background foam , respectively , and @xmath56 and @xmath57 are the cartesian volumes of cell @xmath54 and @xmath55 , respectively . + the statistical uncertainty on the discriminant @xmath58 is obtained in analogy to eq .",
    "[ eq : stat_error ] .",
    "2 .   _ one foam for discriminant distribution _ + during the _ training phase _",
    "one foam is created containing the distribution of the discriminant .",
    "the splitting of cells is based on the sampled discriminant distribution calculated according to the ` pde - rs ` approach .",
    "each cell @xmath54 contains a discriminant value @xmath59 calculated as : @xmath60 where @xmath61 ( @xmath62 ) are the number of signal ( background ) events contained in cell @xmath54 . the statistical uncertainty on the discriminant obtained in analogy to eq .",
    "[ eq : stat_error ] is also stored with each cell . during the _ classification phase _ the value of the discriminant for a given event @xmath63 from and independent testing sample and its statistical uncertainty",
    "are retrieved from the corresponding cell @xmath54 .",
    "for the same number of total foam cells , the performance of the two implementations was found to be similar .",
    "figures  [ fig : discr_roc_foam](a)-(c ) show the distribution of the discriminant for 500000 signal and 500000 background testing events of the gaussian - ring example introduced above .",
    "the classification is performed with single discriminant foams of 100 , 500 and 2000 active cells , respectively .",
    "the foams are created using 200000 signal and 200000 background training events . besides the histograms for the events classified with ` pde - foam ` ,",
    "also the corresponding curves for an analytical calculation are shown in the figures .",
    "for this example , both signal and background input distribution peak at the same values and are only distinguished by their different width .",
    "therefore the resulting discriminant distributions for signal and background show a large overlap and poor separation .",
    "the peaked structure of the histograms reflects the finite granularity of the foams .",
    "the distributions become smoother and approach the analytically calculated curves with increasing number of foam cells .",
    "figure  [ fig : roc_foam ] shows the resulting roc curves for the same example .",
    "the curve for the theoretical optimum obtained from the analytical calculation is also shown .",
    "the area under the curve increases with increasing number of foam cells from 0.655 ( 100 cells ) to 0.699 ( 2000 cells ) .",
    "the theoretical optimum corresponds to an area of 0.705 . for a background rejection of 99% ,",
    "the respective signal - efficiency values are between 1.5% ( 100 cells ) and 1.85% ( 2000 cells ) for the classification with ` pde - foam ` and 2.0% for the theoretical optimum .",
    "the foam algorithm is implemented within the tmva framework  @xcite as method ` pde - foam ` .",
    "the core foam functionality is inherited from the tfoam class included in root  @xcite .",
    "parameter steering , classification output and persistency mechanism follow tmva standards , thus allowing to use it like other mva methods implemented in tmva and to compare the performance . the initial binary trees , which contain the training events , needed to evaluate the densities for the foam build - up based on the ` pde - rs ` method , are discarded after the training phase .    the memory consumption for the foam is @xmath64 bytes per foam cell plus an overhead of @xmath65 kbytes for the ` pde - foam ` object on a @xmath66-bit architecture .",
    "note that in the foam all cells created during the growing phase are stored within a binary tree structure .",
    "cells which have been split are marked as inactive and remain empty . to reduce memory consumption ,",
    "the geometry of a cell is not stored with the cell , but rather obtained recursively from the information about the division edge of the corresponding mother cell .",
    "this way only two short integer numbers per cell contain the information about the entire foam geometry : the division coordinate and the bin number of the division edge .",
    "the foam object can be stored in xml or root format .",
    "a projection method is available for visible inspection .",
    "table  [ tab : parameters ] summarises the main ` pde - foam ` parameters that can be set by the user together with their default values .",
    "optimisation of these parameters is needed to reach optimal classification performance . in the following ,",
    "we discuss the dependence of the foam performance on the choice of parameters for some representative examples .",
    "the size of the box used for the phase - space sampling is a common parameter of both the ` pde - foam ` method and the original ` pde - rs ` method . in case of ` pde - foam ` , the box size is only relevant for the density sampling during the training phase , while for ` pde - rs ` the box size is only used for the calculation of the discriminant during the classification phase .",
    "a larger box leads to a reduced statistical uncertainty for small training samples and to smoother sampling . a smaller box on the other hand increases the sensitivity to statistical fluctuations in the training samples , but for sufficiently large training samples it will result in a more precise local estimate of the sampled density .",
    "besides affecting the estimator performance , the box size influences the training time in case of ` pde - foam ` and the classification time in case of ` pde - rs `",
    ". a larger box increases the cpu time during sampling , due to the larger number of nodes to be considered in the binary search  @xcite .    in general , higher dimensional problems require larger box sizes , due to the reduced average number of events per box volume . for uniformly distributed variables ,",
    "the volume size containing a given number of events grows with the power of the number of dimensions . to collect 10% of the training events inside the sampling volume for a case with 10 variables , a box with edge length of 80% of the full range in each dimension",
    "is needed .",
    "figure  [ fig : boxsize ] shows the estimator performance , measured as the area under the roc curve , as function of the size of the sampling box and for examples with 2 - 10 observables ( = dimensions ) .",
    "the examples are constructed as uncorrelated @xmath67-dimensional gauss distributions with shifted means and different widths for signal and background observables ( @xmath68 ) are generated from gauss distributions with mean values @xmath69 and @xmath70 and widths @xmath71 and @xmath72 for signal and background , respectively . ] .    the ` pde - foam ` performance ( see fig .  [",
    "fig : volfrac ] ) is compared to the performance of the original ` pde - rs ` method ( see fig .  [",
    "fig : deltafrac ] ) . in both cases 50000",
    "signal and 50000 background training events were used . for `",
    "pde - foam ` , a target value of 1000 active cells was selected and a cut on the minimum number of events per cell of 100 ( cf .",
    "discussion in section  [ sec : nmin ] ) was applied during the foam build - up . here and in the following",
    ", the performance values have been obtained from independent testing samples of 500000 signal and 500000 background events .",
    "the performance increases for both methods with the number of observables and with the size of the sampling box .",
    "it reaches a maximum for both methods , after which it drops again slightly with further increasing box size , due to the less precise local estimate of the larger boxes . `",
    "pde - foam ` is less sensitive to statistical fluctuations in the training samples , due to the additional averaging stage during the density sampling inside the cells .",
    "therefore the ` pde - foam ` performance reaches the optimum for smaller sampling boxes and has a wider range of stable performance , compared to the original ` pde - rs ` implementation . for a small number of up to approximately four observables ,",
    "there is almost no visible dependency of the performance on the box size .",
    "the default box size of 1/30 gives close to optimal results up to approximately five observables for this example .",
    "for the original ` pde - rs ` method , on the other hand , a more careful optimisation of the box size is required , as the box size for optimal performance depends strongly on the number of dimensions and the convergence towards optimal performance is slower .",
    "the target number of cells for the final foam is the main parameter impacting the accuracy of the phase - space binning .",
    "an increased number of cells leads in general to improved performance provided that sufficiently large training samples are available .",
    "however , for an increasing number of cells with small training samples , the foam becomes more vulnerable to statistical fluctuations in the training samples in particular in less populated regions of the phase space and the performance might drop when further increasing the target number of cells ( overtraining ) .",
    "both the training time and the memory needed to store the foam object increase linearly with the number of cells , while the classification time scales approximately as @xmath73 .",
    "figure  [ fig : ncells ] shows the dependence of the estimator performance as function of the number of active cells for an example with five moderately correlated observables constructed from gaussian distributions for signal and background .",
    "the two curves correspond to foams build - up from small and large training samples , respectively .",
    "the small training sample consist of 50000 signal and 50000 background events , whereas the large training sample contain 500000 signal and 500000 background events .",
    "no restriction on the number of events contained in each cell was applied .",
    "as expected , the performance of the foams built from the large training sample exceeds the one of the foams based on the small training sample . in case of the large training sample ,",
    "the performance for this particular example increases over a wide range of number of cells and reaches its maximum for about 20000 cells , after which it drops due to the decrease in statistical precision resulting in overtraining .",
    "for the small training sample , the maximum is already reached for foams with approximately 5000 cells and the drop in performance afterwards is steeper .",
    "the cell splitting algorithm assumes sufficient statistical accuracy of the sampled density distributions in all cells .",
    "this might not be guaranteed in case of small training samples , where cell splitting in scarcely populated phase - space regions can lead to overtraining effects .",
    "therefore cells should not be taken into account for further splitting , if the number of training events contained inside a cell is too small .",
    "an adjustable parameter ` nmin ` has been implemented , which sets the minimum number of events contained in any cell which is considered for further splitting . if the number of events is below ` nmin ` , the cell is not considered for further splitting . if no more cells are available with sufficient number of events , the cell splitting stops , even if the target number of cells is not yet reached .",
    "note that the ` nmin ` requirement only affects the further splitting of cells .",
    "therefore it is possible to have cells containing less than ` nmin ` events in the final foam(s ) .",
    "the cut on ` nmin ` reduces the sensitivity to statistical fluctuations in the training samples and improves drastically the performance for small number of training events , as shown in fig .",
    "[ fig : nminperf ] for the example with 5 observables and only 10000 training signal and background events each . without the cut on",
    "` nmin ` , the foams with larger number of target cells suffer from overtraining and show a significantly decreased performance . starting from a value of @xmath74 , the effective number of final cells , as shown in fig .  [",
    "fig : nmincells ] is limited to a value below 10000 and therefore the performance curves for 10000 and 30000 target cells become identical ( fig .  [",
    "fig : nminperf ] ) . for a value of @xmath75",
    ", this number drops to 2000 cells , visible in both figures as the points where all three curves merge .",
    "for very large values of ` nmin ` the performance drops again , as the size of the few remaining cells becomes too large .",
    "the default value of @xmath76 leads to a good performance for most cases studied .",
    "it can be combined with a large target number of cells , as it limits the effective number of cells sufficiently and thus avoids overtraining even for small training - sample sizes .",
    "for the example shown in fig .",
    "[ fig : nmin ] , the value of @xmath76 corresponds to approximately 450 active cells in the final foams .",
    "the number of samplings per cell and cell - division step affects the phase - space sampling procedure during the foam build - up .",
    "the value has to be large enough to fill the density histograms used for the evaluation of the variance with sufficient statistical accuracy . on the other hand ,",
    "increasing this parameter to a value much larger than the average number of training events contained in a cell will not improve the performance any further , as the sampling accuracy is limited in this case by the available number of training events in the cells .",
    "the foam build - up time scales approximately linearly with the number of samplings .",
    "the default value of 2000 is sufficiently large for optimal performance with all examples studied . for many cases with a small number of observables or small training - sample sizes ,",
    "a reduced value of 500 - 1000 can be chosen without loss in performance .",
    "histograms are used to evaluate the variance across the cells projected on the cell axes .",
    "cell splits are only performed at the bin boundaries .",
    "the accuracy of the determination of the division point increases with every iteration , as the histograms are refined with respect to the base cell after each division step .",
    "for all examples studied , the dependence both of the performance and the foam build - up time on the number of bins was found to be very small .",
    "the default value of 5 was found to be sufficient to achieve optimal performance .",
    "foams with small number of cells and which are based on small training - sample sizes can suffer from large cell - to - cell fluctuations leading to large discontinuities at the cell boundaries .",
    "a gaussian smearing can be applied during the classification phase to reduce the effect of these discontinouities  @xcite . in this case , all cells contribute to the discriminant calculation for a given event , convoluted with their gauss - weighted distance to the event .",
    "the width parameter of the gauss function used for the smearing is set to the length of the sampling box in each dimension ( ` volfrac ` ) .",
    "figure  [ fig : foam_kernel ] shows the geometry of a foam with 250 active cells and the reconstructed event density , based on 5000 signal and 5000 background training events generated according to a two - dimensional gaussian distribution with width 1.0 in each dimension and centred at ( 0.5,0 ) .",
    "the reconstructed event densities with and without gaussian kernel smearing are compared in fig .",
    "[ fig : foam - nokernel ] and  [ fig : foam - kernel ] .",
    "the width of the gaussian kernel used for the smearing corresponds to 0.33 in the units of the original distributions .",
    "the improvement with kernel smearing is clearly visible . in most cases this procedure leads to an improved separation between signal and background .",
    "figure  [ fig : foamperf_kernel ] shows the performance as function of the number of signal and background training events for an example with two - dimensional gauss distributions and in comparison with the original ` pde - rs ` method .",
    "signal and background distributions have shifted means but identical widths in this example .",
    "the foams contain 250 active cells and a cut on the minimum number of events per cell was not applied .",
    "the gaussian kernel smearing improves the performance , in particular for small training samples . for this example",
    ", it also exceeds the one of the original ` pde - rs ` method .",
    "however , the gaussian kernel smearing also largely increases the classification time .",
    "the classification times obtained using training signal and background samples of 100000 events each and testing signal and background samples of 500000 events each were approximately 1.5  min for ` pde - rs ` , 1  min for ` pde - foam ` without gaussian kernel smearing and 1  h for ` pde - foam ` with gaussian kernel smearing .",
    "the two main limitations of the original ` pde - rs ` method are :    * the performance of the ` pde - rs ` method increases only slowly with the size of the training samples . for good results ,",
    "typically large samples of the order of 500000 events are needed . * both the cpu time needed for classification and the memory consumption during classification increase with the number of training events . `",
    "pde - rs ` is therefore considered to be a slowly responding classifier for most applications .    in",
    "the following we present a comparison of the performance and cpu - time consumption between ` pde - foam ` and the original ` pde - rs ` method .",
    "the results are shown for the example with five moderately correlated observables .",
    "other examples have been studied and similar results were obtained .",
    "figure  [ fig : performance ] shows the estimator performance as function of the number of signal and background training events for foams of 1000 and 20000 active cells , respectively .",
    "the left figure displays the area under the roc curve as a performance measure , while the right figure shows the signal efficiency for a background rejection of 99% .",
    "the performance of the original ` pde - rs ` method is also shown .",
    "single foams were built for these examples with @xmath77 samplings , a sampling - box size of @xmath78 and a cut on the minimum number of events per cell of @xmath76 . in case of ` pde - rs ` , the sampling - box size was 1.2 in units of the original observables , corresponding to approximately 0.12 in normalised coordinates .    for small training samples up to approximately 100000 events ,",
    "the foams perform significantly better than the original ` pde - rs ` method .",
    "apparently the geometry of the foams is well adapted to the event distributions and the implicit averaging of the event densities over the cell volumes leads to better performance than the sampling with fixed box size performed by the original ` pde - rs ` method . for training samples of less than 200000 events ,",
    "the original ` pde - rs ` method does not even reach a background rejection of 99% .    for very small training samples of 30000 events and less , the foams with 1000 and 20000 cells behave identically , since the cut on the minimum number of events per cell of 100 limits the effective number of final cells to a value below 1000 .    for large training samples above 50000 events , the foam with 20000 cells performs better than the one with 1000 , taking advantage of its finer granularity and the increased statistical precision of the larger training samples .",
    "however , for training - sample sizes of more than 200000 events , it does not quite reach the performance of the original ` pde - rs ` method . for such large sample sizes ,",
    "the local density estimates obtained with the ` pde - rs ` method by counting events in the vicinity of the events to be classified are more precise than the density estimates from counting events in foam cells of finite granularity .",
    "figure  [ fig : training_time ] shows the training time as function of the number of training signal and background events for foams of 1000 and 20000 active cells , respectively , for the example and parameters described above .",
    "the training time for the original ` pde - rs ` method is also shown . for `",
    "pde - rs ` , the training time consists only of the creation of the binary - search trees used to store the training samples . for `",
    "pde - foam ` , the training time is dominated by the repeated density sampling during the iterative build - up of the foam structure .",
    "therefore the training time is larger than for the ` pde - rs ` method .",
    "the training time for small training samples is identical for the foams with 1000 and 20000 cells , due to the cut on ` nmin ` .",
    "figure  [ fig : testing_time ] shows the cpu time used for classification of 500000 signal and 500000 background testing events as function of the number of signal and background training events . for the foams , the classification time depends mostly on the number of cells in the final foam and is almost independent of the number of training events . the slight variation with the number of training events is due to the corresponding increase of the number of cells and due to slight variations of the foam geometry .",
    "for the original ` pde - rs ` method , on the other hand , the classification time rises with the number of training events , due to the larger size of the binary trees . for @xmath79 signal and background training events",
    "each , the classification time reaches approximately 40 minutes for ` pde - rs ` , while for the foams with 20000 cells it is below 3 minutes . on the other hand , for small training samples of less than approximately 30000 events ,",
    "the recursive reconstruction of the foam geometry during classification takes longer than the density sampling within the ` pde - rs ` binary tree .",
    "the foam can be extended to reconstruct event quantities ( regression analysis ) . in this case",
    "@xmath80 target values depend on @xmath10 observables .",
    "two different methods have been implemented : the first method stores a single target value in every foam cell .",
    "the second method saves the target values in further foam dimensions .",
    "since the first method can only be used if only one target is given , it is called mono target regression. in order to do regression with multiple targets one has to use the second method , called multi target regression.    in case of the mono - target regression , the density @xmath81 used during the foam build - up phase , is given by the mean target value @xmath82 within the sampling box , divided by the box volume ( given by the ` volfrac ` option ) : @xmath83 where the sum goes over all events @xmath84 within the sampling box and @xmath85 is the target value of the event @xmath86 ( @xmath87 ) . during the foam build - up phase ,",
    "the relative variance of the density @xmath50 is minimised in the same way as described in section  [ foam - build - up - algorithm ] .",
    "after build - up of the foam , each cell is filled with the average target value . during classification",
    "the target value is estimated for any given event @xmath9 and is given as the content of the corresponding foam cell .    in case of multi - target regression ,",
    "the @xmath80 target values are treated as additional dimensions during the foam build - up .",
    "the density used for the foam build - up is estimated from the number of events in a box of fixed size in the @xmath88-dimensional phase space .",
    "the number of events contained in the volume of each cell of the final foam is stored with the foam .",
    "the target values for any given event @xmath9 are estimated as the projections of the centre of the corresponding cell onto the corresponding axes formed by the @xmath80 target values .",
    "figure  [ fig : foam_reg ] shows the geometry and the target density for a mono - target foam with 1500 active cells , calculated for an example with two observables and a quadratic dependence of the target value @xmath89 on two uniformly distributed observables , @xmath90 and @xmath91 : @xmath92 where @xmath93 , @xmath94 and @xmath33 are constant and @xmath95 is a small random number simulating noise . the accuracy of the event reconstruction with this foam is shown in fig .",
    "[ fig : reg_accuracy ] , where the relative difference between the reconstructed and true target value is displayed . the mean value is reconstructed with an accuracy of approximately 0.5 per mille .",
    "the rms of the distribution is about 3% .",
    "also shown in the figure is the relative difference between the generated events before and after adding the noise term .",
    "the width of this distribution is approximately 0.7% .",
    "it can be considered as the optimal value for the accuracy of the event reconstruction .",
    "a new method for multi - variate analysis , ` pde - foam ` , has been developed .",
    "it combines the adaptive binning algorithm of the ` foam ` method so far only used for monte - carlo event generation with probability density estimation based on range searching . `",
    "pde - foam ` has been implemented within the tmva package for multi - variate analysis .",
    "we demonstrated that the default set of foam build - up parameters leads to robust results for the various examples studied and we gave guidance for further parameter optimisation .",
    "we showed that the performance of ` pde - foam ` exceeds the classification performance of the original ` pde - rs ` method for small training samples .",
    "furthermore , it leads to largely reduced classification time .",
    "both the classification time and the memory consumption are independent of the number of training events",
    ". the main limitations of the original ` pde - rs ` implementation have therefore been overcome .",
    "in addition to event classification we have implemented a method to reconstruct event quantities with ` pde - foam ` .",
    "the authors would like to thank stanislaw jadach for many explanations and enlighting discussions concerning the ` foam ` algorithm .",
    "we are grateful to andreas hcker for fruitful discussions in the development phase and his support in implementing the ` pde - foam ` within the tmva framework .",
    "we would also like to thank the other main tmva developers and in particular jrg stelzer and helge voss for their help .",
    "we are very grateful to birger koblitz and yair mahalalel for their help with the ` pde - rs ` method .",
    "we also benefited a lot from a discussion with fred james ."
  ],
  "abstract_text": [
    "<S> probability density estimation ( pde ) is a multivariate discrimination technique based on sampling signal and background densities defined by event samples from data or monte - carlo ( mc ) simulations in a multi - dimensional phase space . in this paper </S>",
    "<S> , we present a modification of the pde method that uses a self - adapting binning method to divide the multi - dimensional phase space in a finite number of hyper - rectangles ( cells ) . </S>",
    "<S> the binning algorithm adjusts the size and position of a predefined number of cells inside the multi - dimensional phase space , minimising the variance of the signal and background densities inside the cells . </S>",
    "<S> the implementation of the binning algorithm ( ` pde - foam ` ) is based on the mc event - generation package ` foam ` . </S>",
    "<S> we present performance results for representative examples ( toy models ) and discuss the dependence of the obtained results on the choice of parameters . </S>",
    "<S> the new ` pde - foam ` shows improved classification capability for small training samples and reduced classification time compared to the original pde method based on range searching .    </S>",
    "<S> dominik dannheim@xmath0 , tancredi carli@xmath1 , alexander voigt@xmath1 , + karl - johan grahn@xmath2 , peter speckmayer@xmath3    @xmath1  * cern , geneva , switzerland * + @xmath2  * kth , stockholm , sweden * + @xmath4  * technische universitt , wien , austria * +    ' '' ''    @xmath5  corresponding author ; e - mail : dominik.dannheim@cern.ch .    @xmath6  now with max - planck - institut fr physik , mnchen , germany .    </S>",
    "<S> @xmath7  now with cern . </S>"
  ]
}