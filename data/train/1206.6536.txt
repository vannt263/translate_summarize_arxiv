{
  "article_text": [
    "in the classical estimation problem with linear regression model , one observes a noisy @xmath3 of some @xmath4 where @xmath5 for a given @xmath6 matrix @xmath7 ( called the _ design matrix _ ) and an unknown @xmath8 and wishes to estimate @xmath9 or @xmath10 .",
    "recently , there have been enormous interests in the high - dimensional setting which in addition assumes that the design matrix is high - dimensional , that is , when @xmath11 , and @xmath10 satisfies certain sparsity constraints .",
    "such sparsity constraints can be `` hard '' , when it bounds the number of nonzero components in @xmath10 , or `` soft '' , when @xmath10 is assumed to belong to the unit @xmath0 ball for @xmath1 . in the existing study ,",
    "the focus has so far been on the condition needed for @xmath7 such that certain ( typically polynomial time ) estimators are nearly optimal or achieve lowest possible error for the given parameters .",
    "the work along this line has been quite successful @xcite and produced many characterization of @xmath7 ( typically gaussian random matrix ) for which a polynomial time nearly optimal estimator exists .",
    "the main departure point of this study is that we consider the problem of designing nearly optimal estimator for _ any _ given design matrix @xmath7 , that is , we make no assumption about @xmath7 . as the main contribution of this paper",
    ", we present a family of estimators , which we call _ the projected nearest neighbor estimator _",
    "( pnn ) , and show that for any design matrix @xmath7 , there is a projected nearest neighbor estimator that is nearly optimal in terms of the prediction risk for the corresponding linear regression problem over soft sparsity constraints . as a consequence , we obtain a polynomial time algorithm to compute the approximate minimax risk for any such problem and a polynomial time estimator in the important case of @xmath12 .",
    "our results represent the first provably nearly optimal estimators without any constraint on the design matrix for @xmath1 .",
    "we also design an adaptive estimator for the case when the @xmath2 radius is not given .",
    "we believe that studying optimal estimator for arbitrary @xmath7 is important for multiple reasons .",
    "first , in practice we often do not have control over the design matrix or even the distribution of the design matrix .",
    "the design matrix might be `` ill''-conditioned such that no estimator can achieve good accuracy . on the other hand",
    ", the design matrix may have a structure , as is often the case in practice , rather than completely random . in this case",
    ", it is important to take advantage of such structure to obtain better accuracy .",
    "second , while there have been many characterization ( typically some isometry property on @xmath7 ) known for certain algorithms to work well , it is often difficult to tell if the required property holds for a given @xmath7 .",
    "so most results assume that @xmath7 come from gaussian random matrix .",
    "third , relaxing the requirement about the design @xmath7 calls for the development of new algorithms as well as new analysis tools . indeed , to argue the optimality of our estimator",
    ", we have to utilize novel tools from convex geometry ( the classical restricted invertibility result by bourgain and tzafriri @xcite ) .      in the linear regression problem , one observes @xmath13 , where @xmath5 for a given @xmath6 matrix @xmath7 and an unknown vector @xmath14 for @xmath1 , where @xmath15 .",
    "in addition , the noise @xmath16 is a random vector drawn from the multivariate gaussian distribution with the covariance matrix @xmath17 . in this paper",
    ", we only consider the prediction estimation , that is , on the estimation of @xmath9 but not @xmath10 .",
    "we use the standard total squared loss to measure the error of an estimation , that is , @xmath18    for an estimator @xmath19 , we define the expected error of @xmath20 on an input @xmath9 and on gaussian error as @xmath21    following @xcite , for @xmath22 , the risk of @xmath20 over @xmath23 is defined as @xmath24    define the minimax risk , denote by @xmath25 , as the minimum achieve - able risk among all the possible estimators , that is , @xmath26    for the aforementioned linear model with sparsity constraint @xmath27 , we have @xmath28 for an @xmath6 design matrix @xmath7 .",
    "clearly , the minimax risk @xmath29 ranges between @xmath30 and @xmath31 and depends on the structure of @xmath7 .",
    "the main goal of this paper is to design an estimator @xmath20 such that @xmath32 is close to @xmath33 for any given @xmath7 . for our main results",
    ", we consider the case where the sparsity radius @xmath34 is given .",
    "since we will only consider the prediction risk , we can assume , by rescaling @xmath7 , that @xmath35 .",
    "in what follows , we write @xmath0 for @xmath36 .",
    "in addition , we only consider the high - dimensional case where @xmath37 because for @xmath38 , we can apply a rotation to the design matrix so that the last @xmath39 rows are entirely @xmath30 .",
    "since gaussian noise is invariant under rotation , this does not affect the minimax risk , and the dimensions of the design matrix is effectively reduced to @xmath40 .",
    "we present a family of estimators , called the _ projected nearest neighbor estimator _ ( pnn ) , that can achieve nearly optimal risk for _ any _ design matrix @xmath7 and any given @xmath1 .",
    "the projected nearest neighbor estimator is a combination of two classic estimators : the _ orthogonal projection estimator _ , in which the estimation is obtained by projecting the observation @xmath3 to a properly chosen subspace , and the _ nearest neighbor estimator _ , in which @xmath3 is mapped to the closest point ( in terms of @xmath41 distance ) on the ground truth set @xmath23 .",
    "the projected nearest neighbor estimator is defined with respect to an orthogonal projection @xmath42 .",
    "it is the summation of two components : one , similar to the orthogonal projection estimator , is the projection @xmath43 of @xmath3 by @xmath42 ; the other , similar to the nearest neighbor estimator , is the nearest neighbor projection of @xmath44 on @xmath45 , where @xmath46 is the projection orthogonal to @xmath42 . as the main contribution of this work , we show that for any @xmath7 , @xmath1 , and @xmath47 , there always exists a projection @xmath42 so that the corresponding projected nearest neighbor estimator for @xmath48 is nearly minimax optimal .",
    "more precisely , we show the following theorem .",
    "notation only hides some absolute constant , that is , a constant independent of any of the parameters , such as @xmath49 . ]    [ thmmain ] for any given @xmath6 matrix @xmath7 , @xmath1 , and @xmath50 , there exists a projected nearest neighbor estimator @xmath20 such that @xmath51 where @xmath52 is a constant dependent on @xmath53 only .    in the above theorem",
    ", the projection @xmath42 is chosen in two steps : ( 1 ) for each @xmath54 , a @xmath55-dimensional projection @xmath56 is chosen to minimize @xmath57 where @xmath58 s are column vectors of @xmath7 ; ( 2 ) a proper @xmath59 is chosen to minimize the risk among all the @xmath56 s . finding the projection in step 1 turns out to be np - hard .",
    "however , by using the semi - definite programming technique in @xcite , we can compute an approximately optimal projection and therefore an approximate minimax risk in polynomial time .",
    "[ thmalgo ] for any given @xmath6 matrix @xmath7 , @xmath1 , and @xmath50 , we can compute an @xmath60 approximation is a @xmath61-approximation of @xmath62 , if @xmath63 .",
    "] of @xmath64 in polynomial time . when @xmath12 , there is a randomized polynomial time estimator that is within @xmath65 factor of the optimal .",
    "the above two results assume that the radius of @xmath0 ball is given . for @xmath12",
    ", we can extend the estimator to the adaptive case when @xmath66 is unknown .",
    "using the similar idea to the projected nearest neighbor estimator , we have that    [ thmadaptive ] there is a polynomial time adaptive estimator @xmath67 such that for any given @xmath6 matrix @xmath7 , @xmath10 , and @xmath47 , @xmath68    notice that the first term of the above error is @xmath65 factor within the oracle risk bound when @xmath66 is given .",
    "while we do not quite get the true oracle bound due to the presences of the additive term of @xmath69 , the bound becomes a true ( and nontrivial ) oracle bound for a rather large range of @xmath66 .",
    "see remark [ rmkada ] for a more detailed discussion .",
    "we provide some high level intuition of the projected nearest neighbor estimator .",
    "the orthogonal projection estimator , by projecting the observation to a chosen subspace , effectively identifies the `` leading factors '' in the ground truth set .",
    "it works well when @xmath23 is `` skewed '' .",
    "however by simple projection , it ignores the detailed local geometry of @xmath23 .",
    "this makes it less effective when @xmath23 has many constraints or has constraints involving many dimensions , for example , when @xmath23 satisfies sparse constraints . on the other hand ,",
    "the nearest neighbor estimator , by projecting to the nearest neighbor , depends more on the local geometry of @xmath23 .",
    "but it ignores the global geometry of @xmath23 so it works well when the body is not skewed along any direction . in some sense ,",
    "the projected nearest neighbor estimator achieves the optimality by taking both global and local geometry into account : it first identifies the skewed dimensions and then applies the nearest neighbor estimator to the `` residual '' space which is less biased .",
    "it is long known that the nearest neighbor estimator may be far away from the optimal when there is strong correlation among column vectors of the design matrix @xmath7 @xcite .",
    "there have been many methods proposed to deal with this problem .",
    "the projection phase can be viewed as one way to remove the correlation such that the residual vectors are less biased .",
    "this might not be obvious as the projection only minimizes the maximum of @xmath41 norm of the projection , a seemingly different quantity .",
    "however , in order for the projected vectors to be all short , they necessarily `` span '' all the directions because otherwise we could `` tilt '' the projection to reduce the longest projection . this intuition can actually be made rigorous with the help of tools from convex geometry @xcite .",
    "the technical analysis of the projected nearest neighbor estimator is inspired by two recent works , one is the analysis on the nearest neighbor estimator by raskutti , wainwright and yu @xcite ; the other is on the optimality of the orthogonal projection estimator by javanmard and the author @xcite . in @xcite , it is shown that if @xmath7 satisfies a certain isometry property , then the nearest neighbor estimator is close to optimal .",
    "on the other hand , @xcite shows that for symmetric convex bodies there always exists a projection such that the orthogonal projection estimator is close to optimal . at the very high level",
    ", we combine the analysis of these two results and show that there always exists a nearly optimal projection of @xmath7 such that the bound in @xcite is nearly optimal on the projected body .",
    "while the main machinery in our analysis is similar to what is in @xcite and @xcite , we need further insights for our problem . for the nearest neighbor analysis",
    ", we need a slightly different analysis than @xcite to obtain an upper bound suit our purpose .",
    "this also allows our result hold for all ranges of @xmath70 .",
    "the lower bound is obtained by extending the techniques in @xcite to the sets of the form @xmath71 for @xmath1 .",
    "the technique utilizes some classical results from banach space geometry , first started by bourgain and tzafriri @xcite and fully developed by szarek , talagrand , and giannopoulous @xcite .    despite its somewhat involved analysis",
    ", the projected nearest neighbor estimator suggests a quite natural heuristic : project @xmath48 to a subspace to make it more `` round '' before applying other estimators ( in our case the nearest neighbor estimator ) .",
    "this approach is probably already being used in practice . as the main result in this paper , we prove that such heuristics can actually lead to a nearly optimal estimator .",
    "in addition , a nearly optimal projection can be found in polynomial time via semi - definite programming technique in @xcite .    for the adaptive estimator",
    ", we consider the case of @xmath12 .",
    "the well - known lasso @xcite and dantzig selector @xcite can be viewed as the adaptive version of the nearest negibhor estimator . according to @xcite",
    ", these estimators can achieve an error bound dependent on @xmath66 , which is the same as the oracle risk bound of pnn when the projection is taken as the identity projection .",
    "we can apply lasso or dantzig selector to the projection of @xmath7 and to obtain the oracle risk bound of pnn under different projection dimensions .",
    "this way , we can obtain a set of estimations among which one achieves the true oracle risk bound !",
    "unfortunately , we can not reliably determine which one it is . by using ideas from hypothesis testing , we can only choose one within @xmath72 error , which accounts for the additive bound in theorem [ thmadaptive ] .    more concretely , in pnn , the optimal projection dimension is a staircase function of the parameter radius .",
    "so we try to `` guess '' @xmath66 at those critical values at which the optimal dimension changes value .",
    "the problem then reduces to a hypothesis testing problem on whether @xmath5 belongs to some convex body . by using the statistics of @xmath73",
    ", we can achieve the claimed bound .",
    "our procedure is similar in spirit to the classical lepski s recipe @xcite for converting a nonadaptive estimator to an adaptive one .",
    "but there is a significant difference as the pnn estimator is nonlinear , and the projections at different dimensions lack a nested structure . as a result ,",
    "our bound leaves an additive gap of @xmath74 .",
    "there are vast amounts of work on the minimax risk estimator .",
    "we refer to @xcite for comprehensive surveys . despite many studies on this subject , optimal or nearly optimal estimators are only known for special types of bodies .",
    "one particularly interesting case is when the parameter space is sparse .",
    "it is long known that no linear estimator works well under such constraints ( see , e.g. , @xcite ) .",
    "instead , one needs nonlinear estimator such as the thresholding estimator to achieve nearly optimal risk .",
    "recently , much attention has been paid to the ( hard ) sparsity constraint defined as the number of nonzero components , dubbed as @xmath75 quantity , of a vector .",
    "this problem , called _ compressive sensing _ in the literature , is computationally infeasible in general so the study has focused on the condition under which nearly optimal polynomial time estimator exists @xcite .",
    "the case of @xmath12 is closely related to lasso @xcite , which is the nearest neighbor estimator for the case of @xmath12 and later evolves to solving a regularized nearest neighbor problem with the @xmath2 norm penalty .",
    "while lasso has proved to be very effective , it is known that when the design matrix has strong correlation , the lasso estimator may not produce a good estimation @xcite .",
    "various methods have been proposed to remove the correlations @xcite by using different penalty terms .",
    "the projected nearest neighbor estimator can also be viewed as a way to remove correlation .",
    "the difference is that our method can be shown to be close to the optimal solution for any design matrix @xmath7 . in the projected nearest neighbor estimator",
    ", we choose the projection dimension that balance two error terms .",
    "similar technique has appeared before .",
    "for example , in @xcite , the estimation is chosen among greedy approximations of the span of vectors of varying size , and the optimal choice is by balancing two error terms . in @xcite , the dimension",
    "is controlled by a stopping rule dependent on the noise structure . despite these similarity",
    ", the optimality of the projected nearest neighbor estimator requires careful choice of the projection via solving a semi - definite program .",
    "it is unlikely that the greedy algorithm can achieve the same goal . on the other hand ,",
    "the computational efficiency of the greedy algorithm makes it ( or some variation ) an attractive practical alternative to the more complex projection phase in this paper",
    ".    many authors also consider ( arguably more flexible and realistic ) soft sparsity constraints in the form of @xmath76 for @xmath77 , the setting considered in this paper . in @xcite , asymptotically tight bounds are obtained for @xmath78 , the identity matrix . a  similar notion of roughness",
    "was studied in @xcite in which soft - thresholding estimator is shown to be nearly optimal , again for @xmath78 , but extended to more general noise and loss models . in @xcite , it is shown that there exists design matrices @xmath7 which allow fairly accurate estimation when there is no noise . in @xcite",
    ", the authors presented several upper bounds , dependent on the design matrix @xmath7 , on the loss of the lasso and dantzig selector methods when applied to soft sparsity constraints .",
    "they also show that the upper bound is nearly optimal for a family of @xmath7 s .",
    "then in  @xcite , it is shown that the nearest neighbor estimator is nearly optimal if @xmath7 satisfies certain isometry property which holds for gaussian random matrix @xmath7 . in @xcite",
    ", it is shown that for gaussian random matrix , the ( polynomial time ) @xmath2 penalized least squares is nearly optimal . despite all these studies ,",
    "no nearly optimal estimator is known for general design matrix @xmath7 .",
    "so our knowledge is limited to the case where @xmath7 is a diagonal matrix or when @xmath7 satisfies strong isometry properties . in @xcite",
    ", the authors showed a lower bound of the minimax risk on the estimation of @xmath10 for any design matrix and with the hard sparsity constraint , but it could be far away from the upper bound in general .    among previous work , @xcite is particularly relevant to our current work . in @xcite ,",
    "the authors show , among many other results , an upper bound for the nearest neighbor estimator which depends on @xmath53 and the radius of @xmath23 .",
    "while this could be far away from the optimal , it turns out if we apply proper projection of @xmath23 , the radius of the projection can be made so that the resulted bound is near optimal . for this , we follow a similar approach as @xcite , in which they show that the orthogonal projection estimator is nearly optimal for symmetric linear constraints .",
    "but we need to adapt the argument in @xcite as @xmath79 have exponentially many faces and can be nonconvex .    as mentioned earlier , the transformation from nonadaptive estimator to the adaptive one is similar to lepski s method @xcite but there are significant differences as our nonadaptive estimator does not quite satisfy the properties required by lepski s method .",
    "for a vector @xmath80 and @xmath81 , denote by @xmath82 .",
    "when @xmath83 , @xmath84 is a norm . when @xmath85 , @xmath84 is not a norm but it is quasi - convex as there is a constant @xmath61 dependent on @xmath53 such that for any @xmath86 , @xmath87 .",
    "we use @xmath88 to denote the @xmath89-dimensional @xmath53-ball with radius @xmath90 , that is , @xmath91    we often drop @xmath89 when the dimension is clear from the context .",
    "we use @xmath0 as a short hand for @xmath36 . for a set @xmath22 containing the origin , define the @xmath53-radius of @xmath23 as @xmath92 . in all these notations ,",
    "whenever @xmath53 is omitted , it means @xmath93 .",
    "we use @xmath94 to denote the distribution of @xmath95-dimensional gaussian random variable with covariance matrix @xmath96 .",
    "again , we often drop @xmath95 and @xmath97 when they are clear from the context .    as standard , @xmath98 if there exists a constant @xmath99 such that @xmath100 and @xmath101 if there exists a constant @xmath99 such that @xmath102 . throughout this paper , high probability",
    "is understood as the probability of @xmath103 .",
    "an estimator @xmath20 is a map from @xmath104 to @xmath104 : it takes a noisy observation @xmath105 of an unknown vector @xmath4 and maps it to an estimation @xmath106 .",
    "here we consider the noise drawn from @xmath107 .",
    "as described early , the risk @xmath108 of @xmath20 is defined as the maximum expected error among @xmath9 in @xmath23 , that is , @xmath109.\\ ] ]    the minimax risk of @xmath23 is defined as the minimum achievable risk for @xmath23 , that is , @xmath110 .",
    "we state a well - known lower bound on the minimax risk of euclidean balls which we will use later .",
    "[ lemlower ] @xmath111 .",
    "the orthogonal projection estimator @xmath112 is a special type of linear estimator .",
    "it is defined with respect to some linear subspace .",
    "the estimation is simply by projecting the observation @xmath113 to the subspace .",
    "let @xmath114 denotes all the @xmath55-dimensional linear subspaces in @xmath104 . for @xmath115 , we also use @xmath42 denote the orthogonal projection to @xmath42 .",
    "the estimator @xmath116 is then defined as @xmath117 .",
    "since gaussian random vector is invariant under the rotation , we have that @xmath118 , where @xmath119 denotes the @xmath120-dimensional subspace orthogonal to @xmath42 . for @xmath121 , define _ kolmogorov width _ ( as in @xcite ) as @xmath122    for @xmath41 norm , this definition is equivalent to following more convenient form , which we will use through the paper : @xmath123    clearly , @xmath124 is monotonically decreasing with @xmath55 .",
    "kolmogorov width determines the minimax risk of the orthogonal projection estimators @xcite .",
    "let @xmath125 denote the minimum risk among all the orthogonal projection estimators .",
    "[ lemtrunx ] @xmath126 .",
    "the orthogonal projection estimator is long known to be nearly optimal for ellipsoids @xcite and more generally for quadratically convex and orthosymmetric objects @xcite .",
    "however , it is also well known that the orthogonal projection estimator ( actually any linear estimator ) can be far away from optimal for the @xmath2 ball and therefore does not work well for linear regression with sparsity constraints .",
    "@xmath127      the nearest neighbor estimator is another well - known estimator .",
    "it maps an observation to the nearest point on @xmath23 , that is , @xmath128 .",
    "the nearest neighbor estimator is a nonlinear estimator and works well for `` skinny '' objects such as the @xmath2 ball .",
    "however , we can construct an example ( section [ subsecbadexample ] ) to demonstrate it is far from optimal .",
    "denote by @xmath129 the risk of the nearest neighbor estimator .",
    "[ lemnnbad ] there exist ellipsoids @xmath130 for @xmath131 such that @xmath132 .",
    "we now describe the projected nearest neighbor estimator , which is defined with respect to some low - dimensional orthogonal projection . given a @xmath55-dimensional subspace @xmath115 , we define the projected nearest neighbor estimator @xmath133 as follows .",
    "let @xmath119 denote the @xmath120-dimensional subspace orthogonal to @xmath42 . recall that we also use @xmath134 , @xmath135 to denote , respectively , the orthogonal projection to the space @xmath42 and @xmath46 .",
    "the estimator @xmath133 is defined as @xmath136    in other words , @xmath133 consists of two components , one of which is the projection to the subspace @xmath42 and the other the nearest neighbor of @xmath137 to @xmath138 .",
    "we use @xmath139 to denote the minimum risk achievable by the projected nearest neighbor estimator for given @xmath140 .    when the projection is set as the identity projection , the corresponding pnn is the same as the nearest neighbor estimator .",
    "in addition , for the same projection , the projected nearest neighbor estimator outperforms the corresponding orthogonal projection estimator .",
    "so the projected nearest neighbor estimator subsumes both the nearest neighbor and the orthogonal projection estimators . in the following , we give an example to show the projected nearest neighbor estimator can outperform both the orthogonal projection and the nearest neighbor estimators by a large factor .",
    "consider the ellipsoid defined as @xmath141    let @xmath142 with @xmath143 . by the above discussion , we can see that for the orthogonal projection estimator @xmath144 , and for the nearest neighbor estimator @xmath145 , but @xmath146 by setting @xmath42 to be the @xmath55-dimensional projection spanned by the @xmath55 long axes of @xmath147 .",
    "this demonstrates a large gap between the projected nearest neighbor estimator and both the orthogonal projection and the nearest neighbor estimators .    to study the performance of the projected nearest neighbor estimator",
    ", we first need the following error bound for the nearest neighbor estimator from  @xcite .",
    "[ proplp ] for @xmath148 , the nearest neighbor estimator @xmath149 has risk @xmath150 where @xmath151 is a constant dependent on @xmath53 only .",
    "the above bound is almost identical to theorem 4(a ) in @xcite .",
    "we will present a slightly different proof which applies to wider combination of parameters . for clarity and completeness",
    ", we present the proof in section [ subsecproplp ] . according to proposition [ proplp ] ,",
    "the error is bounded by @xmath152 .",
    "hence , if we fix the dimension of the projection in a pnn estimator , in order to minimize the risk , we should seek the projection @xmath42 that minimizes @xmath153 , that is , realizes kolmogorov width . by using this projection ,",
    "we obtain the following upper bound of the projected nearest neighbor estimator .",
    "[ corupperbound ] for any @xmath1 and any @xmath154 , @xmath155 where @xmath156 is the same as in proposition [ proplp ] .    for any fixed @xmath55 ,",
    "the error consists of two terms : @xmath157 for the projection , and @xmath158 for the nearest neighbor estimation .",
    "the second term comes from proposition [ proplp ] with @xmath159 replaced by @xmath124 if we apply the projection that realizes @xmath124 .",
    "clearly , we can choose @xmath55 with the minimum bound .",
    "to show ( [ equpperbound ] ) is nearly optimal , we prove an almost matching lower bound in terms of the kolmogorov width .",
    "this is the key technical contribution of the paper and relies on the classic restricted invertibility property developed by bourgain and tzafriri @xcite .",
    "the proof is in section [ subsecthmlowerbound ] .",
    "[ thmlowerbound ] for @xmath154 , @xmath160    theorem [ thmmain ] follows readily from corollary [ corupperbound ] and theorem [ thmlowerbound ] by setting @xmath55 to equalize two terms in ( [ eqlowerbound ] ) .",
    "the details are in section [ subsecthmmain ] .    in the proof of theorem [ thmmain ]",
    ", we choose @xmath59 such that @xmath161 .",
    "when @xmath53 goes to @xmath30 , then @xmath59 goes to @xmath162 .",
    "therefore , when @xmath53 is close to @xmath30 , the projected nearest neighbor estimator becomes the ordinary nearest neighbor algorithm . as stated in theorem 4(b ) in @xcite , the risk of the nearest neighbor estimator is @xmath163 for @xmath164 . on the other hand , if the rank of @xmath7 is at least  @xmath165 , then @xmath166 .",
    "hence , the nearest neighbor estimator ( and the projected nearest neighbor estimator ) is @xmath65 minimax for the hard sparsity constraint .",
    "this is consistent with the bound in theorem [ thmmain ] by letting @xmath167 .    in the proof of theorem [ thmlowerbound ]",
    ", we actually showed that there exists a submatrix @xmath168 which consists of @xmath169 columns of @xmath7 such that the minimax risk of @xmath170 is close to that of @xmath171 . in some sense , this means that there is a hardest sub - problem which has at most @xmath95 columns .",
    "our technique still leaves a gap of @xmath172 .",
    "we do not know if this gap is inherent to the projected nearest neighbor estimator or due to the deficiency of the analysis .",
    "we note that the upperbound can not be improved in general , as demonstrated by the example of @xmath2 ball .",
    "there might be a chance to improve the lowerbound by a factor of @xmath173 by more sophisticated techniques .",
    "but this is still insufficient to close the gap as @xmath55 might be much smaller than @xmath89 .",
    "while pnn may sound similar to the technique of low dimension projection , there are significant differences .",
    "for example , when applying low dimension projection , we typically would like to preserve the original metric structure , and often a random projection suffices . in our case , however , we would like to make the projection as small as possible , and it requires more careful selection of the projection .",
    "indeed , it is easy to show that a random projection would fail for our purpose .",
    "while the analysis of projected nearest neighbor estimators is somewhat involved , the resulted algorithm is quite straightforward .",
    "there are two separate parts in the projected nearest neighbor estimator .",
    "first , for given @xmath23 and  @xmath97 , compute the optimal projection @xmath42 and @xmath55 .",
    "second , for any observation @xmath3 , apply the projection and then compute the nearest neighbor of @xmath137 to @xmath45 .",
    "we will describe these two steps separately .",
    "for the first step , by the proof of theorem [ thmmain ] , it suffices to compute @xmath124 .",
    "this problem is however np - hard @xcite .",
    "but since @xmath154 , @xmath174 must be realized at one of @xmath89 column vectors of @xmath7 ( see the proof of lemma [ lemw ] ) .",
    "let @xmath175 be the @xmath89 column vectors of @xmath7 .",
    "then computing @xmath124 reduces to computing an @xmath120-dimensional projection @xmath176 such that @xmath177 as small as possible .",
    "this problem has been studied in @xcite , and it is shown one can compute an @xmath178 approximation by the semi - definite programming relaxation . the following proposition is the main result of @xcite .",
    "[ propsemi ] for any @xmath6 matrix @xmath7 , @xmath1 , and @xmath54,we can compute in polynomial time an @xmath178 approximation to @xmath179 .",
    "in addition , we can compute an @xmath120-dimensional subspace @xmath176 in randomized polynomial time such that with high probability , @xmath180 .    as for the second step , we need to compute the nearest neighbor on @xmath154 for any given point .",
    "this can be done by convex programming for @xmath12 .",
    "unfortunately , we do not know how to compute it efficiently for @xmath181 .",
    "so we can only claim polynomial time nearly optimal estimator for @xmath182 , as described in algorithm [ algol1 ] . for description simplicity",
    ", we have described the algorithm in which we try all @xmath183 .",
    "since @xmath124 is monotonically decreasing , the complexity can be reduced by using a binary search .",
    "theorem [ thmalgo ] follows from the above discussion .",
    "design matrix @xmath7 and observation @xmath3 .",
    "let @xmath185 be column vectors of @xmath7 .",
    "denote the set by @xmath186 ; compute a projection @xmath56 such that @xmath187 ; compute @xmath188 ; pick @xmath189 , and let @xmath190 and @xmath119 be the subspace orthogonal to  @xmath42 ; compute @xmath191 as the nearest neighbor of @xmath192 to the convex hull of @xmath193 .",
    "this can be done by using any polynomial time convex programming algorithm .",
    "set @xmath194 .",
    "the following proof summarizes our above discussion .",
    "proof of theorem [ thmalgo ] by proposition [ propsemi ] , we can compute an @xmath178 approximation @xmath195 of @xmath179 . using this approximation",
    ", we compute @xmath196    since @xmath197 for some constant @xmath99 , we have that @xmath198    by theorem [ thmmain ] , @xmath199 is an @xmath200 approximation of @xmath29 , so @xmath201 is an @xmath202 approximation of @xmath29 .",
    "when @xmath12 , by proposition [ propsemi ] , we can compute the nearly optimal projection @xmath42 and use convex programming to compute the nearest neighbor of @xmath43 to @xmath203 .",
    "the former can be done in randomized polynomial time and the latter in polynomial time .",
    "the first step of the algorithm uses the semi - definite programming relaxation to compute a nearly optimal projection of @xmath71 .",
    "while it has guaranteed approximation ratio , it can be time consuming . in practice , the projections on the principal subspaces of @xmath204 might serve as a good heuristics",
    ".    we do not have a polynomial time estimator for @xmath85 because of the lack of a polynomial time algorithm for computing the nearest neighbor to the nonconvex body of @xmath48 . while such nearest neighbor problem is hard , for our purpose an approximate nearest neighbor is sufficient .",
    "in addition , we only need to succeed in an average sense as @xmath205 for @xmath206 and @xmath16 an i.i.d .",
    "gaussian noise .",
    "it is interesting to know if there exists an efficient procedure in this particular setting .",
    "we note that this problem can be formulated under the framework of the smoothed analysis @xcite . in both cases , we are interested in minimizing the expected performance of an algorithm ( or an estimator ) in the worst case .",
    "the projected nearest neighbor estimator in the last section is nearly minimax optimal once the sparsity radius is given . in this section ,",
    "we extend the same idea to design an adaptive estimator to deal with the case when the sparsity radius is not known .",
    "write @xmath207 .",
    "ideally , one would like to achieve some kind of oracle inequality with the error bound proportional to @xmath208 , that is , the nearly optimal risk bound assuming @xmath34 is available .",
    "we can only partially achieve this goal with an extra additive term of @xmath69 . here",
    "we will focus on the case of @xmath12 for the simplicity of the exposition .",
    "again let @xmath182 .",
    "intuitively , the adaptive estimator will search for the unknown @xmath34 at some discrete values . in view of the upper bound in corollary  [ corupperbound ] , we will only try those @xmath34 s which equalize the two error terms in ( [ equpperbound ] ) .",
    "define @xmath209 for @xmath210 .",
    "@xmath211 has the following properties :    @xmath212 is monotonically increasing , since @xmath213 is nonincreasing .",
    "there is a constant @xmath99 , for @xmath214 , @xmath215 this follows from theorem [ thmlowerbound ] .",
    "further , we define @xmath56 to be the @xmath120-dimensional projection that realizes @xmath124 , that is , minimizes @xmath216 among all the @xmath120-dimensional projection .",
    "the adaptive estimator will estimate @xmath217 against @xmath218 using the nearest neighbor estimator , starting from @xmath219 .",
    "suppose that the outcome is  @xmath220 .",
    "it is easy to show that among the @xmath95 estimations @xmath220 for @xmath221 , there is one that satisfies the true oracle risk bound , that is , with high probability , there exists @xmath54 such that @xmath222    unfortunately , we can not determine reliably which one it is . instead",
    ", we can only choose one which is within @xmath72 error .",
    "this is by finding the minimum @xmath55 such that @xmath223 is not too large ( defined precisely later ) .",
    "algorithm  [ algoadaptive ] contains a formal description .",
    "design matrix @xmath7 and observation @xmath3 .",
    "estimation @xmath184 .",
    "compute the @xmath120-dimensional projection @xmath56 that approximately minimizes @xmath224 ; compute @xmath217 , @xmath225 , and @xmath226 ; set @xmath227 compute @xmath220 to be the nearest neighbor of @xmath228 on @xmath229 set @xmath230 and return ; set @xmath231 .",
    "now we will show that the estimator given in algorithm [ algoadaptive ] satisfies the bound stated in theorem [ thmadaptive ] .",
    "the proof requires some properties on @xmath232 as described in lemma [ lemada ] .",
    "denote by @xmath233 and @xmath234 .",
    "let @xmath235 denote the @xmath41 distance between @xmath236 and @xmath237 , that is , @xmath238 .",
    "[ lemada ] there are constants @xmath239 such that the following holds with high probability :    if @xmath240 , then @xmath241    if @xmath242 , then @xmath243    if @xmath244 , then @xmath245    by lemma [ lemada](1 ) and ( 2 ) , step 6 in algorithm [ algoadaptive ] serves as a test for whether @xmath236 is sufficiently separated from @xmath237 .",
    "when @xmath240 , then the test is true with high probability , and the algorithm outputs @xmath184 and returns .",
    "but when the separation between @xmath236 and @xmath237 is large enough [ @xmath246 , then step 6 would test false with high probability .",
    "theorem [ thmadaptive ] follows from lemma [ lemada ] .",
    "proof of theorem [ thmadaptive ] if the test at step 6 outputs false for some @xmath55 , then by lemma [ lemada](1 ) , @xmath247",
    ". thus @xmath248 , that is , @xmath249 . by ( [ eqck ] ) , we have that @xmath250 .    on the other hand ,",
    "if step 6 tests true for @xmath55 , then by lemma [ lemada](2 ) , @xmath251 , and by lemma [ lemada](3 ) , @xmath184 returned at step 7 satisfies that @xmath252    we distinguish three outcomes of step 6 .    * step 6 tests true for @xmath219 . in this case , @xmath253 * step 6 test true for some @xmath254 and therefore is false for @xmath255 . in this case",
    "@xmath256 and @xmath257 * step 6 is never true so step 10 is reached . in particular , the test is false for @xmath258 and hence @xmath259 but then @xmath260 .    in all the above cases , the bound in theorem [ thmadaptive ] holds .",
    "[ rmkada ] when @xmath261 , the bound ( [ eqada ] ) in theorem  [ thmadaptive ] becomes a true oracle risk bound ( within @xmath65 factor ) . in view of the proof of theorem [ thmmain ] , this happens when @xmath262 , that is , when @xmath263 .",
    "in such case , the risk ranges between @xmath264 and @xmath31 .",
    "so the bound ( [ eqada ] ) is nearly optimal and nontrivial for a rather large range of @xmath66 .    it might be possible to apply the lasso or dantzig selector estimators to the projection @xmath265 to obtain @xmath220 and then choose one @xmath220 similar to algorithm [ algoadaptive ]",
    "this would probably result in the same bound as in ( [ eqada ] ) .",
    "we choose our current exposition because lemma [ lemada](2 ) relies on the fact that @xmath220 is the nearest neighbor to @xmath266 .",
    "it is not immediately clear whether it also holds for lasso or dantzig selector .",
    "one may wonder if it is possible to get rid of @xmath264 factor and obtain a pure oracle inequality bound . if such a bound is possible , then when @xmath267 , the estimator needs to map all the observations to @xmath30 .",
    "since it is impossible to distinguish @xmath30 and a sphere with radius @xmath268 , there might be a good reason for such an additive separation to be expected .",
    "we will now construct a bad example for the nearest neighbor estimator .",
    "while it is well known that the nearest neighbor estimator can be nonoptimal , we could not find a definitive reference for a large gap . in our example",
    ", we will demonstrate a large gap of @xmath269 .",
    "consider the ellipsoid @xmath270    set @xmath271 .",
    "the orthogonal projection estimator @xmath272 has minimax error @xmath273 \\leq2.\\ ] ]    on the other hand , we show that the nearest neighbor estimator has error @xmath274 . for any @xmath275 , by using lagrangian multiplier",
    ", we have that the nearest point @xmath184 to @xmath3 on @xmath276 satisfies that @xmath277 for @xmath278 and @xmath279 .",
    "now , pick @xmath280 . then with high probability @xmath281 .",
    "by @xmath282 we have @xmath283 . but then @xmath284 for some constant @xmath285 .",
    "thus , with high probability @xmath286 .",
    "so the nearest neighbor estimator has error @xmath287 . since the projection estimator achieves the risk of @xmath288 , we have constructed an example to show that the nearest neighbor estimator can be @xmath274 factor larger than the optimal .",
    "it is well known that the error of the nearest neighbor estimator is determined by the metric structure of @xmath23 . for two bodies @xmath289 ,",
    "define the ( dyadic ) entropy number @xmath290 , for any @xmath291 , as the minimum @xmath292 such that @xmath293 can be covered by @xmath294 copies of @xmath295 .",
    "when @xmath296 is the unit @xmath41 ball , we simply write it as @xmath297 .",
    "for a random vector @xmath298 and any @xmath4 , let @xmath299 denote the random variable @xmath300 .",
    "the classical dudley bound states that there is a constant @xmath99 such that @xmath301 \\leq c \\sum_{k=0}^\\infty2^{k/2 } e_{2^k}(k).\\ ] ]    we need a slight variation of the above bound where the summation is over @xmath55 above some threshold . for @xmath302 , write @xmath303    with the above notation , the following lemma holds .    [ lemchain ] there is a constant @xmath99 , for any @xmath304 , @xmath305 \\leq\\exp\\bigl(-c",
    "t^2 2^{k(\\delta)}\\bigr).\\ ] ]    by the standard chaining argument @xcite . clearly the result holds if we replace @xmath306 with any upper bound of @xmath306 .",
    "now we prove proposition [ proplp ] . without loss of generality , we assume @xmath271 .",
    "we apply the standard technique to bound the error of the nearest neighbor estimator by the supreme of gaussian processes @xcite .",
    "the starting point is the well - known observation that for @xmath307 , @xmath308    since @xmath309 and by the quasi - convexity of @xmath310 for @xmath1 , we have that @xmath311 for @xmath312 .",
    "observe that @xmath313 is a gaussian random vector .",
    "we can bound @xmath314 through dudley bound over @xmath310 ball as follows .",
    "to apply lemma [ lemchain ] , we need an estimate on the entropy number of @xmath154 .",
    "write @xmath315 .",
    "the following is a consequence of @xcite . for completeness",
    ", we include the derivation in the .",
    "[ lemmet ] @xmath316 where @xmath317 is a constant dependent on @xmath53 only .",
    "now the crucial lemma is lemma [ lemmain ] .",
    "[ lemmain ] suppose that @xmath318 and @xmath319 , for any constant @xmath320 , there exists @xmath321 , dependent on @xmath53 and @xmath322 only , such that @xmath323 \\\\ & & \\qquad\\leq p^{-d ( { \\delta}/{\\delta } ) ^{q/2}}.\\end{aligned}\\ ] ]    the proof is by applying lemmas [ lemchain ] and [ lemmet ] . by lemma [ lemmet ] , for @xmath324 we have , @xmath325    therefore , @xmath326    by lemma [ lemmet ] , it is easily seen that for both terms , the dominant term is the first term , that is , when @xmath327 and @xmath328 , respectively . plugging in @xmath306 for these values , we have @xmath329    it is easy to verify that with @xmath330 , @xmath331 for some constant @xmath332 .",
    "so the first term dominates , that is , @xmath333    the claim now follows from lemma [ lemchain ] .    with the above preparation , we are ready to prove proposition [ proplp ] .",
    "proof of proposition [ proplp ] we assume @xmath271 . recall @xmath334 .",
    "we can further assume @xmath335 otherwise the claim follows immediately by using the trivial bound of@xmath336 . together with the assumption that @xmath337 , the upper bound in ( [ eqbound ] ) implies that @xmath338    write @xmath339 for some sufficiently large @xmath61 such that @xmath340 .",
    "this is possible as @xmath341 .",
    "hence , by applying lemma [ lemmain ] , we have that for @xmath342 and any @xmath320 there exists @xmath321 such that @xmath343 \\leq p ^{-d ( { \\delta}/{\\delta } ) ^{q/2}}.\\ ] ]    now denote by @xmath344 the following event : @xmath345    by the peeling argument we show that we can choose @xmath346 , dependent on @xmath53 only , such that @xmath347\\leq p^{-4/q}$ ] .",
    "define @xmath348    clearly @xmath349 and for any @xmath350 , @xmath351 . by these we have @xmath352 \\leq p^{-d ( \\delta/\\delta ) ^{q/2}}.\\ ] ]",
    "hence for any @xmath320 , there is @xmath321 such that @xmath353 & = & { \\operatorname{prob}}\\bigl[\\sup_{y\\in k , \\|y\\|\\geq\\delta_0 } |g_y|\\geq c(q , d ) \\sqrt { \\log p}\\delta^{{q}/({2-q})}\\|y\\|^{({2 - 2q})/({2-q})}\\bigr ] \\\\ & \\leq&\\sum_{k=0}^{\\log(\\delta/\\delta_0 ) } { \\operatorname{prob}}\\bigl[\\sup _ { y\\in \\overline{k}(2^k\\delta_0 ) } |g_y| \\geq c(q , d ) \\sqrt{\\log p}\\delta ^{{q}/({2-q})}\\|y\\|^{({2 - 2q})/({2-q})}\\bigr ] \\\\",
    "& \\leq&\\sum_{k=0}^{\\log(\\delta/\\delta_0 ) } p^{-d(\\delta /(2^k\\delta_0))^{q/2}}.\\end{aligned}\\ ] ]    now choosing @xmath354 and setting @xmath355 , we have that @xmath347=o(p^{-4/q})$ ] .",
    "let @xmath356 .",
    "so for @xmath357 , with probability @xmath358 , @xmath359    that is , @xmath360    hence with probability @xmath358 , @xmath361    since @xmath362 , we have that @xmath363 \\leq\\delta_0 ^ 2 + o\\bigl(p^{-4/q}\\cdot2p ^{2/q}\\bigr ) = o\\bigl ( \\delta^{q}(\\log p)^{1-q/2}\\bigr).\\ ] ]    for general @xmath47 , we apply the standard scaling formula of @xmath364 and complete the proof of proposition [ proplp ] .",
    "the constant of @xmath365 comes from multiplying @xmath366 and @xmath367 in lemma [ lemmet ] .      to establish the lower bound , we consider the largest euclidean ball of various dimension contained in @xmath23 .",
    "intuitively , we show that if kolmogorov width of @xmath23 is large then it has to contain a large enough euclidean ball , in terms of both radius and the dimension , which allows us to nearly match the upper bound .",
    "the crucial technical tool is the restricted invertibility result by bourgain and tzafriri @xcite and developed by szarek and talagrand @xcite and giannopoulous @xcite .    for a set of vectors @xmath368 , let @xmath369 $ ] denote the linear subspace spanned by @xmath368",
    "a set @xmath370 is called @xmath371-wide if for any @xmath372 , @xmath373)\\geq\\delta$ ] , where @xmath374 denotes the minimum distance between @xmath375 and any vector in @xmath42 .",
    "the following proposition can be gleaned from work in @xcite .",
    "see @xcite ( proposition 5.2 ) for a proof .",
    "[ prow ] for any @xmath371-wide set @xmath376 , there exists @xmath377 with @xmath378 such that for any @xmath379 , @xmath380 suppose that @xmath154 and @xmath381 .",
    "then for any , there exists @xmath382 vectors @xmath383 such that @xmath384 is @xmath124 wide .    for a set of points @xmath385 and @xmath386 , let @xmath387 denote the @xmath55-volume of the convex hull of @xmath385 .",
    "we find @xmath382 points @xmath388 in @xmath23 such that the @xmath382 volume of the simplex spanned by the origin @xmath389 and @xmath390 is the maximum , that is , @xmath391    since @xmath23 is a compact set , @xmath392 .",
    "we first show that @xmath384 is @xmath124 wide .",
    "consider the @xmath55-dimensional subspace @xmath42 spanned by @xmath393 . by the definition of @xmath213",
    ", we have @xmath394 . or equivalently @xmath395\\bigr)\\geq d_k(k).\\ ] ]    on the other hand , @xmath396\\\\[-8pt ] & & \\qquad= \\frac{1}{k+1 } { \\operatorname{vol}}_{k}(o , v_1,\\ldots , v_k ) \\cdot{\\operatorname{dist}}\\bigl(v_{k+1 } , { \\operatorname{span}}\\bigl [ \\{v_1,\\ldots , v_k\\}\\bigr]\\bigr).\\nonumber\\end{aligned}\\ ] ]    by the maximality of @xmath397 and ( [ eqmax ] ) and ( [ eqvol ] ) , we have @xmath398    repeating this argument for each @xmath399 in @xmath384 , we have that @xmath384 is @xmath124-wide .",
    "in addition , for @xmath182 , @xmath23 is the convex hull of @xmath400 .",
    "hence for any projection @xmath42 , @xmath401 has to be a vertex of @xmath23 .",
    "that is , @xmath402 .",
    "it is easy to see that @xmath384 can be chosen such that @xmath383 .",
    "since @xmath403 for @xmath85 , @xmath404 .",
    "this holds for any @xmath1 .",
    "using proposition [ prow ] and lemma [ lemw ] , we have lemma [ lemradius ] .",
    "[ lemradius ] there exists a constant @xmath99 such that for any @xmath154 , @xmath254 , and @xmath405 , there exists a linear sub - space @xmath42 such that @xmath406 contains an @xmath407-dimensional @xmath41 ball with radius @xmath408 .",
    "clearly we can assume that @xmath409 .",
    "let @xmath384 be the @xmath124-wide set as in lemma [ lemw ] .",
    "write @xmath410 . by proposition [ prow ] ,",
    "let @xmath411 be such that @xmath412 and for any @xmath413 , @xmath414    according to reverse hlder inequality , for @xmath415 and @xmath1 , @xmath416 . hence , for any @xmath417 such that @xmath418 , @xmath419\\\\[-8pt ] & \\geq&c \\sqrt{{\\varepsilon}(1-{\\varepsilon } ) } |s|^{1/2 - 1/q } d_k(k).\\nonumber\\end{aligned}\\ ] ]    let @xmath42 be the sub - space spanned by @xmath58 for @xmath420 .",
    "since @xmath421 is @xmath409 wide , they are linearly independent . that is , @xmath422 is fully ( @xmath423 ) dimensional . on the other hand by ( [ eqball ] ) for any @xmath375 on the boundary of @xmath422",
    ", we have that @xmath424    hence , @xmath422 contains an @xmath423-dimensional @xmath41 ball with radius @xmath425 the claim follows by @xmath426 and @xmath427 .    by lemma [ lemlower ] , @xmath428 .",
    "in addition , by definition of minimax risk , for any @xmath429 , @xmath430 ( see , e.g. , @xcite ) . choosing @xmath431 , we have that for @xmath154 , @xmath432      let @xmath433    when there is a tie , we pick @xmath59 to be the smallest among the ties .",
    "clearly @xmath434 since @xmath435 . when @xmath436 , it is easy to show the claim holds . for @xmath437 ,",
    "we distinguish two cases .",
    "_ case _ 1 . @xmath438 .    in this case",
    ", we have that @xmath439 .",
    "otherwise , we would have that @xmath440    this contradicts with the maximality of @xmath59 .",
    "since @xmath441 , @xmath442 .",
    "we apply the lower bound in ( [ eqlowerbound ] ) and obtain that @xmath443    for the upper bound , by taking @xmath444 in ( [ equpperbound ] ) , we have @xmath445    _ case _ 2 .",
    "@xmath446 .    in this case ,",
    "otherwise , we would have that@xmath448 and @xmath449 .",
    "the latter is due to that we pick @xmath59 the smallest @xmath55 in case there is a tie .",
    "this would imply that @xmath450    again it contradicts with the maximality of @xmath59 .",
    "hence for the lower bound , we have that @xmath451    setting @xmath452 in ( [ equpperbound ] ) , we have @xmath453    therefore , for any @xmath1 and @xmath454 , for @xmath154 where @xmath7 is an @xmath6 matrix , we have that @xmath455 .      in what follows , all the statements hold with high probability , say @xmath103 .    since @xmath456 is @xmath120-dimensional gaussian vector , by the property of @xmath457-distribution , @xmath458    since @xmath459 , the statement follows immediately .",
    "let @xmath460 denote the nearest neighbor of @xmath236 on @xmath237 .",
    "so @xmath461 .",
    "further , @xmath462    following the same analysis for the nearest neighbor estimator , we have @xmath463    hence ,",
    "@xmath464    we bound these two terms separately : @xmath465    by the analysis for the nearest neighbor estimator , we have @xmath466    putting them together , we can take @xmath467 for some sufficiently large @xmath468 and obtain @xmath243    if @xmath244 , then according to the above @xmath469    hence , @xmath470    [ app ]",
    "by guedon and litvak ( @xcite , theorem 6 ) @xmath471 where @xmath317 is a constant dependent on @xmath53 only .    and by carl and pajor @xcite , @xmath472    from the definition of @xmath473 , we have ( see also @xcite ) @xmath474    by ( [ eqent ] ) , @xmath475 .",
    "so we have @xmath476",
    "the author would like to thank adel javanmard and tong zhang for useful discussions and anonymous reviewers and the associate editor for many useful suggestions , especially for suggesting the extension to the adaptive estimation and the references of @xcite ."
  ],
  "abstract_text": [
    "<S> we present estimators for a well studied statistical estimation problem : the estimation for the linear regression model with soft sparsity constraints ( @xmath0 constraint with @xmath1 ) in the high - dimensional setting . </S>",
    "<S> we first present a family of estimators , called _ the projected nearest neighbor estimator _ and show , by using results from convex geometry , that such estimator is within a logarithmic factor of the optimal for any design matrix . </S>",
    "<S> then by utilizing a semi - definite programming relaxation technique developed in [ _ siam j. comput . </S>",
    "<S> _ * 36 * ( 2007 ) 17641776 ] , we obtain an approximation algorithm for computing the minimax risk for any such estimation task and also a polynomial time nearly optimal estimator for the important case of @xmath2 sparsity constraint . </S>",
    "<S> such results were only known before for special cases , despite decades of studies on this problem . </S>",
    "<S> we also extend the method to the adaptive case when the parameter radius is unknown . </S>"
  ]
}