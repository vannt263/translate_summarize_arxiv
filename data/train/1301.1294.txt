{
  "article_text": [
    "public clouds have been utilized by web services and internet applications widespread .",
    "they provide high degree of availability , scalability , and data durability . yet",
    ", there exists significant skew in network bound i / o performance necessitating solutions that provide robustness in a cost effective manner @xcite . in this paper , we focus on the cloud storage and present solutions that can provide much better delay performance for putting files into the cloud storage as well as for retrieving them back on demand .",
    "in particular , we base our analysis on amazon s3 service as one of the most popular cloud storage services .    a typical cloud storage stores and retrieves objects via their unique keys .",
    "each object is replicated several times within the cloud and sometimes also further protected by erasure codes to more efficiently use the storage capacity while attaining very high durability guarantees @xcite .",
    "storage provider also monitors the load on each storage node and employs dynamic load balancing to prevent hot storage nodes that might observe high loads or slow nodes that have excessively high response times . although mainly used for repairing data in unavailable storage nodes , some cloud providers also access coded blocks in parallel to uncoded blocks when uncoded blocks are stored in slow nodes @xcite .",
    "despite all these mechanisms , still evaluations of large scale systems indicate that there is a high degree of randomness in delay performance @xcite .",
    "thus , the services that require better delay performance must deploy their own solutions such as sending multiple requests ( in parallel or sequentially ) , chunking large objects into smaller ones and read / write each chunk in parallel , replicate the same object using multiple distinct keys , etc .    to this end",
    ", we conducted our own measurements on amazon s3 for various object sizes to model its delay distribution .",
    "our measurement results confirm that the delay spread is significant even when object sizes are in the order of megabytes .",
    "moreover , our study indicates that when the server accessing the storage cloud is not the bottleneck ( in terms of cpu and network access speed ) , we can substantially improve the distribution of read / write delays . to achieve these gains , one has to consider not only chunking and parallel access to each chunk , but also erasure coding .",
    "in fact without erasure coding , more chunking starts hurting the performance at lower percentile values .",
    "the gains when forward error correction ( fec ) is employed are significant in the average delay performance and they are much better at higher percentile delays .        nonetheless , server accessing the storage cloud has limited cpu and network access speed limiting the number of concurrent connections to the storage cloud without going into a processor sharing mode . with limited system capacity , one has to consider the load and its impact on queueing delays to quantify the total delay .",
    "unfortunately , fec and chunking create redundant load multiplying the arrival rate into the system . unless mean service rate is improved at the same rate , the maximum rate at which end users can be served is reduced .",
    "our observations over amazon s3 indicate that indeed lower code rates reduce the supportable rate region inducing queue instability earlier than higher code rates .",
    "thus , it is imperative to design a load adaptive strategy for changing fec rates on the fly to keep total average delays at the minimum level while remaining in the achievable rate region of the uncoded system .",
    "to have meaningful solutions , one needs to analyze the queuing delay for the system .",
    "as one of the main contributions of the paper , we analyze the average delay performance of a system that incorporates chunking , fec and multiple servers .",
    "this system model is much harder than an m / g / k queue , which itself have only crude approximations , as the service times of servers become interdependent due to the use of erasure coding . to make this point more clear ,",
    "consider the case where an object is divided into two parts and a third part is generated by bit - wise xor .",
    "if three servers are idle , then each part can be accessed in parallel .",
    "as soon as any two server complete their jobs , the third server can preempt its current job as erasure coding renders the completion of this job irrelevant .",
    "except for a very recent work @xcite that targets to solve a much simpler yet still hard case , to the best of our knowledge queuing analysis for such a system model is quite an uncharted area .",
    "our analyses provide a good approximation for capacity and mean delay for homogeneous traffic with one operation type ( e.g. , all reads ) and file size as well as for heterogeneous traffic with mixture of traffic types ( e.g. , both read and write requests with varying chunking and file sizes ) .    as another major contribution , we develop three load adaptive fec schemes that change the coding rate on the fly . using the analysis results",
    ", we can actually identify under what load regimes which fixed fec strategy provides the best average delay performance leading to simple backlog threshold based adaptive algorithms .",
    "we present two schemes bafec(for single type of requests , i.e. , homogeneous traffic ) and mbafec(for multiple types of requests , i.e. , heterogeneous traffic ) that adapt fec rates based on the queue backlog . via",
    "simulations using real service time traces from amazon s3 , we show that both schemes are able to beat the delay performance of any fixed fec rate policy while achieving the rate region of the uncoded strategy .",
    "since both bafecand mbafecrequire a priori knowledge and put constraints on service time distribution of cloud storage to compute the optimal thresholds , we also propose a greedy strategy that opportunistically determines fec rates based on the number of idle servers at the time of request arrivals .",
    "trace driven simulations demonstrate that the greedy strategy performs on a par with the queue backlog based strategies in terms of total mean delay .",
    "nonetheless , the greedy method performs significantly worse in some cases at very high percentile values ( e.g. , at 99.9th percentile ) .",
    "the remaining sections are organized as follows . in section  [ sec :",
    "system ] , we explain our system model in more details . in section  [ sec : measurement ] , we present our measurement results over amazon ec2 and s3 . in section  [ sec : singletype ] , we study the single - class scenario and develop a fec rate adaptive scheme bafecbased on the analysis , and evaluate its performance through trace - driven simulations . in section [ sec : theory ] , we generalize the analysis to multi - class scenario and develop a multi - class fec rate adaptive scheme mbafec . in section  [ sec : related ] , we cover the related literature .",
    "finally , we conclude the paper in section  [ sec : conclusion ] .",
    "fec in connection with multiple paths and/or multiple servers is a well investigated topic in the literature @xcite .",
    "however , there is very little attention devoted to the queueing delays .",
    "fec in the context of network coding or coded scheduling has also been a popular topic from the perspectives of throughput ( or network utility ) maximization and throughput vs. service delay trade - offs @xcite .",
    "although some incorporate queuing delay analysis , the treatment is largely for broadcast wireless channels with quite different system characteristics and constraints .",
    "fec has also been extensively studied in the context of distributed storage from the points of high durability and availability while attaining high storage efficiency @xcite .",
    "two papers @xcite concurrent to ours conducted theoretical study of cloud storage systems using fec in a similar fashion as we do in this paper .",
    "both papers rely on the assumption of exponential task delays , which hardly captures the reality .",
    "therefore , some of their theoretical results are over optimistic and can not be applied in practice .",
    "for example , authors of @xcite proved that using larger code lengths always improves delay without reducing system capacity , contradicting with simulation results using real - world measurements presented in this paper .",
    "another set of works that is closely related to our work looks directly into the delay performance of storage clouds @xcite .",
    "the measurements results and interim conclusions in @xcite on amazon s3 motivated our work .",
    "the paper presents the throughput - delay tradeoffs in service times as object sizes vary .",
    "they establish the skewness and long tails .",
    "they recommend to cancel long pending jobs and send a fresh request instead . although the suggestion would work well for long tails , this would not lead to much delay improvement below 99th percentile .",
    "@xcite on the other hand focuses more closely on the throughput - service delay tradeoff and devise a data batching scheme .",
    "based on the observed congestion , authors increase or reduce the batching size .",
    "thus , at high congestion , a larger batch size is used to improve the throughput while at low congestion a smaller batch size is adopted to reduce the delay .",
    "the chunk size in our work is similar to the batch size considered in @xcite and it remains as a future work how to combine these complementary ideas .",
    "the basic system architecture captures how web services today utilize public or private storage clouds .",
    "the architecture consists of proxy servers in the front end and a key - value store ( referred to as cloud storage ) in the backend .",
    "proxy servers have two main responsibilities : ( 1 ) present a rich service layer that operates on top of the raw cloud storage services / interfaces .",
    "( 2 ) optimize the user perceived performance .",
    "client requests arrive at any of the proxy servers .",
    "when client wants to upload a file , proxy server divides the file into one or more chunks .",
    "each chunk is stored as an individual object with a unique key in the key - value store .",
    "when the entire file is written successfully , the job is completed and a response is sent back to the client . when client wants to download a file , proxy server checks which chunks need to be fetched from the storage cloud .",
    "proxy generates read requests for these chunks and after receiving the complete set of chunks , the job is completed and the file is streamed back to the client .",
    "the solutions we present are deployed on the proxy server side transparent to the cloud storage .",
    "cloud storage has two main purposes : ( 1 ) provide data storage with high durability and availability .",
    "( 2 ) provide on demand scaling of storage needs .",
    "cloud storage does not interpret the objects it stores , but rather treats them as byte strings with a well - defined length . for high durability and availability , typical cloud systems",
    "replicate each object several times in different physical locations and may use fec internally . from proxy",
    "servers perspective , cloud storage is a black box whose internal techniques are unknown .",
    "proxy servers only know the response times for each query ( e.g. , putting , getting , copying , deleting objects ) it sends to the cloud storage .",
    "in our design , we employ maximum distance separable ( mds ) codes @xcite .",
    "suppose a file is divided into @xmath1 equal size chunks ( with padding ) .",
    "an ( n , k ) mds code ( e.g. , reed - soloman codes ) can expand these @xmath1 original chunks into @xmath2 coded chunks such that any @xmath1 chunks out of @xmath3 are sufficient to efficiently restore the @xmath1 original chunks ( hence the file itself ) .",
    "mds codes can help reducing the read delays as follows .",
    "suppose proxy node have already segmented the requested file into @xmath1 chunks , expanded into @xmath4 chunks using an @xmath5 mds code , and written each chunk as a separate object using a unique key into the storage cloud .",
    "when the file is to be read , proxy schedules @xmath3 read tasks for distinct chunks using @xmath3 threads ( not necessarily distinct ones ) such that @xmath6 .",
    "earliest @xmath1 successful responses from the storage cloud would then be sufficient to complete the read operation as @xmath1 chunks can be decoded to the original file chunks without requiring the remaining chunks ( thus the read tasks for those chunks can be canceled ) .",
    "notice that we implicitly assumed parallel independent task handling .",
    "if the tasks can not be served in parallel or have strong correlation in their service latencies , fec would impede the delay performance due to the extra load and processing overheads it generates .",
    "write operations are supported in a similar vein .",
    "proxy can divide the file into @xmath1 chunks of equal size and encode them into @xmath3 coded chunks .",
    "the proxy then creates @xmath3 write tasks , one for each coded chunk .",
    "it schedules the tasks using @xmath3 threads .",
    "as soon as any @xmath1 of the @xmath3 uploading tasks complete , sufficient data has been stored on the cloud storage system .",
    "thus , upon receiving @xmath1 successful responses from the storage cloud , the proxy sends a _ speculative _ success response to the client , without waiting for the remaining @xmath7 task to finish .",
    "such speculative execution is a commonly practiced optimization technique to reduce client perceived delay in many computer systems such as databases and replicated state machines @xcite , etc .",
    "depending on the subsequent read profile on the same file , the proxy can ( 1 ) continue serving the remaining tasks till all @xmath3 tasks finish , or ( 2 ) change them to low priority jobs that will be served only when system utilization is low , or ( 3 ) cancel them preemptively . the proxy can even ( 4 ) run a demon program in the background that generates all @xmath4 coded chunks from the already uploaded chunks when the system is not busy .",
    "we assume that subsequent read requests for an object that has been just written happens at time scales greater than the time necessary to commit all @xmath4 chunks . for the analysis in the later sections as well as for the proposed algorithms ,",
    "this assumption is not a real limitation . in the case of performance results , the workloads that does not conform with this assumption",
    "would have a limited transient impact that will disappear when steady state performance is considered .",
    "furthermore , in practice , such workloads are better handled by caching the most recently written objects in the proxies .",
    "due to shared resources , the level of parallelism achievable by using multiple threads is limited : the system can only support a finite number of simultaneously active threads without significantly degrading the performance of each individual active thread .",
    "thus , we denote the maximum number of simultaneously active threads allowed in our system as @xmath8 . under this constraint , we assume that the performance of each individual active thread is independent of the total number of active threads during the span of its life time .    accordingly , we model our proxy system by the queueing system shown in fig .  [",
    "fig : system ] .",
    "there are two fifo ( first - in - first - out ) queues in the system : one _ request queue _ that buffers all incoming requests that have not started yet , and one _ task queue _ that holds all waiting tasks of requests being served .",
    "@xmath8 threads are attached to the task queue . whenever a thread becomes idle",
    ", it immediately starts serving the head - of - line ( hol ) task in the task queue . the scheduler monitors the state of the queues and the threads , and decides what code rate should be used for each request in the request queue .",
    "the scheduler instructs the dispatcher to remove the hol request from the request queue only if there is at least one idle thread .",
    "the dispatcher then creates the tasks for this request according to the code rate chosen by the scheduler , and injects them into the task queue",
    ". the idle threads immediately start serving ( some of ) the newly injected tasks . at the time when a request is completed , if some of its tasks are waiting , the waiting tasks are removed from the task queue . for a completed request ,",
    "if some of its tasks are still being served , they are canceled and the threads serving them become idle .    depending on the criteria according to which the hol request of the request queue should be admitted into the task queue , scheduling policies can be classified into the two categories below . here",
    ", we assume that the scheduler has decided to serve the hol request with an @xmath9 code .    * * blocking * : the hol request is admitted into the task queue if and only if there are at least @xmath3 idle threads . * * non - blocking * : the hol request is admitted into the task queue if and only if there is at least 1 idle thread .",
    "blocking policies are not work conserving , thus waste system capacity for keeping threads idle unnecessarily .",
    "however , it has a nice structure that facilitates tractable queue analysis and provides good approximation for non - blocking policies .      in general , applications receive requests for both reading and writing for files of various sizes . from our measurement results ( next section )",
    ", it can be seen that the distributions of service times of tasks of different operation types and/or different chunk sizes differ significantly .",
    "also , requests for different applications may have different delay targets ( for example , video streaming has different delay requirements than uploading a document ) . as a result , it would be preferable to use different chunk sizes for different requests to accommodate different delay requirements .",
    "it is then natural to group requests that have the same operation type , similar file sizes and similar delay requirements into one _ class _ and consider a composition of @xmath10 classes of requests .",
    "details of modeling multiple classes of requests will be presented in section [ ssec : model : delay ] .",
    "the following discussion of this paper will concentrate on queue management and adaptation of the amount of redundant read / write operations , based on the assumption that classes are given and the corresponding file / chunk sizes are predetermined .",
    "determining the choices of these parameters as functions of different delay requirements remains part of our future work .",
    "consider a time period @xmath11 $ ] .",
    "we denote the set of requests arrived during this period by @xmath12 , where @xmath13 denotes the @xmath13-th arrived request and @xmath14 is the total requests during the period . for each request @xmath13 , denote @xmath15 as the time when it arrives into the system . given that request @xmath13 is served with an @xmath9 code , we index the corresponding @xmath3 tasks from 1 to @xmath3 , according to the time they start being served , and denote @xmath16 as their starting times",
    ". also denote @xmath17 as the completion time of task @xmath18 of request @xmath13 . note that the tasks are only ordered by their starting times but not the completion times .",
    "so it is possible that @xmath19 even if @xmath20 .",
    "the starting time of a request @xmath13 , denoted as @xmath21 , is defined as the time it gets admitted into the task queue , i.e. , the starting time of its first task .",
    "so @xmath22 .",
    "its finish / completion time , denoted as @xmath23 , should be the time when @xmath1 of its tasks have finished .",
    "let @xmath24 be the sorted permutation of the finish times of request @xmath13 s tasks .",
    "then @xmath25 .",
    "the _ queueing delay _ for request @xmath13 is the length of time that it spends waiting in the request queue , denoted by @xmath26 .",
    "the _ service delay _ for request @xmath13 is the time it spends in the system getting served , denoted by @xmath27 .",
    "we also denote the _ task delay _ for task @xmath18 of request @xmath13 by @xmath28 unless the task is canceled when the task is canceled ( because @xmath1 other tasks for the same request have completed ) , @xmath29 .",
    "to model the distributions of service times ( @xmath30 ) of individual tasks , we run measurements over amazon ec2 and s3 .",
    "ec2 instance served as our proxy node in the system model .",
    "we instantiated an extra large ec2 instance with high i / o capability in the same availability region as the s3 bucket that stores our objects .",
    "we run experiments within north california as well as tokyo regions .",
    "we benchmarked single thread vs. multiple thread environments to measure the impact of thread contention . for the machine type we used we were able to run 16 threads in parallel with almost linear gain in system throughput and observed almost identical delay distribution as single - thread .",
    "this means that for up to 16 parallel threads the bottleneck is neither in the capacity of the ec2 instance nor in the network .",
    "we conducted experiments on different week days in march , april , june , and july 2012 with various packet sizes 128byte , 1 kb , 0.5 mb , 1 mb , 2 mb , and 3 mb using 16 threads in parallel while saturating each thread .",
    "each experiment lasted around 24 hours .",
    "we alternated between different packet sizes to capture similar time of day characteristics across packet sizes .",
    "for the same reasons , we also alternated between write and read jobs by first creating a batch of write jobs using distinct keys , then creating a batch of read jobs for these distinct keys once all the writes are completed successfully . due to lack of space , we only show a limited set of results although the cross - correlation properties and cumulative distribution functions exhibit similar properties .",
    "we briefly present a representative subset of our main findings .",
    "[ fig : ccdf ] plots the complementary cumulative distribution function ( ccdf ) of @xmath30 for read and write tasks of 1 mb chunks .",
    "note that we only measure the time spent in any thread and there are no queuing delays .",
    "read tasks for small to medium object sizes experience lower mean and median delays than the write tasks , yet at higher percentile delays ( in this plot beyond 80th percentile ) reads observe higher delays . although not shown , as object size gets smaller the crossover point moves towards higher delay percentiles .",
    "we also observe negligible correlation between the service times of subsequent tasks : the pearson s correlation coefficient between the @xmath31-th and @xmath32-th tasks is always @xmath33 for all @xmath34 .",
    "this observation is critical as fec techniques would be too costly and with little benefit if there were a strong correlation .",
    "the observation holds for all the packet sizes we experimented with as well as for the write tasks .",
    "based on these results , for further analysis , we will treat task service times as independent and identically distributed ( i.i.d . ) .     for read &",
    "write tasks for 1 mb chunk . ]        to show the impact of using different codes on the service times ( i.e. , @xmath35 as opposed to @xmath30 ) , we plot the case for 2 mb files with codes ranging from @xmath36 to @xmath37 in fig .",
    "[ fig : servicetimes ] . codes @xmath38 do not employ fec , but instead use different chunk sizes .",
    "@xmath39 code provides 23% , 32% , and 56% reduction in mean , 90th percentile , and 99th percentile delays over @xmath36 using 2@xmath0 more storage .",
    "using smaller chunk sizes with fec improves delays at the same or less storage cost .",
    "e.g. , ( 3,2 ) code provides 50% , 55% , and 69% reductions in mean , 90th percentile , and 99th percentile delays over @xmath36 using 1.5@xmath0 storage , ( 5,4 ) code gives more than 60% reductions in the same percentiles using only 1.25@xmath0 storage , and ( 7,4 ) code improves delays by 76% , 80% , and 85% at the expense of 1.75@xmath0 storage . using smaller chunk sizes without fec improves mean delay performance , but at higher percentiles the benefits deteriorate .",
    "this is expected as uncoded chunking requires completion of all tasks and small chunk sizes also have a long tail .",
    "the chances of catching the tail increases as the number of chunks increases .",
    "fec greatly mitigates this all or nothing behavior .",
    "the gains in service delay @xmath40 is only half of the story as chunking and fec both adversely affect the achievable rate region as examined in later sections .      from fig .",
    "[ fig : ccdf ] , it can be observed that for both read and write tasks , despite the delay floors observed at very low percentiles , up to 99th percentile and even beyond that , the ccdf is roughly a constant term ( which probably results from unavoidable overheads in any storage system such as networking delay , protocol - processing , lock acquisitions , transaction log commits , etc . ) plus a linearly decaying term in log scale ( which is a signature for distributions having an exponential tail ) .",
    "so we decide to model the task delays as i.i.d .",
    "random variables in the form of @xmath41 , where @xmath42 is a non - negative constant ( corresponding to the constant term in ccdf ) , and @xmath43 is an exponentially distributed random variable with some mean @xmath44 ( corresponding to the linear term in ccdf ) . for mean delay analysis , our simulations later will show that this approximation works reasonably well .",
    "we assume there are @xmath10 classes of requests .",
    "requests of each class have identical file size and all are divided into chunks of identical size . under this assumption ,",
    "service times of all chunks of the same class follow the same distribution and each class @xmath45 can be characterized by a three - tuple @xmath46 , where @xmath47 and @xmath48 specifies the delay distribution of class-@xmath45 chunks . throughout this paper , we assume @xmath49 s ( and accordingly chunk sizes ) are determined a priori and @xmath50 are given .",
    "our focus will be on the adaptation / choice of @xmath51 s .",
    "in this section , we study the scenario when there is only one class of request , i.e. , @xmath52 .",
    "since there is only one class , we will drop the subscript @xmath45 within this section .",
    "we first investigate the delay and throughput tradeoff with fixed fec , i.e. , a fixed @xmath9 code is used for all requests , for both blocking and non - blocking schemes . due to the interdependent nature of task delays while employing fec ,",
    "the queueing model for these policies is much more complicated than m / g / k queue , which itself has only crude approximations for delays .",
    "we are not able to provide exact analysis at this time .",
    "however , we develop reasonable approximations for both capacity and delay of these policies .",
    "based on these approximation results , we develop a backlog - based adaptive fec scheduler bafec , which achieves the best delay performance against fixed fec schemes for all supportable arrival rates .          given our assumption that task delay is in the form of @xmath41 , it can be considered that after started being served by a thread , a task experiences two phases of services : first a fixed - time service for @xmath42 , then followed by an exponential - time service with mean @xmath44 . recall that in blocking policies , all tasks of a request @xmath45 start at the same time , i.e. , @xmath53 .",
    "then the service received by each request can be modeled in @xmath54 phases .",
    "the first is a fixed - delay phase of length @xmath42 , while all @xmath3 tasks are in their fixed - time service phase .",
    "the second is an exponential phase with mean @xmath55 , while all @xmath3 tasks are receiving exponential - time service and one task finishes by the end ; similarly , the third is an exponential phase with mean @xmath56 , while the remaining @xmath57 tasks are receiving exponential - time service and one more task finishes by the end ; @xmath58 ; the ( @xmath54)-th is an exponential phase with mean @xmath59 , while the last @xmath60 tasks receiving exponential - time service and the @xmath1-th task finishes by the end ( hence the whole request finishes and the remaining tasks are canceled ) .",
    "we will say a request is in phase @xmath42 or phase @xmath18 ( @xmath61 ) depending on the number of its remaining tasks and the phase these tasks are in .",
    "now we can model a blocking policy with the queueing system depicted in fig .",
    "[ fig : blocking ] .",
    "the fifo request queue is followed by a set of parallel pipes of servers .",
    "each pipe consists @xmath54 servers that represents the @xmath54 phases a request experiences during service : the first server has fixed service time @xmath42 consuming @xmath3 active threads , the second has exponential service time with mean @xmath55 consuming @xmath3 active threads , ... , the @xmath62-th has exponential service time with mean @xmath59 consuming @xmath60 active threads . at any time",
    ", a pipe can be `` occupied '' by at most one request , i.e. , at most one of its servers can be active .",
    "there are @xmath63 pipes in total is the maximum number of requests that can be served simultaneously by a blocking policy since every request being served consumes at least @xmath60 active threads . ] so that there will always be at least one unoccupied pipe as long as there are @xmath64 idle threads , no matter which requests these threads were serving previously .",
    "denote @xmath65 and @xmath66 ( @xmath61 ) as the number of requests being served in the corresponding phases at time @xmath31 .",
    "then the number of active threads at @xmath31 is @xmath67 . according to the definition of a blocking policy",
    ", all unoccupied pipes will be _ blocked _ for admission if @xmath68 threads are idle . whenever @xmath69 the unoccupied threads will be unblocked and the hol request in the request queue will be admitted into one of them .",
    "let @xmath70 and @xmath71 denote the time average of @xmath72 and @xmath73 .",
    "assuming the queueing system is stabilized at arrival rate @xmath74 and noticing that arrival rate to each phase equals to @xmath74 when the system is stable , we have the following flow - balance equations from little s law : @xmath75 as a result , the expected number of simultaneously active threads at arrival rate @xmath74 is @xmath76 since there are at most @xmath8 parallel active threads allowed , we have the following constraint on supportable arrival rates : @xmath77    for the study of capacity , defined as the maximum supportable arrival rate @xmath74 of the system , it suffices to consider the case when the system is always backlogged . when always - backlogged , whenever there are at least @xmath3 idle threads , the hol request will be admitted into one pipe .",
    "so the number of active threads is kept @xmath78 .",
    "then we have the following upper and lower bounds on @xmath79 , the capacity of blocking policies using a fixed @xmath9 code : @xmath80 while more accurate approximation is possible , we use the mean of the two bounds as our estimation for @xmath81 : @xmath82    from the above discussion , we can see the capacity with fixed fec is roughly proportional to the inverse of @xmath83 in fact , from our delay model , one can easily verify that @xmath84 = n\\delta + \\sum_{j = n - k+1}^n \\frac{j}{j\\mu } = u(n ) $ ] ( note that the slowest @xmath7 threads are canceled by the time @xmath1 threads finish ) .",
    "in other words , @xmath85 is the expected sum of the amount of time used by @xmath3 threads in serving one request . for this reason , we call @xmath85 the expected per - request system usage for using @xmath9 fec code , or _ usage _ for short . the first term is linear in @xmath3 and represents the constant per - thread cost @xmath42 we pay for having more parallelism . as we can see from eq.[eq : blocking : cap - bound ] ( especially upper bound ) , if @xmath42 is large compared with @xmath44 , the capacity is significantly reduced when a low rate fec code ( large @xmath3 ) is used and the queueing delay will quickly explode even at low arrival rate with respect to the capacity with no coding @xmath86",
    "we are going to investigate the delay issue in more detail in the rest of this section .      according to our model for task delay ,",
    "the expected service delay of a blocking policy is @xmath87 .    for queueing delay ,",
    "we approximate the request queue and dispatcher by a virtual single - server queue .",
    "the virtual server s service time for request @xmath13 is determined by @xmath88 , i.e. , the inter - starting time of the requests @xmath13 and @xmath89 in our original system .",
    "so from the request queue s point of view , the virtual server behaves exactly as the dispatcher , and the virtual queue has the same queueing delay as our original system .    in general , the service times of different requests in the virtual system are not necessarily independent .",
    "in fact , the service time also depends on the arrival process .",
    "so the exact analysis of the queueing delay is very complicated .",
    "we notice that at low utilization , a request will most likely find enough idle threads to start being served immediately upon arrival , as if arriving at an empty m / g/1 queue . on the other hand , when utilization is higher , the system is mostly backlogged and the inter - starting times are weakly correlated because they are determined by the rate at which busy threads become idle . based on these observations ,",
    "we use an m / g/1 queue to approximate the behavior of the request queue , wherein the service times follow an erlang distribution with parameters @xmath3 and mean @xmath90 .    to understand the choice of erlang distribution , consider the case when @xmath91 , @xmath92 .",
    "suppose the system is backlogged and all @xmath8 threads are busy immediately after @xmath21 .",
    "then @xmath93 is the time when the earliest @xmath3 out of @xmath8 threads become idle . since @xmath91 , all task delays are exponential .",
    "then the inter - starting time is the sum of @xmath3 exponential random variables , whose means are @xmath94 .",
    "this is very similar to an erlang distribution with parameter @xmath3 and mean @xmath95 , which is the sum of @xmath3 i.i.d .",
    "exponential random variables with mean @xmath96 .",
    "when @xmath97 is sufficiently large , the inter - starting time distribution converges to the erlang distribution .",
    "when @xmath98 and @xmath99 , this approximation can be quite crude .",
    "but we believe it is good enough as a guideline for policy design .",
    "moreover , it also provides a simple closed - form approximation of the queueing delay , which is used in design of our adaptive fec scheduler . given an erlang random variable @xmath100 with parameter @xmath3 and mean @xmath90 , its second moment @xmath101 = ( 1 + 1/n)/{\\tilde{c}}_b^2 $ ] .",
    "then queueing delay of the aforementioned m / g/1 queue ( using the pollaczek - khinchin formula ) is @xmath102}{2(1-\\lambda \\mathbb{e}[x ] ) }   = \\frac{\\lambda(n+1)}{2n{\\tilde{c}}_b(n , k)({\\tilde{c}}_b(n , k)-\\lambda)}. \\label{eq : blocking : apprxdelay}\\end{aligned}\\ ] ]    .range of errors : @xmath103 [ cols=\"^,^,^,^,^ \" , ]      the only difference between blocking and non - blocking policies is that non - blocking policy starts a task whenever a thread becomes available , while blocking policy waits until @xmath3 threads become available .",
    "this difference is subtle yet it makes non - blocking policies much harder than blocking policies for exact analysis . in this section ,",
    "we derive approximations of the capacity and delays of non - blocking policies .",
    "notice that , when there are @xmath8 busy threads , the rate at which any single thread becomes available is in the order of @xmath104 , which is much higher than the rate at which one particular busy thread becomes idle when @xmath8 is large . as a result , it is highly likely that , in a non - blocking policy , all tasks of a request will get started before any one of them finishes , and the gap between the first and last starting times of tasks are much smaller than the individual task delay . as a result , for large @xmath8 , a non - blocking policy behaves very similarly to a blocking policy that uses the same fec code .",
    "hence , the capacity of a non - blocking policy can be approximated by the capacity of a blocking one .",
    "further notice that , when always backlogged , non - blocking policies always keep all @xmath8 threads busy .",
    "so we approximate the capacity of non - blocking policy with the upper bound for blocking : @xmath105 then we again use pollaczek - khinchin formula to estimate the queueing delay of non - blocking policy @xmath106 , by replacing @xmath107 with @xmath108 in the previous formulation for @xmath109 , and use @xmath110 as an approximation of the service delay .",
    "by doing this , @xmath111 = u(n)/l$ ] for non - blocking .",
    "we compare the approximated delay of blocking and non - blocking policies @xmath112 and @xmath113 against the average delay from simulations using task delays in the form of @xmath114 ( denoted as @xmath115 and @xmath116 ) .",
    "table [ tab : error ] shows the range of estimation errors for @xmath117 , @xmath118 and @xmath119 , while arrival rate varies from @xmath120 to @xmath121 ( @xmath122 or @xmath123 ) . for each setting ,",
    "the lower end of estimation error is observed at low to medium arrival rates while larger error is observed for arrival rates near the estimated capacity .",
    "this is mainly due to the high sensitivity of @xmath124 in @xmath125 when @xmath126 ( because of the @xmath127 term in the denominator ) , so even a small discrepancy between @xmath125 and the actual capacity will be significantly magnified in delay at arrival rate close to capacity",
    ". as we can see , the approximations are quite reasonable , except for the cases when @xmath128 , @xmath129 and @xmath130 .",
    "this is because erlang distribution is not a good approximation for the inter - starting times when @xmath42 and @xmath3 are large compared to @xmath44 and @xmath8 , respectively .",
    "we also observe that approximation for non - blocking policy is generally better than the one for blocking policy .",
    "this is because the erlang distribution is a much better approximation for the inter - starting times of non - blocking schemes since the number of busy threads remains fixed ( equals to @xmath8 ) when the system is backlogged .",
    "we further compare the approximation against trace - driven simulations .",
    "[ fig : estimate_vs_trace ] plots @xmath131 and the average delay from simulations for reading 3 mb files with fixed fec schemes with @xmath117 , @xmath132 and @xmath128 , using traces for read operations we collected in march 2012 and chunk sizes of 1 mb . for computation of @xmath108",
    ", we first filter out the worst 0.1% task delays in the trace , then we set @xmath44 and @xmath133 as the standard deviation and the mean of the remaining task delays , respectively .",
    "we emphasize that although we use the filtered task delays to obtain estimations of @xmath42 and @xmath44 , all unfiltered task delays are used in the simulations .",
    "as we can see , our approximation matches the simulation results very well , which justifies our @xmath134 model for task delays .",
    "the simulation results also suggest that the capacity of non - blocking policies with fixed fec is a decreasing function of @xmath3 , which is consistent with our approximation of @xmath135 from eq.[eq : nonblocking : cap ] .",
    "we also plot the delay for the simple no chunking solution ( using @xmath36 code ) , as well as simple @xmath136 replication solution ( using @xmath39 code ) using traces for chunk size 3 mb collected in the same time period . despite providing a larger capacity",
    ", the simple no chunking solution has very bad delay performance .",
    "even for very low arrival rates , the delay is over 300 ms , while just chunking without fec ( @xmath137 ) improves the delay to about 200 ms with zero storage overhead and using a @xmath138 code with 1/3 storage overhead improves the delay to less than 150 ms .",
    "moreover , the simple replication of unchunked objects not only fails in improving the delay but also significantly reduces the capacity .",
    "this is because read / write operations for large object has a large constant overhead @xmath42 and a relatively small delay spread @xmath44 according to our measurement results .",
    "so there is not enough diversity to gain from parallelism .",
    "this again justifies our motivation for using chunking with fec for delay sensitive applications . with the same amount of storage overhead , using",
    "a @xmath139 code delivers roughly @xmath140 improvement in delay .          in this section ,",
    "we present bafec a backlog - based adaptive fec scheme that achieves the best delay achievable by any fixed fec scheme with @xmath141 , i.e. , @xmath142 for all supportable arrival rates .",
    "the following discussion applies to both blocking and non - blocking policies , so we drop the superscript in the delay terms .",
    "assuming @xmath1 is fixed , our estimation of the expected total delay is a function of @xmath3 and @xmath74 : @xmath143 .",
    "for every @xmath144 , we compute the solution @xmath145 such that @xmath146 in the example of fig .  [",
    "fig : estimate_vs_trace ] we show @xmath147 to @xmath148 , which are the intersection of the red dashed lines . according to our previous analysis , it only requires solving a quadratic equation of @xmath74 and only the smaller solution is meaningful . due to limitation of space",
    ", we would not include the details .",
    "@xmath145 is the crossover point for the delay performance of a @xmath9 code and a @xmath149 code : if @xmath150 , then a @xmath149 code gives smaller total delay than a @xmath9 code does ; and if @xmath151 , a @xmath9 code will give smaller total delay . using little s law , we compute the corresponding crossover backlog size @xmath152 .",
    "it is easy to show that @xmath153 is a decreasing function of @xmath3 , then we can use @xmath154 s as thresholds to adapt the fec code length based on the backlog size . the adaptive scheme is described formally as follows :    ' '' ''    _ * bafec(backlog - based adaptive fec ) * _    ' '' ''    do the following for every request @xmath13    [ 1 ]    @xmath155 backlog size upon arrival of request @xmath13 .",
    "find @xmath3 such that @xmath156 , or @xmath157 for @xmath92 , or @xmath158 for @xmath159 .",
    "serve request @xmath13 with an @xmath9 code when it becomes hol .    ' '' ''       under our delay model , for a given @xmath1 , @xmath92 provides the largest capacity region .",
    "then bafecis in fact throughput - optimal , i.e. , it supports any arrival process supportable by some @xmath9 code when @xmath141 , because it always sets @xmath92 if the queue exceeds a finite threshold @xmath153 .",
    "we conduct trace - driven simulations for performance evaluation . due to lack of space",
    ", we only show results for non - blocking versions , with @xmath117 , @xmath160 and @xmath128 , using traces we collected in march 2012 and chunk size 1 mb .",
    "results for other settings of parameters and blocking versions are similar .",
    "we also develop a simple _ greedy _ heuristic scheme .",
    "unlike bafec , greedy does not require any knowledge of the distribution of task delays , yet it achieves competitive mean delay performance . in greedy ,",
    "the code used to serve request @xmath13 is determined by the number of idle threads upon its arrival : if there are @xmath161 idle threads , use a @xmath162 code ; otherwise use a @xmath163 code , i.e. , no coding .",
    "[ fig : delay : mean ] plots the average delays of fixed fec schemes with @xmath132 , as well as the delays of greedy and bafec . as we can see , greedy and bafechave almost identical performance in terms of average delay .",
    "both adaptive schemes succeed in ( roughly ) achieving our goal of obtaining the lower envelop of the delay performance of the set of fixed fec schemes . at lower utilization ,",
    "they deliver over @xmath140 lower delay compared no chunking and simple @xmath136 replication ( @xmath164 ) , and @xmath136 lower delay compared to naive chunking ( @xmath165 ) .",
    "we plot the average and 99.9% delays of greedy and bafec , normalized by the best delays obtained from fixed fec schemes , in fig .",
    "[ fig : delay : ratio ] . at very low and high arrival rates , these two adaptive schemes perform almost the same as the optimal fixed fec scheme .",
    "this is because ( 1 ) with low arrival rates , there are no backlog most of the time and both schemes behave like a fixed fec scheme with @xmath159 ; and ( 2 ) with high arrival rates , the system is always backlogged and both schemes behave like a fixed fec scheme with @xmath92 . in the intermediate region , bafecstill traces the best performance of fixed fec schemes very well , as it is almost identical to the best fixed fec scheme in mean delay , and it stays within @xmath166 of the optimal 99.9% delay . on the other hand , while greedy also achieves almost optimal mean delay performance , it performs much worse for high percentile delays . for low to medium arrival rates , greedy",
    "is consistently above @xmath136 and can even go beyond @xmath167 of the optimal 99.9% delays .",
    "in this section , the scenario with multiple classes of requests ( @xmath168 ) is studied . as the multi - class problem is even more complicated than the single - class one",
    ", we again based our analysis on the approximations of queueing and service delays .",
    "our analysis shows that the delay - optimal combination of code lengths ( @xmath51 s ) has a well - defined structure that is helpful for designing practical rate adaptation schemes :    * there is an one - to - one mapping between the optimal code lengths and the corresponding expected total queue length ( all classes combined ) , irrespective of the arrival rates ; * the optimal code length of any class is a decreasing function of the expected total queue length .",
    "these analysis results suggest that ( 1 ) expected queue length is a good indicator of the optimal code lengths and ( 2 ) adaptation of each class can be done separately .",
    "based on these insights , we develop a multi - class backlog - based adaptive fec ( mbafec ) scheme . in mbafec , each class @xmath45 is associated with a set of thresholds computed using eq.[eq : threshold ] as in bafec , assuming the single - class scenario with only class-@xmath45 requests ; and code adaptation within each class is performed in the same way as in bafec .",
    "we assume that arrivals of each class @xmath45 follows a poisson process with rate @xmath169 , independent of other classes .",
    "so the combined arrivals consist a poisson process at rate @xmath170 .",
    "the following notations and terminologies will be used for the subsequent discussion .    *",
    "the ( column ) _ rate vector _ @xmath171 $ ] and the _ composition vector _",
    "@xmath172 = { \\hat{\\lambda}}/\\lambda$ ] .",
    "note that @xmath173 and @xmath174 . *",
    "the _ code vector _",
    "$ ] , given that @xmath51 is the code length chosen for class-@xmath45 requests . * the _ usage vector _ @xmath176 $ ] , where @xmath177 is the per - request usage of class-@xmath45 requests .",
    "when it is clear from context , we will omit the function inputs ( @xmath178 and @xmath51 ) .",
    "we can easily generalize the multi - phase queueing model introduced in the previous section ( fig .",
    "[ fig : blocking ] ) to incorporate multiple classes of requests .",
    "there is still one fifo request queue , but instead of only one type of pipes , we construct a set of pipes for every class , with the number of servers in each pipe and their service rates specified by the delay parameters of the class .",
    "a class-@xmath45 request is admitted into a pipe for class @xmath45 only if there are @xmath179 idle threads . according to little s law",
    ", we obtain flow - balance equations in the same vein as section [ ssec : capblocking ] .",
    "similar to eq.[eq : blocking : thread - bound ] , for a given code vector @xmath178 , a supportable rate vector @xmath180 must satisfy @xmath181 for system stability . starting from this point , we only consider non - blocking ( work conserving ) policies .",
    "similar to the single - class scenario , where with eq.[eq : nonblocking : cap ] the capacity region is approximated by @xmath182 , we approximate the multi - class capacity region _ with respect to @xmath178 _ by the convex set @xmath183 and the capacity for a given composition of requests @xmath184 is @xmath185 obviously , the capacity region is maximized when there is no coding , i.e. , @xmath186 $ ] .",
    "we call @xmath187 the full capacity region .",
    "similar to our previous discussion for the single - class scenario , we use a m / g/1 queue approximation to model the request queue and use pollaczek - khinchin formula to estimate the queueing delay . for a given composition vector @xmath184 , the service time of this m / g/1 queue is modeled by some random variable @xmath100 whose mean is @xmath188 = 1/c_{{\\hat{\\alpha}}}({\\hat{n } } ) = { \\hat{\\alpha}}^t { \\hat{u}}/l,\\ ] ] as per similar reason for non - blocking schemes in single - class scenario . in terms of the second moment ,",
    "one possibility is to generalize the erlang approximation for single - class and consider @xmath100 to be a mixture of different erlang random variable : with probability @xmath189 , it follows erlang distribution with parameter @xmath51 . while this is doable , it leads to a complicated expression and we believe it will only provide marginal extra insight for the purpose of scheduler design .",
    "for this reason , we make a simple and rough assumption that @xmath101 = \\beta { \\mathbb{e}}^2[x]$ ] for some constant @xmath190 independent of @xmath184 and @xmath178 .",
    "then the queueing delay is approximated by pollaczek - khinchin formula @xmath191}{2(1-\\lambda { \\mathbb{e}}[x ] ) } = \\frac{\\beta \\lambda { \\mathbb{e}}^2[x]}{2(1-\\lambda { \\mathbb{e}}[x ] ) } = \\frac{\\beta \\lambda ( { \\hat{\\alpha}}^t { \\hat{u}})^2}{2l(l-\\lambda { \\hat{\\alpha}}^t{\\hat{u}})},\\ ] ] and the expected total queue length ( counting all classes ) is @xmath192 we also approximate the service delay of each class @xmath45 by @xmath193 . noticing that requests of all classes have the same expected queueing delay , we formulate the following optimization problem of finding the best fixed fec scheme that minimizes the average delay @xmath194 worth pointing out is that we are only interested in the structure of optimal solution and will make use of it for our scheduler design rather than the accurate expression of the solution . for the following discussion , we will relax the integer requirement for @xmath51 s and allow @xmath51 to be any value @xmath195 .",
    "it is easy to verify that when @xmath196 the objective of the optimization problem eq.[eq : multi : optimization ] is strictly convex in @xmath178 . as a result",
    ", we can denote @xmath197 as the unique optimal solution for rate vector @xmath180 .",
    "also let @xmath198 .",
    "in other words , @xmath199 is the union of all rate vectors for which @xmath178 is the optimal choice of code lengths . in the case",
    "@xmath178 is not the optimal for any rate vector , @xmath200 .",
    "we say a code vector @xmath178 is _ good _ if and only if @xmath201 . theorem [ thm : optimal ] below is the main result of our analysis .",
    "[ thm : optimal ] any good code vector @xmath178 should have the structure @xmath202 where @xmath203 .",
    "for any such good code vector @xmath178 , @xmath199 is the part of the hyperplane defined by @xmath204 within the positive orthant ( @xmath205 ) , where @xmath206 is solely determined by @xmath178 . as a result",
    ", while using the optimal @xmath178 at rates @xmath207 , the corresponding queue length is a function of only @xmath178 : @xmath208    see appendix .    for any pair of good code vectors @xmath209 , define ordering `` @xmath210 '' such that @xmath211 if any only if @xmath212 .",
    "similar for `` @xmath213 '' .",
    "also , for two sets of rate vectors @xmath199 and @xmath214 , we say that @xmath215 if and only if @xmath199 is completely contained in the convex hull defined by @xmath214 and the origin .",
    "[ thm : ordered ] the set of all good code vectors is totally ordered with respect to @xmath210 . moreover , the corresponding rate vector @xmath199 and queue length @xmath216 are both decreasing functions of @xmath178 .",
    "in other words , @xmath217    see appendix .",
    "an intuitive interpretation of theorem [ thm : optimal ] and corollary [ thm : ordered ] is as follows : the full capacity region @xmath187 is `` sliced '' into layers as hyperplanes @xmath199 s .",
    "one single ( fractional ) code vector is optimal for all rates within each layer .",
    "when the optimal code vector is used , it produces identical expected queue length throughout the whole layer .",
    "the layer furthest away from the origin ( heavy workload ) corresponds to the largest expected queue length . since the arrival rates are so close to full capacity , any redundancy is detrimental hence no coding should be used .",
    "as we move to layers closer to the origin ( light workload ) , the corresponding expected queue length reduces and we can afford to increase the amount of redundancy by using coding .    .",
    "left : @xmath218 .",
    "right : @xmath219.,title=\"fig : \" ] . left : @xmath218 .",
    "right : @xmath219.,title=\"fig : \" ]     and @xmath220 .",
    "left : @xmath218 .",
    "right : @xmath219.,title=\"fig : \" ]   and @xmath220 . left : @xmath218 .",
    "right : @xmath219.,title=\"fig : \" ]    remember that theorem [ thm : optimal ] and corollary [ thm : ordered ] are derived based on non - integer relaxation of code lengths , as well as approximations of the queueing and service delay especially the assumption that @xmath101 = \\beta { \\mathbb{e}}^2[x]$ ] . to verify the validity of these results in reality ,",
    "we perform simulations with @xmath221 classes of requests , literally read and write , with @xmath222 , @xmath223 and @xmath128 , using traces we collected in march 2012 and chunk size 1 mb .",
    "we run simulations for at different rate vectors @xmath224 with @xmath225 and @xmath226 varying from @xmath227 to @xmath228 of @xmath229 and @xmath230 respectively , where @xmath231 and @xmath232 are the maximum arrival rates of read and write request the system can support . at each rate vector , we run simulations for all @xmath233 possible combinations of @xmath234 , and find the combination that produces the minimum total delay , and record the corresponding average queue length .",
    "we plot the simulation results in fig .",
    "[ fig : code_3x3 ] .",
    "the x and y axis are the arrival rates of read ( @xmath225 ) and write ( @xmath226 ) requests , respectively .",
    "the full capacity region is the lower - left half below the diagonal dark red colored line .",
    "beyond this line ( top - right ) the queue is unstable .",
    "each block in these figures represents one rate vector and the colors of a block represent the combination of @xmath218 ( left ) and @xmath219 ( right ) that results in the smallest total delay among the simulations .",
    "lightest color represents code length of 6 and the darkest represents 3 .",
    "we also plot contours of queue length levels as colored curves in which points on the same contour / curve have the same average queue length ( blue meaning small and orange meaning large ) .",
    "as we can see , except for a small number of blocks , the rate region is generally divided into 4 layers .",
    "starting from 6 coded blocks in the layer closest to the origin , the number of coded blocks decreases as moving away from the original and eventually becomes 3 in the outmost layer .",
    "the small number of blocks of exception near the boundaries are due to the integer constraint on code lengths as well as randomness in our simulations . moreover ,",
    "both the boundaries of these layers and the contours of queue lengths are roughly straight lines , and the boundaries of layers in general are aligned with the contours of queue lengths at the corresponding arrival rates ( some are not shown in the figures ) .",
    "this validates our predictions from theorem [ thm : optimal ] and corollary [ thm : ordered ] that ( 1 ) @xmath199 is a hyperplane ( which is a line in the 2-dimension space ) ; ( 2 ) @xmath216 is constant within @xmath199 ; and ( 3 ) both @xmath199 and @xmath216 are decreasing functions of @xmath178 . another observation is that as arrival rates increase , @xmath219 drops earlier than @xmath218 does .",
    "this is because , according to our trace , while read and write of 1 mb chunks have similar mean task delay ( both around 140 ms ) , @xmath235 is much larger than @xmath236 ( 114 ms vs. 61 ms ) , and as we discuss before in section [ sec : singletype ] , the queueing delay starts to dominate at lower utilization with larger @xmath42 .",
    "it appears in fig .",
    "[ fig : code_3x3 ] that all contours of queue length are roughly parallel to the boundary of the full capacity region ( the diagonal dark red line ) , which may suggest the illusion of @xmath199 being parallel to full the capacity boundary .",
    "we would point out that this is just a coincidence .",
    "in fig .  [",
    "fig : code_3x2 ] we plot the results for @xmath237 , @xmath220 , and @xmath238 .",
    "it is clear in this case that the contours are not parallel to the full capacity boundary , especially for low arrival rates .",
    "an important implication of corollary [ thm : ordered ] is that there is a one to one mapping from @xmath239 to the corresponding good code vector @xmath178 , since the set of good code vectors is totally ordered and @xmath239 is an strictly decreasing function of the good code vectors .",
    "roughly speaking , the larger @xmath239 is , the smaller ( good ) code vector should be .",
    "this suggests that generalizing the single - class scheduler bafecto accommodate multiple classes of requests is plausible .",
    "a natural and intuitive way of generalizing bafecis to first enumerate the set of good code vectors using the structure of good code vectors provided by eq.[eq : opt : structure ] , then sort these code vectors and solve for the corresponding backlog thresholds for every pair of consecutive code vectors as we did for the single - class scheduler bafec .",
    "at last , depending on which range between the thresholds the backlog size falls into , we pick the corresponding @xmath178 .",
    "however , this approach is not quite feasible when the number of classes @xmath240 is large , mainly due to the integer requirement for @xmath178 .",
    "notice that eq.[eq : opt : structure ] can be converted into a polynomial equation of @xmath51 and @xmath241 , each of degree @xmath242 and @xmath243 respectively . a straightforward way of finding the set of good codes is to first pick the code length for a certain class , say @xmath244 without loss of generality , to be an integer under consideration , then solve eq.[eq : opt : structure ] numerically for the corresponding code lengths of the other classes .",
    "however , the solutions obtained by doing this are not necessarily integers .",
    "in fact , they will most likely be non - integers unless the values of @xmath245 s happen to pair up perfectly .",
    "so for every such fractional solution of @xmath178 ( except for @xmath244 ) , we need to decide which of @xmath246 and @xmath247 to pick , for all @xmath248 .",
    "there is no obvious way to solve this other than enumerating all @xmath249 potential solutions , computing the expected delays and picking the best one .",
    "so the computational complexity is exponential in @xmath240 for each integer value of @xmath244 .",
    "such exponential complexity may be affordable for static algorithms which assume statistics of task delays ( @xmath42 and @xmath250 ) to be fixed .",
    "but in reality delay statistics of cloud storage systems vary over time and need to be updated regularly in order to harvest the best performance .",
    "more importantly , stale delay statistics can be dangerous because if they are too optimistic compared to reality then the scheduling algorithm will tend to allocate more tasks per request than it should , which will result in large backlog and queueing delay . in such cases ,",
    "the exponential complexity is forbiddingly expensive .",
    "in fact , the exponential complexity of computing the backlog thresholds can be avoided .",
    "the key is to observe that @xmath239 is also a decreasing function of each individual @xmath51 and there is also a one to one mapping from @xmath239 to @xmath51 , assuming the other classes are using the corresponding optimal code lengths .",
    "so instead of adapting @xmath178 as a whole , adaptation can be done for each @xmath51 separately .",
    "so instead of computing one set of @xmath251 backlog thresholds across which a transition in the code vector @xmath178 occurs , we compute one smaller set of @xmath252 thresholds for each class @xmath45 individually across which a transition in only @xmath51 occurs . here",
    "@xmath253 denotes the maximum number of tasks allowed for a class-@xmath45 request .",
    "these two approaches should produce the same set of thresholds but the separated approach avoids the combinatorial problem of enumerating the set of good code vectors at the first place . denoting @xmath254 as the set of thresholds computed for class @xmath45 ,",
    "the pseudo - code for the mbafecscheduler we develop using the separate approach is as follows :    ' '' ''    _ * mbafec(multi - class backlog - based adaptive fec ) * _    ' '' ''    do the following for every request @xmath13    [ 1 ]    @xmath155 backlog size upon arrival of request @xmath13 .",
    "@xmath255 class that the request @xmath13 belongs to .",
    "find @xmath3 such that @xmath256 for @xmath257 , or @xmath258 , or @xmath259 for @xmath260 .",
    "serve request @xmath45 with an @xmath261 code when it becomes hol .    ' '' ''       to compute the set of thresholds for each class , recall that @xmath216 stays fixed for all @xmath207 according to theorem [ thm : optimal ] .",
    "so it suffices to consider rate vectors along a certain direction specified by a fix composition vector @xmath184 and find the crossover backlog sizes along that direction . in particular , for class @xmath45 , we consider the direction along the @xmath45-th axis . in other words ,",
    "we consider the class-@xmath45-only arrival case with @xmath262 and @xmath263 . in the example of fig .",
    "[ fig : code_3x3 ] , this is equivalent to finding the intersections for the boundaries of layers with the x axis ( read - only arrival ) in the left plot for the thresholds of read requests , and finding the intersections with the y axis ( write - only arrival ) for write requests .",
    "further , noticing that mbafecbehaves identically to bafecwhen arrivals are single - class , these intersections with the @xmath45-th axis can be computed using eq.[eq : threshold ] , with parameters @xmath245 , just as we do for bafecin the previous section .      +    +         for performance evaluation , we perform simulations with @xmath221 classes of requests , literally read and write , with @xmath222 , @xmath223 and @xmath128 , using traces we collected in march 2012 and chunk size 1 mb .",
    "we simulated three scenarios : read heavy ( @xmath264 ) , balanced ( @xmath265 ) , and write heavy ( @xmath266 ) .",
    "we also extended greedy to accommodate multiple classes : each class-@xmath45 request uses @xmath267 or @xmath268 code , depending on the number of idle threads @xmath269 upon arrival .",
    "[ fig : multi : delay ] illustrates the delay performance for mbafecand greedy .",
    "we also run simulations with fixed fec scheme with all 16 combinations of code lengths at every arrival rate of each scenario and use the best average delays ( @xmath270 with @xmath271 = mean and 99.9% .",
    "@xmath272 and @xmath273 represent the mean and 99.9% delay for read and write requests ) , the best delays ( mean and 99.9% ) for read requests , and the best delay ( mean and 99.9% ) for write requests as baselines .",
    "we want to point out here that the combinations of code lengths that result in the best average delay , read delay , and write delay are not necessarily the same .",
    "we observe in our simulations that the combination that results in best read delay usually uses a large code length for read requests and the minimum code length for write requests @xmath274 , which results in high write delay .",
    "it is the opposite observation for codes that produce the best write delay .",
    "the combination that produces the best average delay is usually in between . so in these figures",
    ", we are comparing 6 delay metrics of one adaptive scheme mbafec(or greedy ) against multiple fixed fec schemes , each of which excels in one particular delay metric .    in the left column of fig .",
    "[ fig : multi : delay ] we plot the average delays .",
    "similar to the results for the single class case , both mbafecand greedy perform well and achieves roughly the same average mean delays as the best fixed fec schemes throughout the full capacity region .",
    "mbafecalso achieves the lower envelop of fixed fec schemes in terms of average 99.9% delay and outperforms greedy .",
    "more interesting are the middle and right columns , in which we plot the read and write delays of mbafecand greedy , normalized by the best corresponding delays with fixed fec .",
    "mbafecand greedy perform similarly in terms of mean delays and both stay within @xmath166 of the best mean delays with fixed fec .",
    "remember this comparison is made against the fixed fec scheme that produces the best mean read or write delay , which is different from the one that produces the best average delay .",
    "these two adaptive schemes perform quite differently in terms of 99.9% delays . for 99.9% write delay ,",
    "mbafecand greedy are similar and stay within @xmath166 of the best fixed fec for most arrival rates in all three scenarios . on the other hand , mbafecconstantly outperforms greedy significantly in terms of 99.9% read delays in all three scenarios .",
    "mbafecstays within @xmath166 , @xmath275 and @xmath276 of the best delay from fixed fec in read heavy , balanced and write heavy scenarios respectively , while greedy can perform as bad as @xmath277 , @xmath278 and @xmath279 in each scenario .",
    "there are two reasons for such difference of performance in read and write requests .",
    "firstly , in our trace read operations have a much larger delay spread than write operations have . as a result , read requests benefit significantly by reducing service delay from parallelism with appropriately chosen code length , while write requests can not benefit much due to its smaller delay spread .",
    "more importantly , greedy is `` class - oblivious '' and it does not make use of the difference in delay statistics of different classes of requests in deciding the code length for each class .    to better understand how mbafecand greedy behave differently , we plot the code composition ( the fractions of requests served by different code lengths ) of read and write requests using mbafecand greedy from 10% to 100% utilization levels . fig .",
    "[ fig : code_compo ] shows the code compositions for the balanced arrival scenario ( plots for read / write heavy scenarios are similar ) . at each utilization level , the four bars represent the code compositions of read requests with mbafec , write requests with mbafec , read requests with greedy , and write requests with greedy , from left to right . for each bar ,",
    "the colors represent the fraction of requests served with code length 3 , 4 , 5 and 6 , from bottom to top . generally speaking ,",
    "both schemes behave as expected : at low utilization , both schemes mostly use code length 6 since service delay dominates ; as utilization increases , both become less aggressive and increase the fraction of requests served by smaller code lengths ; at very high utilization , both reduce to no coding for both read and write requests ( @xmath280 and @xmath281 ) .",
    "the major difference we observe between mbafecand greedy is that the code compositions for read and write requests differs significantly with mbafecexcept for at very low and very high utilization levels , while they are almost identical with greedy at all utilization levels .",
    "remember that in greedy , the code length used to serve a request is determined by the number of idle threads upon arrival of the request and the range of code lengths allowed to serve the request .",
    "since we assume poisson arrivals , both read and write requests should statistically observe the same distribution of number of idle threads .",
    "also because both read and write requests have the same range of code lengths in our simulations , they result in having the same code composition .",
    "if different types of requests have different ranges of code lengths , the code compositions will be slightly different for the edge cases ( not enough idle threads or too many idle threads ) .",
    "on the other hand , mbafectreats read and write requests very differently , given that read and write operations have very different delay distributions .",
    "for read requests , since delay of read operations has a small fixed component ( @xmath236 ) and a large exponential tail ( @xmath282 ) , the overhead in queueing delay of parallelism is much smaller than the benefit from service delay .",
    "so mbafecis more aggressive in using large code lengths ( @xmath283 ) . for write requests , since write operations has a large fixed delay component , mbafecis more conservative . for medium to high utilization levels , mbafecis even more conservative than greedy for write requests ( mbafecserves fewer write requests with @xmath284 than greedy does at 80% to 100% utilization ) .",
    "we also observe that at all utilization level , greedy serves most requests with either the maximum or minimum value of @xmath3 while mbafecserves a much larger fraction of requests with medium values of @xmath3 .",
    "this all - or - nothing behavior of greedy is the main reason for its poor performance at high percentile delays , since the service delay distribution of simple chunking without coding ( @xmath285 ) is only slightly better than doing nothing ( @xmath286 ) .",
    "we presented novel solutions that combine parallel thread scheduling and fec for accessing data stored in public clouds substantially faster in the sense of mean , 90th percentile , 99th and higher percentile latencies .",
    "the solutions can be applied to other distributed data storage technologies that exhibit high delay variations for object or block storage .    in the analysis of the problem",
    ", we admitted a mixed traffic load with multiple classes of files read / write requests .",
    "but , chunk and file sizes of each class were predetermined and fixed . in general , better",
    "performance might be achieved if chunk size is also adaptable .",
    "for example , smaller @xmath1 could extend the capacity region at high utilization , and larger @xmath1 may reduce service delay at low utilization . extending the adaptation schemes in this paper to incorporate adaptive chunk sizing",
    "is the next step in our research plan .    in our work , we neglected the dollar amount cost of using redundant requests , e.g. , amazon s3 charges 0.01 $ per 1000 requests for put , copy , post , or list requests and 0.01 $ per 10,000 requests for get and all other requests .",
    "for now , by limiting the code rate and level of chunking , we put upper bounds on these costs in our work . since not all parts of data are delay sensitive , such costs can be managed by applying our techniques on a smaller fraction of the load ( e.g. , initial segments of a video file ) .",
    "extensions to capture the cloud pricing in the problem formulation and devise scheduling schemes accordingly are part of our ongoing work .",
    "first observe that the first term of the objective approaches @xmath287 as @xmath288 and the second term approaches @xmath287 as @xmath289 . since both terms are lower bounded by 0 , it follows that the objective approaches @xmath287 at the boundary of the feasible region .",
    "together with the fact that the objective is a strictly convex function of @xmath178 , it follows that for any given feasible @xmath180 , the optimal solution @xmath197 is strictly inside the feasible region .",
    "since the objective is differentiable , its partial derivative equals @xmath290 only at @xmath291 . in other words ,",
    "if for some @xmath178 @xmath292 equals to 0 for all @xmath45 , then @xmath293 . here @xmath294 .",
    "this condition is equivalent to @xmath295 due to the uniqueness of the optimal solution , the other direction is also true : for any given good code vector @xmath178 , if @xmath180 satisfies eq.[eq : opt : condition ] , then @xmath293 or equivalently @xmath207 .",
    "an important property of good code vectors implied by eq.[eq : opt : condition ] is that all good code vectors line up on the curve specified by @xmath296 given this , for any good @xmath178 , denote @xmath297 for any @xmath45 , when eq.[app : eq : opt : structure ] is satisfied .",
    "then eq.[eq : opt : condition ] can be rewritten as @xmath298 in other words , @xmath299    it is obvious that @xmath300 is strictly decreasing of @xmath301 , for all @xmath45 .",
    "so @xmath302 is invertible and for any @xmath303 in the range of @xmath302 we have @xmath304 this implies that the good code vectors are totally ordered in decreasing order of @xmath302 .",
    "consider any two good code vectors @xmath305 .",
    "for any @xmath207 , @xmath306 . note that @xmath206 is a strictly increasing function of @xmath300 , so it is a strictly decreasing function of @xmath178 .",
    "hence @xmath307 , and we have @xmath308 the first inequality is due to the fact that both @xmath180 and @xmath309 are @xmath310 .",
    "now we can conclude that any @xmath207 is strictly within the convex hull defined by @xmath214 and the origin .",
    "so @xmath215 .",
    "guanfeng liang ( s06-m12 ) received his b.e .",
    "degree from university of science and technology of china , hefei , anhui , china , in 2004 , m.a.sc .",
    "degree in electrical and computer engineering from university of toronto , canada , in 2007 , and ph.d .",
    "degree in electrical and computer engineering from the university of illinois at urbana - chanpaign , in 2012 .",
    "he currently works with docomo innovations ( formerly docomo usa labs ) , palo alto , ca , as a research engineer .",
    "ula  c.  kozat ( s97-m04-sm10 ) received his b.sc .",
    "degree in electrical and electronics engineering from bilkent university , ankara , turkey , in 1997 , m.sc .",
    "degree in electrical engineering from the george washington university , washington , dc , in 1999 , and ph.d .",
    "degree in electrical and computer engineering from the university of maryland , college park , in 2004 .",
    "he currently works at docomo innovations ( formerly docomo usa labs ) , palo alto , ca , as a principal researcher ."
  ],
  "abstract_text": [
    "<S> our paper presents solutions that can significantly improve the delay performance of putting and retrieving data in and out of cloud storage . </S>",
    "<S> we first focus on measuring the delay performance of a very popular cloud storage service amazon s3 . </S>",
    "<S> we establish that there is significant randomness in service times for reading and writing small and medium size objects when assigned distinct keys . </S>",
    "<S> we further demonstrate that using erasure coding , parallel connections to storage cloud and limited chunking ( i.e. , dividing the object into a few smaller objects ) together pushes the envelope on service time distributions significantly ( e.g. , 76% , 80% , and 85% reductions in mean , 90th , and 99th percentiles for 2 mbyte files ) at the expense of additional storage ( e.g. , 1.75@xmath0 ) . however , chunking and erasure coding increase the load and hence the queuing delays while reducing the supportable rate region in number of requests per second per node . </S>",
    "<S> thus , in the second part of our paper we focus on analyzing the delay performance when chunking , fec , and parallel connections are used together . based on this analysis , we develop load adaptive algorithms that can pick the best code rate on a per request basis by using off - line computed queue backlog thresholds . </S>",
    "<S> the solutions work with homogeneous services with fixed object sizes , chunk sizes , operation type ( e.g. , read or write ) as well as heterogeneous services with mixture of object sizes , chunk sizes , and operation types </S>",
    "<S> . we also present a simple greedy solution that opportunistically uses idle connections and picks the erasure coding rate accordingly on the fly . </S>",
    "<S> both backlog and greedy solutions support the full rate region and provide best mean delay performance when compared to the best fixed coding rate policy . </S>",
    "<S> our evaluations show that backlog based solutions achieve better delay performance at higher percentile values than the greedy solution .    </S>",
    "<S> fec , cloud storage , queueing , delay </S>"
  ]
}